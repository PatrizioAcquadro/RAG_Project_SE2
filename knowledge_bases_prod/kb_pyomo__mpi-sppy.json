[
  "def main():\n    assert(mpi.COMM_WORLD.Get_size() == 2)\n    rank = mpi.COMM_WORLD.Get_rank()\n\n    array_size = 10\n    win = mpi.Win.Allocate(mpi.DOUBLE.size*array_size, mpi.DOUBLE.size, \n                           comm=mpi.COMM_WORLD)\n    buff = np.ndarray(buffer=win.tomemory(), dtype='d', shape=(array_size,)) \n     \n    if (rank == 0):\n        buff[:] = 3. * np.ones(array_size, dtype='d')\n        time.sleep(3)\n        buff[:] = np.arange(array_size)\n\n        win.Lock(1)\n        win.Put((buff, array_size, mpi.DOUBLE), target_rank=1)\n        win.Unlock(1)\n\n    elif (rank == 1):\n        buff = np.ones(array_size, dtype='d')\n        time.sleep(1)\n\n        win.Lock(0)\n        win.Get((buff, array_size, mpi.DOUBLE), target_rank=0)\n        win.Unlock(0)\n\n        assert(buff[-1] == 3.)\n\n        time.sleep(3)\n\n        win.Lock(1)\n        win.Get((buff, array_size, mpi.DOUBLE), target_rank=1)\n        win.Unlock(1)\n\n        assert(buff[-1] == array_size-1)\n\n    del buff # Important!\n    win.Free()",
  "def setup(app):\n        app.add_css_file('theme_overrides.css')",
  "def _parse_args():\n    parser = baseparsers.make_parser(\"uc_cylinders\")\n    parser = baseparsers.two_sided_args(parser)\n    parser = baseparsers.fixer_args(parser)\n    parser = baseparsers.fwph_args(parser)\n    parser = baseparsers.lagrangian_args(parser)\n    parser = baseparsers.xhatlooper_args(parser)\n    parser = baseparsers.xhatshuffle_args(parser)\n    parser = baseparsers.cross_scenario_cuts_args(parser)\n    parser.add_argument(\"--ph-mipgaps-json\",\n                        help=\"json file with mipgap schedule (default None)\",\n                        dest=\"ph_mipgaps_json\",\n                        type=str,\n                        default=None)                \n    \n    args = parser.parse_args()\n    return args",
  "def main():\n    \n    args = _parse_args()\n\n    num_scen = args.num_scens\n\n    with_fwph = args.with_fwph\n    with_xhatlooper = args.with_xhatlooper\n    with_xhatshuffle = args.with_xhatshuffle\n    with_lagrangian = args.with_lagrangian\n    with_fixer = args.with_fixer\n    fixer_tol = args.fixer_tol\n    with_cross_scenario_cuts = args.with_cross_scenario_cuts\n\n    scensavail = [3,50,100,250,500,750,1000]\n    if num_scen not in scensavail:\n        raise RuntimeError(\"num-scen was {}, but must be in {}\".\\\n                           format(num_scen, scensavail))\n    \n    scenario_creator_kwargs = {\n        \"scenario_count\": num_scen,\n        \"path\": str(num_scen) + \"scenarios_wind\",\n    }\n    scenario_creator = uc.scenario_creator\n    scenario_denouement = uc.scenario_denouement\n    all_scenario_names = [f\"Scenario{i+1}\" for i in range(num_scen)]\n    rho_setter = uc._rho_setter\n    \n    # Things needed for vanilla cylinders\n    beans = (args, scenario_creator, scenario_denouement, all_scenario_names)\n\n    ### start ph spoke ###\n    # Start with Vanilla PH hub\n    hub_dict = vanilla.ph_hub(\n        *beans,\n        scenario_creator_kwargs=scenario_creator_kwargs,\n        ph_extensions=MultiExtension,\n        rho_setter=rho_setter,\n    )\n    # Extend and/or correct the vanilla dictionary\n    if with_fixer:\n        multi_ext = {\"ext_classes\": [Fixer, Gapper]}\n    else:\n        multi_ext = {\"ext_classes\": [Gapper]}\n    if with_cross_scenario_cuts:\n        multi_ext[\"ext_classes\"].append(CrossScenarioExtension)\n        \n    hub_dict[\"opt_kwargs\"][\"extension_kwargs\"] = multi_ext\n    if with_cross_scenario_cuts:\n        hub_dict[\"opt_kwargs\"][\"options\"][\"cross_scen_options\"]\\\n            = {\"check_bound_improve_iterations\" : args.cross_scenario_iter_cnt}\n\n    if with_fixer:\n        hub_dict[\"opt_kwargs\"][\"options\"][\"fixeroptions\"] = {\n            \"verbose\": args.with_verbose,\n            \"boundtol\": fixer_tol,\n            \"id_fix_list_fct\": uc.id_fix_list_fct,\n        }\n    if args.ph_mipgaps_json is not None:\n        with open(args.ph_mipgaps_json) as fin:\n            din = json.load(fin)\n        mipgapdict = {int(i): din[i] for i in din}\n    else:\n        mipgapdict = None\n    hub_dict[\"opt_kwargs\"][\"options\"][\"gapperoptions\"] = {\n        \"verbose\": args.with_verbose,\n        \"mipgapdict\": mipgapdict\n        }\n        \n    if args.default_rho is None:\n        # since we are using a rho_setter anyway\n        hub_dict.opt_kwargs.options[\"defaultPHrho\"] = 1  \n    ### end ph spoke ###\n    \n    # FWPH spoke\n    if with_fwph:\n        fw_spoke = vanilla.fwph_spoke(\n            *beans, scenario_creator_kwargs=scenario_creator_kwargs\n        )\n\n    # Standard Lagrangian bound spoke\n    if with_lagrangian:\n        lagrangian_spoke = vanilla.lagrangian_spoke(\n            *beans,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            rho_setter=rho_setter,\n        )\n\n    # xhat looper bound spoke\n    if with_xhatlooper:\n        xhatlooper_spoke = vanilla.xhatlooper_spoke(\n            *beans,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n        )\n\n    # xhat shuffle bound spoke\n    if with_xhatshuffle:\n        xhatshuffle_spoke = vanilla.xhatshuffle_spoke(\n            *beans,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n        )\n       \n    # cross scenario cut spoke\n    if with_cross_scenario_cuts:\n        cross_scenario_cuts_spoke = vanilla.cross_scenario_cuts_spoke(\n            *beans,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n        )\n\n    list_of_spoke_dict = list()\n    if with_fwph:\n        list_of_spoke_dict.append(fw_spoke)\n    if with_lagrangian:\n        list_of_spoke_dict.append(lagrangian_spoke)\n    if with_xhatlooper:\n        list_of_spoke_dict.append(xhatlooper_spoke)\n    if with_xhatshuffle:\n        list_of_spoke_dict.append(xhatshuffle_spoke)\n    if with_cross_scenario_cuts:\n        list_of_spoke_dict.append(cross_scenario_cuts_spoke)\n\n    spin_the_wheel(hub_dict, list_of_spoke_dict)",
  "def buildBusZone(m):\n    an_element = six.next(m.Zones.__iter__())\n    if len(m.Zones) == 1 and an_element == 'SingleZone':\n        for b in m.Buses:\n            m.BusZone[b] = an_element\n    else:\n        print(\"Multiple buses is not supported by buildBusZone in ReferenceModel.py -- someone should fix that!\")\n        exit(1)",
  "def total_load_coefficient_per_zone(m, z):\n    return sum(m.LoadCoefficient[b] for b in m.Buses if str(value(m.BusZone[b]))==str(z))",
  "def load_factors_per_bus(m,b):\n    if (m.TotalLoadCoefficientPerZone[value(m.BusZone[b])] != 0.0):\n        return m.LoadCoefficient[b]/m.TotalLoadCoefficientPerZone[value(m.BusZone[b])]\n    else:\n        return 0.0",
  "def check_stage_set(m):\n   return (len(m.StageSet) != 0)",
  "def derive_connections_to(m, b):\n   return (l for l in m.TransmissionLines if m.BusTo[l]==b)",
  "def derive_connections_from(m, b):\n   return (l for l in m.TransmissionLines if m.BusFrom[l]==b)",
  "def get_b_from_Reactance(m, l):\n    return 1/float(m.Reactance[l])",
  "def verify_thermal_generator_buses_rule(m, g):\n   for b in m.Buses:\n      if g in m.ThermalGeneratorsAtBus[b]:\n         return \n   print(\"DATA ERROR: No bus assigned for thermal generator=%s\" % g)\n   assert(False)",
  "def init_quick_start_generators(m):\n    return [g for g in m.ThermalGenerators if value(m.QuickStart[g]) == 1]",
  "def init_must_run_generators(m):\n    return [g for g in m.ThermalGenerators if value(m.MustRun[g]) == 1]",
  "def nd_gen_init(m,b):\n    return []",
  "def NonNoBus_init(m):\n    retval = set()\n    for b in m.Buses:\n        retval = retval.union([gen for gen in m.NondispatchableGeneratorsAtBus[b]])\n    return retval",
  "def form_thermal_generator_reserve_zones(m,rz):\n    return (g for g in m.ThermalGenerators if m.ReserveZoneLocation[g]==rz)",
  "def demand_per_bus_from_demand_per_zone(m,b,t):\n    return m.DemandPerZone[value(m.BusZone[b]), t] * m.LoadFactor[b]",
  "def calculate_total_demand(m, t):\n    return sum(value(m.Demand[b,t]) for b in m.Buses)",
  "def warn_about_negative_demand_rule(m, b, t):\n   this_demand = value(m.Demand[b,t])\n   if this_demand < 0.0:\n      print(\"***WARNING: The demand at bus=\"+str(b)+\" for time period=\"+str(t)+\" is negative - value=\"+str(this_demand)+\"; model=\"+str(m.name)+\".\")",
  "def populate_reserve_requirements_rule(m):\n   reserve_factor = value(m.ReserveFactor)\n   if reserve_factor > 0.0:\n      for t in m.TimePeriods:\n         demand = sum(value(m.Demand[b,t]) for b in m.Buses)\n         m.ReserveRequirement[t] = reserve_factor * demand",
  "def probability_failure_validator(m, v, g):\n   return v >= 0.0 and v <= 1.0",
  "def maximum_power_output_validator(m, v, g):\n   return v >= value(m.MinimumPowerOutput[g])",
  "def maximum_nd_output_validator(m, v, g, t):\n   return v >= value(m.MinNondispatchablePower[g,t])",
  "def ramp_limit_validator(m, v, g):\n   return True",
  "def scale_ramp_up(m, g):\n    temp = value(m.NominalRampUpLimit[g]) * m.TimePeriodLength\n    if temp > m.MaximumPowerOutput[g]:\n        return m.MaximumPowerOutput[g]\n    else:\n        return temp",
  "def scale_ramp_down(m, g):\n    temp = value(m.NominalRampDownLimit[g]) * m.TimePeriodLength\n    if temp > m.MaximumPowerOutput[g]:\n        return m.MaximumPowerOutput[g]\n    else:\n        return temp",
  "def scale_startup_limit(m, g):\n    temp = value(m.StartupRampLimit[g]) #* m.TimePeriodLength\n    if temp > m.MaximumPowerOutput[g]:\n        return m.MaximumPowerOutput[g]\n    else:\n        return temp",
  "def scale_shutdown_limit(m, g):\n    temp = value(m.ShutdownRampLimit[g]) #* m.TimePeriodLength\n    if temp > m.MaximumPowerOutput[g]:\n        return m.MaximumPowerOutput[g]\n    else:\n        return temp",
  "def scale_min_uptime(m, g):\n    scaled_up_time = int(round(m.MinimumUpTime[g] / m.TimePeriodLength))\n    return min(value(scaled_up_time), value(m.NumTimePeriods))",
  "def scale_min_downtime(m, g):\n    scaled_down_time = int(round(m.MinimumDownTime[g] / m.TimePeriodLength))\n    return min(value(scaled_down_time), value(m.NumTimePeriods))",
  "def t0_state_nonzero_validator(m, v, g):\n    return v != 0",
  "def t0_unit_on_rule(m, g):\n    return int(value(m.UnitOnT0State[g]) >= 1)",
  "def verify_must_run_t0_state_consistency_rule(m, g):\n    if value(m.MustRun[g]):\n        t0_state = value(m.UnitOnT0State[g])\n        if t0_state < 0:\n            min_down_time = value(m.MinimumDownTime[g])\n            if abs(t0_state) < min_down_time:\n                print(\"DATA ERROR: The generator %s has been flagged as must-run, but its T0 state=%d is inconsistent with its minimum down time=%d\" % (g, t0_state, min_down_time))\n                return False\n    return True",
  "def initial_time_periods_online_rule(m, g):\n   if not value(m.UnitOnT0[g]):\n      return 0\n   else:\n      return int(min(value(m.NumTimePeriods),\n             round(max(0, value(m.MinimumUpTime[g]) - value(m.UnitOnT0State[g])) / value(m.TimePeriodLength))))",
  "def initial_time_periods_offline_rule(m, g):\n   if value(m.UnitOnT0[g]):\n      return 0\n   else:\n      return int(min(value(m.NumTimePeriods),\n             round(max(0, value(m.MinimumDownTime[g]) + value(m.UnitOnT0State[g])) / value(m.TimePeriodLength))))",
  "def between_limits_validator(m, v, g):\n   status = (v <= (value(m.MaximumPowerOutput[g]) * value(m.UnitOnT0[g]))  and v >= (value(m.MinimumPowerOutput[g]) * value(m.UnitOnT0[g])))\n   if status == False:\n      print(\"Failed to validate PowerGeneratedT0 value for g=\"+g+\"; new value=\"+str(v)+\", UnitOnT0=\"+str(value(m.UnitOnT0[g])))\n   return v <= (value(m.MaximumPowerOutput[g]) * value(m.UnitOnT0[g]))  and v >= (value(m.MinimumPowerOutput[g]) * value(m.UnitOnT0[g]))",
  "def piecewise_type_validator(m, v):\n   return (v == \"NoPiecewise\") or (v == \"Absolute\") or (v == \"Incremental\") or (v != \"SubSegementation\")",
  "def piecewise_type_init(m):\n    boo = False\n    for g in m.ThermalGenerators:\n        if not (m.ProductionCostA0[g] == 0.0 and m.ProductionCostA1[g] == 0.0 and m.ProductionCostA2[g] == 0.0):\n            boo = True\n            break\n    if boo:\n        return \"NoPiecewise\"\n    else:\n        return \"Absolute\"",
  "def piecewise_init(m, g):\n    return []",
  "def validate_cost_piecewise_points_and_values_rule(m, g):\n\n    # IGNACIO: LOOK HERE - CAN YOU MERGE IN THE FOLLOWING INTO THIS RULE?\n#    if m.CostPiecewisePoints[g][1] > m.CorrectCostPiecewisePoints[g][1]:\n#        this_generator_piecewise_values.insert(0,0)\n#    if m.CostPiecewisePoints[g][len(m.CostPiecewisePoints[g])] < m.CorrectCostPiecewisePoints[g][len(m.CorrectCostPiecewisePoints[g])]:\n#        this_generator_piecewise_values.append(this_generator_piecewise_values[-1] + 1)\n    \n    if value(m.PiecewiseType) == \"NoPiecewise\":\n        # if there isn't any piecewise data specified, we shouldn't find any.\n        if len(m.CostPiecewisePoints[g]) > 0:\n            print(\"DATA ERROR: The PiecewiseType parameter was set to NoPiecewise, but piecewise point data was specified!\")\n            return False\n        # if there isn't anything to validate and we didn't expect piecewise \n        # points, we can safely skip the remaining validation steps.\n        return True\n    else:\n        # if the user said there was going to be piecewise data and none was \n        # supplied, they should be notified as to such.\n        if len(m.CostPiecewisePoints[g]) == 0:\n            print(\"DATA ERROR: The PiecewiseType parameter was set to something other than NoPiecewise, but no piecewise point data was specified!\")\n            return False\n\n   # per the requirement below, there have to be at least two piecewise points if there are any.\n\n    min_output = value(m.MinimumPowerOutput[g])\n    max_output = value(m.MaximumPowerOutput[g])   \n\n    new_points = sorted(list(m.CostPiecewisePoints[g]))\n    new_values = sorted(list(m.CostPiecewiseValues[g]))\n\n    if min_output not in new_points:\n        print(\"DATA WARNING: Cost piecewise points for generator g=\"+str(g)+\" must contain the minimum output level=\"+str(min_output)+\" - so we added it.\")\n        new_points.insert(0, min_output)\n\n    if max_output not in new_points:\n        print(\"DATA WARNING: Cost piecewise points for generator g=\"+str(g)+\" must contain the maximum output level=\"+str(max_output)+\" - so we added it.\")\n        new_points.append(max_output)\n\n    # We delete those values which are not in the interval [min_output, max_output]\n    new_points = [new_points[i] for i in range(len(new_points)) if (min_output <= new_points[i] and new_points[i] <= max_output)]\n\n    # We have to make sure that we have the same number of Points and Values\n    if len(new_points) < len(new_values): # if the number of points is less than the number of values, we take the first len(new_points) elements of new_values\n        new_values = [new_values[i] for i in range(len(new_points))]\n    if len(new_points) > len(new_values): # if the number of values is lower, then we add values at the end of new_values increasing by 1 each time.\n        i = 1\n        while len(new_points) != len(new_values):\n            new_values.append(new_values[-1] + i)\n            i += 1 \n\n    if list(m.CostPiecewisePoints[g]) != new_points:\n        m.CostPiecewisePoints[g].clear()\n        #m.CostPiecewisePoints[g].add(*new_points) # dlw and Julia July 2014 - changed to below.\n        for pcwpoint in new_points:\n            m.CostPiecewisePoints[g].add(pcwpoint) \n\n    if list(m.CostPiecewiseValues[g]) != new_values:\n        m.CostPiecewiseValues[g].clear()\n        # m.CostPiecewiseValues[g].add(*new_values) # dlw and Julia July 2014 - changed to below.\n        for pcwvalue in new_values:\n            m.CostPiecewiseValues[g].add(pcwvalue)\n\n    return True",
  "def minimum_production_cost(m, g):\n    if len(m.CostPiecewisePoints[g]) > 1:\n        return m.CostPiecewiseValues[g].first() * m.FuelCost[g]\n    else:\n        return  m.FuelCost[g] * \\\n               (m.ProductionCostA0[g] + \\\n                m.ProductionCostA1[g] * m.MinimumPowerOutput[g] + \\\n                m.ProductionCostA2[g] * (m.MinimumPowerOutput[g]**2))",
  "def power_generation_piecewise_points_rule(m, g, t):\n\n    # factor out the fuel cost here, as the piecewise approximation is scaled by fuel cost\n    # elsewhere in the model (i.e., in the Piecewise construct below).\n    minimum_production_cost = value(m.MinimumProductionCost[g]) / value(m.FuelCost[g])\n\n    # minimum output\n    minimum_power_output = value(m.MinimumPowerOutput[g])\n    \n    piecewise_type = value(m.PiecewiseType)\n\n    if piecewise_type == \"Absolute\":\n\n       piecewise_values = list(m.CostPiecewiseValues[g])\n       piecewise_points = list(m.CostPiecewisePoints[g])\n       m.PowerGenerationPiecewiseValues[g,t] = {}\n       m.PowerGenerationPiecewisePoints[g,t] = [] \n       for i in range(len(piecewise_points)):\n          this_point = piecewise_points[i] - minimum_power_output\n          m.PowerGenerationPiecewisePoints[g,t].append(this_point)\n          m.PowerGenerationPiecewiseValues[g,t][this_point] = piecewise_values[i] - minimum_production_cost\n\n\n    elif piecewise_type == \"Incremental\":\n       # NOTE: THIS DOESN'T WORK!!!\n       if len(m.CostPiecewisePoints[g]) > 0:\n          PowerBreakpoints = list(m.CostPiecewisePoints[g])\n          ## adjust these down to min power\n          for i in range(len(PowerBreakpoints)):\n              PowerBreakpoints[i] = PowerBreakpoints[i] - minimum_power_output \n          assert(PowerBreakPoint[0] == 0.0) \n          IncrementalCost = list(m.CostPiecewiseValues[g])\n          CostBreakpoints = {}\n          CostBreakpoints[0] = 0\n          for i in range(1, len(IncrementalCost) + 1):\n             CostBreakpoints[PowerBreakpoints[i]] = CostBreakpoints[PowerBreakpoints[i-1]] + \\\n                 (PowerBreakpoints[i] - PowerBreakpoints[i-1])* IncrementalCost[i-1]\n          m.PowerGenerationPiecewisePoints[g,t] = list(PowerBreakpoints)\n          m.PowerGenerationPiecewiseValues[g,t] = dict(CostBreakpoints)\n          #        print g, t, m.PowerGenerationPiecewisePoints[g, t]\n          #        print g, t, m.PowerGenerationPiecewiseValues[g, t]\n\n       else:\n          print(\"***BADGenerators must have at least 1 point in their incremental cost curve\")\n          assert(False)\n\n    elif piecewise_type == \"SubSegmentation\":\n\n       print(\"***BAD - we don't have logic for generating piecewise points of type SubSegmentation!!!\")\n       assert(False)\n\n    else: # piecewise_type == \"NoPiecewise\"\n\n       if value(m.ProductionCostA2[g]) == 0:\n          # If cost is linear, we only need two points -- (0,0) and (MaxOutput-MinOutput, MaxCost-MinCost))\n          min_power = value(m.MinimumPowerOutput[g])\n          max_power = value(m.MaximumPowerOutput[g])\n          if min_power == max_power:\n             m.PowerGenerationPiecewisePoints[g, t] = [0.0]\n          else:\n             m.PowerGenerationPiecewisePoints[g, t] = [0.0, max_power-min_power]\n\n          m.PowerGenerationPiecewiseValues[g,t] = {}\n\n          m.PowerGenerationPiecewiseValues[g,t][0.0] = 0.0\n\n          if min_power != max_power:\n             m.PowerGenerationPiecewiseValues[g,t][max_power-min_power] = \\\n                 value(m.ProductionCostA0[g]) + \\\n                 value(m.ProductionCostA1[g]) * max_power \\\n                 - minimum_production_cost\n\n       else:\n           min_power = value(m.MinimumPowerOutput[g])\n           max_power = value(m.MaximumPowerOutput[g])\n           n = value(m.NumGeneratorCostCurvePieces)\n           width = (max_power - min_power) / float(n)\n           if width == 0:\n               m.PowerGenerationPiecewisePoints[g, t] = [0]\n           else:\n               m.PowerGenerationPiecewisePoints[g, t] = []\n               m.PowerGenerationPiecewisePoints[g, t].extend([0 + i*width for i in range(0,n+1)])\n               # NOTE: due to numerical precision limitations, the last point in the x-domain\n               #       of the generation piecewise cost curve may not be precisely equal to the \n               #       maximum power output level of the generator. this can cause Piecewise to\n               #       sqawk, as it would like the upper bound of the variable to be represented\n               #       in the domain. so, we will make it so.\n               m.PowerGenerationPiecewisePoints[g, t][-1] = max_power - min_power\n           m.PowerGenerationPiecewiseValues[g,t] = {}\n           for i in range(len(m.PowerGenerationPiecewisePoints[g, t])):\n               m.PowerGenerationPiecewiseValues[g,t][m.PowerGenerationPiecewisePoints[g,t][i]] = \\\n                          value(m.ProductionCostA0[g]) + \\\n                          value(m.ProductionCostA1[g]) * (m.PowerGenerationPiecewisePoints[g, t][i] + min_power) + \\\n                          value(m.ProductionCostA2[g]) * (m.PowerGenerationPiecewisePoints[g, t][i] + min_power)**2 \\\n                          - minimum_production_cost\n           assert(m.PowerGenerationPiecewisePoints[g, t][0] == 0)\n    \n    # validate the computed points, independent of the method used to generate them.\n    # nothing should be negative, and the costs should be monotonically non-decreasing.\n    for i in range(0, len(m.PowerGenerationPiecewisePoints[g, t])):\n       this_level = m.PowerGenerationPiecewisePoints[g, t][i]\n       assert this_level >= 0.0",
  "def startup_lags_init_rule(m, g):\n   return [value(m.MinimumDownTime[g])]",
  "def startup_costs_init_rule(m, g):\n   return [0.0]",
  "def validate_startup_lags_rule(m, g):\n   startup_lags = list(m.StartupLags[g])\n\n   if len(startup_lags) == 0:\n      print(\"DATA ERROR: The number of startup lags for thermal generator=\"+str(g)+\" must be >= 1.\")\n      assert(False)\n\n   if startup_lags[0] != value(m.MinimumDownTime[g]):\n      print(\"DATA ERROR: The first startup lag for thermal generator=\"+str(g)+\" must be equal the minimum down time=\"+str(value(m.MinimumDownTime[g]))+\".\")\n      assert(False)      \n\n   for i in range(0, len(startup_lags)-1):\n      if startup_lags[i] >= startup_lags[i+1]:\n         print(\"DATA ERROR: Startup lags for thermal generator=\"+str(g)+\" must be monotonically increasing.\")\n         assert(False)",
  "def validate_startup_costs_rule(m, g):\n   startup_costs = list(m.StartupCosts[g])\n   for i in range(0, len(startup_costs)-2):\n      if startup_costs[i] > startup_costs[i+1]:\n         print(\"DATA ERROR: Startup costs for thermal generator=\"+str(g)+\" must be monotonically non-decreasing.\")\n         assert(False)",
  "def validate_startup_lag_cost_cardinalities(m, g):\n   if len(m.StartupLags[g]) != len(m.StartupCosts[g]):\n      print(\"DATA ERROR: The number of startup lag entries (\"+str(len(m.StartupLags[g]))+\") for thermal generator=\"+str(g)+\" must equal the number of startup cost entries (\"+str(len(m.StartupCosts[g]))+\")\")\n      assert(False)",
  "def startup_cost_indices_init_rule(m, g):\n   return range(1, len(m.StartupLags[g])+1)",
  "def ValidShutdownTimePeriods_generator(m,g):\n                                    ## adds the necessary index for starting-up after a shutdown before the time horizon began\n    return (t for t in (list(m.TimePeriods)+([] if (value(m.UnitOnT0State[g]) >= 0) else [m.InitialTime + value(m.UnitOnT0State[g])])))",
  "def ShutdownHotStartupPairs_generator(m,g):\n    return ((t_prime, t) for t_prime in m.ValidShutdownTimePeriods[g] for t in m.TimePeriods if (m.StartupLags[g].first() <= t - t_prime < m.StartupLags[g].last()))",
  "def StartupIndicator_domain_generator(m):\n    return ((g,t_prime,t) for g in m.ThermalGenerators for t_prime,t in m.ShutdownHotStartupPairs[g])",
  "def power_bounds_rule(m, g, t):\n    return (0, m.MaximumPowerOutput[g]-m.MinimumPowerOutput[g])",
  "def power_generated_expr_rule(m, g, t):\n    return m.PowerGeneratedAboveMinimum[g,t] + m.MinimumPowerOutput[g]*m.UnitOn[g,t]",
  "def line_power_bounds_rule(m, l, t):\n   return (-m.ThermalLimit[l], m.ThermalLimit[l])",
  "def nd_bounds_rule(m,n,t):\n    return (m.MinNondispatchablePower[n,t], m.MaxNondispatchablePower[n,t])",
  "def maximum_power_avaiable_expr_rule(m, g, t):\n    return m.MaximumPowerAvailableAboveMinimum[g,t] + m.MinimumPowerOutput[g]*m.UnitOn[g,t]",
  "def fix_first_angle_rule(m,t):\n    first_bus = six.next(m.Buses.__iter__())\n    return m.Angle[first_bus,t] == 0.0",
  "def pos_load_generate_mismatch_tolerance_rule(m, b):\n   return sum((m.posLoadGenerateMismatch[b,t] for t in m.TimePeriods)) >= 0.0",
  "def neg_load_generate_mismatch_tolerance_rule(m, b):\n   return sum((m.negLoadGenerateMismatch[b,t] for t in m.TimePeriods)) >= 0.0",
  "def line_power_rule(m, l, t):\n   return m.LinePower[l,t] == m.B[l] * (m.Angle[m.BusFrom[l], t] - m.Angle[m.BusTo[l], t])",
  "def power_balance(m, b, t):\n    # bus b, time t (S)\n    if storage_services:\n        return sum((1 - m.GeneratorForcedOutage[g,t]) * m.PowerGenerated[g, t] for g in m.ThermalGeneratorsAtBus[b]) \\\n            + sum(m.PowerOutputStorage[s, t]*m.OutputEfficiencyEnergy[s] for s in m.StorageAtBus[b])\\\n            - sum(m.PowerInputStorage[s, t] for s in m.StorageAtBus[b])\\\n            + sum(m.NondispatchablePowerUsed[g, t] for g in m.NondispatchableGeneratorsAtBus[b]) \\\n            + sum(m.LinePower[l,t] for l in m.LinesTo[b]) \\\n            - sum(m.LinePower[l,t] for l in m.LinesFrom[b]) \\\n            + m.LoadGenerateMismatch[b,t] \\\n            == m.Demand[b, t] \n    else:\n        return sum((1 - m.GeneratorForcedOutage[g,t]) * m.PowerGenerated[g, t] for g in m.ThermalGeneratorsAtBus[b]) \\\n            + sum(m.NondispatchablePowerUsed[g, t] for g in m.NondispatchableGeneratorsAtBus[b]) \\\n            + sum(m.LinePower[l,t] for l in m.LinesTo[b]) \\\n            - sum(m.LinePower[l,t] for l in m.LinesFrom[b]) \\\n            + m.LoadGenerateMismatch[b,t] \\\n            == m.Demand[b, t]",
  "def define_pos_neg_load_generate_mismatch_rule(m, b, t):\n    return m.posLoadGenerateMismatch[b, t] - m.negLoadGenerateMismatch[b, t] == m.LoadGenerateMismatch[b, t]",
  "def bound_reserve_shortfall_rule(m, t):\n    return m.ReserveShortfall[t] <= m.ReserveRequirement[t]",
  "def enforce_reserve_requirements_rule(m, t):\n    if storage_services:\n        return sum(m.MaximumPowerAvailable[g, t] for g in m.ThermalGenerators) \\\n             + sum(m.NondispatchablePowerUsed[n,t] for n in m.AllNondispatchableGenerators) \\\n             + sum(m.PowerOutputStorage[s,t]*m.OutputEfficiencyEnergy[s] for s in m.Storage) \\\n             - sum(m.PowerInputStorage[s,t] for s in m.Storage) \\\n             + sum(m.LoadGenerateMismatch[b,t] for b in m.Buses) \\\n             + m.ReserveShortfall[t] \\\n             >= \\\n             m.TotalDemand[t] + m.ReserveRequirement[t]\n    else:\n        return sum(m.MaximumPowerAvailable[g, t] for g in m.ThermalGenerators) \\\n             + sum(m.NondispatchablePowerUsed[n,t] for n in m.AllNondispatchableGenerators) \\\n             + sum(m.LoadGenerateMismatch[b,t] for b in m.Buses) \\\n             + m.ReserveShortfall[t] \\\n             >= \\\n             m.TotalDemand[t] + m.ReserveRequirement[t]",
  "def enforce_generator_output_limits_rule_part_b(m, g, t):\n   return m.PowerGeneratedAboveMinimum[g,t] <= m.MaximumPowerAvailableAboveMinimum[g, t]",
  "def enforce_must_run_rule(m,g,t):\n    return m.UnitOn[g,t] == 1",
  "def enforce_generator_output_limits_rule_part_c(m, g, t):\n   if regulation_services:\n       return m.MaximumPowerAvailableAboveMinimum[g,t] <= (m.MaximumPowerOutput[g] - m.MinimumPowerOutput[g]) * m.UnitOn[g, t] \\\n                                                            - m.MaxPowerOutputMinusRegHighLimit[g] * m.RegulationOn[g, t]\n   else:\n       return Constraint.Skip",
  "def power_limit_from_start_rule(m,g,t):\n    if m.MinimumUpTime[g] != 1:\n        return Constraint.Skip\n    return m.MaximumPowerAvailableAboveMinimum[g,t] <= (m.MaximumPowerOutput[g] - m.MinimumPowerOutput[g]) * m.UnitOn[g,t] \\\n                                            - (m.MaximumPowerOutput[g] - m.ScaledStartupRampLimit[g])*m.UnitStart[g,t]",
  "def power_limit_from_stop_rule(m,g,t):\n    if m.MinimumUpTime[g] != 1: \n        return Constraint.Skip\n    if t == m.NumTimePeriods:\n        return m.MaximumPowerAvailableAboveMinimum[g,t] <= (m.MaximumPowerOutput[g] - m.MinimumPowerOutput[g]) * m.UnitOn[g,t]\n    return m.MaximumPowerAvailableAboveMinimum[g,t] <= (m.MaximumPowerOutput[g] - m.MinimumPowerOutput[g]) * m.UnitOn[g,t] \\\n                                            - (m.MaximumPowerOutput[g] - m.ScaledShutdownRampLimit[g])*m.UnitStop[g,t+1]",
  "def power_limit_from_start_stop_rule(m,g,t):\n    if m.MinimumUpTime[g] == 1:\n        return Constraint.Skip\n    if t == m.NumTimePeriods: \n        return m.MaximumPowerAvailableAboveMinimum[g,t] <= (m.MaximumPowerOutput[g] - m.MinimumPowerOutput[g]) *m.UnitOn[g,t] \\\n                                              - (m.MaximumPowerOutput[g] - m.ScaledStartupRampLimit[g])*m.UnitStart[g,t]\n    return m.MaximumPowerAvailableAboveMinimum[g,t] <= (m.MaximumPowerOutput[g] - m.MinimumPowerOutput[g]) *m.UnitOn[g,t] \\\n                                          - (m.MaximumPowerOutput[g] - m.ScaledStartupRampLimit[g])*m.UnitStart[g,t] \\\n                                          - (m.MaximumPowerOutput[g] - m.ScaledShutdownRampLimit[g])*m.UnitStop[g,t+1]",
  "def enforce_max_available_ramp_up_rates_rule(m, g, t):\n   if t == m.InitialTime:\n      # if the unit was on in t0, then it's m.PowerGeneratedT0[g] >= m.MinimumPowerOutput[g], and m.UnitOnT0 == 1\n      # if not, then m.UnitOnT0[g] == 0 and so (m.PowerGeneratedT0[g] - m.MinimumPowerOutput[g]) * m.UnitOnT0[g] is 0\n      return m.MaximumPowerAvailableAboveMinimum[g, t] <= (m.PowerGeneratedT0[g] - m.MinimumPowerOutput[g]) * m.UnitOnT0[g] + \\\n                                              m.ScaledNominalRampUpLimit[g] \n   else:\n      return m.MaximumPowerAvailableAboveMinimum[g, t] <= m.PowerGeneratedAboveMinimum[g, t-1] + m.ScaledNominalRampUpLimit[g]",
  "def enforce_ramp_down_limits_rule(m, g, t):\n    if t == m.InitialTime:\n        if not enforce_t1_ramp_rates:\n            return Constraint.Skip\n        else:\n            return (m.PowerGeneratedT0[g] - m.MinimumPowerOutput[g])*m.UnitOnT0[g] - m.PowerGeneratedAboveMinimum[g, t] <= \\\n                    m.ScaledNominalRampDownLimit[g] \n    else:\n        return m.PowerGeneratedAboveMinimum[g, t-1] - m.PowerGeneratedAboveMinimum[g, t] <= \\\n                    m.ScaledNominalRampDownLimit[g]",
  "def production_cost_function(m, g, t, x):\n    return m.TimePeriodLength * m.PowerGenerationPiecewiseValues[g,t][x] * m.FuelCost[g]",
  "def piecewise_production_costs_index_set_generator(m):\n    return ((g,t,i) for g in m.ThermalGenerators for t in m.TimePeriods for i in range(len(m.PowerGenerationPiecewisePoints[g,t])-1))",
  "def piecewise_production_bounds_rule(m, g, t, i):\n    return (0, m.PowerGenerationPiecewisePoints[g,t][i+1] - m.PowerGenerationPiecewisePoints[g,t][i])",
  "def piecewise_production_sum_rule(m, g, t):\n    return sum( m.PiecewiseProduction[g,t,i] for i in range(len(m.PowerGenerationPiecewisePoints[g,t])-1) ) == m.PowerGeneratedAboveMinimum[g,t]",
  "def piecewise_production_limits_rule(m, g, t, i):\n    return m.PiecewiseProduction[g,t,i] <= (m.PowerGenerationPiecewisePoints[g,t][i+1] - m.PowerGenerationPiecewisePoints[g,t][i])*m.UnitOn[g,t]",
  "def piecewise_production_costs_rule(m, g, t, i):\n    return m.ProductionCost[g,t] == sum( (production_cost_function(m,g,t, m.PowerGenerationPiecewisePoints[g,t][i+1]) - production_cost_function(m,g,t, m.PowerGenerationPiecewisePoints[g,t][i]))/ (m.PowerGenerationPiecewisePoints[g,t][i+1]- m.PowerGenerationPiecewisePoints[g,t][i]) *\n       m.PiecewiseProduction[g,t,i] for i in range(len(m.PowerGenerationPiecewisePoints[g,t])-1))",
  "def compute_production_costs_rule(m, g, t, avg_power):\n    ## piecewise points for power\n    piecewise_points = m.PowerGenerationPiecewisePoints[g,t]\n    ## buckets\n    piecewise_eval = [0]*(len(piecewise_points)-1)\n    ## fill the buckets (skip the first since it's min power)\n    for l in range(len(piecewise_eval)):\n        ## fill this bucket all the way\n        if avg_power >= piecewise_points[l+1]:\n            piecewise_eval[l] = piecewise_points[l+1] - piecewise_points[l]\n        ## fill the bucket part way and stop\n        elif avg_power < piecewise_points[l+1]:\n            piecewise_eval[l] = avg_power - piecewise_points[l]\n            break\n\n            #slope * production\n    return sum( (production_cost_function(m,g,t,piecewise_points[l+1]) - production_cost_function(m,g,t,piecewise_points[l])) / (piecewise_points[l+1] - piecewise_points[l]) * piecewise_eval[l] for l in range(len(piecewise_eval)))",
  "def compute_total_production_cost_rule(m, t):\n    return m.TotalProductionCost[t] == sum(m.ProductionCost[g, t] for g in m.ThermalGenerators)",
  "def compute_total_no_load_cost_rule(m,t):\n    return m.TotalNoLoadCost[t] == sum(m.MinimumProductionCost[g] * m.UnitOn[g,t] for g in m.ThermalGenerators)",
  "def startup_match_rule(m, g, t):\n    return sum(m.StartupIndicator[g, t_prime, s] for (t_prime, s) in m.ShutdownHotStartupPairs[g] if s == t) <= m.UnitStart[g,t]",
  "def GeneratorShutdownPeriods_generator(m):\n    return ((g,t) for g in m.ThermalGenerators for t in m.ValidShutdownTimePeriods[g])",
  "def shutdown_match_rule(m, g, t):\n    if t < m.InitialTime:\n        begin_pairs = [(s, t_prime) for (s, t_prime) in m.ShutdownHotStartupPairs[g] if s == t]\n        if not begin_pairs: ##if this is empty\n            return Constraint.Feasible\n        else:\n            return sum(m.StartupIndicator[g, s, t_prime] for (s, t_prime) in begin_pairs) <= 1\n    else:\n        return sum(m.StartupIndicator[g, s, t_prime] for (s, t_prime) in m.ShutdownHotStartupPairs[g] if s == t) <= m.UnitStop[g,t]",
  "def ComputeStartupCost2_rule(m,g,t):\n    return m.StartupCost[g,t] == m.StartupCosts[g].last()*m.UnitStart[g,t] + \\\n                                  sum( (list(m.StartupCosts[g])[s-1] - m.StartupCosts[g].last()) * \\\n                                     sum( m.StartupIndicator[g,tp,t] for tp in m.ValidShutdownTimePeriods[g] \\\n                                       if (list(m.StartupLags[g])[s-1] <= t - tp < (list(m.StartupLags[g])[s])) ) \\\n                                     for s in m.StartupCostIndices[g] if s < len(m.StartupCostIndices[g]))",
  "def compute_shutdown_costs_rule(m, g, t):\n    return m.ShutdownCost[g, t] == m.ShutdownFixedCost[g] * (m.UnitStop[g, t])",
  "def enforce_up_time_constraints_initial(m, g):\n   if value(m.InitialTimePeriodsOnLine[g]) == 0:\n      return Constraint.Skip\n   return sum((1 - m.UnitOn[g, t]) for t in m.TimePeriods if t <= value(m.InitialTimePeriodsOnLine[g])) == 0.0",
  "def unit_start_rule(m,g,t):\n    if t < value(m.ScaledMinimumUpTime[g]):\n        return Constraint.Skip\n    else: \n        return sum(m.UnitStart[g,i] for i in m.TimePeriods if i >= t - value(m.ScaledMinimumUpTime[g]) + 1 and i <= t) <= m.UnitOn[g,t]",
  "def enforce_down_time_constraints_initial(m, g):\n   if value(m.InitialTimePeriodsOffLine[g]) == 0:\n      return Constraint.Skip\n   return sum(m.UnitOn[g, t] for t in m.TimePeriods if t <= value(m.InitialTimePeriodsOffLine[g])) == 0.0",
  "def unit_stop_rule(m,g,t):\n    if t < value(m.ScaledMinimumDownTime[g]):\n        return Constraint.Skip\n    else: \n        return sum(m.UnitStop[g,i] for i in m.TimePeriods if i >= t - value(m.ScaledMinimumDownTime[g]) + 1 and i <= t) <= 1 - m.UnitOn[g,t]",
  "def start_stop_rule(m,g,t):\n    if t==1:\n        return m.UnitOn[g, t] - m.UnitOnT0[g] == m.UnitStart[g,t] - m.UnitStop[g,t]\n    return m.UnitOn[g,t] - m.UnitOn[g,t-1] == m.UnitStart[g,t] - m.UnitStop[g,t]",
  "def commitment_stage_cost_expression_rule(m, st):\n    cc = (sum(m.StartupCost[g,t] + m.ShutdownCost[g,t] for g in m.ThermalGenerators for t in m.CommitmentTimeInStage[st]) + sum(sum(m.UnitOn[g,t] for t in m.CommitmentTimeInStage[st]) * m.MinimumProductionCost[g] * m.TimePeriodLength for g in m.ThermalGenerators))\n    if regulation_services:\n        cc += sum(m.RegulationCost[t] for t in m.CommitmentTimeInStage[st])\n    return cc",
  "def generation_stage_cost_expression_rule(m, st):\n    return sum(m.ProductionCost[g, t] for g in m.ThermalGenerators for t in m.GenerationTimeInStage[st]) + \\\n           m.LoadMismatchPenalty * sum(m.posLoadGenerateMismatch[b, t] + m.negLoadGenerateMismatch[b, t] for b in m.Buses for t in m.GenerationTimeInStage[st]) + \\\n           m.ReserveShortfallPenalty * sum(m.ReserveShortfall[t] for t in m.GenerationTimeInStage[st])",
  "def stage_cost_expression_rule(m, st):\n    return m.GenerationStageCost[st] + m.CommitmentStageCost[st]",
  "def first_stage_cost_expression_rule(m):\n    return m.StageCost[\"FirstStage\"]",
  "def total_cost_objective_rule(m):\n   return sum(m.StageCost[st] for st in m.StageSet)",
  "def regulation_high_limit_validator(m, v, g):\n        return v <= value(m.MaximumPowerOutput[g])",
  "def regulation_high_limit_init(m, g):\n        return value(m.MaximumPowerOutput[g])",
  "def calculate_max_power_minus_reg_high_limit_rule(m, g):\n        return m.MaximumPowerOutput[g] - m.RegulationHighLimit[g]",
  "def regulation_low_limit_validator(m, v, g):\n        return (v <= value(m.RegulationHighLimit[g]) and v >= value(m.MinimumPowerOutput[g]))",
  "def regulation_low_limit_init(m, g):\n        return value(m.MinimumPowerOutput[g])",
  "def calculate_regulation_capability_rule(m, g):\n        temp1 = 5 * m.AutomaticResponseRate[g]\n        temp2 = (m.RegulationHighLimit[g] - m.RegulationLowLimit[g])/2\n        if temp1 > temp2:\n            return temp2\n        else:\n            return temp1",
  "def verify_storage_buses_rule(m, s):\n        for b in m.Buses:\n            if s in m.StorageAtBus[b]:\n                return\n        print(\"DATA ERROR: No bus assigned for storage element=%s\" % s)\n        assert(False)",
  "def maximum_power_output_validator_storage(m, v, s):\n        return v >= value(m.MinimumPowerOutputStorage[s])",
  "def maximum_power_input_validator_storage(m, v, s):\n        return v >= value(m.MinimumPowerInputStorage[s])",
  "def scale_storage_ramp_up_out(m, s):\n        return m.NominalRampUpLimitStorageOutput[s] * m.TimePeriodLength",
  "def scale_storage_ramp_down_out(m, s):\n        return m.NominalRampDownLimitStorageOutput[s] * m.TimePeriodLength",
  "def scale_storage_ramp_up_in(m, s):\n        return m.NominalRampUpLimitStorageInput[s] * m.TimePeriodLength",
  "def scale_storage_ramp_down_in(m, s):\n        return m.NominalRampDownLimitStorageInput[s] * m.TimePeriodLength",
  "def t0_storage_power_input_validator(m, v, s):\n        return (v >= value(m.MinimumPowerInputStorage[s])) and (v <= value(m.MaximumPowerInputStorage[s]))",
  "def t0_storage_power_output_validator(m, v, s):\n        return (v >= value(m.MinimumPowerInputStorage[s])) and (v <= value(m.MaximumPowerInputStorage[s]))",
  "def power_output_storage_bounds_rule(m, s, t):\n        return (0, m.MaximumPowerOutputStorage[s])",
  "def power_input_storage_bounds_rule(m, s, t):\n        return (0, m.MaximumPowerInputStorage[s])",
  "def min_soc_rule(model, m, t):\n        return model.SocStorage[m,t] >= model.MinimumSocStorage[m]",
  "def calculate_regulating_reserve_up_available_per_generator(m, g, t):\n        return m.RegulatingReserveUpAvailable[g, t] == m.MaximumPowerAvailableAboveMinimum[g,t] - m.PowerGeneratedAboveMinimum[g,t]",
  "def enforce_zonal_reserve_requirement_rule(m, rz, t):\n        return sum(m.RegulatingReserveUpAvailable[g,t] for g in m.ThermalGeneratorsInReserveZone[rz]) >= m.ZonalReserveRequirement[rz, t]",
  "def enforce_storage_input_limits_rule_part_a(m, s, t):\n        return m.MinimumPowerInputStorage[s] * (1-m.OutputStorage[s, t]) <= m.PowerInputStorage[s,t]",
  "def enforce_storage_input_limits_rule_part_b(m, s, t):\n        return m.PowerInputStorage[s,t] <= m.MaximumPowerInputStorage[s] * (1-m.OutputStorage[s, t])",
  "def enforce_storage_output_limits_rule_part_a(m, s, t):\n        return m.MinimumPowerOutputStorage[s] * m.OutputStorage[s, t] <= m.PowerOutputStorage[s,t]",
  "def enforce_storage_output_limits_rule_part_b(m, s, t):\n        return m.PowerOutputStorage[s,t] <= m.MaximumPowerOutputStorage[s] * m.OutputStorage[s, t]",
  "def enforce_ramp_up_rates_power_output_storage_rule(m, s, t):\n        if t == m.InitialTime:\n            return m.PowerOutputStorage[s, t] <= m.StoragePowerOutputOnT0[s] + m.ScaledNominalRampUpLimitStorageOutput[s]\n        else:\n            return m.PowerOutputStorage[s, t] <= m.PowerOutputStorage[s, t-1] + m.ScaledNominalRampUpLimitStorageOutput[s]",
  "def enforce_ramp_down_rates_power_output_storage_rule(m, s, t):\n        if t == m.InitialTime:\n            return m.PowerOutputStorage[s, t] >= m.StoragePowerOutputOnT0[s] - m.ScaledNominalRampDownLimitStorageOutput[s]\n        else:\n            return m.PowerOutputStorage[s, t] >= m.PowerOutputStorage[s, t-1] - m.ScaledNominalRampDownLimitStorageOutput[s]",
  "def enforce_ramp_up_rates_power_input_storage_rule(m, s, t):\n        if t == m.InitialTime:\n            return m.PowerInputStorage[s, t] <= m.StoragePowerInputOnT0[s] + m.ScaledNominalRampUpLimitStorageInput[s]\n        else:\n            return m.PowerInputStorage[s, t] <= m.PowerInputStorage[s, t-1] + m.ScaledNominalRampUpLimitStorageInput[s]",
  "def enforce_ramp_down_rates_power_input_storage_rule(m, s, t):\n        if t == m.InitialTime:\n            return m.PowerInputStorage[s, t] >= m.StoragePowerInputOnT0[s] - m.ScaledNominalRampDownLimitStorageInput[s]\n        else:\n            return m.PowerInputStorage[s, t] >= m.PowerInputStorage[s, t-1] - m.ScaledNominalRampDownLimitStorageInput[s]",
  "def energy_conservation_rule(m, s, t):\n        # storage s, time t\n        if t == m.InitialTime:\n            return m.SocStorage[s, t] == m.StorageSocOnT0[s]  + \\\n                (- m.PowerOutputStorage[s, t] + m.PowerInputStorage[s,t]*m.InputEfficiencyEnergy[s])/m.MaximumEnergyStorage[s]\n        else:\n            return m.SocStorage[s, t] == m.SocStorage[s, t-1]*m.RetentionRate[s]  + \\\n                (- m.PowerOutputStorage[s, t] + m.PowerInputStorage[s,t]*m.InputEfficiencyEnergy[s])/m.MaximumEnergyStorage[s]",
  "def storage_end_point_soc_rule(m, s):\n        # storage s, last time period\n        return m.SocStorage[s, value(m.NumTimePeriods)] == m.EndPointSocStorage[s]",
  "def identify_regulation_providers_rule(m, g, t):\n        if m.RegulationProvider[g] == 0:\n            return m.RegulationOn[g, t] == 0\n        else:\n            return Constraint.Skip",
  "def enforce_generator_ouptut_low_limit_regulation_rule(m, g, t):\n        return (m.RegulationLowLimit[g]-m.MinimumPowerOutput[g]) * m.RegulationOn[g, t] <= m.PowerGeneratedAboveMinimum[g, t]",
  "def provide_regulation_when_unit_on_rule(m, g, t):\n        return m.RegulationOn[g, t] <= m.UnitOn[g, t]",
  "def enforce_zonal_regulation_requirement_rule(m, rz, t):\n        expr = (m.ZonalRegulationRequirement[rz, t] == 0.0)\n        if expr == True:\n            return Constraint.Feasible\n        else:\n            return m.ZonalRegulationRequirement[rz, t] <= sum(m.RegulationOn[g, t] * m.RegulationCapability[g] for g in m.ThermalGeneratorsInReserveZone[rz])",
  "def calculate_total_regulation_capability_available(m, t):\n        return m.TotalRegulationCapabilityAvailable[t] == sum(m.RegulationOn[g, t] * m.RegulationCapability[g] for g in m.ThermalGenerators)",
  "def enforce_global_regulation_requirement_rule(m, t):\n        expr = (m.GlobalRegulationRequirement[t] == 0.0)\n        if expr == True:\n            return Constraint.Feasible\n        else:\n            return m.GlobalRegulationRequirement[t] <= sum(m.RegulationOn[g, t] * m.RegulationCapability[g] for g in m.ThermalGenerators)",
  "def compute_regulation_cost_rule(m, t):\n        return m.RegulationCost[t] >= sum(m.RegulationOffer[g] * m.RegulationOn[g, t] for g in m.ThermalGenerators)",
  "def compute_total_regulation_cost_rule(m):\n        return m.TotalRegulationCost == sum(m.RegulationCost[t] for t in m.TimePeriods)",
  "def calculate_spinning_reserve_available_rule_part_a(m, g, t):\n        return m.SpinningReserveDispatched[g, t] <= m.MaximumPowerOutput[g] * m.UnitOn[g, t] - m.PowerGenerated[g, t]",
  "def calculate_spinning_reserve_available_rule_part_b(m, g, t):\n        return m.SpinningReserveDispatched[g, t] <= m.NominalRampUpLimit[g] * m.SpinningReserveTime",
  "def enforce_zonal_spinning_reserve_requirement(m, rz, t):\n        return m.ZonalSpinningReserveRequirement[rz, t] <= sum(m.SpinningReserveDispatched[g, t] for g in m.ThermalGeneratorsInReserveZone[rz])",
  "def enforce_global_spinning_reserve_requirement(m, t):\n        return m.SystemSpinningReserveRequirement[t] <= sum(m.SpinningReserveDispatched[g, t] for g in m.ThermalGenerators)",
  "def compute_spinning_reserve_cost(m, g, t):\n        return m.SpinningReserveCost[g, t] >= m.SpinningReserveDispatched[g, t] * m.SpinningReserveOffer[g] * m.TimePeriodLength",
  "def compute_total_spinning_reserve_cost(m):\n        return m.TotalSpinningReserveCost >= sum(m.SpinningReserveCost[g, t]  for g in m.ThermalGenerators for t in m.TimePeriods)",
  "def calculate_non_spinning_reserve_limit_rule(m, g, t):\n        return m.NonSpinningReserveDispatched[g, t] <= m.NonSpinningReserveAvailable[g] * (1 - m.UnitOn[g, t])",
  "def calculate_non_spinning_reserve_cost(m, g, t):\n        return m.NonSpinningReserveCost[g, t] >= m.NonSpinningReserveDispatched[g, t] * m.NonSpinningReserveOffer[g] * m.TimePeriodLength",
  "def enforce_zonal_non_spinning_reserve_rule(m, rz, t):\n        return m.ZonalTenMinuteReserveRequirement[rz, t] <= sum(m.SpinningReserveDispatched[g, t] + m.NonSpinningReserveDispatched[g, t] \\\n                                                                for g in m.ThermalGeneratorsInReserveZone[rz])",
  "def enforce_system_ten_minute_reserve_requirement(m, t):\n        return m.SystemTenMinuteReserveRequirement[t] <= sum(m.SpinningReserveDispatched[g, t] + m.NonSpinningReserveDispatched[g, t] for g in m.ThermalGenerators)",
  "def compute_non_spinning_reserve_total_cost(m):\n        return m.TotalNonSpinningReserveCost >= sum(m.NonSpinningReserveCost[g, t] for g in m.ThermalGenerators for t in m.TimePeriods)",
  "def calculate_operating_reserve_limit_rule(m, g, t):\n        return m.OperatingReserveDispatched[g, t] + m.NonSpinningReserveDispatched[g, t] <= m.OperatingReserveAvailable[g] * (1 - m.UnitOn[g, t])",
  "def enforce_zonal_operating_reserve_requirement_rule(m, rz, t):\n        return m.ZonalOperatingReserveRequirement[rz, t] <= sum(m.SpinningReserveDispatched[g, t] + m.NonSpinningReserveDispatched[g, t] \\\n                                                                + m.OperatingReserveDispatched[g, t] for g in m.ThermalGeneratorsInReserveZone[rz])",
  "def enforce_system_operating_reserve_requirement(m, t):\n        return m.SystemOperatingReserveRequirement[t] <= sum(m.SpinningReserveDispatched[g, t] + m.NonSpinningReserveDispatched[g, t] \\\n                                                             + m.OperatingReserveDispatched[g, t] for g in m.ThermalGenerators)",
  "def calculate_operating_reserve_cost_rule(m, g, t):\n        return m.OperatingReserveCost[g, t] >= m.OperatingReserveDispatched[g, t] * m.OperatingReserveOffer[g] * m.TimePeriodLength",
  "def calculate_operating_reserve_total_cost(m):\n        return m.TotalOperatingReserveCost >= sum(m.OperatingReserveCost[g, t] for g in m.ThermalGenerators for t in m.TimePeriods)",
  "def pysp_instance_creation_callback(scenario_name, path=None, scenario_count=None):\n    \"\"\"\n    Notes:\n    - The uc_cylinders.py code has a `scenario_count` kwarg that gets passed to\n      the spokes, but it seems to be unused here...\n    \"\"\"\n    if path is None:\n        raise ValueError(\"UC scenario creator requires a path kwarg\")\n\n    #print(\"Building instance for scenario =\", scenario_name)\n    scennum = sputils.extract_num(scenario_name)\n\n    uc_model_params = pdp.get_uc_model()\n\n    scenario_data = DataPortal(model=uc_model_params)\n    scenario_data.load(filename=path+os.sep+\"RootNode.dat\")\n    scenario_data.load(filename=path+os.sep+\"Node\"+str(scennum)+\".dat\")\n\n    scenario_params = uc_model_params.create_instance(scenario_data,\n                                                      report_timing=False,\n                                                      name=scenario_name)\n\n    scenario_md = md.ModelData(pdp.create_model_data_dict_params(scenario_params, keep_names=True))\n\n    ## TODO: use the \"power_balance_constraints\" for now. In the future, networks should be\n    ##       handled with a custom callback -- also consider other base models\n    scenario_instance = uc.create_tight_unit_commitment_model(scenario_md,\n                                                    network_constraints='power_balance_constraints')\n\n    # hold over string attribute from Egret,\n    # causes warning wth LShaped/Benders\n    del scenario_instance.objective\n\n    return scenario_instance",
  "def scenario_creator(scenario_name, path=None, scenario_count=None):\n    return pysp2_callback(scenario_name, path=path, scenario_count=scenario_count)",
  "def pysp2_callback(scenario_name, path=None, scenario_count=None):\n    ''' The callback needs to create an instance and then attach\n        the PySP nodes to it in a list _mpisppy_node_list ordered by stages.\n        Optionally attach _PHrho. Standard (1.0) PySP signature for now...\n    '''\n\n    instance = pysp_instance_creation_callback(\n        scenario_name, path=path, scenario_count=scenario_count,\n    )\n\n    # now attach the one and only tree node (ROOT is a reserved word)\n    # UnitOn[*,*] is the only set of nonant variables\n    \"\"\"\n    instance._mpisppy_node_list = [scenario_tree.ScenarioNode(\"ROOT\",\n                                                          1.0,\n                                                          1,\n                                                          instance.StageCost[\"Stage_1\"], #\"Stage_1\" hardcodes the commitments in all time periods\n                                                          [instance.UnitOn],\n                                                          instance,\n                                                          [instance.UnitStart, instance.UnitStop, instance.StartupIndicator],\n                                                          )]\n    \"\"\"\n    sputils.attach_root_node(instance,\n                             instance.StageCost[\"Stage_1\"],\n                             [instance.UnitOn],\n                             nonant_ef_suppl_list = [instance.UnitStart,\n                                                     instance.UnitStop,\n                                                     instance.StartupIndicator])\n    return instance",
  "def scenario_denouement(rank, scenario_name, scenario):\n#    print(\"First stage cost for scenario\",scenario_name,\"is\",pyo.value(scenario.StageCost[\"FirstStage\"]))\n#    print(\"Second stage cost for scenario\",scenario_name,\"is\",pyo.value(scenario.StageCost[\"SecondStage\"]))\n    pass",
  "def scenario_rhosa(scenario_instance):\n\n    return scenario_rhos(scenario_instance)",
  "def _rho_setter(scenario_instance):\n\n    return scenario_rhos(scenario_instance)",
  "def scenario_rhos(scenario_instance, rho_scale_factor=0.1):\n    computed_rhos = []\n    for t in scenario_instance.TimePeriods:\n        for g in scenario_instance.ThermalGenerators:\n            max_capacity = pyo.value(scenario_instance.MaximumPowerOutput[g,t])\n            min_power = pyo.value(scenario_instance.MinimumPowerOutput[g,t])\n            max_power = pyo.value(scenario_instance.MaximumPowerOutput[g,t])\n            avg_power = min_power + ((max_power - min_power) / 2.0)\n\n            min_cost = pyo.value(scenario_instance.MinimumProductionCost[g,t])\n\n            avg_cost = scenario_instance.ComputeProductionCosts(scenario_instance, g, t, avg_power) + min_cost\n            #max_cost = scenario_instance.ComputeProductionCosts(scenario_instance, g, t, max_power) + min_cost\n\n            computed_rho = rho_scale_factor * avg_cost\n            computed_rhos.append((id(scenario_instance.UnitOn[g,t]), computed_rho))\n                             \n    return computed_rhos",
  "def scenario_rhos_trial_from_file(scenario_instance, rho_scale_factor=0.01,\n                                    fname=None):\n    ''' First computes the standard rho values (as computed by scenario_rhos()\n        above). Then reads rho values from the specified file (raises error if\n        no file specified) which is a csv formatted (var_name,rho_value). If\n        the rho_value specified in the file is strictly positive, it replaces\n        the value computed by scenario_rhos().\n\n        DTM: I wrote this function to test some specific things--I don't think\n        this will have a general purpose use, and can probably be deleted.\n    '''\n    if (fname is None):\n        raise RuntimeError('Please provide an \"fname\" kwarg to '\n                           'the \"rho_setter_kwargs\" option in options')\n    computed_rhos = scenario_rhos(scenario_instance,\n                                    rho_scale_factor=rho_scale_factor)\n    try:\n        trial_rhos = _get_saved_rhos(fname)\n    except:\n        raise RuntimeError('Formatting issue in specified rho file ' + fname +\n                           '. Format should be (variable_name,rho_value) for '\n                           'each row, with no blank lines, and no '\n                           'extra/commented lines')\n    \n    index = 0\n    for b in sorted(scenario_instance.Buses):\n        for t in sorted(scenario_instance.TimePeriods):\n            for g in sorted(scenario_instance.ThermalGeneratorsAtBus[b]):\n                var = scenario_instance.UnitOn[g,t]\n                computed_rho = computed_rhos[index]\n                try:\n                    trial_rho = trial_rhos[var.name]\n                except KeyError:\n                    raise RuntimeError(var.name + ' is missing from '\n                                       'the specified rho file ' + fname)\n                if (trial_rho >= 1e-14):\n                    print('Using a trial rho')\n                    computed_rhos[index] = (id(var), trial_rho)\n                index += 1\n                             \n    return computed_rhos",
  "def _get_saved_rhos(fname):\n    ''' Return a dict of trial rho values, indexed by variable name.\n    '''\n    rhos = dict()\n    with open(fname, 'r') as f:\n        for line in f:\n            line = line.split(',')\n            vname = ','.join(line[:-1])\n            rho = float(line[-1])\n            rhos[vname] = rho\n    return rhos",
  "def id_fix_list_fct(scenario_instance):\n    \"\"\" specify tuples used by the fixer.\n\n    Args:\n        s (ConcreteModel): the sizes instance.\n    Returns:\n         i0, ik (tuples): one for iter 0 and other for general iterations.\n             Var id,  threshold, nb, lb, ub\n             The threshold is on the square root of the xbar squared differnce\n             nb, lb an bu an \"no bound\", \"upper\" and \"lower\" and give the numver\n                 of iterations or None for ik and for i0 anything other than None\n                 or None. In both cases, None indicates don't fix.\n    \"\"\"\n    import mpisppy.extensions.fixer as fixer\n\n    iter0tuples = []\n    iterktuples = []\n\n    for b in sorted(scenario_instance.Buses):\n        for t in sorted(scenario_instance.TimePeriods):\n            for g in sorted(scenario_instance.ThermalGeneratorsAtBus[b]):\n\n                iter0tuples.append(fixer.Fixer_tuple(scenario_instance.UnitOn[g,t],\n                                                     th=0.01, nb=None, lb=0, ub=None))\n                \n                iterktuples.append(fixer.Fixer_tuple(scenario_instance.UnitOn[g,t],\n                                                     th=0.01, nb=None, lb=6, ub=6))\n\n    return iter0tuples, iterktuples",
  "def write_solution(spcomm, opt_dict, solution_dir):\n    from mpisppy.cylinders.xhatshufflelooper_bounder import XhatShuffleInnerBound\n    from mpisppy.extensions.xhatclosest import XhatClosest\n    from mpisppy.opt.ph import PH\n\n    if spcomm.global_rank == 0:\n        if spcomm.last_ib_idx is None:\n            best_strata_rank = -1\n            print(\"No incumbent solution to print\")\n        else:\n            best_strata_rank = spcomm.last_ib_idx\n    else:\n        best_strata_rank = None\n\n    best_strata_rank = spcomm.fullcomm.bcast(best_strata_rank, root=0)\n\n    if spcomm.strata_rank != best_strata_rank:\n        # Nothing to do\n        return\n    ## else this spoke/hub is the winner!\n\n    # do some checks, to make sure the solution we print will be nonantipative\n    if best_strata_rank != 0:\n        assert opt_dict[\"spoke_class\"] in (XhatShuffleInnerBound, )\n    else: # this is the hub, TODO: also could check for XhatSpecific\n        assert opt_dict[\"opt_class\"] in (PH, )\n        assert XhatClosest in opt_dict[\"opt_kwargs\"][\"extension_kwargs\"][\"ext_classes\"]\n        assert \"keep_solution\" in opt_dict[\"opt_kwargs\"][\"options\"][\"xhat_closest_options\"]\n        assert opt_dict[\"opt_kwargs\"][\"options\"][\"xhat_closest_options\"][\"keep_solution\"] is True\n\n    ## if we've passed the above checks, the scenarios should have the tree solution\n\n    ## make solution dir if it doesn't exist,\n    ## but only on rank 0\n    if spcomm.cylinder_rank == 0:\n        if not os.path.exists(solution_dir):\n            os.makedirs(solution_dir)\n\n    spcomm.cylinder_comm.Barrier()\n\n    for sname, s in spcomm.opt.local_scenarios.items():\n        file_name = os.path.join(solution_dir, sname+'.json')\n        mds = uc._save_uc_results(s, relaxed=False)\n        mds.write(file_name)\n\n    return",
  "class WheelSpinner:\n\n    def __init__(self, hub_dict, list_of_spoke_dict):\n        \"\"\" top level for the hub and spoke system\n        Args:\n            hub_dict(dict): controls hub creation\n            list_of_spoke_dict(list dict): controls creation of spokes\n    \n        Returns:\n            spcomm (Hub or Spoke object): the object that did the work (windowless)\n            opt_dict (dict): the dictionary that controlled creation for this rank\n    \n        NOTE: the return is after termination; the objects are provided for query.\n    \n        \"\"\"\n        if not haveMPI:\n            raise RuntimeError(\"spin_the_wheel called, but cannot import mpi4py\")\n        self.hub_dict = hub_dict\n        self.list_of_spoke_dict = list_of_spoke_dict\n\n        self._ran = False\n\n    def spin(self, comm_world=None):\n        return self.run(comm_world=comm_world)\n\n    def run(self, comm_world=None):\n        \"\"\" top level for the hub and spoke system\n        Args:\n            comm_world (MPI comm): the world for this hub-spoke system\n        \"\"\"\n        if self._ran:\n            raise RuntimeError(\"WheelSpinner can only be run once\")\n\n        hub_dict = self.hub_dict\n        list_of_spoke_dict = self.list_of_spoke_dict\n\n        # Confirm that the provided dictionaries specifying\n        # the hubs and spokes contain the appropriate keys\n        if \"hub_class\" not in hub_dict:\n            raise RuntimeError(\n                \"The hub_dict must contain a 'hub_class' key specifying \"\n                \"the hub class to use\"\n            )\n        if \"opt_class\" not in hub_dict:\n            raise RuntimeError(\n                \"The hub_dict must contain an 'opt_class' key specifying \"\n                \"the SPBase class to use (e.g. PHBase, etc.)\"\n            )\n        if \"hub_kwargs\" not in hub_dict:\n            hub_dict[\"hub_kwargs\"] = dict()\n        if \"opt_kwargs\" not in hub_dict:\n            hub_dict[\"opt_kwargs\"] = dict()\n        for spoke_dict in list_of_spoke_dict:\n            if \"spoke_class\" not in spoke_dict:\n                raise RuntimeError(\n                    \"Each spoke_dict must contain a 'spoke_class' key \"\n                    \"specifying the spoke class to use\"\n                )\n            if \"opt_class\" not in spoke_dict:\n                raise RuntimeError(\n                    \"Each spoke_dict must contain an 'opt_class' key \"\n                    \"specifying the SPBase class to use (e.g. PHBase, etc.)\"\n                )\n            if \"spoke_kwargs\" not in spoke_dict:\n                spoke_dict[\"spoke_kwargs\"] = dict()\n            if \"opt_kwargs\" not in spoke_dict:\n                spoke_dict[\"opt_kwargs\"] = dict()\n    \n        if comm_world is None:\n            comm_world = MPI.COMM_WORLD\n        n_spokes = len(list_of_spoke_dict)\n    \n        # Create the necessary communicators\n        fullcomm = comm_world\n        strata_comm, cylinder_comm = _make_comms(n_spokes, fullcomm=fullcomm)\n        strata_rank = strata_comm.Get_rank()\n        cylinder_rank = cylinder_comm.Get_rank()\n        global_rank = fullcomm.Get_rank()\n    \n        # Assign hub/spokes to individual ranks\n        if strata_rank == 0: # This rank is a hub\n            sp_class = hub_dict[\"hub_class\"]\n            sp_kwargs = hub_dict[\"hub_kwargs\"]\n            opt_class = hub_dict[\"opt_class\"]\n            opt_kwargs = hub_dict[\"opt_kwargs\"]\n            opt_dict = hub_dict\n        else: # This rank is a spoke\n            spoke_dict = list_of_spoke_dict[strata_rank - 1]\n            sp_class = spoke_dict[\"spoke_class\"]\n            sp_kwargs = spoke_dict[\"spoke_kwargs\"]\n            opt_class = spoke_dict[\"opt_class\"]\n            opt_kwargs = spoke_dict[\"opt_kwargs\"]\n            opt_dict = spoke_dict\n\n        # Create the appropriate opt object locally\n        opt_kwargs[\"mpicomm\"] = cylinder_comm\n        opt = opt_class(**opt_kwargs)\n    \n        # Create the SPCommunicator object (hub/spoke) with\n        # the appropriate SPBase object attached\n        if strata_rank == 0: # Hub\n            spcomm = sp_class(opt, fullcomm, strata_comm, cylinder_comm,\n                              list_of_spoke_dict, **sp_kwargs) \n        else: # Spokes\n            spcomm = sp_class(opt, fullcomm, strata_comm, cylinder_comm, **sp_kwargs) \n    \n        # Create the windows, run main(), destroy the windows\n        spcomm.make_windows()\n        if strata_rank == 0:\n            spcomm.setup_hub()\n\n        global_toc(\"Starting spcomm.main()\")\n        spcomm.main()\n        if strata_rank == 0: # If this is the hub\n            spcomm.send_terminate()\n    \n        # Anything that's left to do\n        spcomm.finalize()\n    \n        # to ensure the messages below are True\n        cylinder_comm.Barrier()\n        global_toc(f\"Hub algorithm {opt_class.__name__} complete, waiting for spoke finalization\")\n        global_toc(f\"Spoke {sp_class.__name__} finalized\", (cylinder_rank == 0 and strata_rank != 0))\n    \n        fullcomm.Barrier()\n    \n        ## give the hub the chance to catch new values\n        spcomm.hub_finalize()\n    \n        fullcomm.Barrier()\n\n        spcomm.free_windows()\n        global_toc(\"Windows freed\")\n\n        self.spcomm = spcomm\n        self.opt_dict = opt_dict\n        self.global_rank = global_rank\n        self.strata_rank = strata_rank\n        self.cylinder_rank = cylinder_rank\n\n        if self.strata_rank == 0:\n            self.BestInnerBound = spcomm.BestInnerBound\n            self.BestOuterBound = spcomm.BestOuterBound\n        else: # the cylinder ranks don't track the inner / outer bounds\n            self.BestInnerBound = None\n            self.BestOuterBound = None\n\n        self._ran = True\n\n    def on_hub(self):\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before finding out.\")\n        return (\"hub_class\" in self.opt_dict)\n\n    def write_first_stage_solution(self, solution_file_name,\n            first_stage_solution_writer=first_stage_nonant_writer):\n        \"\"\" Write a solution file, if a solution is available, to the solution_file_name provided\n        Args:\n            solution_file_name : filename to write the solution to\n            first_stage_solution_writer (optional) : custom first stage solution writer function\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        winner = self._determine_innerbound_winner()\n        if winner:\n            self.spcomm.opt.write_first_stage_solution(solution_file_name,first_stage_solution_writer)\n    \n    def write_tree_solution(self, solution_directory_name,\n            scenario_tree_solution_writer=scenario_tree_solution_writer):\n        \"\"\" Write a tree solution directory, if available, to the solution_directory_name provided\n        Args:\n            solution_file_name : filename to write the solution to\n            scenario_tree_solution_writer (optional) : custom scenario solution writer function\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        winner = self._determine_innerbound_winner()\n        if winner:\n            self.spcomm.opt.write_tree_solution(solution_directory_name,scenario_tree_solution_writer)\n\n    def local_nonant_cache(self):\n        \"\"\" Returns a dict with non-anticipative values at each local node\n            We assume that the optimization has been done before calling this\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        local_xhats = dict()\n        for k,s in self.spcomm.opt.local_scenarios.items():\n            for node in s._mpisppy_node_list:\n                if node.name not in local_xhats:\n                    local_xhats[node.name] = [\n                        value(var) for var in node.nonant_vardata_list]\n        return local_xhats\n\n    def _determine_innerbound_winner(self):\n        if self.spcomm.global_rank == 0:\n            if self.spcomm.last_ib_idx is None:\n                best_strata_rank = -1\n                global_toc(\"No incumbent solution available to write!\")\n            else:\n                best_strata_rank = self.spcomm.last_ib_idx\n        else:\n            best_strata_rank = None\n    \n        best_strata_rank = self.spcomm.fullcomm.bcast(best_strata_rank, root=0)\n        return (self.spcomm.strata_rank == best_strata_rank)",
  "def _make_comms(n_spokes, fullcomm=None):\n    \"\"\" Create the strata_comm and cylinder_comm for hub/spoke style runs\n    \"\"\"\n    if not haveMPI:\n        raise RuntimeError(\"make_comms called, but cannot import mpi4py\")\n    # Ensure that the proper number of processes have been invoked\n    nsp1 = n_spokes + 1 # Add 1 for the hub\n    if fullcomm is None:\n        fullcomm = MPI.COMM_WORLD\n    n_proc = fullcomm.Get_size() \n    if n_proc % nsp1 != 0:\n        raise RuntimeError(f\"Need a multiple of {nsp1} processes (got {n_proc})\")\n\n    # Create the strata_comm and cylinder_comm\n    # Cryptic comment: intra is vertical, inter is around the hub\n    global_rank = fullcomm.Get_rank()\n    strata_comm = fullcomm.Split(key=global_rank, color=global_rank // nsp1)\n    cylinder_comm = fullcomm.Split(key=global_rank, color=global_rank % nsp1)\n    return strata_comm, cylinder_comm",
  "def __init__(self, hub_dict, list_of_spoke_dict):\n        \"\"\" top level for the hub and spoke system\n        Args:\n            hub_dict(dict): controls hub creation\n            list_of_spoke_dict(list dict): controls creation of spokes\n    \n        Returns:\n            spcomm (Hub or Spoke object): the object that did the work (windowless)\n            opt_dict (dict): the dictionary that controlled creation for this rank\n    \n        NOTE: the return is after termination; the objects are provided for query.\n    \n        \"\"\"\n        if not haveMPI:\n            raise RuntimeError(\"spin_the_wheel called, but cannot import mpi4py\")\n        self.hub_dict = hub_dict\n        self.list_of_spoke_dict = list_of_spoke_dict\n\n        self._ran = False",
  "def spin(self, comm_world=None):\n        return self.run(comm_world=comm_world)",
  "def run(self, comm_world=None):\n        \"\"\" top level for the hub and spoke system\n        Args:\n            comm_world (MPI comm): the world for this hub-spoke system\n        \"\"\"\n        if self._ran:\n            raise RuntimeError(\"WheelSpinner can only be run once\")\n\n        hub_dict = self.hub_dict\n        list_of_spoke_dict = self.list_of_spoke_dict\n\n        # Confirm that the provided dictionaries specifying\n        # the hubs and spokes contain the appropriate keys\n        if \"hub_class\" not in hub_dict:\n            raise RuntimeError(\n                \"The hub_dict must contain a 'hub_class' key specifying \"\n                \"the hub class to use\"\n            )\n        if \"opt_class\" not in hub_dict:\n            raise RuntimeError(\n                \"The hub_dict must contain an 'opt_class' key specifying \"\n                \"the SPBase class to use (e.g. PHBase, etc.)\"\n            )\n        if \"hub_kwargs\" not in hub_dict:\n            hub_dict[\"hub_kwargs\"] = dict()\n        if \"opt_kwargs\" not in hub_dict:\n            hub_dict[\"opt_kwargs\"] = dict()\n        for spoke_dict in list_of_spoke_dict:\n            if \"spoke_class\" not in spoke_dict:\n                raise RuntimeError(\n                    \"Each spoke_dict must contain a 'spoke_class' key \"\n                    \"specifying the spoke class to use\"\n                )\n            if \"opt_class\" not in spoke_dict:\n                raise RuntimeError(\n                    \"Each spoke_dict must contain an 'opt_class' key \"\n                    \"specifying the SPBase class to use (e.g. PHBase, etc.)\"\n                )\n            if \"spoke_kwargs\" not in spoke_dict:\n                spoke_dict[\"spoke_kwargs\"] = dict()\n            if \"opt_kwargs\" not in spoke_dict:\n                spoke_dict[\"opt_kwargs\"] = dict()\n    \n        if comm_world is None:\n            comm_world = MPI.COMM_WORLD\n        n_spokes = len(list_of_spoke_dict)\n    \n        # Create the necessary communicators\n        fullcomm = comm_world\n        strata_comm, cylinder_comm = _make_comms(n_spokes, fullcomm=fullcomm)\n        strata_rank = strata_comm.Get_rank()\n        cylinder_rank = cylinder_comm.Get_rank()\n        global_rank = fullcomm.Get_rank()\n    \n        # Assign hub/spokes to individual ranks\n        if strata_rank == 0: # This rank is a hub\n            sp_class = hub_dict[\"hub_class\"]\n            sp_kwargs = hub_dict[\"hub_kwargs\"]\n            opt_class = hub_dict[\"opt_class\"]\n            opt_kwargs = hub_dict[\"opt_kwargs\"]\n            opt_dict = hub_dict\n        else: # This rank is a spoke\n            spoke_dict = list_of_spoke_dict[strata_rank - 1]\n            sp_class = spoke_dict[\"spoke_class\"]\n            sp_kwargs = spoke_dict[\"spoke_kwargs\"]\n            opt_class = spoke_dict[\"opt_class\"]\n            opt_kwargs = spoke_dict[\"opt_kwargs\"]\n            opt_dict = spoke_dict\n\n        # Create the appropriate opt object locally\n        opt_kwargs[\"mpicomm\"] = cylinder_comm\n        opt = opt_class(**opt_kwargs)\n    \n        # Create the SPCommunicator object (hub/spoke) with\n        # the appropriate SPBase object attached\n        if strata_rank == 0: # Hub\n            spcomm = sp_class(opt, fullcomm, strata_comm, cylinder_comm,\n                              list_of_spoke_dict, **sp_kwargs) \n        else: # Spokes\n            spcomm = sp_class(opt, fullcomm, strata_comm, cylinder_comm, **sp_kwargs) \n    \n        # Create the windows, run main(), destroy the windows\n        spcomm.make_windows()\n        if strata_rank == 0:\n            spcomm.setup_hub()\n\n        global_toc(\"Starting spcomm.main()\")\n        spcomm.main()\n        if strata_rank == 0: # If this is the hub\n            spcomm.send_terminate()\n    \n        # Anything that's left to do\n        spcomm.finalize()\n    \n        # to ensure the messages below are True\n        cylinder_comm.Barrier()\n        global_toc(f\"Hub algorithm {opt_class.__name__} complete, waiting for spoke finalization\")\n        global_toc(f\"Spoke {sp_class.__name__} finalized\", (cylinder_rank == 0 and strata_rank != 0))\n    \n        fullcomm.Barrier()\n    \n        ## give the hub the chance to catch new values\n        spcomm.hub_finalize()\n    \n        fullcomm.Barrier()\n\n        spcomm.free_windows()\n        global_toc(\"Windows freed\")\n\n        self.spcomm = spcomm\n        self.opt_dict = opt_dict\n        self.global_rank = global_rank\n        self.strata_rank = strata_rank\n        self.cylinder_rank = cylinder_rank\n\n        if self.strata_rank == 0:\n            self.BestInnerBound = spcomm.BestInnerBound\n            self.BestOuterBound = spcomm.BestOuterBound\n        else: # the cylinder ranks don't track the inner / outer bounds\n            self.BestInnerBound = None\n            self.BestOuterBound = None\n\n        self._ran = True",
  "def on_hub(self):\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before finding out.\")\n        return (\"hub_class\" in self.opt_dict)",
  "def write_first_stage_solution(self, solution_file_name,\n            first_stage_solution_writer=first_stage_nonant_writer):\n        \"\"\" Write a solution file, if a solution is available, to the solution_file_name provided\n        Args:\n            solution_file_name : filename to write the solution to\n            first_stage_solution_writer (optional) : custom first stage solution writer function\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        winner = self._determine_innerbound_winner()\n        if winner:\n            self.spcomm.opt.write_first_stage_solution(solution_file_name,first_stage_solution_writer)",
  "def write_tree_solution(self, solution_directory_name,\n            scenario_tree_solution_writer=scenario_tree_solution_writer):\n        \"\"\" Write a tree solution directory, if available, to the solution_directory_name provided\n        Args:\n            solution_file_name : filename to write the solution to\n            scenario_tree_solution_writer (optional) : custom scenario solution writer function\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        winner = self._determine_innerbound_winner()\n        if winner:\n            self.spcomm.opt.write_tree_solution(solution_directory_name,scenario_tree_solution_writer)",
  "def local_nonant_cache(self):\n        \"\"\" Returns a dict with non-anticipative values at each local node\n            We assume that the optimization has been done before calling this\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        local_xhats = dict()\n        for k,s in self.spcomm.opt.local_scenarios.items():\n            for node in s._mpisppy_node_list:\n                if node.name not in local_xhats:\n                    local_xhats[node.name] = [\n                        value(var) for var in node.nonant_vardata_list]\n        return local_xhats",
  "def _determine_innerbound_winner(self):\n        if self.spcomm.global_rank == 0:\n            if self.spcomm.last_ib_idx is None:\n                best_strata_rank = -1\n                global_toc(\"No incumbent solution available to write!\")\n            else:\n                best_strata_rank = self.spcomm.last_ib_idx\n        else:\n            best_strata_rank = None\n    \n        best_strata_rank = self.spcomm.fullcomm.bcast(best_strata_rank, root=0)\n        return (self.spcomm.strata_rank == best_strata_rank)",
  "def profile(filename=None, comm=MPI.COMM_WORLD):\n    pass",
  "def _Compute_Xbar(opt, verbose=False):\n    \"\"\" Gather xbar and x squared bar for each node in the list and\n    distribute the values back to the scenarios.\n\n    Args:\n        opt (phbase or xhat_eval object): object with the local scenarios\n        verbose (boolean):\n            If True, prints verbose output.\n    \"\"\"\n\n    \"\"\"\n    Note:\n        Each scenario knows its own probability and its nodes.\n    Note:\n        The scenario only \"sends a reduce\" to its own node's comms so even\n        though the rank is a member of many comms, the scenario won't\n        contribute to the wrong node.\n    Note:\n        As of March 2019, we concatenate xbar and xsqbar into one long\n        vector to make it easier to use the current asynch code.\n    \"\"\"\n\n    nodenames = [] # to transmit to comms\n    local_concats = {}   # keys are tree node names\n    global_concats =  {} # values are concat of xbar and xsqbar\n\n    # we need to accumulate all local contributions before the reduce\n    for k,s in opt.local_scenarios.items():\n        nlens = s._mpisppy_data.nlens\n        for node in s._mpisppy_node_list:\n            if node.name not in nodenames:\n                ndn = node.name\n                nodenames.append(ndn)\n                mylen = 2*nlens[ndn]\n\n                local_concats[ndn] = np.zeros(mylen, dtype='d')\n                global_concats[ndn] = np.zeros(mylen, dtype='d')\n\n    # compute the local xbar and sqbar (put the sq in the 2nd 1/2 of concat)\n    for k,s in opt.local_scenarios.items():\n        nlens = s._mpisppy_data.nlens\n        for node in s._mpisppy_node_list:\n            ndn = node.name\n            nlen = nlens[ndn]\n\n            xbars = local_concats[ndn][:nlen]\n            xsqbars = local_concats[ndn][nlen:]\n\n            nonants_array = np.fromiter((v._value for v in node.nonant_vardata_list),\n                                        dtype='d', count=nlen)\n            probs = s._mpisppy_data.prob_coeff[ndn] * np.ones(nlen)\n            xbars += probs * nonants_array\n            xsqbars += probs * nonants_array**2\n\n    # compute node xbar values(reduction)\n    for nodename in nodenames:\n        opt.comms[nodename].Allreduce(\n            [local_concats[nodename], MPI.DOUBLE],\n            [global_concats[nodename], MPI.DOUBLE],\n            op=MPI.SUM)\n\n    # set the xbar and xsqbar in all the scenarios\n    for k,s in opt.local_scenarios.items():\n        logger.debug('  top of assign xbar loop for {} on rank {}'.\\\n                     format(k, opt.cylinder_rank))\n        nlens = s._mpisppy_data.nlens\n        for node in s._mpisppy_node_list:\n            ndn = node.name\n            nlen = nlens[ndn]\n\n            xbars = global_concats[ndn][:nlen]\n            xsqbars = global_concats[ndn][nlen:]\n\n            for i in range(nlen):\n                s._mpisppy_model.xbars[(ndn,i)]._value = xbars[i]\n                s._mpisppy_model.xsqbars[(ndn,i)]._value = xsqbars[i]\n                if verbose: # and opt.cylinder_rank == 0:\n                    print (\"cylinder rank, scen, node, var, xbar:\",\n                           opt.cylinder_rank, k, ndn, node.nonant_vardata_list[i].name,\n                           pyo.value(s._mpisppy_model.xbars[(ndn,i)]))",
  "def _Compute_Wbar(opt, verbose=False):\n    \"\"\" Seldom used (mainly for diagnostics); gather  Wbar for each node.\n\n    Args:\n        opt (phbase or xhat_eval object): object with the local scenarios\n        verbose (boolean):\n            If True, prints verbose output.\n\n    NOTE: for now, we will just report on the non-zeros, since that is\n          the only use-case as of August 2023\n    \"\"\"\n    nodenames = [] # to transmit to comms\n    local_concats = {}   # keys are tree node names\n    global_concats =  {} # values are concat of xbar and xsqbar\n\n    # we need to accumulate all local contributions before the reduce\n    for k,s in opt.local_scenarios.items():\n        nlens = s._mpisppy_data.nlens\n        for node in s._mpisppy_node_list:\n            if node.name not in nodenames:\n                ndn = node.name\n                nodenames.append(ndn)\n                mylen = nlens[ndn]\n\n                local_concats[ndn] = np.zeros(mylen, dtype='d')\n                global_concats[ndn] = np.zeros(mylen, dtype='d')\n\n    # compute the local Wbar\n    for k,s in opt.local_scenarios.items():\n        nlens = s._mpisppy_data.nlens\n        for node in s._mpisppy_node_list:\n            ndn = node.name\n            nlen = nlens[ndn]\n\n            Wbars = local_concats[ndn][:nlen]\n\n            # s._mpisppy_data.nonant_indices.keys() indexes the W Param\n            Wnonants_array = np.fromiter((pyo.value(s._mpisppy_model.W[idx]) for idx in s._mpisppy_data.nonant_indices if idx[0] == ndn),\n                                        dtype='d', count=nlen)\n            probs = s._mpisppy_data.prob_coeff[ndn] * np.ones(nlen)\n            Wbars += probs * Wnonants_array\n\n    # compute node xbar values(reduction)\n    for nodename in nodenames:\n        opt.comms[nodename].Allreduce(\n            [local_concats[nodename], MPI.DOUBLE],\n            [global_concats[nodename], MPI.DOUBLE],\n            op=MPI.SUM)\n\n    # check the Wbar\n    for k,s in opt.local_scenarios.items():\n        logger.debug('  top of Wbar loop for {} on rank {}'.\\\n                     format(k, opt.cylinder_rank))\n        nlens = s._mpisppy_data.nlens\n        for node in s._mpisppy_node_list:\n            ndn = node.name\n            nlen = nlens[ndn]\n\n            Wbars = global_concats[ndn][:nlen]\n\n            for i in range(nlen):\n                if abs(Wbars[i]) > opt.E1_tolerance and opt.cylinder_rank == 0:\n                    print(f\"EW={Wbars[i]} for {node.nonant_vardata_list[i].name}\")",
  "class PHBase(mpisppy.spopt.SPOpt):\n    \"\"\" Base class for all PH-based algorithms.\n\n        Based on mpi4py (but should run with, or without, mpi)\n        EVERY INDEX IS ZERO-BASED! (Except stages, which are one based).\n\n        Node names other than ROOT, although strings, must be a number or end\n        in a number because mpi4py comms need a number. PH using a smart\n        referencemodel that knows how to make its own tree nodes and just wants\n        a trailing number in the scenario name. Assume we have only non-leaf\n        nodes.\n\n        To check for rank 0 use self.cylinder_rank == 0.\n\n        Attributes:\n            local_scenarios (dict):\n                Dictionary mapping scenario names (strings) to scenarios (Pyomo\n                conrete model objects). These are only the scenarios managed by\n                the current rank (not all scenarios in the entire model).\n            comms (dict):\n                Dictionary mapping node names (strings) to MPI communicator\n                objects.\n            local_scenario_names (list):\n                List of local scenario names (strings). Should match the keys\n                of the local_scenarios dict.\n            current_solver_options (dict): from options, but callbacks might\n                Dictionary of solver options provided in options. Note that\n                callbacks could change these options.\n\n        Args:\n            options (dict):\n                Options for the PH algorithm.\n            all_scenario_names (list):\n                List of all scenario names in the model (strings).\n            scenario_creator (callable):\n                Function which take a scenario name (string) and returns a\n                Pyomo Concrete model with some things attached.\n            scenario_denouement (callable, optional):\n                Function which does post-processing and reporting.\n            all_nodenames (list, optional):\n                List of all node name (strings). Can be `None` for two-stage\n                problems.\n            mpicomm (MPI comm, optional):\n                MPI communicator to use between all scenarios. Default is\n                `MPI.COMM_WORLD`.\n            scenario_creator_kwargs (dict, optional):\n                Keyword arguments passed to `scenario_creator`.\n            extensions (object, optional):\n                PH extension object.\n            extension_kwargs (dict, optional):\n                Keyword arguments to pass to the extensions.\n            ph_converger (object, optional):\n                PH converger object.\n            rho_setter (callable, optional):\n                Function to set rho values throughout the PH algorithm.\n            variable_probability (callable, optional):\n                Function to set variable specific probabilities.\n\n    \"\"\"\n    def __init__(\n        self,\n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,\n        mpicomm=None,\n        scenario_creator_kwargs=None,\n        extensions=None,\n        extension_kwargs=None,\n        ph_converger=None,\n        rho_setter=None,\n        variable_probability=None,\n    ):\n        \"\"\" PHBase constructor. \"\"\"\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            extensions=extensions,\n            extension_kwargs=extension_kwargs,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            variable_probability=variable_probability,\n        )\n\n        global_toc(\"Initializing PHBase\")\n\n        # Note that options can be manipulated from outside on-the-fly.\n        # self.options (from super) will archive the original options.\n        self.options = options\n        self.options_check()\n        self.ph_converger = ph_converger\n        self.rho_setter = rho_setter\n\n        self.iter0_solver_options = options[\"iter0_solver_options\"]\n        self.iterk_solver_options = options[\"iterk_solver_options\"]\n        self.current_solver_options = self.iter0_solver_options\n\n        # flags to complete the invariant\n        self.convobject = None  # PH converger\n        self.attach_xbars()\n\n\n    def Compute_Xbar(self, verbose=False):\n        \"\"\" Gather xbar and x squared bar for each node in the list and\n        distribute the values back to the scenarios.\n\n        Args:\n            verbose (boolean):\n                If True, prints verbose output.\n        \"\"\"\n        _Compute_Xbar(self, verbose=verbose)\n\n\n    def Update_W(self, verbose):\n        \"\"\" Update the dual weights during the PH algorithm.\n\n        Args:\n            verbose (bool):\n                If True, displays verbose output during update.\n        \"\"\"\n        # Assumes the scenarios are up to date\n        for k,s in self.local_scenarios.items():\n            for ndn_i, nonant in s._mpisppy_data.nonant_indices.items():\n\n                ##if nonant._value == None:\n                ##    print(f\"***_value is None for nonant var {nonant.name}\")\n\n                xdiff = nonant._value \\\n                        - s._mpisppy_model.xbars[ndn_i]._value\n                s._mpisppy_model.W[ndn_i]._value += pyo.value(s._mpisppy_model.rho[ndn_i]) * xdiff\n                if verbose and self.cylinder_rank == 0:\n                    print (\"rank, node, scen, var, W\", ndn_i[0], k,\n                           self.cylinder_rank, nonant.name,\n                           pyo.value(s._mpisppy_model.W[ndn_i]))\n            # Special code for variable probabilities to mask W; rarely used.\n            if s._mpisppy_data.has_variable_probability:\n                for ndn_i in s._mpisppy_data.nonant_indices:\n                    (lndn, li) = ndn_i\n                    s._mpisppy_model.W[ndn_i] *= s._mpisppy_data.prob0_mask[lndn][li]\n\n\n    def convergence_diff(self):\n        \"\"\" Compute the convergence metric ||x_s - \\\\bar{x}||_1 / num_scenarios.\n\n            Returns:\n                float:\n                    The convergence metric ||x_s - \\\\bar{x}||_1 / num_scenarios.\n\n        \"\"\"\n        # Every scenario has its own node list, with a vardata list\n        global_diff = np.zeros(1)\n        local_diff = np.zeros(1)\n        varcount = 0\n        for k,s in self.local_scenarios.items():\n            for ndn_i, nonant in s._mpisppy_data.nonant_indices.items():\n                xval = nonant._value\n                xdiff = xval - s._mpisppy_model.xbars[ndn_i]._value\n                local_diff[0] += abs(xdiff)\n                varcount += 1\n        local_diff[0] /= varcount\n\n        self.comms[\"ROOT\"].Allreduce(local_diff, global_diff, op=MPI.SUM)\n\n        return global_diff[0] / self.n_proc\n\n\n    def _populate_W_cache(self, cache):\n        \"\"\" Copy the W values for noants *for all local scenarios*\n        Args:\n            cache (np vector) to receive the W's for all local scenarios (for sending)\n\n        NOTE: This is not the same as the nonant Vars because it puts all local W\n              values into the same cache and the cache is *not* attached to the scenario.\n\n        \"\"\"\n        ci = 0 # Cache index\n        for model in self.local_scenarios.values():\n            if (ci + len(model._mpisppy_data.nonant_indices)) >= len(cache):\n                tlen = len(model._mpisppy_data.nonant_indices) * len(self.local_scenarios)\n                raise RuntimeError(\"W cache length mismatch detected by \"\n                                   f\"{self.__class__.__name__} that has \"\n                                   f\"total W len {tlen} but passed cache len-1={len(cache)-1}; \"\n                                   f\"len(nonants)={len(model._mpisppy_data.nonant_indices)}\")\n            for ix in model._mpisppy_data.nonant_indices:\n                cache[ci] = pyo.value(model._mpisppy_model.W[ix])\n                ci += 1\n        assert(ci == len(cache) - 1)  # the other cylinder will fail above\n\n\n    def W_from_flat_list(self, flat_list):\n        \"\"\" Set the dual weight values (Ws) for all local scenarios from a\n        flat list.\n\n        Args:\n            flat_list (list):\n                One-dimensional list of dual weights.\n\n        Warning:\n            We are counting on Pyomo indices not to change order between list\n            creation and use.\n        \"\"\"\n        ci = 0 # Cache index\n        for model in self.local_scenarios.values():\n            for ndn_i in model._mpisppy_data.nonant_indices:\n                model._mpisppy_model.W[ndn_i].value = flat_list[ci]\n                ci += 1\n\n    def _use_rho_setter(self, verbose):\n        \"\"\" set rho values using a function self.rho_setter\n        that gives us a list of (id(vardata), rho)]\n        \"\"\"\n        if self.rho_setter is None:\n            return\n        didit = 0\n        skipped = 0\n        rho_setter_kwargs = self.options['rho_setter_kwargs'] \\\n                            if 'rho_setter_kwargs' in self.options \\\n                            else dict()\n        for sname, scenario in self.local_scenarios.items():\n            rholist = self.rho_setter(scenario, **rho_setter_kwargs)\n            for (vid, rho) in rholist:\n                (ndn, i) = scenario._mpisppy_data.varid_to_nonant_index[vid]\n                scenario._mpisppy_model.rho[(ndn, i)] = rho\n            didit += len(rholist)\n            skipped += len(scenario._mpisppy_data.varid_to_nonant_index) - didit\n        if verbose and self.cylinder_rank == 0:\n            print (\"rho_setter set\",didit,\"and skipped\",skipped)\n\n\n    def _disable_prox(self):\n        for k, scenario in self.local_scenarios.items():\n            scenario._mpisppy_model.prox_on = 0\n\n\n    def _disable_W(self):\n        # It would be odd to disable W and not prox.\n        # TODO: we should eliminate this method\n        #       probably not mathematically useful\n        for scenario in self.local_scenarios.values():\n            scenario._mpisppy_model.W_on = 0\n\n\n    def disable_W_and_prox(self):\n        self._disable_W()\n        self._disable_prox()\n\n\n    def _reenable_prox(self):\n        for k, scenario in self.local_scenarios.items():\n            scenario._mpisppy_model.prox_on = 1\n\n\n    def _reenable_W(self):\n        # TODO: we should eliminate this method\n        for k, scenario in self.local_scenarios.items():\n            scenario._mpisppy_model.W_on = 1\n\n\n    def reenable_W_and_prox(self):\n        self._reenable_W()\n        self._reenable_prox()\n\n\n    def post_solve_bound(self, solver_options=None, verbose=False):\n        ''' Compute a bound Lagrangian bound using the existing weights.\n\n        Args:\n            solver_options (dict, optional):\n                Options for these solves.\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n\n        Returns:\n            float:\n                An outer bound on the optimal objective function value.\n\n        Note:\n            This function overwrites current variable values. This is only\n            suitable for use at the end of the solves, or if you really know\n            what you are doing.  It is not suitable as a general, per-iteration\n            Lagrangian bound solver.\n        '''\n        if (self.cylinder_rank == 0):\n            print('Warning: Lagrangian bounds might not be correct in certain '\n                  'cases where there are integers not subject to '\n                  'non-anticipativity and those integers do not reach integrality.')\n        if (verbose and self.cylinder_rank == 0):\n            print('Beginning post-solve Lagrangian bound computation')\n\n        if (self.W_disabled):\n            self._reenable_W()\n        self._disable_prox()\n\n        # Fixed variables can lead to an invalid lower bound\n        self._restore_original_fixedness()\n\n        # If dis_prox=True, they are enabled at the end, and Ebound returns\n        # the incorrect value (unless you explicitly disable them again)\n        self.solve_loop(solver_options=solver_options,\n                        dis_prox=False, # Important\n                        gripe=True,\n                        tee=False,\n                        verbose=verbose)\n\n        bound = self.Ebound(verbose)\n\n        # A half-hearted attempt to restore the state\n        self._reenable_prox()\n\n        if (verbose and self.cylinder_rank == 0):\n            print(f'Post-solve Lagrangian bound: {bound:.4f}')\n        return bound\n\n\n    def solve_loop(self, solver_options=None,\n                   use_scenarios_not_subproblems=False,\n                   dtiming=False,\n                   dis_W=False,\n                   dis_prox=False,\n                   gripe=False,\n                   disable_pyomo_signal_handling=False,\n                   tee=False,\n                   verbose=False):\n        \"\"\" Loop over `local_subproblems` and solve them in a manner\n        dicated by the arguments.\n\n        In addition to changing the Var values in the scenarios, this function\n        also updates the `_PySP_feas_indictor` to indicate which scenarios were\n        feasible/infeasible.\n\n        Args:\n            solver_options (dict, optional):\n                The scenario solver options.\n            use_scenarios_not_subproblems (boolean, optional):\n                If True, solves individual scenario problems, not subproblems.\n                This distinction matters when using bundling. Default is False.\n            dtiming (boolean, optional):\n                If True, reports solve timing information. Default is False.\n            dis_W (boolean, optional):\n                If True, duals weights (Ws) are disabled before solve, then\n                re-enabled after solve. Default is False.\n            dis_prox (boolean, optional):\n                If True, prox terms are disabled before solve, then\n                re-enabled after solve. Default is False.\n            gripe (boolean, optional):\n                If True, output a message when a solve fails. Default is False.\n            disable_pyomo_signal_handling (boolean, optional):\n                True for asynchronous PH; ignored for persistent solvers.\n                Default False.\n            tee (boolean, optional):\n                If True, displays solver output. Default False.\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n        \"\"\"\n\n        \"\"\" Developer notes:\n\n        This function assumes that every scenario already has a\n        `_solver_plugin` attached.\n\n        I am not sure what happens with solver_options None for a persistent\n        solver. Do options persist?\n\n        set_objective takes care of W and prox changes.\n        \"\"\"\n        if dis_W and dis_prox:\n            self.disable_W_and_prox()\n        elif dis_W:\n            self._disable_W()\n        elif dis_prox:\n            self._disable_prox()\n\n        if self._prox_approx and (not self.prox_disabled):\n            self._update_prox_approx()\n\n        super().solve_loop(solver_options,\n                   use_scenarios_not_subproblems,\n                   dtiming,\n                   gripe,\n                   disable_pyomo_signal_handling,\n                   tee,\n                   verbose)\n\n        if dis_W and dis_prox:\n            self.reenable_W_and_prox()\n        elif dis_W:\n            self._reenable_W()\n        elif dis_prox:\n            self._reenable_prox()\n\n\n    def _update_prox_approx(self):\n        \"\"\"\n        update proximal term approximation by potentially\n        adding a linear cut near each current xvar value\n\n        NOTE: This is badly inefficient for bundles, but works\n        \"\"\"\n        tol = self.prox_approx_tol\n        for sn, s in self.local_scenarios.items():\n            persistent_solver = (s._solver_plugin if sputils.is_persistent(s._solver_plugin) else None)\n            for prox_approx_manager in s._mpisppy_data.xsqvar_prox_approx.values():\n                prox_approx_manager.check_tol_add_cut(tol, persistent_solver)\n\n\n    def attach_Ws_and_prox(self):\n        \"\"\" Attach the dual and prox terms to the models in `local_scenarios`.\n        \"\"\"\n        for (sname, scenario) in self.local_scenarios.items():\n            # these are bound by index to the vardata list at the node\n            scenario._mpisppy_model.W = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                        initialize=0.0,\n                                        mutable=True)\n\n            # create ph objective terms, but disabled\n            scenario._mpisppy_model.W_on = pyo.Param(initialize=0, mutable=True, within=pyo.Binary)\n\n            scenario._mpisppy_model.prox_on = pyo.Param(initialize=0, mutable=True, within=pyo.Binary)\n\n            # note that rho is per var and scenario here\n            scenario._mpisppy_model.rho = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                        mutable=True,\n                                        default=self.options[\"defaultPHrho\"])\n\n\n    @property\n    def W_disabled(self):\n        assert hasattr(self.local_scenarios[self.local_scenario_names[0]]._mpisppy_model, 'W_on')\n        return not bool(self.local_scenarios[self.local_scenario_names[0]]._mpisppy_model.W_on.value)\n\n\n    @property\n    def prox_disabled(self):\n        assert hasattr(self.local_scenarios[self.local_scenario_names[0]]._mpisppy_model, 'prox_on')\n        return not bool(self.local_scenarios[self.local_scenario_names[0]]._mpisppy_model.prox_on.value)\n\n\n    def attach_PH_to_objective(self, add_duals, add_prox):\n        \"\"\" Attach dual weight and prox terms to the objective function of the\n        models in `local_scenarios`.\n\n        Args:\n            add_duals (boolean):\n                If True, adds dual weight (Ws) to the objective.\n            add_prox (boolean):\n                If True, adds the prox term to the objective.\n        \"\"\"\n\n        if ('linearize_binary_proximal_terms' in self.options):\n            lin_bin_prox = self.options['linearize_binary_proximal_terms']\n        else:\n            lin_bin_prox = False\n\n        if ('linearize_proximal_terms' in self.options):\n            self._prox_approx = self.options['linearize_proximal_terms']\n            if 'proximal_linearization_tolerance' in self.options:\n                self.prox_approx_tol = self.options['proximal_linearization_tolerance']\n            else:\n                self.prox_approx_tol = 1.e-1\n            if 'initial_proximal_cut_count' in self.options:\n                initial_prox_cuts = self.options['initial_proximal_cut_count']\n            else:\n                initial_prox_cuts = 2\n        else:\n            self._prox_approx = False\n\n        for (sname, scenario) in self.local_scenarios.items():\n            \"\"\"Attach the dual and prox terms to the objective.\n            \"\"\"\n            if ((not add_duals) and (not add_prox)):\n                return\n            objfct = sputils.find_active_objective(scenario)\n            is_min_problem = objfct.is_minimizing()\n\n            xbars = scenario._mpisppy_model.xbars\n\n            if self._prox_approx:\n                # set-up pyomo IndexVar, but keep it sparse\n                # since some nonants might be binary\n                # Define the first cut to be _xsqvar >= 0\n                scenario._mpisppy_model.xsqvar = pyo.Var(scenario._mpisppy_data.nonant_indices, dense=False,\n                                            within=pyo.NonNegativeReals)\n                scenario._mpisppy_model.xsqvar_cuts = pyo.Constraint(scenario._mpisppy_data.nonant_indices, pyo.Integers)\n                scenario._mpisppy_data.xsqvar_prox_approx = {}\n            else:\n                scenario._mpisppy_model.xsqvar = None\n                scenario._mpisppy_data.xsqvar_prox_approx = False\n\n            ph_term = 0\n            # Dual term (weights W)\n            if (add_duals):\n                scenario._mpisppy_model.WExpr = pyo.Expression(expr=\\\n                        sum(scenario._mpisppy_model.W[ndn_i] * xvar \\\n                            for ndn_i, xvar in scenario._mpisppy_data.nonant_indices.items()) )\n                ph_term += scenario._mpisppy_model.W_on * scenario._mpisppy_model.WExpr\n\n            # Prox term (quadratic)\n            if (add_prox):\n                prox_expr = 0.\n                for ndn_i, xvar in scenario._mpisppy_data.nonant_indices.items():\n                    # expand (x - xbar)**2 to (x**2 - 2*xbar*x + xbar**2)\n                    # x**2 is the only qradratic term, which might be\n                    # dealt with differently depending on user-set options\n                    if xvar.is_binary() and (lin_bin_prox or self._prox_approx):\n                        xvarsqrd = xvar\n                    elif self._prox_approx:\n                        xvarsqrd = scenario._mpisppy_model.xsqvar[ndn_i]\n                        scenario._mpisppy_data.xsqvar_prox_approx[ndn_i] = \\\n                                ProxApproxManager(xvar, xvarsqrd, scenario._mpisppy_model.xsqvar_cuts, ndn_i, initial_prox_cuts)\n                    else:\n                        xvarsqrd = xvar**2\n                    prox_expr += (scenario._mpisppy_model.rho[ndn_i] / 2.0) * \\\n                                 (xvarsqrd - 2.0 * xbars[ndn_i] * xvar + xbars[ndn_i]**2)\n                scenario._mpisppy_model.ProxExpr = pyo.Expression(expr=prox_expr)\n                ph_term += scenario._mpisppy_model.prox_on * scenario._mpisppy_model.ProxExpr\n\n            if (is_min_problem):\n                objfct.expr += ph_term\n            else:\n                objfct.expr -= ph_term\n\n\n    def PH_Prep(\n        self,\n        attach_duals=True,\n        attach_prox=True,\n    ):\n        \"\"\" Set up PH objectives (duals and prox terms), and prepare\n        extensions, if available.\n\n        Args:\n            add_duals (boolean, optional):\n                If True, adds dual weight (Ws) to the objective. Default True.\n            add_prox (boolean, optional):\n                If True, adds prox terms to the objective. Default True.\n\n        Note:\n            This function constructs an Extension object if one was specified\n            at the time the PH object was created. It also calls the\n            `pre_iter0` method of the Extension object.\n        \"\"\"\n\n        self.attach_Ws_and_prox()\n        self.attach_PH_to_objective(attach_duals, attach_prox)\n\n\n    def options_check(self):\n        \"\"\" Check whether the options in the `options` attribute are\n        acceptable.\n\n        Required options are\n\n        - solver_name (string): The name of the solver to use.\n        - PHIterLimit (int): The maximum number of PH iterations to execute.\n        - defaultPHrho (float): The default value of rho (penalty parameter) to\n          use for PH.\n        - convthresh (float): The convergence tolerance of the PH algorithm.\n        - verbose (boolean): Flag indicating whether to display verbose output.\n        - display_progress (boolean): Flag indicating whether to display\n          information about the progression of the algorithm.\n        - iter0_solver_options (dict): Dictionary of solver options to use on\n          the first solve loop.\n        - iterk_solver_options (dict): Dictionary of solver options to use on\n          subsequent solve loops (after iteration 0).\n\n        \"\"\"\n        required = [\n            \"solver_name\", \"PHIterLimit\", \"defaultPHrho\",\n            \"convthresh\", \"verbose\", \"display_progress\",\n        ]\n        self._options_check(required, self.options)\n        # Display timing and display convergence detail are special for no good reason.\n        if \"display_timing\" not in self.options:\n            self.options[\"display_timing\"] = False\n        if \"display_convergence_detail\" not in self.options:\n            self.options[\"display_convergence_detail\"] = False\n\n\n    def Iter0(self):\n        \"\"\" Create solvers and perform the initial PH solve (with no dual\n        weights or prox terms).\n\n        This function quits() if the scenario probabilities do not sum to one,\n        or if any of the scenario subproblems are infeasible. It also calls the\n        `post_iter0` method of any extensions, and uses the rho setter (if\n        present) after the inital solve.\n\n        Returns:\n            float:\n                The so-called \"trivial bound\", i.e., the objective value of the\n                stochastic program with the nonanticipativity constraints\n                removed.\n        \"\"\"\n        if (self.extensions is not None):\n            self.extobject.pre_iter0()\n\n        verbose = self.options[\"verbose\"]\n        dprogress = self.options[\"display_progress\"]\n        dtiming = self.options[\"display_timing\"]\n        dconvergence_detail = self.options[\"display_convergence_detail\"]\n        have_extensions = self.extensions is not None\n        have_converger = self.ph_converger is not None\n\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"(rank0)\", msg)\n\n        self._PHIter = 0\n        self._save_original_nonants()\n\n        global_toc(\"Creating solvers\")\n        self._create_solvers()\n\n        teeme = (\"tee-rank0-solves\" in self.options\n                 and self.options['tee-rank0-solves']\n                 and self.cylinder_rank == 0\n                 )\n\n        if self.options[\"verbose\"]:\n            print (\"About to call PH Iter0 solve loop on rank={}\".format(self.cylinder_rank))\n        global_toc(\"Entering solve loop in PHBase.Iter0\")\n\n        self.solve_loop(solver_options=self.current_solver_options,\n                        dtiming=dtiming,\n                        gripe=True,\n                        tee=teeme,\n                        verbose=verbose)\n\n        if self.options[\"verbose\"]:\n            print (\"PH Iter0 solve loop complete on rank={}\".format(self.cylinder_rank))\n\n        self._update_E1()  # Apologies for doing this after the solves...\n        if (abs(1 - self.E1) > self.E1_tolerance):\n            if self.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.E1)\n                print(\"E1_tolerance = \", self.E1_tolerance)\n            quit()\n        feasP = self.feas_prob()\n        if feasP != self.E1:\n            if self.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Infeasibility detected; E_feas, E1=\", feasP, self.E1)\n            quit()\n\n        \"\"\"\n        with open('mpi.out-{}'.format(rank), 'w') as fd:\n            for sname in self.local_scenario_names:\n                fd.write('*** {} ***\\n'.format(sname))\n        \"\"\"\n        #global_toc('Rank: {} - Building and solving models 0th iteration'.format(rank), True)\n\n        #global_toc('Rank: {} - assigning rho'.format(rank), True)\n\n        if have_extensions:\n            self.extobject.post_iter0()\n\n        if self.spcomm is not None:\n            self.spcomm.sync()\n\n        if have_extensions:\n            self.extobject.post_iter0_after_sync()\n\n        if self.rho_setter is not None:\n            if self.cylinder_rank == 0:\n                self._use_rho_setter(verbose)\n            else:\n                self._use_rho_setter(False)\n\n        converged = False\n        if have_converger:\n            # Call the constructor of the converger object\n            self.convobject = self.ph_converger(self)\n        #global_toc('Rank: {} - Before iter loop'.format(self.cylinder_rank), True)\n        self.conv = None\n\n        self.trivial_bound = self.Ebound(verbose)\n\n        if dprogress and self.cylinder_rank == 0:\n            print(\"\")\n            print(\"After PH Iteration\",self._PHIter)\n            print(\"Trivial bound =\", self.trivial_bound)\n            print(\"PHBase Convergence Metric =\",self.conv)\n            print(\"Elapsed time: %6.2f\" % (time.perf_counter() - self.start_time))\n\n        if dconvergence_detail:\n            self.report_var_values_at_rank0(header=\"Convergence detail:\")\n\n        self.reenable_W_and_prox()\n\n        self.current_solver_options = self.options[\"iterk_solver_options\"]\n\n        return self.trivial_bound\n\n\n    def iterk_loop(self):\n        \"\"\" Perform all PH iterations after iteration 0.\n\n        This function terminates if any of the following occur:\n\n        1. The maximum number of iterations is reached.\n        2. The user specifies a converger, and the `is_converged()` method of\n           that converger returns True.\n        3. The hub tells it to terminate.\n        4. The user does not specify a converger, and the default convergence\n           criteria are met (i.e. the convergence value falls below the\n           user-specified threshold).\n\n        Args: None\n\n        \"\"\"\n        verbose = self.options[\"verbose\"]\n        have_extensions = self.extensions is not None\n        have_converger = self.ph_converger is not None\n        dprogress = self.options[\"display_progress\"]\n        dtiming = self.options[\"display_timing\"]\n        dconvergence_detail = self.options[\"display_convergence_detail\"]\n        self.conv = None\n\n        max_iterations = int(self.options[\"PHIterLimit\"])\n\n        for self._PHIter in range(1, max_iterations+1):\n            iteration_start_time = time.time()\n\n            if dprogress:\n                global_toc(f\"\\nInitiating PH Iteration {self._PHIter}\\n\", self.cylinder_rank == 0)\n\n            # Compute xbar\n            #global_toc('Rank: {} - Before Compute_Xbar'.format(self.cylinder_rank), True)\n            self.Compute_Xbar(verbose)\n            #global_toc('Rank: {} - After Compute_Xbar'.format(self.cylinder_rank), True)\n\n            # update the weights\n            self.Update_W(verbose)\n            #global_toc('Rank: {} - After Update_W'.format(self.cylinder_rank), True)\n\n            self.conv = self.convergence_diff()\n            #global_toc('Rank: {} - After convergence_diff'.format(self.cylinder_rank), True)\n            if have_extensions:\n                self.extobject.miditer()\n\n            # The hub object takes precedence\n            # over the converger, such that\n            # the spokes will always have the\n            # latest data, even at termination\n            if have_converger:\n                if self.convobject.is_converged():\n                    converged = True\n                    global_toc(\"User-supplied converger determined termination criterion reached\", self.cylinder_rank == 0)\n                    break\n            elif self.conv is not None:\n                if self.conv < self.options[\"convthresh\"]:\n                    converged = True\n                    global_toc(\"Convergence metric=%f dropped below user-supplied threshold=%f\" % (self.conv, self.options[\"convthresh\"]), self.cylinder_rank == 0)\n                    break\n\n            teeme = (\n                \"tee-rank0-solves\" in self.options\n                 and self.options[\"tee-rank0-solves\"]\n                and self.cylinder_rank == 0\n            )\n            self.solve_loop(\n                solver_options=self.current_solver_options,\n                dtiming=dtiming,\n                gripe=True,\n                disable_pyomo_signal_handling=False,\n                tee=teeme,\n                verbose=verbose\n            )\n\n            if have_extensions:\n                self.extobject.enditer()\n\n            if self.spcomm is not None:\n                self.spcomm.sync()\n                if self.spcomm.is_converged():\n                    global_toc(\"Cylinder convergence\", self.cylinder_rank == 0)\n                    break\n\n            if have_extensions:\n                self.extobject.enditer_after_sync()\n\n            if dprogress and self.cylinder_rank == 0:\n                print(\"\")\n                print(\"After PH Iteration\",self._PHIter)\n                print(\"Scaled PHBase Convergence Metric=\",self.conv)\n                print(\"Iteration time: %6.2f\" % (time.time() - iteration_start_time))\n                print(\"Elapsed time:   %6.2f\" % (time.perf_counter() - self.start_time))\n\n            if dconvergence_detail:\n                self.report_var_values_at_rank0(header=\"Convergence detail:\")\n\n        else: # no break, (self._PHIter == max_iterations)\n            # NOTE: If we return for any other reason things are reasonably in-sync.\n            #       due to the convergence check. However, here we return we'll be\n            #       out-of-sync because of the solve_loop could take vasty different\n            #       times on different threads. This can especially mess up finalization.\n            #       As a guard, we'll put a barrier here.\n            self.mpicomm.Barrier()\n            global_toc(\"Reached user-specified limit=%d on number of PH iterations\" % max_iterations, self.cylinder_rank == 0)\n\n\n    def post_loops(self, extensions=None):\n        \"\"\" Call scenario denouement methods, and report the expected objective\n        value.\n\n        Args:\n            extensions (object, optional):\n                PH extension object.\n        Returns:\n            float:\n                Pretty useless weighted, proxed objective value.\n        \"\"\"\n        verbose = self.options[\"verbose\"]\n        have_extensions = extensions is not None\n        dprogress = self.options[\"display_progress\"]\n        dtiming = self.options[\"display_timing\"]\n\n        # for reporting sanity\n        self.mpicomm.Barrier()\n\n        if self.cylinder_rank == 0 and dprogress:\n            print(\"\")\n            print(\"Invoking scenario reporting functions, if applicable\")\n            print(\"\")\n\n        if self.scenario_denouement is not None:\n            for sname,s in self.local_scenarios.items():\n                self.scenario_denouement(self.cylinder_rank, sname, s)\n\n        self.mpicomm.Barrier()\n\n        if self.cylinder_rank == 0 and dprogress:\n            print(\"\")\n            print(\"Invoking PH extension finalization, if applicable\")\n            print(\"\")\n\n        if have_extensions:\n            self.extobject.post_everything()\n\n        if self.ph_converger is not None and hasattr(self.ph_converger, 'post_everything'):\n            self.convobject.post_everything()\n\n        Eobj = self.Eobjective(verbose)\n\n        self.mpicomm.Barrier()\n\n        if dprogress and self.cylinder_rank == 0:\n            print(\"\")\n            print(\"Current ***weighted*** E[objective] =\", Eobj)\n            print(\"\")\n\n        if dtiming and self.cylinder_rank == 0:\n            print(\"\")\n            print(\"Cumulative execution time=%5.2f\" % (time.perf_counter()-self.start_time))\n            print(\"\")\n\n        return Eobj\n\n\n    def attach_xbars(self):\n        \"\"\" Attach xbar and xbar^2 Pyomo parameters to each model in\n        `local_scenarios`.\n        \"\"\"\n        for scenario in self.local_scenarios.values():\n            scenario._mpisppy_model.xbars = pyo.Param(\n                scenario._mpisppy_data.nonant_indices.keys(), initialize=0.0, mutable=True\n            )\n            scenario._mpisppy_model.xsqbars = pyo.Param(\n                scenario._mpisppy_data.nonant_indices.keys(), initialize=0.0, mutable=True\n            )",
  "def __init__(\n        self,\n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,\n        mpicomm=None,\n        scenario_creator_kwargs=None,\n        extensions=None,\n        extension_kwargs=None,\n        ph_converger=None,\n        rho_setter=None,\n        variable_probability=None,\n    ):\n        \"\"\" PHBase constructor. \"\"\"\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            extensions=extensions,\n            extension_kwargs=extension_kwargs,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            variable_probability=variable_probability,\n        )\n\n        global_toc(\"Initializing PHBase\")\n\n        # Note that options can be manipulated from outside on-the-fly.\n        # self.options (from super) will archive the original options.\n        self.options = options\n        self.options_check()\n        self.ph_converger = ph_converger\n        self.rho_setter = rho_setter\n\n        self.iter0_solver_options = options[\"iter0_solver_options\"]\n        self.iterk_solver_options = options[\"iterk_solver_options\"]\n        self.current_solver_options = self.iter0_solver_options\n\n        # flags to complete the invariant\n        self.convobject = None  # PH converger\n        self.attach_xbars()",
  "def Compute_Xbar(self, verbose=False):\n        \"\"\" Gather xbar and x squared bar for each node in the list and\n        distribute the values back to the scenarios.\n\n        Args:\n            verbose (boolean):\n                If True, prints verbose output.\n        \"\"\"\n        _Compute_Xbar(self, verbose=verbose)",
  "def Update_W(self, verbose):\n        \"\"\" Update the dual weights during the PH algorithm.\n\n        Args:\n            verbose (bool):\n                If True, displays verbose output during update.\n        \"\"\"\n        # Assumes the scenarios are up to date\n        for k,s in self.local_scenarios.items():\n            for ndn_i, nonant in s._mpisppy_data.nonant_indices.items():\n\n                ##if nonant._value == None:\n                ##    print(f\"***_value is None for nonant var {nonant.name}\")\n\n                xdiff = nonant._value \\\n                        - s._mpisppy_model.xbars[ndn_i]._value\n                s._mpisppy_model.W[ndn_i]._value += pyo.value(s._mpisppy_model.rho[ndn_i]) * xdiff\n                if verbose and self.cylinder_rank == 0:\n                    print (\"rank, node, scen, var, W\", ndn_i[0], k,\n                           self.cylinder_rank, nonant.name,\n                           pyo.value(s._mpisppy_model.W[ndn_i]))\n            # Special code for variable probabilities to mask W; rarely used.\n            if s._mpisppy_data.has_variable_probability:\n                for ndn_i in s._mpisppy_data.nonant_indices:\n                    (lndn, li) = ndn_i\n                    s._mpisppy_model.W[ndn_i] *= s._mpisppy_data.prob0_mask[lndn][li]",
  "def convergence_diff(self):\n        \"\"\" Compute the convergence metric ||x_s - \\\\bar{x}||_1 / num_scenarios.\n\n            Returns:\n                float:\n                    The convergence metric ||x_s - \\\\bar{x}||_1 / num_scenarios.\n\n        \"\"\"\n        # Every scenario has its own node list, with a vardata list\n        global_diff = np.zeros(1)\n        local_diff = np.zeros(1)\n        varcount = 0\n        for k,s in self.local_scenarios.items():\n            for ndn_i, nonant in s._mpisppy_data.nonant_indices.items():\n                xval = nonant._value\n                xdiff = xval - s._mpisppy_model.xbars[ndn_i]._value\n                local_diff[0] += abs(xdiff)\n                varcount += 1\n        local_diff[0] /= varcount\n\n        self.comms[\"ROOT\"].Allreduce(local_diff, global_diff, op=MPI.SUM)\n\n        return global_diff[0] / self.n_proc",
  "def _populate_W_cache(self, cache):\n        \"\"\" Copy the W values for noants *for all local scenarios*\n        Args:\n            cache (np vector) to receive the W's for all local scenarios (for sending)\n\n        NOTE: This is not the same as the nonant Vars because it puts all local W\n              values into the same cache and the cache is *not* attached to the scenario.\n\n        \"\"\"\n        ci = 0 # Cache index\n        for model in self.local_scenarios.values():\n            if (ci + len(model._mpisppy_data.nonant_indices)) >= len(cache):\n                tlen = len(model._mpisppy_data.nonant_indices) * len(self.local_scenarios)\n                raise RuntimeError(\"W cache length mismatch detected by \"\n                                   f\"{self.__class__.__name__} that has \"\n                                   f\"total W len {tlen} but passed cache len-1={len(cache)-1}; \"\n                                   f\"len(nonants)={len(model._mpisppy_data.nonant_indices)}\")\n            for ix in model._mpisppy_data.nonant_indices:\n                cache[ci] = pyo.value(model._mpisppy_model.W[ix])\n                ci += 1\n        assert(ci == len(cache) - 1)",
  "def W_from_flat_list(self, flat_list):\n        \"\"\" Set the dual weight values (Ws) for all local scenarios from a\n        flat list.\n\n        Args:\n            flat_list (list):\n                One-dimensional list of dual weights.\n\n        Warning:\n            We are counting on Pyomo indices not to change order between list\n            creation and use.\n        \"\"\"\n        ci = 0 # Cache index\n        for model in self.local_scenarios.values():\n            for ndn_i in model._mpisppy_data.nonant_indices:\n                model._mpisppy_model.W[ndn_i].value = flat_list[ci]\n                ci += 1",
  "def _use_rho_setter(self, verbose):\n        \"\"\" set rho values using a function self.rho_setter\n        that gives us a list of (id(vardata), rho)]\n        \"\"\"\n        if self.rho_setter is None:\n            return\n        didit = 0\n        skipped = 0\n        rho_setter_kwargs = self.options['rho_setter_kwargs'] \\\n                            if 'rho_setter_kwargs' in self.options \\\n                            else dict()\n        for sname, scenario in self.local_scenarios.items():\n            rholist = self.rho_setter(scenario, **rho_setter_kwargs)\n            for (vid, rho) in rholist:\n                (ndn, i) = scenario._mpisppy_data.varid_to_nonant_index[vid]\n                scenario._mpisppy_model.rho[(ndn, i)] = rho\n            didit += len(rholist)\n            skipped += len(scenario._mpisppy_data.varid_to_nonant_index) - didit\n        if verbose and self.cylinder_rank == 0:\n            print (\"rho_setter set\",didit,\"and skipped\",skipped)",
  "def _disable_prox(self):\n        for k, scenario in self.local_scenarios.items():\n            scenario._mpisppy_model.prox_on = 0",
  "def _disable_W(self):\n        # It would be odd to disable W and not prox.\n        # TODO: we should eliminate this method\n        #       probably not mathematically useful\n        for scenario in self.local_scenarios.values():\n            scenario._mpisppy_model.W_on = 0",
  "def disable_W_and_prox(self):\n        self._disable_W()\n        self._disable_prox()",
  "def _reenable_prox(self):\n        for k, scenario in self.local_scenarios.items():\n            scenario._mpisppy_model.prox_on = 1",
  "def _reenable_W(self):\n        # TODO: we should eliminate this method\n        for k, scenario in self.local_scenarios.items():\n            scenario._mpisppy_model.W_on = 1",
  "def reenable_W_and_prox(self):\n        self._reenable_W()\n        self._reenable_prox()",
  "def post_solve_bound(self, solver_options=None, verbose=False):\n        ''' Compute a bound Lagrangian bound using the existing weights.\n\n        Args:\n            solver_options (dict, optional):\n                Options for these solves.\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n\n        Returns:\n            float:\n                An outer bound on the optimal objective function value.\n\n        Note:\n            This function overwrites current variable values. This is only\n            suitable for use at the end of the solves, or if you really know\n            what you are doing.  It is not suitable as a general, per-iteration\n            Lagrangian bound solver.\n        '''\n        if (self.cylinder_rank == 0):\n            print('Warning: Lagrangian bounds might not be correct in certain '\n                  'cases where there are integers not subject to '\n                  'non-anticipativity and those integers do not reach integrality.')\n        if (verbose and self.cylinder_rank == 0):\n            print('Beginning post-solve Lagrangian bound computation')\n\n        if (self.W_disabled):\n            self._reenable_W()\n        self._disable_prox()\n\n        # Fixed variables can lead to an invalid lower bound\n        self._restore_original_fixedness()\n\n        # If dis_prox=True, they are enabled at the end, and Ebound returns\n        # the incorrect value (unless you explicitly disable them again)\n        self.solve_loop(solver_options=solver_options,\n                        dis_prox=False, # Important\n                        gripe=True,\n                        tee=False,\n                        verbose=verbose)\n\n        bound = self.Ebound(verbose)\n\n        # A half-hearted attempt to restore the state\n        self._reenable_prox()\n\n        if (verbose and self.cylinder_rank == 0):\n            print(f'Post-solve Lagrangian bound: {bound:.4f}')\n        return bound",
  "def solve_loop(self, solver_options=None,\n                   use_scenarios_not_subproblems=False,\n                   dtiming=False,\n                   dis_W=False,\n                   dis_prox=False,\n                   gripe=False,\n                   disable_pyomo_signal_handling=False,\n                   tee=False,\n                   verbose=False):\n        \"\"\" Loop over `local_subproblems` and solve them in a manner\n        dicated by the arguments.\n\n        In addition to changing the Var values in the scenarios, this function\n        also updates the `_PySP_feas_indictor` to indicate which scenarios were\n        feasible/infeasible.\n\n        Args:\n            solver_options (dict, optional):\n                The scenario solver options.\n            use_scenarios_not_subproblems (boolean, optional):\n                If True, solves individual scenario problems, not subproblems.\n                This distinction matters when using bundling. Default is False.\n            dtiming (boolean, optional):\n                If True, reports solve timing information. Default is False.\n            dis_W (boolean, optional):\n                If True, duals weights (Ws) are disabled before solve, then\n                re-enabled after solve. Default is False.\n            dis_prox (boolean, optional):\n                If True, prox terms are disabled before solve, then\n                re-enabled after solve. Default is False.\n            gripe (boolean, optional):\n                If True, output a message when a solve fails. Default is False.\n            disable_pyomo_signal_handling (boolean, optional):\n                True for asynchronous PH; ignored for persistent solvers.\n                Default False.\n            tee (boolean, optional):\n                If True, displays solver output. Default False.\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n        \"\"\"\n\n        \"\"\" Developer notes:\n\n        This function assumes that every scenario already has a\n        `_solver_plugin` attached.\n\n        I am not sure what happens with solver_options None for a persistent\n        solver. Do options persist?\n\n        set_objective takes care of W and prox changes.\n        \"\"\"\n        if dis_W and dis_prox:\n            self.disable_W_and_prox()\n        elif dis_W:\n            self._disable_W()\n        elif dis_prox:\n            self._disable_prox()\n\n        if self._prox_approx and (not self.prox_disabled):\n            self._update_prox_approx()\n\n        super().solve_loop(solver_options,\n                   use_scenarios_not_subproblems,\n                   dtiming,\n                   gripe,\n                   disable_pyomo_signal_handling,\n                   tee,\n                   verbose)\n\n        if dis_W and dis_prox:\n            self.reenable_W_and_prox()\n        elif dis_W:\n            self._reenable_W()\n        elif dis_prox:\n            self._reenable_prox()",
  "def _update_prox_approx(self):\n        \"\"\"\n        update proximal term approximation by potentially\n        adding a linear cut near each current xvar value\n\n        NOTE: This is badly inefficient for bundles, but works\n        \"\"\"\n        tol = self.prox_approx_tol\n        for sn, s in self.local_scenarios.items():\n            persistent_solver = (s._solver_plugin if sputils.is_persistent(s._solver_plugin) else None)\n            for prox_approx_manager in s._mpisppy_data.xsqvar_prox_approx.values():\n                prox_approx_manager.check_tol_add_cut(tol, persistent_solver)",
  "def attach_Ws_and_prox(self):\n        \"\"\" Attach the dual and prox terms to the models in `local_scenarios`.\n        \"\"\"\n        for (sname, scenario) in self.local_scenarios.items():\n            # these are bound by index to the vardata list at the node\n            scenario._mpisppy_model.W = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                        initialize=0.0,\n                                        mutable=True)\n\n            # create ph objective terms, but disabled\n            scenario._mpisppy_model.W_on = pyo.Param(initialize=0, mutable=True, within=pyo.Binary)\n\n            scenario._mpisppy_model.prox_on = pyo.Param(initialize=0, mutable=True, within=pyo.Binary)\n\n            # note that rho is per var and scenario here\n            scenario._mpisppy_model.rho = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                        mutable=True,\n                                        default=self.options[\"defaultPHrho\"])",
  "def W_disabled(self):\n        assert hasattr(self.local_scenarios[self.local_scenario_names[0]]._mpisppy_model, 'W_on')\n        return not bool(self.local_scenarios[self.local_scenario_names[0]]._mpisppy_model.W_on.value)",
  "def prox_disabled(self):\n        assert hasattr(self.local_scenarios[self.local_scenario_names[0]]._mpisppy_model, 'prox_on')\n        return not bool(self.local_scenarios[self.local_scenario_names[0]]._mpisppy_model.prox_on.value)",
  "def attach_PH_to_objective(self, add_duals, add_prox):\n        \"\"\" Attach dual weight and prox terms to the objective function of the\n        models in `local_scenarios`.\n\n        Args:\n            add_duals (boolean):\n                If True, adds dual weight (Ws) to the objective.\n            add_prox (boolean):\n                If True, adds the prox term to the objective.\n        \"\"\"\n\n        if ('linearize_binary_proximal_terms' in self.options):\n            lin_bin_prox = self.options['linearize_binary_proximal_terms']\n        else:\n            lin_bin_prox = False\n\n        if ('linearize_proximal_terms' in self.options):\n            self._prox_approx = self.options['linearize_proximal_terms']\n            if 'proximal_linearization_tolerance' in self.options:\n                self.prox_approx_tol = self.options['proximal_linearization_tolerance']\n            else:\n                self.prox_approx_tol = 1.e-1\n            if 'initial_proximal_cut_count' in self.options:\n                initial_prox_cuts = self.options['initial_proximal_cut_count']\n            else:\n                initial_prox_cuts = 2\n        else:\n            self._prox_approx = False\n\n        for (sname, scenario) in self.local_scenarios.items():\n            \"\"\"Attach the dual and prox terms to the objective.\n            \"\"\"\n            if ((not add_duals) and (not add_prox)):\n                return\n            objfct = sputils.find_active_objective(scenario)\n            is_min_problem = objfct.is_minimizing()\n\n            xbars = scenario._mpisppy_model.xbars\n\n            if self._prox_approx:\n                # set-up pyomo IndexVar, but keep it sparse\n                # since some nonants might be binary\n                # Define the first cut to be _xsqvar >= 0\n                scenario._mpisppy_model.xsqvar = pyo.Var(scenario._mpisppy_data.nonant_indices, dense=False,\n                                            within=pyo.NonNegativeReals)\n                scenario._mpisppy_model.xsqvar_cuts = pyo.Constraint(scenario._mpisppy_data.nonant_indices, pyo.Integers)\n                scenario._mpisppy_data.xsqvar_prox_approx = {}\n            else:\n                scenario._mpisppy_model.xsqvar = None\n                scenario._mpisppy_data.xsqvar_prox_approx = False\n\n            ph_term = 0\n            # Dual term (weights W)\n            if (add_duals):\n                scenario._mpisppy_model.WExpr = pyo.Expression(expr=\\\n                        sum(scenario._mpisppy_model.W[ndn_i] * xvar \\\n                            for ndn_i, xvar in scenario._mpisppy_data.nonant_indices.items()) )\n                ph_term += scenario._mpisppy_model.W_on * scenario._mpisppy_model.WExpr\n\n            # Prox term (quadratic)\n            if (add_prox):\n                prox_expr = 0.\n                for ndn_i, xvar in scenario._mpisppy_data.nonant_indices.items():\n                    # expand (x - xbar)**2 to (x**2 - 2*xbar*x + xbar**2)\n                    # x**2 is the only qradratic term, which might be\n                    # dealt with differently depending on user-set options\n                    if xvar.is_binary() and (lin_bin_prox or self._prox_approx):\n                        xvarsqrd = xvar\n                    elif self._prox_approx:\n                        xvarsqrd = scenario._mpisppy_model.xsqvar[ndn_i]\n                        scenario._mpisppy_data.xsqvar_prox_approx[ndn_i] = \\\n                                ProxApproxManager(xvar, xvarsqrd, scenario._mpisppy_model.xsqvar_cuts, ndn_i, initial_prox_cuts)\n                    else:\n                        xvarsqrd = xvar**2\n                    prox_expr += (scenario._mpisppy_model.rho[ndn_i] / 2.0) * \\\n                                 (xvarsqrd - 2.0 * xbars[ndn_i] * xvar + xbars[ndn_i]**2)\n                scenario._mpisppy_model.ProxExpr = pyo.Expression(expr=prox_expr)\n                ph_term += scenario._mpisppy_model.prox_on * scenario._mpisppy_model.ProxExpr\n\n            if (is_min_problem):\n                objfct.expr += ph_term\n            else:\n                objfct.expr -= ph_term",
  "def PH_Prep(\n        self,\n        attach_duals=True,\n        attach_prox=True,\n    ):\n        \"\"\" Set up PH objectives (duals and prox terms), and prepare\n        extensions, if available.\n\n        Args:\n            add_duals (boolean, optional):\n                If True, adds dual weight (Ws) to the objective. Default True.\n            add_prox (boolean, optional):\n                If True, adds prox terms to the objective. Default True.\n\n        Note:\n            This function constructs an Extension object if one was specified\n            at the time the PH object was created. It also calls the\n            `pre_iter0` method of the Extension object.\n        \"\"\"\n\n        self.attach_Ws_and_prox()\n        self.attach_PH_to_objective(attach_duals, attach_prox)",
  "def options_check(self):\n        \"\"\" Check whether the options in the `options` attribute are\n        acceptable.\n\n        Required options are\n\n        - solver_name (string): The name of the solver to use.\n        - PHIterLimit (int): The maximum number of PH iterations to execute.\n        - defaultPHrho (float): The default value of rho (penalty parameter) to\n          use for PH.\n        - convthresh (float): The convergence tolerance of the PH algorithm.\n        - verbose (boolean): Flag indicating whether to display verbose output.\n        - display_progress (boolean): Flag indicating whether to display\n          information about the progression of the algorithm.\n        - iter0_solver_options (dict): Dictionary of solver options to use on\n          the first solve loop.\n        - iterk_solver_options (dict): Dictionary of solver options to use on\n          subsequent solve loops (after iteration 0).\n\n        \"\"\"\n        required = [\n            \"solver_name\", \"PHIterLimit\", \"defaultPHrho\",\n            \"convthresh\", \"verbose\", \"display_progress\",\n        ]\n        self._options_check(required, self.options)\n        # Display timing and display convergence detail are special for no good reason.\n        if \"display_timing\" not in self.options:\n            self.options[\"display_timing\"] = False\n        if \"display_convergence_detail\" not in self.options:\n            self.options[\"display_convergence_detail\"] = False",
  "def Iter0(self):\n        \"\"\" Create solvers and perform the initial PH solve (with no dual\n        weights or prox terms).\n\n        This function quits() if the scenario probabilities do not sum to one,\n        or if any of the scenario subproblems are infeasible. It also calls the\n        `post_iter0` method of any extensions, and uses the rho setter (if\n        present) after the inital solve.\n\n        Returns:\n            float:\n                The so-called \"trivial bound\", i.e., the objective value of the\n                stochastic program with the nonanticipativity constraints\n                removed.\n        \"\"\"\n        if (self.extensions is not None):\n            self.extobject.pre_iter0()\n\n        verbose = self.options[\"verbose\"]\n        dprogress = self.options[\"display_progress\"]\n        dtiming = self.options[\"display_timing\"]\n        dconvergence_detail = self.options[\"display_convergence_detail\"]\n        have_extensions = self.extensions is not None\n        have_converger = self.ph_converger is not None\n\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"(rank0)\", msg)\n\n        self._PHIter = 0\n        self._save_original_nonants()\n\n        global_toc(\"Creating solvers\")\n        self._create_solvers()\n\n        teeme = (\"tee-rank0-solves\" in self.options\n                 and self.options['tee-rank0-solves']\n                 and self.cylinder_rank == 0\n                 )\n\n        if self.options[\"verbose\"]:\n            print (\"About to call PH Iter0 solve loop on rank={}\".format(self.cylinder_rank))\n        global_toc(\"Entering solve loop in PHBase.Iter0\")\n\n        self.solve_loop(solver_options=self.current_solver_options,\n                        dtiming=dtiming,\n                        gripe=True,\n                        tee=teeme,\n                        verbose=verbose)\n\n        if self.options[\"verbose\"]:\n            print (\"PH Iter0 solve loop complete on rank={}\".format(self.cylinder_rank))\n\n        self._update_E1()  # Apologies for doing this after the solves...\n        if (abs(1 - self.E1) > self.E1_tolerance):\n            if self.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.E1)\n                print(\"E1_tolerance = \", self.E1_tolerance)\n            quit()\n        feasP = self.feas_prob()\n        if feasP != self.E1:\n            if self.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Infeasibility detected; E_feas, E1=\", feasP, self.E1)\n            quit()\n\n        \"\"\"\n        with open('mpi.out-{}'.format(rank), 'w') as fd:\n            for sname in self.local_scenario_names:\n                fd.write('*** {} ***\\n'.format(sname))\n        \"\"\"\n        #global_toc('Rank: {} - Building and solving models 0th iteration'.format(rank), True)\n\n        #global_toc('Rank: {} - assigning rho'.format(rank), True)\n\n        if have_extensions:\n            self.extobject.post_iter0()\n\n        if self.spcomm is not None:\n            self.spcomm.sync()\n\n        if have_extensions:\n            self.extobject.post_iter0_after_sync()\n\n        if self.rho_setter is not None:\n            if self.cylinder_rank == 0:\n                self._use_rho_setter(verbose)\n            else:\n                self._use_rho_setter(False)\n\n        converged = False\n        if have_converger:\n            # Call the constructor of the converger object\n            self.convobject = self.ph_converger(self)\n        #global_toc('Rank: {} - Before iter loop'.format(self.cylinder_rank), True)\n        self.conv = None\n\n        self.trivial_bound = self.Ebound(verbose)\n\n        if dprogress and self.cylinder_rank == 0:\n            print(\"\")\n            print(\"After PH Iteration\",self._PHIter)\n            print(\"Trivial bound =\", self.trivial_bound)\n            print(\"PHBase Convergence Metric =\",self.conv)\n            print(\"Elapsed time: %6.2f\" % (time.perf_counter() - self.start_time))\n\n        if dconvergence_detail:\n            self.report_var_values_at_rank0(header=\"Convergence detail:\")\n\n        self.reenable_W_and_prox()\n\n        self.current_solver_options = self.options[\"iterk_solver_options\"]\n\n        return self.trivial_bound",
  "def iterk_loop(self):\n        \"\"\" Perform all PH iterations after iteration 0.\n\n        This function terminates if any of the following occur:\n\n        1. The maximum number of iterations is reached.\n        2. The user specifies a converger, and the `is_converged()` method of\n           that converger returns True.\n        3. The hub tells it to terminate.\n        4. The user does not specify a converger, and the default convergence\n           criteria are met (i.e. the convergence value falls below the\n           user-specified threshold).\n\n        Args: None\n\n        \"\"\"\n        verbose = self.options[\"verbose\"]\n        have_extensions = self.extensions is not None\n        have_converger = self.ph_converger is not None\n        dprogress = self.options[\"display_progress\"]\n        dtiming = self.options[\"display_timing\"]\n        dconvergence_detail = self.options[\"display_convergence_detail\"]\n        self.conv = None\n\n        max_iterations = int(self.options[\"PHIterLimit\"])\n\n        for self._PHIter in range(1, max_iterations+1):\n            iteration_start_time = time.time()\n\n            if dprogress:\n                global_toc(f\"\\nInitiating PH Iteration {self._PHIter}\\n\", self.cylinder_rank == 0)\n\n            # Compute xbar\n            #global_toc('Rank: {} - Before Compute_Xbar'.format(self.cylinder_rank), True)\n            self.Compute_Xbar(verbose)\n            #global_toc('Rank: {} - After Compute_Xbar'.format(self.cylinder_rank), True)\n\n            # update the weights\n            self.Update_W(verbose)\n            #global_toc('Rank: {} - After Update_W'.format(self.cylinder_rank), True)\n\n            self.conv = self.convergence_diff()\n            #global_toc('Rank: {} - After convergence_diff'.format(self.cylinder_rank), True)\n            if have_extensions:\n                self.extobject.miditer()\n\n            # The hub object takes precedence\n            # over the converger, such that\n            # the spokes will always have the\n            # latest data, even at termination\n            if have_converger:\n                if self.convobject.is_converged():\n                    converged = True\n                    global_toc(\"User-supplied converger determined termination criterion reached\", self.cylinder_rank == 0)\n                    break\n            elif self.conv is not None:\n                if self.conv < self.options[\"convthresh\"]:\n                    converged = True\n                    global_toc(\"Convergence metric=%f dropped below user-supplied threshold=%f\" % (self.conv, self.options[\"convthresh\"]), self.cylinder_rank == 0)\n                    break\n\n            teeme = (\n                \"tee-rank0-solves\" in self.options\n                 and self.options[\"tee-rank0-solves\"]\n                and self.cylinder_rank == 0\n            )\n            self.solve_loop(\n                solver_options=self.current_solver_options,\n                dtiming=dtiming,\n                gripe=True,\n                disable_pyomo_signal_handling=False,\n                tee=teeme,\n                verbose=verbose\n            )\n\n            if have_extensions:\n                self.extobject.enditer()\n\n            if self.spcomm is not None:\n                self.spcomm.sync()\n                if self.spcomm.is_converged():\n                    global_toc(\"Cylinder convergence\", self.cylinder_rank == 0)\n                    break\n\n            if have_extensions:\n                self.extobject.enditer_after_sync()\n\n            if dprogress and self.cylinder_rank == 0:\n                print(\"\")\n                print(\"After PH Iteration\",self._PHIter)\n                print(\"Scaled PHBase Convergence Metric=\",self.conv)\n                print(\"Iteration time: %6.2f\" % (time.time() - iteration_start_time))\n                print(\"Elapsed time:   %6.2f\" % (time.perf_counter() - self.start_time))\n\n            if dconvergence_detail:\n                self.report_var_values_at_rank0(header=\"Convergence detail:\")\n\n        else: # no break, (self._PHIter == max_iterations)\n            # NOTE: If we return for any other reason things are reasonably in-sync.\n            #       due to the convergence check. However, here we return we'll be\n            #       out-of-sync because of the solve_loop could take vasty different\n            #       times on different threads. This can especially mess up finalization.\n            #       As a guard, we'll put a barrier here.\n            self.mpicomm.Barrier()\n            global_toc(\"Reached user-specified limit=%d on number of PH iterations\" % max_iterations, self.cylinder_rank == 0)",
  "def post_loops(self, extensions=None):\n        \"\"\" Call scenario denouement methods, and report the expected objective\n        value.\n\n        Args:\n            extensions (object, optional):\n                PH extension object.\n        Returns:\n            float:\n                Pretty useless weighted, proxed objective value.\n        \"\"\"\n        verbose = self.options[\"verbose\"]\n        have_extensions = extensions is not None\n        dprogress = self.options[\"display_progress\"]\n        dtiming = self.options[\"display_timing\"]\n\n        # for reporting sanity\n        self.mpicomm.Barrier()\n\n        if self.cylinder_rank == 0 and dprogress:\n            print(\"\")\n            print(\"Invoking scenario reporting functions, if applicable\")\n            print(\"\")\n\n        if self.scenario_denouement is not None:\n            for sname,s in self.local_scenarios.items():\n                self.scenario_denouement(self.cylinder_rank, sname, s)\n\n        self.mpicomm.Barrier()\n\n        if self.cylinder_rank == 0 and dprogress:\n            print(\"\")\n            print(\"Invoking PH extension finalization, if applicable\")\n            print(\"\")\n\n        if have_extensions:\n            self.extobject.post_everything()\n\n        if self.ph_converger is not None and hasattr(self.ph_converger, 'post_everything'):\n            self.convobject.post_everything()\n\n        Eobj = self.Eobjective(verbose)\n\n        self.mpicomm.Barrier()\n\n        if dprogress and self.cylinder_rank == 0:\n            print(\"\")\n            print(\"Current ***weighted*** E[objective] =\", Eobj)\n            print(\"\")\n\n        if dtiming and self.cylinder_rank == 0:\n            print(\"\")\n            print(\"Cumulative execution time=%5.2f\" % (time.perf_counter()-self.start_time))\n            print(\"\")\n\n        return Eobj",
  "def attach_xbars(self):\n        \"\"\" Attach xbar and xbar^2 Pyomo parameters to each model in\n        `local_scenarios`.\n        \"\"\"\n        for scenario in self.local_scenarios.values():\n            scenario._mpisppy_model.xbars = pyo.Param(\n                scenario._mpisppy_data.nonant_indices.keys(), initialize=0.0, mutable=True\n            )\n            scenario._mpisppy_model.xsqbars = pyo.Param(\n                scenario._mpisppy_data.nonant_indices.keys(), initialize=0.0, mutable=True\n            )",
  "def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"(rank0)\", msg)",
  "def build_vardatalist(self, model, varlist=None):\n    \"\"\"\n    Convert a list of pyomo variables to a list of SimpleVar and _GeneralVarData. If varlist is none, builds a\n    list of all variables in the model. Written by CD Laird\n\n    Parameters\n    ----------\n    model: ConcreteModel\n    varlist: None or list of pyo.Var\n    \"\"\"\n    vardatalist = None\n\n    # if the varlist is None, then assume we want all the active variables\n    if varlist is None:\n        raise RuntimeError(\"varlist is None in scenario_tree.build_vardatalist\")\n        vardatalist = [v for v in model.component_data_objects(pyo.Var, active=True, sort=True)]\n    elif isinstance(varlist, (pyo.Var, IndexedComponent_slice)):\n        # user provided a variable, not a list of variables. Let's work with it anyway\n        varlist = [varlist]\n\n    if vardatalist is None:\n        # expand any indexed components in the list to their\n        # component data objects\n        vardatalist = list()\n        for v in varlist:\n            if isinstance(v, IndexedComponent_slice):\n                vardatalist.extend(v.__iter__())\n            elif v.is_indexed():\n                vardatalist.extend((v[i] for i in sorted(v.keys())))\n            else:\n                vardatalist.append(v)\n    return vardatalist",
  "class ScenarioNode:\n    \"\"\"Store a node in the scenario tree.\n\n    Note:\n      This can only be created programatically from a scenario\n      creation function. (maybe that function reads data)\n\n    Args:\n      name (str): name of the node; one node must be named \"ROOT\"\n      cond_prob (float): conditional probability\n      stage (int): stage number (root is 1)\n      cost_expression (pyo Expression or Var):  stage cost \n      nonant_list (list of pyo Var, Vardata or slices): the Vars that\n              require nonanticipativity at the node (might not be a list)\n      scen_model (pyo concrete model): the (probably not 'a') concrete model\n      nonant_ef_suppl_list (list of pyo Var, Vardata or slices):\n              vars for which nonanticipativity constraints tighten the EF\n              (important for bundling)\n      parent_name (str): name of the parent node      \n\n    Lists:\n      nonant_vardata(list of vardata objects): vardatas to blend\n      x_bar_list(list of floats): bound by index to nonant_vardata\n    \"\"\"\n    def __init__(self, name, cond_prob, stage, cost_expression,\n                 nonant_list, scen_model, nonant_ef_suppl_list=None,\n                 parent_name=None):\n        \"\"\"Initialize a ScenarioNode object. Assume most error detection is\n        done elsewhere.\n        \"\"\"\n        self.name = name\n        self.cond_prob = cond_prob\n        self.stage = stage\n        self.cost_expression = cost_expression\n        self.nonant_list = nonant_list\n        self.nonant_ef_suppl_list = nonant_ef_suppl_list\n        self.parent_name = parent_name # None for ROOT\n        # now make the vardata lists\n        if self.nonant_list is not None:\n            self.nonant_vardata_list = build_vardatalist(self,\n                                                         scen_model,\n                                                         self.nonant_list)\n        else:\n            logger.warning(\"nonant_list is empty for node {},\".format(name) +\\\n                    \"No nonanticipativity will be enforced at this node by default\")\n            self.nonant_vardata_list = []\n\n        if self.nonant_ef_suppl_list is not None:\n            self.nonant_ef_suppl_vardata_list = build_vardatalist(self,\n                                                         scen_model,\n                                                         self.nonant_ef_suppl_list)\n        else:\n            self.nonant_ef_suppl_vardata_list = []",
  "def __init__(self, name, cond_prob, stage, cost_expression,\n                 nonant_list, scen_model, nonant_ef_suppl_list=None,\n                 parent_name=None):\n        \"\"\"Initialize a ScenarioNode object. Assume most error detection is\n        done elsewhere.\n        \"\"\"\n        self.name = name\n        self.cond_prob = cond_prob\n        self.stage = stage\n        self.cost_expression = cost_expression\n        self.nonant_list = nonant_list\n        self.nonant_ef_suppl_list = nonant_ef_suppl_list\n        self.parent_name = parent_name # None for ROOT\n        # now make the vardata lists\n        if self.nonant_list is not None:\n            self.nonant_vardata_list = build_vardatalist(self,\n                                                         scen_model,\n                                                         self.nonant_list)\n        else:\n            logger.warning(\"nonant_list is empty for node {},\".format(name) +\\\n                    \"No nonanticipativity will be enforced at this node by default\")\n            self.nonant_vardata_list = []\n\n        if self.nonant_ef_suppl_list is not None:\n            self.nonant_ef_suppl_vardata_list = build_vardatalist(self,\n                                                         scen_model,\n                                                         self.nonant_ef_suppl_list)\n        else:\n            self.nonant_ef_suppl_vardata_list = []",
  "class SPOpt(SPBase):\n    \"\"\" Defines optimization methods for hubs and spokes \"\"\"\n\n    def __init__(\n            self,\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=None,\n            all_nodenames=None,\n            mpicomm=None,\n            extensions=None,\n            extension_kwargs=None,\n            scenario_creator_kwargs=None,\n            variable_probability=None,\n            E1_tolerance=1e-5\n    ):\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            variable_probability=variable_probability,\n        )\n        self.current_solver_options = None\n        self.extensions = extensions\n        self.extension_kwargs = extension_kwargs\n\n        if (self.extensions is not None):\n            if self.extension_kwargs is None:\n                self.extobject = self.extensions(self)\n            else:\n                self.extobject = self.extensions(\n                    self, **self.extension_kwargs\n                )\n\n    def _check_staleness(self, s):\n        # check for staleness in *scenario* s\n        # Look at the feasible scenarios. If we have any stale non-anticipative\n        # variables, complain. Otherwise the user will hit this issue later when\n        # we attempt to access the variables `value` attribute (see Issue #170).\n        for v in s._mpisppy_data.nonant_indices.values():\n            if (not v.fixed) and v.stale:\n                if self.is_zero_prob(s, v) and v._value is None:\n                    raise RuntimeError(\n                            f\"Non-anticipative zero-probability variable {v.name} \"\n                            f\"on scenario {s.name} reported as stale and has no value. \"\n                             \"Zero-probability variables must have a value (e.g., fixed).\")\n                else:\n                    try:\n                        float(pyo.value(v))\n                    except:\n                        raise RuntimeError(\n                            f\"Non-anticipative variable {v.name} on scenario {s.name} \"\n                            \"reported as stale. This usually means this variable \"\n                            \"did not appear in any (active) components, and hence \"\n                            \"was not communicated to the subproblem solver. \")\n\n\n    def solve_one(self, solver_options, k, s,\n                  dtiming=False,\n                  gripe=False,\n                  tee=False,\n                  verbose=False,\n                  disable_pyomo_signal_handling=False,\n                  update_objective=True):\n        \"\"\" Solve one subproblem.\n\n        Args:\n            solver_options (dict or None):\n                The scenario solver options.\n            k (str):\n                Subproblem name.\n            s (ConcreteModel with appendages):\n                The subproblem to solve.\n            dtiming (boolean, optional):\n                If True, reports timing values. Default False.\n            gripe (boolean, optional):\n                If True, outputs a message when a solve fails. Default False.\n            tee (boolean, optional):\n                If True, displays solver output. Default False.\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n            disable_pyomo_signal_handling (boolean, optional):\n                True for asynchronous PH; ignored for persistent solvers.\n                Default False.\n            update_objective (boolean, optional):\n                If True, and a persistent solver is used, update\n                the persistent solver's objective\n\n        Returns:\n            float:\n                Pyomo solve time in seconds.\n        \"\"\"\n\n\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)\n\n        # if using a persistent solver plugin,\n        # re-compile the objective due to changed weights and x-bars\n        # high variance in set objective time (Feb 2023)?\n        if update_objective and (sputils.is_persistent(s._solver_plugin)):\n            set_objective_start_time = time.time()\n\n            active_objective_datas = list(s.component_data_objects(\n                pyo.Objective, active=True, descend_into=True))\n            if len(active_objective_datas) > 1:\n                raise RuntimeError('Multiple active objectives identified '\n                                   'for scenario {sn}'.format(sn=s._name))\n            elif len(active_objective_datas) < 1:\n                raise RuntimeError('Could not find any active objectives '\n                                   'for scenario {sn}'.format(sn=s._name))\n            else:\n                s._solver_plugin.set_objective(active_objective_datas[0])\n                set_objective_time = time.time() - set_objective_start_time\n        else:\n            set_objective_time = 0\n\n        if self.extensions is not None:\n            results = self.extobject.pre_solve(s)\n\n        solve_start_time = time.time()\n        if (solver_options):\n            _vb(\"Using sub-problem solver options=\"\n                + str(solver_options))\n            for option_key,option_value in solver_options.items():\n                s._solver_plugin.options[option_key] = option_value\n\n        solve_keyword_args = dict()\n        if self.cylinder_rank == 0:\n            if tee is not None and tee is True:\n                solve_keyword_args[\"tee\"] = True\n        if (sputils.is_persistent(s._solver_plugin)):\n            solve_keyword_args[\"save_results\"] = False\n        elif disable_pyomo_signal_handling:\n            solve_keyword_args[\"use_signal_handling\"] = False\n\n        try:\n            results = s._solver_plugin.solve(s,\n                                             **solve_keyword_args,\n                                             load_solutions=False)\n            solver_exception = None\n        except Exception as e:\n            results = None\n            solver_exception = e\n\n        pyomo_solve_time = time.time() - solve_start_time\n        if (results is None) or (len(results.solution) == 0) or \\\n                (results.solution(0).status == SolutionStatus.infeasible) or \\\n                (results.solver.termination_condition == TerminationCondition.infeasible) or \\\n                (results.solver.termination_condition == TerminationCondition.infeasibleOrUnbounded) or \\\n                (results.solver.termination_condition == TerminationCondition.unbounded):\n\n            s._mpisppy_data.scenario_feasible = False\n\n            if gripe:\n                name = self.__class__.__name__\n                if self.spcomm:\n                    name = self.spcomm.__class__.__name__\n                print (f\"[{name}] Solve failed for scenario {s.name}\")\n                if results is not None:\n                    print (\"status=\", results.solver.status)\n                    print (\"TerminationCondition=\",\n                           results.solver.termination_condition)\n\n            if solver_exception is not None:\n                raise solver_exception\n\n        else:\n            if sputils.is_persistent(s._solver_plugin):\n                s._solver_plugin.load_vars()\n            else:\n                s.solutions.load_from(results)\n            if self.is_minimizing:\n                s._mpisppy_data.outer_bound = results.Problem[0].Lower_bound\n                s._mpisppy_data.inner_bound = results.Problem[0].Upper_bound\n            else:\n                s._mpisppy_data.outer_bound = results.Problem[0].Upper_bound\n                s._mpisppy_data.inner_bound = results.Problem[0].Lower_bound\n            s._mpisppy_data.scenario_feasible = True\n        # TBD: get this ready for IPopt (e.g., check feas_prob every time)\n        # propogate down\n        if self.bundling: # must be a bundle\n            for sname in s._ef_scenario_names:\n                 self.local_scenarios[sname]._mpisppy_data.scenario_feasible\\\n                     = s._mpisppy_data.scenario_feasible\n                 if s._mpisppy_data.scenario_feasible:\n                     self._check_staleness(self.local_scenarios[sname])\n        else:  # not a bundle\n            if s._mpisppy_data.scenario_feasible:\n                self._check_staleness(s)\n\n        if self.extensions is not None:\n            results = self.extobject.post_solve(s, results)\n\n        return pyomo_solve_time + set_objective_time  # set_objective_time added Feb 2023\n\n\n    def solve_loop(self, solver_options=None,\n                   use_scenarios_not_subproblems=False,\n                   dtiming=False,\n                   gripe=False,\n                   disable_pyomo_signal_handling=False,\n                   tee=False,\n                   verbose=False):\n        \"\"\" Loop over `local_subproblems` and solve them in a manner\n        dicated by the arguments.\n\n        In addition to changing the Var values in the scenarios, this function\n        also updates the `_PySP_feas_indictor` to indicate which scenarios were\n        feasible/infeasible.\n\n        Args:\n            solver_options (dict, optional):\n                The scenario solver options.\n            use_scenarios_not_subproblems (boolean, optional):\n                If True, solves individual scenario problems, not subproblems.\n                This distinction matters when using bundling. Default is False.\n            dtiming (boolean, optional):\n                If True, reports solve timing information. Default is False.\n            gripe (boolean, optional):\n                If True, output a message when a solve fails. Default is False.\n            disable_pyomo_signal_handling (boolean, optional):\n                True for asynchronous PH; ignored for persistent solvers.\n                Default False.\n            tee (boolean, optional):\n                If True, displays solver output. Default False.\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n        \"\"\"\n\n        \"\"\" Developer notes:\n\n        This function assumes that every scenario already has a\n        `_solver_plugin` attached.\n\n        I am not sure what happens with solver_options None for a persistent\n        solver. Do options persist?\n\n        set_objective takes care of W and prox changes.\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)\n        _vb(\"Entering solve_loop function.\")\n        logger.debug(\"  early solve_loop for rank={}\".format(self.cylinder_rank))\n\n        if self.extensions is not None:\n                self.extobject.pre_solve_loop()\n\n        # note that when there is no bundling, scenarios are subproblems\n        if use_scenarios_not_subproblems:\n            s_source = self.local_scenarios\n        else:\n            s_source = self.local_subproblems\n        pyomo_solve_times = list()\n        for k,s in s_source.items():\n            logger.debug(\"  in loop solve_loop k={}, rank={}\".format(k, self.cylinder_rank))\n            if tee:\n                print(f\"Tee solve for {k} on global rank {self.global_rank}\")\n            pyomo_solve_times.append(self.solve_one(solver_options, k, s,\n                                              dtiming=dtiming,\n                                              verbose=verbose,\n                                              tee=tee,\n                                              gripe=gripe,\n                disable_pyomo_signal_handling=disable_pyomo_signal_handling\n            ))\n\n        if self.extensions is not None:\n                self.extobject.post_solve_loop()\n\n        if dtiming:\n            all_pyomo_solve_times = self.mpicomm.gather(pyomo_solve_times, root=0)\n            if self.cylinder_rank == 0:\n                apst = [pst for l_pst in all_pyomo_solve_times for pst in l_pst]\n                print(\"Pyomo solve times (seconds):\")\n                print(\"\\tmin=%4.2f@%d mean=%4.2f max=%4.2f@%d\" %\n                      (np.min(apst), np.argmin(apst),\n                       np.mean(apst),\n                       np.max(apst), np.argmax(apst)))\n\n\n    def Eobjective(self, verbose=False):\n        \"\"\" Compute the expected objective function across all scenarios.\n\n        Note:\n            Assumes the optimization is done beforehand,\n            therefore DOES NOT CHECK FEASIBILITY or NON-ANTICIPATIVITY!\n            This method uses whatever the current value of the objective\n            function is.\n\n        Args:\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n\n        Returns:\n            float:\n                The expected objective function value\n        \"\"\"\n        local_Eobjs = []\n        for k,s in self.local_scenarios.items():\n            if self.bundling:\n                objfct = self.saved_objs[k]\n            else:\n                objfct = sputils.find_active_objective(s)\n            local_Eobjs.append(s._mpisppy_probability * pyo.value(objfct))\n            if verbose:\n                print (\"caller\", inspect.stack()[1][3])\n                print (\"E_Obj Scenario {}, prob={}, Obj={}, ObjExpr={}\"\\\n                       .format(k, s._mpisppy_probability, pyo.value(objfct), objfct.expr))\n\n        local_Eobj = np.array([math.fsum(local_Eobjs)])\n        global_Eobj = np.zeros(1)\n        self.mpicomm.Allreduce(local_Eobj, global_Eobj, op=MPI.SUM)\n\n        return global_Eobj[0]\n\n\n    def Ebound(self, verbose=False, extra_sum_terms=None):\n        \"\"\" Compute the expected outer bound across all scenarios.\n\n        Note:\n            Assumes the optimization is done beforehand.\n            Uses whatever bound is currently  attached to the subproblems.\n\n        Args:\n            verbose (boolean):\n                If True, displays verbose output. Default False.\n            extra_sum_terms: (None or iterable)\n                If iterable, additional terms to put in the floating-point\n                sum reduction\n\n        Returns:\n            float:\n                The expected objective outer bound.\n        \"\"\"\n        local_Ebounds = []\n        for k,s in self.local_subproblems.items():\n            logger.debug(\"  in loop Ebound k={}, rank={}\".format(k, self.cylinder_rank))\n            try:\n                eb = s._mpisppy_probability * float(s._mpisppy_data.outer_bound)\n            except:\n                print(f\"eb calc failed for {s._mpisppy_probability} * {s._mpisppy_data.outer_bound}\")\n                raise\n            local_Ebounds.append(eb)\n            if verbose:\n                print (\"caller\", inspect.stack()[1][3])\n                print (\"E_Bound Scenario {}, prob={}, bound={}\"\\\n                       .format(k, s._mpisppy_probability, s._mpisppy_data.outer_bound))\n\n        if extra_sum_terms is not None:\n            local_Ebound_list = [math.fsum(local_Ebounds)] + list(extra_sum_terms)\n        else:\n            local_Ebound_list = [math.fsum(local_Ebounds)]\n\n        local_Ebound = np.array(local_Ebound_list)\n        global_Ebound = np.zeros(len(local_Ebound_list))\n\n        self.mpicomm.Allreduce(local_Ebound, global_Ebound, op=MPI.SUM)\n\n        if extra_sum_terms is None:\n            return global_Ebound[0]\n        else:\n            return global_Ebound[0], global_Ebound[1:]\n\n\n    def _update_E1(self):\n        \"\"\" Add up the probabilities of all scenarios using a reduce call.\n            then attach it to the PH object as a float.\n        \"\"\"\n        localP = np.zeros(1, dtype='d')\n        globalP = np.zeros(1, dtype='d')\n\n        for k,s in self.local_scenarios.items():\n            localP[0] +=  s._mpisppy_probability\n\n        self.mpicomm.Allreduce([localP, MPI.DOUBLE],\n                           [globalP, MPI.DOUBLE],\n                           op=MPI.SUM)\n\n        self.E1 = float(globalP[0])\n\n\n    def feas_prob(self):\n        \"\"\" Compute the total probability of all feasible scenarios.\n\n        This function can be used to check whether all scenarios are feasible\n        by comparing the return value to one.\n\n        Note:\n            This function assumes the scenarios have a boolean\n            `_mpisppy_data.scenario_feasible` attribute.\n\n        Returns:\n            float:\n                Sum of the scenario probabilities over all feasible scenarios.\n                This value equals E1 if all scenarios are feasible.\n        \"\"\"\n\n        # locals[0] is E_feas and locals[1] is E_1\n        locals = np.zeros(1, dtype='d')\n        globals = np.zeros(1, dtype='d')\n\n        for k,s in self.local_scenarios.items():\n            if s._mpisppy_data.scenario_feasible:\n                locals[0] += s._mpisppy_probability\n\n        self.mpicomm.Allreduce([locals, MPI.DOUBLE],\n                           [globals, MPI.DOUBLE],\n                           op=MPI.SUM)\n\n        return float(globals[0])\n\n\n    def infeas_prob(self):\n        \"\"\" Sum the total probability for all infeasible scenarios.\n\n        Note:\n            This function assumes the scenarios have a boolean\n            `_mpisppy_data.scenario_feasible` attribute.\n\n        Returns:\n            float:\n                Sum of the scenario probabilities over all infeasible scenarios.\n                This value equals 0 if all scenarios are feasible.\n        \"\"\"\n\n        locals = np.zeros(1, dtype='d')\n        globals = np.zeros(1, dtype='d')\n\n        for k,s in self.local_scenarios.items():\n            if not s._mpisppy_data.scenario_feasible:\n                locals[0] += s._mpisppy_probability\n\n        self.mpicomm.Allreduce([locals, MPI.DOUBLE],\n                           [globals, MPI.DOUBLE],\n                           op=MPI.SUM)\n\n        return float(globals[0])\n\n\n    def avg_min_max(self, compstr):\n        \"\"\" Can be used to track convergence progress.\n\n        Args:\n            compstr (str):\n                The name of the Pyomo component. Should not be indexed.\n\n        Returns:\n            tuple:\n                Tuple containing\n\n                avg (float):\n                    Average across all scenarios.\n                min (float):\n                    Minimum across all scenarios.\n                max (float):\n                    Maximum across all scenarios.\n\n        Note:\n            WARNING: Does a Allreduce.\n            Not user-friendly. If you give a bad compstr, it will just crash.\n        \"\"\"\n        firsttime = True\n        localavg = np.zeros(1, dtype='d')\n        localmin = np.zeros(1, dtype='d')\n        localmax = np.zeros(1, dtype='d')\n        globalavg = np.zeros(1, dtype='d')\n        globalmin = np.zeros(1, dtype='d')\n        globalmax = np.zeros(1, dtype='d')\n\n        v_cuid = pyo.ComponentUID(compstr)\n\n        for k,s in self.local_scenarios.items():\n\n            compv = pyo.value(v_cuid.find_component_on(s))\n\n\n            ###compv = pyo.value(getattr(s, compstr))\n            localavg[0] += s._mpisppy_probability * compv\n            if compv < localmin[0] or firsttime:\n                localmin[0] = compv\n            if compv > localmax[0] or firsttime:\n                localmax[0] = compv\n            firsttime = False\n\n        self.comms[\"ROOT\"].Allreduce([localavg, MPI.DOUBLE],\n                                     [globalavg, MPI.DOUBLE],\n                                     op=MPI.SUM)\n        self.comms[\"ROOT\"].Allreduce([localmin, MPI.DOUBLE],\n                                     [globalmin, MPI.DOUBLE],\n                                     op=MPI.MIN)\n        self.comms[\"ROOT\"].Allreduce([localmax, MPI.DOUBLE],\n                                     [globalmax, MPI.DOUBLE],\n                                     op=MPI.MAX)\n        return (float(globalavg[0]),\n                float(globalmin[0]),\n                float(globalmax[0]))\n\n\n    def _put_nonant_cache(self, cache):\n        \"\"\" Put the value in the cache for nonants *for all local scenarios*\n        Args:\n            cache (np vector) to receive the nonant's for all local scenarios\n\n        \"\"\"\n        ci = 0 # Cache index\n        for sname, model in self.local_scenarios.items():\n            if model._mpisppy_data.nonant_cache is None:\n                raise RuntimeError(f\"Rank {self.global_rank} Scenario {sname}\"\n                                   \" nonant_cache is None\"\n                                   \" (call _save_nonants first?)\")\n            for i,_ in enumerate(model._mpisppy_data.nonant_indices):\n                assert(ci < len(cache))\n                model._mpisppy_data.nonant_cache[i] = cache[ci]\n                ci += 1\n\n\n    def _restore_original_fixedness(self):\n        # We are going to hack a little to get the original fixedness, but current values\n        # (We are assuming that algorithms are not fixing anticipative vars; but if they\n        # do, they had better put their fixedness back to its correct state.)\n        self._save_nonants()\n        for k,s in self.local_scenarios.items():\n            for ci, _ in enumerate(s._mpisppy_data.nonant_indices):\n                s._mpisppy_data.fixedness_cache[ci] = s._mpisppy_data.original_fixedness[ci]\n        self._restore_nonants()\n\n\n    def _fix_nonants(self, cache):\n        \"\"\" Fix the Vars subject to non-anticipativity at given values.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n        Args:\n            cache (ndn dict of list or numpy vector): values at which to fix\n        WARNING:\n            We are counting on Pyomo indices not to change order between\n            when the cache_list is created and used.\n        NOTE:\n            You probably want to call _save_nonants right before calling this\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                if ndn not in cache:\n                    raise RuntimeError(\"Could not find {} in {}\"\\\n                                       .format(ndn, cache))\n                if cache[ndn] is None:\n                    raise RuntimeError(\"Empty cache for scen={}, node={}\".format(k, ndn))\n                if len(cache[ndn]) != nlens[ndn]:\n                    raise RuntimeError(\"Needed {} nonant Vars for {}, got {}\"\\\n                                       .format(nlens[ndn], ndn, len(cache[ndn])))\n                for i in range(nlens[ndn]):\n                    this_vardata = node.nonant_vardata_list[i]\n                    this_vardata._value = cache[ndn][i]\n                    this_vardata.fix()\n                    if persistent_solver is not None:\n                        persistent_solver.update_var(this_vardata)\n\n    def _fix_root_nonants(self,root_cache):\n        \"\"\" Fix the 1st stage Vars subject to non-anticipativity at given values.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n            Useful for multistage to find feasible solutions with a given scenario.\n        Args:\n            root_cache (numpy vector): values at which to fix\n        WARNING:\n            We are counting on Pyomo indices not to change order between\n            when the cache_list is created and used.\n        NOTE:\n            You probably want to call _save_nonants right before calling this\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            nlens = s._mpisppy_data.nlens\n\n            rootnode = None\n            for node in s._mpisppy_node_list:\n                if node.name == 'ROOT':\n                    rootnode = node\n                    break\n\n            if rootnode is None:\n                raise RuntimeError(\"Could not find a 'ROOT' node in scen {}\"\\\n                                   .format(k))\n            if root_cache is None:\n                raise RuntimeError(\"Empty root cache for scen={}\".format(k))\n            if len(root_cache) != nlens['ROOT']:\n                raise RuntimeError(\"Needed {} nonant Vars for 'ROOT', got {}\"\\\n                                   .format(nlens['ROOT'], len(root_cache)))\n\n            for i in range(nlens['ROOT']):\n                this_vardata = node.nonant_vardata_list[i]\n                this_vardata._value = root_cache[i]\n                this_vardata.fix()\n                if persistent_solver is not None:\n                    persistent_solver.update_var(this_vardata)\n\n\n\n    def _restore_nonants(self):\n        \"\"\" Restore nonanticipative variables to their original values.\n\n        This function works in conjunction with _save_nonants.\n\n        We loop over the scenarios to restore variables, but loop over\n        subproblems to alert persistent solvers.\n\n        Warning:\n            We are counting on Pyomo indices not to change order between save\n            and restoration. THIS WILL NOT WORK ON BUNDLES (Feb 2019) but\n            hopefully does not need to.\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            for ci, vardata in enumerate(s._mpisppy_data.nonant_indices.values()):\n                vardata._value = s._mpisppy_data.nonant_cache[ci]\n                vardata.fixed = s._mpisppy_data.fixedness_cache[ci]\n\n                if persistent_solver is not None:\n                    persistent_solver.update_var(vardata)\n\n\n    def _save_nonants(self):\n        \"\"\" Save the values and fixedness status of the Vars that are\n        subject to non-anticipativity.\n\n        Note:\n            Assumes nonant_cache is on the scenarios and can be used\n            as a list, or puts it there.\n        Warning:\n            We are counting on Pyomo indices not to change order before the\n            restoration. We also need the Var type to remain stable.\n        Note:\n            The value cache is np because it might be transmitted\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            if not hasattr(s._mpisppy_data,\"nonant_cache\"):\n                clen = sum(nlens[ndn] for ndn in nlens)\n                s._mpisppy_data.nonant_cache = np.zeros(clen, dtype='d')\n                s._mpisppy_data.fixedness_cache = [None for _ in range(clen)]\n\n            for ci, xvar in enumerate(s._mpisppy_data.nonant_indices.values()):\n                s._mpisppy_data.nonant_cache[ci]  = xvar._value\n                s._mpisppy_data.fixedness_cache[ci]  = xvar.is_fixed()\n\n\n    def _save_original_nonants(self):\n        \"\"\" Save the current value of the nonanticipative variables.\n\n        Values are saved in the `_PySP_original_nonants` attribute. Whether\n        the variable was fixed is stored in `_PySP_original_fixedness`.\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n            if hasattr(s,\"_PySP_original_fixedness\"):\n                print (\"ERROR: Attempt to replace original nonants\")\n                raise\n            if not hasattr(s._mpisppy_data,\"nonant_cache\"):\n                # uses nonant cache to signal other things have not\n                # been created\n                # TODO: combine cache creation (or something else)\n                clen = len(s._mpisppy_data.nonant_indices)\n                s._mpisppy_data.original_fixedness = [None] * clen\n                s._mpisppy_data.original_nonants = np.zeros(clen, dtype='d')\n\n            for ci, xvar in enumerate(s._mpisppy_data.nonant_indices.values()):\n                s._mpisppy_data.original_fixedness[ci]  = xvar.is_fixed()\n                s._mpisppy_data.original_nonants[ci]  = xvar._value\n\n\n    def _restore_original_nonants(self):\n        \"\"\" Restore nonanticipative variables to their original values.\n\n        This function works in conjunction with _save_original_nonants.\n\n        We loop over the scenarios to restore variables, but loop over\n        subproblems to alert persistent solvers.\n\n        Warning:\n            We are counting on Pyomo indices not to change order between save\n            and restoration. THIS WILL NOT WORK ON BUNDLES (Feb 2019) but\n            hopefully does not need to.\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if not self.bundling:\n                if (sputils.is_persistent(s._solver_plugin)):\n                    persistent_solver = s._solver_plugin\n            else:\n                print(\"restore_original_nonants called for a bundle\")\n                raise\n\n            for ci, vardata in enumerate(s._mpisppy_data.nonant_indices.values()):\n                vardata._value = s._mpisppy_data.original_nonants[ci]\n                vardata.fixed = s._mpisppy_data.original_fixedness[ci]\n                if persistent_solver != None:\n                    persistent_solver.update_var(vardata)\n\n\n    def FormEF(self, scen_dict, EF_name=None):\n        \"\"\" Make the EF for a list of scenarios.\n\n        This function is mainly to build bundles. To build (and solve) the\n        EF of the entire problem, use the EF class instead.\n\n        Args:\n            scen_dict (dict):\n                Subset of local_scenarios; the scenarios to put in the EF. THe\n                dictionary maps sccneario names (strings) to scenarios (Pyomo\n                concrete model objects).\n            EF_name (string, optional):\n                Name for the resulting EF model.\n\n        Returns:\n            :class:`pyomo.environ.ConcreteModel`:\n                The EF with explicit non-anticipativity constraints.\n\n        Raises:\n            RuntimeError:\n                If the `scen_dict` is empty, or one of the scenarios in\n                `scen_dict` is not owned locally (i.e. is not in\n                `local_scenarios`).\n\n        Note:\n            We attach a list of the scenario names called _PySP_subsecen_names\n        Note:\n            We deactivate the objective on the scenarios.\n        Note:\n            The scenarios are sub-blocks, so they naturally get the EF solution\n            Also the EF objective references Vars and Parms on the scenarios\n            and hence is automatically updated when the scenario\n            objectives are. THIS IS ALL CRITICAL to bundles.\n            xxxx TBD: ask JP about objective function transmittal to persistent solvers\n        Note:\n            Objectives are scaled (normalized) by _mpisppy_probability\n        \"\"\"\n        if len(scen_dict) == 0:\n            raise RuntimeError(\"Empty scenario list for EF\")\n\n        if len(scen_dict) == 1:\n            sname, scenario_instance = list(scen_dict.items())[0]\n            if EF_name is not None:\n                print (\"WARNING: EF_name=\"+EF_name+\" not used; singleton=\"+sname)\n                print (\"MAJOR WARNING: a bundle of size one encountered; if you try to compute bounds it might crash (Feb 2019)\")\n            return scenario_instance\n\n        # The individual scenario instances are sub-blocks of the binding\n        # instance. Needed to facilitate bundles + persistent solvers\n        if not hasattr(self, \"saved_objs\"): # First bundle\n             self.saved_objs = dict()\n\n        for sname, scenario_instance in scen_dict.items():\n            if sname not in self.local_scenarios:\n                raise RuntimeError(\"EF scen not in local_scenarios=\"+sname)\n            self.saved_objs[sname] = sputils.find_active_objective(scenario_instance)\n\n        EF_instance = sputils._create_EF_from_scen_dict(scen_dict, EF_name=EF_name,\n                        nonant_for_fixed_vars=False)\n        return EF_instance\n\n\n    def subproblem_creation(self, verbose=False):\n        \"\"\" Create local subproblems (not local scenarios).\n\n        If bundles are specified, this function creates the bundles.\n        Otherwise, this function simply copies pointers to the already-created\n        `local_scenarios`.\n\n        Args:\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n        \"\"\"\n        self.local_subproblems = dict()\n        if self.bundling:\n            rank_local = self.cylinder_rank\n            for bun in self.names_in_bundles[rank_local]:\n                sdict = dict()\n                bname = \"rank\" + str(self.cylinder_rank) + \"bundle\" + str(bun)\n                for sname in self.names_in_bundles[rank_local][bun]:\n                    if (verbose and self.cylinder_rank==0):\n                        print (\"bundling \"+sname+\" into \"+bname)\n                    scen = self.local_scenarios[sname]\n                    scen._mpisppy_data.bundlename = bname\n                    sdict[sname] = scen\n                self.local_subproblems[bname] = self.FormEF(sdict, bname)\n                self.local_subproblems[bname].scen_list = \\\n                    self.names_in_bundles[rank_local][bun]\n                self.local_subproblems[bname]._mpisppy_probability = \\\n                                    sum(s._mpisppy_probability for s in sdict.values())\n        else:\n            for sname, s in self.local_scenarios.items():\n                self.local_subproblems[sname] = s\n                self.local_subproblems[sname].scen_list = [sname]\n\n\n    def _create_solvers(self):\n\n        dtiming = (\"display_timing\" in self.options) and self.options[\"display_timing\"]\n        local_sit = [] # Local set instance time for time tracking\n        for sname, s in self.local_subproblems.items(): # solver creation\n            s._solver_plugin = SolverFactory(self.options[\"solver_name\"])\n            if (sputils.is_persistent(s._solver_plugin)):\n                if dtiming:\n                    set_instance_start_time = time.time()\n\n                set_instance_retry(s, s._solver_plugin, sname)\n\n                if dtiming:\n                    local_sit.append( time.time() - set_instance_start_time )\n\n            ## if we have bundling, attach\n            ## the solver plugin to the scenarios\n            ## as well to avoid some gymnastics\n            if self.bundling:\n                for scen_name in s.scen_list:\n                    scen = self.local_scenarios[scen_name]\n                    scen._solver_plugin = s._solver_plugin\n        if dtiming:\n            all_set_instance_times = self.mpicomm.gather(local_sit,\n                                                     root=0)\n            if self.cylinder_rank == 0:\n                asit = [sit for l_sit in all_set_instance_times for sit in l_sit]\n                print(\"Set instance times:\")\n                print(\"\\tmin=%4.2f mean=%4.2f max=%4.2f\" %\n                      (np.min(asit), np.mean(asit), np.max(asit)))",
  "def set_instance_retry(subproblem, solver_plugin, subproblem_name):\n\n    sname = subproblem_name\n    # this loop is required to address the sitution where license\n    # token servers become temporarily over-subscribed / non-responsive\n    # when large numbers of ranks are in use.\n\n    num_retry_attempts = 0\n    while True:\n        try:\n            solver_plugin.set_instance(subproblem)\n            if num_retry_attempts > 0:\n                print(\"Acquired solver license (call to set_instance() for scenario=%s) after %d retry attempts\" % (sname, num_retry_attempts))\n            break\n        # pyomo presently has no general way to trap a license acquisition\n        # error - so we're stuck with trapping on \"any\" exception. not ideal.\n        except:\n            if num_retry_attempts == 0:\n                print(\"Failed to acquire solver license (call to set_instance() for scenario=%s) after first attempt\" % (sname))\n            else:\n                print(\"Failed to acquire solver license (call to set_instance() for scenario=%s) after %d retry attempts\" % (sname, num_retry_attempts))\n            if num_retry_attempts == MAX_ACQUIRE_LICENSE_RETRY_ATTEMPTS:\n                raise RuntimeError(\"Failed to acquire solver license - call to set_instance() for scenario=%s failed after %d retry attempts\" % (sname, num_retry_attempts))\n            else:\n                print(\"Sleeping for %d seconds before re-attempting\" % LICENSE_RETRY_SLEEP_TIME)\n                time.sleep(LICENSE_RETRY_SLEEP_TIME)\n                num_retry_attempts += 1",
  "def __init__(\n            self,\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=None,\n            all_nodenames=None,\n            mpicomm=None,\n            extensions=None,\n            extension_kwargs=None,\n            scenario_creator_kwargs=None,\n            variable_probability=None,\n            E1_tolerance=1e-5\n    ):\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            variable_probability=variable_probability,\n        )\n        self.current_solver_options = None\n        self.extensions = extensions\n        self.extension_kwargs = extension_kwargs\n\n        if (self.extensions is not None):\n            if self.extension_kwargs is None:\n                self.extobject = self.extensions(self)\n            else:\n                self.extobject = self.extensions(\n                    self, **self.extension_kwargs\n                )",
  "def _check_staleness(self, s):\n        # check for staleness in *scenario* s\n        # Look at the feasible scenarios. If we have any stale non-anticipative\n        # variables, complain. Otherwise the user will hit this issue later when\n        # we attempt to access the variables `value` attribute (see Issue #170).\n        for v in s._mpisppy_data.nonant_indices.values():\n            if (not v.fixed) and v.stale:\n                if self.is_zero_prob(s, v) and v._value is None:\n                    raise RuntimeError(\n                            f\"Non-anticipative zero-probability variable {v.name} \"\n                            f\"on scenario {s.name} reported as stale and has no value. \"\n                             \"Zero-probability variables must have a value (e.g., fixed).\")\n                else:\n                    try:\n                        float(pyo.value(v))\n                    except:\n                        raise RuntimeError(\n                            f\"Non-anticipative variable {v.name} on scenario {s.name} \"\n                            \"reported as stale. This usually means this variable \"\n                            \"did not appear in any (active) components, and hence \"\n                            \"was not communicated to the subproblem solver. \")",
  "def solve_one(self, solver_options, k, s,\n                  dtiming=False,\n                  gripe=False,\n                  tee=False,\n                  verbose=False,\n                  disable_pyomo_signal_handling=False,\n                  update_objective=True):\n        \"\"\" Solve one subproblem.\n\n        Args:\n            solver_options (dict or None):\n                The scenario solver options.\n            k (str):\n                Subproblem name.\n            s (ConcreteModel with appendages):\n                The subproblem to solve.\n            dtiming (boolean, optional):\n                If True, reports timing values. Default False.\n            gripe (boolean, optional):\n                If True, outputs a message when a solve fails. Default False.\n            tee (boolean, optional):\n                If True, displays solver output. Default False.\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n            disable_pyomo_signal_handling (boolean, optional):\n                True for asynchronous PH; ignored for persistent solvers.\n                Default False.\n            update_objective (boolean, optional):\n                If True, and a persistent solver is used, update\n                the persistent solver's objective\n\n        Returns:\n            float:\n                Pyomo solve time in seconds.\n        \"\"\"\n\n\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)\n\n        # if using a persistent solver plugin,\n        # re-compile the objective due to changed weights and x-bars\n        # high variance in set objective time (Feb 2023)?\n        if update_objective and (sputils.is_persistent(s._solver_plugin)):\n            set_objective_start_time = time.time()\n\n            active_objective_datas = list(s.component_data_objects(\n                pyo.Objective, active=True, descend_into=True))\n            if len(active_objective_datas) > 1:\n                raise RuntimeError('Multiple active objectives identified '\n                                   'for scenario {sn}'.format(sn=s._name))\n            elif len(active_objective_datas) < 1:\n                raise RuntimeError('Could not find any active objectives '\n                                   'for scenario {sn}'.format(sn=s._name))\n            else:\n                s._solver_plugin.set_objective(active_objective_datas[0])\n                set_objective_time = time.time() - set_objective_start_time\n        else:\n            set_objective_time = 0\n\n        if self.extensions is not None:\n            results = self.extobject.pre_solve(s)\n\n        solve_start_time = time.time()\n        if (solver_options):\n            _vb(\"Using sub-problem solver options=\"\n                + str(solver_options))\n            for option_key,option_value in solver_options.items():\n                s._solver_plugin.options[option_key] = option_value\n\n        solve_keyword_args = dict()\n        if self.cylinder_rank == 0:\n            if tee is not None and tee is True:\n                solve_keyword_args[\"tee\"] = True\n        if (sputils.is_persistent(s._solver_plugin)):\n            solve_keyword_args[\"save_results\"] = False\n        elif disable_pyomo_signal_handling:\n            solve_keyword_args[\"use_signal_handling\"] = False\n\n        try:\n            results = s._solver_plugin.solve(s,\n                                             **solve_keyword_args,\n                                             load_solutions=False)\n            solver_exception = None\n        except Exception as e:\n            results = None\n            solver_exception = e\n\n        pyomo_solve_time = time.time() - solve_start_time\n        if (results is None) or (len(results.solution) == 0) or \\\n                (results.solution(0).status == SolutionStatus.infeasible) or \\\n                (results.solver.termination_condition == TerminationCondition.infeasible) or \\\n                (results.solver.termination_condition == TerminationCondition.infeasibleOrUnbounded) or \\\n                (results.solver.termination_condition == TerminationCondition.unbounded):\n\n            s._mpisppy_data.scenario_feasible = False\n\n            if gripe:\n                name = self.__class__.__name__\n                if self.spcomm:\n                    name = self.spcomm.__class__.__name__\n                print (f\"[{name}] Solve failed for scenario {s.name}\")\n                if results is not None:\n                    print (\"status=\", results.solver.status)\n                    print (\"TerminationCondition=\",\n                           results.solver.termination_condition)\n\n            if solver_exception is not None:\n                raise solver_exception\n\n        else:\n            if sputils.is_persistent(s._solver_plugin):\n                s._solver_plugin.load_vars()\n            else:\n                s.solutions.load_from(results)\n            if self.is_minimizing:\n                s._mpisppy_data.outer_bound = results.Problem[0].Lower_bound\n                s._mpisppy_data.inner_bound = results.Problem[0].Upper_bound\n            else:\n                s._mpisppy_data.outer_bound = results.Problem[0].Upper_bound\n                s._mpisppy_data.inner_bound = results.Problem[0].Lower_bound\n            s._mpisppy_data.scenario_feasible = True\n        # TBD: get this ready for IPopt (e.g., check feas_prob every time)\n        # propogate down\n        if self.bundling: # must be a bundle\n            for sname in s._ef_scenario_names:\n                 self.local_scenarios[sname]._mpisppy_data.scenario_feasible\\\n                     = s._mpisppy_data.scenario_feasible\n                 if s._mpisppy_data.scenario_feasible:\n                     self._check_staleness(self.local_scenarios[sname])\n        else:  # not a bundle\n            if s._mpisppy_data.scenario_feasible:\n                self._check_staleness(s)\n\n        if self.extensions is not None:\n            results = self.extobject.post_solve(s, results)\n\n        return pyomo_solve_time + set_objective_time",
  "def solve_loop(self, solver_options=None,\n                   use_scenarios_not_subproblems=False,\n                   dtiming=False,\n                   gripe=False,\n                   disable_pyomo_signal_handling=False,\n                   tee=False,\n                   verbose=False):\n        \"\"\" Loop over `local_subproblems` and solve them in a manner\n        dicated by the arguments.\n\n        In addition to changing the Var values in the scenarios, this function\n        also updates the `_PySP_feas_indictor` to indicate which scenarios were\n        feasible/infeasible.\n\n        Args:\n            solver_options (dict, optional):\n                The scenario solver options.\n            use_scenarios_not_subproblems (boolean, optional):\n                If True, solves individual scenario problems, not subproblems.\n                This distinction matters when using bundling. Default is False.\n            dtiming (boolean, optional):\n                If True, reports solve timing information. Default is False.\n            gripe (boolean, optional):\n                If True, output a message when a solve fails. Default is False.\n            disable_pyomo_signal_handling (boolean, optional):\n                True for asynchronous PH; ignored for persistent solvers.\n                Default False.\n            tee (boolean, optional):\n                If True, displays solver output. Default False.\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n        \"\"\"\n\n        \"\"\" Developer notes:\n\n        This function assumes that every scenario already has a\n        `_solver_plugin` attached.\n\n        I am not sure what happens with solver_options None for a persistent\n        solver. Do options persist?\n\n        set_objective takes care of W and prox changes.\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)\n        _vb(\"Entering solve_loop function.\")\n        logger.debug(\"  early solve_loop for rank={}\".format(self.cylinder_rank))\n\n        if self.extensions is not None:\n                self.extobject.pre_solve_loop()\n\n        # note that when there is no bundling, scenarios are subproblems\n        if use_scenarios_not_subproblems:\n            s_source = self.local_scenarios\n        else:\n            s_source = self.local_subproblems\n        pyomo_solve_times = list()\n        for k,s in s_source.items():\n            logger.debug(\"  in loop solve_loop k={}, rank={}\".format(k, self.cylinder_rank))\n            if tee:\n                print(f\"Tee solve for {k} on global rank {self.global_rank}\")\n            pyomo_solve_times.append(self.solve_one(solver_options, k, s,\n                                              dtiming=dtiming,\n                                              verbose=verbose,\n                                              tee=tee,\n                                              gripe=gripe,\n                disable_pyomo_signal_handling=disable_pyomo_signal_handling\n            ))\n\n        if self.extensions is not None:\n                self.extobject.post_solve_loop()\n\n        if dtiming:\n            all_pyomo_solve_times = self.mpicomm.gather(pyomo_solve_times, root=0)\n            if self.cylinder_rank == 0:\n                apst = [pst for l_pst in all_pyomo_solve_times for pst in l_pst]\n                print(\"Pyomo solve times (seconds):\")\n                print(\"\\tmin=%4.2f@%d mean=%4.2f max=%4.2f@%d\" %\n                      (np.min(apst), np.argmin(apst),\n                       np.mean(apst),\n                       np.max(apst), np.argmax(apst)))",
  "def Eobjective(self, verbose=False):\n        \"\"\" Compute the expected objective function across all scenarios.\n\n        Note:\n            Assumes the optimization is done beforehand,\n            therefore DOES NOT CHECK FEASIBILITY or NON-ANTICIPATIVITY!\n            This method uses whatever the current value of the objective\n            function is.\n\n        Args:\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n\n        Returns:\n            float:\n                The expected objective function value\n        \"\"\"\n        local_Eobjs = []\n        for k,s in self.local_scenarios.items():\n            if self.bundling:\n                objfct = self.saved_objs[k]\n            else:\n                objfct = sputils.find_active_objective(s)\n            local_Eobjs.append(s._mpisppy_probability * pyo.value(objfct))\n            if verbose:\n                print (\"caller\", inspect.stack()[1][3])\n                print (\"E_Obj Scenario {}, prob={}, Obj={}, ObjExpr={}\"\\\n                       .format(k, s._mpisppy_probability, pyo.value(objfct), objfct.expr))\n\n        local_Eobj = np.array([math.fsum(local_Eobjs)])\n        global_Eobj = np.zeros(1)\n        self.mpicomm.Allreduce(local_Eobj, global_Eobj, op=MPI.SUM)\n\n        return global_Eobj[0]",
  "def Ebound(self, verbose=False, extra_sum_terms=None):\n        \"\"\" Compute the expected outer bound across all scenarios.\n\n        Note:\n            Assumes the optimization is done beforehand.\n            Uses whatever bound is currently  attached to the subproblems.\n\n        Args:\n            verbose (boolean):\n                If True, displays verbose output. Default False.\n            extra_sum_terms: (None or iterable)\n                If iterable, additional terms to put in the floating-point\n                sum reduction\n\n        Returns:\n            float:\n                The expected objective outer bound.\n        \"\"\"\n        local_Ebounds = []\n        for k,s in self.local_subproblems.items():\n            logger.debug(\"  in loop Ebound k={}, rank={}\".format(k, self.cylinder_rank))\n            try:\n                eb = s._mpisppy_probability * float(s._mpisppy_data.outer_bound)\n            except:\n                print(f\"eb calc failed for {s._mpisppy_probability} * {s._mpisppy_data.outer_bound}\")\n                raise\n            local_Ebounds.append(eb)\n            if verbose:\n                print (\"caller\", inspect.stack()[1][3])\n                print (\"E_Bound Scenario {}, prob={}, bound={}\"\\\n                       .format(k, s._mpisppy_probability, s._mpisppy_data.outer_bound))\n\n        if extra_sum_terms is not None:\n            local_Ebound_list = [math.fsum(local_Ebounds)] + list(extra_sum_terms)\n        else:\n            local_Ebound_list = [math.fsum(local_Ebounds)]\n\n        local_Ebound = np.array(local_Ebound_list)\n        global_Ebound = np.zeros(len(local_Ebound_list))\n\n        self.mpicomm.Allreduce(local_Ebound, global_Ebound, op=MPI.SUM)\n\n        if extra_sum_terms is None:\n            return global_Ebound[0]\n        else:\n            return global_Ebound[0], global_Ebound[1:]",
  "def _update_E1(self):\n        \"\"\" Add up the probabilities of all scenarios using a reduce call.\n            then attach it to the PH object as a float.\n        \"\"\"\n        localP = np.zeros(1, dtype='d')\n        globalP = np.zeros(1, dtype='d')\n\n        for k,s in self.local_scenarios.items():\n            localP[0] +=  s._mpisppy_probability\n\n        self.mpicomm.Allreduce([localP, MPI.DOUBLE],\n                           [globalP, MPI.DOUBLE],\n                           op=MPI.SUM)\n\n        self.E1 = float(globalP[0])",
  "def feas_prob(self):\n        \"\"\" Compute the total probability of all feasible scenarios.\n\n        This function can be used to check whether all scenarios are feasible\n        by comparing the return value to one.\n\n        Note:\n            This function assumes the scenarios have a boolean\n            `_mpisppy_data.scenario_feasible` attribute.\n\n        Returns:\n            float:\n                Sum of the scenario probabilities over all feasible scenarios.\n                This value equals E1 if all scenarios are feasible.\n        \"\"\"\n\n        # locals[0] is E_feas and locals[1] is E_1\n        locals = np.zeros(1, dtype='d')\n        globals = np.zeros(1, dtype='d')\n\n        for k,s in self.local_scenarios.items():\n            if s._mpisppy_data.scenario_feasible:\n                locals[0] += s._mpisppy_probability\n\n        self.mpicomm.Allreduce([locals, MPI.DOUBLE],\n                           [globals, MPI.DOUBLE],\n                           op=MPI.SUM)\n\n        return float(globals[0])",
  "def infeas_prob(self):\n        \"\"\" Sum the total probability for all infeasible scenarios.\n\n        Note:\n            This function assumes the scenarios have a boolean\n            `_mpisppy_data.scenario_feasible` attribute.\n\n        Returns:\n            float:\n                Sum of the scenario probabilities over all infeasible scenarios.\n                This value equals 0 if all scenarios are feasible.\n        \"\"\"\n\n        locals = np.zeros(1, dtype='d')\n        globals = np.zeros(1, dtype='d')\n\n        for k,s in self.local_scenarios.items():\n            if not s._mpisppy_data.scenario_feasible:\n                locals[0] += s._mpisppy_probability\n\n        self.mpicomm.Allreduce([locals, MPI.DOUBLE],\n                           [globals, MPI.DOUBLE],\n                           op=MPI.SUM)\n\n        return float(globals[0])",
  "def avg_min_max(self, compstr):\n        \"\"\" Can be used to track convergence progress.\n\n        Args:\n            compstr (str):\n                The name of the Pyomo component. Should not be indexed.\n\n        Returns:\n            tuple:\n                Tuple containing\n\n                avg (float):\n                    Average across all scenarios.\n                min (float):\n                    Minimum across all scenarios.\n                max (float):\n                    Maximum across all scenarios.\n\n        Note:\n            WARNING: Does a Allreduce.\n            Not user-friendly. If you give a bad compstr, it will just crash.\n        \"\"\"\n        firsttime = True\n        localavg = np.zeros(1, dtype='d')\n        localmin = np.zeros(1, dtype='d')\n        localmax = np.zeros(1, dtype='d')\n        globalavg = np.zeros(1, dtype='d')\n        globalmin = np.zeros(1, dtype='d')\n        globalmax = np.zeros(1, dtype='d')\n\n        v_cuid = pyo.ComponentUID(compstr)\n\n        for k,s in self.local_scenarios.items():\n\n            compv = pyo.value(v_cuid.find_component_on(s))\n\n\n            ###compv = pyo.value(getattr(s, compstr))\n            localavg[0] += s._mpisppy_probability * compv\n            if compv < localmin[0] or firsttime:\n                localmin[0] = compv\n            if compv > localmax[0] or firsttime:\n                localmax[0] = compv\n            firsttime = False\n\n        self.comms[\"ROOT\"].Allreduce([localavg, MPI.DOUBLE],\n                                     [globalavg, MPI.DOUBLE],\n                                     op=MPI.SUM)\n        self.comms[\"ROOT\"].Allreduce([localmin, MPI.DOUBLE],\n                                     [globalmin, MPI.DOUBLE],\n                                     op=MPI.MIN)\n        self.comms[\"ROOT\"].Allreduce([localmax, MPI.DOUBLE],\n                                     [globalmax, MPI.DOUBLE],\n                                     op=MPI.MAX)\n        return (float(globalavg[0]),\n                float(globalmin[0]),\n                float(globalmax[0]))",
  "def _put_nonant_cache(self, cache):\n        \"\"\" Put the value in the cache for nonants *for all local scenarios*\n        Args:\n            cache (np vector) to receive the nonant's for all local scenarios\n\n        \"\"\"\n        ci = 0 # Cache index\n        for sname, model in self.local_scenarios.items():\n            if model._mpisppy_data.nonant_cache is None:\n                raise RuntimeError(f\"Rank {self.global_rank} Scenario {sname}\"\n                                   \" nonant_cache is None\"\n                                   \" (call _save_nonants first?)\")\n            for i,_ in enumerate(model._mpisppy_data.nonant_indices):\n                assert(ci < len(cache))\n                model._mpisppy_data.nonant_cache[i] = cache[ci]\n                ci += 1",
  "def _restore_original_fixedness(self):\n        # We are going to hack a little to get the original fixedness, but current values\n        # (We are assuming that algorithms are not fixing anticipative vars; but if they\n        # do, they had better put their fixedness back to its correct state.)\n        self._save_nonants()\n        for k,s in self.local_scenarios.items():\n            for ci, _ in enumerate(s._mpisppy_data.nonant_indices):\n                s._mpisppy_data.fixedness_cache[ci] = s._mpisppy_data.original_fixedness[ci]\n        self._restore_nonants()",
  "def _fix_nonants(self, cache):\n        \"\"\" Fix the Vars subject to non-anticipativity at given values.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n        Args:\n            cache (ndn dict of list or numpy vector): values at which to fix\n        WARNING:\n            We are counting on Pyomo indices not to change order between\n            when the cache_list is created and used.\n        NOTE:\n            You probably want to call _save_nonants right before calling this\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                if ndn not in cache:\n                    raise RuntimeError(\"Could not find {} in {}\"\\\n                                       .format(ndn, cache))\n                if cache[ndn] is None:\n                    raise RuntimeError(\"Empty cache for scen={}, node={}\".format(k, ndn))\n                if len(cache[ndn]) != nlens[ndn]:\n                    raise RuntimeError(\"Needed {} nonant Vars for {}, got {}\"\\\n                                       .format(nlens[ndn], ndn, len(cache[ndn])))\n                for i in range(nlens[ndn]):\n                    this_vardata = node.nonant_vardata_list[i]\n                    this_vardata._value = cache[ndn][i]\n                    this_vardata.fix()\n                    if persistent_solver is not None:\n                        persistent_solver.update_var(this_vardata)",
  "def _fix_root_nonants(self,root_cache):\n        \"\"\" Fix the 1st stage Vars subject to non-anticipativity at given values.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n            Useful for multistage to find feasible solutions with a given scenario.\n        Args:\n            root_cache (numpy vector): values at which to fix\n        WARNING:\n            We are counting on Pyomo indices not to change order between\n            when the cache_list is created and used.\n        NOTE:\n            You probably want to call _save_nonants right before calling this\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            nlens = s._mpisppy_data.nlens\n\n            rootnode = None\n            for node in s._mpisppy_node_list:\n                if node.name == 'ROOT':\n                    rootnode = node\n                    break\n\n            if rootnode is None:\n                raise RuntimeError(\"Could not find a 'ROOT' node in scen {}\"\\\n                                   .format(k))\n            if root_cache is None:\n                raise RuntimeError(\"Empty root cache for scen={}\".format(k))\n            if len(root_cache) != nlens['ROOT']:\n                raise RuntimeError(\"Needed {} nonant Vars for 'ROOT', got {}\"\\\n                                   .format(nlens['ROOT'], len(root_cache)))\n\n            for i in range(nlens['ROOT']):\n                this_vardata = node.nonant_vardata_list[i]\n                this_vardata._value = root_cache[i]\n                this_vardata.fix()\n                if persistent_solver is not None:\n                    persistent_solver.update_var(this_vardata)",
  "def _restore_nonants(self):\n        \"\"\" Restore nonanticipative variables to their original values.\n\n        This function works in conjunction with _save_nonants.\n\n        We loop over the scenarios to restore variables, but loop over\n        subproblems to alert persistent solvers.\n\n        Warning:\n            We are counting on Pyomo indices not to change order between save\n            and restoration. THIS WILL NOT WORK ON BUNDLES (Feb 2019) but\n            hopefully does not need to.\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            for ci, vardata in enumerate(s._mpisppy_data.nonant_indices.values()):\n                vardata._value = s._mpisppy_data.nonant_cache[ci]\n                vardata.fixed = s._mpisppy_data.fixedness_cache[ci]\n\n                if persistent_solver is not None:\n                    persistent_solver.update_var(vardata)",
  "def _save_nonants(self):\n        \"\"\" Save the values and fixedness status of the Vars that are\n        subject to non-anticipativity.\n\n        Note:\n            Assumes nonant_cache is on the scenarios and can be used\n            as a list, or puts it there.\n        Warning:\n            We are counting on Pyomo indices not to change order before the\n            restoration. We also need the Var type to remain stable.\n        Note:\n            The value cache is np because it might be transmitted\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            if not hasattr(s._mpisppy_data,\"nonant_cache\"):\n                clen = sum(nlens[ndn] for ndn in nlens)\n                s._mpisppy_data.nonant_cache = np.zeros(clen, dtype='d')\n                s._mpisppy_data.fixedness_cache = [None for _ in range(clen)]\n\n            for ci, xvar in enumerate(s._mpisppy_data.nonant_indices.values()):\n                s._mpisppy_data.nonant_cache[ci]  = xvar._value\n                s._mpisppy_data.fixedness_cache[ci]  = xvar.is_fixed()",
  "def _save_original_nonants(self):\n        \"\"\" Save the current value of the nonanticipative variables.\n\n        Values are saved in the `_PySP_original_nonants` attribute. Whether\n        the variable was fixed is stored in `_PySP_original_fixedness`.\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n            if hasattr(s,\"_PySP_original_fixedness\"):\n                print (\"ERROR: Attempt to replace original nonants\")\n                raise\n            if not hasattr(s._mpisppy_data,\"nonant_cache\"):\n                # uses nonant cache to signal other things have not\n                # been created\n                # TODO: combine cache creation (or something else)\n                clen = len(s._mpisppy_data.nonant_indices)\n                s._mpisppy_data.original_fixedness = [None] * clen\n                s._mpisppy_data.original_nonants = np.zeros(clen, dtype='d')\n\n            for ci, xvar in enumerate(s._mpisppy_data.nonant_indices.values()):\n                s._mpisppy_data.original_fixedness[ci]  = xvar.is_fixed()\n                s._mpisppy_data.original_nonants[ci]  = xvar._value",
  "def _restore_original_nonants(self):\n        \"\"\" Restore nonanticipative variables to their original values.\n\n        This function works in conjunction with _save_original_nonants.\n\n        We loop over the scenarios to restore variables, but loop over\n        subproblems to alert persistent solvers.\n\n        Warning:\n            We are counting on Pyomo indices not to change order between save\n            and restoration. THIS WILL NOT WORK ON BUNDLES (Feb 2019) but\n            hopefully does not need to.\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if not self.bundling:\n                if (sputils.is_persistent(s._solver_plugin)):\n                    persistent_solver = s._solver_plugin\n            else:\n                print(\"restore_original_nonants called for a bundle\")\n                raise\n\n            for ci, vardata in enumerate(s._mpisppy_data.nonant_indices.values()):\n                vardata._value = s._mpisppy_data.original_nonants[ci]\n                vardata.fixed = s._mpisppy_data.original_fixedness[ci]\n                if persistent_solver != None:\n                    persistent_solver.update_var(vardata)",
  "def FormEF(self, scen_dict, EF_name=None):\n        \"\"\" Make the EF for a list of scenarios.\n\n        This function is mainly to build bundles. To build (and solve) the\n        EF of the entire problem, use the EF class instead.\n\n        Args:\n            scen_dict (dict):\n                Subset of local_scenarios; the scenarios to put in the EF. THe\n                dictionary maps sccneario names (strings) to scenarios (Pyomo\n                concrete model objects).\n            EF_name (string, optional):\n                Name for the resulting EF model.\n\n        Returns:\n            :class:`pyomo.environ.ConcreteModel`:\n                The EF with explicit non-anticipativity constraints.\n\n        Raises:\n            RuntimeError:\n                If the `scen_dict` is empty, or one of the scenarios in\n                `scen_dict` is not owned locally (i.e. is not in\n                `local_scenarios`).\n\n        Note:\n            We attach a list of the scenario names called _PySP_subsecen_names\n        Note:\n            We deactivate the objective on the scenarios.\n        Note:\n            The scenarios are sub-blocks, so they naturally get the EF solution\n            Also the EF objective references Vars and Parms on the scenarios\n            and hence is automatically updated when the scenario\n            objectives are. THIS IS ALL CRITICAL to bundles.\n            xxxx TBD: ask JP about objective function transmittal to persistent solvers\n        Note:\n            Objectives are scaled (normalized) by _mpisppy_probability\n        \"\"\"\n        if len(scen_dict) == 0:\n            raise RuntimeError(\"Empty scenario list for EF\")\n\n        if len(scen_dict) == 1:\n            sname, scenario_instance = list(scen_dict.items())[0]\n            if EF_name is not None:\n                print (\"WARNING: EF_name=\"+EF_name+\" not used; singleton=\"+sname)\n                print (\"MAJOR WARNING: a bundle of size one encountered; if you try to compute bounds it might crash (Feb 2019)\")\n            return scenario_instance\n\n        # The individual scenario instances are sub-blocks of the binding\n        # instance. Needed to facilitate bundles + persistent solvers\n        if not hasattr(self, \"saved_objs\"): # First bundle\n             self.saved_objs = dict()\n\n        for sname, scenario_instance in scen_dict.items():\n            if sname not in self.local_scenarios:\n                raise RuntimeError(\"EF scen not in local_scenarios=\"+sname)\n            self.saved_objs[sname] = sputils.find_active_objective(scenario_instance)\n\n        EF_instance = sputils._create_EF_from_scen_dict(scen_dict, EF_name=EF_name,\n                        nonant_for_fixed_vars=False)\n        return EF_instance",
  "def subproblem_creation(self, verbose=False):\n        \"\"\" Create local subproblems (not local scenarios).\n\n        If bundles are specified, this function creates the bundles.\n        Otherwise, this function simply copies pointers to the already-created\n        `local_scenarios`.\n\n        Args:\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n        \"\"\"\n        self.local_subproblems = dict()\n        if self.bundling:\n            rank_local = self.cylinder_rank\n            for bun in self.names_in_bundles[rank_local]:\n                sdict = dict()\n                bname = \"rank\" + str(self.cylinder_rank) + \"bundle\" + str(bun)\n                for sname in self.names_in_bundles[rank_local][bun]:\n                    if (verbose and self.cylinder_rank==0):\n                        print (\"bundling \"+sname+\" into \"+bname)\n                    scen = self.local_scenarios[sname]\n                    scen._mpisppy_data.bundlename = bname\n                    sdict[sname] = scen\n                self.local_subproblems[bname] = self.FormEF(sdict, bname)\n                self.local_subproblems[bname].scen_list = \\\n                    self.names_in_bundles[rank_local][bun]\n                self.local_subproblems[bname]._mpisppy_probability = \\\n                                    sum(s._mpisppy_probability for s in sdict.values())\n        else:\n            for sname, s in self.local_scenarios.items():\n                self.local_subproblems[sname] = s\n                self.local_subproblems[sname].scen_list = [sname]",
  "def _create_solvers(self):\n\n        dtiming = (\"display_timing\" in self.options) and self.options[\"display_timing\"]\n        local_sit = [] # Local set instance time for time tracking\n        for sname, s in self.local_subproblems.items(): # solver creation\n            s._solver_plugin = SolverFactory(self.options[\"solver_name\"])\n            if (sputils.is_persistent(s._solver_plugin)):\n                if dtiming:\n                    set_instance_start_time = time.time()\n\n                set_instance_retry(s, s._solver_plugin, sname)\n\n                if dtiming:\n                    local_sit.append( time.time() - set_instance_start_time )\n\n            ## if we have bundling, attach\n            ## the solver plugin to the scenarios\n            ## as well to avoid some gymnastics\n            if self.bundling:\n                for scen_name in s.scen_list:\n                    scen = self.local_scenarios[scen_name]\n                    scen._solver_plugin = s._solver_plugin\n        if dtiming:\n            all_set_instance_times = self.mpicomm.gather(local_sit,\n                                                     root=0)\n            if self.cylinder_rank == 0:\n                asit = [sit for l_sit in all_set_instance_times for sit in l_sit]\n                print(\"Set instance times:\")\n                print(\"\\tmin=%4.2f mean=%4.2f max=%4.2f\" %\n                      (np.min(asit), np.mean(asit), np.max(asit)))",
  "def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)",
  "def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)",
  "def setup_logger(name, out, level=logging.DEBUG, mode='w', fmt=None):\n    ''' Set up a custom logger quickly\n        https://stackoverflow.com/a/17037016/8516804\n    '''\n    if fmt is None:\n        fmt = \"(%(asctime)s) %(message)s\"\n    l = logging.getLogger(name)\n    l.setLevel(level)\n    l.propagate = False\n    formatter = logging.Formatter(fmt)\n    if out in (sys.stdout, sys.stderr):\n        handler = logging.StreamHandler(out)\n    else: # out is a filename\n        handler = logging.FileHandler(out, mode=mode) \n    handler.setFormatter(formatter)\n    l.addHandler(handler)",
  "class SPBase:\n    \"\"\" Defines an interface to all strata (hubs and spokes)\n\n        Args:\n            options (dict): options\n            all_scenario_names (list): all scenario names\n            scenario_creator (fct): returns a concrete model with special things\n            scenario_denouement (fct): for post processing and reporting\n            all_nodenames (list): all node names (including leaves); can be None for 2 Stage\n            mpicomm (MPI comm): if not given, use the global fullcomm\n            scenario_creator_kwargs (dict): kwargs passed directly to\n                scenario_creator.\n            variable_probability (fct): returns a list of tuples of (id(var), prob)\n                to set variable-specific probability (similar to PHBase.rho_setter).\n\n        Attributes:\n          local_scenarios (dict of scenario objects): concrete models with \n                extra data, key is name\n          comms (dict): keys are node names values are comm objects.\n          local_scenario_names (list): names of locals \n    \"\"\"\n\n    def __init__(\n            self,\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=None,\n            all_nodenames=None,\n            mpicomm=None,\n            scenario_creator_kwargs=None,\n            variable_probability=None,\n            E1_tolerance=1e-5\n    ):\n        # TODO add missing and private attributes (JP)\n        # TODO add a class attribute called ROOTNODENAME = \"ROOT\"\n        # TODO? add decorators to the class attributes\n\n        self.start_time = time.perf_counter()\n        self.options = options\n        self.all_scenario_names = all_scenario_names\n        self.scenario_creator = scenario_creator\n        self.scenario_denouement = scenario_denouement\n        self.comms = dict()\n        self.local_scenarios = dict()\n        self.local_scenario_names = list()\n        self.E1_tolerance = E1_tolerance  # probs must sum to almost 1\n        self.names_in_bundles = None\n        self.scenarios_constructed = False\n        if all_nodenames is None:\n            self.all_nodenames = [\"ROOT\"]\n        elif \"ROOT\" in all_nodenames:\n            self.all_nodenames = all_nodenames\n            self._check_nodenames()\n        else:\n            raise RuntimeError(\"'ROOT' must be in the list of node names\")\n        self.variable_probability = variable_probability\n        self.multistage = (len(self.all_nodenames) > 1)\n\n        # Set up MPI communicator and rank\n        if mpicomm is not None:\n            self.mpicomm = mpicomm\n        else:\n            self.mpicomm = MPI.COMM_WORLD\n        self.cylinder_rank = self.mpicomm.Get_rank()\n        self.n_proc = self.mpicomm.Get_size()\n        self.global_rank = MPI.COMM_WORLD.Get_rank()\n\n\n        if options.get(\"toc\", True):\n            global_toc(\"Initializing SPBase\")\n\n        if self.n_proc > len(self.all_scenario_names):\n            raise RuntimeError(\"More ranks than scenarios\")\n\n        self._calculate_scenario_ranks()\n        if \"bundles_per_rank\" in self.options and self.options[\"bundles_per_rank\"] > 0:\n            self._assign_bundles()\n            self.bundling = True\n        else:\n            self.bundling = False\n        self._create_scenarios(scenario_creator_kwargs)\n        self._look_and_leap()\n        self._compute_unconditional_node_probabilities()\n        self._attach_nlens()\n        self._attach_nonant_indices()\n        self._attach_varid_to_nonant_index()\n        self._create_communicators()\n        self._verify_nonant_lengths()\n        self._set_sense()\n        self._use_variable_probability_setter()\n\n        ## SPCommunicator object\n        self._spcomm = None\n\n        # for writers, if the appropriate\n        # solution is loaded into the subproblems\n        self.tree_solution_available = False\n        self.first_stage_solution_available = False\n\n    def _set_sense(self, comm=None):\n        \"\"\" Check to confirm that all the models constructed by scenario_crator\n            have the same sense (min v. max), and set self.is_minimizing\n            accordingly.\n        \"\"\"\n        is_min, clear = sputils._models_have_same_sense(self.local_scenarios)\n        if not clear:\n            raise RuntimeError(\n                \"All scenario models must have the same \"\n                \"model sense (minimize or maximize)\"\n            )\n        self.is_minimizing = is_min\n\n        if self.n_proc <= 1:\n            return\n\n        # Check that all the ranks agree\n        global_senses = self.mpicomm.gather(is_min, root=0)\n        if self.cylinder_rank != 0:\n            return\n        sense = global_senses[0]\n        clear = all(val == sense for val in global_senses)\n        if not clear:\n            raise RuntimeError(\n                \"All scenario models must have the same \"\n                \"model sense (minimize or maximize)\"\n            )\n\n    def _verify_nonant_lengths(self):\n        local_node_nonant_lengths = {}   # keys are tree node names\n\n        # we need to accumulate all local contributions before the reduce\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                mylen = nlens[ndn]\n                if ndn not in local_node_nonant_lengths:\n                    local_node_nonant_lengths[ndn] = mylen\n                elif local_node_nonant_lengths[ndn] != mylen:\n                    raise RuntimeError(f\"Tree node {ndn} has different number of non-anticipative \"\n                            f\"variables between scenarios {mylen} vs. {local_node_nonant_lengths[ndn]}\")\n\n        # compute node values(reduction)\n        for ndn, val in local_node_nonant_lengths.items():\n            local_val = np.array([val], 'i')\n            max_val = np.zeros(1, 'i')\n            self.comms[ndn].Allreduce([local_val, MPI.INT],\n                                      [max_val, MPI.INT],\n                                      op=MPI.MAX)\n\n            if val != int(max_val[0]):\n                raise RuntimeError(f\"Tree node {ndn} has different number of non-anticipative \"\n                        f\"variables between scenarios {val} vs. max {max_val[0]}\")\n                \n    def _check_nodenames(self):\n        for ndn in self.all_nodenames:\n            if ndn != 'ROOT' and sputils.parent_ndn(ndn) not in self.all_nodenames:\n                raise RuntimeError(f\"all_nodenames is inconsistent:\"\n                                   f\"The node {sputils.parent_ndn(ndn)}, parent of {ndn}, is missing.\")\n\n\n    def _calculate_scenario_ranks(self):\n        \"\"\" Populate the following attributes\n            1. self.scenario_names_to_rank (dict of dict):\n                keys are comms (i.e., tree nodes); values are dicts with keys\n                that are scenario names and values that are ranks within that comm\n\n            2. self._rank_slices (list of lists)\n                indices correspond to ranks in self.mpicomm and the values are a list\n                of scenario indices\n                rank -> list of scenario indices for that rank\n\n            3. self._scenario_slices (list)\n                indices are scenario indices and values are the rank of that scenario\n                within self.mpicomm\n                scenario index -> rank\n\n            4. self._scenario_tree (instance of sputils._ScenTree)\n\n            5. self.local_scenario_names (list)\n               List of index names owned by the local rank\n\n        \"\"\"\n        tree = sputils._ScenTree(self.all_nodenames, self.all_scenario_names)\n\n        self.scenario_names_to_rank, self._rank_slices, self._scenario_slices =\\\n                tree.scen_names_to_ranks(self.n_proc)\n        self._scenario_tree = tree\n        self.nonleaves = {node.name : node for node in tree.nonleaves}\n\n        # list of scenario names owned locally\n        self.local_scenario_names = [\n            self.all_scenario_names[i] for i in self._rank_slices[self.cylinder_rank]\n        ]\n\n\n    def _assign_bundles(self):\n        \"\"\" Create self.names_in_bundles, a dict of dicts\n            \n            self.names_in_bundles[rank number][bundle number] = \n                    list of scenarios in that bundle\n\n        \"\"\"\n        scen_count = len(self.all_scenario_names)\n\n        if self.options[\"verbose\"] and self.cylinder_rank == 0:\n            print(\"(rank0)\", self.options[\"bundles_per_rank\"], \"bundles per rank\")\n        if self.n_proc * self.options[\"bundles_per_rank\"] > scen_count:\n            raise RuntimeError(\n                \"Not enough scenarios to satisfy the bundles_per_rank requirement\"\n            )\n\n        # dict: rank number --> list of scenario names owned by rank\n        names_at_rank = {\n            curr_rank: [self.all_scenario_names[i] for i in slc]\n            for (curr_rank, slc) in enumerate(self._rank_slices)\n        }\n\n        self.names_in_bundles = dict()\n        num_bundles = self.options[\"bundles_per_rank\"]\n\n        for curr_rank in range(self.n_proc):\n            scen_count = len(names_at_rank[curr_rank])\n            avg = scen_count / num_bundles\n            slices = [\n                range(int(i * avg), int((i + 1) * avg)) for i in range(num_bundles)\n            ]\n            self.names_in_bundles[curr_rank] = {\n                curr_bundle: [names_at_rank[curr_rank][i] for i in slc]\n                for (curr_bundle, slc) in enumerate(slices)\n            }\n\n    def _create_scenarios(self, scenario_creator_kwargs):\n        \"\"\" Call the scenario_creator for every local scenario, and store the\n            results in self.local_scenarios (dict indexed by scenario names).\n\n            Notes:\n                If a scenario probability is not specified as an attribute\n                _mpisppy_probability of the ConcreteModel returned by ScenarioCreator,\n                this function automatically assumes uniform probabilities.\n        \"\"\"\n        if self.scenarios_constructed:\n            raise RuntimeError(\"Scenarios already constructed.\")\n\n        if scenario_creator_kwargs is None:\n            scenario_creator_kwargs = dict()\n\n        local_ict = list() # Local instance creation times for time tracking\n        for sname in self.local_scenario_names:\n            instance_creation_start_time = time.time()\n            s = self.scenario_creator(sname, **scenario_creator_kwargs)\n            self.local_scenarios[sname] = s\n            if self.multistage:\n                #Checking that the scenario can have an associated leaf node in all_nodenames\n                stmax = np.argmax([nd.stage for nd in s._mpisppy_node_list])\n                if(s._mpisppy_node_list[stmax].name)+'_0' not in self.all_nodenames:\n                    raise RuntimeError(\"The leaf node associated with this scenario is not on all_nodenames\"\n                        f\"Its last non-leaf node {s._mpisppy_node_list[stmax].name} has no first child {s._mpisppy_node_list[stmax].name+'_0'}\")\n            local_ict.append(time.time() - instance_creation_start_time)\n            \n        if self.options.get(\"display_timing\", False):\n            all_instance_creation_times = self.mpicomm.gather(\n                local_ict, root=0\n            )\n            if self.cylinder_rank == 0:\n                aict = [ict for l_ict in all_instance_creation_times for ict in l_ict]\n                print(\"Scenario instance creation times:\")\n                print(f\"\\tmin={np.min(aict):4.2f} mean={np.mean(aict):4.2f} max={np.max(aict):4.2f}\")\n        self.scenarios_constructed = True\n\n    def _attach_nonant_indices(self):\n        for (sname, scenario) in self.local_scenarios.items():\n            _nonant_indices = dict()\n            nlens = scenario._mpisppy_data.nlens        \n            for node in scenario._mpisppy_node_list:\n                ndn = node.name\n                for i in range(nlens[ndn]):\n                    _nonant_indices[ndn,i] = node.nonant_vardata_list[i]\n            scenario._mpisppy_data.nonant_indices = _nonant_indices\n        self.nonant_length = len(_nonant_indices)\n\n\n    def _attach_nlens(self):\n        for (sname, scenario) in self.local_scenarios.items():\n            # Things need to be by node so we can bind to the\n            # indices of the vardata lists for the nodes.\n            scenario._mpisppy_data.nlens = {\n                node.name: len(node.nonant_vardata_list)\n                for node in scenario._mpisppy_node_list\n            }\n\n            # NOTE: This only is used by extensions.xhatbase.XhatBase._try_one.\n            #       If that is re-factored, we can remove it here.\n            scenario._mpisppy_data.cistart = dict()\n            sofar = 0\n            for ndn, ndn_len in scenario._mpisppy_data.nlens.items():\n                scenario._mpisppy_data.cistart[ndn] = sofar\n                sofar += ndn_len\n\n                \n    def _attach_varid_to_nonant_index(self):\n        \"\"\" Create a map from the id of nonant variables to their Pyomo index.\n        \"\"\"\n        for (sname, scenario) in self.local_scenarios.items():\n            # In order to support rho setting, create a map\n            # from the id of vardata object back its _nonant_index.\n            scenario._mpisppy_data.varid_to_nonant_index =\\\n                {id(var): ndn_i for ndn_i, var in scenario._mpisppy_data.nonant_indices.items()}\n            \n\n    def _create_communicators(self):\n\n        # Create communicator objects, one for each node\n        nonleafnodes = dict()\n        for (sname, scenario) in self.local_scenarios.items():\n            for node in scenario._mpisppy_node_list:\n                nonleafnodes[node.name] = node  # might be assigned&reassigned\n\n        # check the node names given by the scenarios\n        for nodename in nonleafnodes:\n            if nodename not in self.all_nodenames:\n                raise RuntimeError(f\"Tree node '{nodename}' not in all_nodenames list {self.all_nodenames}\")\n\n        # loop over all nodes and make the comms (split requires all ranks)\n        # make sure we loop in the same order, so every rank iterate over\n        # the nodelist\n        for nodename in self.all_nodenames:\n            if nodename == \"ROOT\":\n                self.comms[\"ROOT\"] = self.mpicomm\n            elif nodename in nonleafnodes:\n                #The position in all_nodenames is an integer unique id.\n                nodenumber = self.all_nodenames.index(nodename)\n                # IMPORTANT: See note in sputils._ScenTree.scen_names_to_ranks. Need to keep\n                #            this split aligned with self.scenario_names_to_rank\n                self.comms[nodename] = self.mpicomm.Split(color=nodenumber, key=self.cylinder_rank)\n            else: # this rank is not included in the communicator\n                self.mpicomm.Split(color=MPI.UNDEFINED, key=self.n_proc)\n\n        ## ensure we've set things up correctly for all comms\n        for nodename, comm in self.comms.items():\n            scenario_names_to_comm_rank = self.scenario_names_to_rank[nodename]\n            for sname, rank in scenario_names_to_comm_rank.items():\n                if sname in self.local_scenarios:\n                    if rank != comm.Get_rank():\n                        raise RuntimeError(f\"For the node {nodename}, the scenario {sname} has the rank {rank} from scenario_names_to_rank and {comm.Get_rank()} from its comm.\")\n                        \n        ## ensure we've set things up correctly for all local scenarios\n        for sname in self.local_scenarios:\n            for nodename, comm in self.comms.items():\n                scenario_names_to_comm_rank = self.scenario_names_to_rank[nodename]\n                if sname in scenario_names_to_comm_rank:\n                    if comm.Get_rank() != scenario_names_to_comm_rank[sname]:\n                        raise RuntimeError(f\"For the node {nodename}, the scenario {sname} has the rank {rank} from scenario_names_to_rank and {comm.Get_rank()} from its comm.\")\n\n\n    def _compute_unconditional_node_probabilities(self):\n        \"\"\" calculates unconditional node probabilities and prob_coeff\n            and prob0_mask is set to a scalar 1 (used by variable_probability)\"\"\"\n        for k,s in self.local_scenarios.items():\n            root = s._mpisppy_node_list[0]\n            root.uncond_prob = 1.0\n            for parent,child in zip(s._mpisppy_node_list[:-1],s._mpisppy_node_list[1:]):\n                child.uncond_prob = parent.uncond_prob * child.cond_prob\n            if not hasattr(s._mpisppy_data, 'prob_coeff'):\n                s._mpisppy_data.prob_coeff = dict()\n                s._mpisppy_data.prob0_mask = dict()\n                for node in s._mpisppy_node_list:\n                    s._mpisppy_data.prob_coeff[node.name] = (s._mpisppy_probability / node.uncond_prob)\n                    s._mpisppy_data.prob0_mask[node.name] = 1.0  # needs to be a float\n\n\n    def _use_variable_probability_setter(self, verbose=False):\n        \"\"\" set variable probability unconditional values using a function self.variable_probability\n        that gives us a list of (id(vardata), probability)]\n        ALSO set prob0_mask, which is a mask for W calculations (mask out zero probs)\n        Note: We estimate that less than 0.01 of mpi-sppy runs will call this.\n        \"\"\"\n        if self.variable_probability is None:\n            for s in self.local_scenarios.values():\n                s._mpisppy_data.has_variable_probability = False\n            return\n        didit = 0\n        skipped = 0\n        variable_probability_kwargs = self.options['variable_probability_kwargs'] \\\n                            if 'variable_probability_kwargs' in self.options \\\n                            else dict()\n        sum_probs = {} # indexed by (ndn,i) - maps to sum of probs for that variable\n        for sname, s in self.local_scenarios.items():\n            variable_probability = self.variable_probability(s, **variable_probability_kwargs)\n            s._mpisppy_data.has_variable_probability = True\n            for (vid, prob) in variable_probability:\n                ndn, i = s._mpisppy_data.varid_to_nonant_index[vid]\n                # If you are going to do any variables at a node, you have to do all.\n                if type(s._mpisppy_data.prob_coeff[ndn]) is float:  # not yet a vector\n                    defprob = s._mpisppy_data.prob_coeff[ndn]\n                    s._mpisppy_data.prob_coeff[ndn] = np.full(s._mpisppy_data.nlens[ndn], defprob, dtype='d')\n                    s._mpisppy_data.prob0_mask[ndn] = np.ones(s._mpisppy_data.nlens[ndn], dtype='d')\n                s._mpisppy_data.prob_coeff[ndn][i] = prob\n                if prob == 0:  # there's probably a way to do this in numpy...\n                    s._mpisppy_data.prob0_mask[ndn][i] = 0\n                sum_probs[(ndn,i)] = sum_probs.get((ndn,i),0.0) + prob\n            didit += len(variable_probability)\n            skipped += len(s._mpisppy_data.varid_to_nonant_index) - didit\n        \n        \"\"\" this needs to be MPIized; but check below should do the trick\n        for (ndn,i),prob in sum_probs.items():\n            if not math.isclose(prob, 1.0, abs_tol=self.E1_tolerance):\n                raise RuntimeError(f\"Probability sum for variable with nonant index={i} at node={ndn} is not unity - computed sum={prob}\")\n        \"\"\"\n\n        if verbose and self.cylinder_rank == 0:\n            print (\"variable_probability set\",didit,\"and skipped\",skipped)\n\n        if not self.options.get('do_not_check_variable_probabilities', False):\n            self._check_variable_probabilities_sum(verbose)\n\n    def is_zero_prob( self, scenario_model, var ):\n        \"\"\"\n        Args:\n            scenario_model : a value in SPBase.local_scenarios\n            var : a Pyomo Var on the scenario_model\n\n        Returns:\n            True if the variable has 0 probability, False otherwise\n        \"\"\"\n        if self.variable_probability is None:\n            return False\n        _mpisppy_data = scenario_model._mpisppy_data\n        ndn, i = _mpisppy_data.varid_to_nonant_index[id(var)]\n        if isinstance(_mpisppy_data.prob_coeff[ndn], np.ndarray):\n            return float(_mpisppy_data.prob_coeff[ndn][i]) == 0.\n        else:\n            return False\n\n    def _check_variable_probabilities_sum(self, verbose):\n\n        nodenames = [] # to transmit to comms\n        local_concats = {}   # keys are tree node names\n        global_concats =  {} # values sums of node conditional probabilities\n\n        # we need to accumulate all local contributions before the reduce\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                if node.name not in nodenames:\n                    ndn = node.name\n                    nodenames.append(ndn)\n                    local_concats[ndn] = np.zeros(nlens[ndn], dtype='d')\n                    global_concats[ndn] = np.zeros(nlens[ndn], dtype='d')\n\n        # sum local conditional probabilities\n        for k,s in self.local_scenarios.items():\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                local_concats[ndn] += s._mpisppy_data.prob_coeff[ndn]\n\n        # compute sum node conditional probabilities (reduction)\n        for ndn in nodenames:\n            self.comms[ndn].Allreduce(\n                [local_concats[ndn], MPI.DOUBLE],\n                [global_concats[ndn], MPI.DOUBLE],\n                op=MPI.SUM)\n\n        tol = self.E1_tolerance\n        checked_nodes = list()\n        # check sum node conditional probabilites are close to 1\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                if ndn not in checked_nodes:\n                    if not np.allclose(global_concats[ndn], 1., atol=tol):\n                        notclose = ~np.isclose(global_concats[ndn], 1., atol=tol)\n                        indices = np.nonzero(notclose)[0]\n                        bad_vars = [ s._mpisppy_data.nonant_indices[ndn,idx].name for idx in indices ]\n                        badprobs = [ global_concats[ndn][idx] for idx in indices]\n                        raise RuntimeError(f\"Node {ndn}, variables {bad_vars} have respective\"\n                                           f\" conditional probability sum {badprobs}\"\n                                           \" which are not 1\")\n                    checked_nodes.append(ndn)\n\n\n    def _look_and_leap(self):\n        for (sname, scenario) in self.local_scenarios.items():\n\n            if not hasattr(scenario, \"_mpisppy_data\"):\n                scenario._mpisppy_data = pyo.Block(name=\"For non-Pyomo mpi-sppy data\")\n            if not hasattr(scenario, \"_mpisppy_model\"):\n                scenario._mpisppy_model = pyo.Block(name=\"For mpi-sppy Pyomo additions to the scenario model\")\n\n            if hasattr(scenario, \"PySP_prob\"):\n                raise RuntimeError(f\"PySP_prob is deprecated; use _mpisppy_probability\")\n            pspec =  scenario._mpisppy_probability if hasattr(scenario, \"_mpisppy_probability\") else None\n            if pspec is None or pspec == \"uniform\":\n                prob = 1./len(self.all_scenario_names)\n                if self.cylinder_rank == 0 and pspec is None:\n                    print(f\"Did not find _mpisppy_probability, assuming uniform probability {prob}\")\n                scenario._mpisppy_probability = prob\n            if not hasattr(scenario, \"_mpisppy_node_list\"):\n                raise RuntimeError(f\"_mpisppy_node_list not found on scenario {sname}\")\n\n    def _options_check(self, required_options, given_options):\n        \"\"\" Confirm that the specified list of options contains the specified\n            list of required options. Raises a ValueError if anything is\n            missing.\n        \"\"\"\n        missing = [option for option in required_options if given_options.get(option) is None] \n        if missing:\n            raise ValueError(f\"Missing the following required options: {', '.join(missing)}\")\n\n    @property\n    def spcomm(self):\n        if self._spcomm is None:\n            return None\n        return self._spcomm()\n\n    @spcomm.setter\n    def spcomm(self, value):\n        if self._spcomm is None:\n            self._spcomm = weakref.ref(value)\n        else:\n            raise RuntimeError(\"SPBase.spcomm should only be set once\")\n\n\n    def gather_var_values_to_rank0(self, get_zero_prob_values=False):\n        \"\"\" Gather the values of the nonanticipative variables to the root of\n        the `mpicomm` for the cylinder\n\n        Returns:\n            dict or None:\n                On the root (rank0), returns a dictionary mapping\n                (scenario_name, variable_name) pairs to their values. On other\n                ranks, returns None.\n        \"\"\"\n        var_values = dict()\n        for (sname, model) in self.local_scenarios.items():\n            for node in model._mpisppy_node_list:\n                for var in node.nonant_vardata_list:\n                    var_name = var.name\n                    if self.bundling:\n                        dot_index = var_name.find('.')\n                        assert dot_index >= 0\n                        var_name = var_name[(dot_index+1):]\n                    if (self.is_zero_prob(model, var)) and (not get_zero_prob_values):\n                        var_values[sname, var_name] = None\n                    else:\n                        var_values[sname, var_name] = pyo.value(var)\n\n        if self.n_proc == 1:\n            return var_values\n\n        result = self.mpicomm.gather(var_values, root=0)\n\n        if (self.cylinder_rank == 0):\n            result = {key: value\n                for dic in result\n                for (key, value) in dic.items()\n            }\n            return result\n\n\n    def report_var_values_at_rank0(self, header=\"\", print_zero_prob_values=False):\n        \"\"\" Pretty-print the values and associated statistics for\n        non-anticipative variables across all scenarios. \"\"\"\n\n        var_values = self.gather_var_values_to_rank0(get_zero_prob_values=print_zero_prob_values)\n\n        if self.cylinder_rank == 0:\n\n            if len(header) != 0:\n                print(header)\n\n            scenario_names = sorted(set(x for (x,y) in var_values))\n            max_scenario_name_len = max(len(s) for s in scenario_names)\n            variable_names = sorted(set(y for (x,y) in var_values))\n            max_variable_name_len = max(len(v) for v in variable_names)\n            # the \"10\" below is a reasonable minimum for floating-point output\n            value_field_len = max(10, max_scenario_name_len)\n\n            print(\"{0: <{width}s} | \".format(\"\", width=max_variable_name_len), end='')\n            for this_scenario in scenario_names:\n                print(\"{0: ^{width}s} \".format(this_scenario, width=value_field_len), end='')\n            print(\"\")\n\n            for this_var in variable_names:\n                print(\"{0: <{width}} | \".format(this_var, width=max_variable_name_len), end='')\n                for this_scenario in scenario_names:\n                    this_var_value = var_values[this_scenario, this_var]\n                    if (this_var_value == None) and (not print_zero_prob_values):\n                        print(\"{0: ^{width}s}\".format(\"-\", width=value_field_len), end='')\n                    else:\n                        print(\"{0: {width}.4f}\".format(this_var_value, width=value_field_len), end='')\n                    print(\" \", end='')\n                print(\"\")\n\n    def write_first_stage_solution(self, file_name,\n            first_stage_solution_writer=sputils.first_stage_nonant_writer):\n        \"\"\" Writes the first-stage solution, if this object reports one available.\n\n            Args:\n                file_name: path of file to write first stage solution to\n                first_stage_solution_writer (optional): custom first stage solution writer function\n        \"\"\"\n\n        if not self.first_stage_solution_available:\n            raise RuntimeError(\"No first stage solution available\")\n        if self.cylinder_rank == 0:\n            dirname = os.path.dirname(file_name)\n            if dirname != '':\n                os.makedirs(os.path.dirname(file_name), exist_ok=True)\n            representative_scenario = self.local_scenarios[self.local_scenario_names[0]]\n            first_stage_solution_writer(file_name, representative_scenario, self.bundling)\n\n    def write_tree_solution(self, directory_name,\n            scenario_tree_solution_writer=sputils.scenario_tree_solution_writer):\n        \"\"\" Writes the tree solution, if this object reports one available.\n            Raises a RuntimeError if it is not.\n\n            Args:\n                directory_name: directory to write tree solution to\n                scenario_tree_solution_writer (optional): custom scenario solution writer function\n        \"\"\"\n        if not self.tree_solution_available:\n            raise RuntimeError(\"No tree solution available\")\n        if self.cylinder_rank == 0:\n            os.makedirs(directory_name, exist_ok=True)\n        self.mpicomm.Barrier()\n        for scenario_name, scenario in self.local_scenarios.items():\n            scenario_tree_solution_writer(directory_name, scenario_name, scenario, self.bundling)",
  "def __init__(\n            self,\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=None,\n            all_nodenames=None,\n            mpicomm=None,\n            scenario_creator_kwargs=None,\n            variable_probability=None,\n            E1_tolerance=1e-5\n    ):\n        # TODO add missing and private attributes (JP)\n        # TODO add a class attribute called ROOTNODENAME = \"ROOT\"\n        # TODO? add decorators to the class attributes\n\n        self.start_time = time.perf_counter()\n        self.options = options\n        self.all_scenario_names = all_scenario_names\n        self.scenario_creator = scenario_creator\n        self.scenario_denouement = scenario_denouement\n        self.comms = dict()\n        self.local_scenarios = dict()\n        self.local_scenario_names = list()\n        self.E1_tolerance = E1_tolerance  # probs must sum to almost 1\n        self.names_in_bundles = None\n        self.scenarios_constructed = False\n        if all_nodenames is None:\n            self.all_nodenames = [\"ROOT\"]\n        elif \"ROOT\" in all_nodenames:\n            self.all_nodenames = all_nodenames\n            self._check_nodenames()\n        else:\n            raise RuntimeError(\"'ROOT' must be in the list of node names\")\n        self.variable_probability = variable_probability\n        self.multistage = (len(self.all_nodenames) > 1)\n\n        # Set up MPI communicator and rank\n        if mpicomm is not None:\n            self.mpicomm = mpicomm\n        else:\n            self.mpicomm = MPI.COMM_WORLD\n        self.cylinder_rank = self.mpicomm.Get_rank()\n        self.n_proc = self.mpicomm.Get_size()\n        self.global_rank = MPI.COMM_WORLD.Get_rank()\n\n\n        if options.get(\"toc\", True):\n            global_toc(\"Initializing SPBase\")\n\n        if self.n_proc > len(self.all_scenario_names):\n            raise RuntimeError(\"More ranks than scenarios\")\n\n        self._calculate_scenario_ranks()\n        if \"bundles_per_rank\" in self.options and self.options[\"bundles_per_rank\"] > 0:\n            self._assign_bundles()\n            self.bundling = True\n        else:\n            self.bundling = False\n        self._create_scenarios(scenario_creator_kwargs)\n        self._look_and_leap()\n        self._compute_unconditional_node_probabilities()\n        self._attach_nlens()\n        self._attach_nonant_indices()\n        self._attach_varid_to_nonant_index()\n        self._create_communicators()\n        self._verify_nonant_lengths()\n        self._set_sense()\n        self._use_variable_probability_setter()\n\n        ## SPCommunicator object\n        self._spcomm = None\n\n        # for writers, if the appropriate\n        # solution is loaded into the subproblems\n        self.tree_solution_available = False\n        self.first_stage_solution_available = False",
  "def _set_sense(self, comm=None):\n        \"\"\" Check to confirm that all the models constructed by scenario_crator\n            have the same sense (min v. max), and set self.is_minimizing\n            accordingly.\n        \"\"\"\n        is_min, clear = sputils._models_have_same_sense(self.local_scenarios)\n        if not clear:\n            raise RuntimeError(\n                \"All scenario models must have the same \"\n                \"model sense (minimize or maximize)\"\n            )\n        self.is_minimizing = is_min\n\n        if self.n_proc <= 1:\n            return\n\n        # Check that all the ranks agree\n        global_senses = self.mpicomm.gather(is_min, root=0)\n        if self.cylinder_rank != 0:\n            return\n        sense = global_senses[0]\n        clear = all(val == sense for val in global_senses)\n        if not clear:\n            raise RuntimeError(\n                \"All scenario models must have the same \"\n                \"model sense (minimize or maximize)\"\n            )",
  "def _verify_nonant_lengths(self):\n        local_node_nonant_lengths = {}   # keys are tree node names\n\n        # we need to accumulate all local contributions before the reduce\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                mylen = nlens[ndn]\n                if ndn not in local_node_nonant_lengths:\n                    local_node_nonant_lengths[ndn] = mylen\n                elif local_node_nonant_lengths[ndn] != mylen:\n                    raise RuntimeError(f\"Tree node {ndn} has different number of non-anticipative \"\n                            f\"variables between scenarios {mylen} vs. {local_node_nonant_lengths[ndn]}\")\n\n        # compute node values(reduction)\n        for ndn, val in local_node_nonant_lengths.items():\n            local_val = np.array([val], 'i')\n            max_val = np.zeros(1, 'i')\n            self.comms[ndn].Allreduce([local_val, MPI.INT],\n                                      [max_val, MPI.INT],\n                                      op=MPI.MAX)\n\n            if val != int(max_val[0]):\n                raise RuntimeError(f\"Tree node {ndn} has different number of non-anticipative \"\n                        f\"variables between scenarios {val} vs. max {max_val[0]}\")",
  "def _check_nodenames(self):\n        for ndn in self.all_nodenames:\n            if ndn != 'ROOT' and sputils.parent_ndn(ndn) not in self.all_nodenames:\n                raise RuntimeError(f\"all_nodenames is inconsistent:\"\n                                   f\"The node {sputils.parent_ndn(ndn)}, parent of {ndn}, is missing.\")",
  "def _calculate_scenario_ranks(self):\n        \"\"\" Populate the following attributes\n            1. self.scenario_names_to_rank (dict of dict):\n                keys are comms (i.e., tree nodes); values are dicts with keys\n                that are scenario names and values that are ranks within that comm\n\n            2. self._rank_slices (list of lists)\n                indices correspond to ranks in self.mpicomm and the values are a list\n                of scenario indices\n                rank -> list of scenario indices for that rank\n\n            3. self._scenario_slices (list)\n                indices are scenario indices and values are the rank of that scenario\n                within self.mpicomm\n                scenario index -> rank\n\n            4. self._scenario_tree (instance of sputils._ScenTree)\n\n            5. self.local_scenario_names (list)\n               List of index names owned by the local rank\n\n        \"\"\"\n        tree = sputils._ScenTree(self.all_nodenames, self.all_scenario_names)\n\n        self.scenario_names_to_rank, self._rank_slices, self._scenario_slices =\\\n                tree.scen_names_to_ranks(self.n_proc)\n        self._scenario_tree = tree\n        self.nonleaves = {node.name : node for node in tree.nonleaves}\n\n        # list of scenario names owned locally\n        self.local_scenario_names = [\n            self.all_scenario_names[i] for i in self._rank_slices[self.cylinder_rank]\n        ]",
  "def _assign_bundles(self):\n        \"\"\" Create self.names_in_bundles, a dict of dicts\n            \n            self.names_in_bundles[rank number][bundle number] = \n                    list of scenarios in that bundle\n\n        \"\"\"\n        scen_count = len(self.all_scenario_names)\n\n        if self.options[\"verbose\"] and self.cylinder_rank == 0:\n            print(\"(rank0)\", self.options[\"bundles_per_rank\"], \"bundles per rank\")\n        if self.n_proc * self.options[\"bundles_per_rank\"] > scen_count:\n            raise RuntimeError(\n                \"Not enough scenarios to satisfy the bundles_per_rank requirement\"\n            )\n\n        # dict: rank number --> list of scenario names owned by rank\n        names_at_rank = {\n            curr_rank: [self.all_scenario_names[i] for i in slc]\n            for (curr_rank, slc) in enumerate(self._rank_slices)\n        }\n\n        self.names_in_bundles = dict()\n        num_bundles = self.options[\"bundles_per_rank\"]\n\n        for curr_rank in range(self.n_proc):\n            scen_count = len(names_at_rank[curr_rank])\n            avg = scen_count / num_bundles\n            slices = [\n                range(int(i * avg), int((i + 1) * avg)) for i in range(num_bundles)\n            ]\n            self.names_in_bundles[curr_rank] = {\n                curr_bundle: [names_at_rank[curr_rank][i] for i in slc]\n                for (curr_bundle, slc) in enumerate(slices)\n            }",
  "def _create_scenarios(self, scenario_creator_kwargs):\n        \"\"\" Call the scenario_creator for every local scenario, and store the\n            results in self.local_scenarios (dict indexed by scenario names).\n\n            Notes:\n                If a scenario probability is not specified as an attribute\n                _mpisppy_probability of the ConcreteModel returned by ScenarioCreator,\n                this function automatically assumes uniform probabilities.\n        \"\"\"\n        if self.scenarios_constructed:\n            raise RuntimeError(\"Scenarios already constructed.\")\n\n        if scenario_creator_kwargs is None:\n            scenario_creator_kwargs = dict()\n\n        local_ict = list() # Local instance creation times for time tracking\n        for sname in self.local_scenario_names:\n            instance_creation_start_time = time.time()\n            s = self.scenario_creator(sname, **scenario_creator_kwargs)\n            self.local_scenarios[sname] = s\n            if self.multistage:\n                #Checking that the scenario can have an associated leaf node in all_nodenames\n                stmax = np.argmax([nd.stage for nd in s._mpisppy_node_list])\n                if(s._mpisppy_node_list[stmax].name)+'_0' not in self.all_nodenames:\n                    raise RuntimeError(\"The leaf node associated with this scenario is not on all_nodenames\"\n                        f\"Its last non-leaf node {s._mpisppy_node_list[stmax].name} has no first child {s._mpisppy_node_list[stmax].name+'_0'}\")\n            local_ict.append(time.time() - instance_creation_start_time)\n            \n        if self.options.get(\"display_timing\", False):\n            all_instance_creation_times = self.mpicomm.gather(\n                local_ict, root=0\n            )\n            if self.cylinder_rank == 0:\n                aict = [ict for l_ict in all_instance_creation_times for ict in l_ict]\n                print(\"Scenario instance creation times:\")\n                print(f\"\\tmin={np.min(aict):4.2f} mean={np.mean(aict):4.2f} max={np.max(aict):4.2f}\")\n        self.scenarios_constructed = True",
  "def _attach_nonant_indices(self):\n        for (sname, scenario) in self.local_scenarios.items():\n            _nonant_indices = dict()\n            nlens = scenario._mpisppy_data.nlens        \n            for node in scenario._mpisppy_node_list:\n                ndn = node.name\n                for i in range(nlens[ndn]):\n                    _nonant_indices[ndn,i] = node.nonant_vardata_list[i]\n            scenario._mpisppy_data.nonant_indices = _nonant_indices\n        self.nonant_length = len(_nonant_indices)",
  "def _attach_nlens(self):\n        for (sname, scenario) in self.local_scenarios.items():\n            # Things need to be by node so we can bind to the\n            # indices of the vardata lists for the nodes.\n            scenario._mpisppy_data.nlens = {\n                node.name: len(node.nonant_vardata_list)\n                for node in scenario._mpisppy_node_list\n            }\n\n            # NOTE: This only is used by extensions.xhatbase.XhatBase._try_one.\n            #       If that is re-factored, we can remove it here.\n            scenario._mpisppy_data.cistart = dict()\n            sofar = 0\n            for ndn, ndn_len in scenario._mpisppy_data.nlens.items():\n                scenario._mpisppy_data.cistart[ndn] = sofar\n                sofar += ndn_len",
  "def _attach_varid_to_nonant_index(self):\n        \"\"\" Create a map from the id of nonant variables to their Pyomo index.\n        \"\"\"\n        for (sname, scenario) in self.local_scenarios.items():\n            # In order to support rho setting, create a map\n            # from the id of vardata object back its _nonant_index.\n            scenario._mpisppy_data.varid_to_nonant_index =\\\n                {id(var): ndn_i for ndn_i, var in scenario._mpisppy_data.nonant_indices.items()}",
  "def _create_communicators(self):\n\n        # Create communicator objects, one for each node\n        nonleafnodes = dict()\n        for (sname, scenario) in self.local_scenarios.items():\n            for node in scenario._mpisppy_node_list:\n                nonleafnodes[node.name] = node  # might be assigned&reassigned\n\n        # check the node names given by the scenarios\n        for nodename in nonleafnodes:\n            if nodename not in self.all_nodenames:\n                raise RuntimeError(f\"Tree node '{nodename}' not in all_nodenames list {self.all_nodenames}\")\n\n        # loop over all nodes and make the comms (split requires all ranks)\n        # make sure we loop in the same order, so every rank iterate over\n        # the nodelist\n        for nodename in self.all_nodenames:\n            if nodename == \"ROOT\":\n                self.comms[\"ROOT\"] = self.mpicomm\n            elif nodename in nonleafnodes:\n                #The position in all_nodenames is an integer unique id.\n                nodenumber = self.all_nodenames.index(nodename)\n                # IMPORTANT: See note in sputils._ScenTree.scen_names_to_ranks. Need to keep\n                #            this split aligned with self.scenario_names_to_rank\n                self.comms[nodename] = self.mpicomm.Split(color=nodenumber, key=self.cylinder_rank)\n            else: # this rank is not included in the communicator\n                self.mpicomm.Split(color=MPI.UNDEFINED, key=self.n_proc)\n\n        ## ensure we've set things up correctly for all comms\n        for nodename, comm in self.comms.items():\n            scenario_names_to_comm_rank = self.scenario_names_to_rank[nodename]\n            for sname, rank in scenario_names_to_comm_rank.items():\n                if sname in self.local_scenarios:\n                    if rank != comm.Get_rank():\n                        raise RuntimeError(f\"For the node {nodename}, the scenario {sname} has the rank {rank} from scenario_names_to_rank and {comm.Get_rank()} from its comm.\")\n                        \n        ## ensure we've set things up correctly for all local scenarios\n        for sname in self.local_scenarios:\n            for nodename, comm in self.comms.items():\n                scenario_names_to_comm_rank = self.scenario_names_to_rank[nodename]\n                if sname in scenario_names_to_comm_rank:\n                    if comm.Get_rank() != scenario_names_to_comm_rank[sname]:\n                        raise RuntimeError(f\"For the node {nodename}, the scenario {sname} has the rank {rank} from scenario_names_to_rank and {comm.Get_rank()} from its comm.\")",
  "def _compute_unconditional_node_probabilities(self):\n        \"\"\" calculates unconditional node probabilities and prob_coeff\n            and prob0_mask is set to a scalar 1 (used by variable_probability)\"\"\"\n        for k,s in self.local_scenarios.items():\n            root = s._mpisppy_node_list[0]\n            root.uncond_prob = 1.0\n            for parent,child in zip(s._mpisppy_node_list[:-1],s._mpisppy_node_list[1:]):\n                child.uncond_prob = parent.uncond_prob * child.cond_prob\n            if not hasattr(s._mpisppy_data, 'prob_coeff'):\n                s._mpisppy_data.prob_coeff = dict()\n                s._mpisppy_data.prob0_mask = dict()\n                for node in s._mpisppy_node_list:\n                    s._mpisppy_data.prob_coeff[node.name] = (s._mpisppy_probability / node.uncond_prob)\n                    s._mpisppy_data.prob0_mask[node.name] = 1.0",
  "def _use_variable_probability_setter(self, verbose=False):\n        \"\"\" set variable probability unconditional values using a function self.variable_probability\n        that gives us a list of (id(vardata), probability)]\n        ALSO set prob0_mask, which is a mask for W calculations (mask out zero probs)\n        Note: We estimate that less than 0.01 of mpi-sppy runs will call this.\n        \"\"\"\n        if self.variable_probability is None:\n            for s in self.local_scenarios.values():\n                s._mpisppy_data.has_variable_probability = False\n            return\n        didit = 0\n        skipped = 0\n        variable_probability_kwargs = self.options['variable_probability_kwargs'] \\\n                            if 'variable_probability_kwargs' in self.options \\\n                            else dict()\n        sum_probs = {} # indexed by (ndn,i) - maps to sum of probs for that variable\n        for sname, s in self.local_scenarios.items():\n            variable_probability = self.variable_probability(s, **variable_probability_kwargs)\n            s._mpisppy_data.has_variable_probability = True\n            for (vid, prob) in variable_probability:\n                ndn, i = s._mpisppy_data.varid_to_nonant_index[vid]\n                # If you are going to do any variables at a node, you have to do all.\n                if type(s._mpisppy_data.prob_coeff[ndn]) is float:  # not yet a vector\n                    defprob = s._mpisppy_data.prob_coeff[ndn]\n                    s._mpisppy_data.prob_coeff[ndn] = np.full(s._mpisppy_data.nlens[ndn], defprob, dtype='d')\n                    s._mpisppy_data.prob0_mask[ndn] = np.ones(s._mpisppy_data.nlens[ndn], dtype='d')\n                s._mpisppy_data.prob_coeff[ndn][i] = prob\n                if prob == 0:  # there's probably a way to do this in numpy...\n                    s._mpisppy_data.prob0_mask[ndn][i] = 0\n                sum_probs[(ndn,i)] = sum_probs.get((ndn,i),0.0) + prob\n            didit += len(variable_probability)\n            skipped += len(s._mpisppy_data.varid_to_nonant_index) - didit\n        \n        \"\"\" this needs to be MPIized; but check below should do the trick\n        for (ndn,i),prob in sum_probs.items():\n            if not math.isclose(prob, 1.0, abs_tol=self.E1_tolerance):\n                raise RuntimeError(f\"Probability sum for variable with nonant index={i} at node={ndn} is not unity - computed sum={prob}\")\n        \"\"\"\n\n        if verbose and self.cylinder_rank == 0:\n            print (\"variable_probability set\",didit,\"and skipped\",skipped)\n\n        if not self.options.get('do_not_check_variable_probabilities', False):\n            self._check_variable_probabilities_sum(verbose)",
  "def is_zero_prob( self, scenario_model, var ):\n        \"\"\"\n        Args:\n            scenario_model : a value in SPBase.local_scenarios\n            var : a Pyomo Var on the scenario_model\n\n        Returns:\n            True if the variable has 0 probability, False otherwise\n        \"\"\"\n        if self.variable_probability is None:\n            return False\n        _mpisppy_data = scenario_model._mpisppy_data\n        ndn, i = _mpisppy_data.varid_to_nonant_index[id(var)]\n        if isinstance(_mpisppy_data.prob_coeff[ndn], np.ndarray):\n            return float(_mpisppy_data.prob_coeff[ndn][i]) == 0.\n        else:\n            return False",
  "def _check_variable_probabilities_sum(self, verbose):\n\n        nodenames = [] # to transmit to comms\n        local_concats = {}   # keys are tree node names\n        global_concats =  {} # values sums of node conditional probabilities\n\n        # we need to accumulate all local contributions before the reduce\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                if node.name not in nodenames:\n                    ndn = node.name\n                    nodenames.append(ndn)\n                    local_concats[ndn] = np.zeros(nlens[ndn], dtype='d')\n                    global_concats[ndn] = np.zeros(nlens[ndn], dtype='d')\n\n        # sum local conditional probabilities\n        for k,s in self.local_scenarios.items():\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                local_concats[ndn] += s._mpisppy_data.prob_coeff[ndn]\n\n        # compute sum node conditional probabilities (reduction)\n        for ndn in nodenames:\n            self.comms[ndn].Allreduce(\n                [local_concats[ndn], MPI.DOUBLE],\n                [global_concats[ndn], MPI.DOUBLE],\n                op=MPI.SUM)\n\n        tol = self.E1_tolerance\n        checked_nodes = list()\n        # check sum node conditional probabilites are close to 1\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                if ndn not in checked_nodes:\n                    if not np.allclose(global_concats[ndn], 1., atol=tol):\n                        notclose = ~np.isclose(global_concats[ndn], 1., atol=tol)\n                        indices = np.nonzero(notclose)[0]\n                        bad_vars = [ s._mpisppy_data.nonant_indices[ndn,idx].name for idx in indices ]\n                        badprobs = [ global_concats[ndn][idx] for idx in indices]\n                        raise RuntimeError(f\"Node {ndn}, variables {bad_vars} have respective\"\n                                           f\" conditional probability sum {badprobs}\"\n                                           \" which are not 1\")\n                    checked_nodes.append(ndn)",
  "def _look_and_leap(self):\n        for (sname, scenario) in self.local_scenarios.items():\n\n            if not hasattr(scenario, \"_mpisppy_data\"):\n                scenario._mpisppy_data = pyo.Block(name=\"For non-Pyomo mpi-sppy data\")\n            if not hasattr(scenario, \"_mpisppy_model\"):\n                scenario._mpisppy_model = pyo.Block(name=\"For mpi-sppy Pyomo additions to the scenario model\")\n\n            if hasattr(scenario, \"PySP_prob\"):\n                raise RuntimeError(f\"PySP_prob is deprecated; use _mpisppy_probability\")\n            pspec =  scenario._mpisppy_probability if hasattr(scenario, \"_mpisppy_probability\") else None\n            if pspec is None or pspec == \"uniform\":\n                prob = 1./len(self.all_scenario_names)\n                if self.cylinder_rank == 0 and pspec is None:\n                    print(f\"Did not find _mpisppy_probability, assuming uniform probability {prob}\")\n                scenario._mpisppy_probability = prob\n            if not hasattr(scenario, \"_mpisppy_node_list\"):\n                raise RuntimeError(f\"_mpisppy_node_list not found on scenario {sname}\")",
  "def _options_check(self, required_options, given_options):\n        \"\"\" Confirm that the specified list of options contains the specified\n            list of required options. Raises a ValueError if anything is\n            missing.\n        \"\"\"\n        missing = [option for option in required_options if given_options.get(option) is None] \n        if missing:\n            raise ValueError(f\"Missing the following required options: {', '.join(missing)}\")",
  "def spcomm(self):\n        if self._spcomm is None:\n            return None\n        return self._spcomm()",
  "def spcomm(self, value):\n        if self._spcomm is None:\n            self._spcomm = weakref.ref(value)\n        else:\n            raise RuntimeError(\"SPBase.spcomm should only be set once\")",
  "def gather_var_values_to_rank0(self, get_zero_prob_values=False):\n        \"\"\" Gather the values of the nonanticipative variables to the root of\n        the `mpicomm` for the cylinder\n\n        Returns:\n            dict or None:\n                On the root (rank0), returns a dictionary mapping\n                (scenario_name, variable_name) pairs to their values. On other\n                ranks, returns None.\n        \"\"\"\n        var_values = dict()\n        for (sname, model) in self.local_scenarios.items():\n            for node in model._mpisppy_node_list:\n                for var in node.nonant_vardata_list:\n                    var_name = var.name\n                    if self.bundling:\n                        dot_index = var_name.find('.')\n                        assert dot_index >= 0\n                        var_name = var_name[(dot_index+1):]\n                    if (self.is_zero_prob(model, var)) and (not get_zero_prob_values):\n                        var_values[sname, var_name] = None\n                    else:\n                        var_values[sname, var_name] = pyo.value(var)\n\n        if self.n_proc == 1:\n            return var_values\n\n        result = self.mpicomm.gather(var_values, root=0)\n\n        if (self.cylinder_rank == 0):\n            result = {key: value\n                for dic in result\n                for (key, value) in dic.items()\n            }\n            return result",
  "def report_var_values_at_rank0(self, header=\"\", print_zero_prob_values=False):\n        \"\"\" Pretty-print the values and associated statistics for\n        non-anticipative variables across all scenarios. \"\"\"\n\n        var_values = self.gather_var_values_to_rank0(get_zero_prob_values=print_zero_prob_values)\n\n        if self.cylinder_rank == 0:\n\n            if len(header) != 0:\n                print(header)\n\n            scenario_names = sorted(set(x for (x,y) in var_values))\n            max_scenario_name_len = max(len(s) for s in scenario_names)\n            variable_names = sorted(set(y for (x,y) in var_values))\n            max_variable_name_len = max(len(v) for v in variable_names)\n            # the \"10\" below is a reasonable minimum for floating-point output\n            value_field_len = max(10, max_scenario_name_len)\n\n            print(\"{0: <{width}s} | \".format(\"\", width=max_variable_name_len), end='')\n            for this_scenario in scenario_names:\n                print(\"{0: ^{width}s} \".format(this_scenario, width=value_field_len), end='')\n            print(\"\")\n\n            for this_var in variable_names:\n                print(\"{0: <{width}} | \".format(this_var, width=max_variable_name_len), end='')\n                for this_scenario in scenario_names:\n                    this_var_value = var_values[this_scenario, this_var]\n                    if (this_var_value == None) and (not print_zero_prob_values):\n                        print(\"{0: ^{width}s}\".format(\"-\", width=value_field_len), end='')\n                    else:\n                        print(\"{0: {width}.4f}\".format(this_var_value, width=value_field_len), end='')\n                    print(\" \", end='')\n                print(\"\")",
  "def write_first_stage_solution(self, file_name,\n            first_stage_solution_writer=sputils.first_stage_nonant_writer):\n        \"\"\" Writes the first-stage solution, if this object reports one available.\n\n            Args:\n                file_name: path of file to write first stage solution to\n                first_stage_solution_writer (optional): custom first stage solution writer function\n        \"\"\"\n\n        if not self.first_stage_solution_available:\n            raise RuntimeError(\"No first stage solution available\")\n        if self.cylinder_rank == 0:\n            dirname = os.path.dirname(file_name)\n            if dirname != '':\n                os.makedirs(os.path.dirname(file_name), exist_ok=True)\n            representative_scenario = self.local_scenarios[self.local_scenario_names[0]]\n            first_stage_solution_writer(file_name, representative_scenario, self.bundling)",
  "def write_tree_solution(self, directory_name,\n            scenario_tree_solution_writer=sputils.scenario_tree_solution_writer):\n        \"\"\" Writes the tree solution, if this object reports one available.\n            Raises a RuntimeError if it is not.\n\n            Args:\n                directory_name: directory to write tree solution to\n                scenario_tree_solution_writer (optional): custom scenario solution writer function\n        \"\"\"\n        if not self.tree_solution_available:\n            raise RuntimeError(\"No tree solution available\")\n        if self.cylinder_rank == 0:\n            os.makedirs(directory_name, exist_ok=True)\n        self.mpicomm.Barrier()\n        for scenario_name, scenario in self.local_scenarios.items():\n            scenario_tree_solution_writer(directory_name, scenario_name, scenario, self.bundling)",
  "class _MockMPIComm:\n    \n        @property\n        def rank(self):\n            return 0\n    \n        @property\n        def size(self):\n            return 1\n\n        def allreduce(self, sendobj, op=SUM):\n            return _cp.deepcopy(sendobj)\n    \n        def barrier(self):\n            pass\n\n        def bcast(self, data, root=0):\n            return data\n    \n        def gather(self, obj, root=0):\n            if root != self.rank:\n                return\n            else:\n                return [obj]\n    \n        def Gatherv(self, sendbuf, recvbuf, root=0):\n            # TBD: check this\n            _set_data(sendbuf, recvbuf)            \n    \n        def Allreduce(self, sendbuf, recvbuf, op=SUM):\n            _set_data(sendbuf, recvbuf)\n    \n        def Barrier(self):\n            pass\n\n        def Bcast(self, data, root=0):\n            pass\n    \n        def Get_rank(self):\n            return self.rank\n    \n        def Get_size(self):\n            return self.size\n    \n        def Split(self, color=0, key=0):\n            return _MockMPIComm()",
  "def _process_BufSpec(bufspec):\n        if isinstance(bufspec, (list, tuple)):\n            bufspec = bufspec[0]\n        return bufspec, bufspec.size, bufspec.dtype",
  "def _set_data(sendbuf, recvbuf):\n        send_data, send_size, send_type = _process_BufSpec(sendbuf)\n        recv_data, recv_size, recv_type = _process_BufSpec(recvbuf)\n    \n        if send_size != recv_size:\n            raise RuntimeError(f\"Send and receive buffers should be of the same size\")\n        if send_type != recv_type:\n            raise RuntimeError(f\"Send and receive buffers should be of the same type\")\n    \n        recv_data[:] = send_data",
  "def rank(self):\n            return 0",
  "def size(self):\n            return 1",
  "def allreduce(self, sendobj, op=SUM):\n            return _cp.deepcopy(sendobj)",
  "def barrier(self):\n            pass",
  "def bcast(self, data, root=0):\n            return data",
  "def gather(self, obj, root=0):\n            if root != self.rank:\n                return\n            else:\n                return [obj]",
  "def Gatherv(self, sendbuf, recvbuf, root=0):\n            # TBD: check this\n            _set_data(sendbuf, recvbuf)",
  "def Allreduce(self, sendbuf, recvbuf, op=SUM):\n            _set_data(sendbuf, recvbuf)",
  "def Barrier(self):\n            pass",
  "def Bcast(self, data, root=0):\n            pass",
  "def Get_rank(self):\n            return self.rank",
  "def Get_size(self):\n            return self.size",
  "def Split(self, color=0, key=0):\n            return _MockMPIComm()",
  "def profile(filename=None, comm=mpi.COMM_WORLD):\n    pass",
  "class PH(mpisppy.phbase.PHBase):\n    \"\"\" PH. See PHBase for list of args. \"\"\"\n\n    #======================================================================\n    # uncomment the line below to get per-rank profile outputs, which can \n    # be examined with snakeviz (or your favorite profile output analyzer)\n    #@profile(filename=\"profile_out\")\n    def ph_main(self, finalize=True):\n        \"\"\" Execute the PH algorithm.\n\n        Args:\n            finalize (bool, optional, default=True):\n                If True, call `PH.post_loops()`, if False, do not,\n                and return None for Eobj\n\n        Returns:\n            tuple:\n                Tuple containing\n\n                conv (float):\n                    The convergence value (not easily interpretable).\n                Eobj (float or `None`):\n                    If `finalize=True`, this is the expected, weighted\n                    objective value with the proximal term included. This value\n                    is not directly useful. If `finalize=False`, this value is\n                    `None`.\n                trivial_bound (float):\n                    The \"trivial bound\", computed by solving the model with no\n                    nonanticipativity constraints (immediately after iter 0).\n\n        NOTE:\n            You need an xhat finder either in denoument or in an extension.\n        \"\"\"\n        verbose = self.options['verbose']\n        self.PH_Prep()\n        # Why is subproblem_creation() not called in PH_Prep? Answer: xhat_eval.\n        self.subproblem_creation(verbose)\n\n        if (verbose):\n            print('Calling PH Iter0 on global rank {}'.format(global_rank))\n        trivial_bound = self.Iter0()\n        if (verbose):\n            print ('Completed PH Iter0 on global rank {}'.format(global_rank))\n        if ('asynchronousPH' in self.options) and (self.options['asynchronousPH']):\n            raise RuntimeError(\"asynchronousPH is deprecated; use APH\")\n\n        self.iterk_loop()\n\n        if finalize:\n            Eobj = self.post_loops(self.extensions)\n        else:\n            Eobj = None\n\n        return self.conv, Eobj, trivial_bound",
  "def ph_main(self, finalize=True):\n        \"\"\" Execute the PH algorithm.\n\n        Args:\n            finalize (bool, optional, default=True):\n                If True, call `PH.post_loops()`, if False, do not,\n                and return None for Eobj\n\n        Returns:\n            tuple:\n                Tuple containing\n\n                conv (float):\n                    The convergence value (not easily interpretable).\n                Eobj (float or `None`):\n                    If `finalize=True`, this is the expected, weighted\n                    objective value with the proximal term included. This value\n                    is not directly useful. If `finalize=False`, this value is\n                    `None`.\n                trivial_bound (float):\n                    The \"trivial bound\", computed by solving the model with no\n                    nonanticipativity constraints (immediately after iter 0).\n\n        NOTE:\n            You need an xhat finder either in denoument or in an extension.\n        \"\"\"\n        verbose = self.options['verbose']\n        self.PH_Prep()\n        # Why is subproblem_creation() not called in PH_Prep? Answer: xhat_eval.\n        self.subproblem_creation(verbose)\n\n        if (verbose):\n            print('Calling PH Iter0 on global rank {}'.format(global_rank))\n        trivial_bound = self.Iter0()\n        if (verbose):\n            print ('Completed PH Iter0 on global rank {}'.format(global_rank))\n        if ('asynchronousPH' in self.options) and (self.options['asynchronousPH']):\n            raise RuntimeError(\"asynchronousPH is deprecated; use APH\")\n\n        self.iterk_loop()\n\n        if finalize:\n            Eobj = self.post_loops(self.extensions)\n        else:\n            Eobj = None\n\n        return self.conv, Eobj, trivial_bound",
  "def _assert_continuous(m: _BlockData):\n    for v in m.component_data_objects(pyo.Var, descend_into=True, active=True):\n        if not v.is_continuous():\n            raise RuntimeError(f'Variable {v} in block {m} is not continuous; The Schur-Complement method only supports continuous problems.')",
  "class _SCInterface(parapint.interfaces.MPIStochasticSchurComplementInteriorPointInterface):\n    def __init__(self,\n                 local_scenario_models: Dict[str, _BlockData],\n                 all_scenario_names: List[str],\n                 comm: MPI.Comm,\n                 ownership_map: Dict):\n        self.local_scenario_models = local_scenario_models\n\n        models = list(local_scenario_models.values())\n        ref_model = models[0]\n        models = models[1:]\n        self.nonant_vars = list(ref_model._mpisppy_data.nonant_indices.keys())\n\n        super(_SCInterface, self).__init__(scenarios=all_scenario_names,\n                                           nonanticipative_var_identifiers=self.nonant_vars,\n                                           comm=comm,\n                                           ownership_map=ownership_map)\n\n    def build_model_for_scenario(self,\n                                 scenario_identifier: str) -> Tuple[_BlockData, Dict[Any, _GeneralVarData]]:\n        m = self.local_scenario_models[scenario_identifier]\n\n        _assert_continuous(m)\n\n        active_obj = find_active_objective(m)\n        active_obj.deactivate()\n        m._mpisppy_model.weighted_obj = pyo.Objective(expr=m._mpisppy_probability * active_obj.expr, sense=active_obj.sense)\n\n        nonant_vars = m._mpisppy_data.nonant_indices\n        if len(nonant_vars) != len(self.nonant_vars):\n            raise ValueError(f'Number of non-anticipative variables is not consistent in scenario {scenario_identifier}.')\n\n        return m, nonant_vars",
  "class SchurComplement(SPBase):\n    def __init__(self,\n                 options: Union[Dict, SCOptions],\n                 all_scenario_names: List,\n                 scenario_creator: Callable,\n                 scenario_creator_kwargs: Optional[Dict] = None,\n                 all_nodenames=None,\n                 mpicomm=None,\n                 model_name=None,\n                 suppress_warnings=False):\n        super(SchurComplement, self).__init__(options=options,\n                                              all_scenario_names=all_scenario_names,\n                                              scenario_creator=scenario_creator,\n                                              scenario_creator_kwargs=scenario_creator_kwargs,\n                                              all_nodenames=all_nodenames,\n                                              mpicomm=mpicomm)\n\n        if self.bundling:\n            raise ValueError('The Schur-Complement method does not support bundling')\n\n        ownership_map = dict()\n        for _rank, scenario_index_list in enumerate(self._rank_slices):\n            for _scenario_ndx in scenario_index_list:\n                ownership_map[_scenario_ndx] = _rank\n\n        self.interface = _SCInterface(local_scenario_models=self.local_scenarios,\n                                      all_scenario_names=self.all_scenario_names,\n                                      comm=self.mpicomm,\n                                      ownership_map=ownership_map)\n\n    def solve(self):\n        if isinstance(self.options, SCOptions):\n            options = self.options()\n        else:\n            options = SCOptions()(self.options)\n        if options.linalg.solver is None:\n            options.linalg.solver = parapint.linalg.MPISchurComplementLinearSolver(\n                subproblem_solvers={ndx: parapint.linalg.InteriorPointMA27Interface(cntl_options={1: 1e-6}) for ndx in range(len(self.all_scenario_names))},\n                schur_complement_solver=parapint.linalg.InteriorPointMA27Interface(cntl_options={1: 1e-6}))\n\n        status = parapint.algorithms.ip_solve(interface=self.interface,\n                                              options=options)\n        if status != parapint.algorithms.InteriorPointStatus.optimal:\n            raise RuntimeError('Schur-Complement Interior Point algorithm did not converge')\n\n        self.interface.load_primals_into_pyomo_model()\n\n        return status",
  "def __init__(self,\n                 local_scenario_models: Dict[str, _BlockData],\n                 all_scenario_names: List[str],\n                 comm: MPI.Comm,\n                 ownership_map: Dict):\n        self.local_scenario_models = local_scenario_models\n\n        models = list(local_scenario_models.values())\n        ref_model = models[0]\n        models = models[1:]\n        self.nonant_vars = list(ref_model._mpisppy_data.nonant_indices.keys())\n\n        super(_SCInterface, self).__init__(scenarios=all_scenario_names,\n                                           nonanticipative_var_identifiers=self.nonant_vars,\n                                           comm=comm,\n                                           ownership_map=ownership_map)",
  "def build_model_for_scenario(self,\n                                 scenario_identifier: str) -> Tuple[_BlockData, Dict[Any, _GeneralVarData]]:\n        m = self.local_scenario_models[scenario_identifier]\n\n        _assert_continuous(m)\n\n        active_obj = find_active_objective(m)\n        active_obj.deactivate()\n        m._mpisppy_model.weighted_obj = pyo.Objective(expr=m._mpisppy_probability * active_obj.expr, sense=active_obj.sense)\n\n        nonant_vars = m._mpisppy_data.nonant_indices\n        if len(nonant_vars) != len(self.nonant_vars):\n            raise ValueError(f'Number of non-anticipative variables is not consistent in scenario {scenario_identifier}.')\n\n        return m, nonant_vars",
  "def __init__(self,\n                 options: Union[Dict, SCOptions],\n                 all_scenario_names: List,\n                 scenario_creator: Callable,\n                 scenario_creator_kwargs: Optional[Dict] = None,\n                 all_nodenames=None,\n                 mpicomm=None,\n                 model_name=None,\n                 suppress_warnings=False):\n        super(SchurComplement, self).__init__(options=options,\n                                              all_scenario_names=all_scenario_names,\n                                              scenario_creator=scenario_creator,\n                                              scenario_creator_kwargs=scenario_creator_kwargs,\n                                              all_nodenames=all_nodenames,\n                                              mpicomm=mpicomm)\n\n        if self.bundling:\n            raise ValueError('The Schur-Complement method does not support bundling')\n\n        ownership_map = dict()\n        for _rank, scenario_index_list in enumerate(self._rank_slices):\n            for _scenario_ndx in scenario_index_list:\n                ownership_map[_scenario_ndx] = _rank\n\n        self.interface = _SCInterface(local_scenario_models=self.local_scenarios,\n                                      all_scenario_names=self.all_scenario_names,\n                                      comm=self.mpicomm,\n                                      ownership_map=ownership_map)",
  "def solve(self):\n        if isinstance(self.options, SCOptions):\n            options = self.options()\n        else:\n            options = SCOptions()(self.options)\n        if options.linalg.solver is None:\n            options.linalg.solver = parapint.linalg.MPISchurComplementLinearSolver(\n                subproblem_solvers={ndx: parapint.linalg.InteriorPointMA27Interface(cntl_options={1: 1e-6}) for ndx in range(len(self.all_scenario_names))},\n                schur_complement_solver=parapint.linalg.InteriorPointMA27Interface(cntl_options={1: 1e-6}))\n\n        status = parapint.algorithms.ip_solve(interface=self.interface,\n                                              options=options)\n        if status != parapint.algorithms.InteriorPointStatus.optimal:\n            raise RuntimeError('Schur-Complement Interior Point algorithm did not converge')\n\n        self.interface.load_primals_into_pyomo_model()\n\n        return status",
  "class ExtensiveForm(mpisppy.spbase.SPBase):\n    \"\"\" Create and solve an extensive form. \n\n    Attributes:\n        ef (:class:`pyomo.environ.ConcreteModel`):\n            Pyomo model of the extensive form.\n        solver:\n            Solver produced by the Pyomo solver factory.\n\n    Args:\n        options (dict):\n            Dictionary of options. May include a `solver` key to\n            specify which solver name to use on the EF.\n        all_scenario_names (list):\n            List of the names of each scenario in the EF (strings).\n        scenario_creator (callable):\n            Scenario creator function, which takes as input a scenario\n            name, and returns a Pyomo model of that scenario.\n        model_name (str, optional):\n            Name of the resulting EF model object.\n        scenario_creator_kwargs (dict):\n            Keyword args passed to `scenario_creator`.\n        suppress_warnings (bool, optional):\n            Boolean to suppress warnings when building the EF. Default\n            is False.\n\n    Note: allowing use of the \"solver\" option key is for backward compatibility\n\n    \"\"\"\n    def __init__(\n        self,\n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_creator_kwargs=None,\n        all_nodenames=None,\n        model_name=None,\n        suppress_warnings=False,\n    ):\n        \"\"\" Create the EF and associated solver. \"\"\"\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            all_nodenames=all_nodenames\n        )\n        self.bundling = True\n        if self.n_proc > 1 and self.cylinder_rank == 0:\n            logger.warning(\"Creating an ExtensiveForm object in parallel. Why?\")\n        required = [\"solver\"]\n        self._options_check(required, self.options)\n        self.solver = pyo.SolverFactory(self.options[\"solver\"])\n        self.ef = sputils._create_EF_from_scen_dict(self.local_scenarios,\n                EF_name=model_name)\n\n    def solve_extensive_form(self, solver_options=None, tee=False):\n        \"\"\" Solve the extensive form.\n            \n            Args:\n                solver_options (dict, optional):\n                    Dictionary of solver-specific options (e.g. Gurobi options,\n                    CPLEX options, etc.).\n                tee (bool, optional):\n                    If True, displays solver output. Default False.\n\n            Returns:\n                :class:`pyomo.opt.results.results_.SolverResults`:\n                    Result returned by the Pyomo solve method.\n                \n        \"\"\"\n        if \"persistent\" in self.options[\"solver\"]:\n            self.solver.set_instance(self.ef)\n        # Pass solver-specifiec (e.g. Gurobi, CPLEX) options\n        if solver_options is not None:\n            for (opt, value) in solver_options.items():\n                self.solver.options[opt] = value\n        results = self.solver.solve(self.ef, tee=tee, load_solutions=False)\n        if len(results.solution) > 0:\n            if sputils.is_persistent(self.solver):\n                self.solver.load_vars()\n            else:\n                self.ef.solutions.load_from(results)\n            self.first_stage_solution_available = True\n            self.tree_solution_available = True\n        return results\n\n    def get_objective_value(self):\n        \"\"\" Retrieve the objective value.\n        \n        Returns:\n            float:\n                Objective value.\n\n        Raises:\n            ValueError:\n                If optimal objective value could not be retrieved.\n        \"\"\"\n        try:\n            obj_val = pyo.value(self.ef.EF_Obj)\n        except Exception as e:\n            raise ValueError(f\"Could not extract EF objective value with error: {str(e)}\")\n        return obj_val\n\n    def get_root_solution(self):\n        \"\"\" Get the value of the variables at the root node.\n\n        Returns:\n            dict:\n                Dictionary mapping variable name (str) to variable value\n                (float) for all variables at the root node.\n        \"\"\"\n        result = dict()\n        for var in self.ef.ref_vars.values():\n            var_name = var.name\n            dot_index = var_name.find(\".\")\n            if dot_index >= 0 and var_name[:dot_index] in self.all_scenario_names:\n                var_name = var_name[dot_index+1:]\n            result[var_name] = var.value\n        return result\n\n    def nonants(self):\n        \"\"\" An iterator to give representative Vars subject to non-anticipitivity\n        Args: None\n\n        Yields:\n            tree node name, full EF Var name, Var value\n        \"\"\"\n        yield from sputils.ef_nonants(self.ef)\n\n\n    def nonants_to_csv(self, filename):\n        \"\"\" Dump the nonant vars from an ef to a csv file; truly a dump...\n        Args:\n            filename (str): the full name of the csv output file\n        \"\"\"\n        sputils.ef_nonants_csv(self.ef, filename)\n\n\n    def scenarios(self):\n        \"\"\" An iterator to give the scenario sub-models in an ef\n        Args: None\n\n        Yields:\n            scenario name, scenario instance (str, ConcreteModel)\n        \"\"\"\n        yield from self.local_scenarios.items()",
  "def __init__(\n        self,\n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_creator_kwargs=None,\n        all_nodenames=None,\n        model_name=None,\n        suppress_warnings=False,\n    ):\n        \"\"\" Create the EF and associated solver. \"\"\"\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            all_nodenames=all_nodenames\n        )\n        self.bundling = True\n        if self.n_proc > 1 and self.cylinder_rank == 0:\n            logger.warning(\"Creating an ExtensiveForm object in parallel. Why?\")\n        required = [\"solver\"]\n        self._options_check(required, self.options)\n        self.solver = pyo.SolverFactory(self.options[\"solver\"])\n        self.ef = sputils._create_EF_from_scen_dict(self.local_scenarios,\n                EF_name=model_name)",
  "def solve_extensive_form(self, solver_options=None, tee=False):\n        \"\"\" Solve the extensive form.\n            \n            Args:\n                solver_options (dict, optional):\n                    Dictionary of solver-specific options (e.g. Gurobi options,\n                    CPLEX options, etc.).\n                tee (bool, optional):\n                    If True, displays solver output. Default False.\n\n            Returns:\n                :class:`pyomo.opt.results.results_.SolverResults`:\n                    Result returned by the Pyomo solve method.\n                \n        \"\"\"\n        if \"persistent\" in self.options[\"solver\"]:\n            self.solver.set_instance(self.ef)\n        # Pass solver-specifiec (e.g. Gurobi, CPLEX) options\n        if solver_options is not None:\n            for (opt, value) in solver_options.items():\n                self.solver.options[opt] = value\n        results = self.solver.solve(self.ef, tee=tee, load_solutions=False)\n        if len(results.solution) > 0:\n            if sputils.is_persistent(self.solver):\n                self.solver.load_vars()\n            else:\n                self.ef.solutions.load_from(results)\n            self.first_stage_solution_available = True\n            self.tree_solution_available = True\n        return results",
  "def get_objective_value(self):\n        \"\"\" Retrieve the objective value.\n        \n        Returns:\n            float:\n                Objective value.\n\n        Raises:\n            ValueError:\n                If optimal objective value could not be retrieved.\n        \"\"\"\n        try:\n            obj_val = pyo.value(self.ef.EF_Obj)\n        except Exception as e:\n            raise ValueError(f\"Could not extract EF objective value with error: {str(e)}\")\n        return obj_val",
  "def get_root_solution(self):\n        \"\"\" Get the value of the variables at the root node.\n\n        Returns:\n            dict:\n                Dictionary mapping variable name (str) to variable value\n                (float) for all variables at the root node.\n        \"\"\"\n        result = dict()\n        for var in self.ef.ref_vars.values():\n            var_name = var.name\n            dot_index = var_name.find(\".\")\n            if dot_index >= 0 and var_name[:dot_index] in self.all_scenario_names:\n                var_name = var_name[dot_index+1:]\n            result[var_name] = var.value\n        return result",
  "def nonants(self):\n        \"\"\" An iterator to give representative Vars subject to non-anticipitivity\n        Args: None\n\n        Yields:\n            tree node name, full EF Var name, Var value\n        \"\"\"\n        yield from sputils.ef_nonants(self.ef)",
  "def nonants_to_csv(self, filename):\n        \"\"\" Dump the nonant vars from an ef to a csv file; truly a dump...\n        Args:\n            filename (str): the full name of the csv output file\n        \"\"\"\n        sputils.ef_nonants_csv(self.ef, filename)",
  "def scenarios(self):\n        \"\"\" An iterator to give the scenario sub-models in an ef\n        Args: None\n\n        Yields:\n            scenario name, scenario instance (str, ConcreteModel)\n        \"\"\"\n        yield from self.local_scenarios.items()",
  "class LShapedMethod(spbase.SPBase):\n    \"\"\" Base class for the L-shaped method for two-stage stochastic programs.\n\n    Warning:\n        This class explicitly assumes minimization.\n\n    Args:\n        options (dict):\n            Dictionary of options. Possible (optional) options include\n\n            - root_scenarios (list) - List of scenario names to include as\n              part of the root problem (default [])\n            - store_subproblems (boolean) - If True, the BendersDecomp object\n              will maintain a dictionary containing the subproblems created by\n              the BendersCutGenerator.\n            - relax_root (boolean) - If True, the LP relaxation of the root\n              problem is solved (i.e. integer variables in the root problem\n              are relaxed).\n            - scenario_creator_kwargs (dict) - Keyword args to pass to the scenario_creator.\n            - valid_eta_lb (dict) - Dictionary mapping scenario names to valid\n              lower bounds for the eta variables--i.e., a valid lower (outer)\n              bound on the optimal objective value for each scenario. If none\n              are provided, the lower bound is set to -sys.maxsize *\n              scenario_prob, which may cause numerical errors.\n            - indx_to_stage (dict) - Dictionary mapping the index of every\n              variable in the model to the stage they belong to.\n        all_scenario_names (list):\n            List of all scenarios names present in the model (strings).\n        scenario_creator (callable): \n            Function which take a scenario name (string) and returns a\n            Pyomo Concrete model with some things attached.\n        scenario_denouement (callable, optional):\n            Function which does post-processing and reporting.\n        all_nodenames (list, optional): \n            List of all node name (strings). Can be `None` for two-stage\n            problems.\n        mpicomm (MPI comm, optional):\n            MPI communicator to use between all scenarios. Default is\n            `MPI.COMM_WORLD`.\n        scenario_creator_kwargs (dict, optional): \n            Keyword arguments to pass to `scenario_creator`.\n    \"\"\"\n    def __init__(\n        self, \n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,\n        mpicomm=None,\n        scenario_creator_kwargs=None,\n    ):\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n        )\n        if self.multistage:\n            raise Exception(\"LShaped does not currently support multiple stages\")\n        self.options = options\n        self.options_check()\n        self.all_scenario_names = all_scenario_names\n        self.root = None\n        self.root_vars = None\n        self.scenario_count = len(all_scenario_names)\n\n        self.store_subproblems = False\n        if \"store_subproblems\" in options:\n            self.store_subproblems = options[\"store_subproblems\"]\n\n        self.root_scenarios = None\n        if \"root_scenarios\" in options:\n            self.root_scenarios = options[\"root_scenarios\"]\n\n        self.relax_root = False \n        if \"relax_root\" in options:\n            self.relax_root = options[\"relax_root\"]\n\n        self.valid_eta_lb = None\n        if \"valid_eta_lb\" in options:\n            self.valid_eta_lb = options[\"valid_eta_lb\"]\n            self.compute_eta_bound = False\n        else: # fit the user does not provide a bound, compute one\n            self.valid_eta_lb = { scen :  (-sys.maxsize - 1) * 1. / len(self.all_scenario_names) \\\n                                    for scen in self.all_scenario_names }\n            self.compute_eta_bound = True\n\n        if scenario_creator_kwargs is None:\n            self.scenario_creator_kwargs = dict()\n        else:\n            self.scenario_creator_kwargs = scenario_creator_kwargs\n        self.indx_to_stage = None\n        self.has_valid_eta_lb = self.valid_eta_lb is not None\n        self.has_root_scens = self.root_scenarios is not None\n\n        if self.store_subproblems:\n            self.subproblems = dict.fromkeys(scenario_names)\n\n    def options_check(self):\n        \"\"\" Check to ensure that the user-specified options are valid. Requried\n        options are:\n\n        - root_solver (string) - Solver to use for the root problem.\n        - sp_solver (string) - Solver to use for the subproblems.\n        \"\"\"\n        required = [\"root_solver\", \"sp_solver\"]\n        if \"root_solver_options\" not in self.options:\n            self.options[\"root_solver_options\"] = dict()\n        if \"sp_solver_options\" not in self.options:\n            self.options[\"sp_solver_options\"] = dict()\n        self._options_check(required, self.options)\n\n    def _add_root_etas(self, root, index):\n        def _eta_bounds(m, s):\n            return (self.valid_eta_lb[s],None)\n        root.eta = pyo.Var(index, within=pyo.Reals, bounds=_eta_bounds)\n\n    def _create_root_no_scenarios(self):\n\n        # using the first scenario as a basis\n        root = self.scenario_creator(\n            self.all_scenario_names[0], **self.scenario_creator_kwargs\n        )\n\n        if self.relax_root:\n            RelaxIntegerVars().apply_to(root)\n\n        nonant_list, nonant_ids = _get_nonant_ids(root)\n\n        self.root_vars = nonant_list\n\n        for constr_data in list(itertools.chain(\n                root.component_data_objects(SOSConstraint, active=True, descend_into=True)\n                , root.component_data_objects(Constraint, active=True, descend_into=True))):\n            if not _first_stage_only(constr_data, nonant_ids):\n                _del_con(constr_data)\n\n        # delete the second stage variables\n        for var in list(root.component_data_objects(Var, active=True, descend_into=True)):\n            if id(var) not in nonant_ids:\n                _del_var(var)\n\n        self._add_root_etas(root, self.all_scenario_names)\n\n        # pulls the current objective expression, adds in the eta variables,\n        # and removes the second stage variables from the expression\n        obj = find_active_objective(root)\n\n        repn = generate_standard_repn(obj.expr, quadratic=True)\n        if len(repn.nonlinear_vars) > 0:\n            raise ValueError(\"LShaped does not support models with nonlinear objective functions\")\n\n        linear_vars = list()\n        linear_coefs = list()\n        quadratic_vars = list()\n        quadratic_coefs = list()\n        ## we'll assume the constant is part of stage 1 (wlog it is), just\n        ## like the first-stage bits of the objective\n        constant = repn.constant \n\n        ## only keep the first stage variables in the objective\n        for coef, var in zip(repn.linear_coefs, repn.linear_vars):\n            id_var = id(var)\n            if id_var in nonant_ids:\n                linear_vars.append(var)\n                linear_coefs.append(coef)\n        for coef, (x,y) in zip(repn.quadratic_coefs, repn.quadratic_vars):\n            id_x = id(x)\n            id_y = id(y)\n            if id_x in nonant_ids and id_y in nonant_ids:\n                quadratic_coefs.append(coef)\n                quadratic_vars.append((x,y))\n\n        # checks if model sense is max, if so negates the objective\n        if not self.is_minimizing:\n            for i,coef in enumerate(linear_coefs):\n                linear_coefs[i] = -coef\n            for i,coef in enumerate(quadratic_coefs):\n                quadratic_coefs[i] = -coef\n\n        # add the etas\n        for var in root.eta.values():\n            linear_vars.append(var)\n            linear_coefs.append(1)\n\n        expr = LinearExpression(constant=constant, linear_coefs=linear_coefs,\n                                linear_vars=linear_vars)\n        if quadratic_coefs:\n            expr += pyo.quicksum(\n                        (coef*x*y for coef,(x,y) in zip(quadratic_coefs, quadratic_vars))\n                    )\n\n        root.del_component(obj)\n\n        # set root objective function\n        root.obj = pyo.Objective(expr=expr, sense=pyo.minimize)\n\n        self.root = root\n\n    def _create_root_with_scenarios(self):\n\n        ef_scenarios = self.root_scenarios\n\n        ## we want the correct probabilities to be set when\n        ## calling create_EF\n        if len(ef_scenarios) > 1:\n            def scenario_creator_wrapper(name, **creator_options):\n                scenario = self.scenario_creator(name, **creator_options)\n                if not hasattr(scenario, '_mpisppy_probability'):\n                    scenario._mpisppy_probability = 1./len(self.all_scenario_names)\n                return scenario\n            root = sputils.create_EF(\n                ef_scenarios,\n                scenario_creator_wrapper,\n                scenario_creator_kwargs=self.scenario_creator_kwargs,\n            )\n\n            nonant_list, nonant_ids = _get_nonant_ids_EF(root)\n        else:\n            root = self.scenario_creator(\n                ef_scenarios[0],\n                **self.scenario_creator_kwargs,\n            )\n            if not hasattr(root, '_mpisppy_probability'):\n                root._mpisppy_probability = 1./len(self.all_scenario_names)\n\n            nonant_list, nonant_ids = _get_nonant_ids(root)\n\n        self.root_vars = nonant_list\n\n        # creates the eta variables for scenarios that are NOT selected to be\n        # included in the root problem\n        eta_indx = [scenario_name for scenario_name in self.all_scenario_names\n                        if scenario_name not in self.root_scenarios]\n        self._add_root_etas(root, eta_indx)\n\n        obj = find_active_objective(root)\n\n        repn = generate_standard_repn(obj.expr, quadratic=True)\n        if len(repn.nonlinear_vars) > 0:\n            raise ValueError(\"LShaped does not support models with nonlinear objective functions\")\n        linear_vars = list(repn.linear_vars)\n        linear_coefs = list(repn.linear_coefs)\n        quadratic_coefs = list(repn.quadratic_coefs)\n\n        # adjust coefficients by scenario/bundle probability\n        scen_prob = root._mpisppy_probability\n        for i,var in enumerate(repn.linear_vars):\n            if id(var) not in nonant_ids:\n                linear_coefs[i] *= scen_prob\n\n        for i,(x,y) in enumerate(repn.quadratic_vars):\n            # only multiply through once\n            if id(x) not in nonant_ids:\n                quadratic_coefs[i] *= scen_prob\n            elif id(y) not in nonant_ids:\n                quadratic_coefs[i] *= scen_prob\n\n        # NOTE: the LShaped code negates the objective, so\n        #       we do the same here for consistency\n        if not self.is_minimizing:\n            for i,coef in enumerate(linear_coefs):\n                linear_coefs[i] = -coef\n            for i,coef in enumerate(quadratic_coefs):\n                quadratic_coefs[i] = -coef\n\n        # add the etas\n        for var in root.eta.values():\n            linear_vars.append(var)\n            linear_coefs.append(1)\n\n        expr = LinearExpression(constant=repn.constant, linear_coefs=linear_coefs,\n                                linear_vars=linear_vars)\n        if repn.quadratic_vars:\n            expr += pyo.quicksum(\n                (coef*x*y for coef,(x,y) in zip(quadratic_coefs, repn.quadratic_vars))\n            )\n\n        root.del_component(obj)\n\n        # set root objective function\n        root.obj = pyo.Objective(expr=expr, sense=pyo.minimize)\n\n        self.root = root\n\n    def _create_shadow_root(self):\n\n        root = pyo.ConcreteModel()\n\n        arb_scen = self.local_scenarios[self.local_scenario_names[0]]\n        nonants = arb_scen._mpisppy_node_list[0].nonant_vardata_list\n\n        root_vars = list()\n        for v in nonants:\n            nonant_shadow = pyo.Var(name=v.name)\n            root.add_component(v.name, nonant_shadow)\n            root_vars.append(nonant_shadow)\n        \n        if self.has_root_scens:\n            eta_indx = [scenario_name for scenario_name in self.all_scenario_names\n                            if scenario_name not in self.root_scenarios]\n        else:\n            eta_indx = self.all_scenario_names\n        self._add_root_etas(root, eta_indx)\n\n        root.obj = None\n        self.root = root\n        self.root_vars = root_vars\n\n    def set_eta_bounds(self):\n        if self.compute_eta_bound:\n            ## for scenarios not in self.local_scenarios, these will be a large negative number\n            this_etas_lb = np.fromiter((self.valid_eta_lb[scen] for scen in self.all_scenario_names),\n                                    float, count=len(self.all_scenario_names))\n\n            all_etas_lb = np.empty_like(this_etas_lb)\n\n            self.mpicomm.Allreduce(this_etas_lb, all_etas_lb, op=MPI.MAX)\n\n            for idx, s in enumerate(self.all_scenario_names):\n                self.valid_eta_lb[s] = all_etas_lb[idx]\n            \n            # root may not have etas for every scenarios\n            for s, v in self.root.eta.items():\n                v.setlb(self.valid_eta_lb[s])\n\n    def create_root(self):\n        \"\"\" creates a ConcreteModel from one of the problem scenarios then\n            modifies the model to serve as the root problem \n        \"\"\"\n        if self.cylinder_rank == 0:\n            if self.has_root_scens:\n                self._create_root_with_scenarios()\n            else:\n                self._create_root_no_scenarios()\n        else: \n            ## if we're not rank0, just create a root to\n            ## hold the nonants and etas; rank0 will do \n            ## the optimizing\n            self._create_shadow_root()\n        \n    def attach_nonant_var_map(self, scenario_name):\n        instance = self.local_scenarios[scenario_name]\n\n        subproblem_to_root_vars_map = pyo.ComponentMap()\n        for var, rvar in zip(instance._mpisppy_data.nonant_indices.values(), self.root_vars):\n            if var.name not in rvar.name:\n                raise Exception(\"Error: Complicating variable mismatch, sub-problem variables changed order\")\n            subproblem_to_root_vars_map[var] = rvar \n\n        # this is for interefacing with PH code\n        instance._mpisppy_model.subproblem_to_root_vars_map = subproblem_to_root_vars_map\n\n    def create_subproblem(self, scenario_name):\n        \"\"\" the subproblem creation function passed into the\n            BendersCutsGenerator \n        \"\"\"\n        instance = self.local_scenarios[scenario_name]\n\n        nonant_list, nonant_ids = _get_nonant_ids(instance) \n\n        # NOTE: since we use generate_standard_repn below, we need\n        #       to unfix any nonants so they'll properly appear\n        #       in the objective\n        fixed_nonants = [ var for var in nonant_list if var.fixed ]\n        for var in fixed_nonants:\n            var.fixed = False\n\n        # pulls the scenario objective expression, removes the first stage variables, and sets the new objective\n        obj = find_active_objective(instance)\n\n        if not hasattr(instance, \"_mpisppy_probability\"):\n            instance._mpisppy_probability = 1. / self.scenario_count\n        _mpisppy_probability = instance._mpisppy_probability\n\n        repn = generate_standard_repn(obj.expr, quadratic=True)\n        if len(repn.nonlinear_vars) > 0:\n            raise ValueError(\"LShaped does not support models with nonlinear objective functions\")\n\n        linear_vars = list()\n        linear_coefs = list()\n        quadratic_vars = list()\n        quadratic_coefs = list()\n        ## we'll assume the constant is part of stage 1 (wlog it is), just\n        ## like the first-stage bits of the objective\n        constant = repn.constant \n\n        ## only keep the second stage variables in the objective\n        for coef, var in zip(repn.linear_coefs, repn.linear_vars):\n            id_var = id(var)\n            if id_var not in nonant_ids:\n                linear_vars.append(var)\n                linear_coefs.append(_mpisppy_probability*coef)\n        for coef, (x,y) in zip(repn.quadratic_coefs, repn.quadratic_vars):\n            id_x = id(x)\n            id_y = id(y)\n            if id_x not in nonant_ids or id_y not in nonant_ids:\n                quadratic_coefs.append(_mpisppy_probability*coef)\n                quadratic_vars.append((x,y))\n\n        # checks if model sense is max, if so negates the objective\n        if not self.is_minimizing:\n            for i,coef in enumerate(linear_coefs):\n                linear_coefs[i] = -coef\n            for i,coef in enumerate(quadratic_coefs):\n                quadratic_coefs[i] = -coef\n\n        expr = LinearExpression(constant=constant, linear_coefs=linear_coefs,\n                                linear_vars=linear_vars)\n        if quadratic_coefs:\n            expr += pyo.quicksum(\n                        (coef*x*y for coef,(x,y) in zip(quadratic_coefs, quadratic_vars))\n                    )\n\n        instance.del_component(obj)\n\n        # set subproblem objective function\n        instance.obj = pyo.Objective(expr=expr, sense=pyo.minimize)\n\n        ## need to do this here for validity if computing the eta bound\n        if self.relax_root:\n            # relaxes any integrality constraints for the subproblem\n            RelaxIntegerVars().apply_to(instance)\n\n        if self.compute_eta_bound:\n            for var in fixed_nonants:\n                var.fixed = True\n            opt = pyo.SolverFactory(self.options[\"sp_solver\"])\n            if self.options[\"sp_solver_options\"]:\n                for k,v in self.options[\"sp_solver_options\"].items():\n                    opt.options[k] = v\n\n            if sputils.is_persistent(opt):\n                set_instance_retry(instance, opt, scenario_name)\n                res = opt.solve(tee=False)\n            else:\n                res = opt.solve(instance, tee=False)\n\n            eta_lb = res.Problem[0].Lower_bound\n\n            self.valid_eta_lb[scenario_name] = eta_lb\n\n        # if not done above\n        if not self.relax_root:\n            # relaxes any integrality constraints for the subproblem\n            RelaxIntegerVars().apply_to(instance)\n\n        # iterates through constraints and removes first stage constraints from the model\n        # the id dict is used to improve the speed of identifying the stage each variables belongs to\n        for constr_data in list(itertools.chain(\n                instance.component_data_objects(SOSConstraint, active=True, descend_into=True)\n                , instance.component_data_objects(Constraint, active=True, descend_into=True))):\n            if _first_stage_only(constr_data, nonant_ids):\n                _del_con(constr_data)\n\n        # creates the sub map to remove first stage variables from objective expression\n        complicating_vars_map = pyo.ComponentMap()\n        subproblem_to_root_vars_map = pyo.ComponentMap()\n\n        # creates the complicating var map that connects the first stage variables in the sub problem to those in\n        # the root problem -- also set the bounds on the subproblem root vars to be none for better cuts\n        for var, rvar in zip(nonant_list, self.root_vars):\n            if var.name not in rvar.name: # rvar.name may be part of a bundle\n                raise Exception(\"Error: Complicating variable mismatch, sub-problem variables changed order\")\n            complicating_vars_map[rvar] = var\n            subproblem_to_root_vars_map[var] = rvar \n\n            # these are already enforced in the root\n            # don't need to be enfored in the subproblems\n            var.setlb(None)\n            var.setub(None)\n            var.fixed = False\n\n        # this is for interefacing with PH code\n        instance._mpisppy_model.subproblem_to_root_vars_map = subproblem_to_root_vars_map\n\n        if self.store_subproblems:\n            self.subproblems[scenario_name] = instance\n\n        return instance, complicating_vars_map\n\n    def lshaped_algorithm(self, converger=None):\n        \"\"\" function that runs the lshaped.py algorithm\n        \"\"\"\n        if converger:\n            converger = converger(self, self.cylinder_rank, self.n_proc)\n        max_iter = 30\n        if \"max_iter\" in self.options:\n            max_iter = self.options[\"max_iter\"]\n        tol = 1e-8\n        if \"tol\" in self.options:\n            tol = self.options[\"tol\"]\n        verbose = True\n        if \"verbose\" in self.options:\n            verbose = self.options[\"verbose\"]\n        root_solver = self.options[\"root_solver\"]\n        sp_solver = self.options[\"sp_solver\"]\n\n        # creates the root problem\n        self.create_root()\n        m = self.root\n        assert hasattr(m, \"obj\")\n\n        # prevents problems from first stage variables becoming unconstrained\n        # after processing\n        _init_vars(self.root_vars)\n\n        # sets up the BendersCutGenerator object\n        m.bender = LShapedCutGenerator()\n\n        m.bender.set_input(root_vars=self.root_vars, tol=tol, comm=self.mpicomm)\n\n        # let the cut generator know who's using it, probably should check that this is called after set input\n        m.bender.set_ls(self)\n\n        # set the eta variables, removing this from the add_suproblem function so we can\n\n        # Pass all the scenarios in the problem to bender.add_subproblem\n        # and let it internally handle which ranks get which scenarios\n        if self.has_root_scens:\n            sub_scenarios = [\n                scenario_name for scenario_name in self.local_scenario_names\n                if scenario_name not in self.root_scenarios\n            ]\n        else:\n            sub_scenarios = self.local_scenario_names\n        for scenario_name in self.local_scenario_names:\n            if scenario_name in sub_scenarios:\n                subproblem_fn_kwargs = dict()\n                subproblem_fn_kwargs['scenario_name'] = scenario_name\n                m.bender.add_subproblem(\n                    subproblem_fn=self.create_subproblem,\n                    subproblem_fn_kwargs=subproblem_fn_kwargs,\n                    root_eta=m.eta[scenario_name],\n                    subproblem_solver=sp_solver,\n                    subproblem_solver_options=self.options[\"sp_solver_options\"]\n                )\n            else:\n                self.attach_nonant_var_map(scenario_name)\n\n        # set the eta bounds if computed\n        # by self.create_subproblem\n        self.set_eta_bounds()\n\n        if self.cylinder_rank == 0:\n            opt = pyo.SolverFactory(root_solver)\n            if opt is None:\n                raise Exception(\"Error: Failed to Create Master Solver\")\n\n            # set options\n            for k,v in self.options[\"root_solver_options\"].items():\n                opt.options[k] = v\n\n            is_persistent = sputils.is_persistent(opt)\n            if is_persistent:\n                set_instance_retry(m, opt, \"root\")\n\n        t = time.time()\n        res, t1, t2 = None, None, None\n\n        # benders solve loop, repeats the benders root - subproblem\n        # loop until either a no more cuts can are generated\n        # or the maximum iterations limit is reached\n        for self.iter in range(max_iter):\n            if verbose and self.cylinder_rank == 0:\n                if self.iter > 0:\n                    print(\"Current Iteration:\", self.iter + 1, \"Time Elapsed:\", \"%7.2f\" % (time.time() - t), \"Time Spent on Last Master:\", \"%7.2f\" % t1,\n                          \"Time Spent Generating Last Cut Set:\", \"%7.2f\" % t2, \"Current Objective:\", \"%7.2f\" % m.obj.expr())\n                else:\n                    print(\"Current Iteration:\", self.iter + 1, \"Time Elapsed:\", \"%7.2f\" % (time.time() - t), \"Current Objective: -Inf\")\n            t1 = time.time()\n            x_vals = np.zeros(len(self.root_vars))\n            eta_vals = np.zeros(self.scenario_count)\n            outer_bound = np.zeros(1)\n            if self.cylinder_rank == 0:\n                if is_persistent:\n                    res = opt.solve(tee=False)\n                else:\n                    res = opt.solve(m, tee=False)\n                # LShaped is always minimizing\n                outer_bound[0] = res.Problem[0].Lower_bound\n                for i, var in enumerate(self.root_vars):\n                    x_vals[i] = var.value\n                for i, eta in enumerate(m.eta.values()):\n                    eta_vals[i] = eta.value\n\n            self.mpicomm.Bcast(x_vals, root=0)\n            self.mpicomm.Bcast(eta_vals, root=0)\n            self.mpicomm.Bcast(outer_bound, root=0)\n\n            if self.is_minimizing:\n                self._LShaped_bound = outer_bound[0]\n            else:\n                # LShaped is always minimizing, so negate\n                # the outer bound for sharing broadly\n                self._LShaped_bound = -outer_bound[0]\n\n            if self.cylinder_rank != 0:\n                for i, var in enumerate(self.root_vars):\n                    var._value = x_vals[i]\n                for i, eta in enumerate(m.eta.values()):\n                    eta._value = eta_vals[i]\n            t1 = time.time() - t1\n\n            # The hub object takes precedence over the converger\n            # We'll send the nonants now, and check for a for\n            # convergence\n            if self.spcomm:\n                self.spcomm.sync(send_nonants=True)\n                if self.spcomm.is_converged():\n                    break\n\n            t2 = time.time()\n            cuts_added = m.bender.generate_cut()\n            t2 = time.time() - t2\n            if self.cylinder_rank == 0:\n                for c in cuts_added:\n                    if is_persistent:\n                        opt.add_constraint(c)\n                if verbose and len(cuts_added) == 0:\n                    print(\n                        f\"Converged in {self.iter+1} iterations.\\n\"\n                        f\"Total Time Elapsed: {time.time()-t:7.2f} \"\n                        f\"Time Spent on Last Master: {t1:7.2f} \"\n                        f\"Time spent verifying second stage: {t2:7.2f} \"\n                        f\"Final Objective: {m.obj.expr():7.2f}\"\n                    )\n                    self.first_stage_solution_available = True\n                    self.tree_solution_available = True\n                    break\n                if verbose and self.iter == max_iter - 1:\n                    print(\"WARNING MAX ITERATION LIMIT REACHED !!! \")\n            else:\n                if len(cuts_added) == 0:\n                    break\n            # The hub object takes precedence over the converger\n            if self.spcomm:\n                self.spcomm.sync(send_nonants=False)\n                if self.spcomm.is_converged():\n                    break\n            if converger:\n                converger.convergence_value()\n                if converger.is_converged():\n                    if verbose and self.cylinder_rank == 0:\n                        print(\n                            f\"Converged to user criteria in {self.iter+1} iterations.\\n\"\n                            f\"Total Time Elapsed: {time.time()-t:7.2f} \"\n                            f\"Time Spent on Last Master: {t1:7.2f} \"\n                            f\"Time spent verifying second stage: {t2:7.2f} \"\n                            f\"Final Objective: {m.obj.expr():7.2f}\"\n                        )\n                    break\n        return res",
  "def _del_con(c):\n    parent = c.parent_component()\n    if parent.is_indexed():\n        parent.__delitem__(c.index())\n    else:\n        assert parent is c\n        c.parent_block().del_component(c)",
  "def _del_var(v):\n    parent = v.parent_component()\n    if parent.is_indexed():\n        parent.__delitem__(v.index())\n    else:\n        assert parent is v\n        block = v.parent_block()\n        block.del_component(v)",
  "def _get_nonant_ids(instance):\n    assert len(instance._mpisppy_node_list) == 1\n    # set comprehension\n    nonant_list = instance._mpisppy_node_list[0].nonant_vardata_list\n    return nonant_list, { id(var) for var in nonant_list }",
  "def _get_nonant_ids_EF(instance):\n    assert len(instance._mpisppy_data.nlens) == 1\n\n    ndn, nlen = list(instance._mpisppy_data.nlens.items())[0]\n\n    ## this is for the cut variables, so we just need (and want)\n    ## exactly one set of them\n    nonant_list = list(instance.ref_vars[ndn,i] for i in range(nlen))\n\n    ## this is for adjusting the objective, so needs all the nonants\n    ## in the EF\n    snames = instance._ef_scenario_names\n\n    nonant_ids = set()\n    for s in snames:\n        nonant_ids.update( (id(v) for v in \\\n                getattr(instance, s)._mpisppy_node_list[0].nonant_vardata_list)\n                )\n    return nonant_list, nonant_ids",
  "def _first_stage_only(constr_data, nonant_ids):\n    \"\"\" iterates through the constraint in a scenario and returns if it only\n        has first stage variables\n    \"\"\"\n    for var in identify_variables(constr_data.body):\n        if id(var) not in nonant_ids: \n            return False\n    return True",
  "def _init_vars(varlist):\n    '''\n    for every pyomo var in varlist without a value,\n    sets it to the lower bound (if it exists), or\n    the upper bound (if it exists, and the lower bound\n    does note) or 0 (if neither bound exists).\n    '''\n    value = pyo.value\n    for var in varlist:\n        if var.value is not None:\n            continue\n        if var.lb is not None:\n            var.set_value(value(var.lb))\n        elif var.ub is not None:\n            var.set_value(value(var.ub))\n        else:\n            var.set_value(0)",
  "def main():\n    import mpisppy.tests.examples.farmer as ref\n    import os\n    # Turn off output from all ranks except rank 1\n    if MPI.COMM_WORLD.Get_rank() != 0:\n        sys.stdout = open(os.devnull, 'w')\n    scenario_names = ['scen' + str(i) for i in range(3)]\n    bounds = {i:-432000 for i in scenario_names}\n    options = {\n        \"root_solver\": \"gurobi_persistent\",\n        \"sp_solver\": \"gurobi_persistent\",\n        \"sp_solver_options\" : {\"threads\" : 1},\n        \"valid_eta_lb\": bounds,\n        \"max_iter\": 10,\n   }\n\n    ls = LShapedMethod(options, scenario_names, ref.scenario_creator)\n    res = ls.lshaped_algorithm()\n    if ls.cylinder_rank == 0:\n        print(res)",
  "def __init__(\n        self, \n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,\n        mpicomm=None,\n        scenario_creator_kwargs=None,\n    ):\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n        )\n        if self.multistage:\n            raise Exception(\"LShaped does not currently support multiple stages\")\n        self.options = options\n        self.options_check()\n        self.all_scenario_names = all_scenario_names\n        self.root = None\n        self.root_vars = None\n        self.scenario_count = len(all_scenario_names)\n\n        self.store_subproblems = False\n        if \"store_subproblems\" in options:\n            self.store_subproblems = options[\"store_subproblems\"]\n\n        self.root_scenarios = None\n        if \"root_scenarios\" in options:\n            self.root_scenarios = options[\"root_scenarios\"]\n\n        self.relax_root = False \n        if \"relax_root\" in options:\n            self.relax_root = options[\"relax_root\"]\n\n        self.valid_eta_lb = None\n        if \"valid_eta_lb\" in options:\n            self.valid_eta_lb = options[\"valid_eta_lb\"]\n            self.compute_eta_bound = False\n        else: # fit the user does not provide a bound, compute one\n            self.valid_eta_lb = { scen :  (-sys.maxsize - 1) * 1. / len(self.all_scenario_names) \\\n                                    for scen in self.all_scenario_names }\n            self.compute_eta_bound = True\n\n        if scenario_creator_kwargs is None:\n            self.scenario_creator_kwargs = dict()\n        else:\n            self.scenario_creator_kwargs = scenario_creator_kwargs\n        self.indx_to_stage = None\n        self.has_valid_eta_lb = self.valid_eta_lb is not None\n        self.has_root_scens = self.root_scenarios is not None\n\n        if self.store_subproblems:\n            self.subproblems = dict.fromkeys(scenario_names)",
  "def options_check(self):\n        \"\"\" Check to ensure that the user-specified options are valid. Requried\n        options are:\n\n        - root_solver (string) - Solver to use for the root problem.\n        - sp_solver (string) - Solver to use for the subproblems.\n        \"\"\"\n        required = [\"root_solver\", \"sp_solver\"]\n        if \"root_solver_options\" not in self.options:\n            self.options[\"root_solver_options\"] = dict()\n        if \"sp_solver_options\" not in self.options:\n            self.options[\"sp_solver_options\"] = dict()\n        self._options_check(required, self.options)",
  "def _add_root_etas(self, root, index):\n        def _eta_bounds(m, s):\n            return (self.valid_eta_lb[s],None)\n        root.eta = pyo.Var(index, within=pyo.Reals, bounds=_eta_bounds)",
  "def _create_root_no_scenarios(self):\n\n        # using the first scenario as a basis\n        root = self.scenario_creator(\n            self.all_scenario_names[0], **self.scenario_creator_kwargs\n        )\n\n        if self.relax_root:\n            RelaxIntegerVars().apply_to(root)\n\n        nonant_list, nonant_ids = _get_nonant_ids(root)\n\n        self.root_vars = nonant_list\n\n        for constr_data in list(itertools.chain(\n                root.component_data_objects(SOSConstraint, active=True, descend_into=True)\n                , root.component_data_objects(Constraint, active=True, descend_into=True))):\n            if not _first_stage_only(constr_data, nonant_ids):\n                _del_con(constr_data)\n\n        # delete the second stage variables\n        for var in list(root.component_data_objects(Var, active=True, descend_into=True)):\n            if id(var) not in nonant_ids:\n                _del_var(var)\n\n        self._add_root_etas(root, self.all_scenario_names)\n\n        # pulls the current objective expression, adds in the eta variables,\n        # and removes the second stage variables from the expression\n        obj = find_active_objective(root)\n\n        repn = generate_standard_repn(obj.expr, quadratic=True)\n        if len(repn.nonlinear_vars) > 0:\n            raise ValueError(\"LShaped does not support models with nonlinear objective functions\")\n\n        linear_vars = list()\n        linear_coefs = list()\n        quadratic_vars = list()\n        quadratic_coefs = list()\n        ## we'll assume the constant is part of stage 1 (wlog it is), just\n        ## like the first-stage bits of the objective\n        constant = repn.constant \n\n        ## only keep the first stage variables in the objective\n        for coef, var in zip(repn.linear_coefs, repn.linear_vars):\n            id_var = id(var)\n            if id_var in nonant_ids:\n                linear_vars.append(var)\n                linear_coefs.append(coef)\n        for coef, (x,y) in zip(repn.quadratic_coefs, repn.quadratic_vars):\n            id_x = id(x)\n            id_y = id(y)\n            if id_x in nonant_ids and id_y in nonant_ids:\n                quadratic_coefs.append(coef)\n                quadratic_vars.append((x,y))\n\n        # checks if model sense is max, if so negates the objective\n        if not self.is_minimizing:\n            for i,coef in enumerate(linear_coefs):\n                linear_coefs[i] = -coef\n            for i,coef in enumerate(quadratic_coefs):\n                quadratic_coefs[i] = -coef\n\n        # add the etas\n        for var in root.eta.values():\n            linear_vars.append(var)\n            linear_coefs.append(1)\n\n        expr = LinearExpression(constant=constant, linear_coefs=linear_coefs,\n                                linear_vars=linear_vars)\n        if quadratic_coefs:\n            expr += pyo.quicksum(\n                        (coef*x*y for coef,(x,y) in zip(quadratic_coefs, quadratic_vars))\n                    )\n\n        root.del_component(obj)\n\n        # set root objective function\n        root.obj = pyo.Objective(expr=expr, sense=pyo.minimize)\n\n        self.root = root",
  "def _create_root_with_scenarios(self):\n\n        ef_scenarios = self.root_scenarios\n\n        ## we want the correct probabilities to be set when\n        ## calling create_EF\n        if len(ef_scenarios) > 1:\n            def scenario_creator_wrapper(name, **creator_options):\n                scenario = self.scenario_creator(name, **creator_options)\n                if not hasattr(scenario, '_mpisppy_probability'):\n                    scenario._mpisppy_probability = 1./len(self.all_scenario_names)\n                return scenario\n            root = sputils.create_EF(\n                ef_scenarios,\n                scenario_creator_wrapper,\n                scenario_creator_kwargs=self.scenario_creator_kwargs,\n            )\n\n            nonant_list, nonant_ids = _get_nonant_ids_EF(root)\n        else:\n            root = self.scenario_creator(\n                ef_scenarios[0],\n                **self.scenario_creator_kwargs,\n            )\n            if not hasattr(root, '_mpisppy_probability'):\n                root._mpisppy_probability = 1./len(self.all_scenario_names)\n\n            nonant_list, nonant_ids = _get_nonant_ids(root)\n\n        self.root_vars = nonant_list\n\n        # creates the eta variables for scenarios that are NOT selected to be\n        # included in the root problem\n        eta_indx = [scenario_name for scenario_name in self.all_scenario_names\n                        if scenario_name not in self.root_scenarios]\n        self._add_root_etas(root, eta_indx)\n\n        obj = find_active_objective(root)\n\n        repn = generate_standard_repn(obj.expr, quadratic=True)\n        if len(repn.nonlinear_vars) > 0:\n            raise ValueError(\"LShaped does not support models with nonlinear objective functions\")\n        linear_vars = list(repn.linear_vars)\n        linear_coefs = list(repn.linear_coefs)\n        quadratic_coefs = list(repn.quadratic_coefs)\n\n        # adjust coefficients by scenario/bundle probability\n        scen_prob = root._mpisppy_probability\n        for i,var in enumerate(repn.linear_vars):\n            if id(var) not in nonant_ids:\n                linear_coefs[i] *= scen_prob\n\n        for i,(x,y) in enumerate(repn.quadratic_vars):\n            # only multiply through once\n            if id(x) not in nonant_ids:\n                quadratic_coefs[i] *= scen_prob\n            elif id(y) not in nonant_ids:\n                quadratic_coefs[i] *= scen_prob\n\n        # NOTE: the LShaped code negates the objective, so\n        #       we do the same here for consistency\n        if not self.is_minimizing:\n            for i,coef in enumerate(linear_coefs):\n                linear_coefs[i] = -coef\n            for i,coef in enumerate(quadratic_coefs):\n                quadratic_coefs[i] = -coef\n\n        # add the etas\n        for var in root.eta.values():\n            linear_vars.append(var)\n            linear_coefs.append(1)\n\n        expr = LinearExpression(constant=repn.constant, linear_coefs=linear_coefs,\n                                linear_vars=linear_vars)\n        if repn.quadratic_vars:\n            expr += pyo.quicksum(\n                (coef*x*y for coef,(x,y) in zip(quadratic_coefs, repn.quadratic_vars))\n            )\n\n        root.del_component(obj)\n\n        # set root objective function\n        root.obj = pyo.Objective(expr=expr, sense=pyo.minimize)\n\n        self.root = root",
  "def _create_shadow_root(self):\n\n        root = pyo.ConcreteModel()\n\n        arb_scen = self.local_scenarios[self.local_scenario_names[0]]\n        nonants = arb_scen._mpisppy_node_list[0].nonant_vardata_list\n\n        root_vars = list()\n        for v in nonants:\n            nonant_shadow = pyo.Var(name=v.name)\n            root.add_component(v.name, nonant_shadow)\n            root_vars.append(nonant_shadow)\n        \n        if self.has_root_scens:\n            eta_indx = [scenario_name for scenario_name in self.all_scenario_names\n                            if scenario_name not in self.root_scenarios]\n        else:\n            eta_indx = self.all_scenario_names\n        self._add_root_etas(root, eta_indx)\n\n        root.obj = None\n        self.root = root\n        self.root_vars = root_vars",
  "def set_eta_bounds(self):\n        if self.compute_eta_bound:\n            ## for scenarios not in self.local_scenarios, these will be a large negative number\n            this_etas_lb = np.fromiter((self.valid_eta_lb[scen] for scen in self.all_scenario_names),\n                                    float, count=len(self.all_scenario_names))\n\n            all_etas_lb = np.empty_like(this_etas_lb)\n\n            self.mpicomm.Allreduce(this_etas_lb, all_etas_lb, op=MPI.MAX)\n\n            for idx, s in enumerate(self.all_scenario_names):\n                self.valid_eta_lb[s] = all_etas_lb[idx]\n            \n            # root may not have etas for every scenarios\n            for s, v in self.root.eta.items():\n                v.setlb(self.valid_eta_lb[s])",
  "def create_root(self):\n        \"\"\" creates a ConcreteModel from one of the problem scenarios then\n            modifies the model to serve as the root problem \n        \"\"\"\n        if self.cylinder_rank == 0:\n            if self.has_root_scens:\n                self._create_root_with_scenarios()\n            else:\n                self._create_root_no_scenarios()\n        else: \n            ## if we're not rank0, just create a root to\n            ## hold the nonants and etas; rank0 will do \n            ## the optimizing\n            self._create_shadow_root()",
  "def attach_nonant_var_map(self, scenario_name):\n        instance = self.local_scenarios[scenario_name]\n\n        subproblem_to_root_vars_map = pyo.ComponentMap()\n        for var, rvar in zip(instance._mpisppy_data.nonant_indices.values(), self.root_vars):\n            if var.name not in rvar.name:\n                raise Exception(\"Error: Complicating variable mismatch, sub-problem variables changed order\")\n            subproblem_to_root_vars_map[var] = rvar \n\n        # this is for interefacing with PH code\n        instance._mpisppy_model.subproblem_to_root_vars_map = subproblem_to_root_vars_map",
  "def create_subproblem(self, scenario_name):\n        \"\"\" the subproblem creation function passed into the\n            BendersCutsGenerator \n        \"\"\"\n        instance = self.local_scenarios[scenario_name]\n\n        nonant_list, nonant_ids = _get_nonant_ids(instance) \n\n        # NOTE: since we use generate_standard_repn below, we need\n        #       to unfix any nonants so they'll properly appear\n        #       in the objective\n        fixed_nonants = [ var for var in nonant_list if var.fixed ]\n        for var in fixed_nonants:\n            var.fixed = False\n\n        # pulls the scenario objective expression, removes the first stage variables, and sets the new objective\n        obj = find_active_objective(instance)\n\n        if not hasattr(instance, \"_mpisppy_probability\"):\n            instance._mpisppy_probability = 1. / self.scenario_count\n        _mpisppy_probability = instance._mpisppy_probability\n\n        repn = generate_standard_repn(obj.expr, quadratic=True)\n        if len(repn.nonlinear_vars) > 0:\n            raise ValueError(\"LShaped does not support models with nonlinear objective functions\")\n\n        linear_vars = list()\n        linear_coefs = list()\n        quadratic_vars = list()\n        quadratic_coefs = list()\n        ## we'll assume the constant is part of stage 1 (wlog it is), just\n        ## like the first-stage bits of the objective\n        constant = repn.constant \n\n        ## only keep the second stage variables in the objective\n        for coef, var in zip(repn.linear_coefs, repn.linear_vars):\n            id_var = id(var)\n            if id_var not in nonant_ids:\n                linear_vars.append(var)\n                linear_coefs.append(_mpisppy_probability*coef)\n        for coef, (x,y) in zip(repn.quadratic_coefs, repn.quadratic_vars):\n            id_x = id(x)\n            id_y = id(y)\n            if id_x not in nonant_ids or id_y not in nonant_ids:\n                quadratic_coefs.append(_mpisppy_probability*coef)\n                quadratic_vars.append((x,y))\n\n        # checks if model sense is max, if so negates the objective\n        if not self.is_minimizing:\n            for i,coef in enumerate(linear_coefs):\n                linear_coefs[i] = -coef\n            for i,coef in enumerate(quadratic_coefs):\n                quadratic_coefs[i] = -coef\n\n        expr = LinearExpression(constant=constant, linear_coefs=linear_coefs,\n                                linear_vars=linear_vars)\n        if quadratic_coefs:\n            expr += pyo.quicksum(\n                        (coef*x*y for coef,(x,y) in zip(quadratic_coefs, quadratic_vars))\n                    )\n\n        instance.del_component(obj)\n\n        # set subproblem objective function\n        instance.obj = pyo.Objective(expr=expr, sense=pyo.minimize)\n\n        ## need to do this here for validity if computing the eta bound\n        if self.relax_root:\n            # relaxes any integrality constraints for the subproblem\n            RelaxIntegerVars().apply_to(instance)\n\n        if self.compute_eta_bound:\n            for var in fixed_nonants:\n                var.fixed = True\n            opt = pyo.SolverFactory(self.options[\"sp_solver\"])\n            if self.options[\"sp_solver_options\"]:\n                for k,v in self.options[\"sp_solver_options\"].items():\n                    opt.options[k] = v\n\n            if sputils.is_persistent(opt):\n                set_instance_retry(instance, opt, scenario_name)\n                res = opt.solve(tee=False)\n            else:\n                res = opt.solve(instance, tee=False)\n\n            eta_lb = res.Problem[0].Lower_bound\n\n            self.valid_eta_lb[scenario_name] = eta_lb\n\n        # if not done above\n        if not self.relax_root:\n            # relaxes any integrality constraints for the subproblem\n            RelaxIntegerVars().apply_to(instance)\n\n        # iterates through constraints and removes first stage constraints from the model\n        # the id dict is used to improve the speed of identifying the stage each variables belongs to\n        for constr_data in list(itertools.chain(\n                instance.component_data_objects(SOSConstraint, active=True, descend_into=True)\n                , instance.component_data_objects(Constraint, active=True, descend_into=True))):\n            if _first_stage_only(constr_data, nonant_ids):\n                _del_con(constr_data)\n\n        # creates the sub map to remove first stage variables from objective expression\n        complicating_vars_map = pyo.ComponentMap()\n        subproblem_to_root_vars_map = pyo.ComponentMap()\n\n        # creates the complicating var map that connects the first stage variables in the sub problem to those in\n        # the root problem -- also set the bounds on the subproblem root vars to be none for better cuts\n        for var, rvar in zip(nonant_list, self.root_vars):\n            if var.name not in rvar.name: # rvar.name may be part of a bundle\n                raise Exception(\"Error: Complicating variable mismatch, sub-problem variables changed order\")\n            complicating_vars_map[rvar] = var\n            subproblem_to_root_vars_map[var] = rvar \n\n            # these are already enforced in the root\n            # don't need to be enfored in the subproblems\n            var.setlb(None)\n            var.setub(None)\n            var.fixed = False\n\n        # this is for interefacing with PH code\n        instance._mpisppy_model.subproblem_to_root_vars_map = subproblem_to_root_vars_map\n\n        if self.store_subproblems:\n            self.subproblems[scenario_name] = instance\n\n        return instance, complicating_vars_map",
  "def lshaped_algorithm(self, converger=None):\n        \"\"\" function that runs the lshaped.py algorithm\n        \"\"\"\n        if converger:\n            converger = converger(self, self.cylinder_rank, self.n_proc)\n        max_iter = 30\n        if \"max_iter\" in self.options:\n            max_iter = self.options[\"max_iter\"]\n        tol = 1e-8\n        if \"tol\" in self.options:\n            tol = self.options[\"tol\"]\n        verbose = True\n        if \"verbose\" in self.options:\n            verbose = self.options[\"verbose\"]\n        root_solver = self.options[\"root_solver\"]\n        sp_solver = self.options[\"sp_solver\"]\n\n        # creates the root problem\n        self.create_root()\n        m = self.root\n        assert hasattr(m, \"obj\")\n\n        # prevents problems from first stage variables becoming unconstrained\n        # after processing\n        _init_vars(self.root_vars)\n\n        # sets up the BendersCutGenerator object\n        m.bender = LShapedCutGenerator()\n\n        m.bender.set_input(root_vars=self.root_vars, tol=tol, comm=self.mpicomm)\n\n        # let the cut generator know who's using it, probably should check that this is called after set input\n        m.bender.set_ls(self)\n\n        # set the eta variables, removing this from the add_suproblem function so we can\n\n        # Pass all the scenarios in the problem to bender.add_subproblem\n        # and let it internally handle which ranks get which scenarios\n        if self.has_root_scens:\n            sub_scenarios = [\n                scenario_name for scenario_name in self.local_scenario_names\n                if scenario_name not in self.root_scenarios\n            ]\n        else:\n            sub_scenarios = self.local_scenario_names\n        for scenario_name in self.local_scenario_names:\n            if scenario_name in sub_scenarios:\n                subproblem_fn_kwargs = dict()\n                subproblem_fn_kwargs['scenario_name'] = scenario_name\n                m.bender.add_subproblem(\n                    subproblem_fn=self.create_subproblem,\n                    subproblem_fn_kwargs=subproblem_fn_kwargs,\n                    root_eta=m.eta[scenario_name],\n                    subproblem_solver=sp_solver,\n                    subproblem_solver_options=self.options[\"sp_solver_options\"]\n                )\n            else:\n                self.attach_nonant_var_map(scenario_name)\n\n        # set the eta bounds if computed\n        # by self.create_subproblem\n        self.set_eta_bounds()\n\n        if self.cylinder_rank == 0:\n            opt = pyo.SolverFactory(root_solver)\n            if opt is None:\n                raise Exception(\"Error: Failed to Create Master Solver\")\n\n            # set options\n            for k,v in self.options[\"root_solver_options\"].items():\n                opt.options[k] = v\n\n            is_persistent = sputils.is_persistent(opt)\n            if is_persistent:\n                set_instance_retry(m, opt, \"root\")\n\n        t = time.time()\n        res, t1, t2 = None, None, None\n\n        # benders solve loop, repeats the benders root - subproblem\n        # loop until either a no more cuts can are generated\n        # or the maximum iterations limit is reached\n        for self.iter in range(max_iter):\n            if verbose and self.cylinder_rank == 0:\n                if self.iter > 0:\n                    print(\"Current Iteration:\", self.iter + 1, \"Time Elapsed:\", \"%7.2f\" % (time.time() - t), \"Time Spent on Last Master:\", \"%7.2f\" % t1,\n                          \"Time Spent Generating Last Cut Set:\", \"%7.2f\" % t2, \"Current Objective:\", \"%7.2f\" % m.obj.expr())\n                else:\n                    print(\"Current Iteration:\", self.iter + 1, \"Time Elapsed:\", \"%7.2f\" % (time.time() - t), \"Current Objective: -Inf\")\n            t1 = time.time()\n            x_vals = np.zeros(len(self.root_vars))\n            eta_vals = np.zeros(self.scenario_count)\n            outer_bound = np.zeros(1)\n            if self.cylinder_rank == 0:\n                if is_persistent:\n                    res = opt.solve(tee=False)\n                else:\n                    res = opt.solve(m, tee=False)\n                # LShaped is always minimizing\n                outer_bound[0] = res.Problem[0].Lower_bound\n                for i, var in enumerate(self.root_vars):\n                    x_vals[i] = var.value\n                for i, eta in enumerate(m.eta.values()):\n                    eta_vals[i] = eta.value\n\n            self.mpicomm.Bcast(x_vals, root=0)\n            self.mpicomm.Bcast(eta_vals, root=0)\n            self.mpicomm.Bcast(outer_bound, root=0)\n\n            if self.is_minimizing:\n                self._LShaped_bound = outer_bound[0]\n            else:\n                # LShaped is always minimizing, so negate\n                # the outer bound for sharing broadly\n                self._LShaped_bound = -outer_bound[0]\n\n            if self.cylinder_rank != 0:\n                for i, var in enumerate(self.root_vars):\n                    var._value = x_vals[i]\n                for i, eta in enumerate(m.eta.values()):\n                    eta._value = eta_vals[i]\n            t1 = time.time() - t1\n\n            # The hub object takes precedence over the converger\n            # We'll send the nonants now, and check for a for\n            # convergence\n            if self.spcomm:\n                self.spcomm.sync(send_nonants=True)\n                if self.spcomm.is_converged():\n                    break\n\n            t2 = time.time()\n            cuts_added = m.bender.generate_cut()\n            t2 = time.time() - t2\n            if self.cylinder_rank == 0:\n                for c in cuts_added:\n                    if is_persistent:\n                        opt.add_constraint(c)\n                if verbose and len(cuts_added) == 0:\n                    print(\n                        f\"Converged in {self.iter+1} iterations.\\n\"\n                        f\"Total Time Elapsed: {time.time()-t:7.2f} \"\n                        f\"Time Spent on Last Master: {t1:7.2f} \"\n                        f\"Time spent verifying second stage: {t2:7.2f} \"\n                        f\"Final Objective: {m.obj.expr():7.2f}\"\n                    )\n                    self.first_stage_solution_available = True\n                    self.tree_solution_available = True\n                    break\n                if verbose and self.iter == max_iter - 1:\n                    print(\"WARNING MAX ITERATION LIMIT REACHED !!! \")\n            else:\n                if len(cuts_added) == 0:\n                    break\n            # The hub object takes precedence over the converger\n            if self.spcomm:\n                self.spcomm.sync(send_nonants=False)\n                if self.spcomm.is_converged():\n                    break\n            if converger:\n                converger.convergence_value()\n                if converger.is_converged():\n                    if verbose and self.cylinder_rank == 0:\n                        print(\n                            f\"Converged to user criteria in {self.iter+1} iterations.\\n\"\n                            f\"Total Time Elapsed: {time.time()-t:7.2f} \"\n                            f\"Time Spent on Last Master: {t1:7.2f} \"\n                            f\"Time spent verifying second stage: {t2:7.2f} \"\n                            f\"Final Objective: {m.obj.expr():7.2f}\"\n                        )\n                    break\n        return res",
  "def _eta_bounds(m, s):\n            return (self.valid_eta_lb[s],None)",
  "def scenario_creator_wrapper(name, **creator_options):\n                scenario = self.scenario_creator(name, **creator_options)\n                if not hasattr(scenario, '_mpisppy_probability'):\n                    scenario._mpisppy_probability = 1./len(self.all_scenario_names)\n                return scenario",
  "class APH(ph_base.PHBase):\n    \"\"\"\n    Args:\n        options (dict): PH options\n        all_scenario_names (list): all scenario names\n        scenario_creator (fct): returns a concrete model with special things\n        scenario_denouement (fct): for post processing and reporting\n        all_node_names (list of str): non-leaf node names\n        scenario_creator_kwargs (dict): keyword arguments passed to\n            `scenario_creator`.\n\n    Attributes (partial list):\n        local_scenarios (dict of scenario objects): concrete models with \n              extra data, key is name\n        comms (dict): keys are node names values are comm objects.\n        scenario_name_to_rank (dict): all scenario names\n        local_scenario_names (list): names of locals \n        current_solver_options (dict): from options; callbacks might change\n        synchronizer (object): asynch listener management\n        scenario_creator_kwargs (dict): keyword arguments passed to\n            `scenario_creator`.\n\n    \"\"\"\n    def __init__(\n        self,\n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,            \n        mpicomm=None,\n        scenario_creator_kwargs=None,\n        extensions=None,\n        extension_kwargs=None,\n        ph_converger=None,\n        rho_setter=None,\n        variable_probability=None,\n    ):\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement,\n            mpicomm=mpicomm,\n            all_nodenames=all_nodenames,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            extensions=extensions,\n            extension_kwargs=extension_kwargs,\n            ph_converger=ph_converger,\n            rho_setter=rho_setter,\n            variable_probability=variable_probability,\n        )\n\n        self.phis = {} # phi values, indexed by scenario names\n        self.tau_summand = 0  # place holder for iteration 1 reduce\n        self.phi_summand = 0\n        self.global_tau = 0\n        self.global_phi = 0\n        self.global_pusqnorm = 0 # ... may be out of date...\n        self.global_pvsqnorm = 0\n        self.global_pwsqnorm = 0\n        self.global_pzsqnorm = 0\n        self.local_pwsqnorm = 0\n        self.local_pzsqnorm = 0\n        self.conv = None\n        self.use_lag = False if \"APHuse_lag\" not in options\\\n                        else options[\"APHuse_lag\"]\n        self.APHgamma = 1 if \"APHgamma\" not in options\\\n                        else options[\"APHgamma\"]\n        assert(self.APHgamma > 0)\n        self.shelf_life = options.get(\"shelf_life\", 99)  # 99 is intended to be large\n        self.round_robin_dispatch = options.get(\"round_robin_dispatch\", False)\n        # TBD: use a property decorator for nu to enforce 0 < nu < 2\n        self.nu = 1 # might be changed dynamically by an extension\n        if \"APHnu\" in options:\n            self.nu = options[\"APHnu\"]\n        assert 0 < self.nu and self.nu < 2\n        self.dispatchrecord = dict()   # for local subproblems sname: (iter, phi)\n\n    #============================\n    def setup_Lens(self):\n        \"\"\" We need to know the lengths of c-style vectors for listener_util\n        \"\"\"\n        self.Lens = collections.OrderedDict({\"FirstReduce\": {},\n                                            \"SecondReduce\": {}})\n\n        for sname, scenario in self.local_scenarios.items():\n            for node in scenario._mpisppy_node_list:\n                self.Lens[\"FirstReduce\"][node.name] \\\n                    = 3 * len(node.nonant_vardata_list)\n                self.Lens[\"SecondReduce\"][node.name] = 0 # only use root?\n        self.Lens[\"FirstReduce\"][\"ROOT\"] += self.n_proc  # for time of update\n        # tau, phi, pusqnorm, pvsqnorm, pwsqnorm, pzsqnorm, secs\n        self.Lens[\"SecondReduce\"][\"ROOT\"] += 6 + self.n_proc \n\n\n    #============================\n    def setup_dispatchrecord(self):\n        # Start with a small number for iteration to randomize fist dispatch.\n        for sname in self.local_subproblems:\n            r = np.random.rand()\n            self.dispatchrecord[sname] = [(r,0)]\n\n\n    #============================\n    def Update_y(self, dlist, verbose):\n        # compute the new y (or set to zero if it is iter 1)\n        # iter 1 is iter 0 post-solves when seen from the paper\n        # dlist is used only after iter0 (it has the dispatched scen names)\n\n        slist = [d[0] for d in dlist]  # just the names\n        if self._PHIter != 1:\n            for k,s in self.local_scenarios.items():\n                if (not self.bundling and k in slist) \\\n                   or (self.bundling and s._mpisppy_data.bundlename in slist):\n                    for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                        if not self.use_lag:\n                            z_touse = s._mpisppy_model.z[(ndn,i)]._value\n                            W_touse = pyo.value(s._mpisppy_model.W[(ndn,i)])\n                        else:\n                            z_touse = s._mpisppy_model.z_foropt[(ndn,i)]._value\n                            W_touse = pyo.value(s._mpisppy_model.W_foropt[(ndn,i)])\n                        # pyo.value vs. _value ??\n                        s._mpisppy_model.y[(ndn,i)]._value = W_touse \\\n                                              + pyo.value(s._mpisppy_model.rho[(ndn,i)]) \\\n                                              * (xvar._value - z_touse)\n                        if verbose and self.cylinder_rank == 0:\n                            print (\"node, scen, var, y\", ndn, k,\n                                   self.cylinder_rank, xvar.name,\n                                   pyo.value(s._mpisppy_model.y[(ndn,i)]))\n        else:\n            for k,s in self.local_scenarios.items():\n                for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                    s._mpisppy_model.y[(ndn,i)]._value = 0\n            if verbose and self.cylinder_rank == 0:\n                print (\"All y=0 for iter1\")\n\n\n    #============================\n    def compute_phis_summand(self):\n        # update phis, return summand\n        summand = 0.0\n        for k,s in self.local_scenarios.items():\n            self.phis[k] = 0.0\n            for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                self.phis[k] += (pyo.value(s._mpisppy_model.z[(ndn,i)]) - xvar._value) \\\n                    *(pyo.value(s._mpisppy_model.W[(ndn,i)]) - pyo.value(s._mpisppy_model.y[(ndn,i)]))\n            self.phis[k] *= pyo.value(s._mpisppy_probability)\n            summand += self.phis[k]\n        return summand\n\n    #============================***********=========\n    def listener_side_gig(self, synchro):\n        \"\"\" Called by the listener after the first reduce.\n        First, see if there are enough xbar contributions to proceed.\n        If there are, then compute tau and phi.\n        NOTE: it gets the synchronizer as an arg but self already has it.\n        [WIP]\n        We are going to disable the side_gig on self if we\n        updated tau and phi.\n        Massive side-effects: e.g., update xbar etc.\n\n        Iter 1 (iter 0) in the paper is special: the v := u, which is a little\n        complicated because we only compute y-bar.        \n\n        \"\"\"\n        # This does unsafe things, so it can only be called when the worker is\n        # in a tight loop that respects the data lock.\n\n        verbose = self.options[\"verbose\"]\n        # See if we have enough xbars to proceed (need not be perfect)\n        self.synchronizer._unsafe_get_global_data(\"FirstReduce\",\n                                                  self.node_concats)\n        self.synchronizer._unsafe_get_global_data(\"SecondReduce\",\n                                                  self.node_concats)\n        # last_phi_tau_update_time\n        # (the last time this side-gig did the calculations)\n        # We are going to see how many rank's xbars have been computed\n        # since then. If enough (determined by frac_needed), the do the calcs.\n        # The six is because the reduced data (e.g. phi) are in the first 6.\n        lptut =  np.max(self.node_concats[\"SecondReduce\"][\"ROOT\"][6:])\n\n        logging.debug('   +++ debug enter listener_side_gig on cylinder_rank {} last phi update {}'\\\n              .format(self.cylinder_rank, lptut))\n\n        xbarin = 0 # count ranks (close enough to be a proxy for scenarios)\n        for cr in range(self.n_proc):\n            backdist = self.n_proc - cr  # how far back into the vector\n            ##logging.debug('      *side_gig* cr {} on rank {} time {}'.\\\n            ##    format(cr, self.cylinder_rank,\n            ##        self.node_concats[\"FirstReduce\"][\"ROOT\"][-backdist]))\n            if  self.node_concats[\"FirstReduce\"][\"ROOT\"][-backdist] \\\n                > lptut:\n                xbarin += 1\n\n        fracin = xbarin/self.n_proc + EPSILON\n        if  fracin < self.options[\"async_frac_needed\"]:\n            # We have not really \"done\" the side gig.\n            logging.debug('  ^ debug not good to go listener_side_gig on cylinder_rank {}; xbarin={}; fracin={}'\\\n                  .format(self.cylinder_rank, xbarin, fracin))\n            return\n\n        # If we are still here, we have enough to do the calculations\n        logging.debug('^^^ debug good to go  listener_side_gig on cylinder_rank {}; xbarin={}'\\\n              .format(self.cylinder_rank, xbarin))\n        if verbose and self.cylinder_rank == 0:\n            print (\"(%d)\" % xbarin)\n            \n        # set the xbar, xsqbar, and ybar in all the scenarios\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens        \n            for (ndn,i) in s._mpisppy_data.nonant_indices:\n                s._mpisppy_model.xbars[(ndn,i)]._value \\\n                    = self.node_concats[\"FirstReduce\"][ndn][i]\n                s._mpisppy_model.xsqbars[(ndn,i)]._value \\\n                    = self.node_concats[\"FirstReduce\"][ndn][nlens[ndn]+i]\n                s._mpisppy_model.ybars[(ndn,i)]._value \\\n                    = self.node_concats[\"FirstReduce\"][ndn][2*nlens[ndn]+i]\n\n                if verbose and self.cylinder_rank == 0:\n                    print (\"rank, scen, node, var, xbar:\",\n                           self.cylinder_rank,k,ndn,s._mpisppy_data.nonant_indices[ndn,i].name,\n                           pyo.value(s._mpisppy_model.xbars[(ndn,i)]))\n\n        # There is one tau_summand for the rank; global_tau is out of date when\n        # we get here because we could not compute it until the averages were.\n        # vk is just going to be ybar directly\n        if not hasattr(self, \"uk\"):\n            # indexed by sname and nonant index [sname][(ndn,i)]\n            self.uk = {sname: dict() for sname in self.local_scenarios.keys()} \n        self.local_pusqnorm = 0  # local summand for probability weighted sqnorm\n        self.local_pvsqnorm = 0\n        new_tau_summand = 0  # for this rank\n        for sname,s in self.local_scenarios.items():\n            scen_usqnorm = 0.0\n            scen_vsqnorm = 0.0\n            nlens = s._mpisppy_data.nlens        \n            for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                self.uk[sname][(ndn,i)] = xvar._value \\\n                                          - pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                # compute the usqnorm and vsqnorm (squared L2 norms)\n                scen_usqnorm += self.uk[sname][(ndn,i)] \\\n                              * self.uk[sname][(ndn,i)]\n                scen_vsqnorm += pyo.value(s._mpisppy_model.ybars[(ndn,i)]) \\\n                              * pyo.value(s._mpisppy_model.ybars[(ndn,i)])\n            self.local_pusqnorm += pyo.value(s._mpisppy_probability) * scen_usqnorm\n            self.local_pvsqnorm += pyo.value(s._mpisppy_probability) * scen_vsqnorm\n            new_tau_summand += pyo.value(s._mpisppy_probability) \\\n                               * (scen_usqnorm + scen_vsqnorm/self.APHgamma)\n                \n\n            \n        # tauk is the expecation of the sum sum of squares; update for this calc\n        logging.debug('  in side-gig, old global_tau={}'.format(self.global_tau))\n        logging.debug('  in side-gig, old summand={}'.format(self.tau_summand))\n        logging.debug('  in side-gig, new summand={}'.format(new_tau_summand))\n        self.global_tau = self.global_tau - self.tau_summand + new_tau_summand\n        self.tau_summand = new_tau_summand # make available for the next reduce\n        logging.debug('  in side-gig, new global_tau={}'.format(self.global_tau))\n\n        # now we can get the local contribution to the phi_sum \n        if self.global_tau <= 0:\n            logging.debug('  *** Negative tau={} on rank {}'\\\n                          .format(self.global_tau, self.cylinder_rank))\n        self.phi_summand = self.compute_phis_summand()\n\n        # prepare for the reduction that will take place after this side-gig\n        # (this is where the 6 comes from)\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][0] = self.tau_summand\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][1] = self.phi_summand\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][2] = self.local_pusqnorm\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][3] = self.local_pvsqnorm\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][4] = self.local_pwsqnorm\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][5] = self.local_pzsqnorm\n        # we have updated our summands and the listener will do a reduction\n        secs_so_far = time.perf_counter() - self.start_time\n        # Put in a time only for this rank, so the \"sum\" is really a report\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][6+self.cylinder_rank] = secs_so_far\n        # This is run by the listener, so don't tell the worker you have done\n        # it until you are sure you have.\n        self.synchronizer._unsafe_put_local_data(\"SecondReduce\",\n                                                 self.local_concats)\n        self.synchronizer.enable_side_gig = False  # we did it\n        logging.debug(' exit side_gid on rank {}'.format(self.cylinder_rank))\n        \n    #============================\n    def Compute_Averages(self, verbose=False):\n        \"\"\"Gather ybar, xbar and x squared bar for each node \n           and also distribute the values back to the scenarios.\n           Compute the tau summand from self and distribute back tauk\n           (tau_k is a scalar and special with respect to synchronizing).\n           Compute the phi summand and reduce it.\n\n        Args:\n          verbose (boolean): verbose output\n\n        note: this is a long routine because we need a reduce before\n              we can do more calcs that need another reduce and I want\n              to keep the reduce calls together.\n        NOTE: see compute_xbar for more notes.\n        note: DLW: think about multi-stage harder (March 2019); e.g. tau and phi\n\n        \"\"\"\n        if not hasattr(self, \"local_concats\"):\n            nodenames = [] # avoid repeated work\n            self.local_concats = {\"FirstReduce\": {}, # keys are tree node names\n                             \"SecondReduce\": {}}\n            self.node_concats = {\"FirstReduce\": {}, # concat of xbar and xsqbar\n                             \"SecondReduce\": {}} \n\n            # accumulate & concatenate all local contributions before the reduce\n\n            # create the c-style storage for the concats\n            for k,s in self.local_scenarios.items():\n                nlens = s._mpisppy_data.nlens        \n                for node in s._mpisppy_node_list:\n                    if node.name not in nodenames:\n                        ndn = node.name\n                        nodenames.append(ndn)\n                        mylen = self.Lens[\"FirstReduce\"][ndn]\n                        self.local_concats[\"FirstReduce\"][ndn]\\\n                            = np.zeros(mylen, dtype='d')\n                        self.node_concats[\"FirstReduce\"][ndn]\\\n                            = np.zeros(mylen, dtype='d')\n            # second reduce is tau and phi\n            mylen = self.Lens[\"SecondReduce\"][\"ROOT\"]\n            self.local_concats[\"SecondReduce\"][\"ROOT\"]\\\n                = np.zeros(mylen, dtype='d') \n            self.node_concats[\"SecondReduce\"][\"ROOT\"]\\\n                = np.zeros(mylen, dtype='d')\n        else: # concats are here, just zero them out. \n            # We zero them so we can use an accumulator in the next loop and\n            # that seems to be OK.\n            nodenames = []\n            for k,s in self.local_scenarios.items():\n                nlens = s._mpisppy_data.nlens        \n                for node in s._mpisppy_node_list:\n                    if node.name not in nodenames:\n                        ndn = node.name\n                        nodenames.append(ndn)\n                        self.local_concats[\"FirstReduce\"][ndn].fill(0)\n                        self.node_concats[\"FirstReduce\"][ndn].fill(0)                    \n            self.local_concats[\"SecondReduce\"][\"ROOT\"].fill(0)\n            self.node_concats[\"SecondReduce\"][\"ROOT\"].fill(0)\n\n        # Compute the locals and concat them for the first reduce.\n        # We don't need to lock here because the direct buffers are only accessed\n        # by compute_global_data.\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens        \n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                for i in range(nlens[node.name]):\n                    v_value = node.nonant_vardata_list[i]._value\n                    self.local_concats[\"FirstReduce\"][node.name][i] += \\\n                        (s._mpisppy_probability / node.uncond_prob) * v_value\n                    logging.debug(\"  rank= {} scen={}, i={}, v_value={}\".\\\n                                  format(global_rank, k, i, v_value))\n                    self.local_concats[\"FirstReduce\"][node.name][nlens[ndn]+i]\\\n                        += (s._mpisppy_probability / node.uncond_prob) * v_value * v_value\n                    self.local_concats[\"FirstReduce\"][node.name][2*nlens[ndn]+i]\\\n                        += (s._mpisppy_probability / node.uncond_prob) \\\n                           * pyo.value(s._mpisppy_model.y[(node.name,i)])\n\n        # record the time\n        secs_sofar = time.perf_counter() - self.start_time\n        # only this rank puts a time for this rank, so the sum is a report\n        self.local_concats[\"FirstReduce\"][\"ROOT\"][3*nlens[\"ROOT\"]+self.cylinder_rank] \\\n            = secs_sofar\n        logging.debug('Compute_Averages at secs_sofar {} on rank {}'\\\n                      .format(secs_sofar, self.cylinder_rank))\n                    \n        self.synchronizer.compute_global_data(self.local_concats,\n                                              self.node_concats,\n                                              enable_side_gig = True,\n                                              rednames = [\"FirstReduce\"],\n                                              keep_up = True)\n        # The lock is something to worry about here.\n        while self.synchronizer.global_quitting == 0 \\\n              and self.synchronizer.enable_side_gig:\n            # Other ranks could be reporting, so keep looking for them.\n            self.synchronizer.compute_global_data(self.local_concats,\n                                                  self.node_concats)\n            if not self.synchronizer.enable_side_gig:\n                logging.debug(' did side gig break on rank {}'.format(self.cylinder_rank))\n                break\n            else:\n                logging.debug('   gig wait sleep on rank {}'.format(self.cylinder_rank))\n                if verbose and self.cylinder_rank == 0:\n                    print ('s'),\n                time.sleep(self.options[\"async_sleep_secs\"])\n\n        # (if the listener still has the lock, compute_global_will wait for it)\n        self.synchronizer.compute_global_data(self.local_concats,\n                                              self.node_concats)\n        # We  assign the global xbar, etc. as side-effect in the side gig, btw\n        self.global_tau = self.node_concats[\"SecondReduce\"][\"ROOT\"][0]\n        self.global_phi = self.node_concats[\"SecondReduce\"][\"ROOT\"][1]\n        self.global_pusqnorm = self.node_concats[\"SecondReduce\"][\"ROOT\"][2]\n        self.global_pvsqnorm = self.node_concats[\"SecondReduce\"][\"ROOT\"][3]\n        self.global_pwsqnorm = self.node_concats[\"SecondReduce\"][\"ROOT\"][4]\n        self.global_pzsqnorm = self.node_concats[\"SecondReduce\"][\"ROOT\"][5]\n\n        logging.debug('Assigned global tau {} and phi {} on rank {}'\\\n                      .format(self.global_tau, self.global_phi, self.cylinder_rank))\n \n    #============================\n    def Update_theta_zw(self, verbose):\n        \"\"\"\n        Compute and store theta, then update z and w and update\n        the probability weighted norms.\n        \"\"\"\n        if self.global_tau <= 0:\n            logging.debug('|tau {}, rank {}'.format(self.global_tau, self.cylinder_rank))\n            self.theta = 0   \n        elif self.global_phi <= 0:\n            logging.debug('|phi {}, rank {}'.format(self.global_phi, self.cylinder_rank))\n            self.theta = 0\n        else:\n            self.theta = self.global_phi * self.nu / self.global_tau\n        logging.debug('Iter {} assigned theta {} on rank {}'\\\n                      .format(self._PHIter, self.theta, self.cylinder_rank))\n\n        oldpw = self.local_pwsqnorm\n        oldpz = self.local_pzsqnorm\n        self.local_pwsqnorm = 0\n        self.local_pzsqnorm = 0\n        # v is just ybar\n        for k,s in self.local_scenarios.items():\n            probs = pyo.value(s._mpisppy_probability)\n            for (ndn, i) in s._mpisppy_data.nonant_indices:\n                Wupdate = self.theta * self.uk[k][(ndn,i)]\n                Ws = pyo.value(s._mpisppy_model.W[(ndn,i)]) + Wupdate\n                s._mpisppy_model.W[(ndn,i)] = Ws \n                self.local_pwsqnorm += probs * Ws * Ws\n                # iter 1 is iter 0 post-solves when seen from the paper\n                if self._PHIter != 1:\n                    zs = pyo.value(s._mpisppy_model.z[(ndn,i)])\\\n                     + self.theta * pyo.value(s._mpisppy_model.ybars[(ndn,i)])/self.APHgamma\n                else:\n                     zs = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                s._mpisppy_model.z[(ndn,i)] = zs \n                self.local_pzsqnorm += probs * zs * zs\n                logging.debug(\"rank={}, scen={}, i={}, Ws={}, zs={}\".\\\n                              format(global_rank, k, i, Ws, zs))\n        # ? so they will be there next time? (we really need a third reduction)\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][4] = self.local_pwsqnorm\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][5] = self.local_pzsqnorm\n        # The values we just computed can't be in the global yet, so update here\n        self.global_pwsqnorm += (self.local_pwsqnorm - oldpw)\n        self.global_pzsqnorm += (self.local_pzsqnorm - oldpz)\n                \n    #============================\n    def Compute_Convergence(self, verbose=False):\n        \"\"\"\n        The convergence metric is the sqrt of the sum of\n        probability weighted unorm scaled by the probability weighted w norm\n        probability weighted vnorm scaled by the probability weighted z norm\n \n        Returns:\n            update self.conv if appropriate\n        \"\"\"\n        # dlw to dlw, April 2019: wnorm and znorm are in update_zw;\n        # the u and v should be in the side gig.\n        # you need a reduction on all the norms!!\n\n        punorm = math.sqrt(self.global_pusqnorm)\n        pwnorm = math.sqrt(self.global_pwsqnorm)\n        pvnorm = math.sqrt(self.global_pvsqnorm)\n        pznorm = math.sqrt(self.global_pzsqnorm)\n\n        if pwnorm > 0 and pznorm > 0:\n            self.conv = punorm / pwnorm + pvnorm / pznorm\n\n        logging.debug('self.conv={} self.global_pusqnorm={} self.global_pwsqnorm={} self.global_pvsqnorm={} self.global_pzsqnorm={})'\\\n                      .format(self.conv, self.global_pusqnorm, self.global_pwsqnorm, self.global_pvsqnorm, self.global_pzsqnorm))\n        # allow a PH converger, mainly for mpisspy to get xhat from a wheel conv\n        if hasattr(self, \"ph_conobject\") and self.ph_convobject is not None:\n            phc = self.ph_convobject(self, self.cylinder_rank, self.n_proc)\n            logging.debug(\"PH converger called (returned {})\".format(phc))\n\n\n    #==========\n    def _update_foropt(self, dlist):\n        # dlist is a list of subproblem names that were dispatched\n        assert self.use_lag\n        \"\"\"\n        if not self.bundling:\n            phidict = self.phis\n        else:\n            phidict = {k: self.phis[self.local_subproblems[k].scen_list[0]]}\n        \"\"\"\n        if not self.bundling:\n            for dl in dlist:\n                scenario = self.local_scenarios[dl[0]]\n                for (ndn,i), xvar in scenario._mpisppy_data.nonant_indices.items():\n                    scenario._mpisppy_model.z_foropt[(ndn,i)] = scenario._mpisppy_model.z[(ndn,i)]\n                    scenario._mpisppy_model.W_foropt[(ndn,i)] = scenario._mpisppy_model.W[(ndn,i)]\n        else:\n            for dl in dlist:\n                for sname in self.local_subproblems[dl[0]].scen_list:\n                    scenario = self.local_scenarios[sname]\n                    for (ndn,i), xvar in scenario._mpisppy_data.nonant_indices.items():\n                        scenario._mpisppy_model.z_foropt[(ndn,i)] = scenario._mpisppy_model.z[(ndn,i)]\n                        scenario._mpisppy_model.W_foropt[(ndn,i)] = scenario._mpisppy_model.W[(ndn,i)]\n\n\n    #====================================================================\n    def APH_solve_loop(self, solver_options=None,\n                       use_scenarios_not_subproblems=False,\n                       dtiming=False,\n                       gripe=False,\n                       disable_pyomo_signal_handling=False,\n                       tee=False,\n                       verbose=False,\n                       dispatch_frac=1):\n        \"\"\"See phbase.solve_loop. Loop over self.local_subproblems and solve\n            them in a manner dicated by the arguments. In addition to\n            changing the Var values in the scenarios, update\n            _PySP_feas_indictor for each.\n\n        Args:\n            solver_options (dict or None): the scenario solver options\n            use_scenarios_not_subproblems (boolean): for use by bounds\n            dtiming (boolean): indicates that timing should be reported\n            gripe (boolean): output a message if a solve fails\n            disable_pyomo_signal_handling (boolean): set to true for asynch, \n                                                     ignored for persistent solvers.\n            tee (boolean): show solver output to screen if possible\n            verbose (boolean): indicates verbose output\n            dispatch_frac (float): fraction to send out for solution based on phi\n\n        Returns:\n            dlist (list of (str, float): (dispatched name, phi )\n        \"\"\"\n        #==========\n        def _vb(msg): \n            if verbose and self.cylinder_rank == 0:\n                print (\"(cylinder rank {}) {}\".format(self.cylinder_rank, msg))\n        _vb(\"Entering solve_loop function.\")\n\n\n        if use_scenarios_not_subproblems:\n            s_source = self.local_scenarios\n            phidict = self.phis\n        else:\n            s_source = self.local_subproblems\n            if not self.bundling:\n                phidict = self.phis\n            else:\n                phidict = {k: self.phis[self.local_subproblems[k].scen_list[0]] for k in s_source.keys()}\n        # dict(sorted(phidict.items(), key=lambda item: item[1]))\n        # sortedbyphi = {k: v for k, v in sorted(phidict.items(), key=lambda item: item[1])}\n\n\n        #========\n        def _dispatch_list(scnt):\n            # Return the list of scnt (subproblems,phi) \n            # pairs for dispatch.\n            # There is an option to allow for round-robin for research purposes.\n            # NOTE: intermediate lists are created to help with verification.\n            # reminder: dispatchrecord is sname:[(iter,phi)...]\n            if self.round_robin_dispatch:\n                # TBD: check this sort\n                sortedbyI = {k: v for k, v in sorted(self.dispatchrecord.items(), \n                                                     key=lambda item: item[1][-1])}\n                _vb(\"  sortedbyI={}.format(sortedbyI)\")\n                # There is presumably a pythonic way to do this...\n                retval = list()\n                i = 0\n                for k,v in sortedbyI.items():\n                    retval.append((k, phidict[k]))  # sname, phi\n                    i += 1\n                    if i >= scnt:\n                        return retval\n                raise RuntimeError(f\"bad scnt={scnt} in _dispatch_list;\"\n                                   f\" len(sortedbyI)={len(sortedbyI)}\")\n            else:\n                # Not doing round robin\n                # k is sname\n                tosort = [(k, -max(self.dispatchrecord[k][-1][0], self.shelf_life-1), phidict[k])\\\n                          for k in self.dispatchrecord.keys()]\n                sortedlist = sorted(tosort, key=lambda element: (element[1], element[2]))\n                retval = [(sortedlist[k][0], sortedlist[k][2]) for k in range(scnt)]\n                # TBD: See if there were enough w/negative phi values and warn.\n                # TBD: see if shelf-life is hitting and warn\n                return retval\n\n\n        # body of APH_solve_loop fct starts hare\n        logging.debug(\"  early APH solve_loop for rank={}\".format(self.cylinder_rank))\n\n        scnt = max(1, round(len(self.dispatchrecord) * dispatch_frac))\n        dispatch_list = _dispatch_list(scnt)\n        _vb(\"dispatch list before dispath: {}\".format(dispatch_list))\n        pyomo_solve_times = list()\n        for dguy in dispatch_list:\n            k = dguy[0]   # name of who to dispatch\n            p = dguy[1]   # phi\n            s = s_source[k]\n            self.dispatchrecord[k].append((self._PHIter, p))\n            _vb(\"dispatch k={}; phi={}\".format(k, p))\n            logging.debug(\"  in APH solve_loop rank={}, k={}, phi={}\".\\\n                          format(self.cylinder_rank, k, p))\n            # the lower lever dtiming does a gather\n            pyomo_solve_times.append(self.solve_one(solver_options, k, s,\n                                              dtiming=False,\n                                              verbose=verbose,\n                                              tee=tee,\n                                              gripe=gripe,\n                disable_pyomo_signal_handling=disable_pyomo_signal_handling\n            ))\n\n        if dtiming:\n            print(\"Pyomo solve times (seconds):\")\n            print(\"\\trank=,%d, n=,%d, min=,%4.2f, mean=,%4.2f, max=,%4.2f\" %\n                  (self.global_rank,\n                   len(pyomo_solve_times),\n                   np.min(pyomo_solve_times),\n                   np.mean(pyomo_solve_times),\n                   np.max(pyomo_solve_times)))\n\n        return dispatch_list\n\n    #========\n    def _print_conv_detail(self):\n        print(\"Convergence Metric=\",self.conv)\n        punorm = math.sqrt(self.global_pusqnorm)\n        pwnorm = math.sqrt(self.global_pwsqnorm)\n        pvnorm = math.sqrt(self.global_pvsqnorm)\n        pznorm = math.sqrt(self.global_pzsqnorm)\n        print(f'   punorm={punorm} pwnorm={pwnorm} pvnorm={pvnorm} pznorm={pznorm}')\n        if pwnorm > 0 and pznorm > 0:\n            print(f\"    scaled U term={punorm / pwnorm}; scaled V term={pvnorm / pznorm}\")\n        else:\n            print(f\"    ! convergence metric cannot be computed due to zero-divide\")\n\n\n    #========\n    def display_details(self, msg):\n        \"\"\"Ouput as much as you can about the current state\"\"\"\n        print(f\"hello {msg}\")\n        print(f\"*** global rank {global_rank} display details: {msg}\")\n        print(f\"zero-based iteration number {self._PHIter}\")\n        self._print_conv_detail()\n        print(f\"phi={self.global_phi}, nu={self.nu}, tau={self.global_tau} so theta={self.theta}\")\n        print(f\"{'Nonants for':19} {'x':8} {'z':8} {'W':8} {'u':8} \")\n        for k,s in self.local_scenarios.items():\n            print(f\"   Scenario {k}\")\n            for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                print(f\"   {(ndn,i)} {float(xvar._value):9.3} \"\n                      f\"{float(s._mpisppy_model.z[(ndn,i)]._value):9.3}\"\n                      f\"{float(s._mpisppy_model.W[(ndn,i)]._value):9.3}\"\n                      f\"{float(self.uk[k][(ndn,i)]):9.3}\")\n        ph_base._Compute_Wbar(self)\n\n\n    #====================================================================\n    def APH_iterk(self, spcomm):\n        \"\"\" Loop for the main iterations (called by synchronizer).\n\n        Args:\n        spcomm (SPCommunitator object): to communicate intra and inter\n\n        Updates: \n            self.conv (): APH convergence\n\n        \"\"\"\n        logging.debug('==== enter iterk on rank {}'.format(self.cylinder_rank))\n        verbose = self.options[\"verbose\"]\n        have_extensions = self.extensions is not None\n\n        # We have the \"bottom of the loop at the top\"\n        # so we need a dlist to get the ball rolling (it might not be used)\n        dlist = [(sn, 0.0) for sn in self.local_scenario_names]\n        \n        # put dispatch_frac on the object so extensions can modify it\n        self.dispatch_frac = self.options[\"dispatch_frac\"]\\\n                             if \"dispatch_frac\" in self.options else 1\n\n        have_converger = self.ph_converger is not None\n        dprogress = self.options[\"display_progress\"]\n        dtiming = self.options[\"display_timing\"]\n        ddetail = \"display_convergence_detail\" in self.options and\\\n            self.options[\"display_convergence_detail\"]\n        self.conv = None\n        # The notion of an iteration is unclear\n        # we enter after the iteration 0 solves, so do updates first\n        for self._PHIter in range(1, self.options[\"PHIterLimit\"]+1):\n            if self.synchronizer.global_quitting:\n                break\n            iteration_start_time = time.time()\n\n            if dprogress and self.cylinder_rank == 0:\n                print(\"\")\n                print (\"Initiating APH Iteration\",self._PHIter)\n                print(\"\")\n\n            self.Update_y(dlist, verbose)\n            # Compute xbar, etc\n            logging.debug('pre Compute_Averages on rank {}'.format(self.cylinder_rank))\n            self.Compute_Averages(verbose)\n            logging.debug('post Compute_Averages on rank {}'.format(self.cylinder_rank))\n            if self.global_tau <= 0:\n                logging.critical('***tau is 0 on rank {}'.format(self.cylinder_rank))\n\n            # Apr 2019 dlw: If you want the convergence crit. to be up to date,\n            # do this as a listener side-gig and add another reduction.\n            self.Update_theta_zw(verbose)\n            self.Compute_Convergence()  # updates conv\n            phisum = self.compute_phis_summand() # post-step phis for dispatch\n            logging.debug('phisum={} after step on {}'.format(phisum, self.cylinder_rank))\n\n            # ORed checks for convergence\n            if spcomm is not None and type(spcomm) is not mpi.Intracomm:\n                spcomm.sync_with_spokes()\n                logging.debug('post sync_with_spokes on rank {}'.format(self.cylinder_rank))\n                if spcomm.is_converged():\n                    break    \n            if have_converger:\n                if self.convobject.is_converged():\n                    converged = True\n                    if self.cylinder_rank == 0:\n                        print(\"User-supplied converger determined termination criterion reached\")\n                    break\n            if ddetail:\n                self.display_details(\"pre-solve loop (everything is updated from prev iter)\")\n            # slight divergence from PH, where mid-iter is before conv\n            if have_extensions:\n                self.extobject.miditer()\n            \n            teeme = (\"tee-rank0-solves\" in self.options) \\\n                 and (self.options[\"tee-rank0-solves\"] == True\n                      and self.cylinder_rank == 0)\n            # Let the solve loop deal with persistent solvers & signal handling\n            # Aug2020 switch to a partial loop xxxxx maybe that is enough.....\n            # Aug2020 ... at least you would get dispatch\n            # Oct 2021: still need full dispatch in iter 1 (as well as iter 0)\n            # TBD: ? restructure so iter 1 can have partial dispatch\n            if self._PHIter == 1:\n                savefrac = self.dispatch_frac\n                self.dispatch_frac = 1   # to get a decent w for everyone\n            logging.debug('pre APH_solve_loop on rank {}'.format(self.cylinder_rank))\n            dlist = self.APH_solve_loop(solver_options = \\\n                                        self.current_solver_options,\n                                        dtiming=dtiming,\n                                        gripe=True,\n                                        disable_pyomo_signal_handling=True,\n                                        tee=teeme,\n                                        verbose=verbose,\n                                        dispatch_frac=self.dispatch_frac)\n\n            logging.debug('post APH_solve_loop on rank {}'.format(self.cylinder_rank))\n            if self._PHIter == 1:\n                 self.dispatch_frac = savefrac\n            if have_extensions:\n                self.extobject.enditer()\n\n            if dprogress and self.cylinder_rank == 0:\n                print(\"\")\n                print(\"After APH Iteration\",self._PHIter)\n                if not ddetail:\n                    self._print_conv_detail()\n                print(\"Iteration time: %6.2f\" \\\n                      % (time.time() - iteration_start_time))\n                print(\"Elapsed time:   %6.2f\" \\\n                      % (time.perf_counter() - self.start_time))\n            if self.use_lag:\n                self._update_foropt(dlist)\n\n        logging.debug('Setting synchronizer.quitting on rank %d' % self.cylinder_rank)\n        self.synchronizer.quitting = 1\n\n    #====================================================================\n    def APH_main(self, spcomm=None, finalize=True):\n\n        \"\"\"Execute the APH algorithm.\n        Args:\n            spcomm (SPCommunitator object): for intra or inter communications\n            finalize (bool, optional, default=True):\n                        If True, call self.post_loops(), if False, do not,\n                        and return None for Eobj\n\n        Returns:\n            conv, Eobj, trivial_bound: \n                        The first two CANNOT BE EASILY INTERPRETED. \n                        Eobj is the expected,  weighted objective with \n                        proximal term. It is not directly useful.\n                        The trivial bound is computed after iter 0\n        NOTE:\n            You need an xhat finder either in denoument or in an extension.\n        \"\"\"\n        # Prep needs to be before iter 0 for bundling\n        # (It could be split up)\n        logging.debug('enter aph main on cylinder_rank {}'.format(self.cylinder_rank))\n        self.PH_Prep(attach_duals=False, attach_prox=False)\n\n        # Begin APH-specific Prep\n        for sname, scenario in self.local_scenarios.items():    \n            # ys is plural of y\n            scenario._mpisppy_model.y = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                     initialize = 0.0,\n                                     mutable = True)\n            scenario._mpisppy_model.ybars = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                        initialize = 0.0,\n                                        mutable = True)\n            scenario._mpisppy_model.z = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                     initialize = 0.0,\n                                     mutable = True)\n            # lag: we will support lagging back only to the last solve\n            # IMPORTANT: pyomo does not support a second reference so no:\n            # scenario._mpisppy_model.z_foropt = scenario._mpisppy_model.z\n            \n            if self.use_lag:\n                scenario._mpisppy_model.z_foropt = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                         initialize = 0.0,\n                                         mutable = True)\n                scenario._mpisppy_model.W_foropt = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                         initialize = 0.0,\n                                         mutable = True)\n                \n            objfct = find_active_objective(scenario)\n                \n            if self.use_lag:\n                for (ndn,i), xvar in scenario._mpisppy_data.nonant_indices.items():\n                    # proximal term\n                    objfct.expr +=  scenario._mpisppy_model.prox_on * \\\n                        (scenario._mpisppy_model.rho[(ndn,i)] /2.0) * \\\n                        (xvar**2 - 2.0*xvar*scenario._mpisppy_model.z_foropt[(ndn,i)] + scenario._mpisppy_model.z_foropt[(ndn,i)]**2)                                            \n                    # W term\n                    objfct.expr +=  scenario._mpisppy_model.W_on * scenario._mpisppy_model.W_foropt[ndn,i] * xvar\n            else:\n                for (ndn,i), xvar in scenario._mpisppy_data.nonant_indices.items():\n                    # proximal term\n                    objfct.expr +=  scenario._mpisppy_model.prox_on * \\\n                        (scenario._mpisppy_model.rho[(ndn,i)] /2.0) * \\\n                        (xvar**2 - 2.0*xvar*scenario._mpisppy_model.z[(ndn,i)] + scenario._mpisppy_model.z[(ndn,i)]**2)                        \n                    # W term\n                    objfct.expr +=  scenario._mpisppy_model.W_on * scenario._mpisppy_model.W[ndn,i] * xvar\n\n        # End APH-specific Prep\n        \n        self.subproblem_creation(self.options[\"verbose\"])\n\n        trivial_bound = self.Iter0()\n\n        self.setup_Lens()\n        self.setup_dispatchrecord()\n\n        sleep_secs = self.options[\"async_sleep_secs\"]\n\n        lkwargs = None  # nothing beyond synchro\n        listener_gigs = {\"FirstReduce\": (self.listener_side_gig, lkwargs),\n                         \"SecondReduce\": None}\n        self.synchronizer = listener_util.Synchronizer(comms = self.comms,\n                                                    Lens = self.Lens,\n                                                    work_fct = self.APH_iterk,\n                                                    rank = self.cylinder_rank,\n                                                    sleep_secs = sleep_secs,\n                                                    asynch = True,\n                                                    listener_gigs = listener_gigs)\n        args = [spcomm] if spcomm is not None else [fullcomm]\n        kwargs = None  # {\"extensions\": extensions}\n        self.synchronizer.run(args, kwargs)\n\n        if finalize:\n            Eobj = self.post_loops()\n        else:\n            Eobj = None\n\n#        print(f\"Debug: here's the dispatch record for rank={self.global_rank}\")\n#        for k,v in self.dispatchrecord.items():\n#            print(k, v)\n#            print()\n#        print(\"End dispatch record\")\n\n        return self.conv, Eobj, trivial_bound",
  "def __init__(\n        self,\n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,            \n        mpicomm=None,\n        scenario_creator_kwargs=None,\n        extensions=None,\n        extension_kwargs=None,\n        ph_converger=None,\n        rho_setter=None,\n        variable_probability=None,\n    ):\n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement,\n            mpicomm=mpicomm,\n            all_nodenames=all_nodenames,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            extensions=extensions,\n            extension_kwargs=extension_kwargs,\n            ph_converger=ph_converger,\n            rho_setter=rho_setter,\n            variable_probability=variable_probability,\n        )\n\n        self.phis = {} # phi values, indexed by scenario names\n        self.tau_summand = 0  # place holder for iteration 1 reduce\n        self.phi_summand = 0\n        self.global_tau = 0\n        self.global_phi = 0\n        self.global_pusqnorm = 0 # ... may be out of date...\n        self.global_pvsqnorm = 0\n        self.global_pwsqnorm = 0\n        self.global_pzsqnorm = 0\n        self.local_pwsqnorm = 0\n        self.local_pzsqnorm = 0\n        self.conv = None\n        self.use_lag = False if \"APHuse_lag\" not in options\\\n                        else options[\"APHuse_lag\"]\n        self.APHgamma = 1 if \"APHgamma\" not in options\\\n                        else options[\"APHgamma\"]\n        assert(self.APHgamma > 0)\n        self.shelf_life = options.get(\"shelf_life\", 99)  # 99 is intended to be large\n        self.round_robin_dispatch = options.get(\"round_robin_dispatch\", False)\n        # TBD: use a property decorator for nu to enforce 0 < nu < 2\n        self.nu = 1 # might be changed dynamically by an extension\n        if \"APHnu\" in options:\n            self.nu = options[\"APHnu\"]\n        assert 0 < self.nu and self.nu < 2\n        self.dispatchrecord = dict()",
  "def setup_Lens(self):\n        \"\"\" We need to know the lengths of c-style vectors for listener_util\n        \"\"\"\n        self.Lens = collections.OrderedDict({\"FirstReduce\": {},\n                                            \"SecondReduce\": {}})\n\n        for sname, scenario in self.local_scenarios.items():\n            for node in scenario._mpisppy_node_list:\n                self.Lens[\"FirstReduce\"][node.name] \\\n                    = 3 * len(node.nonant_vardata_list)\n                self.Lens[\"SecondReduce\"][node.name] = 0 # only use root?\n        self.Lens[\"FirstReduce\"][\"ROOT\"] += self.n_proc  # for time of update\n        # tau, phi, pusqnorm, pvsqnorm, pwsqnorm, pzsqnorm, secs\n        self.Lens[\"SecondReduce\"][\"ROOT\"] += 6 + self.n_proc",
  "def setup_dispatchrecord(self):\n        # Start with a small number for iteration to randomize fist dispatch.\n        for sname in self.local_subproblems:\n            r = np.random.rand()\n            self.dispatchrecord[sname] = [(r,0)]",
  "def Update_y(self, dlist, verbose):\n        # compute the new y (or set to zero if it is iter 1)\n        # iter 1 is iter 0 post-solves when seen from the paper\n        # dlist is used only after iter0 (it has the dispatched scen names)\n\n        slist = [d[0] for d in dlist]  # just the names\n        if self._PHIter != 1:\n            for k,s in self.local_scenarios.items():\n                if (not self.bundling and k in slist) \\\n                   or (self.bundling and s._mpisppy_data.bundlename in slist):\n                    for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                        if not self.use_lag:\n                            z_touse = s._mpisppy_model.z[(ndn,i)]._value\n                            W_touse = pyo.value(s._mpisppy_model.W[(ndn,i)])\n                        else:\n                            z_touse = s._mpisppy_model.z_foropt[(ndn,i)]._value\n                            W_touse = pyo.value(s._mpisppy_model.W_foropt[(ndn,i)])\n                        # pyo.value vs. _value ??\n                        s._mpisppy_model.y[(ndn,i)]._value = W_touse \\\n                                              + pyo.value(s._mpisppy_model.rho[(ndn,i)]) \\\n                                              * (xvar._value - z_touse)\n                        if verbose and self.cylinder_rank == 0:\n                            print (\"node, scen, var, y\", ndn, k,\n                                   self.cylinder_rank, xvar.name,\n                                   pyo.value(s._mpisppy_model.y[(ndn,i)]))\n        else:\n            for k,s in self.local_scenarios.items():\n                for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                    s._mpisppy_model.y[(ndn,i)]._value = 0\n            if verbose and self.cylinder_rank == 0:\n                print (\"All y=0 for iter1\")",
  "def compute_phis_summand(self):\n        # update phis, return summand\n        summand = 0.0\n        for k,s in self.local_scenarios.items():\n            self.phis[k] = 0.0\n            for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                self.phis[k] += (pyo.value(s._mpisppy_model.z[(ndn,i)]) - xvar._value) \\\n                    *(pyo.value(s._mpisppy_model.W[(ndn,i)]) - pyo.value(s._mpisppy_model.y[(ndn,i)]))\n            self.phis[k] *= pyo.value(s._mpisppy_probability)\n            summand += self.phis[k]\n        return summand",
  "def listener_side_gig(self, synchro):\n        \"\"\" Called by the listener after the first reduce.\n        First, see if there are enough xbar contributions to proceed.\n        If there are, then compute tau and phi.\n        NOTE: it gets the synchronizer as an arg but self already has it.\n        [WIP]\n        We are going to disable the side_gig on self if we\n        updated tau and phi.\n        Massive side-effects: e.g., update xbar etc.\n\n        Iter 1 (iter 0) in the paper is special: the v := u, which is a little\n        complicated because we only compute y-bar.        \n\n        \"\"\"\n        # This does unsafe things, so it can only be called when the worker is\n        # in a tight loop that respects the data lock.\n\n        verbose = self.options[\"verbose\"]\n        # See if we have enough xbars to proceed (need not be perfect)\n        self.synchronizer._unsafe_get_global_data(\"FirstReduce\",\n                                                  self.node_concats)\n        self.synchronizer._unsafe_get_global_data(\"SecondReduce\",\n                                                  self.node_concats)\n        # last_phi_tau_update_time\n        # (the last time this side-gig did the calculations)\n        # We are going to see how many rank's xbars have been computed\n        # since then. If enough (determined by frac_needed), the do the calcs.\n        # The six is because the reduced data (e.g. phi) are in the first 6.\n        lptut =  np.max(self.node_concats[\"SecondReduce\"][\"ROOT\"][6:])\n\n        logging.debug('   +++ debug enter listener_side_gig on cylinder_rank {} last phi update {}'\\\n              .format(self.cylinder_rank, lptut))\n\n        xbarin = 0 # count ranks (close enough to be a proxy for scenarios)\n        for cr in range(self.n_proc):\n            backdist = self.n_proc - cr  # how far back into the vector\n            ##logging.debug('      *side_gig* cr {} on rank {} time {}'.\\\n            ##    format(cr, self.cylinder_rank,\n            ##        self.node_concats[\"FirstReduce\"][\"ROOT\"][-backdist]))\n            if  self.node_concats[\"FirstReduce\"][\"ROOT\"][-backdist] \\\n                > lptut:\n                xbarin += 1\n\n        fracin = xbarin/self.n_proc + EPSILON\n        if  fracin < self.options[\"async_frac_needed\"]:\n            # We have not really \"done\" the side gig.\n            logging.debug('  ^ debug not good to go listener_side_gig on cylinder_rank {}; xbarin={}; fracin={}'\\\n                  .format(self.cylinder_rank, xbarin, fracin))\n            return\n\n        # If we are still here, we have enough to do the calculations\n        logging.debug('^^^ debug good to go  listener_side_gig on cylinder_rank {}; xbarin={}'\\\n              .format(self.cylinder_rank, xbarin))\n        if verbose and self.cylinder_rank == 0:\n            print (\"(%d)\" % xbarin)\n            \n        # set the xbar, xsqbar, and ybar in all the scenarios\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens        \n            for (ndn,i) in s._mpisppy_data.nonant_indices:\n                s._mpisppy_model.xbars[(ndn,i)]._value \\\n                    = self.node_concats[\"FirstReduce\"][ndn][i]\n                s._mpisppy_model.xsqbars[(ndn,i)]._value \\\n                    = self.node_concats[\"FirstReduce\"][ndn][nlens[ndn]+i]\n                s._mpisppy_model.ybars[(ndn,i)]._value \\\n                    = self.node_concats[\"FirstReduce\"][ndn][2*nlens[ndn]+i]\n\n                if verbose and self.cylinder_rank == 0:\n                    print (\"rank, scen, node, var, xbar:\",\n                           self.cylinder_rank,k,ndn,s._mpisppy_data.nonant_indices[ndn,i].name,\n                           pyo.value(s._mpisppy_model.xbars[(ndn,i)]))\n\n        # There is one tau_summand for the rank; global_tau is out of date when\n        # we get here because we could not compute it until the averages were.\n        # vk is just going to be ybar directly\n        if not hasattr(self, \"uk\"):\n            # indexed by sname and nonant index [sname][(ndn,i)]\n            self.uk = {sname: dict() for sname in self.local_scenarios.keys()} \n        self.local_pusqnorm = 0  # local summand for probability weighted sqnorm\n        self.local_pvsqnorm = 0\n        new_tau_summand = 0  # for this rank\n        for sname,s in self.local_scenarios.items():\n            scen_usqnorm = 0.0\n            scen_vsqnorm = 0.0\n            nlens = s._mpisppy_data.nlens        \n            for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                self.uk[sname][(ndn,i)] = xvar._value \\\n                                          - pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                # compute the usqnorm and vsqnorm (squared L2 norms)\n                scen_usqnorm += self.uk[sname][(ndn,i)] \\\n                              * self.uk[sname][(ndn,i)]\n                scen_vsqnorm += pyo.value(s._mpisppy_model.ybars[(ndn,i)]) \\\n                              * pyo.value(s._mpisppy_model.ybars[(ndn,i)])\n            self.local_pusqnorm += pyo.value(s._mpisppy_probability) * scen_usqnorm\n            self.local_pvsqnorm += pyo.value(s._mpisppy_probability) * scen_vsqnorm\n            new_tau_summand += pyo.value(s._mpisppy_probability) \\\n                               * (scen_usqnorm + scen_vsqnorm/self.APHgamma)\n                \n\n            \n        # tauk is the expecation of the sum sum of squares; update for this calc\n        logging.debug('  in side-gig, old global_tau={}'.format(self.global_tau))\n        logging.debug('  in side-gig, old summand={}'.format(self.tau_summand))\n        logging.debug('  in side-gig, new summand={}'.format(new_tau_summand))\n        self.global_tau = self.global_tau - self.tau_summand + new_tau_summand\n        self.tau_summand = new_tau_summand # make available for the next reduce\n        logging.debug('  in side-gig, new global_tau={}'.format(self.global_tau))\n\n        # now we can get the local contribution to the phi_sum \n        if self.global_tau <= 0:\n            logging.debug('  *** Negative tau={} on rank {}'\\\n                          .format(self.global_tau, self.cylinder_rank))\n        self.phi_summand = self.compute_phis_summand()\n\n        # prepare for the reduction that will take place after this side-gig\n        # (this is where the 6 comes from)\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][0] = self.tau_summand\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][1] = self.phi_summand\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][2] = self.local_pusqnorm\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][3] = self.local_pvsqnorm\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][4] = self.local_pwsqnorm\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][5] = self.local_pzsqnorm\n        # we have updated our summands and the listener will do a reduction\n        secs_so_far = time.perf_counter() - self.start_time\n        # Put in a time only for this rank, so the \"sum\" is really a report\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][6+self.cylinder_rank] = secs_so_far\n        # This is run by the listener, so don't tell the worker you have done\n        # it until you are sure you have.\n        self.synchronizer._unsafe_put_local_data(\"SecondReduce\",\n                                                 self.local_concats)\n        self.synchronizer.enable_side_gig = False  # we did it\n        logging.debug(' exit side_gid on rank {}'.format(self.cylinder_rank))",
  "def Compute_Averages(self, verbose=False):\n        \"\"\"Gather ybar, xbar and x squared bar for each node \n           and also distribute the values back to the scenarios.\n           Compute the tau summand from self and distribute back tauk\n           (tau_k is a scalar and special with respect to synchronizing).\n           Compute the phi summand and reduce it.\n\n        Args:\n          verbose (boolean): verbose output\n\n        note: this is a long routine because we need a reduce before\n              we can do more calcs that need another reduce and I want\n              to keep the reduce calls together.\n        NOTE: see compute_xbar for more notes.\n        note: DLW: think about multi-stage harder (March 2019); e.g. tau and phi\n\n        \"\"\"\n        if not hasattr(self, \"local_concats\"):\n            nodenames = [] # avoid repeated work\n            self.local_concats = {\"FirstReduce\": {}, # keys are tree node names\n                             \"SecondReduce\": {}}\n            self.node_concats = {\"FirstReduce\": {}, # concat of xbar and xsqbar\n                             \"SecondReduce\": {}} \n\n            # accumulate & concatenate all local contributions before the reduce\n\n            # create the c-style storage for the concats\n            for k,s in self.local_scenarios.items():\n                nlens = s._mpisppy_data.nlens        \n                for node in s._mpisppy_node_list:\n                    if node.name not in nodenames:\n                        ndn = node.name\n                        nodenames.append(ndn)\n                        mylen = self.Lens[\"FirstReduce\"][ndn]\n                        self.local_concats[\"FirstReduce\"][ndn]\\\n                            = np.zeros(mylen, dtype='d')\n                        self.node_concats[\"FirstReduce\"][ndn]\\\n                            = np.zeros(mylen, dtype='d')\n            # second reduce is tau and phi\n            mylen = self.Lens[\"SecondReduce\"][\"ROOT\"]\n            self.local_concats[\"SecondReduce\"][\"ROOT\"]\\\n                = np.zeros(mylen, dtype='d') \n            self.node_concats[\"SecondReduce\"][\"ROOT\"]\\\n                = np.zeros(mylen, dtype='d')\n        else: # concats are here, just zero them out. \n            # We zero them so we can use an accumulator in the next loop and\n            # that seems to be OK.\n            nodenames = []\n            for k,s in self.local_scenarios.items():\n                nlens = s._mpisppy_data.nlens        \n                for node in s._mpisppy_node_list:\n                    if node.name not in nodenames:\n                        ndn = node.name\n                        nodenames.append(ndn)\n                        self.local_concats[\"FirstReduce\"][ndn].fill(0)\n                        self.node_concats[\"FirstReduce\"][ndn].fill(0)                    \n            self.local_concats[\"SecondReduce\"][\"ROOT\"].fill(0)\n            self.node_concats[\"SecondReduce\"][\"ROOT\"].fill(0)\n\n        # Compute the locals and concat them for the first reduce.\n        # We don't need to lock here because the direct buffers are only accessed\n        # by compute_global_data.\n        for k,s in self.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens        \n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                for i in range(nlens[node.name]):\n                    v_value = node.nonant_vardata_list[i]._value\n                    self.local_concats[\"FirstReduce\"][node.name][i] += \\\n                        (s._mpisppy_probability / node.uncond_prob) * v_value\n                    logging.debug(\"  rank= {} scen={}, i={}, v_value={}\".\\\n                                  format(global_rank, k, i, v_value))\n                    self.local_concats[\"FirstReduce\"][node.name][nlens[ndn]+i]\\\n                        += (s._mpisppy_probability / node.uncond_prob) * v_value * v_value\n                    self.local_concats[\"FirstReduce\"][node.name][2*nlens[ndn]+i]\\\n                        += (s._mpisppy_probability / node.uncond_prob) \\\n                           * pyo.value(s._mpisppy_model.y[(node.name,i)])\n\n        # record the time\n        secs_sofar = time.perf_counter() - self.start_time\n        # only this rank puts a time for this rank, so the sum is a report\n        self.local_concats[\"FirstReduce\"][\"ROOT\"][3*nlens[\"ROOT\"]+self.cylinder_rank] \\\n            = secs_sofar\n        logging.debug('Compute_Averages at secs_sofar {} on rank {}'\\\n                      .format(secs_sofar, self.cylinder_rank))\n                    \n        self.synchronizer.compute_global_data(self.local_concats,\n                                              self.node_concats,\n                                              enable_side_gig = True,\n                                              rednames = [\"FirstReduce\"],\n                                              keep_up = True)\n        # The lock is something to worry about here.\n        while self.synchronizer.global_quitting == 0 \\\n              and self.synchronizer.enable_side_gig:\n            # Other ranks could be reporting, so keep looking for them.\n            self.synchronizer.compute_global_data(self.local_concats,\n                                                  self.node_concats)\n            if not self.synchronizer.enable_side_gig:\n                logging.debug(' did side gig break on rank {}'.format(self.cylinder_rank))\n                break\n            else:\n                logging.debug('   gig wait sleep on rank {}'.format(self.cylinder_rank))\n                if verbose and self.cylinder_rank == 0:\n                    print ('s'),\n                time.sleep(self.options[\"async_sleep_secs\"])\n\n        # (if the listener still has the lock, compute_global_will wait for it)\n        self.synchronizer.compute_global_data(self.local_concats,\n                                              self.node_concats)\n        # We  assign the global xbar, etc. as side-effect in the side gig, btw\n        self.global_tau = self.node_concats[\"SecondReduce\"][\"ROOT\"][0]\n        self.global_phi = self.node_concats[\"SecondReduce\"][\"ROOT\"][1]\n        self.global_pusqnorm = self.node_concats[\"SecondReduce\"][\"ROOT\"][2]\n        self.global_pvsqnorm = self.node_concats[\"SecondReduce\"][\"ROOT\"][3]\n        self.global_pwsqnorm = self.node_concats[\"SecondReduce\"][\"ROOT\"][4]\n        self.global_pzsqnorm = self.node_concats[\"SecondReduce\"][\"ROOT\"][5]\n\n        logging.debug('Assigned global tau {} and phi {} on rank {}'\\\n                      .format(self.global_tau, self.global_phi, self.cylinder_rank))",
  "def Update_theta_zw(self, verbose):\n        \"\"\"\n        Compute and store theta, then update z and w and update\n        the probability weighted norms.\n        \"\"\"\n        if self.global_tau <= 0:\n            logging.debug('|tau {}, rank {}'.format(self.global_tau, self.cylinder_rank))\n            self.theta = 0   \n        elif self.global_phi <= 0:\n            logging.debug('|phi {}, rank {}'.format(self.global_phi, self.cylinder_rank))\n            self.theta = 0\n        else:\n            self.theta = self.global_phi * self.nu / self.global_tau\n        logging.debug('Iter {} assigned theta {} on rank {}'\\\n                      .format(self._PHIter, self.theta, self.cylinder_rank))\n\n        oldpw = self.local_pwsqnorm\n        oldpz = self.local_pzsqnorm\n        self.local_pwsqnorm = 0\n        self.local_pzsqnorm = 0\n        # v is just ybar\n        for k,s in self.local_scenarios.items():\n            probs = pyo.value(s._mpisppy_probability)\n            for (ndn, i) in s._mpisppy_data.nonant_indices:\n                Wupdate = self.theta * self.uk[k][(ndn,i)]\n                Ws = pyo.value(s._mpisppy_model.W[(ndn,i)]) + Wupdate\n                s._mpisppy_model.W[(ndn,i)] = Ws \n                self.local_pwsqnorm += probs * Ws * Ws\n                # iter 1 is iter 0 post-solves when seen from the paper\n                if self._PHIter != 1:\n                    zs = pyo.value(s._mpisppy_model.z[(ndn,i)])\\\n                     + self.theta * pyo.value(s._mpisppy_model.ybars[(ndn,i)])/self.APHgamma\n                else:\n                     zs = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                s._mpisppy_model.z[(ndn,i)] = zs \n                self.local_pzsqnorm += probs * zs * zs\n                logging.debug(\"rank={}, scen={}, i={}, Ws={}, zs={}\".\\\n                              format(global_rank, k, i, Ws, zs))\n        # ? so they will be there next time? (we really need a third reduction)\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][4] = self.local_pwsqnorm\n        self.local_concats[\"SecondReduce\"][\"ROOT\"][5] = self.local_pzsqnorm\n        # The values we just computed can't be in the global yet, so update here\n        self.global_pwsqnorm += (self.local_pwsqnorm - oldpw)\n        self.global_pzsqnorm += (self.local_pzsqnorm - oldpz)",
  "def Compute_Convergence(self, verbose=False):\n        \"\"\"\n        The convergence metric is the sqrt of the sum of\n        probability weighted unorm scaled by the probability weighted w norm\n        probability weighted vnorm scaled by the probability weighted z norm\n \n        Returns:\n            update self.conv if appropriate\n        \"\"\"\n        # dlw to dlw, April 2019: wnorm and znorm are in update_zw;\n        # the u and v should be in the side gig.\n        # you need a reduction on all the norms!!\n\n        punorm = math.sqrt(self.global_pusqnorm)\n        pwnorm = math.sqrt(self.global_pwsqnorm)\n        pvnorm = math.sqrt(self.global_pvsqnorm)\n        pznorm = math.sqrt(self.global_pzsqnorm)\n\n        if pwnorm > 0 and pznorm > 0:\n            self.conv = punorm / pwnorm + pvnorm / pznorm\n\n        logging.debug('self.conv={} self.global_pusqnorm={} self.global_pwsqnorm={} self.global_pvsqnorm={} self.global_pzsqnorm={})'\\\n                      .format(self.conv, self.global_pusqnorm, self.global_pwsqnorm, self.global_pvsqnorm, self.global_pzsqnorm))\n        # allow a PH converger, mainly for mpisspy to get xhat from a wheel conv\n        if hasattr(self, \"ph_conobject\") and self.ph_convobject is not None:\n            phc = self.ph_convobject(self, self.cylinder_rank, self.n_proc)\n            logging.debug(\"PH converger called (returned {})\".format(phc))",
  "def _update_foropt(self, dlist):\n        # dlist is a list of subproblem names that were dispatched\n        assert self.use_lag\n        \"\"\"\n        if not self.bundling:\n            phidict = self.phis\n        else:\n            phidict = {k: self.phis[self.local_subproblems[k].scen_list[0]]}\n        \"\"\"\n        if not self.bundling:\n            for dl in dlist:\n                scenario = self.local_scenarios[dl[0]]\n                for (ndn,i), xvar in scenario._mpisppy_data.nonant_indices.items():\n                    scenario._mpisppy_model.z_foropt[(ndn,i)] = scenario._mpisppy_model.z[(ndn,i)]\n                    scenario._mpisppy_model.W_foropt[(ndn,i)] = scenario._mpisppy_model.W[(ndn,i)]\n        else:\n            for dl in dlist:\n                for sname in self.local_subproblems[dl[0]].scen_list:\n                    scenario = self.local_scenarios[sname]\n                    for (ndn,i), xvar in scenario._mpisppy_data.nonant_indices.items():\n                        scenario._mpisppy_model.z_foropt[(ndn,i)] = scenario._mpisppy_model.z[(ndn,i)]\n                        scenario._mpisppy_model.W_foropt[(ndn,i)] = scenario._mpisppy_model.W[(ndn,i)]",
  "def APH_solve_loop(self, solver_options=None,\n                       use_scenarios_not_subproblems=False,\n                       dtiming=False,\n                       gripe=False,\n                       disable_pyomo_signal_handling=False,\n                       tee=False,\n                       verbose=False,\n                       dispatch_frac=1):\n        \"\"\"See phbase.solve_loop. Loop over self.local_subproblems and solve\n            them in a manner dicated by the arguments. In addition to\n            changing the Var values in the scenarios, update\n            _PySP_feas_indictor for each.\n\n        Args:\n            solver_options (dict or None): the scenario solver options\n            use_scenarios_not_subproblems (boolean): for use by bounds\n            dtiming (boolean): indicates that timing should be reported\n            gripe (boolean): output a message if a solve fails\n            disable_pyomo_signal_handling (boolean): set to true for asynch, \n                                                     ignored for persistent solvers.\n            tee (boolean): show solver output to screen if possible\n            verbose (boolean): indicates verbose output\n            dispatch_frac (float): fraction to send out for solution based on phi\n\n        Returns:\n            dlist (list of (str, float): (dispatched name, phi )\n        \"\"\"\n        #==========\n        def _vb(msg): \n            if verbose and self.cylinder_rank == 0:\n                print (\"(cylinder rank {}) {}\".format(self.cylinder_rank, msg))\n        _vb(\"Entering solve_loop function.\")\n\n\n        if use_scenarios_not_subproblems:\n            s_source = self.local_scenarios\n            phidict = self.phis\n        else:\n            s_source = self.local_subproblems\n            if not self.bundling:\n                phidict = self.phis\n            else:\n                phidict = {k: self.phis[self.local_subproblems[k].scen_list[0]] for k in s_source.keys()}\n        # dict(sorted(phidict.items(), key=lambda item: item[1]))\n        # sortedbyphi = {k: v for k, v in sorted(phidict.items(), key=lambda item: item[1])}\n\n\n        #========\n        def _dispatch_list(scnt):\n            # Return the list of scnt (subproblems,phi) \n            # pairs for dispatch.\n            # There is an option to allow for round-robin for research purposes.\n            # NOTE: intermediate lists are created to help with verification.\n            # reminder: dispatchrecord is sname:[(iter,phi)...]\n            if self.round_robin_dispatch:\n                # TBD: check this sort\n                sortedbyI = {k: v for k, v in sorted(self.dispatchrecord.items(), \n                                                     key=lambda item: item[1][-1])}\n                _vb(\"  sortedbyI={}.format(sortedbyI)\")\n                # There is presumably a pythonic way to do this...\n                retval = list()\n                i = 0\n                for k,v in sortedbyI.items():\n                    retval.append((k, phidict[k]))  # sname, phi\n                    i += 1\n                    if i >= scnt:\n                        return retval\n                raise RuntimeError(f\"bad scnt={scnt} in _dispatch_list;\"\n                                   f\" len(sortedbyI)={len(sortedbyI)}\")\n            else:\n                # Not doing round robin\n                # k is sname\n                tosort = [(k, -max(self.dispatchrecord[k][-1][0], self.shelf_life-1), phidict[k])\\\n                          for k in self.dispatchrecord.keys()]\n                sortedlist = sorted(tosort, key=lambda element: (element[1], element[2]))\n                retval = [(sortedlist[k][0], sortedlist[k][2]) for k in range(scnt)]\n                # TBD: See if there were enough w/negative phi values and warn.\n                # TBD: see if shelf-life is hitting and warn\n                return retval\n\n\n        # body of APH_solve_loop fct starts hare\n        logging.debug(\"  early APH solve_loop for rank={}\".format(self.cylinder_rank))\n\n        scnt = max(1, round(len(self.dispatchrecord) * dispatch_frac))\n        dispatch_list = _dispatch_list(scnt)\n        _vb(\"dispatch list before dispath: {}\".format(dispatch_list))\n        pyomo_solve_times = list()\n        for dguy in dispatch_list:\n            k = dguy[0]   # name of who to dispatch\n            p = dguy[1]   # phi\n            s = s_source[k]\n            self.dispatchrecord[k].append((self._PHIter, p))\n            _vb(\"dispatch k={}; phi={}\".format(k, p))\n            logging.debug(\"  in APH solve_loop rank={}, k={}, phi={}\".\\\n                          format(self.cylinder_rank, k, p))\n            # the lower lever dtiming does a gather\n            pyomo_solve_times.append(self.solve_one(solver_options, k, s,\n                                              dtiming=False,\n                                              verbose=verbose,\n                                              tee=tee,\n                                              gripe=gripe,\n                disable_pyomo_signal_handling=disable_pyomo_signal_handling\n            ))\n\n        if dtiming:\n            print(\"Pyomo solve times (seconds):\")\n            print(\"\\trank=,%d, n=,%d, min=,%4.2f, mean=,%4.2f, max=,%4.2f\" %\n                  (self.global_rank,\n                   len(pyomo_solve_times),\n                   np.min(pyomo_solve_times),\n                   np.mean(pyomo_solve_times),\n                   np.max(pyomo_solve_times)))\n\n        return dispatch_list",
  "def _print_conv_detail(self):\n        print(\"Convergence Metric=\",self.conv)\n        punorm = math.sqrt(self.global_pusqnorm)\n        pwnorm = math.sqrt(self.global_pwsqnorm)\n        pvnorm = math.sqrt(self.global_pvsqnorm)\n        pznorm = math.sqrt(self.global_pzsqnorm)\n        print(f'   punorm={punorm} pwnorm={pwnorm} pvnorm={pvnorm} pznorm={pznorm}')\n        if pwnorm > 0 and pznorm > 0:\n            print(f\"    scaled U term={punorm / pwnorm}; scaled V term={pvnorm / pznorm}\")\n        else:\n            print(f\"    ! convergence metric cannot be computed due to zero-divide\")",
  "def display_details(self, msg):\n        \"\"\"Ouput as much as you can about the current state\"\"\"\n        print(f\"hello {msg}\")\n        print(f\"*** global rank {global_rank} display details: {msg}\")\n        print(f\"zero-based iteration number {self._PHIter}\")\n        self._print_conv_detail()\n        print(f\"phi={self.global_phi}, nu={self.nu}, tau={self.global_tau} so theta={self.theta}\")\n        print(f\"{'Nonants for':19} {'x':8} {'z':8} {'W':8} {'u':8} \")\n        for k,s in self.local_scenarios.items():\n            print(f\"   Scenario {k}\")\n            for (ndn,i), xvar in s._mpisppy_data.nonant_indices.items():\n                print(f\"   {(ndn,i)} {float(xvar._value):9.3} \"\n                      f\"{float(s._mpisppy_model.z[(ndn,i)]._value):9.3}\"\n                      f\"{float(s._mpisppy_model.W[(ndn,i)]._value):9.3}\"\n                      f\"{float(self.uk[k][(ndn,i)]):9.3}\")\n        ph_base._Compute_Wbar(self)",
  "def APH_iterk(self, spcomm):\n        \"\"\" Loop for the main iterations (called by synchronizer).\n\n        Args:\n        spcomm (SPCommunitator object): to communicate intra and inter\n\n        Updates: \n            self.conv (): APH convergence\n\n        \"\"\"\n        logging.debug('==== enter iterk on rank {}'.format(self.cylinder_rank))\n        verbose = self.options[\"verbose\"]\n        have_extensions = self.extensions is not None\n\n        # We have the \"bottom of the loop at the top\"\n        # so we need a dlist to get the ball rolling (it might not be used)\n        dlist = [(sn, 0.0) for sn in self.local_scenario_names]\n        \n        # put dispatch_frac on the object so extensions can modify it\n        self.dispatch_frac = self.options[\"dispatch_frac\"]\\\n                             if \"dispatch_frac\" in self.options else 1\n\n        have_converger = self.ph_converger is not None\n        dprogress = self.options[\"display_progress\"]\n        dtiming = self.options[\"display_timing\"]\n        ddetail = \"display_convergence_detail\" in self.options and\\\n            self.options[\"display_convergence_detail\"]\n        self.conv = None\n        # The notion of an iteration is unclear\n        # we enter after the iteration 0 solves, so do updates first\n        for self._PHIter in range(1, self.options[\"PHIterLimit\"]+1):\n            if self.synchronizer.global_quitting:\n                break\n            iteration_start_time = time.time()\n\n            if dprogress and self.cylinder_rank == 0:\n                print(\"\")\n                print (\"Initiating APH Iteration\",self._PHIter)\n                print(\"\")\n\n            self.Update_y(dlist, verbose)\n            # Compute xbar, etc\n            logging.debug('pre Compute_Averages on rank {}'.format(self.cylinder_rank))\n            self.Compute_Averages(verbose)\n            logging.debug('post Compute_Averages on rank {}'.format(self.cylinder_rank))\n            if self.global_tau <= 0:\n                logging.critical('***tau is 0 on rank {}'.format(self.cylinder_rank))\n\n            # Apr 2019 dlw: If you want the convergence crit. to be up to date,\n            # do this as a listener side-gig and add another reduction.\n            self.Update_theta_zw(verbose)\n            self.Compute_Convergence()  # updates conv\n            phisum = self.compute_phis_summand() # post-step phis for dispatch\n            logging.debug('phisum={} after step on {}'.format(phisum, self.cylinder_rank))\n\n            # ORed checks for convergence\n            if spcomm is not None and type(spcomm) is not mpi.Intracomm:\n                spcomm.sync_with_spokes()\n                logging.debug('post sync_with_spokes on rank {}'.format(self.cylinder_rank))\n                if spcomm.is_converged():\n                    break    \n            if have_converger:\n                if self.convobject.is_converged():\n                    converged = True\n                    if self.cylinder_rank == 0:\n                        print(\"User-supplied converger determined termination criterion reached\")\n                    break\n            if ddetail:\n                self.display_details(\"pre-solve loop (everything is updated from prev iter)\")\n            # slight divergence from PH, where mid-iter is before conv\n            if have_extensions:\n                self.extobject.miditer()\n            \n            teeme = (\"tee-rank0-solves\" in self.options) \\\n                 and (self.options[\"tee-rank0-solves\"] == True\n                      and self.cylinder_rank == 0)\n            # Let the solve loop deal with persistent solvers & signal handling\n            # Aug2020 switch to a partial loop xxxxx maybe that is enough.....\n            # Aug2020 ... at least you would get dispatch\n            # Oct 2021: still need full dispatch in iter 1 (as well as iter 0)\n            # TBD: ? restructure so iter 1 can have partial dispatch\n            if self._PHIter == 1:\n                savefrac = self.dispatch_frac\n                self.dispatch_frac = 1   # to get a decent w for everyone\n            logging.debug('pre APH_solve_loop on rank {}'.format(self.cylinder_rank))\n            dlist = self.APH_solve_loop(solver_options = \\\n                                        self.current_solver_options,\n                                        dtiming=dtiming,\n                                        gripe=True,\n                                        disable_pyomo_signal_handling=True,\n                                        tee=teeme,\n                                        verbose=verbose,\n                                        dispatch_frac=self.dispatch_frac)\n\n            logging.debug('post APH_solve_loop on rank {}'.format(self.cylinder_rank))\n            if self._PHIter == 1:\n                 self.dispatch_frac = savefrac\n            if have_extensions:\n                self.extobject.enditer()\n\n            if dprogress and self.cylinder_rank == 0:\n                print(\"\")\n                print(\"After APH Iteration\",self._PHIter)\n                if not ddetail:\n                    self._print_conv_detail()\n                print(\"Iteration time: %6.2f\" \\\n                      % (time.time() - iteration_start_time))\n                print(\"Elapsed time:   %6.2f\" \\\n                      % (time.perf_counter() - self.start_time))\n            if self.use_lag:\n                self._update_foropt(dlist)\n\n        logging.debug('Setting synchronizer.quitting on rank %d' % self.cylinder_rank)\n        self.synchronizer.quitting = 1",
  "def APH_main(self, spcomm=None, finalize=True):\n\n        \"\"\"Execute the APH algorithm.\n        Args:\n            spcomm (SPCommunitator object): for intra or inter communications\n            finalize (bool, optional, default=True):\n                        If True, call self.post_loops(), if False, do not,\n                        and return None for Eobj\n\n        Returns:\n            conv, Eobj, trivial_bound: \n                        The first two CANNOT BE EASILY INTERPRETED. \n                        Eobj is the expected,  weighted objective with \n                        proximal term. It is not directly useful.\n                        The trivial bound is computed after iter 0\n        NOTE:\n            You need an xhat finder either in denoument or in an extension.\n        \"\"\"\n        # Prep needs to be before iter 0 for bundling\n        # (It could be split up)\n        logging.debug('enter aph main on cylinder_rank {}'.format(self.cylinder_rank))\n        self.PH_Prep(attach_duals=False, attach_prox=False)\n\n        # Begin APH-specific Prep\n        for sname, scenario in self.local_scenarios.items():    \n            # ys is plural of y\n            scenario._mpisppy_model.y = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                     initialize = 0.0,\n                                     mutable = True)\n            scenario._mpisppy_model.ybars = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                        initialize = 0.0,\n                                        mutable = True)\n            scenario._mpisppy_model.z = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                     initialize = 0.0,\n                                     mutable = True)\n            # lag: we will support lagging back only to the last solve\n            # IMPORTANT: pyomo does not support a second reference so no:\n            # scenario._mpisppy_model.z_foropt = scenario._mpisppy_model.z\n            \n            if self.use_lag:\n                scenario._mpisppy_model.z_foropt = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                         initialize = 0.0,\n                                         mutable = True)\n                scenario._mpisppy_model.W_foropt = pyo.Param(scenario._mpisppy_data.nonant_indices.keys(),\n                                         initialize = 0.0,\n                                         mutable = True)\n                \n            objfct = find_active_objective(scenario)\n                \n            if self.use_lag:\n                for (ndn,i), xvar in scenario._mpisppy_data.nonant_indices.items():\n                    # proximal term\n                    objfct.expr +=  scenario._mpisppy_model.prox_on * \\\n                        (scenario._mpisppy_model.rho[(ndn,i)] /2.0) * \\\n                        (xvar**2 - 2.0*xvar*scenario._mpisppy_model.z_foropt[(ndn,i)] + scenario._mpisppy_model.z_foropt[(ndn,i)]**2)                                            \n                    # W term\n                    objfct.expr +=  scenario._mpisppy_model.W_on * scenario._mpisppy_model.W_foropt[ndn,i] * xvar\n            else:\n                for (ndn,i), xvar in scenario._mpisppy_data.nonant_indices.items():\n                    # proximal term\n                    objfct.expr +=  scenario._mpisppy_model.prox_on * \\\n                        (scenario._mpisppy_model.rho[(ndn,i)] /2.0) * \\\n                        (xvar**2 - 2.0*xvar*scenario._mpisppy_model.z[(ndn,i)] + scenario._mpisppy_model.z[(ndn,i)]**2)                        \n                    # W term\n                    objfct.expr +=  scenario._mpisppy_model.W_on * scenario._mpisppy_model.W[ndn,i] * xvar\n\n        # End APH-specific Prep\n        \n        self.subproblem_creation(self.options[\"verbose\"])\n\n        trivial_bound = self.Iter0()\n\n        self.setup_Lens()\n        self.setup_dispatchrecord()\n\n        sleep_secs = self.options[\"async_sleep_secs\"]\n\n        lkwargs = None  # nothing beyond synchro\n        listener_gigs = {\"FirstReduce\": (self.listener_side_gig, lkwargs),\n                         \"SecondReduce\": None}\n        self.synchronizer = listener_util.Synchronizer(comms = self.comms,\n                                                    Lens = self.Lens,\n                                                    work_fct = self.APH_iterk,\n                                                    rank = self.cylinder_rank,\n                                                    sleep_secs = sleep_secs,\n                                                    asynch = True,\n                                                    listener_gigs = listener_gigs)\n        args = [spcomm] if spcomm is not None else [fullcomm]\n        kwargs = None  # {\"extensions\": extensions}\n        self.synchronizer.run(args, kwargs)\n\n        if finalize:\n            Eobj = self.post_loops()\n        else:\n            Eobj = None\n\n#        print(f\"Debug: here's the dispatch record for rank={self.global_rank}\")\n#        for k,v in self.dispatchrecord.items():\n#            print(k, v)\n#            print()\n#        print(\"End dispatch record\")\n\n        return self.conv, Eobj, trivial_bound",
  "def _vb(msg): \n            if verbose and self.cylinder_rank == 0:\n                print (\"(cylinder rank {}) {}\".format(self.cylinder_rank, msg))",
  "def _dispatch_list(scnt):\n            # Return the list of scnt (subproblems,phi) \n            # pairs for dispatch.\n            # There is an option to allow for round-robin for research purposes.\n            # NOTE: intermediate lists are created to help with verification.\n            # reminder: dispatchrecord is sname:[(iter,phi)...]\n            if self.round_robin_dispatch:\n                # TBD: check this sort\n                sortedbyI = {k: v for k, v in sorted(self.dispatchrecord.items(), \n                                                     key=lambda item: item[1][-1])}\n                _vb(\"  sortedbyI={}.format(sortedbyI)\")\n                # There is presumably a pythonic way to do this...\n                retval = list()\n                i = 0\n                for k,v in sortedbyI.items():\n                    retval.append((k, phidict[k]))  # sname, phi\n                    i += 1\n                    if i >= scnt:\n                        return retval\n                raise RuntimeError(f\"bad scnt={scnt} in _dispatch_list;\"\n                                   f\" len(sortedbyI)={len(sortedbyI)}\")\n            else:\n                # Not doing round robin\n                # k is sname\n                tosort = [(k, -max(self.dispatchrecord[k][-1][0], self.shelf_life-1), phidict[k])\\\n                          for k in self.dispatchrecord.keys()]\n                sortedlist = sorted(tosort, key=lambda element: (element[1], element[2]))\n                retval = [(sortedlist[k][0], sortedlist[k][2]) for k in range(scnt)]\n                # TBD: See if there were enough w/negative phi values and warn.\n                # TBD: see if shelf-life is hitting and warn\n                return retval",
  "def remove_None(d):\n    if d is None:\n        return {}\n    d_copy = {}\n    for (key,value) in d.items():\n        if value is not None:\n            d_copy[key] = value\n    return d_copy",
  "class MMWConfidenceIntervals():\n    \"\"\"Takes a model and options as input. \n    Args:\n        refmodel (str): path of the model we use (e.g. farmer, uc)\n        cfg (Config): useful options to run amalgamator or xhat_eval, \n                        including EF_solver_options and EF_solver_name\n                        May include the options used to compute xhat\n        xhat_one (dict): Non-anticipative first stage solution, computed before\n        num_batches (int): Number of batches used to compute the MMW estimator\n        batch_size (int): Size of MMW batches, default None. \n                If batch_size is None, then batch_size=options['num_scens'] is used\n        start (int): first scenario used to run the MMW estimator, \n                    default None\n                If start is None, then start=options[num_scens]+options[start] is used\n    \n    Note:\n        cfg can include the following things:\n            -The type of our solving. for now, MMWci only support EF, so it must\n            have an attribute 'EF-2stage' or 'EF-mstage' set equal to True\n            - Solver-related options ('EF_solver_name' and 'EF_solver_options')\n            \n    \n    \"\"\"\n    def __init__(self,\n                 refmodel,\n                 cfg,\n                 xhat_one,\n                 num_batches,\n                 batch_size=None,\n                 start=None,\n                 verbose=True,\n                 mpicomm=None\n                 ):\n        self.refmodel = importlib.import_module(refmodel)\n        self.refmodelname = refmodel\n        self.cfg = cfg\n        self.xhat_one = xhat_one\n        self.num_batches = num_batches\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.mpicomm=mpicomm\n\n        #Getting the start\n        if start is None :\n            raise RuntimeError( \"Start must be specified\")\n        self.start = start\n            \n        #Type of our problem\n        if ama._bool_option(cfg, \"EF_2stage\"):\n            self.type = \"EF_2stage\"\n            self.multistage = False\n            self.numstages = 2\n        elif ama._bool_option(cfg, \"EF_mstage\"):\n            self.type = \"EF_mstage\"\n            self.multistage = True\n            self.numstages = len(cfg['branching_factors'])+1\n        else:\n            raise RuntimeError(\"Only EF is currently supported; \"\n                \"cfg should have an attribute 'EF-2stage' or 'EF-mstage' set to True\")\n        \n        #Check if refmodel and args have all needed attributes\n        everything = [\"scenario_names_creator\", \"scenario_creator\", \"kw_creator\"]\n        if self.multistage:\n            everything[0] = \"sample_tree_scen_creator\"\n    \n        you_can_have_it_all = True\n        for ething in everything:\n            if not hasattr(self.refmodel, ething):\n                print(f\"Module {refmodel} is missing {ething}\")\n                you_can_have_it_all = False\n        if not you_can_have_it_all:\n            raise RuntimeError(f\"Module {refmodel} not complete for MMW\")\n        \n        if \"EF_solver_name\" not in self.cfg:\n            raise RuntimeError(\"EF_solver_name not in Argument list for MMW\")\n\n    def run(self, confidence_level=0.95):\n\n        # We get the MMW right term, then xhat, then the MMW left term.\n\n        #Compute the nonant xhat (the one used in the left term of MMW (9) ) using\n        #                                                        the first scenarios\n        \n        ############### get the parameters\n        start = self.start\n        scenario_denouement = self.refmodel.scenario_denouement\n\n        #Introducing batches otpions\n        num_batches = self.num_batches\n        batch_size = self.batch_size\n        sample_cfg = self.cfg()  # ephemeral changes\n        \n        #Some options are specific to 2-stage or multi-stage problems\n        if self.multistage:\n            sampling_branching_factors = ciutils.branching_factors_from_numscens(batch_size,self.numstages)\n            #TODO: Change this to get a more logical way to compute branching_factors\n            batch_size = np.prod(sampling_branching_factors)\n        else:\n            if batch_size == 0:\n                raise RuntimeError(\"batch size can't be zero for two stage problems\")\n        sample_cfg.quick_assign('num_scens', int, batch_size)\n        sample_cfg.quick_assign('_mpisppy_probability', float, 1/batch_size)\n\n        #Solver settings\n        solver_name = self.cfg['EF_solver_name']\n        solver_options = self.cfg.get('EF_solver_options')\n        solver_options = remove_None(solver_options)\n            \n        #Now we compute for each batch the whole Gn term from MMW (9)\n\n        G = np.zeros(num_batches) #the Gbar of MMW (10)\n        #we will compute the mean via a loop (to be parallelized ?)\n\n        for i in range(num_batches) :\n            scenstart = None if self.multistage else start\n            gap_options = {'seed':start,'branching_factors':sampling_branching_factors} if self.multistage else None\n            scenario_names = self.refmodel.scenario_names_creator(batch_size,start=scenstart)\n            estim = ciutils.gap_estimators(self.xhat_one, self.refmodelname,\n                                           solving_type=self.type,\n                                           scenario_names=scenario_names,\n                                           sample_options=gap_options,\n                                           ArRP=1,\n                                           cfg=sample_cfg,\n                                           scenario_denouement=scenario_denouement,\n                                           solver_name=solver_name,\n                                           solver_options=solver_options,\n                                           mpicomm=self.mpicomm\n                                           )\n            Gn = estim['G']\n            start = estim['seed']\n\n            #objective_gap removed Sept.29th 2022\n\n            if(self.verbose):\n                global_toc(f\"Gn={Gn} for the batch {i}\")  # Left term of LHS of (9)\n            G[i]=Gn \n\n        s_g = np.std(G) #Standard deviation of gap\n\n        Gbar = np.mean(G)\n\n        t_g = scipy.stats.t.ppf(confidence_level,num_batches-1)\n\n        epsilon_g = t_g*s_g/np.sqrt(num_batches)\n\n        gap_inner_bound =  Gbar + epsilon_g\n        gap_outer_bound = 0\n \n        self.result={\"gap_inner_bound\": gap_inner_bound,\n                      \"gap_outer_bound\": gap_outer_bound,\n                      \"Gbar\": Gbar,\n                      \"std\": s_g,\n                      \"Glist\": G}\n        \n        return(self.result)",
  "def __init__(self,\n                 refmodel,\n                 cfg,\n                 xhat_one,\n                 num_batches,\n                 batch_size=None,\n                 start=None,\n                 verbose=True,\n                 mpicomm=None\n                 ):\n        self.refmodel = importlib.import_module(refmodel)\n        self.refmodelname = refmodel\n        self.cfg = cfg\n        self.xhat_one = xhat_one\n        self.num_batches = num_batches\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.mpicomm=mpicomm\n\n        #Getting the start\n        if start is None :\n            raise RuntimeError( \"Start must be specified\")\n        self.start = start\n            \n        #Type of our problem\n        if ama._bool_option(cfg, \"EF_2stage\"):\n            self.type = \"EF_2stage\"\n            self.multistage = False\n            self.numstages = 2\n        elif ama._bool_option(cfg, \"EF_mstage\"):\n            self.type = \"EF_mstage\"\n            self.multistage = True\n            self.numstages = len(cfg['branching_factors'])+1\n        else:\n            raise RuntimeError(\"Only EF is currently supported; \"\n                \"cfg should have an attribute 'EF-2stage' or 'EF-mstage' set to True\")\n        \n        #Check if refmodel and args have all needed attributes\n        everything = [\"scenario_names_creator\", \"scenario_creator\", \"kw_creator\"]\n        if self.multistage:\n            everything[0] = \"sample_tree_scen_creator\"\n    \n        you_can_have_it_all = True\n        for ething in everything:\n            if not hasattr(self.refmodel, ething):\n                print(f\"Module {refmodel} is missing {ething}\")\n                you_can_have_it_all = False\n        if not you_can_have_it_all:\n            raise RuntimeError(f\"Module {refmodel} not complete for MMW\")\n        \n        if \"EF_solver_name\" not in self.cfg:\n            raise RuntimeError(\"EF_solver_name not in Argument list for MMW\")",
  "def run(self, confidence_level=0.95):\n\n        # We get the MMW right term, then xhat, then the MMW left term.\n\n        #Compute the nonant xhat (the one used in the left term of MMW (9) ) using\n        #                                                        the first scenarios\n        \n        ############### get the parameters\n        start = self.start\n        scenario_denouement = self.refmodel.scenario_denouement\n\n        #Introducing batches otpions\n        num_batches = self.num_batches\n        batch_size = self.batch_size\n        sample_cfg = self.cfg()  # ephemeral changes\n        \n        #Some options are specific to 2-stage or multi-stage problems\n        if self.multistage:\n            sampling_branching_factors = ciutils.branching_factors_from_numscens(batch_size,self.numstages)\n            #TODO: Change this to get a more logical way to compute branching_factors\n            batch_size = np.prod(sampling_branching_factors)\n        else:\n            if batch_size == 0:\n                raise RuntimeError(\"batch size can't be zero for two stage problems\")\n        sample_cfg.quick_assign('num_scens', int, batch_size)\n        sample_cfg.quick_assign('_mpisppy_probability', float, 1/batch_size)\n\n        #Solver settings\n        solver_name = self.cfg['EF_solver_name']\n        solver_options = self.cfg.get('EF_solver_options')\n        solver_options = remove_None(solver_options)\n            \n        #Now we compute for each batch the whole Gn term from MMW (9)\n\n        G = np.zeros(num_batches) #the Gbar of MMW (10)\n        #we will compute the mean via a loop (to be parallelized ?)\n\n        for i in range(num_batches) :\n            scenstart = None if self.multistage else start\n            gap_options = {'seed':start,'branching_factors':sampling_branching_factors} if self.multistage else None\n            scenario_names = self.refmodel.scenario_names_creator(batch_size,start=scenstart)\n            estim = ciutils.gap_estimators(self.xhat_one, self.refmodelname,\n                                           solving_type=self.type,\n                                           scenario_names=scenario_names,\n                                           sample_options=gap_options,\n                                           ArRP=1,\n                                           cfg=sample_cfg,\n                                           scenario_denouement=scenario_denouement,\n                                           solver_name=solver_name,\n                                           solver_options=solver_options,\n                                           mpicomm=self.mpicomm\n                                           )\n            Gn = estim['G']\n            start = estim['seed']\n\n            #objective_gap removed Sept.29th 2022\n\n            if(self.verbose):\n                global_toc(f\"Gn={Gn} for the batch {i}\")  # Left term of LHS of (9)\n            G[i]=Gn \n\n        s_g = np.std(G) #Standard deviation of gap\n\n        Gbar = np.mean(G)\n\n        t_g = scipy.stats.t.ppf(confidence_level,num_batches-1)\n\n        epsilon_g = t_g*s_g/np.sqrt(num_batches)\n\n        gap_inner_bound =  Gbar + epsilon_g\n        gap_outer_bound = 0\n \n        self.result={\"gap_inner_bound\": gap_inner_bound,\n                      \"gap_outer_bound\": gap_outer_bound,\n                      \"Gbar\": Gbar,\n                      \"std\": s_g,\n                      \"Glist\": G}\n        \n        return(self.result)",
  "def evaluate_sample_trees(xhat_one, \n                          num_samples,\n                          cfg,\n                          InitSeed=0,  \n                          model_module = None):\n    \"\"\" Create and evaluate multiple sampled trees.\n    Args:\n        xhat_one : list or np.array of float (*not* a dict)\n            A feasible and nonanticipative first stage solution.\n        num_samples (int): number of trees to sample\n        cfg (Config): options for/from the amalgamator\n        InitSeed (int): starting seed (but might be used for a scenario name offset)\n        model_modules: an imported module with the functions needed by, e.g., amalgamator\n    Returns:\n        zhats (list as np.array): the objective functions\n        seed (int): the updated seed or offset for scenario name sampling        \n    \"\"\"\n    ''' creates batch_size sample trees with first-stage solution xhat_one\n    using SampleSubtree class from sample_tree\n    used to approximate E_{xi_2} phi(x_1, xi_2) for confidence interval coverage experiments\n    '''\n    cfg = cfg()  # so the seed can be ephemeral\n    mname = cfg.model_module_name\n    seed = InitSeed\n    zhats = list()\n    bfs = cfg[\"branching_factors\"]\n    scenario_count = np.prod(bfs)\n    solver_name = cfg[\"EF_solver_name\"]\n    #sampling_bfs = ciutils.scalable_BFs(batch_size, bfs) # use for variance?\n    xhat_eval_options = {\"iter0_solver_options\": None,\n                     \"iterk_solver_options\": None,\n                     \"display_timing\": False,\n                     \"solver_name\": solver_name,\n                     \"verbose\": False,\n                     \"solver_options\":{}}\n\n    cfg.dict_assign('seed', 'seed', int, None, seed)\n\n    for j in range(num_samples): # number of sample trees to create\n        samp_tree = sample_tree.SampleSubtree(mname,\n                                              xhats = [],\n                                              root_scen=None,\n                                              starting_stage=1, \n                                              branching_factors=bfs,\n                                              seed=seed, \n                                              cfg=cfg,\n                                              solver_name=solver_name,\n                                              solver_options={})\n        samp_tree.run()\n        ama_object = samp_tree.ama\n        cfg = ama_object.cfg\n        scenario_creator_kwargs = ama_object.kwargs\n        if len(samp_tree.ef._ef_scenario_names)>1:\n            local_scenarios = {sname: getattr(samp_tree.ef, sname)\n                               for sname in samp_tree.ef._ef_scenario_names}\n        else:\n            local_scenarios = {samp_tree.ef._ef_scenario_names[0]:samp_tree.ef}\n\n        xhats, seed = sample_tree.walking_tree_xhats(mname,\n                                                     local_scenarios,\n                                                     xhat_one,\n                                                     bfs,\n                                                     seed,\n                                                     cfg,\n                                                     solver_name=solver_name,\n                                                     solver_options=None)\n        # for Xhat_Eval\n        # scenario_creator_kwargs = ama_object.kwargs\n        scenario_names = ama_object.scenario_names\n        all_nodenames = sputils.create_nodenames_from_branching_factors(bfs)\n\n        # Evaluate objective value of feasible policy for this tree\n        ev = Xhat_Eval(xhat_eval_options,\n                        scenario_names,\n                        ama_object.scenario_creator,\n                        model_module.scenario_denouement,\n                        scenario_creator_kwargs=scenario_creator_kwargs,\n                        all_nodenames=all_nodenames)\n\n        zhats.append(ev.evaluate(xhats))\n\n        seed += scenario_count\n\n    return np.array(zhats), seed",
  "def run_samples(cfg, model_module):\n    # Does all the work for zhat4xhat    \n\n    # Read xhats from xhatpath\n    xhat_one = ciutils.read_xhat(cfg.xhatpath)[\"ROOT\"]\n\n    num_samples = cfg.num_samples\n\n    zhats,seed = evaluate_sample_trees(xhat_one, num_samples,\n                                       cfg, InitSeed=0,\n                                       model_module=model_module)\n\n    confidence_level = cfg.confidence_level\n    zhatbar = np.mean(zhats)\n    s_zhat = np.std(np.array(zhats))\n    t_zhat = scipy.stats.t.ppf(confidence_level, len(zhats)-1)\n    eps_z = t_zhat*s_zhat/np.sqrt(len(zhats))\n\n    print('zhatbar: ', zhatbar)\n    print('estimate: ', [zhatbar-eps_z, zhatbar+eps_z])\n    print('confidence_level', confidence_level)\n\n    return zhatbar, eps_z",
  "def _parser_setup():\n    # set up the config object and return it, but don't parse\n    # parsers for the non-model-specific arguments; but the model_module_name will be pulled off first\n    cfg = config.Config()\n    cfg.add_to_config(\"EF_solver_name\",\n                         description=\"solver name (default gurobi_direct)\",\n                         domain=str,\n                         default=\"gurobi_direct\")\n    cfg.add_branching_factors()\n    cfg.add_to_config(\"num_samples\",\n                         description=\"Number of independent sample trees to construct\",\n                         domain=int,\n                         default=10)\n    cfg.add_to_config(\"solver_options\",\n                         description=\"space separated string of solver options, e.g. 'option1=value1 option2 = value2'\",\n                         domain=str,\n                         default='')\n    \n    cfg.add_to_config(\"xhatpath\",\n                         description=\"path to npy file with xhat\",\n                         domain=str,\n                         default='')\n    \n    cfg.add_to_config(\"confidence_level\",\n                         description=\"one minus alpha (default 0.95)\",\n                         domain=float,\n                         default=0.95)\n    return cfg",
  "def _main_body(model_module, cfg):\n    # body of main, pulled out for testing\n\n    lcfg = cfg()  # make a copy because of EF-mstage\n    solver_options_dict = option_string_to_dict(cfg.solver_options)\n    lcfg.add_and_assign(\"EF_solver_options\", \"solver options dict\", dict, None, solver_options_dict)\n    \n    lcfg.quick_assign(\"EF_mstage\", domain=bool, value=True)\n\n    return run_samples(lcfg, model_module)",
  "def _prime_factors(n):\n    \"\"\"\n    Parameters\n    ----------\n    nin (int): postive integer to factor\n\n    Returns:\n    Returns a dictionary containing the prime factors of n as keys\n    and their respective multiplicities as values.\n    \"\"\"\n    if n < 0:\n        raise ValueError(f\"_prime_factors require positive input ({n})\")\n    if n == 0:\n        return {0: 1}\n    elif n == 1:\n        return {}\n\n    retval = {}\n    # collect the 2's\n    while (n % 2 == 0):\n        retval[2] = retval.get(2, 0) + 1\n        n /= 2\n    # traverse the odds\n    i = 3\n    while i <= math.sqrt(n):\n        while n % i == 0:\n            retval[i] = retval.get(i, 0) + 1\n            n /= i\n        i += 2\n    if n > 2:\n        retval[n] = retval.get(n, 0) + 1\n    return retval",
  "def branching_factors_from_numscens(numscens,num_stages):\n    \"\"\" Create branching factors to be used for sampling a tree.\n    Since most use cases are balanced trees, for now, we do not create unbalanced trees. \n    If numscens cannot be written as the product of a num_stages branching factors,\n    we take numscens <-numscens+1\n    \n    Parameters\n    -----------\n    numscens : int\n        Number of leaf nodes/scenarios.\n    num_stages: int\n        Number of stages in the tree\n\n    Returns\n    --------\n    branching_factors: list of int\n        The branching factors for approximately numscens\n    \"\"\"\n    if num_stages == 2:\n        return None\n    else:\n        for i in range(2**(num_stages-1)):\n            n = numscens+i\n            prime_fact = _prime_factors(n)\n            if sum(prime_fact.values())>=num_stages-1: #Checking that we have enough factors\n                branching_factors = [0]*(num_stages-1)\n                fact_list = [factor for (factor,mult) in prime_fact.items() for i in range(mult) ]\n                for k in range(num_stages-1):\n                    branching_factors[k] = np.prod([fact_list[(num_stages-1)*i+k] for i in range(1+len(fact_list)//(num_stages-1)) if (num_stages-1)*i+k<len(fact_list)])\n                return branching_factors\n        raise RuntimeError(\"branching_factors_from_numscens is not working correctly.\")",
  "def scalable_branching_factors(numscens, ref_branching_factors):\n    '''\n    This utilitary find a good branching factor list to create a scenario tree\n    containing at least numscens leaf nodes, and scaled like ref_branching_factors.\n    For instance, if numscens=233 and ref_branching_factors=[5,3,2], it returns [10,6,4], \n    branching factors for a 240-leafs tree\n    \n    NOTE: This method increasing in priority first stages branching factors,\n          so that a branching factor is an increasing function of numscens\n\n    Parameters\n    ----------\n    numscens : int\n        Number of leaf nodes/scenarios of the tree.\n    ref_branching_factors : list of int\n        Reference shape of the branching factors. It length must be equal to\n        number_of_stages-1\n\n    Returns\n    -------\n    new_branching_factors: list of int\n        The branching factors for approximately numscens that \"looks like\" the reference\n\n    '''\n    numstages = len(ref_branching_factors)+1\n    if numscens < 2**(numstages-1):\n        return [2]*(numstages-1)\n    mult_coef = (numscens/np.prod(ref_branching_factors))**(1/(numstages-1))\n    # branching_factors have to be positive integers\n    new_branching_factors = np.maximum(np.floor(np.array(ref_branching_factors)*mult_coef),1.) \n    i=0\n    while np.prod(new_branching_factors)<numscens:\n        if i == numstages-1:\n            raise RuntimeError(\"scalable branching_factors is failing\")\n        new_branching_factors[i]+=1\n        i+=1\n    new_branching_factors = list(new_branching_factors.astype(int))\n    return new_branching_factors",
  "def is_sorted(nodelist):\n    #Take a list of scenario_tree.ScenarioNode and check that it is well constructed\n    parent=None\n    for (t,node) in enumerate(nodelist):\n        if (t+1 != node.stage) or (node.parent_name != parent):\n            raise RuntimeError(\"The node list is not well-constructed\"\n                               f\"The stage {node.stage} node is the {t+1}th element of the list.\"\n                               f\"The node {node.name} has a parent named {node.parent_name}, but is right after the node {parent}\")\n        parent = node.name",
  "def writetxt_xhat(xhat,path=\"xhat.txt\",num_stages=2):\n    if num_stages ==2:\n        np.savetxt(path,xhat['ROOT'])\n    else:\n        raise RuntimeError(\"Only 2-stage is suported to write/read xhat to a file\")",
  "def readtxt_xhat(path=\"xhat.txt\",num_stages=2,delete_file=False):\n    if num_stages==2:\n        xhat = {'ROOT': np.loadtxt(path)}\n    else:\n        raise RuntimeError(\"Only 2-stage is suported to write/read xhat to a file\")\n    if delete_file and global_rank ==0:\n        os.remove(path)        \n    return(xhat)",
  "def write_xhat(xhat,path=\"xhat.npy\",num_stages=2):\n    if num_stages==2:\n        np.save(path,xhat['ROOT'])\n    else:\n        raise RuntimeError(\"Only 2-stage is suported to write/read xhat to a file\")",
  "def read_xhat(path=\"xhat.npy\",num_stages=2,delete_file=False):\n    if num_stages==2:\n        xhat = {'ROOT': np.load(path)}\n    else:\n        raise RuntimeError(\"Only 2-stage is suported to write/read xhat to a file\")\n    if delete_file and global_rank ==0:\n        os.remove(path)\n    return(xhat)",
  "def _fct_check(module, fct):\n    if not hasattr(module, fct):\n        raise RuntimeError(f\"pyomo_opt_sense needs the module to have the {fct} function\")",
  "def pyomo_opt_sense(module_name, cfg):\n    \"\"\" update cfg to have the optimization sense\"\"\"\n    module = importlib.import_module(module_name)\n    _fct_check(module, \"scenario_names_creator\")\n    sn = module.scenario_names_creator(1)  # an arbitrary scenario name\n    _fct_check(module, \"kw_creator\")\n    kw = module.kw_creator(cfg)\n    m = module.scenario_creator(sn[0], **kw)\n    objs = sputils.get_objs(m)\n    if objs[0].is_minimizing:\n        cfg.quick_assign(\"pyomo_opt_sense\", int, pyo.minimize)\n    else:\n        cfg.quick_assign(\"pyomo_opt_sense\", int, pyo.maximize)",
  "def correcting_numeric(G, cfg, relative_error=True, threshold=1e-4, objfct=None):\n    #Correcting small negative G due to numerical error while solving EF\n    sense = cfg.get(\"pyo_opt_sense\", pyo.minimize)  # 1 is minimize, -1 max\n    assert sense == 1 or sense == -1\n    if relative_error:\n        crit = threshold*np.abs(objfct)\n    else:\n        crit = threshold\n    if objfct is None:\n        raise RuntimeError(\"We need a value of the objective function to remove numerically small G\")\n    elif sense == pyo.minimize and G <= -crit:\n        print(f\"WARNING: The gap estimator is the wrong sign: {G}\")\n        return G\n    elif sense == pyo.maximize and G >= crit:\n        print(f\"WARNING: The gap estimator is the wrong sign: {G}\")\n        return G\n    else:\n        if sense == pyo.minimize:\n            return max(0, G)\n        else:\n            return min(0, G)",
  "def gap_estimators(xhat_one,\n                   mname, \n                   solving_type=\"EF_2stage\",\n                   scenario_names=None,\n                   sample_options=None,\n                   ArRP=1,\n                   cfg=None,   # was: options; before that: scenario_creator_kwargs={}\n                   scenario_denouement=None,\n                   solver_name=None, \n                   solver_options=None,\n                   verbose=False,\n                   mpicomm=None,\n                   ):\n    ''' Given a xhat, scenario names, a scenario creator and options, \n    gap_estimators creates a scenario tree and the associatd estimators \n    G and s from \u00a72 of [bm2011].\n    Returns G and s evaluated at xhat.\n    If ArRP>1, G and s are pooled, from a number ArRP of estimators,\n        computed with different scenario trees.\n    \n\n    Parameters\n    ----------\n    xhat_one : dict\n        A candidate first stage solution\n    mname: str\n        Name of the reference model, e.g. 'mpisppy.tests.examples.farmer'.\n    solving_type: str, optional\n        The way we solve the approximate problem. Can be \"EF_2stage\" (default)\n        or \"EF_mstage\".\n    scenario_names: list, optional\n        List of scenario names used to compute G_n and s_n. Default is None\n        Must be specified for 2 stage, but can be missing for multistage\n    sample_options: dict, optional\n        Only for multistage. Must contain a 'seed' and a 'branching_factors' attribute,\n        specifying the starting seed and the branching factors \n        of the scenario tree\n    ArRP:int,optional\n        Number of batches (we create a ArRP model). Default is 1 (one batch).\n    cfg: Config, not really optional\n        Additional arguments for scenario_creator. Default is {}\n    scenario_denouement: function, optional\n        Function to run after scenario creation. Default is None.\n    solver_name : str, optional\n        Solver. Default is None\n    solver_options: dict, optional\n        Solving options. Default is None\n    verbose: bool, optional\n        Should it print the gap estimator ? Default is True\n\n    branching_factors: list, optional\n        Only for multistage. List of branching factors of the sample scenario tree.\n\n    Returns\n    -------\n    G_k and s_k, gap estimator and associated standard deviation estimator.\n\n    '''\n    global_toc(\"Enter gap_estimators\")\n    if solving_type not in [\"EF_2stage\",\"EF_mstage\"]:\n        print(f\"solving type=\", solving_type)\n        raise RuntimeError(\"Only EF solve for the approximate problem is supported yet.\")\n    else:\n        is_multi = (solving_type==\"EF_mstage\")\n    \n    m = importlib.import_module(mname)\n    ama.check_module_ama(m)\n    scenario_creator_kwargs=m.kw_creator(cfg)\n        \n    if is_multi:\n        try:\n            branching_factors = sample_options['branching_factors']\n            start = sample_options['seed']\n        except (TypeError,KeyError,RuntimeError):\n            raise RuntimeError('For multistage problems, sample_options must be a dict with branching_factors and seed attributes.')\n    else:\n        start = sputils.extract_num(scenario_names[0])\n    if ArRP>1: #Special case : ArRP, G and s are pooled from r>1 estimators.\n        if is_multi:\n            raise RuntimeError(\"Pooled estimators are not supported for multistage problems yet.\")\n        n = len(scenario_names)\n        if(n%ArRP != 0):\n            raise RuntimeWarning(\"You put as an input a number of scenarios\"+\\\n                                 f\" which is not a mutliple of {ArRP}.\")\n            n = n- n%ArRP\n        G =[]\n        s = []\n\n        for k in range(ArRP):\n            scennames = scenario_names[k*(n//ArRP):(k+1)*(n//ArRP)]\n            tmp = gap_estimators(xhat_one, mname,\n                                   solver_name=solver_name,\n                                   scenario_names=scennames, ArRP=1,\n                                   cfg=cfg,\n                                   scenario_denouement=scenario_denouement,\n                                   solver_options=solver_options,\n                                   solving_type=solving_type\n                                   )\n            G.append(tmp['G'])\n            s.append(tmp['s'])\n            global_toc(f\"ArRP {k} of {ArRP}\")\n\n        #Pooling\n        G = np.mean(G)\n        s = np.linalg.norm(s)/np.sqrt(n//ArRP)\n        return {\"G\": G, \"s\": s, \"seed\": start}\n    \n\n    #A1RP\n    \n    #We start by computing the optimal solution to the approximate problem induced by our scenarios\n    if is_multi:\n        #Sample a scenario tree: this is a subtree, but starting from stage 1\n        samp_tree = sample_tree.SampleSubtree(mname,\n                                              xhats =[],\n                                              root_scen=None,\n                                              starting_stage=1, \n                                              branching_factors=branching_factors,\n                                              seed=start, \n                                              cfg=cfg,\n                                              solver_name=solver_name,\n                                              solver_options=solver_options)\n        samp_tree.run()\n        start += sputils.number_of_nodes(branching_factors)\n        ama_object = samp_tree.ama\n    else:\n        #We use amalgamator to do it\n        num_scens = len(scenario_names)\n        ama_cfg = cfg()\n        ama_cfg.quick_assign(solving_type, bool, True)\n        ama_cfg.quick_assign(\"EF_solver_name\", str, solver_name)\n        solver_options_str= sputils.option_dict_to_string(solver_options)  # cfg need str\n        ama_cfg.quick_assign(\"EF_solver_options\", str, solver_options_str)\n        ama_cfg.quick_assign(\"num_scens\", int, num_scens)\n        ama_cfg.quick_assign(\"_mpisppy_probability\", float, 1/num_scens)\n        ama_cfg.quick_assign(\"start\", int, start)\n        ama_object = ama.from_module(mname, ama_cfg, use_command_line=False)\n        ama_object.scenario_names = scenario_names\n        ama_object.verbose = False\n        ama_object.run()\n        start += len(scenario_names)\n        \n    #Optimal solution of the approximate problem\n    zn_star = ama_object.best_outer_bound\n    #Associated policies\n    xstars = sputils.nonant_cache_from_ef(ama_object.ef)\n    \n    #Then, we evaluate the function value induced by the scenario at xstar.\n    \n    if is_multi:\n        # Find feasible policies (i.e. xhats) for every non-leaf nodes\n        if len(samp_tree.ef._ef_scenario_names)>1:\n            local_scenarios = {sname:getattr(samp_tree.ef,sname) for sname in samp_tree.ef._ef_scenario_names}\n        else:\n            local_scenarios = {samp_tree.ef._ef_scenario_names[0]:samp_tree.ef}\n            \n        xhats,start = sample_tree.walking_tree_xhats(mname,\n                                                    local_scenarios,\n                                                    xhat_one['ROOT'],\n                                                    branching_factors,\n                                                    start,\n                                                    cfg,\n                                                    solver_name=solver_name,\n                                                    solver_options=solver_options)\n        \n        #Compute then the average function value with this policy\n        scenario_creator_kwargs = samp_tree.ama.kwargs\n        all_nodenames = sputils.create_nodenames_from_branching_factors(branching_factors)\n    else:\n        #In a 2 stage problem, the only non-leaf is the ROOT node\n        xhats = xhat_one\n        all_nodenames = None\n    \n    xhat_eval_options = {\"iter0_solver_options\": None,\n                         \"iterk_solver_options\": None,\n                         \"display_timing\": False,\n                         \"solver_name\": solver_name,\n                         \"verbose\": False,\n                         \"solver_options\":solver_options}\n    ev = xhat_eval.Xhat_Eval(xhat_eval_options,\n                            scenario_names,\n                            ama_object.scenario_creator,\n                            scenario_denouement,\n                            scenario_creator_kwargs=scenario_creator_kwargs,\n                            all_nodenames = all_nodenames,mpicomm=mpicomm)\n    #Evaluating xhat and xstar and getting the value of the objective function \n    #for every (local) scenario\n    zn_hat=ev.evaluate(xhats)\n    objs_at_xhat = ev.objs_dict\n    zn_star=ev.evaluate(xstars)\n    objs_at_xstar = ev.objs_dict\n    \n    eval_scen_at_xhat = []\n    eval_scen_at_xstar = []\n    scen_probs = []\n    for k,s in ev.local_scenarios.items():\n        eval_scen_at_xhat.append(objs_at_xhat[k])\n        eval_scen_at_xstar.append(objs_at_xstar[k])\n        scen_probs.append(s._mpisppy_probability)\n    \n    scen_gaps = np.array(eval_scen_at_xhat)-np.array(eval_scen_at_xstar)\n    local_gap = np.dot(scen_gaps,scen_probs)\n    local_ssq = np.dot(scen_gaps**2,scen_probs)\n    local_prob_sqnorm = np.linalg.norm(scen_probs)**2\n    local_obj_at_xhat = np.dot(eval_scen_at_xhat,scen_probs)\n    local_estim = np.array([local_gap,local_ssq,local_prob_sqnorm,local_obj_at_xhat])\n    global_estim = np.zeros(4)\n    ev.mpicomm.Allreduce(local_estim, global_estim, op=mpi.SUM) \n    G,ssq, prob_sqnorm,obj_at_xhat = global_estim\n    if global_rank==0 and verbose:\n        print(f\"G = {G}\")\n    sample_var = (ssq - G**2)/(1-prob_sqnorm) #Unbiased sample variance\n    s = np.sqrt(sample_var)\n    \n    use_relative_error = (np.abs(zn_star)>1)\n    G = correcting_numeric(G,cfg,objfct=obj_at_xhat,\n                           relative_error=use_relative_error)\n  \n    #objective_gap removed Sept.29 2022\n    return {\"G\":G,\"s\":s,\"seed\":start}",
  "class SampleSubtree():\n    '''\n    The Sample Subtree class is generating a scenario tree such that every\n    scenario shares the same common nodes up to a starting stage t. These nodes\n    are the first t nodes of a given scenario 'root_scen'.\n    The aim of this class is to compute a feasible policy at stage t for 'root_scen'\n    We take as an argument xhats, feasible policies for stages 1 to t-1, and \n    fix the nonants up to stage t-1 to the values given by 'xhats' for every scenario.\n    \n    The run() method is solving directly the Extensive Form. \n    After callling the method, one can fetch the feasible policy for stage t \n    by taking the attribute xhat_at_stage.\n\n    Parameters\n    ----------\n    mname : str\n        name of the module used to sample.\n    xhats : list of lists\n        List of nonanticipative feasible policies for stages 1 to t-1 for root_scen.\n    root_scen : pyomo ConcreteModel\n        A scenario. Every scenario in the subtree shares the same nodes\n        for stages 1 to t with root_scen\n    starting_stage : int\n        Stage t>=1.\n    branching_factors : list of int\n        Branching factors for sample trees. \n        branching_factors[i] is the branching factor for stage i+2\n    seed : int\n        Seed to create scenarios.\n    cfg : Config\n       To create scenario creator arguments.\n    solver_name : str, optional\n        Solver name. The default is None.\n    solver_options : dict, optional\n        Solver options. The default is None.\n\n    '''\n    def __init__(self, mname, xhats, root_scen, starting_stage, branching_factors,\n                 seed, cfg, solver_name=None, solver_options=None):\n\n        self.refmodel = importlib.import_module(mname)\n        #Checking that refmodel has all the needed attributes\n        attrs = [\"sample_tree_scen_creator\",\"kw_creator\",\"scenario_names_creator\"]\n        for attr in attrs:\n            if not hasattr(self.refmodel, attr):\n                raise RuntimeError(f\"The construction of a sample subtree failed because the reference model has no {attr} function.\")\n        self.xhats = xhats\n        self.root_scen = root_scen\n        self.stage = starting_stage\n        self.sampling_branching_factors = branching_factors[(self.stage-1):]\n        self.original_branching_factors = branching_factors\n        self.numscens = np.prod(self.sampling_branching_factors)\n        self.seed = seed\n        self.cfg = cfg\n        self.solver_name = solver_name\n        self.solver_options = solver_options\n        \n        #Create an amalgamator object to solve the subtree problem\n        self._create_amalgamator()\n\n                \n    def _sample_creator(self, sname, **scenario_creator_kwargs):\n        '''\n        This method is similar to scenario_creator function, but for subtrees.\n        Given a scenario names and kwargs, it creates a scenario from our subtree\n        \n        WARNING: The multistage model (aka refmodel) must contain a \n        sample_tree_scen_creator function\n        \n        '''\n        # gymnastics because options get passed around through a variety of paths\n        s_c_k = scenario_creator_kwargs.copy()\n        if \"seed\" in scenario_creator_kwargs:\n            s_c_k.pop(\"seed\", None)  # we want control over the seed here\n\n        s = self.refmodel.sample_tree_scen_creator(sname,\n                                                   given_scenario=self.root_scen,\n                                                   stage=self.stage,\n                                                   sample_branching_factors=self.sampling_branching_factors,\n                                                   seed = self.seed,\n                                                   **s_c_k)\n        nlens = {node.name: len(node.nonant_vardata_list) \n                 for node in s._mpisppy_node_list}\n        \n        #Fixing the xhats\n        for k,ndn in enumerate(self.fixed_nodes):\n            node = s._mpisppy_node_list[k]\n            if len(self.xhats[k]) != nlens[ndn]:\n                raise RuntimeError(\"xhats does not have the right size \"\n                                   f\"for stage {k+1}, xhats has {len(self.xhats[k])} nonant variables, but should have {nlens[ndn]} of them\")\n            for i in range(nlens[ndn]):\n                #node.nonant_vardata_list[i].fix(self.xhats[k][i])\n                node.nonant_vardata_list[i].value = self.xhats[k][i]\n                node.nonant_vardata_list[i].fixed = True\n        return s\n        \n    def _create_amalgamator(self):\n        '''\n        This method attaches an Amalgamator object to a sample subtree.\n        Changes to self.cfg will be ephemeral.\n        \n        WARNING: sample_creator must be called before using for stages beyond 1\n        '''\n        self.fixed_nodes = [\"ROOT\"+\"_0\"*i for i in range(self.stage-1)]\n        self.scenario_creator = self._sample_creator\n\n        ###cfg = copy.deepcopy(self.cfg)\n        cfg = self.cfg()   # ephemeral\n        cfg.quick_assign('EF-mstage', domain=str, value=len(self.original_branching_factors) > 1)\n        cfg.quick_assign('EF-2stage', domain=str, value=len(self.original_branching_factors) <= 1)\n        cfg.quick_assign('EF_solver_name', domain=str, value=self.solver_name)\n        if self.solver_options is not None:\n            cfg.add_and_assign(\"solver_options\", \"solver options dict\", dict, None, self.solver_options)\n        cfg.quick_assign('num_scens', domain=int, value=self.numscens)\n        if \"start_seed\" not in cfg:\n            cfg.add_and_assign(\"start_seed\", description=\"first seed\", domain=int, default=None, value=self.seed)\n        #### cfg['_mpisppy_probability'] = 1/self.numscens #Probably not used\n        # TBD: better options flow (Dec 2021; working on it June 2022)\n        if \"branching_factors\" not in cfg:\n            raise RuntimeError(\"branching_factors is not in cfg\")\n            \"\"\"\n            if \"kwargs\" in ama_options:\n                ama_options[\"branching_factors\"] = ama_options[\"kwargs\"][\"branching_factors\"]\n            \"\"\"\n        scen_names = self.refmodel.scenario_names_creator(self.numscens,\n                                                          start=self.seed)\n        denouement = self.refmodel.scenario_denouement if hasattr(self.refmodel, 'scenario_denouement') else None\n        \n        self.ama = amalgamator.Amalgamator(cfg, scen_names,\n                                           self.scenario_creator,\n                                           self.refmodel.kw_creator,\n                                           denouement,\n                                           verbose = False)\n    def run(self):\n        #Running the Amalgamator and attaching the result to the SampleSubtree object\n        #global_toc(\"Enter SampleSubTree run\")\n        self.ama.run()\n        self.ef = self.ama.ef\n        self.EF_Obj = self.ama.EF_Obj\n        pseudo_root = \"ROOT\"+\"_0\"*(self.stage-1)\n        self.xhat_at_stage =[self.ef.ref_vars[(pseudo_root,i)].value for i in range(self.ef._nlens[pseudo_root])]",
  "def _feasible_solution(mname,scenario,xhat_one,branching_factors,seed,cfg,\n                       solver_name=None, solver_options=None):\n    '''\n    Given a scenario and a first-stage policy xhat_one, this method computes\n    non-anticipative feasible policies for the following stages.\n\n    '''\n    assert solver_name is not None\n    if xhat_one is None:\n        raise RuntimeError(\"Xhat_one can't be None for now\")\n    ciutils.is_sorted(scenario._mpisppy_node_list)\n    nodenames = [node.name for node in scenario._mpisppy_node_list]\n    num_stages = len(branching_factors)+1\n    xhats = [xhat_one]\n    for t in range(2,num_stages): #We do not compute xhat for the final stage\n    \n        subtree = SampleSubtree(mname, xhats, scenario, \n                                t, branching_factors, seed, cfg,\n                                solver_name, solver_options)\n        subtree.run()\n        xhats.append(subtree.xhat_at_stage)\n        seed+=sputils.number_of_nodes(branching_factors[(t-1):])\n    xhat_dict = {ndn:xhat for (ndn,xhat) in zip(nodenames,xhats)}\n    return xhat_dict,seed",
  "def walking_tree_xhats(mname, local_scenarios, xhat_one,branching_factors, seed, cfg,\n                       solver_name=None, solver_options=None):\n    \"\"\"\n    This methods takes a scenario tree (represented by a scenario list) as an input, \n    a first stage policy xhat_one and several settings, and computes \n    a feasible policy for every scenario, i.e. finds nonanticipative xhats \n    using the SampleSubtree class.\n    We use a tree traversal approach, so that for every non-leaf node of\n    the scenario tree we compute an associated sample tree only once.\n\n    Parameters\n    ----------\n    mname : str\n        name of the module used to sample.\n    local_scenarios : dict of pyomo.ConcreteModel\n        Scenarios forming the scenario tree.\n    xhat_one : list or np.array of float\n        A feasible and nonanticipative first stage policy.\n    branching_factors : list of int\n        Branching factors for sample trees. \n        branching_factors[i] is the branching factor for stage i+2.\n    seed : int\n        Starting seed to create scenarios.\n    cfg : Config\n        Parameters\n\n    Returns\n    -------\n    xhats : dict\n        Dict of values for the nonanticipative variable for every node.\n        keys are node names and values are lists of nonant variables.\n        \n    NOTE: The local_scenarios do not need to form a regular tree (unbalanced trees are authorized)\n\n    \"\"\"\n    assert xhat_one is not None, \"Xhat_one can't be None for now\"\n        \n    xhats = {'ROOT':xhat_one}\n    \n    #Special case if we only have one scenario\n    if len(local_scenarios)==1:\n        scen = list(local_scenarios.values())[0]\n        res = _feasible_solution(mname, scen, xhat_one, branching_factors, seed, cfg,\n                                solver_name=solver_name,\n                                solver_options=solver_options)\n        return res\n        \n    \n    for k,s in local_scenarios.items():\n        scen_xhats = []\n        ciutils.is_sorted(s._mpisppy_node_list)\n        for node in s._mpisppy_node_list:\n            if node.name in xhats:\n               scen_xhats.append(xhats[node.name])\n            else:\n               subtree = SampleSubtree(mname, scen_xhats, s, \n                                       node.stage, branching_factors, seed, cfg,\n                                       solver_name, solver_options)\n               subtree.run()\n               xhat = subtree.xhat_at_stage\n\n               # TBD: is this needed\n               seed += sputils.number_of_nodes(branching_factors[(node.stage-1):])\n               \n               xhats[node.name] = xhat\n               scen_xhats.append(xhat)\n\n    return xhats, seed",
  "def __init__(self, mname, xhats, root_scen, starting_stage, branching_factors,\n                 seed, cfg, solver_name=None, solver_options=None):\n\n        self.refmodel = importlib.import_module(mname)\n        #Checking that refmodel has all the needed attributes\n        attrs = [\"sample_tree_scen_creator\",\"kw_creator\",\"scenario_names_creator\"]\n        for attr in attrs:\n            if not hasattr(self.refmodel, attr):\n                raise RuntimeError(f\"The construction of a sample subtree failed because the reference model has no {attr} function.\")\n        self.xhats = xhats\n        self.root_scen = root_scen\n        self.stage = starting_stage\n        self.sampling_branching_factors = branching_factors[(self.stage-1):]\n        self.original_branching_factors = branching_factors\n        self.numscens = np.prod(self.sampling_branching_factors)\n        self.seed = seed\n        self.cfg = cfg\n        self.solver_name = solver_name\n        self.solver_options = solver_options\n        \n        #Create an amalgamator object to solve the subtree problem\n        self._create_amalgamator()",
  "def _sample_creator(self, sname, **scenario_creator_kwargs):\n        '''\n        This method is similar to scenario_creator function, but for subtrees.\n        Given a scenario names and kwargs, it creates a scenario from our subtree\n        \n        WARNING: The multistage model (aka refmodel) must contain a \n        sample_tree_scen_creator function\n        \n        '''\n        # gymnastics because options get passed around through a variety of paths\n        s_c_k = scenario_creator_kwargs.copy()\n        if \"seed\" in scenario_creator_kwargs:\n            s_c_k.pop(\"seed\", None)  # we want control over the seed here\n\n        s = self.refmodel.sample_tree_scen_creator(sname,\n                                                   given_scenario=self.root_scen,\n                                                   stage=self.stage,\n                                                   sample_branching_factors=self.sampling_branching_factors,\n                                                   seed = self.seed,\n                                                   **s_c_k)\n        nlens = {node.name: len(node.nonant_vardata_list) \n                 for node in s._mpisppy_node_list}\n        \n        #Fixing the xhats\n        for k,ndn in enumerate(self.fixed_nodes):\n            node = s._mpisppy_node_list[k]\n            if len(self.xhats[k]) != nlens[ndn]:\n                raise RuntimeError(\"xhats does not have the right size \"\n                                   f\"for stage {k+1}, xhats has {len(self.xhats[k])} nonant variables, but should have {nlens[ndn]} of them\")\n            for i in range(nlens[ndn]):\n                #node.nonant_vardata_list[i].fix(self.xhats[k][i])\n                node.nonant_vardata_list[i].value = self.xhats[k][i]\n                node.nonant_vardata_list[i].fixed = True\n        return s",
  "def _create_amalgamator(self):\n        '''\n        This method attaches an Amalgamator object to a sample subtree.\n        Changes to self.cfg will be ephemeral.\n        \n        WARNING: sample_creator must be called before using for stages beyond 1\n        '''\n        self.fixed_nodes = [\"ROOT\"+\"_0\"*i for i in range(self.stage-1)]\n        self.scenario_creator = self._sample_creator\n\n        ###cfg = copy.deepcopy(self.cfg)\n        cfg = self.cfg()   # ephemeral\n        cfg.quick_assign('EF-mstage', domain=str, value=len(self.original_branching_factors) > 1)\n        cfg.quick_assign('EF-2stage', domain=str, value=len(self.original_branching_factors) <= 1)\n        cfg.quick_assign('EF_solver_name', domain=str, value=self.solver_name)\n        if self.solver_options is not None:\n            cfg.add_and_assign(\"solver_options\", \"solver options dict\", dict, None, self.solver_options)\n        cfg.quick_assign('num_scens', domain=int, value=self.numscens)\n        if \"start_seed\" not in cfg:\n            cfg.add_and_assign(\"start_seed\", description=\"first seed\", domain=int, default=None, value=self.seed)\n        #### cfg['_mpisppy_probability'] = 1/self.numscens #Probably not used\n        # TBD: better options flow (Dec 2021; working on it June 2022)\n        if \"branching_factors\" not in cfg:\n            raise RuntimeError(\"branching_factors is not in cfg\")\n            \"\"\"\n            if \"kwargs\" in ama_options:\n                ama_options[\"branching_factors\"] = ama_options[\"kwargs\"][\"branching_factors\"]\n            \"\"\"\n        scen_names = self.refmodel.scenario_names_creator(self.numscens,\n                                                          start=self.seed)\n        denouement = self.refmodel.scenario_denouement if hasattr(self.refmodel, 'scenario_denouement') else None\n        \n        self.ama = amalgamator.Amalgamator(cfg, scen_names,\n                                           self.scenario_creator,\n                                           self.refmodel.kw_creator,\n                                           denouement,\n                                           verbose = False)",
  "def run(self):\n        #Running the Amalgamator and attaching the result to the SampleSubtree object\n        #global_toc(\"Enter SampleSubTree run\")\n        self.ama.run()\n        self.ef = self.ama.ef\n        self.EF_Obj = self.ama.EF_Obj\n        pseudo_root = \"ROOT\"+\"_0\"*(self.stage-1)\n        self.xhat_at_stage =[self.ef.ref_vars[(pseudo_root,i)].value for i in range(self.ef._nlens[pseudo_root])]",
  "def confidence_config(cfg):\n\n    cfg.add_to_config(\"confidence_level\",\n                  description=\"1-alpha (default 0.95)\",\n                  domain=float,\n                  default=0.95)",
  "def sequential_config(cfg):\n\n    cfg.add_to_config(\"sample_size_ratio\",\n                  description=\"xhat/gap sample size ratio (default 1)\",\n                  domain=float,\n                  default=1.0)\n\n    cfg.add_to_config(\"ArRP\",\n                  description=\"How many to pool to comute G and s (default 1)\",\n                  domain=int,\n                  default=1)\n\n    cfg.add_to_config(\"kf_GS\",\n                  description=\"Resampling frequence for CI estimators (default 1)\",\n                  domain=int,\n                  default=1)\n\n    cfg.add_to_config(\"kf_xhat\",\n                  description=\"Resampling frequence for xhat (default 1)\",\n                  domain=int,\n                  default=1)",
  "def BM_config(cfg):\n    # Bayraksan and Morton sequential (relative width)\n\n    cfg.add_to_config(\"BM_h\",\n                  description=\"Controls width of confidence interval (default 1.75)\",\n                  domain=float,\n                  default=1.75)\n\n    cfg.add_to_config(\"BM_hprime\",\n                  description=\"Controls tradeoff between width and sample size (default 0.5)\",\n                  domain=float,\n                  default=0.5)\n    \n    cfg.add_to_config(\"BM_eps\",\n                  description=\"Controls termination (default 0.2)\",\n                  domain=float,\n                  default=0.2)\n    \n    cfg.add_to_config(\"BM_eps_prime\",\n                  description=\"Controls termination (default 0.1)\",\n                  domain=float,\n                  default=0.1)\n    \n    cfg.add_to_config(\"BM_p\",\n                  description=\"Controls sample size (default 0.1)\",\n                  domain=float,\n                  default=0.1)\n\n    cfg.add_to_config(\"BM_q\",\n                  description=\"Related to sample size growth (default 1.2)\",\n                  domain=float,\n                  default=1.2)",
  "def BPL_config(cfg):\n    # Bayraksan and Pierre-Louis\n\n    cfg.add_to_config(\"BPL_eps\",\n                  description=\"Controls termination (default 1)\",\n                  domain=float,\n                  default=1)\n\n    cfg.add_to_config(\"BPL_c0\",\n                  description=\"Starting sample size (default 20)\",\n                  domain=int,\n                  default=20)\n\n    cfg.add_to_config(\"BPL_n0min\",\n                  description=\"Non-zero implies stochastic sampling (default 0)\",\n                  domain=int,\n                  default=0)",
  "def is_needed(cfg, needed_things, message=\"\"):\n    absent = list()\n    for i in needed_things:\n        if i not in cfg:\n            absent.append(i)\n            print(f\"{i =}, {hasattr(cfg,i) =}\")\n    if len(absent) > 0:\n        raise RuntimeError(\"Some options are missing from this list of required options:\\n\"\n                           f\"{needed_things}\\n\"\n                           f\"missing: {absent}\\n\"\n                           f\"{message}\")",
  "def add_options(cfg, optional_things):\n    # allow for defaults on options that Bayraksan et al establish \n    for i,v  in optional_things.items():\n        if not i in cfg:\n            # there must be a better way...\n            if isinstance(v, str):\n                cfg.quick_assign(i, str, v)\n            elif isinstance(v, int):\n                cfg.quick_assign(i, int, v)\n            elif isinstance(v, float):\n                cfg.quick_assign(i, float, v)\n            elif isinstance(v, bool):\n                cfg.quick_assign(i, bool, v)\n            elif isinstance(v, dict):\n                cfg.quick_assign(i, dict, v)\n            else:\n                raise RuntimeError(f\"add_options cannot process type {type(v)} for option {i}:{v}\")",
  "def xhat_generator_farmer(scenario_names, solver_name=None, solver_options=None, crops_multiplier=1):\n    ''' For developer testing: Given scenario names and\n    options, create the scenarios and compute the xhat that is minimizing the\n    approximate problem associated with these scenarios.\n\n    Parameters\n    ----------\n    scenario_names: int\n        Names of the scenario we use\n    solver_name: str, optional\n        Name of the solver used. The default is \"gurobi\".\n    solver_options: dict, optional\n        Solving options. The default is None.\n    crops_multiplier: int, optional\n        A parameter of the farmer model. The default is 1.\n\n    Returns\n    -------\n    xhat: xhat object (dict containing a 'ROOT' key with a np.array)\n        A generated xhat.\n\n    NOTE: this is here for testing during development.\n\n    '''\n    num_scens = len(scenario_names)\n    \n    cfg = config.Config()\n    cfg.quick_assign(\"EF_2stage\", bool, True)\n    cfg.quick_assign(\"EF_solver_name\", str, solver_name)\n    cfg.quick_assign(\"EF_solver_options\", dict, solver_options)\n    cfg.quick_assign(\"num_scens\", int, num_scens)\n    cfg.quick_assign(\"_mpisppy_probability\", float, 1/num_scens)\n\n    #We use from_module to build easily an Amalgamator object\n    ama = amalgamator.from_module(\"mpisppy.tests.examples.farmer\",\n                                  cfg, use_command_line=False)\n    #Correcting the building by putting the right scenarios.\n    ama.scenario_names = scenario_names\n    ama.run()\n    \n    # get the xhat\n    xhat = sputils.nonant_cache_from_ef(ama.ef)\n\n    return xhat",
  "class SeqSampling():\n    \"\"\"\n    Computing a solution xhat and a confidence interval for the optimality gap sequentially,\n    by taking an increasing number of scenarios.\n    \n    Args:\n        refmodel (str): path of the model we use (e.g. farmer, uc)\n        xhat_generator (function): a function that takes scenario_names (and \n                                    and optional solver_name and solver_options) \n                                    as input and returns a first stage policy \n                                    xhat.\n\n        cfg (Config): multiple parameters, e.g.:\n                        - \"solver_name\", str, the name of the solver we use\n                        - \"solver_options\", dict containing solver options \n                            (default is {}, an empty dict)\n                        - \"sample_size_ratio\", float, the ratio (xhat sample size)/(gap estimators sample size)\n                            (default is 1)\n                        - \"xhat_gen_kwargs\" dict containing options passed to the xhat generator\n                            (default is {}, an empty dict)\n                        - \"ArRP\", int, how many estimators should be pooled to compute G and s ?\n                            (default is 1, no pooling)\n                        - \"kf_Gs\", int, resampling frequency to compute estimators\n                            (default is 1, always resample completely)\n                        - \"kf_xhat\", int, resampling frequency to compute xhat\n                            (default is 1, always resample completely)\n                        -\"confidence_level\", float, asymptotic confidence level \n                            of the output confidence interval\n                            (default is 0.95)\n                        -Some other parameters, depending on what model \n                            (BM or BPL, deterministic or sequential sampling)\n                        \n        stochastic_sampling (bool, default False):  should we compute sample sizes using estimators ?\n            if stochastic_sampling is True, we compute sample size using \u00a75 of [Bayraksan and Pierre-Louis]\n            else, we compute them using [Bayraksan and Morton] technique\n        stopping_criterion (str, default 'BM'): which stopping criterion should be used ?\n            2 criterions are supported : 'BM' for [Bayraksan and Morton] and 'BPL' for [Bayraksan and Pierre-Louis]\n        solving_type (str, default 'EF-2stage'): how do we solve the approximate problems ?\n            Must be one of 'EF-2stage' and 'EF-mstage' (for problems with more than 2 stages).\n            Solving methods outside EF are not supported yet.\n    \"\"\"\n    \n    def __init__(self,\n                 refmodel,\n                 xhat_generator,\n                 cfg,\n                 stochastic_sampling = False,\n                 stopping_criterion = \"BM\",\n                 solving_type = \"None\"):\n        \n        if not isinstance(cfg, config.Config):\n            raise RuntimeError(f\"SeqSampling bad cfg type={type(cfg)}; should be Config\")\n        self.refmodel = importlib.import_module(refmodel)\n        self.refmodelname = refmodel\n        self.xhat_generator = xhat_generator\n        self.cfg = cfg\n        self.stochastic_sampling = stochastic_sampling\n        self.stopping_criterion = stopping_criterion\n        self.solving_type = solving_type\n        self.sample_size_ratio = cfg.get(\"sample_size_ratio\", 1)\n        self.xhat_gen_kwargs = cfg.get(\"xhat_gen_kwargs\", {})\n        \n        #Check if refmodel has all needed attributes\n        everything = [\"scenario_names_creator\",\n                 \"scenario_creator\",\n                 \"kw_creator\"]  # denouement can be missing.\n        you_can_have_it_all = True\n        for ething in everything:\n            if not hasattr(self.refmodel, ething):\n                print(f\"Module {refmodel} is missing {ething}\")\n                you_can_have_it_all = False\n        if not you_can_have_it_all:\n            raise RuntimeError(f\"Module {refmodel} not complete for seqsampling\")\n\n        \"\"\" delete this on or after July 14, 2022\n        #Manage options\n        optional_options = {\"ArRP\": 1,\n                            \"kf_Gs\": 1,\n                            \"kf_xhat\": 1,\n                            \"confidence_level\": 0.95}\n        add_options(cfg, optional_options)\n        \"\"\"        \n\n        if self.stochastic_sampling :\n                add_options(options, [\"n0min\"], [50])\n                \n                \n        if self.stopping_criterion == \"BM\":\n            needed_things = [\"BM_eps_prime\",\"BM_hprime\",\"BM_eps\",\"BM_h\",\"BM_p\"]\n            is_needed(cfg, needed_things)\n        elif self.stopping_criterion == \"BPL\":\n            is_needed(cfg, [\"BPL_eps\"])\n            if not self.stochastic_sampling :\n                # The Pyomo config object cannot take a function directly\n                optional_things = {\"BPL_c1\":2, \"functions_dict\": {\"growth_function\":(lambda x : x-1)}}\n                add_options(cfg, optional_things)\n        else:\n            raise RuntimeError(\"Only BM and BPL criteria are supported at this time.\")\n\n        # TBD: do a better job with options\n        for oname in cfg:\n            setattr(self, oname, cfg[oname]) # Set every option as an attribute\n        sroot, self.solver_name, self.solver_options = solver_spec.solver_specification(cfg, [\"EF\",\"\"])\n        assert self.solver_name is not None\n        \n        #Check the solving_type, and find if the problem is multistage\n        two_stage_types = ['EF_2stage']\n        multistage_types = ['EF_mstage']\n        if self.solving_type in two_stage_types:\n            self.multistage = False\n        elif self.solving_type in multistage_types:\n            self.multistage = True\n        else:\n            raise RuntimeError(f\"The solving_type {self.solving_type} is not supported. \"\n                               f\"If you want to run a 2-stage problem, please use a solving_type in {two_stage_types}. \"\n                               f\"If you want to run a multistage stage problem, please use a solving_type in {multistage_types}\")\n        \n        #Check the multistage options\n        if self.multistage:\n            needed_things = [\"branching_factors\"]\n            is_needed(cfg, needed_things)\n            # check resampling frequencies\n            if not hasattr(cfg, 'kf_Gs') or cfg['kf_Gs'] != 1:\n                print(\"kf_Gs must be 1 for multi-stage, so assigning 1\")\n                cfg.quick_assign(\"kf_Gs\", int, 1)\n            if not hasattr(cfg, 'kf_xhat') or cfg['kf_xhat'] != 1:\n                print(\"kf_Gs must be 1 for multi-stage, so assigning 1\")\n                cfg.quick_assign(\"kf_xhat\", int, 1)\n        \n        #Get the stopping criterion\n        if self.stopping_criterion == \"BM\":\n            self.stop_criterion = self.bm_stopping_criterion\n        elif self.stopping_criterion == \"BPL\":\n            self.stop_criterion = self.bpl_stopping_criterion\n        else:\n            raise RuntimeError(\"Only BM and BPL criteria are supported.\")\n            \n        #Get the function computing sample size\n        if self.stochastic_sampling:\n            self.sample_size = self.stochastic_sampsize\n        elif self.stopping_criterion == \"BM\":\n            self.sample_size = self.bm_sampsize\n        elif self.stopping_criterion == \"BPL\":\n            self.sample_size = self.bpl_fsp_sampsize\n        else:\n            raise RuntimeError(\"Only BM and BPL sample sizes are supported yet\")\n        \n        #To be sure to always use new scenarios, we set a ScenCount that is \n        #telling us how many scenarios has been used so far\n        self.ScenCount = 0\n        \n        #If we are running a multistage problem, we also need a seed count\n        self.SeedCount = 0\n            \n            \n    def bm_stopping_criterion(self,G,s,nk):\n        # arguments defined in [bm2011]\n        return(G>self.BM_hprime*s+self.BM_eps_prime)\n    \n    def bpl_stopping_criterion(self,G,s,nk):\n        # arguments defined in [bpl2012]\n        t = scipy.stats.t.ppf(self.confidence_level,nk-1)\n        sample_error = t*s/np.sqrt(nk)\n        inflation_factor = 1/np.sqrt(nk)\n        return(G+sample_error+inflation_factor>self.BPL_eps)\n    \n    def bm_sampsize(self,k,G,s,nk_m1, r=2):\n        # arguments defined in [bm2011]\n        h = self.BM_h\n        hprime = self.BM_hprime\n        p = self.BM_p\n        q = self.BM_q\n        confidence_level = self.confidence_level\n        if q is None :\n            # Computing n_k as in (5) of [Bayraksan and Morton, 2009]\n            if hasattr(self, \"c\") :\n                c = self.c\n            else:\n                if confidence_level is None :\n                    raise RuntimeError(\"We need the confidence level to compute the constant cp\")\n                j = np.arange(1,1000)\n                s = sum(np.power(j,-p*np.log(j)))\n                c = max(1,2*np.log(s/(np.sqrt(2*np.pi)*(1-confidence_level))))\n            \n            lower_bound = (c+2*p* np.log(k)**2)/((h-hprime)**2)\n        else :\n            # Computing n_k as in (14) of [Bayraksan and Morton, 2009]\n            if hasattr(self, \"c\") :\n                c = self.c\n            else:\n                if confidence_level is None :\n                    RuntimeError(\"We need the confidence level to compute the constant c_pq\")\n                j = np.arange(1,1000)\n                s = sum(np.exp(-p*np.power(j,2*q/r)))\n                c = max(1,2*np.log(s/(np.sqrt(2*np.pi)*(1-confidence_level))))\n            \n            lower_bound = (c+2*p*np.power(k,2*q/r))/((h-hprime)**2)   \n        #print(f\"nk={lower_bound}\")\n        return int(np.ceil(lower_bound))\n    \n    def bpl_fsp_sampsize(self,k,G,s,nk_m1):\n        # arguments defined in [bpl2012]\n        return(int(np.ceil(self.BPL_c0+self.BPL_c1*self.functions_dict[\"growth_function\"](k))))\n        \n    def stochastic_sampsize(self,k,G,s,nk_m1):\n        # arguments defined in [bpl2012]\n        if (k==1):\n            #Initialization\n            return(int(np.ceil(max(self.BPL_n0min,np.log(1/self.BPL_eps)))))\n        #\u00a75 of [Bayraksan and Pierre-Louis] : solving a 2nd degree equation in sqrt(n)\n        t = scipy.stats.t.ppf(self.confidence_level,nk_m1-1)\n        a = - self.BPL_eps\n        b = 1+t*s\n        c = nk_m1*G\n        maxroot = -(np.sqrt(b**2-4*a*c)+b)/(2*a)\n        print(f\"s={s}, t={t}, G={G}\")\n        print(f\"a={a}, b={b},c={c},delta={b**2-4*a*c}\")\n        print(f\"At iteration {k}, we took n_k={int(np.ceil((maxroot**2)))}\")\n        return(int(np.ceil(maxroot**2)))\n    \n    \n    def run(self,maxit=200):\n        \"\"\" Execute a sequental sampling algorithm\n        Args:\n            maxit (int): override the stopping criteria based on iterations\n        Returns:\n            {\"T\":T,\"Candidate_solution\":final_xhat,\"CI\":CI,}\n        \"\"\"\n        if self.multistage:\n            raise RuntimeWarning(\"Multistage sequential sampling can be done \"\n                                 \"using the SeqSampling, but dependent samples\\n\"\n                                 \"will be used. The class IndepScens_SeqSampling uses independent samples and therefor has better theoretical support.\")\n        refmodel = self.refmodel\n        mult = self.sample_size_ratio # used to set m_k= mult*n_k\n        \n        \n        #----------------------------Step 0 -------------------------------------#\n        #Initialization\n        k =1\n        \n        \n        #Computing the lower bound for n_1\n\n\n        if self.stopping_criterion == \"BM\":\n            #Finding a constant used to compute nk\n            r = 2 #TODO : we could add flexibility here\n            j = np.arange(1,1000)\n            if self.BM_q is None:\n                s = sum(np.power(j,-self.BM_p*np.log(j)))\n            else:\n                if self.BM_q<1:\n                    raise RuntimeError(\"Parameter q should be greater than 1.\")\n                s = sum(np.exp(-self.BM_p*np.power(j,2*self.BM_q/r)))\n            self.c = max(1,2*np.log(s/(np.sqrt(2*np.pi)*(1-self.confidence_level))))\n                \n        lower_bound_k = self.sample_size(k, None, None, None)\n        \n        #Computing xhat_1.\n\n        #We use sample_size_ratio*n_k observations to compute xhat_k\n        if self.multistage:\n            xhat_branching_factors = ciutils.scalable_branching_factors(mult*lower_bound_k, self.cfg['branching_factors'])\n            mk = np.prod(xhat_branching_factors)\n            self.xhat_gen_kwargs['start_seed'] = self.SeedCount #TODO: Maybe find a better way to manage seed\n            xhat_scenario_names = refmodel.scenario_names_creator(mk)\n            \n        else:\n            mk = int(np.floor(mult*lower_bound_k))\n            xhat_scenario_names = refmodel.scenario_names_creator(mk, start=self.ScenCount)\n            self.ScenCount+=mk\n\n        xgo = self.xhat_gen_kwargs.copy()\n        xgo.pop(\"solver_name\", None)  # it will be given explicitly\n        xgo.pop(\"solver_options\", None)  # it will be given explicitly\n        xgo.pop(\"scenario_names\", None)  # given explicitly\n\n        xhat_k = self.xhat_generator(xhat_scenario_names,\n                                   solver_name=self.solver_name,\n                                   solver_options=self.solver_options,\n                                     **xgo)\n\n    \n        #----------------------------Step 1 -------------------------------------#\n        #Computing n_1 and associated scenario names\n        if self.multistage:\n            self.SeedCount += sputils.number_of_nodes(xhat_branching_factors)\n            \n            gap_branching_factors = ciutils.scalable_branching_factors(lower_bound_k, self.cfg['branching_factors'])\n            nk = np.prod(gap_branching_factors)\n            estimator_scenario_names = refmodel.scenario_names_creator(nk)\n            sample_options = {'branching_factors':gap_branching_factors, 'seed':self.SeedCount}\n        else:\n            nk = self.ArRP *int(np.ceil(lower_bound_k/self.ArRP))\n            estimator_scenario_names = refmodel.scenario_names_creator(nk,\n                                                                       start=self.ScenCount)\n            sample_options = None\n            self.ScenCount+= nk\n        \n        #Computing G_nkand s_k associated with xhat_1\n\n        lcfg = self.cfg()   # local copy\n        lcfg.quick_assign('num_scens', int, nk)\n        scenario_denouement = refmodel.scenario_denouement if hasattr(refmodel, \"scenario_denouement\") else None\n        estim = ciutils.gap_estimators(xhat_k, self.refmodelname,\n                                       solving_type=self.solving_type,\n                                       scenario_names=estimator_scenario_names,\n                                       sample_options=sample_options,\n                                       ArRP=self.ArRP,\n                                       cfg=lcfg,\n                                       scenario_denouement=scenario_denouement,\n                                       solver_name=self.solver_name,\n                                       solver_options=self.solver_options)\n        Gk,sk = estim['G'],estim['s']\n        if self.multistage:\n            self.SeedCount = estim['seed']\n        \n        #----------------------------Step 2 -------------------------------------#\n\n        while( self.stop_criterion(Gk,sk,nk) and k<maxit):\n        #----------------------------Step 3 -------------------------------------#       \n            k+=1\n            nk_m1 = nk #n_{k-1}\n            mk_m1 = mk\n            lower_bound_k = self.sample_size(k, Gk, sk, nk_m1)\n            \n            #Computing m_k and associated scenario names\n            if self.multistage:\n                xhat_branching_factors = ciutils.scalable_branching_factors(mult*lower_bound_k, lcfg['branching_factors'])\n                mk = np.prod(xhat_branching_factors)\n                self.xhat_gen_kwargs['start_seed'] = self.SeedCount #TODO: Maybe find a better way to manage seed\n                xhat_scenario_names = refmodel.scenario_names_creator(mk)\n            \n            else:\n                mk = int(np.floor(mult*lower_bound_k))\n                assert mk>= mk_m1, \"Our sample size should be increasing\"\n                if (k%self.kf_xhat==0):\n                    #We use only new scenarios to compute xhat\n                    xhat_scenario_names = refmodel.scenario_names_creator(int(mult*nk),\n                                                                          start=self.ScenCount)\n                    self.ScenCount+= mk\n                else:\n                    #We reuse the previous scenarios\n                    xhat_scenario_names+= refmodel.scenario_names_creator(mult*(nk-nk_m1),\n                                                                          start=self.ScenCount)\n                    self.ScenCount+= mk-mk_m1\n            \n            #Computing xhat_k\n            xgo = self.xhat_gen_kwargs.copy()\n            xgo.pop(\"solver_name\", None)  # it will be given explicitly\n            xgo.pop(\"solver_options\", None)  # it will be given explicitly\n            xgo.pop(\"scenario_names\", None)  # given explicitly\n            xhat_k = self.xhat_generator(xhat_scenario_names,\n                                         solver_name=self.solver_name,\n                                         solver_options=self.solver_options,\n                                         **xgo)\n            \n            #Computing n_k and associated scenario names\n            if self.multistage:\n                self.SeedCount += sputils.number_of_nodes(xhat_branching_factors)\n                \n                gap_branching_factors = ciutils.scalable_branching_factors(lower_bound_k, lcfg['branching_factors'])\n                nk = np.prod(gap_branching_factors)\n                estimator_scenario_names = refmodel.scenario_names_creator(nk)\n                sample_options = {'branching_factors':gap_branching_factors, 'seed':self.SeedCount}\n            else:\n                nk = self.ArRP *int(np.ceil(lower_bound_k/self.ArRP))\n                assert nk>= nk_m1, \"Our sample size should be increasing\"\n                if (k%self.kf_GS==0):\n                    #We use only new scenarios to compute gap estimators\n                    estimator_scenario_names = refmodel.scenario_names_creator(nk,\n                                                                               start=self.ScenCount)\n                    self.ScenCount+=nk\n                else:\n                    #We reuse the previous scenarios\n                    estimator_scenario_names+= refmodel.scenario_names_creator((nk-nk_m1),\n                                                                               start=self.ScenCount)\n                    self.ScenCount+= (nk-nk_m1)\n                sample_options = None\n            \n            \n            #Computing G_k and s_k\n            lcfg['num_scens'] = nk\n            estim = ciutils.gap_estimators(xhat_k, self.refmodelname,\n                                           solving_type=self.solving_type,\n                                           scenario_names=estimator_scenario_names,\n                                           sample_options=sample_options,\n                                           ArRP=self.ArRP,\n                                           cfg=lcfg,\n                                           scenario_denouement=scenario_denouement,\n                                           solver_name=self.solver_name,\n                                           solver_options=self.solver_options)\n            if self.multistage:\n                self.SeedCount = estim['seed']\n            Gk,sk = estim['G'],estim['s']\n\n            if (k%10==0) and global_rank==0:\n                print(f\"k={k}\")\n                print(f\"n_k={nk}\")\n                print(f\"G_k={Gk}\")\n                print(f\"s_k={sk}\")\n        #----------------------------Step 4 -------------------------------------#\n        if (k==maxit) :\n            raise RuntimeError(f\"The loop terminated after {maxit} iteration with no acceptable solution\")\n        T = k\n        final_xhat=xhat_k\n        if self.stopping_criterion == \"BM\":\n            upper_bound=self.BM_h*sk+self.BM_eps\n        elif self.stopping_criterion == \"BPL\":\n            upper_bound = self.BPL_eps\n        else:\n            raise RuntimeError(\"Only BM and BPL criterion are supported yet.\")\n        CI=[0,upper_bound]\n        global_toc(f\"G={Gk} sk={sk}; xhat has been computed with {nk*mult} observations.\")\n        return {\"T\":T,\"Candidate_solution\":final_xhat,\"CI\":CI,}",
  "def __init__(self,\n                 refmodel,\n                 xhat_generator,\n                 cfg,\n                 stochastic_sampling = False,\n                 stopping_criterion = \"BM\",\n                 solving_type = \"None\"):\n        \n        if not isinstance(cfg, config.Config):\n            raise RuntimeError(f\"SeqSampling bad cfg type={type(cfg)}; should be Config\")\n        self.refmodel = importlib.import_module(refmodel)\n        self.refmodelname = refmodel\n        self.xhat_generator = xhat_generator\n        self.cfg = cfg\n        self.stochastic_sampling = stochastic_sampling\n        self.stopping_criterion = stopping_criterion\n        self.solving_type = solving_type\n        self.sample_size_ratio = cfg.get(\"sample_size_ratio\", 1)\n        self.xhat_gen_kwargs = cfg.get(\"xhat_gen_kwargs\", {})\n        \n        #Check if refmodel has all needed attributes\n        everything = [\"scenario_names_creator\",\n                 \"scenario_creator\",\n                 \"kw_creator\"]  # denouement can be missing.\n        you_can_have_it_all = True\n        for ething in everything:\n            if not hasattr(self.refmodel, ething):\n                print(f\"Module {refmodel} is missing {ething}\")\n                you_can_have_it_all = False\n        if not you_can_have_it_all:\n            raise RuntimeError(f\"Module {refmodel} not complete for seqsampling\")\n\n        \"\"\" delete this on or after July 14, 2022\n        #Manage options\n        optional_options = {\"ArRP\": 1,\n                            \"kf_Gs\": 1,\n                            \"kf_xhat\": 1,\n                            \"confidence_level\": 0.95}\n        add_options(cfg, optional_options)\n        \"\"\"        \n\n        if self.stochastic_sampling :\n                add_options(options, [\"n0min\"], [50])\n                \n                \n        if self.stopping_criterion == \"BM\":\n            needed_things = [\"BM_eps_prime\",\"BM_hprime\",\"BM_eps\",\"BM_h\",\"BM_p\"]\n            is_needed(cfg, needed_things)\n        elif self.stopping_criterion == \"BPL\":\n            is_needed(cfg, [\"BPL_eps\"])\n            if not self.stochastic_sampling :\n                # The Pyomo config object cannot take a function directly\n                optional_things = {\"BPL_c1\":2, \"functions_dict\": {\"growth_function\":(lambda x : x-1)}}\n                add_options(cfg, optional_things)\n        else:\n            raise RuntimeError(\"Only BM and BPL criteria are supported at this time.\")\n\n        # TBD: do a better job with options\n        for oname in cfg:\n            setattr(self, oname, cfg[oname]) # Set every option as an attribute\n        sroot, self.solver_name, self.solver_options = solver_spec.solver_specification(cfg, [\"EF\",\"\"])\n        assert self.solver_name is not None\n        \n        #Check the solving_type, and find if the problem is multistage\n        two_stage_types = ['EF_2stage']\n        multistage_types = ['EF_mstage']\n        if self.solving_type in two_stage_types:\n            self.multistage = False\n        elif self.solving_type in multistage_types:\n            self.multistage = True\n        else:\n            raise RuntimeError(f\"The solving_type {self.solving_type} is not supported. \"\n                               f\"If you want to run a 2-stage problem, please use a solving_type in {two_stage_types}. \"\n                               f\"If you want to run a multistage stage problem, please use a solving_type in {multistage_types}\")\n        \n        #Check the multistage options\n        if self.multistage:\n            needed_things = [\"branching_factors\"]\n            is_needed(cfg, needed_things)\n            # check resampling frequencies\n            if not hasattr(cfg, 'kf_Gs') or cfg['kf_Gs'] != 1:\n                print(\"kf_Gs must be 1 for multi-stage, so assigning 1\")\n                cfg.quick_assign(\"kf_Gs\", int, 1)\n            if not hasattr(cfg, 'kf_xhat') or cfg['kf_xhat'] != 1:\n                print(\"kf_Gs must be 1 for multi-stage, so assigning 1\")\n                cfg.quick_assign(\"kf_xhat\", int, 1)\n        \n        #Get the stopping criterion\n        if self.stopping_criterion == \"BM\":\n            self.stop_criterion = self.bm_stopping_criterion\n        elif self.stopping_criterion == \"BPL\":\n            self.stop_criterion = self.bpl_stopping_criterion\n        else:\n            raise RuntimeError(\"Only BM and BPL criteria are supported.\")\n            \n        #Get the function computing sample size\n        if self.stochastic_sampling:\n            self.sample_size = self.stochastic_sampsize\n        elif self.stopping_criterion == \"BM\":\n            self.sample_size = self.bm_sampsize\n        elif self.stopping_criterion == \"BPL\":\n            self.sample_size = self.bpl_fsp_sampsize\n        else:\n            raise RuntimeError(\"Only BM and BPL sample sizes are supported yet\")\n        \n        #To be sure to always use new scenarios, we set a ScenCount that is \n        #telling us how many scenarios has been used so far\n        self.ScenCount = 0\n        \n        #If we are running a multistage problem, we also need a seed count\n        self.SeedCount = 0",
  "def bm_stopping_criterion(self,G,s,nk):\n        # arguments defined in [bm2011]\n        return(G>self.BM_hprime*s+self.BM_eps_prime)",
  "def bpl_stopping_criterion(self,G,s,nk):\n        # arguments defined in [bpl2012]\n        t = scipy.stats.t.ppf(self.confidence_level,nk-1)\n        sample_error = t*s/np.sqrt(nk)\n        inflation_factor = 1/np.sqrt(nk)\n        return(G+sample_error+inflation_factor>self.BPL_eps)",
  "def bm_sampsize(self,k,G,s,nk_m1, r=2):\n        # arguments defined in [bm2011]\n        h = self.BM_h\n        hprime = self.BM_hprime\n        p = self.BM_p\n        q = self.BM_q\n        confidence_level = self.confidence_level\n        if q is None :\n            # Computing n_k as in (5) of [Bayraksan and Morton, 2009]\n            if hasattr(self, \"c\") :\n                c = self.c\n            else:\n                if confidence_level is None :\n                    raise RuntimeError(\"We need the confidence level to compute the constant cp\")\n                j = np.arange(1,1000)\n                s = sum(np.power(j,-p*np.log(j)))\n                c = max(1,2*np.log(s/(np.sqrt(2*np.pi)*(1-confidence_level))))\n            \n            lower_bound = (c+2*p* np.log(k)**2)/((h-hprime)**2)\n        else :\n            # Computing n_k as in (14) of [Bayraksan and Morton, 2009]\n            if hasattr(self, \"c\") :\n                c = self.c\n            else:\n                if confidence_level is None :\n                    RuntimeError(\"We need the confidence level to compute the constant c_pq\")\n                j = np.arange(1,1000)\n                s = sum(np.exp(-p*np.power(j,2*q/r)))\n                c = max(1,2*np.log(s/(np.sqrt(2*np.pi)*(1-confidence_level))))\n            \n            lower_bound = (c+2*p*np.power(k,2*q/r))/((h-hprime)**2)   \n        #print(f\"nk={lower_bound}\")\n        return int(np.ceil(lower_bound))",
  "def bpl_fsp_sampsize(self,k,G,s,nk_m1):\n        # arguments defined in [bpl2012]\n        return(int(np.ceil(self.BPL_c0+self.BPL_c1*self.functions_dict[\"growth_function\"](k))))",
  "def stochastic_sampsize(self,k,G,s,nk_m1):\n        # arguments defined in [bpl2012]\n        if (k==1):\n            #Initialization\n            return(int(np.ceil(max(self.BPL_n0min,np.log(1/self.BPL_eps)))))\n        #\u00a75 of [Bayraksan and Pierre-Louis] : solving a 2nd degree equation in sqrt(n)\n        t = scipy.stats.t.ppf(self.confidence_level,nk_m1-1)\n        a = - self.BPL_eps\n        b = 1+t*s\n        c = nk_m1*G\n        maxroot = -(np.sqrt(b**2-4*a*c)+b)/(2*a)\n        print(f\"s={s}, t={t}, G={G}\")\n        print(f\"a={a}, b={b},c={c},delta={b**2-4*a*c}\")\n        print(f\"At iteration {k}, we took n_k={int(np.ceil((maxroot**2)))}\")\n        return(int(np.ceil(maxroot**2)))",
  "def run(self,maxit=200):\n        \"\"\" Execute a sequental sampling algorithm\n        Args:\n            maxit (int): override the stopping criteria based on iterations\n        Returns:\n            {\"T\":T,\"Candidate_solution\":final_xhat,\"CI\":CI,}\n        \"\"\"\n        if self.multistage:\n            raise RuntimeWarning(\"Multistage sequential sampling can be done \"\n                                 \"using the SeqSampling, but dependent samples\\n\"\n                                 \"will be used. The class IndepScens_SeqSampling uses independent samples and therefor has better theoretical support.\")\n        refmodel = self.refmodel\n        mult = self.sample_size_ratio # used to set m_k= mult*n_k\n        \n        \n        #----------------------------Step 0 -------------------------------------#\n        #Initialization\n        k =1\n        \n        \n        #Computing the lower bound for n_1\n\n\n        if self.stopping_criterion == \"BM\":\n            #Finding a constant used to compute nk\n            r = 2 #TODO : we could add flexibility here\n            j = np.arange(1,1000)\n            if self.BM_q is None:\n                s = sum(np.power(j,-self.BM_p*np.log(j)))\n            else:\n                if self.BM_q<1:\n                    raise RuntimeError(\"Parameter q should be greater than 1.\")\n                s = sum(np.exp(-self.BM_p*np.power(j,2*self.BM_q/r)))\n            self.c = max(1,2*np.log(s/(np.sqrt(2*np.pi)*(1-self.confidence_level))))\n                \n        lower_bound_k = self.sample_size(k, None, None, None)\n        \n        #Computing xhat_1.\n\n        #We use sample_size_ratio*n_k observations to compute xhat_k\n        if self.multistage:\n            xhat_branching_factors = ciutils.scalable_branching_factors(mult*lower_bound_k, self.cfg['branching_factors'])\n            mk = np.prod(xhat_branching_factors)\n            self.xhat_gen_kwargs['start_seed'] = self.SeedCount #TODO: Maybe find a better way to manage seed\n            xhat_scenario_names = refmodel.scenario_names_creator(mk)\n            \n        else:\n            mk = int(np.floor(mult*lower_bound_k))\n            xhat_scenario_names = refmodel.scenario_names_creator(mk, start=self.ScenCount)\n            self.ScenCount+=mk\n\n        xgo = self.xhat_gen_kwargs.copy()\n        xgo.pop(\"solver_name\", None)  # it will be given explicitly\n        xgo.pop(\"solver_options\", None)  # it will be given explicitly\n        xgo.pop(\"scenario_names\", None)  # given explicitly\n\n        xhat_k = self.xhat_generator(xhat_scenario_names,\n                                   solver_name=self.solver_name,\n                                   solver_options=self.solver_options,\n                                     **xgo)\n\n    \n        #----------------------------Step 1 -------------------------------------#\n        #Computing n_1 and associated scenario names\n        if self.multistage:\n            self.SeedCount += sputils.number_of_nodes(xhat_branching_factors)\n            \n            gap_branching_factors = ciutils.scalable_branching_factors(lower_bound_k, self.cfg['branching_factors'])\n            nk = np.prod(gap_branching_factors)\n            estimator_scenario_names = refmodel.scenario_names_creator(nk)\n            sample_options = {'branching_factors':gap_branching_factors, 'seed':self.SeedCount}\n        else:\n            nk = self.ArRP *int(np.ceil(lower_bound_k/self.ArRP))\n            estimator_scenario_names = refmodel.scenario_names_creator(nk,\n                                                                       start=self.ScenCount)\n            sample_options = None\n            self.ScenCount+= nk\n        \n        #Computing G_nkand s_k associated with xhat_1\n\n        lcfg = self.cfg()   # local copy\n        lcfg.quick_assign('num_scens', int, nk)\n        scenario_denouement = refmodel.scenario_denouement if hasattr(refmodel, \"scenario_denouement\") else None\n        estim = ciutils.gap_estimators(xhat_k, self.refmodelname,\n                                       solving_type=self.solving_type,\n                                       scenario_names=estimator_scenario_names,\n                                       sample_options=sample_options,\n                                       ArRP=self.ArRP,\n                                       cfg=lcfg,\n                                       scenario_denouement=scenario_denouement,\n                                       solver_name=self.solver_name,\n                                       solver_options=self.solver_options)\n        Gk,sk = estim['G'],estim['s']\n        if self.multistage:\n            self.SeedCount = estim['seed']\n        \n        #----------------------------Step 2 -------------------------------------#\n\n        while( self.stop_criterion(Gk,sk,nk) and k<maxit):\n        #----------------------------Step 3 -------------------------------------#       \n            k+=1\n            nk_m1 = nk #n_{k-1}\n            mk_m1 = mk\n            lower_bound_k = self.sample_size(k, Gk, sk, nk_m1)\n            \n            #Computing m_k and associated scenario names\n            if self.multistage:\n                xhat_branching_factors = ciutils.scalable_branching_factors(mult*lower_bound_k, lcfg['branching_factors'])\n                mk = np.prod(xhat_branching_factors)\n                self.xhat_gen_kwargs['start_seed'] = self.SeedCount #TODO: Maybe find a better way to manage seed\n                xhat_scenario_names = refmodel.scenario_names_creator(mk)\n            \n            else:\n                mk = int(np.floor(mult*lower_bound_k))\n                assert mk>= mk_m1, \"Our sample size should be increasing\"\n                if (k%self.kf_xhat==0):\n                    #We use only new scenarios to compute xhat\n                    xhat_scenario_names = refmodel.scenario_names_creator(int(mult*nk),\n                                                                          start=self.ScenCount)\n                    self.ScenCount+= mk\n                else:\n                    #We reuse the previous scenarios\n                    xhat_scenario_names+= refmodel.scenario_names_creator(mult*(nk-nk_m1),\n                                                                          start=self.ScenCount)\n                    self.ScenCount+= mk-mk_m1\n            \n            #Computing xhat_k\n            xgo = self.xhat_gen_kwargs.copy()\n            xgo.pop(\"solver_name\", None)  # it will be given explicitly\n            xgo.pop(\"solver_options\", None)  # it will be given explicitly\n            xgo.pop(\"scenario_names\", None)  # given explicitly\n            xhat_k = self.xhat_generator(xhat_scenario_names,\n                                         solver_name=self.solver_name,\n                                         solver_options=self.solver_options,\n                                         **xgo)\n            \n            #Computing n_k and associated scenario names\n            if self.multistage:\n                self.SeedCount += sputils.number_of_nodes(xhat_branching_factors)\n                \n                gap_branching_factors = ciutils.scalable_branching_factors(lower_bound_k, lcfg['branching_factors'])\n                nk = np.prod(gap_branching_factors)\n                estimator_scenario_names = refmodel.scenario_names_creator(nk)\n                sample_options = {'branching_factors':gap_branching_factors, 'seed':self.SeedCount}\n            else:\n                nk = self.ArRP *int(np.ceil(lower_bound_k/self.ArRP))\n                assert nk>= nk_m1, \"Our sample size should be increasing\"\n                if (k%self.kf_GS==0):\n                    #We use only new scenarios to compute gap estimators\n                    estimator_scenario_names = refmodel.scenario_names_creator(nk,\n                                                                               start=self.ScenCount)\n                    self.ScenCount+=nk\n                else:\n                    #We reuse the previous scenarios\n                    estimator_scenario_names+= refmodel.scenario_names_creator((nk-nk_m1),\n                                                                               start=self.ScenCount)\n                    self.ScenCount+= (nk-nk_m1)\n                sample_options = None\n            \n            \n            #Computing G_k and s_k\n            lcfg['num_scens'] = nk\n            estim = ciutils.gap_estimators(xhat_k, self.refmodelname,\n                                           solving_type=self.solving_type,\n                                           scenario_names=estimator_scenario_names,\n                                           sample_options=sample_options,\n                                           ArRP=self.ArRP,\n                                           cfg=lcfg,\n                                           scenario_denouement=scenario_denouement,\n                                           solver_name=self.solver_name,\n                                           solver_options=self.solver_options)\n            if self.multistage:\n                self.SeedCount = estim['seed']\n            Gk,sk = estim['G'],estim['s']\n\n            if (k%10==0) and global_rank==0:\n                print(f\"k={k}\")\n                print(f\"n_k={nk}\")\n                print(f\"G_k={Gk}\")\n                print(f\"s_k={sk}\")\n        #----------------------------Step 4 -------------------------------------#\n        if (k==maxit) :\n            raise RuntimeError(f\"The loop terminated after {maxit} iteration with no acceptable solution\")\n        T = k\n        final_xhat=xhat_k\n        if self.stopping_criterion == \"BM\":\n            upper_bound=self.BM_h*sk+self.BM_eps\n        elif self.stopping_criterion == \"BPL\":\n            upper_bound = self.BPL_eps\n        else:\n            raise RuntimeError(\"Only BM and BPL criterion are supported yet.\")\n        CI=[0,upper_bound]\n        global_toc(f\"G={Gk} sk={sk}; xhat has been computed with {nk*mult} observations.\")\n        return {\"T\":T,\"Candidate_solution\":final_xhat,\"CI\":CI,}",
  "class IndepScens_SeqSampling(SeqSampling):\n    def __init__(self,\n                 refmodel,\n                 xhat_generator,\n                 cfg,\n                 stopping_criterion = \"BM\",\n                 stochastic_sampling = False,\n                 solving_type=\"EF-mstage\",\n                 ):\n        super().__init__(\n                 refmodel,\n                 xhat_generator,\n                 cfg,\n                 stochastic_sampling = stochastic_sampling,\n                 stopping_criterion = stopping_criterion,\n                 solving_type = solving_type)\n        self.numstages = len(self.cfg['branching_factors'])+1\n        self.batch_branching_factors = [1]*(self.numstages-1)\n        self.batch_size = 1\n    \n    #TODO: Add an override specifier if it exists\n    def run(self, maxit=200):\n        # Do everything as specified by the options (maxit is provided as a safety net).\n        refmodel = self.refmodel\n        mult = self.sample_size_ratio # used to set m_k= mult*n_k\n        scenario_denouement = refmodel.scenario_denouement if hasattr(refmodel, \"scenario_denouement\") else None\n         #----------------------------Step 0 -------------------------------------#\n        #Initialization\n        k=1\n        \n        if self.stopping_criterion == \"BM\":\n            #Finding a constant used to compute nk\n            r = 2 #TODO : we could add flexibility here\n            j = np.arange(1,1000)\n            if self.BM_q is None:\n                s = sum(np.power(j,-self.BM_p*np.log(j)))\n            else:\n                if self.BM_q<1:\n                    raise RuntimeError(\"Parameter BM_q should be greater than 1.\")\n                s = sum(np.exp(-self.BM_p*np.power(j,2*self.BM_q/r)))\n            self.c = max(1,2*np.log(s/(np.sqrt(2*np.pi)*(1-self.confidence_level))))\n        \n        lower_bound_k = self.sample_size(k, None, None, None)\n        \n        #Computing xhat_1.\n        \n        #We use sample_size_ratio*n_k observations to compute xhat_k\n        xhat_branching_factors = ciutils.scalable_branching_factors(mult*lower_bound_k, self.cfg['branching_factors'])\n        mk = np.prod(xhat_branching_factors)\n        self.xhat_gen_kwargs['start_seed'] = self.SeedCount #TODO: Maybe find a better way to manage seed\n        xhat_scenario_names = refmodel.scenario_names_creator(mk)\n\n        xgo = self.xhat_gen_kwargs.copy()\n        xgo[\"solver_name\"] = self.solver_name\n        xgo.pop(\"solver_options\", None)  # it will be given explicitly\n        xgo.pop(\"scenario_names\", None)  # it will be given explicitly\n        xgo[\"branching_factors\"] = xhat_branching_factors\n        xhat_k = self.xhat_generator(xhat_scenario_names,\n                                   solver_options=self.solver_options,\n                                   **xgo)\n        self.SeedCount += sputils.number_of_nodes(xhat_branching_factors)\n\n        #----------------------------Step 1 -------------------------------------#\n        #Computing n_1 and associated scenario names\n        \n        nk = np.prod(ciutils.scalable_branching_factors(lower_bound_k, self.cfg['branching_factors'])) #To ensure the same growth that in the one-tree seqsampling\n        estimator_scenario_names = refmodel.scenario_names_creator(nk)\n        \n        #Computing G_nk and s_k associated with xhat_1\n        \n        Gk, sk = self._gap_estimators_with_independent_scenarios(xhat_k,\n                                                                nk,\n                                                                estimator_scenario_names,\n                                                                scenario_denouement)\n\n        \n        #----------------------------Step 2 -------------------------------------#\n\n        while( self.stop_criterion(Gk,sk,nk) and k<maxit):\n        #----------------------------Step 3 -------------------------------------#       \n            k+=1\n            nk_m1 = nk #n_{k-1}\n            mk_m1 = mk\n            lower_bound_k = self.sample_size(k, Gk, sk, nk_m1)\n            \n            #Computing m_k and associated scenario names\n            xhat_branching_factors = ciutils.scalable_branching_factors(mult*lower_bound_k, self.cfg['branching_factors'])\n            mk = np.prod(xhat_branching_factors)\n            self.xhat_gen_kwargs['start_seed'] = self.SeedCount #TODO: Maybe find a better way to manage seed\n            xhat_scenario_names = refmodel.scenario_names_creator(mk)\n            \n            #Computing xhat_k\n           \n            xgo = self.xhat_gen_kwargs.copy()\n            xgo.pop(\"solver_options\", None)  # it will be given explicitly\n            xgo.pop(\"scenario_names\", None)  # it will be given explicitly\n            xhat_k = self.xhat_generator(xhat_scenario_names,\n                                        solver_options=self.solver_options,\n                                         **xgo)\n            \n            #Computing n_k and associated scenario names\n            self.SeedCount += sputils.number_of_nodes(xhat_branching_factors)\n            \n            nk = np.prod(ciutils.scalable_branching_factors(lower_bound_k, self.cfg['branching_factors'])) #To ensure the same growth that in the one-tree seqsampling\n            nk += self.batch_size - nk%self.batch_size\n            estimator_scenario_names = refmodel.scenario_names_creator(nk)\n            \n            \n            Gk, sk = self._gap_estimators_with_independent_scenarios(xhat_k,nk,estimator_scenario_names,scenario_denouement)\n\n            if (k%10==0):\n                print(f\"k={k}\")\n                print(f\"n_k={nk}\")\n\n        #----------------------------Step 4 -------------------------------------#\n        if (k==maxit) :\n            raise RuntimeError(f\"The loop terminated after {maxit} iteration with no acceptable solution\")\n        T = k\n        final_xhat=xhat_k\n        if self.stopping_criterion == \"BM\":\n            upper_bound=self.BM_h*sk+self.BM_eps\n        elif self.stopping_criterion == \"BPL\":\n            upper_bound = self.BPL_eps\n        else:\n            raise RuntimeError(\"Only BM and BPL criterion are supported yet.\")\n        CI=[0,upper_bound]\n        global_toc(f\"G={Gk}\")\n        global_toc(f\"s={sk}\")\n        global_toc(f\"xhat has been computed with {nk*mult} observations.\")\n        return {\"T\":T,\"Candidate_solution\":final_xhat,\"CI\":CI,}\n\n    \"\"\"\n    def _independent_scenario_creator(self, sname, **scenario_creator_kwargs):\n                bfs = [1]*(self.numstages-1)\n                snum = sputils.extract_num(sname)\n                scenario_creator = self.refmodel.scenario_creator\n                return scenario_creator(sname,start_seed=self.SeedCount+snum*(self.numstages-1),**scenario_creator_kwargs)\n    \"\"\"\n\n    def _kw_creator_without_seed(self, cfg):\n        kwargs = self.refmodel.kw_creator(cfg)\n        kwargs.pop(\"start_seed\")\n        return kwargs\n\n\n    def _gap_estimators_with_independent_scenarios(self, xhat_k, nk,\n                                                  estimator_scenario_names, scenario_denouement):\n        \"\"\" Sample a scenario tree: this is a subtree, but starting from stage 1.\n        Args:\n            xhat_k (dict[nodename] of list): the solution to lead the walk\n            nk (int): number of scenarios,\n            estimator_scenario_names(list of str): scenario names\n            scenario_denouement (fct): called for each scenario at the end\n                 (TBD: drop this arg and just use the function in refmodel)\n        Returns:\n            Gk, Sk (float): mean and standard devation of the gap estimate\n        Note:\n            Seed management is mainly in the form of updates to SeedCount\n\n        \"\"\"\n        cfg = config.Config()\n        cfg.quick_assign(\"EF_mstage\", bool, True)\n        cfg.quick_assign(\"EF_solver_name\", str, self.solver_name)\n        cfg.quick_assign(\"EF_solver_options\", dict, self.solver_options)\n        cfg.quick_assign(\"num_scens\", int, nk)\n        cfg.quick_assign(\"_mpisppy_probability\", float, 1/nk)\n        cfg.quick_assign(\"start_seed\", int, self.SeedCount)        \n        \n        pseudo_branching_factors = [nk]+[1]*(self.numstages-2)\n        cfg.quick_assign('branching_factors', pyofig.ListOf(int), pseudo_branching_factors)\n        ama = amalgamator.Amalgamator(cfg, \n                                      scenario_names=estimator_scenario_names,\n                                      scenario_creator=self.refmodel.scenario_creator,\n                                      kw_creator=self.refmodel.kw_creator,\n                                      scenario_denouement=scenario_denouement)\n        ama.run()\n        #Optimal solution of the approximate problem\n        zstar = ama.best_outer_bound\n        #Associated policies\n        xstars = sputils.nonant_cache_from_ef(ama.ef)\n        scenario_creator_kwargs = ama.kwargs\n        # Find feasible policies (i.e. xhats) for every non-leaf nodes\n        local_scenarios = {sname:getattr(ama.ef,sname) for sname in ama.ef._ef_scenario_names}\n        xhats,start = sample_tree.walking_tree_xhats(self.refmodelname,\n                                                    local_scenarios,\n                                                    xhat_k['ROOT'],\n                                                    self.cfg['branching_factors'],  # psuedo\n                                                    self.SeedCount,\n                                                     self.cfg,\n                                                    solver_name=self.solver_name,\n                                                    solver_options=self.solver_options)\n        \n        #Compute then the average function value with this policy\n        all_nodenames = sputils.create_nodenames_from_branching_factors(pseudo_branching_factors)\n        xhat_eval_options = {\"iter0_solver_options\": None,\n                         \"iterk_solver_options\": None,\n                         \"display_timing\": False,\n                         \"solver_name\": self.solver_name,\n                         \"verbose\": False,\n                         \"solver_options\":self.solver_options}\n        ev = xhat_eval.Xhat_Eval(xhat_eval_options,\n                                estimator_scenario_names,\n                                self.refmodel.scenario_creator,\n                                scenario_denouement,\n                                scenario_creator_kwargs=scenario_creator_kwargs,\n                                all_nodenames = all_nodenames)\n        #Evaluating xhat and xstar and getting the value of the objective function \n        #for every (local) scenario\n        ev.evaluate(xhats)\n        objs_at_xhat = ev.objs_dict\n        ev.evaluate(xstars)\n        objs_at_xstar = ev.objs_dict\n        \n        eval_scen_at_xhat = []\n        eval_scen_at_xstar = []\n        scen_probs = []\n        for k,s in ev.local_scenarios.items():\n            eval_scen_at_xhat.append(objs_at_xhat[k])\n            eval_scen_at_xstar.append(objs_at_xstar[k])\n            scen_probs.append(s._mpisppy_probability)\n    \n        \n        scen_gaps = np.array(eval_scen_at_xhat)-np.array(eval_scen_at_xstar)\n        local_gap = np.dot(scen_gaps,scen_probs)\n        local_ssq = np.dot(scen_gaps**2,scen_probs)\n        local_prob_sqnorm = np.linalg.norm(scen_probs)**2\n        local_obj_at_xhat = np.dot(eval_scen_at_xhat,scen_probs)\n        local_estim = np.array([local_gap,local_ssq,local_prob_sqnorm,local_obj_at_xhat])\n        global_estim = np.zeros(4)\n        ev.mpicomm.Allreduce(local_estim, global_estim, op=mpi.SUM) \n        G,ssq, prob_sqnorm,obj_at_xhat = global_estim\n        if global_rank==0:\n            print(f\"G = {G}\")\n        sample_var = (ssq - G**2)/(1-prob_sqnorm) #Unbiased sample variance\n        sk = np.sqrt(sample_var)\n        \n        use_relative_error = (np.abs(zstar)>1)\n        Gk = ciutils.correcting_numeric(G,cfg,objfct=obj_at_xhat,\n                               relative_error=use_relative_error)\n        \n        self.SeedCount = start\n        \n        return Gk,sk",
  "def __init__(self,\n                 refmodel,\n                 xhat_generator,\n                 cfg,\n                 stopping_criterion = \"BM\",\n                 stochastic_sampling = False,\n                 solving_type=\"EF-mstage\",\n                 ):\n        super().__init__(\n                 refmodel,\n                 xhat_generator,\n                 cfg,\n                 stochastic_sampling = stochastic_sampling,\n                 stopping_criterion = stopping_criterion,\n                 solving_type = solving_type)\n        self.numstages = len(self.cfg['branching_factors'])+1\n        self.batch_branching_factors = [1]*(self.numstages-1)\n        self.batch_size = 1",
  "def run(self, maxit=200):\n        # Do everything as specified by the options (maxit is provided as a safety net).\n        refmodel = self.refmodel\n        mult = self.sample_size_ratio # used to set m_k= mult*n_k\n        scenario_denouement = refmodel.scenario_denouement if hasattr(refmodel, \"scenario_denouement\") else None\n         #----------------------------Step 0 -------------------------------------#\n        #Initialization\n        k=1\n        \n        if self.stopping_criterion == \"BM\":\n            #Finding a constant used to compute nk\n            r = 2 #TODO : we could add flexibility here\n            j = np.arange(1,1000)\n            if self.BM_q is None:\n                s = sum(np.power(j,-self.BM_p*np.log(j)))\n            else:\n                if self.BM_q<1:\n                    raise RuntimeError(\"Parameter BM_q should be greater than 1.\")\n                s = sum(np.exp(-self.BM_p*np.power(j,2*self.BM_q/r)))\n            self.c = max(1,2*np.log(s/(np.sqrt(2*np.pi)*(1-self.confidence_level))))\n        \n        lower_bound_k = self.sample_size(k, None, None, None)\n        \n        #Computing xhat_1.\n        \n        #We use sample_size_ratio*n_k observations to compute xhat_k\n        xhat_branching_factors = ciutils.scalable_branching_factors(mult*lower_bound_k, self.cfg['branching_factors'])\n        mk = np.prod(xhat_branching_factors)\n        self.xhat_gen_kwargs['start_seed'] = self.SeedCount #TODO: Maybe find a better way to manage seed\n        xhat_scenario_names = refmodel.scenario_names_creator(mk)\n\n        xgo = self.xhat_gen_kwargs.copy()\n        xgo[\"solver_name\"] = self.solver_name\n        xgo.pop(\"solver_options\", None)  # it will be given explicitly\n        xgo.pop(\"scenario_names\", None)  # it will be given explicitly\n        xgo[\"branching_factors\"] = xhat_branching_factors\n        xhat_k = self.xhat_generator(xhat_scenario_names,\n                                   solver_options=self.solver_options,\n                                   **xgo)\n        self.SeedCount += sputils.number_of_nodes(xhat_branching_factors)\n\n        #----------------------------Step 1 -------------------------------------#\n        #Computing n_1 and associated scenario names\n        \n        nk = np.prod(ciutils.scalable_branching_factors(lower_bound_k, self.cfg['branching_factors'])) #To ensure the same growth that in the one-tree seqsampling\n        estimator_scenario_names = refmodel.scenario_names_creator(nk)\n        \n        #Computing G_nk and s_k associated with xhat_1\n        \n        Gk, sk = self._gap_estimators_with_independent_scenarios(xhat_k,\n                                                                nk,\n                                                                estimator_scenario_names,\n                                                                scenario_denouement)\n\n        \n        #----------------------------Step 2 -------------------------------------#\n\n        while( self.stop_criterion(Gk,sk,nk) and k<maxit):\n        #----------------------------Step 3 -------------------------------------#       \n            k+=1\n            nk_m1 = nk #n_{k-1}\n            mk_m1 = mk\n            lower_bound_k = self.sample_size(k, Gk, sk, nk_m1)\n            \n            #Computing m_k and associated scenario names\n            xhat_branching_factors = ciutils.scalable_branching_factors(mult*lower_bound_k, self.cfg['branching_factors'])\n            mk = np.prod(xhat_branching_factors)\n            self.xhat_gen_kwargs['start_seed'] = self.SeedCount #TODO: Maybe find a better way to manage seed\n            xhat_scenario_names = refmodel.scenario_names_creator(mk)\n            \n            #Computing xhat_k\n           \n            xgo = self.xhat_gen_kwargs.copy()\n            xgo.pop(\"solver_options\", None)  # it will be given explicitly\n            xgo.pop(\"scenario_names\", None)  # it will be given explicitly\n            xhat_k = self.xhat_generator(xhat_scenario_names,\n                                        solver_options=self.solver_options,\n                                         **xgo)\n            \n            #Computing n_k and associated scenario names\n            self.SeedCount += sputils.number_of_nodes(xhat_branching_factors)\n            \n            nk = np.prod(ciutils.scalable_branching_factors(lower_bound_k, self.cfg['branching_factors'])) #To ensure the same growth that in the one-tree seqsampling\n            nk += self.batch_size - nk%self.batch_size\n            estimator_scenario_names = refmodel.scenario_names_creator(nk)\n            \n            \n            Gk, sk = self._gap_estimators_with_independent_scenarios(xhat_k,nk,estimator_scenario_names,scenario_denouement)\n\n            if (k%10==0):\n                print(f\"k={k}\")\n                print(f\"n_k={nk}\")\n\n        #----------------------------Step 4 -------------------------------------#\n        if (k==maxit) :\n            raise RuntimeError(f\"The loop terminated after {maxit} iteration with no acceptable solution\")\n        T = k\n        final_xhat=xhat_k\n        if self.stopping_criterion == \"BM\":\n            upper_bound=self.BM_h*sk+self.BM_eps\n        elif self.stopping_criterion == \"BPL\":\n            upper_bound = self.BPL_eps\n        else:\n            raise RuntimeError(\"Only BM and BPL criterion are supported yet.\")\n        CI=[0,upper_bound]\n        global_toc(f\"G={Gk}\")\n        global_toc(f\"s={sk}\")\n        global_toc(f\"xhat has been computed with {nk*mult} observations.\")\n        return {\"T\":T,\"Candidate_solution\":final_xhat,\"CI\":CI,}",
  "def _kw_creator_without_seed(self, cfg):\n        kwargs = self.refmodel.kw_creator(cfg)\n        kwargs.pop(\"start_seed\")\n        return kwargs",
  "def _gap_estimators_with_independent_scenarios(self, xhat_k, nk,\n                                                  estimator_scenario_names, scenario_denouement):\n        \"\"\" Sample a scenario tree: this is a subtree, but starting from stage 1.\n        Args:\n            xhat_k (dict[nodename] of list): the solution to lead the walk\n            nk (int): number of scenarios,\n            estimator_scenario_names(list of str): scenario names\n            scenario_denouement (fct): called for each scenario at the end\n                 (TBD: drop this arg and just use the function in refmodel)\n        Returns:\n            Gk, Sk (float): mean and standard devation of the gap estimate\n        Note:\n            Seed management is mainly in the form of updates to SeedCount\n\n        \"\"\"\n        cfg = config.Config()\n        cfg.quick_assign(\"EF_mstage\", bool, True)\n        cfg.quick_assign(\"EF_solver_name\", str, self.solver_name)\n        cfg.quick_assign(\"EF_solver_options\", dict, self.solver_options)\n        cfg.quick_assign(\"num_scens\", int, nk)\n        cfg.quick_assign(\"_mpisppy_probability\", float, 1/nk)\n        cfg.quick_assign(\"start_seed\", int, self.SeedCount)        \n        \n        pseudo_branching_factors = [nk]+[1]*(self.numstages-2)\n        cfg.quick_assign('branching_factors', pyofig.ListOf(int), pseudo_branching_factors)\n        ama = amalgamator.Amalgamator(cfg, \n                                      scenario_names=estimator_scenario_names,\n                                      scenario_creator=self.refmodel.scenario_creator,\n                                      kw_creator=self.refmodel.kw_creator,\n                                      scenario_denouement=scenario_denouement)\n        ama.run()\n        #Optimal solution of the approximate problem\n        zstar = ama.best_outer_bound\n        #Associated policies\n        xstars = sputils.nonant_cache_from_ef(ama.ef)\n        scenario_creator_kwargs = ama.kwargs\n        # Find feasible policies (i.e. xhats) for every non-leaf nodes\n        local_scenarios = {sname:getattr(ama.ef,sname) for sname in ama.ef._ef_scenario_names}\n        xhats,start = sample_tree.walking_tree_xhats(self.refmodelname,\n                                                    local_scenarios,\n                                                    xhat_k['ROOT'],\n                                                    self.cfg['branching_factors'],  # psuedo\n                                                    self.SeedCount,\n                                                     self.cfg,\n                                                    solver_name=self.solver_name,\n                                                    solver_options=self.solver_options)\n        \n        #Compute then the average function value with this policy\n        all_nodenames = sputils.create_nodenames_from_branching_factors(pseudo_branching_factors)\n        xhat_eval_options = {\"iter0_solver_options\": None,\n                         \"iterk_solver_options\": None,\n                         \"display_timing\": False,\n                         \"solver_name\": self.solver_name,\n                         \"verbose\": False,\n                         \"solver_options\":self.solver_options}\n        ev = xhat_eval.Xhat_Eval(xhat_eval_options,\n                                estimator_scenario_names,\n                                self.refmodel.scenario_creator,\n                                scenario_denouement,\n                                scenario_creator_kwargs=scenario_creator_kwargs,\n                                all_nodenames = all_nodenames)\n        #Evaluating xhat and xstar and getting the value of the objective function \n        #for every (local) scenario\n        ev.evaluate(xhats)\n        objs_at_xhat = ev.objs_dict\n        ev.evaluate(xstars)\n        objs_at_xstar = ev.objs_dict\n        \n        eval_scen_at_xhat = []\n        eval_scen_at_xstar = []\n        scen_probs = []\n        for k,s in ev.local_scenarios.items():\n            eval_scen_at_xhat.append(objs_at_xhat[k])\n            eval_scen_at_xstar.append(objs_at_xstar[k])\n            scen_probs.append(s._mpisppy_probability)\n    \n        \n        scen_gaps = np.array(eval_scen_at_xhat)-np.array(eval_scen_at_xstar)\n        local_gap = np.dot(scen_gaps,scen_probs)\n        local_ssq = np.dot(scen_gaps**2,scen_probs)\n        local_prob_sqnorm = np.linalg.norm(scen_probs)**2\n        local_obj_at_xhat = np.dot(eval_scen_at_xhat,scen_probs)\n        local_estim = np.array([local_gap,local_ssq,local_prob_sqnorm,local_obj_at_xhat])\n        global_estim = np.zeros(4)\n        ev.mpicomm.Allreduce(local_estim, global_estim, op=mpi.SUM) \n        G,ssq, prob_sqnorm,obj_at_xhat = global_estim\n        if global_rank==0:\n            print(f\"G = {G}\")\n        sample_var = (ssq - G**2)/(1-prob_sqnorm) #Unbiased sample variance\n        sk = np.sqrt(sample_var)\n        \n        use_relative_error = (np.abs(zstar)>1)\n        Gk = ciutils.correcting_numeric(G,cfg,objfct=obj_at_xhat,\n                               relative_error=use_relative_error)\n        \n        self.SeedCount = start\n        \n        return Gk,sk",
  "class XhatLooperInnerBound(spoke.InnerBoundNonantSpoke):\n\n    converger_spoke_char = 'X'\n\n    def xhatlooper_prep(self):\n        verbose = self.opt.options['verbose']\n        if \"bundles_per_rank\" in self.opt.options\\\n           and self.opt.options[\"bundles_per_rank\"] != 0:\n            raise RuntimeError(\"xhat spokes cannot have bundles (yet)\")\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatShuffleInnerBound must be used with Xhat_Eval.\")\n\n        xhatter = XhatLooper(self.opt)\n\n        ### begin iter0 stuff\n        xhatter.pre_iter0()\n        self.opt._save_original_nonants()\n        \n        self.opt._lazy_create_solvers()  # no iter0 loop, but we need the solvers\n\n        self.opt._update_E1()\n        if abs(1 - self.opt.E1) > self.opt.E1_tolerance:\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.opt.E1)\n                print(\"E1_tolerance = \", self.opt.E1_tolerance)\n            quit()\n        ### end iter0 stuff\n\n        xhatter.post_iter0()\n        self.opt._save_nonants() # make the cache\n\n        return xhatter\n\n    def main(self):\n        verbose = self.opt.options[\"verbose\"] # typing aid  \n        logger.debug(f\"Entering main on xhatlooper spoke rank {self.global_rank}\")\n\n        xhatter = self.xhatlooper_prep()\n\n        scen_limit = self.opt.options['xhat_looper_options']['scen_limit']\n\n        xh_iter = 1\n        while not self.got_kill_signal():\n            if (xh_iter-1) % 10000 == 0:\n                logger.debug(f'   Xhatlooper loop iter={xh_iter} on rank {self.global_rank}')\n                logger.debug(f'   Xhatlooper got from opt on rank {self.global_rank}')\n\n            if self.new_nonants:\n                logger.debug(f'   *Xhatlooper loop iter={xh_iter}')\n                logger.debug(f'   *got a new one! on rank {self.global_rank}')\n                logger.debug(f'   *localnonants={str(self.localnonants)}')\n\n                self.opt._put_nonant_cache(self.localnonants)\n                self.opt._restore_nonants()\n                upperbound, srcsname = xhatter.xhat_looper(scen_limit=scen_limit, restore_nonants=False)\n\n                # send a bound to the opt companion\n                self.update_if_improving(upperbound)\n            xh_iter += 1",
  "def xhatlooper_prep(self):\n        verbose = self.opt.options['verbose']\n        if \"bundles_per_rank\" in self.opt.options\\\n           and self.opt.options[\"bundles_per_rank\"] != 0:\n            raise RuntimeError(\"xhat spokes cannot have bundles (yet)\")\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatShuffleInnerBound must be used with Xhat_Eval.\")\n\n        xhatter = XhatLooper(self.opt)\n\n        ### begin iter0 stuff\n        xhatter.pre_iter0()\n        self.opt._save_original_nonants()\n        \n        self.opt._lazy_create_solvers()  # no iter0 loop, but we need the solvers\n\n        self.opt._update_E1()\n        if abs(1 - self.opt.E1) > self.opt.E1_tolerance:\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.opt.E1)\n                print(\"E1_tolerance = \", self.opt.E1_tolerance)\n            quit()\n        ### end iter0 stuff\n\n        xhatter.post_iter0()\n        self.opt._save_nonants() # make the cache\n\n        return xhatter",
  "def main(self):\n        verbose = self.opt.options[\"verbose\"] # typing aid  \n        logger.debug(f\"Entering main on xhatlooper spoke rank {self.global_rank}\")\n\n        xhatter = self.xhatlooper_prep()\n\n        scen_limit = self.opt.options['xhat_looper_options']['scen_limit']\n\n        xh_iter = 1\n        while not self.got_kill_signal():\n            if (xh_iter-1) % 10000 == 0:\n                logger.debug(f'   Xhatlooper loop iter={xh_iter} on rank {self.global_rank}')\n                logger.debug(f'   Xhatlooper got from opt on rank {self.global_rank}')\n\n            if self.new_nonants:\n                logger.debug(f'   *Xhatlooper loop iter={xh_iter}')\n                logger.debug(f'   *got a new one! on rank {self.global_rank}')\n                logger.debug(f'   *localnonants={str(self.localnonants)}')\n\n                self.opt._put_nonant_cache(self.localnonants)\n                self.opt._restore_nonants()\n                upperbound, srcsname = xhatter.xhat_looper(scen_limit=scen_limit, restore_nonants=False)\n\n                # send a bound to the opt companion\n                self.update_if_improving(upperbound)\n            xh_iter += 1",
  "class CrossScenarioHub(PHHub):\n    def setup_hub(self):\n        super().setup_hub()\n        if self.opt.multistage:\n            raise RuntimeError('CrossScenarioHub only supports '\n                                'two-stage models at this time')\n        idx = self.cut_gen_spoke_index\n        self.all_nonants_and_etas = np.zeros(self.local_lengths[idx - 1] + 1)\n\n        self.nonant_len = self.opt.nonant_length\n\n        # save the best bounds so far\n        self.best_inner_bound = inf\n        self.best_outer_bound = -inf\n\n        # helping the extension track cuts\n        self.new_cuts = False\n\n    def initialize_spoke_indices(self):\n        super().initialize_spoke_indices()\n        for (i, spoke) in enumerate(self.spokes):\n            if spoke[\"spoke_class\"] == CrossScenarioCutSpoke:\n                self.cut_gen_spoke_index = i + 1\n\n    def sync(self):\n        super().sync()\n        self.send_to_cross_cuts()\n        self.get_from_cross_cuts()\n\n    def get_from_cross_cuts(self):\n        idx = self.cut_gen_spoke_index\n        receive_buffer = np.empty(self.remote_lengths[idx - 1] + 1, dtype=\"d\") # Must be doubles\n        is_new = self.hub_from_spoke(receive_buffer, idx)\n        if is_new:\n            self.make_cuts(receive_buffer)\n\n    def send_to_cross_cuts(self):\n        idx = self.cut_gen_spoke_index\n\n        # get the stuff we want to send\n        self.opt._save_nonants()\n        ci = 0  ## index to self.nonant_send_buffer\n\n        # get all the nonants\n        all_nonants_and_etas = self.all_nonants_and_etas\n        for k, s in self.opt.local_scenarios.items():\n            for xvar in s._mpisppy_data.nonant_indices.values():\n                all_nonants_and_etas[ci] = xvar._value\n                ci += 1\n\n        # get all the etas\n        for k, s in self.opt.local_scenarios.items():\n            for sn in self.opt.all_scenario_names:\n                all_nonants_and_etas[ci] = s._mpisppy_model.eta[sn]._value\n                ci += 1\n        self.hub_to_spoke(all_nonants_and_etas, idx)\n\n\n    def make_cuts(self, coefs):\n        # take the coefficient array and assemble cuts accordingly\n\n        # this should have already been set in the extension !\n        opt = self.opt\n\n        # rows are \n        # [ const, eta_coeff, *nonant_coeffs ]\n        row_len = 1+1+self.nonant_len\n        outer_iter = int(coefs[-1])\n\n        bundling = opt.bundling\n        if opt.bundling:\n            for bn,b in opt.local_subproblems.items():\n                persistent_solver = sputils.is_persistent(b._solver_plugin)\n                ## get an arbitrary scenario\n                s = opt.local_scenarios[b.scen_list[0]]\n                for idx, k in enumerate(opt.all_scenario_names):\n                    row = coefs[row_len*idx:row_len*(idx+1)]\n                    # the row could be all zeros,\n                    # which doesn't do anything\n                    if (row == 0.).all():\n                        continue\n                    # rows are \n                    # [ const, eta_coeff, *nonant_coeffs ]\n                    linear_const = row[0]\n                    linear_coefs = list(row[1:])\n                    linear_vars = [b._mpisppy_model.eta[k]]\n\n                    for ndn_i in s._mpisppy_data.nonant_indices:\n                        ## for bundles, we add the constrains only\n                        ## to the reference first stage variables\n                        linear_vars.append(b.ref_vars[ndn_i])\n\n                    cut_expr = LinearExpression(constant=linear_const, linear_coefs=linear_coefs,\n                                                linear_vars=linear_vars)\n                    b._mpisppy_model.benders_cuts[outer_iter, k] = (None, cut_expr, 0)\n                    if persistent_solver:\n                        b._solver_plugin.add_constraint(b._mpisppy_model.benders_cuts[outer_iter, k])\n\n        else:\n            for sn,s in opt.local_subproblems.items():\n                persistent_solver = sputils.is_persistent(s._solver_plugin)\n                for idx, k in enumerate(opt.all_scenario_names):\n                    row = coefs[row_len*idx:row_len*(idx+1)]\n                    # the row could be all zeros,\n                    # which doesn't do anything\n                    if (row == 0.).all():\n                        continue\n                    # rows are \n                    # [ const, eta_coeff, *nonant_coeffs ]\n                    linear_const = row[0]\n                    linear_coefs = list(row[1:])\n                    linear_vars = [s._mpisppy_model.eta[k]]\n                    linear_vars.extend(s._mpisppy_data.nonant_indices.values())\n\n                    cut_expr = LinearExpression(constant=linear_const, linear_coefs=linear_coefs,\n                                                linear_vars=linear_vars)\n                    s._mpisppy_model.benders_cuts[outer_iter, k] = (None, cut_expr, 0.)\n                    if persistent_solver:\n                        s._solver_plugin.add_constraint(s._mpisppy_model.benders_cuts[outer_iter, k])\n\n        # NOTE: the LShaped code negates the objective, so\n        #       we do the same here for consistency\n        ib = self.BestInnerBound\n        ob = self.BestOuterBound\n        if not opt.is_minimizing:\n            ib = -ib\n            ob = -ob\n        add_cut = (isfinite(ib) or isfinite(ob)) and \\\n                ((ib < self.best_inner_bound) or (ob > self.best_outer_bound))\n        if add_cut:\n            self.best_inner_bound = ib\n            self.best_outer_bound = ob\n            for sn,s in opt.local_subproblems.items():\n                persistent_solver = sputils.is_persistent(s._solver_plugin)\n                prior_outer_iter = list(s._mpisppy_model.inner_bound_constr.keys())\n                s._mpisppy_model.inner_bound_constr[outer_iter] = (ob, s._mpisppy_model.EF_obj, ib)\n                if persistent_solver:\n                    s._solver_plugin.add_constraint(s._mpisppy_model.inner_bound_constr[outer_iter])\n                # remove other ib constraints (we only need the tightest)\n                for it in prior_outer_iter:\n                    if persistent_solver:\n                        s._solver_plugin.remove_constraint(s._mpisppy_model.inner_bound_constr[it])\n                    del s._mpisppy_model.inner_bound_constr[it]\n\n        ## helping the extention track cuts\n        self.new_cuts = True",
  "def setup_hub(self):\n        super().setup_hub()\n        if self.opt.multistage:\n            raise RuntimeError('CrossScenarioHub only supports '\n                                'two-stage models at this time')\n        idx = self.cut_gen_spoke_index\n        self.all_nonants_and_etas = np.zeros(self.local_lengths[idx - 1] + 1)\n\n        self.nonant_len = self.opt.nonant_length\n\n        # save the best bounds so far\n        self.best_inner_bound = inf\n        self.best_outer_bound = -inf\n\n        # helping the extension track cuts\n        self.new_cuts = False",
  "def initialize_spoke_indices(self):\n        super().initialize_spoke_indices()\n        for (i, spoke) in enumerate(self.spokes):\n            if spoke[\"spoke_class\"] == CrossScenarioCutSpoke:\n                self.cut_gen_spoke_index = i + 1",
  "def sync(self):\n        super().sync()\n        self.send_to_cross_cuts()\n        self.get_from_cross_cuts()",
  "def get_from_cross_cuts(self):\n        idx = self.cut_gen_spoke_index\n        receive_buffer = np.empty(self.remote_lengths[idx - 1] + 1, dtype=\"d\") # Must be doubles\n        is_new = self.hub_from_spoke(receive_buffer, idx)\n        if is_new:\n            self.make_cuts(receive_buffer)",
  "def send_to_cross_cuts(self):\n        idx = self.cut_gen_spoke_index\n\n        # get the stuff we want to send\n        self.opt._save_nonants()\n        ci = 0  ## index to self.nonant_send_buffer\n\n        # get all the nonants\n        all_nonants_and_etas = self.all_nonants_and_etas\n        for k, s in self.opt.local_scenarios.items():\n            for xvar in s._mpisppy_data.nonant_indices.values():\n                all_nonants_and_etas[ci] = xvar._value\n                ci += 1\n\n        # get all the etas\n        for k, s in self.opt.local_scenarios.items():\n            for sn in self.opt.all_scenario_names:\n                all_nonants_and_etas[ci] = s._mpisppy_model.eta[sn]._value\n                ci += 1\n        self.hub_to_spoke(all_nonants_and_etas, idx)",
  "def make_cuts(self, coefs):\n        # take the coefficient array and assemble cuts accordingly\n\n        # this should have already been set in the extension !\n        opt = self.opt\n\n        # rows are \n        # [ const, eta_coeff, *nonant_coeffs ]\n        row_len = 1+1+self.nonant_len\n        outer_iter = int(coefs[-1])\n\n        bundling = opt.bundling\n        if opt.bundling:\n            for bn,b in opt.local_subproblems.items():\n                persistent_solver = sputils.is_persistent(b._solver_plugin)\n                ## get an arbitrary scenario\n                s = opt.local_scenarios[b.scen_list[0]]\n                for idx, k in enumerate(opt.all_scenario_names):\n                    row = coefs[row_len*idx:row_len*(idx+1)]\n                    # the row could be all zeros,\n                    # which doesn't do anything\n                    if (row == 0.).all():\n                        continue\n                    # rows are \n                    # [ const, eta_coeff, *nonant_coeffs ]\n                    linear_const = row[0]\n                    linear_coefs = list(row[1:])\n                    linear_vars = [b._mpisppy_model.eta[k]]\n\n                    for ndn_i in s._mpisppy_data.nonant_indices:\n                        ## for bundles, we add the constrains only\n                        ## to the reference first stage variables\n                        linear_vars.append(b.ref_vars[ndn_i])\n\n                    cut_expr = LinearExpression(constant=linear_const, linear_coefs=linear_coefs,\n                                                linear_vars=linear_vars)\n                    b._mpisppy_model.benders_cuts[outer_iter, k] = (None, cut_expr, 0)\n                    if persistent_solver:\n                        b._solver_plugin.add_constraint(b._mpisppy_model.benders_cuts[outer_iter, k])\n\n        else:\n            for sn,s in opt.local_subproblems.items():\n                persistent_solver = sputils.is_persistent(s._solver_plugin)\n                for idx, k in enumerate(opt.all_scenario_names):\n                    row = coefs[row_len*idx:row_len*(idx+1)]\n                    # the row could be all zeros,\n                    # which doesn't do anything\n                    if (row == 0.).all():\n                        continue\n                    # rows are \n                    # [ const, eta_coeff, *nonant_coeffs ]\n                    linear_const = row[0]\n                    linear_coefs = list(row[1:])\n                    linear_vars = [s._mpisppy_model.eta[k]]\n                    linear_vars.extend(s._mpisppy_data.nonant_indices.values())\n\n                    cut_expr = LinearExpression(constant=linear_const, linear_coefs=linear_coefs,\n                                                linear_vars=linear_vars)\n                    s._mpisppy_model.benders_cuts[outer_iter, k] = (None, cut_expr, 0.)\n                    if persistent_solver:\n                        s._solver_plugin.add_constraint(s._mpisppy_model.benders_cuts[outer_iter, k])\n\n        # NOTE: the LShaped code negates the objective, so\n        #       we do the same here for consistency\n        ib = self.BestInnerBound\n        ob = self.BestOuterBound\n        if not opt.is_minimizing:\n            ib = -ib\n            ob = -ob\n        add_cut = (isfinite(ib) or isfinite(ob)) and \\\n                ((ib < self.best_inner_bound) or (ob > self.best_outer_bound))\n        if add_cut:\n            self.best_inner_bound = ib\n            self.best_outer_bound = ob\n            for sn,s in opt.local_subproblems.items():\n                persistent_solver = sputils.is_persistent(s._solver_plugin)\n                prior_outer_iter = list(s._mpisppy_model.inner_bound_constr.keys())\n                s._mpisppy_model.inner_bound_constr[outer_iter] = (ob, s._mpisppy_model.EF_obj, ib)\n                if persistent_solver:\n                    s._solver_plugin.add_constraint(s._mpisppy_model.inner_bound_constr[outer_iter])\n                # remove other ib constraints (we only need the tightest)\n                for it in prior_outer_iter:\n                    if persistent_solver:\n                        s._solver_plugin.remove_constraint(s._mpisppy_model.inner_bound_constr[it])\n                    del s._mpisppy_model.inner_bound_constr[it]\n\n        ## helping the extention track cuts\n        self.new_cuts = True",
  "class Hub(SPCommunicator):\n    def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, spokes, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options=options)\n        assert len(spokes) == self.n_spokes\n        self.local_write_ids = np.zeros(self.n_spokes, dtype=np.int64)\n        self.remote_write_ids = np.zeros(self.n_spokes, dtype=np.int64)\n        self.local_lengths = np.zeros(self.n_spokes, dtype=np.int64)\n        self.remote_lengths = np.zeros(self.n_spokes, dtype=np.int64)\n        # ^^^ Does NOT include +1\n        self.spokes = spokes  # List of dicts\n        logger.debug(f\"Built the hub object on global rank {fullcomm.Get_rank()}\")\n        # for logging\n        self.print_init = True\n        self.latest_ib_char = None\n        self.latest_ob_char = None\n        self.last_ib_idx = None\n        self.last_ob_idx = None\n        # for termination based on stalling out\n        self.stalled_iter_cnt = 0\n        self.last_gap = float('inf')  # abs_gap tracker\n\n    @abc.abstractmethod\n    def setup_hub(self):\n        pass\n\n    @abc.abstractmethod\n    def sync(self):\n        \"\"\" To be called within the whichever optimization algorithm\n            is being run on the hub (e.g. PH)\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def is_converged(self):\n        \"\"\" The hub has the ability to halt the optimization algorithm on the\n            hub before any local convergers.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def current_iteration(self):\n        \"\"\" Returns the current iteration count - however the hub defines it.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def main(self):\n        pass\n\n    def clear_latest_chars(self):\n        self.latest_ib_char = None\n        self.latest_ob_char = None\n\n        \n    def compute_gaps(self):\n        \"\"\" Compute the current absolute and relative gaps, \n            using the current self.BestInnerBound and self.BestOuterBound\n        \"\"\"\n        if self.opt.is_minimizing:\n            abs_gap = self.BestInnerBound - self.BestOuterBound\n        else:\n            abs_gap = self.BestOuterBound - self.BestInnerBound\n\n        ## define by the best solution, as is common\n        nano = float(\"nan\")  # typing aid\n        if (\n            abs_gap != nano\n            and abs_gap != float(\"inf\")\n            and abs_gap != float(\"-inf\")\n            and self.BestOuterBound != nano\n            and self.BestOuterBound != 0\n        ):\n            rel_gap = abs_gap / abs(self.BestOuterBound)\n        else:\n            rel_gap = float(\"inf\")\n        return abs_gap, rel_gap\n\n    \n    def get_update_string(self):\n        if self.latest_ib_char is None and \\\n                self.latest_ob_char is None:\n            return '   '\n        if self.latest_ib_char is None:\n            return self.latest_ob_char + '  '\n        if self.latest_ob_char is None:\n            return '  ' + self.latest_ib_char\n        return self.latest_ob_char+' '+self.latest_ib_char\n\n    def screen_trace(self):\n        current_iteration = self.current_iteration()\n        abs_gap, rel_gap = self.compute_gaps()\n        best_solution = self.BestInnerBound\n        best_bound = self.BestOuterBound\n        update_source = self.get_update_string()\n        if self.print_init:\n            row = f'{\"Iter.\":>5s}  {\"   \"}  {\"Best Bound\":>14s}  {\"Best Incumbent\":>14s}  {\"Rel. Gap\":>12s}  {\"Abs. Gap\":>14s}'\n            global_toc(row, True)\n            self.print_init = False\n        row = f\"{current_iteration:5d}  {update_source}  {best_bound:14.4f}  {best_solution:14.4f}  {rel_gap*100:12.3f}%  {abs_gap:14.4f}\"\n        global_toc(row, True)\n        self.clear_latest_chars()\n\n    def determine_termination(self):\n        # return True if termination is indicated, otherwise return False\n        \n        if not hasattr(self,\"options\") or self.options is None\\\n           or (\"rel_gap\" not in self.options and \"abs_gap\" not in self.options\\\n           and \"max_stalled_iters\" not in self.options):\n            return False  # Nothing to see here folks...\n        \n        # If we are still here, there is some option for termination\n        abs_gap, rel_gap = self.compute_gaps()\n        \n        abs_gap_satisfied = False\n        rel_gap_satisfied = False\n        max_stalled_satisfied = False\n\n        if \"rel_gap\" in self.options and rel_gap <= self.options[\"rel_gap\"]:\n            rel_gap_satisfied = True \n        if \"abs_gap\" in self.options and abs_gap <= self.options[\"abs_gap\"]:\n            abs_gap_satisfied = True             \n\n        if \"max_stalled_iters\" in self.options:\n            if abs_gap < self.last_gap:  # liberal test (we could use an epsilon)\n                self.last_gap = abs_gap\n                self.stalled_iter_cnt = 0\n            else:\n                self.stalled_iter_cnt += 1\n                if self.stalled_iter_cnt >= self.options[\"max_stalled_iters\"]:\n                    max_stalled_satisfied = True\n            \n        if abs_gap_satisfied:\n            global_toc(f\"Terminating based on inter-cylinder absolute gap {abs_gap:12.4f}\")\n        if rel_gap_satisfied:\n            global_toc(f\"Terminating based on inter-cylinder relative gap {rel_gap*100:12.3f}%\")\n        if max_stalled_satisfied:\n            global_toc(f\"Terminating based on max-stalled-iters {self.stalled_iter_cnt}\")\n\n        return abs_gap_satisfied or rel_gap_satisfied or max_stalled_satisfied\n\n    def hub_finalize(self):\n        if self.has_outerbound_spokes:\n            self.receive_outerbounds()\n        if self.has_innerbound_spokes:\n            self.receive_innerbounds()\n\n        if self.global_rank == 0:\n            self.print_init = True\n            global_toc(f\"Statistics at termination\", True)\n            self.screen_trace()\n\n    def receive_innerbounds(self):\n        \"\"\" Get inner bounds from inner bound spokes\n            NOTE: Does not check if there _are_ innerbound spokes\n            (but should be harmless to call if there are none)\n        \"\"\"\n        logging.debug(\"Hub is trying to receive from InnerBounds\")\n        for idx in self.innerbound_spoke_indices:\n            is_new = self.hub_from_spoke(self.innerbound_receive_buffers[idx], idx)\n            if is_new:\n                bound = self.innerbound_receive_buffers[idx][0]\n                logging.debug(\"!! new InnerBound to opt {}\".format(bound))\n                self.BestInnerBound = self.InnerBoundUpdate(bound, idx)\n        logging.debug(\"ph back from InnerBounds\")\n\n    def receive_outerbounds(self):\n        \"\"\" Get outer bounds from outer bound spokes\n            NOTE: Does not check if there _are_ outerbound spokes\n            (but should be harmless to call if there are none)\n        \"\"\"\n        logging.debug(\"Hub is trying to receive from OuterBounds\")\n        for idx in self.outerbound_spoke_indices:\n            is_new = self.hub_from_spoke(self.outerbound_receive_buffers[idx], idx)\n            if is_new:\n                bound = self.outerbound_receive_buffers[idx][0]\n                logging.debug(\"!! new OuterBound to opt {}\".format(bound))\n                self.BestOuterBound = self.OuterBoundUpdate(bound, idx)\n        logging.debug(\"ph back from OuterBounds\")\n\n    def OuterBoundUpdate(self, new_bound, idx=None, char='*'):\n        current_bound = self.BestOuterBound\n        if self._outer_bound_update(new_bound, current_bound):\n            if idx is None:\n                self.latest_ob_char = char\n                self.last_ob_idx = 0\n            else:\n                self.latest_ob_char = self.outerbound_spoke_chars[idx]\n                self.last_ob_idx = idx\n            return new_bound\n        else:\n            return current_bound\n\n    def InnerBoundUpdate(self, new_bound, idx=None, char='*'):\n        current_bound = self.BestInnerBound\n        if self._inner_bound_update(new_bound, current_bound):\n            if idx is None:\n                self.latest_ib_char = char\n                self.last_ib_idx = 0\n            else:\n                self.latest_ib_char = self.innerbound_spoke_chars[idx]\n                self.last_ib_idx = idx\n            return new_bound\n        else:\n            return current_bound\n\n    def initialize_bound_values(self):\n        if self.opt.is_minimizing:\n            self.BestInnerBound = inf\n            self.BestOuterBound = -inf\n            self._inner_bound_update = lambda new, old : (new < old)\n            self._outer_bound_update = lambda new, old : (new > old)\n        else:\n            self.BestInnerBound = -inf\n            self.BestOuterBound = inf\n            self._inner_bound_update = lambda new, old : (new > old)\n            self._outer_bound_update = lambda new, old : (new < old)\n\n    def initialize_outer_bound_buffers(self):\n        \"\"\" Initialize value of BestOuterBound, and outer bound receive buffers\n        \"\"\"\n        self.outerbound_receive_buffers = dict()\n        for idx in self.outerbound_spoke_indices:\n            self.outerbound_receive_buffers[idx] = np.zeros(\n                self.remote_lengths[idx - 1] + 1\n            )\n\n    def initialize_inner_bound_buffers(self):\n        \"\"\" Initialize value of BestInnerBound, and inner bound receive buffers\n        \"\"\"\n        self.innerbound_receive_buffers = dict()\n        for idx in self.innerbound_spoke_indices:\n            self.innerbound_receive_buffers[idx] = np.zeros(\n                self.remote_lengths[idx - 1] + 1\n            )\n\n    def initialize_nonants(self):\n        \"\"\" Initialize the buffer for the hub to send nonants\n            to the appropriate spokes\n        \"\"\"\n        self.nonant_send_buffer = None\n        for idx in self.nonant_spoke_indices:\n            if self.nonant_send_buffer is None:\n                self.nonant_send_buffer = np.zeros(self.local_lengths[idx - 1] + 1)\n            elif self.local_lengths[idx - 1] + 1 != len(self.nonant_send_buffer):\n                raise RuntimeError(\"Nonant buffers disagree on size\")\n\n    def initialize_spoke_indices(self):\n        \"\"\" Figure out what types of spokes we have, \n        and sort them into the appropriate classes.\n\n        Note: \n            Some spokes may be multiple types (e.g. outerbound and nonant),\n            though not all combinations are supported.\n        \"\"\"\n        self.outerbound_spoke_indices = set()\n        self.innerbound_spoke_indices = set()\n        self.nonant_spoke_indices = set()\n        self.w_spoke_indices = set()\n        self.boundsout_spoke_indices = set()\n\n        self.outerbound_spoke_chars = dict()\n        self.innerbound_spoke_chars = dict()\n\n        for (i, spoke) in enumerate(self.spokes):\n            spoke_class = spoke[\"spoke_class\"]\n            if hasattr(spoke_class, \"converger_spoke_types\"):\n                for cst in spoke_class.converger_spoke_types:\n                    if cst == ConvergerSpokeType.OUTER_BOUND:\n                        self.outerbound_spoke_indices.add(i + 1)\n                        self.outerbound_spoke_chars[i+1] = spoke_class.converger_spoke_char\n                    elif cst == ConvergerSpokeType.INNER_BOUND:\n                        self.innerbound_spoke_indices.add(i + 1)\n                        self.innerbound_spoke_chars[i+1] = spoke_class.converger_spoke_char\n                    elif cst == ConvergerSpokeType.W_GETTER:\n                        self.w_spoke_indices.add(i + 1)\n                    elif cst == ConvergerSpokeType.NONANT_GETTER:\n                        self.nonant_spoke_indices.add(i + 1)\n                    elif cst == ConvergerSpokeType.BOUNDS_GETTER:\n                        self.boundsout_spoke_indices.add(i + 1)\n                    else:\n                        raise RuntimeError(f\"Unrecognized converger_spoke_type {cst}\")\n            else:  ##this isn't necessarily wrong, i.e., cut generators\n                logger.debug(f\"Spoke class {spoke_class} not recognized by hub\")\n\n        self.has_outerbound_spokes = len(self.outerbound_spoke_indices) > 0\n        self.has_innerbound_spokes = len(self.innerbound_spoke_indices) > 0\n        self.has_nonant_spokes = len(self.nonant_spoke_indices) > 0\n        self.has_w_spokes = len(self.w_spoke_indices) > 0\n        self.has_boundsout_spokes = len(self.boundsout_spoke_indices) > 0\n\n    def make_windows(self):\n        if self._windows_constructed:\n            # different parts of the hub may call make_windows,\n            # we just care about the first call\n            return\n\n        # Spokes notify the hub of the buffer sizes\n        for i in range(self.n_spokes):\n            pair_of_sizes = np.zeros(2, dtype=\"i\")\n            self.strata_comm.Recv((pair_of_sizes, MPI.INT), source=i + 1, tag=i + 1)\n            self.remote_lengths[i] = pair_of_sizes[0]\n            self.local_lengths[i] = pair_of_sizes[1]\n\n        # Make the windows of the appropriate buffer sizes\n        self.windows = [None for _ in range(self.n_spokes)]\n        self.buffers = [None for _ in range(self.n_spokes)]\n        for i in range(self.n_spokes):\n            length = self.local_lengths[i]\n            win, buff = self._make_window(length)\n            self.windows[i] = win\n            self.buffers[i] = buff\n\n        # flag this for multiple calls from the hub\n        self._windows_constructed = True\n\n    def hub_to_spoke(self, values, spoke_strata_rank):\n        \"\"\" Put the specified values into the specified locally-owned buffer\n            for the spoke to pick up.\n\n            Notes:\n                This automatically does the -1 indexing\n\n                This assumes that values contains a slot at the end for the\n                write_id\n        \"\"\"\n        expected_length = self.local_lengths[spoke_strata_rank - 1] + 1\n        if len(values) != expected_length:\n            raise RuntimeError(\n                f\"Attempting to put array of length {len(values)} \"\n                f\"into local buffer of length {expected_length}\"\n            )\n        # this is so the spoke ranks all get the same write_id at approximately the same time\n        if not isinstance(self.opt, APH):\n            self.cylinder_comm.Barrier()\n        self.local_write_ids[spoke_strata_rank - 1] += 1\n        values[-1] = self.local_write_ids[spoke_strata_rank - 1]\n        window = self.windows[spoke_strata_rank - 1]\n        window.Lock(self.strata_rank)\n        window.Put((values, len(values), MPI.DOUBLE), self.strata_rank)\n        window.Unlock(self.strata_rank)\n\n    def hub_from_spoke(self, values, spoke_num):\n        \"\"\" spoke_num is the rank in the strata_comm, so it is 1-based not 0-based\n            \n            Returns:\n                is_new (bool): Indicates whether the \"gotten\" values are new,\n                    based on the write_id.\n        \"\"\"\n        expected_length = self.remote_lengths[spoke_num - 1] + 1\n        if len(values) != expected_length:\n            raise RuntimeError(\n                f\"Hub trying to get buffer of length {expected_length} \"\n                f\"from spoke, but provided buffer has length {len(values)}.\"\n            )\n        # so the window in each rank gets read at approximately the same time,\n        # and so has the same write_id\n        if not isinstance(self.opt, APH):\n            self.cylinder_comm.Barrier()\n        window = self.windows[spoke_num - 1]\n        window.Lock(spoke_num)\n        window.Get((values, len(values), MPI.DOUBLE), spoke_num)\n        window.Unlock(spoke_num)\n\n        if isinstance(self.opt, APH):\n            # reverting part of changes from Ben getting rid of spoke sleep DLW jan 2023\n            if values[-1] > self.remote_write_ids[spoke_num - 1]:\n                self.remote_write_ids[spoke_num - 1] = values[-1]\n                return True\n        else:\n            new_id = int(values[-1])\n            local_val = np.array((new_id,), 'i')\n            sum_ids = np.zeros(1, 'i')\n            self.cylinder_comm.Allreduce((local_val, MPI.INT),\n                                         (sum_ids, MPI.INT),\n                                         op=MPI.SUM)\n            if new_id != sum_ids[0] / self.cylinder_comm.size:\n                return False\n\n            if (new_id > self.remote_write_ids[spoke_num - 1]) or (new_id < 0):\n                self.remote_write_ids[spoke_num - 1] = new_id\n                return True\n        return False\n\n    def send_terminate(self):\n        \"\"\" Send an array of zeros with a -1 appended to the\n            end to indicate termination. This function puts to the local\n            buffer, so every spoke will see it simultaneously. \n            processes (don't need to call them one at a time).\n        \"\"\"\n        for rank in range(1, self.n_spokes + 1):\n            dummies = np.zeros(self.local_lengths[rank - 1] + 1)\n            dummies[-1] = -1\n            window = self.windows[rank - 1]\n            window.Lock(0)\n            window.Put((dummies, len(dummies), MPI.DOUBLE), 0)\n            window.Unlock(0)",
  "class PHHub(Hub):\n    def setup_hub(self):\n        \"\"\" Must be called after make_windows(), so that \n            the hub knows the sizes of all the spokes windows\n        \"\"\"\n        if not self._windows_constructed:\n            raise RuntimeError(\n                \"Cannot call setup_hub before memory windows are constructed\"\n            )\n\n        self.initialize_spoke_indices()\n        self.initialize_bound_values()\n\n        if self.has_outerbound_spokes:\n            self.initialize_outer_bound_buffers()\n        if self.has_innerbound_spokes:\n            self.initialize_inner_bound_buffers()\n        if self.has_w_spokes:\n            self.initialize_ws()\n        if self.has_nonant_spokes:\n            self.initialize_nonants()\n        if self.has_boundsout_spokes:\n            self.initialize_boundsout()  # bounds going out\n\n        ## Do some checking for things we currently don't support\n        if len(self.outerbound_spoke_indices & self.innerbound_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke providing both inner and outer \"\n                \"bounds is currently unsupported\"\n            )\n        if len(self.w_spoke_indices & self.nonant_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke needing both Ws and nonants is currently unsupported\"\n            )\n\n        ## Generate some warnings if nothing is giving bounds\n        if not self.has_outerbound_spokes:\n            logger.warn(\n                \"No OuterBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )\n\n        if not self.has_innerbound_spokes:\n            logger.warn(\n                \"No InnerBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )\n\n    def sync(self):\n        \"\"\"\n            Manages communication with Spokes\n        \"\"\"\n        if self.has_w_spokes:\n            self.send_ws()\n        if self.has_nonant_spokes:\n            self.send_nonants()\n        if self.has_boundsout_spokes:\n            self.send_boundsout()\n        if self.has_outerbound_spokes:\n            self.receive_outerbounds()\n        if self.has_innerbound_spokes:\n            self.receive_innerbounds()\n\n    def sync_with_spokes(self):\n        self.sync()\n\n    def is_converged(self):\n        ## might as well get a bound, in this case\n        if self.opt._PHIter == 1:\n            self.BestOuterBound = self.OuterBoundUpdate(self.opt.trivial_bound)\n\n        if not self.has_innerbound_spokes:\n            if self.opt._PHIter == 1:\n                logger.warning(\n                    \"PHHub cannot compute convergence without \"\n                    \"inner bound spokes.\"\n                )\n\n            ## you still want to output status, even without inner bounders configured\n            if self.global_rank == 0:                \n                self.screen_trace()\n                \n            return False\n\n        if not self.has_outerbound_spokes:\n            if self.opt._PHIter == 1:\n                global_toc(\n                    \"Without outer bound spokes, no progress \"\n                    \"will be made on the Best Bound\")\n\n        ## log some output\n        if self.global_rank == 0:\n            self.screen_trace()\n\n        return self.determine_termination()\n\n    def current_iteration(self):\n        \"\"\" Return the current PH iteration.\"\"\"\n        return self.opt._PHIter\n    \n    def main(self):\n        \"\"\" SPComm gets attached in self.__init__ \"\"\"\n        self.opt.ph_main(finalize=False)\n\n    def finalize(self):\n        \"\"\" does PH.post_loops, returns Eobj \"\"\"\n        Eobj = self.opt.post_loops(self.opt.extensions)\n        return Eobj\n\n    def send_nonants(self):\n        \"\"\" Gather nonants and send them to the appropriate spokes\n            TODO: Will likely fail with bundling\n        \"\"\"\n        self.opt._save_nonants()\n        ci = 0  ## index to self.nonant_send_buffer\n        nonant_send_buffer = self.nonant_send_buffer\n        for k, s in self.opt.local_scenarios.items():\n            for xvar in s._mpisppy_data.nonant_indices.values():\n                nonant_send_buffer[ci] = xvar._value\n                ci += 1\n        logging.debug(\"hub is sending X nonants={}\".format(nonant_send_buffer))\n        for idx in self.nonant_spoke_indices:\n            self.hub_to_spoke(nonant_send_buffer, idx)\n\n    def initialize_ws(self):\n        \"\"\" Initialize the buffer for the hub to send dual weights\n            to the appropriate spokes\n        \"\"\"\n        self.w_send_buffer = None\n        for idx in self.w_spoke_indices:\n            if self.w_send_buffer is None:\n                self.w_send_buffer = np.zeros(self.local_lengths[idx - 1] + 1)\n            elif self.local_lengths[idx - 1] + 1 != len(self.w_send_buffer):\n                raise RuntimeError(\"W buffers disagree on size\")\n\n    def send_ws(self):\n        \"\"\" Send dual weights to the appropriate spokes\n        \"\"\"\n        self.opt._populate_W_cache(self.w_send_buffer)\n        logging.debug(\"hub is sending Ws={}\".format(self.w_send_buffer))\n        for idx in self.w_spoke_indices:\n            self.hub_to_spoke(self.w_send_buffer, idx)\n\n    def initialize_boundsout(self):\n        \"\"\" Initialize the buffer for the hub to send bounds\n            to the appropriate spokes\n        \"\"\"\n        self.boundsout_send_buffer = None\n        for idx in self.boundsout_spoke_indices:\n            if self.boundsout_send_buffer is None:\n                self.boundsout_send_buffer = np.zeros(self.local_lengths[idx - 1] + 1)\n            elif self.local_lengths[idx - 1] + 1 != len(self.boundsout_send_buffer):\n                raise RuntimeError(\"boundsout buffers disagree on size\")\n\n    def _populate_boundsout_cache(self):\n        self.boundsout_send_buffer[0] = self.BestOuterBound\n        self.boundsout_send_buffer[1] = self.BestInnerBound\n            \n    def send_boundsout(self):\n        \"\"\" Send bounds to the appropriate spokes\n        \"\"\"\n        self._populate_boundsout_cache()\n        logging.debug(\"hub is sending bounds={}\".format(self.boundsout_send_buffer))\n        for idx in self.boundsout_spoke_indices:\n            self.hub_to_spoke(self.boundsout_send_buffer, idx)",
  "class LShapedHub(Hub):\n\n    def setup_hub(self):\n        \"\"\" Must be called after make_windows(), so that \n            the hub knows the sizes of all the spokes windows\n        \"\"\"\n        if not self._windows_constructed:\n            raise RuntimeError(\n                \"Cannot call setup_hub before memory windows are constructed\"\n            )\n\n        self.initialize_spoke_indices()\n        self.initialize_bound_values()\n\n        if self.has_outerbound_spokes:\n            self.initialize_outer_bound_buffers()\n        if self.has_innerbound_spokes:\n            self.initialize_inner_bound_buffers()\n\n        ## Do some checking for things we currently\n        ## do not support\n        if self.has_w_spokes:\n            raise RuntimeError(\"LShaped hub does not compute dual weights (Ws)\")\n        if self.has_nonant_spokes:\n            self.initialize_nonants()\n        if len(self.outerbound_spoke_indices & self.innerbound_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke providing both inner and outer \"\n                \"bounds is currently unsupported\"\n            )\n\n        ## Generate some warnings if nothing is giving bounds\n        if not self.has_innerbound_spokes:\n            logger.warn(\n                \"No InnerBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )\n\n    def sync(self, send_nonants=True):\n        \"\"\" \n        Manages communication with Bound Spokes\n        \"\"\"\n        if send_nonants and self.has_nonant_spokes:\n            self.send_nonants()\n        if self.has_outerbound_spokes:\n            self.receive_outerbounds()\n        if self.has_innerbound_spokes:\n            self.receive_innerbounds()\n\n    def is_converged(self):\n        \"\"\" Returns a boolean. If True, then LShaped will terminate\n\n        Side-effects:\n            The L-shaped method produces outer bounds during execution,\n            so we will check it as well.\n        \"\"\"\n        bound = self.opt._LShaped_bound\n        self.BestOuterBound = self.OuterBoundUpdate(bound)\n\n        ## log some output\n        if self.global_rank == 0:                \n            self.screen_trace()\n\n        return self.determine_termination()\n\n    def current_iteration(self):\n        \"\"\" Return the current L-shaped iteration.\"\"\"\n        return self.opt.iter\n\n    def main(self):\n        \"\"\" SPComm gets attached in self.__init__ \"\"\" \n        self.opt.lshaped_algorithm()\n\n    def send_nonants(self):\n        \"\"\" Gather nonants and send them to the appropriate spokes\n            TODO: Will likely fail with bundling\n        \"\"\"\n        ci = 0  ## index to self.nonant_send_buffer\n        nonant_send_buffer = self.nonant_send_buffer\n        for k, s in self.opt.local_scenarios.items():\n            nonant_to_root_var_map = s._mpisppy_model.subproblem_to_root_vars_map\n            for xvar in s._mpisppy_data.nonant_indices.values():\n                ## Grab the value from the associated root variable\n                nonant_send_buffer[ci] = nonant_to_root_var_map[xvar]._value\n                ci += 1\n        logging.debug(\"hub is sending X nonants={}\".format(nonant_send_buffer))\n        for idx in self.nonant_spoke_indices:\n            self.hub_to_spoke(nonant_send_buffer, idx)",
  "class APHHub(PHHub):\n    def setup_hub(self):\n        \"\"\" Must be called after make_windows(), so that \n            the hub knows the sizes of all the spokes windows\n        \"\"\"\n        if not self._windows_constructed:\n            raise RuntimeError(\n                \"Cannot call setup_hub before memory windows are constructed\"\n            )\n\n        self.initialize_spoke_indices()\n        self.initialize_bound_values()\n\n        if self.has_outerbound_spokes:\n            ###raise RuntimeError(\"APH not ready for outer bound spokes yet\")\n            self.initialize_outer_bound_buffers()\n        if self.has_innerbound_spokes:\n            self.initialize_inner_bound_buffers()\n        if self.has_w_spokes:\n            ###raise RuntimeError(\"APH not ready for W spokes\")\n            self.initialize_ws()\n        if self.has_nonant_spokes:\n            self.initialize_nonants()\n\n        ## Do some checking for things we currently don't support\n        if len(self.outerbound_spoke_indices & self.innerbound_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke providing both inner and outer \"\n                \"bounds is currently unsupported\"\n            )\n        if len(self.w_spoke_indices & self.nonant_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke needing both Ws and nonants is currently unsupported\"\n            )\n\n        ## Generate some warnings if nothing is giving bounds\n        if not self.has_outerbound_spokes:\n            logger.warn(\n                \"No OuterBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )\n\n        if not self.has_innerbound_spokes:\n            logger.warn(\n                \"No InnerBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )\n\n\n    def sync(self):\n        \"\"\"\n            Manages communication with Spokes\n        \"\"\"\n        if self.has_w_spokes:\n            self.send_ws()\n        if self.has_nonant_spokes:\n            self.send_nonants()\n        if self.has_outerbound_spokes:\n            self.receive_outerbounds()\n        if self.has_innerbound_spokes:\n            self.receive_innerbounds()\n\n\n    def sync_with_spokes(self):\n        self.sync()\n\n    def current_iteration(self):\n        \"\"\" Return the current APH iteration.\"\"\"\n        return self.opt._PHIter\n\n    def main(self):\n        \"\"\" SPComm gets attached by self.__init___; holding APH harmless \"\"\"\n        logger.critical(\"aph debug main in hub.py\")\n        self.opt.APH_main(spcomm=self, finalize=False)\n\n    def finalize(self):\n        \"\"\" does PH.post_loops, returns Eobj \"\"\"\n        # NOTE: APH_main does NOT pass in extensions\n        #       to APH.post_loops\n        Eobj = self.opt.post_loops()\n        return Eobj",
  "def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, spokes, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options=options)\n        assert len(spokes) == self.n_spokes\n        self.local_write_ids = np.zeros(self.n_spokes, dtype=np.int64)\n        self.remote_write_ids = np.zeros(self.n_spokes, dtype=np.int64)\n        self.local_lengths = np.zeros(self.n_spokes, dtype=np.int64)\n        self.remote_lengths = np.zeros(self.n_spokes, dtype=np.int64)\n        # ^^^ Does NOT include +1\n        self.spokes = spokes  # List of dicts\n        logger.debug(f\"Built the hub object on global rank {fullcomm.Get_rank()}\")\n        # for logging\n        self.print_init = True\n        self.latest_ib_char = None\n        self.latest_ob_char = None\n        self.last_ib_idx = None\n        self.last_ob_idx = None\n        # for termination based on stalling out\n        self.stalled_iter_cnt = 0\n        self.last_gap = float('inf')",
  "def setup_hub(self):\n        pass",
  "def sync(self):\n        \"\"\" To be called within the whichever optimization algorithm\n            is being run on the hub (e.g. PH)\n        \"\"\"\n        pass",
  "def is_converged(self):\n        \"\"\" The hub has the ability to halt the optimization algorithm on the\n            hub before any local convergers.\n        \"\"\"\n        pass",
  "def current_iteration(self):\n        \"\"\" Returns the current iteration count - however the hub defines it.\n        \"\"\"\n        pass",
  "def main(self):\n        pass",
  "def clear_latest_chars(self):\n        self.latest_ib_char = None\n        self.latest_ob_char = None",
  "def compute_gaps(self):\n        \"\"\" Compute the current absolute and relative gaps, \n            using the current self.BestInnerBound and self.BestOuterBound\n        \"\"\"\n        if self.opt.is_minimizing:\n            abs_gap = self.BestInnerBound - self.BestOuterBound\n        else:\n            abs_gap = self.BestOuterBound - self.BestInnerBound\n\n        ## define by the best solution, as is common\n        nano = float(\"nan\")  # typing aid\n        if (\n            abs_gap != nano\n            and abs_gap != float(\"inf\")\n            and abs_gap != float(\"-inf\")\n            and self.BestOuterBound != nano\n            and self.BestOuterBound != 0\n        ):\n            rel_gap = abs_gap / abs(self.BestOuterBound)\n        else:\n            rel_gap = float(\"inf\")\n        return abs_gap, rel_gap",
  "def get_update_string(self):\n        if self.latest_ib_char is None and \\\n                self.latest_ob_char is None:\n            return '   '\n        if self.latest_ib_char is None:\n            return self.latest_ob_char + '  '\n        if self.latest_ob_char is None:\n            return '  ' + self.latest_ib_char\n        return self.latest_ob_char+' '+self.latest_ib_char",
  "def screen_trace(self):\n        current_iteration = self.current_iteration()\n        abs_gap, rel_gap = self.compute_gaps()\n        best_solution = self.BestInnerBound\n        best_bound = self.BestOuterBound\n        update_source = self.get_update_string()\n        if self.print_init:\n            row = f'{\"Iter.\":>5s}  {\"   \"}  {\"Best Bound\":>14s}  {\"Best Incumbent\":>14s}  {\"Rel. Gap\":>12s}  {\"Abs. Gap\":>14s}'\n            global_toc(row, True)\n            self.print_init = False\n        row = f\"{current_iteration:5d}  {update_source}  {best_bound:14.4f}  {best_solution:14.4f}  {rel_gap*100:12.3f}%  {abs_gap:14.4f}\"\n        global_toc(row, True)\n        self.clear_latest_chars()",
  "def determine_termination(self):\n        # return True if termination is indicated, otherwise return False\n        \n        if not hasattr(self,\"options\") or self.options is None\\\n           or (\"rel_gap\" not in self.options and \"abs_gap\" not in self.options\\\n           and \"max_stalled_iters\" not in self.options):\n            return False  # Nothing to see here folks...\n        \n        # If we are still here, there is some option for termination\n        abs_gap, rel_gap = self.compute_gaps()\n        \n        abs_gap_satisfied = False\n        rel_gap_satisfied = False\n        max_stalled_satisfied = False\n\n        if \"rel_gap\" in self.options and rel_gap <= self.options[\"rel_gap\"]:\n            rel_gap_satisfied = True \n        if \"abs_gap\" in self.options and abs_gap <= self.options[\"abs_gap\"]:\n            abs_gap_satisfied = True             \n\n        if \"max_stalled_iters\" in self.options:\n            if abs_gap < self.last_gap:  # liberal test (we could use an epsilon)\n                self.last_gap = abs_gap\n                self.stalled_iter_cnt = 0\n            else:\n                self.stalled_iter_cnt += 1\n                if self.stalled_iter_cnt >= self.options[\"max_stalled_iters\"]:\n                    max_stalled_satisfied = True\n            \n        if abs_gap_satisfied:\n            global_toc(f\"Terminating based on inter-cylinder absolute gap {abs_gap:12.4f}\")\n        if rel_gap_satisfied:\n            global_toc(f\"Terminating based on inter-cylinder relative gap {rel_gap*100:12.3f}%\")\n        if max_stalled_satisfied:\n            global_toc(f\"Terminating based on max-stalled-iters {self.stalled_iter_cnt}\")\n\n        return abs_gap_satisfied or rel_gap_satisfied or max_stalled_satisfied",
  "def hub_finalize(self):\n        if self.has_outerbound_spokes:\n            self.receive_outerbounds()\n        if self.has_innerbound_spokes:\n            self.receive_innerbounds()\n\n        if self.global_rank == 0:\n            self.print_init = True\n            global_toc(f\"Statistics at termination\", True)\n            self.screen_trace()",
  "def receive_innerbounds(self):\n        \"\"\" Get inner bounds from inner bound spokes\n            NOTE: Does not check if there _are_ innerbound spokes\n            (but should be harmless to call if there are none)\n        \"\"\"\n        logging.debug(\"Hub is trying to receive from InnerBounds\")\n        for idx in self.innerbound_spoke_indices:\n            is_new = self.hub_from_spoke(self.innerbound_receive_buffers[idx], idx)\n            if is_new:\n                bound = self.innerbound_receive_buffers[idx][0]\n                logging.debug(\"!! new InnerBound to opt {}\".format(bound))\n                self.BestInnerBound = self.InnerBoundUpdate(bound, idx)\n        logging.debug(\"ph back from InnerBounds\")",
  "def receive_outerbounds(self):\n        \"\"\" Get outer bounds from outer bound spokes\n            NOTE: Does not check if there _are_ outerbound spokes\n            (but should be harmless to call if there are none)\n        \"\"\"\n        logging.debug(\"Hub is trying to receive from OuterBounds\")\n        for idx in self.outerbound_spoke_indices:\n            is_new = self.hub_from_spoke(self.outerbound_receive_buffers[idx], idx)\n            if is_new:\n                bound = self.outerbound_receive_buffers[idx][0]\n                logging.debug(\"!! new OuterBound to opt {}\".format(bound))\n                self.BestOuterBound = self.OuterBoundUpdate(bound, idx)\n        logging.debug(\"ph back from OuterBounds\")",
  "def OuterBoundUpdate(self, new_bound, idx=None, char='*'):\n        current_bound = self.BestOuterBound\n        if self._outer_bound_update(new_bound, current_bound):\n            if idx is None:\n                self.latest_ob_char = char\n                self.last_ob_idx = 0\n            else:\n                self.latest_ob_char = self.outerbound_spoke_chars[idx]\n                self.last_ob_idx = idx\n            return new_bound\n        else:\n            return current_bound",
  "def InnerBoundUpdate(self, new_bound, idx=None, char='*'):\n        current_bound = self.BestInnerBound\n        if self._inner_bound_update(new_bound, current_bound):\n            if idx is None:\n                self.latest_ib_char = char\n                self.last_ib_idx = 0\n            else:\n                self.latest_ib_char = self.innerbound_spoke_chars[idx]\n                self.last_ib_idx = idx\n            return new_bound\n        else:\n            return current_bound",
  "def initialize_bound_values(self):\n        if self.opt.is_minimizing:\n            self.BestInnerBound = inf\n            self.BestOuterBound = -inf\n            self._inner_bound_update = lambda new, old : (new < old)\n            self._outer_bound_update = lambda new, old : (new > old)\n        else:\n            self.BestInnerBound = -inf\n            self.BestOuterBound = inf\n            self._inner_bound_update = lambda new, old : (new > old)\n            self._outer_bound_update = lambda new, old : (new < old)",
  "def initialize_outer_bound_buffers(self):\n        \"\"\" Initialize value of BestOuterBound, and outer bound receive buffers\n        \"\"\"\n        self.outerbound_receive_buffers = dict()\n        for idx in self.outerbound_spoke_indices:\n            self.outerbound_receive_buffers[idx] = np.zeros(\n                self.remote_lengths[idx - 1] + 1\n            )",
  "def initialize_inner_bound_buffers(self):\n        \"\"\" Initialize value of BestInnerBound, and inner bound receive buffers\n        \"\"\"\n        self.innerbound_receive_buffers = dict()\n        for idx in self.innerbound_spoke_indices:\n            self.innerbound_receive_buffers[idx] = np.zeros(\n                self.remote_lengths[idx - 1] + 1\n            )",
  "def initialize_nonants(self):\n        \"\"\" Initialize the buffer for the hub to send nonants\n            to the appropriate spokes\n        \"\"\"\n        self.nonant_send_buffer = None\n        for idx in self.nonant_spoke_indices:\n            if self.nonant_send_buffer is None:\n                self.nonant_send_buffer = np.zeros(self.local_lengths[idx - 1] + 1)\n            elif self.local_lengths[idx - 1] + 1 != len(self.nonant_send_buffer):\n                raise RuntimeError(\"Nonant buffers disagree on size\")",
  "def initialize_spoke_indices(self):\n        \"\"\" Figure out what types of spokes we have, \n        and sort them into the appropriate classes.\n\n        Note: \n            Some spokes may be multiple types (e.g. outerbound and nonant),\n            though not all combinations are supported.\n        \"\"\"\n        self.outerbound_spoke_indices = set()\n        self.innerbound_spoke_indices = set()\n        self.nonant_spoke_indices = set()\n        self.w_spoke_indices = set()\n        self.boundsout_spoke_indices = set()\n\n        self.outerbound_spoke_chars = dict()\n        self.innerbound_spoke_chars = dict()\n\n        for (i, spoke) in enumerate(self.spokes):\n            spoke_class = spoke[\"spoke_class\"]\n            if hasattr(spoke_class, \"converger_spoke_types\"):\n                for cst in spoke_class.converger_spoke_types:\n                    if cst == ConvergerSpokeType.OUTER_BOUND:\n                        self.outerbound_spoke_indices.add(i + 1)\n                        self.outerbound_spoke_chars[i+1] = spoke_class.converger_spoke_char\n                    elif cst == ConvergerSpokeType.INNER_BOUND:\n                        self.innerbound_spoke_indices.add(i + 1)\n                        self.innerbound_spoke_chars[i+1] = spoke_class.converger_spoke_char\n                    elif cst == ConvergerSpokeType.W_GETTER:\n                        self.w_spoke_indices.add(i + 1)\n                    elif cst == ConvergerSpokeType.NONANT_GETTER:\n                        self.nonant_spoke_indices.add(i + 1)\n                    elif cst == ConvergerSpokeType.BOUNDS_GETTER:\n                        self.boundsout_spoke_indices.add(i + 1)\n                    else:\n                        raise RuntimeError(f\"Unrecognized converger_spoke_type {cst}\")\n            else:  ##this isn't necessarily wrong, i.e., cut generators\n                logger.debug(f\"Spoke class {spoke_class} not recognized by hub\")\n\n        self.has_outerbound_spokes = len(self.outerbound_spoke_indices) > 0\n        self.has_innerbound_spokes = len(self.innerbound_spoke_indices) > 0\n        self.has_nonant_spokes = len(self.nonant_spoke_indices) > 0\n        self.has_w_spokes = len(self.w_spoke_indices) > 0\n        self.has_boundsout_spokes = len(self.boundsout_spoke_indices) > 0",
  "def make_windows(self):\n        if self._windows_constructed:\n            # different parts of the hub may call make_windows,\n            # we just care about the first call\n            return\n\n        # Spokes notify the hub of the buffer sizes\n        for i in range(self.n_spokes):\n            pair_of_sizes = np.zeros(2, dtype=\"i\")\n            self.strata_comm.Recv((pair_of_sizes, MPI.INT), source=i + 1, tag=i + 1)\n            self.remote_lengths[i] = pair_of_sizes[0]\n            self.local_lengths[i] = pair_of_sizes[1]\n\n        # Make the windows of the appropriate buffer sizes\n        self.windows = [None for _ in range(self.n_spokes)]\n        self.buffers = [None for _ in range(self.n_spokes)]\n        for i in range(self.n_spokes):\n            length = self.local_lengths[i]\n            win, buff = self._make_window(length)\n            self.windows[i] = win\n            self.buffers[i] = buff\n\n        # flag this for multiple calls from the hub\n        self._windows_constructed = True",
  "def hub_to_spoke(self, values, spoke_strata_rank):\n        \"\"\" Put the specified values into the specified locally-owned buffer\n            for the spoke to pick up.\n\n            Notes:\n                This automatically does the -1 indexing\n\n                This assumes that values contains a slot at the end for the\n                write_id\n        \"\"\"\n        expected_length = self.local_lengths[spoke_strata_rank - 1] + 1\n        if len(values) != expected_length:\n            raise RuntimeError(\n                f\"Attempting to put array of length {len(values)} \"\n                f\"into local buffer of length {expected_length}\"\n            )\n        # this is so the spoke ranks all get the same write_id at approximately the same time\n        if not isinstance(self.opt, APH):\n            self.cylinder_comm.Barrier()\n        self.local_write_ids[spoke_strata_rank - 1] += 1\n        values[-1] = self.local_write_ids[spoke_strata_rank - 1]\n        window = self.windows[spoke_strata_rank - 1]\n        window.Lock(self.strata_rank)\n        window.Put((values, len(values), MPI.DOUBLE), self.strata_rank)\n        window.Unlock(self.strata_rank)",
  "def hub_from_spoke(self, values, spoke_num):\n        \"\"\" spoke_num is the rank in the strata_comm, so it is 1-based not 0-based\n            \n            Returns:\n                is_new (bool): Indicates whether the \"gotten\" values are new,\n                    based on the write_id.\n        \"\"\"\n        expected_length = self.remote_lengths[spoke_num - 1] + 1\n        if len(values) != expected_length:\n            raise RuntimeError(\n                f\"Hub trying to get buffer of length {expected_length} \"\n                f\"from spoke, but provided buffer has length {len(values)}.\"\n            )\n        # so the window in each rank gets read at approximately the same time,\n        # and so has the same write_id\n        if not isinstance(self.opt, APH):\n            self.cylinder_comm.Barrier()\n        window = self.windows[spoke_num - 1]\n        window.Lock(spoke_num)\n        window.Get((values, len(values), MPI.DOUBLE), spoke_num)\n        window.Unlock(spoke_num)\n\n        if isinstance(self.opt, APH):\n            # reverting part of changes from Ben getting rid of spoke sleep DLW jan 2023\n            if values[-1] > self.remote_write_ids[spoke_num - 1]:\n                self.remote_write_ids[spoke_num - 1] = values[-1]\n                return True\n        else:\n            new_id = int(values[-1])\n            local_val = np.array((new_id,), 'i')\n            sum_ids = np.zeros(1, 'i')\n            self.cylinder_comm.Allreduce((local_val, MPI.INT),\n                                         (sum_ids, MPI.INT),\n                                         op=MPI.SUM)\n            if new_id != sum_ids[0] / self.cylinder_comm.size:\n                return False\n\n            if (new_id > self.remote_write_ids[spoke_num - 1]) or (new_id < 0):\n                self.remote_write_ids[spoke_num - 1] = new_id\n                return True\n        return False",
  "def send_terminate(self):\n        \"\"\" Send an array of zeros with a -1 appended to the\n            end to indicate termination. This function puts to the local\n            buffer, so every spoke will see it simultaneously. \n            processes (don't need to call them one at a time).\n        \"\"\"\n        for rank in range(1, self.n_spokes + 1):\n            dummies = np.zeros(self.local_lengths[rank - 1] + 1)\n            dummies[-1] = -1\n            window = self.windows[rank - 1]\n            window.Lock(0)\n            window.Put((dummies, len(dummies), MPI.DOUBLE), 0)\n            window.Unlock(0)",
  "def setup_hub(self):\n        \"\"\" Must be called after make_windows(), so that \n            the hub knows the sizes of all the spokes windows\n        \"\"\"\n        if not self._windows_constructed:\n            raise RuntimeError(\n                \"Cannot call setup_hub before memory windows are constructed\"\n            )\n\n        self.initialize_spoke_indices()\n        self.initialize_bound_values()\n\n        if self.has_outerbound_spokes:\n            self.initialize_outer_bound_buffers()\n        if self.has_innerbound_spokes:\n            self.initialize_inner_bound_buffers()\n        if self.has_w_spokes:\n            self.initialize_ws()\n        if self.has_nonant_spokes:\n            self.initialize_nonants()\n        if self.has_boundsout_spokes:\n            self.initialize_boundsout()  # bounds going out\n\n        ## Do some checking for things we currently don't support\n        if len(self.outerbound_spoke_indices & self.innerbound_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke providing both inner and outer \"\n                \"bounds is currently unsupported\"\n            )\n        if len(self.w_spoke_indices & self.nonant_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke needing both Ws and nonants is currently unsupported\"\n            )\n\n        ## Generate some warnings if nothing is giving bounds\n        if not self.has_outerbound_spokes:\n            logger.warn(\n                \"No OuterBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )\n\n        if not self.has_innerbound_spokes:\n            logger.warn(\n                \"No InnerBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )",
  "def sync(self):\n        \"\"\"\n            Manages communication with Spokes\n        \"\"\"\n        if self.has_w_spokes:\n            self.send_ws()\n        if self.has_nonant_spokes:\n            self.send_nonants()\n        if self.has_boundsout_spokes:\n            self.send_boundsout()\n        if self.has_outerbound_spokes:\n            self.receive_outerbounds()\n        if self.has_innerbound_spokes:\n            self.receive_innerbounds()",
  "def sync_with_spokes(self):\n        self.sync()",
  "def is_converged(self):\n        ## might as well get a bound, in this case\n        if self.opt._PHIter == 1:\n            self.BestOuterBound = self.OuterBoundUpdate(self.opt.trivial_bound)\n\n        if not self.has_innerbound_spokes:\n            if self.opt._PHIter == 1:\n                logger.warning(\n                    \"PHHub cannot compute convergence without \"\n                    \"inner bound spokes.\"\n                )\n\n            ## you still want to output status, even without inner bounders configured\n            if self.global_rank == 0:                \n                self.screen_trace()\n                \n            return False\n\n        if not self.has_outerbound_spokes:\n            if self.opt._PHIter == 1:\n                global_toc(\n                    \"Without outer bound spokes, no progress \"\n                    \"will be made on the Best Bound\")\n\n        ## log some output\n        if self.global_rank == 0:\n            self.screen_trace()\n\n        return self.determine_termination()",
  "def current_iteration(self):\n        \"\"\" Return the current PH iteration.\"\"\"\n        return self.opt._PHIter",
  "def main(self):\n        \"\"\" SPComm gets attached in self.__init__ \"\"\"\n        self.opt.ph_main(finalize=False)",
  "def finalize(self):\n        \"\"\" does PH.post_loops, returns Eobj \"\"\"\n        Eobj = self.opt.post_loops(self.opt.extensions)\n        return Eobj",
  "def send_nonants(self):\n        \"\"\" Gather nonants and send them to the appropriate spokes\n            TODO: Will likely fail with bundling\n        \"\"\"\n        self.opt._save_nonants()\n        ci = 0  ## index to self.nonant_send_buffer\n        nonant_send_buffer = self.nonant_send_buffer\n        for k, s in self.opt.local_scenarios.items():\n            for xvar in s._mpisppy_data.nonant_indices.values():\n                nonant_send_buffer[ci] = xvar._value\n                ci += 1\n        logging.debug(\"hub is sending X nonants={}\".format(nonant_send_buffer))\n        for idx in self.nonant_spoke_indices:\n            self.hub_to_spoke(nonant_send_buffer, idx)",
  "def initialize_ws(self):\n        \"\"\" Initialize the buffer for the hub to send dual weights\n            to the appropriate spokes\n        \"\"\"\n        self.w_send_buffer = None\n        for idx in self.w_spoke_indices:\n            if self.w_send_buffer is None:\n                self.w_send_buffer = np.zeros(self.local_lengths[idx - 1] + 1)\n            elif self.local_lengths[idx - 1] + 1 != len(self.w_send_buffer):\n                raise RuntimeError(\"W buffers disagree on size\")",
  "def send_ws(self):\n        \"\"\" Send dual weights to the appropriate spokes\n        \"\"\"\n        self.opt._populate_W_cache(self.w_send_buffer)\n        logging.debug(\"hub is sending Ws={}\".format(self.w_send_buffer))\n        for idx in self.w_spoke_indices:\n            self.hub_to_spoke(self.w_send_buffer, idx)",
  "def initialize_boundsout(self):\n        \"\"\" Initialize the buffer for the hub to send bounds\n            to the appropriate spokes\n        \"\"\"\n        self.boundsout_send_buffer = None\n        for idx in self.boundsout_spoke_indices:\n            if self.boundsout_send_buffer is None:\n                self.boundsout_send_buffer = np.zeros(self.local_lengths[idx - 1] + 1)\n            elif self.local_lengths[idx - 1] + 1 != len(self.boundsout_send_buffer):\n                raise RuntimeError(\"boundsout buffers disagree on size\")",
  "def _populate_boundsout_cache(self):\n        self.boundsout_send_buffer[0] = self.BestOuterBound\n        self.boundsout_send_buffer[1] = self.BestInnerBound",
  "def send_boundsout(self):\n        \"\"\" Send bounds to the appropriate spokes\n        \"\"\"\n        self._populate_boundsout_cache()\n        logging.debug(\"hub is sending bounds={}\".format(self.boundsout_send_buffer))\n        for idx in self.boundsout_spoke_indices:\n            self.hub_to_spoke(self.boundsout_send_buffer, idx)",
  "def setup_hub(self):\n        \"\"\" Must be called after make_windows(), so that \n            the hub knows the sizes of all the spokes windows\n        \"\"\"\n        if not self._windows_constructed:\n            raise RuntimeError(\n                \"Cannot call setup_hub before memory windows are constructed\"\n            )\n\n        self.initialize_spoke_indices()\n        self.initialize_bound_values()\n\n        if self.has_outerbound_spokes:\n            self.initialize_outer_bound_buffers()\n        if self.has_innerbound_spokes:\n            self.initialize_inner_bound_buffers()\n\n        ## Do some checking for things we currently\n        ## do not support\n        if self.has_w_spokes:\n            raise RuntimeError(\"LShaped hub does not compute dual weights (Ws)\")\n        if self.has_nonant_spokes:\n            self.initialize_nonants()\n        if len(self.outerbound_spoke_indices & self.innerbound_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke providing both inner and outer \"\n                \"bounds is currently unsupported\"\n            )\n\n        ## Generate some warnings if nothing is giving bounds\n        if not self.has_innerbound_spokes:\n            logger.warn(\n                \"No InnerBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )",
  "def sync(self, send_nonants=True):\n        \"\"\" \n        Manages communication with Bound Spokes\n        \"\"\"\n        if send_nonants and self.has_nonant_spokes:\n            self.send_nonants()\n        if self.has_outerbound_spokes:\n            self.receive_outerbounds()\n        if self.has_innerbound_spokes:\n            self.receive_innerbounds()",
  "def is_converged(self):\n        \"\"\" Returns a boolean. If True, then LShaped will terminate\n\n        Side-effects:\n            The L-shaped method produces outer bounds during execution,\n            so we will check it as well.\n        \"\"\"\n        bound = self.opt._LShaped_bound\n        self.BestOuterBound = self.OuterBoundUpdate(bound)\n\n        ## log some output\n        if self.global_rank == 0:                \n            self.screen_trace()\n\n        return self.determine_termination()",
  "def current_iteration(self):\n        \"\"\" Return the current L-shaped iteration.\"\"\"\n        return self.opt.iter",
  "def main(self):\n        \"\"\" SPComm gets attached in self.__init__ \"\"\" \n        self.opt.lshaped_algorithm()",
  "def send_nonants(self):\n        \"\"\" Gather nonants and send them to the appropriate spokes\n            TODO: Will likely fail with bundling\n        \"\"\"\n        ci = 0  ## index to self.nonant_send_buffer\n        nonant_send_buffer = self.nonant_send_buffer\n        for k, s in self.opt.local_scenarios.items():\n            nonant_to_root_var_map = s._mpisppy_model.subproblem_to_root_vars_map\n            for xvar in s._mpisppy_data.nonant_indices.values():\n                ## Grab the value from the associated root variable\n                nonant_send_buffer[ci] = nonant_to_root_var_map[xvar]._value\n                ci += 1\n        logging.debug(\"hub is sending X nonants={}\".format(nonant_send_buffer))\n        for idx in self.nonant_spoke_indices:\n            self.hub_to_spoke(nonant_send_buffer, idx)",
  "def setup_hub(self):\n        \"\"\" Must be called after make_windows(), so that \n            the hub knows the sizes of all the spokes windows\n        \"\"\"\n        if not self._windows_constructed:\n            raise RuntimeError(\n                \"Cannot call setup_hub before memory windows are constructed\"\n            )\n\n        self.initialize_spoke_indices()\n        self.initialize_bound_values()\n\n        if self.has_outerbound_spokes:\n            ###raise RuntimeError(\"APH not ready for outer bound spokes yet\")\n            self.initialize_outer_bound_buffers()\n        if self.has_innerbound_spokes:\n            self.initialize_inner_bound_buffers()\n        if self.has_w_spokes:\n            ###raise RuntimeError(\"APH not ready for W spokes\")\n            self.initialize_ws()\n        if self.has_nonant_spokes:\n            self.initialize_nonants()\n\n        ## Do some checking for things we currently don't support\n        if len(self.outerbound_spoke_indices & self.innerbound_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke providing both inner and outer \"\n                \"bounds is currently unsupported\"\n            )\n        if len(self.w_spoke_indices & self.nonant_spoke_indices) > 0:\n            raise RuntimeError(\n                \"A Spoke needing both Ws and nonants is currently unsupported\"\n            )\n\n        ## Generate some warnings if nothing is giving bounds\n        if not self.has_outerbound_spokes:\n            logger.warn(\n                \"No OuterBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )\n\n        if not self.has_innerbound_spokes:\n            logger.warn(\n                \"No InnerBound Spokes defined, this converger \"\n                \"will not cause the hub to terminate\"\n            )",
  "def sync(self):\n        \"\"\"\n            Manages communication with Spokes\n        \"\"\"\n        if self.has_w_spokes:\n            self.send_ws()\n        if self.has_nonant_spokes:\n            self.send_nonants()\n        if self.has_outerbound_spokes:\n            self.receive_outerbounds()\n        if self.has_innerbound_spokes:\n            self.receive_innerbounds()",
  "def sync_with_spokes(self):\n        self.sync()",
  "def current_iteration(self):\n        \"\"\" Return the current APH iteration.\"\"\"\n        return self.opt._PHIter",
  "def main(self):\n        \"\"\" SPComm gets attached by self.__init___; holding APH harmless \"\"\"\n        logger.critical(\"aph debug main in hub.py\")\n        self.opt.APH_main(spcomm=self, finalize=False)",
  "def finalize(self):\n        \"\"\" does PH.post_loops, returns Eobj \"\"\"\n        # NOTE: APH_main does NOT pass in extensions\n        #       to APH.post_loops\n        Eobj = self.opt.post_loops()\n        return Eobj",
  "class ConvergerSpokeType(enum.Enum):\n    OUTER_BOUND = 1\n    INNER_BOUND = 2\n    W_GETTER = 3\n    NONANT_GETTER = 4\n    BOUNDS_GETTER = 5",
  "class Spoke(SPCommunicator):\n    def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options)\n        self.local_write_id = 0\n        self.remote_write_id = 0\n        self.local_length = 0  # Does NOT include the + 1\n        self.remote_length = 0  # Length on hub; does NOT include + 1\n\n        self.last_call_to_got_kill_signal = time.time()\n\n    def _make_windows(self, local_length, remote_length):\n        # Spokes notify the hub of the buffer sizes\n        pair_of_lengths = np.array([local_length, remote_length], dtype=\"i\")\n        self.strata_comm.Send((pair_of_lengths, MPI.INT), dest=0, tag=self.strata_rank)\n        self.local_length = local_length\n        self.remote_length = remote_length\n        \n        # Make the windows of the appropriate buffer sizes\n        # To do?: Spoke should not need to know how many other spokes there are.\n        # Just call a single _make_window()? Do you need to create empty\n        # windows?\n        # ANSWER (dlw July 2020): Since the windows have zero length and since\n        # the number of spokes is not expected to be large, it is probably OK.\n        # The (minor) benefit is that free_windows does not need to know if it\n        # was called by a hub or a spoke. If we ever move to dynamic spoke\n        # creation, then this needs to be reimagined.\n        self.windows = [None for _ in range(self.n_spokes)]\n        self.buffers = [None for _ in range(self.n_spokes)]\n        for i in range(self.n_spokes):\n            length = self.local_length if self.strata_rank == i + 1 else 0\n            win, buff = self._make_window(length)\n            self.windows[i] = win\n            self.buffers[i] = buff\n\n        self._windows_constructed = True\n\n    def spoke_to_hub(self, values):\n        \"\"\" Put the specified values into the locally-owned buffer for the hub\n            to pick up.\n\n            Notes:\n                This automatically does the -1 indexing\n\n                This assumes that values contains a slot at the end for the\n                write_id\n        \"\"\"\n        expected_length = self.local_length + 1\n        if len(values) != expected_length:\n            raise RuntimeError(\n                f\"Attempting to put array of length {len(values)} \"\n                f\"into local buffer of length {expected_length}\"\n            )\n        self.cylinder_comm.Barrier()\n        self.local_write_id += 1\n        values[-1] = self.local_write_id\n        window = self.windows[self.strata_rank - 1]\n        window.Lock(self.strata_rank)\n        window.Put((values, len(values), MPI.DOUBLE), self.strata_rank)\n        window.Unlock(self.strata_rank)\n\n    def spoke_from_hub(self, values):\n        \"\"\"\n        \"\"\"\n        expected_length = self.remote_length + 1\n        if len(values) != expected_length:\n            raise RuntimeError(\n                f\"Spoke trying to get buffer of length {expected_length} \"\n                f\"from hub, but provided buffer has length {len(values)}.\"\n            )\n        self.cylinder_comm.Barrier()\n        window = self.windows[self.strata_rank - 1]\n        window.Lock(0)\n        window.Get((values, len(values), MPI.DOUBLE), 0)\n        window.Unlock(0)\n\n        new_id = int(values[-1])\n        local_val = np.array((new_id,-new_id), 'i')\n        max_min_ids = np.zeros(2, 'i')\n        self.cylinder_comm.Allreduce((local_val, MPI.INT),\n                                     (max_min_ids, MPI.INT),\n                                     op=MPI.MAX)\n\n        max_id = max_min_ids[0]\n        min_id = -max_min_ids[1]\n        # NOTE: we only proceed if all the ranks agree\n        #       on the ID\n        if max_id != min_id:\n            return False\n\n        assert max_id == min_id == new_id\n\n        if (new_id > self.remote_write_id) or (new_id < 0):\n            self.remote_write_id = new_id\n            return True\n        return False\n\n    def got_kill_signal(self):\n        \"\"\" Spoke should call this method at least every iteration\n            to see if the Hub terminated\n        \"\"\"\n        return self._got_kill_signal() \n\n    @abc.abstractmethod\n    def main(self):\n        \"\"\"\n        The main call for the Spoke. Derived classe\n        should call the got_kill_signal method \n        regularly to ensure all ranks terminate \n        with the Hub.\n        \"\"\"\n        pass\n\n    def get_serial_number(self):\n        return self.remote_write_id\n\n    @abc.abstractmethod\n    def _got_kill_signal(self):\n        \"\"\" Every spoke needs a way to get the signal to terminate\n            from the hub\n        \"\"\"\n        pass",
  "class _BoundSpoke(Spoke):\n    \"\"\" A base class for bound spokes\n    \"\"\"\n    def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options)\n        if self.cylinder_rank == 0 and \\\n                'trace_prefix' in spbase_object.options and \\\n                spbase_object.options['trace_prefix'] is not None:\n            trace_prefix = spbase_object.options['trace_prefix']\n\n            filen = trace_prefix+self.__class__.__name__+'.csv'\n            if os.path.exists(filen):\n                raise RuntimeError(f\"Spoke trace file {filen} already exists!\")\n            with open(filen, 'w') as f:\n                f.write(\"time,bound\\n\")\n            self.trace_filen = filen\n            self.start_time = spbase_object.start_time\n        else:\n            self.trace_filen = None\n\n    def make_windows(self):\n        \"\"\" Makes the bound window and a remote window to\n            look for a kill signal\n        \"\"\"\n\n        ## need a remote_length for the kill signal\n        self._make_windows(1, 0)\n        self._kill_sig = np.zeros(0 + 1)\n        self._bound = np.zeros(1 + 1)\n\n    @property\n    def bound(self):\n        return self._bound[0]\n\n    @bound.setter\n    def bound(self, value):\n        self._append_trace(value)\n        self._bound[0] = value\n        self.spoke_to_hub(self._bound)\n\n    def _got_kill_signal(self):\n        \"\"\"Looks for the kill signal and returns True if sent\"\"\"\n        self.spoke_from_hub(self._kill_sig)\n        return self.remote_write_id == -1\n\n    def _append_trace(self, value):\n        if self.cylinder_rank != 0 or self.trace_filen is None:\n            return\n        with open(self.trace_filen, 'a') as f:\n            f.write(f\"{time.perf_counter()-self.start_time},{value}\\n\")",
  "class _BoundNonantLenSpoke(_BoundSpoke):\n    \"\"\" A base class for bound spokes which also\n        want something of len nonants from OPT\n    \"\"\"\n\n    def make_windows(self):\n        \"\"\" Makes the bound window and with a remote buffer long enough\n            to hold an array as long as the nonants.\n\n            Input:\n                opt (SPBase): Must have local_scenarios attached already!\n\n        \"\"\"\n        if not hasattr(self.opt, \"local_scenarios\"):\n            raise RuntimeError(\"Provided SPBase object does not have local_scenarios attribute\")\n\n        if len(self.opt.local_scenarios) == 0:\n            raise RuntimeError(f\"Rank has zero local_scenarios\")\n\n        vbuflen = 0\n        for s in self.opt.local_scenarios.values():\n            vbuflen += len(s._mpisppy_data.nonant_indices)\n\n        self._make_windows(1, vbuflen)\n        self._locals = np.zeros(vbuflen + 1) # Also has kill signal\n        self._bound = np.zeros(1 + 1)\n        self._new_locals = False\n\n    def _got_kill_signal(self):\n        \"\"\" returns True if a kill signal was received, \n            and refreshes the array and _locals\"\"\"\n        self._new_locals = self.spoke_from_hub(self._locals)\n        return self.remote_write_id == -1",
  "class _BoundBoundsOnlySpoke(_BoundSpoke):\n    \"\"\" A base class for bound spokes which only\n        want the best-so-far inner and outer bounds from OPT\n    \"\"\"\n\n    def make_windows(self):\n        \"\"\" Makes the bound window and with a remote buffer long enough\n        \"\"\"\n        vbuflen = 2  # inner and outerbounds\n\n        self._make_windows(1, vbuflen)\n        self._locals = np.zeros(vbuflen + 1) # Also has kill signal\n        self._bound = np.zeros(1 + 1)  # the bound going back to the hub\n        self._new_locals = False\n\n    def _got_kill_signal(self):\n        \"\"\" returns True if a kill signal was received, \n            and refreshes the array and _locals\"\"\"\n        self._new_locals = self.spoke_from_hub(self._locals)\n        return self.remote_write_id == -1\n\n    @property\n    def local_outer_bound(self):\n        \"\"\"Returns the local copy of the bound from the hub\"\"\"\n        return self._locals[0]\n\n    @property\n    def local_inner_bound(self):\n        \"\"\"Returns the local copy of the bound from the hub\"\"\"\n        return self._locals[1]",
  "class InnerBoundSpoke(_BoundSpoke):\n    \"\"\" For Spokes that provide an inner bound through self.bound to the\n        Hub, and do not need information from the main PH OPT hub.\n    \"\"\"\n    converger_spoke_types = (ConvergerSpokeType.INNER_BOUND,)\n    converger_spoke_char = 'I'",
  "class OuterBoundSpoke(_BoundSpoke):\n    \"\"\" For Spokes that provide an outer bound through self.bound to the\n        Hub, and do not need information from the main PH OPT hub.\n    \"\"\"\n    converger_spoke_types = (ConvergerSpokeType.OUTER_BOUND,)\n    converger_spoke_char = 'O'",
  "class _BoundWSpoke(_BoundNonantLenSpoke):\n    \"\"\" A base class for bound spokes which also want the W's from the OPT\n        threads\n    \"\"\"\n\n    @property\n    def localWs(self):\n        \"\"\"Returns the local copy of the weights\"\"\"\n        return self._locals[:-1]\n\n    @property\n    def new_Ws(self):\n        \"\"\" Returns True if the local copy of \n            the weights has been updated since\n            the last call to got_kill_signal\n        \"\"\"\n        return self._new_locals",
  "class OuterBoundWSpoke(_BoundWSpoke):\n    \"\"\"\n    For Spokes that provide an outer bound\n    through self.bound to the Hub,\n    and receive the Ws (or weights) from\n    the main PH OPT hub.\n    \"\"\"\n\n    converger_spoke_types = (\n        ConvergerSpokeType.OUTER_BOUND,\n        ConvergerSpokeType.W_GETTER,\n    )\n    converger_spoke_char = 'O'",
  "class _BoundNonantSpoke(_BoundNonantLenSpoke):\n    \"\"\" A base class for bound spokes which also\n        want the xhat's from the OPT threads\n    \"\"\"\n\n    @property\n    def localnonants(self):\n        \"\"\"Returns the local copy of the nonants\"\"\"\n        return self._locals[:-1]\n\n    @property\n    def new_nonants(self):\n        \"\"\"Returns True if the local copy of \n           the nonants has been updated since\n           the last call to got_kill_signal\"\"\"\n        return self._new_locals",
  "class InnerBoundNonantSpoke(_BoundNonantSpoke):\n    \"\"\" For Spokes that provide an inner (incumbent) \n        bound through self.bound to the Hub,\n        and receive the nonants from\n        the main SPOpt hub.\n\n        Includes some helpful methods for saving\n        and restoring results\n    \"\"\"\n    converger_spoke_types = (\n        ConvergerSpokeType.INNER_BOUND,\n        ConvergerSpokeType.NONANT_GETTER,\n    )\n    converger_spoke_char = 'I'\n\n    def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options)\n        self.is_minimizing = self.opt.is_minimizing\n        self.best_inner_bound = math.inf if self.is_minimizing else -math.inf\n        self.solver_options = None # can be overwritten by derived classes\n\n        # set up best solution cache\n        for k,s in self.opt.local_scenarios.items():\n            s._mpisppy_data.best_solution_cache = None\n\n    def update_if_improving(self, candidate_inner_bound):\n        if candidate_inner_bound is None:\n            return False\n        update = (candidate_inner_bound < self.best_inner_bound) \\\n                if self.is_minimizing else \\\n                (self.best_inner_bound < candidate_inner_bound)\n        if not update:\n            return False\n\n        self.best_inner_bound = candidate_inner_bound\n        # send to hub\n        self.bound = candidate_inner_bound\n        self._cache_best_solution()\n        return True\n\n    def finalize(self):\n        for k,s in self.opt.local_scenarios.items():\n            if s._mpisppy_data.best_solution_cache is None:\n                return None\n            for var, value in s._mpisppy_data.best_solution_cache.items():\n                var.set_value(value, skip_validation=True)\n\n        self.opt.first_stage_solution_available = True\n        self.opt.tree_solution_available = True\n        self.final_bound = self.bound\n        return self.final_bound\n\n    def _cache_best_solution(self):\n        for k,s in self.opt.local_scenarios.items():\n            scenario_cache = ComponentMap()\n            for var in s.component_data_objects(Var):\n                scenario_cache[var] = var.value\n            s._mpisppy_data.best_solution_cache = scenario_cache",
  "class OuterBoundNonantSpoke(_BoundNonantSpoke):\n    \"\"\" For Spokes that provide an outer\n        bound through self.bound to the Hub,\n        and receive the nonants from\n        the main OPT hub.\n    \"\"\"\n    converger_spoke_types = (\n        ConvergerSpokeType.OUTER_BOUND,\n        ConvergerSpokeType.NONANT_GETTER,\n    )\n    converger_spoke_char = 'A'",
  "class OuterBoundBoundsOnlySpoke(_BoundBoundsOnlySpoke):\n    \"\"\" For Spokes that provide an outer\n        bound through self.bound to the Hub,\n        and receive the bounds\n        the main OPT hub.\n    \"\"\"\n    converger_spoke_types = (\n        ConvergerSpokeType.OUTER_BOUND,\n        ConvergerSpokeType.BOUNDS_GETTER,\n    )\n    converger_spoke_char = 'A'",
  "def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options)\n        self.local_write_id = 0\n        self.remote_write_id = 0\n        self.local_length = 0  # Does NOT include the + 1\n        self.remote_length = 0  # Length on hub; does NOT include + 1\n\n        self.last_call_to_got_kill_signal = time.time()",
  "def _make_windows(self, local_length, remote_length):\n        # Spokes notify the hub of the buffer sizes\n        pair_of_lengths = np.array([local_length, remote_length], dtype=\"i\")\n        self.strata_comm.Send((pair_of_lengths, MPI.INT), dest=0, tag=self.strata_rank)\n        self.local_length = local_length\n        self.remote_length = remote_length\n        \n        # Make the windows of the appropriate buffer sizes\n        # To do?: Spoke should not need to know how many other spokes there are.\n        # Just call a single _make_window()? Do you need to create empty\n        # windows?\n        # ANSWER (dlw July 2020): Since the windows have zero length and since\n        # the number of spokes is not expected to be large, it is probably OK.\n        # The (minor) benefit is that free_windows does not need to know if it\n        # was called by a hub or a spoke. If we ever move to dynamic spoke\n        # creation, then this needs to be reimagined.\n        self.windows = [None for _ in range(self.n_spokes)]\n        self.buffers = [None for _ in range(self.n_spokes)]\n        for i in range(self.n_spokes):\n            length = self.local_length if self.strata_rank == i + 1 else 0\n            win, buff = self._make_window(length)\n            self.windows[i] = win\n            self.buffers[i] = buff\n\n        self._windows_constructed = True",
  "def spoke_to_hub(self, values):\n        \"\"\" Put the specified values into the locally-owned buffer for the hub\n            to pick up.\n\n            Notes:\n                This automatically does the -1 indexing\n\n                This assumes that values contains a slot at the end for the\n                write_id\n        \"\"\"\n        expected_length = self.local_length + 1\n        if len(values) != expected_length:\n            raise RuntimeError(\n                f\"Attempting to put array of length {len(values)} \"\n                f\"into local buffer of length {expected_length}\"\n            )\n        self.cylinder_comm.Barrier()\n        self.local_write_id += 1\n        values[-1] = self.local_write_id\n        window = self.windows[self.strata_rank - 1]\n        window.Lock(self.strata_rank)\n        window.Put((values, len(values), MPI.DOUBLE), self.strata_rank)\n        window.Unlock(self.strata_rank)",
  "def spoke_from_hub(self, values):\n        \"\"\"\n        \"\"\"\n        expected_length = self.remote_length + 1\n        if len(values) != expected_length:\n            raise RuntimeError(\n                f\"Spoke trying to get buffer of length {expected_length} \"\n                f\"from hub, but provided buffer has length {len(values)}.\"\n            )\n        self.cylinder_comm.Barrier()\n        window = self.windows[self.strata_rank - 1]\n        window.Lock(0)\n        window.Get((values, len(values), MPI.DOUBLE), 0)\n        window.Unlock(0)\n\n        new_id = int(values[-1])\n        local_val = np.array((new_id,-new_id), 'i')\n        max_min_ids = np.zeros(2, 'i')\n        self.cylinder_comm.Allreduce((local_val, MPI.INT),\n                                     (max_min_ids, MPI.INT),\n                                     op=MPI.MAX)\n\n        max_id = max_min_ids[0]\n        min_id = -max_min_ids[1]\n        # NOTE: we only proceed if all the ranks agree\n        #       on the ID\n        if max_id != min_id:\n            return False\n\n        assert max_id == min_id == new_id\n\n        if (new_id > self.remote_write_id) or (new_id < 0):\n            self.remote_write_id = new_id\n            return True\n        return False",
  "def got_kill_signal(self):\n        \"\"\" Spoke should call this method at least every iteration\n            to see if the Hub terminated\n        \"\"\"\n        return self._got_kill_signal()",
  "def main(self):\n        \"\"\"\n        The main call for the Spoke. Derived classe\n        should call the got_kill_signal method \n        regularly to ensure all ranks terminate \n        with the Hub.\n        \"\"\"\n        pass",
  "def get_serial_number(self):\n        return self.remote_write_id",
  "def _got_kill_signal(self):\n        \"\"\" Every spoke needs a way to get the signal to terminate\n            from the hub\n        \"\"\"\n        pass",
  "def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options)\n        if self.cylinder_rank == 0 and \\\n                'trace_prefix' in spbase_object.options and \\\n                spbase_object.options['trace_prefix'] is not None:\n            trace_prefix = spbase_object.options['trace_prefix']\n\n            filen = trace_prefix+self.__class__.__name__+'.csv'\n            if os.path.exists(filen):\n                raise RuntimeError(f\"Spoke trace file {filen} already exists!\")\n            with open(filen, 'w') as f:\n                f.write(\"time,bound\\n\")\n            self.trace_filen = filen\n            self.start_time = spbase_object.start_time\n        else:\n            self.trace_filen = None",
  "def make_windows(self):\n        \"\"\" Makes the bound window and a remote window to\n            look for a kill signal\n        \"\"\"\n\n        ## need a remote_length for the kill signal\n        self._make_windows(1, 0)\n        self._kill_sig = np.zeros(0 + 1)\n        self._bound = np.zeros(1 + 1)",
  "def bound(self):\n        return self._bound[0]",
  "def bound(self, value):\n        self._append_trace(value)\n        self._bound[0] = value\n        self.spoke_to_hub(self._bound)",
  "def _got_kill_signal(self):\n        \"\"\"Looks for the kill signal and returns True if sent\"\"\"\n        self.spoke_from_hub(self._kill_sig)\n        return self.remote_write_id == -1",
  "def _append_trace(self, value):\n        if self.cylinder_rank != 0 or self.trace_filen is None:\n            return\n        with open(self.trace_filen, 'a') as f:\n            f.write(f\"{time.perf_counter()-self.start_time},{value}\\n\")",
  "def make_windows(self):\n        \"\"\" Makes the bound window and with a remote buffer long enough\n            to hold an array as long as the nonants.\n\n            Input:\n                opt (SPBase): Must have local_scenarios attached already!\n\n        \"\"\"\n        if not hasattr(self.opt, \"local_scenarios\"):\n            raise RuntimeError(\"Provided SPBase object does not have local_scenarios attribute\")\n\n        if len(self.opt.local_scenarios) == 0:\n            raise RuntimeError(f\"Rank has zero local_scenarios\")\n\n        vbuflen = 0\n        for s in self.opt.local_scenarios.values():\n            vbuflen += len(s._mpisppy_data.nonant_indices)\n\n        self._make_windows(1, vbuflen)\n        self._locals = np.zeros(vbuflen + 1) # Also has kill signal\n        self._bound = np.zeros(1 + 1)\n        self._new_locals = False",
  "def _got_kill_signal(self):\n        \"\"\" returns True if a kill signal was received, \n            and refreshes the array and _locals\"\"\"\n        self._new_locals = self.spoke_from_hub(self._locals)\n        return self.remote_write_id == -1",
  "def make_windows(self):\n        \"\"\" Makes the bound window and with a remote buffer long enough\n        \"\"\"\n        vbuflen = 2  # inner and outerbounds\n\n        self._make_windows(1, vbuflen)\n        self._locals = np.zeros(vbuflen + 1) # Also has kill signal\n        self._bound = np.zeros(1 + 1)  # the bound going back to the hub\n        self._new_locals = False",
  "def _got_kill_signal(self):\n        \"\"\" returns True if a kill signal was received, \n            and refreshes the array and _locals\"\"\"\n        self._new_locals = self.spoke_from_hub(self._locals)\n        return self.remote_write_id == -1",
  "def local_outer_bound(self):\n        \"\"\"Returns the local copy of the bound from the hub\"\"\"\n        return self._locals[0]",
  "def local_inner_bound(self):\n        \"\"\"Returns the local copy of the bound from the hub\"\"\"\n        return self._locals[1]",
  "def localWs(self):\n        \"\"\"Returns the local copy of the weights\"\"\"\n        return self._locals[:-1]",
  "def new_Ws(self):\n        \"\"\" Returns True if the local copy of \n            the weights has been updated since\n            the last call to got_kill_signal\n        \"\"\"\n        return self._new_locals",
  "def localnonants(self):\n        \"\"\"Returns the local copy of the nonants\"\"\"\n        return self._locals[:-1]",
  "def new_nonants(self):\n        \"\"\"Returns True if the local copy of \n           the nonants has been updated since\n           the last call to got_kill_signal\"\"\"\n        return self._new_locals",
  "def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options)\n        self.is_minimizing = self.opt.is_minimizing\n        self.best_inner_bound = math.inf if self.is_minimizing else -math.inf\n        self.solver_options = None # can be overwritten by derived classes\n\n        # set up best solution cache\n        for k,s in self.opt.local_scenarios.items():\n            s._mpisppy_data.best_solution_cache = None",
  "def update_if_improving(self, candidate_inner_bound):\n        if candidate_inner_bound is None:\n            return False\n        update = (candidate_inner_bound < self.best_inner_bound) \\\n                if self.is_minimizing else \\\n                (self.best_inner_bound < candidate_inner_bound)\n        if not update:\n            return False\n\n        self.best_inner_bound = candidate_inner_bound\n        # send to hub\n        self.bound = candidate_inner_bound\n        self._cache_best_solution()\n        return True",
  "def finalize(self):\n        for k,s in self.opt.local_scenarios.items():\n            if s._mpisppy_data.best_solution_cache is None:\n                return None\n            for var, value in s._mpisppy_data.best_solution_cache.items():\n                var.set_value(value, skip_validation=True)\n\n        self.opt.first_stage_solution_available = True\n        self.opt.tree_solution_available = True\n        self.final_bound = self.bound\n        return self.final_bound",
  "def _cache_best_solution(self):\n        for k,s in self.opt.local_scenarios.items():\n            scenario_cache = ComponentMap()\n            for var in s.component_data_objects(Var):\n                scenario_cache[var] = var.value\n            s._mpisppy_data.best_solution_cache = scenario_cache",
  "class XhatLShapedInnerBound(spoke.InnerBoundNonantSpoke):\n\n    converger_spoke_char = 'X'\n\n    def xhatlshaped_prep(self):\n        verbose = self.opt.options['verbose']\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatLShapedInnerBound must be used with Xhat_Eval.\")\n\n        teeme = False\n        if \"tee-rank0-solves\" in self.opt.options:\n            teeme = self.opt.options['tee-rank0-solves']\n\n        self.opt.solve_loop(\n            solver_options=self.opt.current_solver_options,\n            dtiming=False,\n            gripe=True,\n            tee=teeme,\n            verbose=verbose\n        )\n        self.opt._update_E1()  # Apologies for doing this after the solves...\n        if abs(1 - self.opt.E1) > self.opt.E1_tolerance:\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.opt.E1)\n                print(\"E1_tolerance = \", self.opt.E1_tolerance)\n            quit()\n        infeasP = self.opt.infeas_prob()\n        if infeasP != 0.:\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Infeasibility detected; E_infeas, E1=\", infeasP, self.opt.E1)\n            quit()\n\n        self.opt._save_nonants() # make the cache\n\n        self.opt.current_solver_options = self.opt.options[\"iterk_solver_options\"]\n        ### end iter0 stuff\n\n    def main(self):\n\n        self.xhatlshaped_prep()\n        is_minimizing = self.opt.is_minimizing\n\n        self.ib = inf if is_minimizing else -inf\n\n        #xh_iter = 1\n        while not self.got_kill_signal():\n\n            if self.new_nonants:\n                \n                self.opt._put_nonant_cache(self.localnonants)\n                self.opt._restore_nonants()\n                obj = self.opt.calculate_incumbent(fix_nonants=True)\n\n                if obj is None:\n                    continue\n\n                self.update_if_improving(obj)",
  "def xhatlshaped_prep(self):\n        verbose = self.opt.options['verbose']\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatLShapedInnerBound must be used with Xhat_Eval.\")\n\n        teeme = False\n        if \"tee-rank0-solves\" in self.opt.options:\n            teeme = self.opt.options['tee-rank0-solves']\n\n        self.opt.solve_loop(\n            solver_options=self.opt.current_solver_options,\n            dtiming=False,\n            gripe=True,\n            tee=teeme,\n            verbose=verbose\n        )\n        self.opt._update_E1()  # Apologies for doing this after the solves...\n        if abs(1 - self.opt.E1) > self.opt.E1_tolerance:\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.opt.E1)\n                print(\"E1_tolerance = \", self.opt.E1_tolerance)\n            quit()\n        infeasP = self.opt.infeas_prob()\n        if infeasP != 0.:\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Infeasibility detected; E_infeas, E1=\", infeasP, self.opt.E1)\n            quit()\n\n        self.opt._save_nonants() # make the cache\n\n        self.opt.current_solver_options = self.opt.options[\"iterk_solver_options\"]",
  "def main(self):\n\n        self.xhatlshaped_prep()\n        is_minimizing = self.opt.is_minimizing\n\n        self.ib = inf if is_minimizing else -inf\n\n        #xh_iter = 1\n        while not self.got_kill_signal():\n\n            if self.new_nonants:\n                \n                self.opt._put_nonant_cache(self.localnonants)\n                self.opt._restore_nonants()\n                obj = self.opt.calculate_incumbent(fix_nonants=True)\n\n                if obj is None:\n                    continue\n\n                self.update_if_improving(obj)",
  "def _attach_xbars(opt):\n    # attach xbars to an Xhat_Eval object given as opt\n    for scenario in opt.local_scenarios.values():\n        scenario._mpisppy_model.xbars = pyo.Param(\n            scenario._mpisppy_data.nonant_indices.keys(), initialize=0.0, mutable=True\n        )\n        scenario._mpisppy_model.xsqbars = pyo.Param(\n            scenario._mpisppy_data.nonant_indices.keys(), initialize=0.0, mutable=True\n        )",
  "class XhatXbarInnerBound(spoke.InnerBoundNonantSpoke):\n\n    converger_spoke_char = 'B'\n\n    def ib_prep(self):\n        \"\"\"\n        Set up the objects needed for bounding.\n\n        Returns:\n            xhatter (xhatxbar object): Constructed by a call to Prep\n        \"\"\"\n        if \"bundles_per_rank\" in self.opt.options\\\n           and self.opt.options[\"bundles_per_rank\"] != 0:\n            raise RuntimeError(\"xhat spokes cannot have bundles (yet)\")\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatXbarInnerBound must be used with Xhat_Eval.\")\n\n        verbose = self.opt.options['verbose']\n        xhatter = XhatXbar(self.opt)\n        # somehow deal with the prox option .... TBD .... important for aph APH\n\n        # begin iter0 stuff\n        xhatter.pre_iter0()\n        self.opt._save_original_nonants()\n\n        self.opt._lazy_create_solvers()  # no iter0 loop, but we need the solvers\n\n        self.opt._update_E1()  \n        if (abs(1 - self.opt.E1) > self.opt.E1_tolerance):\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.opt.E1)\n                print(\"E1_tolerance = \", self.opt.E1_tolerance)\n            quit()\n\n        ### end iter0 stuff\n\n        xhatter.post_iter0()\n        print(\"about to attach xbars\")\n        _attach_xbars(self.opt)\n        self.opt._save_nonants()  # make the cache\n\n        return xhatter\n\n    def main(self):\n        \"\"\"\n        Entry point. Communicates with the optimization companion.\n\n        \"\"\"\n        dtm = logging.getLogger(f'dtm{global_rank}')\n        verbose = self.opt.options[\"verbose\"] # typing aid  \n        logging.debug(\"Enter xhatxbar main on rank {}\".format(global_rank))\n\n        xhatter = self.ib_prep()\n\n        ib_iter = 1  # ib is for inner bound\n        got_kill_signal = False\n        while (not self.got_kill_signal()):\n            logging.debug('   IB loop iter={} on global rank {}'.\\\n                          format(ib_iter, global_rank))\n            # _log_values(ib_iter, self._locals, dtm)\n\n            logging.debug('   IB got from opt on global rank {}'.\\\n                          format(global_rank))\n            if (self.new_nonants):\n                logging.debug('  and its new! on global rank {}'.\\\n                              format(global_rank))\n                logging.debug('  localnonants={}'.format(str(self.localnonants)))\n                self.opt._put_nonant_cache(self.localnonants)  # don't really need all caches\n                self.opt._restore_nonants()\n                innerbound = xhatter.xhat_tryit(restore_nonants=False)\n\n                self.update_if_improving(innerbound)\n\n            ib_iter += 1\n\n        dtm.debug(f'IB xbar thread ran {ib_iter} iterations\\n')",
  "def ib_prep(self):\n        \"\"\"\n        Set up the objects needed for bounding.\n\n        Returns:\n            xhatter (xhatxbar object): Constructed by a call to Prep\n        \"\"\"\n        if \"bundles_per_rank\" in self.opt.options\\\n           and self.opt.options[\"bundles_per_rank\"] != 0:\n            raise RuntimeError(\"xhat spokes cannot have bundles (yet)\")\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatXbarInnerBound must be used with Xhat_Eval.\")\n\n        verbose = self.opt.options['verbose']\n        xhatter = XhatXbar(self.opt)\n        # somehow deal with the prox option .... TBD .... important for aph APH\n\n        # begin iter0 stuff\n        xhatter.pre_iter0()\n        self.opt._save_original_nonants()\n\n        self.opt._lazy_create_solvers()  # no iter0 loop, but we need the solvers\n\n        self.opt._update_E1()  \n        if (abs(1 - self.opt.E1) > self.opt.E1_tolerance):\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.opt.E1)\n                print(\"E1_tolerance = \", self.opt.E1_tolerance)\n            quit()\n\n        ### end iter0 stuff\n\n        xhatter.post_iter0()\n        print(\"about to attach xbars\")\n        _attach_xbars(self.opt)\n        self.opt._save_nonants()  # make the cache\n\n        return xhatter",
  "def main(self):\n        \"\"\"\n        Entry point. Communicates with the optimization companion.\n\n        \"\"\"\n        dtm = logging.getLogger(f'dtm{global_rank}')\n        verbose = self.opt.options[\"verbose\"] # typing aid  \n        logging.debug(\"Enter xhatxbar main on rank {}\".format(global_rank))\n\n        xhatter = self.ib_prep()\n\n        ib_iter = 1  # ib is for inner bound\n        got_kill_signal = False\n        while (not self.got_kill_signal()):\n            logging.debug('   IB loop iter={} on global rank {}'.\\\n                          format(ib_iter, global_rank))\n            # _log_values(ib_iter, self._locals, dtm)\n\n            logging.debug('   IB got from opt on global rank {}'.\\\n                          format(global_rank))\n            if (self.new_nonants):\n                logging.debug('  and its new! on global rank {}'.\\\n                              format(global_rank))\n                logging.debug('  localnonants={}'.format(str(self.localnonants)))\n                self.opt._put_nonant_cache(self.localnonants)  # don't really need all caches\n                self.opt._restore_nonants()\n                innerbound = xhatter.xhat_tryit(restore_nonants=False)\n\n                self.update_if_improving(innerbound)\n\n            ib_iter += 1\n\n        dtm.debug(f'IB xbar thread ran {ib_iter} iterations\\n')",
  "class _SlamHeuristic(spoke.InnerBoundNonantSpoke):\n\n    converger_spoke_char = 'S'\n\n    @property\n    @abc.abstractmethod\n    def numpy_op(self):\n        pass\n\n    @property\n    @abc.abstractmethod\n    def mpi_op(self):\n        pass\n\n    def slam_heur_prep(self):\n        if self.opt.multistage:\n            raise RuntimeError(f'The {self.__class__.__name__} only supports '\n                               'two-stage models at this time.')\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(f\"{self.__class__.__name__} must be used with Xhat_Eval.\")\n        verbose = self.opt.options['verbose']\n\n        logger.debug(f\"{self.__class__.__name__} spoke back from PH_Prep rank {self.global_rank}\")\n\n        self.tee = False\n        if \"tee-rank0-solves\" in self.opt.options:\n            self.tee = self.opt.options['tee-rank0-solves']\n\n        self.verbose = verbose\n\n        self.opt._update_E1()\n        self.opt._lazy_create_solvers()\n\n    def extract_local_candidate_soln(self):\n        num_scen = len(self.opt.local_scenarios)\n        num_vars = len(self.localnonants) // num_scen\n        assert(num_scen * num_vars == len(self.localnonants))\n        ## matrix with num_scen rows and num_vars columns\n        nonant_matrix = np.reshape(self.localnonants, (num_scen, num_vars))\n\n        ## maximize almong the local sceanrios\n        local_candidate = self.numpy_op(nonant_matrix, axis=0)\n        assert len(local_candidate) == num_vars\n        return local_candidate\n\n    def main(self):\n        self.slam_heur_prep()\n\n        slam_iter = 1\n        while not self.got_kill_signal():\n            if (slam_iter-1) % 10000 == 0:\n                logger.debug(f'   {self.__class__.__name__} loop iter={slam_iter} on rank {self.global_rank}')\n                logger.debug(f'   {self.__class__.__name__} got from opt on rank {self.global_rank}')\n\n            if self.new_nonants:\n                \n                local_candidate = self.extract_local_candidate_soln()\n\n                global_candidate = np.empty_like(local_candidate)\n\n                self.cylinder_comm.Allreduce(local_candidate, global_candidate, op=self.mpi_op)\n\n                '''\n                ## round the candidate\n                candidate = global_candidate.round()\n                '''\n\n                # Everyone has the candidate solution at this point\n                for s in self.opt.local_scenarios.values():\n                    is_pers = sputils.is_persistent(s._solver_plugin)\n                    solver = s._solver_plugin if is_pers else None\n\n                    for ix, var in enumerate(s._mpisppy_data.nonant_indices.values()):\n                        var.fix(global_candidate[ix])\n                        if (is_pers):\n                            solver.update_var(var)\n\n                obj = self.opt.calculate_incumbent(fix_nonants=False)\n\n                self.update_if_improving(obj)\n                \n            slam_iter += 1",
  "class SlamMaxHeuristic(_SlamHeuristic):\n\n    @property\n    def numpy_op(self):\n        return np.amax\n\n    @property\n    def mpi_op(self):\n        return mpi.MAX",
  "class SlamMinHeuristic(_SlamHeuristic):\n\n    @property\n    def numpy_op(self):\n        return np.amin\n\n    @property\n    def mpi_op(self):\n        return mpi.MIN",
  "def numpy_op(self):\n        pass",
  "def mpi_op(self):\n        pass",
  "def slam_heur_prep(self):\n        if self.opt.multistage:\n            raise RuntimeError(f'The {self.__class__.__name__} only supports '\n                               'two-stage models at this time.')\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(f\"{self.__class__.__name__} must be used with Xhat_Eval.\")\n        verbose = self.opt.options['verbose']\n\n        logger.debug(f\"{self.__class__.__name__} spoke back from PH_Prep rank {self.global_rank}\")\n\n        self.tee = False\n        if \"tee-rank0-solves\" in self.opt.options:\n            self.tee = self.opt.options['tee-rank0-solves']\n\n        self.verbose = verbose\n\n        self.opt._update_E1()\n        self.opt._lazy_create_solvers()",
  "def extract_local_candidate_soln(self):\n        num_scen = len(self.opt.local_scenarios)\n        num_vars = len(self.localnonants) // num_scen\n        assert(num_scen * num_vars == len(self.localnonants))\n        ## matrix with num_scen rows and num_vars columns\n        nonant_matrix = np.reshape(self.localnonants, (num_scen, num_vars))\n\n        ## maximize almong the local sceanrios\n        local_candidate = self.numpy_op(nonant_matrix, axis=0)\n        assert len(local_candidate) == num_vars\n        return local_candidate",
  "def main(self):\n        self.slam_heur_prep()\n\n        slam_iter = 1\n        while not self.got_kill_signal():\n            if (slam_iter-1) % 10000 == 0:\n                logger.debug(f'   {self.__class__.__name__} loop iter={slam_iter} on rank {self.global_rank}')\n                logger.debug(f'   {self.__class__.__name__} got from opt on rank {self.global_rank}')\n\n            if self.new_nonants:\n                \n                local_candidate = self.extract_local_candidate_soln()\n\n                global_candidate = np.empty_like(local_candidate)\n\n                self.cylinder_comm.Allreduce(local_candidate, global_candidate, op=self.mpi_op)\n\n                '''\n                ## round the candidate\n                candidate = global_candidate.round()\n                '''\n\n                # Everyone has the candidate solution at this point\n                for s in self.opt.local_scenarios.values():\n                    is_pers = sputils.is_persistent(s._solver_plugin)\n                    solver = s._solver_plugin if is_pers else None\n\n                    for ix, var in enumerate(s._mpisppy_data.nonant_indices.values()):\n                        var.fix(global_candidate[ix])\n                        if (is_pers):\n                            solver.update_var(var)\n\n                obj = self.opt.calculate_incumbent(fix_nonants=False)\n\n                self.update_if_improving(obj)\n                \n            slam_iter += 1",
  "def numpy_op(self):\n        return np.amax",
  "def mpi_op(self):\n        return mpi.MAX",
  "def numpy_op(self):\n        return np.amin",
  "def mpi_op(self):\n        return mpi.MIN",
  "class XhatSpecificInnerBound(spoke.InnerBoundNonantSpoke):\n\n    converger_spoke_char = 'S'\n\n    def ib_prep(self):\n        \"\"\"\n        Set up the objects needed for bounding.\n\n        Returns:\n            xhatter (xhatspecific object): Constructed by a call to Prep\n        \"\"\"\n        if \"bundles_per_rank\" in self.opt.options\\\n           and self.opt.options[\"bundles_per_rank\"] != 0:\n            raise RuntimeError(\"xhat spokes cannot have bundles (yet)\")\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatShuffleInnerBound must be used with Xhat_Eval.\")\n\n        verbose = self.opt.options['verbose']\n        xhatter = XhatSpecific(self.opt)\n        # somehow deal with the prox option .... TBD .... important for aph APH\n\n        # begin iter0 stuff\n        xhatter.pre_iter0()\n        self.opt._save_original_nonants()\n\n        self.opt._lazy_create_solvers()  # no iter0 loop, but we need the solvers\n\n        self.opt._update_E1()  \n        if (abs(1 - self.opt.E1) > self.opt.E1_tolerance):\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.opt.E1)\n                print(\"E1_tolerance = \", self.opt.E1_tolerance)\n            quit()\n\n        ### end iter0 stuff\n\n        xhatter.post_iter0()\n        self.opt._save_nonants()  # make the cache\n\n        return xhatter\n\n    def main(self):\n        \"\"\"\n        Entry point. Communicates with the optimization companion.\n\n        \"\"\"\n        dtm = logging.getLogger(f'dtm{global_rank}')\n        verbose = self.opt.options[\"verbose\"] # typing aid  \n        logging.debug(\"Enter xhatspecific main on rank {}\".format(global_rank))\n\n        # What to try does not change, but the data in the scenarios should\n        xhat_scenario_dict = self.opt.options[\"xhat_specific_options\"]\\\n                                             [\"xhat_scenario_dict\"]\n\n        xhatter = self.ib_prep()\n\n        ib_iter = 1  # ib is for inner bound\n        got_kill_signal = False\n        while (not self.got_kill_signal()):\n            logging.debug('   IB loop iter={} on global rank {}'.\\\n                          format(ib_iter, global_rank))\n            # _log_values(ib_iter, self._locals, dtm)\n\n            logging.debug('   IB got from opt on global rank {}'.\\\n                          format(global_rank))\n            if (self.new_nonants):\n                logging.debug('  and its new! on global rank {}'.\\\n                              format(global_rank))\n                logging.debug('  localnonants={}'.format(str(self.localnonants)))\n\n                self.opt._put_nonant_cache(self.localnonants)  # don't really need all caches\n                self.opt._restore_nonants()\n                innerbound = xhatter.xhat_tryit(xhat_scenario_dict, restore_nonants=False)\n\n                self.update_if_improving(innerbound)\n\n            ib_iter += 1\n\n        dtm.debug(f'IB specific thread ran {ib_iter} iterations\\n')",
  "def ib_prep(self):\n        \"\"\"\n        Set up the objects needed for bounding.\n\n        Returns:\n            xhatter (xhatspecific object): Constructed by a call to Prep\n        \"\"\"\n        if \"bundles_per_rank\" in self.opt.options\\\n           and self.opt.options[\"bundles_per_rank\"] != 0:\n            raise RuntimeError(\"xhat spokes cannot have bundles (yet)\")\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatShuffleInnerBound must be used with Xhat_Eval.\")\n\n        verbose = self.opt.options['verbose']\n        xhatter = XhatSpecific(self.opt)\n        # somehow deal with the prox option .... TBD .... important for aph APH\n\n        # begin iter0 stuff\n        xhatter.pre_iter0()\n        self.opt._save_original_nonants()\n\n        self.opt._lazy_create_solvers()  # no iter0 loop, but we need the solvers\n\n        self.opt._update_E1()  \n        if (abs(1 - self.opt.E1) > self.opt.E1_tolerance):\n            if self.opt.cylinder_rank == 0:\n                print(\"ERROR\")\n                print(\"Total probability of scenarios was \", self.opt.E1)\n                print(\"E1_tolerance = \", self.opt.E1_tolerance)\n            quit()\n\n        ### end iter0 stuff\n\n        xhatter.post_iter0()\n        self.opt._save_nonants()  # make the cache\n\n        return xhatter",
  "def main(self):\n        \"\"\"\n        Entry point. Communicates with the optimization companion.\n\n        \"\"\"\n        dtm = logging.getLogger(f'dtm{global_rank}')\n        verbose = self.opt.options[\"verbose\"] # typing aid  \n        logging.debug(\"Enter xhatspecific main on rank {}\".format(global_rank))\n\n        # What to try does not change, but the data in the scenarios should\n        xhat_scenario_dict = self.opt.options[\"xhat_specific_options\"]\\\n                                             [\"xhat_scenario_dict\"]\n\n        xhatter = self.ib_prep()\n\n        ib_iter = 1  # ib is for inner bound\n        got_kill_signal = False\n        while (not self.got_kill_signal()):\n            logging.debug('   IB loop iter={} on global rank {}'.\\\n                          format(ib_iter, global_rank))\n            # _log_values(ib_iter, self._locals, dtm)\n\n            logging.debug('   IB got from opt on global rank {}'.\\\n                          format(global_rank))\n            if (self.new_nonants):\n                logging.debug('  and its new! on global rank {}'.\\\n                              format(global_rank))\n                logging.debug('  localnonants={}'.format(str(self.localnonants)))\n\n                self.opt._put_nonant_cache(self.localnonants)  # don't really need all caches\n                self.opt._restore_nonants()\n                innerbound = xhatter.xhat_tryit(xhat_scenario_dict, restore_nonants=False)\n\n                self.update_if_improving(innerbound)\n\n            ib_iter += 1\n\n        dtm.debug(f'IB specific thread ran {ib_iter} iterations\\n')",
  "class LagrangianOuterBound(mpisppy.cylinders.spoke.OuterBoundWSpoke):\n\n    converger_spoke_char = 'L'\n\n    def lagrangian_prep(self):\n        verbose = self.opt.options['verbose']\n        # Split up PH_Prep? Prox option is important for APH.\n        # Seems like we shouldn't need the Lagrangian stuff, so attach_prox=False\n        # Scenarios are created here\n        self.opt.PH_Prep(attach_prox=False)\n        self.opt._reenable_W()\n        self.opt.subproblem_creation(verbose)\n        self.opt._create_solvers()\n\n    def lagrangian(self):\n        verbose = self.opt.options['verbose']\n        # This is sort of a hack, but might help folks:\n        if \"ipopt\" in self.opt.options[\"solver_name\"]:\n            print(\"\\n WARNING: An ipopt solver will not give outer bounds\\n\")\n        teeme = False\n        if \"tee-rank0-solves\" in self.opt.options:\n            teeme = self.opt.options['tee-rank0-solves']\n\n        self.opt.solve_loop(\n            solver_options=self.opt.current_solver_options,\n            dtiming=False,\n            gripe=True,\n            tee=teeme,\n            verbose=verbose\n        )\n        ''' DTM (dlw edits): This is where PHBase Iter0 checks for scenario\n            probabilities that don't sum to one and infeasibility and\n            will send a kill signal if needed. For now we are relying\n            on the fact that the OPT thread is solving the same\n            models, and hence would detect both of those things on its\n            own--the Lagrangian spoke doesn't need to check again.  '''\n\n        # Compute the resulting bound, checking to be sure\n        # the weights came from the same PH iteration\n        serial_number = self.get_serial_number()\n        bound, extra_sums  = self.opt.Ebound(verbose, extra_sum_terms=[serial_number])\n        serial_number_sum = int(round(extra_sums[0]))\n\n        total = int(self.cylinder_comm.Get_size())*serial_number\n        if total == serial_number_sum:\n            return bound\n        elif self.cylinder_rank == 0:\n            # TODO: this whole check can probably be removed as its done\n            #       within `got_kill_signal`. Leaving it for now as an\n            #       additional check.\n            raise RuntimeError(\"Lagrangian spokes unexpectly out of snyc\")\n        return None\n\n    def _set_weights_and_solve(self):\n        self.opt.W_from_flat_list(self.localWs) # Sets the weights\n        return self.lagrangian()\n\n    def main(self):\n        # The rho_setter should be attached to the opt object\n        rho_setter = None\n        if hasattr(self.opt, 'rho_setter'):\n            rho_setter = self.opt.rho_setter\n\n        self.lagrangian_prep()\n\n        self.dk_iter = 1\n        self.trivial_bound = self.lagrangian()\n\n        self.opt.current_solver_options = self.opt.iterk_solver_options\n\n        self.bound = self.trivial_bound\n\n        while not self.got_kill_signal():\n            if self.new_Ws:\n                bound = self._set_weights_and_solve()\n                if bound is not None:\n                    self.bound = bound\n                self.dk_iter += 1\n\n    def finalize(self):\n        '''\n        Do one final lagrangian pass with the final\n        PH weights. Useful for when PH convergence\n        and/or iteration limit is the cause of termination\n        '''\n        self.final_bound = self._set_weights_and_solve()\n        self.bound = self.final_bound\n        if self.opt.extensions is not None and \\\n            hasattr(self.opt.extobject, 'post_everything'):\n            self.opt.extobject.post_everything()\n        return self.final_bound",
  "def lagrangian_prep(self):\n        verbose = self.opt.options['verbose']\n        # Split up PH_Prep? Prox option is important for APH.\n        # Seems like we shouldn't need the Lagrangian stuff, so attach_prox=False\n        # Scenarios are created here\n        self.opt.PH_Prep(attach_prox=False)\n        self.opt._reenable_W()\n        self.opt.subproblem_creation(verbose)\n        self.opt._create_solvers()",
  "def lagrangian(self):\n        verbose = self.opt.options['verbose']\n        # This is sort of a hack, but might help folks:\n        if \"ipopt\" in self.opt.options[\"solver_name\"]:\n            print(\"\\n WARNING: An ipopt solver will not give outer bounds\\n\")\n        teeme = False\n        if \"tee-rank0-solves\" in self.opt.options:\n            teeme = self.opt.options['tee-rank0-solves']\n\n        self.opt.solve_loop(\n            solver_options=self.opt.current_solver_options,\n            dtiming=False,\n            gripe=True,\n            tee=teeme,\n            verbose=verbose\n        )\n        ''' DTM (dlw edits): This is where PHBase Iter0 checks for scenario\n            probabilities that don't sum to one and infeasibility and\n            will send a kill signal if needed. For now we are relying\n            on the fact that the OPT thread is solving the same\n            models, and hence would detect both of those things on its\n            own--the Lagrangian spoke doesn't need to check again.  '''\n\n        # Compute the resulting bound, checking to be sure\n        # the weights came from the same PH iteration\n        serial_number = self.get_serial_number()\n        bound, extra_sums  = self.opt.Ebound(verbose, extra_sum_terms=[serial_number])\n        serial_number_sum = int(round(extra_sums[0]))\n\n        total = int(self.cylinder_comm.Get_size())*serial_number\n        if total == serial_number_sum:\n            return bound\n        elif self.cylinder_rank == 0:\n            # TODO: this whole check can probably be removed as its done\n            #       within `got_kill_signal`. Leaving it for now as an\n            #       additional check.\n            raise RuntimeError(\"Lagrangian spokes unexpectly out of snyc\")\n        return None",
  "def _set_weights_and_solve(self):\n        self.opt.W_from_flat_list(self.localWs) # Sets the weights\n        return self.lagrangian()",
  "def main(self):\n        # The rho_setter should be attached to the opt object\n        rho_setter = None\n        if hasattr(self.opt, 'rho_setter'):\n            rho_setter = self.opt.rho_setter\n\n        self.lagrangian_prep()\n\n        self.dk_iter = 1\n        self.trivial_bound = self.lagrangian()\n\n        self.opt.current_solver_options = self.opt.iterk_solver_options\n\n        self.bound = self.trivial_bound\n\n        while not self.got_kill_signal():\n            if self.new_Ws:\n                bound = self._set_weights_and_solve()\n                if bound is not None:\n                    self.bound = bound\n                self.dk_iter += 1",
  "def finalize(self):\n        '''\n        Do one final lagrangian pass with the final\n        PH weights. Useful for when PH convergence\n        and/or iteration limit is the cause of termination\n        '''\n        self.final_bound = self._set_weights_and_solve()\n        self.bound = self.final_bound\n        if self.opt.extensions is not None and \\\n            hasattr(self.opt.extobject, 'post_everything'):\n            self.opt.extobject.post_everything()\n        return self.final_bound",
  "class CrossScenarioCutSpoke(spoke.Spoke):\n    def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options=options)\n\n    def make_windows(self):\n        nscen = len(self.opt.all_scenario_names)\n        if nscen == 0:\n            raise RuntimeError(f\"(rank: {self.cylinder_rank}), no local_scenarios\")\n\n        self.nscen = nscen\n        vbuflen = 0\n        self.nonant_per_scen = 0\n        for s in self.opt.local_scenarios.values():\n            vbuflen += len(s._mpisppy_data.nonant_indices)\n        local_scen_count = len(self.opt.local_scenario_names)\n        self.nonant_per_scen = int(vbuflen / local_scen_count)\n\n        ## the _locals will also have the kill signal\n        self.all_nonant_len = vbuflen\n        self.all_eta_len = nscen*local_scen_count\n        self._locals = np.zeros(nscen*local_scen_count + vbuflen + 1)\n        self._coefs = np.zeros(nscen*(nscen + self.nonant_per_scen) + 1 + 1)\n        self._new_locals = False\n\n        # local, remote\n        # send, receive\n        self._make_windows(nscen*(self.nonant_per_scen + 1 + 1), nscen*local_scen_count + vbuflen)\n\n    def _got_kill_signal(self):\n        ''' returns True if a kill signal was received,\n            and refreshes the array and _locals'''\n        self._new_locals = self.spoke_from_hub(self._locals)\n        return self.remote_write_id == -1 \n\n    def prep_cs_cuts(self):\n        # create a map scenario -> index, this index is used for various lists containing scenario dependent info.\n        self.scenario_to_index = { scen : indx for indx, scen in enumerate(self.opt.all_scenario_names) }\n\n        # create concrete model to use as pseudo-root\n        self.opt.root = pyo.ConcreteModel()\n\n        ##get the nonants off an arbitrary scenario\n        arb_scen = self.opt.local_scenarios[self.opt.local_scenario_names[0]]\n        non_ants = arb_scen._mpisppy_node_list[0].nonant_vardata_list\n\n        # add copies of the nonanticipatory variables to the root problem\n        # NOTE: the LShaped code expects the nonant vars to be in a particular\n        #       order and with a particular *name*.\n        #       We're also creating an index for reference against later \n        nonant_vid_to_copy_map = dict()\n        root_vars = list()\n        for v in non_ants:\n            non_ant_copy = pyo.Var(name=v.name)\n            self.opt.root.add_component(v.name, non_ant_copy)\n            root_vars.append(non_ant_copy)\n            nonant_vid_to_copy_map[id(v)] = non_ant_copy\n\n        self.opt.root_vars = root_vars\n\n        # create an index of these non_ant_copies to be in the same\n        # order as PH, used below\n        nonants = dict()\n        for ndn_i, nonant in arb_scen._mpisppy_data.nonant_indices.items():\n            vid = id(nonant)\n            nonants[ndn_i] = nonant_vid_to_copy_map[vid]\n\n        self.root_nonants = nonants\n        self.opt.root.eta = pyo.Var(self.opt.all_scenario_names)\n\n        self.opt.root.bender = LShapedCutGenerator()\n        self.opt.root.bender.set_input(root_vars=self.opt.root_vars, \n                                            tol=1e-4, comm=self.cylinder_comm)\n        self.opt.root.bender.set_ls(self.opt)\n\n        ## the below for loop can take some time,\n        ## so return early if we get a kill signal,\n        ## but only after a barrier\n        self.cylinder_comm.Barrier()\n        if self.got_kill_signal():\n            return\n\n        # add the subproblems for all\n        for scen in self.opt.all_scenario_names:\n            subproblem_fn_kwargs = dict()\n\n            # need to modify this to accept in user kwargs as well\n            subproblem_fn_kwargs['scenario_name'] = scen\n            self.opt.root.bender.add_subproblem(subproblem_fn=self.opt.create_subproblem,\n                                                 subproblem_fn_kwargs=subproblem_fn_kwargs,\n                                                 root_eta=self.opt.root.eta[scen],\n                                                 subproblem_solver=self.opt.options[\"sp_solver\"],\n                                                 subproblem_solver_options=self.opt.options[\"sp_solver_options\"])\n\n        ## the above for loop can take some time,\n        ## so return early if we get a kill signal,\n        ## but only after a barrier\n        self.cylinder_comm.Barrier()\n        if self.got_kill_signal():\n            return\n\n        ## This call is blocking, depending on the\n        ## configuration. This necessitates the barrier\n        ## above.\n        self.opt.set_eta_bounds()\n        self._eta_lb_array = np.fromiter(\n                (self.opt.valid_eta_lb[s] for s in self.opt.all_scenario_names),\n                dtype='d', count=len(self.opt.all_scenario_names))\n        self.make_eta_lb_cut()\n\n    def make_eta_lb_cut(self):\n        ## we'll be storing a matrix as an array\n        ## row_len is the length of each row\n        row_len = 1+1+len(self.root_nonants)\n        all_coefs = np.zeros( self.nscen*row_len+1, dtype='d')\n        for idx, k in enumerate(self.opt.all_scenario_names):\n            ## cut_array -- [ constant, eta_coef, *nonant_coefs ]\n            ## this cut  -- [ LB, -1, *0s ], i.e., -1*\\eta + LB <= 0\n            all_coefs[row_len*idx] = self._eta_lb_array[idx]\n            all_coefs[row_len*idx+1] = -1\n        self.spoke_to_hub(all_coefs)\n\n    def make_cut(self):\n\n        ## cache opt\n        opt = self.opt\n\n        ## unpack these the way they were packed:\n        all_nonants_and_etas = self._locals\n        nonants = dict()\n        etas = dict()\n        ci = 0\n        for k, s in opt.local_scenarios.items():\n            for ndn, i in s._mpisppy_data.nonant_indices:\n                nonants[k, ndn, i] = all_nonants_and_etas[ci]\n                ci += 1\n\n        # get all the etas\n        for k, s in opt.local_scenarios.items():\n            for sn in opt.all_scenario_names:\n                etas[k, sn] = all_nonants_and_etas[ci]\n                ci += 1\n\n        ## self.nscen == len(opt.all_scenario_names)\n        # compute local min etas\n        min_eta_vals = np.fromiter(( min(etas[k,sn] for k in opt.local_scenarios) \\\n                                       for sn in opt.all_scenario_names ),\n                                    dtype='d', count=self.nscen)\n        # Allreduce the etas to take the minimum\n        global_eta_vals = np.empty(self.nscen, dtype='d')\n        self.cylinder_comm.Allreduce(min_eta_vals, global_eta_vals, op=MPI.MIN)\n\n        eta_lb_viol = (global_eta_vals + np.full_like(global_eta_vals, 1e-3) \\\n                        < self._eta_lb_array).any()\n        if eta_lb_viol:\n            self.make_eta_lb_cut()\n            return\n\n        # set the root etas to be the minimum from every scenario\n        root_etas = opt.root.eta\n        for idx, scen_name in enumerate(opt.all_scenario_names):\n            root_etas[scen_name].set_value(global_eta_vals[idx])\n\n        # sum the local nonants for average computation\n        root_nonants = self.root_nonants\n\n        local_nonant_sum = np.fromiter( ( sum(nonants[k, nname, ix] for k in opt.local_scenarios)\n                                          for nname, ix in root_nonants),\n                                          dtype='d', count=len(root_nonants) )\n\n\n        # Allreduce the xhats to get averages\n        global_nonant_sum = np.empty(len(local_nonant_sum), dtype='d')\n        self.cylinder_comm.Allreduce(local_nonant_sum, global_nonant_sum, op = MPI.SUM)\n        # need to divide through by the number of different spoke processes\n        global_xbar = global_nonant_sum / self.nscen\n\n        local_dist = np.array([0],dtype='d')\n        local_winner = None\n        # iterate through the ranks xhats to get the ranks maximum dist\n        for i, k in enumerate(opt.local_scenarios):\n            scenario_xhat = np.fromiter( (nonants[k, nname, ix] for nname, ix in root_nonants),\n                                         dtype='d', count=len(root_nonants) )\n            scenario_dist = np.linalg.norm(scenario_xhat - global_xbar)\n            local_dist[0] = max(local_dist[0], scenario_dist)\n            if local_winner is None:\n                local_winner = k\n            elif scenario_dist >= local_dist[0]:\n                local_winner = k\n\n        # Allreduce to find the biggest distance\n        global_dist = np.empty(1, dtype='d')\n        self.cylinder_comm.Allreduce(local_dist, global_dist, op=MPI.MAX)\n        vote = np.array([-1], dtype='i')\n        if local_dist[0] >= global_dist[0]:\n            vote[0] = self.cylinder_comm.Get_rank()\n\n        global_rank = np.empty(1, dtype='i')\n        self.cylinder_comm.Allreduce(vote, global_rank, op=MPI.MAX)\n\n        # if we are the winner, grab the xhat and bcast it to the other ranks\n        if self.cylinder_comm.Get_rank() == global_rank[0]:\n            farthest_xhat = np.fromiter( (nonants[local_winner, nname, ix] \n                                            for nname, ix in root_nonants),\n                                         dtype='d', count=len(root_nonants) )\n        else:\n            farthest_xhat = np.zeros(len(root_nonants), dtype='d')\n\n        self.cylinder_comm.Bcast(farthest_xhat, root=global_rank)\n\n        # set the first stage in the lshape object to correspond to farthest_xhat\n        for ci, k in enumerate(root_nonants):\n            root_nonants[k].set_value(farthest_xhat[ci])\n\n        # generate cuts\n        cuts = opt.root.bender.generate_cut()\n\n        # eta var_id map:\n        eta_id_map = { id(var) : k for k,var in root_etas.items()}\n        coef_dict = dict()\n        feas_cuts = list()\n        # package cuts, slightly silly in that we reconstruct the coefficients from the cuts\n        # TODO: modify the lshaped_cuts method to have a separate generate_coeffs function\n        for cut in cuts:\n            repn = generate_standard_repn(cut.body)\n            if len(repn.nonlinear_vars) > 0:\n                raise RuntimeError(\"BendersCutGenerator returned nonlinear cut\")\n\n            ## create a map from id(var) to index in repn\n            id_var_to_idx = { id(var) : i for i,var in enumerate(repn.linear_vars) }\n\n            ## find the eta index\n            for vid in eta_id_map:\n                if vid in id_var_to_idx:\n                    scen_name = eta_id_map[vid]\n                    ## cut_array -- [ constant, eta_coef, *nonant_coefs ]\n                    cut_array = [repn.constant, repn.linear_coefs[id_var_to_idx[vid]]]\n                    # each eta_s should only appear at most once per set of cuts\n                    del eta_id_map[vid]\n                    # each variable should only appear once in repn.linear_vars\n                    del id_var_to_idx[vid]\n                    break\n            else: # no break,\n                # so no scenario recourse cost variables appear in the cut\n                cut_array = [repn.constant, 0.]\n                # we don't know what scenario,\n                # but since eta_s is 0, it doesn't\n                # matter\n                scen_name = None\n\n            ## be intentional about how these are loaded\n            ## unloaded the same way\n            ## root_vars is in the order PH expects\n            ## (per above)\n            for var in root_nonants.values():\n                # each variable should only appear at most once in repn.linear_vars\n                idx = id_var_to_idx.pop(id(var), None)\n                if idx is not None:\n                    cut_array.append(repn.linear_coefs[idx])\n                else:\n                    cut_array.append(0)\n\n            if scen_name is not None:\n                coef_dict[scen_name] = np.array(cut_array, dtype='d')\n            else:\n                feas_cuts.append( np.array(cut_array, dtype='d') )\n\n        ## we'll be storing a matrix as an array\n        ## row_len is the length of each row\n        row_len = 1+1+len(root_nonants)\n        all_coefs = np.zeros( self.nscen*row_len +1, dtype='d')\n        for idx, k in enumerate(opt.all_scenario_names):\n            if k in coef_dict:\n                all_coefs[row_len*idx:row_len*(idx+1)] = coef_dict[k]\n            elif feas_cuts:\n                all_coefs[row_len*idx:row_len*(idx+1)] = feas_cuts.pop()\n        self.spoke_to_hub(all_coefs)\n\n    def main(self):\n        # call main cut generation routine\n\n        # prep cut generation\n        self.prep_cs_cuts()\n\n        # main loop\n        while not (self.got_kill_signal()):\n            if self._new_locals:\n                self.make_cut()",
  "def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        super().__init__(spbase_object, fullcomm, strata_comm, cylinder_comm, options=options)",
  "def make_windows(self):\n        nscen = len(self.opt.all_scenario_names)\n        if nscen == 0:\n            raise RuntimeError(f\"(rank: {self.cylinder_rank}), no local_scenarios\")\n\n        self.nscen = nscen\n        vbuflen = 0\n        self.nonant_per_scen = 0\n        for s in self.opt.local_scenarios.values():\n            vbuflen += len(s._mpisppy_data.nonant_indices)\n        local_scen_count = len(self.opt.local_scenario_names)\n        self.nonant_per_scen = int(vbuflen / local_scen_count)\n\n        ## the _locals will also have the kill signal\n        self.all_nonant_len = vbuflen\n        self.all_eta_len = nscen*local_scen_count\n        self._locals = np.zeros(nscen*local_scen_count + vbuflen + 1)\n        self._coefs = np.zeros(nscen*(nscen + self.nonant_per_scen) + 1 + 1)\n        self._new_locals = False\n\n        # local, remote\n        # send, receive\n        self._make_windows(nscen*(self.nonant_per_scen + 1 + 1), nscen*local_scen_count + vbuflen)",
  "def _got_kill_signal(self):\n        ''' returns True if a kill signal was received,\n            and refreshes the array and _locals'''\n        self._new_locals = self.spoke_from_hub(self._locals)\n        return self.remote_write_id == -1",
  "def prep_cs_cuts(self):\n        # create a map scenario -> index, this index is used for various lists containing scenario dependent info.\n        self.scenario_to_index = { scen : indx for indx, scen in enumerate(self.opt.all_scenario_names) }\n\n        # create concrete model to use as pseudo-root\n        self.opt.root = pyo.ConcreteModel()\n\n        ##get the nonants off an arbitrary scenario\n        arb_scen = self.opt.local_scenarios[self.opt.local_scenario_names[0]]\n        non_ants = arb_scen._mpisppy_node_list[0].nonant_vardata_list\n\n        # add copies of the nonanticipatory variables to the root problem\n        # NOTE: the LShaped code expects the nonant vars to be in a particular\n        #       order and with a particular *name*.\n        #       We're also creating an index for reference against later \n        nonant_vid_to_copy_map = dict()\n        root_vars = list()\n        for v in non_ants:\n            non_ant_copy = pyo.Var(name=v.name)\n            self.opt.root.add_component(v.name, non_ant_copy)\n            root_vars.append(non_ant_copy)\n            nonant_vid_to_copy_map[id(v)] = non_ant_copy\n\n        self.opt.root_vars = root_vars\n\n        # create an index of these non_ant_copies to be in the same\n        # order as PH, used below\n        nonants = dict()\n        for ndn_i, nonant in arb_scen._mpisppy_data.nonant_indices.items():\n            vid = id(nonant)\n            nonants[ndn_i] = nonant_vid_to_copy_map[vid]\n\n        self.root_nonants = nonants\n        self.opt.root.eta = pyo.Var(self.opt.all_scenario_names)\n\n        self.opt.root.bender = LShapedCutGenerator()\n        self.opt.root.bender.set_input(root_vars=self.opt.root_vars, \n                                            tol=1e-4, comm=self.cylinder_comm)\n        self.opt.root.bender.set_ls(self.opt)\n\n        ## the below for loop can take some time,\n        ## so return early if we get a kill signal,\n        ## but only after a barrier\n        self.cylinder_comm.Barrier()\n        if self.got_kill_signal():\n            return\n\n        # add the subproblems for all\n        for scen in self.opt.all_scenario_names:\n            subproblem_fn_kwargs = dict()\n\n            # need to modify this to accept in user kwargs as well\n            subproblem_fn_kwargs['scenario_name'] = scen\n            self.opt.root.bender.add_subproblem(subproblem_fn=self.opt.create_subproblem,\n                                                 subproblem_fn_kwargs=subproblem_fn_kwargs,\n                                                 root_eta=self.opt.root.eta[scen],\n                                                 subproblem_solver=self.opt.options[\"sp_solver\"],\n                                                 subproblem_solver_options=self.opt.options[\"sp_solver_options\"])\n\n        ## the above for loop can take some time,\n        ## so return early if we get a kill signal,\n        ## but only after a barrier\n        self.cylinder_comm.Barrier()\n        if self.got_kill_signal():\n            return\n\n        ## This call is blocking, depending on the\n        ## configuration. This necessitates the barrier\n        ## above.\n        self.opt.set_eta_bounds()\n        self._eta_lb_array = np.fromiter(\n                (self.opt.valid_eta_lb[s] for s in self.opt.all_scenario_names),\n                dtype='d', count=len(self.opt.all_scenario_names))\n        self.make_eta_lb_cut()",
  "def make_eta_lb_cut(self):\n        ## we'll be storing a matrix as an array\n        ## row_len is the length of each row\n        row_len = 1+1+len(self.root_nonants)\n        all_coefs = np.zeros( self.nscen*row_len+1, dtype='d')\n        for idx, k in enumerate(self.opt.all_scenario_names):\n            ## cut_array -- [ constant, eta_coef, *nonant_coefs ]\n            ## this cut  -- [ LB, -1, *0s ], i.e., -1*\\eta + LB <= 0\n            all_coefs[row_len*idx] = self._eta_lb_array[idx]\n            all_coefs[row_len*idx+1] = -1\n        self.spoke_to_hub(all_coefs)",
  "def make_cut(self):\n\n        ## cache opt\n        opt = self.opt\n\n        ## unpack these the way they were packed:\n        all_nonants_and_etas = self._locals\n        nonants = dict()\n        etas = dict()\n        ci = 0\n        for k, s in opt.local_scenarios.items():\n            for ndn, i in s._mpisppy_data.nonant_indices:\n                nonants[k, ndn, i] = all_nonants_and_etas[ci]\n                ci += 1\n\n        # get all the etas\n        for k, s in opt.local_scenarios.items():\n            for sn in opt.all_scenario_names:\n                etas[k, sn] = all_nonants_and_etas[ci]\n                ci += 1\n\n        ## self.nscen == len(opt.all_scenario_names)\n        # compute local min etas\n        min_eta_vals = np.fromiter(( min(etas[k,sn] for k in opt.local_scenarios) \\\n                                       for sn in opt.all_scenario_names ),\n                                    dtype='d', count=self.nscen)\n        # Allreduce the etas to take the minimum\n        global_eta_vals = np.empty(self.nscen, dtype='d')\n        self.cylinder_comm.Allreduce(min_eta_vals, global_eta_vals, op=MPI.MIN)\n\n        eta_lb_viol = (global_eta_vals + np.full_like(global_eta_vals, 1e-3) \\\n                        < self._eta_lb_array).any()\n        if eta_lb_viol:\n            self.make_eta_lb_cut()\n            return\n\n        # set the root etas to be the minimum from every scenario\n        root_etas = opt.root.eta\n        for idx, scen_name in enumerate(opt.all_scenario_names):\n            root_etas[scen_name].set_value(global_eta_vals[idx])\n\n        # sum the local nonants for average computation\n        root_nonants = self.root_nonants\n\n        local_nonant_sum = np.fromiter( ( sum(nonants[k, nname, ix] for k in opt.local_scenarios)\n                                          for nname, ix in root_nonants),\n                                          dtype='d', count=len(root_nonants) )\n\n\n        # Allreduce the xhats to get averages\n        global_nonant_sum = np.empty(len(local_nonant_sum), dtype='d')\n        self.cylinder_comm.Allreduce(local_nonant_sum, global_nonant_sum, op = MPI.SUM)\n        # need to divide through by the number of different spoke processes\n        global_xbar = global_nonant_sum / self.nscen\n\n        local_dist = np.array([0],dtype='d')\n        local_winner = None\n        # iterate through the ranks xhats to get the ranks maximum dist\n        for i, k in enumerate(opt.local_scenarios):\n            scenario_xhat = np.fromiter( (nonants[k, nname, ix] for nname, ix in root_nonants),\n                                         dtype='d', count=len(root_nonants) )\n            scenario_dist = np.linalg.norm(scenario_xhat - global_xbar)\n            local_dist[0] = max(local_dist[0], scenario_dist)\n            if local_winner is None:\n                local_winner = k\n            elif scenario_dist >= local_dist[0]:\n                local_winner = k\n\n        # Allreduce to find the biggest distance\n        global_dist = np.empty(1, dtype='d')\n        self.cylinder_comm.Allreduce(local_dist, global_dist, op=MPI.MAX)\n        vote = np.array([-1], dtype='i')\n        if local_dist[0] >= global_dist[0]:\n            vote[0] = self.cylinder_comm.Get_rank()\n\n        global_rank = np.empty(1, dtype='i')\n        self.cylinder_comm.Allreduce(vote, global_rank, op=MPI.MAX)\n\n        # if we are the winner, grab the xhat and bcast it to the other ranks\n        if self.cylinder_comm.Get_rank() == global_rank[0]:\n            farthest_xhat = np.fromiter( (nonants[local_winner, nname, ix] \n                                            for nname, ix in root_nonants),\n                                         dtype='d', count=len(root_nonants) )\n        else:\n            farthest_xhat = np.zeros(len(root_nonants), dtype='d')\n\n        self.cylinder_comm.Bcast(farthest_xhat, root=global_rank)\n\n        # set the first stage in the lshape object to correspond to farthest_xhat\n        for ci, k in enumerate(root_nonants):\n            root_nonants[k].set_value(farthest_xhat[ci])\n\n        # generate cuts\n        cuts = opt.root.bender.generate_cut()\n\n        # eta var_id map:\n        eta_id_map = { id(var) : k for k,var in root_etas.items()}\n        coef_dict = dict()\n        feas_cuts = list()\n        # package cuts, slightly silly in that we reconstruct the coefficients from the cuts\n        # TODO: modify the lshaped_cuts method to have a separate generate_coeffs function\n        for cut in cuts:\n            repn = generate_standard_repn(cut.body)\n            if len(repn.nonlinear_vars) > 0:\n                raise RuntimeError(\"BendersCutGenerator returned nonlinear cut\")\n\n            ## create a map from id(var) to index in repn\n            id_var_to_idx = { id(var) : i for i,var in enumerate(repn.linear_vars) }\n\n            ## find the eta index\n            for vid in eta_id_map:\n                if vid in id_var_to_idx:\n                    scen_name = eta_id_map[vid]\n                    ## cut_array -- [ constant, eta_coef, *nonant_coefs ]\n                    cut_array = [repn.constant, repn.linear_coefs[id_var_to_idx[vid]]]\n                    # each eta_s should only appear at most once per set of cuts\n                    del eta_id_map[vid]\n                    # each variable should only appear once in repn.linear_vars\n                    del id_var_to_idx[vid]\n                    break\n            else: # no break,\n                # so no scenario recourse cost variables appear in the cut\n                cut_array = [repn.constant, 0.]\n                # we don't know what scenario,\n                # but since eta_s is 0, it doesn't\n                # matter\n                scen_name = None\n\n            ## be intentional about how these are loaded\n            ## unloaded the same way\n            ## root_vars is in the order PH expects\n            ## (per above)\n            for var in root_nonants.values():\n                # each variable should only appear at most once in repn.linear_vars\n                idx = id_var_to_idx.pop(id(var), None)\n                if idx is not None:\n                    cut_array.append(repn.linear_coefs[idx])\n                else:\n                    cut_array.append(0)\n\n            if scen_name is not None:\n                coef_dict[scen_name] = np.array(cut_array, dtype='d')\n            else:\n                feas_cuts.append( np.array(cut_array, dtype='d') )\n\n        ## we'll be storing a matrix as an array\n        ## row_len is the length of each row\n        row_len = 1+1+len(root_nonants)\n        all_coefs = np.zeros( self.nscen*row_len +1, dtype='d')\n        for idx, k in enumerate(opt.all_scenario_names):\n            if k in coef_dict:\n                all_coefs[row_len*idx:row_len*(idx+1)] = coef_dict[k]\n            elif feas_cuts:\n                all_coefs[row_len*idx:row_len*(idx+1)] = feas_cuts.pop()\n        self.spoke_to_hub(all_coefs)",
  "def main(self):\n        # call main cut generation routine\n\n        # prep cut generation\n        self.prep_cs_cuts()\n\n        # main loop\n        while not (self.got_kill_signal()):\n            if self._new_locals:\n                self.make_cut()",
  "class LagrangerOuterBound(mpisppy.cylinders.spoke.OuterBoundNonantSpoke):\n    \"\"\"Indepedent Lagrangian that takes x values as input and updates its own W.\n    \"\"\"\n    converger_spoke_char = 'A'\n\n    def lagrangian_prep(self):\n        verbose = self.opt.options['verbose']\n        # Scenarios are created here\n        self.opt.PH_Prep(attach_prox=False)\n        self.opt._reenable_W()\n        self.opt.subproblem_creation(verbose)\n        self.opt._create_solvers()\n        if \"lagranger_rho_rescale_factors_json\" in self.opt.options and\\\n            self.opt.options[\"lagranger_rho_rescale_factors_json\"] is not None:\n            with open(self.opt.options[\"lagranger_rho_rescale_factors_json\"], \"r\") as fin:\n                din = json.load(fin)\n            self.rho_rescale_factors = {int(i): float(din[i]) for i in din}\n        else:\n            self.rho_rescale_factors = None\n        # side-effect is needed: create the nonant_cache\n        self.opt._save_nonants()\n\n    def _lagrangian(self, iternum):\n        verbose = self.opt.options['verbose']\n        # see if rho should be rescaled\n        if self.rho_rescale_factors is not None\\\n           and iternum in self.rho_rescale_factors:\n            self._rescale_rho(self.rho_rescale_factors[iternum])\n        teeme = False\n        if \"tee-rank0-solves\" in self.opt.options and self.opt.cylinder_rank == 0:\n            teeme = self.opt.options['tee-rank0-solves']\n\n        self.opt.solve_loop(\n            solver_options=self.opt.current_solver_options,\n            dtiming=False,\n            gripe=True,\n            tee=teeme,\n            verbose=verbose\n        )\n\n        # Compute the resulting bound\n        return self.opt.Ebound(verbose)\n\n\n    def _rescale_rho(self,rf):\n        # IMPORTANT: the scalings accumulate.\n        # E.g., 0.5 then 2.0 gets you back where you started.\n        for (sname, scenario) in self.opt.local_scenarios.items():\n            for ndn_i, xvar in scenario._mpisppy_data.nonant_indices.items():\n                scenario._mpisppy_model.rho[ndn_i] *= rf\n\n    def _write_W_and_xbar(self, iternum):\n        if self.opt.options.get(\"lagranger_write_W\", False):\n            w_fname = 'lagranger_w_vals.csv'\n            with open(w_fname, 'a') as f:\n                writer = csv.writer(f)\n                writer.writerow(['#iteration number', iternum])\n            mpisppy.utils.wxbarutils.write_W_to_file(self.opt, w_fname,\n                                                     sep_files=False)\n        if self.opt.options.get(\"lagranger_write_xbar\", False):\n            xbar_fname = 'lagranger_xbar_vals.csv'\n            with open(xbar_fname, 'a') as f:\n                writer = csv.writer(f)\n                writer.writerow(['#iteration number', iternum])\n            mpisppy.utils.wxbarutils.write_xbar_to_file(self.opt, xbar_fname)\n\n    def _update_weights_and_solve(self, iternum):\n        # Work with the nonants that we have (and we might not have any yet).\n        self.opt._put_nonant_cache(self.localnonants)\n        self.opt._restore_nonants()\n        verbose = self.opt.options[\"verbose\"]\n        self.opt.Compute_Xbar(verbose=verbose)\n        self.opt.Update_W(verbose=verbose)\n        ## writes Ws here\n        self._write_W_and_xbar(iternum)\n        return self._lagrangian(iternum)\n\n    def main(self):\n        # The rho_setter should be attached to the opt object\n        rho_setter = None\n        if hasattr(self.opt, 'rho_setter'):\n            rho_setter = self.opt.rho_setter\n\n        self.lagrangian_prep()\n\n        self.A_iter = 1\n        self.trivial_bound = self._lagrangian(0)\n\n        self.bound = self.trivial_bound\n\n        self.opt.current_solver_options = self.opt.iterk_solver_options\n\n        while not self.got_kill_signal():\n            # because of aph, do not check for new data, just go for it\n            self.bound = self._update_weights_and_solve(self.A_iter)\n            self.A_iter += 1\n\n    def finalize(self):\n        '''\n        Do one final lagrangian pass with the final\n        PH weights. Useful for when PH convergence\n        and/or iteration limit is the cause of termination\n        '''\n        self.final_bound = self._update_weights_and_solve(self.A_iter)\n        self.bound = self.final_bound\n        if self.opt.extensions is not None and \\\n            hasattr(self.opt.extobject, 'post_everything'):\n            self.opt.extobject.post_everything()\n        return self.final_bound",
  "def lagrangian_prep(self):\n        verbose = self.opt.options['verbose']\n        # Scenarios are created here\n        self.opt.PH_Prep(attach_prox=False)\n        self.opt._reenable_W()\n        self.opt.subproblem_creation(verbose)\n        self.opt._create_solvers()\n        if \"lagranger_rho_rescale_factors_json\" in self.opt.options and\\\n            self.opt.options[\"lagranger_rho_rescale_factors_json\"] is not None:\n            with open(self.opt.options[\"lagranger_rho_rescale_factors_json\"], \"r\") as fin:\n                din = json.load(fin)\n            self.rho_rescale_factors = {int(i): float(din[i]) for i in din}\n        else:\n            self.rho_rescale_factors = None\n        # side-effect is needed: create the nonant_cache\n        self.opt._save_nonants()",
  "def _lagrangian(self, iternum):\n        verbose = self.opt.options['verbose']\n        # see if rho should be rescaled\n        if self.rho_rescale_factors is not None\\\n           and iternum in self.rho_rescale_factors:\n            self._rescale_rho(self.rho_rescale_factors[iternum])\n        teeme = False\n        if \"tee-rank0-solves\" in self.opt.options and self.opt.cylinder_rank == 0:\n            teeme = self.opt.options['tee-rank0-solves']\n\n        self.opt.solve_loop(\n            solver_options=self.opt.current_solver_options,\n            dtiming=False,\n            gripe=True,\n            tee=teeme,\n            verbose=verbose\n        )\n\n        # Compute the resulting bound\n        return self.opt.Ebound(verbose)",
  "def _rescale_rho(self,rf):\n        # IMPORTANT: the scalings accumulate.\n        # E.g., 0.5 then 2.0 gets you back where you started.\n        for (sname, scenario) in self.opt.local_scenarios.items():\n            for ndn_i, xvar in scenario._mpisppy_data.nonant_indices.items():\n                scenario._mpisppy_model.rho[ndn_i] *= rf",
  "def _write_W_and_xbar(self, iternum):\n        if self.opt.options.get(\"lagranger_write_W\", False):\n            w_fname = 'lagranger_w_vals.csv'\n            with open(w_fname, 'a') as f:\n                writer = csv.writer(f)\n                writer.writerow(['#iteration number', iternum])\n            mpisppy.utils.wxbarutils.write_W_to_file(self.opt, w_fname,\n                                                     sep_files=False)\n        if self.opt.options.get(\"lagranger_write_xbar\", False):\n            xbar_fname = 'lagranger_xbar_vals.csv'\n            with open(xbar_fname, 'a') as f:\n                writer = csv.writer(f)\n                writer.writerow(['#iteration number', iternum])\n            mpisppy.utils.wxbarutils.write_xbar_to_file(self.opt, xbar_fname)",
  "def _update_weights_and_solve(self, iternum):\n        # Work with the nonants that we have (and we might not have any yet).\n        self.opt._put_nonant_cache(self.localnonants)\n        self.opt._restore_nonants()\n        verbose = self.opt.options[\"verbose\"]\n        self.opt.Compute_Xbar(verbose=verbose)\n        self.opt.Update_W(verbose=verbose)\n        ## writes Ws here\n        self._write_W_and_xbar(iternum)\n        return self._lagrangian(iternum)",
  "def main(self):\n        # The rho_setter should be attached to the opt object\n        rho_setter = None\n        if hasattr(self.opt, 'rho_setter'):\n            rho_setter = self.opt.rho_setter\n\n        self.lagrangian_prep()\n\n        self.A_iter = 1\n        self.trivial_bound = self._lagrangian(0)\n\n        self.bound = self.trivial_bound\n\n        self.opt.current_solver_options = self.opt.iterk_solver_options\n\n        while not self.got_kill_signal():\n            # because of aph, do not check for new data, just go for it\n            self.bound = self._update_weights_and_solve(self.A_iter)\n            self.A_iter += 1",
  "def finalize(self):\n        '''\n        Do one final lagrangian pass with the final\n        PH weights. Useful for when PH convergence\n        and/or iteration limit is the cause of termination\n        '''\n        self.final_bound = self._update_weights_and_solve(self.A_iter)\n        self.bound = self.final_bound\n        if self.opt.extensions is not None and \\\n            hasattr(self.opt.extobject, 'post_everything'):\n            self.opt.extobject.post_everything()\n        return self.final_bound",
  "class XhatShuffleInnerBound(spoke.InnerBoundNonantSpoke):\n\n    converger_spoke_char = 'X'\n\n    def xhatbase_prep(self):\n\n        verbose = self.opt.options['verbose']\n        if \"bundles_per_rank\" in self.opt.options\\\n           and self.opt.options[\"bundles_per_rank\"] != 0:\n            raise RuntimeError(\"xhat spokes cannot have bundles (yet)\")\n\n        ## for later\n        self.verbose = self.opt.options[\"verbose\"] # typing aid  \n        self.solver_options = self.opt.options[\"xhat_looper_options\"][\"xhat_solver_options\"]\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatShuffleInnerBound must be used with Xhat_Eval.\")\n            \n        xhatter = XhatBase(self.opt)\n        self.xhatter = xhatter\n\n        ### begin iter0 stuff\n        xhatter.pre_iter0()  # for an extension\n        self.opt._save_original_nonants()\n\n        self.opt._lazy_create_solvers()  # no iter0 loop, but we need the solvers\n\n        self.opt._update_E1()\n        if abs(1 - self.opt.E1) > self.opt.E1_tolerance:\n            raise ValueError(f\"Total probability of scenarios was {self.opt.E1} \"+\\\n                                 f\"(E1_tolerance is {self.opt.E1_tolerance})\")\n        ### end iter0 stuff (but note: no need for iter 0 solves in an xhatter)\n\n        xhatter.post_iter0()\n \n        self.opt._save_nonants() # make the cache\n\n        ## option drive this? (could be dangerous)\n        self.random_seed = 42\n        # Have a separate stream for shuffling\n        self.random_stream = random.Random()\n\n\n    def try_scenario_dict(self, xhat_scenario_dict):\n        \"\"\" wrapper for _try_one\"\"\"\n        snamedict = xhat_scenario_dict\n\n        stage2EFsolvern = self.opt.options.get(\"stage2EFsolvern\", None)\n        branching_factors = self.opt.options.get(\"branching_factors\", None)  # for stage2ef\n        obj = self.xhatter._try_one(snamedict,\n                                    solver_options = self.solver_options,\n                                    verbose=False,\n                                    restore_nonants=False,\n                                    stage2EFsolvern=stage2EFsolvern,\n                                    branching_factors=branching_factors)\n        def _vb(msg): \n            if self.verbose and self.opt.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)\n\n        if obj is None:\n            _vb(f\"    Infeasible {snamedict}\")\n            return False\n        _vb(f\"    Feasible {snamedict}, obj: {obj}\")\n\n        update = self.update_if_improving(obj)\n        logger.debug(f'   bottom of try_scenario_dict on rank {self.global_rank}')\n        return update\n\n    def main(self):\n        verbose = self.opt.options[\"verbose\"] # typing aid  \n        logger.debug(f\"Entering main on xhatshuffle spoke rank {self.global_rank}\")\n\n        self.xhatbase_prep()\n        if \"reverse\" in self.opt.options[\"xhat_looper_options\"]:\n            self.reverse = self.opt.options[\"xhat_looper_options\"][\"reverse\"]\n        else:\n            self.reverse = True\n        if \"iter_step\" in self.opt.options[\"xhat_looper_options\"]:\n            self.iter_step = self.opt.options[\"xhat_looper_options\"][\"iter_step\"]\n        else:\n            self.iter_step = None\n\n        # give all ranks the same seed\n        self.random_stream.seed(self.random_seed)\n        \n        #We need to keep track of the way scenario_names were sorted\n        scen_names = list(enumerate(self.opt.all_scenario_names))\n        \n        # shuffle the scenarios associated (i.e., sample without replacement)\n        shuffled_scenarios = self.random_stream.sample(scen_names, \n                                                       len(scen_names))\n        \n        scenario_cycler = ScenarioCycler(shuffled_scenarios,\n                                         self.opt.nonleaves,\n                                         self.reverse,\n                                         self.iter_step)\n\n        def _vb(msg): \n            if self.verbose and self.opt.cylinder_rank == 0:\n                print(\"(rank0) \" + msg)\n\n        xh_iter = 1\n        while not self.got_kill_signal():\n            # When there is no iter0, the serial number must be checked.\n            # (unrelated: uncomment the next line to see the source of delay getting an xhat)\n            if self.get_serial_number() == 0:\n                continue\n\n            if (xh_iter-1) % 100 == 0:\n                logger.debug(f'   Xhatshuffle loop iter={xh_iter} on rank {self.global_rank}')\n                logger.debug(f'   Xhatshuffle got from opt on rank {self.global_rank}')\n\n            if self.new_nonants:\n                # similar to above, not all ranks will agree on\n                # when there are new_nonants (in the same loop)\n                logger.debug(f'   *Xhatshuffle loop iter={xh_iter}')\n                logger.debug(f'   *got a new one! on rank {self.global_rank}')\n                logger.debug(f'   *localnonants={str(self.localnonants)}')\n\n                # update the caches\n                self.opt._put_nonant_cache(self.localnonants)\n                self.opt._restore_nonants()\n\n            next_scendict = scenario_cycler.get_next()\n            if next_scendict is not None:\n                _vb(f\"   Trying next {next_scendict}\")\n                update = self.try_scenario_dict(next_scendict)\n                if update:\n                    _vb(f\"   Updating best to {next_scendict}\")\n                    scenario_cycler.best = next_scendict\n            else:\n                scenario_cycler.begin_epoch()\n\n            #_vb(f\"    scenario_cycler._scenarios_this_epoch {scenario_cycler._scenarios_this_epoch}\")\n\n            xh_iter += 1",
  "class ScenarioCycler:\n\n    def __init__(self, shuffled_scenarios,nonleaves,reverse,iter_step):\n        root_kids = nonleaves['ROOT'].kids if 'ROOT' in nonleaves else None\n        if root_kids is None or len(root_kids)==0 or root_kids[0].is_leaf:\n            self._multi = False\n            self._iter_shift = 1 if iter_step is None else iter_step\n            self._use_reverse = False #It is useless to reverse for 2stage SP\n        else:\n            self._multi = True\n            self.BF0 = len(root_kids)\n            self._nonleaves = nonleaves\n            \n            self._iter_shift = self.BF0 if iter_step is None else iter_step\n            self._use_reverse = True if reverse is None else reverse\n            self._reversed = False #Do we iter in reverse mode ?\n        self._shuffled_scenarios = shuffled_scenarios\n        self._num_scenarios = len(shuffled_scenarios)\n        \n        self._begin_normal_epoch()\n        \n        self._best = None\n\n    @property\n    def best(self):\n        return self._best\n\n    @best.setter\n    def best(self, value):\n        self._best = value\n        \n    def _fill_nodescen_dict(self,empty_nodes):   \n        filling_idx = self._cycle_idx\n        while len(empty_nodes) >0:\n            #Sanity check to make no infinite loop.\n            if filling_idx == self._cycle_idx and 'ROOT' in self.nodescen_dict and self.nodescen_dict['ROOT'] is not None:\n                print(self.nodescen_dict)\n                raise RuntimeError(\"_fill_nodescen_dict looped over every scenario but was not able to find a scen for every nonleaf node.\")\n            sname = self._shuffled_snames[filling_idx]\n            snum = self._original_order[filling_idx]\n            \n            def _add_sname_to_node(ndn):\n                first = self._nonleaves[ndn].scenfirst\n                last = self._nonleaves[ndn].scenlast\n                if snum>=first and snum<=last:\n                    self.nodescen_dict[ndn] = sname\n                    return False\n                else:\n                    return True\n            #Adding sname to every nodes it goes by, and removing the nodes from empty_nodes\n            empty_nodes = list(filter(_add_sname_to_node,empty_nodes))\n            filling_idx +=1\n            filling_idx %= self._num_scenarios\n        \n    def create_nodescen_dict(self):\n        '''\n        Creates an attribute nodescen_dict. \n        Keys are nonleaf names, values are local scenario names \n        (a value can be None if the associated scenario is not in our rank)\n        \n        WARNING: _cur_ROOTscen must be up to date when calling this method\n        '''\n        if not self._multi:\n            self.nodescen_dict = {'ROOT':self._cur_ROOTscen}\n        else:\n            self.nodescen_dict = dict()\n            self._fill_nodescen_dict(self._nonleaves.keys())\n    \n    def update_nodescen_dict(self,snames_to_remove):\n        '''\n        WARNING: _cur_ROOTscen must be up to date when calling this method\n        '''\n        if not self._multi:\n            self.nodescen_dict = {'ROOT':self._cur_ROOTscen}\n        else:\n            empty_nodes = []\n            for ndn in self._nonleaves.keys():\n                if self.nodescen_dict[ndn] in snames_to_remove:\n                    self.nodescen_dict[ndn] = None\n                    empty_nodes.append(ndn)\n            self._fill_nodescen_dict(empty_nodes)\n        \n\n    def begin_epoch(self):\n        if self._multi and self._use_reverse and not self._reversed:\n            self._begin_reverse_epoch()\n        else:\n            self._begin_normal_epoch()\n        \n    def _begin_normal_epoch(self):\n        if self._multi:\n            self._reversed = False\n        self._shuffled_snames = [s[1] for s in self._shuffled_scenarios]\n        self._original_order = [s[0] for s in self._shuffled_scenarios]\n        self._cycle_idx = 0\n        self._cur_ROOTscen = self._shuffled_snames[0]\n        self.create_nodescen_dict()\n        \n        self._scenarios_this_epoch = set()\n    \n    def _begin_reverse_epoch(self):\n        self._reversed = True\n        self._shuffled_snames = [s[1] for s in reversed(self._shuffled_scenarios)]\n        self._original_order = [s[0] for s in reversed(self._shuffled_scenarios)]\n        self._cycle_idx = 0\n        self._cur_ROOTscen = self._shuffled_snames[0]\n        self.create_nodescen_dict()\n        \n        self._scenarios_this_epoch = set()\n\n    def get_next(self):\n        next_scen = self._cur_ROOTscen\n        next_scendict = self.nodescen_dict\n        if next_scen in self._scenarios_this_epoch:\n            return None\n        self._scenarios_this_epoch.add(next_scen)\n        self._iter_scen()\n        return next_scendict\n\n    def _iter_scen(self):\n        old_idx = self._cycle_idx\n        self._cycle_idx += self._iter_shift\n        ## wrap around\n        self._cycle_idx %= self._num_scenarios\n        \n        #do not reuse a previously visited scenario for 'ROOT'\n        tmp_cycle_idx = self._cycle_idx\n        while self._shuffled_snames[tmp_cycle_idx] in self._scenarios_this_epoch and (\n                (tmp_cycle_idx+1)%self._num_scenarios != self._cycle_idx):\n            tmp_cycle_idx +=1\n            tmp_cycle_idx %= self._num_scenarios\n        \n        self._cycle_idx = tmp_cycle_idx\n        \n        #Updating scenarios\n        self._cur_ROOTscen = self._shuffled_snames[self._cycle_idx]\n        if old_idx<self._cycle_idx:\n            scens_to_remove = self._shuffled_snames[old_idx:self._cycle_idx]\n        else:\n            scens_to_remove = self._shuffled_snames[old_idx:]+self._shuffled_snames[:self._cycle_idx]\n        self.update_nodescen_dict(scens_to_remove)",
  "def xhatbase_prep(self):\n\n        verbose = self.opt.options['verbose']\n        if \"bundles_per_rank\" in self.opt.options\\\n           and self.opt.options[\"bundles_per_rank\"] != 0:\n            raise RuntimeError(\"xhat spokes cannot have bundles (yet)\")\n\n        ## for later\n        self.verbose = self.opt.options[\"verbose\"] # typing aid  \n        self.solver_options = self.opt.options[\"xhat_looper_options\"][\"xhat_solver_options\"]\n\n        if not isinstance(self.opt, Xhat_Eval):\n            raise RuntimeError(\"XhatShuffleInnerBound must be used with Xhat_Eval.\")\n            \n        xhatter = XhatBase(self.opt)\n        self.xhatter = xhatter\n\n        ### begin iter0 stuff\n        xhatter.pre_iter0()  # for an extension\n        self.opt._save_original_nonants()\n\n        self.opt._lazy_create_solvers()  # no iter0 loop, but we need the solvers\n\n        self.opt._update_E1()\n        if abs(1 - self.opt.E1) > self.opt.E1_tolerance:\n            raise ValueError(f\"Total probability of scenarios was {self.opt.E1} \"+\\\n                                 f\"(E1_tolerance is {self.opt.E1_tolerance})\")\n        ### end iter0 stuff (but note: no need for iter 0 solves in an xhatter)\n\n        xhatter.post_iter0()\n \n        self.opt._save_nonants() # make the cache\n\n        ## option drive this? (could be dangerous)\n        self.random_seed = 42\n        # Have a separate stream for shuffling\n        self.random_stream = random.Random()",
  "def try_scenario_dict(self, xhat_scenario_dict):\n        \"\"\" wrapper for _try_one\"\"\"\n        snamedict = xhat_scenario_dict\n\n        stage2EFsolvern = self.opt.options.get(\"stage2EFsolvern\", None)\n        branching_factors = self.opt.options.get(\"branching_factors\", None)  # for stage2ef\n        obj = self.xhatter._try_one(snamedict,\n                                    solver_options = self.solver_options,\n                                    verbose=False,\n                                    restore_nonants=False,\n                                    stage2EFsolvern=stage2EFsolvern,\n                                    branching_factors=branching_factors)\n        def _vb(msg): \n            if self.verbose and self.opt.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)\n\n        if obj is None:\n            _vb(f\"    Infeasible {snamedict}\")\n            return False\n        _vb(f\"    Feasible {snamedict}, obj: {obj}\")\n\n        update = self.update_if_improving(obj)\n        logger.debug(f'   bottom of try_scenario_dict on rank {self.global_rank}')\n        return update",
  "def main(self):\n        verbose = self.opt.options[\"verbose\"] # typing aid  \n        logger.debug(f\"Entering main on xhatshuffle spoke rank {self.global_rank}\")\n\n        self.xhatbase_prep()\n        if \"reverse\" in self.opt.options[\"xhat_looper_options\"]:\n            self.reverse = self.opt.options[\"xhat_looper_options\"][\"reverse\"]\n        else:\n            self.reverse = True\n        if \"iter_step\" in self.opt.options[\"xhat_looper_options\"]:\n            self.iter_step = self.opt.options[\"xhat_looper_options\"][\"iter_step\"]\n        else:\n            self.iter_step = None\n\n        # give all ranks the same seed\n        self.random_stream.seed(self.random_seed)\n        \n        #We need to keep track of the way scenario_names were sorted\n        scen_names = list(enumerate(self.opt.all_scenario_names))\n        \n        # shuffle the scenarios associated (i.e., sample without replacement)\n        shuffled_scenarios = self.random_stream.sample(scen_names, \n                                                       len(scen_names))\n        \n        scenario_cycler = ScenarioCycler(shuffled_scenarios,\n                                         self.opt.nonleaves,\n                                         self.reverse,\n                                         self.iter_step)\n\n        def _vb(msg): \n            if self.verbose and self.opt.cylinder_rank == 0:\n                print(\"(rank0) \" + msg)\n\n        xh_iter = 1\n        while not self.got_kill_signal():\n            # When there is no iter0, the serial number must be checked.\n            # (unrelated: uncomment the next line to see the source of delay getting an xhat)\n            if self.get_serial_number() == 0:\n                continue\n\n            if (xh_iter-1) % 100 == 0:\n                logger.debug(f'   Xhatshuffle loop iter={xh_iter} on rank {self.global_rank}')\n                logger.debug(f'   Xhatshuffle got from opt on rank {self.global_rank}')\n\n            if self.new_nonants:\n                # similar to above, not all ranks will agree on\n                # when there are new_nonants (in the same loop)\n                logger.debug(f'   *Xhatshuffle loop iter={xh_iter}')\n                logger.debug(f'   *got a new one! on rank {self.global_rank}')\n                logger.debug(f'   *localnonants={str(self.localnonants)}')\n\n                # update the caches\n                self.opt._put_nonant_cache(self.localnonants)\n                self.opt._restore_nonants()\n\n            next_scendict = scenario_cycler.get_next()\n            if next_scendict is not None:\n                _vb(f\"   Trying next {next_scendict}\")\n                update = self.try_scenario_dict(next_scendict)\n                if update:\n                    _vb(f\"   Updating best to {next_scendict}\")\n                    scenario_cycler.best = next_scendict\n            else:\n                scenario_cycler.begin_epoch()\n\n            #_vb(f\"    scenario_cycler._scenarios_this_epoch {scenario_cycler._scenarios_this_epoch}\")\n\n            xh_iter += 1",
  "def __init__(self, shuffled_scenarios,nonleaves,reverse,iter_step):\n        root_kids = nonleaves['ROOT'].kids if 'ROOT' in nonleaves else None\n        if root_kids is None or len(root_kids)==0 or root_kids[0].is_leaf:\n            self._multi = False\n            self._iter_shift = 1 if iter_step is None else iter_step\n            self._use_reverse = False #It is useless to reverse for 2stage SP\n        else:\n            self._multi = True\n            self.BF0 = len(root_kids)\n            self._nonleaves = nonleaves\n            \n            self._iter_shift = self.BF0 if iter_step is None else iter_step\n            self._use_reverse = True if reverse is None else reverse\n            self._reversed = False #Do we iter in reverse mode ?\n        self._shuffled_scenarios = shuffled_scenarios\n        self._num_scenarios = len(shuffled_scenarios)\n        \n        self._begin_normal_epoch()\n        \n        self._best = None",
  "def best(self):\n        return self._best",
  "def best(self, value):\n        self._best = value",
  "def _fill_nodescen_dict(self,empty_nodes):   \n        filling_idx = self._cycle_idx\n        while len(empty_nodes) >0:\n            #Sanity check to make no infinite loop.\n            if filling_idx == self._cycle_idx and 'ROOT' in self.nodescen_dict and self.nodescen_dict['ROOT'] is not None:\n                print(self.nodescen_dict)\n                raise RuntimeError(\"_fill_nodescen_dict looped over every scenario but was not able to find a scen for every nonleaf node.\")\n            sname = self._shuffled_snames[filling_idx]\n            snum = self._original_order[filling_idx]\n            \n            def _add_sname_to_node(ndn):\n                first = self._nonleaves[ndn].scenfirst\n                last = self._nonleaves[ndn].scenlast\n                if snum>=first and snum<=last:\n                    self.nodescen_dict[ndn] = sname\n                    return False\n                else:\n                    return True\n            #Adding sname to every nodes it goes by, and removing the nodes from empty_nodes\n            empty_nodes = list(filter(_add_sname_to_node,empty_nodes))\n            filling_idx +=1\n            filling_idx %= self._num_scenarios",
  "def create_nodescen_dict(self):\n        '''\n        Creates an attribute nodescen_dict. \n        Keys are nonleaf names, values are local scenario names \n        (a value can be None if the associated scenario is not in our rank)\n        \n        WARNING: _cur_ROOTscen must be up to date when calling this method\n        '''\n        if not self._multi:\n            self.nodescen_dict = {'ROOT':self._cur_ROOTscen}\n        else:\n            self.nodescen_dict = dict()\n            self._fill_nodescen_dict(self._nonleaves.keys())",
  "def update_nodescen_dict(self,snames_to_remove):\n        '''\n        WARNING: _cur_ROOTscen must be up to date when calling this method\n        '''\n        if not self._multi:\n            self.nodescen_dict = {'ROOT':self._cur_ROOTscen}\n        else:\n            empty_nodes = []\n            for ndn in self._nonleaves.keys():\n                if self.nodescen_dict[ndn] in snames_to_remove:\n                    self.nodescen_dict[ndn] = None\n                    empty_nodes.append(ndn)\n            self._fill_nodescen_dict(empty_nodes)",
  "def begin_epoch(self):\n        if self._multi and self._use_reverse and not self._reversed:\n            self._begin_reverse_epoch()\n        else:\n            self._begin_normal_epoch()",
  "def _begin_normal_epoch(self):\n        if self._multi:\n            self._reversed = False\n        self._shuffled_snames = [s[1] for s in self._shuffled_scenarios]\n        self._original_order = [s[0] for s in self._shuffled_scenarios]\n        self._cycle_idx = 0\n        self._cur_ROOTscen = self._shuffled_snames[0]\n        self.create_nodescen_dict()\n        \n        self._scenarios_this_epoch = set()",
  "def _begin_reverse_epoch(self):\n        self._reversed = True\n        self._shuffled_snames = [s[1] for s in reversed(self._shuffled_scenarios)]\n        self._original_order = [s[0] for s in reversed(self._shuffled_scenarios)]\n        self._cycle_idx = 0\n        self._cur_ROOTscen = self._shuffled_snames[0]\n        self.create_nodescen_dict()\n        \n        self._scenarios_this_epoch = set()",
  "def get_next(self):\n        next_scen = self._cur_ROOTscen\n        next_scendict = self.nodescen_dict\n        if next_scen in self._scenarios_this_epoch:\n            return None\n        self._scenarios_this_epoch.add(next_scen)\n        self._iter_scen()\n        return next_scendict",
  "def _iter_scen(self):\n        old_idx = self._cycle_idx\n        self._cycle_idx += self._iter_shift\n        ## wrap around\n        self._cycle_idx %= self._num_scenarios\n        \n        #do not reuse a previously visited scenario for 'ROOT'\n        tmp_cycle_idx = self._cycle_idx\n        while self._shuffled_snames[tmp_cycle_idx] in self._scenarios_this_epoch and (\n                (tmp_cycle_idx+1)%self._num_scenarios != self._cycle_idx):\n            tmp_cycle_idx +=1\n            tmp_cycle_idx %= self._num_scenarios\n        \n        self._cycle_idx = tmp_cycle_idx\n        \n        #Updating scenarios\n        self._cur_ROOTscen = self._shuffled_snames[self._cycle_idx]\n        if old_idx<self._cycle_idx:\n            scens_to_remove = self._shuffled_snames[old_idx:self._cycle_idx]\n        else:\n            scens_to_remove = self._shuffled_snames[old_idx:]+self._shuffled_snames[:self._cycle_idx]\n        self.update_nodescen_dict(scens_to_remove)",
  "def _vb(msg): \n            if self.verbose and self.opt.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)",
  "def _vb(msg): \n            if self.verbose and self.opt.cylinder_rank == 0:\n                print(\"(rank0) \" + msg)",
  "def _add_sname_to_node(ndn):\n                first = self._nonleaves[ndn].scenfirst\n                last = self._nonleaves[ndn].scenlast\n                if snum>=first and snum<=last:\n                    self.nodescen_dict[ndn] = sname\n                    return False\n                else:\n                    return True",
  "class FrankWolfeOuterBound(mpisppy.cylinders.spoke.OuterBoundSpoke):\n\n    converger_spoke_char = 'F'\n\n    def main(self):\n        self.opt.fwph_main()\n\n    def is_converged(self):\n        return self.got_kill_signal()\n\n    def sync(self):\n        # The FWPH spoke can call \"sync\" before it\n        # even starts doing anything, so its possible\n        # to get here without any bound information\n        if not hasattr(self.opt, '_local_bound'):\n            return\n        # Tell the hub about the most recent bound\n        self.bound = self.opt._local_bound\n\n    def finalize(self):\n        # The FWPH spoke can call \"finalize\" before it\n        # even starts doing anything, so its possible\n        # to get here without any bound information\n        # if we terminated early\n        if not hasattr(self.opt, '_local_bound'):\n            return\n        self.bound = self.opt._local_bound\n        self.final_bound = self.opt._local_bound\n        return self.final_bound",
  "def main(self):\n        self.opt.fwph_main()",
  "def is_converged(self):\n        return self.got_kill_signal()",
  "def sync(self):\n        # The FWPH spoke can call \"sync\" before it\n        # even starts doing anything, so its possible\n        # to get here without any bound information\n        if not hasattr(self.opt, '_local_bound'):\n            return\n        # Tell the hub about the most recent bound\n        self.bound = self.opt._local_bound",
  "def finalize(self):\n        # The FWPH spoke can call \"finalize\" before it\n        # even starts doing anything, so its possible\n        # to get here without any bound information\n        # if we terminated early\n        if not hasattr(self.opt, '_local_bound'):\n            return\n        self.bound = self.opt._local_bound\n        self.final_bound = self.opt._local_bound\n        return self.final_bound",
  "class SPCommunicator:\n    \"\"\" Notes: TODO\n    \"\"\"\n\n    def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        # flag for if the windows have been constructed\n        self._windows_constructed = False\n        self.fullcomm = fullcomm\n        self.strata_comm = strata_comm\n        self.cylinder_comm = cylinder_comm\n        self.global_rank = fullcomm.Get_rank()\n        self.strata_rank = strata_comm.Get_rank()\n        self.cylinder_rank = cylinder_comm.Get_rank()\n        self.n_spokes = strata_comm.Get_size() - 1\n        self.opt = spbase_object\n        self.inst_time = time.time() # For diagnostics\n        if options is None:\n            self.options = dict()\n        else:\n            self.options = options\n\n        # attach the SPCommunicator to\n        # the SPBase object\n        self.opt.spcomm = self\n\n    @abc.abstractmethod\n    def main(self):\n        \"\"\" Every hub/spoke must have a main function\n        \"\"\"\n        pass\n\n    def sync(self):\n        \"\"\" Every hub/spoke may have a sync function\n        \"\"\"\n        pass\n\n    def is_converged(self):\n        \"\"\" Every hub/spoke may have a is_converged function\n        \"\"\"\n        return False\n\n    def finalize(self):\n        \"\"\" Every hub/spoke may have a finalize function,\n            which does some final calculations/flushing to\n            disk after convergence\n        \"\"\"\n        pass\n\n    def hub_finalize(self):\n        \"\"\" Every hub may have another finalize function,\n            which collects any results from finalize\n        \"\"\"\n        pass\n\n    def allreduce_or(self, val):\n        local_val = np.array([val], dtype='int8')\n        global_val = np.zeros(1, dtype='int8')\n        self.cylinder_comm.Allreduce(local_val, global_val, op=MPI.LOR)\n        if global_val[0] > 0:\n            return True\n        else:\n            return False\n\n    def free_windows(self):\n        \"\"\"\n        \"\"\"\n        if self._windows_constructed:\n            for i in range(self.n_spokes):\n                self.windows[i].Free()\n            del self.buffers\n        self._windows_constructed = False\n\n    def _make_window(self, length, comm=None):\n        \"\"\" Create a local window object and its corresponding \n            memory buffer using MPI.Win.Allocate()\n\n            Args: \n                length (int): length of the buffer to create\n                comm (MPI Communicator, optional): MPI communicator object to\n                    create the window over. Default is self.strata_comm.\n\n            Returns:\n                window (MPI.Win object): The created window\n                buff (ndarray): Pointer to corresponding memory\n\n            Notes:\n                The created buffer will actually be +1 longer than length.\n                The last entry is a write number to keep track of new info.\n\n                This function assumes that the user has provided the correct\n                window size for the local buffer based on whether this process\n                is a hub or spoke, etc.\n        \"\"\"\n        if comm is None:\n            comm = self.strata_comm\n        size = MPI.DOUBLE.size * (length + 1)\n        window = MPI.Win.Allocate(size, MPI.DOUBLE.size, comm=comm)\n        buff = np.ndarray(dtype=\"d\", shape=(length + 1,), buffer=window.tomemory())\n        buff[-1] = 0. # Initialize the write number to zero\n        return window, buff",
  "def __init__(self, spbase_object, fullcomm, strata_comm, cylinder_comm, options=None):\n        # flag for if the windows have been constructed\n        self._windows_constructed = False\n        self.fullcomm = fullcomm\n        self.strata_comm = strata_comm\n        self.cylinder_comm = cylinder_comm\n        self.global_rank = fullcomm.Get_rank()\n        self.strata_rank = strata_comm.Get_rank()\n        self.cylinder_rank = cylinder_comm.Get_rank()\n        self.n_spokes = strata_comm.Get_size() - 1\n        self.opt = spbase_object\n        self.inst_time = time.time() # For diagnostics\n        if options is None:\n            self.options = dict()\n        else:\n            self.options = options\n\n        # attach the SPCommunicator to\n        # the SPBase object\n        self.opt.spcomm = self",
  "def main(self):\n        \"\"\" Every hub/spoke must have a main function\n        \"\"\"\n        pass",
  "def sync(self):\n        \"\"\" Every hub/spoke may have a sync function\n        \"\"\"\n        pass",
  "def is_converged(self):\n        \"\"\" Every hub/spoke may have a is_converged function\n        \"\"\"\n        return False",
  "def finalize(self):\n        \"\"\" Every hub/spoke may have a finalize function,\n            which does some final calculations/flushing to\n            disk after convergence\n        \"\"\"\n        pass",
  "def hub_finalize(self):\n        \"\"\" Every hub may have another finalize function,\n            which collects any results from finalize\n        \"\"\"\n        pass",
  "def allreduce_or(self, val):\n        local_val = np.array([val], dtype='int8')\n        global_val = np.zeros(1, dtype='int8')\n        self.cylinder_comm.Allreduce(local_val, global_val, op=MPI.LOR)\n        if global_val[0] > 0:\n            return True\n        else:\n            return False",
  "def free_windows(self):\n        \"\"\"\n        \"\"\"\n        if self._windows_constructed:\n            for i in range(self.n_spokes):\n                self.windows[i].Free()\n            del self.buffers\n        self._windows_constructed = False",
  "def _make_window(self, length, comm=None):\n        \"\"\" Create a local window object and its corresponding \n            memory buffer using MPI.Win.Allocate()\n\n            Args: \n                length (int): length of the buffer to create\n                comm (MPI Communicator, optional): MPI communicator object to\n                    create the window over. Default is self.strata_comm.\n\n            Returns:\n                window (MPI.Win object): The created window\n                buff (ndarray): Pointer to corresponding memory\n\n            Notes:\n                The created buffer will actually be +1 longer than length.\n                The last entry is a write number to keep track of new info.\n\n                This function assumes that the user has provided the correct\n                window size for the local buffer based on whether this process\n                is a hub or spoke, etc.\n        \"\"\"\n        if comm is None:\n            comm = self.strata_comm\n        size = MPI.DOUBLE.size * (length + 1)\n        window = MPI.Win.Allocate(size, MPI.DOUBLE.size, comm=comm)\n        buff = np.ndarray(dtype=\"d\", shape=(length + 1,), buffer=window.tomemory())\n        buff[-1] = 0. # Initialize the write number to zero\n        return window, buff",
  "class PrimalDualConverger(mpisppy.convergers.converger.Converger):\n    \"\"\" Convergence checker for the primal-dual metrics.\n        Primal convergence is measured as weighted sum over all scenarios s\n        p_{s} * ||x_{s} - \\bar{x}||_1.\n        Dual convergence is measured as\n        rho * ||\\bar{x}_{t} - \\bar{x}_{t-1}||_1\n    \"\"\"\n    def __init__(self, ph):\n        \"\"\" Initialization method for the PrimalDualConverger class.\"\"\"\n        super().__init__(ph)\n\n        self.options = ph.options.get('primal_dual_converger_options', {})\n        self._verbose = self.options.get('verbose', False)\n        self._ph = ph\n        self.convergence_threshold = self.options.get('tol', 1)\n        self.tracking = self.options.get('tracking', False)\n        self.prev_xbars = self._get_xbars()\n        self._rank = self._ph.cylinder_rank\n\n        if self.tracking and self._rank == 0:\n            # if phtracker is set up, save the results in the phtracker/hub folder\n            if 'phtracker_options' in self._ph.options:\n                tracker_options = self._ph.options[\"phtracker_options\"]\n                cylinder_name = tracker_options.get(\n                    \"cylinder_name\", type(self._ph.spcomm).__name__)\n                results_folder = tracker_options.get(\n                    \"results_folder\", \"results\")\n                results_folder = os.path.join(results_folder, cylinder_name)\n            else:\n                results_folder = self.options.get('results_folder', 'results')\n            self.tracker = TrackedData('pd', results_folder, plot=True, verbose=self._verbose)\n            os.makedirs(results_folder, exist_ok=True)\n            self.tracker.initialize_fnames(name=self.options.get('pd_fname', None))\n            self.tracker.initialize_df(['iteration', 'primal_gap', 'dual_gap'])\n\n    def _get_xbars(self):\n        \"\"\"\n        Get the current xbar values from the local scenarios\n        Returns:\n            xbars (dict): dictionary of xbar values indexed by\n                          (decision node name, index)\n        \"\"\"\n        xbars = {}\n        for s in self._ph.local_scenarios.values():\n            for ndn_i, xbar in s._mpisppy_model.xbars.items():\n                xbars[ndn_i] = xbar.value\n            break\n        return xbars\n\n    def _compute_primal_convergence(self):\n        \"\"\"\n        Compute the primal convergence metric\n        Returns:\n            global_sum_diff (float): primal convergence metric\n        \"\"\"\n        local_sum_diff = np.zeros(1)\n        global_sum_diff = np.zeros(1)\n        for _, s in self._ph.local_scenarios.items():\n            # we iterate over decision nodes instead of\n            # s._mpisppy_data.nonant_indices to use numpy\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                nlen = s._mpisppy_data.nlens[ndn]\n                x_bars = np.fromiter((s._mpisppy_model.xbars[ndn,i]._value\n                                      for i in range(nlen)), dtype='d')\n\n                nonants_array = np.fromiter(\n                    (v._value for v in node.nonant_vardata_list),\n                    dtype='d', count=nlen)\n                _l1 = np.abs(x_bars - nonants_array)\n\n                # invariant to prob_coeff being a scalar or array\n                prob = s._mpisppy_data.prob_coeff[ndn] * np.ones(nlen)\n                local_sum_diff[0] += np.dot(prob, _l1)\n\n        self._ph.comms[\"ROOT\"].Allreduce(local_sum_diff, global_sum_diff, op=MPI.SUM)\n        return global_sum_diff[0]\n\n    def _compute_dual_residual(self):\n        \"\"\" Compute the dual residual\n\n        Returns:\n           global_diff (float): difference between to consecutive x bars\n\n        \"\"\"\n        local_sum_diff = np.zeros(1)\n        global_sum_diff = np.zeros(1)\n        for s in self._ph.local_scenarios.values():\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                nlen = s._mpisppy_data.nlens[ndn]\n                rhos = np.fromiter((s._mpisppy_model.rho[ndn,i]._value\n                                    for i in range(nlen)), dtype='d')\n                xbars = np.fromiter((s._mpisppy_model.xbars[ndn,i]._value\n                                        for i in range(nlen)), dtype='d')\n                prev_xbars = np.fromiter((self.prev_xbars[ndn,i]\n                                            for i in range(nlen)), dtype='d')\n\n                local_sum_diff[0] += np.sum(rhos * np.abs(xbars - prev_xbars))\n\n        self._ph.comms[\"ROOT\"].Allreduce(local_sum_diff, global_sum_diff, op=MPI.SUM)\n        return global_sum_diff[0]\n\n    def is_converged(self):\n        \"\"\" check for convergence\n        Args:\n            self (object): create by prep\n\n        Returns:\n           converged?: True if converged, False otherwise\n        \"\"\"\n\n        primal_gap = self._compute_primal_convergence()\n        dual_gap = self._compute_dual_residual()\n        self.prev_xbars = self._get_xbars()\n        ret_val = max(primal_gap, dual_gap) <= self.convergence_threshold\n\n        if self._verbose and self._rank == 0:\n            print(f\"primal gap = {round(primal_gap, 5)}, dual gap = {round(dual_gap, 5)}\")\n\n            if ret_val:\n                print(\"Dual convergence check passed\")\n            else:\n                print(\"Dual convergence check failed \"\n                      f\"(requires primal + dual gaps) <= {self.convergence_threshold}\")\n        if self.tracking and self._rank == 0:\n            self.tracker.add_row([self._ph._PHIter, primal_gap, dual_gap])\n            self.tracker.write_out_data()\n        return ret_val\n\n    def plot_results(self):\n        \"\"\"\n        Plot the results of the convergence checks\n        by reading in csv file and plotting\n        \"\"\"\n        plot_fname = self.tracker.plot_fname\n        conv_data = pd.read_csv(self.tracker.fname)\n\n        # Create a log-scale plot\n        plt.semilogy(conv_data['iteration'], conv_data['primal_gap'], label='Primal Gap')\n        plt.semilogy(conv_data['iteration'], conv_data['dual_gap'], label='Dual Gap')\n\n        plt.xlabel('Iteration')\n        plt.ylabel('Convergence Metric')\n        plt.legend()\n        plt.savefig(plot_fname)\n        plt.close()\n\n    def post_everything(self):\n        '''\n        Reading the convergence data and plotting the results\n        '''\n        if self.tracking and self._rank == 0:\n            self.plot_results()",
  "def __init__(self, ph):\n        \"\"\" Initialization method for the PrimalDualConverger class.\"\"\"\n        super().__init__(ph)\n\n        self.options = ph.options.get('primal_dual_converger_options', {})\n        self._verbose = self.options.get('verbose', False)\n        self._ph = ph\n        self.convergence_threshold = self.options.get('tol', 1)\n        self.tracking = self.options.get('tracking', False)\n        self.prev_xbars = self._get_xbars()\n        self._rank = self._ph.cylinder_rank\n\n        if self.tracking and self._rank == 0:\n            # if phtracker is set up, save the results in the phtracker/hub folder\n            if 'phtracker_options' in self._ph.options:\n                tracker_options = self._ph.options[\"phtracker_options\"]\n                cylinder_name = tracker_options.get(\n                    \"cylinder_name\", type(self._ph.spcomm).__name__)\n                results_folder = tracker_options.get(\n                    \"results_folder\", \"results\")\n                results_folder = os.path.join(results_folder, cylinder_name)\n            else:\n                results_folder = self.options.get('results_folder', 'results')\n            self.tracker = TrackedData('pd', results_folder, plot=True, verbose=self._verbose)\n            os.makedirs(results_folder, exist_ok=True)\n            self.tracker.initialize_fnames(name=self.options.get('pd_fname', None))\n            self.tracker.initialize_df(['iteration', 'primal_gap', 'dual_gap'])",
  "def _get_xbars(self):\n        \"\"\"\n        Get the current xbar values from the local scenarios\n        Returns:\n            xbars (dict): dictionary of xbar values indexed by\n                          (decision node name, index)\n        \"\"\"\n        xbars = {}\n        for s in self._ph.local_scenarios.values():\n            for ndn_i, xbar in s._mpisppy_model.xbars.items():\n                xbars[ndn_i] = xbar.value\n            break\n        return xbars",
  "def _compute_primal_convergence(self):\n        \"\"\"\n        Compute the primal convergence metric\n        Returns:\n            global_sum_diff (float): primal convergence metric\n        \"\"\"\n        local_sum_diff = np.zeros(1)\n        global_sum_diff = np.zeros(1)\n        for _, s in self._ph.local_scenarios.items():\n            # we iterate over decision nodes instead of\n            # s._mpisppy_data.nonant_indices to use numpy\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                nlen = s._mpisppy_data.nlens[ndn]\n                x_bars = np.fromiter((s._mpisppy_model.xbars[ndn,i]._value\n                                      for i in range(nlen)), dtype='d')\n\n                nonants_array = np.fromiter(\n                    (v._value for v in node.nonant_vardata_list),\n                    dtype='d', count=nlen)\n                _l1 = np.abs(x_bars - nonants_array)\n\n                # invariant to prob_coeff being a scalar or array\n                prob = s._mpisppy_data.prob_coeff[ndn] * np.ones(nlen)\n                local_sum_diff[0] += np.dot(prob, _l1)\n\n        self._ph.comms[\"ROOT\"].Allreduce(local_sum_diff, global_sum_diff, op=MPI.SUM)\n        return global_sum_diff[0]",
  "def _compute_dual_residual(self):\n        \"\"\" Compute the dual residual\n\n        Returns:\n           global_diff (float): difference between to consecutive x bars\n\n        \"\"\"\n        local_sum_diff = np.zeros(1)\n        global_sum_diff = np.zeros(1)\n        for s in self._ph.local_scenarios.values():\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                nlen = s._mpisppy_data.nlens[ndn]\n                rhos = np.fromiter((s._mpisppy_model.rho[ndn,i]._value\n                                    for i in range(nlen)), dtype='d')\n                xbars = np.fromiter((s._mpisppy_model.xbars[ndn,i]._value\n                                        for i in range(nlen)), dtype='d')\n                prev_xbars = np.fromiter((self.prev_xbars[ndn,i]\n                                            for i in range(nlen)), dtype='d')\n\n                local_sum_diff[0] += np.sum(rhos * np.abs(xbars - prev_xbars))\n\n        self._ph.comms[\"ROOT\"].Allreduce(local_sum_diff, global_sum_diff, op=MPI.SUM)\n        return global_sum_diff[0]",
  "def is_converged(self):\n        \"\"\" check for convergence\n        Args:\n            self (object): create by prep\n\n        Returns:\n           converged?: True if converged, False otherwise\n        \"\"\"\n\n        primal_gap = self._compute_primal_convergence()\n        dual_gap = self._compute_dual_residual()\n        self.prev_xbars = self._get_xbars()\n        ret_val = max(primal_gap, dual_gap) <= self.convergence_threshold\n\n        if self._verbose and self._rank == 0:\n            print(f\"primal gap = {round(primal_gap, 5)}, dual gap = {round(dual_gap, 5)}\")\n\n            if ret_val:\n                print(\"Dual convergence check passed\")\n            else:\n                print(\"Dual convergence check failed \"\n                      f\"(requires primal + dual gaps) <= {self.convergence_threshold}\")\n        if self.tracking and self._rank == 0:\n            self.tracker.add_row([self._ph._PHIter, primal_gap, dual_gap])\n            self.tracker.write_out_data()\n        return ret_val",
  "def plot_results(self):\n        \"\"\"\n        Plot the results of the convergence checks\n        by reading in csv file and plotting\n        \"\"\"\n        plot_fname = self.tracker.plot_fname\n        conv_data = pd.read_csv(self.tracker.fname)\n\n        # Create a log-scale plot\n        plt.semilogy(conv_data['iteration'], conv_data['primal_gap'], label='Primal Gap')\n        plt.semilogy(conv_data['iteration'], conv_data['dual_gap'], label='Dual Gap')\n\n        plt.xlabel('Iteration')\n        plt.ylabel('Convergence Metric')\n        plt.legend()\n        plt.savefig(plot_fname)\n        plt.close()",
  "def post_everything(self):\n        '''\n        Reading the convergence data and plotting the results\n        '''\n        if self.tracking and self._rank == 0:\n            self.plot_results()",
  "class NormRhoConverger(mpisppy.convergers.converger.Converger):\n\n    def __init__(self, ph):\n        if 'norm_rho_converger_options' in ph.options and \\\n                'verbose' in ph.options['norm_rho_converger_options'] and \\\n                ph.options['norm_rho_converger_options']['verbose']:\n            self._verbose = True\n        else:\n            self._verbose = False\n        self.ph = ph\n\n    def _compute_rho_norm(self, ph):\n        local_rho_norm = np.zeros(1)\n        global_rho_norm = np.zeros(1)\n        local_rho_norm[0] = sum(s._mpisppy_probability*sum( rho._value for rho in s._mpisppy_model.rho.values())\\\n                                for s in ph.local_scenarios.values() )\n        ph.mpicomm.Allreduce(local_rho_norm, global_rho_norm, op=MPI.SUM)\n        return float(global_rho_norm[0])\n\n    def is_converged(self):\n        \"\"\" check for convergence\n        Args:\n            self (object): create by prep\n\n        Returns:\n           converged?: True if converged, False otherwise\n        \"\"\"\n        ## This will never do anything unless the norm rho updater is also used\n        if not hasattr(self.ph, \"_mpisppy_norm_rho_update_inuse\")\\\n                       or not self.ph._mpisppy_norm_rho_update_inuse:\n            raise RuntimeError(\"NormRhoConverger can only be used if NormRhoUpdater is\")\n        \n        log_rho_norm = math.log(self._compute_rho_norm(self.ph))\n\n        ret_val = log_rho_norm < self.ph.options['convthresh']\n        self.conv = log_rho_norm\n        if self._verbose and self.ph.cylinder_rank == 0:\n            print(f\"log(|rho|) = {log_rho_norm}\")\n            if ret_val:\n                print(\"Norm rho convergence check passed\")\n            else:\n                print(\"Adaptive rho convergence check failed \"\n                      f\"(requires log(|rho|) < {self.ph.options['convthresh']}\")\n                print(\"Continuing PH with updated rho\")\n        return ret_val",
  "def __init__(self, ph):\n        if 'norm_rho_converger_options' in ph.options and \\\n                'verbose' in ph.options['norm_rho_converger_options'] and \\\n                ph.options['norm_rho_converger_options']['verbose']:\n            self._verbose = True\n        else:\n            self._verbose = False\n        self.ph = ph",
  "def _compute_rho_norm(self, ph):\n        local_rho_norm = np.zeros(1)\n        global_rho_norm = np.zeros(1)\n        local_rho_norm[0] = sum(s._mpisppy_probability*sum( rho._value for rho in s._mpisppy_model.rho.values())\\\n                                for s in ph.local_scenarios.values() )\n        ph.mpicomm.Allreduce(local_rho_norm, global_rho_norm, op=MPI.SUM)\n        return float(global_rho_norm[0])",
  "def is_converged(self):\n        \"\"\" check for convergence\n        Args:\n            self (object): create by prep\n\n        Returns:\n           converged?: True if converged, False otherwise\n        \"\"\"\n        ## This will never do anything unless the norm rho updater is also used\n        if not hasattr(self.ph, \"_mpisppy_norm_rho_update_inuse\")\\\n                       or not self.ph._mpisppy_norm_rho_update_inuse:\n            raise RuntimeError(\"NormRhoConverger can only be used if NormRhoUpdater is\")\n        \n        log_rho_norm = math.log(self._compute_rho_norm(self.ph))\n\n        ret_val = log_rho_norm < self.ph.options['convthresh']\n        self.conv = log_rho_norm\n        if self._verbose and self.ph.cylinder_rank == 0:\n            print(f\"log(|rho|) = {log_rho_norm}\")\n            if ret_val:\n                print(\"Norm rho convergence check passed\")\n            else:\n                print(\"Adaptive rho convergence check failed \"\n                      f\"(requires log(|rho|) < {self.ph.options['convthresh']}\")\n                print(\"Continuing PH with updated rho\")\n        return ret_val",
  "class FractionalConverger(mpisppy.convergers.converger.Converger):\n    \"\"\" Illustrate a class to contain data used by the converger \n    NOTE: unlike the extensions, we get only our object back so\n          we need to keep references to everything we might want\n          to look at.\n    Args:\n        options (dict): keys are option names\n        local_scenarios (dict): keys are names, \n                                vals are concrete models with attachments\n        comms (dict): key is node name; val is a comm object\n        rank (int): mpi process rank\n    \"\"\"\n    def __init__(self, phb):\n        options = phb.options\n        self.name = \"fractintsnotconv\"\n        self.verbose = options[\"verbose\"]\n        self._options = options\n        self._local_scenarios = phb.local_scenarios\n        self._comms = phb.comms\n        self._rank = phb.cylinder_rank\n        if self.verbose:\n            print (\"Created converger=\",self.name)\n        \n    def _convergence_value(self):\n        \"\"\" compute the fraction of *not* converged ints\n        Args:\n            self (object): create by prep\n\n        Returns:\n            number of converged ints divided by number of ints\n        \"\"\"\n        numints = 0\n        numconv = 0\n        for k,s in self._local_scenarios.items():\n            nlens = s._mpisppy_data.nlens        \n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                for i in range(nlens[ndn]):\n                    xvar = node.nonant_vardata_list[i]\n                    if xvar.is_integer() or xvar.is_binary():\n                        numints += 1\n                        xb = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                        #print (\"dlw debug\",xb*xb, pyo.value(s._mpisppy_model.xsqbars[(ndn,i)]))\n                        if math.isclose(xb * xb, pyo.value(s._mpisppy_model.xsqbars[(ndn,i)]), abs_tol=1e-09):\n                            numconv += 1\n        if self.verbose:\n            print (self.name,\": numints=\",numints)\n        if numints > 0:\n            retval = 1.0 - numconv / numints\n        else:\n            retval = 0\n        if self.verbose:\n            print (self.name,\": convergence value=\",retval)\n        return retval\n\n    def is_converged(self):\n        \"\"\" check for convergence\n        Args:\n            self (object): create by prep\n\n        Returns:\n           converged?: True if converged, False otherwise\n        \"\"\"\n        self.conv = self._convergence_value()\n        return self.conv < self._options['convthresh']",
  "def __init__(self, phb):\n        options = phb.options\n        self.name = \"fractintsnotconv\"\n        self.verbose = options[\"verbose\"]\n        self._options = options\n        self._local_scenarios = phb.local_scenarios\n        self._comms = phb.comms\n        self._rank = phb.cylinder_rank\n        if self.verbose:\n            print (\"Created converger=\",self.name)",
  "def _convergence_value(self):\n        \"\"\" compute the fraction of *not* converged ints\n        Args:\n            self (object): create by prep\n\n        Returns:\n            number of converged ints divided by number of ints\n        \"\"\"\n        numints = 0\n        numconv = 0\n        for k,s in self._local_scenarios.items():\n            nlens = s._mpisppy_data.nlens        \n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                for i in range(nlens[ndn]):\n                    xvar = node.nonant_vardata_list[i]\n                    if xvar.is_integer() or xvar.is_binary():\n                        numints += 1\n                        xb = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                        #print (\"dlw debug\",xb*xb, pyo.value(s._mpisppy_model.xsqbars[(ndn,i)]))\n                        if math.isclose(xb * xb, pyo.value(s._mpisppy_model.xsqbars[(ndn,i)]), abs_tol=1e-09):\n                            numconv += 1\n        if self.verbose:\n            print (self.name,\": numints=\",numints)\n        if numints > 0:\n            retval = 1.0 - numconv / numints\n        else:\n            retval = 0\n        if self.verbose:\n            print (self.name,\": convergence value=\",retval)\n        return retval",
  "def is_converged(self):\n        \"\"\" check for convergence\n        Args:\n            self (object): create by prep\n\n        Returns:\n           converged?: True if converged, False otherwise\n        \"\"\"\n        self.conv = self._convergence_value()\n        return self.conv < self._options['convthresh']",
  "class Converger:\n    ''' Abstract base class for converger monitors.\n\n        Args:\n            opt (SPBase): The SPBase object for the current model\n    '''\n    def __init__(self, opt):\n        self.conv = None  # intended to be the value used for comparison\n\n    @abc.abstractmethod\n    def is_converged(self):\n        ''' Indicated whether the algorithm has converged.\n\n            Must return a boolean. If True, the algorithm will terminate at the\n            current iteration--no more solves will be performed by SPBase.\n            Otherwise, the iterations will continue.\n        '''\n        pass\n\n    def post_loops(self):\n        '''Method called after the termination of the algorithm.\n            This method is called after the post_loops of any extensions\n        '''\n        pass",
  "def __init__(self, opt):\n        self.conv = None",
  "def is_converged(self):\n        ''' Indicated whether the algorithm has converged.\n\n            Must return a boolean. If True, the algorithm will terminate at the\n            current iteration--no more solves will be performed by SPBase.\n            Otherwise, the iterations will continue.\n        '''\n        pass",
  "def post_loops(self):\n        '''Method called after the termination of the algorithm.\n            This method is called after the post_loops of any extensions\n        '''\n        pass",
  "class MultRhoUpdater(mpisppy.extensions.extension.Extension):\n\n    def __init__(self, ph):\n\n        self.ph = ph\n        self.mult_rho_options = \\\n            ph.options['mult_rho_options'] if 'mult_rho_options' in ph.options else dict()\n\n        self._set_options()\n        self._first_rho = None\n        self.best_conv = float(\"inf\")\n\n        \n    def _conv(self):\n        if self.ph.convobject is not None:\n            return self.ph.convobject.conv\n        else:\n            return self.ph.conv \n\n\n    def _set_options(self):\n        options = self.mult_rho_options\n        for attr_name, opt_name in _attr_to_option_name_map.items():\n            setattr(self, attr_name, options[opt_name] if opt_name in options else _mult_rho_defaults[opt_name])\n\n            \n    def _attach_rho_ratio_data(self, ph, conv):\n        if conv == None or conv == self._tol:\n            return\n        self.first_c = conv\n        if not self.ph.multistage:\n            # two stage\n            for s in ph.local_scenarios.values():\n                 break # arbitrary scenario\n            self._first_rho = {ndn_i: rho._value for ndn_i, rho in s._mpisppy_model.rho.items()}\n        else:\n            # loop over all scenarios to get all nodes when multi-stage (wastes time...)\n            self._first_rho = dict()\n            for k, s in ph.local_scenarios.items():\n                for ndn_i, rho  in s._mpisppy_model.rho.items():\n                    self._first_rho[ndn_i] = rho._value\n\n\n    def pre_iter0(self):\n        pass\n\n    def post_iter0(self):\n        pass\n\n    def miditer(self):\n\n        ph = self.ph\n        ph_iter = ph._PHIter\n        if (self._stop_iter is not None and \\\n            ph_iter > self._stop_iter) \\\n            or \\\n            (self._start_iter is not None and \\\n             ph_iter < self._start_iter):\n            return\n        conv =  self._conv()\n        if conv < self.best_conv:\n            self.best_conv = conv\n        else:\n            return   # only do something if we have a new best\n        if self._first_rho is None:\n            self._attach_rho_ratio_data(ph, conv)  # rho / conv\n        elif conv != 0:\n            for s in ph.local_scenarios.values():\n                for ndn_i, rho in s._mpisppy_model.rho.items():\n                    rho._value = self._first_rho[ndn_i] * self.first_c / conv\n            if ph.cylinder_rank == 0:\n                print(f\"MultRhoUpdater iter={ph_iter}; {ndn_i} now has value {rho._value}\")\n\n    def enditer(self):\n        pass\n\n    def post_everything(self):\n        pass",
  "def __init__(self, ph):\n\n        self.ph = ph\n        self.mult_rho_options = \\\n            ph.options['mult_rho_options'] if 'mult_rho_options' in ph.options else dict()\n\n        self._set_options()\n        self._first_rho = None\n        self.best_conv = float(\"inf\")",
  "def _conv(self):\n        if self.ph.convobject is not None:\n            return self.ph.convobject.conv\n        else:\n            return self.ph.conv",
  "def _set_options(self):\n        options = self.mult_rho_options\n        for attr_name, opt_name in _attr_to_option_name_map.items():\n            setattr(self, attr_name, options[opt_name] if opt_name in options else _mult_rho_defaults[opt_name])",
  "def _attach_rho_ratio_data(self, ph, conv):\n        if conv == None or conv == self._tol:\n            return\n        self.first_c = conv\n        if not self.ph.multistage:\n            # two stage\n            for s in ph.local_scenarios.values():\n                 break # arbitrary scenario\n            self._first_rho = {ndn_i: rho._value for ndn_i, rho in s._mpisppy_model.rho.items()}\n        else:\n            # loop over all scenarios to get all nodes when multi-stage (wastes time...)\n            self._first_rho = dict()\n            for k, s in ph.local_scenarios.items():\n                for ndn_i, rho  in s._mpisppy_model.rho.items():\n                    self._first_rho[ndn_i] = rho._value",
  "def pre_iter0(self):\n        pass",
  "def post_iter0(self):\n        pass",
  "def miditer(self):\n\n        ph = self.ph\n        ph_iter = ph._PHIter\n        if (self._stop_iter is not None and \\\n            ph_iter > self._stop_iter) \\\n            or \\\n            (self._start_iter is not None and \\\n             ph_iter < self._start_iter):\n            return\n        conv =  self._conv()\n        if conv < self.best_conv:\n            self.best_conv = conv\n        else:\n            return   # only do something if we have a new best\n        if self._first_rho is None:\n            self._attach_rho_ratio_data(ph, conv)  # rho / conv\n        elif conv != 0:\n            for s in ph.local_scenarios.values():\n                for ndn_i, rho in s._mpisppy_model.rho.items():\n                    rho._value = self._first_rho[ndn_i] * self.first_c / conv\n            if ph.cylinder_rank == 0:\n                print(f\"MultRhoUpdater iter={ph_iter}; {ndn_i} now has value {rho._value}\")",
  "def enditer(self):\n        pass",
  "def post_everything(self):\n        pass",
  "class Gradient_extension(mpisppy.extensions.extension.Extension):\n    \"\"\"\n    This extension makes PH use gradient-rho and the corresponding rho setter.\n    \n    Args:\n       opt (PHBase object): gives the problem\n       cfg (Config object): config object\n    \n    Attributes:\n       grad_object (Find_Grad object): gradient object\n    \n    \"\"\"\n    def __init__(self, opt, comm=None):\n        super().__init__(opt)\n        self.cylinder_rank = self.opt.cylinder_rank\n        self.cfg = opt.options[\"gradient_extension_options\"][\"cfg\"]\n        self.cfg_args_cache = {'grad_cost_file': self.cfg.grad_cost_file,\n                               'grad_rho_file': self.cfg.grad_rho_file,\n                               'rho_path': self.cfg.rho_path,\n                               'rho_setter': self.cfg.rho_setter}\n        self.cfg.grad_cost_file = './_temp_grad_cost_file.csv'\n        self.cfg.grad_rho_file = './_temp_grad_rho_file.csv'\n        self.cfg.rho_path = './_temp_grad_rho_file.csv'\n        self.grad_object = grad.Find_Grad(opt, self.cfg)\n        self.rho_setter = find_rho.Set_Rho(self.cfg).rho_setter\n        self.primal_conv_cache = []\n        self.dual_conv_cache = []\n        self.wt = WTracker(self.opt)\n\n    def _display_rho_values(self):\n        for sname, scenario in self.opt.local_scenarios.items():\n            rho_list = [scenario._mpisppy_model.rho[ndn_i]._value\n                      for ndn_i, _ in scenario._mpisppy_data.nonant_indices.items()]\n            print(sname, 'rho values: ', rho_list)\n            break\n\n    def _display_W_values(self):\n        for (sname, scenario) in self.opt.local_scenarios.items():\n            W_list = [w._value for w in scenario._mpisppy_model.W.values()]\n            print(sname, 'W values: ', W_list)\n            break\n\n    def _update_rho_primal_based(self):\n        curr_conv, last_conv = self.primal_conv_cache[-1], self.primal_conv_cache[-2]\n        primal_diff =  np.abs((last_conv - curr_conv) / last_conv)\n        return (primal_diff <= 0.05)\n\n    def _update_rho_dual_based(self):\n        curr_conv, last_conv = self.dual_conv_cache[-1], self.dual_conv_cache[-2]\n        dual_diff =  np.abs((last_conv - curr_conv) / last_conv)\n        return (dual_diff <= 0.05)\n\n\n    def pre_iter0(self):\n        pass\n\n    def post_iter0(self):\n        global_toc(\"Using gradient-based rho setter\")\n        self.primal_conv_cache.append(self.opt.convergence_diff())\n        self.dual_conv_cache.append(self.wt.W_diff())\n        self._display_rho_values()\n\n    def miditer(self):\n        self.primal_conv_cache.append(self.opt.convergence_diff())\n        self.dual_conv_cache.append(self.wt.W_diff())\n        if self.opt._PHIter == 1:\n            self.grad_object.write_grad_cost()\n        if self.opt._PHIter >= 0 and (self._update_rho_dual_based()):\n            self.grad_object.write_grad_rho()\n            rho_setter_kwargs = self.opt.options['rho_setter_kwargs'] \\\n                                if 'rho_setter_kwargs' in self.opt.options \\\n                                   else dict()\n            for sname, scenario in self.opt.local_scenarios.items():\n                rholist = self.rho_setter(scenario, **rho_setter_kwargs)\n                for (vid, rho) in rholist:\n                    (ndn, i) = scenario._mpisppy_data.varid_to_nonant_index[vid]\n                    scenario._mpisppy_model.rho[(ndn, i)] = rho\n            self._display_rho_values()\n\n\n    def enditer(self):\n        pass\n\n    def post_everything(self):\n        if self.cylinder_rank == 0 and os.path.exists(self.cfg.grad_rho_file):\n            os.remove(self.cfg.grad_rho_file)\n        if self.cylinder_rank == 0 and os.path.exists(self.cfg.grad_cost_file):\n            os.remove(self.cfg.grad_cost_file)\n        self.cfg.grad_cost_file = self.cfg_args_cache['grad_cost_file']\n        self.cfg.grad_rho_file = self.cfg_args_cache['grad_rho_file']\n        self.cfg.rho_path = self.cfg_args_cache['rho_path']",
  "def __init__(self, opt, comm=None):\n        super().__init__(opt)\n        self.cylinder_rank = self.opt.cylinder_rank\n        self.cfg = opt.options[\"gradient_extension_options\"][\"cfg\"]\n        self.cfg_args_cache = {'grad_cost_file': self.cfg.grad_cost_file,\n                               'grad_rho_file': self.cfg.grad_rho_file,\n                               'rho_path': self.cfg.rho_path,\n                               'rho_setter': self.cfg.rho_setter}\n        self.cfg.grad_cost_file = './_temp_grad_cost_file.csv'\n        self.cfg.grad_rho_file = './_temp_grad_rho_file.csv'\n        self.cfg.rho_path = './_temp_grad_rho_file.csv'\n        self.grad_object = grad.Find_Grad(opt, self.cfg)\n        self.rho_setter = find_rho.Set_Rho(self.cfg).rho_setter\n        self.primal_conv_cache = []\n        self.dual_conv_cache = []\n        self.wt = WTracker(self.opt)",
  "def _display_rho_values(self):\n        for sname, scenario in self.opt.local_scenarios.items():\n            rho_list = [scenario._mpisppy_model.rho[ndn_i]._value\n                      for ndn_i, _ in scenario._mpisppy_data.nonant_indices.items()]\n            print(sname, 'rho values: ', rho_list)\n            break",
  "def _display_W_values(self):\n        for (sname, scenario) in self.opt.local_scenarios.items():\n            W_list = [w._value for w in scenario._mpisppy_model.W.values()]\n            print(sname, 'W values: ', W_list)\n            break",
  "def _update_rho_primal_based(self):\n        curr_conv, last_conv = self.primal_conv_cache[-1], self.primal_conv_cache[-2]\n        primal_diff =  np.abs((last_conv - curr_conv) / last_conv)\n        return (primal_diff <= 0.05)",
  "def _update_rho_dual_based(self):\n        curr_conv, last_conv = self.dual_conv_cache[-1], self.dual_conv_cache[-2]\n        dual_diff =  np.abs((last_conv - curr_conv) / last_conv)\n        return (dual_diff <= 0.05)",
  "def pre_iter0(self):\n        pass",
  "def post_iter0(self):\n        global_toc(\"Using gradient-based rho setter\")\n        self.primal_conv_cache.append(self.opt.convergence_diff())\n        self.dual_conv_cache.append(self.wt.W_diff())\n        self._display_rho_values()",
  "def miditer(self):\n        self.primal_conv_cache.append(self.opt.convergence_diff())\n        self.dual_conv_cache.append(self.wt.W_diff())\n        if self.opt._PHIter == 1:\n            self.grad_object.write_grad_cost()\n        if self.opt._PHIter >= 0 and (self._update_rho_dual_based()):\n            self.grad_object.write_grad_rho()\n            rho_setter_kwargs = self.opt.options['rho_setter_kwargs'] \\\n                                if 'rho_setter_kwargs' in self.opt.options \\\n                                   else dict()\n            for sname, scenario in self.opt.local_scenarios.items():\n                rholist = self.rho_setter(scenario, **rho_setter_kwargs)\n                for (vid, rho) in rholist:\n                    (ndn, i) = scenario._mpisppy_data.varid_to_nonant_index[vid]\n                    scenario._mpisppy_model.rho[(ndn, i)] = rho\n            self._display_rho_values()",
  "def enditer(self):\n        pass",
  "def post_everything(self):\n        if self.cylinder_rank == 0 and os.path.exists(self.cfg.grad_rho_file):\n            os.remove(self.cfg.grad_rho_file)\n        if self.cylinder_rank == 0 and os.path.exists(self.cfg.grad_cost_file):\n            os.remove(self.cfg.grad_cost_file)\n        self.cfg.grad_cost_file = self.cfg_args_cache['grad_cost_file']\n        self.cfg.grad_rho_file = self.cfg_args_cache['grad_rho_file']\n        self.cfg.rho_path = self.cfg_args_cache['rho_path']",
  "class TrackedData():\n    ''' A class to manage the data for a single variable (e.g. gaps, bounds, etc.)\n    '''\n    def __init__(self, name, folder, plot=False, verbose=False):\n        self.name = name\n        self.folder = folder\n        self.plot = plot\n        self.verbose = verbose\n        self.columns = None\n        self.df = None\n        self.fname = None\n        self.plot_fname = None\n        self.seen_iters = set()\n\n    def initialize_fnames(self, name=None):\n        \"\"\" Initialize filenames for saving and plotting\n        \"\"\"\n        name = self.name if name is None else name\n        name = name[:-4] if name.endswith('.csv') else name\n\n        self.fname = os.path.join(self.folder, f'{name}.csv')\n        if self.plot:\n            self.plot_fname = os.path.join(self.folder, f'{name}.png')\n\n    def initialize_df(self, columns):\n        \"\"\" Initialize the dataframe for saving the data and write out the column names\n        as future rows will be appended to the dataframe\n        \"\"\"\n        self.columns = columns\n        self.df = pd.DataFrame(columns=columns)\n        self.df.to_csv(self.fname, index=False, header=True)\n\n    def add_row(self, row):\n        \"\"\" Add a row to the dataframe;\n        Assumes the first column is the iteration number if row is a list\n        \"\"\"\n        assert len(row) == len(self.columns)\n        if isinstance(row, dict):\n            row_iter = row['iteration']\n        elif isinstance(row, list):\n            row_iter = row[0]\n        else:\n            raise RuntimeError(\"row must be a dict or list\")\n        if row_iter in self.seen_iters:\n            if self.verbose:\n                print(f\"WARNING: Iteration {row_iter} already seen for {self.name}\")\n            return\n        self.seen_iters.add(row_iter)\n        # since append is deprecated\n        new_dict = pd.DataFrame([row], columns=self.columns)\n        self.df = pd.concat([self.df, new_dict], ignore_index=True)\n\n    def write_out_data(self):\n        \"\"\" Write out the cached data to csv file and clear the cache\n        \"\"\"\n        self.df.to_csv(self.fname, mode='a', header=False, index=False)\n        self.df = pd.DataFrame(columns=self.columns)",
  "class PHTracker(Extension):\n    \"\"\" Class for tracking the PH algorithm\n\n    NOTE:\n    Can generalize this code to beyond PH by subclassing TrackedData for each\n    variable type, e.g. TrackedGaps, TrackedBounds, etc. and then adding\n    the initialize_*, add_* and plot_* functions to the respective classes.\n    This seems like a lot of classes for the benefit of one extension, so will hold off.\n\n    Must pass cylinder_name in options if multiple cylinders of the same class are being used\n    \"\"\"\n    def __init__(self, opt):\n        \"\"\"\n        Args:\n            PH object (mpisppy.opt.ph.PH)\n        \"\"\"\n        super().__init__(opt)\n        self.verbose = self.opt.options[\"verbose\"]\n        if 'phtracker_options' in self.opt.options:\n            self.tracker_options = self.opt.options[\"phtracker_options\"]\n        else:\n            raise RuntimeError(\"phtracker_options not specified in options dict\")\n\n        self.results_folder = self.tracker_options.get(\"results_folder\", \"results\")\n        self.save_every = self.tracker_options.get(\"save_every\", 1)\n        self.write_every = self.tracker_options.get(\"write_every\", 3)\n        self._rank = self.opt.cylinder_rank\n        self._reduce_types = ['nonants', 'duals', 'scen_gaps']\n\n        self._track_var_to_func = {\n            'gaps': {'track': self.add_gaps,\n                     'finalize': self.plot_xbars_bounds_gaps},\n            'bounds': {'track': self.add_bounds,\n                       'finalize': self.plot_xbars_bounds_gaps},\n            'nonants': {'track': self.add_nonants,\n                        'finalize': self.plot_nonants_sgaps_duals},\n            'duals': {'track': self.add_duals,\n                      'finalize': self.plot_nonants_sgaps_duals},\n            'xbars': {'track': self.add_xbars,\n                      'finalize': self.plot_xbars_bounds_gaps},\n            'scen_gaps': {'track': self.add_scen_gaps,\n                          'finalize': self.plot_nonants_sgaps_duals}\n        }\n\n        # will initialize these after spcomm is initialized\n        self.spcomm = None\n        self.cylinder_folder = None\n        self.track_dict = None\n        self.finished_init = False\n\n    def finish_init(self):\n        \"\"\" Finish initialization of the extension as we need the spcomm object\n        to be initialized and the tracker is initialized with opt before spcomm\n        \"\"\"\n        self.spcomm = self.opt.spcomm\n        cylinder_name = self.tracker_options.get(\n            \"cylinder_name\", type(self.spcomm).__name__)\n        self.cylinder_folder = os.path.join(self.results_folder, cylinder_name)\n        track_types = ['gaps', 'bounds', 'nonants', 'duals', 'xbars', 'scen_gaps']\n\n        self.track_dict = {}\n        for t in track_types:\n            if self.tracker_options.get(f'track_{t}', False):\n                val = True\n                # only rank 0 needs to create tracker objects; other ranks need to\n                # know what data is being tracked\n                if self._rank == 0:\n                    plot = self.tracker_options.get(f'plot_{t}', False)\n                    val = TrackedData(t, self.cylinder_folder, plot, self.verbose)\n                    user_fname = self.tracker_options.get(f\"{t}_fname\", None)\n                    val.initialize_fnames(name=user_fname)\n\n                self.track_dict[t] = val\n\n        if self._rank == 0:\n            self.verify_tracking()\n            os.makedirs(self.cylinder_folder, exist_ok=True)\n\n        for t in self.track_dict.keys():\n            self.initialize_df_columns(t)\n        self.finished_init = True\n\n    @property\n    def curr_iter(self):\n        \"\"\" Get the current iteration number\n\n        NOTE: This should probably made less ad hoc\n        _PHIter could be inaccurate in that most spokes currently make\n        specific function calls rather than ph_main(). Therefore, _PHIter\n        may not be up to date.\n        \"\"\"\n        if hasattr(self.spcomm, 'A_iter'):\n            return self.spcomm.A_iter\n        if hasattr(self.spcomm, 'dk_iter'):\n            return self.spcomm.dk_iter\n        if hasattr(self.opt, '_PHIter'):\n            return self.opt._PHIter\n        raise RuntimeError(\"Iteration not found\")\n\n    def verify_tracking(self):\n        \"\"\" Verify that the user has specified the correct tracking options\n        \"\"\"\n        if 'gaps' in self.track_dict or 'bounds' in self.track_dict:\n            if isinstance(self.spcomm, Spoke) and 'get_hub_bounds' not in self.spcomm.options:\n                raise RuntimeError(\"Cannot save access hub gaps without passing\"\n                                   \" them in spcomm through get_hub_bounds\")\n\n    def get_var_names(self, xbar=False):\n        \"\"\" Get the names of the variables\n        \"\"\"\n        var_names = []\n        for (sname, model) in self.opt.local_scenarios.items():\n            for node in model._mpisppy_node_list:\n                for var in node.nonant_vardata_list:\n                        var_names.append(var.name if xbar else (sname, var.name))\n            if xbar:\n                break\n\n        return var_names\n\n    def get_scen_colnames(self):\n        \"\"\" Get the names of the scenarios\n        \"\"\"\n        scen_names = [(sname, b) for sname in self.opt.local_scenarios.keys()\n                      for b in ['ub', 'lb']]\n        return scen_names\n\n    def initialize_df_columns(self, track_var):\n        \"\"\" Create dataframes for saving the data by defining the columns\n        \"\"\"\n        if (track_var not in self._reduce_types) and self._rank != 0:\n            return\n\n        if track_var == 'gaps':\n            df_columns = ['hub abs. gap', 'hub rel. gap']\n            if isinstance(self.spcomm, Spoke):\n                df_columns += ['spoke abs. gap', 'spoke rel. gap']\n        elif track_var == 'bounds':\n            df_columns = ['hub upper bound', 'hub lower bound']\n            if isinstance(self.spcomm, Spoke):\n                df_columns += ['spoke bound']\n        elif track_var == 'nonants':\n            df_columns = self.get_var_names()\n        elif track_var == 'duals':\n            df_columns = self.get_var_names()\n        elif track_var == 'xbars':\n            df_columns = self.get_var_names(xbar=True)\n        elif track_var == 'scen_gaps':\n            df_columns = self.get_scen_colnames()\n        else:\n            raise RuntimeError(\"track_var not recognized\")\n\n        if self._rank == 0 and track_var not in self._reduce_types:\n            df_columns.insert(0, 'iteration')\n            self.track_dict[track_var].initialize_df(df_columns)\n        else:\n            comm = self.opt.comms['ROOT']\n            df_columns = comm.gather(df_columns, root=0)\n            if self._rank == 0:\n                df_columns = df_columns[0]\n                df_columns.insert(0, 'iteration')\n                self.track_dict[track_var].initialize_df(df_columns)\n\n    def _add_data_and_write(self, track_var, data, gather=True, final=False):\n        \"\"\" Gather the data from all ranks and write it out\n        Args:\n            track_var (str): the variable to track\n            data (dict): the data to write out\n            gather (bool): whether to gather the data or not\n        \"\"\"\n        if gather and track_var not in self._reduce_types:\n            if self.verbose:\n                print(f\"WARNING: Cannot gather {track_var} data; not a reduce type\")\n            return\n\n        if gather:\n            comm = self.opt.comms['ROOT']\n            data = comm.gather(data, root=0)\n            data = data[0]\n\n        if isinstance(data, dict):\n            data['iteration'] = self.curr_iter\n        elif isinstance(data, list):\n            data.insert(0, self.curr_iter)\n\n        if self._rank == 0:\n            self.track_dict[track_var].add_row(data)\n            if final or self.curr_iter % self.write_every == 0:\n                self.track_dict[track_var].write_out_data()\n\n    def _get_bounds(self):\n        spoke_bound = None\n        if isinstance(self.spcomm, Spoke):\n            hub_inner_bound = self.spcomm.hub_inner_bound\n            hub_outer_bound = self.spcomm.hub_outer_bound\n            spoke_bound = self.spcomm.bound\n        else:\n            hub_inner_bound = self.spcomm.BestInnerBound\n            hub_outer_bound = self.spcomm.BestOuterBound\n        return hub_outer_bound, hub_inner_bound, spoke_bound\n\n    def _ob_ib_process(self, ob, ib):\n        \"\"\" process the outer and inner bounds\n        Args:\n            ob (float): outer bound\n            ib (float): inner bound\n        Returns:\n            ub (float): upper bound\n            lb (float): lower bound\n        \"\"\"\n        if self.opt.is_minimizing:\n            ub = ib\n            lb = ob\n        else:\n            ub = ob\n            lb = ib\n        return ub, lb\n\n    def add_bounds(self, final=False):\n        \"\"\" add iteration bounds to row\n        \"\"\"\n        if self._rank != 0:\n            return\n        hub_outer_bound, hub_inner_bound, spoke_bound = self._get_bounds()\n        upper_bound, lower_bound = self._ob_ib_process(hub_outer_bound, hub_inner_bound)\n\n        if isinstance(self.spcomm, Spoke):\n            row = [upper_bound, lower_bound, spoke_bound]\n        else:\n            row = [upper_bound, lower_bound]\n        self._add_data_and_write('bounds', row, gather=False, final=final)\n\n    def _compute_rel_gap(self, abs_gap, outer_bound):\n        \"\"\" compute the relative gap using outer bound as the denominator\n        \"\"\"\n        ## define by the best solution, as is common\n        nano = float(\"nan\")  # typing aid\n        if (\n            abs_gap != nano\n            and abs_gap != float(\"inf\")\n            and abs_gap != float(\"-inf\")\n            and outer_bound != nano\n            and outer_bound != 0\n        ):\n            rel_gap = abs_gap / abs(outer_bound)\n        else:\n            rel_gap = float(\"inf\")\n\n        return rel_gap\n\n    def add_gaps(self, final=False):\n        \"\"\" add iteration gaps to row; spoke gap is computed relative to if it is\n            an outer bound or inner bound spoke\n        \"\"\"\n        if self._rank != 0:\n            return\n\n        hub_outer_bound, hub_inner_bound, spoke_bound = self._get_bounds()\n        upper_bound, lower_bound = self._ob_ib_process(hub_outer_bound, hub_inner_bound)\n        hub_abs_gap = upper_bound - lower_bound\n        hub_rel_gap = self._compute_rel_gap(hub_abs_gap, hub_outer_bound)\n\n        if isinstance(self.spcomm, Spoke):\n            # compute spoke gap relative to hub bound so that negative gap means\n            # the spoke is worse than the hub\n\n            if ConvergerSpokeType.OUTER_BOUND in self.spcomm.converger_spoke_types:\n                spoke_abs_gap = spoke_bound - hub_outer_bound  \\\n                    if self.opt.is_minimizing else hub_outer_bound - spoke_bound\n                spoke_rel_gap = self._compute_rel_gap(spoke_abs_gap, hub_outer_bound)\n            elif ConvergerSpokeType.INNER_BOUND in self.spcomm.converger_spoke_types:\n                spoke_abs_gap = hub_inner_bound - spoke_bound \\\n                    if self.opt.is_minimizing else spoke_bound - hub_inner_bound\n                spoke_rel_gap = self._compute_rel_gap(spoke_abs_gap, hub_inner_bound)\n            else:\n                raise RuntimeError(\"Converger spoke type not recognized\")\n            row = [hub_abs_gap, hub_rel_gap, spoke_abs_gap, spoke_rel_gap]\n        else:\n            row = [hub_abs_gap, hub_rel_gap]\n        self._add_data_and_write('gaps', row, gather=False, final=final)\n\n    def add_scen_gaps(self, final=False):\n        \"\"\" add iteration scenario gaps to row\n        \"\"\"\n        s_gaps = {}\n        for (sname, scenario) in self.opt.local_scenarios.items():\n            ob, ib = None, None\n            if hasattr(scenario._mpisppy_data, 'outer_bound'):\n                ob = scenario._mpisppy_data.outer_bound\n            if hasattr(scenario._mpisppy_data, 'inner_bound'):\n                ib = scenario._mpisppy_data.inner_bound\n            ub, lb = self._ob_ib_process(ob, ib)\n            s_gaps[(sname, 'ub')] = ub\n            s_gaps[(sname, 'lb')] = lb\n\n        self._add_data_and_write('scen_gaps', s_gaps, gather=True, final=final)\n\n    def add_nonants(self, final=False):\n        \"\"\" add iteration nonants to row\n        \"\"\"\n        nonants = {}\n        for k, s in self.opt.local_scenarios.items():\n            for node in s._mpisppy_node_list:\n                for var in node.nonant_vardata_list:\n                    nonants[(k, var.name)] = var.value\n\n        self._add_data_and_write('nonants', nonants, gather=True, final=final)\n\n    def add_duals(self, final=False):\n        \"\"\" add iteration duals to rpw\n        \"\"\"\n        local_duals_data = {(sname, var.name):\n                            scenario._mpisppy_model.W[node.name, ix]._value\n                    for (sname, scenario) in self.opt.local_scenarios.items()\n                    for node in scenario._mpisppy_node_list\n                    for (ix, var) in enumerate(node.nonant_vardata_list)}\n\n        self._add_data_and_write('duals', local_duals_data, gather=True, final=final)\n\n    def add_xbars(self, final=False):\n        \"\"\" add iteration xbars to xbars_df\n        \"\"\"\n        if self._rank != 0:\n            return\n        sname = list(self.opt.local_scenarios.keys())[0]\n        scenario = self.opt.local_scenarios[sname]\n        xbars = {var.name: scenario._mpisppy_model.xbars[node.name, ix]._value\n                    for node in scenario._mpisppy_node_list\n                    for (ix, var) in enumerate(node.nonant_vardata_list)}\n\n        self._add_data_and_write('xbars', xbars, gather=False, final=final)\n\n    def plot_gaps(self, var):\n        ''' plot the gaps; Assumes gaps are saved in a csv file\n        '''\n\n\n        df = pd.read_csv(self.track_dict[var].fname, sep=',')\n        df = df.replace([np.inf, -np.inf], np.nan)\n\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['iteration'], df['hub abs. gap'], marker='o', label='Hub Abs. Gap')\n        plt.plot(df['iteration'], df['hub rel. gap'], marker='o', label='Hub Rel. Gap')\n        if isinstance(self.spcomm, Spoke):\n            plt.plot(df['iteration'], df['spoke abs. gap'], marker='o', label='Spoke Abs. Gap')\n            plt.plot(df['iteration'], df['spoke rel. gap'], marker='o', label='Spoke Rel. Gap')\n\n        plt.xlabel('Iteration')\n        plt.ylabel('Value (log scale)')\n        plt.title('Absolute and Relative Gaps Over Iterations')\n        plt.legend()\n        plt.grid(True, which='major', linestyle='-', linewidth='0.5')\n        plt.grid(True, which='minor', linestyle='--', linewidth='0.5')\n\n        plt.savefig(self.track_dict[var].plot_fname)\n        plt.close()\n\n    def plot_nonants_sgaps_duals(self, var):\n        ''' plot the nonants/scene gaps/dual values; Assumes var is saved in a csv file\n        '''\n        if self._rank != 0:\n            return\n\n        if var not in ['nonants', 'duals', 'scen_gaps']:\n            raise RuntimeError(\"var must be either nonants or duals\")\n\n        df = pd.read_csv(self.track_dict[var].fname, sep=',')\n        df = df.replace([np.inf, -np.inf], np.nan)\n        df.columns = [col if col == 'iteration' else ast.literal_eval(col) for col in df.columns]\n\n        plt.figure(figsize=(16, 6))  # Adjust the figure size as needed\n        column_names = df.columns[1:]\n\n        for col in column_names:\n            scenario, variable = col\n\n            label = f'{scenario} {variable}'\n            plt.plot(df['iteration'], df[col], label=label)\n\n        plt.legend(loc='lower right')\n        plt.xlabel('Iteration')\n        plt.ylabel(f'{var.capitalize()} Value')\n        plt.grid(True)\n        plt.savefig(self.track_dict[var].plot_fname)\n        plt.close()\n\n    def plot_xbars_bounds_gaps(self, var):\n        ''' plot the xbar/bounds/gaps values;\n            Assumes xbars/bounds/gaps are saved in a csv file\n        '''\n        if self._rank != 0:\n            return\n\n        if var not in ['xbars', 'bounds', 'gaps']:\n            raise RuntimeError('var must be either xbars, bounds, or gaps')\n\n        df = pd.read_csv(self.track_dict[var].fname, sep=',')\n        df = df.replace([np.inf, -np.inf], np.nan)\n\n        plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n        column_names = df.columns[1:]\n\n        for col in column_names:\n            plt.plot(df['iteration'], df[col], label=col.capitalize())\n\n        if var == 'gaps':\n            df = df.dropna()\n            nonzero_gaps = df[df['hub rel. gap'] != 0]['hub rel. gap']\n            if len(nonzero_gaps) > 0:\n                threshold = np.percentile(nonzero_gaps, 10)\n                plt.yscale('symlog', linthresh=threshold)\n            else:\n                if self.verbose:\n                    print(\"WARNING: No nonzero gaps to compute threshold\")\n\n        plt.xlabel('Iteration')\n        plt.ylabel(f'{var.capitalize()} values')\n        plt.title(f'{var.capitalize()} Over Iterations')\n        plt.legend()\n        plt.grid(True)\n        plt.savefig(self.track_dict[var].plot_fname)\n        plt.close()\n\n    def pre_solve_loop(self):\n        if not self.finished_init:\n            self.finish_init()\n        if self.curr_iter % self.save_every == 0:\n            for track_var in self.track_dict.keys():\n                self._track_var_to_func[track_var]['track']()\n\n    def post_everything(self):\n        for track_var in self.track_dict.keys():\n                self._track_var_to_func[track_var]['track'](final=True)\n                self._track_var_to_func[track_var]['finalize'](var=track_var)",
  "def __init__(self, name, folder, plot=False, verbose=False):\n        self.name = name\n        self.folder = folder\n        self.plot = plot\n        self.verbose = verbose\n        self.columns = None\n        self.df = None\n        self.fname = None\n        self.plot_fname = None\n        self.seen_iters = set()",
  "def initialize_fnames(self, name=None):\n        \"\"\" Initialize filenames for saving and plotting\n        \"\"\"\n        name = self.name if name is None else name\n        name = name[:-4] if name.endswith('.csv') else name\n\n        self.fname = os.path.join(self.folder, f'{name}.csv')\n        if self.plot:\n            self.plot_fname = os.path.join(self.folder, f'{name}.png')",
  "def initialize_df(self, columns):\n        \"\"\" Initialize the dataframe for saving the data and write out the column names\n        as future rows will be appended to the dataframe\n        \"\"\"\n        self.columns = columns\n        self.df = pd.DataFrame(columns=columns)\n        self.df.to_csv(self.fname, index=False, header=True)",
  "def add_row(self, row):\n        \"\"\" Add a row to the dataframe;\n        Assumes the first column is the iteration number if row is a list\n        \"\"\"\n        assert len(row) == len(self.columns)\n        if isinstance(row, dict):\n            row_iter = row['iteration']\n        elif isinstance(row, list):\n            row_iter = row[0]\n        else:\n            raise RuntimeError(\"row must be a dict or list\")\n        if row_iter in self.seen_iters:\n            if self.verbose:\n                print(f\"WARNING: Iteration {row_iter} already seen for {self.name}\")\n            return\n        self.seen_iters.add(row_iter)\n        # since append is deprecated\n        new_dict = pd.DataFrame([row], columns=self.columns)\n        self.df = pd.concat([self.df, new_dict], ignore_index=True)",
  "def write_out_data(self):\n        \"\"\" Write out the cached data to csv file and clear the cache\n        \"\"\"\n        self.df.to_csv(self.fname, mode='a', header=False, index=False)\n        self.df = pd.DataFrame(columns=self.columns)",
  "def __init__(self, opt):\n        \"\"\"\n        Args:\n            PH object (mpisppy.opt.ph.PH)\n        \"\"\"\n        super().__init__(opt)\n        self.verbose = self.opt.options[\"verbose\"]\n        if 'phtracker_options' in self.opt.options:\n            self.tracker_options = self.opt.options[\"phtracker_options\"]\n        else:\n            raise RuntimeError(\"phtracker_options not specified in options dict\")\n\n        self.results_folder = self.tracker_options.get(\"results_folder\", \"results\")\n        self.save_every = self.tracker_options.get(\"save_every\", 1)\n        self.write_every = self.tracker_options.get(\"write_every\", 3)\n        self._rank = self.opt.cylinder_rank\n        self._reduce_types = ['nonants', 'duals', 'scen_gaps']\n\n        self._track_var_to_func = {\n            'gaps': {'track': self.add_gaps,\n                     'finalize': self.plot_xbars_bounds_gaps},\n            'bounds': {'track': self.add_bounds,\n                       'finalize': self.plot_xbars_bounds_gaps},\n            'nonants': {'track': self.add_nonants,\n                        'finalize': self.plot_nonants_sgaps_duals},\n            'duals': {'track': self.add_duals,\n                      'finalize': self.plot_nonants_sgaps_duals},\n            'xbars': {'track': self.add_xbars,\n                      'finalize': self.plot_xbars_bounds_gaps},\n            'scen_gaps': {'track': self.add_scen_gaps,\n                          'finalize': self.plot_nonants_sgaps_duals}\n        }\n\n        # will initialize these after spcomm is initialized\n        self.spcomm = None\n        self.cylinder_folder = None\n        self.track_dict = None\n        self.finished_init = False",
  "def finish_init(self):\n        \"\"\" Finish initialization of the extension as we need the spcomm object\n        to be initialized and the tracker is initialized with opt before spcomm\n        \"\"\"\n        self.spcomm = self.opt.spcomm\n        cylinder_name = self.tracker_options.get(\n            \"cylinder_name\", type(self.spcomm).__name__)\n        self.cylinder_folder = os.path.join(self.results_folder, cylinder_name)\n        track_types = ['gaps', 'bounds', 'nonants', 'duals', 'xbars', 'scen_gaps']\n\n        self.track_dict = {}\n        for t in track_types:\n            if self.tracker_options.get(f'track_{t}', False):\n                val = True\n                # only rank 0 needs to create tracker objects; other ranks need to\n                # know what data is being tracked\n                if self._rank == 0:\n                    plot = self.tracker_options.get(f'plot_{t}', False)\n                    val = TrackedData(t, self.cylinder_folder, plot, self.verbose)\n                    user_fname = self.tracker_options.get(f\"{t}_fname\", None)\n                    val.initialize_fnames(name=user_fname)\n\n                self.track_dict[t] = val\n\n        if self._rank == 0:\n            self.verify_tracking()\n            os.makedirs(self.cylinder_folder, exist_ok=True)\n\n        for t in self.track_dict.keys():\n            self.initialize_df_columns(t)\n        self.finished_init = True",
  "def curr_iter(self):\n        \"\"\" Get the current iteration number\n\n        NOTE: This should probably made less ad hoc\n        _PHIter could be inaccurate in that most spokes currently make\n        specific function calls rather than ph_main(). Therefore, _PHIter\n        may not be up to date.\n        \"\"\"\n        if hasattr(self.spcomm, 'A_iter'):\n            return self.spcomm.A_iter\n        if hasattr(self.spcomm, 'dk_iter'):\n            return self.spcomm.dk_iter\n        if hasattr(self.opt, '_PHIter'):\n            return self.opt._PHIter\n        raise RuntimeError(\"Iteration not found\")",
  "def verify_tracking(self):\n        \"\"\" Verify that the user has specified the correct tracking options\n        \"\"\"\n        if 'gaps' in self.track_dict or 'bounds' in self.track_dict:\n            if isinstance(self.spcomm, Spoke) and 'get_hub_bounds' not in self.spcomm.options:\n                raise RuntimeError(\"Cannot save access hub gaps without passing\"\n                                   \" them in spcomm through get_hub_bounds\")",
  "def get_var_names(self, xbar=False):\n        \"\"\" Get the names of the variables\n        \"\"\"\n        var_names = []\n        for (sname, model) in self.opt.local_scenarios.items():\n            for node in model._mpisppy_node_list:\n                for var in node.nonant_vardata_list:\n                        var_names.append(var.name if xbar else (sname, var.name))\n            if xbar:\n                break\n\n        return var_names",
  "def get_scen_colnames(self):\n        \"\"\" Get the names of the scenarios\n        \"\"\"\n        scen_names = [(sname, b) for sname in self.opt.local_scenarios.keys()\n                      for b in ['ub', 'lb']]\n        return scen_names",
  "def initialize_df_columns(self, track_var):\n        \"\"\" Create dataframes for saving the data by defining the columns\n        \"\"\"\n        if (track_var not in self._reduce_types) and self._rank != 0:\n            return\n\n        if track_var == 'gaps':\n            df_columns = ['hub abs. gap', 'hub rel. gap']\n            if isinstance(self.spcomm, Spoke):\n                df_columns += ['spoke abs. gap', 'spoke rel. gap']\n        elif track_var == 'bounds':\n            df_columns = ['hub upper bound', 'hub lower bound']\n            if isinstance(self.spcomm, Spoke):\n                df_columns += ['spoke bound']\n        elif track_var == 'nonants':\n            df_columns = self.get_var_names()\n        elif track_var == 'duals':\n            df_columns = self.get_var_names()\n        elif track_var == 'xbars':\n            df_columns = self.get_var_names(xbar=True)\n        elif track_var == 'scen_gaps':\n            df_columns = self.get_scen_colnames()\n        else:\n            raise RuntimeError(\"track_var not recognized\")\n\n        if self._rank == 0 and track_var not in self._reduce_types:\n            df_columns.insert(0, 'iteration')\n            self.track_dict[track_var].initialize_df(df_columns)\n        else:\n            comm = self.opt.comms['ROOT']\n            df_columns = comm.gather(df_columns, root=0)\n            if self._rank == 0:\n                df_columns = df_columns[0]\n                df_columns.insert(0, 'iteration')\n                self.track_dict[track_var].initialize_df(df_columns)",
  "def _add_data_and_write(self, track_var, data, gather=True, final=False):\n        \"\"\" Gather the data from all ranks and write it out\n        Args:\n            track_var (str): the variable to track\n            data (dict): the data to write out\n            gather (bool): whether to gather the data or not\n        \"\"\"\n        if gather and track_var not in self._reduce_types:\n            if self.verbose:\n                print(f\"WARNING: Cannot gather {track_var} data; not a reduce type\")\n            return\n\n        if gather:\n            comm = self.opt.comms['ROOT']\n            data = comm.gather(data, root=0)\n            data = data[0]\n\n        if isinstance(data, dict):\n            data['iteration'] = self.curr_iter\n        elif isinstance(data, list):\n            data.insert(0, self.curr_iter)\n\n        if self._rank == 0:\n            self.track_dict[track_var].add_row(data)\n            if final or self.curr_iter % self.write_every == 0:\n                self.track_dict[track_var].write_out_data()",
  "def _get_bounds(self):\n        spoke_bound = None\n        if isinstance(self.spcomm, Spoke):\n            hub_inner_bound = self.spcomm.hub_inner_bound\n            hub_outer_bound = self.spcomm.hub_outer_bound\n            spoke_bound = self.spcomm.bound\n        else:\n            hub_inner_bound = self.spcomm.BestInnerBound\n            hub_outer_bound = self.spcomm.BestOuterBound\n        return hub_outer_bound, hub_inner_bound, spoke_bound",
  "def _ob_ib_process(self, ob, ib):\n        \"\"\" process the outer and inner bounds\n        Args:\n            ob (float): outer bound\n            ib (float): inner bound\n        Returns:\n            ub (float): upper bound\n            lb (float): lower bound\n        \"\"\"\n        if self.opt.is_minimizing:\n            ub = ib\n            lb = ob\n        else:\n            ub = ob\n            lb = ib\n        return ub, lb",
  "def add_bounds(self, final=False):\n        \"\"\" add iteration bounds to row\n        \"\"\"\n        if self._rank != 0:\n            return\n        hub_outer_bound, hub_inner_bound, spoke_bound = self._get_bounds()\n        upper_bound, lower_bound = self._ob_ib_process(hub_outer_bound, hub_inner_bound)\n\n        if isinstance(self.spcomm, Spoke):\n            row = [upper_bound, lower_bound, spoke_bound]\n        else:\n            row = [upper_bound, lower_bound]\n        self._add_data_and_write('bounds', row, gather=False, final=final)",
  "def _compute_rel_gap(self, abs_gap, outer_bound):\n        \"\"\" compute the relative gap using outer bound as the denominator\n        \"\"\"\n        ## define by the best solution, as is common\n        nano = float(\"nan\")  # typing aid\n        if (\n            abs_gap != nano\n            and abs_gap != float(\"inf\")\n            and abs_gap != float(\"-inf\")\n            and outer_bound != nano\n            and outer_bound != 0\n        ):\n            rel_gap = abs_gap / abs(outer_bound)\n        else:\n            rel_gap = float(\"inf\")\n\n        return rel_gap",
  "def add_gaps(self, final=False):\n        \"\"\" add iteration gaps to row; spoke gap is computed relative to if it is\n            an outer bound or inner bound spoke\n        \"\"\"\n        if self._rank != 0:\n            return\n\n        hub_outer_bound, hub_inner_bound, spoke_bound = self._get_bounds()\n        upper_bound, lower_bound = self._ob_ib_process(hub_outer_bound, hub_inner_bound)\n        hub_abs_gap = upper_bound - lower_bound\n        hub_rel_gap = self._compute_rel_gap(hub_abs_gap, hub_outer_bound)\n\n        if isinstance(self.spcomm, Spoke):\n            # compute spoke gap relative to hub bound so that negative gap means\n            # the spoke is worse than the hub\n\n            if ConvergerSpokeType.OUTER_BOUND in self.spcomm.converger_spoke_types:\n                spoke_abs_gap = spoke_bound - hub_outer_bound  \\\n                    if self.opt.is_minimizing else hub_outer_bound - spoke_bound\n                spoke_rel_gap = self._compute_rel_gap(spoke_abs_gap, hub_outer_bound)\n            elif ConvergerSpokeType.INNER_BOUND in self.spcomm.converger_spoke_types:\n                spoke_abs_gap = hub_inner_bound - spoke_bound \\\n                    if self.opt.is_minimizing else spoke_bound - hub_inner_bound\n                spoke_rel_gap = self._compute_rel_gap(spoke_abs_gap, hub_inner_bound)\n            else:\n                raise RuntimeError(\"Converger spoke type not recognized\")\n            row = [hub_abs_gap, hub_rel_gap, spoke_abs_gap, spoke_rel_gap]\n        else:\n            row = [hub_abs_gap, hub_rel_gap]\n        self._add_data_and_write('gaps', row, gather=False, final=final)",
  "def add_scen_gaps(self, final=False):\n        \"\"\" add iteration scenario gaps to row\n        \"\"\"\n        s_gaps = {}\n        for (sname, scenario) in self.opt.local_scenarios.items():\n            ob, ib = None, None\n            if hasattr(scenario._mpisppy_data, 'outer_bound'):\n                ob = scenario._mpisppy_data.outer_bound\n            if hasattr(scenario._mpisppy_data, 'inner_bound'):\n                ib = scenario._mpisppy_data.inner_bound\n            ub, lb = self._ob_ib_process(ob, ib)\n            s_gaps[(sname, 'ub')] = ub\n            s_gaps[(sname, 'lb')] = lb\n\n        self._add_data_and_write('scen_gaps', s_gaps, gather=True, final=final)",
  "def add_nonants(self, final=False):\n        \"\"\" add iteration nonants to row\n        \"\"\"\n        nonants = {}\n        for k, s in self.opt.local_scenarios.items():\n            for node in s._mpisppy_node_list:\n                for var in node.nonant_vardata_list:\n                    nonants[(k, var.name)] = var.value\n\n        self._add_data_and_write('nonants', nonants, gather=True, final=final)",
  "def add_duals(self, final=False):\n        \"\"\" add iteration duals to rpw\n        \"\"\"\n        local_duals_data = {(sname, var.name):\n                            scenario._mpisppy_model.W[node.name, ix]._value\n                    for (sname, scenario) in self.opt.local_scenarios.items()\n                    for node in scenario._mpisppy_node_list\n                    for (ix, var) in enumerate(node.nonant_vardata_list)}\n\n        self._add_data_and_write('duals', local_duals_data, gather=True, final=final)",
  "def add_xbars(self, final=False):\n        \"\"\" add iteration xbars to xbars_df\n        \"\"\"\n        if self._rank != 0:\n            return\n        sname = list(self.opt.local_scenarios.keys())[0]\n        scenario = self.opt.local_scenarios[sname]\n        xbars = {var.name: scenario._mpisppy_model.xbars[node.name, ix]._value\n                    for node in scenario._mpisppy_node_list\n                    for (ix, var) in enumerate(node.nonant_vardata_list)}\n\n        self._add_data_and_write('xbars', xbars, gather=False, final=final)",
  "def plot_gaps(self, var):\n        ''' plot the gaps; Assumes gaps are saved in a csv file\n        '''\n\n\n        df = pd.read_csv(self.track_dict[var].fname, sep=',')\n        df = df.replace([np.inf, -np.inf], np.nan)\n\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['iteration'], df['hub abs. gap'], marker='o', label='Hub Abs. Gap')\n        plt.plot(df['iteration'], df['hub rel. gap'], marker='o', label='Hub Rel. Gap')\n        if isinstance(self.spcomm, Spoke):\n            plt.plot(df['iteration'], df['spoke abs. gap'], marker='o', label='Spoke Abs. Gap')\n            plt.plot(df['iteration'], df['spoke rel. gap'], marker='o', label='Spoke Rel. Gap')\n\n        plt.xlabel('Iteration')\n        plt.ylabel('Value (log scale)')\n        plt.title('Absolute and Relative Gaps Over Iterations')\n        plt.legend()\n        plt.grid(True, which='major', linestyle='-', linewidth='0.5')\n        plt.grid(True, which='minor', linestyle='--', linewidth='0.5')\n\n        plt.savefig(self.track_dict[var].plot_fname)\n        plt.close()",
  "def plot_nonants_sgaps_duals(self, var):\n        ''' plot the nonants/scene gaps/dual values; Assumes var is saved in a csv file\n        '''\n        if self._rank != 0:\n            return\n\n        if var not in ['nonants', 'duals', 'scen_gaps']:\n            raise RuntimeError(\"var must be either nonants or duals\")\n\n        df = pd.read_csv(self.track_dict[var].fname, sep=',')\n        df = df.replace([np.inf, -np.inf], np.nan)\n        df.columns = [col if col == 'iteration' else ast.literal_eval(col) for col in df.columns]\n\n        plt.figure(figsize=(16, 6))  # Adjust the figure size as needed\n        column_names = df.columns[1:]\n\n        for col in column_names:\n            scenario, variable = col\n\n            label = f'{scenario} {variable}'\n            plt.plot(df['iteration'], df[col], label=label)\n\n        plt.legend(loc='lower right')\n        plt.xlabel('Iteration')\n        plt.ylabel(f'{var.capitalize()} Value')\n        plt.grid(True)\n        plt.savefig(self.track_dict[var].plot_fname)\n        plt.close()",
  "def plot_xbars_bounds_gaps(self, var):\n        ''' plot the xbar/bounds/gaps values;\n            Assumes xbars/bounds/gaps are saved in a csv file\n        '''\n        if self._rank != 0:\n            return\n\n        if var not in ['xbars', 'bounds', 'gaps']:\n            raise RuntimeError('var must be either xbars, bounds, or gaps')\n\n        df = pd.read_csv(self.track_dict[var].fname, sep=',')\n        df = df.replace([np.inf, -np.inf], np.nan)\n\n        plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n        column_names = df.columns[1:]\n\n        for col in column_names:\n            plt.plot(df['iteration'], df[col], label=col.capitalize())\n\n        if var == 'gaps':\n            df = df.dropna()\n            nonzero_gaps = df[df['hub rel. gap'] != 0]['hub rel. gap']\n            if len(nonzero_gaps) > 0:\n                threshold = np.percentile(nonzero_gaps, 10)\n                plt.yscale('symlog', linthresh=threshold)\n            else:\n                if self.verbose:\n                    print(\"WARNING: No nonzero gaps to compute threshold\")\n\n        plt.xlabel('Iteration')\n        plt.ylabel(f'{var.capitalize()} values')\n        plt.title(f'{var.capitalize()} Over Iterations')\n        plt.legend()\n        plt.grid(True)\n        plt.savefig(self.track_dict[var].plot_fname)\n        plt.close()",
  "def pre_solve_loop(self):\n        if not self.finished_init:\n            self.finish_init()\n        if self.curr_iter % self.save_every == 0:\n            for track_var in self.track_dict.keys():\n                self._track_var_to_func[track_var]['track']()",
  "def post_everything(self):\n        for track_var in self.track_dict.keys():\n                self._track_var_to_func[track_var]['track'](final=True)\n                self._track_var_to_func[track_var]['finalize'](var=track_var)",
  "class XhatXbar(mpisppy.extensions.xhatbase.XhatBase):\n    \"\"\"\n    Args:\n        spo (SPOpt object): the calling object\n    \"\"\"\n    def __init__(self, spo):\n        super().__init__(spo)\n        self.options = spo.options[\"xhat_xbar_options\"]\n        self.solver_options = self.options[\"xhat_solver_options\"]\n        self.keep_solution = True\n        if ('keep_solution' in self.options) and (not self.options['keep_solution']):\n            self.keep_solution = False\n\n    def _fix_nonants_xhat(self):\n        \"\"\" Fix the Vars subject to non-anticipativity at given values.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n        Args:\n            cache (ndn dict of list or numpy vector): values at which to fix\n        WARNING: \n            We are counting on Pyomo indices not to change order between\n            when the cache_list is created and used.\n        NOTE:\n            You probably want to call _save_nonants right before calling this\n            copy/pasted from phabse _fix_nonants\n        \"\"\"\n        for k,s in self.opt.local_scenarios.items():\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                for i in range(nlens[ndn]): \n                    this_vardata = node.nonant_vardata_list[i]\n                    if this_vardata.is_integer() or this_vardata.is_binary():\n                        this_vardata._value = round(s._mpisppy_model.xbars[(ndn,i)]._value)\n                    else:\n                        this_vardata._value = s._mpisppy_model.xbars[(ndn,i)]._value\n\n                    this_vardata.fix()\n                    if persistent_solver is not None:\n                        persistent_solver.update_var(this_vardata)\n\n            \n    #==========\n    def xhat_tryit(self,\n                   verbose=False,\n                   restore_nonants=True):\n        \"\"\"Use xbar to set the nonants; round integers\n\n        Args:\n            verbose (boolean): controls debugging output\n            restore_nonants (boolean): put back the nonants\n        Returns:\n            xhatobjective (float or None): the objective function\n                or None if one could not be obtained.\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"  xhat_xbar: \" + msg)\n\n        obj = None\n        sname = None\n\n        _vb(\"Enter XhatXbar.xhat_tryit\")\n\n        _vb(\"   Solver options=\"+str(self.solver_options))\n\n        # This might be an extension for a Xhat_Eval object or a PH object, so we will assume the worst\n        phbase._Compute_Xbar(self.opt)\n        self._fix_nonants_xhat() # (BTW: for all local scenarios)\n\n        # NOTE: for APH we may need disable_pyomo_signal_handling\n        self.opt.solve_loop(solver_options=self.solver_options,\n                           #dis_W=True, dis_prox=True,\n                           verbose=verbose,\n                            tee=False)\n\n        infeasP = self.opt.infeas_prob()\n        if infeasP != 0.:\n            # restoring does no harm\n            # if this solution is infeasible\n            self.opt._restore_nonants()\n            return None\n        else:\n            if verbose and src_rank == self.cylinder_rank:\n                print(\"   Feasible xhat found at xbar\")\n            obj = self.opt.Eobjective(verbose=verbose)\n            if restore_nonants:\n                self._restore_nonants()\n\n        if obj is None:\n            _vb(\"Infeasible\")\n        else:\n            _vb(\"Feasible, returning \" + str(obj))\n\n        return obj\n\n    def pre_iter0(self):\n        pass\n\n    def post_iter0(self):\n        # a little bit silly\n        self.comms = self.opt.comms\n        \n    def miditer(self):\n        pass\n\n    def enditer(self):\n        pass\n\n    def post_everything(self):\n        # if we're keeping the solution, we *do not* restore the nonants\n        restore_nonants = not self.keep_solution\n        self.opt.disable_W_and_prox()\n        obj = self.xhat_tryit(verbose=self.verbose,\n                              restore_nonants=restore_nonants)\n        self.opt.reenable_W_and_prox()\n        # to make available to tester\n        self._xhat_xbar_obj_final = obj",
  "def __init__(self, spo):\n        super().__init__(spo)\n        self.options = spo.options[\"xhat_xbar_options\"]\n        self.solver_options = self.options[\"xhat_solver_options\"]\n        self.keep_solution = True\n        if ('keep_solution' in self.options) and (not self.options['keep_solution']):\n            self.keep_solution = False",
  "def _fix_nonants_xhat(self):\n        \"\"\" Fix the Vars subject to non-anticipativity at given values.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n        Args:\n            cache (ndn dict of list or numpy vector): values at which to fix\n        WARNING: \n            We are counting on Pyomo indices not to change order between\n            when the cache_list is created and used.\n        NOTE:\n            You probably want to call _save_nonants right before calling this\n            copy/pasted from phabse _fix_nonants\n        \"\"\"\n        for k,s in self.opt.local_scenarios.items():\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                for i in range(nlens[ndn]): \n                    this_vardata = node.nonant_vardata_list[i]\n                    if this_vardata.is_integer() or this_vardata.is_binary():\n                        this_vardata._value = round(s._mpisppy_model.xbars[(ndn,i)]._value)\n                    else:\n                        this_vardata._value = s._mpisppy_model.xbars[(ndn,i)]._value\n\n                    this_vardata.fix()\n                    if persistent_solver is not None:\n                        persistent_solver.update_var(this_vardata)",
  "def xhat_tryit(self,\n                   verbose=False,\n                   restore_nonants=True):\n        \"\"\"Use xbar to set the nonants; round integers\n\n        Args:\n            verbose (boolean): controls debugging output\n            restore_nonants (boolean): put back the nonants\n        Returns:\n            xhatobjective (float or None): the objective function\n                or None if one could not be obtained.\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"  xhat_xbar: \" + msg)\n\n        obj = None\n        sname = None\n\n        _vb(\"Enter XhatXbar.xhat_tryit\")\n\n        _vb(\"   Solver options=\"+str(self.solver_options))\n\n        # This might be an extension for a Xhat_Eval object or a PH object, so we will assume the worst\n        phbase._Compute_Xbar(self.opt)\n        self._fix_nonants_xhat() # (BTW: for all local scenarios)\n\n        # NOTE: for APH we may need disable_pyomo_signal_handling\n        self.opt.solve_loop(solver_options=self.solver_options,\n                           #dis_W=True, dis_prox=True,\n                           verbose=verbose,\n                            tee=False)\n\n        infeasP = self.opt.infeas_prob()\n        if infeasP != 0.:\n            # restoring does no harm\n            # if this solution is infeasible\n            self.opt._restore_nonants()\n            return None\n        else:\n            if verbose and src_rank == self.cylinder_rank:\n                print(\"   Feasible xhat found at xbar\")\n            obj = self.opt.Eobjective(verbose=verbose)\n            if restore_nonants:\n                self._restore_nonants()\n\n        if obj is None:\n            _vb(\"Infeasible\")\n        else:\n            _vb(\"Feasible, returning \" + str(obj))\n\n        return obj",
  "def pre_iter0(self):\n        pass",
  "def post_iter0(self):\n        # a little bit silly\n        self.comms = self.opt.comms",
  "def miditer(self):\n        pass",
  "def enditer(self):\n        pass",
  "def post_everything(self):\n        # if we're keeping the solution, we *do not* restore the nonants\n        restore_nonants = not self.keep_solution\n        self.opt.disable_W_and_prox()\n        obj = self.xhat_tryit(verbose=self.verbose,\n                              restore_nonants=restore_nonants)\n        self.opt.reenable_W_and_prox()\n        # to make available to tester\n        self._xhat_xbar_obj_final = obj",
  "def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"  xhat_xbar: \" + msg)",
  "class MinMaxAvg(mpisppy.extensions.xhatbase.XhatBase):\n    \"\"\"\n    Args:\n        ph (PH object): the calling object\n        rank (int): mpi process rank of currently running process\n    \"\"\"\n    def __init__(self, ph, rank, n_proc):\n        super().__init__(ph, rank, n_proc)\n        self.compstr = self.ph.options[\"avgminmax_name\"]\n\n    def pre_iter0(self):\n        return\n\n    def post_iter0(self):\n        avgv, minv, maxv = self.ph.avg_min_max(self.compstr)\n        if (self.cylinder_rank == 0):\n            print (\"  ### \", self.compstr,\": avg, min, max, max-min\", avgv, minv, maxv, maxv-minv)\n        \n    def miditer(self, PHIter, conv):\n        return\n\n    def enditer(self, PHIter):\n        avgv, minv, maxv = self.ph.avg_min_max(self.compstr)\n        if (self.cylinder_rank == 0):\n            print (\"  ### \", self.compstr,\": avg, min, max, max-min\", avgv, minv, maxv, maxv-minv)\n\n    def post_everything(self, PHIter, conv):\n        return",
  "def __init__(self, ph, rank, n_proc):\n        super().__init__(ph, rank, n_proc)\n        self.compstr = self.ph.options[\"avgminmax_name\"]",
  "def pre_iter0(self):\n        return",
  "def post_iter0(self):\n        avgv, minv, maxv = self.ph.avg_min_max(self.compstr)\n        if (self.cylinder_rank == 0):\n            print (\"  ### \", self.compstr,\": avg, min, max, max-min\", avgv, minv, maxv, maxv-minv)",
  "def miditer(self, PHIter, conv):\n        return",
  "def enditer(self, PHIter):\n        avgv, minv, maxv = self.ph.avg_min_max(self.compstr)\n        if (self.cylinder_rank == 0):\n            print (\"  ### \", self.compstr,\": avg, min, max, max-min\", avgv, minv, maxv, maxv-minv)",
  "def post_everything(self, PHIter, conv):\n        return",
  "class Diagnoser(mpisppy.extensions.xhatbase.XhatBase):\n    \"\"\"\n    Args:\n        ph (PH object): the calling object\n        rank (int): mpi process rank of currently running process\n    \"\"\"\n    def __init__(self, ph):\n        dirname = ph.options[\"diagnoser_options\"][\"diagnoser_outdir\"]\n        if os.path.exists(dirname):\n            if ph.cylinder_rank == 0:\n                print (\"Shutting down because Diagnostic directory exists:\",\n                       dirname)\n            quit()\n        if ph.cylinder_rank == 0:\n            os.mkdir(dirname) # just let it crash\n\n        super().__init__(ph)\n        self.options = self.ph.options[\"diagnoser_options\"]\n        self.dirname = self.options[\"diagnoser_outdir\"]\n\n    def write_loop(self):\n        \"\"\" Bundles are special. Also: this code needs help\n        from the ph object to be more efficient...\n        \"\"\"\n        for sname, s in self.ph.local_scenarios.items():\n            bundling = self.ph.bundling\n            fname = self.dirname+os.sep+sname+\".dag\"\n            with open(fname, \"a\") as f:\n                f.write(str(self.ph._PHIter)+\",\")\n                if not bundling:\n                    objfct = find_active_objective(s)\n                    f.write(str(pyo.value(objfct)))\n                else:\n                    f.write(\"Bundling\"+\",\")\n                    f.write(str(pyo.value(self.ph.saved_objs[sname])))\n                f.write(\"\\n\")\n\n    def pre_iter0(self):\n        return\n\n    def post_iter0(self):\n        for sname, s in self.ph.local_scenarios.items():\n            fname = self.dirname+os.sep+sname+\".dag\"\n            with open(fname, \"w\") as f:\n                f.write(str(dt.datetime.now())+\", diagnoser\\n\")\n\n        self.write_loop()\n        \n    def miditer(self, PHIter, conv):\n        return\n\n    def enditer(self, PHIter):\n        self.write_loop()\n\n    def post_everything(self, PHIter, conv):\n        return",
  "def __init__(self, ph):\n        dirname = ph.options[\"diagnoser_options\"][\"diagnoser_outdir\"]\n        if os.path.exists(dirname):\n            if ph.cylinder_rank == 0:\n                print (\"Shutting down because Diagnostic directory exists:\",\n                       dirname)\n            quit()\n        if ph.cylinder_rank == 0:\n            os.mkdir(dirname) # just let it crash\n\n        super().__init__(ph)\n        self.options = self.ph.options[\"diagnoser_options\"]\n        self.dirname = self.options[\"diagnoser_outdir\"]",
  "def write_loop(self):\n        \"\"\" Bundles are special. Also: this code needs help\n        from the ph object to be more efficient...\n        \"\"\"\n        for sname, s in self.ph.local_scenarios.items():\n            bundling = self.ph.bundling\n            fname = self.dirname+os.sep+sname+\".dag\"\n            with open(fname, \"a\") as f:\n                f.write(str(self.ph._PHIter)+\",\")\n                if not bundling:\n                    objfct = find_active_objective(s)\n                    f.write(str(pyo.value(objfct)))\n                else:\n                    f.write(\"Bundling\"+\",\")\n                    f.write(str(pyo.value(self.ph.saved_objs[sname])))\n                f.write(\"\\n\")",
  "def pre_iter0(self):\n        return",
  "def post_iter0(self):\n        for sname, s in self.ph.local_scenarios.items():\n            fname = self.dirname+os.sep+sname+\".dag\"\n            with open(fname, \"w\") as f:\n                f.write(str(dt.datetime.now())+\", diagnoser\\n\")\n\n        self.write_loop()",
  "def miditer(self, PHIter, conv):\n        return",
  "def enditer(self, PHIter):\n        self.write_loop()",
  "def post_everything(self, PHIter, conv):\n        return",
  "class NormRhoUpdater(mpisppy.extensions.extension.Extension):\n\n    def __init__(self, ph):\n\n        self.ph = ph\n        self.norm_rho_options = \\\n            ph.options['norm_rho_options'] if 'norm_rho_options' in ph.options else dict()\n\n        self._set_options()\n        self._prev_avg = {}\n        self.ph._mpisppy_norm_rho_update_inuse = True  # allow NormRhoConverger\n        \n    def _set_options(self):\n        options = self.norm_rho_options\n        for attr_name, opt_name in _attr_to_option_name_map.items():\n            setattr(self, attr_name, options[opt_name] if opt_name in options else _norm_rho_defaults[opt_name])\n\n    def _snapshot_avg(self, ph):\n        for s in ph.local_scenarios.values():\n            for ndn_i, xbar in s._mpisppy_model.xbars.items():\n                self._prev_avg[ndn_i] = xbar.value\n\n    def _compute_primal_residual_norm(self, ph):\n\n        local_nodenames = []\n        local_primal_residuals = {}\n        global_primal_residuals = {}\n\n        for k,s in ph.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens        \n            for node in s._mpisppy_node_list:\n                if node.name not in local_nodenames:\n\n                    ndn = node.name\n                    local_nodenames.append(ndn)\n                    nlen = nlens[ndn]\n\n                    local_primal_residuals[ndn] = np.zeros(nlen, dtype='d')\n                    global_primal_residuals[ndn] = np.zeros(nlen, dtype='d')\n\n        for k,s in ph.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            xbars = s._mpisppy_model.xbars\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                primal_residuals = local_primal_residuals[ndn]\n\n                unweighted_primal_residuals = \\\n                        np.fromiter((abs(v._value - xbars[ndn,i]._value) for i,v in enumerate(node.nonant_vardata_list)),\n                                    dtype='d', count=nlens[ndn] )\n                primal_residuals += s._mpisppy_probability * unweighted_primal_residuals\n\n        for nodename in local_nodenames:\n            ph.comms[nodename].Allreduce(\n                [local_primal_residuals[nodename], MPI.DOUBLE],\n                [global_primal_residuals[nodename], MPI.DOUBLE],\n                op=MPI.SUM)\n\n        primal_resid = {}\n        for ndn, global_primal_resid in global_primal_residuals.items():\n            for i, v in enumerate(global_primal_resid):\n                primal_resid[ndn,i] = v\n\n        return primal_resid\n\n    def _compute_dual_residual_norm(self, ph):\n        dual_resid = {}\n        for s in ph.local_scenarios.values():\n            for ndn_i in s._mpisppy_data.nonant_indices:\n                dual_resid[ndn_i] = s._mpisppy_model.rho[ndn_i].value * \\\n                        math.fabs( s._mpisppy_model.xbars[ndn_i].value - self._prev_avg[ndn_i] )\n        return dual_resid\n\n    def pre_iter0(self):\n        pass\n\n    def post_iter0(self):\n        pass\n\n    def miditer(self):\n\n        ph = self.ph\n        ph_iter = ph._PHIter\n        if self._stop_iter_rho_update is not None and \\\n                (ph_iter > self._stop_iter_rho_update):\n            return\n        if not self._prev_avg:\n            self._snapshot_avg(ph)\n        else:\n            primal_residuals = self._compute_primal_residual_norm(ph)\n            dual_residuals = self._compute_dual_residual_norm(ph)\n            self._snapshot_avg(ph)\n            primal_dual_difference_factor = self._primal_dual_difference_factor\n            first = True\n            first_scenario = True\n            for s in ph.local_scenarios.values():\n                for ndn_i, rho in s._mpisppy_model.rho.items():\n                    primal_resid = primal_residuals[ndn_i]\n                    dual_resid = dual_residuals[ndn_i]\n\n                    action = None\n                    if (primal_resid > primal_dual_difference_factor*dual_resid) and (primal_resid > self._tol):\n                        rho._value *= self._rho_increase\n                        action = \"Increasing\"\n                    elif (dual_resid > primal_dual_difference_factor*primal_resid) and (dual_resid > self._tol):\n                        if ph_iter >= self._required_converged_before_decrease:\n                            rho._value /= self._rho_decrease\n                            action = \"Decreasing\"\n                    elif (primal_resid < self._tol) and (dual_resid < self._tol):\n                        rho._value /= self._rho_converged_residual_decrease\n                        action = \"Converged, Decreasing\"\n                    if self._verbose and ph.cylinder_rank == 0 and action is not None:\n                        if first:\n                            first = False\n                            first_line = (\"Updating rho values:\\n%21s %40s %16s %16s %16s\"\n                                          % (\"Action\",\n                                             \"Variable\",\n                                             \"Primal Residual\",\n                                             \"Dual Residual\",\n                                             \"New Rho\"))\n                            print(first_line)\n                        if first_scenario:\n                            print(\"%21s %40s %16g %16g %16g\"\n                                  % (action, s._mpisppy_data.nonant_indices[ndn_i].name,\n                                     primal_resid, dual_resid, rho.value))\n                first_scenario = False\n\n    def enditer(self):\n        pass\n\n    def post_everything(self):\n        pass",
  "def __init__(self, ph):\n\n        self.ph = ph\n        self.norm_rho_options = \\\n            ph.options['norm_rho_options'] if 'norm_rho_options' in ph.options else dict()\n\n        self._set_options()\n        self._prev_avg = {}\n        self.ph._mpisppy_norm_rho_update_inuse = True",
  "def _set_options(self):\n        options = self.norm_rho_options\n        for attr_name, opt_name in _attr_to_option_name_map.items():\n            setattr(self, attr_name, options[opt_name] if opt_name in options else _norm_rho_defaults[opt_name])",
  "def _snapshot_avg(self, ph):\n        for s in ph.local_scenarios.values():\n            for ndn_i, xbar in s._mpisppy_model.xbars.items():\n                self._prev_avg[ndn_i] = xbar.value",
  "def _compute_primal_residual_norm(self, ph):\n\n        local_nodenames = []\n        local_primal_residuals = {}\n        global_primal_residuals = {}\n\n        for k,s in ph.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens        \n            for node in s._mpisppy_node_list:\n                if node.name not in local_nodenames:\n\n                    ndn = node.name\n                    local_nodenames.append(ndn)\n                    nlen = nlens[ndn]\n\n                    local_primal_residuals[ndn] = np.zeros(nlen, dtype='d')\n                    global_primal_residuals[ndn] = np.zeros(nlen, dtype='d')\n\n        for k,s in ph.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            xbars = s._mpisppy_model.xbars\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                primal_residuals = local_primal_residuals[ndn]\n\n                unweighted_primal_residuals = \\\n                        np.fromiter((abs(v._value - xbars[ndn,i]._value) for i,v in enumerate(node.nonant_vardata_list)),\n                                    dtype='d', count=nlens[ndn] )\n                primal_residuals += s._mpisppy_probability * unweighted_primal_residuals\n\n        for nodename in local_nodenames:\n            ph.comms[nodename].Allreduce(\n                [local_primal_residuals[nodename], MPI.DOUBLE],\n                [global_primal_residuals[nodename], MPI.DOUBLE],\n                op=MPI.SUM)\n\n        primal_resid = {}\n        for ndn, global_primal_resid in global_primal_residuals.items():\n            for i, v in enumerate(global_primal_resid):\n                primal_resid[ndn,i] = v\n\n        return primal_resid",
  "def _compute_dual_residual_norm(self, ph):\n        dual_resid = {}\n        for s in ph.local_scenarios.values():\n            for ndn_i in s._mpisppy_data.nonant_indices:\n                dual_resid[ndn_i] = s._mpisppy_model.rho[ndn_i].value * \\\n                        math.fabs( s._mpisppy_model.xbars[ndn_i].value - self._prev_avg[ndn_i] )\n        return dual_resid",
  "def pre_iter0(self):\n        pass",
  "def post_iter0(self):\n        pass",
  "def miditer(self):\n\n        ph = self.ph\n        ph_iter = ph._PHIter\n        if self._stop_iter_rho_update is not None and \\\n                (ph_iter > self._stop_iter_rho_update):\n            return\n        if not self._prev_avg:\n            self._snapshot_avg(ph)\n        else:\n            primal_residuals = self._compute_primal_residual_norm(ph)\n            dual_residuals = self._compute_dual_residual_norm(ph)\n            self._snapshot_avg(ph)\n            primal_dual_difference_factor = self._primal_dual_difference_factor\n            first = True\n            first_scenario = True\n            for s in ph.local_scenarios.values():\n                for ndn_i, rho in s._mpisppy_model.rho.items():\n                    primal_resid = primal_residuals[ndn_i]\n                    dual_resid = dual_residuals[ndn_i]\n\n                    action = None\n                    if (primal_resid > primal_dual_difference_factor*dual_resid) and (primal_resid > self._tol):\n                        rho._value *= self._rho_increase\n                        action = \"Increasing\"\n                    elif (dual_resid > primal_dual_difference_factor*primal_resid) and (dual_resid > self._tol):\n                        if ph_iter >= self._required_converged_before_decrease:\n                            rho._value /= self._rho_decrease\n                            action = \"Decreasing\"\n                    elif (primal_resid < self._tol) and (dual_resid < self._tol):\n                        rho._value /= self._rho_converged_residual_decrease\n                        action = \"Converged, Decreasing\"\n                    if self._verbose and ph.cylinder_rank == 0 and action is not None:\n                        if first:\n                            first = False\n                            first_line = (\"Updating rho values:\\n%21s %40s %16s %16s %16s\"\n                                          % (\"Action\",\n                                             \"Variable\",\n                                             \"Primal Residual\",\n                                             \"Dual Residual\",\n                                             \"New Rho\"))\n                            print(first_line)\n                        if first_scenario:\n                            print(\"%21s %40s %16g %16g %16g\"\n                                  % (action, s._mpisppy_data.nonant_indices[ndn_i].name,\n                                     primal_resid, dual_resid, rho.value))\n                first_scenario = False",
  "def enditer(self):\n        pass",
  "def post_everything(self):\n        pass",
  "class XhatClosest(mpisppy.extensions.xhatbase.XhatBase):\n    \"\"\"\n    Args:\n        rank (int): mpi process rank of currently running process\n    \"\"\"\n    def __init__(self, ph):\n        super().__init__(ph)\n        self.options = ph.options[\"xhat_closest_options\"]\n        self.solver_options = self.options[\"xhat_solver_options\"]\n        self.keep_solution = True\n        if ('keep_solution' in self.options) and (not self.options['keep_solution']):\n            self.keep_solution = False\n\n    def xhat_closest_to_xbar(self, verbose=False, restore_nonants=True):\n        \"\"\" Get a truncated z score and look for the closest overall.\n\n        Returns:\n            obj (float or None): objective value, none if infeasible.\n            snamedict (str): closest scenarios\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) xhat_looper: \" + msg)\n\n        localmindist = np.zeros(1, dtype='d')\n        globalmindist = np.zeros(1, dtype='d')\n        localwinnername = None\n        for k, s in self.opt.local_scenarios.items():\n            dist = 0\n            for ndn_i, xvar in s._mpisppy_data.nonant_indices.items():\n                diff = pyo.value(xvar) - pyo.value(s._mpisppy_model.xbars[ndn_i])\n                variance = pyo.value(s._mpisppy_model.xsqbars[ndn_i]) \\\n                  - pyo.value(s._mpisppy_model.xbars[ndn_i])*pyo.value(s._mpisppy_model.xbars[ndn_i])\n                if variance > 0:\n                    stdev = np.sqrt(variance)\n                    dist += min(3, abs(diff)/stdev)\n            if localwinnername is None:\n                localmindist[0] = dist\n                localwinnername = k\n            elif dist < localmindist[0]:\n                localmindist[0] = dist\n                localwinnername = k\n\n        self.comms[\"ROOT\"].Allreduce([localmindist, mpi.DOUBLE],\n                                     [globalmindist, mpi.DOUBLE],\n                                     op=mpi.MIN)\n        # ties are possible, so break the tie\n        localwinrank = np.zeros(1, dtype='d')  # could use a python variable.\n        globalwinrank = np.zeros(1, dtype='d')\n        if globalmindist[0] < localmindist[0]:\n            localwinrank[0] = -1  # we lost\n        else:\n            localwinrank[0] = self.cylinder_rank\n        self.comms[\"ROOT\"].Allreduce([localwinrank, mpi.DOUBLE],\n                                     [globalwinrank, mpi.DOUBLE],\n                                     op=mpi.MAX)\n\n        # We only used the rank to break a possible tie.\n        if self.cylinder_rank == int(globalwinrank[0]):\n            globalwinnername = localwinnername\n        else:\n            globalwinnername = None\n\n        sroot = globalwinrank[0]\n\n        sname = self.comms[\"ROOT\"].bcast(globalwinnername, root=sroot)\n        _vb(\"Trying scenario \"+sname)\n        _vb(\"   Solver options=\"+str(self.solver_options))\n        # xxx TBD mult-stage\n        snamedict = {\"ROOT\": sname}\n        obj = self._try_one(snamedict,\n                            solver_options=self.solver_options,\n                            verbose=False,\n                            restore_nonants=restore_nonants)\n        if obj is None:\n            _vb(\"    Infeasible\")\n        else:\n            _vb(\"    Feasible, returning \" + str(obj))\n            \n        return obj, snamedict\n        \n    def pre_iter0(self):\n        if self.opt.multistage:\n            raise RuntimeError(\"xhatclosest not done for multi-stage\")\n\n    def post_iter0(self):\n        # a little bit silly\n        self.comms = self.opt.comms\n        \n    def miditer(self):\n        pass\n\n    def enditer(self):\n        pass\n\n    def post_everything(self):\n        # if we're keeping the solution, we *do not* restore the nonants\n        restore_nonants = not self.keep_solution\n        self.opt.disable_W_and_prox()\n        obj, srcsname = self.xhat_closest_to_xbar(verbose=self.verbose, restore_nonants=restore_nonants)\n        self.opt.reenable_W_and_prox()\n        self.xhat_common_post_everything(\"closest to xbar\", obj, srcsname, restore_nonants)\n        self._final_xhat_closest_obj = obj",
  "def __init__(self, ph):\n        super().__init__(ph)\n        self.options = ph.options[\"xhat_closest_options\"]\n        self.solver_options = self.options[\"xhat_solver_options\"]\n        self.keep_solution = True\n        if ('keep_solution' in self.options) and (not self.options['keep_solution']):\n            self.keep_solution = False",
  "def xhat_closest_to_xbar(self, verbose=False, restore_nonants=True):\n        \"\"\" Get a truncated z score and look for the closest overall.\n\n        Returns:\n            obj (float or None): objective value, none if infeasible.\n            snamedict (str): closest scenarios\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) xhat_looper: \" + msg)\n\n        localmindist = np.zeros(1, dtype='d')\n        globalmindist = np.zeros(1, dtype='d')\n        localwinnername = None\n        for k, s in self.opt.local_scenarios.items():\n            dist = 0\n            for ndn_i, xvar in s._mpisppy_data.nonant_indices.items():\n                diff = pyo.value(xvar) - pyo.value(s._mpisppy_model.xbars[ndn_i])\n                variance = pyo.value(s._mpisppy_model.xsqbars[ndn_i]) \\\n                  - pyo.value(s._mpisppy_model.xbars[ndn_i])*pyo.value(s._mpisppy_model.xbars[ndn_i])\n                if variance > 0:\n                    stdev = np.sqrt(variance)\n                    dist += min(3, abs(diff)/stdev)\n            if localwinnername is None:\n                localmindist[0] = dist\n                localwinnername = k\n            elif dist < localmindist[0]:\n                localmindist[0] = dist\n                localwinnername = k\n\n        self.comms[\"ROOT\"].Allreduce([localmindist, mpi.DOUBLE],\n                                     [globalmindist, mpi.DOUBLE],\n                                     op=mpi.MIN)\n        # ties are possible, so break the tie\n        localwinrank = np.zeros(1, dtype='d')  # could use a python variable.\n        globalwinrank = np.zeros(1, dtype='d')\n        if globalmindist[0] < localmindist[0]:\n            localwinrank[0] = -1  # we lost\n        else:\n            localwinrank[0] = self.cylinder_rank\n        self.comms[\"ROOT\"].Allreduce([localwinrank, mpi.DOUBLE],\n                                     [globalwinrank, mpi.DOUBLE],\n                                     op=mpi.MAX)\n\n        # We only used the rank to break a possible tie.\n        if self.cylinder_rank == int(globalwinrank[0]):\n            globalwinnername = localwinnername\n        else:\n            globalwinnername = None\n\n        sroot = globalwinrank[0]\n\n        sname = self.comms[\"ROOT\"].bcast(globalwinnername, root=sroot)\n        _vb(\"Trying scenario \"+sname)\n        _vb(\"   Solver options=\"+str(self.solver_options))\n        # xxx TBD mult-stage\n        snamedict = {\"ROOT\": sname}\n        obj = self._try_one(snamedict,\n                            solver_options=self.solver_options,\n                            verbose=False,\n                            restore_nonants=restore_nonants)\n        if obj is None:\n            _vb(\"    Infeasible\")\n        else:\n            _vb(\"    Feasible, returning \" + str(obj))\n            \n        return obj, snamedict",
  "def pre_iter0(self):\n        if self.opt.multistage:\n            raise RuntimeError(\"xhatclosest not done for multi-stage\")",
  "def post_iter0(self):\n        # a little bit silly\n        self.comms = self.opt.comms",
  "def miditer(self):\n        pass",
  "def enditer(self):\n        pass",
  "def post_everything(self):\n        # if we're keeping the solution, we *do not* restore the nonants\n        restore_nonants = not self.keep_solution\n        self.opt.disable_W_and_prox()\n        obj, srcsname = self.xhat_closest_to_xbar(verbose=self.verbose, restore_nonants=restore_nonants)\n        self.opt.reenable_W_and_prox()\n        self.xhat_common_post_everything(\"closest to xbar\", obj, srcsname, restore_nonants)\n        self._final_xhat_closest_obj = obj",
  "def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) xhat_looper: \" + msg)",
  "class XhatBase(mpisppy.extensions.extension.Extension):\n    \"\"\"\n        Any inherited class must implement the preiter0, postiter etc. methods\n        \n        Args:\n            opt (SPOpt object): gives the problem that we bound\n\n        Attributes:\n          scenario_name_to_rank (dict of dict): nodes (i.e. comms) scen names\n                keys are comms (i.e., tree nodes); values are dicts with keys\n                that are scenario names and values that are ranks\n    \"\"\"\n    def __init__(self, opt):\n        super().__init__(opt)\n        self.cylinder_rank = self.opt.cylinder_rank\n        self.n_proc = self.opt.n_proc\n        self.verbose = self.opt.options[\"verbose\"]\n\n        scen_count = len(opt.all_scenario_names)\n\n        self.scenario_name_to_rank = opt.scenario_names_to_rank\n        # dict: scenario names --> LOCAL rank number (needed mainly for xhat)\n        \n     #**********\n    def _try_one(self, snamedict, solver_options=None, verbose=False,\n                 restore_nonants=True, stage2EFsolvern=None, branching_factors=None):\n        \"\"\" try the scenario named sname in self.opt.local_scenarios\n       Args:\n            snamedict (dict): key: scenario tree non-leaf name, val: scen name\n                            the scen name can be None if it is not local\n            solver_options (dict): passed through to the solver\n            verbose (boolean): controls output\n            restore_nonants (bool): if True, restores the nonants to their original\n                                    values in all scenarios. If False, leaves the\n                                    nonants as they are in the tried scenario\n            stage2EFsolvern: use this for EFs based on second stage nodes for multi-stage\n            branching_factors (list): list of branching factors for stage2ef\n       NOTE: The solve loop is with fixed nonants so W and rho do not\n             matter to the optimization. When we want to check the obj\n             value we need to drop them, but we don't need to re-optimize\n             since all other Vars are optimized already with the nonants\n             fixed.\n        Returns:\n             obj (float or None): the expected value for sname as xhat or None\n        NOTE:\n             the stage2ef stuff is a little bit hacked-in\n        \"\"\"\n        xhats = dict()  # to pass to _fix_nonants\n        self.opt._save_nonants()  # (BTW: for all local scenarios)\n\n        # Special Tee option for xhat\n        sopt = solver_options\n        Tee=False\n        if solver_options is not None and \"Tee\" in solver_options:\n            sopt = dict(solver_options)\n            Tee = sopt[\"Tee\"]\n            del sopt[\"Tee\"]\n        \n        # For now, we are going to treat two-stage as a special case\n        if len(snamedict) == 1:\n            sname = snamedict[\"ROOT\"]  # also serves as an assert\n            if sname in self.opt.local_scenarios:\n                xhat = self.opt.local_scenarios[sname]._mpisppy_data.nonant_cache\n            else:\n                xhat = None\n            src_rank = self.scenario_name_to_rank[\"ROOT\"][sname]\n            try:\n                xhats[\"ROOT\"] = self.comms[\"ROOT\"].bcast(xhat, root=src_rank)\n            except:\n                print(\"rank=\",self.cylinder_rank, \"xhats bcast failed on src_rank={}\"\\\n                      .format(src_rank))\n                print(\"root comm size={}\".format(self.comms[\"ROOT\"].size))\n                raise\n        elif stage2EFsolvern is None:  # regular multi-stage\n            # assemble parts and put it in xhats\n            # send to ranks in the comm or receive ANY_SOURCE\n            # (for closest do allreduce with loc operator) rank\n            nlens = dict()\n            cistart = dict()  # ci start for each local node\n            for k, s in self.opt.local_scenarios.items():\n                for nnode in s._mpisppy_node_list:\n                    ndn = nnode.name\n                    if ndn not in cistart:\n                        # NOTE: _mpisppy_data.cistart is defined in SPBase._attach_nlens()\n                        #       and only used here\n                        cistart[ndn] = s._mpisppy_data.cistart[ndn]\n                    if ndn not in nlens:\n                        nlens[ndn] = s._mpisppy_data.nlens[ndn]\n                    if ndn not in xhats:\n                        xhats[ndn] = None\n                    if ndn not in snamedict:\n                        raise RuntimeError(f\"{ndn} not in snamedict={snamedict}\")\n                    if snamedict[ndn] == k:\n                        # cache lists are just concated node lists\n                        xhats[ndn] = [s._mpisppy_data.nonant_cache[i+cistart[ndn]]\n                                      for i in range(nlens[ndn])]\n            for ndn in cistart:  # local nodes\n                if snamedict[ndn] not in self.scenario_name_to_rank[ndn]:\n                    print (f\"For ndn={ndn}, snamedict[ndn] not in \"\n                           \"self.scenario_name_to_rank[ndn]\")\n                    print(f\"snamedict[ndn]={snamedict[ndn]}\")\n                    print(f\"self.scenario_name_to_rank[ndn]={self.scenario_name_to_rank[ndn]}\")\n                    raise RuntimeError(\"Bad scenario selection for xhat\")\n                src_rank = self.scenario_name_to_rank[ndn][snamedict[ndn]]\n                try:\n                    xhats[ndn] = self.comms[ndn].bcast(xhats[ndn], root=src_rank)\n                except:\n                    print(\"rank=\",self.cylinder_rank, \"xhats bcast failed on ndn={}, src_rank={}\"\\\n                          .format(ndn,src_rank))\n                    raise\n        else:  # we are multi-stage with stage2ef\n            # Form an ef for all local scenarios and then fix the first stage\n            # vars based on the chosen scenario\n            # based strictly on the root node nonant.\n            sname = snamedict[\"ROOT\"]  # also serves as an assert\n            if sname in self.opt.local_scenarios:\n                rnlen = self.opt.local_scenarios[sname]._mpisppy_data.nlens[\"ROOT\"]\n                xhat = [self.opt.local_scenarios[sname]._mpisppy_data.nonant_cache[i] for i in range(rnlen)]\n            else:\n                xhat = None\n            src_rank = self.scenario_name_to_rank[\"ROOT\"][sname]\n            try:\n                xhats[\"ROOT\"] = self.comms[\"ROOT\"].bcast(xhat, root=src_rank)\n            except:\n                print(\"rank=\",self.cylinder_rank, \"xhats bcast failed on src_rank={}\"\\\n                      .format(src_rank))\n                print(\"root comm size={}\".format(self.comms[\"ROOT\"].size))\n                raise\n            # now form the EF for the appropriate number of second-stage scenario tree nodes\n            # Use the branching factors to figure out how many second-stage nodes.\n            stage2cnt = branching_factors[1]\n            rankcnt = self.n_proc\n            # The next assert is important.\n            assert stage2cnt % rankcnt== 0, \"for stage2ef, ranks must be a multiple of stage2 nodes\"            \n            nodes_per_rank = stage2cnt // rankcnt\n            # to keep the code easier to read, we will first collect the nodenames\n            # Important: we are assuming a standard node naming pattern using underscores.\n            # TBD: do something that would work with underscores *in* the stage branch name\n            # TBD: factor this\n            local_2ndns = dict()  # dict of dicts (inner dicts are for FormEF)\n            for k,s in self.opt.local_scenarios.items():\n                if hasattr(self, \"_EFs\"):\n                    sputils.deact_objs(s)  # create EF does this the first time\n                for node in s._mpisppy_node_list:\n                    ndn = node.name\n                    if len(re.findall(\"_\", ndn)) == 1:\n                        if ndn not in local_2ndns:\n                            local_2ndns[ndn] = {k: s}\n                        local_2ndns[ndn][k] = s\n            if len(local_2ndns) != nodes_per_rank:\n                print(\"stage2ef failure: nodes_per_rank=\",nodes_per_rank, \"local_2ndns=\",local_2ndns)\n                raise RuntimeError(\"stagecnt assumes regular scenario tree and standard _ node naming\")\n            # create an EF for each second stage node and solve with fixed nonant\n            # TBD: factor out the EF creation!\n            if not hasattr(self, \"_EFs\"):\n                for k,s in self.opt.local_scenarios.items():\n                    sputils.stash_ref_objs(s)\n                self._EFs = dict()\n                for ndn2, sdict in local_2ndns.items():  # ndn2 will be a the node name\n                    if ndn2 not in self._EFs:  # this will only happen once\n                        self._EFs[ndn2] = self.opt.FormEF(sdict, ndn2)  # spopt.py\n            # We have EFs so fix noants, solve, and etc.\n            for ndn2, sdict in local_2ndns.items():  # ndn2 will be a the node name\n                wxbarutils.fix_ef_ROOT_nonants(self._EFs[ndn2], xhats[\"ROOT\"])\n                # solve EF\n                solver = pyo.SolverFactory(stage2EFsolvern)\n                if 'persistent' in stage2EFsolvern:\n                    solver.set_instance(self._EFs[ndn2], symbolic_solver_labels=True)\n                    results = solver.solve(tee=Tee)\n                else:\n                    results = solver.solve(self._EFs[ndn2], tee=Tee, symbolic_solver_labels=True,)\n\n                # restore objectives so Ebojective will work\n                for s in sdict.values():\n                    sputils.reactivate_objs(s)\n                # if you hit infeas, return None\n                if not pyo.check_optimal_termination(results):\n                   self.opt._restore_nonants()\n                   return None\n               \n            # feasible xhat found, so finish up 2EF part and return\n            if verbose and src_rank == self.cylinder_rank:\n                print(\"   Feasible xhat found:\")\n                self.opt.local_scenarios[sname].pprint()\n            # get the global obj\n            obj = self.opt.Eobjective(verbose=verbose)\n            if restore_nonants:\n                self.opt._restore_nonants()\n            return obj\n\n        # end of multi-stage with stage2ef\n\n        # Code from here down executes for 2-stage and regular multi-stage\n        # The save is done above\n        self.opt._fix_nonants(xhats)  # (BTW: for all local scenarios)\n\n        # NOTE: for APH we may need disable_pyomo_signal_handling\n        self.opt.solve_loop(solver_options=sopt,\n                           #dis_W=True, dis_prox=True,\n                           verbose=verbose,\n                           tee=Tee)\n\n        infeasP = self.opt.infeas_prob()\n        if infeasP != 0.:\n            # restoring does no harm\n            # if this solution is infeasible\n            self.opt._restore_nonants()\n            return None\n        else:\n            if verbose and src_rank == self.cylinder_rank:\n                print(\"   Feasible xhat found:\")\n                self.opt.local_scenarios[sname].pprint()\n            obj = self.opt.Eobjective(verbose=verbose)\n            if restore_nonants:\n                self.opt._restore_nonants()\n            return obj\n\n    #**********\n    def csv_nonants(self, snamedict, fname):\n        \"\"\" write the non-ants in csv format to files based on the file name\n            (we will over-write files if they already exists)\n            Args:\n               snamedic (str): the names of the scenarios to use\n               fname (str): the full name of the file to which to write\n        \"\"\"\n        # only the rank with the requested scenario writes\n        for ndn, sname in snamedict.items():\n            if sname not in self.opt.local_scenarios:\n                continue\n            scen = self.opt.local_scenarios[sname]\n            with open(fname+\"_\"+ndn+\"_\"+sname+\".csv\", \"w\") as f:\n                for node in scen._mpisppy_node_list:\n                    if node.name == ndn:\n                        break\n                nlens = scen._mpisppy_data.nlens\n                f.write(ndn)\n                for i in range(nlens[ndn]):\n                    vardata = node.nonant_vardata_list[i]\n                    f.write(', \"'+vardata.name+'\", '+str(vardata._value))\n                f.write(\"\\n\")\n\n    #**********\n    def csv_allvars(self, snamedict, fname):\n        \"\"\" write all Vars in csv format to files based on the file name\n            (we will over-write files if they already exists)\n            Args:\n               snamedict (dict): scenario names\n               fname (str): the full name of the file to which to write\n        \"\"\"\n        # only the rank with the requested scenario writes\n        for ndn, sname in snamedict.items():\n            if sname not in self.opt.local_scenarios:\n                continue\n            scen = self.opt.local_scenarios[sname]\n            for node in scen._mpisppy_node_list:\n                if node.name == ndn:\n                    break\n                with open(fname+\"_\"+ndn+\"_\"+sname,\"w\") as f:\n                    for ((v_name, v_index), v_data)\\\n                        in scen.component_data_iterindex(pyo.Var, active=True):\n                        f.write(v_name + \", \" + str(pyo.value(v_data)) + \"\\n\")\n\n    \"\"\" Functions to be called by xhat extensions. This is just code\n    common to all, or almost all xhat extensions. It happens to be the\n    case that whatever extobject that gets passed in will have been\n    derived from XhatBase, but that is not why the code is here. It was\n    factored simply for the usual reasons to factor code.  \"\"\"\n\n    def xhat_common_post_everything(self, extname, obj, snamedict, restored_nonants):\n        \"\"\" Code that most xhat post_everything routines will want to call.\n        Args:\n            extname (str): the name of the extension for reporting\n            obj (float): the xhat objective function\n            snamedict (dict): the (scenario) names upon which xhat is based\n            restored_nonants (bool): if the restore_nonants flag was True on the last\n                call to _try_one.\n        \"\"\"\n        if (obj is not None) and (not restored_nonants):\n            # a tree solution is available\n            self.opt.tree_solution_available = True\n            self.opt.first_stage_solution_available = True\n        if (obj is not None) and (self.opt.spcomm is not None):\n            self.opt.spcomm.BestInnerBound = self.opt.spcomm.InnerBoundUpdate(obj, char='E')\n        if self.cylinder_rank == 0 and self.verbose:\n            print (\"****\", extname ,\"Used scenarios\",\n                   str(snamedict),\"to get xhat Eobj=\",obj)\n\n        if \"csvname\" in self.options:\n            self.csv_nonants(snamedict, self.options[\"csvname\"])\n\n        if \"dump_prefix\" in self.options:\n            prefpref = self.options[\"dump_prefix\"]\n            pref = extname\n            self.csv_nonants(snamedict, prefpref + \"_nonant_\" + pref)\n            self.csv_allvars(snamedict, prefpref + \"_allvars_\" + pref)\n\n    def post_iter0(self):\n        # the base class needs this\n        self.comms = self.opt.comms",
  "def __init__(self, opt):\n        super().__init__(opt)\n        self.cylinder_rank = self.opt.cylinder_rank\n        self.n_proc = self.opt.n_proc\n        self.verbose = self.opt.options[\"verbose\"]\n\n        scen_count = len(opt.all_scenario_names)\n\n        self.scenario_name_to_rank = opt.scenario_names_to_rank",
  "def _try_one(self, snamedict, solver_options=None, verbose=False,\n                 restore_nonants=True, stage2EFsolvern=None, branching_factors=None):\n        \"\"\" try the scenario named sname in self.opt.local_scenarios\n       Args:\n            snamedict (dict): key: scenario tree non-leaf name, val: scen name\n                            the scen name can be None if it is not local\n            solver_options (dict): passed through to the solver\n            verbose (boolean): controls output\n            restore_nonants (bool): if True, restores the nonants to their original\n                                    values in all scenarios. If False, leaves the\n                                    nonants as they are in the tried scenario\n            stage2EFsolvern: use this for EFs based on second stage nodes for multi-stage\n            branching_factors (list): list of branching factors for stage2ef\n       NOTE: The solve loop is with fixed nonants so W and rho do not\n             matter to the optimization. When we want to check the obj\n             value we need to drop them, but we don't need to re-optimize\n             since all other Vars are optimized already with the nonants\n             fixed.\n        Returns:\n             obj (float or None): the expected value for sname as xhat or None\n        NOTE:\n             the stage2ef stuff is a little bit hacked-in\n        \"\"\"\n        xhats = dict()  # to pass to _fix_nonants\n        self.opt._save_nonants()  # (BTW: for all local scenarios)\n\n        # Special Tee option for xhat\n        sopt = solver_options\n        Tee=False\n        if solver_options is not None and \"Tee\" in solver_options:\n            sopt = dict(solver_options)\n            Tee = sopt[\"Tee\"]\n            del sopt[\"Tee\"]\n        \n        # For now, we are going to treat two-stage as a special case\n        if len(snamedict) == 1:\n            sname = snamedict[\"ROOT\"]  # also serves as an assert\n            if sname in self.opt.local_scenarios:\n                xhat = self.opt.local_scenarios[sname]._mpisppy_data.nonant_cache\n            else:\n                xhat = None\n            src_rank = self.scenario_name_to_rank[\"ROOT\"][sname]\n            try:\n                xhats[\"ROOT\"] = self.comms[\"ROOT\"].bcast(xhat, root=src_rank)\n            except:\n                print(\"rank=\",self.cylinder_rank, \"xhats bcast failed on src_rank={}\"\\\n                      .format(src_rank))\n                print(\"root comm size={}\".format(self.comms[\"ROOT\"].size))\n                raise\n        elif stage2EFsolvern is None:  # regular multi-stage\n            # assemble parts and put it in xhats\n            # send to ranks in the comm or receive ANY_SOURCE\n            # (for closest do allreduce with loc operator) rank\n            nlens = dict()\n            cistart = dict()  # ci start for each local node\n            for k, s in self.opt.local_scenarios.items():\n                for nnode in s._mpisppy_node_list:\n                    ndn = nnode.name\n                    if ndn not in cistart:\n                        # NOTE: _mpisppy_data.cistart is defined in SPBase._attach_nlens()\n                        #       and only used here\n                        cistart[ndn] = s._mpisppy_data.cistart[ndn]\n                    if ndn not in nlens:\n                        nlens[ndn] = s._mpisppy_data.nlens[ndn]\n                    if ndn not in xhats:\n                        xhats[ndn] = None\n                    if ndn not in snamedict:\n                        raise RuntimeError(f\"{ndn} not in snamedict={snamedict}\")\n                    if snamedict[ndn] == k:\n                        # cache lists are just concated node lists\n                        xhats[ndn] = [s._mpisppy_data.nonant_cache[i+cistart[ndn]]\n                                      for i in range(nlens[ndn])]\n            for ndn in cistart:  # local nodes\n                if snamedict[ndn] not in self.scenario_name_to_rank[ndn]:\n                    print (f\"For ndn={ndn}, snamedict[ndn] not in \"\n                           \"self.scenario_name_to_rank[ndn]\")\n                    print(f\"snamedict[ndn]={snamedict[ndn]}\")\n                    print(f\"self.scenario_name_to_rank[ndn]={self.scenario_name_to_rank[ndn]}\")\n                    raise RuntimeError(\"Bad scenario selection for xhat\")\n                src_rank = self.scenario_name_to_rank[ndn][snamedict[ndn]]\n                try:\n                    xhats[ndn] = self.comms[ndn].bcast(xhats[ndn], root=src_rank)\n                except:\n                    print(\"rank=\",self.cylinder_rank, \"xhats bcast failed on ndn={}, src_rank={}\"\\\n                          .format(ndn,src_rank))\n                    raise\n        else:  # we are multi-stage with stage2ef\n            # Form an ef for all local scenarios and then fix the first stage\n            # vars based on the chosen scenario\n            # based strictly on the root node nonant.\n            sname = snamedict[\"ROOT\"]  # also serves as an assert\n            if sname in self.opt.local_scenarios:\n                rnlen = self.opt.local_scenarios[sname]._mpisppy_data.nlens[\"ROOT\"]\n                xhat = [self.opt.local_scenarios[sname]._mpisppy_data.nonant_cache[i] for i in range(rnlen)]\n            else:\n                xhat = None\n            src_rank = self.scenario_name_to_rank[\"ROOT\"][sname]\n            try:\n                xhats[\"ROOT\"] = self.comms[\"ROOT\"].bcast(xhat, root=src_rank)\n            except:\n                print(\"rank=\",self.cylinder_rank, \"xhats bcast failed on src_rank={}\"\\\n                      .format(src_rank))\n                print(\"root comm size={}\".format(self.comms[\"ROOT\"].size))\n                raise\n            # now form the EF for the appropriate number of second-stage scenario tree nodes\n            # Use the branching factors to figure out how many second-stage nodes.\n            stage2cnt = branching_factors[1]\n            rankcnt = self.n_proc\n            # The next assert is important.\n            assert stage2cnt % rankcnt== 0, \"for stage2ef, ranks must be a multiple of stage2 nodes\"            \n            nodes_per_rank = stage2cnt // rankcnt\n            # to keep the code easier to read, we will first collect the nodenames\n            # Important: we are assuming a standard node naming pattern using underscores.\n            # TBD: do something that would work with underscores *in* the stage branch name\n            # TBD: factor this\n            local_2ndns = dict()  # dict of dicts (inner dicts are for FormEF)\n            for k,s in self.opt.local_scenarios.items():\n                if hasattr(self, \"_EFs\"):\n                    sputils.deact_objs(s)  # create EF does this the first time\n                for node in s._mpisppy_node_list:\n                    ndn = node.name\n                    if len(re.findall(\"_\", ndn)) == 1:\n                        if ndn not in local_2ndns:\n                            local_2ndns[ndn] = {k: s}\n                        local_2ndns[ndn][k] = s\n            if len(local_2ndns) != nodes_per_rank:\n                print(\"stage2ef failure: nodes_per_rank=\",nodes_per_rank, \"local_2ndns=\",local_2ndns)\n                raise RuntimeError(\"stagecnt assumes regular scenario tree and standard _ node naming\")\n            # create an EF for each second stage node and solve with fixed nonant\n            # TBD: factor out the EF creation!\n            if not hasattr(self, \"_EFs\"):\n                for k,s in self.opt.local_scenarios.items():\n                    sputils.stash_ref_objs(s)\n                self._EFs = dict()\n                for ndn2, sdict in local_2ndns.items():  # ndn2 will be a the node name\n                    if ndn2 not in self._EFs:  # this will only happen once\n                        self._EFs[ndn2] = self.opt.FormEF(sdict, ndn2)  # spopt.py\n            # We have EFs so fix noants, solve, and etc.\n            for ndn2, sdict in local_2ndns.items():  # ndn2 will be a the node name\n                wxbarutils.fix_ef_ROOT_nonants(self._EFs[ndn2], xhats[\"ROOT\"])\n                # solve EF\n                solver = pyo.SolverFactory(stage2EFsolvern)\n                if 'persistent' in stage2EFsolvern:\n                    solver.set_instance(self._EFs[ndn2], symbolic_solver_labels=True)\n                    results = solver.solve(tee=Tee)\n                else:\n                    results = solver.solve(self._EFs[ndn2], tee=Tee, symbolic_solver_labels=True,)\n\n                # restore objectives so Ebojective will work\n                for s in sdict.values():\n                    sputils.reactivate_objs(s)\n                # if you hit infeas, return None\n                if not pyo.check_optimal_termination(results):\n                   self.opt._restore_nonants()\n                   return None\n               \n            # feasible xhat found, so finish up 2EF part and return\n            if verbose and src_rank == self.cylinder_rank:\n                print(\"   Feasible xhat found:\")\n                self.opt.local_scenarios[sname].pprint()\n            # get the global obj\n            obj = self.opt.Eobjective(verbose=verbose)\n            if restore_nonants:\n                self.opt._restore_nonants()\n            return obj\n\n        # end of multi-stage with stage2ef\n\n        # Code from here down executes for 2-stage and regular multi-stage\n        # The save is done above\n        self.opt._fix_nonants(xhats)  # (BTW: for all local scenarios)\n\n        # NOTE: for APH we may need disable_pyomo_signal_handling\n        self.opt.solve_loop(solver_options=sopt,\n                           #dis_W=True, dis_prox=True,\n                           verbose=verbose,\n                           tee=Tee)\n\n        infeasP = self.opt.infeas_prob()\n        if infeasP != 0.:\n            # restoring does no harm\n            # if this solution is infeasible\n            self.opt._restore_nonants()\n            return None\n        else:\n            if verbose and src_rank == self.cylinder_rank:\n                print(\"   Feasible xhat found:\")\n                self.opt.local_scenarios[sname].pprint()\n            obj = self.opt.Eobjective(verbose=verbose)\n            if restore_nonants:\n                self.opt._restore_nonants()\n            return obj",
  "def csv_nonants(self, snamedict, fname):\n        \"\"\" write the non-ants in csv format to files based on the file name\n            (we will over-write files if they already exists)\n            Args:\n               snamedic (str): the names of the scenarios to use\n               fname (str): the full name of the file to which to write\n        \"\"\"\n        # only the rank with the requested scenario writes\n        for ndn, sname in snamedict.items():\n            if sname not in self.opt.local_scenarios:\n                continue\n            scen = self.opt.local_scenarios[sname]\n            with open(fname+\"_\"+ndn+\"_\"+sname+\".csv\", \"w\") as f:\n                for node in scen._mpisppy_node_list:\n                    if node.name == ndn:\n                        break\n                nlens = scen._mpisppy_data.nlens\n                f.write(ndn)\n                for i in range(nlens[ndn]):\n                    vardata = node.nonant_vardata_list[i]\n                    f.write(', \"'+vardata.name+'\", '+str(vardata._value))\n                f.write(\"\\n\")",
  "def csv_allvars(self, snamedict, fname):\n        \"\"\" write all Vars in csv format to files based on the file name\n            (we will over-write files if they already exists)\n            Args:\n               snamedict (dict): scenario names\n               fname (str): the full name of the file to which to write\n        \"\"\"\n        # only the rank with the requested scenario writes\n        for ndn, sname in snamedict.items():\n            if sname not in self.opt.local_scenarios:\n                continue\n            scen = self.opt.local_scenarios[sname]\n            for node in scen._mpisppy_node_list:\n                if node.name == ndn:\n                    break\n                with open(fname+\"_\"+ndn+\"_\"+sname,\"w\") as f:\n                    for ((v_name, v_index), v_data)\\\n                        in scen.component_data_iterindex(pyo.Var, active=True):\n                        f.write(v_name + \", \" + str(pyo.value(v_data)) + \"\\n\")",
  "def xhat_common_post_everything(self, extname, obj, snamedict, restored_nonants):\n        \"\"\" Code that most xhat post_everything routines will want to call.\n        Args:\n            extname (str): the name of the extension for reporting\n            obj (float): the xhat objective function\n            snamedict (dict): the (scenario) names upon which xhat is based\n            restored_nonants (bool): if the restore_nonants flag was True on the last\n                call to _try_one.\n        \"\"\"\n        if (obj is not None) and (not restored_nonants):\n            # a tree solution is available\n            self.opt.tree_solution_available = True\n            self.opt.first_stage_solution_available = True\n        if (obj is not None) and (self.opt.spcomm is not None):\n            self.opt.spcomm.BestInnerBound = self.opt.spcomm.InnerBoundUpdate(obj, char='E')\n        if self.cylinder_rank == 0 and self.verbose:\n            print (\"****\", extname ,\"Used scenarios\",\n                   str(snamedict),\"to get xhat Eobj=\",obj)\n\n        if \"csvname\" in self.options:\n            self.csv_nonants(snamedict, self.options[\"csvname\"])\n\n        if \"dump_prefix\" in self.options:\n            prefpref = self.options[\"dump_prefix\"]\n            pref = extname\n            self.csv_nonants(snamedict, prefpref + \"_nonant_\" + pref)\n            self.csv_allvars(snamedict, prefpref + \"_allvars_\" + pref)",
  "def post_iter0(self):\n        # the base class needs this\n        self.comms = self.opt.comms",
  "class Gapper(mpisppy.extensions.extension.Extension):\n\n    def __init__(self, ph):\n        self.ph = ph\n        self.cylinder_rank = self.ph.cylinder_rank\n        self.gapperoptions = self.ph.options[\"gapperoptions\"] # required\n        self.mipgapdict = self.gapperoptions[\"mipgapdict\"]\n        self.verbose = self.ph.options[\"verbose\"] \\\n                       or self.gapperoptions[\"verbose\"]\n                       \n    def _vb(self, str):\n        if self.verbose and self.cylinder_rank == 0:\n            print (\"(rank0) mipgapper:\" + str)\n\n    def set_mipgap(self, mipgap):\n        \"\"\" set the mipgap\n        Args:\n            float (mipgap): the gap to set\n        \"\"\"\n        oldgap = None\n        if \"mipgap\" in self.ph.current_solver_options:\n            oldgap = self.ph.current_solver_options[\"mipgap\"]\n        self._vb(\"Changing mipgap from \"+str(oldgap)+\" to \"+str(mipgap))\n        self.ph.current_solver_options[\"mipgap\"] = float(mipgap)\n        \n    def pre_iter0(self):\n        if self.mipgapdict is None:\n            return\n        if 0 in self.mipgapdict:\n            self.set_mipgap(self.mipgapdict[0])\n                                        \n    def post_iter0(self):\n        return\n\n    def miditer(self):\n        if self.mipgapdict is None:\n            return\n        PHIter = self.ph._PHIter\n        if PHIter in self.mipgapdict:\n            self.set_mipgap(self.mipgapdict[PHIter])\n\n\n    def enditer(self):\n        return\n\n    def post_everything(self):\n        return",
  "def __init__(self, ph):\n        self.ph = ph\n        self.cylinder_rank = self.ph.cylinder_rank\n        self.gapperoptions = self.ph.options[\"gapperoptions\"] # required\n        self.mipgapdict = self.gapperoptions[\"mipgapdict\"]\n        self.verbose = self.ph.options[\"verbose\"] \\\n                       or self.gapperoptions[\"verbose\"]",
  "def _vb(self, str):\n        if self.verbose and self.cylinder_rank == 0:\n            print (\"(rank0) mipgapper:\" + str)",
  "def set_mipgap(self, mipgap):\n        \"\"\" set the mipgap\n        Args:\n            float (mipgap): the gap to set\n        \"\"\"\n        oldgap = None\n        if \"mipgap\" in self.ph.current_solver_options:\n            oldgap = self.ph.current_solver_options[\"mipgap\"]\n        self._vb(\"Changing mipgap from \"+str(oldgap)+\" to \"+str(mipgap))\n        self.ph.current_solver_options[\"mipgap\"] = float(mipgap)",
  "def pre_iter0(self):\n        if self.mipgapdict is None:\n            return\n        if 0 in self.mipgapdict:\n            self.set_mipgap(self.mipgapdict[0])",
  "def post_iter0(self):\n        return",
  "def miditer(self):\n        if self.mipgapdict is None:\n            return\n        PHIter = self.ph._PHIter\n        if PHIter in self.mipgapdict:\n            self.set_mipgap(self.mipgapdict[PHIter])",
  "def enditer(self):\n        return",
  "def post_everything(self):\n        return",
  "class Wtracker_extension(mpisppy.extensions.extension.Extension):\n    \"\"\"\n        wrap the wtracker code as an extension\n        \n        Args:\n            opt (PHBase (inherets from SPOpt) object): gives the problem that we bound\n\n        Attributes:\n          scenario_name_to_rank (dict of dict): nodes (i.e. comms) scen names\n                keys are comms (i.e., tree nodes); values are dicts with keys\n                that are scenario names and values that are ranks\n    \"\"\"\n    def __init__(self, opt, comm=None):\n        super().__init__(opt)\n        self.cylinder_rank = self.opt.cylinder_rank\n        self.verbose = self.opt.options[\"verbose\"]\n        self.wtracker = wtracker.WTracker(opt)\n        self.options = opt.options[\"wtracker_options\"]\n        # TBD: more graceful death if options are bad\n        self.wlen = self.options[\"wlen\"]\n\n    def pre_iter0(self):\n        pass\n\n    def post_iter0(self):\n        pass\n        \n    def miditer(self):\n        pass\n\n    def enditer(self):\n        self.wtracker.grab_local_Ws()\n\n    def post_everything(self):\n        reportlen = self.options.get(\"reportlen\")\n        stdevthresh = self.options.get(\"stdevthresh\")\n        file_prefix = self.options.get(\"file_prefix\")\n        self.wtracker.report_by_moving_stats(self.wlen,\n                                             reportlen=reportlen,\n                                             stdevthresh=stdevthresh,\n                                             file_prefix=file_prefix)",
  "def __init__(self, opt, comm=None):\n        super().__init__(opt)\n        self.cylinder_rank = self.opt.cylinder_rank\n        self.verbose = self.opt.options[\"verbose\"]\n        self.wtracker = wtracker.WTracker(opt)\n        self.options = opt.options[\"wtracker_options\"]\n        # TBD: more graceful death if options are bad\n        self.wlen = self.options[\"wlen\"]",
  "def pre_iter0(self):\n        pass",
  "def post_iter0(self):\n        pass",
  "def miditer(self):\n        pass",
  "def enditer(self):\n        self.wtracker.grab_local_Ws()",
  "def post_everything(self):\n        reportlen = self.options.get(\"reportlen\")\n        stdevthresh = self.options.get(\"stdevthresh\")\n        file_prefix = self.options.get(\"file_prefix\")\n        self.wtracker.report_by_moving_stats(self.wlen,\n                                             reportlen=reportlen,\n                                             stdevthresh=stdevthresh,\n                                             file_prefix=file_prefix)",
  "def Fixer_tuple(xvar, th=None, nb=None, lb=None, ub=None):\n    \"\"\" Somewhat self-documenting way to make a fixer tuple.\n        For use in/by/for the so-called Reference Model.\n    Args:\n        xvar (Var): the Pyomo Var\n        th (float): compared to sqrt(abs(xbar_sqared - xsquared_bar))\n        nb: (int) None means ignore; for iter k, number of iters \n                  converged anywhere.\n        lb: (int) None means ignore; for iter k, number of iters \n                  converged within th of xvar lower bound.\n        ub: (int) None means ignore; for iter k, number of iters\n                  converged within th of xvar upper bound\n\n    Returns:\n        tuple: a tuple to be appended to the iter0 or iterk list of tuples\n    \"\"\"\n    if th is None and nb is None and lb is None and ub is None:\n        print (\"warning: Fixer_tuple called for Var=\", xvar.name,\n               \"but no arguments were given\")\n    if th is None:\n        th = 0\n    if nb is not None and lb is not None and nb < lb:\n        print (\"warning: Fixer_tuple called for Var=\", xvar.name,\n               \"with nb < lb, which means lb will be ignored.\")\n    if  nb is not None and ub is not None and nb < ub:\n        print (\"warning: Fixer_tuple called for Var=\", xvar.name,\n               \"with nb < ub, which means ub will be ignored.\")\n        \n    return (id(xvar), th, nb, lb, ub)",
  "class Fixer(mpisppy.extensions.extension.Extension):\n\n    def __init__(self, ph):\n        self.ph = ph\n        self.cylinder_rank = self.ph.cylinder_rank\n        self.options = ph.options\n        self.fixeroptions = self.options[\"fixeroptions\"] # required\n        self.verbose = self.options[\"verbose\"] \\\n                       or self.fixeroptions[\"verbose\"]\n        # this function is scenario specific (takes a scenario as an arg)\n        self.id_fix_list_fct = self.fixeroptions[\"id_fix_list_fct\"]\n        self.dprogress = ph.options[\"display_progress\"]\n        self.fixed_prior_iter0 = 0\n        self.fixed_so_far = 0        \n        self.boundtol = self.fixeroptions[\"boundtol\"]\n\n    def populate(self, local_scenarios):\n        # [(ndn, i)] = iter count (i indices into nonantlist)\n        self.local_scenarios = local_scenarios\n\n        self.iter0_fixer_tuples = {} # caller data\n        self.fixer_tuples = {} # caller data\n        self.threshold = {}  \n        self.iter0_threshold = {}  \n        # This count dict drives the loops later\n        # NOTE: every scenario has a list.\n        for k,s in self.local_scenarios.items():\n            s._mpisppy_data.conv_iter_count = {} # key is (ndn, i)\n\n            self.iter0_fixer_tuples[s], self.fixer_tuples[s] = \\\n                    self.id_fix_list_fct(s)\n\n            if self.iter0_fixer_tuples[s] is not None:\n                for (varid, th, nb, lb, ub) in self.iter0_fixer_tuples[s]:\n                    (ndn, i) = s._mpisppy_data.varid_to_nonant_index[varid] #\n                    if (ndn, i) not in self.iter0_threshold:\n                        self.iter0_threshold[(ndn, i)] = th\n                    else:\n                        if th != self.iter0_threshold[(ndn, i)]:\n                            print (s.name, ndn, i, th)\n                            raise RuntimeError(\"Attempt to vary iter0 fixer \"+\\\n                                               \"threshold across scenarios.\")\n            if self.fixer_tuples[s] is not None:\n                for (varid, th, nb, lb, ub) in self.fixer_tuples[s]:\n                    (ndn, i) = s._mpisppy_data.varid_to_nonant_index[varid]\n                    s._mpisppy_data.conv_iter_count[(ndn, i)] = 0\n                    if (ndn, i) not in self.threshold:\n                        self.threshold[(ndn, i)] = th\n                    else:\n                        if th != self.threshold[(ndn, i)]:\n                            print (s.name, ndn, i, th)\n                            raise RuntimeError(\"Attempt to vary fixer \"+\\\n                                               \"threshold across scenarios\")\n\n    # verbose utility\n    def _vb(self, str):\n        if self.verbose and self.cylinder_rank == 0:\n            print (\"(rank0) \" + str)\n\n    # display progress utility\n    def _dp(self, str):\n        if (self.dprogress or self.verbose) and self.cylinder_rank == 0:\n            print (\"(rank0) \" + str)\n\n    def _update_fix_counts(self):\n        nodesdone = []  # avoid multiple updates of a node's Vars\n        for k,s in self.local_scenarios.items():\n            for ndn_i, xvar in s._mpisppy_data.nonant_indices.items():\n                if xvar.is_fixed():\n                    continue\n                xb = pyo.value(s._mpisppy_model.xbars[ndn_i])\n                diff = xb * xb - pyo.value(s._mpisppy_model.xsqbars[ndn_i])\n                tolval = self.threshold[ndn_i]\n                tolval *= tolval  # the tol is on sqrt\n                if -diff < tolval and diff < tolval:\n                    ##print (\"debug += diff, tolval\", diff, tolval)\n                    s._mpisppy_data.conv_iter_count[ndn_i] += 1\n                else:\n                    s._mpisppy_data.conv_iter_count[ndn_i] = 0\n                    ##print (\"debug reset fix diff, tolval\", diff, tolval)\n                    \n    def iter0(self, local_scenarios):\n\n        # first, do some persistent solver with bundles gymnastics        \n        have_bundles = hasattr(self.ph, \"saved_objs\") # indicates bundles\n        if have_bundles:\n            subpname = next(iter(self.ph.local_subproblems))\n            subp = self.ph.local_subproblems[subpname]\n            solver_is_persistent = isinstance(subp._solver_plugin,\n            pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)\n            if solver_is_persistent:\n                vars_to_update = {}\n\n        fixoptions = self.fixeroptions\n        # modelers might have already fixed variables - count those up and output the result\n        raw_fixed_on_arrival = 0\n        raw_fixed_this_iter = 0   \n        for sname,s in self.ph.local_scenarios.items():\n            if self.iter0_fixer_tuples[s] is None:\n                print (\"WARNING: No Iter0 fixer tuple for s.name=\",s.name)\n                return\n            \n            if not have_bundles:\n                solver_is_persistent = isinstance(s._solver_plugin,\n                    pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)\n\n            for (varid, th, nb, lb, ub) in self.iter0_fixer_tuples[s]:\n                was_fixed = False\n                try:\n                    (ndn, i) = s._mpisppy_data.varid_to_nonant_index[varid]\n                except:\n                    print (\"Are you trying to fix a Var that is not nonant?\")\n                    raise\n                xvar = s._mpisppy_data.nonant_indices[ndn,i]\n                if not xvar.is_fixed():\n                    xb = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                    diff = xb * xb - pyo.value(s._mpisppy_model.xsqbars[(ndn,i)])\n                    tolval = self.iter0_threshold[(ndn, i)]\n                    sqtolval = tolval*tolval  # the tol is on sqrt\n                    if -diff > sqtolval or diff > sqtolval:\n                        ##print (\"debug0 NO fix diff, sqtolval\", diff, sqtolval)\n                        continue\n                    else:\n                        ##print (\"debug0 fix diff, sqtolval\", diff, sqtolval)\n                        # if we are still here, it is converged\n                        if nb is not None:\n                            xvar.fix(xb)\n                            self._vb(\"Fixed0 nb %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n                        elif lb is not None and xb - xvar.lb < self.boundtol:\n                            xvar.fix(xvar.lb)\n                            self._vb(\"Fixed0 lb %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n                        elif ub is not None and xvar.ub - xb < self.boundtol:\n                            xvar.fix(xvar.ub)\n                            self._vb(\"Fixed0 ub %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n\n                    if was_fixed:\n                        raw_fixed_this_iter += 1\n                        if not have_bundles and solver_is_persistent:\n                            s._solver_plugin.update_var(xvar)\n                        if have_bundles and solver_is_persistent:\n                            if sname not in vars_to_update:\n                                vars_to_update[sname] = []\n                            vars_to_update[sname].append(xvar)\n                else:\n                    # TODO: a paranoid would and should put a check to ensure\n                    # that variables are fixed in all scenarios and are fixed\n                    # to the same value.\n                    raw_fixed_on_arrival += 1\n\n        if have_bundles and solver_is_persistent:\n            for k,subp in self.ph.local_subproblems.items():\n                subpnum = sputils.extract_num(k)\n                rank_local = self.ph.cylinder_rank\n                for sname in self.ph.names_in_bundles[rank_local][subpnum]:\n                    if sname in vars_to_update:\n                        for xvar in vars_to_update[sname]:\n                            subp._solver_plugin.update_var(xvar)\n                        \n        self.fixed_prior_iter0 += raw_fixed_on_arrival / len(local_scenarios)\n        self.fixed_so_far += raw_fixed_this_iter / len(local_scenarios)\n        self._dp(\"Unique vars fixed so far - %d (%d prior to iteration 0)\" % (self.fixed_so_far+self.fixed_prior_iter0, self.fixed_prior_iter0))\n        if raw_fixed_this_iter % len(local_scenarios) != 0:\n            raise RuntimeError (\"Variation in fixing across scenarios detected \"\n                                \"in fixer.py (iter0)\")\n            # maybe to do mpicomm.abort()        \n        if raw_fixed_on_arrival % len(local_scenarios) != 0:\n            raise RuntimeError (\"Variation in fixing across scenarios prior to iteration 0 detected \"\n                                \"in fixer.py (iter0)\")        \n\n\n    def iterk(self, PHIter):\n        \"\"\" Before iter k>1 solves, but after x-bar update.\n        \"\"\"\n        # first, do some persistent solver with bundles gymnastics        \n        have_bundles = hasattr(self.ph, \"saved_objs\") # indicates bundles\n        if have_bundles:\n            subpname = next(iter(self.ph.local_subproblems))\n            subp = self.ph.local_subproblems[subpname]\n            solver_is_persistent = isinstance(subp._solver_plugin,\n            pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)\n            if solver_is_persistent:\n                vars_to_update = {}\n\n        fixoptions = self.fixeroptions\n        raw_fixed_this_iter = 0\n        self._update_fix_counts()\n        for sname,s in self.local_scenarios.items():\n            if self.fixer_tuples[s] is None:\n                print (\"MAJOR WARNING: No Iter k fixer tuple for s.name=\",s.name)\n                return\n            if not have_bundles:\n                solver_is_persistent = isinstance(s._solver_plugin, pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)\n            for (varid, th, nb, lb, ub) in self.fixer_tuples[s]:\n                was_fixed = False\n                try:\n                    (ndn, i) = s._mpisppy_data.varid_to_nonant_index[varid]\n                except:\n                    print (\"Are you trying to fix a Var that is not nonant?\")\n                    raise\n                tolval = self.threshold[(ndn, i)]\n                xvar = s._mpisppy_data.nonant_indices[ndn,i]\n                if not xvar.is_fixed():\n                    xb = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                    fx = s._mpisppy_data.conv_iter_count[(ndn,i)]\n                    if fx > 0:\n                        xbar = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                        was_fixed = False\n                        if  nb is not None and nb <= fx:\n                            xvar.fix(xbar)\n                            self._vb(\"Fixed nb %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n                        elif lb is not None and lb < fx \\\n                             and xb - xvar.lb < self.boundtol:\n                            xvar.fix(xvar.lb)\n                            self._vb(\"Fixed lb %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n                        elif ub is not None and ub < fx \\\n                             and xvar.ub - xb < self.boundtol:\n                            xvar.fix(xvar.ub)\n                            self._vb(\"Fixed ub %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n\n                    if was_fixed:\n                        raw_fixed_this_iter += 1\n                        if not have_bundles and solver_is_persistent:\n                            s._solver_plugin.update_var(xvar)\n                        if have_bundles and solver_is_persistent:\n                            if sname not in vars_to_update:\n                                vars_to_update[sname] = []\n                            vars_to_update[sname].append(xvar)\n\n\n        if have_bundles and solver_is_persistent:\n            for k,subp in self.ph.local_subproblems.items():\n                subpnum = sputils.extract_num(k)\n                rank_local = self.ph.cylinder_rank\n                for sname in self.ph.names_in_bundles[rank_local][subpnum]:\n                    if sname in vars_to_update:\n                        for xvar in vars_to_update[sname]:\n                            subp._solver_plugin.update_var(xvar)\n\n        self.fixed_so_far += raw_fixed_this_iter / len(self.local_scenarios)\n        self._dp(\"Unique vars fixed so far - %d (%d prior to iteration 0)\" % (self.fixed_so_far+self.fixed_prior_iter0, self.fixed_prior_iter0))        \n        if raw_fixed_this_iter % len(self.local_scenarios) != 0:\n            raise RuntimeError (\"Variation in fixing across scenarios detected \"\n                                \"in fixer.py\")\n\n    def pre_iter0(self):\n        return\n                                        \n    def post_iter0(self):\n        \"\"\" initialize data structures; that's all we can do at this point\n        \"\"\"\n        self.populate(self.ph.local_scenarios)\n\n    def miditer(self):\n        \"\"\" Check for fixing before in the middle of PHIter (after\n        the xbar update for PHiter-1).\n        \"\"\"\n        PHIter = self.ph._PHIter\n        if PHIter == 1:  # before iter 1 solves\n            self.iter0(self.ph.local_scenarios)\n        else:\n            self.iterk(PHIter)\n\n    def enditer(self):\n        return\n\n    def post_everything(self):\n        self._dp(\"Final unique vars fixed by fixer= %s\" % \\\n                      (self.fixed_so_far))",
  "def __init__(self, ph):\n        self.ph = ph\n        self.cylinder_rank = self.ph.cylinder_rank\n        self.options = ph.options\n        self.fixeroptions = self.options[\"fixeroptions\"] # required\n        self.verbose = self.options[\"verbose\"] \\\n                       or self.fixeroptions[\"verbose\"]\n        # this function is scenario specific (takes a scenario as an arg)\n        self.id_fix_list_fct = self.fixeroptions[\"id_fix_list_fct\"]\n        self.dprogress = ph.options[\"display_progress\"]\n        self.fixed_prior_iter0 = 0\n        self.fixed_so_far = 0        \n        self.boundtol = self.fixeroptions[\"boundtol\"]",
  "def populate(self, local_scenarios):\n        # [(ndn, i)] = iter count (i indices into nonantlist)\n        self.local_scenarios = local_scenarios\n\n        self.iter0_fixer_tuples = {} # caller data\n        self.fixer_tuples = {} # caller data\n        self.threshold = {}  \n        self.iter0_threshold = {}  \n        # This count dict drives the loops later\n        # NOTE: every scenario has a list.\n        for k,s in self.local_scenarios.items():\n            s._mpisppy_data.conv_iter_count = {} # key is (ndn, i)\n\n            self.iter0_fixer_tuples[s], self.fixer_tuples[s] = \\\n                    self.id_fix_list_fct(s)\n\n            if self.iter0_fixer_tuples[s] is not None:\n                for (varid, th, nb, lb, ub) in self.iter0_fixer_tuples[s]:\n                    (ndn, i) = s._mpisppy_data.varid_to_nonant_index[varid] #\n                    if (ndn, i) not in self.iter0_threshold:\n                        self.iter0_threshold[(ndn, i)] = th\n                    else:\n                        if th != self.iter0_threshold[(ndn, i)]:\n                            print (s.name, ndn, i, th)\n                            raise RuntimeError(\"Attempt to vary iter0 fixer \"+\\\n                                               \"threshold across scenarios.\")\n            if self.fixer_tuples[s] is not None:\n                for (varid, th, nb, lb, ub) in self.fixer_tuples[s]:\n                    (ndn, i) = s._mpisppy_data.varid_to_nonant_index[varid]\n                    s._mpisppy_data.conv_iter_count[(ndn, i)] = 0\n                    if (ndn, i) not in self.threshold:\n                        self.threshold[(ndn, i)] = th\n                    else:\n                        if th != self.threshold[(ndn, i)]:\n                            print (s.name, ndn, i, th)\n                            raise RuntimeError(\"Attempt to vary fixer \"+\\\n                                               \"threshold across scenarios\")",
  "def _vb(self, str):\n        if self.verbose and self.cylinder_rank == 0:\n            print (\"(rank0) \" + str)",
  "def _dp(self, str):\n        if (self.dprogress or self.verbose) and self.cylinder_rank == 0:\n            print (\"(rank0) \" + str)",
  "def _update_fix_counts(self):\n        nodesdone = []  # avoid multiple updates of a node's Vars\n        for k,s in self.local_scenarios.items():\n            for ndn_i, xvar in s._mpisppy_data.nonant_indices.items():\n                if xvar.is_fixed():\n                    continue\n                xb = pyo.value(s._mpisppy_model.xbars[ndn_i])\n                diff = xb * xb - pyo.value(s._mpisppy_model.xsqbars[ndn_i])\n                tolval = self.threshold[ndn_i]\n                tolval *= tolval  # the tol is on sqrt\n                if -diff < tolval and diff < tolval:\n                    ##print (\"debug += diff, tolval\", diff, tolval)\n                    s._mpisppy_data.conv_iter_count[ndn_i] += 1\n                else:\n                    s._mpisppy_data.conv_iter_count[ndn_i] = 0",
  "def iter0(self, local_scenarios):\n\n        # first, do some persistent solver with bundles gymnastics        \n        have_bundles = hasattr(self.ph, \"saved_objs\") # indicates bundles\n        if have_bundles:\n            subpname = next(iter(self.ph.local_subproblems))\n            subp = self.ph.local_subproblems[subpname]\n            solver_is_persistent = isinstance(subp._solver_plugin,\n            pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)\n            if solver_is_persistent:\n                vars_to_update = {}\n\n        fixoptions = self.fixeroptions\n        # modelers might have already fixed variables - count those up and output the result\n        raw_fixed_on_arrival = 0\n        raw_fixed_this_iter = 0   \n        for sname,s in self.ph.local_scenarios.items():\n            if self.iter0_fixer_tuples[s] is None:\n                print (\"WARNING: No Iter0 fixer tuple for s.name=\",s.name)\n                return\n            \n            if not have_bundles:\n                solver_is_persistent = isinstance(s._solver_plugin,\n                    pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)\n\n            for (varid, th, nb, lb, ub) in self.iter0_fixer_tuples[s]:\n                was_fixed = False\n                try:\n                    (ndn, i) = s._mpisppy_data.varid_to_nonant_index[varid]\n                except:\n                    print (\"Are you trying to fix a Var that is not nonant?\")\n                    raise\n                xvar = s._mpisppy_data.nonant_indices[ndn,i]\n                if not xvar.is_fixed():\n                    xb = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                    diff = xb * xb - pyo.value(s._mpisppy_model.xsqbars[(ndn,i)])\n                    tolval = self.iter0_threshold[(ndn, i)]\n                    sqtolval = tolval*tolval  # the tol is on sqrt\n                    if -diff > sqtolval or diff > sqtolval:\n                        ##print (\"debug0 NO fix diff, sqtolval\", diff, sqtolval)\n                        continue\n                    else:\n                        ##print (\"debug0 fix diff, sqtolval\", diff, sqtolval)\n                        # if we are still here, it is converged\n                        if nb is not None:\n                            xvar.fix(xb)\n                            self._vb(\"Fixed0 nb %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n                        elif lb is not None and xb - xvar.lb < self.boundtol:\n                            xvar.fix(xvar.lb)\n                            self._vb(\"Fixed0 lb %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n                        elif ub is not None and xvar.ub - xb < self.boundtol:\n                            xvar.fix(xvar.ub)\n                            self._vb(\"Fixed0 ub %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n\n                    if was_fixed:\n                        raw_fixed_this_iter += 1\n                        if not have_bundles and solver_is_persistent:\n                            s._solver_plugin.update_var(xvar)\n                        if have_bundles and solver_is_persistent:\n                            if sname not in vars_to_update:\n                                vars_to_update[sname] = []\n                            vars_to_update[sname].append(xvar)\n                else:\n                    # TODO: a paranoid would and should put a check to ensure\n                    # that variables are fixed in all scenarios and are fixed\n                    # to the same value.\n                    raw_fixed_on_arrival += 1\n\n        if have_bundles and solver_is_persistent:\n            for k,subp in self.ph.local_subproblems.items():\n                subpnum = sputils.extract_num(k)\n                rank_local = self.ph.cylinder_rank\n                for sname in self.ph.names_in_bundles[rank_local][subpnum]:\n                    if sname in vars_to_update:\n                        for xvar in vars_to_update[sname]:\n                            subp._solver_plugin.update_var(xvar)\n                        \n        self.fixed_prior_iter0 += raw_fixed_on_arrival / len(local_scenarios)\n        self.fixed_so_far += raw_fixed_this_iter / len(local_scenarios)\n        self._dp(\"Unique vars fixed so far - %d (%d prior to iteration 0)\" % (self.fixed_so_far+self.fixed_prior_iter0, self.fixed_prior_iter0))\n        if raw_fixed_this_iter % len(local_scenarios) != 0:\n            raise RuntimeError (\"Variation in fixing across scenarios detected \"\n                                \"in fixer.py (iter0)\")\n            # maybe to do mpicomm.abort()        \n        if raw_fixed_on_arrival % len(local_scenarios) != 0:\n            raise RuntimeError (\"Variation in fixing across scenarios prior to iteration 0 detected \"\n                                \"in fixer.py (iter0)\")",
  "def iterk(self, PHIter):\n        \"\"\" Before iter k>1 solves, but after x-bar update.\n        \"\"\"\n        # first, do some persistent solver with bundles gymnastics        \n        have_bundles = hasattr(self.ph, \"saved_objs\") # indicates bundles\n        if have_bundles:\n            subpname = next(iter(self.ph.local_subproblems))\n            subp = self.ph.local_subproblems[subpname]\n            solver_is_persistent = isinstance(subp._solver_plugin,\n            pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)\n            if solver_is_persistent:\n                vars_to_update = {}\n\n        fixoptions = self.fixeroptions\n        raw_fixed_this_iter = 0\n        self._update_fix_counts()\n        for sname,s in self.local_scenarios.items():\n            if self.fixer_tuples[s] is None:\n                print (\"MAJOR WARNING: No Iter k fixer tuple for s.name=\",s.name)\n                return\n            if not have_bundles:\n                solver_is_persistent = isinstance(s._solver_plugin, pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)\n            for (varid, th, nb, lb, ub) in self.fixer_tuples[s]:\n                was_fixed = False\n                try:\n                    (ndn, i) = s._mpisppy_data.varid_to_nonant_index[varid]\n                except:\n                    print (\"Are you trying to fix a Var that is not nonant?\")\n                    raise\n                tolval = self.threshold[(ndn, i)]\n                xvar = s._mpisppy_data.nonant_indices[ndn,i]\n                if not xvar.is_fixed():\n                    xb = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                    fx = s._mpisppy_data.conv_iter_count[(ndn,i)]\n                    if fx > 0:\n                        xbar = pyo.value(s._mpisppy_model.xbars[(ndn,i)])\n                        was_fixed = False\n                        if  nb is not None and nb <= fx:\n                            xvar.fix(xbar)\n                            self._vb(\"Fixed nb %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n                        elif lb is not None and lb < fx \\\n                             and xb - xvar.lb < self.boundtol:\n                            xvar.fix(xvar.lb)\n                            self._vb(\"Fixed lb %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n                        elif ub is not None and ub < fx \\\n                             and xvar.ub - xb < self.boundtol:\n                            xvar.fix(xvar.ub)\n                            self._vb(\"Fixed ub %s %s at %s\" % \\\n                                     (s.name, xvar.name, str(xvar._value)))\n                            was_fixed = True\n\n                    if was_fixed:\n                        raw_fixed_this_iter += 1\n                        if not have_bundles and solver_is_persistent:\n                            s._solver_plugin.update_var(xvar)\n                        if have_bundles and solver_is_persistent:\n                            if sname not in vars_to_update:\n                                vars_to_update[sname] = []\n                            vars_to_update[sname].append(xvar)\n\n\n        if have_bundles and solver_is_persistent:\n            for k,subp in self.ph.local_subproblems.items():\n                subpnum = sputils.extract_num(k)\n                rank_local = self.ph.cylinder_rank\n                for sname in self.ph.names_in_bundles[rank_local][subpnum]:\n                    if sname in vars_to_update:\n                        for xvar in vars_to_update[sname]:\n                            subp._solver_plugin.update_var(xvar)\n\n        self.fixed_so_far += raw_fixed_this_iter / len(self.local_scenarios)\n        self._dp(\"Unique vars fixed so far - %d (%d prior to iteration 0)\" % (self.fixed_so_far+self.fixed_prior_iter0, self.fixed_prior_iter0))        \n        if raw_fixed_this_iter % len(self.local_scenarios) != 0:\n            raise RuntimeError (\"Variation in fixing across scenarios detected \"\n                                \"in fixer.py\")",
  "def pre_iter0(self):\n        return",
  "def post_iter0(self):\n        \"\"\" initialize data structures; that's all we can do at this point\n        \"\"\"\n        self.populate(self.ph.local_scenarios)",
  "def miditer(self):\n        \"\"\" Check for fixing before in the middle of PHIter (after\n        the xbar update for PHiter-1).\n        \"\"\"\n        PHIter = self.ph._PHIter\n        if PHIter == 1:  # before iter 1 solves\n            self.iter0(self.ph.local_scenarios)\n        else:\n            self.iterk(PHIter)",
  "def enditer(self):\n        return",
  "def post_everything(self):\n        self._dp(\"Final unique vars fixed by fixer= %s\" % \\\n                      (self.fixed_so_far))",
  "class CrossScenarioExtension(Extension):\n    def __init__(self, spbase_object):\n        super().__init__(spbase_object)\n\n        opt = self.opt\n        if 'cross_scen_options' in opt.options and \\\n                'check_bound_improve_iterations' in opt.options['cross_scen_options']:\n            self.check_bound_iterations = opt.options['cross_scen_options']['check_bound_improve_iterations']\n        else:\n            self.check_bound_iterations = None\n\n        self.cur_ib = None\n        self.iter_at_cur_ib = 1\n\n        self.cur_ob = None\n\n        self.reenable_W = None\n        self.reenable_prox = None\n\n        self.any_cuts = False\n        self.iter_since_last_check = 0\n\n    def _disable_W_and_prox(self):\n        assert self.reenable_W is None\n        assert self.reenable_prox is None\n        ## hold the PH object harmless\n        opt = self.opt\n        self.reenable_W = False\n        self.reenable_prox = False\n        if not opt.W_disabled and not opt.prox_disabled:\n            opt.disable_W_and_prox()\n            self.reenable_W = True\n            self.reenable_prox = True \n        elif not opt.W_disabled:\n            opt._disable_W()\n            self.eenable_W = True\n        elif not opt.prox_disabled:\n            opt._disable_prox()\n            self.reenable_prox = True \n\n    def _enable_W_and_prox(self):\n        assert self.reenable_W is not None\n        assert self.reenable_prox is not None\n        \n        opt = self.opt\n        if self.reenable_W and self.reenable_prox:\n            opt.reenable_W_and_prox()\n        elif self.reenable_W:\n            opt._reenable_W()\n        elif self.reenable_prox:\n            opt._reenable_prox()\n\n        self.reenable_W = None\n        self.reenable_prox = None\n\n    def _check_bound(self):\n        opt = self.opt\n\n        chached_ph_obj = dict()\n\n        for k,s in opt.local_subproblems.items():\n            phobj = find_active_objective(s)\n            phobj.deactivate()\n            chached_ph_obj[k] = phobj\n            s._mpisppy_model.EF_Obj.activate()\n\n        teeme = (\n            \"tee-rank0-solves\" in opt.options\n             and opt.options[\"tee-rank0-solves\"]\n        )\n        opt.solve_loop(\n                solver_options=opt.current_solver_options,\n                dtiming=opt.options[\"display_timing\"],\n                gripe=True,\n                disable_pyomo_signal_handling=False,\n                tee=teeme,\n                verbose=opt.options[\"verbose\"],\n        )\n\n        local_obs = np.fromiter((s._mpisppy_data.outer_bound for s in opt.local_subproblems.values()),\n                                dtype=\"d\", count=len(opt.local_subproblems))\n\n        local_ob = np.empty(1)\n        if opt.is_minimizing:\n            local_ob[0] = local_obs.max()\n        else:\n            local_ob[0] = local_obs.min()\n\n        global_ob = np.empty(1)\n\n        if opt.is_minimizing:\n            opt.mpicomm.Allreduce(local_ob, global_ob, op=mpi.MAX)\n        else: \n            opt.mpicomm.Allreduce(local_ob, global_ob, op=mpi.MIN)\n\n        #print(f\"CrossScenarioExtension OB: {global_ob[0]}\")\n\n        opt.spcomm.BestOuterBound = opt.spcomm.OuterBoundUpdate(global_ob[0], char='C')\n\n        for k,s in opt.local_subproblems.items():\n            s._mpisppy_model.EF_Obj.deactivate()\n            chached_ph_obj[k].activate()\n\n    def pre_iter0(self):\n        if self.opt.multistage:\n            raise RuntimeError(\"CrossScenarioExtension does not support \"\n                               \"multi-stage problems at this time\")\n        ## hack as this provides logs for outer bounds\n        self.opt.spcomm.has_outerbound_spokes = True\n\n    def post_iter0(self):\n        opt = self.opt\n        # NOTE: the LShaped code negates the objective, so\n        #       we do the same here for consistency\n        if 'cross_scen_options' in opt.options and \\\n                'valid_eta_bound' in opt.options['cross_scen_options']:\n            valid_eta_bound = opt.options['cross_scen_options']['valid_eta_bound']\n            if not opt.is_minimizing:\n                _eta_init = { k: -v for k,v in valid_eta_bound.items() }\n            else:\n                _eta_init = valid_eta_bound\n            _eta_bounds = lambda m,k : (_eta_init[k], None)\n        else:\n            lb = (-sys.maxsize - 1) * 1. / len(opt.all_scenario_names)\n            _eta_init = lambda m,k : lb\n            _eta_bounds = lambda m,k : (lb, None)\n\n        # eta is attached to each subproblem, regardless of bundles\n        bundling = opt.bundling\n        for k,s in opt.local_subproblems.items():\n            s._mpisppy_model.eta = pyo.Var(opt.all_scenario_names, initialize=_eta_init, bounds=_eta_bounds)\n            if sputils.is_persistent(s._solver_plugin):\n                for var in s._mpisppy_model.eta.values():\n                    s._solver_plugin.add_var(var)\n            if bundling: ## create a refence to eta on each subproblem\n                for sn in s.scen_list:\n                    scenario = opt.local_scenarios[sn]\n                    scenario._mpisppy_model.eta = { k : s._mpisppy_model.eta[k] for k in opt.all_scenario_names }\n\n        ## hold the PH object harmless\n        self._disable_W_and_prox()\n        \n        for k,s in opt.local_subproblems.items():\n\n            obj = find_active_objective(s)\n\n            repn = generate_standard_repn(obj.expr, quadratic=True)\n            if len(repn.nonlinear_vars) > 0:\n                raise ValueError(\"CrossScenario does not support models with nonlinear objective functions\")\n\n            if bundling:\n                ## NOTE: this is slighly wasteful, in that for a bundle\n                ##       the first-stage cost appears len(s.scen_list) times\n                ##       If this really made a difference, we could use s.ref_vars\n                ##       to do the substitution\n                nonant_vardata_list = list()\n                for sn in s.scen_list:\n                    nonant_vardata_list.extend( \\\n                            opt.local_scenarios[sn]._mpisppy_node_list[0].nonant_vardata_list)\n            else:\n                nonant_vardata_list = s._mpisppy_node_list[0].nonant_vardata_list\n\n            nonant_ids = set((id(var) for var in nonant_vardata_list))\n\n            linear_coefs = list(repn.linear_coefs)\n            linear_vars = list(repn.linear_vars)\n\n            quadratic_coefs = list(repn.quadratic_coefs)\n\n            # adjust coefficients by scenario/bundle probability\n            scen_prob = s._mpisppy_probability\n            for i,var in enumerate(repn.linear_vars):\n                if id(var) not in nonant_ids:\n                    linear_coefs[i] *= scen_prob\n\n            for i,(x,y) in enumerate(repn.quadratic_vars):\n                # only multiply through once\n                if id(x) not in nonant_ids:\n                    quadratic_coefs[i] *= scen_prob\n                elif id(y) not in nonant_ids:\n                    quadratic_coefs[i] *= scen_prob\n\n            # NOTE: the LShaped code negates the objective, so\n            #       we do the same here for consistency\n            if not opt.is_minimizing:\n                for i,coef in enumerate(linear_coefs):\n                    linear_coefs[i] = -coef\n                for i,coef in enumerate(quadratic_coefs):\n                    quadratic_coefs[i] = -coef\n\n            # add the other etas\n            if bundling:\n                these_scenarios = set(s.scen_list)\n            else:\n                these_scenarios = [k]\n\n            eta_scenarios = list()\n            for sn in opt.all_scenario_names:\n                if sn not in these_scenarios:\n                    linear_coefs.append(1)\n                    linear_vars.append(s._mpisppy_model.eta[sn])\n                    eta_scenarios.append(sn)\n\n            expr = LinearExpression(constant=repn.constant, linear_coefs=linear_coefs,\n                                    linear_vars=linear_vars)\n\n            if repn.quadratic_vars:\n                expr += pyo.quicksum(\n                    (coef*x*y for coef,(x,y) in zip(quadratic_coefs, repn.quadratic_vars))\n                )\n\n            s._mpisppy_model.EF_obj = pyo.Expression(expr=expr)\n\n            if opt.is_minimizing:\n                s._mpisppy_model.EF_Obj = pyo.Objective(expr=s._mpisppy_model.EF_obj, sense=pyo.minimize)\n            else:\n                s._mpisppy_model.EF_Obj = pyo.Objective(expr=-s._mpisppy_model.EF_obj, sense=pyo.maximize)\n            s._mpisppy_model.EF_Obj.deactivate()\n\n            # add cut constraint dicts\n            s._mpisppy_model.benders_cuts = pyo.Constraint(pyo.Any)\n            s._mpisppy_model.inner_bound_constr = pyo.Constraint(pyo.Any)\n\n        self._enable_W_and_prox()\n\n        # try to get the initial eta LB cuts\n        # (may not be available)\n        opt.spcomm.get_from_cross_cuts()\n\n    def miditer(self):\n        self.iter_since_last_check += 1\n\n        ib = self.opt.spcomm.BestInnerBound\n        if ib != self.cur_ib:\n            self.cur_ib = ib\n            self.iter_at_cur_ib = 1\n        elif self.cur_ib is not None and math.isfinite(self.cur_ib):\n            self.iter_at_cur_ib += 1\n\n        ob = self.opt.spcomm.BestOuterBound\n        if self.cur_ob is not None and math.isclose(ob, self.cur_ob):\n            ob_new = False\n        else:\n            self.cur_ob = ob\n            ob_new = True\n        \n        if not self.any_cuts:\n            if self.opt.spcomm.new_cuts:\n                self.any_cuts = True\n\n        ## if its the second time or more with this IB, we'll only check\n        ## if the last improved the OB, or if the OB is new itself (from somewhere else)\n        check = (self.check_bound_iterations is not None) and self.any_cuts and ( \\\n                (self.iter_at_cur_ib == self.check_bound_iterations) or \\\n                (self.iter_at_cur_ib > self.check_bound_iterations and ob_new) or \\\n                ((self.iter_since_last_check%self.check_bound_iterations == 0) and self.opt.spcomm.new_cuts))\n                # if there hasn't been OB movement, check every so often if we have new cuts\n        if check:\n            global_toc(f\"Attempting to update Best Bound with CrossScenarioExtension\")\n            self._check_bound()\n            self.opt.spcomm.new_cuts = False\n            self.iter_since_last_check = 0\n\n    def enditer(self):\n        pass\n\n    def post_everything(self):\n        pass",
  "def __init__(self, spbase_object):\n        super().__init__(spbase_object)\n\n        opt = self.opt\n        if 'cross_scen_options' in opt.options and \\\n                'check_bound_improve_iterations' in opt.options['cross_scen_options']:\n            self.check_bound_iterations = opt.options['cross_scen_options']['check_bound_improve_iterations']\n        else:\n            self.check_bound_iterations = None\n\n        self.cur_ib = None\n        self.iter_at_cur_ib = 1\n\n        self.cur_ob = None\n\n        self.reenable_W = None\n        self.reenable_prox = None\n\n        self.any_cuts = False\n        self.iter_since_last_check = 0",
  "def _disable_W_and_prox(self):\n        assert self.reenable_W is None\n        assert self.reenable_prox is None\n        ## hold the PH object harmless\n        opt = self.opt\n        self.reenable_W = False\n        self.reenable_prox = False\n        if not opt.W_disabled and not opt.prox_disabled:\n            opt.disable_W_and_prox()\n            self.reenable_W = True\n            self.reenable_prox = True \n        elif not opt.W_disabled:\n            opt._disable_W()\n            self.eenable_W = True\n        elif not opt.prox_disabled:\n            opt._disable_prox()\n            self.reenable_prox = True",
  "def _enable_W_and_prox(self):\n        assert self.reenable_W is not None\n        assert self.reenable_prox is not None\n        \n        opt = self.opt\n        if self.reenable_W and self.reenable_prox:\n            opt.reenable_W_and_prox()\n        elif self.reenable_W:\n            opt._reenable_W()\n        elif self.reenable_prox:\n            opt._reenable_prox()\n\n        self.reenable_W = None\n        self.reenable_prox = None",
  "def _check_bound(self):\n        opt = self.opt\n\n        chached_ph_obj = dict()\n\n        for k,s in opt.local_subproblems.items():\n            phobj = find_active_objective(s)\n            phobj.deactivate()\n            chached_ph_obj[k] = phobj\n            s._mpisppy_model.EF_Obj.activate()\n\n        teeme = (\n            \"tee-rank0-solves\" in opt.options\n             and opt.options[\"tee-rank0-solves\"]\n        )\n        opt.solve_loop(\n                solver_options=opt.current_solver_options,\n                dtiming=opt.options[\"display_timing\"],\n                gripe=True,\n                disable_pyomo_signal_handling=False,\n                tee=teeme,\n                verbose=opt.options[\"verbose\"],\n        )\n\n        local_obs = np.fromiter((s._mpisppy_data.outer_bound for s in opt.local_subproblems.values()),\n                                dtype=\"d\", count=len(opt.local_subproblems))\n\n        local_ob = np.empty(1)\n        if opt.is_minimizing:\n            local_ob[0] = local_obs.max()\n        else:\n            local_ob[0] = local_obs.min()\n\n        global_ob = np.empty(1)\n\n        if opt.is_minimizing:\n            opt.mpicomm.Allreduce(local_ob, global_ob, op=mpi.MAX)\n        else: \n            opt.mpicomm.Allreduce(local_ob, global_ob, op=mpi.MIN)\n\n        #print(f\"CrossScenarioExtension OB: {global_ob[0]}\")\n\n        opt.spcomm.BestOuterBound = opt.spcomm.OuterBoundUpdate(global_ob[0], char='C')\n\n        for k,s in opt.local_subproblems.items():\n            s._mpisppy_model.EF_Obj.deactivate()\n            chached_ph_obj[k].activate()",
  "def pre_iter0(self):\n        if self.opt.multistage:\n            raise RuntimeError(\"CrossScenarioExtension does not support \"\n                               \"multi-stage problems at this time\")\n        ## hack as this provides logs for outer bounds\n        self.opt.spcomm.has_outerbound_spokes = True",
  "def post_iter0(self):\n        opt = self.opt\n        # NOTE: the LShaped code negates the objective, so\n        #       we do the same here for consistency\n        if 'cross_scen_options' in opt.options and \\\n                'valid_eta_bound' in opt.options['cross_scen_options']:\n            valid_eta_bound = opt.options['cross_scen_options']['valid_eta_bound']\n            if not opt.is_minimizing:\n                _eta_init = { k: -v for k,v in valid_eta_bound.items() }\n            else:\n                _eta_init = valid_eta_bound\n            _eta_bounds = lambda m,k : (_eta_init[k], None)\n        else:\n            lb = (-sys.maxsize - 1) * 1. / len(opt.all_scenario_names)\n            _eta_init = lambda m,k : lb\n            _eta_bounds = lambda m,k : (lb, None)\n\n        # eta is attached to each subproblem, regardless of bundles\n        bundling = opt.bundling\n        for k,s in opt.local_subproblems.items():\n            s._mpisppy_model.eta = pyo.Var(opt.all_scenario_names, initialize=_eta_init, bounds=_eta_bounds)\n            if sputils.is_persistent(s._solver_plugin):\n                for var in s._mpisppy_model.eta.values():\n                    s._solver_plugin.add_var(var)\n            if bundling: ## create a refence to eta on each subproblem\n                for sn in s.scen_list:\n                    scenario = opt.local_scenarios[sn]\n                    scenario._mpisppy_model.eta = { k : s._mpisppy_model.eta[k] for k in opt.all_scenario_names }\n\n        ## hold the PH object harmless\n        self._disable_W_and_prox()\n        \n        for k,s in opt.local_subproblems.items():\n\n            obj = find_active_objective(s)\n\n            repn = generate_standard_repn(obj.expr, quadratic=True)\n            if len(repn.nonlinear_vars) > 0:\n                raise ValueError(\"CrossScenario does not support models with nonlinear objective functions\")\n\n            if bundling:\n                ## NOTE: this is slighly wasteful, in that for a bundle\n                ##       the first-stage cost appears len(s.scen_list) times\n                ##       If this really made a difference, we could use s.ref_vars\n                ##       to do the substitution\n                nonant_vardata_list = list()\n                for sn in s.scen_list:\n                    nonant_vardata_list.extend( \\\n                            opt.local_scenarios[sn]._mpisppy_node_list[0].nonant_vardata_list)\n            else:\n                nonant_vardata_list = s._mpisppy_node_list[0].nonant_vardata_list\n\n            nonant_ids = set((id(var) for var in nonant_vardata_list))\n\n            linear_coefs = list(repn.linear_coefs)\n            linear_vars = list(repn.linear_vars)\n\n            quadratic_coefs = list(repn.quadratic_coefs)\n\n            # adjust coefficients by scenario/bundle probability\n            scen_prob = s._mpisppy_probability\n            for i,var in enumerate(repn.linear_vars):\n                if id(var) not in nonant_ids:\n                    linear_coefs[i] *= scen_prob\n\n            for i,(x,y) in enumerate(repn.quadratic_vars):\n                # only multiply through once\n                if id(x) not in nonant_ids:\n                    quadratic_coefs[i] *= scen_prob\n                elif id(y) not in nonant_ids:\n                    quadratic_coefs[i] *= scen_prob\n\n            # NOTE: the LShaped code negates the objective, so\n            #       we do the same here for consistency\n            if not opt.is_minimizing:\n                for i,coef in enumerate(linear_coefs):\n                    linear_coefs[i] = -coef\n                for i,coef in enumerate(quadratic_coefs):\n                    quadratic_coefs[i] = -coef\n\n            # add the other etas\n            if bundling:\n                these_scenarios = set(s.scen_list)\n            else:\n                these_scenarios = [k]\n\n            eta_scenarios = list()\n            for sn in opt.all_scenario_names:\n                if sn not in these_scenarios:\n                    linear_coefs.append(1)\n                    linear_vars.append(s._mpisppy_model.eta[sn])\n                    eta_scenarios.append(sn)\n\n            expr = LinearExpression(constant=repn.constant, linear_coefs=linear_coefs,\n                                    linear_vars=linear_vars)\n\n            if repn.quadratic_vars:\n                expr += pyo.quicksum(\n                    (coef*x*y for coef,(x,y) in zip(quadratic_coefs, repn.quadratic_vars))\n                )\n\n            s._mpisppy_model.EF_obj = pyo.Expression(expr=expr)\n\n            if opt.is_minimizing:\n                s._mpisppy_model.EF_Obj = pyo.Objective(expr=s._mpisppy_model.EF_obj, sense=pyo.minimize)\n            else:\n                s._mpisppy_model.EF_Obj = pyo.Objective(expr=-s._mpisppy_model.EF_obj, sense=pyo.maximize)\n            s._mpisppy_model.EF_Obj.deactivate()\n\n            # add cut constraint dicts\n            s._mpisppy_model.benders_cuts = pyo.Constraint(pyo.Any)\n            s._mpisppy_model.inner_bound_constr = pyo.Constraint(pyo.Any)\n\n        self._enable_W_and_prox()\n\n        # try to get the initial eta LB cuts\n        # (may not be available)\n        opt.spcomm.get_from_cross_cuts()",
  "def miditer(self):\n        self.iter_since_last_check += 1\n\n        ib = self.opt.spcomm.BestInnerBound\n        if ib != self.cur_ib:\n            self.cur_ib = ib\n            self.iter_at_cur_ib = 1\n        elif self.cur_ib is not None and math.isfinite(self.cur_ib):\n            self.iter_at_cur_ib += 1\n\n        ob = self.opt.spcomm.BestOuterBound\n        if self.cur_ob is not None and math.isclose(ob, self.cur_ob):\n            ob_new = False\n        else:\n            self.cur_ob = ob\n            ob_new = True\n        \n        if not self.any_cuts:\n            if self.opt.spcomm.new_cuts:\n                self.any_cuts = True\n\n        ## if its the second time or more with this IB, we'll only check\n        ## if the last improved the OB, or if the OB is new itself (from somewhere else)\n        check = (self.check_bound_iterations is not None) and self.any_cuts and ( \\\n                (self.iter_at_cur_ib == self.check_bound_iterations) or \\\n                (self.iter_at_cur_ib > self.check_bound_iterations and ob_new) or \\\n                ((self.iter_since_last_check%self.check_bound_iterations == 0) and self.opt.spcomm.new_cuts))\n                # if there hasn't been OB movement, check every so often if we have new cuts\n        if check:\n            global_toc(f\"Attempting to update Best Bound with CrossScenarioExtension\")\n            self._check_bound()\n            self.opt.spcomm.new_cuts = False\n            self.iter_since_last_check = 0",
  "def enditer(self):\n        pass",
  "def post_everything(self):\n        pass",
  "class XhatSpecific(mpisppy.extensions.xhatbase.XhatBase):\n    \"\"\"\n    Args:\n        spo (SPOpt object): the calling object\n        rank (int): mpi process rank of currently running process\n    \"\"\"\n    def __init__(self, spo):\n        super().__init__(spo)\n        self.options = spo.options[\"xhat_specific_options\"]\n        self.solver_options = self.options[\"xhat_solver_options\"]\n        self.keep_solution = True\n        if ('keep_solution' in self.options) and (not self.options['keep_solution']):\n            self.keep_solution = False\n\n    #==========\n    def xhat_tryit(self,\n                   xhat_scenario_dict,\n                   verbose=False,\n                   restore_nonants=True):\n        \"\"\"If your rank has\n        the chosen guy, bcast, if not, recieve the bcast. In any event, fix the vars\n        at the bcast values and see if it is feasible. \n\n        Args:\n            xhat_scenario_dict (string): keys are nodes; values are scen names\n            verbose (boolean): controls debugging output\n        Returns:\n            xhatobjective (float or None): the objective function\n                or None if one could not be obtained.\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"  xhat_specific: \" + msg)\n\n        obj = None\n        sname = None\n\n        _vb(\"Enter XhatSpecific.xhat_tryit to try: \"+str(xhat_scenario_dict))\n\n        _vb(\"   Solver options=\"+str(self.solver_options))\n        obj = self._try_one(xhat_scenario_dict,\n                            solver_options=self.solver_options,\n                            verbose=False,\n                            restore_nonants=restore_nonants)\n        if obj is None:\n            _vb(\"Infeasible\")\n        else:\n            _vb(\"Feasible, returning \" + str(obj))\n\n        return obj\n\n    def pre_iter0(self):\n        pass\n\n    def post_iter0(self):\n        # a little bit silly\n        self.comms = self.opt.comms\n        \n    def miditer(self):\n        pass\n\n    def enditer(self):\n        pass\n\n    def post_everything(self):\n        # if we're keeping the solution, we *do not* restore the nonants\n        restore_nonants = not self.keep_solution\n        self.opt.disable_W_and_prox()\n        xhat_scenario_dict = self.options[\"xhat_scenario_dict\"]\n        obj = self.xhat_tryit(xhat_scenario_dict,\n                              verbose=self.verbose,\n                              restore_nonants=restore_nonants)\n        self.opt.reenable_W_and_prox()\n        # to make available to tester\n        self._xhat_specific_obj_final = obj\n        self.xhat_common_post_everything(\"xhat specified scenario\", obj, xhat_scenario_dict, restore_nonants)",
  "def __init__(self, spo):\n        super().__init__(spo)\n        self.options = spo.options[\"xhat_specific_options\"]\n        self.solver_options = self.options[\"xhat_solver_options\"]\n        self.keep_solution = True\n        if ('keep_solution' in self.options) and (not self.options['keep_solution']):\n            self.keep_solution = False",
  "def xhat_tryit(self,\n                   xhat_scenario_dict,\n                   verbose=False,\n                   restore_nonants=True):\n        \"\"\"If your rank has\n        the chosen guy, bcast, if not, recieve the bcast. In any event, fix the vars\n        at the bcast values and see if it is feasible. \n\n        Args:\n            xhat_scenario_dict (string): keys are nodes; values are scen names\n            verbose (boolean): controls debugging output\n        Returns:\n            xhatobjective (float or None): the objective function\n                or None if one could not be obtained.\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"  xhat_specific: \" + msg)\n\n        obj = None\n        sname = None\n\n        _vb(\"Enter XhatSpecific.xhat_tryit to try: \"+str(xhat_scenario_dict))\n\n        _vb(\"   Solver options=\"+str(self.solver_options))\n        obj = self._try_one(xhat_scenario_dict,\n                            solver_options=self.solver_options,\n                            verbose=False,\n                            restore_nonants=restore_nonants)\n        if obj is None:\n            _vb(\"Infeasible\")\n        else:\n            _vb(\"Feasible, returning \" + str(obj))\n\n        return obj",
  "def pre_iter0(self):\n        pass",
  "def post_iter0(self):\n        # a little bit silly\n        self.comms = self.opt.comms",
  "def miditer(self):\n        pass",
  "def enditer(self):\n        pass",
  "def post_everything(self):\n        # if we're keeping the solution, we *do not* restore the nonants\n        restore_nonants = not self.keep_solution\n        self.opt.disable_W_and_prox()\n        xhat_scenario_dict = self.options[\"xhat_scenario_dict\"]\n        obj = self.xhat_tryit(xhat_scenario_dict,\n                              verbose=self.verbose,\n                              restore_nonants=restore_nonants)\n        self.opt.reenable_W_and_prox()\n        # to make available to tester\n        self._xhat_specific_obj_final = obj\n        self.xhat_common_post_everything(\"xhat specified scenario\", obj, xhat_scenario_dict, restore_nonants)",
  "def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print(\"  xhat_specific: \" + msg)",
  "class XhatLooper(mpisppy.extensions.xhatbase.XhatBase):\n    \"\"\"\n    Args:\n        opt (SPBase object): problem that we are bounding\n        rank (int): mpi process rank of currently running process\n    \"\"\"\n    def __init__(self, ph):\n        super().__init__(ph)\n        self.options = ph.options[\"xhat_looper_options\"]\n        self.solver_options = self.options[\"xhat_solver_options\"]\n        self._xhat_looper_obj_final = None\n        self.keep_solution = True\n        if ('keep_solution' in self.options) and (not self.options['keep_solution']):\n            self.keep_solution = False\n\n    #==========\n    def xhat_looper(self,\n                    scen_limit=1,\n                    seed=None,\n                    verbose=False,\n                    restore_nonants=True):\n        \"\"\"Loop over some number of the global scenarios; if your rank has\n        the chosen guy, bcast, if not, recieve the bcast. In any event, fix the vars\n        at the bcast values and see if it is feasible. If so, stop and \n        leave the nonants fixed.\n\n        Args:\n            scen_limit (int): number of scenarios to try\n            seed (int): if none, loop starting at first scen; o.w. randomize\n            verbose (boolean): controls debugging output\n            restore_nonants (bool): if True, restores the nonants to their original\n                                    values in all scenarios. If False, leaves the\n                                    nonants as they are in the tried scenario\n        Returns:\n            xhojbective (float or None), sname (string): the objective function\n                or None if one could not be obtained.\n        NOTE:\n            If options has an append_file_name, write to it\n            Also attach the resulting bound to the object\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"    rank {} xhat_looper: {}\".\\\n                       format(self.cylinder_rank,msg))\n        obj = None\n        sname = None\n        snumlists = dict()\n        llim = min(scen_limit, len(self.opt.all_scenario_names))\n        _vb(\"Enter xhat_looper to try \"+str(llim)+\" scenarios.\")\n        # The tedious task of collecting the tree information for\n        # local scenario tree nodes (maybe move to the constructor)\n        for k, s in self.opt.local_scenarios.items():\n            for nnode in s._mpisppy_node_list:\n                ndn = nnode.name\n                nsize = self.comms[ndn].size\n                if seed is None:\n                    snumlists[ndn] = [i % nsize for i in range(llim)]\n                else:\n                    print (\"need a random permutation in snumlist xxxx quitting\")\n                    quit()\n        \n        self.opt._save_nonants() # to cache for use in fixing\n        # for the moment (dec 2019) treat two-stage as special\n        if len(snumlists) == 1:\n            for snum in snumlists[\"ROOT\"]:\n                sname = self.opt.all_scenario_names[snum]\n                _vb(\"Trying scenario \"+sname)\n                _vb(\"   Solver options=\"+str(self.solver_options))\n                snamedict = {\"ROOT\": sname}\n                obj = self._try_one(snamedict,\n                                    solver_options=self.solver_options,\n                                    verbose=False,\n                                    restore_nonants=restore_nonants)\n                if obj is None:\n                    _vb(\"    Infeasible\")\n                else:\n                    _vb(\"    Feasible, returning \" + str(obj))\n                    break\n        else:\n            raise RuntimeError(\"xhatlooper cannot do multi-stage\")            \n\n        if \"append_file_name\" in self.options and self.opt.cylinder_rank == 0:\n            with open(self.options[\"append_file_name\"], \"a\") as f:\n                f.write(\", \"+str(obj))\n\n        self.xhatlooper_obj = obj\n        return obj, snamedict\n\n    def pre_iter0(self):\n        if self.opt.multistage:\n            raise RuntimeError(\"xhatlooper cannot do multi-stage\")            \n\n    def post_iter0(self):\n        # a little bit silly\n        self.comms = self.opt.comms\n        \n    def miditer(self):\n        pass\n\n    def enditer(self):\n        pass\n\n    def post_everything(self):\n        restore_nonants = not self.keep_solution\n\n        self.opt.disable_W_and_prox()\n        obj, snamedict = self.xhat_looper(\n            scen_limit=self.options[\"scen_limit\"],\n            verbose=self.verbose,\n            restore_nonants=restore_nonants,\n        )\n        self.opt.reenable_W_and_prox()\n        # \"secret menu\" way to see the value in a script\n        self._xhat_looper_obj_final = obj\n        self.xhat_common_post_everything(\"xhatlooper\", obj, snamedict, restore_nonants)",
  "def __init__(self, ph):\n        super().__init__(ph)\n        self.options = ph.options[\"xhat_looper_options\"]\n        self.solver_options = self.options[\"xhat_solver_options\"]\n        self._xhat_looper_obj_final = None\n        self.keep_solution = True\n        if ('keep_solution' in self.options) and (not self.options['keep_solution']):\n            self.keep_solution = False",
  "def xhat_looper(self,\n                    scen_limit=1,\n                    seed=None,\n                    verbose=False,\n                    restore_nonants=True):\n        \"\"\"Loop over some number of the global scenarios; if your rank has\n        the chosen guy, bcast, if not, recieve the bcast. In any event, fix the vars\n        at the bcast values and see if it is feasible. If so, stop and \n        leave the nonants fixed.\n\n        Args:\n            scen_limit (int): number of scenarios to try\n            seed (int): if none, loop starting at first scen; o.w. randomize\n            verbose (boolean): controls debugging output\n            restore_nonants (bool): if True, restores the nonants to their original\n                                    values in all scenarios. If False, leaves the\n                                    nonants as they are in the tried scenario\n        Returns:\n            xhojbective (float or None), sname (string): the objective function\n                or None if one could not be obtained.\n        NOTE:\n            If options has an append_file_name, write to it\n            Also attach the resulting bound to the object\n        \"\"\"\n        def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"    rank {} xhat_looper: {}\".\\\n                       format(self.cylinder_rank,msg))\n        obj = None\n        sname = None\n        snumlists = dict()\n        llim = min(scen_limit, len(self.opt.all_scenario_names))\n        _vb(\"Enter xhat_looper to try \"+str(llim)+\" scenarios.\")\n        # The tedious task of collecting the tree information for\n        # local scenario tree nodes (maybe move to the constructor)\n        for k, s in self.opt.local_scenarios.items():\n            for nnode in s._mpisppy_node_list:\n                ndn = nnode.name\n                nsize = self.comms[ndn].size\n                if seed is None:\n                    snumlists[ndn] = [i % nsize for i in range(llim)]\n                else:\n                    print (\"need a random permutation in snumlist xxxx quitting\")\n                    quit()\n        \n        self.opt._save_nonants() # to cache for use in fixing\n        # for the moment (dec 2019) treat two-stage as special\n        if len(snumlists) == 1:\n            for snum in snumlists[\"ROOT\"]:\n                sname = self.opt.all_scenario_names[snum]\n                _vb(\"Trying scenario \"+sname)\n                _vb(\"   Solver options=\"+str(self.solver_options))\n                snamedict = {\"ROOT\": sname}\n                obj = self._try_one(snamedict,\n                                    solver_options=self.solver_options,\n                                    verbose=False,\n                                    restore_nonants=restore_nonants)\n                if obj is None:\n                    _vb(\"    Infeasible\")\n                else:\n                    _vb(\"    Feasible, returning \" + str(obj))\n                    break\n        else:\n            raise RuntimeError(\"xhatlooper cannot do multi-stage\")            \n\n        if \"append_file_name\" in self.options and self.opt.cylinder_rank == 0:\n            with open(self.options[\"append_file_name\"], \"a\") as f:\n                f.write(\", \"+str(obj))\n\n        self.xhatlooper_obj = obj\n        return obj, snamedict",
  "def pre_iter0(self):\n        if self.opt.multistage:\n            raise RuntimeError(\"xhatlooper cannot do multi-stage\")",
  "def post_iter0(self):\n        # a little bit silly\n        self.comms = self.opt.comms",
  "def miditer(self):\n        pass",
  "def enditer(self):\n        pass",
  "def post_everything(self):\n        restore_nonants = not self.keep_solution\n\n        self.opt.disable_W_and_prox()\n        obj, snamedict = self.xhat_looper(\n            scen_limit=self.options[\"scen_limit\"],\n            verbose=self.verbose,\n            restore_nonants=restore_nonants,\n        )\n        self.opt.reenable_W_and_prox()\n        # \"secret menu\" way to see the value in a script\n        self._xhat_looper_obj_final = obj\n        self.xhat_common_post_everything(\"xhatlooper\", obj, snamedict, restore_nonants)",
  "def _vb(msg):\n            if verbose and self.cylinder_rank == 0:\n                print (\"    rank {} xhat_looper: {}\".\\\n                       format(self.cylinder_rank,msg))",
  "class Extension:\n    \"\"\" Abstract base class for extensions to general SPOpt objects.\n    \"\"\"\n    def __init__(self, spopt_object):\n        self.opt = spopt_object\n\n    def pre_solve(self, subproblem):\n        '''\n        Method called before every subproblem solve\n\n        Inputs\n        ------\n        subproblem : Pyomo subproblem (could be a scenario or bundle)\n\n        Returns\n        -------\n        None\n        '''\n        pass\n\n    def post_solve(self, subproblem, results):\n        '''\n        Method called after every subproblem solve\n\n        Inputs\n        ------\n        subproblem : Pyomo subproblem (could be a scenario or bundle)\n        results : Pyomo results object from initial solve or None if solve failed\n\n        Returns\n        -------\n        results : Pyomo results objects from most recent solve\n        '''\n        return results\n\n    def pre_solve_loop(self):\n        ''' Method called before every solve loop within\n            mpisppy.spot.SPOpt.solve_loop()\n        '''\n        pass\n\n    def post_solve_loop(self):\n        ''' Method called after every solve loop within\n            mpisppy.spot.SPOpt.solve_loop()\n        '''\n        pass\n\n    def pre_iter0(self):\n        ''' Method called at the end of PH_Prep().\n            When this method is called, all scenarios have been created, and\n            the dual/prox terms have been attached to the objective, but the\n            solvers have not yet been created.\n        '''\n        pass\n\n    def post_iter0(self):\n        ''' Method called after the first PH iteration.\n            When this method is called, one call to solve_loop() has been\n            completed, and we have ensured that none of the models are\n            infeasible. The rho_setter, if present, has not yet been applied.\n        '''\n        pass\n\n    def post_iter0_after_sync(self):\n        ''' Method called after the first PH iteration, after the\n            synchronization of sending messages between cylinders\n            has completed.\n        '''\n        pass\n\n    def miditer(self):\n        ''' Method called after x-bar has been computed and the dual weights\n            have been updated, but before solve_loop().\n            If a converger is present, this method is called between the\n            convergence_value() method and the is_converged() method.\n        '''\n        pass\n\n    def enditer(self):\n        ''' Method called after the solve_loop(), but before the next x-bar and\n            weight update.\n        '''\n        pass\n\n    def enditer_after_sync(self):\n        ''' Method called after the solve_loop(), after the\n            synchronization of sending messages between cylinders\n            has completed.\n        '''\n        pass\n\n    def post_everything(self):\n        ''' Method called after the termination of the algorithm.\n            This method is called after the scenario_denouement, if a\n            denouement is present. This function will not begin on any rank\n            within self.opt.mpicomm until the scenario_denouement has completed\n            on all other ranks.\n        '''\n        pass",
  "class MultiExtension(Extension):\n    \"\"\" Container for all the extension classes we are using.\n        Also grabs ph and rank, so ad hoc calls (e.g., lagrangian) can use them.\n    \"\"\"\n    def __init__(self, ph, ext_classes):\n        super().__init__(ph)\n        self.extdict = dict()\n\n        # Construct multiple extension objects\n        for constr in ext_classes:\n            name = constr.__name__\n            self.extdict[name] = constr(ph)\n\n    def pre_solve(self, subproblem):\n        for lobject in self.extdict.values():\n            lobject.pre_solve(subproblem)\n\n    def post_solve(self, subproblem, results):\n        for lobject in self.extdict.values():\n            results = lobject.post_solve(subproblem, results)\n        return results\n\n    def pre_solve_loop(self):\n        for lobject in self.extdict.values():\n            lobject.pre_solve_loop()\n\n    def post_solve_loop(self):\n        for lobject in self.extdict.values():\n            lobject.post_solve_loop()\n\n    def pre_iter0(self):\n        for lobject in self.extdict.values():\n            lobject.pre_iter0()\n\n    def post_iter0(self):\n        for lobject in self.extdict.values():\n            lobject.post_iter0()\n\n    def post_iter0_after_sync(self):\n        for lobject in self.extdict.values():\n            lobject.post_iter0_after_sync()\n\n    def miditer(self):\n        for lobject in self.extdict.values():\n            lobject.miditer()\n\n    def enditer(self):\n        for lobject in self.extdict.values():\n            lobject.enditer()\n\n    def enditer_after_sync(self):\n        for lobject in self.extdict.values():\n            lobject.enditer_after_sync()\n\n    def post_everything(self):\n        for lobject in self.extdict.values():\n            lobject.post_everything()",
  "def __init__(self, spopt_object):\n        self.opt = spopt_object",
  "def pre_solve(self, subproblem):\n        '''\n        Method called before every subproblem solve\n\n        Inputs\n        ------\n        subproblem : Pyomo subproblem (could be a scenario or bundle)\n\n        Returns\n        -------\n        None\n        '''\n        pass",
  "def post_solve(self, subproblem, results):\n        '''\n        Method called after every subproblem solve\n\n        Inputs\n        ------\n        subproblem : Pyomo subproblem (could be a scenario or bundle)\n        results : Pyomo results object from initial solve or None if solve failed\n\n        Returns\n        -------\n        results : Pyomo results objects from most recent solve\n        '''\n        return results",
  "def pre_solve_loop(self):\n        ''' Method called before every solve loop within\n            mpisppy.spot.SPOpt.solve_loop()\n        '''\n        pass",
  "def post_solve_loop(self):\n        ''' Method called after every solve loop within\n            mpisppy.spot.SPOpt.solve_loop()\n        '''\n        pass",
  "def pre_iter0(self):\n        ''' Method called at the end of PH_Prep().\n            When this method is called, all scenarios have been created, and\n            the dual/prox terms have been attached to the objective, but the\n            solvers have not yet been created.\n        '''\n        pass",
  "def post_iter0(self):\n        ''' Method called after the first PH iteration.\n            When this method is called, one call to solve_loop() has been\n            completed, and we have ensured that none of the models are\n            infeasible. The rho_setter, if present, has not yet been applied.\n        '''\n        pass",
  "def post_iter0_after_sync(self):\n        ''' Method called after the first PH iteration, after the\n            synchronization of sending messages between cylinders\n            has completed.\n        '''\n        pass",
  "def miditer(self):\n        ''' Method called after x-bar has been computed and the dual weights\n            have been updated, but before solve_loop().\n            If a converger is present, this method is called between the\n            convergence_value() method and the is_converged() method.\n        '''\n        pass",
  "def enditer(self):\n        ''' Method called after the solve_loop(), but before the next x-bar and\n            weight update.\n        '''\n        pass",
  "def enditer_after_sync(self):\n        ''' Method called after the solve_loop(), after the\n            synchronization of sending messages between cylinders\n            has completed.\n        '''\n        pass",
  "def post_everything(self):\n        ''' Method called after the termination of the algorithm.\n            This method is called after the scenario_denouement, if a\n            denouement is present. This function will not begin on any rank\n            within self.opt.mpicomm until the scenario_denouement has completed\n            on all other ranks.\n        '''\n        pass",
  "def __init__(self, ph, ext_classes):\n        super().__init__(ph)\n        self.extdict = dict()\n\n        # Construct multiple extension objects\n        for constr in ext_classes:\n            name = constr.__name__\n            self.extdict[name] = constr(ph)",
  "def pre_solve(self, subproblem):\n        for lobject in self.extdict.values():\n            lobject.pre_solve(subproblem)",
  "def post_solve(self, subproblem, results):\n        for lobject in self.extdict.values():\n            results = lobject.post_solve(subproblem, results)\n        return results",
  "def pre_solve_loop(self):\n        for lobject in self.extdict.values():\n            lobject.pre_solve_loop()",
  "def post_solve_loop(self):\n        for lobject in self.extdict.values():\n            lobject.post_solve_loop()",
  "def pre_iter0(self):\n        for lobject in self.extdict.values():\n            lobject.pre_iter0()",
  "def post_iter0(self):\n        for lobject in self.extdict.values():\n            lobject.post_iter0()",
  "def post_iter0_after_sync(self):\n        for lobject in self.extdict.values():\n            lobject.post_iter0_after_sync()",
  "def miditer(self):\n        for lobject in self.extdict.values():\n            lobject.miditer()",
  "def enditer(self):\n        for lobject in self.extdict.values():\n            lobject.enditer()",
  "def enditer_after_sync(self):\n        for lobject in self.extdict.values():\n            lobject.enditer_after_sync()",
  "def post_everything(self):\n        for lobject in self.extdict.values():\n            lobject.post_everything()",
  "def write_W_to_file(PHB, fname, sep_files=False):\n    '''\n    Args:\n        PHB (PHBase object) -- Where the W values live\n        fname (str) -- name of file to which we write.\n        sep_files (bool, optional) -- If True, one file will be written for\n            each scenario, rather than one main file. The names of the files\n            are the names of the scenarios.\n\n    Notes:\n        All ranks pass their information to rank 0, which then writes a single\n        file. This can apparently be accomplished using Collective MPI I/O (see\n        https://mpi4py.readthedocs.io/en/stable/tutorial.html#mpi-io), but I am\n        lazy and doing this for now.\n    '''\n\n    if (sep_files):\n        for (sname, scenario) in PHB.local_scenarios.items():\n            scenario_Ws = {var.name: pyo.value(scenario._mpisppy_model.W[node.name, ix])\n                for node in scenario._mpisppy_node_list\n                for (ix, var) in enumerate(node.nonant_vardata_list)}\n            scenario_fname = os.path.join(fname, sname + '_weights.csv')\n            with open(scenario_fname, 'w') as f:\n                for (vname, val) in scenario_Ws.items():\n                    row = ','.join([vname, str(val)]) + '\\n'\n                    f.write(row)\n    else:\n        local_Ws = {(sname, var.name): pyo.value(scenario._mpisppy_model.W[node.name, ix])\n                    for (sname, scenario) in PHB.local_scenarios.items()\n                    for node in scenario._mpisppy_node_list\n                    for (ix, var) in enumerate(node.nonant_vardata_list)}\n        comm = PHB.comms['ROOT']\n        Ws = comm.gather(local_Ws, root=0)\n        if (PHB.cylinder_rank == 0):\n            with open(fname, 'a') as f:\n                for W in Ws:\n                    for (key, val) in W.items():\n                        sname, vname = key[0], key[1]\n                        row = ','.join([sname, vname, str(val)]) + '\\n'\n                        f.write(row)",
  "def set_W_from_file(fname, PHB, rank, sep_files=False, disable_check=False):\n    ''' \n    Args:\n        fname (str) -- if sep_files=False, file containing the dual weights.\n            Otherwise, path of the directory containing the dual weight files\n            (one per scenario).\n        PHB (PHBase object) -- Where the W values will be put\n        rank (int) -- rank number\n        sep_files (bool, optional) -- If True, attempt to read weights from\n            individual files, one per scenario. The files must be contained in\n            the same directory, and must be named <sname>_weights.csv for each\n            scenario name <sname>.\n    \n    Notes:\n        Calls _check_W, which ensures that all required values were specified,\n        and that the specified weights satisfy the dual feasibility condition \n        sum_{s\\in S} p_s * w_s = 0.\n    '''\n    scenario_names_local  = list(PHB.local_scenarios.keys())\n    scenario_names_global = PHB.all_scenario_names\n\n    if (sep_files):\n        w_val_dict = dict()\n        for sname in scenario_names_local:\n            scenario_fname = os.path.join(fname, sname + '_weights.csv')\n            w_val_dict[sname] = _parse_W_csv_single(scenario_fname)\n    else:\n        w_val_dict = _parse_W_csv(fname, scenario_names_local,\n                                    scenario_names_global, rank)\n\n    if not disable_check:\n        _check_W(w_val_dict, PHB, rank)\n\n    mp = {(sname, var.name): (node.name, ix)\n            for (sname, scenario) in PHB.local_scenarios.items()\n            for node in scenario._mpisppy_node_list\n            for (ix,var) in enumerate(node.nonant_vardata_list)}\n\n    for (sname, d) in w_val_dict.items():\n        for vname in d.keys():\n            scenario = PHB.local_scenarios[sname]\n            node_name, ix = mp[sname, vname]\n            scenario._mpisppy_model.W[node_name, ix] = w_val_dict[sname][vname]",
  "def _parse_W_csv_single(fname):\n    ''' Read a file containing the weights for a single scenario. The file must\n        be formatted as \n\n        variable_name,variable_value\n\n        (comma separated). Lines beginning with a \"#\" are treated as comments\n        and ignored.\n    '''\n    if (not os.path.exists(fname)):\n        raise RuntimeError('Could not find file {fn}'.format(fn=fname))\n    results = dict()\n    with open(fname, 'r') as f:\n        for line in f:\n            if (line.startswith('#')):\n                continue\n            line  = line.split(',')\n            vname = ','.join(line[:-1])\n            wval  = float(line[-1])\n            results[vname] = wval\n    return results",
  "def _parse_W_csv(fname, scenario_names_local, scenario_names_global, rank):\n    ''' Read a csv file containing weight information. \n        \n        Args:\n            fname (str) -- Filename of csv file to read\n            scenario_names_local (list of str) -- List of local scenario names\n            scenario_names_global (list of str) -- List of global scenario\n                names (i.e. all the scenario names in the entire model across\n                all ranks--each PHBase object stores this information).\n\n        Return:\n            results (dict) -- Doubly-nested dict mapping \n                results[scenario_name][var_name] --> weight value (float)\n    \n        Notes:\n            This function is only called if sep_files=False, i.e., if all of\n            the weights are stored in a single root file. The file must be\n            formatted as:\n\n            scenario_name,variable_name,weight_value\n\n            Rows that begin with a \"#\" character are treated as comments.\n            The variable names _may_ contain commas (confusing, but simpler for\n            the user)\n\n            Raises a RuntimeError if there are any missing scenarios. Prints a\n            warning if there are any extra scenarios.\n\n            When this function returns, we are certain that \n                results.keys() == PHB.local_scenarios.keys()\n\n            When run in parallel, this method requires multiple ranks to open\n            and read from the same file simultaneously. Apparently there are\n            safer ways to do this using MPI collective communication, but since\n            all we're doing here is reading files, I'm being lazy and doing it\n            this way.\n    '''\n    results = dict()\n    seen = {name: False for name in scenario_names_local}\n    with open(fname, 'r') as f:\n        for line in f:\n            if (line.startswith('#')):\n                continue\n            line  = line.split(',')\n            sname = line[0]\n            vname = ','.join(line[1:-1])\n            wval  = float(line[-1])\n            \n            if (sname not in scenario_names_global):\n                if (rank == 0):\n                    print('WARNING: Ignoring unknown scenario name', sname)\n                continue\n            if (sname not in scenario_names_local):\n                continue\n            if (sname in results):\n                results[sname][vname] = wval\n            else:\n                seen[sname] = True\n                results[sname] = {vname: wval}\n    missing = [name for (name,is_seen) in seen.items() if not is_seen]\n    if (missing):\n        raise RuntimeError('rank ' + str(rank) +' could not find the following '\n                'scenarios in the provided weight file: ' + ', '.join(missing)) \n        \n    return results",
  "def _check_W(w_val_dict, PHB, rank):\n    '''\n    Args:\n        w_val_dict (dict) -- doubly-nested dict mapping \n            w_val_dict[scenario_name][variable_name] = weight value.\n        PHB (PHBase object) -- PHBase object\n        rank (int) -- local rank\n\n    Notes:\n        Checks for three conditions:\n         \n         1. Missing variables --> raises a RuntimeError\n         2. Extra variables --> prints a warning\n         3. Dual feasibility --> raises a RuntimeError\n    '''\n    # By this point, we are certain that\n    # w_val_dict.keys() == PHB.local_scenarios.keys()\n    for (sname, scenario) in PHB.local_scenarios.items():\n        vn_model = set([var.name for node in scenario._mpisppy_node_list\n                                 for var  in node.nonant_vardata_list])\n        vn_provided = set(w_val_dict[sname].keys())\n        diff = vn_model.difference(vn_provided)\n        if (diff):\n            raise RuntimeError(sname + ' is missing '\n                'the following variables: ' + ', '.join(list(diff)))\n        diff = vn_provided.difference(vn_model)\n        if (diff):\n            print('Removing unknown variables:', ', '.join(list(diff)))\n            for vname in diff:\n                w_val_dict[sname].pop(vname, None)\n        \n    # At this point, we are sure that every local \n    # scenario has the same set of variables\n    probs = {name: model._mpisppy_probability for (name, model) in\n                    PHB.local_scenarios.items()}\n\n    checks = dict()\n    for vname in vn_model: # Ensured vn_model = vn_provided\n        checks[vname] = sum(probs[name] * w_val_dict[name][vname]\n                            for name in PHB.local_scenarios.keys())\n\n    checks = PHB.comms['ROOT'].gather(checks, root=0)\n    if (rank == 0):\n        for vname in vn_model:\n            dual = sum(c[vname] for c in checks)\n            if (abs(dual) > 1e-7):\n                raise RuntimeError('Provided weights do not satisfy '\n                    'dual feasibility: \\sum_{scenarios} prob(s) * w(s) != 0. '\n                    'Error on variable ' + vname)",
  "def write_xbar_to_file(PHB, fname):\n    '''\n    Args:\n        PHB (PHBase object) -- Where the W values live\n        fname (str) -- name of file to which we write.\n\n    Notes:\n        Each scenario maintains its own copy of xbars. We only need to write\n        one of them to the file (i.e. no parallelism required).\n    '''\n    if (PHB.cylinder_rank != 0):\n        return\n    sname = list(PHB.local_scenarios.keys())[0]\n    scenario = PHB.local_scenarios[sname]\n    xbars = {var.name: pyo.value(scenario._mpisppy_model.xbars[node.name, ix])\n                for node in scenario._mpisppy_node_list\n                for (ix, var) in enumerate(node.nonant_vardata_list)}\n    with open(fname, 'a') as f:\n        for (var_name, val) in xbars.items():\n            row = ','.join([var_name, str(val)]) + '\\n'\n            f.write(row)",
  "def set_xbar_from_file(fname, PHB):\n    ''' Read all of the csv files in a directory and use them to populate\n        _xbars and _xsqbars\n\n    Args:\n        fname (str) -- file containing the dual weights\n        PHB (PHBase object) -- Where the W values will be put\n\n    Notes:\n        Raises a RuntimeError if the provided file is missing any values for\n        xbar (i.e. does not assume a default value for missing variables).\n    '''\n    xbar_val_dict = _parse_xbar_csv(fname)\n\n    if (PHB.cylinder_rank == 0):\n        _check_xbar(xbar_val_dict, PHB)\n\n    for (sname, scenario) in PHB.local_scenarios.items():\n        for node in scenario._mpisppy_node_list:\n            for (ix,var) in enumerate(node.nonant_vardata_list):\n                val = xbar_val_dict[var.name]\n                scenario._mpisppy_model.xbars[node.name, ix] = val\n                scenario._mpisppy_model.xsqbars[node.name, ix] = val * val",
  "def _parse_xbar_csv(fname):\n    ''' Read a csv file containing weight information. \n        \n        Args:\n            fname (str) -- Filename of csv file to read\n        Return:\n            results (dict) -- Dict mapping var_name --> variable value (float)\n    \n        Notes:\n            The file must be formatted as:\n\n            variable_name,value\n\n            Rows that begin with a \"#\" character are treated as comments.\n            The variable names _may_ contain commas (confusing, but simpler for\n            the user)\n\n            When run in parallel, this method requires multiple ranks to open\n            and read from the same file simultaneously. Apparently there are\n            safer ways to do this using MPI collective communication, but since\n            all we're doing here is reading files, I'm being lazy and doing it\n            this way.\n    '''\n    results = dict()\n    with open(fname, 'r') as f:\n        for line in f:\n            if (line.startswith('#')):\n                continue\n            line  = line.split(',')\n            vname = ','.join(line[:-1])\n            val  = float(line[-1])\n            \n            results[vname] = val\n\n    return results",
  "def _check_xbar(xbar_val_dict, PHB):\n    ''' Make sure that a value was provided for every non-anticipative\n        variable. If any extra variable values were provided in the input file,\n        this function prints a warning.\n    '''\n    sname = list(PHB.local_scenarios.keys())[0]\n    scenario = PHB.local_scenarios[sname]\n    var_names = set([var.name for node in scenario._mpisppy_node_list\n                          for var  in node.nonant_vardata_list])\n    provided_vars = set(xbar_val_dict.keys())\n    set1 = var_names.difference(provided_vars)\n    if (set1):\n        raise RuntimeError('Could not find the following required variable '\n            'values in the provided input file: ' + ', '.join([v for v in set1]))\n    set2 = provided_vars.difference(var_names)\n    if (set2):\n        print('Ignoring the following variables values provided in the '\n              'input file: ' + ', '.join([v for v in set2]))",
  "def ROOT_xbar_npy_serializer(PHB, fname):\n    \"\"\" Write the root node xbar to be read by a numpy load.\n    Args:\n        PHB (PHBase object) -- Where the W values live\n        fname (str) -- name of file to which we write.\n\n    \"\"\"\n    arbitrary_scen = PHB.local_scenarios[list(PHB.local_scenarios.keys())[0]]\n    root_nlen = arbitrary_scen._mpisppy_data.nlens[\"ROOT\"]\n    root_xbar_list = [pyo.value(arbitrary_scen._mpisppy_model.xbars[\"ROOT\", ix]) for ix in range(root_nlen)]\n    np.savetxt(fname, root_xbar_list)",
  "def fix_ef_ROOT_nonants(ef, root_nonants):\n    \"\"\" modify ef to have fixed values for the root nonants\n    Args:\n        ef (Pyomo ConcreteModel for an EF): the extensive form to modify\n        root_nonants(list): the nonant values for the root node nonants\n    \"\"\"\n    varlist = [var for (ndn,i), var in ef.ref_vars.items() if ndn == \"ROOT\"]\n    assert len(varlist) == len(root_nonants)\n    for var, vval in zip(varlist, root_nonants):\n        var.fix(vval)",
  "class LShapedCutGeneratorData(bc.BendersCutGeneratorData):\n    def __init__(self, component):\n        super().__init__(component)\n        # self.local_subproblem_count = 0\n        # self.global_subproblem_count = 0\n\n    def set_ls(self, ls):\n        self.ls = ls\n        self.global_subproblem_count = len(self.ls.all_scenario_names)\n        self._subproblem_ndx_map = dict.fromkeys(range(len(self.ls.local_scenario_names)))\n        for s in self._subproblem_ndx_map.keys():\n            self._subproblem_ndx_map[s] = self.ls.all_scenario_names.index(self.ls.local_scenario_names[s])\n        # print(self._subproblem_ndx_map)\n        self.all_root_etas = list(self.ls.root.eta.values())\n\n    def global_num_subproblems(self):\n        return self.global_subproblem_count\n\n    def add_subproblem(self, subproblem_fn, subproblem_fn_kwargs, root_eta, subproblem_solver='gurobi_persistent',\n                       relax_subproblem_cons=False, subproblem_solver_options=None):\n        # print(self._subproblem_ndx_map)\n        # self.all_root_etas.append(root_eta)\n        # self.global_subproblem_count += 1\n        if subproblem_fn_kwargs['scenario_name'] in self.ls.local_scenario_names:\n            # self.local_subproblem_count += 1\n            self.root_etas.append(root_eta)\n            subproblem, complicating_vars_map = subproblem_fn(**subproblem_fn_kwargs)\n            self.subproblems.append(subproblem)\n            self.complicating_vars_maps.append(complicating_vars_map)\n            bc._setup_subproblem(subproblem, root_vars=[complicating_vars_map[i] for i in self.root_vars if\n                                                       i in complicating_vars_map],\n                              relax_subproblem_cons=relax_subproblem_cons)\n\n            # self._subproblem_ndx_map[self.local_subproblem_count - 1] = self.global_subproblem_count - 1\n\n            if isinstance(subproblem_solver, str):\n                subproblem_solver = pe.SolverFactory(subproblem_solver)\n            self.subproblem_solvers.append(subproblem_solver)\n            if isinstance(subproblem_solver, PersistentSolver):\n                set_instance_retry(subproblem, subproblem_solver, subproblem_fn_kwargs['scenario_name'])\n            if subproblem_solver_options:\n                for k,v in subproblem_solver_options.items():\n                    subproblem_solver.options[k] = v",
  "def __init__(self, component):\n        super().__init__(component)",
  "def set_ls(self, ls):\n        self.ls = ls\n        self.global_subproblem_count = len(self.ls.all_scenario_names)\n        self._subproblem_ndx_map = dict.fromkeys(range(len(self.ls.local_scenario_names)))\n        for s in self._subproblem_ndx_map.keys():\n            self._subproblem_ndx_map[s] = self.ls.all_scenario_names.index(self.ls.local_scenario_names[s])\n        # print(self._subproblem_ndx_map)\n        self.all_root_etas = list(self.ls.root.eta.values())",
  "def global_num_subproblems(self):\n        return self.global_subproblem_count",
  "def add_subproblem(self, subproblem_fn, subproblem_fn_kwargs, root_eta, subproblem_solver='gurobi_persistent',\n                       relax_subproblem_cons=False, subproblem_solver_options=None):\n        # print(self._subproblem_ndx_map)\n        # self.all_root_etas.append(root_eta)\n        # self.global_subproblem_count += 1\n        if subproblem_fn_kwargs['scenario_name'] in self.ls.local_scenario_names:\n            # self.local_subproblem_count += 1\n            self.root_etas.append(root_eta)\n            subproblem, complicating_vars_map = subproblem_fn(**subproblem_fn_kwargs)\n            self.subproblems.append(subproblem)\n            self.complicating_vars_maps.append(complicating_vars_map)\n            bc._setup_subproblem(subproblem, root_vars=[complicating_vars_map[i] for i in self.root_vars if\n                                                       i in complicating_vars_map],\n                              relax_subproblem_cons=relax_subproblem_cons)\n\n            # self._subproblem_ndx_map[self.local_subproblem_count - 1] = self.global_subproblem_count - 1\n\n            if isinstance(subproblem_solver, str):\n                subproblem_solver = pe.SolverFactory(subproblem_solver)\n            self.subproblem_solvers.append(subproblem_solver)\n            if isinstance(subproblem_solver, PersistentSolver):\n                set_instance_retry(subproblem, subproblem_solver, subproblem_fn_kwargs['scenario_name'])\n            if subproblem_solver_options:\n                for k,v in subproblem_solver_options.items():\n                    subproblem_solver.options[k] = v",
  "def spin_the_wheel(hub_dict, list_of_spoke_dict, comm_world=None):\n    raise RuntimeError(\n            _spin_the_wheel_move_msg + \\\n            \" See the example code below for a fix:\\n\"\n    '''\n    from mpisppy.spin_the_wheel import WheelSpinner\n    ws = WheelSpinner(hub_dict, list_of_spoke_dict)\n    ws.spin(comm_world=comm_world)\n    '''\n    )",
  "def first_stage_nonant_npy_serializer(file_name, scenario, bundling):\n    # write just the nonants for ROOT in an npy file (e.g. for Conf Int)\n    root = scenario._mpisppy_node_list[0]\n    assert root.name == \"ROOT\"\n    root_nonants = np.fromiter((pyo.value(var) for var in root.nonant_vardata_list), float)\n    np.save(file_name, root_nonants)",
  "def first_stage_nonant_writer( file_name, scenario, bundling ):\n    with open(file_name, 'w') as f:\n        root = scenario._mpisppy_node_list[0]\n        assert root.name == \"ROOT\"\n        for var in root.nonant_vardata_list:\n            var_name = var.name\n            if bundling:\n                dot_index = var_name.find('.')\n                assert dot_index >= 0\n                var_name = var_name[(dot_index+1):]\n            f.write(f\"{var_name},{pyo.value(var)}\\n\")",
  "def scenario_tree_solution_writer( directory_name, scenario_name, scenario, bundling ):\n    with open(os.path.join(directory_name, scenario_name+'.csv'), 'w') as f:\n        for var in scenario.component_data_objects(\n                ctype=(pyo.Var, pyo.Expression),\n                descend_into=True,\n                active=True,\n                sort=True):\n            var_name = var.name\n            if bundling:\n                dot_index = var_name.find('.')\n                assert dot_index >= 0\n                var_name = var_name[(dot_index+1):]\n            f.write(f\"{var_name},{pyo.value(var)}\\n\")",
  "def write_spin_the_wheel_first_stage_solution(spcomm, opt_dict, solution_file_name,\n        first_stage_solution_writer=first_stage_nonant_writer):\n    raise RuntimeError(_spin_the_wheel_move_msg)",
  "def write_spin_the_wheel_tree_solution(spcomm, opt_dict, solution_directory_name,\n        scenario_tree_solution_writer=scenario_tree_solution_writer):\n    raise RuntimeError(_spin_the_wheel_move_msg)",
  "def local_nonant_cache(spcomm):\n    raise RuntimeError(_spin_the_wheel_move_msg)",
  "def get_objs(scenario_instance, allow_none=False):\n    \"\"\" return the list of objective functions for scenario_instance\"\"\"\n    scenario_objs = scenario_instance.component_data_objects(pyo.Objective,\n                    active=True, descend_into=True)\n    scenario_objs = list(scenario_objs)\n    if (len(scenario_objs) == 0) and not allow_none:\n        raise RuntimeError(f\"Scenario {scenario_instance.name} has no active \"\n                           \"objective functions.\")\n    if (len(scenario_objs) > 1):\n        print(\"WARNING: Scenario\", sname, \"has multiple active \"\n              \"objectives. Selecting the first objective.\")\n    return scenario_objs",
  "def stash_ref_objs(scenario_instance):\n    \"\"\"Stash a reference to active objs so\n        Reactivate_obj can use the reference to reactivate them/it later.\n    \"\"\"\n    scenario_instance._mpisppy_data.obj_list = get_objs(scenario_instance)",
  "def deact_objs(scenario_instance):\n    \"\"\" Deactivate objs \n    Args:\n        scenario_instance (Pyomo ConcreteModel): the scenario\n    Returns:\n        obj_list (list of Pyomo Objectives): the deactivated objs\n    Note: If none are active, just do nothing\n    \"\"\"\n    obj_list = get_objs(scenario_instance, allow_none=True)\n    for obj in obj_list:\n        obj.deactivate()\n    return obj_list",
  "def reactivate_objs(scenario_instance):\n    \"\"\" Reactivate ojbs stashed by stash_ref_objs \"\"\"\n    if not hasattr(scenario_instance._mpisppy_data, \"obj_list\"):\n        raise RuntimeError(\"reactivate_objs called with prior call to stash_ref_objs\")\n    for obj in scenario_instance._mpisppy_data.obj_list:\n        obj.activate()",
  "def create_EF(scenario_names, scenario_creator, scenario_creator_kwargs=None,\n              EF_name=None, suppress_warnings=False,\n              nonant_for_fixed_vars=True):\n    \"\"\" Create a ConcreteModel of the extensive form.\n\n        Args:\n            scenario_names (list of str):\n                Names for each scenario to be passed to the scenario_creator\n                function.\n            scenario_creator (callable):\n                Function which takes a scenario name as its first argument and\n                returns a concrete model corresponding to that scenario.\n            scenario_creator_kwargs (dict, optional):\n                Options to pass to `scenario_creator`.\n            EF_name (str, optional):\n                Name of the ConcreteModel of the EF.\n            suppress_warnings (boolean, optional):\n                If true, do not display warnings. Default False.\n            nonant_for_fixed_vars (bool--optional): If True, enforces\n                non-anticipativity constraints for all variables, including\n                those which have been fixed. Default is True.\n\n        Returns:\n            EF_instance (ConcreteModel):\n                ConcreteModel of extensive form with explicit\n                non-anticipativity constraints.\n\n        Note:\n            If any of the scenarios produced by scenario_creator do not have a\n            ._mpisppy_probability attribute, this function displays a warning, and assumes\n            that all scenarios are equally likely.\n    \"\"\"\n    if scenario_creator_kwargs is None:\n        scenario_creator_kwargs = dict()\n    scen_dict = {\n        name: scenario_creator(name, **scenario_creator_kwargs)\n        for name in scenario_names\n    }\n\n    if (len(scen_dict) == 0):\n        raise RuntimeError(\"create_EF() received empty scenario list\")\n    elif (len(scen_dict) == 1):\n        scenario_instance = list(scen_dict.values())[0]\n        scenario_instance._ef_scenario_names = list(scen_dict.keys())\n        if not suppress_warnings:\n            print(\"WARNING: passed single scenario to create_EF()\")\n        # special code to patch in ref_vars\n        scenario_instance.ref_vars = dict()\n        scenario_instance._nlens = {node.name: len(node.nonant_vardata_list) \n                                for node in scenario_instance._mpisppy_node_list}\n        for node in scenario_instance._mpisppy_node_list:\n            ndn = node.name\n\n            for i in range(scenario_instance._nlens[ndn]):\n                v = node.nonant_vardata_list[i]\n                if (ndn, i) not in scenario_instance.ref_vars:\n                    scenario_instance.ref_vars[(ndn, i)] = v\n        # patch in EF_Obj        \n        scenario_objs = deact_objs(scenario_instance)        \n        obj = scenario_objs[0]            \n        sense = pyo.minimize if obj.is_minimizing() else pyo.maximize\n        scenario_instance.EF_Obj = pyo.Objective(expr=obj.expr, sense=sense)\n\n        return scenario_instance  #### special return for single scenario\n\n    # Check if every scenario has a specified probability\n    probs_specified = \\\n        all(hasattr(scen, '_mpisppy_probability') for scen in scen_dict.values())\n    uniform_specified = \\\n        probs_specified and all(scen._mpisppy_probability == \"uniform\" for scen in scen_dict.values())\n    if not probs_specified or uniform_specified:\n        for scen in scen_dict.values():\n            scen._mpisppy_probability = 1 / len(scen_dict)\n        if not suppress_warnings and not uniform_specified:\n            print('WARNING: At least one scenario is missing _mpisppy_probability attribute.',\n                  'Assuming equally-likely scenarios...')\n\n    EF_instance = _create_EF_from_scen_dict(scen_dict,\n                                            EF_name=EF_name,\n                                            nonant_for_fixed_vars=True)\n    return EF_instance",
  "def _create_EF_from_scen_dict(scen_dict, EF_name=None,\n                                nonant_for_fixed_vars=True):\n    \"\"\" Create a ConcreteModel of the extensive form from a scenario\n        dictionary.\n\n        Args:\n            scen_dict (dict): Dictionary whose keys are scenario names and\n                values are ConcreteModel objects corresponding to each\n                scenario.\n            EF_name (str--optional): Name of the resulting EF model.\n            nonant_for_fixed_vars (bool--optional): If True, enforces\n                non-anticipativity constraints for all variables, including\n                those which have been fixed. Default is True.\n\n        Returns:\n            EF_instance (ConcreteModel): ConcreteModel of extensive form with\n                explicity non-anticipativity constraints.\n\n        Notes:\n            The non-anticipativity constraints are enforced by creating\n            \"reference variables\" at each node in the scenario tree (excluding\n            leaves) and enforcing that all the variables for each scenario at\n            that node are equal to the reference variables.\n\n            This function is called directly when creating bundles for PH.\n \n            Does NOT assume that each scenario is equally likely. Raises an\n            AttributeError if a scenario object is encountered which does not\n            have a ._mpisppy_probability attribute.\n\n            Added the flag nonant_for_fixed_vars because original code only\n            enforced non-anticipativity for non-fixed vars, which is not always\n            desirable in the context of bundling. This allows for more\n            fine-grained control.\n    \"\"\"\n    is_min, clear = _models_have_same_sense(scen_dict)\n    if (not clear):\n        raise RuntimeError('Cannot build the extensive form out of models '\n                           'with different objective senses')\n    sense = pyo.minimize if is_min else pyo.maximize\n    EF_instance = pyo.ConcreteModel(name=EF_name)\n    EF_instance.EF_Obj = pyo.Objective(expr=0.0, sense=sense)\n\n    # we don't strictly need these here, but it allows for eliding\n    # of single scenarios and bundles when convenient\n    EF_instance._mpisppy_data = pyo.Block(name=\"For non-Pyomo mpi-sppy data\")\n    EF_instance._mpisppy_model = pyo.Block(name=\"For mpi-sppy Pyomo additions to the scenario model\")\n    EF_instance._mpisppy_data.scenario_feasible = None\n\n    EF_instance._ef_scenario_names = []\n    EF_instance._mpisppy_probability = 0\n    for (sname, scenario_instance) in scen_dict.items():\n        EF_instance.add_component(sname, scenario_instance)\n        EF_instance._ef_scenario_names.append(sname)\n        # Now deactivate the scenario instance Objective\n        scenario_objs = deact_objs(scenario_instance)\n        obj_func = scenario_objs[0] # Select the first objective\n        try:\n            EF_instance.EF_Obj.expr += scenario_instance._mpisppy_probability * obj_func.expr\n            EF_instance._mpisppy_probability   += scenario_instance._mpisppy_probability\n        except AttributeError as e:\n            raise AttributeError(\"Scenario \" + sname + \" has no specified \"\n                        \"probability. Specify a value for the attribute \"\n                        \" _mpisppy_probability and try again.\") from e\n    # Normalization does nothing when solving the full EF, but is required for\n    # appropraite scaling of EFs used as bundles.\n    EF_instance.EF_Obj.expr /= EF_instance._mpisppy_probability\n\n    # For each node in the scenario tree, we need to collect the\n    # nonanticipative vars and create the constraints for them,\n    # which we do using a reference variable.\n    ref_vars = dict() # keys are _nonant_indices (i.e. a node name and a\n                      # variable number)\n\n    ref_suppl_vars = dict()\n\n    EF_instance._nlens = dict() \n\n    nonant_constr = pyo.Constraint(pyo.Any, name='_C_EF_')\n    EF_instance.add_component('_C_EF_', nonant_constr)\n\n    nonant_constr_suppl = pyo.Constraint(pyo.Any, name='_C_EF_suppl')\n    EF_instance.add_component('_C_EF_suppl', nonant_constr_suppl)\n\n    for (sname, s) in scen_dict.items():\n        nlens = {node.name: len(node.nonant_vardata_list) \n                            for node in s._mpisppy_node_list}\n        \n        for (node_name, num_nonant_vars) in nlens.items(): # copy nlens to EF\n            if (node_name in EF_instance._nlens.keys() and\n                num_nonant_vars != EF_instance._nlens[node_name]):\n                raise RuntimeError(\"Number of non-anticipative variables is \"\n                    \"not consistent at node \" + node_name + \" in scenario \" +\n                    sname)\n            EF_instance._nlens[node_name] = num_nonant_vars\n\n        nlens_ef_suppl = {node.name: len(node.nonant_ef_suppl_vardata_list)\n                                   for node in s._mpisppy_node_list}\n\n        for node in s._mpisppy_node_list:\n            ndn = node.name\n            for i in range(nlens[ndn]):\n                v = node.nonant_vardata_list[i]\n                if (ndn, i) not in ref_vars:\n                    # create the reference variable as a singleton with long name\n                    # xxxx maybe index by _nonant_index ???? rather than singleton VAR ???\n                    ref_vars[(ndn, i)] = v\n                # Add a non-anticipativity constraint, except in the case when\n                # the variable is fixed and nonant_for_fixed_vars=False.\n                elif (nonant_for_fixed_vars) or (not v.is_fixed()):\n                    expr = LinearExpression(linear_coefs=[1,-1],\n                                            linear_vars=[v,ref_vars[(ndn,i)]],\n                                            constant=0.)\n                    nonant_constr[(ndn,i,sname)] = (expr, 0.0)\n\n            for i in range(nlens_ef_suppl[ndn]):\n                v = node.nonant_ef_suppl_vardata_list[i]\n                if (ndn, i) not in ref_suppl_vars:\n                    # create the reference variable as a singleton with long name\n                    # xxxx maybe index by _nonant_index ???? rather than singleton VAR ???\n                    ref_suppl_vars[(ndn, i)] = v\n                # Add a non-anticipativity constraint, expect in the case when\n                # the variable is fixed and nonant_for_fixed_vars=False.\n                elif (nonant_for_fixed_vars) or (not v.is_fixed()):\n                        expr = LinearExpression(linear_coefs=[1,-1],\n                                                linear_vars=[v,ref_suppl_vars[(ndn,i)]],\n                                                constant=0.)\n                        nonant_constr_suppl[(ndn,i,sname)] = (expr, 0.0)\n\n    EF_instance.ref_vars = ref_vars\n    EF_instance.ref_suppl_vars = ref_suppl_vars\n                        \n    return EF_instance",
  "def _models_have_same_sense(models):\n    ''' Check if every model in the provided dict has the same objective sense.\n\n        Input:\n            models (dict) -- Keys are scenario names, values are Pyomo\n                ConcreteModel objects.\n        Returns:\n            is_minimizing (bool) -- True if and only if minimizing. None if the\n                check fails.\n            check (bool) -- True only if all the models have the same sense (or\n                no models were provided)\n        Raises:\n            ValueError -- If any of the models has either none or multiple\n                active objectives.\n    '''\n    if (len(models) == 0):\n        return True, True\n    senses = [find_active_objective(scenario).is_minimizing()\n                for scenario in models.values()]\n    sense = senses[0]\n    check = all(val == sense for val in senses)\n    if (check):\n        return (sense == pyo.minimize), check\n    return None, check",
  "def is_persistent(solver):\n    return isinstance(solver,\n        pyo.pyomo.solvers.plugins.solvers.persistent_solver.PersistentSolver)",
  "def ef_scenarios(ef):\n    \"\"\" An iterator to give the scenario sub-models in an ef\n    Args:\n        ef (ConcreteModel): the full extensive form model\n\n    Yields:\n        scenario name, scenario instance (str, ConcreteModel)\n    \"\"\"    \n    for sname in ef._ef_scenario_names:\n        yield (sname, getattr(ef, sname))",
  "def ef_nonants(ef):\n    \"\"\" An iterator to give representative Vars subject to non-anticipitivity\n    Args:\n        ef (ConcreteModel): the full extensive form model\n\n    Yields:\n        tree node name, full EF Var name, Var value\n\n    Note:\n        not on an EF object because not all ef's are part of an EF object\n    \"\"\"\n    for (ndn,i), var in ef.ref_vars.items():\n        yield (ndn, var, pyo.value(var))",
  "def ef_nonants_csv(ef, filename):\n    \"\"\" Dump the nonant vars from an ef to a csv file; truly a dump...\n    Args:\n        ef (ConcreteModel): the full extensive form model\n        filename (str): the full name of the csv output file\n    \"\"\"\n    with open(filename, \"w\") as outfile:\n        outfile.write(\"Node, EF_VarName, Value\\n\")\n        for (ndname, varname, varval) in ef_nonants(ef):\n            outfile.write(\"{}, {}, {}\\n\".format(ndname, varname, varval))",
  "def nonant_cache_from_ef(ef,verbose=False):\n    \"\"\" Populate a nonant_cache from an ef. Also works with multi-stage\n    Args:\n        ef (mpi-sppy ef): a solved ef\n    Returns:\n        nonant_cache (dict of numpy arrays): a special structure for nonant values\n    \"\"\"     \n    nonant_cache = dict()\n    nodenames = set([ndn for (ndn,i) in ef.ref_vars])\n    for ndn in sorted(nodenames):\n        nonant_cache[ndn]=[]\n        i = 0\n        while ((ndn,i) in ef.ref_vars):\n            xvar = pyo.value(ef.ref_vars[(ndn,i)])\n            nonant_cache[ndn].append(xvar)\n            if verbose:\n                print(\"barfoo\", i, xvar)\n            i+=1\n    return nonant_cache",
  "def ef_ROOT_nonants_npy_serializer(ef, filename):\n    \"\"\" write the root node nonants to be ready by a numpy load\n    Args:\n        ef (ConcreteModel): the full extensive form model\n        filename (str): the full name of the .npy output file\n    \"\"\"\n    root_nonants = np.fromiter((v for ndn,var,v in ef_nonants(ef) if ndn == \"ROOT\"), float)\n    np.save(filename, root_nonants)",
  "def write_ef_first_stage_solution(ef,\n                                  solution_file_name,\n                                  first_stage_solution_writer=first_stage_nonant_writer):\n    \"\"\" \n    Write a solution file, if a solution is available, to the solution_file_name provided\n    Args:\n        ef : A Concrete Model of the Extensive Form (output of create_EF). \n             We assume it has already been solved.\n        solution_file_name : filename to write the solution to\n        first_stage_solution_writer (optional) : custom first stage solution writer function\n    \n    NOTE:\n        This utility is replicating WheelSpinner.write_first_stage_solution for EF\n    \"\"\"\n    if global_rank == 0:\n        representative_scenario = getattr(ef,ef._ef_scenario_names[0])\n        first_stage_solution_writer(solution_file_name, \n                                    representative_scenario,\n                                    bundling=False)",
  "def write_ef_tree_solution(ef, solution_directory_name,\n        scenario_tree_solution_writer=scenario_tree_solution_writer):\n    \"\"\" Write a tree solution directory, if available, to the solution_directory_name provided\n    Args:\n        ef : A Concrete Model of the Extensive Form (output of create_EF). \n             We assume it has already been solved.\n        solution_file_name : filename to write the solution to\n        scenario_tree_solution_writer (optional) : custom scenario solution writer function\n        \n    NOTE:\n        This utility is replicating WheelSpinner.write_tree_solution for EF\n    \"\"\"\n    if global_rank==0:\n        os.makedirs(solution_directory_name, exist_ok=True)\n        for scenario_name, scenario in ef_scenarios(ef):\n            scenario_tree_solution_writer(solution_directory_name,\n                                          scenario_name, \n                                          scenario,\n                                          bundling=False)",
  "def extract_num(string):\n    ''' Given a string, extract the longest contiguous\n        integer from the right-hand side of the string.\n\n        Example:\n            scenario324 -> 324\n\n        TODO: Add Exception Handling\n    '''\n    return int(re.compile(r'(\\d+)$').search(string).group(1))",
  "def node_idx(node_path,branching_factors):\n    '''\n    Computes a unique id for a given node in a scenario tree.\n    It follows the path to the node, computing the unique id for each ascendant.\n\n    Parameters\n    ----------\n    node_path : list of int\n        A list of integer, specifying the path of the node.\n    branching_factors : list of int\n        branching_factors of the scenario tree.\n\n    Returns\n    -------\n    node_idx\n        Node unique id.\n        \n    NOTE: Does not work with unbalanced trees.\n\n    '''\n    if node_path == []: #ROOT node\n        return 0\n    else:\n        stage_id = 0 #node unique id among stage t+1 nodes.\n        for t in range(len(node_path)):\n            stage_id = node_path[t]+branching_factors[t]*stage_id\n        node_idx = _nodenum_before_stage(len(node_path),branching_factors)+stage_id\n        return node_idx",
  "def _extract_node_idx(nodename,branching_factors):\n    \"\"\"\n    \n\n    Parameters\n    ----------\n    nodename : str\n        The name of a node, e.g. 'ROOT_2_0_4'.\n    branching_factors : list\n        Branching factor of a scenario tree, e.g. [3,2,8,4,3].\n\n    Returns\n    -------\n    node_idx : int\n        A unique integer that can be used as a key to designate this scenario.\n\n    \"\"\"\n    if nodename =='ROOT':\n        return 0\n    else:\n        to_list = [int(x) for x in re.findall(r'\\d+',nodename)]\n        return node_idx(to_list,branching_factors)",
  "def parent_ndn(nodename):\n    if nodename == 'ROOT':\n        return None\n    else:\n        return re.search('(.+)_(\\d+)',nodename).group(1)",
  "def option_string_to_dict(ostr):\n    \"\"\" Convert a string to the standard dict for solver options.\n    Intended for use in the calling program; not internal use here.\n\n    Args:\n        ostr (string): space seperated options with = for arguments\n\n    Returns:\n        solver_options (dict): solver options\n\n    \"\"\"\n    def convert_value_string_to_number(s):\n        try:\n            return int(s)\n        except ValueError:\n            try:\n                return float(s)\n            except ValueError:\n                return s\n\n    solver_options = dict()\n    if ostr is None or ostr == \"\":\n        return None\n    for this_option_string in ostr.split():\n        this_option_pieces = this_option_string.strip().split(\"=\")\n        if len(this_option_pieces) == 2:\n            option_key = this_option_pieces[0]\n            option_value = convert_value_string_to_number(this_option_pieces[1])\n            solver_options[option_key] = option_value\n        elif len(this_option_pieces) == 1:\n            option_key = this_option_pieces[0]\n            solver_options[option_key] = None\n        else:\n            raise RuntimeError(\"Illegally formed subsolve directive\"\\\n                               + \" option=%s detected\" % this_option)\n    return solver_options",
  "def option_dict_to_string(odict):\n    \"\"\" Convert a standard dict for solver options to a string.\n\n    Args:\n        odict (dict): options dict for Pyomo\n\n    Returns:\n        ostring (str): options string as in mpi-sppy\n\n    Note: None begets None, and empty begets empty\n\n    \"\"\"\n    if odict is None:\n        return None\n    ostr = \"\"\n    for i, v in odict.items():\n        if v is None:\n            ostr += \"{i} \"\n        else:\n            ostr += f\"{i}={v} \"\n    return ostr",
  "def scens_to_ranks(scen_count, n_proc, rank, branching_factors = None):\n    \"\"\" Determine the rank assignments that are made in spbase.\n    NOTE: Callers to this should call _scentree.scen_names_to_ranks\n    Args:\n        scen_count (int): number of scenarios\n        n_proc (int): the number of intra ranks (within the cylinder)\n        rank (int): my rank (i.e., intra; i.e., within the cylinder)\n    Returns:\n        slices (list of ranges): the indices into all all_scenario_names to assign to rank\n                                 (the list entries are ranges that correspond to ranks)\n        scenario_name_to_rank (dict of dict): only for multi-stage\n                keys are comms (i.e., tree nodes); values are dicts with keys\n                that are scenario names and values that are ranks\n\n    \"\"\"\n    if not haveMPI:\n        raise RuntimeError(\"scens_to_ranks called, but cannot import mpi4py\")\n    if scen_count < n_proc:\n        raise RuntimeError(\n            \"More MPI ranks (%d) supplied than needed given the number of scenarios (%d) \"\n            % (n_proc, scen_count)\n        )\n\n    # for now, we are treating two-stage as a special case\n    if (branching_factors is None):\n        avg = scen_count / n_proc\n        slices = [list(range(int(i * avg), int((i + 1) * avg))) for i in range(n_proc)]\n        return slices, None\n    else:\n        # OCT 2020: this block is never triggered and would fail.\n        # indecision as of May 2020 (delete this comment DLW)\n        # just make sure things are consistent with what xhat will do...\n        # TBD: streamline\n        all_scenario_names = [\"ID\"+str(i) for i in range(scen_count)]\n        tree = _ScenTree(branching_factors, all_scenario_names)\n        scenario_names_to_ranks, slices, = tree.scen_name_to_rank(n_proc, rank)\n        return slices, scenario_names_to_ranks",
  "def _nodenum_before_stage(t,branching_factors):\n    #How many nodes in a tree of stage 1,2,...,t ?\n    #Only works with branching factors\n    return int(sum(np.prod(branching_factors[0:i]) for i in range(t)))",
  "def find_leaves(all_nodenames):\n    #Take a list of all nodenames from a tree, and find the leaves of it.\n    #WARNING: We do NOT check that the tree is well constructed\n    \n    if all_nodenames is None or all_nodenames == ['ROOT']:\n        return {'ROOT':False} # 2 stage problem: no leaf nodes in all_nodenames\n    #A leaf is simply a root with no child n\u00b00\n    is_leaf = dict()\n    for ndn in all_nodenames:\n        if ndn+\"_0\" in all_nodenames:\n            is_leaf[ndn] = False\n        else:\n            is_leaf[ndn] = True\n    return is_leaf",
  "class _TreeNode():\n    #Create the subtree generated by a node, with associated scenarios\n    # stages are 1-based, everything else is 0-based\n    # scenario lists are stored as (first, last) indices in all_scenarios\n    #This is also checking that the nodes from all_nodenames are well-named.\n    def __init__(self, Parent, scenfirst, scenlast, desc_leaf_dict, name):\n        #desc_leaf_dict is the output of find_leaves\n        self.scenfirst = scenfirst #id of the first scenario with this node\n        self.scenlast = scenlast #id of the last scenario with this node\n        self.name = name\n        numscens = scenlast - scenfirst + 1 #number of scenarios with this node\n        self.is_leaf = False\n        if Parent is None:\n            assert(self.name == \"ROOT\")\n            self.stage = 1\n        else:\n            self.stage = Parent.stage + 1\n        if len(desc_leaf_dict)==1 and list(desc_leaf_dict.keys()) == ['ROOT']: \n            #2-stage problem, we don't create leaf nodes\n            self.kids = []\n        elif not name+\"_0\" in desc_leaf_dict:\n            self.is_leaf = True\n            self.kids = []\n        else:\n            if len(desc_leaf_dict) < numscens:                \n                raise RuntimeError(f\"There are more scenarios ({numscens}) than remaining leaves, for the node {name}\")\n            # make children\n            first = scenfirst\n            self.kids = list()\n            child_regex = re.compile(name+'_\\d*\\Z')\n            child_list = [x for x in desc_leaf_dict if child_regex.match(x) ]\n            for i in range(len(desc_leaf_dict)):\n                childname = name+f\"_{i}\"\n                if not childname in desc_leaf_dict:\n                    if len(child_list) != i:\n                        raise RuntimeError(\"The all_nodenames argument is giving an inconsistent tree.\"\n                                           f\"The node {name} has {len(child_list)} children, but {childname} is not one of them.\")\n                    break\n                childdesc_regex = re.compile(childname+'(_\\d*)*\\Z')\n                child_leaf_dict = {ndn:desc_leaf_dict[ndn] for ndn in desc_leaf_dict \\\n                                   if childdesc_regex.match(ndn)}\n                #We determine the number of children of this node\n                child_scens_num = sum(child_leaf_dict.values())\n                last = first+child_scens_num - 1\n                self.kids.append(_TreeNode(self, first, last, \n                                           child_leaf_dict, childname))\n                first += child_scens_num\n            if last != scenlast:\n                print(\"numscens, last, scenlast\", numscens, last, scenlast)\n                raise RuntimeError(f\"Tree node did not initialize correctly for node {name}\")\n\n\n    def stage_max(self):\n        #Return the number of stages of a subtree.\n        #Also check that all the subtrees have the same number of stages\n        #i.e. that the leaves are always on the same stage. \n        if self.is_leaf:\n            return 1\n        else:\n            l = [child.stage_max() for child in self.kids]\n            if l.count(l[0]) != len(l):\n                maxstage = max(l)+ self.stage\n                minstage = min(l)+ self.stage\n                raise RuntimeError(\"The all_nodenames argument is giving an inconsistent tree. \"\n                                   f\"The node {self.name} has descendant leaves with stages going from {minstage} to {maxstage}\")\n            return 1+l[0]",
  "class _ScenTree():\n    def __init__(self, all_nodenames, ScenNames):\n        if all_nodenames is None:\n            all_nodenames = ['ROOT'] #2 stage problem: no leaf nodes\n        self.ScenNames = ScenNames\n        self.NumScens = len(ScenNames)\n        first = 0\n        last = self.NumScens - 1\n        desc_leaf_dict = find_leaves(all_nodenames)\n        self.rootnode = _TreeNode(None, first, last, desc_leaf_dict, \"ROOT\")\n        def _nonleaves(nd):\n            if nd.is_leaf:\n                return []\n            else:\n                retval = [nd]\n                for child in nd.kids:\n                    retval+=_nonleaves(child)\n                return retval\n        self.nonleaves = _nonleaves(self.rootnode)\n        \n        self.NumStages = \\\n            2 if all_nodenames == ['ROOT'] else self.rootnode.stage_max() \n        self.NonLeafTerminals = \\\n            [nd for nd in self.nonleaves if nd.stage == self.NumStages-1]\n        \n        self.NumLeaves = len(desc_leaf_dict) - len(self.nonleaves)\n        if self.NumStages>2 and self.NumLeaves != self.NumScens:\n            raise RuntimeError(\"The all_nodenames argument is giving an inconsistent tree.\"\n                               f\"There are {self.NumLeaves} leaves for this tree, but {self.NumScens} scenarios are given.\")\n    def scen_names_to_ranks(self, n_proc):\n        \"\"\" \n        Args:\n            n_proc: number of ranks in the cylinder (i.e., intra)\n\n        Returns:\n            scenario_names_to_rank (dict of dict):\n                keys are comms (i.e., tree nodes); values are dicts with keys\n                that are scenario names and values that are ranks within that comm\n            slices (list of lists)\n                indices correspond to ranks in self.mpicomm and the values are a list\n                of scenario indices\n                rank -> list of scenario indices for that rank\n            list_of_ranks_per_scenario_idx (list)\n                indices are scenario indices and values are the rank of that scenario\n                within self.mpicomm\n                scenario index -> rank\n\n        NOTE:\n            comm names are the same as the corresponding scenario tree node name\n\n        \"\"\"\n        scenario_names_to_rank = dict()  # scenario_name_to_rank dict of dicts\n        # one processor for the cylinder is a special case\n        if n_proc == 1:\n            for nd in self.nonleaves:\n                scenario_names_to_rank[nd.name] = {s: 0 for s in self.ScenNames}\n            return scenario_names_to_rank, [list(range(self.NumScens))], [0]*self.NumScens\n\n        scen_count = len(self.ScenNames)\n        avg = scen_count / n_proc\n\n        # rank -> list of scenario indices for that rank\n        slices = [list(range(int(i * avg), int((i + 1) * avg))) for i in range(n_proc)]\n\n        # scenario index -> rank\n        list_of_ranks_per_scenario_idx = [ rank for rank, scen_idxs in enumerate(slices) for _ in scen_idxs ]\n\n        scenario_names_to_rank[\"ROOT\"] = { s: rank for s,rank in zip(self.ScenNames, list_of_ranks_per_scenario_idx) }\n         \n        def _recurse_do_node(node):\n            for child in node.kids:\n\n                first_scen_idx = child.scenfirst\n                last_scen_idx = child.scenlast\n\n                ranks_in_node = list_of_ranks_per_scenario_idx[first_scen_idx:last_scen_idx+1]\n                minimum_rank_in_node = ranks_in_node[0]\n\n                # IMPORTANT:\n                # this accords with the way SPBase.create_communicators assigns the \"key\" when\n                # creating its comm for this node. E.g., the key is the existing rank, which\n                # will then be offset by the minimum rank. As the ranks within each node are\n                # contiguous, this is enough to infer the rank each scenario will have in this\n                # node's comm\n                within_comm_ranks_in_node = [(rank-minimum_rank_in_node) for rank in ranks_in_node]\n\n                scenarios_in_nodes = self.ScenNames[first_scen_idx:last_scen_idx+1]\n\n                scenario_names_to_rank[child.name] = { s : rank for s,rank in zip(scenarios_in_nodes, within_comm_ranks_in_node) }\n\n                if child not in self.NonLeafTerminals:\n                    _recurse_do_node(child)\n\n        _recurse_do_node(self.rootnode)\n\n        return scenario_names_to_rank, slices, list_of_ranks_per_scenario_idx",
  "def attach_root_node(model, firstobj, varlist, nonant_ef_suppl_list=None):\n    \"\"\" Create a root node as a list to attach to a scenario model\n    Args:\n        model (ConcreteModel): model to which this will be attached\n        firstobj (Pyomo Expression): First stage cost (e.g. model.FC)\n        varlist (list): Pyomo Vars in first stage (e.g. [model.A, model.B])\n        nonant_ef_suppl_list (list of pyo Var, Vardata or slices):\n              vars for which nonanticipativity constraints tighten the EF\n              (important for bundling)\n\n    Note: \n       attaches a list consisting of one scenario node to the model\n    \"\"\"\n    model._mpisppy_node_list = [\n        scenario_tree.ScenarioNode(\"ROOT\", 1.0, 1, firstobj, varlist, model,\n                                   nonant_ef_suppl_list = nonant_ef_suppl_list)\n    ]",
  "def check4losses(numscens, branching_factors,\n                 scenario_names_to_rank,slices,list_of_ranks_per_scenario_idx):\n    \"\"\" Check the data structures; gag and die if it looks bad.\n    Args:\n        numscens (int): number of scenarios\n        branching_factors (list of int): branching factors\n        scenario_names_to_rank (dict of dict):\n            keys are comms (i.e., tree nodes); values are dicts with keys\n            that are scenario names and values that are ranks within that comm\n        slices (list of lists)\n            indices correspond to ranks in self.mpicomm and the values are a list\n            of scenario indices\n            rank -> list of scenario indices for that rank\n        list_of_ranks_per_scenario_idx (list)\n            indices are scenario indices and values are the rank of that scenario\n            within self.mpicomm\n            scenario index -> rank\n\n    \"\"\"\n\n    present = [False for _ in range(numscens)]\n    for rank, scenlist in enumerate(slices):\n        for scen in scenlist:\n            present[scen] = True\n    missingsome = False\n    for scen, there in enumerate(present):\n        if not there:\n            print(f\"Scenario {scen} is not in slices\")\n            missingsome = True\n    if missingsome:\n        raise RuntimeError(\"Internal error: slices is not correct\")\n\n    # not stage presence...\n    stagepresents = {stage: [False for _ in range(numscens)] for stage in range(len(branching_factors))}\n    # loop over the entire structure, marking those found as present\n    for nodename, scenlist in scenario_names_to_rank.items():\n        stagenum = nodename.count('_')\n        for s in scenlist:\n            snum = int(s[8:])\n            stagepresents[stagenum][snum] = True\n    missingone = False\n    for stage in stagepresents:\n        for scen, there in enumerate(stagepresents[stage]):\n            if not there:\n                print(f\"Scenario number {scen} missing from stage {stage}.\")\n                missingsome = True\n    if missingsome:\n        raise RuntimeError(\"Internal error: scenario_name_to_rank\")\n    print(\"check4losses: OK\")",
  "def disable_tictoc_output():\n    f = open(os.devnull,\"w\")\n    tt_timer._ostream = f",
  "def reenable_tictoc_output():\n    # Primarily to re-enable after a disable\n    tt_timer._ostream.close()\n    tt_timer._ostream = sys.stdout",
  "def find_active_objective(pyomomodel):\n    # return the only active objective or raise and error\n    obj = list(pyomomodel.component_data_objects(\n        Objective, active=True, descend_into=True))\n    if len(obj) != 1:\n        raise RuntimeError(\"Could not identify exactly one active \"\n                           \"Objective for model '%s' (found %d objectives)\"\n                           % (pyomomodel.name, len(obj)))\n    return obj[0]",
  "def create_nodenames_from_branching_factors(BFS):\n    \"\"\"\n    This function creates the node names of a tree without creating the whole tree.\n\n    Parameters\n    ----------\n    BFS : list of integers\n        Branching factors.\n\n    Returns\n    -------\n    nodenames : list of str\n        a list of the node names induced by branching_factors, including leaf nodes.\n\n    \"\"\"\n    stage_nodes = [\"ROOT\"]\n    nodenames = ['ROOT']\n    if len(BFS)==1 : #2stage\n        return(nodenames)\n    for bf in BFS[:(len(BFS))]:\n        old_stage_nodes = stage_nodes\n        stage_nodes = []\n        for k in range(len(old_stage_nodes)):\n            stage_nodes += ['%s_%i'%(old_stage_nodes[k],b) for b in range(bf)]\n        nodenames += stage_nodes\n    return nodenames",
  "def get_branching_factors_from_nodenames(all_nodenames):\n    #WARNING: Do not work with unbalanced trees\n    staget_node = \"ROOT\"\n    branching_factors = []\n    while staget_node+\"_0\" in all_nodenames:\n        child_regex = re.compile(staget_node+'_\\d*\\Z')\n        child_list = [x for x in all_nodenames if child_regex.match(x) ]\n        \n        branching_factors.append(len(child_list))\n        staget_node += \"_0\"\n    if len(branching_factors)==1:\n        #2stage\n        return None\n    else:\n        return branching_factors",
  "def number_of_nodes(branching_factors):\n    #How many nodes does a tree with a given branching_factors have ?\n    last_node_stage_num = [i-1 for i in branching_factors]\n    return node_idx(last_node_stage_num, branching_factors)",
  "def convert_value_string_to_number(s):\n        try:\n            return int(s)\n        except ValueError:\n            try:\n                return float(s)\n            except ValueError:\n                return s",
  "def __init__(self, Parent, scenfirst, scenlast, desc_leaf_dict, name):\n        #desc_leaf_dict is the output of find_leaves\n        self.scenfirst = scenfirst #id of the first scenario with this node\n        self.scenlast = scenlast #id of the last scenario with this node\n        self.name = name\n        numscens = scenlast - scenfirst + 1 #number of scenarios with this node\n        self.is_leaf = False\n        if Parent is None:\n            assert(self.name == \"ROOT\")\n            self.stage = 1\n        else:\n            self.stage = Parent.stage + 1\n        if len(desc_leaf_dict)==1 and list(desc_leaf_dict.keys()) == ['ROOT']: \n            #2-stage problem, we don't create leaf nodes\n            self.kids = []\n        elif not name+\"_0\" in desc_leaf_dict:\n            self.is_leaf = True\n            self.kids = []\n        else:\n            if len(desc_leaf_dict) < numscens:                \n                raise RuntimeError(f\"There are more scenarios ({numscens}) than remaining leaves, for the node {name}\")\n            # make children\n            first = scenfirst\n            self.kids = list()\n            child_regex = re.compile(name+'_\\d*\\Z')\n            child_list = [x for x in desc_leaf_dict if child_regex.match(x) ]\n            for i in range(len(desc_leaf_dict)):\n                childname = name+f\"_{i}\"\n                if not childname in desc_leaf_dict:\n                    if len(child_list) != i:\n                        raise RuntimeError(\"The all_nodenames argument is giving an inconsistent tree.\"\n                                           f\"The node {name} has {len(child_list)} children, but {childname} is not one of them.\")\n                    break\n                childdesc_regex = re.compile(childname+'(_\\d*)*\\Z')\n                child_leaf_dict = {ndn:desc_leaf_dict[ndn] for ndn in desc_leaf_dict \\\n                                   if childdesc_regex.match(ndn)}\n                #We determine the number of children of this node\n                child_scens_num = sum(child_leaf_dict.values())\n                last = first+child_scens_num - 1\n                self.kids.append(_TreeNode(self, first, last, \n                                           child_leaf_dict, childname))\n                first += child_scens_num\n            if last != scenlast:\n                print(\"numscens, last, scenlast\", numscens, last, scenlast)\n                raise RuntimeError(f\"Tree node did not initialize correctly for node {name}\")",
  "def stage_max(self):\n        #Return the number of stages of a subtree.\n        #Also check that all the subtrees have the same number of stages\n        #i.e. that the leaves are always on the same stage. \n        if self.is_leaf:\n            return 1\n        else:\n            l = [child.stage_max() for child in self.kids]\n            if l.count(l[0]) != len(l):\n                maxstage = max(l)+ self.stage\n                minstage = min(l)+ self.stage\n                raise RuntimeError(\"The all_nodenames argument is giving an inconsistent tree. \"\n                                   f\"The node {self.name} has descendant leaves with stages going from {minstage} to {maxstage}\")\n            return 1+l[0]",
  "def __init__(self, all_nodenames, ScenNames):\n        if all_nodenames is None:\n            all_nodenames = ['ROOT'] #2 stage problem: no leaf nodes\n        self.ScenNames = ScenNames\n        self.NumScens = len(ScenNames)\n        first = 0\n        last = self.NumScens - 1\n        desc_leaf_dict = find_leaves(all_nodenames)\n        self.rootnode = _TreeNode(None, first, last, desc_leaf_dict, \"ROOT\")\n        def _nonleaves(nd):\n            if nd.is_leaf:\n                return []\n            else:\n                retval = [nd]\n                for child in nd.kids:\n                    retval+=_nonleaves(child)\n                return retval\n        self.nonleaves = _nonleaves(self.rootnode)\n        \n        self.NumStages = \\\n            2 if all_nodenames == ['ROOT'] else self.rootnode.stage_max() \n        self.NonLeafTerminals = \\\n            [nd for nd in self.nonleaves if nd.stage == self.NumStages-1]\n        \n        self.NumLeaves = len(desc_leaf_dict) - len(self.nonleaves)\n        if self.NumStages>2 and self.NumLeaves != self.NumScens:\n            raise RuntimeError(\"The all_nodenames argument is giving an inconsistent tree.\"\n                               f\"There are {self.NumLeaves} leaves for this tree, but {self.NumScens} scenarios are given.\")",
  "def scen_names_to_ranks(self, n_proc):\n        \"\"\" \n        Args:\n            n_proc: number of ranks in the cylinder (i.e., intra)\n\n        Returns:\n            scenario_names_to_rank (dict of dict):\n                keys are comms (i.e., tree nodes); values are dicts with keys\n                that are scenario names and values that are ranks within that comm\n            slices (list of lists)\n                indices correspond to ranks in self.mpicomm and the values are a list\n                of scenario indices\n                rank -> list of scenario indices for that rank\n            list_of_ranks_per_scenario_idx (list)\n                indices are scenario indices and values are the rank of that scenario\n                within self.mpicomm\n                scenario index -> rank\n\n        NOTE:\n            comm names are the same as the corresponding scenario tree node name\n\n        \"\"\"\n        scenario_names_to_rank = dict()  # scenario_name_to_rank dict of dicts\n        # one processor for the cylinder is a special case\n        if n_proc == 1:\n            for nd in self.nonleaves:\n                scenario_names_to_rank[nd.name] = {s: 0 for s in self.ScenNames}\n            return scenario_names_to_rank, [list(range(self.NumScens))], [0]*self.NumScens\n\n        scen_count = len(self.ScenNames)\n        avg = scen_count / n_proc\n\n        # rank -> list of scenario indices for that rank\n        slices = [list(range(int(i * avg), int((i + 1) * avg))) for i in range(n_proc)]\n\n        # scenario index -> rank\n        list_of_ranks_per_scenario_idx = [ rank for rank, scen_idxs in enumerate(slices) for _ in scen_idxs ]\n\n        scenario_names_to_rank[\"ROOT\"] = { s: rank for s,rank in zip(self.ScenNames, list_of_ranks_per_scenario_idx) }\n         \n        def _recurse_do_node(node):\n            for child in node.kids:\n\n                first_scen_idx = child.scenfirst\n                last_scen_idx = child.scenlast\n\n                ranks_in_node = list_of_ranks_per_scenario_idx[first_scen_idx:last_scen_idx+1]\n                minimum_rank_in_node = ranks_in_node[0]\n\n                # IMPORTANT:\n                # this accords with the way SPBase.create_communicators assigns the \"key\" when\n                # creating its comm for this node. E.g., the key is the existing rank, which\n                # will then be offset by the minimum rank. As the ranks within each node are\n                # contiguous, this is enough to infer the rank each scenario will have in this\n                # node's comm\n                within_comm_ranks_in_node = [(rank-minimum_rank_in_node) for rank in ranks_in_node]\n\n                scenarios_in_nodes = self.ScenNames[first_scen_idx:last_scen_idx+1]\n\n                scenario_names_to_rank[child.name] = { s : rank for s,rank in zip(scenarios_in_nodes, within_comm_ranks_in_node) }\n\n                if child not in self.NonLeafTerminals:\n                    _recurse_do_node(child)\n\n        _recurse_do_node(self.rootnode)\n\n        return scenario_names_to_rank, slices, list_of_ranks_per_scenario_idx",
  "def _nonleaves(nd):\n            if nd.is_leaf:\n                return []\n            else:\n                retval = [nd]\n                for child in nd.kids:\n                    retval+=_nonleaves(child)\n                return retval",
  "def _recurse_do_node(node):\n            for child in node.kids:\n\n                first_scen_idx = child.scenfirst\n                last_scen_idx = child.scenlast\n\n                ranks_in_node = list_of_ranks_per_scenario_idx[first_scen_idx:last_scen_idx+1]\n                minimum_rank_in_node = ranks_in_node[0]\n\n                # IMPORTANT:\n                # this accords with the way SPBase.create_communicators assigns the \"key\" when\n                # creating its comm for this node. E.g., the key is the existing rank, which\n                # will then be offset by the minimum rank. As the ranks within each node are\n                # contiguous, this is enough to infer the rank each scenario will have in this\n                # node's comm\n                within_comm_ranks_in_node = [(rank-minimum_rank_in_node) for rank in ranks_in_node]\n\n                scenarios_in_nodes = self.ScenNames[first_scen_idx:last_scen_idx+1]\n\n                scenario_names_to_rank[child.name] = { s : rank for s,rank in zip(scenarios_in_nodes, within_comm_ranks_in_node) }\n\n                if child not in self.NonLeafTerminals:\n                    _recurse_do_node(child)",
  "class Find_Grad():\n    \"\"\"Interface to compute and write gradient cost\n    \n    Args:\n       ph_object (PHBase): ph object\n       cfg (Config): config object\n\n    Attributes:\n       c (dict): gradient cost\n\n    \"\"\"\n\n    def __init__(self,\n                 ph_object,\n                 cfg):\n        self.ph_object = ph_object\n        self.cfg = cfg\n        self.c = dict()\n\n    #======================================================================\n\n    def compute_grad(self, sname, scenario):\n        \"\"\" Computes gradient cost for a given scenario\n        \n        Args:\n           sname (str): the scenario name\n           scenario (Pyomo Concrete Model): scenario\n        \n        Returns:\n           grad_cost (dict): a dictionnary {nonant indice: gradient cost}\n\n        \"\"\"\n        nlp = PyomoNLP(scenario)\n        nlp_vars = nlp.get_pyomo_variables()\n        grad = nlp.evaluate_grad_objective()\n        grad_cost = {ndn_i: -grad[ndn_i[1]]\n                     for ndn_i, var in scenario._mpisppy_data.nonant_indices.items()}\n        return grad_cost\n        \n\n    def find_grad_cost(self):\n        \"\"\" Computes gradient cost for all scenarios.\n        \n        ASSUMES:\n           The cfg object should contain an xhat path corresponding to the xhat file.\n\n        \"\"\"\n        if self.cfg.grad_cost_file == '': pass\n        else:\n            assert self.cfg.xhatpath != '', \"to compute gradient cost, you have to give an xhat path using --xhatpath\"\n            \n            self.ph_object.disable_W_and_prox()\n            xhatfile = self.cfg.xhatpath\n            xhat = ciutils.read_xhat(xhatfile)\n            xhat_one = xhat[\"ROOT\"]\n            self.ph_object._save_nonants()\n            self.ph_object._fix_nonants(xhat)\n            self.ph_object.solve_loop()\n            for (sname, scenario) in self.ph_object.local_scenarios.items():\n                for node in scenario._mpisppy_node_list:\n                    for v in node.nonant_vardata_list:\n                        v.unfix()\n\n            grad_cost ={sname: self.compute_grad(sname, scenario) \n                        for sname, scenario in self.ph_object.local_scenarios.items()} \n            local_costs = {(sname, var.name): grad_cost[sname][node.name, ix]\n                           for (sname, scenario) in self.ph_object.local_scenarios.items()\n                           for node in scenario._mpisppy_node_list\n                           for (ix, var) in enumerate(node.nonant_vardata_list)}\n            comm = self.ph_object.comms['ROOT']\n            costs = comm.gather(local_costs, root=0)\n            rank = self.ph_object.cylinder_rank\n            if (self.ph_object.cylinder_rank == 0):\n                self.c = {key: val \n                          for cost in costs\n                          for key, val in cost.items()}\n            comm.Barrier()\n            self.ph_object._restore_nonants()\n            self.ph_object.reenable_W_and_prox()\n\n\n    def write_grad_cost(self):\n        \"\"\" Writes gradient cost for all scenarios.\n\n        ASSUMES: \n           The cfg object should contain an xhat path corresponding to the xhat file.\n\n        \"\"\"\n        self.find_grad_cost()\n        comm = self.ph_object.comms['ROOT']\n        if (self.ph_object.cylinder_rank == 0):\n            with open(self.cfg.grad_cost_file, 'a') as f:\n                writer = csv.writer(f)\n                writer.writerow(['#grad cost values'])\n                for (key, val) in self.c.items():\n                    sname, vname = key[0], key[1]\n                    writer.writerow([sname, vname, str(val)])\n        comm.Barrier()\n\n\n#====================================================================================\n\n    def find_grad_rho(self):\n        \"\"\"Writes gradient cost for all variables.\n\n        ASSUMES:\n           The cfg object should contain a grad_cost_file.\n\n        \"\"\"\n        assert self.cfg.grad_cost_file != '', \"to compute rho you have to give the name of a csv file (using --grad-cost-file) where grad cost will be written\"\n        if (not os.path.exists(self.cfg.grad_cost_file)):\n            raise RuntimeError('Could not find file {fn}'.format(fn=self.cfg.grad_cost_file))\n        self.cfg.whatpath = self.cfg.grad_cost_file\n        return find_rho.Find_Rho(self.ph_object, self.cfg).compute_rho()\n\n    def write_grad_rho(self):\n         \"\"\"Writes gradient rho for all variables.\n\n        ASSUMES:\n           The cfg object should contain a grad_cost_file.\n\n        \"\"\"\n         if self.cfg.grad_rho_file == '':\n             pass\n         else:\n             rho_data = self.find_grad_rho()\n             if self.ph_object.cylinder_rank == 0:\n                 with open(self.cfg.grad_rho_file, 'a', newline='') as file:\n                     writer = csv.writer(file)\n                     writer.writerow(['#grad rho values'])\n                     for (vname, rho) in rho_data.items():\n                         writer.writerow([vname, rho_data[vname]])",
  "def _parser_setup():\n    \"\"\" Set up config object and return it, but don't parse \n\n    Returns:\n       cfg (Config): config object\n\n    Notes:\n       parsers for the non-model-specific arguments; but the model_module_name will be pulled off first\n\n    \"\"\"\n\n    cfg = config.Config()\n    cfg.add_branching_factors()\n    cfg.num_scens_required()\n    cfg.popular_args()\n    cfg.two_sided_args()\n    cfg.ph_args()\n    \n    cfg.gradient_args()\n\n    return cfg",
  "def grad_cost_and_rho(mname, original_cfg):\n    \"\"\" Creates a ph object from cfg and using the module 'mname' functions. Then computes the corresponding grad cost and rho.\n\n    Args:\n       mname (str): module name\n       original_cfg (Config object): config object\n\n    \"\"\"\n    if  (original_cfg.grad_rho_file == '') and (original_cfg.grad_cost_file == ''): return\n\n    try:\n        model_module = importlib.import_module(mname)\n    except:\n        raise RuntimeError(f\"Could not import module: {mname}\")\n    cfg = copy.deepcopy(original_cfg)\n    cfg.max_iterations = 0 #we only need x0 here\n\n    #create ph_object via vanilla           \n    scenario_creator = model_module.scenario_creator\n    scenario_denouement = model_module.scenario_denouement\n    scen_names_creator_args = inspect.getfullargspec(model_module.scenario_names_creator).args #partition requires to do that\n    if scen_names_creator_args[0] == 'cfg':\n        all_scenario_names = model_module.scenario_names_creator(cfg)\n    else :\n        all_scenario_names = model_module.scenario_names_creator(cfg.num_scens)\n    scenario_creator_kwargs = model_module.kw_creator(cfg)\n    variable_probability = None\n    if hasattr(model_module, '_variable_probability'):\n        variable_probability = model_module._variable_probability\n    beans = (cfg, scenario_creator, scenario_denouement, all_scenario_names)\n    hub_dict = vanilla.ph_hub(*beans,\n                              scenario_creator_kwargs=scenario_creator_kwargs,\n                              ph_extensions=WXBarWriter,\n                              variable_probability=variable_probability)\n    list_of_spoke_dict = list()\n    wheel = WheelSpinner(hub_dict, list_of_spoke_dict)\n    wheel.spin() #TODO: steal only what's needed in  WheelSpinner\n    if wheel.strata_rank == 0:  # don't do this for bound ranks\n        ph_object = wheel.spcomm.opt\n    \n    #============================================================================== \n    # Compute grad cost and rhos\n    Find_Grad(ph_object, cfg).write_grad_cost()\n    Find_Grad(ph_object, cfg).write_grad_rho()",
  "def __init__(self,\n                 ph_object,\n                 cfg):\n        self.ph_object = ph_object\n        self.cfg = cfg\n        self.c = dict()",
  "def compute_grad(self, sname, scenario):\n        \"\"\" Computes gradient cost for a given scenario\n        \n        Args:\n           sname (str): the scenario name\n           scenario (Pyomo Concrete Model): scenario\n        \n        Returns:\n           grad_cost (dict): a dictionnary {nonant indice: gradient cost}\n\n        \"\"\"\n        nlp = PyomoNLP(scenario)\n        nlp_vars = nlp.get_pyomo_variables()\n        grad = nlp.evaluate_grad_objective()\n        grad_cost = {ndn_i: -grad[ndn_i[1]]\n                     for ndn_i, var in scenario._mpisppy_data.nonant_indices.items()}\n        return grad_cost",
  "def find_grad_cost(self):\n        \"\"\" Computes gradient cost for all scenarios.\n        \n        ASSUMES:\n           The cfg object should contain an xhat path corresponding to the xhat file.\n\n        \"\"\"\n        if self.cfg.grad_cost_file == '': pass\n        else:\n            assert self.cfg.xhatpath != '', \"to compute gradient cost, you have to give an xhat path using --xhatpath\"\n            \n            self.ph_object.disable_W_and_prox()\n            xhatfile = self.cfg.xhatpath\n            xhat = ciutils.read_xhat(xhatfile)\n            xhat_one = xhat[\"ROOT\"]\n            self.ph_object._save_nonants()\n            self.ph_object._fix_nonants(xhat)\n            self.ph_object.solve_loop()\n            for (sname, scenario) in self.ph_object.local_scenarios.items():\n                for node in scenario._mpisppy_node_list:\n                    for v in node.nonant_vardata_list:\n                        v.unfix()\n\n            grad_cost ={sname: self.compute_grad(sname, scenario) \n                        for sname, scenario in self.ph_object.local_scenarios.items()} \n            local_costs = {(sname, var.name): grad_cost[sname][node.name, ix]\n                           for (sname, scenario) in self.ph_object.local_scenarios.items()\n                           for node in scenario._mpisppy_node_list\n                           for (ix, var) in enumerate(node.nonant_vardata_list)}\n            comm = self.ph_object.comms['ROOT']\n            costs = comm.gather(local_costs, root=0)\n            rank = self.ph_object.cylinder_rank\n            if (self.ph_object.cylinder_rank == 0):\n                self.c = {key: val \n                          for cost in costs\n                          for key, val in cost.items()}\n            comm.Barrier()\n            self.ph_object._restore_nonants()\n            self.ph_object.reenable_W_and_prox()",
  "def write_grad_cost(self):\n        \"\"\" Writes gradient cost for all scenarios.\n\n        ASSUMES: \n           The cfg object should contain an xhat path corresponding to the xhat file.\n\n        \"\"\"\n        self.find_grad_cost()\n        comm = self.ph_object.comms['ROOT']\n        if (self.ph_object.cylinder_rank == 0):\n            with open(self.cfg.grad_cost_file, 'a') as f:\n                writer = csv.writer(f)\n                writer.writerow(['#grad cost values'])\n                for (key, val) in self.c.items():\n                    sname, vname = key[0], key[1]\n                    writer.writerow([sname, vname, str(val)])\n        comm.Barrier()",
  "def find_grad_rho(self):\n        \"\"\"Writes gradient cost for all variables.\n\n        ASSUMES:\n           The cfg object should contain a grad_cost_file.\n\n        \"\"\"\n        assert self.cfg.grad_cost_file != '', \"to compute rho you have to give the name of a csv file (using --grad-cost-file) where grad cost will be written\"\n        if (not os.path.exists(self.cfg.grad_cost_file)):\n            raise RuntimeError('Could not find file {fn}'.format(fn=self.cfg.grad_cost_file))\n        self.cfg.whatpath = self.cfg.grad_cost_file\n        return find_rho.Find_Rho(self.ph_object, self.cfg).compute_rho()",
  "def write_grad_rho(self):\n         \"\"\"Writes gradient rho for all variables.\n\n        ASSUMES:\n           The cfg object should contain a grad_cost_file.\n\n        \"\"\"\n         if self.cfg.grad_rho_file == '':\n             pass\n         else:\n             rho_data = self.find_grad_rho()\n             if self.ph_object.cylinder_rank == 0:\n                 with open(self.cfg.grad_rho_file, 'a', newline='') as file:\n                     writer = csv.writer(file)\n                     writer.writerow(['#grad rho values'])\n                     for (vname, rho) in rho_data.items():\n                         writer.writerow([vname, rho_data[vname]])",
  "def solver_specification(cfg, prefix=\"\", name_required=True):\n    \"\"\" Look through cfg to find the soler_name and solver_options.\n\n    Args:\n        cfg (Config): options, typically from the command line\n        prefix (str or list of str): the prefix strings (e.g. \"PH\", \"\", \"Lagranger\")\n        name_required (boolean): throw an error if we don't get a solver name\n\n    Returns:\n        sroot (str): the root string found (or None)\n        solver_name (str): the solver name (or None)\n        solver_options (dict): the options dictionary created from the string\n    \"\"\"\n    \n    if isinstance(prefix, (list,tuple)):\n        root_list = prefix\n    else:\n        root_list = [prefix, ]\n\n    idx_list = list()\n    for sroot in root_list:\n        name_idx = \"solver_name\" if sroot == \"\" else f\"{sroot}_solver_name\"\n        idx_list.append(name_idx)\n        if cfg.get(name_idx) is not None:\n            solver_name = cfg[name_idx]\n            options_idx = \"solver_options\" if sroot == \"\" else f\"{sroot}_solver_options\"\n            ostr = cfg.get(options_idx)\n            solver_options = sputils.option_string_to_dict(ostr)  # will return None for None\n            break\n    else:\n        if name_required:\n            # Leaving in underscores even though it might confuse command line users\n            print(f\"\\nsolver name arguments checked in Config object = {idx_list}\\n\")\n            raise RuntimeError(f\"The Config object did not specify a solver\")\n    return sroot, solver_name, solver_options",
  "def _hasit(cfg, argname):\n    # aside: Config objects act like a dict or an object TBD: so why the and?\n    return cfg.get(argname) is not None and cfg[argname] is not None",
  "def shared_options(cfg):\n    shoptions = {\n        \"solver_name\": cfg.solver_name,\n        \"defaultPHrho\": cfg.default_rho,\n        \"convthresh\": 0,\n        \"PHIterLimit\": cfg.max_iterations,  # not needed by all\n        \"verbose\": cfg.verbose,\n        \"display_progress\": cfg.display_progress,\n        \"display_convergence_detail\": cfg.display_convergence_detail,\n        \"iter0_solver_options\": dict(),\n        \"iterk_solver_options\": dict(),\n        \"tee-rank0-solves\": cfg.tee_rank0_solves,\n        \"trace_prefix\" : cfg.trace_prefix,\n    }\n    if _hasit(cfg, \"max_solver_threads\"):\n        shoptions[\"iter0_solver_options\"][\"threads\"] = cfg.max_solver_threads\n        shoptions[\"iterk_solver_options\"][\"threads\"] = cfg.max_solver_threads\n    if _hasit(cfg, \"iter0_mipgap\"):\n        shoptions[\"iter0_solver_options\"][\"mipgap\"] = cfg.iter0_mipgap\n    if _hasit(cfg, \"iterk_mipgap\"):\n        shoptions[\"iterk_solver_options\"][\"mipgap\"] = cfg.iterk_mipgap\n    return shoptions",
  "def add_multistage_options(cylinder_dict,all_nodenames,branching_factors):\n    cylinder_dict = copy.deepcopy(cylinder_dict)\n    if branching_factors is not None:\n        if hasattr(cylinder_dict[\"opt_kwargs\"], \"options\"):\n            cylinder_dict[\"opt_kwargs\"][\"options\"][\"branching_factors\"] = branching_factors\n        if all_nodenames is None:\n            all_nodenames = sputils.create_nodenames_from_branching_factors(branching_factors)\n    if all_nodenames is not None:\n        print(\"Hello, surprise !!\")\n        cylinder_dict[\"opt_kwargs\"][\"all_nodenames\"] = all_nodenames\n    print(\"Hello,\",cylinder_dict)\n    return cylinder_dict",
  "def ph_hub(\n        cfg,\n        scenario_creator,\n        scenario_denouement,\n        all_scenario_names,\n        scenario_creator_kwargs=None,\n        ph_extensions=None,\n        extension_kwargs=None,\n        ph_converger=None,\n        rho_setter=None,\n        variable_probability=None,\n        all_nodenames=None,\n):\n    shoptions = shared_options(cfg)\n    options = copy.deepcopy(shoptions)\n    options[\"convthresh\"] = cfg.intra_hub_conv_thresh\n    options[\"bundles_per_rank\"] = cfg.bundles_per_rank\n    options[\"linearize_binary_proximal_terms\"] = cfg.linearize_binary_proximal_terms\n    options[\"linearize_proximal_terms\"] = cfg.linearize_proximal_terms\n    options[\"proximal_linearization_tolerance\"] = cfg.proximal_linearization_tolerance\n\n    if _hasit(cfg, \"cross_scenario_cuts\") and cfg.cross_scenario_cuts:\n        hub_class = CrossScenarioHub\n    else:\n        hub_class = PHHub\n\n    hub_dict = {\n        \"hub_class\": hub_class,\n        \"hub_kwargs\": {\"options\": {\"rel_gap\": cfg.rel_gap,\n                                   \"abs_gap\": cfg.abs_gap,\n                                   \"max_stalled_iters\": cfg.max_stalled_iters}},\n        \"opt_class\": PH,\n        \"opt_kwargs\": {\n            \"options\": options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement,\n            \"rho_setter\": rho_setter,\n            \"variable_probability\": variable_probability,\n            \"extensions\": ph_extensions,\n            \"extension_kwargs\": extension_kwargs,\n            \"ph_converger\": ph_converger,\n            \"all_nodenames\": all_nodenames\n        }\n    }\n    add_wxbar_read_write(hub_dict, cfg)\n    add_ph_tracking(hub_dict, cfg)\n    return hub_dict",
  "def aph_hub(cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n    ph_extensions=None,\n    extension_kwargs=None,\n    rho_setter=None,\n    variable_probability=None,\n    all_nodenames=None,\n):\n    # TBD: March 2023: multiple extensions needs work\n    hub_dict = ph_hub(cfg,\n                      scenario_creator,\n                      scenario_denouement,\n                      all_scenario_names,\n                      scenario_creator_kwargs=scenario_creator_kwargs,\n                      ph_extensions=ph_extensions,\n                      extension_kwargs=extension_kwargs,\n                      rho_setter=rho_setter,\n                      variable_probability=variable_probability,\n                      all_nodenames = all_nodenames,\n                    )\n\n    hub_dict['hub_class'] = APHHub\n    hub_dict['opt_class'] = APH\n\n    hub_dict['opt_kwargs']['options']['APHgamma'] = cfg.aph_gamma\n    hub_dict['opt_kwargs']['options']['APHnu'] = cfg.aph_nu\n    hub_dict['opt_kwargs']['options']['async_frac_needed'] = cfg.aph_frac_needed\n    hub_dict['opt_kwargs']['options']['dispatch_frac'] = cfg.aph_dispatch_frac\n    hub_dict['opt_kwargs']['options']['async_sleep_secs'] = cfg.aph_sleep_seconds\n\n    return hub_dict",
  "def extension_adder(hub_dict,ext_class):\n    # TBD March 2023: this is not really good enough\n    if \"extensions\" not in hub_dict[\"opt_kwargs\"] or \\\n        hub_dict[\"opt_kwargs\"][\"extensions\"] is None:\n        hub_dict[\"opt_kwargs\"][\"extensions\"] = ext_class\n    elif hub_dict[\"opt_kwargs\"][\"extensions\"] == MultiExtension:\n        if hub_dict[\"opt_kwargs\"][\"extension_kwargs\"] is None:\n            hub_dict[\"opt_kwargs\"][\"extension_kwargs\"] = {\"ext_classes\": []}\n        if not ext_class in hub_dict[\"opt_kwargs\"][\"extension_kwargs\"][\"ext_classes\"]:\n            hub_dict[\"opt_kwargs\"][\"extension_kwargs\"][\"ext_classes\"].append(ext_class)\n    elif hub_dict[\"opt_kwargs\"][\"extensions\"] != ext_class:\n        #ext_class is the second extension\n        if not \"extensions_kwargs\" in hub_dict[\"opt_kwargs\"]:\n            hub_dict[\"opt_kwargs\"][\"extension_kwargs\"] = {}\n        hub_dict[\"opt_kwargs\"][\"extension_kwargs\"][\"ext_classes\"] = \\\n            [hub_dict[\"opt_kwargs\"][\"extensions\"], ext_class]\n        hub_dict[\"opt_kwargs\"][\"extensions\"] = MultiExtension\n    return hub_dict",
  "def add_fixer(hub_dict,\n              cfg,\n              ):\n    hub_dict = extension_adder(hub_dict,Fixer)\n    hub_dict[\"opt_kwargs\"][\"options\"][\"fixeroptions\"] = {\"verbose\":False,\n                                              \"boundtol\": cfg.fixer_tol,\n                                              \"id_fix_list_fct\": cfg.id_fix_list_fct}\n    return hub_dict",
  "def add_cross_scenario_cuts(hub_dict,\n                            cfg,\n                            ):\n    #WARNING: Do not use without a cross_scenario_cuts spoke\n    hub_dict = extension_adder(hub_dict, CrossScenarioExtension)\n    hub_dict[\"opt_kwargs\"][\"options\"][\"cross_scen_options\"]\\\n            = {\"check_bound_improve_iterations\" : cfg.cross_scenario_iter_cnt}\n    return hub_dict",
  "def add_wxbar_read_write(hub_dict, cfg):\n    \"\"\"\n    Add the wxbar read and write extensions to the hub_dict\n\n    NOTE\n    At the moment, the options are not stored in a extension options dict()\n    but are 'loose' in the hub options dict\n    \"\"\"\n    if _hasit(cfg, 'init_W_fname') or _hasit(cfg, 'init_Xbar_fname'):\n        hub_dict = extension_adder(hub_dict, WXBarReader)\n        hub_dict[\"opt_kwargs\"][\"options\"].update(\n            {\"init_W_fname\" : cfg.init_W_fname,\n             \"init_Xbar_fname\" : cfg.init_Xbar_fname,\n             \"init_separate_W_files\" : cfg.init_separate_W_files\n            })\n    if _hasit(cfg, 'W_fname') or _hasit(cfg, 'Xbar_fname'):\n        hub_dict = extension_adder(hub_dict, WXBarWriter)\n        hub_dict[\"opt_kwargs\"][\"options\"].update(\n            {\"W_fname\" : cfg.W_fname,\n             \"Xbar_fname\" : cfg.Xbar_fname,\n             \"separate_W_files\" : cfg.separate_W_files\n            })\n    return hub_dict",
  "def add_ph_tracking(cylinder_dict, cfg, spoke=False):\n    \"\"\" Manage the phtracker extension and bridge gap between config and ph options dict\n        Args:\n            cylinder_dict (dict): the hub or spoke dictionary\n            cfg (dict): the configuration dictionary\n            spoke (bool, optional): Whether the cylinder is a spoke. Defaults to False.\n        Returns:\n            cylinder_dict (dict): the updated hub or spoke dictionary\n\n        for cfg.track_* flags, the ints are mapped as followed:\n\n        0: do not track\n        1: track for all cylinders\n        2: track for hub only\n        3: track for spokes only\n        4: track and plot for all cylinders\n        5: track and plot for hub\n        6: track and plot for spokes\n\n        If 'ph_track_progress' is True in the cfg dictionary, this function adds the\n        ph tracking extension to the cylinder dict with the specified tracking options.\n    \"\"\"\n    if _hasit(cfg, 'ph_track_progress') and cfg.ph_track_progress:\n        cylinder_dict = extension_adder(cylinder_dict, PHTracker)\n        phtrackeroptions = {\"results_folder\": cfg.tracking_folder}\n\n        t_vars = ['convergence', 'xbars', 'duals', 'nonants', 'scen_gaps']\n        for t_var in t_vars:\n            if _hasit(cfg, f'track_{t_var}'):\n                trval = cfg[f'track_{t_var}']\n                if ((trval in {1, 4} or \\\n                    (not spoke and trval in {2, 5}) or \\\n                    (spoke and trval in {3, 6}))):\n                    phtrackeroptions[f'track_{t_var}'] = True\n\n                    if trval in {4, 5, 6}:\n                        phtrackeroptions[f'plot_{t_var}'] = True\n\n        # disabled until we finalize hub bounds passing\n        # # because convergence maps to multiple tracking options\n        # if phtrackeroptions.get('track_convergence'):\n        #     phtrackeroptions['track_bounds'] = True\n        #     phtrackeroptions['track_gaps'] = True\n        # if phtrackeroptions.get('plot_convergence'):\n        #     phtrackeroptions['plot_bounds'] = True\n        #     phtrackeroptions['plot_gaps'] = True\n\n        cylinder_dict[\"opt_kwargs\"][\"options\"][\"phtracker_options\"] = phtrackeroptions\n\n        # only needed if buffers need to be dynamically resized to track hub bounds\n        if spoke and ('track_gaps' in phtrackeroptions and phtrackeroptions['track_gaps'] \\\n            or 'track_bounds' in phtrackeroptions and phtrackeroptions['track_bounds']):\n            if 'spoke_kwargs' not in cylinder_dict:\n                cylinder_dict['spoke_kwargs'] = {}\n            if 'options' not in cylinder_dict['spoke_kwargs']:\n                cylinder_dict['spoke_kwargs']['options'] = {}\n            cylinder_dict['spoke_kwargs']['options']['get_hub_bounds'] = True\n\n    return cylinder_dict",
  "def fwph_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n    all_nodenames=None,\n):\n    shoptions = shared_options(cfg)\n\n    mip_solver_options, qp_solver_options = dict(), dict()\n    if _hasit(cfg, \"max_solver_threads\"):\n        mip_solver_options[\"threads\"] = cfg.max_solver_threads\n        qp_solver_options[\"threads\"] = cfg.max_solver_threads\n    if _hasit(cfg, \"fwph_mipgap\"):\n        mip_solver_options[\"mipgap\"] = cfg.fwph_mipgap\n\n    fw_options = {\n        \"FW_iter_limit\": cfg.fwph_iter_limit,\n        \"FW_weight\": cfg.fwph_weight,\n        \"FW_conv_thresh\": cfg.fwph_conv_thresh,\n        \"stop_check_tol\": cfg.fwph_stop_check_tol,\n        \"solver_name\": cfg.solver_name,\n        \"FW_verbose\": cfg.verbose,\n        \"mip_solver_options\" : mip_solver_options,\n        \"qp_solver_options\" : qp_solver_options,\n    }\n    fw_dict = {\n        \"spoke_class\": FrankWolfeOuterBound,\n        \"opt_class\": FWPH,\n        \"opt_kwargs\": {\n            \"PH_options\": shoptions,  # be sure convthresh is zero for fwph\n            \"FW_options\": fw_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement,\n            \"all_nodenames\": all_nodenames\n        },\n    }\n    return fw_dict",
  "def lagrangian_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n    rho_setter=None,\n    all_nodenames=None,\n):\n    shoptions = shared_options(cfg)\n    lagrangian_spoke = {\n        \"spoke_class\": LagrangianOuterBound,\n        \"opt_class\": PHBase,\n        \"opt_kwargs\": {\n            \"options\": shoptions,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            'scenario_denouement': scenario_denouement,\n            \"rho_setter\": rho_setter,\n            \"all_nodenames\": all_nodenames\n\n        }\n    }\n    if cfg.lagrangian_iter0_mipgap is not None:\n        lagrangian_spoke[\"opt_kwargs\"][\"options\"][\"iter0_solver_options\"]\\\n            [\"mipgap\"] = cfg.lagrangian_iter0_mipgap\n    if cfg.lagrangian_iterk_mipgap is not None:\n        lagrangian_spoke[\"opt_kwargs\"][\"options\"][\"iterk_solver_options\"]\\\n            [\"mipgap\"] = cfg.lagrangian_iterk_mipgap\n    add_ph_tracking(lagrangian_spoke, cfg, spoke=True)\n\n    return lagrangian_spoke",
  "def lagranger_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n    rho_setter=None,\n    all_nodenames = None,\n):\n    shoptions = shared_options(cfg)\n    lagranger_spoke = {\n        \"spoke_class\": LagrangerOuterBound,\n        \"opt_class\": PHBase,\n        \"opt_kwargs\": {\n            \"options\": shoptions,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            'scenario_denouement': scenario_denouement,\n            \"rho_setter\": rho_setter,\n            \"all_nodenames\": all_nodenames\n        }\n    }\n    if cfg.lagranger_iter0_mipgap is not None:\n        lagranger_spoke[\"opt_kwargs\"][\"options\"][\"iter0_solver_options\"]\\\n            [\"mipgap\"] = cfg.lagranger_iter0_mipgap\n    if cfg.lagranger_iterk_mipgap is not None:\n        lagranger_spoke[\"opt_kwargs\"][\"options\"][\"iterk_solver_options\"]\\\n            [\"mipgap\"] = cfg.lagranger_iterk_mipgap\n    if cfg.lagranger_rho_rescale_factors_json is not None:\n        lagranger_spoke[\"opt_kwargs\"][\"options\"]\\\n            [\"lagranger_rho_rescale_factors_json\"]\\\n            = cfg.lagranger_rho_rescale_factors_json\n    add_ph_tracking(lagranger_spoke, cfg, spoke=True)\n    return lagranger_spoke",
  "def xhatlooper_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n):\n\n    shoptions = shared_options(cfg)\n    xhat_options = copy.deepcopy(shoptions)\n    xhat_options['bundles_per_rank'] = 0 #  no bundles for xhat\n    xhat_options[\"xhat_looper_options\"] = {\n        \"xhat_solver_options\": shoptions[\"iterk_solver_options\"],\n        \"scen_limit\": cfg.xhat_scen_limit,\n        \"dump_prefix\": \"delme\",\n        \"csvname\": \"looper.csv\",\n    }\n    xhatlooper_dict = {\n        \"spoke_class\": XhatLooperInnerBound,\n        \"opt_class\": Xhat_Eval,\n        \"opt_kwargs\": {\n            \"options\": xhat_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement\n        },\n    }\n    return xhatlooper_dict",
  "def xhatxbar_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n    variable_probability=None\n):\n\n    shoptions = shared_options(cfg)\n    xhat_options = copy.deepcopy(shoptions)\n    xhat_options['bundles_per_rank'] = 0 #  no bundles for xhat\n    xhat_options[\"xhat_xbar_options\"] = {\n        \"xhat_solver_options\": shoptions[\"iterk_solver_options\"],\n        \"scen_limit\": cfg.xhat_scen_limit,\n        \"dump_prefix\": \"delme\",\n        \"csvname\": \"xbar.csv\",\n    }\n    xhatxbar_dict = {\n        \"spoke_class\": XhatXbarInnerBound,\n        \"opt_class\": Xhat_Eval,\n        \"opt_kwargs\": {\n            \"options\": xhat_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement,\n            \"variable_probability\": variable_probability\n        },\n    }\n    return xhatxbar_dict",
  "def xhatshuffle_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    all_nodenames=None,\n    scenario_creator_kwargs=None,\n):\n\n    shoptions = shared_options(cfg)\n    xhat_options = copy.deepcopy(shoptions)\n    xhat_options['bundles_per_rank'] = 0 #  no bundles for xhat\n    xhat_options[\"xhat_looper_options\"] = {\n        \"xhat_solver_options\": shoptions[\"iterk_solver_options\"],\n        \"dump_prefix\": \"delme\",\n        \"csvname\": \"looper.csv\",\n    }\n    if _hasit(cfg, \"add_reversed_shuffle\"):\n        xhat_options[\"xhat_looper_options\"][\"reverse\"] = cfg.add_reversed_shuffle\n    if _hasit(cfg, \"add_reversed_shuffle\"):\n        xhat_options[\"xhat_looper_options\"][\"xhatshuffle_iter_step\"] = cfg.xhatshuffle_iter_step\n\n    xhatlooper_dict = {\n        \"spoke_class\": XhatShuffleInnerBound,\n        \"opt_class\": Xhat_Eval,\n        \"opt_kwargs\": {\n            \"options\": xhat_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement,\n            \"all_nodenames\": all_nodenames\n        },\n    }\n\n    return xhatlooper_dict",
  "def xhatspecific_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_dict,\n    all_nodenames=None,\n    scenario_creator_kwargs=None,\n):\n\n    shoptions = shared_options(cfg)\n    xhat_options = copy.deepcopy(shoptions)\n    xhat_options[\"xhat_specific_options\"] = {\n        \"xhat_solver_options\": shoptions[\"iterk_solver_options\"],\n        \"xhat_scenario_dict\": scenario_dict,\n        \"csvname\": \"specific.csv\",\n    }\n\n    xhat_options['bundles_per_rank'] = 0 #  no bundles for xhat\n    xhatspecific_dict = {\n        \"spoke_class\": XhatSpecificInnerBound,\n        \"opt_class\": Xhat_Eval,\n        \"opt_kwargs\": {\n            \"options\": xhat_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement,\n            \"all_nodenames\": all_nodenames\n        },\n    }\n\n    return xhatspecific_dict",
  "def xhatlshaped_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n):\n\n    shoptions = shared_options(cfg)\n    xhat_options = copy.deepcopy(shoptions)\n    xhat_options['bundles_per_rank'] = 0 #  no bundles for xhat\n\n    xhatlshaped_dict = {\n        \"spoke_class\": XhatLShapedInnerBound,\n        \"opt_class\": Xhat_Eval,\n        \"opt_kwargs\": {\n            \"options\": xhat_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement\n        },\n    }\n    return xhatlshaped_dict",
  "def slammax_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n):\n\n    shoptions = shared_options(cfg)\n    xhat_options = copy.deepcopy(shoptions)\n    xhat_options['bundles_per_rank'] = 0 #  no bundles for xhat\n    xhatlooper_dict = {\n        \"spoke_class\": SlamMaxHeuristic,\n        \"opt_class\": Xhat_Eval,\n        \"opt_kwargs\": {\n            \"options\": xhat_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement\n        },\n    }\n    return xhatlooper_dict",
  "def slammin_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n):\n\n    shoptions = shared_options(cfg)\n    xhat_options = copy.deepcopy(shoptions)\n    xhat_options['bundles_per_rank'] = 0 #  no bundles for xhat\n    xhatlooper_dict = {\n        \"spoke_class\": SlamMinHeuristic,\n        \"opt_class\": Xhat_Eval,\n        \"opt_kwargs\": {\n            \"options\": xhat_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement\n        },\n    }\n    return xhatlooper_dict",
  "def cross_scenario_cuts_spoke(\n    cfg,\n    scenario_creator,\n    scenario_denouement,\n    all_scenario_names,\n    scenario_creator_kwargs=None,\n    all_nodenames=None,\n):\n\n    if _hasit(cfg, \"max_solver_threads\"):\n        sp_solver_options = {\"threads\":cfg.max_solver_threads}\n    else:\n        sp_solver_options = dict()\n\n    if _hasit(cfg, \"eta_bounds_mipgap\"):\n        sp_solver_options[\"mipgap\"] = cfg.eta_bounds_mipgap\n\n    ls_options = { \"root_solver\" : cfg.solver_name,\n                   \"sp_solver\": cfg.solver_name,\n                   \"sp_solver_options\" : sp_solver_options,\n                    \"verbose\": cfg.verbose,\n                 }\n    cut_spoke = {\n        \"spoke_class\": CrossScenarioCutSpoke,\n        \"opt_class\": LShapedMethod,\n        \"opt_kwargs\": {\n            \"options\": ls_options,\n            \"all_scenario_names\": all_scenario_names,\n            \"scenario_creator\": scenario_creator,\n            \"scenario_creator_kwargs\": scenario_creator_kwargs,\n            \"scenario_denouement\": scenario_denouement,\n            \"all_nodenames\": all_nodenames\n            },\n        }\n\n    return cut_spoke",
  "def _common_args(inparser):\n    # NOTE: if you want abbreviations, override the arguments in your example\n    # do not add abbreviations here.\n\n    parser = inparser\n    parser.add_argument(\"--max-iterations\",\n                        help=\"ph max-iterations (default 1)\",\n                        dest=\"max_iterations\",\n                        type=int,\n                        default=1)\n\n    parser.add_argument(\"--solver-name\",\n                        help = \"solver name (default gurobi)\",\n                        dest=\"solver_name\",\n                        type = str,\n                        default=\"gurobi\")\n\n    parser.add_argument(\"--seed\",\n                        help=\"Seed for random numbers (default is 1134)\",\n                        dest=\"seed\",\n                        type=int,\n                        default=1134)\n\n    parser.add_argument(\"--default-rho\",\n                        help=\"Global rho for PH (default None)\",\n                        dest=\"default_rho\",\n                        type=float,\n                        default=None)\n\n    parser.add_argument(\"--bundles-per-rank\",\n                        help=\"bundles per rank (default 0 (no bundles))\",\n                        dest=\"bundles_per_rank\",\n                        type=int,\n                        default=0)                \n\n    parser.add_argument('--with-verbose',\n                        help=\"verbose output\",\n                        dest='with_verbose',\n                        action='store_true')\n    parser.add_argument('--no-verbose',\n                        help=\"do not verbose output (default)\",\n                        dest='with_verbose',\n                        action='store_false')\n    parser.set_defaults(with_verbose=False)\n\n    parser.add_argument('--with-display-progress',\n                        help=\"display progress at each iteration\",\n                        dest='with_display_progress',\n                        action='store_true')\n    parser.add_argument('--no-display-progress',\n                        help=\"do not display progress at each iteration (default)\",\n                        dest='with_display_progress',\n                        action='store_false')\n    parser.set_defaults(with_display_progress=False)\n\n    parser.add_argument('--with-display-convergence-detail',\n                        help=\"display non-anticipative variable convergence statistics at each iteration\",\n                        dest='with_display_convergence_detail',\n                        action='store_true')\n    parser.add_argument('--no-display-convergence-detail',\n                        help=\"do not display non-anticipative variable convergence statistics at each iteration (default)\",\n                        dest='with_display_convergence_detail',\n                        action='store_false')\n    parser.set_defaults(with_display_convergence_detail=False)    \n\n    parser.add_argument(\"--max-solver-threads\",\n                        help=\"Limit on threads per solver (default None)\",\n                        dest=\"max_solver_threads\",\n                        type=int,\n                        default=None)\n\n    parser.add_argument(\"--intra-hub-conv-thresh\",\n                        help=\"Within hub convergence threshold (default 1e-10)\",\n                        dest=\"intra_hub_conv_thresh\",\n                        type=float,\n                        default=1e-10)\n\n    parser.add_argument(\"--trace-prefix\",\n                        help=\"Prefix for bound spoke trace files. If None \"\n                             \"bound spoke trace files are not written.\",\n                        dest=\"trace_prefix\",\n                        type=str,\n                        default=None)\n\n    parser.add_argument(\"--with-tee-rank0-solves\",\n                        help=\"Some cylinders support tee of rank 0 solves.\"\n                        \"(With multiple cylinder this could be confusing.)\",\n                        dest=\"tee_rank0_solves\",\n                        action='store_true')\n    parser.add_argument(\"--no-tee-rank0-solves\",\n                        help=\"Some cylinders support tee of rank 0 solves.\",\n                        dest=\"tee_rank0_solves\",\n                        action='store_false')\n    parser.set_defaults(tee_rank0_solves=False)\n\n    parser.add_argument(\"--auxilliary\",\n                        help=\"Free text for use by hackers (default '').\",\n                        dest=\"auxilliary\",\n                        type=str,\n                        default='')\n\n    parser.add_argument(\"--linearize-binary-proximal-terms\",\n                        help=\"For PH, linearize the proximal terms for \"\n                        \"all binary nonanticipative variables\",\n                        dest=\"linearize_binary_proximal_terms\",\n                        action='store_true')\n\n    parser.add_argument(\"--linearize-proximal-terms\",\n                        help=\"For PH, linearize the proximal terms for \"\n                        \"all nonanticipative variables\",\n                        dest=\"linearize_proximal_terms\",\n                        action='store_true')\n\n    parser.add_argument(\"--proximal-linearization-tolerance\",\n                        help=\"For PH, when linearizing proximal terms, \"\n                        \"a cut will be added if the proximal term approximation \"\n                        \"is looser than this value (default 1e-1)\",\n                        dest=\"proximal_linearization_tolerance\",\n                        type=float,\n                        default=1.e-1)\n\n    return parser",
  "def make_parser(progname=None, num_scens_reqd=False):\n    # make a parser for the program named progname\n    # NOTE: if you want abbreviations, override the arguments in your example\n    # do not add abbreviations here.\n    parser = argparse.ArgumentParser(prog=progname, conflict_handler=\"resolve\")\n\n    if num_scens_reqd:\n        parser.add_argument(\n            \"num_scens\", help=\"Number of scenarios\", type=int\n        )\n    else:\n        parser.add_argument(\n            \"--num-scens\",\n            help=\"Number of scenarios (default None)\",\n            dest=\"num_scens\",\n            type=int,\n            default=None,\n        )\n    parser = _common_args(parser)\n    return parser",
  "def _basic_multistage(progname=None, num_scens_reqd=False):\n    parser = argparse.ArgumentParser(prog=progname, conflict_handler=\"resolve\")\n\n    parser.add_argument(\"--branching-factors\",\n                        help=\"Spaces delimited branching factors (e.g., 2 2)\",\n                        dest=\"branching_factors\",\n                        nargs=\"*\",\n                        type=int,\n                        default=None)\n        \n    return parser",
  "def make_multistage_parser(progname=None):\n    # make a parser for the program named progname\n    # NOTE: if you want abbreviations, override the arguments in your example\n    # do not add abbreviations here.\n    parser = _basic_multistage(progname=None)\n    parser = _common_args(parser)\n    return parser",
  "def make_EF2_parser(progname=None, num_scens_reqd=False):\n    # create a parser just for EF two-stage (does not call _common_args)\n    # NOTE: if you want abbreviations, override the arguments in your example\n    # do not add abbreviations here.\n    parser = argparse.ArgumentParser(prog=progname, conflict_handler=\"resolve\")\n\n    if num_scens_reqd:\n        parser.add_argument(\n            \"num_scens\", help=\"Number of scenarios\", type=int\n        )\n    else:\n        parser.add_argument(\n            \"--num-scens\",\n            help=\"Number of scenarios (default None)\",\n            dest=\"num_scens\",\n            type=int,\n            default=None,\n        )\n        \n    parser.add_argument(\"--EF-solver-name\",\n                        help = \"solver name (default gurobi)\",\n                        dest=\"EF_solver_name\",\n                        type = str,\n                        default=\"gurobi\")\n\n\n    parser.add_argument(\"--EF-mipgap\",\n                        help=\"mip gap option for the solver if needed (default None)\",\n                        dest=\"EF_mipgap\",\n                        type=float,\n                        default=None)\n    return parser",
  "def make_EF_multistage_parser(progname=None, num_scens_reqd=False):\n    # create a parser just for EF multi-stage (does not call _common_args)\n    # NOTE: if you want abbreviations, override the arguments in your example\n    # do not add abbreviations here.\n    parser = _basic_multistage(progname=None)\n    \n    if num_scens_reqd:\n        parser.add_argument(\n            \"num_scens\", help=\"Number of scenarios\", type=int\n        )\n    else:\n        parser.add_argument(\n            \"--num-scens\",\n            help=\"Number of scenarios (default None)\",\n            dest=\"num_scens\",\n            type=int,\n            default=None,\n        )\n        \n    parser.add_argument(\"--EF-solver-name\",\n                        help = \"solver name (default gurobi)\",\n                        dest=\"EF_solver_name\",\n                        type = str,\n                        default=\"gurobi\")\n\n\n    parser.add_argument(\"--EF-mipgap\",\n                        help=\"mip gap option for the solver if needed (default None)\",\n                        dest=\"EF_mipgap\",\n                        type=float,\n                        default=None)\n    return parser",
  "def two_sided_args(inparser):\n    # add commands to inparser and also return the result\n    parser = inparser\n    parser.add_argument(\"--rel-gap\",\n                        help=\"relative termination gap (default 0.05)\",\n                        dest=\"rel_gap\",\n                        type=float,\n                        default=0.05)\n\n    parser.add_argument(\"--abs-gap\",\n                        help=\"absolute termination gap (default 0)\",\n                        dest=\"abs_gap\",\n                        type=float,\n                        default=0.)\n    \n    parser.add_argument(\"--max-stalled-iters\",\n                        help=\"maximum iterations with no reduction in gap (default 100)\",\n                        dest=\"max_stalled_iters\",\n                        type=int,\n                        default=100)\n\n    return parser",
  "def mip_options(inparser):\n    parser = inparser\n    parser.add_argument(\"--iter0-mipgap\",\n                        help=\"mip gap option for iteration 0 (default None)\",\n                        dest=\"iter0_mipgap\",\n                        type=float,\n                        default=None)\n\n    parser.add_argument(\"--iterk-mipgap\",\n                        help=\"mip gap option non-zero iterations (default None)\",\n                        dest=\"iterk_mipgap\",\n                        type=float,\n                        default=None)\n    return parser",
  "def aph_args(inparser):\n    parser = inparser\n    parser.add_argument('--aph-gamma',\n                        help='Gamma parameter associated with asychronous projective hedging (default 1.0)',\n                        dest='aph_gamma',\n                        type=float,\n                        default=1.0)\n    parser.add_argument('--aph-nu',\n                        help='Nu parameter associated with asychronous projective hedging (default 1.0)',\n                        dest=\"aph_nu\",\n                        type=float,\n                        default=1.0)\n    parser.add_argument('--aph-frac-needed',\n                        help='Fraction of sub-problems required before computing projective step (default 1.0)',\n                        dest='aph_frac_needed',\n                        type=float,\n                        default=1.0)\n    parser.add_argument('--aph-dispatch-frac',\n                        help='Fraction of sub-problems to dispatch at each step of asychronous projective hedging (default 1.0)',\n                        dest='aph_dispatch_frac',\n                        type=float,\n                        default=1.0)\n    parser.add_argument('--aph-sleep-seconds',\n                        help='Spin-lock sleep time for APH (default 0.01)',\n                        dest='aph_sleep_seconds',\n                        type=float,\n                        default=0.01)    \n    return parser",
  "def fixer_args(inparser):\n    parser = inparser\n    parser.add_argument('--with-fixer',\n                        help=\"have an integer fixer extension (default)\",\n                        dest='with_fixer',\n                        action='store_true')\n    parser.add_argument('--no-fixer',\n                        help=\"do not have an integer fixer extension\",\n                        dest='with_fixer',\n                        action='store_false')\n    parser.set_defaults(with_fixer=True)\n\n    parser.add_argument(\"--fixer-tol\",\n                        help=\"fixer bounds tolerance  (default 1e-4)\",\n                        dest=\"fixer_tol\",\n                        type=float,\n                        default=1e-2)\n    return parser",
  "def fwph_args(inparser):\n    parser = inparser\n    parser.add_argument('--with-fwph',\n                        help=\"have an fwph spoke (default)\",\n                        dest='with_fwph',\n                        action='store_true')\n    parser.add_argument('--no-fwph',\n                        help=\"do not have an fwph spoke\",\n                        dest='with_fwph',\n                        action='store_false')\n    parser.set_defaults(with_fwph=True)\n\n    parser.add_argument(\"--fwph-iter-limit\",\n                        help=\"maximum fwph iterations (default 10)\",\n                        dest=\"fwph_iter_limit\",\n                        type=int,\n                        default=10)\n\n    parser.add_argument(\"--fwph-weight\",\n                        help=\"fwph weight (default 0)\",\n                        dest=\"fwph_weight\",\n                        type=float,\n                        default=0.0)\n\n    parser.add_argument(\"--fwph-conv-thresh\",\n                        help=\"fwph convergence threshold  (default 1e-4)\",\n                        dest=\"fwph_conv_thresh\",\n                        type=float,\n                        default=1e-4)\n\n    parser.add_argument(\"--fwph-stop-check-tol\",\n                        help=\"fwph tolerance for Gamma^t (default 1e-4)\",\n                        dest=\"fwph_stop_check_tol\",\n                        type=float,\n                        default=1e-4)\n\n    parser.add_argument(\"--fwph-mipgap\",\n                        help=\"mip gap option FW subproblems iterations (default None)\",\n                        dest=\"fwph_mipgap\",\n                        type=float,\n                        default=None)\n\n    return parser",
  "def lagrangian_args(inparser):\n    parser = inparser\n    parser.add_argument('--with-lagrangian',\n                        help=\"have a lagrangian spoke (default)\",\n                        dest='with_lagrangian',\n                        action='store_true')\n    parser.add_argument('--no-lagrangian',\n                        help=\"do not have a lagrangian spoke\",\n                        dest='with_lagrangian',\n                        action='store_false')\n    parser.set_defaults(with_lagrangian=True)\n\n    parser.add_argument(\"--lagrangian-iter0-mipgap\",\n                        help=\"lgr. iter0 solver option mipgap (default None)\",\n                        dest=\"lagrangian_iter0_mipgap\",\n                        type=float,\n                        default=None)\n\n    parser.add_argument(\"--lagrangian-iterk-mipgap\",\n                        help=\"lgr. iterk solver option mipgap (default None)\",\n                        dest=\"lagrangian_iterk_mipgap\",\n                        type=float,\n                        default=None)\n\n    return parser",
  "def lagranger_args(inparser):\n    parser = inparser\n    parser.add_argument('--with-lagranger',\n                        help=\"have a special lagranger spoke (default)\",\n                        dest='with_lagranger',\n                        action='store_true')\n    parser.add_argument('--no-lagranger',\n                        help=\"do not have a special lagranger spoke\",\n                        dest='with_lagranger',\n                        action='store_false')\n    parser.set_defaults(with_lagranger=True)\n\n    parser.add_argument(\"--lagranger-iter0-mipgap\",\n                        help=\"lagranger iter0 mipgap (default None)\",\n                        dest=\"lagranger_iter0_mipgap\",\n                        type=float,\n                        default=None)\n\n    parser.add_argument(\"--lagranger-iterk-mipgap\",\n                        help=\"lagranger iterk mipgap (default None)\",\n                        dest=\"lagranger_iterk_mipgap\",\n                        type=float,\n                        default=None)\n\n    parser.add_argument(\"--lagranger-rho-rescale-factors-json\",\n                        help=\"json file: rho rescale factors (default None)\",\n                        dest=\"lagranger_rho_rescale_factors_json\",\n                        type=str,\n                        default=None)\n\n    return parser",
  "def xhatlooper_args(inparser):\n    parser = inparser\n    parser.add_argument('--with-xhatlooper',\n                        help=\"have an xhatlooper spoke (default)\",\n                        dest='with_xhatlooper',\n                        action='store_true')\n    parser.add_argument('--no-xhatlooper',\n                        help=\"do not have an xhatlooper spoke\",\n                        dest='with_xhatlooper',\n                        action='store_false')\n    parser.set_defaults(with_xhatlooper=False)\n    parser.add_argument(\"--xhat-scen-limit\",\n                        help=\"scenario limit xhat looper to try (default 3)\",\n                        dest=\"xhat_scen_limit\",\n                        type=int,\n                        default=3)\n\n    return parser",
  "def xhatshuffle_args(inparser):\n    parser = inparser\n    parser.add_argument('--with-xhatshuffle',\n                        help=\"have an xhatshuffle spoke (default)\",\n                        dest='with_xhatshuffle',\n                        action='store_true')\n    parser.add_argument('--no-xhatshuffle',\n                        help=\"do not have an xhatshuffle spoke\",\n                        dest='with_xhatshuffle',\n                        action='store_false')\n    parser.set_defaults(with_xhatshuffle=True)\n    parser.add_argument('--add-reversed-shuffle',\n                        help=\"using also the reversed shuffling (multistage only, default True)\",\n                        dest = 'add_reversed_shuffle',\n                        action='store_true')\n    parser.set_defaults(add_reversed_shuffle=True)\n    parser.add_argument('--xhatshuffle-iter-step',\n                        help=\"step in shuffled list between 2 scenarios to try (default None)\",\n                        dest=\"xhatshuffle_iter_step\",\n                        type=int,\n                        default=None)\n\n    return parser",
  "def xhatspecific_args(inparser):\n    # we will not try to get the specification from the command line\n    parser = inparser\n    parser.add_argument('--with-xhatspecific',\n                        help=\"have an xhatspecific spoke (default)\",\n                        dest='with_xhatspecific',\n                        action='store_true')\n    parser.add_argument('--no-xhatspecific',\n                        help=\"do not have an xhatspecific spoke\",\n                        dest='with_xhatspecific',\n                        action='store_false')\n    parser.set_defaults(with_xhatspecific=False)\n\n    return parser",
  "def xhatlshaped_args(inparser):\n    # we will not try to get the specification from the command line\n    parser = inparser\n    parser.add_argument('--with-xhatlshaped',\n                        help=\"have an xhatlshaped spoke (default)\",\n                        dest='with_xhatlshaped',\n                        action='store_true')\n    parser.add_argument('--no-xhatlshaped',\n                        help=\"do not have an xhatlshaped spoke\",\n                        dest='with_xhatlshaped',\n                        action='store_false')\n    parser.set_defaults(with_xhatlshaped=True)\n\n    return parser",
  "def slammax_args(inparser):\n    # we will not try to get the specification from the command line\n    parser = inparser\n    parser.add_argument('--with-slammax',\n                        help=\"have an slammax spoke (default)\",\n                        dest='with_slammax',\n                        action='store_true')\n    parser.add_argument('--no-slammax',\n                        help=\"do not have an slammax spoke\",\n                        dest='with_slammax',\n                        action='store_false')\n    parser.set_defaults(with_slammax=True)\n\n    return parser",
  "def slammin_args(inparser):\n    # we will not try to get the specification from the command line\n    parser = inparser\n    parser.add_argument('--with-slammin',\n                        help=\"have an slammin spoke (default)\",\n                        dest='with_slammin',\n                        action='store_true')\n    parser.add_argument('--no-slammin',\n                        help=\"do not have an slammin spoke\",\n                        dest='with_slammin',\n                        action='store_false')\n    parser.set_defaults(with_slammin=True)\n\n    return parser",
  "def cross_scenario_cuts_args(inparser):\n    # we will not try to get the specification from the command line\n    parser = inparser\n    parser.add_argument('--with-cross-scenario-cuts',\n                        help=\"have a cross scenario cuts spoke (default)\",\n                        dest='with_cross_scenario_cuts',\n                        action='store_true')\n    parser.add_argument('--no-cross-scenario-cuts',\n                        help=\"do not have a cross scenario cuts spoke\",\n                        dest='with_cross_scenario_cuts',\n                        action='store_false')\n    parser.set_defaults(with_cross_scenario_cuts=True)\n\n    parser.add_argument(\"--cross-scenario-iter-cnt\",\n                        help=\"cross scen check bound improve iterations \"\n                        \"(default 4)\",\n                        dest=\"cross_scenario_iter_cnt\",\n                        type=int,\n                        default=4)\n\n    parser.add_argument(\"--eta-bounds-mipgap\",\n                        help=\"mipgap for determining eta bounds for cross \"\n                        \"scenario cuts (default 0.01)\",\n                        dest=\"eta_bounds_mipgap\",\n                        type=float,\n                        default=0.01)\n\n    return parser",
  "def nonsense(arg1, arg2, arg3): # Empty scenario_denouement\n    pass",
  "def read_test():\n    scen_count          = 3\n    scenario_creator    = uc_funcs.pysp2_callback\n    scenario_denouement = nonsense\n    scenario_rhosetter  = uc_funcs.scenario_rhos\n\n    PH_options = {\n        'solver_name': 'gurobi',\n        'PHIterLimit': 2,\n        'defaultPHrho': 1,\n        'convthresh': 1e-6,\n        'verbose': False,\n        'display_timing': False,\n        'display_progress': False,\n        'iter0_solver_options': dict(),\n        'iterk_solver_options': dict(),\n        'init_W_fname': 'david/weights.csv', # Path to the weight files\n        'init_separate_W_files': False,\n        'init_Xbar_fname': 'david/somexbars.csv',\n        'extensions':WXBarReader,\n        'rho_setter':scenario_rhosetter,\n    }\n\n    names = ['Scenario' + str(i+1) for i in range(scen_count)]\n\n    ph = PH(PH_options, names, scenario_creator, scenario_denouement)\n\n    conv, obj, bound = ph.ph_main()",
  "def write_test():\n    scen_count          = 3\n    scenario_creator    = uc_funcs.pysp2_callback\n    scenario_denouement = nonsense\n    scenario_rhosetter  = uc_funcs.scenario_rhos\n\n    PH_options = {\n        'solver_name': 'gurobi',\n        'PHIterLimit': 2,\n        'defaultPHrho': 1,\n        'convthresh': 1e-6,\n        'verbose': False,\n        'display_timing': False,\n        'display_progress': False,\n        'iter0_solver_options': dict(),\n        'iterk_solver_options': dict(),\n        'W_fname': 'david/weights.csv',\n        'separate_W_files': False,\n        'Xbar_fname': 'somexbars.csv',\n        'extensions':WXBarReader,\n        'rho_setter':scenario_rhosetter,\n    }\n\n    names = ['Scenario' + str(i+1) for i in range(scen_count)]\n\n    ph = PH(PH_options, names, scenario_creator, scenario_denouement)\n\n    conv, obj, bound = ph.ph_main()",
  "def _f(val, x_pnt, y_pnt):\n    return (( val - x_pnt )**2 + ( val**2 - y_pnt )**2)/2.",
  "def _df(val, x_pnt, y_pnt):\n    #return 2*(val - x_pnt) + 4*(val**2 - y_pnt)*val\n    return val*(1 - 2*y_pnt + 2*val*val) - x_pnt",
  "def _d2f(val, x_pnt, y_pnt):\n    return 1 + 6*val*val - 2*y_pnt",
  "def _newton_step(val, x_pnt, y_pnt):\n    return val - _df(val, x_pnt, y_pnt) / _d2f(val, x_pnt, y_pnt)",
  "class ProxApproxManager:\n    __slots__ = ()\n\n    def __new__(cls, xvar, xvarsqrd, xsqvar_cuts, ndn_i, initial_cut_quantity=2):\n        if xvar.is_integer():\n            return ProxApproxManagerDiscrete(xvar, xvarsqrd, xsqvar_cuts, ndn_i, initial_cut_quantity)\n        else:\n            return ProxApproxManagerContinuous(xvar, xvarsqrd, xsqvar_cuts, ndn_i, initial_cut_quantity)",
  "class _ProxApproxManager:\n    '''\n    A helper class to manage proximal approximations\n    '''\n    __slots__ = ()\n\n    def __init__(self, xvar, xvarsqrd, xsqvar_cuts, ndn_i, initial_cut_quantity):\n        self.xvar = xvar\n        self.xvarsqrd = xvarsqrd\n        self.var_index = ndn_i\n        self.cuts = xsqvar_cuts\n        self.cut_index = 0\n        self._verify_store_bounds(xvar)\n        self._create_initial_cuts(initial_cut_quantity)\n\n    def _verify_store_bounds(self, xvar):\n        if not (xvar.has_lb() and xvar.has_ub()):\n            raise RuntimeError(f\"linearize_nonbinary_proximal_terms requires all \"\n                                \"nonanticipative variables to have bounds\")\n        self.lb = value(xvar.lb)\n        self.ub = value(xvar.ub)\n\n    def _get_additional_points(self, initial_cut_quantity):\n        '''\n        calculate additional points for initial cuts\n        '''\n        # we add 2 cuts at the bound\n        if initial_cut_quantity <= 2:\n            return ()\n\n        lb, ub = self.lb, self.ub\n        bound_range = ub - lb\n        # n+1 points is n hyperplanes,\n        # but we've already added the bounds\n        delta = bound_range / (initial_cut_quantity-1)\n\n        return (lb + i*delta for i in range(1,initial_cut_quantity-1))\n\n    def _create_initial_cuts(self, initial_cut_quantity):\n        '''\n        create initial cuts at val\n        '''\n        pass\n\n    def add_cut(self, val, persistent_solver=None):\n        '''\n        create a cut at val\n        '''\n        pass\n\n    def check_tol_add_cut(self, tolerance, persistent_solver=None):\n        '''\n        add a cut if the tolerance is not satified\n        '''\n        x_pnt = self.xvar.value\n        y_pnt = self.xvarsqrd.value\n        f_val = x_pnt**2\n\n        #print(f\"y-distance: {actual_val - measured_val})\")\n\n        if (f_val - y_pnt) > tolerance:\n            '''\n            In this case, we project the point x_pnt, y_pnt onto\n            the curve y = x**2 by finding the minimum distance\n            between y = x**2 and x_pnt, y_pnt.\n\n            This involves solving a cubic equation, so instead\n            we start at x_pnt, y_pnt and run newtons algorithm\n            to get an approximate good-enough solution.\n            '''\n            this_val = x_pnt\n            #print(f\"initial distance: {_f(this_val, x_pnt, y_pnt)**(0.5)}\")\n            #print(f\"this_val: {this_val}\")\n            next_val = _newton_step(this_val, x_pnt, y_pnt)\n            while not isclose(this_val, next_val, rel_tol=1e-6, abs_tol=1e-6):\n                #print(f\"newton step distance: {_f(next_val, x_pnt, y_pnt)**(0.5)}\")\n                #print(f\"next_val: {next_val}\")\n                this_val = next_val\n                next_val = _newton_step(this_val, x_pnt, y_pnt)\n            #self.add_cut(x_pnt, persistent_solver)\n            self.add_cut(next_val, persistent_solver)\n            return True\n        return False",
  "class ProxApproxManagerContinuous(_ProxApproxManager):\n\n    def _create_initial_cuts(self, initial_cut_quantity):\n\n        lb, ub = self.lb, self.ub\n\n        # we get zero for free\n        if lb != 0.:\n            self.add_cut(lb)\n\n        if lb == ub:\n            # var is fixed\n            return\n\n        if ub != 0.:\n            self.add_cut(ub)\n\n        additional_points = self._get_additional_points(initial_cut_quantity)\n        for ptn in additional_points:\n            self.add_cut(ptn)\n\n    def add_cut(self, val, persistent_solver=None):\n        '''\n        create a cut at val using a taylor approximation\n        '''\n        #print(f\"adding cut for {val}\")\n        # f'(a) = 2*val\n        # f(a) - f'(a)a = val*val - 2*val*val\n        f_p_a = 2*val\n        const = -(val*val)\n\n        ## f(x) >= f(a) + f'(a)(x - a)\n        ## f(x) >= f'(a) x + (f(a) - f'(a)a)\n        ## (0 , f(x) - f'(a) x - (f(a) - f'(a)a) , None)\n        expr = LinearExpression( linear_coefs=[1, -f_p_a],\n                                 linear_vars=[self.xvarsqrd, self.xvar],\n                                 constant=-const )\n        self.cuts[self.var_index, self.cut_index] = (0, expr, None)\n        if persistent_solver is not None:\n            persistent_solver.add_constraint(self.cuts[self.var_index, self.cut_index])\n        self.cut_index += 1\n\n        return 1",
  "def _compute_mb(val):\n    ## [(n+1)^2 - n^2] = 2n+1\n    ## [(n+1) - n] = 1\n    ## -> m = 2n+1\n    m = 2*val+1\n\n    ## b = n^2 - (2n+1)*n\n    ## = -n^2 - n\n    ## = -n (n+1)\n    b = -val*(val+1)\n    return m,b",
  "class ProxApproxManagerDiscrete(_ProxApproxManager):\n\n    def _create_initial_cuts(self, initial_cut_quantity):\n        lb, ub = self.lb, self.ub\n\n        if lb == ub:\n            # var is fixed\n            self.create_cut(lb)\n            return\n\n        #print(f\"adding cut for lb {lb}\")\n        self.add_cut(lb)\n        #print(f\"adding cut for ub {ub}\")\n        self.add_cut(ub)\n\n        # there's a left and right cut associated with each discrete point\n        # so there's only half the points we cut on total\n        # This rounds down, e.g., 7 cuts specified becomes 6\n        additional_points = self._get_additional_points(initial_cut_quantity//2+1)\n        for ptn in additional_points:\n            self.add_cut(ptn)\n\n    def add_cut(self, val, persistent_solver=None):\n        '''\n        create up to two cuts at val, exploiting integrality\n        '''\n        val = int(round(val))\n\n        ## cuts are indexed by the x-value to the right\n        ## e.g., the cut for (2,3) is indexed by 3\n        ##       the cut for (-2,-1) is indexed by -1\n        cuts_added = 0\n\n        ## So, a cut to the RIGHT of the point 3 is the cut for (3,4),\n        ## which is indexed by 4\n        if (*self.var_index, val+1) not in self.cuts and val < self.ub:\n            m,b = _compute_mb(val)\n            expr = LinearExpression( linear_coefs=[1, -m],\n                                     linear_vars=[self.xvarsqrd, self.xvar],\n                                     constant=-b )\n            #print(f\"adding cut for {(val, val+1)}\")\n            self.cuts[self.var_index, val+1] = (0, expr, None)\n            if persistent_solver is not None:\n                persistent_solver.add_constraint(self.cuts[self.var_index, val+1])\n            cuts_added += 1\n\n        ## Similarly, a cut to the LEFT of the point 3 is the cut for (2,3),\n        ## which is indexed by 3\n        if (*self.var_index, val) not in self.cuts and val > self.lb:\n            m,b = _compute_mb(val-1)\n            expr = LinearExpression( linear_coefs=[1, -m],\n                                     linear_vars=[self.xvarsqrd, self.xvar],\n                                     constant=-b )\n            #print(f\"adding cut for {(val-1, val)}\")\n            self.cuts[self.var_index, val] = (0, expr, None)\n            if persistent_solver is not None:\n                persistent_solver.add_constraint(self.cuts[self.var_index, val])\n            cuts_added += 1\n\n        return cuts_added",
  "def __new__(cls, xvar, xvarsqrd, xsqvar_cuts, ndn_i, initial_cut_quantity=2):\n        if xvar.is_integer():\n            return ProxApproxManagerDiscrete(xvar, xvarsqrd, xsqvar_cuts, ndn_i, initial_cut_quantity)\n        else:\n            return ProxApproxManagerContinuous(xvar, xvarsqrd, xsqvar_cuts, ndn_i, initial_cut_quantity)",
  "def __init__(self, xvar, xvarsqrd, xsqvar_cuts, ndn_i, initial_cut_quantity):\n        self.xvar = xvar\n        self.xvarsqrd = xvarsqrd\n        self.var_index = ndn_i\n        self.cuts = xsqvar_cuts\n        self.cut_index = 0\n        self._verify_store_bounds(xvar)\n        self._create_initial_cuts(initial_cut_quantity)",
  "def _verify_store_bounds(self, xvar):\n        if not (xvar.has_lb() and xvar.has_ub()):\n            raise RuntimeError(f\"linearize_nonbinary_proximal_terms requires all \"\n                                \"nonanticipative variables to have bounds\")\n        self.lb = value(xvar.lb)\n        self.ub = value(xvar.ub)",
  "def _get_additional_points(self, initial_cut_quantity):\n        '''\n        calculate additional points for initial cuts\n        '''\n        # we add 2 cuts at the bound\n        if initial_cut_quantity <= 2:\n            return ()\n\n        lb, ub = self.lb, self.ub\n        bound_range = ub - lb\n        # n+1 points is n hyperplanes,\n        # but we've already added the bounds\n        delta = bound_range / (initial_cut_quantity-1)\n\n        return (lb + i*delta for i in range(1,initial_cut_quantity-1))",
  "def _create_initial_cuts(self, initial_cut_quantity):\n        '''\n        create initial cuts at val\n        '''\n        pass",
  "def add_cut(self, val, persistent_solver=None):\n        '''\n        create a cut at val\n        '''\n        pass",
  "def check_tol_add_cut(self, tolerance, persistent_solver=None):\n        '''\n        add a cut if the tolerance is not satified\n        '''\n        x_pnt = self.xvar.value\n        y_pnt = self.xvarsqrd.value\n        f_val = x_pnt**2\n\n        #print(f\"y-distance: {actual_val - measured_val})\")\n\n        if (f_val - y_pnt) > tolerance:\n            '''\n            In this case, we project the point x_pnt, y_pnt onto\n            the curve y = x**2 by finding the minimum distance\n            between y = x**2 and x_pnt, y_pnt.\n\n            This involves solving a cubic equation, so instead\n            we start at x_pnt, y_pnt and run newtons algorithm\n            to get an approximate good-enough solution.\n            '''\n            this_val = x_pnt\n            #print(f\"initial distance: {_f(this_val, x_pnt, y_pnt)**(0.5)}\")\n            #print(f\"this_val: {this_val}\")\n            next_val = _newton_step(this_val, x_pnt, y_pnt)\n            while not isclose(this_val, next_val, rel_tol=1e-6, abs_tol=1e-6):\n                #print(f\"newton step distance: {_f(next_val, x_pnt, y_pnt)**(0.5)}\")\n                #print(f\"next_val: {next_val}\")\n                this_val = next_val\n                next_val = _newton_step(this_val, x_pnt, y_pnt)\n            #self.add_cut(x_pnt, persistent_solver)\n            self.add_cut(next_val, persistent_solver)\n            return True\n        return False",
  "def _create_initial_cuts(self, initial_cut_quantity):\n\n        lb, ub = self.lb, self.ub\n\n        # we get zero for free\n        if lb != 0.:\n            self.add_cut(lb)\n\n        if lb == ub:\n            # var is fixed\n            return\n\n        if ub != 0.:\n            self.add_cut(ub)\n\n        additional_points = self._get_additional_points(initial_cut_quantity)\n        for ptn in additional_points:\n            self.add_cut(ptn)",
  "def add_cut(self, val, persistent_solver=None):\n        '''\n        create a cut at val using a taylor approximation\n        '''\n        #print(f\"adding cut for {val}\")\n        # f'(a) = 2*val\n        # f(a) - f'(a)a = val*val - 2*val*val\n        f_p_a = 2*val\n        const = -(val*val)\n\n        ## f(x) >= f(a) + f'(a)(x - a)\n        ## f(x) >= f'(a) x + (f(a) - f'(a)a)\n        ## (0 , f(x) - f'(a) x - (f(a) - f'(a)a) , None)\n        expr = LinearExpression( linear_coefs=[1, -f_p_a],\n                                 linear_vars=[self.xvarsqrd, self.xvar],\n                                 constant=-const )\n        self.cuts[self.var_index, self.cut_index] = (0, expr, None)\n        if persistent_solver is not None:\n            persistent_solver.add_constraint(self.cuts[self.var_index, self.cut_index])\n        self.cut_index += 1\n\n        return 1",
  "def _create_initial_cuts(self, initial_cut_quantity):\n        lb, ub = self.lb, self.ub\n\n        if lb == ub:\n            # var is fixed\n            self.create_cut(lb)\n            return\n\n        #print(f\"adding cut for lb {lb}\")\n        self.add_cut(lb)\n        #print(f\"adding cut for ub {ub}\")\n        self.add_cut(ub)\n\n        # there's a left and right cut associated with each discrete point\n        # so there's only half the points we cut on total\n        # This rounds down, e.g., 7 cuts specified becomes 6\n        additional_points = self._get_additional_points(initial_cut_quantity//2+1)\n        for ptn in additional_points:\n            self.add_cut(ptn)",
  "def add_cut(self, val, persistent_solver=None):\n        '''\n        create up to two cuts at val, exploiting integrality\n        '''\n        val = int(round(val))\n\n        ## cuts are indexed by the x-value to the right\n        ## e.g., the cut for (2,3) is indexed by 3\n        ##       the cut for (-2,-1) is indexed by -1\n        cuts_added = 0\n\n        ## So, a cut to the RIGHT of the point 3 is the cut for (3,4),\n        ## which is indexed by 4\n        if (*self.var_index, val+1) not in self.cuts and val < self.ub:\n            m,b = _compute_mb(val)\n            expr = LinearExpression( linear_coefs=[1, -m],\n                                     linear_vars=[self.xvarsqrd, self.xvar],\n                                     constant=-b )\n            #print(f\"adding cut for {(val, val+1)}\")\n            self.cuts[self.var_index, val+1] = (0, expr, None)\n            if persistent_solver is not None:\n                persistent_solver.add_constraint(self.cuts[self.var_index, val+1])\n            cuts_added += 1\n\n        ## Similarly, a cut to the LEFT of the point 3 is the cut for (2,3),\n        ## which is indexed by 3\n        if (*self.var_index, val) not in self.cuts and val > self.lb:\n            m,b = _compute_mb(val-1)\n            expr = LinearExpression( linear_coefs=[1, -m],\n                                     linear_vars=[self.xvarsqrd, self.xvar],\n                                     constant=-b )\n            #print(f\"adding cut for {(val-1, val)}\")\n            self.cuts[self.var_index, val] = (0, expr, None)\n            if persistent_solver is not None:\n                persistent_solver.add_constraint(self.cuts[self.var_index, val])\n            cuts_added += 1\n\n        return cuts_added",
  "def _get_file(filepath):\n    cline = -1\n    licline = -1\n    pyomofile = False\n    with open(filepath,'r') as f:\n        lines = f.readlines()\n    for lno, line in enumerate(lines):\n        if cstmt in line:\n            cline = lno\n            print\n        if lic in line:\n            licline = lno\n        if pyomoscent in line:\n            pyomofile = True\n    return lines, cline, licline, pyomofile",
  "def _write_file(filepath, lines, cline, licline, pyomofile):\n    print (filepath, end='')\n\n    if pyomofile:\n        print(\": seems to have Pyomo header; skipping\")\n        return\n    \n    cbefore = -1  # if only adding copyright\n    licbefore = -1  # if only adding license\n    if cline == -1 and licline != -1:\n        cbefore = licline\n    if cline != -1 and licline == -1:\n        licbefore = licline + 1\n    if cline != -1 and licline != -1:\n        print(\" .\")\n        return\n    \n    with open(filepath, \"w\") as f:\n        if cline == -1 and licline == -1:\n            f.write(cstmt+'\\n')\n            f.write(lic+'\\n')\n            print(\": add both\")\n        for lno, line in enumerate(lines):\n            if cbefore == lno:\n                f.write(cstmt+'\\n')\n                print(\": add copyright\")\n            if licbefore == lno:\n                f.write(lic+'\\n')\n                print(\": add license\")\n            f.write(line)",
  "def _bool_option(cfg, oname):\n    return oname in cfg and cfg[oname]",
  "def add_options(cfg, parser_choice=None):\n    #  parser_choice is a string referring to the component (e.g., \"slammin\")\n    # (note: by \"parser\" we mean \"config\")\n    assert parser_choice is not None\n\n    parser_name = parser_choice+\"_args\"\n    adder = getattr(cfg, parser_name)\n    adder()",
  "def find_hub(cylinders, is_multi=False):\n    hubs = set(cylinders).intersection(set(hubs_and_multi_compatibility.keys()))\n    if len(hubs) == 1:\n        hub = list(hubs)[0]\n        if is_multi and not hubs_and_multi_compatibility[hub]:\n            raise RuntimeError(f\"The hub {hub} does not work with multistage problems\" )\n    else:\n        raise RuntimeError(\"There must be exactly one hub among cylinders\")\n    return hub",
  "def find_spokes(cylinders, is_multi=False):\n    spokes = []\n    for c in cylinders:\n        if not c in hubs_and_multi_compatibility:\n            if c not in spokes_and_multi_compatibility:\n                raise RuntimeError(f\"The cylinder {c} do not exist or cannot be called via amalgamator.\")\n            if is_multi and not spokes_and_multi_compatibility[c]:\n                raise RuntimeError(f\"The spoke {c} does not work with multistage problems\" )\n            if c in default_unused_spokes:\n                print(f\"{c} is unused by default. Please specify --with-{c}=True in the command line to activate this spoke\")\n            spokes.append(c)\n    return spokes",
  "def check_module_ama(module):\n    # Complain if the module lacks things needed.\n    everything = [\"scenario_names_creator\",\n                 \"scenario_creator\",\n                 \"inparser_adder\",\n                 \"kw_creator\"]  # start and denouement can be missing.\n    you_can_have_it_all = True\n    for ething in everything:\n        if not hasattr(module, ething):\n            print(f\"Module {mname} is missing {ething}\")\n            you_can_have_it_all = False\n    if not you_can_have_it_all:\n        raise RuntimeError(f\"Module {mname} not complete for from_module\")",
  "def from_module(mname, cfg, extraargs_fct=None, use_command_line=True):\n    \"\"\" Try to get everything from one file (this will not always be possible).\n    Args:\n        mname (str): the module name (module must have certain functions)\n                     or you can pass in a module that has already been imported\n        cfg (Config): Amalgamator options or extra arguments to use \n                        in addition with the command line\n        extraargs_fct (fct) : a function to add extra arguments, e.g. for MMW\n        use_command_line (bool): should we take into account the command line to populate cfg ?\n                                 default is True\n                    \n    Returns:\n        ama (Amalgamator): the instantiated object\n    \n    \"\"\"\n    if not isinstance(cfg, config.Config):\n        raise RuntimeError(f\"amalgamator from_model bad cfg type={type(cfg)}; should be Config\")\n\n    if inspect.ismodule(mname):\n        m = mname\n    else:\n        m = importlib.import_module(mname)\n    check_module_ama(m)\n\n    cfg = Amalgamator_parser(cfg, m.inparser_adder,\n                                 extraargs_fct=extraargs_fct,\n                                 use_command_line=use_command_line)\n    cfg.add_and_assign('_mpisppy_probability', description=\"Uniform prob.\", domain=float, default=None, value= 1/cfg['num_scens'])\n    start = cfg['start'] if 'start' in cfg else 0\n    sn = m.scenario_names_creator(cfg['num_scens'], start=start)\n    dn = m.scenario_denouement if hasattr(m, \"scenario_denouement\") else None\n    ama = Amalgamator(cfg,\n                      sn,\n                      m.scenario_creator,\n                      m.kw_creator,\n                      scenario_denouement=dn)\n    return ama",
  "def Amalgamator_parser(cfg, inparser_adder, extraargs_fct=None, use_command_line=True):\n    \"\"\" Helper function for Amalgamator.\n    Args:\n        cfg (Config): Amalgamator control options, etc; might be added to or changed\n        inparser_adder (fct): returns updated ArgumentParser the problem\n        extraargs_fct (fct) : a function to add extra arguments, e.g. for MMW\n        use_command_line (bool): should we take into account the command line to add options ?\n                                 default is True\n    Returns;\n        cfg (Cofig): the modifed cfg object containing the options, both parsed values and pre-set options\n    \"\"\"\n\n    # TBD: should we copy?\n    \n    if use_command_line:\n        if _bool_option(cfg, \"EF_2stage\"):\n            cfg.EF2()\n        elif _bool_option(cfg, \"EF_mstage\"):\n            cfg.EF_multistage()\n        else:\n            if _bool_option(cfg, \"2stage\"):\n                cfg.popular_args()\n            elif _bool_option(cfg, \"mstage\"):\n                cfg.multistage()\n            else:\n                raise RuntimeError(\"The problem type (2stage or mstage) must be specified\")\n            cfg.two_sided_args()\n            cfg.mip_options()\n                \n            #Adding cylinders\n            if not \"cylinders\" in cfg:\n                raise RuntimeError(\"A cylinder list must be specified\")\n            \n            for cylinder in cfg['cylinders']:\n                #NOTE: This returns an error if the cylinder yyyy has no yyyy_args in config.py\n                add_options(cfg, cylinder)\n            \n            #Adding extensions\n            if \"extensions\" in cfg:\n                for extension in cfg['extensions']:\n                    add_options(cfg, extension)\n    \n        inparser_adder(cfg)\n        \n        if extraargs_fct is not None:\n            extraargs_fct()\n        \n        prg = cfg.get(\"program_name\")\n        cfg.parse_command_line(prg)\n\n        \"\"\"\n        print(\"Amalgamator needs work for solver options!!\")\n        # TBD: deal with proliferation of solver options specifications\n        if _bool_option(options_dict, \"EF-2stage\") or _bool_option(options_dict, \"EF-mstage\"): \n            if ('EF_solver_options' in options_dict):\n                options_dict[\"EF_solver_options\"][\"mipgap\"] = options_dict[\"EF_mipgap\"]\n            else:\n                options_dict[\"EF_solver_options\"] = {\"mipgap\": options_dict[\"EF_mipgap\"]}\n        \"\"\"\n    else:\n        #Checking if cfg has all the options we need \n        if not (_bool_option(cfg, \"EF_2stage\") or _bool_option(cfg, \"EF_mstage\")):\n            raise RuntimeError(\"For now, completly bypassing command line only works with EF.\" )\n        if not ('EF_solver_name' in cfg):\n            raise RuntimeError(\"EF_solver_name must be specified for the amalgamator.\" )\n        if not ('num_scens' in cfg):\n            raise RuntimeWarning(\"cfg should have a number of scenarios to compute a xhat\")\n        if _bool_option(cfg, 'EF-mstage') and 'branching_factors' not in cfg:\n            raise RuntimeError(\"For a multistage problem, cfg must have a 'branching_factors' attribute with branching factors\")\n\n    return cfg",
  "class Amalgamator():\n    \"\"\"Takes a scenario list and a scenario creator (and options)\n    as input. The ides is to produce an outer bound on the objective function (solving the EF directly\n    or by decomposition) and/or an x-hat with inner bound; however, what it does is controlled by\n    its constructor options and by user options.\n\n    This thing basically wraps the functionality of the \"standard\" *_cylinder examples.\n    \n    It may be an extenisble base class, but not abstract.\n\n    Args:\n        cfg (Config): controls the amalgamation and may be added to or changed\n        scenario_names (list of str) the full set of scenario names\n        scenario_creator (fct): returns a concrete model with special things\n        kw_creator (fct): takes an options dict and returns scenario_creator kwargs\n        scenario_denouement (fct): (optional) called at conclusion\n    \"\"\"\n\n    def __init__(self, cfg,\n                 scenario_names, scenario_creator, kw_creator, \n                 scenario_denouement=None, verbose=True):\n        self.cfg = cfg\n        self.scenario_names = scenario_names\n        self.scenario_creator = scenario_creator\n        self.scenario_denouement = scenario_denouement\n        self.kw_creator = kw_creator\n        self.kwargs = self.kw_creator(self.cfg)\n        self.verbose = verbose\n        self.is_EF = _bool_option(cfg, \"EF_2stage\") or _bool_option(cfg, \"EF_mstage\")\n        if self.is_EF:\n            sroot, self.solver_name, self.solver_options = solver_spec.solver_specification(cfg, [\"EF\", \"\"])\n        self.is_multi = _bool_option(cfg, \"EF-mstage\") or _bool_option(cfg, \"mstage\")\n        if self.is_multi and not \"all_nodenames\" in cfg:\n            if \"branching_factors\" in cfg:\n                ndnms = sputils.create_nodenames_from_branching_factors(cfg[\"branching_factors\"])\n                self.cfg.quick_assign(\"all_nodenames\", domain=pyofig.ListOf(str), value=ndnms)\n            else:\n                raise RuntimeError(\"For a multistage problem, please provide branching_factors or all_nodenames\")\n        \n    def run(self):\n\n        \"\"\" Top-level execution.\"\"\"\n        if self.is_EF:\n            ef = sputils.create_EF(\n                self.scenario_names,\n                self.scenario_creator,\n                scenario_creator_kwargs=self.kwargs,\n                suppress_warnings=True,\n            )\n\n            tee_ef_solves = self.cfg.get('tee_ef_solves',False)\n            \n            solver_name = self.solver_name\n            solver = pyo.SolverFactory(solver_name)\n            if hasattr(self, \"solver_options\") and (self.solver_options is not None):\n                for option_key,option_value in self.solver_options.items():\n                    solver.options[option_key] = option_value\n            if self.verbose :\n                global_toc(\"Starting EF solve\")\n            if 'persistent' in solver_name:\n                solver.set_instance(ef, symbolic_solver_labels=True)\n                results = solver.solve(tee=tee_ef_solves)\n            else:\n                results = solver.solve(ef, tee=tee_ef_solves, symbolic_solver_labels=True,)\n            if self.verbose:\n                global_toc(\"Completed EF solve\")\n\n            \n            self.EF_Obj = pyo.value(ef.EF_Obj)\n\n            objs = sputils.get_objs(ef)\n            \n            self.is_minimizing = objs[0].is_minimizing\n            #TBD : Write a function doing this\n            if self.is_minimizing:\n                self.best_outer_bound = results.Problem[0]['Lower bound']\n                self.best_inner_bound = results.Problem[0]['Upper bound']\n            else:\n                self.best_inner_bound = results.Problem[0]['Upper bound']\n                self.best_outer_bound = results.Problem[0]['Lower bound']\n            self.ef = ef\n            \n            if 'write_solution' in self.cfg:\n                if 'first_stage_solution' in self.cfg['write_solution']:\n                    sputils.write_ef_first_stage_solution(self.ef,\n                                                          self.cfg['write_solution']['first_stage_solution'])\n                if 'tree_solution' in self.cfg['write_solution']:\n                    sputils.write_ef_tree_solution(self.ef,\n                                                   self.cfg['write_solution']['tree_solution'])\n            \n            self.xhats = sputils.nonant_cache_from_ef(ef)\n            self.local_xhats = self.xhats  # Every scenario is local for EF\n            self.first_stage_solution = {\"ROOT\": self.xhats[\"ROOT\"]}\n\n        else:\n            self.ef = None\n\n            #Create a hub dict\n            hub_name = find_hub(self.cfg['cylinders'], self.is_multi)\n            hub_creator = getattr(vanilla, hub_name+'_hub')\n            beans = {\"cfg\": self.cfg,\n                     \"scenario_creator\": self.scenario_creator,\n                     \"scenario_denouement\": self.scenario_denouement,\n                     \"all_scenario_names\": self.scenario_names,\n                     \"scenario_creator_kwargs\": self.kwargs}\n            if self.is_multi:\n                beans[\"all_nodenames\"] = self.cfg[\"all_nodenames\"]\n            hub_dict = hub_creator(**beans)\n            \n            #Add extensions\n            if 'extensions' in self.cfg:\n                for extension in self.cfg['extensions']:\n                    extension_creator = getattr(vanilla, 'add_'+extension)\n                    hub_dict = extension_creator(hub_dict, self.cfg)\n            \n            #Create spoke dicts\n            potential_spokes = find_spokes(self.cfg['cylinders'],\n                                           self.is_multi)\n            #We only use the spokes with an associated command line arg set to True\n            spokes = [spoke for spoke in potential_spokes if self.cfg[spoke]]\n            list_of_spoke_dict = list()\n            for spoke in spokes:\n                spoke_creator = getattr(vanilla, spoke+'_spoke')\n                spoke_beans = copy.deepcopy(beans)\n                if spoke == \"xhatspecific\":\n                    spoke_beans[\"scenario_dict\"] = self.cfg[\"scenario_dict\"]\n                spoke_dict = spoke_creator(**spoke_beans)\n                list_of_spoke_dict.append(spoke_dict)\n                \n            ws =  WheelSpinner(hub_dict, list_of_spoke_dict)\n            ws.run()\n\n            spcomm = ws.spcomm\n            \n            self.opt = spcomm.opt\n            self.on_hub = ws.on_hub()\n            \n            if self.on_hub:  # we are on a hub rank\n                self.best_inner_bound = spcomm.BestInnerBound\n                self.best_outer_bound = spcomm.BestOuterBound\n                #NOTE: We do not get bounds on every rank, only on hub\n                #      This should change if we want to use cylinders for MMW\n                \n            # prior to June 2022, the wite options were a two-level dictionary; now flattened\n            if 'first_stage_solution_csv' in self.cfg:\n                ws.write_first_stage_solution(self.cfg['first_stage_solution_csv'])\n            if 'tree_solution_csv' in self.cfg:\n                ws.write_tree_solution(self.cfg['tree_solution_csv'])\n            \n            if self.on_hub: #we are on a hub rank\n                a_sname = self.opt.local_scenario_names[0]\n                root = self.opt.local_scenarios[a_sname]._mpisppy_node_list[0]\n                self.first_stage_solution = {\"ROOT\":[pyo.value(var) for var in root.nonant_vardata_list]}\n                self.local_xhats = ws.local_nonant_cache()",
  "def __init__(self, cfg,\n                 scenario_names, scenario_creator, kw_creator, \n                 scenario_denouement=None, verbose=True):\n        self.cfg = cfg\n        self.scenario_names = scenario_names\n        self.scenario_creator = scenario_creator\n        self.scenario_denouement = scenario_denouement\n        self.kw_creator = kw_creator\n        self.kwargs = self.kw_creator(self.cfg)\n        self.verbose = verbose\n        self.is_EF = _bool_option(cfg, \"EF_2stage\") or _bool_option(cfg, \"EF_mstage\")\n        if self.is_EF:\n            sroot, self.solver_name, self.solver_options = solver_spec.solver_specification(cfg, [\"EF\", \"\"])\n        self.is_multi = _bool_option(cfg, \"EF-mstage\") or _bool_option(cfg, \"mstage\")\n        if self.is_multi and not \"all_nodenames\" in cfg:\n            if \"branching_factors\" in cfg:\n                ndnms = sputils.create_nodenames_from_branching_factors(cfg[\"branching_factors\"])\n                self.cfg.quick_assign(\"all_nodenames\", domain=pyofig.ListOf(str), value=ndnms)\n            else:\n                raise RuntimeError(\"For a multistage problem, please provide branching_factors or all_nodenames\")",
  "def run(self):\n\n        \"\"\" Top-level execution.\"\"\"\n        if self.is_EF:\n            ef = sputils.create_EF(\n                self.scenario_names,\n                self.scenario_creator,\n                scenario_creator_kwargs=self.kwargs,\n                suppress_warnings=True,\n            )\n\n            tee_ef_solves = self.cfg.get('tee_ef_solves',False)\n            \n            solver_name = self.solver_name\n            solver = pyo.SolverFactory(solver_name)\n            if hasattr(self, \"solver_options\") and (self.solver_options is not None):\n                for option_key,option_value in self.solver_options.items():\n                    solver.options[option_key] = option_value\n            if self.verbose :\n                global_toc(\"Starting EF solve\")\n            if 'persistent' in solver_name:\n                solver.set_instance(ef, symbolic_solver_labels=True)\n                results = solver.solve(tee=tee_ef_solves)\n            else:\n                results = solver.solve(ef, tee=tee_ef_solves, symbolic_solver_labels=True,)\n            if self.verbose:\n                global_toc(\"Completed EF solve\")\n\n            \n            self.EF_Obj = pyo.value(ef.EF_Obj)\n\n            objs = sputils.get_objs(ef)\n            \n            self.is_minimizing = objs[0].is_minimizing\n            #TBD : Write a function doing this\n            if self.is_minimizing:\n                self.best_outer_bound = results.Problem[0]['Lower bound']\n                self.best_inner_bound = results.Problem[0]['Upper bound']\n            else:\n                self.best_inner_bound = results.Problem[0]['Upper bound']\n                self.best_outer_bound = results.Problem[0]['Lower bound']\n            self.ef = ef\n            \n            if 'write_solution' in self.cfg:\n                if 'first_stage_solution' in self.cfg['write_solution']:\n                    sputils.write_ef_first_stage_solution(self.ef,\n                                                          self.cfg['write_solution']['first_stage_solution'])\n                if 'tree_solution' in self.cfg['write_solution']:\n                    sputils.write_ef_tree_solution(self.ef,\n                                                   self.cfg['write_solution']['tree_solution'])\n            \n            self.xhats = sputils.nonant_cache_from_ef(ef)\n            self.local_xhats = self.xhats  # Every scenario is local for EF\n            self.first_stage_solution = {\"ROOT\": self.xhats[\"ROOT\"]}\n\n        else:\n            self.ef = None\n\n            #Create a hub dict\n            hub_name = find_hub(self.cfg['cylinders'], self.is_multi)\n            hub_creator = getattr(vanilla, hub_name+'_hub')\n            beans = {\"cfg\": self.cfg,\n                     \"scenario_creator\": self.scenario_creator,\n                     \"scenario_denouement\": self.scenario_denouement,\n                     \"all_scenario_names\": self.scenario_names,\n                     \"scenario_creator_kwargs\": self.kwargs}\n            if self.is_multi:\n                beans[\"all_nodenames\"] = self.cfg[\"all_nodenames\"]\n            hub_dict = hub_creator(**beans)\n            \n            #Add extensions\n            if 'extensions' in self.cfg:\n                for extension in self.cfg['extensions']:\n                    extension_creator = getattr(vanilla, 'add_'+extension)\n                    hub_dict = extension_creator(hub_dict, self.cfg)\n            \n            #Create spoke dicts\n            potential_spokes = find_spokes(self.cfg['cylinders'],\n                                           self.is_multi)\n            #We only use the spokes with an associated command line arg set to True\n            spokes = [spoke for spoke in potential_spokes if self.cfg[spoke]]\n            list_of_spoke_dict = list()\n            for spoke in spokes:\n                spoke_creator = getattr(vanilla, spoke+'_spoke')\n                spoke_beans = copy.deepcopy(beans)\n                if spoke == \"xhatspecific\":\n                    spoke_beans[\"scenario_dict\"] = self.cfg[\"scenario_dict\"]\n                spoke_dict = spoke_creator(**spoke_beans)\n                list_of_spoke_dict.append(spoke_dict)\n                \n            ws =  WheelSpinner(hub_dict, list_of_spoke_dict)\n            ws.run()\n\n            spcomm = ws.spcomm\n            \n            self.opt = spcomm.opt\n            self.on_hub = ws.on_hub()\n            \n            if self.on_hub:  # we are on a hub rank\n                self.best_inner_bound = spcomm.BestInnerBound\n                self.best_outer_bound = spcomm.BestOuterBound\n                #NOTE: We do not get bounds on every rank, only on hub\n                #      This should change if we want to use cylinders for MMW\n                \n            # prior to June 2022, the wite options were a two-level dictionary; now flattened\n            if 'first_stage_solution_csv' in self.cfg:\n                ws.write_first_stage_solution(self.cfg['first_stage_solution_csv'])\n            if 'tree_solution_csv' in self.cfg:\n                ws.write_tree_solution(self.cfg['tree_solution_csv'])\n            \n            if self.on_hub: #we are on a hub rank\n                a_sname = self.opt.local_scenario_names[0]\n                root = self.opt.local_scenarios[a_sname]._mpisppy_node_list[0]\n                self.first_stage_solution = {\"ROOT\":[pyo.value(var) for var in root.nonant_vardata_list]}\n                self.local_xhats = ws.local_nonant_cache()",
  "class Config(pyofig.ConfigDict):\n    # remember that the parent uses slots\n\n    #===============\n    def add_to_config(self, name, description, domain, default,\n                      argparse=True,\n                      complain=False,\n                      argparse_args=None):\n        \"\"\" Add an arg to the self dict.\n        Args:\n            name (str): the argument name, underscore seperated\n            description (str): free text description\n            domain (type): see pyomo config docs\n            default (domain): value before argparse\n            argparse (bool): if True put on command ine\n            complain (bool): if True, output a message for a duplicate\n            argparse_args (dict): args to pass to argpars (option; e.g. required, or group)\n        \"\"\"\n        if name in self:\n            if complain:\n                print(f\"Duplicate {name} will not be added to self.\")\n                # raise RuntimeError(f\"Trying to add duplicate {name} to self.\")\n        else:\n            c = self.declare(name, pyofig.ConfigValue(\n                description = description,\n                domain = domain,\n                default = default))\n            if argparse:\n                if argparse_args is not None:\n                    c.declare_as_argument(**argparse_args)\n                else:\n                    c.declare_as_argument()\n\n\n    #===============\n    def add_and_assign(self, name, description, domain, default, value, complain=False):\n        \"\"\" Add an arg to the self dict and assign it a value\n        Args:\n             name (str): the argument name, underscore separated\n            description (str): free text description\n            domain (type): see pyomo config docs\n            default (domain): probably unused, but here to avoid cut-and-paste errors\n            value (domain): the value to assign\n            complain (bool): if True, output a message for a duplicate\n        \"\"\"\n        if name in self:\n            if complain:\n                print(f\"Duplicate {name} will not be added to self by add_and_assign {value}.\")\n                # raise RuntimeError(f\"Trying to add duplicate {name} to self.\")\n        else:\n            self.add_to_config(name, description, domain, default, argparse=False)\n            self[name] = value\n\n\n    #===============\n    def dict_assign(self, name, description, domain, default, value):\n        \"\"\" mimic dict assignment\n        Args:\n            name (str): the argument name, underscore separated\n            description (str): free text description\n            domain (type): see pyomo config docs\n            default (domain): probably unused, but here to avoid cut-and-paste errors\n            value (domain): the value to assign\n        \"\"\"\n        if name not in self:\n            self.add_and_assign(name, description, domain, default, value)\n        else:\n            self[name] = value\n\n\n    #===============\n    def quick_assign(self, name, domain, value):\n        \"\"\" mimic dict assignment with fewer args\n        Args:\n            name (str): the argument name, underscore separated\n            domain (type): see pyomo config docs\n            value (domain): the value to assign\n        \"\"\"\n        self.dict_assign(name, f\"field for {name}\", domain, None, value)\n\n\n    #===============\n    def get(self, name, ifmissing=None):\n        \"\"\" replcate the behavior of dict get\"\"\"\n        if name in self:\n            return self[name]\n        else:\n            return ifmissing\n\n    def add_solver_specs(self, prefix=\"\"):\n        sstr = f\"{prefix}_solver\" if prefix != \"\" else \"solver\"\n        self.add_to_config(f\"{sstr}_name\",\n                            description= \"solver name (default None)\",\n                            domain = str,\n                            default=None)\n\n        self.add_to_config(f\"{sstr}_options\",\n                            description= \"solver options; space delimited with = for values (default None)\",\n                            domain = str,\n                            default=None)\n\n    def _common_args(self):\n        raise RuntimeError(\"_common_args is no longer used. See comments at top of config.py\")\n\n    def popular_args(self):\n        self.add_to_config(\"max_iterations\",\n                            description=\"hub max iiterations (default 1)\",\n                            domain=int,\n                            default=1)\n\n        self.add_solver_specs(prefix=\"\")\n\n        self.add_to_config(\"seed\",\n                            description=\"Seed for random numbers (default is 1134)\",\n                            domain=int,\n                            default=1134)\n\n        self.add_to_config(\"default_rho\",\n                            description=\"Global rho for PH (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"bundles_per_rank\",\n                            description=\"bundles per rank (default 0 (no bundles))\",\n                            domain=int,\n                            default=0)\n\n        self.add_to_config('verbose',\n                              description=\"verbose output\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config('display_progress',\n                              description=\"display progress at each iteration\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config('display_convergence_detail',\n                              description=\"display non-anticipative variable convergence statistics at each iteration\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"max_solver_threads\",\n                            description=\"Limit on threads per solver (default None)\",\n                            domain=int,\n                            default=None)\n\n        self.add_to_config(\"intra_hub_conv_thresh\",\n                            description=\"Within hub convergence threshold (default 1e-10)\",\n                            domain=float,\n                            default=1e-10)\n\n        self.add_to_config(\"trace_prefix\",\n                            description=\"Prefix for bound spoke trace files. If None \"\n                                 \"bound spoke trace files are not written.\",\n                            domain=str,\n                            default=None)\n\n        self.add_to_config(\"tee_rank0_solves\",\n                              description=\"Some cylinders support tee of rank 0 solves.\"\n                              \"(With multiple cylinders this could be confusing.)\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"auxilliary\",\n                            description=\"Free text for use by hackers (default '').\",\n                            domain=str,\n                            default='')\n\n    def ph_args(self):\n        self.add_to_config(\"linearize_binary_proximal_terms\",\n                              description=\"For PH, linearize the proximal terms for \"\n                              \"all binary nonanticipative variables\",\n                              domain=bool,\n                              default=False)\n\n\n        self.add_to_config(\"linearize_proximal_terms\",\n                              description=\"For PH, linearize the proximal terms for \"\n                              \"all nonanticipative variables\",\n                              domain=bool,\n                              default=False)\n\n\n        self.add_to_config(\"proximal_linearization_tolerance\",\n                            description=\"For PH, when linearizing proximal terms, \"\n                            \"a cut will be added if the proximal term approximation \"\n                            \"is looser than this value (default 1e-1)\",\n                            domain=float,\n                            default=1.e-1)\n\n    def make_parser(self, progname=None, num_scens_reqd=False):\n        raise RuntimeError(\"make_parser is no longer used. See comments at top of config.py\")\n\n\n    def num_scens_optional(self):\n        self.add_to_config(\n            \"num_scens\",\n            description=\"Number of scenarios (default None)\",\n            domain=int,\n            default=None,\n        )\n\n    def num_scens_required(self):\n        # required, but not postional\n        self.add_to_config(\n            \"num_scens\",\n            description=\"Number of scenarios (default None)\",\n            domain=int,\n            default=None,\n            argparse_args = {\"required\": True}\n        )\n\n\n    def _basic_multistage(self, progname=None, num_scens_reqd=False):\n        raise RuntimeError(\"_basic_multistage is no longer used. See comments at top of config.py\")\n\n    def add_branching_factors(self):\n        self.add_to_config(\"branching_factors\",\n                            description=\"Spaces delimited branching factors (e.g., 2 2)\",\n                            domain=pyofig.ListOf(int, pyofig.PositiveInt),\n                            default=None)\n\n\n    def make_multistage_parser(self, progname=None):\n        raise RuntimeError(\"make_multistage_parser is no longer used. See comments at top of config.py\")\n\n    def multistage(self):\n        self.add_branching_factors()\n        self.popular_args()\n\n\n    #### EF ####\n    def make_EF2_parser(self, progname=None, num_scens_reqd=False):\n        raise RuntimeError(\"make_EF2_parser is no longer used. See comments at top of config.py\")\n\n    def _EF_base(self):\n\n        self.add_solver_specs(prefix=\"EF\")\n\n        self.add_to_config(\"EF_mipgap\",\n                           description=\"mip gap option for the solver if needed (default None)\",\n                           domain=float,\n                           default=None)\n\n\n    def EF2(self):\n        self._EF_base()\n        self.add_to_config(\"num_scens\",\n                           description=\"Number of scenarios (default None)\",\n                           domain=int,\n                           default=None)\n\n\n    def make_EF_multistage_parser(self, progname=None, num_scens_reqd=False):\n        raise RuntimeError(\"make_EF_multistage_parser is no longer used. See comments at top of config.py\")\n\n    def EF_multistage(self):\n\n        self._EF_base()\n        # branching factors???\n\n    ##### common additions to the command line #####\n\n    def two_sided_args(self):\n        # add commands to  and also return the result\n\n        self.add_to_config(\"rel_gap\",\n                            description=\"relative termination gap (default 0.05)\",\n                            domain=float,\n                            default=0.05)\n\n        self.add_to_config(\"abs_gap\",\n                            description=\"absolute termination gap (default 0)\",\n                            domain=float,\n                            default=0.)\n\n        self.add_to_config(\"max_stalled_iters\",\n                            description=\"maximum iterations with no reduction in gap (default 100)\",\n                            domain=int,\n                            default=100)\n\n\n\n    def mip_options(self):\n\n        self.add_to_config(\"iter0_mipgap\",\n                            description=\"mip gap option for iteration 0 (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"iterk_mipgap\",\n                            description=\"mip gap option non-zero iterations (default None)\",\n                            domain=float,\n                            default=None)\n\n\n    def aph_args(self):\n\n        self.add_to_config('aph_gamma',\n                            description='Gamma parameter associated with asychronous projective hedging (default 1.0)',\n                            domain=float,\n                            default=1.0)\n        self.add_to_config('aph_nu',\n                            description='Nu parameter associated with asychronous projective hedging (default 1.0)',\n                            domain=float,\n                            default=1.0)\n        self.add_to_config('aph_frac_needed',\n                            description='Fraction of sub-problems required before computing projective step (default 1.0)',\n                            domain=float,\n                            default=1.0)\n        self.add_to_config('aph_dispatch_frac',\n                            description='Fraction of sub-problems to dispatch at each step of asychronous projective hedging (default 1.0)',\n                            domain=float,\n                            default=1.0)\n        self.add_to_config('aph_sleep_seconds',\n                            description='Spin-lock sleep time for APH (default 0.01)',\n                            domain=float,\n                            default=0.01)\n\n\n    def fixer_args(self):\n\n        self.add_to_config('fixer',\n                           description=\"have an integer fixer extension\",\n                           domain=bool,\n                           default=False)\n\n        self.add_to_config(\"fixer_tol\",\n                           description=\"fixer bounds tolerance  (default 1e-4)\",\n                           domain=float,\n                           default=1e-2)\n\n\n\n    def fwph_args(self):\n\n        self.add_to_config('fwph',\n                           description=\"have an fwph spoke\",\n                           domain=bool,\n                           default=False)\n\n        self.add_to_config(\"fwph_iter_limit\",\n                            description=\"maximum fwph iterations (default 10)\",\n                            domain=int,\n                            default=10)\n\n        self.add_to_config(\"fwph_weight\",\n                            description=\"fwph weight (default 0)\",\n                            domain=float,\n                            default=0.0)\n\n        self.add_to_config(\"fwph_conv_thresh\",\n                            description=\"fwph convergence threshold  (default 1e-4)\",\n                            domain=float,\n                            default=1e-4)\n\n        self.add_to_config(\"fwph_stop_check_tol\",\n                            description=\"fwph tolerance for Gamma^t (default 1e-4)\",\n                            domain=float,\n                            default=1e-4)\n\n        self.add_to_config(\"fwph_mipgap\",\n                            description=\"mip gap option FW subproblems iterations (default None)\",\n                            domain=float,\n                            default=None)\n\n\n\n    def lagrangian_args(self):\n\n        self.add_to_config('lagrangian',\n                              description=\"have a lagrangian spoke\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"lagrangian_iter0_mipgap\",\n                            description=\"lgr. iter0 solver option mipgap (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"lagrangian_iterk_mipgap\",\n                            description=\"lgr. iterk solver option mipgap (default None)\",\n                            domain=float,\n                            default=None)\n\n\n    def lagranger_args(self):\n\n        self.add_to_config('lagranger',\n                            description=\"have a special lagranger spoke\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"lagranger_iter0_mipgap\",\n                            description=\"lagranger iter0 mipgap (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"lagranger_iterk_mipgap\",\n                            description=\"lagranger iterk mipgap (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"lagranger_rho_rescale_factors_json\",\n                            description=\"json file: rho rescale factors (default None)\",\n                            domain=str,\n                            default=None)\n\n\n    def xhatlooper_args(self):\n\n        self.add_to_config('xhatlooper',\n                              description=\"have an xhatlooper spoke\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"xhat_scen_limit\",\n                            description=\"scenario limit xhat looper to try (default 3)\",\n                            domain=int,\n                            default=3)\n\n    def xhatshuffle_args(self):\n\n        self.add_to_config('xhatshuffle',\n                           description=\"have an xhatshuffle spoke\",\n                           domain=bool,\n                           default=False)\n\n        self.add_to_config('add_reversed_shuffle',\n                           description=\"using also the reversed shuffling (multistage only, default True)\",\n                           domain=bool,\n                           default=False)\n\n        self.add_to_config('xhatshuffle_iter_step',\n                           description=\"step in shuffled list between 2 scenarios to try (default None)\",\n                           domain=int,\n                           default=None)\n\n\n    def mult_rho_args(self):\n\n        self.add_to_config('mult_rho',\n                              description=\"Have mult_rho extension (default False)\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config('mult_rho_convergence_tolerance',\n                            description=\"rhomult does nothing with convergence below this (default 1e-4)\",\n                              domain=float,\n                              default=1e-4)\n\n        self.add_to_config('mult_rho_update_stop_iteration',\n                            description=\"stop doing rhomult rho updates after this iteration (default None)\",\n                            domain=int,\n                            default=None)\n\n        self.add_to_config('mult_rho_update_start_iteration',\n                            description=\"start doing rhomult rho updates on this iteration (default 2)\",\n                            domain=int,\n                            default=2)\n\n    def mult_rho_to_dict(self):\n        assert hasattr(self, \"mult_rho\")\n        return {\"mult_rho\": self.mult_rho,\n                \"convergence_tolerance\": self.mult_rho_convergence_tolerance,\n                \"rho_update_stop_iteration\": self.mult_rho_update_stop_iteration,\n                \"rho_update_start_iteration\": self.mult_rho_update_start_iteration,\n                \"verbose\": False}\n\n\n\n    def xhatspecific_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('xhatspecific',\n                              description=\"have an xhatspecific spoke\",\n                              domain=bool,\n                              default=False)\n\n\n    def xhatxbar_args(self):\n\n        self.add_to_config('xhatxbar',\n                              description=\"have an xhatxbar spoke\",\n                              domain=bool,\n                              default=False)\n\n\n\n\n    def xhatlshaped_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('xhatlshaped',\n                              description=\"have an xhatlshaped spoke\",\n                              domain=bool,\n                              default=False)\n\n    def wtracker_args(self):\n\n        self.add_to_config('wtracker',\n                              description=\"Use a wtracker extension\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config('wtracker_file_prefix',\n                            description=\"prefix for rank by rank wtracker files (default '')\",\n                            domain=str,\n                            default='')\n\n        self.add_to_config('wtracker_wlen',\n                            description=\"max length of iteration window for xtracker (default 20)\",\n                            domain=int,\n                            default=20)\n\n        self.add_to_config('wtracker_reportlen',\n                            description=\"max length of long reports for xtracker (default 100)\",\n                            domain=int,\n                            default=100)\n\n        self.add_to_config('wtracker_stdevthresh',\n                            description=\"Ignore moving std dev below this value (default None)\",\n                            domain=float,\n                            default=None)\n\n\n    def slammax_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('slammax',\n                            description=\"have a slammax spoke\",\n                              domain=bool,\n                              default=False)\n\n\n    def slammin_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('slammin',\n                            description=\"have a slammin spoke\",\n                              domain=bool,\n                              default=False)\n\n\n    def cross_scenario_cuts_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('cross_scenario_cuts',\n                              description=\"have a cross scenario cuts spoke\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"cross_scenario_iter_cnt\",\n                              description=\"cross scen check bound improve iterations \"\n                              \"(default 4)\",\n                              domain=int,\n                              default=4)\n\n        self.add_to_config(\"eta_bounds_mipgap\",\n                              description=\"mipgap for determining eta bounds for cross \"\n                              \"scenario cuts (default 0.01)\",\n                              domain=float,\n                              default=0.01)\n\n\n    def gradient_args(self):\n         # we will not try to get the specification from the command line\n\n        self.add_to_config(\"xhatpath\",\n                           description=\"path to npy file with xhat\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"grad_cost_file\",\n                           description=\"name of the gradient cost file (must be csv)\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"grad_rho_file\",\n                           description=\"name of the gradient rho file (must be csv)\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"order_stat\",\n                           description=\"order statistic for rho (must be between 0 and 1)\",\n                           domain=float,\n                           default=-1.0)\n\n    def rho_args(self):\n        self.add_to_config(\"whatpath\",\n                           description=\"path to csv file with what\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"rho_file\",\n                           description=\"name of the rho file (must be csv)\",\n                           domain=str,\n                           default='')\n        self.add_to_config('rho_setter',\n                           description=\"use rho setter from a rho file\",\n                           domain=bool,\n                           default=False)\n        self.add_to_config(\"rho_path\",\n                           description=\"csv file for the rho setter\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"order_stat\",\n                           description=\"order statistic for rho: must be between 0 (the min) and 1 (the max); 0.5 iis the average\",\n                           domain=float,\n                           default=-1.0)\n        self.add_to_config(\"rho_relative_bound\",\n                           description=\"factor that bounds rho/cost\",\n                           domain=float,\n                           default=1e3)\n\n    def converger_args(self):\n        self.add_to_config(\"use_norm_rho_converger\",\n                         description=\"Use the norm rho converger\",\n                         domain=bool,\n                         default=False)\n        self.add_to_config(\"primal_dual_converger\",\n                            description=\"Use the primal dual converger\",\n                            domain=bool,\n                            default=False)\n        self.add_to_config(\"primal_dual_converger_tol\",\n                            description=\"Tolerance for primal dual converger (default 1e-2)\",\n                            domain=float,\n                            default=1e-2)\n\n    def tracking_args(self):\n        self.add_to_config(\"tracking_folder\",\n                            description=\"Path of results folder (default results)\",\n                            domain=str,\n                            default=\"results\")\n        self.add_to_config(\"ph_track_progress\",\n                            description=\"Adds tracking extension to all\"\n                            \" ph opt cylinders (default False). Use --track_*\"\n                            \" to specificy what and how to track.\"\n                            \" See mpisppy.utils.cfg_vanilla.add_ph_tracking for details\",\n                            domain=bool,\n                            default=False)\n        self.add_to_config(\"track_convergence\",\n                            description=\"Adds convergence tracking ie\"\n                                \" gaps and bounds (default 0)\",\n                            domain=int,\n                            default=0)\n        self.add_to_config(\"track_xbars\",\n                            description=\"Adds xbar tracking (default 0)\",\n                            domain=int,\n                            default=0)\n        self.add_to_config(\"track_duals\",\n                            description=\"Adds w tracking (default 0)\",\n                            domain=int,\n                            default=0)\n        self.add_to_config(\"track_nonants\",\n                            description=\"Adds nonant tracking (default 0)\",\n                            domain=int,\n                            default=0)\n        self.add_to_config('track_scen_gaps',\n                            description=\"Adds scenario gap tracking (default 0)\",\n                            domain=int,\n                            default=0)\n\n    def wxbar_read_write_args(self):\n        self.add_to_config(\"init_W_fname\",\n                                description=\"Path of initial W file (default None)\",\n                                domain=str,\n                                default=None)\n        self.add_to_config(\"init_Xbar_fname\",\n                                description=\"Path of initial Xbar file (default None)\",\n                                domain=str,\n                                default=None)\n        self.add_to_config(\"init_separate_W_files\",\n                                description=\"If True, W is read from separate files (default False)\",\n                                domain=bool,\n                                default=False)\n        self.add_to_config(\"W_fname\",\n                                description=\"Path of final W file (default None)\",\n                                domain=str,\n                                default=None)\n        self.add_to_config(\"Xbar_fname\",\n                                description=\"Path of final Xbar file (default None)\",\n                                domain=str,\n                                default=None)\n        self.add_to_config(\"separate_W_files\",\n                                description=\"If True, writes W to separate files (default False)\",\n                                domain=bool,\n                                default=False)\n\n\n    #================\n    def create_parser(self,progname=None):\n        # seldom used\n        if len(self) == 0:\n            raise RuntimeError(\"create parser called before Config is populated\")\n        parser = argparse.ArgumentParser(progname, conflict_handler=\"resolve\")\n        self.initialize_argparse(parser)\n        return parser\n\n    #================\n    def parse_command_line(self, progname=None):\n        # often used, but the return value less so\n        if len(self) == 0:\n            raise RuntimeError(\"create parser called before Config is populated\")\n        parser = self.create_parser(progname)\n        args = parser.parse_args()\n        args = self.import_argparse(args)\n        return args",
  "def add_to_config(self, name, description, domain, default,\n                      argparse=True,\n                      complain=False,\n                      argparse_args=None):\n        \"\"\" Add an arg to the self dict.\n        Args:\n            name (str): the argument name, underscore seperated\n            description (str): free text description\n            domain (type): see pyomo config docs\n            default (domain): value before argparse\n            argparse (bool): if True put on command ine\n            complain (bool): if True, output a message for a duplicate\n            argparse_args (dict): args to pass to argpars (option; e.g. required, or group)\n        \"\"\"\n        if name in self:\n            if complain:\n                print(f\"Duplicate {name} will not be added to self.\")\n                # raise RuntimeError(f\"Trying to add duplicate {name} to self.\")\n        else:\n            c = self.declare(name, pyofig.ConfigValue(\n                description = description,\n                domain = domain,\n                default = default))\n            if argparse:\n                if argparse_args is not None:\n                    c.declare_as_argument(**argparse_args)\n                else:\n                    c.declare_as_argument()",
  "def add_and_assign(self, name, description, domain, default, value, complain=False):\n        \"\"\" Add an arg to the self dict and assign it a value\n        Args:\n             name (str): the argument name, underscore separated\n            description (str): free text description\n            domain (type): see pyomo config docs\n            default (domain): probably unused, but here to avoid cut-and-paste errors\n            value (domain): the value to assign\n            complain (bool): if True, output a message for a duplicate\n        \"\"\"\n        if name in self:\n            if complain:\n                print(f\"Duplicate {name} will not be added to self by add_and_assign {value}.\")\n                # raise RuntimeError(f\"Trying to add duplicate {name} to self.\")\n        else:\n            self.add_to_config(name, description, domain, default, argparse=False)\n            self[name] = value",
  "def dict_assign(self, name, description, domain, default, value):\n        \"\"\" mimic dict assignment\n        Args:\n            name (str): the argument name, underscore separated\n            description (str): free text description\n            domain (type): see pyomo config docs\n            default (domain): probably unused, but here to avoid cut-and-paste errors\n            value (domain): the value to assign\n        \"\"\"\n        if name not in self:\n            self.add_and_assign(name, description, domain, default, value)\n        else:\n            self[name] = value",
  "def quick_assign(self, name, domain, value):\n        \"\"\" mimic dict assignment with fewer args\n        Args:\n            name (str): the argument name, underscore separated\n            domain (type): see pyomo config docs\n            value (domain): the value to assign\n        \"\"\"\n        self.dict_assign(name, f\"field for {name}\", domain, None, value)",
  "def get(self, name, ifmissing=None):\n        \"\"\" replcate the behavior of dict get\"\"\"\n        if name in self:\n            return self[name]\n        else:\n            return ifmissing",
  "def add_solver_specs(self, prefix=\"\"):\n        sstr = f\"{prefix}_solver\" if prefix != \"\" else \"solver\"\n        self.add_to_config(f\"{sstr}_name\",\n                            description= \"solver name (default None)\",\n                            domain = str,\n                            default=None)\n\n        self.add_to_config(f\"{sstr}_options\",\n                            description= \"solver options; space delimited with = for values (default None)\",\n                            domain = str,\n                            default=None)",
  "def _common_args(self):\n        raise RuntimeError(\"_common_args is no longer used. See comments at top of config.py\")",
  "def popular_args(self):\n        self.add_to_config(\"max_iterations\",\n                            description=\"hub max iiterations (default 1)\",\n                            domain=int,\n                            default=1)\n\n        self.add_solver_specs(prefix=\"\")\n\n        self.add_to_config(\"seed\",\n                            description=\"Seed for random numbers (default is 1134)\",\n                            domain=int,\n                            default=1134)\n\n        self.add_to_config(\"default_rho\",\n                            description=\"Global rho for PH (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"bundles_per_rank\",\n                            description=\"bundles per rank (default 0 (no bundles))\",\n                            domain=int,\n                            default=0)\n\n        self.add_to_config('verbose',\n                              description=\"verbose output\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config('display_progress',\n                              description=\"display progress at each iteration\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config('display_convergence_detail',\n                              description=\"display non-anticipative variable convergence statistics at each iteration\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"max_solver_threads\",\n                            description=\"Limit on threads per solver (default None)\",\n                            domain=int,\n                            default=None)\n\n        self.add_to_config(\"intra_hub_conv_thresh\",\n                            description=\"Within hub convergence threshold (default 1e-10)\",\n                            domain=float,\n                            default=1e-10)\n\n        self.add_to_config(\"trace_prefix\",\n                            description=\"Prefix for bound spoke trace files. If None \"\n                                 \"bound spoke trace files are not written.\",\n                            domain=str,\n                            default=None)\n\n        self.add_to_config(\"tee_rank0_solves\",\n                              description=\"Some cylinders support tee of rank 0 solves.\"\n                              \"(With multiple cylinders this could be confusing.)\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"auxilliary\",\n                            description=\"Free text for use by hackers (default '').\",\n                            domain=str,\n                            default='')",
  "def ph_args(self):\n        self.add_to_config(\"linearize_binary_proximal_terms\",\n                              description=\"For PH, linearize the proximal terms for \"\n                              \"all binary nonanticipative variables\",\n                              domain=bool,\n                              default=False)\n\n\n        self.add_to_config(\"linearize_proximal_terms\",\n                              description=\"For PH, linearize the proximal terms for \"\n                              \"all nonanticipative variables\",\n                              domain=bool,\n                              default=False)\n\n\n        self.add_to_config(\"proximal_linearization_tolerance\",\n                            description=\"For PH, when linearizing proximal terms, \"\n                            \"a cut will be added if the proximal term approximation \"\n                            \"is looser than this value (default 1e-1)\",\n                            domain=float,\n                            default=1.e-1)",
  "def make_parser(self, progname=None, num_scens_reqd=False):\n        raise RuntimeError(\"make_parser is no longer used. See comments at top of config.py\")",
  "def num_scens_optional(self):\n        self.add_to_config(\n            \"num_scens\",\n            description=\"Number of scenarios (default None)\",\n            domain=int,\n            default=None,\n        )",
  "def num_scens_required(self):\n        # required, but not postional\n        self.add_to_config(\n            \"num_scens\",\n            description=\"Number of scenarios (default None)\",\n            domain=int,\n            default=None,\n            argparse_args = {\"required\": True}\n        )",
  "def _basic_multistage(self, progname=None, num_scens_reqd=False):\n        raise RuntimeError(\"_basic_multistage is no longer used. See comments at top of config.py\")",
  "def add_branching_factors(self):\n        self.add_to_config(\"branching_factors\",\n                            description=\"Spaces delimited branching factors (e.g., 2 2)\",\n                            domain=pyofig.ListOf(int, pyofig.PositiveInt),\n                            default=None)",
  "def make_multistage_parser(self, progname=None):\n        raise RuntimeError(\"make_multistage_parser is no longer used. See comments at top of config.py\")",
  "def multistage(self):\n        self.add_branching_factors()\n        self.popular_args()",
  "def make_EF2_parser(self, progname=None, num_scens_reqd=False):\n        raise RuntimeError(\"make_EF2_parser is no longer used. See comments at top of config.py\")",
  "def _EF_base(self):\n\n        self.add_solver_specs(prefix=\"EF\")\n\n        self.add_to_config(\"EF_mipgap\",\n                           description=\"mip gap option for the solver if needed (default None)\",\n                           domain=float,\n                           default=None)",
  "def EF2(self):\n        self._EF_base()\n        self.add_to_config(\"num_scens\",\n                           description=\"Number of scenarios (default None)\",\n                           domain=int,\n                           default=None)",
  "def make_EF_multistage_parser(self, progname=None, num_scens_reqd=False):\n        raise RuntimeError(\"make_EF_multistage_parser is no longer used. See comments at top of config.py\")",
  "def EF_multistage(self):\n\n        self._EF_base()",
  "def two_sided_args(self):\n        # add commands to  and also return the result\n\n        self.add_to_config(\"rel_gap\",\n                            description=\"relative termination gap (default 0.05)\",\n                            domain=float,\n                            default=0.05)\n\n        self.add_to_config(\"abs_gap\",\n                            description=\"absolute termination gap (default 0)\",\n                            domain=float,\n                            default=0.)\n\n        self.add_to_config(\"max_stalled_iters\",\n                            description=\"maximum iterations with no reduction in gap (default 100)\",\n                            domain=int,\n                            default=100)",
  "def mip_options(self):\n\n        self.add_to_config(\"iter0_mipgap\",\n                            description=\"mip gap option for iteration 0 (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"iterk_mipgap\",\n                            description=\"mip gap option non-zero iterations (default None)\",\n                            domain=float,\n                            default=None)",
  "def aph_args(self):\n\n        self.add_to_config('aph_gamma',\n                            description='Gamma parameter associated with asychronous projective hedging (default 1.0)',\n                            domain=float,\n                            default=1.0)\n        self.add_to_config('aph_nu',\n                            description='Nu parameter associated with asychronous projective hedging (default 1.0)',\n                            domain=float,\n                            default=1.0)\n        self.add_to_config('aph_frac_needed',\n                            description='Fraction of sub-problems required before computing projective step (default 1.0)',\n                            domain=float,\n                            default=1.0)\n        self.add_to_config('aph_dispatch_frac',\n                            description='Fraction of sub-problems to dispatch at each step of asychronous projective hedging (default 1.0)',\n                            domain=float,\n                            default=1.0)\n        self.add_to_config('aph_sleep_seconds',\n                            description='Spin-lock sleep time for APH (default 0.01)',\n                            domain=float,\n                            default=0.01)",
  "def fixer_args(self):\n\n        self.add_to_config('fixer',\n                           description=\"have an integer fixer extension\",\n                           domain=bool,\n                           default=False)\n\n        self.add_to_config(\"fixer_tol\",\n                           description=\"fixer bounds tolerance  (default 1e-4)\",\n                           domain=float,\n                           default=1e-2)",
  "def fwph_args(self):\n\n        self.add_to_config('fwph',\n                           description=\"have an fwph spoke\",\n                           domain=bool,\n                           default=False)\n\n        self.add_to_config(\"fwph_iter_limit\",\n                            description=\"maximum fwph iterations (default 10)\",\n                            domain=int,\n                            default=10)\n\n        self.add_to_config(\"fwph_weight\",\n                            description=\"fwph weight (default 0)\",\n                            domain=float,\n                            default=0.0)\n\n        self.add_to_config(\"fwph_conv_thresh\",\n                            description=\"fwph convergence threshold  (default 1e-4)\",\n                            domain=float,\n                            default=1e-4)\n\n        self.add_to_config(\"fwph_stop_check_tol\",\n                            description=\"fwph tolerance for Gamma^t (default 1e-4)\",\n                            domain=float,\n                            default=1e-4)\n\n        self.add_to_config(\"fwph_mipgap\",\n                            description=\"mip gap option FW subproblems iterations (default None)\",\n                            domain=float,\n                            default=None)",
  "def lagrangian_args(self):\n\n        self.add_to_config('lagrangian',\n                              description=\"have a lagrangian spoke\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"lagrangian_iter0_mipgap\",\n                            description=\"lgr. iter0 solver option mipgap (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"lagrangian_iterk_mipgap\",\n                            description=\"lgr. iterk solver option mipgap (default None)\",\n                            domain=float,\n                            default=None)",
  "def lagranger_args(self):\n\n        self.add_to_config('lagranger',\n                            description=\"have a special lagranger spoke\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"lagranger_iter0_mipgap\",\n                            description=\"lagranger iter0 mipgap (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"lagranger_iterk_mipgap\",\n                            description=\"lagranger iterk mipgap (default None)\",\n                            domain=float,\n                            default=None)\n\n        self.add_to_config(\"lagranger_rho_rescale_factors_json\",\n                            description=\"json file: rho rescale factors (default None)\",\n                            domain=str,\n                            default=None)",
  "def xhatlooper_args(self):\n\n        self.add_to_config('xhatlooper',\n                              description=\"have an xhatlooper spoke\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"xhat_scen_limit\",\n                            description=\"scenario limit xhat looper to try (default 3)\",\n                            domain=int,\n                            default=3)",
  "def xhatshuffle_args(self):\n\n        self.add_to_config('xhatshuffle',\n                           description=\"have an xhatshuffle spoke\",\n                           domain=bool,\n                           default=False)\n\n        self.add_to_config('add_reversed_shuffle',\n                           description=\"using also the reversed shuffling (multistage only, default True)\",\n                           domain=bool,\n                           default=False)\n\n        self.add_to_config('xhatshuffle_iter_step',\n                           description=\"step in shuffled list between 2 scenarios to try (default None)\",\n                           domain=int,\n                           default=None)",
  "def mult_rho_args(self):\n\n        self.add_to_config('mult_rho',\n                              description=\"Have mult_rho extension (default False)\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config('mult_rho_convergence_tolerance',\n                            description=\"rhomult does nothing with convergence below this (default 1e-4)\",\n                              domain=float,\n                              default=1e-4)\n\n        self.add_to_config('mult_rho_update_stop_iteration',\n                            description=\"stop doing rhomult rho updates after this iteration (default None)\",\n                            domain=int,\n                            default=None)\n\n        self.add_to_config('mult_rho_update_start_iteration',\n                            description=\"start doing rhomult rho updates on this iteration (default 2)\",\n                            domain=int,\n                            default=2)",
  "def mult_rho_to_dict(self):\n        assert hasattr(self, \"mult_rho\")\n        return {\"mult_rho\": self.mult_rho,\n                \"convergence_tolerance\": self.mult_rho_convergence_tolerance,\n                \"rho_update_stop_iteration\": self.mult_rho_update_stop_iteration,\n                \"rho_update_start_iteration\": self.mult_rho_update_start_iteration,\n                \"verbose\": False}",
  "def xhatspecific_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('xhatspecific',\n                              description=\"have an xhatspecific spoke\",\n                              domain=bool,\n                              default=False)",
  "def xhatxbar_args(self):\n\n        self.add_to_config('xhatxbar',\n                              description=\"have an xhatxbar spoke\",\n                              domain=bool,\n                              default=False)",
  "def xhatlshaped_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('xhatlshaped',\n                              description=\"have an xhatlshaped spoke\",\n                              domain=bool,\n                              default=False)",
  "def wtracker_args(self):\n\n        self.add_to_config('wtracker',\n                              description=\"Use a wtracker extension\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config('wtracker_file_prefix',\n                            description=\"prefix for rank by rank wtracker files (default '')\",\n                            domain=str,\n                            default='')\n\n        self.add_to_config('wtracker_wlen',\n                            description=\"max length of iteration window for xtracker (default 20)\",\n                            domain=int,\n                            default=20)\n\n        self.add_to_config('wtracker_reportlen',\n                            description=\"max length of long reports for xtracker (default 100)\",\n                            domain=int,\n                            default=100)\n\n        self.add_to_config('wtracker_stdevthresh',\n                            description=\"Ignore moving std dev below this value (default None)\",\n                            domain=float,\n                            default=None)",
  "def slammax_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('slammax',\n                            description=\"have a slammax spoke\",\n                              domain=bool,\n                              default=False)",
  "def slammin_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('slammin',\n                            description=\"have a slammin spoke\",\n                              domain=bool,\n                              default=False)",
  "def cross_scenario_cuts_args(self):\n        # we will not try to get the specification from the command line\n\n        self.add_to_config('cross_scenario_cuts',\n                              description=\"have a cross scenario cuts spoke\",\n                              domain=bool,\n                              default=False)\n\n        self.add_to_config(\"cross_scenario_iter_cnt\",\n                              description=\"cross scen check bound improve iterations \"\n                              \"(default 4)\",\n                              domain=int,\n                              default=4)\n\n        self.add_to_config(\"eta_bounds_mipgap\",\n                              description=\"mipgap for determining eta bounds for cross \"\n                              \"scenario cuts (default 0.01)\",\n                              domain=float,\n                              default=0.01)",
  "def gradient_args(self):\n         # we will not try to get the specification from the command line\n\n        self.add_to_config(\"xhatpath\",\n                           description=\"path to npy file with xhat\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"grad_cost_file\",\n                           description=\"name of the gradient cost file (must be csv)\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"grad_rho_file\",\n                           description=\"name of the gradient rho file (must be csv)\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"order_stat\",\n                           description=\"order statistic for rho (must be between 0 and 1)\",\n                           domain=float,\n                           default=-1.0)",
  "def rho_args(self):\n        self.add_to_config(\"whatpath\",\n                           description=\"path to csv file with what\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"rho_file\",\n                           description=\"name of the rho file (must be csv)\",\n                           domain=str,\n                           default='')\n        self.add_to_config('rho_setter',\n                           description=\"use rho setter from a rho file\",\n                           domain=bool,\n                           default=False)\n        self.add_to_config(\"rho_path\",\n                           description=\"csv file for the rho setter\",\n                           domain=str,\n                           default='')\n        self.add_to_config(\"order_stat\",\n                           description=\"order statistic for rho: must be between 0 (the min) and 1 (the max); 0.5 iis the average\",\n                           domain=float,\n                           default=-1.0)\n        self.add_to_config(\"rho_relative_bound\",\n                           description=\"factor that bounds rho/cost\",\n                           domain=float,\n                           default=1e3)",
  "def converger_args(self):\n        self.add_to_config(\"use_norm_rho_converger\",\n                         description=\"Use the norm rho converger\",\n                         domain=bool,\n                         default=False)\n        self.add_to_config(\"primal_dual_converger\",\n                            description=\"Use the primal dual converger\",\n                            domain=bool,\n                            default=False)\n        self.add_to_config(\"primal_dual_converger_tol\",\n                            description=\"Tolerance for primal dual converger (default 1e-2)\",\n                            domain=float,\n                            default=1e-2)",
  "def tracking_args(self):\n        self.add_to_config(\"tracking_folder\",\n                            description=\"Path of results folder (default results)\",\n                            domain=str,\n                            default=\"results\")\n        self.add_to_config(\"ph_track_progress\",\n                            description=\"Adds tracking extension to all\"\n                            \" ph opt cylinders (default False). Use --track_*\"\n                            \" to specificy what and how to track.\"\n                            \" See mpisppy.utils.cfg_vanilla.add_ph_tracking for details\",\n                            domain=bool,\n                            default=False)\n        self.add_to_config(\"track_convergence\",\n                            description=\"Adds convergence tracking ie\"\n                                \" gaps and bounds (default 0)\",\n                            domain=int,\n                            default=0)\n        self.add_to_config(\"track_xbars\",\n                            description=\"Adds xbar tracking (default 0)\",\n                            domain=int,\n                            default=0)\n        self.add_to_config(\"track_duals\",\n                            description=\"Adds w tracking (default 0)\",\n                            domain=int,\n                            default=0)\n        self.add_to_config(\"track_nonants\",\n                            description=\"Adds nonant tracking (default 0)\",\n                            domain=int,\n                            default=0)\n        self.add_to_config('track_scen_gaps',\n                            description=\"Adds scenario gap tracking (default 0)\",\n                            domain=int,\n                            default=0)",
  "def wxbar_read_write_args(self):\n        self.add_to_config(\"init_W_fname\",\n                                description=\"Path of initial W file (default None)\",\n                                domain=str,\n                                default=None)\n        self.add_to_config(\"init_Xbar_fname\",\n                                description=\"Path of initial Xbar file (default None)\",\n                                domain=str,\n                                default=None)\n        self.add_to_config(\"init_separate_W_files\",\n                                description=\"If True, W is read from separate files (default False)\",\n                                domain=bool,\n                                default=False)\n        self.add_to_config(\"W_fname\",\n                                description=\"Path of final W file (default None)\",\n                                domain=str,\n                                default=None)\n        self.add_to_config(\"Xbar_fname\",\n                                description=\"Path of final Xbar file (default None)\",\n                                domain=str,\n                                default=None)\n        self.add_to_config(\"separate_W_files\",\n                                description=\"If True, writes W to separate files (default False)\",\n                                domain=bool,\n                                default=False)",
  "def create_parser(self,progname=None):\n        # seldom used\n        if len(self) == 0:\n            raise RuntimeError(\"create parser called before Config is populated\")\n        parser = argparse.ArgumentParser(progname, conflict_handler=\"resolve\")\n        self.initialize_argparse(parser)\n        return parser",
  "def parse_command_line(self, progname=None):\n        # often used, but the return value less so\n        if len(self) == 0:\n            raise RuntimeError(\"create parser called before Config is populated\")\n        parser = self.create_parser(progname)\n        args = parser.parse_args()\n        args = self.import_argparse(args)\n        return args",
  "def dill_pickle(model, fname):\n    \"\"\" serialize model using dill to file name\"\"\"\n    global_toc(f\"about to pickle to {fname}\")\n    with open(fname, \"wb\") as f:\n        dill.dump(model, f)\n    global_toc(f\"done with pickle {fname}\")",
  "def dill_unpickle(fname):\n    \"\"\" load a model from fname\"\"\"\n    \n    global_toc(f\"about to unpickle {fname}\")\n    with open(fname, \"rb\") as f:\n        m = dill.load(f)\n    global_toc(f\"done with unpickle {fname}\")\n    return m",
  "def pickle_bundle_parser(cfg):\n    \"\"\" Add command line options for creation and use of \"proper\" bundles\n    args:\n        cfg (Config): the Config object to which we add\"\"\"\n    cfg.add_to_config('pickle_bundles_dir',\n                        description=\"Write bundles to a dill pickle files in this dir (default None)\",\n                        domain=str,\n                        default=None)\n    \n    cfg.add_to_config('unpickle_bundles_dir',\n                        description=\"Read bundles from a dill pickle files in this dir; (default None)\",\n                        domain=str,\n                        default=None)\n    cfg.add_to_config(\"scenarios_per_bundle\",\n                        description=\"Used for `proper` bundles only (default None)\",\n                        domain=int,\n                        default=None)",
  "def check_args(cfg):\n    \"\"\" make sure the pickle bundle args make sense\"\"\"\n    assert(cfg.pickle_bundles_dir is None or cfg.unpickle_bundles_dir is None)\n    if cfg.scenarios_per_bundle is None:\n        raise RuntimeError(\"For proper bundles, --scenarios-per-bundle must be specified\")\n    if cfg.get(\"bundles_per_rank\") is not None and cfg.bundles_per_rank != 0:\n        raise RuntimeError(\"For proper bundles, --scenarios-per-bundle must be specified \"\n                           \"and --bundles-per-rank cannot be\")\n    if cfg.pickle_bundles_dir is not None and not os.path.isdir(cfg.pickle_bundles_dir):\n        raise RuntimeError(f\"Directory to pickle into not found: {cfg.pickle_bundles_dir}\")\n    if cfg.unpickle_bundles_dir is not None and not os.path.isdir(cfg.unpickle_bundles_dir):\n        raise RuntimeError(f\"Directory to load pickle files from not found: {cfg.unpickle_bundles_dir}\")",
  "def have_proper_bundles(cfg):\n    \"\"\" boolean to indicate we have pickled bundles\"\"\"\n    return (hasattr(cfg, \"pickle_bundles_dir\") and cfg.pickle_bundles_dir is not None)\\\n       or (hasattr(cfg, \"unpickle_bundles_dir\") and cfg.unpickle_bundles_dir is not None)",
  "def rhos_to_csv(s, filename):\n    \"\"\" write the rho values to a csv \"fullname\", rho\n    Args:\n        s (ConcreteModel): the scenario Pyomo model\n        filenaame (str): file to which to write\n    \"\"\"\n    with open(filename, \"w\") as f:\n        f.write(\"fullname,rho\\n\")\n        for ndn_i, rho in s._mpisppy_model.rho.items():\n            vdata = s._mpisppy_data.nonant_indices[ndn_i]\n            fullname = vdata.name\n            f.write(f'\"{fullname}\",{rho._value}\\n')",
  "def rho_list_from_csv(s, filename):\n    \"\"\" read rho values from a file and return a list suitable for rho_setter\n    Args:\n        s (ConcreteModel): scenario whence the id values come\n        filename (str): name of the csv file to read (fullname, rho)\n    Returns:\n        retlist (list of (id, rho) tuples); list suitable for rho_setter\n   \"\"\"\n    rhodf = pd.read_csv(filename)\n    retlist = list()\n    for idx, row in rhodf.iterrows():\n        fullname = row[\"fullname\"]\n        vo = s.find_component(fullname)\n        if vo is not None:\n            retlist.append((id(vo), row[\"rho\"]))\n        else:\n            raise RuntimeError(f\"rho values from {filename} found Var {fullname} \"\n                               f\"that is not found in the scenario given (name={s._name})\")\n    return retlist",
  "class Xhat_Eval(mpisppy.spopt.SPOpt):\n    \"\"\" See SPOpt for list of args. \"\"\"\n    \n    def __init__(\n        self,\n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,\n        mpicomm=None,\n        scenario_creator_kwargs=None,\n        variable_probability=None,\n        ):\n        \n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            variable_probability=variable_probability,\n        )\n        \n        self.verbose = self.options['verbose']\n        \n        #TODO: CHANGE THIS AFTER UPDATE\n        self.PH_extensions = None\n\n        self._subproblems_solvers_created = False\n        \n\n    def _lazy_create_solvers(self):\n        if self._subproblems_solvers_created:\n            return\n        self.subproblem_creation(self.verbose)\n        self._create_solvers()\n        self._subproblems_solvers_created = True\n\n\n    #======================================================================\n    def solve_one(self, solver_options, k, s,\n                  dtiming=False,\n                  gripe=False,\n                  tee=False,\n                  verbose=False,\n                  disable_pyomo_signal_handling=False,\n                  update_objective=True,\n                  compute_val_at_nonant=False):\n\n        self._lazy_create_solvers()\n        pyomo_solve_time = super().solve_one(solver_options, k, s,\n                                             dtiming=dtiming,\n                                             gripe=gripe,\n                                             tee=tee,\n                                             verbose=verbose,\n                                             disable_pyomo_signal_handling=disable_pyomo_signal_handling,\n                                             update_objective=update_objective)\n\n        solve_keyword_args = dict()\n        if self.cylinder_rank == 0:\n            if tee is not None and tee is True:\n                solve_keyword_args[\"tee\"] = True\n        if (sputils.is_persistent(s._solver_plugin)):\n            solve_keyword_args[\"save_results\"] = False\n        elif disable_pyomo_signal_handling:\n            solve_keyword_args[\"use_signal_handling\"] = False\n\n        try:\n            results = s._solver_plugin.solve(s,\n                                             **solve_keyword_args,\n                                             load_solutions=False)\n            solver_exception = None\n        except Exception as e:\n            results = None\n            solver_exception = e\n\n        if (results is None) or (len(results.solution) == 0) or \\\n                (results.solution(0).status == SolutionStatus.infeasible) or \\\n                (results.solver.termination_condition == TerminationCondition.infeasible) or \\\n                (results.solver.termination_condition == TerminationCondition.infeasibleOrUnbounded) or \\\n                (results.solver.termination_condition == TerminationCondition.unbounded):\n\n            s._mpisppy_data.scenario_feasible = False\n        else:\n            if sputils.is_persistent(s._solver_plugin):\n                s._solver_plugin.load_vars()\n            else:\n                s.solutions.load_from(results)\n            if self.is_minimizing:\n                s._mpisppy_data.outer_bound = results.Problem[0].Lower_bound\n            else:\n                s._mpisppy_data.outer_bound = results.Problem[0].Upper_bound\n                s._mpisppy_data.scenario_feasible = True\n\n\n        if compute_val_at_nonant:\n                if self.bundling:\n                    objfct = self.saved_objs[k]\n                    \n                else:\n                    objfct = sputils.find_active_objective(s)\n                    if self.verbose:\n                        print (\"caller\", inspect.stack()[1][3])\n                        print (\"E_Obj Scenario {}, prob={}, Obj={}, ObjExpr={}\"\\\n                               .format(k, s._mpisppy_probability, pyo.value(objfct), objfct.expr))\n                self.objs_dict[k] = pyo.value(objfct)\n        return(pyomo_solve_time)\n\n\n    def solve_loop(self, solver_options=None,\n                   use_scenarios_not_subproblems=False,\n                   dtiming=False,\n                   gripe=False,\n                   disable_pyomo_signal_handling=False,\n                   tee=False,\n                   verbose=False,\n                   compute_val_at_nonant=False):\n        \"\"\" Loop over self.local_subproblems and solve them in a manner \n            dicated by the arguments. In addition to changing the Var\n            values in the scenarios, update _PySP_feas_indictor for each.\n\n        ASSUMES:\n            Every scenario already has a _solver_plugin attached.\n\n        Args:\n            solver_options (dict or None): the scenario solver options\n            use_scenarios_not_subproblems (boolean): for use by bounds\n            dtiming (boolean): indicates that timing should be reported\n            gripe (boolean): output a message if a solve fails\n            disable_pyomo_signal_handling (boolean): set to true for asynch, \n                                                     ignored for persistent solvers.\n            tee (boolean): show solver output to screen if possible\n            verbose (boolean): indicates verbose output\n            compute_val_at_nonant (boolean): indicate that self.objs_dict should\n                                            be created and computed.\n                                            \n\n        NOTE: I am not sure what happens with solver_options None for\n              a persistent solver. Do options persist?\n\n        NOTE: set_objective takes care of W and prox changes.\n        \"\"\"\n        self._lazy_create_solvers()\n        def _vb(msg): \n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)\n        logger.debug(\"  early solve_loop for rank={}\".format(self.cylinder_rank))\n\n        # note that when there is no bundling, scenarios are subproblems\n        if use_scenarios_not_subproblems:\n            s_source = self.local_scenarios\n        else:\n            s_source = self.local_subproblems\n            \n        if compute_val_at_nonant:\n            self.objs_dict={}\n        \n        for k,s in s_source.items():\n            if tee:\n                print(f\"Tee solve for {k} on global rank {self.global_rank}\")\n            logger.debug(\"  in loop solve_loop k={}, rank={}\".format(k, self.cylinder_rank))\n\n            pyomo_solve_time = self.solve_one(solver_options, k, s,\n                                              dtiming=dtiming,\n                                              verbose=verbose,\n                                              tee=tee,\n                                              gripe=gripe,\n                disable_pyomo_signal_handling=disable_pyomo_signal_handling,\n                compute_val_at_nonant=compute_val_at_nonant)\n\n        if dtiming:\n            all_pyomo_solve_times = self.mpicomm.gather(pyomo_solve_time, root=0)\n            if self.cylinder_rank == 0:\n                print(\"Pyomo solve times (seconds):\")\n                print(\"\\tmin=%4.2f mean=%4.2f max=%4.2f\" %\n                      (np.min(all_pyomo_solve_times),\n                      np.mean(all_pyomo_solve_times),\n                      np.max(all_pyomo_solve_times)))\n                \n    #======================================================================   \n    def Eobjective(self, verbose=False, fct=None):\n        \"\"\" Compute the expected value of the composition of the objective function \n            and a given function fct across all scenarios.\n\n        Note: \n            Assumes the optimization is done beforehand,\n            therefore DOES NOT CHECK FEASIBILITY or NON-ANTICIPATIVITY!\n            This method uses whatever the current value of the objective\n            function is.\n\n        Args:\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n            \n            fct (function, optional):\n                A function R-->R^p, such as x|-->(x,x^2,x^3). Default is None\n                If fct is None, Eobjective returns the exepected value.\n                \n\n        Returns:\n            float or numpy.array:\n                The expected objective function value. \n                If fct is R-->R, returns a float.\n                If fct is R-->R^p with p>1, returns a np.array of length p\n        \"\"\"\n        self._lazy_create_solvers()\n        if fct is None:\n            return super().Eobjective(verbose=verbose)\n        \n        if not hasattr(self, \"objs_dict\"):\n            raise RuntimeError(\"Values of the objective functions for each scenario\"+\n                               \" at xhat have to be computed before running Eobjective\")\n        \n\n        local_Eobjs = []\n        for k,s in self.local_scenarios.items():\n            if not k in self.objs_dict:\n                raise RuntimeError(f\"No value has been calculated for the scenario {k}\")\n            local_Eobjs.append(s._mpisppy_probability * fct(self.objs_dict[k]))\n        local_Eobjs = np.array(local_Eobjs)\n        local_Eobj = np.array([np.sum(local_Eobjs,axis=0)])\n        global_Eobj = np.zeros(len(local_Eobj))\n        self.mpicomm.Allreduce(local_Eobj, global_Eobj, op=MPI.SUM)\n        if len(global_Eobj)==1:\n            global_Eobj = global_Eobj[0]\n        return global_Eobj\n    \n    \n    #==============\n    def evaluate_one(self, nonant_cache,scenario_name,s):\n        \"\"\" Evaluate xhat for one scenario.\n\n        Args:\n            nonant_cache(numpy vector): special numpy vector with nonant values (see spopt)\n            scenario_name(str): TODO\n        \n\n        Returns:\n            Eobj (float or None): Expected value (or None if infeasible)\n\n        \"\"\"\n        self._lazy_create_solvers()\n        self._fix_nonants(nonant_cache)\n        if not hasattr(self, \"objs_dict\"):\n            self.objs_dict = {}\n        \n\n        solver_options = self.options[\"solver_options\"] if \"solver_options\" in self.options else None\n        k = scenario_name\n        pyomo_solve_time = self.solve_one(solver_options,k, s,\n                                          dtiming=False,\n                                          verbose=self.verbose,\n                                          tee=False,\n                                          gripe=True,\n                                          compute_val_at_nonant=True\n                                          )\n        \n        obj = self.objs_dict[k]\n        \n        return obj\n    \n    def evaluate(self, nonant_cache, fct=None):\n        \"\"\" Do the optimization and compute the expected value of the composition of the objective function \n            and a given function fct across all scenarios.\n\n        Args:\n            nonant_cache(ndn dict of numpy vector): special numpy vector with nonant values (see spopt)\n            fct (function, optional):\n                A function R-->R^p, such as x|-->(x,x^2,x^3). Default is None\n                If fct is None, evaluate returns the exepected value.\n\n        Returns:\n            Eobj (float or numpy.array): Expected value\n\n        \"\"\"\n        self._lazy_create_solvers()\n        self._fix_nonants(nonant_cache)\n\n        solver_options = self.options[\"solver_options\"] if \"solver_options\" in self.options else None\n        \n        self.solve_loop(solver_options=solver_options,\n                        use_scenarios_not_subproblems=True,\n                        gripe=True, \n                        tee=False,\n                        verbose=self.verbose,\n                        compute_val_at_nonant=True\n                        )\n        \n        Eobj = self.Eobjective(self.verbose,fct=fct)\n        \n        return Eobj\n    \n    \n    \n    def fix_nonants_upto_stage(self,t,cache):\n        \"\"\" Fix the Vars subject to non-anticipativity at given values for stages 1 to t.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n        Args:\n            cache (ndn dict of list or numpy vector): values at which to fix\n        WARNING: \n            We are counting on Pyomo indices not to change order between\n            when the cache_list is created and used.\n        NOTE:\n            You probably want to call _save_nonants right before calling this\n        \"\"\"\n        self._lazy_create_solvers()\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                if node.stage<=t:\n                    ndn = node.name\n                    if ndn not in cache:\n                        raise RuntimeError(\"Could not find {} in {}\"\\\n                                           .format(ndn, cache))\n                    if cache[ndn] is None:\n                        raise RuntimeError(\"Empty cache for scen={}, node={}\".format(k, ndn))\n                    if len(cache[ndn]) != nlens[ndn]:\n                        raise RuntimeError(\"Needed {} nonant Vars for {}, got {}\"\\\n                                           .format(nlens[ndn], ndn, len(cache[ndn])))\n                    for i in range(nlens[ndn]): \n                        this_vardata = node.nonant_vardata_list[i]\n                        this_vardata._value = cache[ndn][i]\n                        this_vardata.fix()\n                        if persistent_solver is not None:\n                            persistent_solver.update_var(this_vardata)\n        \n    \n    \n        \n    #======================================================================\n    def _fix_nonants_at_value(self):\n        \"\"\" Fix the Vars subject to non-anticipativity at their current values.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if not self.bundling:\n                if (sputils.is_persistent(s._solver_plugin)):\n                    persistent_solver = s._solver_plugin\n\n            for var in s._mpisppy_data.nonant_indices.values():\n                var.fix()\n                if not self.bundling and persistent_solver is not None:\n                    persistent_solver.update_var(var)\n\n        if self.bundling:  # we might need to update persistent solvers\n            rank_local = self.cylinder_rank\n            for k,s in self.local_subproblems.items():\n                if (sputils.is_persistent(s._solver_plugin)):\n                    persistent_solver = s._solver_plugin\n                else:\n                    break  # all solvers should be the same\n\n                # the bundle number is the last number in the name\n                bunnum = sputils.extract_num(k)\n                # for the scenarios in this bundle, update Vars\n                for sname, scen in self.local_scenarios.items():\n                    if sname not in self.names_in_bundles[rank_local][bunnum]:\n                        break\n                    for var in scen._mpisppy_data.nonant_indices.values():\n                        persistent_solver.update_var(var)\n\n    def calculate_incumbent(self, fix_nonants=True, verbose=False):\n        \"\"\"\n        Calculates the current incumbent\n\n        Args:\n            solver_options (dict): passed through to the solver\n            verbose (boolean): controls debugging output\n        Returns:\n            xhatobjective (float or None): the objective function\n                or None if one could not be obtained.\n        \"\"\"\n        self._lazy_create_solvers()\n\n        if fix_nonants:\n            self._fix_nonants_at_value()\n\n        self.solve_loop(solver_options=self.current_solver_options, \n                        verbose=verbose)\n\n        infeasP = self.infeas_prob()\n        if infeasP != 0.:\n            return None\n        else:\n            if verbose and self.cylinder_rank == 0:\n                print(\"  Feasible xhat found\")\n            return self.Eobjective(verbose=verbose)",
  "def __init__(\n        self,\n        options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,\n        mpicomm=None,\n        scenario_creator_kwargs=None,\n        variable_probability=None,\n        ):\n        \n        super().__init__(\n            options,\n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement=scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            variable_probability=variable_probability,\n        )\n        \n        self.verbose = self.options['verbose']\n        \n        #TODO: CHANGE THIS AFTER UPDATE\n        self.PH_extensions = None\n\n        self._subproblems_solvers_created = False",
  "def _lazy_create_solvers(self):\n        if self._subproblems_solvers_created:\n            return\n        self.subproblem_creation(self.verbose)\n        self._create_solvers()\n        self._subproblems_solvers_created = True",
  "def solve_one(self, solver_options, k, s,\n                  dtiming=False,\n                  gripe=False,\n                  tee=False,\n                  verbose=False,\n                  disable_pyomo_signal_handling=False,\n                  update_objective=True,\n                  compute_val_at_nonant=False):\n\n        self._lazy_create_solvers()\n        pyomo_solve_time = super().solve_one(solver_options, k, s,\n                                             dtiming=dtiming,\n                                             gripe=gripe,\n                                             tee=tee,\n                                             verbose=verbose,\n                                             disable_pyomo_signal_handling=disable_pyomo_signal_handling,\n                                             update_objective=update_objective)\n\n        solve_keyword_args = dict()\n        if self.cylinder_rank == 0:\n            if tee is not None and tee is True:\n                solve_keyword_args[\"tee\"] = True\n        if (sputils.is_persistent(s._solver_plugin)):\n            solve_keyword_args[\"save_results\"] = False\n        elif disable_pyomo_signal_handling:\n            solve_keyword_args[\"use_signal_handling\"] = False\n\n        try:\n            results = s._solver_plugin.solve(s,\n                                             **solve_keyword_args,\n                                             load_solutions=False)\n            solver_exception = None\n        except Exception as e:\n            results = None\n            solver_exception = e\n\n        if (results is None) or (len(results.solution) == 0) or \\\n                (results.solution(0).status == SolutionStatus.infeasible) or \\\n                (results.solver.termination_condition == TerminationCondition.infeasible) or \\\n                (results.solver.termination_condition == TerminationCondition.infeasibleOrUnbounded) or \\\n                (results.solver.termination_condition == TerminationCondition.unbounded):\n\n            s._mpisppy_data.scenario_feasible = False\n        else:\n            if sputils.is_persistent(s._solver_plugin):\n                s._solver_plugin.load_vars()\n            else:\n                s.solutions.load_from(results)\n            if self.is_minimizing:\n                s._mpisppy_data.outer_bound = results.Problem[0].Lower_bound\n            else:\n                s._mpisppy_data.outer_bound = results.Problem[0].Upper_bound\n                s._mpisppy_data.scenario_feasible = True\n\n\n        if compute_val_at_nonant:\n                if self.bundling:\n                    objfct = self.saved_objs[k]\n                    \n                else:\n                    objfct = sputils.find_active_objective(s)\n                    if self.verbose:\n                        print (\"caller\", inspect.stack()[1][3])\n                        print (\"E_Obj Scenario {}, prob={}, Obj={}, ObjExpr={}\"\\\n                               .format(k, s._mpisppy_probability, pyo.value(objfct), objfct.expr))\n                self.objs_dict[k] = pyo.value(objfct)\n        return(pyomo_solve_time)",
  "def solve_loop(self, solver_options=None,\n                   use_scenarios_not_subproblems=False,\n                   dtiming=False,\n                   gripe=False,\n                   disable_pyomo_signal_handling=False,\n                   tee=False,\n                   verbose=False,\n                   compute_val_at_nonant=False):\n        \"\"\" Loop over self.local_subproblems and solve them in a manner \n            dicated by the arguments. In addition to changing the Var\n            values in the scenarios, update _PySP_feas_indictor for each.\n\n        ASSUMES:\n            Every scenario already has a _solver_plugin attached.\n\n        Args:\n            solver_options (dict or None): the scenario solver options\n            use_scenarios_not_subproblems (boolean): for use by bounds\n            dtiming (boolean): indicates that timing should be reported\n            gripe (boolean): output a message if a solve fails\n            disable_pyomo_signal_handling (boolean): set to true for asynch, \n                                                     ignored for persistent solvers.\n            tee (boolean): show solver output to screen if possible\n            verbose (boolean): indicates verbose output\n            compute_val_at_nonant (boolean): indicate that self.objs_dict should\n                                            be created and computed.\n                                            \n\n        NOTE: I am not sure what happens with solver_options None for\n              a persistent solver. Do options persist?\n\n        NOTE: set_objective takes care of W and prox changes.\n        \"\"\"\n        self._lazy_create_solvers()\n        def _vb(msg): \n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)\n        logger.debug(\"  early solve_loop for rank={}\".format(self.cylinder_rank))\n\n        # note that when there is no bundling, scenarios are subproblems\n        if use_scenarios_not_subproblems:\n            s_source = self.local_scenarios\n        else:\n            s_source = self.local_subproblems\n            \n        if compute_val_at_nonant:\n            self.objs_dict={}\n        \n        for k,s in s_source.items():\n            if tee:\n                print(f\"Tee solve for {k} on global rank {self.global_rank}\")\n            logger.debug(\"  in loop solve_loop k={}, rank={}\".format(k, self.cylinder_rank))\n\n            pyomo_solve_time = self.solve_one(solver_options, k, s,\n                                              dtiming=dtiming,\n                                              verbose=verbose,\n                                              tee=tee,\n                                              gripe=gripe,\n                disable_pyomo_signal_handling=disable_pyomo_signal_handling,\n                compute_val_at_nonant=compute_val_at_nonant)\n\n        if dtiming:\n            all_pyomo_solve_times = self.mpicomm.gather(pyomo_solve_time, root=0)\n            if self.cylinder_rank == 0:\n                print(\"Pyomo solve times (seconds):\")\n                print(\"\\tmin=%4.2f mean=%4.2f max=%4.2f\" %\n                      (np.min(all_pyomo_solve_times),\n                      np.mean(all_pyomo_solve_times),\n                      np.max(all_pyomo_solve_times)))",
  "def Eobjective(self, verbose=False, fct=None):\n        \"\"\" Compute the expected value of the composition of the objective function \n            and a given function fct across all scenarios.\n\n        Note: \n            Assumes the optimization is done beforehand,\n            therefore DOES NOT CHECK FEASIBILITY or NON-ANTICIPATIVITY!\n            This method uses whatever the current value of the objective\n            function is.\n\n        Args:\n            verbose (boolean, optional):\n                If True, displays verbose output. Default False.\n            \n            fct (function, optional):\n                A function R-->R^p, such as x|-->(x,x^2,x^3). Default is None\n                If fct is None, Eobjective returns the exepected value.\n                \n\n        Returns:\n            float or numpy.array:\n                The expected objective function value. \n                If fct is R-->R, returns a float.\n                If fct is R-->R^p with p>1, returns a np.array of length p\n        \"\"\"\n        self._lazy_create_solvers()\n        if fct is None:\n            return super().Eobjective(verbose=verbose)\n        \n        if not hasattr(self, \"objs_dict\"):\n            raise RuntimeError(\"Values of the objective functions for each scenario\"+\n                               \" at xhat have to be computed before running Eobjective\")\n        \n\n        local_Eobjs = []\n        for k,s in self.local_scenarios.items():\n            if not k in self.objs_dict:\n                raise RuntimeError(f\"No value has been calculated for the scenario {k}\")\n            local_Eobjs.append(s._mpisppy_probability * fct(self.objs_dict[k]))\n        local_Eobjs = np.array(local_Eobjs)\n        local_Eobj = np.array([np.sum(local_Eobjs,axis=0)])\n        global_Eobj = np.zeros(len(local_Eobj))\n        self.mpicomm.Allreduce(local_Eobj, global_Eobj, op=MPI.SUM)\n        if len(global_Eobj)==1:\n            global_Eobj = global_Eobj[0]\n        return global_Eobj",
  "def evaluate_one(self, nonant_cache,scenario_name,s):\n        \"\"\" Evaluate xhat for one scenario.\n\n        Args:\n            nonant_cache(numpy vector): special numpy vector with nonant values (see spopt)\n            scenario_name(str): TODO\n        \n\n        Returns:\n            Eobj (float or None): Expected value (or None if infeasible)\n\n        \"\"\"\n        self._lazy_create_solvers()\n        self._fix_nonants(nonant_cache)\n        if not hasattr(self, \"objs_dict\"):\n            self.objs_dict = {}\n        \n\n        solver_options = self.options[\"solver_options\"] if \"solver_options\" in self.options else None\n        k = scenario_name\n        pyomo_solve_time = self.solve_one(solver_options,k, s,\n                                          dtiming=False,\n                                          verbose=self.verbose,\n                                          tee=False,\n                                          gripe=True,\n                                          compute_val_at_nonant=True\n                                          )\n        \n        obj = self.objs_dict[k]\n        \n        return obj",
  "def evaluate(self, nonant_cache, fct=None):\n        \"\"\" Do the optimization and compute the expected value of the composition of the objective function \n            and a given function fct across all scenarios.\n\n        Args:\n            nonant_cache(ndn dict of numpy vector): special numpy vector with nonant values (see spopt)\n            fct (function, optional):\n                A function R-->R^p, such as x|-->(x,x^2,x^3). Default is None\n                If fct is None, evaluate returns the exepected value.\n\n        Returns:\n            Eobj (float or numpy.array): Expected value\n\n        \"\"\"\n        self._lazy_create_solvers()\n        self._fix_nonants(nonant_cache)\n\n        solver_options = self.options[\"solver_options\"] if \"solver_options\" in self.options else None\n        \n        self.solve_loop(solver_options=solver_options,\n                        use_scenarios_not_subproblems=True,\n                        gripe=True, \n                        tee=False,\n                        verbose=self.verbose,\n                        compute_val_at_nonant=True\n                        )\n        \n        Eobj = self.Eobjective(self.verbose,fct=fct)\n        \n        return Eobj",
  "def fix_nonants_upto_stage(self,t,cache):\n        \"\"\" Fix the Vars subject to non-anticipativity at given values for stages 1 to t.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n        Args:\n            cache (ndn dict of list or numpy vector): values at which to fix\n        WARNING: \n            We are counting on Pyomo indices not to change order between\n            when the cache_list is created and used.\n        NOTE:\n            You probably want to call _save_nonants right before calling this\n        \"\"\"\n        self._lazy_create_solvers()\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if (sputils.is_persistent(s._solver_plugin)):\n                persistent_solver = s._solver_plugin\n\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                if node.stage<=t:\n                    ndn = node.name\n                    if ndn not in cache:\n                        raise RuntimeError(\"Could not find {} in {}\"\\\n                                           .format(ndn, cache))\n                    if cache[ndn] is None:\n                        raise RuntimeError(\"Empty cache for scen={}, node={}\".format(k, ndn))\n                    if len(cache[ndn]) != nlens[ndn]:\n                        raise RuntimeError(\"Needed {} nonant Vars for {}, got {}\"\\\n                                           .format(nlens[ndn], ndn, len(cache[ndn])))\n                    for i in range(nlens[ndn]): \n                        this_vardata = node.nonant_vardata_list[i]\n                        this_vardata._value = cache[ndn][i]\n                        this_vardata.fix()\n                        if persistent_solver is not None:\n                            persistent_solver.update_var(this_vardata)",
  "def _fix_nonants_at_value(self):\n        \"\"\" Fix the Vars subject to non-anticipativity at their current values.\n            Loop over the scenarios to restore, but loop over subproblems\n            to alert persistent solvers.\n        \"\"\"\n        for k,s in self.local_scenarios.items():\n\n            persistent_solver = None\n            if not self.bundling:\n                if (sputils.is_persistent(s._solver_plugin)):\n                    persistent_solver = s._solver_plugin\n\n            for var in s._mpisppy_data.nonant_indices.values():\n                var.fix()\n                if not self.bundling and persistent_solver is not None:\n                    persistent_solver.update_var(var)\n\n        if self.bundling:  # we might need to update persistent solvers\n            rank_local = self.cylinder_rank\n            for k,s in self.local_subproblems.items():\n                if (sputils.is_persistent(s._solver_plugin)):\n                    persistent_solver = s._solver_plugin\n                else:\n                    break  # all solvers should be the same\n\n                # the bundle number is the last number in the name\n                bunnum = sputils.extract_num(k)\n                # for the scenarios in this bundle, update Vars\n                for sname, scen in self.local_scenarios.items():\n                    if sname not in self.names_in_bundles[rank_local][bunnum]:\n                        break\n                    for var in scen._mpisppy_data.nonant_indices.values():\n                        persistent_solver.update_var(var)",
  "def calculate_incumbent(self, fix_nonants=True, verbose=False):\n        \"\"\"\n        Calculates the current incumbent\n\n        Args:\n            solver_options (dict): passed through to the solver\n            verbose (boolean): controls debugging output\n        Returns:\n            xhatobjective (float or None): the objective function\n                or None if one could not be obtained.\n        \"\"\"\n        self._lazy_create_solvers()\n\n        if fix_nonants:\n            self._fix_nonants_at_value()\n\n        self.solve_loop(solver_options=self.current_solver_options, \n                        verbose=verbose)\n\n        infeasP = self.infeas_prob()\n        if infeasP != 0.:\n            return None\n        else:\n            if verbose and self.cylinder_rank == 0:\n                print(\"  Feasible xhat found\")\n            return self.Eobjective(verbose=verbose)",
  "def _vb(msg): \n            if verbose and self.cylinder_rank == 0:\n                print (\"(rank0) \" + msg)",
  "class WXBarWriter(mpisppy.extensions.extension.Extension):\n    \"\"\" Extension class for writing the W values\n    \"\"\"\n    def __init__(self, ph):\n        # Check a bunch of files\n        w_fname, w_grad_fname, x_fname, sep_files = None, None, None, False\n\n        if ('W_fname' in ph.options): # W_fname is a path if separate_W_files=True\n            w_fname = ph.options['W_fname']\n        if ('Xbar_fname' in ph.options):\n            x_fname = ph.options['Xbar_fname']\n        if ('separate_W_files' in ph.options):\n            sep_files = ph.options['separate_W_files']\n\n        if (x_fname is None and w_fname is None and rank==0):\n            print('Warning: no output files provided to WXBarWriter. '\n                  'No values will be saved.')\n\n        if (w_fname and (not sep_files) and os.path.exists(w_fname) and rank==0):\n            print('Warning: specified W_fname ({fn})'.format(fn=w_fname) +\n                  ' already exists. Results will be appended to this file.')\n        elif (w_fname and sep_files and (not os.path.exists(w_fname)) and rank==0):\n            print('Warning: path {path} does not exist. Creating...'.format(\n                    path=w_fname))\n            os.makedirs(w_fname, exist_ok=True)\n\n        if (x_fname and os.path.exists(x_fname) and rank==0):\n            print('Warning: specified Xbar_fname ({fn})'.format(fn=x_fname) +\n                  ' already exists. Results will be appended to this file.')\n\n        self.PHB = ph\n        self.cylinder_rank = rank\n        self.w_fname = w_fname\n        self.w_grad_fname = w_grad_fname\n        self.x_fname = x_fname\n        self.sep_files = sep_files # Write separate files for each \n                                   # scenario's dual weights\n\n    def pre_iter0(self):\n        pass\n\n    def post_iter0(self):\n        pass\n        \n    def miditer(self):\n        pass\n\n    def enditer(self):\n       \"\"\" if (self.w_fname):\n            fname = f'fname{self.PHB._PHIter}.csv'\n            mpisppy.utils.wxbarutils.write_W_to_file(self.PHB, w_fname,\n                sep_files=self.sep_files)\"\"\"\n       pass\n\n\n    def post_everything(self):\n        if (self.w_fname):\n            fname = self.w_fname\n            mpisppy.utils.wxbarutils.write_W_to_file(self.PHB, fname,\n                sep_files=self.sep_files)\n        if (self.w_grad_fname):\n            grad_fname = f'grad_fname.csv'\n            #mpisppy.utils.wxbarutils.write_W_grad_to_file(self.PHB, grad_fname,\n            #sep_files=self.sep_files)\n        if (self.x_fname):\n            mpisppy.utils.wxbarutils.write_xbar_to_file(self.PHB, self.x_fname)",
  "def __init__(self, ph):\n        # Check a bunch of files\n        w_fname, w_grad_fname, x_fname, sep_files = None, None, None, False\n\n        if ('W_fname' in ph.options): # W_fname is a path if separate_W_files=True\n            w_fname = ph.options['W_fname']\n        if ('Xbar_fname' in ph.options):\n            x_fname = ph.options['Xbar_fname']\n        if ('separate_W_files' in ph.options):\n            sep_files = ph.options['separate_W_files']\n\n        if (x_fname is None and w_fname is None and rank==0):\n            print('Warning: no output files provided to WXBarWriter. '\n                  'No values will be saved.')\n\n        if (w_fname and (not sep_files) and os.path.exists(w_fname) and rank==0):\n            print('Warning: specified W_fname ({fn})'.format(fn=w_fname) +\n                  ' already exists. Results will be appended to this file.')\n        elif (w_fname and sep_files and (not os.path.exists(w_fname)) and rank==0):\n            print('Warning: path {path} does not exist. Creating...'.format(\n                    path=w_fname))\n            os.makedirs(w_fname, exist_ok=True)\n\n        if (x_fname and os.path.exists(x_fname) and rank==0):\n            print('Warning: specified Xbar_fname ({fn})'.format(fn=x_fname) +\n                  ' already exists. Results will be appended to this file.')\n\n        self.PHB = ph\n        self.cylinder_rank = rank\n        self.w_fname = w_fname\n        self.w_grad_fname = w_grad_fname\n        self.x_fname = x_fname\n        self.sep_files = sep_files",
  "def pre_iter0(self):\n        pass",
  "def post_iter0(self):\n        pass",
  "def miditer(self):\n        pass",
  "def enditer(self):\n       \"\"\" if (self.w_fname):\n            fname = f'fname{self.PHB._PHIter}.csv'\n            mpisppy.utils.wxbarutils.write_W_to_file(self.PHB, w_fname,\n                sep_files=self.sep_files)\"\"\"\n       pass",
  "def post_everything(self):\n        if (self.w_fname):\n            fname = self.w_fname\n            mpisppy.utils.wxbarutils.write_W_to_file(self.PHB, fname,\n                sep_files=self.sep_files)\n        if (self.w_grad_fname):\n            grad_fname = f'grad_fname.csv'\n            #mpisppy.utils.wxbarutils.write_W_grad_to_file(self.PHB, grad_fname,\n            #sep_files=self.sep_files)\n        if (self.x_fname):\n            mpisppy.utils.wxbarutils.write_xbar_to_file(self.PHB, self.x_fname)",
  "class Find_Rho():\n    \"\"\" Interface to compute rhos from Ws for a given ph object and write them in a file\n\n    Args:\n       ph_object (PHBase): ph object\n       cfg (Config): config object\n\n    Attributes:\n       c (dict): a dictionnary {(scenario name, nonant indice): c value}\n       corresponding to the cost vector in the PH algorithm\n\n    \"\"\"\n\n    def __init__(self, ph_object, cfg):\n        self.ph_object = ph_object\n        self.cfg = cfg\n        self.c = dict()\n\n        if cfg.rho_file == '' and cfg.grad_rho_file == '':\n            pass\n        else:\n            assert self.cfg.whatpath != '', \"to compute rhos you have to give the name of a What csv file (using --whatpath)\"\n            if (not os.path.exists(self.cfg.whatpath)):\n                raise RuntimeError('Could not find file {fn}'.format(fn=self.cfg.whatpath))\n            with open(self.cfg.whatpath, 'r') as f:\n                for line in f:\n                    if (line.startswith('#')):\n                        continue\n                    line  = line.split(',')\n                    cval = float(line[2][:-2])\n                    self.c[(line[0], line[1])] = cval\n\n\n    def _w_denom(self, s, node):\n        \"\"\" Computes the denominator for w-based rho. This denominator is scenario dependant.\n\n        Args:\n           s (Pyomo Concrete Model): scenario\n           node: only ROOT for now\n\n        Returns:\n           w_denom (numpy array): denominator\n\n        \"\"\"\n        assert node.name == \"ROOT\", \"compute rho only works for two stage for now\"\n        nlen = s._mpisppy_data.nlens[node.name]\n        xbar_array = np.array([s._mpisppy_model.xbars[(node.name,j)]._value for j in range(nlen)])\n        nonants_array = np.fromiter((v._value for v in node.nonant_vardata_list),\n                                    dtype='d', count=nlen)\n        w_denom = np.abs(nonants_array - xbar_array)\n        return w_denom\n\n\n    def _prox_denom(self, s, node):\n        \"\"\" Computes the denominator corresponding to the proximal term. This denominator is scenario dependant.\n\n        Args:\n           s (Pyomo Concrete Model): scenario\n           node: only ROOT for now\n\n        Returns:\n           w_denom (numpy array): denominator\n\n        \"\"\"\n        assert node.name == \"ROOT\", \"compute rho only works for two stage for now\"\n        nlen = s._mpisppy_data.nlens[node.name]\n        xbar_array = np.array([s._mpisppy_model.xbars[(node.name,j)]._value for j in range(nlen)])\n        nonants_array = np.fromiter((v._value for v in node.nonant_vardata_list),\n                                    dtype='d', count=nlen)\n        prox_denom = 2 * np.square(nonants_array - xbar_array)\n        return prox_denom\n\n\n    def _grad_denom(self):\n        \"\"\"Computes the scenario independant denominator in the WW heuristic.\n\n        Returns:\n           g_denom (numpy array): denominator\n\n        \"\"\"\n        phbase._Compute_Xbar(self.ph_object)\n        sname, scenario = list(self.ph_object.local_scenarios.items())[0]\n        for node in scenario._mpisppy_node_list:\n            assert node.name == \"ROOT\", \"compute rho only works for two stage for now\"\n        nlen0 = scenario._mpisppy_data.nlens[\"ROOT\"]\n        g_denom = np.zeros(nlen0, dtype='d')\n        xbar_array = np.array([scenario._mpisppy_model.xbars[(\"ROOT\",j)]._value for j in range(nlen0)])\n        denom = 0\n        for k,s in self.ph_object.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                nlen = nlens[ndn]\n                nonants_array = np.fromiter((v._value for v in node.nonant_vardata_list),\n                                            dtype='d', count=nlen)\n                probs = s._mpisppy_data.prob_coeff[ndn] * np.ones(nlen)\n                denom += probs * np.abs(nonants_array - xbar_array)\n        self.ph_object.comms[\"ROOT\"].Allreduce([denom, MPI.DOUBLE],\n                                               [g_denom, MPI.DOUBLE],\n                                               op=MPI.SUM)\n        if self.ph_object.cylinder_rank == 0:\n            g_denom = np.maximum(np.ones(len(g_denom))/self.cfg.rho_relative_bound, g_denom)\n            return g_denom\n\n\n    def _order_stat(self, rho_list):\n        \"\"\" Computes a scenario independant rho from a list of rhos.\n\n        Args:\n           rho_list (list): list of rhos\n\n        Returns:\n           rho (float): rho value\n        \"\"\"\n        alpha = self.cfg.order_stat\n        assert alpha != -1.0, \"you need to set the order statistic parameter for rho using --order-stat\"\n        assert (alpha >= 0 and alpha <= 1), \"0 is the min, 0.5 the average, 1 the max\"\n        rho_mean, rho_min, rho_max = np.mean(rho_list), np.min(rho_list), np.max(rho_list)\n        if alpha == 0.5:\n            return rho_mean\n        if alpha < 0.5:\n            return (rho_min + alpha * 2 * (rho_mean - rho_min))\n        if alpha > 0.5:\n            return (2 * rho_mean - rho_max) + alpha * 2 * (rho_max - rho_mean)\n\n    def compute_rho(self, indep_denom = False):\n        \"\"\" Computes rhos for each scenario and each variable using the WW heuristic.\n\n        Returns:\n           arranged_rho (dict): dict {variable name: list of rhos for this variable}\n        \"\"\"\n        all_vnames, all_snames = [], []\n        for (sname, vname) in self.c.keys():\n            if sname not in all_snames: all_snames.append(sname)\n            if vname not in all_vnames:all_vnames.append(vname)\n        k0, s0 = list(self.ph_object.local_scenarios.items())[0]\n        vname_to_idx = {var.name : ndn_i[1] for ndn_i, var in s0._mpisppy_data.nonant_indices.items()}\n        cost = {k : np.array([self.c[k, vname]\n                for vname in all_vnames])\n                for k in all_snames}\n        if indep_denom:\n            grad_denom = self._grad_denom()\n            denom = {k: grad_denom for k in all_snames}\n        else:\n            loc_denom = {k: np.max((self._w_denom(s, node), self._prox_denom(s, node)))\n                           for k, s in self.ph_object.local_scenarios.items()\n                           for node in s._mpisppy_node_list}\n            global_denom = self.ph_object.comms['ROOT'].gather(loc_denom, root=0)\n            denom = dict()\n            if self.ph_object.cylinder_rank == 0:\n                for loc_denom in global_denom:\n                    denom.update(loc_denom)\n        if self.ph_object.cylinder_rank == 0:\n            rho = dict()\n            for k in all_snames:\n                rho[k] = np.abs(np.divide(cost[k], denom[k]))\n            arranged_rho = {vname: [rho_list[idx] for _, rho_list in rho.items()]\n                            for vname, idx in vname_to_idx.items()}\n            rho = {vname: self._order_stat(rho_list) for (vname, rho_list) in arranged_rho.items()}\n            return rho\n\n\n    def write_rho(self):\n        \"\"\" Write the computed rhos in the file --rho-file.\n        \"\"\"\n        if self.cfg.rho_file == '': pass\n        else:\n            rho_data = self.compute_rho()\n            if self.ph_object.cylinder_rank == 0:\n                with open(self.cfg.rho_file, 'w') as file:\n                    writer = csv.writer(file)\n                    writer.writerow(['#Rho values'])\n                    for (vname, rho) in rho_data.items():\n                        writer.writerow([vname, rho])",
  "class Set_Rho():\n    \"\"\" Interface to set the computed rhos in PH.\n\n    Args:\n       cfg (Config): config object\n\n    \"\"\"\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n    def rho_setter(self, scenario):\n        \"\"\" rho setter to be used in the PH algorithm\n\n        Args:\n        scenario (Pyomo Concrete Model): scenario\n        cfg (Config object): config object\n\n        Returns:\n        rho_list (list): list of (id(variable), rho)\n\n        \"\"\"\n        assert self.cfg != None, \"you have to give the rho_setter a cfg\"\n        assert self.cfg.rho_path != '', \"use --rho-path to give the path of your rhos file\"\n        rhofile = self.cfg.rho_path\n        rho_list = list()\n        with open(rhofile) as infile:\n            reader = csv.reader(infile)\n            for row in reader:\n                if (row[0].startswith('#')):\n                    continue\n                else:\n                    fullname = row[0]\n                    vo = scenario.find_component(fullname)\n                    if vo is not None:\n                        rho_list.append((id(vo), float(row[1])))\n                    else:\n                        raise RuntimeError(f\"rho values from {filename} found Var {fullname} \"\n                                           f\"that is not found in the scenario given (name={s._name})\")\n        return rho_list",
  "def _parser_setup():\n    \"\"\" Set up config object and return it, but don't parse\n\n    Returns:\n       cfg (Config): config object\n\n    Notes:\n       parsers for the non-model-specific arguments; but the model_module_name will be pulled off first\n\n    \"\"\"\n    cfg = config.Config()\n    cfg.add_branching_factors()\n    cfg.num_scens_required()\n    cfg.popular_args()\n    cfg.two_sided_args()\n    cfg.ph_args()\n    cfg.rho_args()\n\n    return cfg",
  "def get_rho_from_W(mname, original_cfg):\n    \"\"\" Creates a ph object from cfg and using the module functions. Then computes rhos from the Ws.\n\n    Args:\n       mname (str): module name\n       original_cfg (Config object): config object\n\n    \"\"\"\n    if  (original_cfg.rho_file == ''): return\n\n    try:\n        model_module = importlib.import_module(mname)\n    except:\n        raise RuntimeError(f\"Could not import module: {mname}\")\n    cfg = copy.deepcopy(original_cfg)\n    cfg.max_iterations = 0 #we only need x0 here\n\n    #create ph_object via vanilla\n    scenario_creator = model_module.scenario_creator\n    scenario_denouement = model_module.scenario_denouement\n    scen_names_creator_args = inspect.getargspec(model_module.scenario_names_creator).args #partition requires to do that\n    if scen_names_creator_args[0] == 'cfg':\n        all_scenario_names = model_module.scenario_names_creator(cfg)\n    else :\n        all_scenario_names = model_module.scenario_names_creator(cfg.num_scens)\n    scenario_creator_kwargs = model_module.kw_creator(cfg)\n    variable_probability = None\n    if hasattr(model_module, '_variable_probability'):\n        variable_probability = model_module._variable_probability\n    beans = (cfg, scenario_creator, scenario_denouement, all_scenario_names)\n    hub_dict = vanilla.ph_hub(*beans,\n                              scenario_creator_kwargs=scenario_creator_kwargs,\n                              ph_extensions=WXBarWriter,\n                              variable_probability=variable_probability)\n    list_of_spoke_dict = list()\n    wheel = WheelSpinner(hub_dict, list_of_spoke_dict)\n    wheel.spin() #TODO: steal only what's needed in  WheelSpinner\n    if wheel.strata_rank == 0:  # don't do this for bound ranks\n        ph_object = wheel.spcomm.opt\n\n    #==============================================================================\n    # Compute rhos\n    Find_Rho(ph_object, cfg).rhos()",
  "def __init__(self, ph_object, cfg):\n        self.ph_object = ph_object\n        self.cfg = cfg\n        self.c = dict()\n\n        if cfg.rho_file == '' and cfg.grad_rho_file == '':\n            pass\n        else:\n            assert self.cfg.whatpath != '', \"to compute rhos you have to give the name of a What csv file (using --whatpath)\"\n            if (not os.path.exists(self.cfg.whatpath)):\n                raise RuntimeError('Could not find file {fn}'.format(fn=self.cfg.whatpath))\n            with open(self.cfg.whatpath, 'r') as f:\n                for line in f:\n                    if (line.startswith('#')):\n                        continue\n                    line  = line.split(',')\n                    cval = float(line[2][:-2])\n                    self.c[(line[0], line[1])] = cval",
  "def _w_denom(self, s, node):\n        \"\"\" Computes the denominator for w-based rho. This denominator is scenario dependant.\n\n        Args:\n           s (Pyomo Concrete Model): scenario\n           node: only ROOT for now\n\n        Returns:\n           w_denom (numpy array): denominator\n\n        \"\"\"\n        assert node.name == \"ROOT\", \"compute rho only works for two stage for now\"\n        nlen = s._mpisppy_data.nlens[node.name]\n        xbar_array = np.array([s._mpisppy_model.xbars[(node.name,j)]._value for j in range(nlen)])\n        nonants_array = np.fromiter((v._value for v in node.nonant_vardata_list),\n                                    dtype='d', count=nlen)\n        w_denom = np.abs(nonants_array - xbar_array)\n        return w_denom",
  "def _prox_denom(self, s, node):\n        \"\"\" Computes the denominator corresponding to the proximal term. This denominator is scenario dependant.\n\n        Args:\n           s (Pyomo Concrete Model): scenario\n           node: only ROOT for now\n\n        Returns:\n           w_denom (numpy array): denominator\n\n        \"\"\"\n        assert node.name == \"ROOT\", \"compute rho only works for two stage for now\"\n        nlen = s._mpisppy_data.nlens[node.name]\n        xbar_array = np.array([s._mpisppy_model.xbars[(node.name,j)]._value for j in range(nlen)])\n        nonants_array = np.fromiter((v._value for v in node.nonant_vardata_list),\n                                    dtype='d', count=nlen)\n        prox_denom = 2 * np.square(nonants_array - xbar_array)\n        return prox_denom",
  "def _grad_denom(self):\n        \"\"\"Computes the scenario independant denominator in the WW heuristic.\n\n        Returns:\n           g_denom (numpy array): denominator\n\n        \"\"\"\n        phbase._Compute_Xbar(self.ph_object)\n        sname, scenario = list(self.ph_object.local_scenarios.items())[0]\n        for node in scenario._mpisppy_node_list:\n            assert node.name == \"ROOT\", \"compute rho only works for two stage for now\"\n        nlen0 = scenario._mpisppy_data.nlens[\"ROOT\"]\n        g_denom = np.zeros(nlen0, dtype='d')\n        xbar_array = np.array([scenario._mpisppy_model.xbars[(\"ROOT\",j)]._value for j in range(nlen0)])\n        denom = 0\n        for k,s in self.ph_object.local_scenarios.items():\n            nlens = s._mpisppy_data.nlens\n            for node in s._mpisppy_node_list:\n                ndn = node.name\n                nlen = nlens[ndn]\n                nonants_array = np.fromiter((v._value for v in node.nonant_vardata_list),\n                                            dtype='d', count=nlen)\n                probs = s._mpisppy_data.prob_coeff[ndn] * np.ones(nlen)\n                denom += probs * np.abs(nonants_array - xbar_array)\n        self.ph_object.comms[\"ROOT\"].Allreduce([denom, MPI.DOUBLE],\n                                               [g_denom, MPI.DOUBLE],\n                                               op=MPI.SUM)\n        if self.ph_object.cylinder_rank == 0:\n            g_denom = np.maximum(np.ones(len(g_denom))/self.cfg.rho_relative_bound, g_denom)\n            return g_denom",
  "def _order_stat(self, rho_list):\n        \"\"\" Computes a scenario independant rho from a list of rhos.\n\n        Args:\n           rho_list (list): list of rhos\n\n        Returns:\n           rho (float): rho value\n        \"\"\"\n        alpha = self.cfg.order_stat\n        assert alpha != -1.0, \"you need to set the order statistic parameter for rho using --order-stat\"\n        assert (alpha >= 0 and alpha <= 1), \"0 is the min, 0.5 the average, 1 the max\"\n        rho_mean, rho_min, rho_max = np.mean(rho_list), np.min(rho_list), np.max(rho_list)\n        if alpha == 0.5:\n            return rho_mean\n        if alpha < 0.5:\n            return (rho_min + alpha * 2 * (rho_mean - rho_min))\n        if alpha > 0.5:\n            return (2 * rho_mean - rho_max) + alpha * 2 * (rho_max - rho_mean)",
  "def compute_rho(self, indep_denom = False):\n        \"\"\" Computes rhos for each scenario and each variable using the WW heuristic.\n\n        Returns:\n           arranged_rho (dict): dict {variable name: list of rhos for this variable}\n        \"\"\"\n        all_vnames, all_snames = [], []\n        for (sname, vname) in self.c.keys():\n            if sname not in all_snames: all_snames.append(sname)\n            if vname not in all_vnames:all_vnames.append(vname)\n        k0, s0 = list(self.ph_object.local_scenarios.items())[0]\n        vname_to_idx = {var.name : ndn_i[1] for ndn_i, var in s0._mpisppy_data.nonant_indices.items()}\n        cost = {k : np.array([self.c[k, vname]\n                for vname in all_vnames])\n                for k in all_snames}\n        if indep_denom:\n            grad_denom = self._grad_denom()\n            denom = {k: grad_denom for k in all_snames}\n        else:\n            loc_denom = {k: np.max((self._w_denom(s, node), self._prox_denom(s, node)))\n                           for k, s in self.ph_object.local_scenarios.items()\n                           for node in s._mpisppy_node_list}\n            global_denom = self.ph_object.comms['ROOT'].gather(loc_denom, root=0)\n            denom = dict()\n            if self.ph_object.cylinder_rank == 0:\n                for loc_denom in global_denom:\n                    denom.update(loc_denom)\n        if self.ph_object.cylinder_rank == 0:\n            rho = dict()\n            for k in all_snames:\n                rho[k] = np.abs(np.divide(cost[k], denom[k]))\n            arranged_rho = {vname: [rho_list[idx] for _, rho_list in rho.items()]\n                            for vname, idx in vname_to_idx.items()}\n            rho = {vname: self._order_stat(rho_list) for (vname, rho_list) in arranged_rho.items()}\n            return rho",
  "def write_rho(self):\n        \"\"\" Write the computed rhos in the file --rho-file.\n        \"\"\"\n        if self.cfg.rho_file == '': pass\n        else:\n            rho_data = self.compute_rho()\n            if self.ph_object.cylinder_rank == 0:\n                with open(self.cfg.rho_file, 'w') as file:\n                    writer = csv.writer(file)\n                    writer.writerow(['#Rho values'])\n                    for (vname, rho) in rho_data.items():\n                        writer.writerow([vname, rho])",
  "def __init__(self, cfg):\n        self.cfg = cfg",
  "def rho_setter(self, scenario):\n        \"\"\" rho setter to be used in the PH algorithm\n\n        Args:\n        scenario (Pyomo Concrete Model): scenario\n        cfg (Config object): config object\n\n        Returns:\n        rho_list (list): list of (id(variable), rho)\n\n        \"\"\"\n        assert self.cfg != None, \"you have to give the rho_setter a cfg\"\n        assert self.cfg.rho_path != '', \"use --rho-path to give the path of your rhos file\"\n        rhofile = self.cfg.rho_path\n        rho_list = list()\n        with open(rhofile) as infile:\n            reader = csv.reader(infile)\n            for row in reader:\n                if (row[0].startswith('#')):\n                    continue\n                else:\n                    fullname = row[0]\n                    vo = scenario.find_component(fullname)\n                    if vo is not None:\n                        rho_list.append((id(vo), float(row[1])))\n                    else:\n                        raise RuntimeError(f\"rho values from {filename} found Var {fullname} \"\n                                           f\"that is not found in the scenario given (name={s._name})\")\n        return rho_list",
  "class WXBarReader(mpisppy.extensions.extension.Extension):\n    \"\"\" Extension class for reading W values\n    \"\"\"\n    def __init__(self, ph):\n\n        ''' Do a bunch of checking if files exist '''\n        w_fname, x_fname, sep_files = None, None, False\n        if ('init_separate_W_files' in ph.options):\n            sep_files = ph.options['init_separate_W_files']\n\n        if ('init_W_fname' in ph.options):\n            w_fname = ph.options['init_W_fname']\n            if (not os.path.exists(w_fname)):\n                if (rank == 0):\n                    if (sep_files):\n                        print('Cannot find path', w_fname)\n                    else:\n                        print('Cannot find file', w_fname)\n                quit()\n\n        if ('init_Xbar_fname' in ph.options):\n            x_fname = ph.options['init_Xbar_fname']\n            if (not os.path.exists(x_fname)):\n                if (rank == 0):\n                    print('Cannot find file', x_fname)\n                quit()\n\n        if (x_fname is None and w_fname is None and rank==0):\n            print('Warning: no input files provided to WXBarReader. '\n                  'W and Xbar will be initialized to their default values.')\n\n        self.PHB = ph\n        self.cylinder_rank = rank\n        self.w_fname = w_fname\n        self.x_fname = x_fname\n        self.sep_files = sep_files\n\n    def pre_iter0(self):\n        pass\n\n\n    def post_iter0(self):\n        pass\n\n    def miditer(self):\n        ''' Called before the solveloop is called '''\n        if self.PHB._PHIter == 1:\n            if self.w_fname:\n                mpisppy.utils.wxbarutils.set_W_from_file(\n                        self.w_fname, self.PHB, self.cylinder_rank,\n                        sep_files=self.sep_files)\n                self.PHB._reenable_W() # This makes a big difference.\n            if self.x_fname:\n                mpisppy.utils.wxbarutils.set_xbar_from_file(self.x_fname, self.PHB)\n                self.PHB._reenable_prox()\n\n    def enditer(self):\n        ''' Called after the solve loop '''\n        pass\n\n    def post_everything(self):\n        pass",
  "def __init__(self, ph):\n\n        ''' Do a bunch of checking if files exist '''\n        w_fname, x_fname, sep_files = None, None, False\n        if ('init_separate_W_files' in ph.options):\n            sep_files = ph.options['init_separate_W_files']\n\n        if ('init_W_fname' in ph.options):\n            w_fname = ph.options['init_W_fname']\n            if (not os.path.exists(w_fname)):\n                if (rank == 0):\n                    if (sep_files):\n                        print('Cannot find path', w_fname)\n                    else:\n                        print('Cannot find file', w_fname)\n                quit()\n\n        if ('init_Xbar_fname' in ph.options):\n            x_fname = ph.options['init_Xbar_fname']\n            if (not os.path.exists(x_fname)):\n                if (rank == 0):\n                    print('Cannot find file', x_fname)\n                quit()\n\n        if (x_fname is None and w_fname is None and rank==0):\n            print('Warning: no input files provided to WXBarReader. '\n                  'W and Xbar will be initialized to their default values.')\n\n        self.PHB = ph\n        self.cylinder_rank = rank\n        self.w_fname = w_fname\n        self.x_fname = x_fname\n        self.sep_files = sep_files",
  "def pre_iter0(self):\n        pass",
  "def post_iter0(self):\n        pass",
  "def miditer(self):\n        ''' Called before the solveloop is called '''\n        if self.PHB._PHIter == 1:\n            if self.w_fname:\n                mpisppy.utils.wxbarutils.set_W_from_file(\n                        self.w_fname, self.PHB, self.cylinder_rank,\n                        sep_files=self.sep_files)\n                self.PHB._reenable_W() # This makes a big difference.\n            if self.x_fname:\n                mpisppy.utils.wxbarutils.set_xbar_from_file(self.x_fname, self.PHB)\n                self.PHB._reenable_prox()",
  "def enditer(self):\n        ''' Called after the solve loop '''\n        pass",
  "def post_everything(self):\n        pass",
  "class WTracker():\n    \"\"\"\n    Args:\n        PHB (PHBase): the object that has all the scenarios and _PHIter\n    Notes:\n    - A utility to track W and compute interesting statistics and/or log the w values\n    - We are going to take a comm as input so we can do a gather if we want to,\n    so be careful if you use this with APH (you might want to call it from a lagrangian spoke)\n    \n    \"\"\"\n\n    \n    def __init__(self, PHB):\n        self.local_Ws = dict()  #  [iteration][sname](list of W vals)\n        self.PHB = PHB\n        # get the nonant variable names, bound by index to W vals\n        arbitrary_scen = PHB.local_scenarios[list(PHB.local_scenarios.keys())[0]]\n        self.varnames = [var.name for node in arbitrary_scen._mpisppy_node_list\n                         for (ix, var) in enumerate(node.nonant_vardata_list)]\n\n        \n    def grab_local_Ws(self):\n        \"\"\" Get the W values from the PHB that we are following\n            NOTE: we assume this is called only once per iteration\n        \"\"\"\n\n        scenario_Ws = {sname: [w._value for w in scenario._mpisppy_model.W.values()]\n                       for (sname, scenario) in self.PHB.local_scenarios.items()}\n\n        self.local_Ws[self.PHB._PHIter] = scenario_Ws\n\n\n    def compute_moving_stats(self, wlen, offsetback=0):\n        \"\"\" Use self.local_Ws to compute moving mean and stdev\n        ASSUMES grab_local_Ws is called before this\n        Args:\n            wlen (int): desired window length\n            offsetback (int): how far back from the most recent observation to start\n        Returns:\n            window_stats (dict): (varname, scenname): [mean, stdev]\n                                  OR returns a warning string\n        NOTE: we sort of treat iterations as one-based\n        \"\"\"\n        cI = self.PHB._PHIter\n        li = cI - offsetback\n        fi = max(1, li - wlen)\n        if li - fi < wlen:\n            return (f\"WARNING: Not enough iterations ({cI}) for window len {wlen} and\"\n                   f\" offsetback {offsetback}\\n\")\n        else:\n            window_stats = dict()\n            for idx, varname in enumerate(self.varnames):\n                for sname in self.PHB.local_scenario_names:\n                    wlist = [self.local_Ws[i][sname][idx] for i in range(fi, li+1)]\n                    window_stats[(varname, sname)] = (np.mean(wlist), np.std(wlist))\n            return window_stats\n\n\n    def report_by_moving_stats(self, wlen, reportlen=None, stdevthresh=None, file_prefix=''):\n        \"\"\" Compute window_stats then sort by \"badness\" to write to three files\n        ASSUMES grab_local_Ws is called before this\n        Args:\n            wlen (int): desired window length\n            reportlen (int): max rows in each report\n            stdevthresh (float): threshold for a good enough std dev\n        NOTE:\n            For large problems, this will create a lot of garbage for the collector\n        \"\"\"\n        fname = f\"{file_prefix}_summary_iter{self.PHB._PHIter}_rank{self.PHB.global_rank}.txt\"\n        stname = f\"{file_prefix}_stdev_iter{self.PHB._PHIter}_rank{self.PHB.global_rank}.csv\"\n        cvname = f\"{file_prefix}_cv_iter{self.PHB._PHIter}_rank{self.PHB.global_rank}.csv\"\n        if self.PHB.cylinder_rank == 0:\n            print(f\"Writing (a) W tracker report(s) to files with names like {fname}, {stname}, and {cvname}\")\n        with open(fname, \"w\") as fil:\n            fil.write(f\"Moving Stats W Report at iteration {self.PHB._PHIter}\\n\")\n            fil.write(f\"    {len(self.varnames)} nonants\\n\"\n                      f\"    {len(self.PHB.local_scenario_names)} scenarios\\n\")\n            \n        total_traces = len(self.varnames) * len(self.PHB.local_scenario_names)\n        \n        wstats = self.compute_moving_stats(wlen)\n        if not isinstance(wstats, str):\n            Wsdf = pd.DataFrame.from_dict(wstats, orient='index',\n                                          columns=[\"mean\", \"stdev\"])\n\n            stt = stdevthresh if stdevthresh is not None else self.PHB.E1_tolerance\n\n            # unscaled\n            goodcnt = len(Wsdf[Wsdf[\"stdev\"] <= stt])\n            total_stdev = Wsdf[\"stdev\"].sum()\n\n            with open(fname, \"a\") as fil:\n                fil.write(f\" {goodcnt} of {total_traces} have windowed stdev (unscaled) below {stt}\\n\")\n                fil.write(f\" sum of stdev={total_stdev}\\n\")\n\n                fil.write(f\"Sorted by windowed stdev, row limit={reportlen}, window len={wlen} in {stname}\\n\")\n            by_stdev = Wsdf.sort_values(by=\"stdev\", ascending=False)\n            by_stdev[0:reportlen].to_csv(path_or_buf=stname, header=True, index=True, index_label=None, mode='w')\n\n            # scaled\n            Wsdf[\"absCV\"] = np.where(Wsdf[\"mean\"] != 0, Wsdf[\"stdev\"]/abs(Wsdf[\"mean\"]), np.nan)\n            goodcnt = len(Wsdf[Wsdf[\"absCV\"] <= stt])\n            mean_absCV = Wsdf[\"absCV\"].mean()\n            stdev_absCV = Wsdf[\"absCV\"].std()\n            zeroWcnt = len(Wsdf[Wsdf[\"mean\"] == 0])\n            with open(fname, \"a\") as fil:\n                fil.write(f\"  {goodcnt} of ({total_traces} less meanzero {zeroWcnt}) have windowed absCV below {stt}\\n\")\n                fil.write(f\"  mean absCV={mean_absCV}, stdev={stdev_absCV}\\n\")\n                fil.write(f\"  Sorted by windowed abs CV, row limit={reportlen}, window len={wlen} in {cvname}\\n\")\n            by_absCV = Wsdf.sort_values(by=\"absCV\", ascending=False)\n            by_absCV[0:reportlen].to_csv(path_or_buf=cvname, header=True, index=True, index_label=None, mode='w')\n        else:  # not enough data\n            with open(fname, \"a\") as fil:\n                fil.write(wstats)   # warning string\n\n\n    def W_diff(self):\n        \"\"\" Compute the norm of the difference between to consecutive Ws / num_scenarios.\n\n        Returns:\n           global_diff (float): difference between to consecutive Ws\n\n        \"\"\"\n        cI = self.PHB._PHIter\n        self.grab_local_Ws()\n        self.local_Ws[-1] = self.local_Ws[0]\n        global_diff = np.zeros(1)\n        local_diff = np.zeros(1)\n        varcount = 0\n        local_diff[0] = 0\n        for (sname, scenario) in self.PHB.local_scenarios.items():\n            local_wdiffs = [w - w1\n                            for w, w1 in zip(self.local_Ws[cI][sname], self.local_Ws[cI-1][sname])]\n            for wdiff in local_wdiffs:\n                local_diff[0] += abs(wdiff)\n                varcount += 1\n        local_diff[0] /= varcount\n        self.PHB.comms[\"ROOT\"].Allreduce(local_diff, global_diff, op=MPI.SUM)\n\n        return global_diff[0] / self.PHB.n_proc",
  "def __init__(self, PHB):\n        self.local_Ws = dict()  #  [iteration][sname](list of W vals)\n        self.PHB = PHB\n        # get the nonant variable names, bound by index to W vals\n        arbitrary_scen = PHB.local_scenarios[list(PHB.local_scenarios.keys())[0]]\n        self.varnames = [var.name for node in arbitrary_scen._mpisppy_node_list\n                         for (ix, var) in enumerate(node.nonant_vardata_list)]",
  "def grab_local_Ws(self):\n        \"\"\" Get the W values from the PHB that we are following\n            NOTE: we assume this is called only once per iteration\n        \"\"\"\n\n        scenario_Ws = {sname: [w._value for w in scenario._mpisppy_model.W.values()]\n                       for (sname, scenario) in self.PHB.local_scenarios.items()}\n\n        self.local_Ws[self.PHB._PHIter] = scenario_Ws",
  "def compute_moving_stats(self, wlen, offsetback=0):\n        \"\"\" Use self.local_Ws to compute moving mean and stdev\n        ASSUMES grab_local_Ws is called before this\n        Args:\n            wlen (int): desired window length\n            offsetback (int): how far back from the most recent observation to start\n        Returns:\n            window_stats (dict): (varname, scenname): [mean, stdev]\n                                  OR returns a warning string\n        NOTE: we sort of treat iterations as one-based\n        \"\"\"\n        cI = self.PHB._PHIter\n        li = cI - offsetback\n        fi = max(1, li - wlen)\n        if li - fi < wlen:\n            return (f\"WARNING: Not enough iterations ({cI}) for window len {wlen} and\"\n                   f\" offsetback {offsetback}\\n\")\n        else:\n            window_stats = dict()\n            for idx, varname in enumerate(self.varnames):\n                for sname in self.PHB.local_scenario_names:\n                    wlist = [self.local_Ws[i][sname][idx] for i in range(fi, li+1)]\n                    window_stats[(varname, sname)] = (np.mean(wlist), np.std(wlist))\n            return window_stats",
  "def report_by_moving_stats(self, wlen, reportlen=None, stdevthresh=None, file_prefix=''):\n        \"\"\" Compute window_stats then sort by \"badness\" to write to three files\n        ASSUMES grab_local_Ws is called before this\n        Args:\n            wlen (int): desired window length\n            reportlen (int): max rows in each report\n            stdevthresh (float): threshold for a good enough std dev\n        NOTE:\n            For large problems, this will create a lot of garbage for the collector\n        \"\"\"\n        fname = f\"{file_prefix}_summary_iter{self.PHB._PHIter}_rank{self.PHB.global_rank}.txt\"\n        stname = f\"{file_prefix}_stdev_iter{self.PHB._PHIter}_rank{self.PHB.global_rank}.csv\"\n        cvname = f\"{file_prefix}_cv_iter{self.PHB._PHIter}_rank{self.PHB.global_rank}.csv\"\n        if self.PHB.cylinder_rank == 0:\n            print(f\"Writing (a) W tracker report(s) to files with names like {fname}, {stname}, and {cvname}\")\n        with open(fname, \"w\") as fil:\n            fil.write(f\"Moving Stats W Report at iteration {self.PHB._PHIter}\\n\")\n            fil.write(f\"    {len(self.varnames)} nonants\\n\"\n                      f\"    {len(self.PHB.local_scenario_names)} scenarios\\n\")\n            \n        total_traces = len(self.varnames) * len(self.PHB.local_scenario_names)\n        \n        wstats = self.compute_moving_stats(wlen)\n        if not isinstance(wstats, str):\n            Wsdf = pd.DataFrame.from_dict(wstats, orient='index',\n                                          columns=[\"mean\", \"stdev\"])\n\n            stt = stdevthresh if stdevthresh is not None else self.PHB.E1_tolerance\n\n            # unscaled\n            goodcnt = len(Wsdf[Wsdf[\"stdev\"] <= stt])\n            total_stdev = Wsdf[\"stdev\"].sum()\n\n            with open(fname, \"a\") as fil:\n                fil.write(f\" {goodcnt} of {total_traces} have windowed stdev (unscaled) below {stt}\\n\")\n                fil.write(f\" sum of stdev={total_stdev}\\n\")\n\n                fil.write(f\"Sorted by windowed stdev, row limit={reportlen}, window len={wlen} in {stname}\\n\")\n            by_stdev = Wsdf.sort_values(by=\"stdev\", ascending=False)\n            by_stdev[0:reportlen].to_csv(path_or_buf=stname, header=True, index=True, index_label=None, mode='w')\n\n            # scaled\n            Wsdf[\"absCV\"] = np.where(Wsdf[\"mean\"] != 0, Wsdf[\"stdev\"]/abs(Wsdf[\"mean\"]), np.nan)\n            goodcnt = len(Wsdf[Wsdf[\"absCV\"] <= stt])\n            mean_absCV = Wsdf[\"absCV\"].mean()\n            stdev_absCV = Wsdf[\"absCV\"].std()\n            zeroWcnt = len(Wsdf[Wsdf[\"mean\"] == 0])\n            with open(fname, \"a\") as fil:\n                fil.write(f\"  {goodcnt} of ({total_traces} less meanzero {zeroWcnt}) have windowed absCV below {stt}\\n\")\n                fil.write(f\"  mean absCV={mean_absCV}, stdev={stdev_absCV}\\n\")\n                fil.write(f\"  Sorted by windowed abs CV, row limit={reportlen}, window len={wlen} in {cvname}\\n\")\n            by_absCV = Wsdf.sort_values(by=\"absCV\", ascending=False)\n            by_absCV[0:reportlen].to_csv(path_or_buf=cvname, header=True, index=True, index_label=None, mode='w')\n        else:  # not enough data\n            with open(fname, \"a\") as fil:\n                fil.write(wstats)",
  "def W_diff(self):\n        \"\"\" Compute the norm of the difference between to consecutive Ws / num_scenarios.\n\n        Returns:\n           global_diff (float): difference between to consecutive Ws\n\n        \"\"\"\n        cI = self.PHB._PHIter\n        self.grab_local_Ws()\n        self.local_Ws[-1] = self.local_Ws[0]\n        global_diff = np.zeros(1)\n        local_diff = np.zeros(1)\n        varcount = 0\n        local_diff[0] = 0\n        for (sname, scenario) in self.PHB.local_scenarios.items():\n            local_wdiffs = [w - w1\n                            for w, w1 in zip(self.local_Ws[cI][sname], self.local_Ws[cI-1][sname])]\n            for wdiff in local_wdiffs:\n                local_diff[0] += abs(wdiff)\n                varcount += 1\n        local_diff[0] /= varcount\n        self.PHB.comms[\"ROOT\"].Allreduce(local_diff, global_diff, op=MPI.SUM)\n\n        return global_diff[0] / self.PHB.n_proc",
  "def _get_cost_expression(model, cost_variable):\n    return model.find_component(cost_variable[0])[cost_variable[1]]",
  "def _yeild_componentdata_from_indexlist(component, index_list):\n    for index in index_list:\n        if type(index) is tuple:\n            indices = tuple(idx if idx != '*' else slice(None) for idx in index)\n            yield from component.__getitem__(indices)\n        elif index == '*' or index == '':\n            yield from component.values()\n        else:\n            yield component.__getitem__(index)",
  "def _get_nonant_list_from_templates(model, templates):\n    nonant_list = []\n    for name, index_list in templates.items():\n        component = model.find_component(name)\n        if isinstance(component, pyo.Var):\n            for vardata in _yeild_componentdata_from_indexlist(component, index_list):\n                nonant_list.append(vardata)\n        elif isinstance(component, pyo.Block):\n            for blockdata in _yeild_componentdata_from_indexlist(component, index_list):\n                for vardata in blockdata.component_data_objects(ctype=pyo.Var, active=True, descend_into=True):\n                    nonant_list.append(vardata)\n        else:\n            raise RuntimeError(\"Cannot extract non-anticipative variables from component {component.name}\")\n    return nonant_list",
  "def _get_nonant_list(model, pysp_node):\n    return _get_nonant_list_from_templates(model, {**pysp_node.stage._variable_templates, **pysp_node._variable_templates})",
  "def _get_derived_nonant_list(model, pysp_node):\n    return _get_nonant_list_from_templates(model, {**pysp_node.stage._derived_variable_templates, **pysp_node._derived_variable_templates})",
  "def _get_nodenames(root_node):\n    root_node._mpisppy_stage = 1\n    root_node._mpisppy_name = \"ROOT\"\n    root_node._mpisppy_parent_name = None\n    nodenames = [\"ROOT\"]\n    _add_next_stage(root_node, nodenames)\n    return nodenames",
  "def _add_next_stage(node, nodenames):\n    for idx, child in enumerate(node.children):\n        child._mpisppy_name = node._mpisppy_name + f\"_{idx}\"\n        child._mpisppy_stage = node._mpisppy_stage + 1\n        child._mpisppy_parent_name = node._mpisppy_name\n        nodenames.append(child._mpisppy_name)\n        _add_next_stage(child, nodenames)",
  "class PySPModel:\n    \"\"\"A class for instantiating PySP models for use in mpisppy. \n\n    Args:\n        model: The reference scenario model. Can be set\n            to Pyomo model or the name of a file\n            containing a Pyomo model. For historical\n            reasons, this argument can also be set to a\n            directory name where it is assumed a file\n            named ReferenceModel.py exists.\n        scenario_tree: The scenario tree. Can be set to\n            a Pyomo model, a file containing a Pyomo\n            model, or a .dat file containing data for an\n            abstract scenario tree model representation,\n            which defines the structure of the scenario\n            tree. It can also be a .py file that\n            contains a networkx scenario tree or a\n            networkx scenario tree object.  For\n            historical reasons, this argument can also\n            be set to a directory name where it is\n            assumed a file named ScenarioStructure.dat\n            exists.\n        data_dir: Directory containing .dat files necessary\n            for building the scenario instances\n            associated with the scenario tree. This\n            argument is required if no directory\n            information can be extracted from the first\n            two arguments and the reference model is an\n            abstract Pyomo model. Otherwise, it is not\n            required or the location will be inferred\n            from the scenario tree location (first) or\n            from the reference model location (second),\n            where it is assumed the data files reside in\n            the same directory.\n\n    Properties:\n      all_scenario_names (list):\n                    A list of scenario names based on the pysp model for\n                    use in mpisppy\n      all_node_names (list):\n                    A list of all node names based on the pysp model for\n                    use in mpisppy\n      scenario_creator (fct):\n                    A scenario creator function based on the pysp model\n                    for use in mpisppy\n      scenario_denouement (fct):\n                    A blank scenario_denouement function for use in mpisppy\n    \"\"\"\n    def __init__(self,\n                 model,\n                 scenario_tree,\n                 data_dir=None):\n\n        self._scenario_tree_instance_factory = \\\n                ScenarioTreeInstanceFactory(model, scenario_tree, data=data_dir)\n        self._pysp_scenario_tree = self._scenario_tree_instance_factory.generate_scenario_tree()\n\n        ## get the things out of the tree model we need\n        self._all_scenario_names = [s.name for s in self._pysp_scenario_tree.scenarios]\n        \n        ## check for more than two stages\n        ## gripe if we see bundles\n        if self._pysp_scenario_tree.bundles:\n            logger.warning(\"Bundles are ignored in PySPModel\")\n\n        self._all_nodenames = _get_nodenames(self._pysp_scenario_tree.findRootNode())\n\n    def scenario_creator_callback(self, scenario_name, **kwargs):\n        ## fist, get the model out\n        model = self._scenario_tree_instance_factory.construct_scenario_instance(\n                scenario_name, self._pysp_scenario_tree)\n\n        tree_scenario = self._pysp_scenario_tree.get_scenario(scenario_name)\n\n        non_leaf_nodes = tree_scenario.node_list[:-1]\n\n        for node in non_leaf_nodes:\n            if node.is_leaf_node():\n                raise Exception(\"Unexpected leaf node\")\n\n            node._mpisppy_cost_expression = _get_cost_expression(model, node._cost_variable)\n            node._mpisppy_nonant_list = _get_nonant_list(model, node)\n            node._mpisppy_nonant_ef_suppl_list = _get_derived_nonant_list(model, node)\n\n        ## add the things mpisppy expects to the model\n        model._mpisppy_probability = tree_scenario.probability\n\n        model._mpisppy_node_list = [mpisppyScenarioNode(\n                                            name=node._mpisppy_name,\n                                            cond_prob=node.conditional_probability,\n                                            stage=node._mpisppy_stage,\n                                            cost_expression=node._mpisppy_cost_expression,\n                                            nonant_list=node._mpisppy_nonant_list,\n                                            scen_model=None,\n                                            nonant_ef_suppl_list=node._mpisppy_nonant_ef_suppl_list,\n                                            parent_name=node._mpisppy_parent_name,\n                                    )\n                                    for node in non_leaf_nodes\n                                   ]\n\n        for _ in model.component_data_objects(pyo.Objective, active=True, descend_into=True):\n            break\n        else: # no break\n            print(\"Provided model has no objective; using PySP auto-generated objective\")\n\n            # attach PySP objective\n            leaf_node = tree_scenario.node_list[-1]\n            leaf_node._mpisppy_cost_expression = _get_cost_expression(model, leaf_node._cost_variable)\n\n            if hasattr(model, \"_PySPModel_objective\"):\n                raise RuntimeError(\"provided model has attribute _PySPModel_objective\")\n\n            model._PySPModel_objective = pyo.Objective(expr=\\\n                    pyo.quicksum(node._mpisppy_cost_expression for node in tree_scenario.node_list))\n\n        return model\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n\n    def close(self):\n        self._scenario_tree_instance_factory.close()\n\n    @property\n    def scenario_creator(self):\n        return lambda *args,**kwargs : self.scenario_creator_callback(*args,**kwargs)\n\n    @property\n    def all_scenario_names(self):\n        return self._all_scenario_names\n\n    @property\n    def all_nodenames(self):\n        return self._all_nodenames\n\n    @property\n    def scenario_denouement(self):\n        return lambda *args,**kwargs: None",
  "def __init__(self,\n                 model,\n                 scenario_tree,\n                 data_dir=None):\n\n        self._scenario_tree_instance_factory = \\\n                ScenarioTreeInstanceFactory(model, scenario_tree, data=data_dir)\n        self._pysp_scenario_tree = self._scenario_tree_instance_factory.generate_scenario_tree()\n\n        ## get the things out of the tree model we need\n        self._all_scenario_names = [s.name for s in self._pysp_scenario_tree.scenarios]\n        \n        ## check for more than two stages\n        ## gripe if we see bundles\n        if self._pysp_scenario_tree.bundles:\n            logger.warning(\"Bundles are ignored in PySPModel\")\n\n        self._all_nodenames = _get_nodenames(self._pysp_scenario_tree.findRootNode())",
  "def scenario_creator_callback(self, scenario_name, **kwargs):\n        ## fist, get the model out\n        model = self._scenario_tree_instance_factory.construct_scenario_instance(\n                scenario_name, self._pysp_scenario_tree)\n\n        tree_scenario = self._pysp_scenario_tree.get_scenario(scenario_name)\n\n        non_leaf_nodes = tree_scenario.node_list[:-1]\n\n        for node in non_leaf_nodes:\n            if node.is_leaf_node():\n                raise Exception(\"Unexpected leaf node\")\n\n            node._mpisppy_cost_expression = _get_cost_expression(model, node._cost_variable)\n            node._mpisppy_nonant_list = _get_nonant_list(model, node)\n            node._mpisppy_nonant_ef_suppl_list = _get_derived_nonant_list(model, node)\n\n        ## add the things mpisppy expects to the model\n        model._mpisppy_probability = tree_scenario.probability\n\n        model._mpisppy_node_list = [mpisppyScenarioNode(\n                                            name=node._mpisppy_name,\n                                            cond_prob=node.conditional_probability,\n                                            stage=node._mpisppy_stage,\n                                            cost_expression=node._mpisppy_cost_expression,\n                                            nonant_list=node._mpisppy_nonant_list,\n                                            scen_model=None,\n                                            nonant_ef_suppl_list=node._mpisppy_nonant_ef_suppl_list,\n                                            parent_name=node._mpisppy_parent_name,\n                                    )\n                                    for node in non_leaf_nodes\n                                   ]\n\n        for _ in model.component_data_objects(pyo.Objective, active=True, descend_into=True):\n            break\n        else: # no break\n            print(\"Provided model has no objective; using PySP auto-generated objective\")\n\n            # attach PySP objective\n            leaf_node = tree_scenario.node_list[-1]\n            leaf_node._mpisppy_cost_expression = _get_cost_expression(model, leaf_node._cost_variable)\n\n            if hasattr(model, \"_PySPModel_objective\"):\n                raise RuntimeError(\"provided model has attribute _PySPModel_objective\")\n\n            model._PySPModel_objective = pyo.Objective(expr=\\\n                    pyo.quicksum(node._mpisppy_cost_expression for node in tree_scenario.node_list))\n\n        return model",
  "def __enter__(self):\n        return self",
  "def __exit__(self, type, value, traceback):\n        self.close()",
  "def close(self):\n        self._scenario_tree_instance_factory.close()",
  "def scenario_creator(self):\n        return lambda *args,**kwargs : self.scenario_creator_callback(*args,**kwargs)",
  "def all_scenario_names(self):\n        return self._all_scenario_names",
  "def all_nodenames(self):\n        return self._all_nodenames",
  "def scenario_denouement(self):\n        return lambda *args,**kwargs: None",
  "def ArchiveReaderFactory(dirname, **kwds):\n    if not os.path.exists(ArchiveReader.normalize_name(dirname)):\n        raise IOError(\"Cannot find file or directory `\" + dirname +\n                      \"'\\nPath expanded to: '\" + ArchiveReader.normalize_name(\n                          dirname) + \"'\")\n    if ArchiveReader.isDir(dirname):\n        return DirArchiveReader(dirname, **kwds)\n    elif zipfile_available and ArchiveReader.isZip(dirname):\n        return ZipArchiveReader(dirname, **kwds)\n    elif tarfile_available and ArchiveReader.isTar(dirname):\n        return TarArchiveReader(dirname, **kwds)\n    elif gzip_available and ArchiveReader.isGzipFile(dirname):\n        return GzipFileArchiveReader(dirname, **kwds)\n    elif bz2_available and ArchiveReader.isBZ2File(dirname):\n        return BZ2FileArchiveReader(dirname, **kwds)\n    elif ArchiveReader.isFile(dirname):\n        return FileArchiveReader(dirname, **kwds)\n    else:\n        raise ValueError(\"ArchiveReaderFactory was given an \"\n                         \"unrecognized archive type with \"\n                         \"name '%s'\" % dirname)",
  "class ArchiveReader:\n\n    @staticmethod\n    def isDir(name):\n        return os.path.isdir(ArchiveReader.normalize_name(name))\n\n    @staticmethod\n    def isZip(name):\n        if not zipfile_available:\n            raise ImportError(\"zipfile support is disabled\")\n        try:\n            return zipfile.is_zipfile(ArchiveReader.normalize_name(name))\n        except:\n            return False\n\n    @staticmethod\n    def isTar(name):\n        if not tarfile_available:\n            raise ImportError(\"tarfile support is disabled\")\n        return tarfile.is_tarfile(ArchiveReader.normalize_name(name))\n\n    @staticmethod\n    def isArchivedFile(name):\n        return (not (ArchiveReader.isDir(name) or ArchiveReader.isFile(name))) and \\\n            (tarfile_available and ArchiveReader.isTar(name)) or \\\n            (zipfile_available and ArchiveReader.isZip(name)) or \\\n            (gzip_available and ArchiveReader.isGzipFile(name)) or \\\n            (bz2_available and ArchiveReader.isBZ2File(name))\n\n    @staticmethod\n    def isGzipFile(name):\n        if not gzip_available:\n            raise ImportError(\"gzip support is disabled\")\n        try:\n            f = gzip.GzipFile(ArchiveReader.normalize_name(name))\n            f.read(1)\n            f.close()\n        except IOError:\n            return False\n        return True\n\n    @staticmethod\n    def isBZ2File(name):\n        if not bz2_available:\n            raise ImportError(\"bz2 support is disabled\")\n        try:\n            f = bz2.BZ2File(ArchiveReader.normalize_name(name))\n            f.read(1)\n            f.close()\n        except IOError:\n            return False\n        except EOFError:\n            return False\n        return True\n\n    @staticmethod\n    def isFile(name):\n        return os.path.isfile(ArchiveReader.normalize_name(name))\n\n    @staticmethod\n    def normalize_name(filename):\n        \"\"\"Turns the given file name into a normalized absolute path\"\"\"\n        if filename is not None:\n            filename = os.path.expanduser(filename)\n            if not os.path.isabs(filename):\n                filename = os.path.abspath(filename)\n            filename = ArchiveReader._posix_name(filename)\n            return posixpath.normpath(filename)\n\n    @staticmethod\n    def _posix_name(filename):\n        if filename is not None:\n            return filename.replace('\\\\', '/')\n\n    def __init__(self, name, *args, **kwds):\n        posixabsname = self.normalize_name(name)\n        if not os.path.exists(posixabsname):\n            raise IOError(\"cannot find file or directory `\" + posixabsname +\n                          \"'\")\n\n        self._abspath = os.path.dirname(posixabsname)\n        self._basename = os.path.basename(posixabsname)\n        self._archive_name = posixabsname\n\n        subdir = kwds.pop('subdir', None)\n        if (subdir is not None) and (subdir.strip() == ''):\n            subdir = None\n        maxdepth = kwds.pop('maxdepth', None)\n        self._filter = kwds.pop('filter', None)\n\n        self._subdir = posixpath.normpath(ArchiveReader._posix_name(subdir))+_sep \\\n                       if (subdir is not None) else None\n\n        self._maxdepth = maxdepth\n        if (self._maxdepth is not None) and (self._maxdepth < 0):\n            raise ValueError(\"maxdepth must be >= 0\")\n\n        self._names_list = []\n        self._artificial_dirs = set()\n        self._extractions = set()\n        # the python zipfile or tarfile object or None for (dir)\n        self._handler = None\n        self._workdir = tempfile.mkdtemp()\n\n        self._init(*args, **kwds)\n\n        if self._filter is not None:\n            self._names_list = [_f for _f in self._names_list \\\n                                if not self._filter(_f)]\n\n    def name(self):\n        return self._archive_name\n\n    def getExtractionDir(self):\n        return self._workdir\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n\n    def close(self):\n        if self._handler is not None:\n            self._handler.close()\n        if (self._workdir is not None) and \\\n           (os.path.exists(self._workdir)):\n            shutil.rmtree(self._workdir, True)\n        self._workdir = None\n\n    def clear_extractions(self):\n        # don't do anything to directories since they may have existed\n        # prior to extracting them and we don't know what they\n        # contained\n        for name in self._extractions:\n            if os.path.exists(name) and \\\n               (not os.path.isdir(name)):\n                os.remove(name)\n        self._extractions.clear()\n        # If the directories existed in the workdir, then they will be\n        # taken care of here\n        if (self._workdir is not None) and \\\n           (os.path.exists(self._workdir)):\n            for root, dirs, files in os.walk(self._workdir):\n                while len(dirs):\n                    shutil.rmtree(posixpath.join(root, dirs.pop()), True)\n                while len(files):\n                    os.remove(posixpath.join(root, files.pop()))\n\n    def getnames(self):\n        return self._names_list\n\n    def contains(self, name):\n        return name in self._names_list\n\n    def _validate_name(self, name):\n        name = self._posix_name(name)\n        if (name is None) and (len(self._names_list) == 0):\n            raise KeyError(\"The archive is empty!\")\n        elif (name is None) and (len(self._names_list) > 1):\n            raise KeyError(\"A name argument is required when \"\n                           \"the archive has more than one member\")\n        elif name is None:\n            name = self._names_list[0]\n\n        if name in self._artificial_dirs:\n            return None, name\n\n        if not self.contains(name):\n            #\n            # There are cases (e.g., Windows zipped folders) where\n            # directory names don't appear in the list of objects\n            # available to in zipfile. We will try to retroactively\n            # handle this case by check if any listed archive members\n            # begin with name+_sep, and if so, adding the directory\n            # name to the _names_list\n            #\n            checkname = name + _sep\n            if (self._maxdepth is None) or (\n                    checkname.count(_sep) <= self._maxdepth + 1):\n                for othername in self._fulldepth_names_list:\n                    if othername.startswith(checkname):\n                        self._artificial_dirs.add(name)\n                        self._names_list.append(name)\n                        return None, name\n            msg = (\"There is no item named '%s' in \"\n                   \"the archive %s\" % (name, self._basename))\n            if self._subdir is not None:\n                msg += \", subdirectory: \" + self._subdir\n            raise KeyError(msg)\n        absname = name if (self._subdir is None) else self._subdir + name\n        return absname, name\n\n    def open(self, name=None, *args, **kwds):\n        absname, relname = self._validate_name(name)\n        return self._openImp(absname, relname, *args, **kwds)\n\n    def _openImp(self, name, *args, **kwds):\n        raise NotImplementedError(\"This method has not been \" \"implemented\")\n\n    def extract(self, member=None, path=None, recursive=False, *args, **kwds):\n        absolute_name, relative_name = self._validate_name(member)\n        dst, children = self._extractImp(absolute_name, relative_name, path,\n                                         recursive, *args, **kwds)\n        self._extractions.add(dst)\n        if not recursive:\n            assert len(children) == 0\n        else:\n            self._extractions.update(children)\n        return dst\n\n    def _extractImp(self, absolute_name, relative_name, path, recursive, *args,\n                    **kwds):\n        raise NotImplementedError(\"This method has not been \" \"implemented\")\n\n    def extractall(self,\n                   path=None,\n                   members=None,\n                   recursive=False,\n                   *args,\n                   **kwds):\n        names = None\n        if members is not None:\n            names = set(members)\n        else:\n            names = set(self._names_list)\n        # Save the expense checking recursions if there is no point\n        if len(names) == len(self._names_list):\n            recursive = False\n        dsts = []\n        while names:\n            absolute_name, relative_name = self._validate_name(names.pop())\n            dst, children = self._extractImp(absolute_name, relative_name, path,\n                                             recursive, *args, **kwds)\n            if not recursive:\n                assert len(children) == 0\n            dsts.append(dst)\n            self._extractions.add(dst)\n            if len(children):\n                self._extractions.update(children)\n                names = names - set(children)\n\n        return dsts\n\n    # like shutil.copytree, but will handle existing\n    # directories the same way tarfile extraction\n    # occurs by adding new content rather than\n    # raising an exception\n    @staticmethod\n    def _copytree(src, dst, ignores=None, maxdepth=None):\n\n        assert os.path.exists(src) and os.path.isdir(src)\n        if not os.path.exists(dst):\n            os.makedirs(dst)\n\n        if (maxdepth is None) or (maxdepth > 0):\n\n            if maxdepth is not None:\n                maxdepth -= 1\n\n            names = os.listdir(src)\n            if ignores is not None:\n                ignored_names = ignores\n            else:\n                ignored_names = set()\n\n            for name in names:\n                srcname = posixpath.join(src, name)\n                if srcname in ignored_names:\n                    continue\n                dstname = posixpath.join(dst, name)\n                if os.path.isdir(srcname):\n                    ArchiveReader._copytree(\n                        srcname, dstname, ignores=ignores, maxdepth=maxdepth)\n                else:\n                    shutil.copy2(srcname, dstname)\n        try:\n            shutil.copystat(src, dst)\n        except _WindowsError:\n            # can't copy file access times on Windows\n            pass",
  "class _ziptar_base(ArchiveReader):\n\n    @staticmethod\n    def _fixnames(names, subdir, maxdepth):\n        names_list = [posixpath.normpath(name) for name in names]\n        subdir_depth = 0\n        if subdir is not None:\n            names_list = [name for name in names_list \\\n                          if name.startswith(subdir)]\n            subdir_depth = subdir.count(_sep)\n        fulldepth_names_list = list(names_list)\n        if maxdepth is not None:\n            names_list = [name for name in fulldepth_names_list \\\n                          if (name.count(_sep) - subdir_depth) <= maxdepth]\n        if subdir is not None:\n            names_list = [name.replace(subdir,'') \\\n                          for name in names_list]\n            fulldepth_names_list = [name.replace(subdir,'') \\\n                               for name in fulldepth_names_list]\n        return names_list, fulldepth_names_list, subdir_depth\n\n    def _extractImp(self, absolute_name, relative_name, path, recursive):\n\n        use_handler = True\n        if absolute_name is None:\n            # This case implies that this was an artificially\n            # added directory that was not appearing in the\n            # archive even though it technically exists\n            # (a rare but possible edge case)\n            use_handler = False\n            absolute_name = relative_name if (\n                self._subdir is None) else self._subdir + relative_name\n\n        tmp_dst = posixpath.join(self._workdir, absolute_name)\n\n        if use_handler:\n            try:\n                self._handler.extract(absolute_name, self._workdir)\n            except KeyError:  # sometimes directories need an _sep ending\n                self._handler.extract(absolute_name + _sep, self._workdir)\n        else:\n            if not os.path.exists(tmp_dst):\n                os.makedirs(tmp_dst)\n\n        dst = tmp_dst\n        if path is not None:\n\n            dst = posixpath.join(path, relative_name)\n            if os.path.isdir(tmp_dst):\n                # updated the timestamp if it exists, otherwise\n                # just create the directory\n                self._copytree(tmp_dst, dst, maxdepth=0)\n            else:\n                if not os.path.exists(os.path.dirname(dst)):\n                    os.makedirs(os.path.dirname(dst))\n                try:\n                    os.rename(tmp_dst, dst)\n                except OSError:\n                    shutil.copy2(tmp_dst, dst)\n\n        children = []\n        if os.path.isdir(dst) and recursive:\n            new_names = [relname for relname in self._names_list \\\n                         if relname.startswith(relative_name)]\n            if relative_name in new_names:\n                new_names.remove(relative_name)\n            for childname in new_names:\n                absolute_childname, relative_childname = self._validate_name(\n                    childname)\n                childdst, recursives = self._extractImp(\n                    absolute_childname, relative_childname, path, False)\n                assert len(recursives) == 0\n                children.append(childdst)\n\n        return dst, children",
  "class ZipArchiveReader(_ziptar_base):\n\n    def _init(self, *args, **kwds):\n        if zipfile is None:\n            raise ImportError(\"zipfile support is disabled\")\n        assert (self._abspath is not None)\n        assert (self._basename is not None)\n        assert (self._archive_name is not None)\n\n        if not self.isZip(self._archive_name):\n            raise TypeError(\"Unrecognized zipfile format for file: %s\" %\n                            (self._archive_name))\n\n        self._handler = zipfile.ZipFile(self._archive_name, *args, **kwds)\n        self._names_list, self._fulldepth_names_list, self._subdir_depth = \\\n            self._fixnames(self._handler.namelist(), self._subdir, self._maxdepth)\n\n    def _openImp(self, absolute_name, relative_name, *args, **kwds):\n        f = None\n        try:\n            if absolute_name is None:\n                raise KeyError\n            f = self._handler.open(absolute_name, *args, **kwds)\n        except KeyError:\n            # when this method is called we have already verified the name\n            # existed in the list, so this must be a directory\n            raise IOError(\"Failed to open with zipfile, this must be a \"\n                          \"directory: %s\" % (absolute_name))\n        return f",
  "class TarArchiveReader(_ziptar_base):\n\n    def _init(self, *args, **kwds):\n        if not tarfile_available:\n            raise ImportError(\"tarfile support is disabled\")\n        assert (self._abspath is not None)\n        assert (self._basename is not None)\n        assert (self._archive_name is not None)\n\n        if not self.isTar(self._archive_name):\n            raise TypeError(\"Unrecognized tarfile format for file: %s\" %\n                            (self._archive_name))\n\n        self._handler = tarfile.open(self._archive_name, *args, **kwds)\n        self._names_list, self._fulldepth_names_list, self._subdir_depth = \\\n            self._fixnames(self._handler.getnames(), self._subdir, self._maxdepth)\n\n    def _openImp(self, absolute_name, relative_name, *args, **kwds):\n        f = None\n        if absolute_name is not None:\n            f = self._handler.extractfile(absolute_name, *args, **kwds)\n        if f is None:\n            # when this method is called we have already verified the name\n            # existed in the list, so this must be a directory\n            raise IOError(\"Failed to open with tarfile, this must be a \"\n                          \"directory: %s\" % (absolute_name))\n        return f",
  "class DirArchiveReader(ArchiveReader):\n\n    def _init(self, *args, **kwds):\n        assert (self._abspath is not None)\n        assert (self._basename is not None)\n        assert (self._archive_name is not None)\n\n        if kwds:\n            raise ValueError(\"Unexpected keyword options found \"\n                             \"while initializing '%s':\\n\\t%s\" %\n                             (type(self).__name__,\n                              ','.join(sorted(kwds.keys()))))\n        if args:\n            raise ValueError(\"Unexpected arguments found \"\n                             \"while initializing '%s':\\n\\t%s\" %\n                             (type(self).__name__, ','.join(args)))\n\n        if not self.isDir(self._archive_name):\n            raise TypeError(\"Path not recognized as a directory: %s\" %\n                            (self._archive_name))\n\n        rootdir = self._archive_name\n        if self._subdir is not None:\n            rootdir = posixpath.join(rootdir, self._subdir)\n            if not os.path.exists(rootdir):\n                raise IOError(\n                    \"Subdirectory '%s' does not exists in root directory: %s\" %\n                    (self._subdir, self._archive_name))\n            self._names_list = self._walk(rootdir, maxdepth=self._maxdepth + 1)\n        else:\n            self._names_list = self._walk(rootdir, maxdepth=self._maxdepth)\n        self._fulldepth_names_list = self._walk(rootdir)\n\n    @staticmethod\n    def _walk(rootdir, maxdepth=None):\n        names_list = []\n        for root, dirs, files in os.walk(rootdir, topdown=True):\n            prefix = posixpath.relpath(ArchiveReader._posix_name(root), rootdir)\n            if prefix.endswith(_sep):\n                prefix = prefix[:-1]\n            if prefix == '.':\n                prefix = ''\n            for dname in dirs:\n                names_list.append(posixpath.join(prefix, dname))\n            if maxdepth is not None and prefix.count(_sep) >= maxdepth:\n                continue\n            for fname in files:\n                names_list.append(posixpath.join(prefix, fname))\n        return names_list\n\n    def _extractImp(self, absolute_name, relative_name, path, recursive):\n\n        assert absolute_name is not None\n\n        if path is not None:\n            dst = posixpath.join(path, relative_name)\n        else:\n            dst = posixpath.join(self._workdir, absolute_name)\n        src = posixpath.join(self._archive_name, absolute_name)\n        children = []\n        if os.path.isdir(src):\n            if recursive:\n                ignores = []\n                for child in self._walk(src):\n                    rname = posixpath.join(relative_name, child)\n                    if rname in self._names_list:\n                        children.append(rname)\n                    else:\n                        ignores.append(posixpath.join(src, child))\n                self._copytree(src, dst, ignores=ignores)\n\n            elif not os.path.exists(dst):\n                os.makedirs(dst)\n            # if not recursive and the destination is an existing\n            # directory, then there is nothing to do\n        else:\n            if not os.path.exists(posixpath.dirname(dst)):\n                os.makedirs(os.path.dirname(dst))\n            shutil.copy2(src, dst)\n\n        return dst, children\n\n    def _openImp(self, absolute_name, relative_name, *args, **kwds):\n        assert absolute_name is not None\n        return open(\n            posixpath.join(self._archive_name, absolute_name), 'rb', *args, **\n            kwds)",
  "class FileArchiveReader(ArchiveReader):\n\n    _handler_class = open\n\n    def _extract_name(self, name):\n        return name\n\n    def _init(self):\n        assert (self._abspath is not None)\n        assert (self._basename is not None)\n        assert (self._archive_name is not None)\n\n        if self._subdir is not None:\n            raise ValueError(\"'subdir' keyword option is not handled by \"\n                             \"'%s'\" % (type(self).__name__))\n        if self._maxdepth is not None:\n            raise ValueError(\"'maxdepth' keyword option is not handled by \"\n                             \"'%s'\" % (type(self).__name__))\n\n        if not self.isFile(self._archive_name):\n            raise TypeError(\"Path does not point to a file: %s\" %\n                            (self._archive_name))\n        extract_name = self._extract_name(self._basename)\n        if extract_name is not None:\n            self._names_list = [extract_name]\n            self._fulldepth_names_list = [extract_name]\n            self._extract_name = extract_name\n        else:\n            self._names_list = [self._basename]\n            self._fulldepth_names_list = [self._basename]\n            self._extract_name = None\n\n    def _extractImp(self, absolute_name, relative_name, path, recursive):\n        assert absolute_name == self._extract_name\n        assert relative_name == self._extract_name\n        if recursive:\n            raise ValueError(\"Recursive extraction does not make \"\n                             \"sense for compressed file archive types\")\n        if path is not None:\n            dst = posixpath.join(path, relative_name)\n        else:\n            dst = posixpath.join(self._workdir, absolute_name)\n        with open(dst, 'wb') as dstf:\n            handler = self._handler_class(self._archive_name, 'rb')\n            shutil.copyfileobj(handler, dstf)\n            handler.close()\n\n        return dst, []\n\n    def _openImp(self, absolute_name, relative_name, *args, **kwds):\n        assert absolute_name == self._extract_name\n        return self._handler_class(self._archive_name, 'rb', *args, **kwds)",
  "class GzipFileArchiveReader(FileArchiveReader):\n\n    _handler_class = gzip.GzipFile if gzip_available else None\n\n    def _extract_name(self, name):\n        # see the man page for gzip\n        basename, ext = os.path.splitext(name)\n        if ext == self._suffix:\n            return basename\n        else:\n            return None\n\n    def __init__(self, *args, **kwds):\n        if not gzip_available:\n            raise ImportError(\"gzip support is disabled\")\n        self._suffix = kwds.pop('suffix', '.gz')\n        super(GzipFileArchiveReader, self).__init__(*args, **kwds)\n        if not self.isGzipFile(self._archive_name):\n            raise TypeError(\"Unrecognized gzip format for file: %s\" %\n                            (self._archive_name))\n\n    def _extractImp(self, absolute_name, relative_name, path, recursive):\n        if self._extract_name is None:\n            raise TypeError(\n                \"Extraction disabled. File suffix %s does not \"\n                \"match expected suffix for compression type %s. \"\n                \"The default suffix can be changed by passing \"\n                \"'suffix=.<ext>' into the ArchiveReader constructor.\" % (\n                    os.path.splitext(self._basename)[1], self._suffix))\n        return FileArchiveReader._extractImp(self, absolute_name, relative_name,\n                                             path, recursive)",
  "class BZ2FileArchiveReader(FileArchiveReader):\n\n    _handler_class = bz2.BZ2File if bz2_available else None\n\n    def _extract_name(self, name):\n        # see the man page for bzip2\n        basename, ext = os.path.splitext(name)\n        if ext in ('.bz2', '.bz'):\n            return basename\n        elif ext in ('.tbz2', '.tbz'):\n            return basename + '.tar'\n        else:\n            return name + '.out'\n\n    def __init__(self, *args, **kwds):\n        if not bz2_available:\n            raise ImportError(\"bz2 support is disabled\")\n        super(BZ2FileArchiveReader, self).__init__(*args, **kwds)\n        if not self.isBZ2File(self._archive_name):\n            raise TypeError(\"Unrecognized bzip2 format for file: %s\" %\n                            (self._archive_name))",
  "def isDir(name):\n        return os.path.isdir(ArchiveReader.normalize_name(name))",
  "def isZip(name):\n        if not zipfile_available:\n            raise ImportError(\"zipfile support is disabled\")\n        try:\n            return zipfile.is_zipfile(ArchiveReader.normalize_name(name))\n        except:\n            return False",
  "def isTar(name):\n        if not tarfile_available:\n            raise ImportError(\"tarfile support is disabled\")\n        return tarfile.is_tarfile(ArchiveReader.normalize_name(name))",
  "def isArchivedFile(name):\n        return (not (ArchiveReader.isDir(name) or ArchiveReader.isFile(name))) and \\\n            (tarfile_available and ArchiveReader.isTar(name)) or \\\n            (zipfile_available and ArchiveReader.isZip(name)) or \\\n            (gzip_available and ArchiveReader.isGzipFile(name)) or \\\n            (bz2_available and ArchiveReader.isBZ2File(name))",
  "def isGzipFile(name):\n        if not gzip_available:\n            raise ImportError(\"gzip support is disabled\")\n        try:\n            f = gzip.GzipFile(ArchiveReader.normalize_name(name))\n            f.read(1)\n            f.close()\n        except IOError:\n            return False\n        return True",
  "def isBZ2File(name):\n        if not bz2_available:\n            raise ImportError(\"bz2 support is disabled\")\n        try:\n            f = bz2.BZ2File(ArchiveReader.normalize_name(name))\n            f.read(1)\n            f.close()\n        except IOError:\n            return False\n        except EOFError:\n            return False\n        return True",
  "def isFile(name):\n        return os.path.isfile(ArchiveReader.normalize_name(name))",
  "def normalize_name(filename):\n        \"\"\"Turns the given file name into a normalized absolute path\"\"\"\n        if filename is not None:\n            filename = os.path.expanduser(filename)\n            if not os.path.isabs(filename):\n                filename = os.path.abspath(filename)\n            filename = ArchiveReader._posix_name(filename)\n            return posixpath.normpath(filename)",
  "def _posix_name(filename):\n        if filename is not None:\n            return filename.replace('\\\\', '/')",
  "def __init__(self, name, *args, **kwds):\n        posixabsname = self.normalize_name(name)\n        if not os.path.exists(posixabsname):\n            raise IOError(\"cannot find file or directory `\" + posixabsname +\n                          \"'\")\n\n        self._abspath = os.path.dirname(posixabsname)\n        self._basename = os.path.basename(posixabsname)\n        self._archive_name = posixabsname\n\n        subdir = kwds.pop('subdir', None)\n        if (subdir is not None) and (subdir.strip() == ''):\n            subdir = None\n        maxdepth = kwds.pop('maxdepth', None)\n        self._filter = kwds.pop('filter', None)\n\n        self._subdir = posixpath.normpath(ArchiveReader._posix_name(subdir))+_sep \\\n                       if (subdir is not None) else None\n\n        self._maxdepth = maxdepth\n        if (self._maxdepth is not None) and (self._maxdepth < 0):\n            raise ValueError(\"maxdepth must be >= 0\")\n\n        self._names_list = []\n        self._artificial_dirs = set()\n        self._extractions = set()\n        # the python zipfile or tarfile object or None for (dir)\n        self._handler = None\n        self._workdir = tempfile.mkdtemp()\n\n        self._init(*args, **kwds)\n\n        if self._filter is not None:\n            self._names_list = [_f for _f in self._names_list \\\n                                if not self._filter(_f)]",
  "def name(self):\n        return self._archive_name",
  "def getExtractionDir(self):\n        return self._workdir",
  "def __enter__(self):\n        return self",
  "def __exit__(self, type, value, traceback):\n        self.close()",
  "def close(self):\n        if self._handler is not None:\n            self._handler.close()\n        if (self._workdir is not None) and \\\n           (os.path.exists(self._workdir)):\n            shutil.rmtree(self._workdir, True)\n        self._workdir = None",
  "def clear_extractions(self):\n        # don't do anything to directories since they may have existed\n        # prior to extracting them and we don't know what they\n        # contained\n        for name in self._extractions:\n            if os.path.exists(name) and \\\n               (not os.path.isdir(name)):\n                os.remove(name)\n        self._extractions.clear()\n        # If the directories existed in the workdir, then they will be\n        # taken care of here\n        if (self._workdir is not None) and \\\n           (os.path.exists(self._workdir)):\n            for root, dirs, files in os.walk(self._workdir):\n                while len(dirs):\n                    shutil.rmtree(posixpath.join(root, dirs.pop()), True)\n                while len(files):\n                    os.remove(posixpath.join(root, files.pop()))",
  "def getnames(self):\n        return self._names_list",
  "def contains(self, name):\n        return name in self._names_list",
  "def _validate_name(self, name):\n        name = self._posix_name(name)\n        if (name is None) and (len(self._names_list) == 0):\n            raise KeyError(\"The archive is empty!\")\n        elif (name is None) and (len(self._names_list) > 1):\n            raise KeyError(\"A name argument is required when \"\n                           \"the archive has more than one member\")\n        elif name is None:\n            name = self._names_list[0]\n\n        if name in self._artificial_dirs:\n            return None, name\n\n        if not self.contains(name):\n            #\n            # There are cases (e.g., Windows zipped folders) where\n            # directory names don't appear in the list of objects\n            # available to in zipfile. We will try to retroactively\n            # handle this case by check if any listed archive members\n            # begin with name+_sep, and if so, adding the directory\n            # name to the _names_list\n            #\n            checkname = name + _sep\n            if (self._maxdepth is None) or (\n                    checkname.count(_sep) <= self._maxdepth + 1):\n                for othername in self._fulldepth_names_list:\n                    if othername.startswith(checkname):\n                        self._artificial_dirs.add(name)\n                        self._names_list.append(name)\n                        return None, name\n            msg = (\"There is no item named '%s' in \"\n                   \"the archive %s\" % (name, self._basename))\n            if self._subdir is not None:\n                msg += \", subdirectory: \" + self._subdir\n            raise KeyError(msg)\n        absname = name if (self._subdir is None) else self._subdir + name\n        return absname, name",
  "def open(self, name=None, *args, **kwds):\n        absname, relname = self._validate_name(name)\n        return self._openImp(absname, relname, *args, **kwds)",
  "def _openImp(self, name, *args, **kwds):\n        raise NotImplementedError(\"This method has not been \" \"implemented\")",
  "def extract(self, member=None, path=None, recursive=False, *args, **kwds):\n        absolute_name, relative_name = self._validate_name(member)\n        dst, children = self._extractImp(absolute_name, relative_name, path,\n                                         recursive, *args, **kwds)\n        self._extractions.add(dst)\n        if not recursive:\n            assert len(children) == 0\n        else:\n            self._extractions.update(children)\n        return dst",
  "def _extractImp(self, absolute_name, relative_name, path, recursive, *args,\n                    **kwds):\n        raise NotImplementedError(\"This method has not been \" \"implemented\")",
  "def extractall(self,\n                   path=None,\n                   members=None,\n                   recursive=False,\n                   *args,\n                   **kwds):\n        names = None\n        if members is not None:\n            names = set(members)\n        else:\n            names = set(self._names_list)\n        # Save the expense checking recursions if there is no point\n        if len(names) == len(self._names_list):\n            recursive = False\n        dsts = []\n        while names:\n            absolute_name, relative_name = self._validate_name(names.pop())\n            dst, children = self._extractImp(absolute_name, relative_name, path,\n                                             recursive, *args, **kwds)\n            if not recursive:\n                assert len(children) == 0\n            dsts.append(dst)\n            self._extractions.add(dst)\n            if len(children):\n                self._extractions.update(children)\n                names = names - set(children)\n\n        return dsts",
  "def _copytree(src, dst, ignores=None, maxdepth=None):\n\n        assert os.path.exists(src) and os.path.isdir(src)\n        if not os.path.exists(dst):\n            os.makedirs(dst)\n\n        if (maxdepth is None) or (maxdepth > 0):\n\n            if maxdepth is not None:\n                maxdepth -= 1\n\n            names = os.listdir(src)\n            if ignores is not None:\n                ignored_names = ignores\n            else:\n                ignored_names = set()\n\n            for name in names:\n                srcname = posixpath.join(src, name)\n                if srcname in ignored_names:\n                    continue\n                dstname = posixpath.join(dst, name)\n                if os.path.isdir(srcname):\n                    ArchiveReader._copytree(\n                        srcname, dstname, ignores=ignores, maxdepth=maxdepth)\n                else:\n                    shutil.copy2(srcname, dstname)\n        try:\n            shutil.copystat(src, dst)\n        except _WindowsError:\n            # can't copy file access times on Windows\n            pass",
  "def _fixnames(names, subdir, maxdepth):\n        names_list = [posixpath.normpath(name) for name in names]\n        subdir_depth = 0\n        if subdir is not None:\n            names_list = [name for name in names_list \\\n                          if name.startswith(subdir)]\n            subdir_depth = subdir.count(_sep)\n        fulldepth_names_list = list(names_list)\n        if maxdepth is not None:\n            names_list = [name for name in fulldepth_names_list \\\n                          if (name.count(_sep) - subdir_depth) <= maxdepth]\n        if subdir is not None:\n            names_list = [name.replace(subdir,'') \\\n                          for name in names_list]\n            fulldepth_names_list = [name.replace(subdir,'') \\\n                               for name in fulldepth_names_list]\n        return names_list, fulldepth_names_list, subdir_depth",
  "def _extractImp(self, absolute_name, relative_name, path, recursive):\n\n        use_handler = True\n        if absolute_name is None:\n            # This case implies that this was an artificially\n            # added directory that was not appearing in the\n            # archive even though it technically exists\n            # (a rare but possible edge case)\n            use_handler = False\n            absolute_name = relative_name if (\n                self._subdir is None) else self._subdir + relative_name\n\n        tmp_dst = posixpath.join(self._workdir, absolute_name)\n\n        if use_handler:\n            try:\n                self._handler.extract(absolute_name, self._workdir)\n            except KeyError:  # sometimes directories need an _sep ending\n                self._handler.extract(absolute_name + _sep, self._workdir)\n        else:\n            if not os.path.exists(tmp_dst):\n                os.makedirs(tmp_dst)\n\n        dst = tmp_dst\n        if path is not None:\n\n            dst = posixpath.join(path, relative_name)\n            if os.path.isdir(tmp_dst):\n                # updated the timestamp if it exists, otherwise\n                # just create the directory\n                self._copytree(tmp_dst, dst, maxdepth=0)\n            else:\n                if not os.path.exists(os.path.dirname(dst)):\n                    os.makedirs(os.path.dirname(dst))\n                try:\n                    os.rename(tmp_dst, dst)\n                except OSError:\n                    shutil.copy2(tmp_dst, dst)\n\n        children = []\n        if os.path.isdir(dst) and recursive:\n            new_names = [relname for relname in self._names_list \\\n                         if relname.startswith(relative_name)]\n            if relative_name in new_names:\n                new_names.remove(relative_name)\n            for childname in new_names:\n                absolute_childname, relative_childname = self._validate_name(\n                    childname)\n                childdst, recursives = self._extractImp(\n                    absolute_childname, relative_childname, path, False)\n                assert len(recursives) == 0\n                children.append(childdst)\n\n        return dst, children",
  "def _init(self, *args, **kwds):\n        if zipfile is None:\n            raise ImportError(\"zipfile support is disabled\")\n        assert (self._abspath is not None)\n        assert (self._basename is not None)\n        assert (self._archive_name is not None)\n\n        if not self.isZip(self._archive_name):\n            raise TypeError(\"Unrecognized zipfile format for file: %s\" %\n                            (self._archive_name))\n\n        self._handler = zipfile.ZipFile(self._archive_name, *args, **kwds)\n        self._names_list, self._fulldepth_names_list, self._subdir_depth = \\\n            self._fixnames(self._handler.namelist(), self._subdir, self._maxdepth)",
  "def _openImp(self, absolute_name, relative_name, *args, **kwds):\n        f = None\n        try:\n            if absolute_name is None:\n                raise KeyError\n            f = self._handler.open(absolute_name, *args, **kwds)\n        except KeyError:\n            # when this method is called we have already verified the name\n            # existed in the list, so this must be a directory\n            raise IOError(\"Failed to open with zipfile, this must be a \"\n                          \"directory: %s\" % (absolute_name))\n        return f",
  "def _init(self, *args, **kwds):\n        if not tarfile_available:\n            raise ImportError(\"tarfile support is disabled\")\n        assert (self._abspath is not None)\n        assert (self._basename is not None)\n        assert (self._archive_name is not None)\n\n        if not self.isTar(self._archive_name):\n            raise TypeError(\"Unrecognized tarfile format for file: %s\" %\n                            (self._archive_name))\n\n        self._handler = tarfile.open(self._archive_name, *args, **kwds)\n        self._names_list, self._fulldepth_names_list, self._subdir_depth = \\\n            self._fixnames(self._handler.getnames(), self._subdir, self._maxdepth)",
  "def _openImp(self, absolute_name, relative_name, *args, **kwds):\n        f = None\n        if absolute_name is not None:\n            f = self._handler.extractfile(absolute_name, *args, **kwds)\n        if f is None:\n            # when this method is called we have already verified the name\n            # existed in the list, so this must be a directory\n            raise IOError(\"Failed to open with tarfile, this must be a \"\n                          \"directory: %s\" % (absolute_name))\n        return f",
  "def _init(self, *args, **kwds):\n        assert (self._abspath is not None)\n        assert (self._basename is not None)\n        assert (self._archive_name is not None)\n\n        if kwds:\n            raise ValueError(\"Unexpected keyword options found \"\n                             \"while initializing '%s':\\n\\t%s\" %\n                             (type(self).__name__,\n                              ','.join(sorted(kwds.keys()))))\n        if args:\n            raise ValueError(\"Unexpected arguments found \"\n                             \"while initializing '%s':\\n\\t%s\" %\n                             (type(self).__name__, ','.join(args)))\n\n        if not self.isDir(self._archive_name):\n            raise TypeError(\"Path not recognized as a directory: %s\" %\n                            (self._archive_name))\n\n        rootdir = self._archive_name\n        if self._subdir is not None:\n            rootdir = posixpath.join(rootdir, self._subdir)\n            if not os.path.exists(rootdir):\n                raise IOError(\n                    \"Subdirectory '%s' does not exists in root directory: %s\" %\n                    (self._subdir, self._archive_name))\n            self._names_list = self._walk(rootdir, maxdepth=self._maxdepth + 1)\n        else:\n            self._names_list = self._walk(rootdir, maxdepth=self._maxdepth)\n        self._fulldepth_names_list = self._walk(rootdir)",
  "def _walk(rootdir, maxdepth=None):\n        names_list = []\n        for root, dirs, files in os.walk(rootdir, topdown=True):\n            prefix = posixpath.relpath(ArchiveReader._posix_name(root), rootdir)\n            if prefix.endswith(_sep):\n                prefix = prefix[:-1]\n            if prefix == '.':\n                prefix = ''\n            for dname in dirs:\n                names_list.append(posixpath.join(prefix, dname))\n            if maxdepth is not None and prefix.count(_sep) >= maxdepth:\n                continue\n            for fname in files:\n                names_list.append(posixpath.join(prefix, fname))\n        return names_list",
  "def _extractImp(self, absolute_name, relative_name, path, recursive):\n\n        assert absolute_name is not None\n\n        if path is not None:\n            dst = posixpath.join(path, relative_name)\n        else:\n            dst = posixpath.join(self._workdir, absolute_name)\n        src = posixpath.join(self._archive_name, absolute_name)\n        children = []\n        if os.path.isdir(src):\n            if recursive:\n                ignores = []\n                for child in self._walk(src):\n                    rname = posixpath.join(relative_name, child)\n                    if rname in self._names_list:\n                        children.append(rname)\n                    else:\n                        ignores.append(posixpath.join(src, child))\n                self._copytree(src, dst, ignores=ignores)\n\n            elif not os.path.exists(dst):\n                os.makedirs(dst)\n            # if not recursive and the destination is an existing\n            # directory, then there is nothing to do\n        else:\n            if not os.path.exists(posixpath.dirname(dst)):\n                os.makedirs(os.path.dirname(dst))\n            shutil.copy2(src, dst)\n\n        return dst, children",
  "def _openImp(self, absolute_name, relative_name, *args, **kwds):\n        assert absolute_name is not None\n        return open(\n            posixpath.join(self._archive_name, absolute_name), 'rb', *args, **\n            kwds)",
  "def _extract_name(self, name):\n        return name",
  "def _init(self):\n        assert (self._abspath is not None)\n        assert (self._basename is not None)\n        assert (self._archive_name is not None)\n\n        if self._subdir is not None:\n            raise ValueError(\"'subdir' keyword option is not handled by \"\n                             \"'%s'\" % (type(self).__name__))\n        if self._maxdepth is not None:\n            raise ValueError(\"'maxdepth' keyword option is not handled by \"\n                             \"'%s'\" % (type(self).__name__))\n\n        if not self.isFile(self._archive_name):\n            raise TypeError(\"Path does not point to a file: %s\" %\n                            (self._archive_name))\n        extract_name = self._extract_name(self._basename)\n        if extract_name is not None:\n            self._names_list = [extract_name]\n            self._fulldepth_names_list = [extract_name]\n            self._extract_name = extract_name\n        else:\n            self._names_list = [self._basename]\n            self._fulldepth_names_list = [self._basename]\n            self._extract_name = None",
  "def _extractImp(self, absolute_name, relative_name, path, recursive):\n        assert absolute_name == self._extract_name\n        assert relative_name == self._extract_name\n        if recursive:\n            raise ValueError(\"Recursive extraction does not make \"\n                             \"sense for compressed file archive types\")\n        if path is not None:\n            dst = posixpath.join(path, relative_name)\n        else:\n            dst = posixpath.join(self._workdir, absolute_name)\n        with open(dst, 'wb') as dstf:\n            handler = self._handler_class(self._archive_name, 'rb')\n            shutil.copyfileobj(handler, dstf)\n            handler.close()\n\n        return dst, []",
  "def _openImp(self, absolute_name, relative_name, *args, **kwds):\n        assert absolute_name == self._extract_name\n        return self._handler_class(self._archive_name, 'rb', *args, **kwds)",
  "def _extract_name(self, name):\n        # see the man page for gzip\n        basename, ext = os.path.splitext(name)\n        if ext == self._suffix:\n            return basename\n        else:\n            return None",
  "def __init__(self, *args, **kwds):\n        if not gzip_available:\n            raise ImportError(\"gzip support is disabled\")\n        self._suffix = kwds.pop('suffix', '.gz')\n        super(GzipFileArchiveReader, self).__init__(*args, **kwds)\n        if not self.isGzipFile(self._archive_name):\n            raise TypeError(\"Unrecognized gzip format for file: %s\" %\n                            (self._archive_name))",
  "def _extractImp(self, absolute_name, relative_name, path, recursive):\n        if self._extract_name is None:\n            raise TypeError(\n                \"Extraction disabled. File suffix %s does not \"\n                \"match expected suffix for compression type %s. \"\n                \"The default suffix can be changed by passing \"\n                \"'suffix=.<ext>' into the ArchiveReader constructor.\" % (\n                    os.path.splitext(self._basename)[1], self._suffix))\n        return FileArchiveReader._extractImp(self, absolute_name, relative_name,\n                                             path, recursive)",
  "def _extract_name(self, name):\n        # see the man page for bzip2\n        basename, ext = os.path.splitext(name)\n        if ext in ('.bz2', '.bz'):\n            return basename\n        elif ext in ('.tbz2', '.tbz'):\n            return basename + '.tar'\n        else:\n            return name + '.out'",
  "def __init__(self, *args, **kwds):\n        if not bz2_available:\n            raise ImportError(\"bz2 support is disabled\")\n        super(BZ2FileArchiveReader, self).__init__(*args, **kwds)\n        if not self.isBZ2File(self._archive_name):\n            raise TypeError(\"Unrecognized bzip2 format for file: %s\" %\n                            (self._archive_name))",
  "class BasicSymbolMap:\n\n    def __init__(self):\n\n        # maps object id()s to their assigned symbol.\n        self.byObject = {}\n\n        # maps assigned symbols to the corresponding objects.\n        self.bySymbol = {}\n\n    def getByObjectDictionary(self):\n        return self.byObject\n\n    def updateSymbols(self, data_stream):\n        # check if the input is a generator / iterator,\n        # if so, we need to copy since we use it twice\n        if hasattr(data_stream, '__iter__') and \\\n           not hasattr(data_stream, '__len__'):\n            obj_symbol_tuples = list(obj_symbol_tuples)\n        self.byObject.update((id(obj), label) for obj,label in data_stream)\n        self.bySymbol.update((label,obj) for obj,label in data_stream)\n\n    def createSymbol(self, obj ,label):\n        self.byObject[id(obj)] = label\n        self.bySymbol[label] = obj\n\n    def addSymbol(self, obj, label):\n        self.byObject[id(obj)] = label\n        self.bySymbol[label] = obj\n\n    def getSymbol(self, obj):\n        return self.byObject[id(obj)]\n\n    def getObject(self, label):\n        return self.bySymbol[label]\n\n    def pprint(self, **kwds):\n        print(\"BasicSymbolMap:\")\n        lines = [repr(label)+\" <-> \"+obj.name+\" (id=\"+str(id(obj))+\")\"\n                 for label, obj in self.bySymbol.items()]\n        print('\\n'.join(sorted(lines)))\n        print(\"\")",
  "def indexToString(index):\n\n    if index is None:\n        return ''\n\n    # if the input type is a string or an int, then this isn't a tuple!\n    # TODO: Why aren't we just checking for tuple?\n    if isinstance(index, _nontuple):\n        return \"[\"+str(index)+\"]\"\n\n    result = \"[\"\n    for i in range(0,len(index)):\n        result += str(index[i])\n        if i != len(index) - 1:\n            result += \",\"\n    result += \"]\"\n    return result",
  "def isVariableNameIndexed(variable_name):\n\n    left_bracket_count = variable_name.count('[')\n    right_bracket_count = variable_name.count(']')\n\n    if (left_bracket_count == 1) and (right_bracket_count == 1):\n        return True\n    elif (left_bracket_count == 1) or (right_bracket_count == 1):\n        raise ValueError(\"Illegally formed variable name=\"+variable_name+\"; if indexed, variable names must contain matching left and right brackets\")\n    else:\n        return False",
  "def extractVariableNameAndIndex(variable_name):\n\n    if not isVariableNameIndexed(variable_name):\n        raise ValueError(\n            \"Non-indexed variable name passed to \"\n            \"function extractVariableNameAndIndex()\")\n\n    pieces = variable_name.split('[')\n    name = pieces[0].strip()\n    full_index = pieces[1].rstrip(']')\n\n    # even nested tuples in pyomo are \"flattened\" into\n    # one-dimensional tuples. to accomplish flattening\n    # replace all parens in the string with commas and\n    # proceed with the split.\n    full_index = full_index.replace(\"(\",\",\").replace(\")\",\",\")\n    indices = full_index.split(',')\n\n    return_index = ()\n\n    for index in indices:\n\n        # unlikely, but strip white-space from the string.\n        index=index.strip()\n\n        # if the tuple contains nested tuples, then the nested\n        # tuples have single quotes - \"'\" characters - around\n        # strings. remove these, as otherwise you have an\n        # illegal index.\n        index = index.replace(\"\\'\",\"\")\n\n        # if the index is an integer, make it one!\n        transformed_index = None\n        try:\n            transformed_index = int(index)\n        except ValueError:\n            transformed_index = index\n        return_index = return_index + (transformed_index,)\n\n    # IMPT: if the tuple is a singleton, return the element itself.\n    if len(return_index) == 1:\n        return name, return_index[0]\n    else:\n        return name, return_index",
  "def extractComponentIndices(component, index_template):\n\n    component_index_dimension = component.dim()\n\n    # do special handling for the case where the component is\n    # not indexed, i.e., of dimension 0. for scalar components,\n    # the match template can be the empty string, or - more\n    # commonly, given that the empty string is hard to specify\n    # in the scenario tree input data - a single wildcard character.\n    if component_index_dimension == 0:\n       if (index_template != '') and (index_template != \"*\"):\n          raise RuntimeError(\n              \"Index template=%r specified for scalar object=%s\"\n              % (index_template, component.name))\n       return [None]\n\n    # from this point on, we're dealing with an indexed component.\n    if index_template == \"\":\n        return [ndx for ndx in component]\n\n    # if the input index template is not a tuple, make it one.\n    # one-dimensional indices in pyomo are not tuples, but\n    # everything else is.\n    if type(index_template) != tuple:\n        index_template = (index_template,)\n\n    if component_index_dimension != len(index_template):\n        raise RuntimeError(\n            \"The dimension of index template=%s (%s) does match \"\n            \"the dimension of component=%s (%s)\"\n            % (index_template,\n               len(index_template),\n               component.name,\n               component_index_dimension))\n\n    # cache for efficiency\n    iterator_range = [i for i,match_str in enumerate(index_template)\n                      if match_str != '*']\n\n    if len(iterator_range) == 0:\n        return list(component)\n    elif len(iterator_range) == component_index_dimension:\n        if (len(index_template) == 1) and \\\n           (index_template[0] in component):\n            return index_template\n        elif index_template in component:\n            return [index_template]\n        else:\n            raise ValueError(\n                \"The index %s is not valid for component named: %s\"\n                % (str(tuple(index_template)), component.name))\n\n    result = []\n\n    for index in component:\n\n        # if the input index is not a tuple, make it one for processing\n        # purposes. however, return the original index always.\n        if component_index_dimension == 1:\n           modified_index = (index,)\n        else:\n           modified_index = index\n\n        match_found = True # until proven otherwise\n        for i in iterator_range:\n            if index_template[i] != modified_index[i]:\n                match_found = False\n                break\n\n        if match_found is True:\n            result.append(index)\n\n    return result",
  "def __init__(self):\n\n        # maps object id()s to their assigned symbol.\n        self.byObject = {}\n\n        # maps assigned symbols to the corresponding objects.\n        self.bySymbol = {}",
  "def getByObjectDictionary(self):\n        return self.byObject",
  "def updateSymbols(self, data_stream):\n        # check if the input is a generator / iterator,\n        # if so, we need to copy since we use it twice\n        if hasattr(data_stream, '__iter__') and \\\n           not hasattr(data_stream, '__len__'):\n            obj_symbol_tuples = list(obj_symbol_tuples)\n        self.byObject.update((id(obj), label) for obj,label in data_stream)\n        self.bySymbol.update((label,obj) for obj,label in data_stream)",
  "def createSymbol(self, obj ,label):\n        self.byObject[id(obj)] = label\n        self.bySymbol[label] = obj",
  "def addSymbol(self, obj, label):\n        self.byObject[id(obj)] = label\n        self.bySymbol[label] = obj",
  "def getSymbol(self, obj):\n        return self.byObject[id(obj)]",
  "def getObject(self, label):\n        return self.bySymbol[label]",
  "def pprint(self, **kwds):\n        print(\"BasicSymbolMap:\")\n        lines = [repr(label)+\" <-> \"+obj.name+\" (id=\"+str(id(obj))+\")\"\n                 for label, obj in self.bySymbol.items()]\n        print('\\n'.join(sorted(lines)))\n        print(\"\")",
  "def _extract_pathspec(\n        pathspec,\n        default_basename,\n        archives=None):\n    \"\"\"Obtain a file location from a pathspec.\n\n    Extracts a file location from the provided input\n    path specification by normalizing the name or by\n    opening an archive reader.\n\n    Args:\n        pathspec (str): The path specification. This can\n            be a standard path to a file or represent a\n            file contained within an archive. In the\n            case of an archived file, the input string\n            consist of two parts separated by a comma,\n            where the first part represents the path to\n            the archive and the second part represents\n            the relative path to a file or directory\n            within that archive.\n        default_basename (str): The default filename to\n            search for when the pathspec represents a\n            directory (or a directory within an\n            archive). This name must have an extension,\n            which is used by this function to interpret\n            whether the pathspec ends in a filename or a\n            directory name. If this argument is None, the\n            function will attempt to extract a directory\n            name instead of a file.\n        archives (list): A list of currently open\n            archive readers to check before opening a\n            new archive. If a new archive is opened, it will\n            be appended to this list.\n\n    Returns:\n        A tuple consisting of the normalized absolute\n        path to the file followed by the current list of\n        open archives that can be passed into this function\n        the next time it is called.\n    \"\"\"\n\n    logger.debug(\"expanding pathspec %s to %s\"\n                 % (pathspec, os.path.expanduser(pathspec)))\n    pathspec = os.path.expanduser(pathspec)\n\n    if archives is None:\n        archives = []\n\n    filename = None\n    normalized_location = None\n    archive = None\n    archive_subdir = None\n    unarchived_dir = None\n    basename = None\n\n    if not os.path.exists(pathspec):\n        logger.debug(\"pathspec does not exist, normalizing name\")\n        (normalized_location, _, archive_subdir) = \\\n            ArchiveReader.normalize_name(pathspec).rpartition(',')\n        if default_basename is not None:\n            extension = os.path.splitext(default_basename)[1].strip()\n            assert extension != ''\n            if archive_subdir.endswith(extension):\n                logger.debug(\"recognized extension type '%s' appears \"\n                             \"after comma, treating as file\" % (extension))\n                basename = os.path.basename(archive_subdir)\n                archive_subdir = os.path.dirname(archive_subdir).strip()\n        if archive_subdir == '':\n            archive_subdir = None\n    else:\n        logger.debug(\"pathspec exists, normalizing name\")\n        normalized_location = \\\n            ArchiveReader.normalize_name(pathspec)\n\n    logger.debug(\"normalized pathspec: (%s, %s, %s)\"\n                 % (normalized_location, archive_subdir, basename))\n    if ArchiveReader.isArchivedFile(normalized_location):\n        logger.debug(\"pathspec defines a recognized archive type\")\n        for prev_archive_inputs, prev_archive, prev_unarchived_dir \\\n              in archives:\n            if (normalized_location == \\\n                prev_archive_inputs[0]) and \\\n                ((prev_archive_inputs[1] is None) or \\\n                 ((archive_subdir is not None) and \\\n                  (archive_subdir.startswith(prev_archive_inputs[1]+'/')))):\n                logger.debug(\"pathspec matches previous archive\")\n                unarchived_dir = prev_unarchived_dir\n                if archive_subdir is not None:\n                    if prev_archive_inputs[1] is not None:\n                        unarchived_dir = posixpath.join(\n                            unarchived_dir,\n                            os.path.relpath(archive_subdir,\n                                            start=prev_archive_inputs[1]))\n                    else:\n                        unarchived_dir = posixpath.join(unarchived_dir,\n                                                        archive_subdir)\n                logger.debug(\"unarchived directory: %s\" % (unarchived_dir))\n                break\n        else: # if no break occurs in previous for-loop\n            archive = ArchiveReaderFactory(\n                normalized_location,\n                subdir=archive_subdir)\n            unarchived_dir = archive.normalize_name(\n                tempfile.mkdtemp(prefix='pysp_unarchived'))\n            archives.append(((normalized_location, archive_subdir),\n                             archive,\n                             unarchived_dir))\n            logger.debug(\"New archive opened. Temporary archive \"\n                         \"extraction directory: %s\" % (unarchived_dir))\n            archive.extractall(path=unarchived_dir)\n        if basename is not None:\n            filename = posixpath.join(unarchived_dir, basename)\n        elif default_basename is not None:\n            filename = posixpath.join(unarchived_dir, default_basename)\n        else:\n            filename = unarchived_dir\n        logger.debug(\"extracted filename: %s\" % (filename))\n    else:\n        logger.debug(\"pathspec defines a standard path\")\n        if archive_subdir is not None:\n            unarchived_dir = posixpath.join(normalized_location,\n                                            archive_subdir)\n        else:\n            unarchived_dir = normalized_location\n\n        if not os.path.isfile(unarchived_dir):\n            if basename is not None:\n                filename = posixpath.join(unarchived_dir, basename)\n            elif default_basename is not None:\n                filename = posixpath.join(unarchived_dir, default_basename)\n            else:\n                filename = unarchived_dir\n        else:\n            filename = unarchived_dir\n\n    return filename, archives",
  "def _find_reference_model_or_callback(src):\n    \"\"\"Tries to find a single reference model or callback for\n    generating scenario models.\"\"\"\n    module = import_file(src, clear_cache=True)\n    reference_model = None\n    callback = None\n    dir_module = dir(module)\n    if \"pysp_instance_creation_callback\" in dir_module:\n        callback = getattr(module, \"pysp_instance_creation_callback\")\n        if not hasattr(callback,\"__call__\"):\n            raise TypeError(\"'pysp_instance_creation_callback' \"\n                            \"object found in source '%s' is not \"\n                            \"callable\" % (src))\n    else:\n        matching_names = []\n        for attr_name in dir_module:\n            obj = getattr(module, attr_name)\n            if isinstance(obj, (_BlockData, Block)):\n                reference_model = obj\n                matching_names.append(attr_name)\n        if len(matching_names) > 1:\n            raise ValueError(\"Multiple objects found in source '%s' \"\n                             \"that could be a reference model. Make \"\n                             \"sure there is only one Pyomo model in \"\n                             \"the source file. Object names: %s\"\n                             % (str(matching_names)))\n\n    return module, reference_model, callback",
  "def _find_scenariotree(src=None, module=None):\n    \"\"\"Tries to find a single reference model or callback for\n    generating scenario models.\"\"\"\n    if module is None:\n        assert src is not None\n        module = import_file(src, clear_cache=True)\n    else:\n        assert src is None\n    scenario_tree_object = None\n    scenario_tree_model = None\n    callback = None\n    dir_module = dir(module)\n    if \"pysp_scenario_tree_model_callback\" in dir_module:\n        callback = getattr(module, \"pysp_scenario_tree_model_callback\")\n        if not hasattr(callback,\"__call__\"):\n            raise TypeError(\"'pysp_scenario_tree_model_callback' \"\n                            \"object found in source '%s' is not \"\n                            \"callable\" % (src))\n        attrs = [(None, callback())]\n    else:\n        attrs = [(attr_name,getattr(module, attr_name))\n                 for attr_name in dir_module]\n    matching_names = []\n    for attr_name, obj in attrs:\n        if isinstance(obj, ScenarioTree):\n            scenario_tree_object = obj\n            matching_names.append(attr_name)\n        elif isinstance(obj, (_BlockData, Block)):\n            scenario_tree_model = obj\n            matching_names.append(attr_name)\n        elif has_networkx and \\\n             isinstance(obj, networkx.DiGraph):\n            scenario_tree_model = obj\n            matching_names.append(attr_name)\n    if len(matching_names) > 1:\n        raise ValueError(\"Multiple objects found in source '%s' \"\n                         \"that could act as a scenario tree \"\n                         \"specification. Make sure there is only \"\n                         \"one Pyomo model, ScenarioTree, or \"\n                         \"networkx.DiGraph object in the source \"\n                         \"file. Object names: %s\"\n                         % (str(matching_names)))\n\n    return module, scenario_tree_object, scenario_tree_model",
  "class ScenarioTreeInstanceFactory:\n\n    def __init__(self,\n                 model,\n                 scenario_tree,\n                 data=None):\n        \"\"\"Class to help manage construction of scenario tree models.\n\n        This class is designed to help manage the various input formats\n        that that are accepted by PySP and provide a unified interface\n        for building scenario trees that are paired with a set of\n        concrete Pyomo models.\n\n        Args:\n            model: The reference scenario model. Can be set\n                to Pyomo model or the name of a file\n                containing a Pyomo model. For historical\n                reasons, this argument can also be set to a\n                directory name where it is assumed a file\n                named ReferenceModel.py exists.\n            scenario_tree: The scenario tree. Can be set to\n                a Pyomo model, a file containing a Pyomo\n                model, or a .dat file containing data for an\n                abstract scenario tree model representation,\n                which defines the structure of the scenario\n                tree. It can also be a .py file that\n                contains a networkx scenario tree or a\n                networkx scenario tree object.  For\n                historical reasons, this argument can also\n                be set to a directory name where it is\n                assumed a file named ScenarioStructure.dat\n                exists.\n            data: Directory containing .dat files necessary\n                for building the scenario instances\n                associated with the scenario tree. This\n                argument is required if no directory\n                information can be extracted from the first\n                two arguments and the reference model is an\n                abstract Pyomo model. Otherwise, it is not\n                required or the location will be inferred\n                from the scenario tree location (first) or\n                from the reference model location (second),\n                where it is assumed the data files reside in\n                the same directory.\n        \"\"\"\n        self._closed = True\n\n        self._archives = []\n\n        self._model_filename = None\n        self._model_module = None\n        self._model_object = None\n        self._model_callback = None\n        self._scenario_tree_filename = None\n        self._scenario_tree_module = None\n        self._scenario_tree_model = None\n        self._scenario_tree = None\n        self._data_directory = None\n        try:\n            self._init(model, scenario_tree, data)\n        except:\n            self.close()\n            raise\n        self._closed = False\n\n    def _init(self, model, scenario_tree, data):\n\n        self._model_filename = None\n        self._model_module = None\n        self._model_object = None\n        self._model_callback = None\n        if isinstance(model, str):\n            logger.debug(\"A model filename was provided.\")\n            self._model_filename, self._archives = \\\n                _extract_pathspec(model,\n                                  \"ReferenceModel.py\",\n                                  archives=self._archives)\n            if not os.path.exists(self._model_filename):\n                logger.error(\"Failed to extract reference model python file \"\n                             \"from path specification: %s\"\n                             % (model))\n                raise IOError(\"path does not exist: %s\"\n                              % (self._model_filename))\n            assert self._model_filename is not None\n            assert self._model_filename.endswith(\".py\")\n            (self._model_module,\n             self._model_object,\n             self._model_callback) = \\\n                _find_reference_model_or_callback(self._model_filename)\n            if (self._model_object is None) and \\\n               (self._model_callback is None):\n                raise AttributeError(\n                    \"No reference Pyomo model or \"\n                    \"'pysp_instance_creation_callback' \"\n                    \"function object found in src: %s\"\n                    % (self._model_filename))\n        elif hasattr(model, \"__call__\"):\n            logger.debug(\"A model callback function was provided.\")\n            self._model_callback = model\n        else:\n            if not isinstance(model, (_BlockData, Block)):\n                raise TypeError(\n                    \"model argument object has incorrect type: %s. \"\n                    \"Must be a string type, a callback, or a Pyomo model.\"\n                    % (type(model)))\n            logger.debug(\"A model object was provided.\")\n            self._model_object = model\n\n        self._scenario_tree_filename = None\n        self._scenario_tree_model = None\n        self._scenario_tree = None\n        if isinstance(scenario_tree, ScenarioTree):\n            for scenario in scenario_tree.scenarios:\n                if scenario.instance is not None:\n                    raise ValueError(\n                        \"The scenario tree can not be linked with instances\")\n            if hasattr(scenario_tree, \"_scenario_instance_factory\"):\n                del scenario_tree._scenario_instance_factory\n            self._scenario_tree = scenario_tree\n        elif has_networkx and \\\n             isinstance(scenario_tree, networkx.DiGraph):\n            self._scenario_tree_model = scenario_tree\n        elif isinstance(scenario_tree, str):\n            logger.debug(\"scenario tree input is a string, attempting \"\n                         \"to load file specification: %s\"\n                         % (scenario_tree))\n            self._scenario_tree_filename = None\n            if not scenario_tree.endswith(\".py\"):\n                self._scenario_tree_filename, self._archives = \\\n                    _extract_pathspec(scenario_tree,\n                                      \"ScenarioStructure.dat\",\n                                      archives=self._archives)\n                if not os.path.exists(self._scenario_tree_filename):\n                    logger.debug(\"Failed to extract scenario tree structure \"\n                                 \".dat file from path specification: %s\"\n                                 % (scenario_tree))\n                    self._scenario_tree_filename = None\n            if self._scenario_tree_filename is None:\n                self._scenario_tree_filename, self._archives = \\\n                    _extract_pathspec(scenario_tree,\n                                      \"ScenarioStructure.py\",\n                                      archives=self._archives)\n                if not os.path.exists(self._scenario_tree_filename):\n                    logger.debug(\"Failed to locate scenario tree structure \"\n                                 \".py file with path specification: %s\"\n                                 % (scenario_tree))\n                    self._scenario_tree_filename = None\n            if self._scenario_tree_filename is None:\n                raise ValueError(\"Failed to extract scenario tree structure \"\n                                 \"file with .dat or .py extension from path \"\n                                 \"specification: %s\" % (scenario_tree))\n            elif self._scenario_tree_filename.endswith(\".py\"):\n                if self._scenario_tree_filename == self._model_filename:\n                    # try not to clobber the model import\n                    (self._scenario_tree_module,\n                     self._scenario_tree,\n                     self._scenario_tree_model) = \\\n                        _find_scenariotree(module=self._model_module)\n                else:\n                    (self._scenario_tree_module,\n                     self._scenario_tree,\n                     self._scenario_tree_model) = \\\n                        _find_scenariotree(src=self._scenario_tree_filename)\n                if (self._scenario_tree is None) and \\\n                   (self._scenario_tree_model is None):\n                    raise AttributeError(\n                        \"No scenario tree or \"\n                        \"'pysp_scenario_tree_model_callback' \"\n                        \"function found in src: %s\"\n                        % (self._scenario_tree_filename))\n            elif self._scenario_tree_filename.endswith(\".dat\"):\n                self._scenario_tree_model = \\\n                    CreateAbstractScenarioTreeModel().\\\n                    create_instance(filename=self._scenario_tree_filename)\n            else:\n                assert False\n        elif scenario_tree is None:\n            if self._model_module is not None:\n                self._scenario_tree_filename = self._model_filename\n                (self._scenario_tree_module,\n                 self._scenario_tree,\n                 self._scenario_tree_model) = \\\n                    _find_scenariotree(module=self._model_module)\n                if (self._scenario_tree is None) and \\\n                   (self._scenario_tree_model is None):\n                    raise ValueError(\n                        \"No input was provided for the scenario tree \"\n                        \"and no callback or scenario tree object was \"\n                        \"found with the model\")\n            else:\n                raise ValueError(\n                    \"No input was provided for the scenario tree \"\n                    \"but there is no module to search for a \"\n                    \"'pysp_scenario_tree_model_callback' function \"\n                    \"or a ScenarioTree object.\")\n        else:\n            self._scenario_tree_model = scenario_tree\n\n        if self._scenario_tree is None:\n            if (not isinstance(self._scenario_tree_model,\n                               (_BlockData, Block))) and \\\n               ((not has_networkx) or \\\n                (not isinstance(self._scenario_tree_model,\n                                networkx.DiGraph))):\n                raise TypeError(\n                    \"scenario tree model object has incorrect type: %s. \"\n                    \"Must be a string type,  Pyomo model, or a \"\n                    \"networkx.DiGraph object.\" % (type(scenario_tree)))\n            if isinstance(self._scenario_tree_model, (_BlockData, Block)):\n                if not self._scenario_tree_model.is_constructed():\n                    raise ValueError(\n                        \"scenario tree model is not constructed\")\n\n        self._data_directory = None\n        if data is None:\n            if self.scenario_tree_directory() is not None:\n                logger.debug(\"data directory is set to the scenario tree \"\n                             \"directory: %s\"\n                             % (self.scenario_tree_directory()))\n                self._data_directory = self.scenario_tree_directory()\n            elif self.model_directory() is not None:\n                logger.debug(\"data directory is set to the reference model \"\n                             \"directory: %s\"\n                             % (self.model_directory()))\n                self._data_directory = self.model_directory()\n            else:\n                if (self._model_callback is None) and \\\n                   isinstance(self._model_object, AbstractModel) and \\\n                   (not self._model_object.is_constructed()):\n                    raise ValueError(\n                        \"A data location is required since no model \"\n                        \"callback was provided and no other location could \"\n                        \"be inferred.\")\n                logger.debug(\"no data directory is required\")\n        else:\n            logger.debug(\"data location is provided, attempting \"\n                         \"to load specification: %s\"\n                         % (data))\n            self._data_directory, self._archives = \\\n                _extract_pathspec(data,\n                                  None,\n                                  archives=self._archives)\n            if not os.path.exists(self._data_directory):\n                logger.error(\"Failed to extract data directory \"\n                             \"from path specification: %s\"\n                             % (data))\n                raise IOError(\"path does not exist: %s\"\n                              % (self._data_directory))\n\n    def __getstate__(self):\n        self.close()\n        raise NotImplementedError(\"Do not deepcopy or serialize this class\")\n\n    def __setstate__(self,d):\n        self.close()\n        raise NotImplementedError(\"Do not deepcopy or serialize this class\")\n\n    def close(self):\n        for _,archive,tmpdir in self._archives:\n            if os.path.exists(tmpdir):\n                shutil.rmtree(tmpdir, True)\n            archive.close()\n        self._archives = []\n        self._closed = True\n\n    #\n    # Support \"with\" statements. Forgetting to call close()\n    # on this class can result in temporary unarchived\n    # directories being left sitting around\n    #\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n\n    def model_directory(self):\n        if self._model_filename is not None:\n            return os.path.dirname(self._model_filename)\n        else:\n            return None\n\n    def scenario_tree_directory(self):\n        if self._scenario_tree_filename is not None:\n            return os.path.dirname(self._scenario_tree_filename)\n        else:\n            return None\n\n    def data_directory(self):\n        return self._data_directory\n\n    #\n    # construct a scenario instance - just like it sounds!\n    #\n    def construct_scenario_instance(self,\n                                    scenario_name,\n                                    scenario_tree,\n                                    profile_memory=False,\n                                    output_instance_construction_time=False,\n                                    compile_instance=False,\n                                    verbose=False):\n        assert not self._closed\n        if not scenario_tree.contains_scenario(scenario_name):\n            raise ValueError(\"ScenarioTree does not contain scenario \"\n                             \"with name %s.\" % (scenario_name))\n\n        scenario = scenario_tree.get_scenario(scenario_name)\n        node_name_list = [n._name for n in scenario._node_list]\n\n        if verbose:\n            print(\"Creating instance for scenario=%s\" % (scenario_name))\n\n        scenario_instance = None\n\n        try:\n\n            if self._model_callback is not None:\n\n                assert self._model_object is None\n                try:\n                    _scenario_tree_arg = None\n                    # new callback signature\n                    if (self._scenario_tree_filename is not None) and \\\n                       self._scenario_tree_filename.endswith('.dat'):\n                        # we started with a .dat file, so\n                        # send the PySP scenario tree\n                        _scenario_tree_arg = scenario_tree\n                    elif self._scenario_tree_model is not None:\n                        # We started from a Pyomo\n                        # scenario tree model instance, or a\n                        # networkx tree.\n                        _scenario_tree_arg = self._scenario_tree_model\n                    else:\n                        # send the PySP scenario tree\n                        _scenario_tree_arg = scenario_tree\n                    scenario_instance = self._model_callback(_scenario_tree_arg,\n                                                             scenario_name,\n                                                             node_name_list)\n                except TypeError:\n                    # old callback signature\n                    # TODO:\n                    #logger.warning(\n                    #    \"DEPRECATED: The 'pysp_instance_creation_callback' function \"\n                    #    \"signature has changed. An additional argument should be \"\n                    #    \"added to the beginning of the arguments list that will be \"\n                    #    \"set to the user provided scenario tree object when called \"\n                    #    \"by PySP (e.g., a Pyomo scenario tree model instance, \"\n                    #    \"a networkx tree, or a PySP ScenarioTree object.\")\n                    scenario_instance = self._model_callback(scenario_name,\n                                                             node_name_list)\n\n            elif self._model_object is not None:\n\n                if (not isinstance(self._model_object, AbstractModel)) or \\\n                   (self._model_object.is_constructed()):\n                    scenario_instance = self._model_object.clone()\n                elif scenario_tree._scenario_based_data:\n                    assert self.data_directory() is not None\n                    scenario_data_filename = \\\n                        os.path.join(self.data_directory(),\n                                     str(scenario_name))\n                    # JPW: The following is a hack to support\n                    #      initialization of block instances, which\n                    #      don't work with .dat files at the\n                    #      moment. Actually, it's not that bad of a\n                    #      hack - it just needs to be extended a bit,\n                    #      and expanded into the node-based data read\n                    #      logic (where yaml is completely ignored at\n                    #      the moment.\n                    if os.path.exists(scenario_data_filename+'.dat'):\n                        scenario_data_filename = \\\n                            scenario_data_filename + \".dat\"\n                        data = None\n                    elif os.path.exists(scenario_data_filename+'.yaml'):\n                        if not yaml_available:\n                            raise ValueError(\n                                \"Found yaml data file for scenario '%s' \"\n                                \"but he PyYAML module is not available\"\n                                % (scenario_name))\n                        scenario_data_filename = \\\n                            scenario_data_filename+\".yaml\"\n                        with open(scenario_data_filename) as f:\n                            data = yaml.load(f, **yaml_load_args)\n                    else:\n                        raise RuntimeError(\n                            \"Cannot find a data file for scenario '%s' \"\n                            \"in directory: %s\\nRecognized formats: .dat, \"\n                            \".yaml\" % (scenario_name, self.data_directory()))\n                    if verbose:\n                        print(\"Data for scenario=%s loads from file=%s\"\n                              % (scenario_name, scenario_data_filename))\n                    if data is None:\n                        scenario_instance = \\\n                            self._model_object.create_instance(\n                                filename=scenario_data_filename,\n                                profile_memory=profile_memory,\n                                report_timing=output_instance_construction_time)\n                    else:\n                        scenario_instance = \\\n                            self._model_object.create_instance(\n                                data,\n                                profile_memory=profile_memory,\n                                report_timing=output_instance_construction_time)\n                else:\n                    assert self.data_directory() is not None\n                    data_files = []\n                    for node_name in node_name_list:\n                        node_data_filename = \\\n                            os.path.join(self.data_directory(),\n                                         str(node_name)+\".dat\")\n                        if not os.path.exists(node_data_filename):\n                            raise RuntimeError(\n                                \"Cannot find a data file for scenario tree \"\n                                \"node '%s' in directory: %s\\nRecognized \"\n                                \"formats: .dat\" % (node_name,\n                                                   self.data_directory()))\n                        data_files.append(node_data_filename)\n\n                    scenario_data = DataPortal(model=self._model_object)\n                    for data_file in data_files:\n                        if verbose:\n                            print(\"Node data for scenario=%s partially \"\n                                  \"loading from file=%s\"\n                                  % (scenario_name, data_file))\n                        scenario_data.load(filename=data_file)\n\n                    scenario_instance = self._model_object.create_instance(\n                        scenario_data,\n                        profile_memory=profile_memory,\n                        report_timing=output_instance_construction_time)\n            else:\n                raise RuntimeError(\"Unable to construct scenario instance. \"\n                                   \"Neither a reference model or callback \"\n                                   \"is defined.\")\n\n            # name each instance with the scenario name\n            scenario_instance._name = scenario_name\n\n            if compile_instance:\n                from pyomo.repn.beta.matrix import \\\n                    compile_block_linear_constraints\n                compile_block_linear_constraints(\n                    scenario_instance,\n                    \"_PySP_compiled_linear_constraints\",\n                    verbose=verbose)\n\n        except:\n            logger.error(\"Failed to create model instance for scenario=%s\"\n                         % (scenario_name))\n            raise\n\n        return scenario_instance\n\n    def construct_instances_for_scenario_tree(\n            self,\n            scenario_tree,\n            profile_memory=False,\n            output_instance_construction_time=False,\n            compile_scenario_instances=False,\n            verbose=False):\n        assert not self._closed\n\n        if scenario_tree._scenario_based_data:\n            if verbose:\n                print(\"Scenario-based instance initialization enabled\")\n        else:\n            if verbose:\n                print(\"Node-based instance initialization enabled\")\n\n        scenario_instances = {}\n        for scenario in scenario_tree._scenarios:\n\n            # the construction of instances takes little overhead in terms\n            # of memory potentially lost in the garbage-collection sense\n            # (mainly only that due to parsing and instance\n            # simplification/prep-processing).  to speed things along,\n            # disable garbage collection if it enabled in the first place\n            # through the instance construction process.\n            # IDEA: If this becomes too much for truly large numbers of\n            #       scenarios, we could manually collect every time X\n            #       instances have been created.\n            scenario_instance = None\n            with PauseGC() as pgc:\n                scenario_instance = \\\n                    self.construct_scenario_instance(\n                        scenario._name,\n                        scenario_tree,\n                        profile_memory=profile_memory,\n                        output_instance_construction_time=output_instance_construction_time,\n                        compile_instance=compile_scenario_instances,\n                        verbose=verbose)\n\n            scenario_instances[scenario._name] = scenario_instance\n            assert scenario_instance.local_name == scenario.name\n\n        return scenario_instances\n\n    def generate_scenario_tree(self,\n                               downsample_fraction=1.0,\n                               include_scenarios=None,\n                               bundles=None,\n                               random_bundles=None,\n                               random_seed=None,\n                               verbose=True):\n\n        scenario_tree_model = self._scenario_tree_model\n        if scenario_tree_model is not None:\n            if has_networkx and \\\n               isinstance(scenario_tree_model, networkx.DiGraph):\n                scenario_tree_model = \\\n                    ScenarioTreeModelFromNetworkX(scenario_tree_model)\n            else:\n                assert isinstance(scenario_tree_model, (_BlockData, Block)), \\\n                    str(scenario_tree_model)+\" \"+str(type(scenario_tree_model))\n\n        if bundles is not None:\n            if isinstance(bundles, str):\n                if scenario_tree_model is None:\n                    raise ValueError(\n                        \"A bundles file can not be used when the \"\n                        \"scenario tree input was not a Pyomo \"\n                        \"model or ScenarioStructure.dat file.\")\n                logger.debug(\"attempting to locate bundle file for input: %s\"\n                             % (bundles))\n                # we interpret the scenario bundle specification in one of\n                # two ways. if the supplied name is a file, it is used\n                # directly. otherwise, it is interpreted as the root of a\n                # file with a .dat suffix to be found in the instance\n                # directory.\n                orig_input = bundles\n                if not bundles.endswith(\".dat\"):\n                    bundles = bundles+\".dat\"\n                bundles = os.path.expanduser(bundles)\n                if not os.path.exists(bundles):\n                    if self.data_directory() is None:\n                        raise ValueError(\n                            \"Could not locate bundle .dat file from input \"\n                            \"'%s'. Path does not exist and there is no data \"\n                            \"directory to search in.\" % (orig_input))\n                    bundles = os.path.join(self.data_directory(), bundles)\n                if not os.path.exists(bundles):\n                    raise ValueError(\"Could not locate bundle .dat file \"\n                                     \"from input '%s' as absolute path or \"\n                                     \"relative to data directory: %s\"\n                                     % (orig_input, self.data_directory()))\n\n                if verbose:\n                    print(\"Scenario tree bundle specification filename=%s\"\n                          % (bundles))\n\n                scenario_tree_model = scenario_tree_model.clone()\n                scenario_tree_model.Bundling = True\n                scenario_tree_model.Bundling._constructed = False\n                scenario_tree_model.Bundling._data.clear()\n                scenario_tree_model.Bundles.clear()\n                scenario_tree_model.Bundles._constructed = False\n                scenario_tree_model.Bundles._data.clear()\n                scenario_tree_model.BundleScenarios.clear()\n                scenario_tree_model.BundleScenarios._constructed = False\n                scenario_tree_model.BundleScenarios._data.clear()\n                scenario_tree_model.load(bundles)\n\n        #\n        # construct the scenario tree\n        #\n        if scenario_tree_model is not None:\n            scenario_tree = ScenarioTree(scenariotreeinstance=scenario_tree_model,\n                                         scenariobundlelist=include_scenarios)\n        else:\n            assert self._scenario_tree is not None\n            if include_scenarios is None:\n                scenario_tree = copy.deepcopy(self._scenario_tree)\n            else:\n                # note: any bundles will be lost\n                if self._scenario_tree.contains_bundles():\n                    raise ValueError(\n                        \"Can not compress a scenario tree that \"\n                        \"contains bundles\")\n                scenario_tree = self._scenario_tree.make_compressed(\n                    include_scenarios,\n                    normalize=True)\n\n        # compress/down-sample the scenario tree, if requested\n        if (downsample_fraction is not None) and \\\n           (downsample_fraction < 1.0):\n            scenario_tree.downsample(downsample_fraction,\n                                     random_seed,\n                                     verbose)\n\n        #\n        # create bundles from a dict, if requested\n        #\n        if bundles is not None:\n            if not isinstance(bundles, str):\n                if verbose:\n                    print(\"Adding bundles to scenario tree from \"\n                          \"user-specified dict\")\n                if scenario_tree.contains_bundles():\n                    if verbose:\n                        print(\"Scenario tree already contains bundles. \"\n                              \"All existing bundles will be removed.\")\n                    for bundle in list(scenario_tree.bundles):\n                        scenario_tree.remove_bundle(bundle.name)\n                for bundle_name in bundles:\n                    scenario_tree.add_bundle(bundle_name,\n                                             bundles[bundle_name])\n\n        #\n        # create random bundles, if requested\n        #\n        if (random_bundles is not None) and \\\n           (random_bundles > 0):\n            if bundles is not None:\n                raise ValueError(\"Cannot specify both random \"\n                                 \"bundles and a bundles specification\")\n\n            num_scenarios = len(scenario_tree._scenarios)\n            if random_bundles > num_scenarios:\n                raise ValueError(\"Cannot create more random bundles \"\n                                 \"than there are scenarios!\")\n\n            if verbose:\n                print(\"Creating \"+str(random_bundles)+\n                      \" random bundles using seed=\"\n                      +str(random_seed))\n\n            scenario_tree.create_random_bundles(random_bundles,\n                                                random_seed)\n\n        scenario_tree._scenario_instance_factory = self\n\n        return scenario_tree",
  "def __init__(self,\n                 model,\n                 scenario_tree,\n                 data=None):\n        \"\"\"Class to help manage construction of scenario tree models.\n\n        This class is designed to help manage the various input formats\n        that that are accepted by PySP and provide a unified interface\n        for building scenario trees that are paired with a set of\n        concrete Pyomo models.\n\n        Args:\n            model: The reference scenario model. Can be set\n                to Pyomo model or the name of a file\n                containing a Pyomo model. For historical\n                reasons, this argument can also be set to a\n                directory name where it is assumed a file\n                named ReferenceModel.py exists.\n            scenario_tree: The scenario tree. Can be set to\n                a Pyomo model, a file containing a Pyomo\n                model, or a .dat file containing data for an\n                abstract scenario tree model representation,\n                which defines the structure of the scenario\n                tree. It can also be a .py file that\n                contains a networkx scenario tree or a\n                networkx scenario tree object.  For\n                historical reasons, this argument can also\n                be set to a directory name where it is\n                assumed a file named ScenarioStructure.dat\n                exists.\n            data: Directory containing .dat files necessary\n                for building the scenario instances\n                associated with the scenario tree. This\n                argument is required if no directory\n                information can be extracted from the first\n                two arguments and the reference model is an\n                abstract Pyomo model. Otherwise, it is not\n                required or the location will be inferred\n                from the scenario tree location (first) or\n                from the reference model location (second),\n                where it is assumed the data files reside in\n                the same directory.\n        \"\"\"\n        self._closed = True\n\n        self._archives = []\n\n        self._model_filename = None\n        self._model_module = None\n        self._model_object = None\n        self._model_callback = None\n        self._scenario_tree_filename = None\n        self._scenario_tree_module = None\n        self._scenario_tree_model = None\n        self._scenario_tree = None\n        self._data_directory = None\n        try:\n            self._init(model, scenario_tree, data)\n        except:\n            self.close()\n            raise\n        self._closed = False",
  "def _init(self, model, scenario_tree, data):\n\n        self._model_filename = None\n        self._model_module = None\n        self._model_object = None\n        self._model_callback = None\n        if isinstance(model, str):\n            logger.debug(\"A model filename was provided.\")\n            self._model_filename, self._archives = \\\n                _extract_pathspec(model,\n                                  \"ReferenceModel.py\",\n                                  archives=self._archives)\n            if not os.path.exists(self._model_filename):\n                logger.error(\"Failed to extract reference model python file \"\n                             \"from path specification: %s\"\n                             % (model))\n                raise IOError(\"path does not exist: %s\"\n                              % (self._model_filename))\n            assert self._model_filename is not None\n            assert self._model_filename.endswith(\".py\")\n            (self._model_module,\n             self._model_object,\n             self._model_callback) = \\\n                _find_reference_model_or_callback(self._model_filename)\n            if (self._model_object is None) and \\\n               (self._model_callback is None):\n                raise AttributeError(\n                    \"No reference Pyomo model or \"\n                    \"'pysp_instance_creation_callback' \"\n                    \"function object found in src: %s\"\n                    % (self._model_filename))\n        elif hasattr(model, \"__call__\"):\n            logger.debug(\"A model callback function was provided.\")\n            self._model_callback = model\n        else:\n            if not isinstance(model, (_BlockData, Block)):\n                raise TypeError(\n                    \"model argument object has incorrect type: %s. \"\n                    \"Must be a string type, a callback, or a Pyomo model.\"\n                    % (type(model)))\n            logger.debug(\"A model object was provided.\")\n            self._model_object = model\n\n        self._scenario_tree_filename = None\n        self._scenario_tree_model = None\n        self._scenario_tree = None\n        if isinstance(scenario_tree, ScenarioTree):\n            for scenario in scenario_tree.scenarios:\n                if scenario.instance is not None:\n                    raise ValueError(\n                        \"The scenario tree can not be linked with instances\")\n            if hasattr(scenario_tree, \"_scenario_instance_factory\"):\n                del scenario_tree._scenario_instance_factory\n            self._scenario_tree = scenario_tree\n        elif has_networkx and \\\n             isinstance(scenario_tree, networkx.DiGraph):\n            self._scenario_tree_model = scenario_tree\n        elif isinstance(scenario_tree, str):\n            logger.debug(\"scenario tree input is a string, attempting \"\n                         \"to load file specification: %s\"\n                         % (scenario_tree))\n            self._scenario_tree_filename = None\n            if not scenario_tree.endswith(\".py\"):\n                self._scenario_tree_filename, self._archives = \\\n                    _extract_pathspec(scenario_tree,\n                                      \"ScenarioStructure.dat\",\n                                      archives=self._archives)\n                if not os.path.exists(self._scenario_tree_filename):\n                    logger.debug(\"Failed to extract scenario tree structure \"\n                                 \".dat file from path specification: %s\"\n                                 % (scenario_tree))\n                    self._scenario_tree_filename = None\n            if self._scenario_tree_filename is None:\n                self._scenario_tree_filename, self._archives = \\\n                    _extract_pathspec(scenario_tree,\n                                      \"ScenarioStructure.py\",\n                                      archives=self._archives)\n                if not os.path.exists(self._scenario_tree_filename):\n                    logger.debug(\"Failed to locate scenario tree structure \"\n                                 \".py file with path specification: %s\"\n                                 % (scenario_tree))\n                    self._scenario_tree_filename = None\n            if self._scenario_tree_filename is None:\n                raise ValueError(\"Failed to extract scenario tree structure \"\n                                 \"file with .dat or .py extension from path \"\n                                 \"specification: %s\" % (scenario_tree))\n            elif self._scenario_tree_filename.endswith(\".py\"):\n                if self._scenario_tree_filename == self._model_filename:\n                    # try not to clobber the model import\n                    (self._scenario_tree_module,\n                     self._scenario_tree,\n                     self._scenario_tree_model) = \\\n                        _find_scenariotree(module=self._model_module)\n                else:\n                    (self._scenario_tree_module,\n                     self._scenario_tree,\n                     self._scenario_tree_model) = \\\n                        _find_scenariotree(src=self._scenario_tree_filename)\n                if (self._scenario_tree is None) and \\\n                   (self._scenario_tree_model is None):\n                    raise AttributeError(\n                        \"No scenario tree or \"\n                        \"'pysp_scenario_tree_model_callback' \"\n                        \"function found in src: %s\"\n                        % (self._scenario_tree_filename))\n            elif self._scenario_tree_filename.endswith(\".dat\"):\n                self._scenario_tree_model = \\\n                    CreateAbstractScenarioTreeModel().\\\n                    create_instance(filename=self._scenario_tree_filename)\n            else:\n                assert False\n        elif scenario_tree is None:\n            if self._model_module is not None:\n                self._scenario_tree_filename = self._model_filename\n                (self._scenario_tree_module,\n                 self._scenario_tree,\n                 self._scenario_tree_model) = \\\n                    _find_scenariotree(module=self._model_module)\n                if (self._scenario_tree is None) and \\\n                   (self._scenario_tree_model is None):\n                    raise ValueError(\n                        \"No input was provided for the scenario tree \"\n                        \"and no callback or scenario tree object was \"\n                        \"found with the model\")\n            else:\n                raise ValueError(\n                    \"No input was provided for the scenario tree \"\n                    \"but there is no module to search for a \"\n                    \"'pysp_scenario_tree_model_callback' function \"\n                    \"or a ScenarioTree object.\")\n        else:\n            self._scenario_tree_model = scenario_tree\n\n        if self._scenario_tree is None:\n            if (not isinstance(self._scenario_tree_model,\n                               (_BlockData, Block))) and \\\n               ((not has_networkx) or \\\n                (not isinstance(self._scenario_tree_model,\n                                networkx.DiGraph))):\n                raise TypeError(\n                    \"scenario tree model object has incorrect type: %s. \"\n                    \"Must be a string type,  Pyomo model, or a \"\n                    \"networkx.DiGraph object.\" % (type(scenario_tree)))\n            if isinstance(self._scenario_tree_model, (_BlockData, Block)):\n                if not self._scenario_tree_model.is_constructed():\n                    raise ValueError(\n                        \"scenario tree model is not constructed\")\n\n        self._data_directory = None\n        if data is None:\n            if self.scenario_tree_directory() is not None:\n                logger.debug(\"data directory is set to the scenario tree \"\n                             \"directory: %s\"\n                             % (self.scenario_tree_directory()))\n                self._data_directory = self.scenario_tree_directory()\n            elif self.model_directory() is not None:\n                logger.debug(\"data directory is set to the reference model \"\n                             \"directory: %s\"\n                             % (self.model_directory()))\n                self._data_directory = self.model_directory()\n            else:\n                if (self._model_callback is None) and \\\n                   isinstance(self._model_object, AbstractModel) and \\\n                   (not self._model_object.is_constructed()):\n                    raise ValueError(\n                        \"A data location is required since no model \"\n                        \"callback was provided and no other location could \"\n                        \"be inferred.\")\n                logger.debug(\"no data directory is required\")\n        else:\n            logger.debug(\"data location is provided, attempting \"\n                         \"to load specification: %s\"\n                         % (data))\n            self._data_directory, self._archives = \\\n                _extract_pathspec(data,\n                                  None,\n                                  archives=self._archives)\n            if not os.path.exists(self._data_directory):\n                logger.error(\"Failed to extract data directory \"\n                             \"from path specification: %s\"\n                             % (data))\n                raise IOError(\"path does not exist: %s\"\n                              % (self._data_directory))",
  "def __getstate__(self):\n        self.close()\n        raise NotImplementedError(\"Do not deepcopy or serialize this class\")",
  "def __setstate__(self,d):\n        self.close()\n        raise NotImplementedError(\"Do not deepcopy or serialize this class\")",
  "def close(self):\n        for _,archive,tmpdir in self._archives:\n            if os.path.exists(tmpdir):\n                shutil.rmtree(tmpdir, True)\n            archive.close()\n        self._archives = []\n        self._closed = True",
  "def __enter__(self):\n        return self",
  "def __exit__(self, type, value, traceback):\n        self.close()",
  "def model_directory(self):\n        if self._model_filename is not None:\n            return os.path.dirname(self._model_filename)\n        else:\n            return None",
  "def scenario_tree_directory(self):\n        if self._scenario_tree_filename is not None:\n            return os.path.dirname(self._scenario_tree_filename)\n        else:\n            return None",
  "def data_directory(self):\n        return self._data_directory",
  "def construct_scenario_instance(self,\n                                    scenario_name,\n                                    scenario_tree,\n                                    profile_memory=False,\n                                    output_instance_construction_time=False,\n                                    compile_instance=False,\n                                    verbose=False):\n        assert not self._closed\n        if not scenario_tree.contains_scenario(scenario_name):\n            raise ValueError(\"ScenarioTree does not contain scenario \"\n                             \"with name %s.\" % (scenario_name))\n\n        scenario = scenario_tree.get_scenario(scenario_name)\n        node_name_list = [n._name for n in scenario._node_list]\n\n        if verbose:\n            print(\"Creating instance for scenario=%s\" % (scenario_name))\n\n        scenario_instance = None\n\n        try:\n\n            if self._model_callback is not None:\n\n                assert self._model_object is None\n                try:\n                    _scenario_tree_arg = None\n                    # new callback signature\n                    if (self._scenario_tree_filename is not None) and \\\n                       self._scenario_tree_filename.endswith('.dat'):\n                        # we started with a .dat file, so\n                        # send the PySP scenario tree\n                        _scenario_tree_arg = scenario_tree\n                    elif self._scenario_tree_model is not None:\n                        # We started from a Pyomo\n                        # scenario tree model instance, or a\n                        # networkx tree.\n                        _scenario_tree_arg = self._scenario_tree_model\n                    else:\n                        # send the PySP scenario tree\n                        _scenario_tree_arg = scenario_tree\n                    scenario_instance = self._model_callback(_scenario_tree_arg,\n                                                             scenario_name,\n                                                             node_name_list)\n                except TypeError:\n                    # old callback signature\n                    # TODO:\n                    #logger.warning(\n                    #    \"DEPRECATED: The 'pysp_instance_creation_callback' function \"\n                    #    \"signature has changed. An additional argument should be \"\n                    #    \"added to the beginning of the arguments list that will be \"\n                    #    \"set to the user provided scenario tree object when called \"\n                    #    \"by PySP (e.g., a Pyomo scenario tree model instance, \"\n                    #    \"a networkx tree, or a PySP ScenarioTree object.\")\n                    scenario_instance = self._model_callback(scenario_name,\n                                                             node_name_list)\n\n            elif self._model_object is not None:\n\n                if (not isinstance(self._model_object, AbstractModel)) or \\\n                   (self._model_object.is_constructed()):\n                    scenario_instance = self._model_object.clone()\n                elif scenario_tree._scenario_based_data:\n                    assert self.data_directory() is not None\n                    scenario_data_filename = \\\n                        os.path.join(self.data_directory(),\n                                     str(scenario_name))\n                    # JPW: The following is a hack to support\n                    #      initialization of block instances, which\n                    #      don't work with .dat files at the\n                    #      moment. Actually, it's not that bad of a\n                    #      hack - it just needs to be extended a bit,\n                    #      and expanded into the node-based data read\n                    #      logic (where yaml is completely ignored at\n                    #      the moment.\n                    if os.path.exists(scenario_data_filename+'.dat'):\n                        scenario_data_filename = \\\n                            scenario_data_filename + \".dat\"\n                        data = None\n                    elif os.path.exists(scenario_data_filename+'.yaml'):\n                        if not yaml_available:\n                            raise ValueError(\n                                \"Found yaml data file for scenario '%s' \"\n                                \"but he PyYAML module is not available\"\n                                % (scenario_name))\n                        scenario_data_filename = \\\n                            scenario_data_filename+\".yaml\"\n                        with open(scenario_data_filename) as f:\n                            data = yaml.load(f, **yaml_load_args)\n                    else:\n                        raise RuntimeError(\n                            \"Cannot find a data file for scenario '%s' \"\n                            \"in directory: %s\\nRecognized formats: .dat, \"\n                            \".yaml\" % (scenario_name, self.data_directory()))\n                    if verbose:\n                        print(\"Data for scenario=%s loads from file=%s\"\n                              % (scenario_name, scenario_data_filename))\n                    if data is None:\n                        scenario_instance = \\\n                            self._model_object.create_instance(\n                                filename=scenario_data_filename,\n                                profile_memory=profile_memory,\n                                report_timing=output_instance_construction_time)\n                    else:\n                        scenario_instance = \\\n                            self._model_object.create_instance(\n                                data,\n                                profile_memory=profile_memory,\n                                report_timing=output_instance_construction_time)\n                else:\n                    assert self.data_directory() is not None\n                    data_files = []\n                    for node_name in node_name_list:\n                        node_data_filename = \\\n                            os.path.join(self.data_directory(),\n                                         str(node_name)+\".dat\")\n                        if not os.path.exists(node_data_filename):\n                            raise RuntimeError(\n                                \"Cannot find a data file for scenario tree \"\n                                \"node '%s' in directory: %s\\nRecognized \"\n                                \"formats: .dat\" % (node_name,\n                                                   self.data_directory()))\n                        data_files.append(node_data_filename)\n\n                    scenario_data = DataPortal(model=self._model_object)\n                    for data_file in data_files:\n                        if verbose:\n                            print(\"Node data for scenario=%s partially \"\n                                  \"loading from file=%s\"\n                                  % (scenario_name, data_file))\n                        scenario_data.load(filename=data_file)\n\n                    scenario_instance = self._model_object.create_instance(\n                        scenario_data,\n                        profile_memory=profile_memory,\n                        report_timing=output_instance_construction_time)\n            else:\n                raise RuntimeError(\"Unable to construct scenario instance. \"\n                                   \"Neither a reference model or callback \"\n                                   \"is defined.\")\n\n            # name each instance with the scenario name\n            scenario_instance._name = scenario_name\n\n            if compile_instance:\n                from pyomo.repn.beta.matrix import \\\n                    compile_block_linear_constraints\n                compile_block_linear_constraints(\n                    scenario_instance,\n                    \"_PySP_compiled_linear_constraints\",\n                    verbose=verbose)\n\n        except:\n            logger.error(\"Failed to create model instance for scenario=%s\"\n                         % (scenario_name))\n            raise\n\n        return scenario_instance",
  "def construct_instances_for_scenario_tree(\n            self,\n            scenario_tree,\n            profile_memory=False,\n            output_instance_construction_time=False,\n            compile_scenario_instances=False,\n            verbose=False):\n        assert not self._closed\n\n        if scenario_tree._scenario_based_data:\n            if verbose:\n                print(\"Scenario-based instance initialization enabled\")\n        else:\n            if verbose:\n                print(\"Node-based instance initialization enabled\")\n\n        scenario_instances = {}\n        for scenario in scenario_tree._scenarios:\n\n            # the construction of instances takes little overhead in terms\n            # of memory potentially lost in the garbage-collection sense\n            # (mainly only that due to parsing and instance\n            # simplification/prep-processing).  to speed things along,\n            # disable garbage collection if it enabled in the first place\n            # through the instance construction process.\n            # IDEA: If this becomes too much for truly large numbers of\n            #       scenarios, we could manually collect every time X\n            #       instances have been created.\n            scenario_instance = None\n            with PauseGC() as pgc:\n                scenario_instance = \\\n                    self.construct_scenario_instance(\n                        scenario._name,\n                        scenario_tree,\n                        profile_memory=profile_memory,\n                        output_instance_construction_time=output_instance_construction_time,\n                        compile_instance=compile_scenario_instances,\n                        verbose=verbose)\n\n            scenario_instances[scenario._name] = scenario_instance\n            assert scenario_instance.local_name == scenario.name\n\n        return scenario_instances",
  "def generate_scenario_tree(self,\n                               downsample_fraction=1.0,\n                               include_scenarios=None,\n                               bundles=None,\n                               random_bundles=None,\n                               random_seed=None,\n                               verbose=True):\n\n        scenario_tree_model = self._scenario_tree_model\n        if scenario_tree_model is not None:\n            if has_networkx and \\\n               isinstance(scenario_tree_model, networkx.DiGraph):\n                scenario_tree_model = \\\n                    ScenarioTreeModelFromNetworkX(scenario_tree_model)\n            else:\n                assert isinstance(scenario_tree_model, (_BlockData, Block)), \\\n                    str(scenario_tree_model)+\" \"+str(type(scenario_tree_model))\n\n        if bundles is not None:\n            if isinstance(bundles, str):\n                if scenario_tree_model is None:\n                    raise ValueError(\n                        \"A bundles file can not be used when the \"\n                        \"scenario tree input was not a Pyomo \"\n                        \"model or ScenarioStructure.dat file.\")\n                logger.debug(\"attempting to locate bundle file for input: %s\"\n                             % (bundles))\n                # we interpret the scenario bundle specification in one of\n                # two ways. if the supplied name is a file, it is used\n                # directly. otherwise, it is interpreted as the root of a\n                # file with a .dat suffix to be found in the instance\n                # directory.\n                orig_input = bundles\n                if not bundles.endswith(\".dat\"):\n                    bundles = bundles+\".dat\"\n                bundles = os.path.expanduser(bundles)\n                if not os.path.exists(bundles):\n                    if self.data_directory() is None:\n                        raise ValueError(\n                            \"Could not locate bundle .dat file from input \"\n                            \"'%s'. Path does not exist and there is no data \"\n                            \"directory to search in.\" % (orig_input))\n                    bundles = os.path.join(self.data_directory(), bundles)\n                if not os.path.exists(bundles):\n                    raise ValueError(\"Could not locate bundle .dat file \"\n                                     \"from input '%s' as absolute path or \"\n                                     \"relative to data directory: %s\"\n                                     % (orig_input, self.data_directory()))\n\n                if verbose:\n                    print(\"Scenario tree bundle specification filename=%s\"\n                          % (bundles))\n\n                scenario_tree_model = scenario_tree_model.clone()\n                scenario_tree_model.Bundling = True\n                scenario_tree_model.Bundling._constructed = False\n                scenario_tree_model.Bundling._data.clear()\n                scenario_tree_model.Bundles.clear()\n                scenario_tree_model.Bundles._constructed = False\n                scenario_tree_model.Bundles._data.clear()\n                scenario_tree_model.BundleScenarios.clear()\n                scenario_tree_model.BundleScenarios._constructed = False\n                scenario_tree_model.BundleScenarios._data.clear()\n                scenario_tree_model.load(bundles)\n\n        #\n        # construct the scenario tree\n        #\n        if scenario_tree_model is not None:\n            scenario_tree = ScenarioTree(scenariotreeinstance=scenario_tree_model,\n                                         scenariobundlelist=include_scenarios)\n        else:\n            assert self._scenario_tree is not None\n            if include_scenarios is None:\n                scenario_tree = copy.deepcopy(self._scenario_tree)\n            else:\n                # note: any bundles will be lost\n                if self._scenario_tree.contains_bundles():\n                    raise ValueError(\n                        \"Can not compress a scenario tree that \"\n                        \"contains bundles\")\n                scenario_tree = self._scenario_tree.make_compressed(\n                    include_scenarios,\n                    normalize=True)\n\n        # compress/down-sample the scenario tree, if requested\n        if (downsample_fraction is not None) and \\\n           (downsample_fraction < 1.0):\n            scenario_tree.downsample(downsample_fraction,\n                                     random_seed,\n                                     verbose)\n\n        #\n        # create bundles from a dict, if requested\n        #\n        if bundles is not None:\n            if not isinstance(bundles, str):\n                if verbose:\n                    print(\"Adding bundles to scenario tree from \"\n                          \"user-specified dict\")\n                if scenario_tree.contains_bundles():\n                    if verbose:\n                        print(\"Scenario tree already contains bundles. \"\n                              \"All existing bundles will be removed.\")\n                    for bundle in list(scenario_tree.bundles):\n                        scenario_tree.remove_bundle(bundle.name)\n                for bundle_name in bundles:\n                    scenario_tree.add_bundle(bundle_name,\n                                             bundles[bundle_name])\n\n        #\n        # create random bundles, if requested\n        #\n        if (random_bundles is not None) and \\\n           (random_bundles > 0):\n            if bundles is not None:\n                raise ValueError(\"Cannot specify both random \"\n                                 \"bundles and a bundles specification\")\n\n            num_scenarios = len(scenario_tree._scenarios)\n            if random_bundles > num_scenarios:\n                raise ValueError(\"Cannot create more random bundles \"\n                                 \"than there are scenarios!\")\n\n            if verbose:\n                print(\"Creating \"+str(random_bundles)+\n                      \" random bundles using seed=\"\n                      +str(random_seed))\n\n            scenario_tree.create_random_bundles(random_bundles,\n                                                random_seed)\n\n        scenario_tree._scenario_instance_factory = self\n\n        return scenario_tree",
  "class _CUIDLabeler:\n    def __init__(self):\n        self._cuid_map = ComponentMap()\n\n    def update_cache(self, block):\n        self._cuid_map.update(\n            ComponentUID.generate_cuid_string_map(\n                block, repr_version=CUID_repr_version))\n\n    def clear_cache(self):\n        self._cuid_map = {}\n\n    def __call__(self, obj):\n        if obj in self._cuid_map:\n            return self._cuid_map[obj]\n        else:\n            cuid = ComponentUID(obj).get_repr(version=1)\n            self._cuid_map[obj] = cuid\n            return cuid",
  "class ScenarioTreeNode:\n\n    \"\"\" Constructor\n    \"\"\"\n\n    VARIABLE_FIXED = 0\n    VARIABLE_FREED = 1\n\n    def __init__(self, name, conditional_probability, stage):\n\n        # self-explanatory!\n        self._name = name\n\n        # the stage to which this tree node belongs.\n        self._stage = stage\n\n        # defines the tree structure\n        self._parent = None\n\n        # a collection of ScenarioTreeNodes\n        self._children = []\n\n        # conditional on parent\n        self._conditional_probability = conditional_probability\n\n        # a collection of all Scenario objects passing through this\n        # node in the tree\n        self._scenarios = []\n\n        # the cumulative probability of scenarios at this node.\n        # cached for efficiency.\n        self._probability = 0.0\n\n        # a map between a variable name and a list of original index\n        # match templates, specified as strings.  we want to maintain\n        # these for a variety of reasons, perhaps the most important\n        # being that for output purposes. specific indices that match\n        # belong to the tree node, as that may be specific to a tree\n        # node.\n        self._variable_templates = {}\n        self._derived_variable_templates = {}\n\n        #\n        # information relating to all variables blended at this node, whether\n        # of the standard or derived varieties.\n        #\n        # maps id -> (name, index)\n        self._variable_ids = {}\n        # maps (name,index) -> id\n        self._name_index_to_id = {}\n        # maps id -> list of (vardata,probability) across all scenarios\n        self._variable_datas = {}\n\n        # keep track of the variable indices at this node, independent\n        # of type.  this is useful for iterating. maps variable name\n        # to a list of indices.\n        self._variable_indices = {}\n\n        # variables are either standard or derived - but not both.\n        # partition the ids into two sets, as we deal with these\n        # differently in algorithmic and reporting contexts.\n        self._standard_variable_ids = set()\n        self._derived_variable_ids = set()\n        # A temporary solution to help wwphextension and other code\n        # for when pyomo instances no longer live on the master node\n        # when using PHPyro\n        self._integer = set()\n        self._binary = set()\n        self._semicontinuous = set()\n\n        # a tuple consisting of (1) the name of the variable that\n        # stores the stage-specific cost in all scenarios and (2) the\n        # corresponding index *string* - this is converted in the tree\n        # node to a real index.\n        # TODO: Change the code so that this is a ComponentUID string\n        self._cost_variable = None\n\n        # a list of _VarData objects, representing the cost variables\n        # for each scenario passing through this tree node.\n        # NOTE: This list actually contains tuples of\n        #       (_VarData, scenario-probability) pairs.\n        self._cost_variable_datas = []\n\n        # node variables ids that are fixed (along with the value to fix)\n        self._fixed = {}\n\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def stage(self):\n        return self._stage\n\n    @property\n    def parent(self):\n        return self._parent\n\n    @property\n    def children(self):\n        return tuple(self._children)\n\n    @property\n    def scenarios(self):\n        return self._scenarios\n\n    @property\n    def conditional_probability(self):\n        return self._conditional_probability\n\n    @property\n    def probability(self):\n        return self._probability\n\n    #\n    # a simple predicate to check if this tree node belongs to the\n    # last stage in the scenario tree.\n    #\n    def is_leaf_node(self):\n\n        return self._stage.is_last_stage()\n\n    #\n    # a utility to determine if the input variable name/index pair is\n    # a derived variable.\n    #\n    def is_derived_variable(self, variable_name, variable_index):\n        return (variable_name, variable_index) in self._name_index_to_id",
  "class ScenarioTreeStage:\n\n    \"\"\" Constructor\n    \"\"\"\n    def __init__(self):\n\n        self._name = \"\"\n\n        # a collection of ScenarioTreeNode objects associated with this stage.\n        self._tree_nodes = []\n\n        # the parent scenario tree for this stage.\n        self._scenario_tree = None\n\n        # a map between a variable name and a list of original index\n        # match templates, specified as strings.  we want to maintain\n        # these for a variety of reasons, perhaps the most important\n        # being that for output purposes. specific indices that match\n        # belong to the tree node, as that may be specific to a tree\n        # node.\n        self._variable_templates = {}\n\n        # same as above, but for derived stage variables.\n        self._derived_variable_templates = {}\n\n        # a tuple consisting of (1) the name of the variable that\n        # stores the stage-specific cost in all scenarios and (2) the\n        # corresponding index *string* - this is converted in the tree\n        # node to a real index.\n        self._cost_variable = None\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def nodes(self):\n        return self._tree_nodes\n\n    @property\n    def scenario_tree(self):\n        return self._scenario_tree\n\n    #\n    # a simple predicate to check if this stage is the last stage in\n    # the scenario tree.\n    #\n    def is_last_stage(self):\n\n        return self == self._scenario_tree._stages[-1]",
  "class Scenario:\n\n    \"\"\" Constructor\n    \"\"\"\n    def __init__(self):\n\n        self._name = None\n        # allows for construction of node list\n        self._leaf_node = None\n        # sequence from parent to leaf of ScenarioTreeNodes\n        self._node_list = []\n        # the unconditional probability for this scenario, computed from the node list\n        self._probability = 0.0\n        # the Pyomo instance corresponding to this scenario.\n        self._instance = None\n        self._instance_cost_expression = None\n        self._instance_objective = None\n        self._instance_original_objective_object = None\n        self._objective_sense = None\n        self._objective_name = None\n\n        # The value of the (possibly augmented) objective function\n        self._objective = None\n        # The value of the original objective expression\n        # (which should be the sum of the stage costs)\n        self._cost = None\n        # The individual stage cost values\n        self._stage_costs = {}\n        # The value of the ph weight term piece of the objective (if it exists)\n        self._weight_term_cost = None\n        # The value of the ph proximal term piece of the objective (if it exists)\n        self._proximal_term_cost = None\n        # The value of the scenariotree variables belonging to this scenario\n        # (dictionary nested by node name)\n        self._x = {}\n        # The value of the weight terms belonging to this scenario\n        # (dictionary nested by node name)\n        self._w = {}\n        # The value of the rho terms belonging to this scenario\n        # (dictionary nested by node name)\n        self._rho = {}\n\n        # This set of fixed or reported stale variables\n        # in each tree node\n        self._fixed = {}\n        self._stale = {}\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def leaf_node(self):\n        return self._leaf_node\n\n    @property\n    def node_list(self):\n        return tuple(self._node_list)\n\n    @property\n    def probability(self):\n        return self._probability\n\n    @property\n    def instance(self):\n        return self._instance\n\n    def get_current_objective(self):\n        return self._objective\n\n    def get_current_cost(self):\n        return self._cost\n\n    def get_current_stagecost(self, stage_name):\n        return self._stage_costs[stage_name]\n\n    #\n    # a utility to compute the stage index for the input tree node.\n    # the returned index is 0-based.\n    #\n\n    def node_stage_index(self, tree_node):\n        return self._node_list.index(tree_node)",
  "class ScenarioTreeBundle:\n\n    def __init__(self):\n\n        self._name = None\n        self._scenario_names = []\n        # This is a compressed scenario tree, just for the bundle.\n        self._scenario_tree = None\n        # the absolute probability of scenarios associated with this\n        # node in the scenario tree.\n        self._probability = 0.0\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def scenario_names(self):\n        return self._scenario_names\n\n    @property\n    def scenario_tree(self):\n        return self._scenario_tree\n\n    @property\n    def probability(self):\n        return self._probability",
  "class ScenarioTree:\n\n    # a utility to construct scenario bundles.\n    def _construct_scenario_bundles(self, bundles):\n\n        for bundle_name in bundles:\n\n            scenario_list = []\n            bundle_probability = 0.0\n            for scenario_name in bundles[bundle_name]:\n                scenario_list.append(scenario_name)\n                bundle_probability += \\\n                    self._scenario_map[scenario_name].probability\n\n            scenario_tree_for_bundle = self.make_compressed(scenario_list,\n                                                            normalize=True)\n\n            scenario_tree_for_bundle.validate()\n\n            new_bundle = ScenarioTreeBundle()\n            new_bundle._name = bundle_name\n            new_bundle._scenario_names = scenario_list\n            new_bundle._scenario_tree = scenario_tree_for_bundle\n            new_bundle._probability = bundle_probability\n\n            self._scenario_bundles.append(new_bundle)\n            self._scenario_bundle_map[new_bundle.name] = new_bundle\n\n    #\n    # a utility to construct the stage objects for this scenario tree.\n    # operates strictly by side effects, initializing the self\n    # _stages and _stage_map attributes.\n    #\n\n    def _construct_stages(self,\n                          stage_names,\n                          stage_variable_names,\n                          stage_cost_variable_names,\n                          stage_derived_variable_names):\n\n        # construct the stage objects, which will leave them\n        # largely uninitialized - no variable information, in particular.\n        for stage_name in stage_names:\n\n            new_stage = ScenarioTreeStage()\n            new_stage._name = stage_name\n            new_stage._scenario_tree = self\n\n            for variable_string in stage_variable_names[stage_name]:\n                if isVariableNameIndexed(variable_string):\n                    variable_name, match_template = \\\n                        extractVariableNameAndIndex(variable_string)\n                else:\n                    variable_name = variable_string\n                    match_template = \"\"\n                if variable_name not in new_stage._variable_templates:\n                    new_stage._variable_templates[variable_name] = []\n                new_stage._variable_templates[variable_name].append(match_template)\n\n            # not all stages have derived variables defined\n            if stage_name in stage_derived_variable_names:\n                for variable_string in stage_derived_variable_names[stage_name]:\n                    if isVariableNameIndexed(variable_string):\n                        variable_name, match_template = \\\n                            extractVariableNameAndIndex(variable_string)\n                    else:\n                        variable_name = variable_string\n                        match_template = \"\"\n                    if variable_name not in new_stage._derived_variable_templates:\n                        new_stage._derived_variable_templates[variable_name] = []\n                    new_stage._derived_variable_templates[variable_name].append(match_template)\n\n            # de-reference is required to access the parameter value\n            # TBD March 2020: make it so the stages always know their cost names.\n            # dlw March 2020: when coming from NetworkX, we don't know these yet!!\n            cost_variable_string = stage_cost_variable_names[stage_name].value\n            if cost_variable_string is not None:\n                if isVariableNameIndexed(cost_variable_string):\n                    cost_variable_name, cost_variable_index = \\\n                        extractVariableNameAndIndex(cost_variable_string)\n                else:\n                    cost_variable_name = cost_variable_string\n                    cost_variable_index = None\n                new_stage._cost_variable = (cost_variable_name, cost_variable_index)\n\n            self._stages.append(new_stage)\n            self._stage_map[stage_name] = new_stage\n\n    \"\"\" Constructor\n        Arguments:\n            scenarioinstance     - the reference (deterministic) scenario instance.\n            scenariotreeinstance - the pyomo model specifying all scenario tree (text) data.\n            scenariobundlelist   - a list of scenario names to retain, i.e., cull the rest to create a reduced tree!\n    \"\"\"\n    def __init__(self,\n                 scenariotreeinstance=None,\n                 scenariobundlelist=None):\n\n        # some arbitrary identifier\n        self._name = None\n\n        # should be called once for each variable blended across a node\n        #self._id_labeler = CounterLabeler()\n        self._id_labeler = _CUIDLabeler()\n\n        #\n        # the core objects defining the scenario tree.\n        #\n\n        # collection of ScenarioTreeNodes\n        self._tree_nodes = []\n        # collection of ScenarioTreeStages - assumed to be in\n        # time-order. the set (provided by the user) itself *must* be\n        # ordered.\n        self._stages = []\n        # collection of Scenarios\n        self._scenarios = []\n        # collection of ScenarioTreeBundles\n        self._scenario_bundles = []\n\n        # dictionaries for the above.\n        self._tree_node_map = {}\n        self._stage_map = {}\n        self._scenario_map = {}\n        self._scenario_bundle_map = {}\n\n        # a boolean indicating how data for scenario instances is specified.\n        # possibly belongs elsewhere, e.g., in the PH algorithm.\n        self._scenario_based_data = None\n\n        if scenariotreeinstance is None:\n            assert scenariobundlelist is None\n            return\n\n        node_ids = scenariotreeinstance.Nodes\n        node_child_ids = scenariotreeinstance.Children\n        node_stage_ids = scenariotreeinstance.NodeStage\n        node_probability_map = scenariotreeinstance.ConditionalProbability\n        stage_ids = scenariotreeinstance.Stages\n        stage_variable_ids = scenariotreeinstance.StageVariables\n        node_variable_ids = scenariotreeinstance.NodeVariables\n        stage_cost_variable_ids = scenariotreeinstance.StageCost\n        node_cost_variable_ids = scenariotreeinstance.NodeCost\n        if any(scenariotreeinstance.StageCostVariable[i].value is not None\n               for i in scenariotreeinstance.StageCostVariable):\n            logger.warning(\"DEPRECATED: The 'StageCostVariable' scenario tree \"\n                           \"model parameter has been renamed to 'StageCost'. \"\n                           \"Please update your scenario tree structure model.\")\n            if any(stage_cost_variable_ids[i].value is not None\n                   for i in stage_cost_variable_ids):\n                raise ValueError(\"The 'StageCostVariable' and 'StageCost' \"\n                                 \"parameters can not both be used on a scenario \"\n                                 \"tree structure model.\")\n            else:\n                stage_cost_variable_ids = scenariotreeinstance.StageCostVariable\n\n        if any(stage_cost_variable_ids[i].value is not None\n               for i in stage_cost_variable_ids) and \\\n           any(node_cost_variable_ids[i].value is not None\n               for i in node_cost_variable_ids):\n            raise ValueError(\n                \"The 'StageCost' and 'NodeCost' parameters \"\n                \"can not both be used on a scenario tree \"\n                \"structure model.\")\n        stage_derived_variable_ids = scenariotreeinstance.StageDerivedVariables\n        node_derived_variable_ids = scenariotreeinstance.NodeDerivedVariables\n        scenario_ids = scenariotreeinstance.Scenarios\n        scenario_leaf_ids = scenariotreeinstance.ScenarioLeafNode\n        scenario_based_data = scenariotreeinstance.ScenarioBasedData\n\n        # save the method for instance data storage.\n        self._scenario_based_data = scenario_based_data()\n\n        # the input stages must be ordered, for both output purposes\n        # and knowledge of the final stage.\n        if not stage_ids.isordered():\n            raise ValueError(\n                \"An ordered set of stage IDs must be supplied in \"\n                \"the ScenarioTree constructor\")\n\n        for node_id in node_ids:\n            node_stage_id = node_stage_ids[node_id].value\n            if node_stage_id != stage_ids.last():\n                if (len(stage_variable_ids[node_stage_id]) == 0) and \\\n                   (len(node_variable_ids[node_id]) == 0):\n                    raise ValueError(\n                        \"Scenario tree node %s, belonging to stage %s, \"\n                        \"has not been declared with any variables. \"\n                        \"To fix this error, make sure that one of \"\n                        \"the sets StageVariables[%s] or NodeVariables[%s] \"\n                        \"is declared with at least one variable string \"\n                        \"template (e.g., x, x[*]) on the scenario tree \"\n                        \"or in ScenarioStructure.dat.\"\n                        % (node_id, node_stage_id, node_stage_id, node_id))\n\n        #\n        # construct the actual tree objects\n        #\n\n        # construct the stage objects w/o any linkages first; link them up\n        # with tree nodes after these have been fully constructed.\n        self._construct_stages(stage_ids,\n                               stage_variable_ids,\n                               stage_cost_variable_ids,\n                               stage_derived_variable_ids)\n\n        # construct the tree node objects themselves in a first pass,\n        # and then link them up in a second pass to form the tree.\n        # can't do a single pass because the objects may not exist.\n        for tree_node_name in node_ids:\n\n            if tree_node_name not in node_stage_ids:\n                raise ValueError(\"No stage is assigned to tree node=%s\"\n                                 % (tree_node_name))\n\n            stage_name = value(node_stage_ids[tree_node_name])\n            if stage_name not in self._stage_map:\n                raise ValueError(\"Unknown stage=%s assigned to tree node=%s\"\n                                 % (stage_name, tree_node_name))\n\n            node_stage = self._stage_map[stage_name]\n            new_tree_node = ScenarioTreeNode(\n                tree_node_name,\n                value(node_probability_map[tree_node_name]),\n                node_stage)\n\n            # extract the node variable match templates\n            for variable_string in node_variable_ids[tree_node_name]:\n                if isVariableNameIndexed(variable_string):\n                    variable_name, match_template = \\\n                        extractVariableNameAndIndex(variable_string)\n                else:\n                    variable_name = variable_string\n                    match_template = \"\"\n                if variable_name not in new_tree_node._variable_templates:\n                    new_tree_node._variable_templates[variable_name] = []\n                new_tree_node._variable_templates[variable_name].append(match_template)\n\n            cost_variable_string = node_cost_variable_ids[tree_node_name].value\n            if cost_variable_string is not None:\n                assert node_stage._cost_variable is None\n                if isVariableNameIndexed(cost_variable_string):\n                    cost_variable_name, cost_variable_index = \\\n                        extractVariableNameAndIndex(cost_variable_string)\n                else:\n                    cost_variable_name = cost_variable_string\n                    cost_variable_index = None\n            else:\n                assert node_stage._cost_variable is not None\n                cost_variable_name, cost_variable_index = \\\n                    node_stage._cost_variable\n            new_tree_node._cost_variable = (cost_variable_name, cost_variable_index)\n\n            # extract the node derived variable match templates\n            for variable_string in node_derived_variable_ids[tree_node_name]:\n                if isVariableNameIndexed(variable_string):\n                    variable_name, match_template = \\\n                        extractVariableNameAndIndex(variable_string)\n                else:\n                    variable_name = variable_string\n                    match_template = \"\"\n                if variable_name not in new_tree_node._derived_variable_templates:\n                    new_tree_node._derived_variable_templates[variable_name] = []\n                new_tree_node._derived_variable_templates[variable_name].append(match_template)\n\n            self._tree_nodes.append(new_tree_node)\n            self._tree_node_map[tree_node_name] = new_tree_node\n            self._stage_map[stage_name]._tree_nodes.append(new_tree_node)\n\n        # link up the tree nodes objects based on the child id sets.\n        for this_node in self._tree_nodes:\n            this_node._children = []\n            # otherwise, you're at a leaf and all is well.\n            if this_node.name in node_child_ids:\n                child_ids = node_child_ids[this_node.name]\n                for child_id in child_ids:\n                    if child_id in self._tree_node_map:\n                        child_node = self._tree_node_map[child_id]\n                        this_node._children.append(child_node)\n                        if child_node._parent is None:\n                            child_node._parent = this_node\n                        else:\n                            raise ValueError(\n                                \"Multiple parents specified for tree node=%s; \"\n                                \"existing parent node=%s; conflicting parent \"\n                                \"node=%s\"\n                                % (child_id,\n                                   child_node._parent.name,\n                                   this_node.name))\n                    else:\n                        raise ValueError(\"Unknown child tree node=%s specified \"\n                                         \"for tree node=%s\"\n                                         % (child_id, this_node.name))\n\n        # at this point, the scenario tree nodes and the stages are set - no\n        # two-pass logic necessary when constructing scenarios.\n        for scenario_name in scenario_ids:\n            new_scenario = Scenario()\n            new_scenario._name = scenario_name\n\n            if scenario_name not in scenario_leaf_ids:\n                raise ValueError(\"No leaf tree node specified for scenario=%s\"\n                                 % (scenario_name))\n            else:\n                scenario_leaf_node_name = value(scenario_leaf_ids[scenario_name])\n                if scenario_leaf_node_name not in self._tree_node_map:\n                    raise ValueError(\"Uknown tree node=%s specified as leaf \"\n                                     \"of scenario=%s\" %\n                                     (scenario_leaf_node_name, scenario_name))\n                else:\n                    new_scenario._leaf_node = \\\n                        self._tree_node_map[scenario_leaf_node_name]\n\n            current_node = new_scenario._leaf_node\n            while current_node is not None:\n                new_scenario._node_list.append(current_node)\n                # links the scenarios to the nodes to enforce\n                # necessary non-anticipativity\n                current_node._scenarios.append(new_scenario)\n                current_node = current_node._parent\n            new_scenario._node_list.reverse()\n            # This now loops root -> leaf\n            probability = 1.0\n            for current_node in new_scenario._node_list:\n                probability *= current_node._conditional_probability\n                # NOTE: The line placement below is a little weird, in that\n                #       it is embedded in a scenario loop - so the probabilities\n                #       for some nodes will be redundantly computed. But this works.\n                current_node._probability = probability\n\n                new_scenario._stage_costs[current_node.stage.name] = None\n                new_scenario._x[current_node.name] = {}\n                new_scenario._w[current_node.name] = {}\n                new_scenario._rho[current_node.name] = {}\n                new_scenario._fixed[current_node.name] = set()\n                new_scenario._stale[current_node.name] = set()\n\n            new_scenario._probability = probability\n\n            self._scenarios.append(new_scenario)\n            self._scenario_map[scenario_name] = new_scenario\n\n        # for output purposes, it is useful to known the maximal\n        # length of identifiers in the scenario tree for any\n        # particular category. I'm building these up incrementally, as\n        # they are needed. 0 indicates unassigned.\n        self._max_scenario_id_length = 0\n\n        # does the actual traversal to populate the members.\n        self.computeIdentifierMaxLengths()\n\n        # if a sub-bundle of scenarios has been specified, mark the\n        # active scenario tree components and compress the tree.\n        if scenariobundlelist is not None:\n            self.compress(scenariobundlelist)\n\n        # NEW SCENARIO BUNDLING STARTS HERE\n        if value(scenariotreeinstance.Bundling[None]):\n            bundles = OrderedDict()\n            for bundle_name in scenariotreeinstance.Bundles:\n                bundles[bundle_name] = \\\n                    list(scenariotreeinstance.BundleScenarios[bundle_name])\n            self._construct_scenario_bundles(bundles)\n\n    @property\n    def scenarios(self):\n        return self._scenarios\n\n    @property\n    def bundles(self):\n        return self._scenario_bundles\n\n    @property\n    def subproblems(self):\n        if self.contains_bundles():\n            return self._scenario_bundles\n        else:\n            return self._scenarios\n\n    @property\n    def stages(self):\n        return self._stages\n\n    @property\n    def nodes(self):\n        return self._tree_nodes\n\n    def is_bundle(self, object_name):\n        return object_name in self._scenario_bundle_map\n\n    def is_scenario(self, object_name):\n        return object_name in self._scenario_map\n\n    #\n    # is the indicated scenario / bundle in the tree?\n    #\n\n    def contains_scenario(self, name):\n        return name in self._scenario_map\n\n    def contains_bundles(self):\n        return len(self._scenario_bundle_map) > 0\n\n    def contains_bundle(self, name):\n        return name in self._scenario_bundle_map\n\n    #\n    # get the scenario / bundle object from the tree.\n    #\n\n    def get_scenario(self, name):\n        return self._scenario_map[name]\n\n    def get_bundle(self, name):\n        return self._scenario_bundle_map[name]\n\n    def get_subproblem(self, name):\n        if self.contains_bundles():\n            return self._scenario_bundle_map[name]\n        else:\n            return self._scenario_map[name]\n\n    def get_scenario_bundle(self, name):\n        if not self.contains_bundles():\n            return None\n        else:\n            return self._scenario_bundle_map[name]\n\n    # there are many contexts where manipulators of a scenario\n    # tree simply need an arbitrary scenario to proceed...\n    def get_arbitrary_scenario(self):\n        return self._scenarios[0]\n\n    def contains_node(self, name):\n        return name in self._tree_node_map\n\n    #\n    # get the scenario tree node object from the tree\n    #\n    def get_node(self, name):\n        return self._tree_node_map[name]\n\n    #\n    # utility for compressing or culling a scenario tree based on\n    # a provided list of scenarios (specified by name) to retain -\n    # all non-referenced components are eliminated. this particular\n    # method compresses *in-place*, i.e., via direct modification\n    # of the scenario tree structure. If normalize=True, all probabilities\n    # (and conditional probabilities) are renormalized.\n    #\n\n    def compress(self,\n                 scenario_bundle_list,\n                 normalize=True):\n\n        # scan for and mark all referenced scenarios and\n        # tree nodes in the bundle list - all stages will\n        # obviously remain.\n        try:\n\n            for scenario_name in scenario_bundle_list:\n\n                scenario = self._scenario_map[scenario_name]\n                scenario.retain = True\n\n                # chase all nodes comprising this scenario,\n                # marking them for retention.\n                for node in scenario._node_list:\n                    node.retain = True\n\n        except KeyError:\n            raise ValueError(\"Scenario=%s selected for \"\n                             \"bundling not present in \"\n                             \"scenario tree\"\n                             % (scenario_name))\n\n        # scan for any non-retained scenarios and tree nodes.\n        scenarios_to_delete = []\n        tree_nodes_to_delete = []\n        for scenario in self._scenarios:\n            if hasattr(scenario, \"retain\"):\n                delattr(scenario, \"retain\")\n            else:\n                scenarios_to_delete.append(scenario)\n                del self._scenario_map[scenario.name]\n\n        for tree_node in self._tree_nodes:\n            if hasattr(tree_node, \"retain\"):\n                delattr(tree_node, \"retain\")\n            else:\n                tree_nodes_to_delete.append(tree_node)\n                del self._tree_node_map[tree_node.name]\n\n        # JPW does not claim the following routines are\n        # the most efficient. rather, they get the job\n        # done while avoiding serious issues with\n        # attempting to remove elements from a list that\n        # you are iterating over.\n\n        # delete all references to unmarked scenarios\n        # and child tree nodes in the scenario tree node\n        # structures.\n        for tree_node in self._tree_nodes:\n            for scenario in scenarios_to_delete:\n                if scenario in tree_node._scenarios:\n                    tree_node._scenarios.remove(scenario)\n            for node_to_delete in tree_nodes_to_delete:\n                if node_to_delete in tree_node._children:\n                    tree_node._children.remove(node_to_delete)\n\n        # delete all references to unmarked tree nodes\n        # in the scenario tree stage structures.\n        for stage in self._stages:\n            for tree_node in tree_nodes_to_delete:\n                if tree_node in stage._tree_nodes:\n                    stage._tree_nodes.remove(tree_node)\n\n        # delete all unreferenced entries from the core scenario\n        # tree data structures.\n        for scenario in scenarios_to_delete:\n            self._scenarios.remove(scenario)\n        for tree_node in tree_nodes_to_delete:\n            self._tree_nodes.remove(tree_node)\n\n        #\n        # Handle re-normalization of probabilities if requested\n        #\n        if normalize:\n\n            # re-normalize the conditional probabilities of the\n            # children at each tree node (leaf-to-root stage order).\n            for stage in reversed(self._stages[:-1]):\n\n                for tree_node in stage._tree_nodes:\n                    norm_factor = sum(child_tree_node._conditional_probability\n                                      for child_tree_node\n                                      in tree_node._children)\n                    # the user may specify that the probability of a\n                    # scenario is 0.0, and while odd, we should allow the\n                    # edge case.\n                    if norm_factor == 0.0:\n                        for child_tree_node in tree_node._children:\n                            child_tree_node._conditional_probability = 0.0\n                    else:\n                        for child_tree_node in tree_node._children:\n                            child_tree_node._conditional_probability /= norm_factor\n\n            # update absolute probabilities (root-to-leaf stage order)\n            for stage in self._stages[1:]:\n                for tree_node in stage._tree_nodes:\n                    tree_node._probability = \\\n                        tree_node._parent._probability * \\\n                        tree_node._conditional_probability\n\n            # update scenario probabilities\n            for scenario in self._scenarios:\n                scenario._probability = \\\n                    scenario._leaf_node._probability\n\n        # now that we've culled the scenarios, cull the bundles. do\n        # this in two passes. in the first pass, we identify the names\n        # of bundles to delete, by looking for bundles with deleted\n        # scenarios. in the second pass, we delete the bundles from\n        # the scenario tree, and normalize the probabilities of the\n        # remaining bundles.\n\n        # indices of the objects in the scenario tree bundle list\n        bundles_to_delete = []\n        for i in range(0,len(self._scenario_bundles)):\n            scenario_bundle = self._scenario_bundles[i]\n            for scenario_name in scenario_bundle._scenario_names:\n                if scenario_name not in self._scenario_map:\n                    bundles_to_delete.append(i)\n                    break\n        bundles_to_delete.reverse()\n        for i in bundles_to_delete:\n            deleted_bundle = self._scenario_bundles.pop(i)\n            del self._scenario_bundle_map[deleted_bundle.name]\n\n        sum_bundle_probabilities = \\\n            sum(bundle._probability for bundle in self._scenario_bundles)\n        for bundle in self._scenario_bundles:\n            bundle._probability /= sum_bundle_probabilities\n\n    #\n    # Returns a compressed tree using operations on the order of the\n    # number of nodes in the compressed tree rather than the number of\n    # nodes in the full tree (this method is more efficient than in-place\n    # compression). If normalize=True, all probabilities\n    # (and conditional probabilities) are renormalized.\n    #\n    # *** Bundles are ignored. The compressed tree will not have them ***\n    #\n    def make_compressed(self,\n                        scenario_bundle_list,\n                        normalize=False):\n\n        compressed_tree = ScenarioTree()\n        compressed_tree._scenario_based_data = self._scenario_based_data\n        #\n        # Copy Stage Data\n        #\n        for stage in self._stages:\n            # copy everything but the list of tree nodes\n            # and the reference to the scenario tree\n            compressed_tree_stage = ScenarioTreeStage()\n            compressed_tree_stage._name = stage.name\n            compressed_tree_stage._variable_templates = copy.deepcopy(stage._variable_templates)\n            compressed_tree_stage._derived_variable_templates = \\\n                copy.deepcopy(stage._derived_variable_templates)\n            compressed_tree_stage._cost_variable = copy.deepcopy(stage._cost_variable)\n            # add the stage object to the compressed tree\n            compressed_tree._stages.append(compressed_tree_stage)\n            compressed_tree._stages[-1]._scenario_tree = compressed_tree\n\n        compressed_tree._stage_map = \\\n            dict((stage.name, stage) for stage in compressed_tree._stages)\n\n        #\n        # Copy Scenario and Node Data\n        #\n        compressed_tree_root = None\n        for scenario_name in scenario_bundle_list:\n            full_tree_scenario = self.get_scenario(scenario_name)\n\n            compressed_tree_scenario = Scenario()\n            compressed_tree_scenario._name = full_tree_scenario.name\n            compressed_tree_scenario._probability = full_tree_scenario._probability\n            compressed_tree._scenarios.append(compressed_tree_scenario)\n\n            full_tree_node = full_tree_scenario._leaf_node\n            ### copy the node\n            compressed_tree_node = ScenarioTreeNode(\n                full_tree_node.name,\n                full_tree_node._conditional_probability,\n                compressed_tree._stage_map[full_tree_node._stage.name])\n            compressed_tree_node._variable_templates = \\\n                copy.deepcopy(full_tree_node._variable_templates)\n            compressed_tree_node._derived_variable_templates = \\\n                copy.deepcopy(full_tree_node._derived_variable_templates)\n            compressed_tree_node._scenarios.append(compressed_tree_scenario)\n            compressed_tree_node._stage._tree_nodes.append(compressed_tree_node)\n            compressed_tree_node._probability = full_tree_node._probability\n            compressed_tree_node._cost_variable = full_tree_node._cost_variable\n            ###\n\n            compressed_tree_scenario._node_list.append(compressed_tree_node)\n            compressed_tree_scenario._leaf_node = compressed_tree_node\n            compressed_tree._tree_nodes.append(compressed_tree_node)\n            compressed_tree._tree_node_map[compressed_tree_node.name] = \\\n                compressed_tree_node\n\n            previous_compressed_tree_node = compressed_tree_node\n            full_tree_node = full_tree_node._parent\n            while full_tree_node.name not in compressed_tree._tree_node_map:\n\n                ### copy the node\n                compressed_tree_node = ScenarioTreeNode(\n                    full_tree_node.name,\n                    full_tree_node._conditional_probability,\n                    compressed_tree._stage_map[full_tree_node.stage.name])\n                compressed_tree_node._variable_templates = \\\n                    copy.deepcopy(full_tree_node._variable_templates)\n                compressed_tree_node._derived_variable_templates = \\\n                    copy.deepcopy(full_tree_node._derived_variable_templates)\n                compressed_tree_node._probability = full_tree_node._probability\n                compressed_tree_node._cost_variable = full_tree_node._cost_variable\n                compressed_tree_node._scenarios.append(compressed_tree_scenario)\n                compressed_tree_node._stage._tree_nodes.append(compressed_tree_node)\n                ###\n\n                compressed_tree_scenario._node_list.append(compressed_tree_node)\n                compressed_tree._tree_nodes.append(compressed_tree_node)\n                compressed_tree._tree_node_map[compressed_tree_node.name] = \\\n                    compressed_tree_node\n                previous_compressed_tree_node._parent = compressed_tree_node\n                compressed_tree_node._children.append(previous_compressed_tree_node)\n                previous_compressed_tree_node = compressed_tree_node\n\n                full_tree_node = full_tree_node._parent\n                if full_tree_node is None:\n                    compressed_tree_root = compressed_tree_node\n                    break\n\n            # traverse the remaining nodes up to the root and update the\n            # tree structure elements\n            if full_tree_node is not None:\n                compressed_tree_node = \\\n                    compressed_tree._tree_node_map[full_tree_node.name]\n                previous_compressed_tree_node._parent = compressed_tree_node\n                compressed_tree_node._scenarios.append(compressed_tree_scenario)\n                compressed_tree_node._children.append(previous_compressed_tree_node)\n                compressed_tree_scenario._node_list.append(compressed_tree_node)\n\n                compressed_tree_node = compressed_tree_node._parent\n                while compressed_tree_node is not None:\n                    compressed_tree_scenario._node_list.append(compressed_tree_node)\n                    compressed_tree_node._scenarios.append(compressed_tree_scenario)\n                    compressed_tree_node = compressed_tree_node._parent\n\n            # makes sure this list is in root to leaf order\n            compressed_tree_scenario._node_list.reverse()\n            assert compressed_tree_scenario._node_list[-1] is \\\n                compressed_tree_scenario._leaf_node\n            assert compressed_tree_scenario._node_list[0] is \\\n                compressed_tree_root\n\n            # initialize solution related dictionaries\n            for compressed_tree_node in compressed_tree_scenario._node_list:\n                compressed_tree_scenario._stage_costs[compressed_tree_node._stage.name] = None\n                compressed_tree_scenario._x[compressed_tree_node.name] = {}\n                compressed_tree_scenario._w[compressed_tree_node.name] = {}\n                compressed_tree_scenario._rho[compressed_tree_node.name] = {}\n                compressed_tree_scenario._fixed[compressed_tree_node.name] = set()\n                compressed_tree_scenario._stale[compressed_tree_node.name] = set()\n\n        compressed_tree._scenario_map = \\\n            dict((scenario.name, scenario) for scenario in compressed_tree._scenarios)\n\n        #\n        # Handle re-normalization of probabilities if requested\n        #\n        if normalize:\n\n            # update conditional probabilities (leaf-to-root stage order)\n            for compressed_tree_stage in reversed(compressed_tree._stages[:-1]):\n\n                for compressed_tree_node in compressed_tree_stage._tree_nodes:\n                    norm_factor = \\\n                        sum(compressed_tree_child_node._conditional_probability\n                            for compressed_tree_child_node\n                            in compressed_tree_node._children)\n                    # the user may specify that the probability of a\n                    # scenario is 0.0, and while odd, we should allow the\n                    # edge case.\n                    if norm_factor == 0.0:\n                        for compressed_tree_child_node in \\\n                               compressed_tree_node._children:\n                            compressed_tree_child_node._conditional_probability = 0.0\n\n                    else:\n                        for compressed_tree_child_node in \\\n                               compressed_tree_node._children:\n                            compressed_tree_child_node.\\\n                                _conditional_probability /= norm_factor\n\n            assert abs(compressed_tree_root._probability - 1.0) < 1e-5\n            assert abs(compressed_tree_root._conditional_probability - 1.0) < 1e-5\n\n            # update absolute probabilities (root-to-leaf stage order)\n            for compressed_tree_stage in compressed_tree._stages[1:]:\n                for compressed_tree_node in compressed_tree_stage._tree_nodes:\n                    compressed_tree_node._probability = \\\n                            compressed_tree_node._parent._probability * \\\n                            compressed_tree_node._conditional_probability\n\n            # update scenario probabilities\n            for compressed_tree_scenario in compressed_tree._scenarios:\n                compressed_tree_scenario._probability = \\\n                    compressed_tree_scenario._leaf_node._probability\n\n        return compressed_tree\n\n    #\n    # Adds a bundle to this scenario tree by calling make compressed\n    # with normalize=True\n    # Returns a compressed tree using operations on the order of the\n    # number of nodes in the compressed tree rather than the number of\n    # nodes in the full tree (this method is more efficient than in-place\n    # compression). If normalize=True, all probabilities\n    # (and conditional probabilities) are renormalized.\n    #\n    #\n    def add_bundle(self, name, scenario_bundle_list):\n\n        if name in self._scenario_bundle_map:\n            raise ValueError(\"Cannot add a new bundle with name '%s', a bundle \"\n                             \"with that name already exists.\" % (name))\n\n        bundle_scenario_tree = self.make_compressed(scenario_bundle_list,\n                                                    normalize=True)\n        bundle = ScenarioTreeBundle()\n        bundle._name = name\n        bundle._scenario_names = scenario_bundle_list\n        bundle._scenario_tree = bundle_scenario_tree\n        # make sure this is computed with the un-normalized bundle scenarios\n        bundle._probability = sum(self._scenario_map[scenario_name]._probability\n                                  for scenario_name in scenario_bundle_list)\n\n        self._scenario_bundle_map[name] = bundle\n        self._scenario_bundles.append(bundle)\n\n    def remove_bundle(self, name):\n\n        if name not in self._scenario_bundle_map:\n            raise KeyError(\"Cannot remove bundle with name '%s', no bundle \"\n                           \"with that name exists.\" % (name))\n        bundle = self._scenario_bundle_map[name]\n        del self._scenario_bundle_map[name]\n        self._scenario_bundles.remove(bundle)\n\n    #\n    # utility for automatically selecting a proportion of scenarios from the\n    # tree to retain, eliminating the rest.\n    #\n\n    def downsample(self, fraction_to_retain, random_seed, verbose=False):\n\n        random_state = random.getstate()\n        random.seed(random_seed)\n        try:\n            number_to_retain = \\\n                max(int(round(float(len(self._scenarios)*fraction_to_retain))), 1)\n            random_list=random.sample(range(len(self._scenarios)), number_to_retain)\n\n            scenario_bundle_list = []\n            for i in range(number_to_retain):\n                scenario_bundle_list.append(self._scenarios[random_list[i]].name)\n\n            if verbose:\n                print(\"Downsampling scenario tree - retained %s \"\n                      \"scenarios: %s\"\n                      % (len(scenario_bundle_list),\n                         str(scenario_bundle_list)))\n\n            self.compress(scenario_bundle_list) # do the downsampling\n        finally:\n            random.setstate(random_state)\n\n\n    #\n    # returns the root node of the scenario tree\n    #\n\n    def findRootNode(self):\n\n        for tree_node in self._tree_nodes:\n            if tree_node._parent is None:\n                return tree_node\n        return None\n\n    #\n    # a utility function to compute, based on the current scenario tree content,\n    # the maximal length of identifiers in various categories.\n    #\n\n    def computeIdentifierMaxLengths(self):\n\n        self._max_scenario_id_length = 0\n        for scenario in self._scenarios:\n            if len(str(scenario.name)) > self._max_scenario_id_length:\n                self._max_scenario_id_length = len(str(scenario.name))\n\n    #\n    # a utility function to (partially, at the moment) validate a scenario tree\n    #\n\n    def validate(self):\n\n        # for any node, the sum of conditional probabilities of the children should sum to 1.\n        for tree_node in self._tree_nodes:\n            sum_probabilities = 0.0\n            if len(tree_node._children) > 0:\n                for child in tree_node._children:\n                    sum_probabilities += child._conditional_probability\n                if abs(1.0 - sum_probabilities) > 0.000001:\n                    raise ValueError(\"ScenarioTree validation failed. \"\n                                     \"Reason: child conditional \"\n                                     \"probabilities for tree node=%s \"\n                                     \" sum to %s\"\n                                     % (tree_node.name,\n                                        sum_probabilities))\n\n        # ensure that there is only one root node in the tree\n        num_roots = 0\n        root_ids = []\n        for tree_node in self._tree_nodes:\n            if tree_node._parent is None:\n                num_roots += 1\n                root_ids.append(tree_node.name)\n\n        if num_roots != 1:\n            raise ValueError(\"ScenarioTree validation failed. \"\n                             \"Reason: illegal set of root \"\n                             \"nodes detected: \" + str(root_ids))\n\n        # there must be at least one scenario passing through each tree node.\n        for tree_node in self._tree_nodes:\n            if len(tree_node._scenarios) == 0:\n                raise ValueError(\"ScenarioTree validation failed. \"\n                                 \"Reason: there are no scenarios \"\n                                 \"associated with tree node=%s\"\n                                 % (tree_node.name))\n                return False\n\n        return True\n\n    def create_random_bundles(self,\n                              num_bundles,\n                              random_seed):\n\n        random_state = random.getstate()\n        random.seed(random_seed)\n        try:\n            num_scenarios = len(self._scenarios)\n\n            sequence = list(range(num_scenarios))\n            random.shuffle(sequence)\n\n            next_scenario_index = 0\n\n            # this is a hack-ish way to re-initialize the Bundles set of a\n            # scenario tree instance, which should already be there\n            # (because it is defined in the abstract model).  however, we\n            # don't have a \"clear\" method on a set, so...\n            bundle_names = [\"Bundle\"+str(i)\n                            for i in range(1, num_bundles+1)]\n            bundles = OrderedDict()\n            for i in range(num_bundles):\n                bundles[bundle_names[i]] = []\n\n            scenario_index = 0\n            while (scenario_index < num_scenarios):\n                for bundle_index in range(num_bundles):\n                    if (scenario_index == num_scenarios):\n                        break\n                    bundles[bundle_names[bundle_index]].append(\n                        self._scenarios[sequence[scenario_index]].name)\n                    scenario_index += 1\n\n            self._construct_scenario_bundles(bundles)\n        finally:\n            random.setstate(random_state)\n\n    #\n    # a utility function to pretty-print the static/non-cost\n    # information associated with a scenario tree\n    #\n\n    def pprint(self):\n\n        print(\"Scenario Tree Detail\")\n\n        print(\"----------------------------------------------------\")\n        print(\"Tree Nodes:\")\n        print(\"\")\n        for tree_node_name in sorted(self._tree_node_map.keys()):\n            tree_node = self._tree_node_map[tree_node_name]\n            print(\"\\tName=%s\" % (tree_node_name))\n            if tree_node._stage is not None:\n                print(\"\\tStage=%s\" % (tree_node._stage._name))\n            else:\n                print(\"\\t Stage=None\")\n            if tree_node._parent is not None:\n                print(\"\\tParent=%s\" % (tree_node._parent._name))\n            else:\n                print(\"\\tParent=\" + \"None\")\n            if tree_node._conditional_probability is not None:\n                print(\"\\tConditional probability=%4.4f\" % tree_node._conditional_probability)\n            else:\n                print(\"\\tConditional probability=\" + \"***Undefined***\")\n            print(\"\\tChildren:\")\n            if len(tree_node._children) > 0:\n                for child_node in sorted(tree_node._children, key=lambda x: x._name):\n                    print(\"\\t\\t%s\" % (child_node._name))\n            else:\n                print(\"\\t\\tNone\")\n            print(\"\\tScenarios:\")\n            if len(tree_node._scenarios) == 0:\n                print(\"\\t\\tNone\")\n            else:\n                for scenario in sorted(tree_node._scenarios, key=lambda x: x._name):\n                    print(\"\\t\\t%s\" % (scenario._name))\n            if len(tree_node._variable_templates) > 0:\n                print(\"\\tVariables: \")\n                for variable_name in sorted(tree_node._variable_templates.keys()):\n                    match_templates = tree_node._variable_templates[variable_name]\n                    sys.stdout.write(\"\\t\\t \"+variable_name+\" : \")\n                    for match_template in match_templates:\n                       sys.stdout.write(indexToString(match_template)+' ')\n                    print(\"\")\n            if len(tree_node._derived_variable_templates) > 0:\n                print(\"\\tDerived Variables: \")\n                for variable_name in sorted(tree_node._derived_variable_templates.keys()):\n                    match_templates = tree_node._derived_variable_templates[variable_name]\n                    sys.stdout.write(\"\\t\\t \"+variable_name+\" : \")\n                    for match_template in match_templates:\n                       sys.stdout.write(indexToString(match_template)+' ')\n                    print(\"\")\n            print(\"\")\n        print(\"----------------------------------------------------\")\n        print(\"Stages:\")\n        for stage_name in sorted(self._stage_map.keys()):\n            stage = self._stage_map[stage_name]\n            print(\"\\tName=%s\" % (stage_name))\n            print(\"\\tTree Nodes: \")\n            for tree_node in sorted(stage._tree_nodes, key=lambda x: x._name):\n                print(\"\\t\\t%s\" % (tree_node._name))\n            if len(stage._variable_templates) > 0:\n                print(\"\\tVariables: \")\n                for variable_name in sorted(stage._variable_templates.keys()):\n                    match_templates = stage._variable_templates[variable_name]\n                    sys.stdout.write(\"\\t\\t \"+variable_name+\" : \")\n                    for match_template in match_templates:\n                       sys.stdout.write(indexToString(match_template)+' ')\n                    print(\"\")\n            if len(stage._derived_variable_templates) > 0:\n                print(\"\\tDerived Variables: \")\n                for variable_name in sorted(stage._derived_variable_templates.keys()):\n                    match_templates = stage._derived_variable_templates[variable_name]\n                    sys.stdout.write(\"\\t\\t \"+variable_name+\" : \")\n                    for match_template in match_templates:\n                       sys.stdout.write(indexToString(match_template)+' ')\n                    print(\"\")\n            print(\"\\tCost Variable: \")\n            if stage._cost_variable is not None:\n                cost_variable_name, cost_variable_index = stage._cost_variable\n            else:\n                # kind of a hackish way to get around the fact that we are transitioning\n                # away from storing the cost_variable identifier on the stages\n                cost_variable_name, cost_variable_index = stage.nodes[0]._cost_variable\n            if cost_variable_index is None:\n                print(\"\\t\\t\" + cost_variable_name)\n            else:\n                print(\"\\t\\t\" + cost_variable_name + indexToString(cost_variable_index))\n            print(\"\")\n        print(\"----------------------------------------------------\")\n        print(\"Scenarios:\")\n        for scenario_name in sorted(self._scenario_map.keys()):\n            scenario = self._scenario_map[scenario_name]\n            print(\"\\tName=%s\" % (scenario_name))\n            print(\"\\tProbability=%4.4f\" % scenario._probability)\n            if scenario._leaf_node is None:\n                print(\"\\tLeaf node=None\")\n            else:\n                print(\"\\tLeaf node=%s\" % (scenario._leaf_node._name))\n            print(\"\\tTree node sequence:\")\n            for tree_node in scenario._node_list:\n                print(\"\\t\\t%s\" % (tree_node._name))\n            print(\"\")\n        print(\"----------------------------------------------------\")\n        if len(self._scenario_bundles) > 0:\n            print(\"Scenario Bundles:\")\n            for bundle_name in sorted(self._scenario_bundle_map.keys()):\n                scenario_bundle = self._scenario_bundle_map[bundle_name]\n                print(\"\\tName=%s\" % (bundle_name))\n                print(\"\\tProbability=%4.4f\" % scenario_bundle._probability            )\n                sys.stdout.write(\"\\tScenarios:  \")\n                for scenario_name in sorted(scenario_bundle._scenario_names):\n                    sys.stdout.write(str(scenario_name)+' ')\n                sys.stdout.write(\"\\n\")\n                print(\"\")\n            print(\"----------------------------------------------------\")\n\n\n    #\n    # Save the tree structure in DOT file format\n    # Nodes are labeled with absolute probabilities and\n    # edges are labeled with conditional probabilities\n    #\n    def save_to_dot(self, filename):\n\n        def _visit_node(node):\n            f.write(\"%s%s [label=\\\"%s\\\"];\\n\"\n                    % (node.name,\n                       id(node),\n                       str(node.name)+(\"\\n(%.6g)\" % (node._probability))))\n            for child_node in node._children:\n                _visit_node(child_node)\n                f.write(\"%s%s -> %s%s [label=\\\"%.6g\\\"];\\n\"\n                        % (node.name,\n                           id(node),\n                           child_node.name,\n                           id(child_node),\n                           child_node._conditional_probability))\n            if len(node._children) == 0:\n                assert len(node._scenarios) == 1\n                scenario = node._scenarios[0]\n                f.write(\"%s%s [label=\\\"%s\\\"];\\n\"\n                        % (scenario.name,\n                           id(scenario),\n                           \"scenario\\n\"+str(scenario.name)))\n                f.write(\"%s%s -> %s%s [style=dashed];\\n\"\n                        % (node.name,\n                           id(node),\n                           scenario.name,\n                           id(scenario)))\n\n        with open(filename, 'w') as f:\n\n            f.write(\"digraph ScenarioTree {\\n\")\n            root_node = self.findRootNode()\n            _visit_node(root_node)\n            f.write(\"}\\n\")",
  "def __init__(self):\n        self._cuid_map = ComponentMap()",
  "def update_cache(self, block):\n        self._cuid_map.update(\n            ComponentUID.generate_cuid_string_map(\n                block, repr_version=CUID_repr_version))",
  "def clear_cache(self):\n        self._cuid_map = {}",
  "def __call__(self, obj):\n        if obj in self._cuid_map:\n            return self._cuid_map[obj]\n        else:\n            cuid = ComponentUID(obj).get_repr(version=1)\n            self._cuid_map[obj] = cuid\n            return cuid",
  "def __init__(self, name, conditional_probability, stage):\n\n        # self-explanatory!\n        self._name = name\n\n        # the stage to which this tree node belongs.\n        self._stage = stage\n\n        # defines the tree structure\n        self._parent = None\n\n        # a collection of ScenarioTreeNodes\n        self._children = []\n\n        # conditional on parent\n        self._conditional_probability = conditional_probability\n\n        # a collection of all Scenario objects passing through this\n        # node in the tree\n        self._scenarios = []\n\n        # the cumulative probability of scenarios at this node.\n        # cached for efficiency.\n        self._probability = 0.0\n\n        # a map between a variable name and a list of original index\n        # match templates, specified as strings.  we want to maintain\n        # these for a variety of reasons, perhaps the most important\n        # being that for output purposes. specific indices that match\n        # belong to the tree node, as that may be specific to a tree\n        # node.\n        self._variable_templates = {}\n        self._derived_variable_templates = {}\n\n        #\n        # information relating to all variables blended at this node, whether\n        # of the standard or derived varieties.\n        #\n        # maps id -> (name, index)\n        self._variable_ids = {}\n        # maps (name,index) -> id\n        self._name_index_to_id = {}\n        # maps id -> list of (vardata,probability) across all scenarios\n        self._variable_datas = {}\n\n        # keep track of the variable indices at this node, independent\n        # of type.  this is useful for iterating. maps variable name\n        # to a list of indices.\n        self._variable_indices = {}\n\n        # variables are either standard or derived - but not both.\n        # partition the ids into two sets, as we deal with these\n        # differently in algorithmic and reporting contexts.\n        self._standard_variable_ids = set()\n        self._derived_variable_ids = set()\n        # A temporary solution to help wwphextension and other code\n        # for when pyomo instances no longer live on the master node\n        # when using PHPyro\n        self._integer = set()\n        self._binary = set()\n        self._semicontinuous = set()\n\n        # a tuple consisting of (1) the name of the variable that\n        # stores the stage-specific cost in all scenarios and (2) the\n        # corresponding index *string* - this is converted in the tree\n        # node to a real index.\n        # TODO: Change the code so that this is a ComponentUID string\n        self._cost_variable = None\n\n        # a list of _VarData objects, representing the cost variables\n        # for each scenario passing through this tree node.\n        # NOTE: This list actually contains tuples of\n        #       (_VarData, scenario-probability) pairs.\n        self._cost_variable_datas = []\n\n        # node variables ids that are fixed (along with the value to fix)\n        self._fixed = {}",
  "def name(self):\n        return self._name",
  "def stage(self):\n        return self._stage",
  "def parent(self):\n        return self._parent",
  "def children(self):\n        return tuple(self._children)",
  "def scenarios(self):\n        return self._scenarios",
  "def conditional_probability(self):\n        return self._conditional_probability",
  "def probability(self):\n        return self._probability",
  "def is_leaf_node(self):\n\n        return self._stage.is_last_stage()",
  "def is_derived_variable(self, variable_name, variable_index):\n        return (variable_name, variable_index) in self._name_index_to_id",
  "def __init__(self):\n\n        self._name = \"\"\n\n        # a collection of ScenarioTreeNode objects associated with this stage.\n        self._tree_nodes = []\n\n        # the parent scenario tree for this stage.\n        self._scenario_tree = None\n\n        # a map between a variable name and a list of original index\n        # match templates, specified as strings.  we want to maintain\n        # these for a variety of reasons, perhaps the most important\n        # being that for output purposes. specific indices that match\n        # belong to the tree node, as that may be specific to a tree\n        # node.\n        self._variable_templates = {}\n\n        # same as above, but for derived stage variables.\n        self._derived_variable_templates = {}\n\n        # a tuple consisting of (1) the name of the variable that\n        # stores the stage-specific cost in all scenarios and (2) the\n        # corresponding index *string* - this is converted in the tree\n        # node to a real index.\n        self._cost_variable = None",
  "def name(self):\n        return self._name",
  "def nodes(self):\n        return self._tree_nodes",
  "def scenario_tree(self):\n        return self._scenario_tree",
  "def is_last_stage(self):\n\n        return self == self._scenario_tree._stages[-1]",
  "def __init__(self):\n\n        self._name = None\n        # allows for construction of node list\n        self._leaf_node = None\n        # sequence from parent to leaf of ScenarioTreeNodes\n        self._node_list = []\n        # the unconditional probability for this scenario, computed from the node list\n        self._probability = 0.0\n        # the Pyomo instance corresponding to this scenario.\n        self._instance = None\n        self._instance_cost_expression = None\n        self._instance_objective = None\n        self._instance_original_objective_object = None\n        self._objective_sense = None\n        self._objective_name = None\n\n        # The value of the (possibly augmented) objective function\n        self._objective = None\n        # The value of the original objective expression\n        # (which should be the sum of the stage costs)\n        self._cost = None\n        # The individual stage cost values\n        self._stage_costs = {}\n        # The value of the ph weight term piece of the objective (if it exists)\n        self._weight_term_cost = None\n        # The value of the ph proximal term piece of the objective (if it exists)\n        self._proximal_term_cost = None\n        # The value of the scenariotree variables belonging to this scenario\n        # (dictionary nested by node name)\n        self._x = {}\n        # The value of the weight terms belonging to this scenario\n        # (dictionary nested by node name)\n        self._w = {}\n        # The value of the rho terms belonging to this scenario\n        # (dictionary nested by node name)\n        self._rho = {}\n\n        # This set of fixed or reported stale variables\n        # in each tree node\n        self._fixed = {}\n        self._stale = {}",
  "def name(self):\n        return self._name",
  "def leaf_node(self):\n        return self._leaf_node",
  "def node_list(self):\n        return tuple(self._node_list)",
  "def probability(self):\n        return self._probability",
  "def instance(self):\n        return self._instance",
  "def get_current_objective(self):\n        return self._objective",
  "def get_current_cost(self):\n        return self._cost",
  "def get_current_stagecost(self, stage_name):\n        return self._stage_costs[stage_name]",
  "def node_stage_index(self, tree_node):\n        return self._node_list.index(tree_node)",
  "def __init__(self):\n\n        self._name = None\n        self._scenario_names = []\n        # This is a compressed scenario tree, just for the bundle.\n        self._scenario_tree = None\n        # the absolute probability of scenarios associated with this\n        # node in the scenario tree.\n        self._probability = 0.0",
  "def name(self):\n        return self._name",
  "def scenario_names(self):\n        return self._scenario_names",
  "def scenario_tree(self):\n        return self._scenario_tree",
  "def probability(self):\n        return self._probability",
  "def _construct_scenario_bundles(self, bundles):\n\n        for bundle_name in bundles:\n\n            scenario_list = []\n            bundle_probability = 0.0\n            for scenario_name in bundles[bundle_name]:\n                scenario_list.append(scenario_name)\n                bundle_probability += \\\n                    self._scenario_map[scenario_name].probability\n\n            scenario_tree_for_bundle = self.make_compressed(scenario_list,\n                                                            normalize=True)\n\n            scenario_tree_for_bundle.validate()\n\n            new_bundle = ScenarioTreeBundle()\n            new_bundle._name = bundle_name\n            new_bundle._scenario_names = scenario_list\n            new_bundle._scenario_tree = scenario_tree_for_bundle\n            new_bundle._probability = bundle_probability\n\n            self._scenario_bundles.append(new_bundle)\n            self._scenario_bundle_map[new_bundle.name] = new_bundle",
  "def _construct_stages(self,\n                          stage_names,\n                          stage_variable_names,\n                          stage_cost_variable_names,\n                          stage_derived_variable_names):\n\n        # construct the stage objects, which will leave them\n        # largely uninitialized - no variable information, in particular.\n        for stage_name in stage_names:\n\n            new_stage = ScenarioTreeStage()\n            new_stage._name = stage_name\n            new_stage._scenario_tree = self\n\n            for variable_string in stage_variable_names[stage_name]:\n                if isVariableNameIndexed(variable_string):\n                    variable_name, match_template = \\\n                        extractVariableNameAndIndex(variable_string)\n                else:\n                    variable_name = variable_string\n                    match_template = \"\"\n                if variable_name not in new_stage._variable_templates:\n                    new_stage._variable_templates[variable_name] = []\n                new_stage._variable_templates[variable_name].append(match_template)\n\n            # not all stages have derived variables defined\n            if stage_name in stage_derived_variable_names:\n                for variable_string in stage_derived_variable_names[stage_name]:\n                    if isVariableNameIndexed(variable_string):\n                        variable_name, match_template = \\\n                            extractVariableNameAndIndex(variable_string)\n                    else:\n                        variable_name = variable_string\n                        match_template = \"\"\n                    if variable_name not in new_stage._derived_variable_templates:\n                        new_stage._derived_variable_templates[variable_name] = []\n                    new_stage._derived_variable_templates[variable_name].append(match_template)\n\n            # de-reference is required to access the parameter value\n            # TBD March 2020: make it so the stages always know their cost names.\n            # dlw March 2020: when coming from NetworkX, we don't know these yet!!\n            cost_variable_string = stage_cost_variable_names[stage_name].value\n            if cost_variable_string is not None:\n                if isVariableNameIndexed(cost_variable_string):\n                    cost_variable_name, cost_variable_index = \\\n                        extractVariableNameAndIndex(cost_variable_string)\n                else:\n                    cost_variable_name = cost_variable_string\n                    cost_variable_index = None\n                new_stage._cost_variable = (cost_variable_name, cost_variable_index)\n\n            self._stages.append(new_stage)\n            self._stage_map[stage_name] = new_stage",
  "def __init__(self,\n                 scenariotreeinstance=None,\n                 scenariobundlelist=None):\n\n        # some arbitrary identifier\n        self._name = None\n\n        # should be called once for each variable blended across a node\n        #self._id_labeler = CounterLabeler()\n        self._id_labeler = _CUIDLabeler()\n\n        #\n        # the core objects defining the scenario tree.\n        #\n\n        # collection of ScenarioTreeNodes\n        self._tree_nodes = []\n        # collection of ScenarioTreeStages - assumed to be in\n        # time-order. the set (provided by the user) itself *must* be\n        # ordered.\n        self._stages = []\n        # collection of Scenarios\n        self._scenarios = []\n        # collection of ScenarioTreeBundles\n        self._scenario_bundles = []\n\n        # dictionaries for the above.\n        self._tree_node_map = {}\n        self._stage_map = {}\n        self._scenario_map = {}\n        self._scenario_bundle_map = {}\n\n        # a boolean indicating how data for scenario instances is specified.\n        # possibly belongs elsewhere, e.g., in the PH algorithm.\n        self._scenario_based_data = None\n\n        if scenariotreeinstance is None:\n            assert scenariobundlelist is None\n            return\n\n        node_ids = scenariotreeinstance.Nodes\n        node_child_ids = scenariotreeinstance.Children\n        node_stage_ids = scenariotreeinstance.NodeStage\n        node_probability_map = scenariotreeinstance.ConditionalProbability\n        stage_ids = scenariotreeinstance.Stages\n        stage_variable_ids = scenariotreeinstance.StageVariables\n        node_variable_ids = scenariotreeinstance.NodeVariables\n        stage_cost_variable_ids = scenariotreeinstance.StageCost\n        node_cost_variable_ids = scenariotreeinstance.NodeCost\n        if any(scenariotreeinstance.StageCostVariable[i].value is not None\n               for i in scenariotreeinstance.StageCostVariable):\n            logger.warning(\"DEPRECATED: The 'StageCostVariable' scenario tree \"\n                           \"model parameter has been renamed to 'StageCost'. \"\n                           \"Please update your scenario tree structure model.\")\n            if any(stage_cost_variable_ids[i].value is not None\n                   for i in stage_cost_variable_ids):\n                raise ValueError(\"The 'StageCostVariable' and 'StageCost' \"\n                                 \"parameters can not both be used on a scenario \"\n                                 \"tree structure model.\")\n            else:\n                stage_cost_variable_ids = scenariotreeinstance.StageCostVariable\n\n        if any(stage_cost_variable_ids[i].value is not None\n               for i in stage_cost_variable_ids) and \\\n           any(node_cost_variable_ids[i].value is not None\n               for i in node_cost_variable_ids):\n            raise ValueError(\n                \"The 'StageCost' and 'NodeCost' parameters \"\n                \"can not both be used on a scenario tree \"\n                \"structure model.\")\n        stage_derived_variable_ids = scenariotreeinstance.StageDerivedVariables\n        node_derived_variable_ids = scenariotreeinstance.NodeDerivedVariables\n        scenario_ids = scenariotreeinstance.Scenarios\n        scenario_leaf_ids = scenariotreeinstance.ScenarioLeafNode\n        scenario_based_data = scenariotreeinstance.ScenarioBasedData\n\n        # save the method for instance data storage.\n        self._scenario_based_data = scenario_based_data()\n\n        # the input stages must be ordered, for both output purposes\n        # and knowledge of the final stage.\n        if not stage_ids.isordered():\n            raise ValueError(\n                \"An ordered set of stage IDs must be supplied in \"\n                \"the ScenarioTree constructor\")\n\n        for node_id in node_ids:\n            node_stage_id = node_stage_ids[node_id].value\n            if node_stage_id != stage_ids.last():\n                if (len(stage_variable_ids[node_stage_id]) == 0) and \\\n                   (len(node_variable_ids[node_id]) == 0):\n                    raise ValueError(\n                        \"Scenario tree node %s, belonging to stage %s, \"\n                        \"has not been declared with any variables. \"\n                        \"To fix this error, make sure that one of \"\n                        \"the sets StageVariables[%s] or NodeVariables[%s] \"\n                        \"is declared with at least one variable string \"\n                        \"template (e.g., x, x[*]) on the scenario tree \"\n                        \"or in ScenarioStructure.dat.\"\n                        % (node_id, node_stage_id, node_stage_id, node_id))\n\n        #\n        # construct the actual tree objects\n        #\n\n        # construct the stage objects w/o any linkages first; link them up\n        # with tree nodes after these have been fully constructed.\n        self._construct_stages(stage_ids,\n                               stage_variable_ids,\n                               stage_cost_variable_ids,\n                               stage_derived_variable_ids)\n\n        # construct the tree node objects themselves in a first pass,\n        # and then link them up in a second pass to form the tree.\n        # can't do a single pass because the objects may not exist.\n        for tree_node_name in node_ids:\n\n            if tree_node_name not in node_stage_ids:\n                raise ValueError(\"No stage is assigned to tree node=%s\"\n                                 % (tree_node_name))\n\n            stage_name = value(node_stage_ids[tree_node_name])\n            if stage_name not in self._stage_map:\n                raise ValueError(\"Unknown stage=%s assigned to tree node=%s\"\n                                 % (stage_name, tree_node_name))\n\n            node_stage = self._stage_map[stage_name]\n            new_tree_node = ScenarioTreeNode(\n                tree_node_name,\n                value(node_probability_map[tree_node_name]),\n                node_stage)\n\n            # extract the node variable match templates\n            for variable_string in node_variable_ids[tree_node_name]:\n                if isVariableNameIndexed(variable_string):\n                    variable_name, match_template = \\\n                        extractVariableNameAndIndex(variable_string)\n                else:\n                    variable_name = variable_string\n                    match_template = \"\"\n                if variable_name not in new_tree_node._variable_templates:\n                    new_tree_node._variable_templates[variable_name] = []\n                new_tree_node._variable_templates[variable_name].append(match_template)\n\n            cost_variable_string = node_cost_variable_ids[tree_node_name].value\n            if cost_variable_string is not None:\n                assert node_stage._cost_variable is None\n                if isVariableNameIndexed(cost_variable_string):\n                    cost_variable_name, cost_variable_index = \\\n                        extractVariableNameAndIndex(cost_variable_string)\n                else:\n                    cost_variable_name = cost_variable_string\n                    cost_variable_index = None\n            else:\n                assert node_stage._cost_variable is not None\n                cost_variable_name, cost_variable_index = \\\n                    node_stage._cost_variable\n            new_tree_node._cost_variable = (cost_variable_name, cost_variable_index)\n\n            # extract the node derived variable match templates\n            for variable_string in node_derived_variable_ids[tree_node_name]:\n                if isVariableNameIndexed(variable_string):\n                    variable_name, match_template = \\\n                        extractVariableNameAndIndex(variable_string)\n                else:\n                    variable_name = variable_string\n                    match_template = \"\"\n                if variable_name not in new_tree_node._derived_variable_templates:\n                    new_tree_node._derived_variable_templates[variable_name] = []\n                new_tree_node._derived_variable_templates[variable_name].append(match_template)\n\n            self._tree_nodes.append(new_tree_node)\n            self._tree_node_map[tree_node_name] = new_tree_node\n            self._stage_map[stage_name]._tree_nodes.append(new_tree_node)\n\n        # link up the tree nodes objects based on the child id sets.\n        for this_node in self._tree_nodes:\n            this_node._children = []\n            # otherwise, you're at a leaf and all is well.\n            if this_node.name in node_child_ids:\n                child_ids = node_child_ids[this_node.name]\n                for child_id in child_ids:\n                    if child_id in self._tree_node_map:\n                        child_node = self._tree_node_map[child_id]\n                        this_node._children.append(child_node)\n                        if child_node._parent is None:\n                            child_node._parent = this_node\n                        else:\n                            raise ValueError(\n                                \"Multiple parents specified for tree node=%s; \"\n                                \"existing parent node=%s; conflicting parent \"\n                                \"node=%s\"\n                                % (child_id,\n                                   child_node._parent.name,\n                                   this_node.name))\n                    else:\n                        raise ValueError(\"Unknown child tree node=%s specified \"\n                                         \"for tree node=%s\"\n                                         % (child_id, this_node.name))\n\n        # at this point, the scenario tree nodes and the stages are set - no\n        # two-pass logic necessary when constructing scenarios.\n        for scenario_name in scenario_ids:\n            new_scenario = Scenario()\n            new_scenario._name = scenario_name\n\n            if scenario_name not in scenario_leaf_ids:\n                raise ValueError(\"No leaf tree node specified for scenario=%s\"\n                                 % (scenario_name))\n            else:\n                scenario_leaf_node_name = value(scenario_leaf_ids[scenario_name])\n                if scenario_leaf_node_name not in self._tree_node_map:\n                    raise ValueError(\"Uknown tree node=%s specified as leaf \"\n                                     \"of scenario=%s\" %\n                                     (scenario_leaf_node_name, scenario_name))\n                else:\n                    new_scenario._leaf_node = \\\n                        self._tree_node_map[scenario_leaf_node_name]\n\n            current_node = new_scenario._leaf_node\n            while current_node is not None:\n                new_scenario._node_list.append(current_node)\n                # links the scenarios to the nodes to enforce\n                # necessary non-anticipativity\n                current_node._scenarios.append(new_scenario)\n                current_node = current_node._parent\n            new_scenario._node_list.reverse()\n            # This now loops root -> leaf\n            probability = 1.0\n            for current_node in new_scenario._node_list:\n                probability *= current_node._conditional_probability\n                # NOTE: The line placement below is a little weird, in that\n                #       it is embedded in a scenario loop - so the probabilities\n                #       for some nodes will be redundantly computed. But this works.\n                current_node._probability = probability\n\n                new_scenario._stage_costs[current_node.stage.name] = None\n                new_scenario._x[current_node.name] = {}\n                new_scenario._w[current_node.name] = {}\n                new_scenario._rho[current_node.name] = {}\n                new_scenario._fixed[current_node.name] = set()\n                new_scenario._stale[current_node.name] = set()\n\n            new_scenario._probability = probability\n\n            self._scenarios.append(new_scenario)\n            self._scenario_map[scenario_name] = new_scenario\n\n        # for output purposes, it is useful to known the maximal\n        # length of identifiers in the scenario tree for any\n        # particular category. I'm building these up incrementally, as\n        # they are needed. 0 indicates unassigned.\n        self._max_scenario_id_length = 0\n\n        # does the actual traversal to populate the members.\n        self.computeIdentifierMaxLengths()\n\n        # if a sub-bundle of scenarios has been specified, mark the\n        # active scenario tree components and compress the tree.\n        if scenariobundlelist is not None:\n            self.compress(scenariobundlelist)\n\n        # NEW SCENARIO BUNDLING STARTS HERE\n        if value(scenariotreeinstance.Bundling[None]):\n            bundles = OrderedDict()\n            for bundle_name in scenariotreeinstance.Bundles:\n                bundles[bundle_name] = \\\n                    list(scenariotreeinstance.BundleScenarios[bundle_name])\n            self._construct_scenario_bundles(bundles)",
  "def scenarios(self):\n        return self._scenarios",
  "def bundles(self):\n        return self._scenario_bundles",
  "def subproblems(self):\n        if self.contains_bundles():\n            return self._scenario_bundles\n        else:\n            return self._scenarios",
  "def stages(self):\n        return self._stages",
  "def nodes(self):\n        return self._tree_nodes",
  "def is_bundle(self, object_name):\n        return object_name in self._scenario_bundle_map",
  "def is_scenario(self, object_name):\n        return object_name in self._scenario_map",
  "def contains_scenario(self, name):\n        return name in self._scenario_map",
  "def contains_bundles(self):\n        return len(self._scenario_bundle_map) > 0",
  "def contains_bundle(self, name):\n        return name in self._scenario_bundle_map",
  "def get_scenario(self, name):\n        return self._scenario_map[name]",
  "def get_bundle(self, name):\n        return self._scenario_bundle_map[name]",
  "def get_subproblem(self, name):\n        if self.contains_bundles():\n            return self._scenario_bundle_map[name]\n        else:\n            return self._scenario_map[name]",
  "def get_scenario_bundle(self, name):\n        if not self.contains_bundles():\n            return None\n        else:\n            return self._scenario_bundle_map[name]",
  "def get_arbitrary_scenario(self):\n        return self._scenarios[0]",
  "def contains_node(self, name):\n        return name in self._tree_node_map",
  "def get_node(self, name):\n        return self._tree_node_map[name]",
  "def compress(self,\n                 scenario_bundle_list,\n                 normalize=True):\n\n        # scan for and mark all referenced scenarios and\n        # tree nodes in the bundle list - all stages will\n        # obviously remain.\n        try:\n\n            for scenario_name in scenario_bundle_list:\n\n                scenario = self._scenario_map[scenario_name]\n                scenario.retain = True\n\n                # chase all nodes comprising this scenario,\n                # marking them for retention.\n                for node in scenario._node_list:\n                    node.retain = True\n\n        except KeyError:\n            raise ValueError(\"Scenario=%s selected for \"\n                             \"bundling not present in \"\n                             \"scenario tree\"\n                             % (scenario_name))\n\n        # scan for any non-retained scenarios and tree nodes.\n        scenarios_to_delete = []\n        tree_nodes_to_delete = []\n        for scenario in self._scenarios:\n            if hasattr(scenario, \"retain\"):\n                delattr(scenario, \"retain\")\n            else:\n                scenarios_to_delete.append(scenario)\n                del self._scenario_map[scenario.name]\n\n        for tree_node in self._tree_nodes:\n            if hasattr(tree_node, \"retain\"):\n                delattr(tree_node, \"retain\")\n            else:\n                tree_nodes_to_delete.append(tree_node)\n                del self._tree_node_map[tree_node.name]\n\n        # JPW does not claim the following routines are\n        # the most efficient. rather, they get the job\n        # done while avoiding serious issues with\n        # attempting to remove elements from a list that\n        # you are iterating over.\n\n        # delete all references to unmarked scenarios\n        # and child tree nodes in the scenario tree node\n        # structures.\n        for tree_node in self._tree_nodes:\n            for scenario in scenarios_to_delete:\n                if scenario in tree_node._scenarios:\n                    tree_node._scenarios.remove(scenario)\n            for node_to_delete in tree_nodes_to_delete:\n                if node_to_delete in tree_node._children:\n                    tree_node._children.remove(node_to_delete)\n\n        # delete all references to unmarked tree nodes\n        # in the scenario tree stage structures.\n        for stage in self._stages:\n            for tree_node in tree_nodes_to_delete:\n                if tree_node in stage._tree_nodes:\n                    stage._tree_nodes.remove(tree_node)\n\n        # delete all unreferenced entries from the core scenario\n        # tree data structures.\n        for scenario in scenarios_to_delete:\n            self._scenarios.remove(scenario)\n        for tree_node in tree_nodes_to_delete:\n            self._tree_nodes.remove(tree_node)\n\n        #\n        # Handle re-normalization of probabilities if requested\n        #\n        if normalize:\n\n            # re-normalize the conditional probabilities of the\n            # children at each tree node (leaf-to-root stage order).\n            for stage in reversed(self._stages[:-1]):\n\n                for tree_node in stage._tree_nodes:\n                    norm_factor = sum(child_tree_node._conditional_probability\n                                      for child_tree_node\n                                      in tree_node._children)\n                    # the user may specify that the probability of a\n                    # scenario is 0.0, and while odd, we should allow the\n                    # edge case.\n                    if norm_factor == 0.0:\n                        for child_tree_node in tree_node._children:\n                            child_tree_node._conditional_probability = 0.0\n                    else:\n                        for child_tree_node in tree_node._children:\n                            child_tree_node._conditional_probability /= norm_factor\n\n            # update absolute probabilities (root-to-leaf stage order)\n            for stage in self._stages[1:]:\n                for tree_node in stage._tree_nodes:\n                    tree_node._probability = \\\n                        tree_node._parent._probability * \\\n                        tree_node._conditional_probability\n\n            # update scenario probabilities\n            for scenario in self._scenarios:\n                scenario._probability = \\\n                    scenario._leaf_node._probability\n\n        # now that we've culled the scenarios, cull the bundles. do\n        # this in two passes. in the first pass, we identify the names\n        # of bundles to delete, by looking for bundles with deleted\n        # scenarios. in the second pass, we delete the bundles from\n        # the scenario tree, and normalize the probabilities of the\n        # remaining bundles.\n\n        # indices of the objects in the scenario tree bundle list\n        bundles_to_delete = []\n        for i in range(0,len(self._scenario_bundles)):\n            scenario_bundle = self._scenario_bundles[i]\n            for scenario_name in scenario_bundle._scenario_names:\n                if scenario_name not in self._scenario_map:\n                    bundles_to_delete.append(i)\n                    break\n        bundles_to_delete.reverse()\n        for i in bundles_to_delete:\n            deleted_bundle = self._scenario_bundles.pop(i)\n            del self._scenario_bundle_map[deleted_bundle.name]\n\n        sum_bundle_probabilities = \\\n            sum(bundle._probability for bundle in self._scenario_bundles)\n        for bundle in self._scenario_bundles:\n            bundle._probability /= sum_bundle_probabilities",
  "def make_compressed(self,\n                        scenario_bundle_list,\n                        normalize=False):\n\n        compressed_tree = ScenarioTree()\n        compressed_tree._scenario_based_data = self._scenario_based_data\n        #\n        # Copy Stage Data\n        #\n        for stage in self._stages:\n            # copy everything but the list of tree nodes\n            # and the reference to the scenario tree\n            compressed_tree_stage = ScenarioTreeStage()\n            compressed_tree_stage._name = stage.name\n            compressed_tree_stage._variable_templates = copy.deepcopy(stage._variable_templates)\n            compressed_tree_stage._derived_variable_templates = \\\n                copy.deepcopy(stage._derived_variable_templates)\n            compressed_tree_stage._cost_variable = copy.deepcopy(stage._cost_variable)\n            # add the stage object to the compressed tree\n            compressed_tree._stages.append(compressed_tree_stage)\n            compressed_tree._stages[-1]._scenario_tree = compressed_tree\n\n        compressed_tree._stage_map = \\\n            dict((stage.name, stage) for stage in compressed_tree._stages)\n\n        #\n        # Copy Scenario and Node Data\n        #\n        compressed_tree_root = None\n        for scenario_name in scenario_bundle_list:\n            full_tree_scenario = self.get_scenario(scenario_name)\n\n            compressed_tree_scenario = Scenario()\n            compressed_tree_scenario._name = full_tree_scenario.name\n            compressed_tree_scenario._probability = full_tree_scenario._probability\n            compressed_tree._scenarios.append(compressed_tree_scenario)\n\n            full_tree_node = full_tree_scenario._leaf_node\n            ### copy the node\n            compressed_tree_node = ScenarioTreeNode(\n                full_tree_node.name,\n                full_tree_node._conditional_probability,\n                compressed_tree._stage_map[full_tree_node._stage.name])\n            compressed_tree_node._variable_templates = \\\n                copy.deepcopy(full_tree_node._variable_templates)\n            compressed_tree_node._derived_variable_templates = \\\n                copy.deepcopy(full_tree_node._derived_variable_templates)\n            compressed_tree_node._scenarios.append(compressed_tree_scenario)\n            compressed_tree_node._stage._tree_nodes.append(compressed_tree_node)\n            compressed_tree_node._probability = full_tree_node._probability\n            compressed_tree_node._cost_variable = full_tree_node._cost_variable\n            ###\n\n            compressed_tree_scenario._node_list.append(compressed_tree_node)\n            compressed_tree_scenario._leaf_node = compressed_tree_node\n            compressed_tree._tree_nodes.append(compressed_tree_node)\n            compressed_tree._tree_node_map[compressed_tree_node.name] = \\\n                compressed_tree_node\n\n            previous_compressed_tree_node = compressed_tree_node\n            full_tree_node = full_tree_node._parent\n            while full_tree_node.name not in compressed_tree._tree_node_map:\n\n                ### copy the node\n                compressed_tree_node = ScenarioTreeNode(\n                    full_tree_node.name,\n                    full_tree_node._conditional_probability,\n                    compressed_tree._stage_map[full_tree_node.stage.name])\n                compressed_tree_node._variable_templates = \\\n                    copy.deepcopy(full_tree_node._variable_templates)\n                compressed_tree_node._derived_variable_templates = \\\n                    copy.deepcopy(full_tree_node._derived_variable_templates)\n                compressed_tree_node._probability = full_tree_node._probability\n                compressed_tree_node._cost_variable = full_tree_node._cost_variable\n                compressed_tree_node._scenarios.append(compressed_tree_scenario)\n                compressed_tree_node._stage._tree_nodes.append(compressed_tree_node)\n                ###\n\n                compressed_tree_scenario._node_list.append(compressed_tree_node)\n                compressed_tree._tree_nodes.append(compressed_tree_node)\n                compressed_tree._tree_node_map[compressed_tree_node.name] = \\\n                    compressed_tree_node\n                previous_compressed_tree_node._parent = compressed_tree_node\n                compressed_tree_node._children.append(previous_compressed_tree_node)\n                previous_compressed_tree_node = compressed_tree_node\n\n                full_tree_node = full_tree_node._parent\n                if full_tree_node is None:\n                    compressed_tree_root = compressed_tree_node\n                    break\n\n            # traverse the remaining nodes up to the root and update the\n            # tree structure elements\n            if full_tree_node is not None:\n                compressed_tree_node = \\\n                    compressed_tree._tree_node_map[full_tree_node.name]\n                previous_compressed_tree_node._parent = compressed_tree_node\n                compressed_tree_node._scenarios.append(compressed_tree_scenario)\n                compressed_tree_node._children.append(previous_compressed_tree_node)\n                compressed_tree_scenario._node_list.append(compressed_tree_node)\n\n                compressed_tree_node = compressed_tree_node._parent\n                while compressed_tree_node is not None:\n                    compressed_tree_scenario._node_list.append(compressed_tree_node)\n                    compressed_tree_node._scenarios.append(compressed_tree_scenario)\n                    compressed_tree_node = compressed_tree_node._parent\n\n            # makes sure this list is in root to leaf order\n            compressed_tree_scenario._node_list.reverse()\n            assert compressed_tree_scenario._node_list[-1] is \\\n                compressed_tree_scenario._leaf_node\n            assert compressed_tree_scenario._node_list[0] is \\\n                compressed_tree_root\n\n            # initialize solution related dictionaries\n            for compressed_tree_node in compressed_tree_scenario._node_list:\n                compressed_tree_scenario._stage_costs[compressed_tree_node._stage.name] = None\n                compressed_tree_scenario._x[compressed_tree_node.name] = {}\n                compressed_tree_scenario._w[compressed_tree_node.name] = {}\n                compressed_tree_scenario._rho[compressed_tree_node.name] = {}\n                compressed_tree_scenario._fixed[compressed_tree_node.name] = set()\n                compressed_tree_scenario._stale[compressed_tree_node.name] = set()\n\n        compressed_tree._scenario_map = \\\n            dict((scenario.name, scenario) for scenario in compressed_tree._scenarios)\n\n        #\n        # Handle re-normalization of probabilities if requested\n        #\n        if normalize:\n\n            # update conditional probabilities (leaf-to-root stage order)\n            for compressed_tree_stage in reversed(compressed_tree._stages[:-1]):\n\n                for compressed_tree_node in compressed_tree_stage._tree_nodes:\n                    norm_factor = \\\n                        sum(compressed_tree_child_node._conditional_probability\n                            for compressed_tree_child_node\n                            in compressed_tree_node._children)\n                    # the user may specify that the probability of a\n                    # scenario is 0.0, and while odd, we should allow the\n                    # edge case.\n                    if norm_factor == 0.0:\n                        for compressed_tree_child_node in \\\n                               compressed_tree_node._children:\n                            compressed_tree_child_node._conditional_probability = 0.0\n\n                    else:\n                        for compressed_tree_child_node in \\\n                               compressed_tree_node._children:\n                            compressed_tree_child_node.\\\n                                _conditional_probability /= norm_factor\n\n            assert abs(compressed_tree_root._probability - 1.0) < 1e-5\n            assert abs(compressed_tree_root._conditional_probability - 1.0) < 1e-5\n\n            # update absolute probabilities (root-to-leaf stage order)\n            for compressed_tree_stage in compressed_tree._stages[1:]:\n                for compressed_tree_node in compressed_tree_stage._tree_nodes:\n                    compressed_tree_node._probability = \\\n                            compressed_tree_node._parent._probability * \\\n                            compressed_tree_node._conditional_probability\n\n            # update scenario probabilities\n            for compressed_tree_scenario in compressed_tree._scenarios:\n                compressed_tree_scenario._probability = \\\n                    compressed_tree_scenario._leaf_node._probability\n\n        return compressed_tree",
  "def add_bundle(self, name, scenario_bundle_list):\n\n        if name in self._scenario_bundle_map:\n            raise ValueError(\"Cannot add a new bundle with name '%s', a bundle \"\n                             \"with that name already exists.\" % (name))\n\n        bundle_scenario_tree = self.make_compressed(scenario_bundle_list,\n                                                    normalize=True)\n        bundle = ScenarioTreeBundle()\n        bundle._name = name\n        bundle._scenario_names = scenario_bundle_list\n        bundle._scenario_tree = bundle_scenario_tree\n        # make sure this is computed with the un-normalized bundle scenarios\n        bundle._probability = sum(self._scenario_map[scenario_name]._probability\n                                  for scenario_name in scenario_bundle_list)\n\n        self._scenario_bundle_map[name] = bundle\n        self._scenario_bundles.append(bundle)",
  "def remove_bundle(self, name):\n\n        if name not in self._scenario_bundle_map:\n            raise KeyError(\"Cannot remove bundle with name '%s', no bundle \"\n                           \"with that name exists.\" % (name))\n        bundle = self._scenario_bundle_map[name]\n        del self._scenario_bundle_map[name]\n        self._scenario_bundles.remove(bundle)",
  "def downsample(self, fraction_to_retain, random_seed, verbose=False):\n\n        random_state = random.getstate()\n        random.seed(random_seed)\n        try:\n            number_to_retain = \\\n                max(int(round(float(len(self._scenarios)*fraction_to_retain))), 1)\n            random_list=random.sample(range(len(self._scenarios)), number_to_retain)\n\n            scenario_bundle_list = []\n            for i in range(number_to_retain):\n                scenario_bundle_list.append(self._scenarios[random_list[i]].name)\n\n            if verbose:\n                print(\"Downsampling scenario tree - retained %s \"\n                      \"scenarios: %s\"\n                      % (len(scenario_bundle_list),\n                         str(scenario_bundle_list)))\n\n            self.compress(scenario_bundle_list) # do the downsampling\n        finally:\n            random.setstate(random_state)",
  "def findRootNode(self):\n\n        for tree_node in self._tree_nodes:\n            if tree_node._parent is None:\n                return tree_node\n        return None",
  "def computeIdentifierMaxLengths(self):\n\n        self._max_scenario_id_length = 0\n        for scenario in self._scenarios:\n            if len(str(scenario.name)) > self._max_scenario_id_length:\n                self._max_scenario_id_length = len(str(scenario.name))",
  "def validate(self):\n\n        # for any node, the sum of conditional probabilities of the children should sum to 1.\n        for tree_node in self._tree_nodes:\n            sum_probabilities = 0.0\n            if len(tree_node._children) > 0:\n                for child in tree_node._children:\n                    sum_probabilities += child._conditional_probability\n                if abs(1.0 - sum_probabilities) > 0.000001:\n                    raise ValueError(\"ScenarioTree validation failed. \"\n                                     \"Reason: child conditional \"\n                                     \"probabilities for tree node=%s \"\n                                     \" sum to %s\"\n                                     % (tree_node.name,\n                                        sum_probabilities))\n\n        # ensure that there is only one root node in the tree\n        num_roots = 0\n        root_ids = []\n        for tree_node in self._tree_nodes:\n            if tree_node._parent is None:\n                num_roots += 1\n                root_ids.append(tree_node.name)\n\n        if num_roots != 1:\n            raise ValueError(\"ScenarioTree validation failed. \"\n                             \"Reason: illegal set of root \"\n                             \"nodes detected: \" + str(root_ids))\n\n        # there must be at least one scenario passing through each tree node.\n        for tree_node in self._tree_nodes:\n            if len(tree_node._scenarios) == 0:\n                raise ValueError(\"ScenarioTree validation failed. \"\n                                 \"Reason: there are no scenarios \"\n                                 \"associated with tree node=%s\"\n                                 % (tree_node.name))\n                return False\n\n        return True",
  "def create_random_bundles(self,\n                              num_bundles,\n                              random_seed):\n\n        random_state = random.getstate()\n        random.seed(random_seed)\n        try:\n            num_scenarios = len(self._scenarios)\n\n            sequence = list(range(num_scenarios))\n            random.shuffle(sequence)\n\n            next_scenario_index = 0\n\n            # this is a hack-ish way to re-initialize the Bundles set of a\n            # scenario tree instance, which should already be there\n            # (because it is defined in the abstract model).  however, we\n            # don't have a \"clear\" method on a set, so...\n            bundle_names = [\"Bundle\"+str(i)\n                            for i in range(1, num_bundles+1)]\n            bundles = OrderedDict()\n            for i in range(num_bundles):\n                bundles[bundle_names[i]] = []\n\n            scenario_index = 0\n            while (scenario_index < num_scenarios):\n                for bundle_index in range(num_bundles):\n                    if (scenario_index == num_scenarios):\n                        break\n                    bundles[bundle_names[bundle_index]].append(\n                        self._scenarios[sequence[scenario_index]].name)\n                    scenario_index += 1\n\n            self._construct_scenario_bundles(bundles)\n        finally:\n            random.setstate(random_state)",
  "def pprint(self):\n\n        print(\"Scenario Tree Detail\")\n\n        print(\"----------------------------------------------------\")\n        print(\"Tree Nodes:\")\n        print(\"\")\n        for tree_node_name in sorted(self._tree_node_map.keys()):\n            tree_node = self._tree_node_map[tree_node_name]\n            print(\"\\tName=%s\" % (tree_node_name))\n            if tree_node._stage is not None:\n                print(\"\\tStage=%s\" % (tree_node._stage._name))\n            else:\n                print(\"\\t Stage=None\")\n            if tree_node._parent is not None:\n                print(\"\\tParent=%s\" % (tree_node._parent._name))\n            else:\n                print(\"\\tParent=\" + \"None\")\n            if tree_node._conditional_probability is not None:\n                print(\"\\tConditional probability=%4.4f\" % tree_node._conditional_probability)\n            else:\n                print(\"\\tConditional probability=\" + \"***Undefined***\")\n            print(\"\\tChildren:\")\n            if len(tree_node._children) > 0:\n                for child_node in sorted(tree_node._children, key=lambda x: x._name):\n                    print(\"\\t\\t%s\" % (child_node._name))\n            else:\n                print(\"\\t\\tNone\")\n            print(\"\\tScenarios:\")\n            if len(tree_node._scenarios) == 0:\n                print(\"\\t\\tNone\")\n            else:\n                for scenario in sorted(tree_node._scenarios, key=lambda x: x._name):\n                    print(\"\\t\\t%s\" % (scenario._name))\n            if len(tree_node._variable_templates) > 0:\n                print(\"\\tVariables: \")\n                for variable_name in sorted(tree_node._variable_templates.keys()):\n                    match_templates = tree_node._variable_templates[variable_name]\n                    sys.stdout.write(\"\\t\\t \"+variable_name+\" : \")\n                    for match_template in match_templates:\n                       sys.stdout.write(indexToString(match_template)+' ')\n                    print(\"\")\n            if len(tree_node._derived_variable_templates) > 0:\n                print(\"\\tDerived Variables: \")\n                for variable_name in sorted(tree_node._derived_variable_templates.keys()):\n                    match_templates = tree_node._derived_variable_templates[variable_name]\n                    sys.stdout.write(\"\\t\\t \"+variable_name+\" : \")\n                    for match_template in match_templates:\n                       sys.stdout.write(indexToString(match_template)+' ')\n                    print(\"\")\n            print(\"\")\n        print(\"----------------------------------------------------\")\n        print(\"Stages:\")\n        for stage_name in sorted(self._stage_map.keys()):\n            stage = self._stage_map[stage_name]\n            print(\"\\tName=%s\" % (stage_name))\n            print(\"\\tTree Nodes: \")\n            for tree_node in sorted(stage._tree_nodes, key=lambda x: x._name):\n                print(\"\\t\\t%s\" % (tree_node._name))\n            if len(stage._variable_templates) > 0:\n                print(\"\\tVariables: \")\n                for variable_name in sorted(stage._variable_templates.keys()):\n                    match_templates = stage._variable_templates[variable_name]\n                    sys.stdout.write(\"\\t\\t \"+variable_name+\" : \")\n                    for match_template in match_templates:\n                       sys.stdout.write(indexToString(match_template)+' ')\n                    print(\"\")\n            if len(stage._derived_variable_templates) > 0:\n                print(\"\\tDerived Variables: \")\n                for variable_name in sorted(stage._derived_variable_templates.keys()):\n                    match_templates = stage._derived_variable_templates[variable_name]\n                    sys.stdout.write(\"\\t\\t \"+variable_name+\" : \")\n                    for match_template in match_templates:\n                       sys.stdout.write(indexToString(match_template)+' ')\n                    print(\"\")\n            print(\"\\tCost Variable: \")\n            if stage._cost_variable is not None:\n                cost_variable_name, cost_variable_index = stage._cost_variable\n            else:\n                # kind of a hackish way to get around the fact that we are transitioning\n                # away from storing the cost_variable identifier on the stages\n                cost_variable_name, cost_variable_index = stage.nodes[0]._cost_variable\n            if cost_variable_index is None:\n                print(\"\\t\\t\" + cost_variable_name)\n            else:\n                print(\"\\t\\t\" + cost_variable_name + indexToString(cost_variable_index))\n            print(\"\")\n        print(\"----------------------------------------------------\")\n        print(\"Scenarios:\")\n        for scenario_name in sorted(self._scenario_map.keys()):\n            scenario = self._scenario_map[scenario_name]\n            print(\"\\tName=%s\" % (scenario_name))\n            print(\"\\tProbability=%4.4f\" % scenario._probability)\n            if scenario._leaf_node is None:\n                print(\"\\tLeaf node=None\")\n            else:\n                print(\"\\tLeaf node=%s\" % (scenario._leaf_node._name))\n            print(\"\\tTree node sequence:\")\n            for tree_node in scenario._node_list:\n                print(\"\\t\\t%s\" % (tree_node._name))\n            print(\"\")\n        print(\"----------------------------------------------------\")\n        if len(self._scenario_bundles) > 0:\n            print(\"Scenario Bundles:\")\n            for bundle_name in sorted(self._scenario_bundle_map.keys()):\n                scenario_bundle = self._scenario_bundle_map[bundle_name]\n                print(\"\\tName=%s\" % (bundle_name))\n                print(\"\\tProbability=%4.4f\" % scenario_bundle._probability            )\n                sys.stdout.write(\"\\tScenarios:  \")\n                for scenario_name in sorted(scenario_bundle._scenario_names):\n                    sys.stdout.write(str(scenario_name)+' ')\n                sys.stdout.write(\"\\n\")\n                print(\"\")\n            print(\"----------------------------------------------------\")",
  "def save_to_dot(self, filename):\n\n        def _visit_node(node):\n            f.write(\"%s%s [label=\\\"%s\\\"];\\n\"\n                    % (node.name,\n                       id(node),\n                       str(node.name)+(\"\\n(%.6g)\" % (node._probability))))\n            for child_node in node._children:\n                _visit_node(child_node)\n                f.write(\"%s%s -> %s%s [label=\\\"%.6g\\\"];\\n\"\n                        % (node.name,\n                           id(node),\n                           child_node.name,\n                           id(child_node),\n                           child_node._conditional_probability))\n            if len(node._children) == 0:\n                assert len(node._scenarios) == 1\n                scenario = node._scenarios[0]\n                f.write(\"%s%s [label=\\\"%s\\\"];\\n\"\n                        % (scenario.name,\n                           id(scenario),\n                           \"scenario\\n\"+str(scenario.name)))\n                f.write(\"%s%s -> %s%s [style=dashed];\\n\"\n                        % (node.name,\n                           id(node),\n                           scenario.name,\n                           id(scenario)))\n\n        with open(filename, 'w') as f:\n\n            f.write(\"digraph ScenarioTree {\\n\")\n            root_node = self.findRootNode()\n            _visit_node(root_node)\n            f.write(\"}\\n\")",
  "def _visit_node(node):\n            f.write(\"%s%s [label=\\\"%s\\\"];\\n\"\n                    % (node.name,\n                       id(node),\n                       str(node.name)+(\"\\n(%.6g)\" % (node._probability))))\n            for child_node in node._children:\n                _visit_node(child_node)\n                f.write(\"%s%s -> %s%s [label=\\\"%.6g\\\"];\\n\"\n                        % (node.name,\n                           id(node),\n                           child_node.name,\n                           id(child_node),\n                           child_node._conditional_probability))\n            if len(node._children) == 0:\n                assert len(node._scenarios) == 1\n                scenario = node._scenarios[0]\n                f.write(\"%s%s [label=\\\"%s\\\"];\\n\"\n                        % (scenario.name,\n                           id(scenario),\n                           \"scenario\\n\"+str(scenario.name)))\n                f.write(\"%s%s -> %s%s [style=dashed];\\n\"\n                        % (node.name,\n                           id(node),\n                           scenario.name,\n                           id(scenario)))",
  "def CreateAbstractScenarioTreeModel():\n    from pyomo.core import (\n        AbstractModel, Set, Param, Boolean, Any, UnitInterval,\n    )\n\n    model = AbstractModel()\n\n    # all set/parameter values are strings, representing the\n    # names of various entities/variables.\n\n    model.Stages = Set(ordered=True)\n    model.Nodes = Set(ordered=True)\n\n    model.NodeStage = Param(model.Nodes,\n                            within=model.Stages,\n                            mutable=True)\n    model.Children = Set(model.Nodes,\n                         within=model.Nodes,\n                         initialize=[],\n                         ordered=True)\n    model.ConditionalProbability = Param(model.Nodes,\n                                         within=UnitInterval,\n                                         mutable=True)\n\n    model.Scenarios = Set(ordered=True)\n    model.ScenarioLeafNode = Param(model.Scenarios,\n                                   within=model.Nodes,\n                                   mutable=True)\n\n    model.StageVariables = Set(model.Stages,\n                               initialize=[],\n                               ordered=True)\n\n    model.NodeVariables = Set(model.Nodes,\n                              initialize=[],\n                              ordered=True)\n\n    model.StageCost = Param(model.Stages,\n                            within=Any,\n                            mutable=True,\n                            default=None)\n    model.NodeCost = Param(model.Nodes,\n                           within=Any,\n                           mutable=True,\n                           default=None)\n\n    # DEPRECATED\n    model.StageCostVariable = Param(model.Stages,\n                                    within=Any,\n                                    mutable=True)\n\n    # it is often the case that a subset of the stage variables are strictly \"derived\"\n    # variables, in that their values are computable once the values of other variables\n    # in that stage are known. it generally useful to know which variables these are,\n    # as it is unnecessary to post non-anticipativity constraints for these variables.\n    # further, attempting to force non-anticipativity - either implicitly or explicitly -\n    # on these variables can cause issues with decomposition algorithms.\n    model.StageDerivedVariables = Set(model.Stages,\n                                      initialize=[],\n                                      ordered=True)\n    model.NodeDerivedVariables = Set(model.Nodes,\n                                     initialize=[],\n                                     ordered=True)\n\n    # scenario data can be populated in one of two ways. the first is \"scenario-based\",\n    # in which a single .dat file contains all of the data for each scenario. the .dat\n    # file prefix must correspond to the scenario name. the second is \"node-based\",\n    # in which a single .dat file contains only the data for each node in the scenario\n    # tree. the node-based method is more compact, but the scenario-based method is\n    # often more natural when parameter data is generated via simulation. the default\n    # is scenario-based.\n    model.ScenarioBasedData = Param(within=Boolean,\n                                    default=True,\n                                    mutable=True)\n\n    # do we bundle, and if so, how?\n    model.Bundling = Param(within=Boolean,\n                           default=False,\n                           mutable=True)\n\n    # bundle names\n    model.Bundles = Set(ordered=True)\n    model.BundleScenarios = Set(model.Bundles,\n                                ordered=True)\n\n    return model",
  "def CreateConcreteTwoStageScenarioTreeModel(num_scenarios):\n    m = CreateAbstractScenarioTreeModel()\n    m = m.create_instance()\n    m.Stages.add('Stage1')\n    m.Stages.add('Stage2')\n    m.Nodes.add('RootNode')\n    for i in range(1, num_scenarios+1):\n        m.Nodes.add('LeafNode_Scenario'+str(i))\n        m.Scenarios.add('Scenario'+str(i))\n    m.NodeStage['RootNode'] = 'Stage1'\n    m.ConditionalProbability['RootNode'] = 1.0\n    for node in m.Nodes:\n        if node != 'RootNode':\n            m.NodeStage[node] = 'Stage2'\n            m.Children['RootNode'].add(node)\n            m.Children[node].clear()\n            m.ConditionalProbability[node] = 1.0/num_scenarios\n            m.ScenarioLeafNode[node.replace('LeafNode_','')] = node\n\n    return m",
  "def ScenarioTreeModelFromNetworkX(\n        tree,\n        node_name_attribute=None,\n        edge_probability_attribute='weight',\n        stage_names=None,\n        scenario_name_attribute=None):\n    \"\"\"\n    Create a scenario tree model from a networkx tree.  The\n    height of the tree must be at least 1 (meaning at least\n    2 stages).\n\n    Required node attributes:\n        - cost (str): A string identifying a component on\n              the model whose value indicates the cost at\n              the time stage of the node for any scenario\n              traveling through it.\n\n    Optional node attributes:\n        - variables (list): A list of variable identifiers\n              that will be tracked by the node. If the node\n              is not a leaf node, these indicate variables\n              with non-anticipativity constraints.\n        - derived_variables (list): A list of variable or\n              expression identifiers that will be tracked by\n              the node (but will never have\n              non-anticipativity constraints enforced).\n        - bundle: A bundle identifier for the scenario\n              defined by a leaf-stage node. This attribute\n              is ignored on non-terminal tree nodes. This\n              attribute appears on at least one leaf-stage\n              node (and is not set to :const:`None`), then\n              it must be set on all leaf-stage nodes (to\n              something other than :const:`None`);\n              otherwise, an exception will be raised.\n\n    Optional edge attributes:\n        - weight (float): Indicates the conditional\n              probability of moving from the parent node to\n              the child node in the directed edge. If not\n              present, it will be assumed that all edges\n              leaving the parent node have equal probability\n              (normalized to sum to one).\n\n    Args:\n        stage_names: Can define a list of stage names to use\n           (assumed in time order). The length of this list\n           much match the number of stages in the tree. The\n           default value of :const:`None` indicates that\n           stage names should be automatically generated in\n           with the form ['Stage1','Stage2',...].\n        node_name_attribute: By default, node names are the\n           same as the node hash in the networkx tree. This\n           keyword can be set to the name of some property\n           of nodes in the graph that will be used for their\n           name in the PySP scenario tree.\n        scenario_name_attribute: By default, scenario names\n           are the same as the leaf-node hash in the\n           networkx tree. This keyword can be set to the\n           name of some property of leaf-nodes in the graph\n           that will be used for their corresponding\n           scenario name in the PySP scenario tree.\n        edge_probability_attribute: Can be set to the name\n           of some property of edges in the graph that\n           defines the conditional probability of that\n           branch (default: 'weight'). If this keyword is\n           set to :const:`None`, then all branches leaving a\n           node are assigned equal conditional\n           probabilities.\n\n    Examples:\n\n        A 2-stage scenario tree with 10 scenarios grouped\n        into 2 bundles:\n\n        >>> G = networkx.DiGraph()\n        >>> G.add_node(\"root\", variables=[\"x\"])\n        >>> N = 10\n        >>> for i in range(N):\n        >>>     node_name = \"s\"+str(i)\n        >>>     bundle_name = \"b\"+str(i%2)\n        >>>     G.add_node(node_name, bundle=bundle)\n        >>>     G.add_edge(\"root\", node_name, weight=1.0/N)\n        >>> model = ScenarioTreeModelFromNetworkX(G)\n\n        A 4-stage scenario tree with 125 scenarios:\n\n        >>> branching_factor = 5\n        >>> height = 3\n        >>> G = networkx.balanced_tree(\n                   branching_factor,\n                   height,\n                   networkx.DiGraph())\n        >>> model = ScenarioTreeModelFromNetworkX(G)\n    \"\"\"\n\n    if not networkx.is_tree(tree):\n        raise TypeError(\n            \"Graph object is not a tree \"\n            \"(see networkx.is_tree)\")\n\n    if not networkx.is_directed(tree):\n        raise TypeError(\n            \"Graph object is not directed \"\n            \"(see networkx.is_directed)\")\n\n    if not networkx.is_branching(tree):\n        raise TypeError(\n            \"Grapn object is not a branching \"\n            \"(see networkx.is_branching\")\n\n    in_degree_items = tree.in_degree()\n    # Prior to networkx ~2.0, in_degree() returned a dictionary.\n    # Now it is a view on items, so only call .items() for the old case\n    if hasattr(in_degree_items, 'items'):\n        in_degree_items = in_degree_items.items()\n    root = [u for u,d in in_degree_items if d == 0]\n    assert len(root) == 1\n    root = root[0]\n    num_stages = networkx.eccentricity(tree, v=root) + 1\n    if num_stages < 2:\n        raise ValueError(\n            \"The number of stages must be at least 2\")\n    m = CreateAbstractScenarioTreeModel()\n    m = m.create_instance()\n    if stage_names is not None:\n        unique_stage_names = set()\n        for cnt, stage_name in enumerate(stage_names,1):\n            m.Stages.add(stage_name)\n            unique_stage_names.add(stage_name)\n        if cnt != num_stages:\n            raise ValueError(\n                \"incorrect number of stages names (%s), should be %s\"\n                % (cnt, num_stages))\n        if len(unique_stage_names) != cnt:\n            raise ValueError(\"all stage names were not unique\")\n    else:\n        for i in range(num_stages):\n            m.Stages.add('Stage'+str(i+1))\n    node_to_name = {}\n    node_to_scenario = {}\n    scenario_bundle = {}\n    def _setup(u, succ):\n        if node_name_attribute is not None:\n            if node_name_attribute not in tree.nodes[u]:\n                raise KeyError(\n                    \"node '%s' missing node name \"\n                    \"attribute: '%s'\"\n                    % (u, node_name_attribute))\n            node_name = tree.nodes[u][node_name_attribute]\n        else:\n            node_name = u\n        node_to_name[u] = node_name\n        m.Nodes.add(node_name)\n        if u in succ:\n            for v in succ[u]:\n                _setup(v, succ)\n        else:\n            # a leaf node\n            if scenario_name_attribute is not None:\n                if scenario_name_attribute not in tree.nodes[u]:\n                    raise KeyError(\n                        \"node '%s' missing scenario name \"\n                        \"attribute: '%s'\"\n                        % (u, scenario_name_attribute))\n                scenario_name = tree.nodes[u][scenario_name_attribute]\n            else:\n                scenario_name = u\n            node_to_scenario[u] = scenario_name\n            m.Scenarios.add(scenario_name)\n            scenario_bundle[scenario_name] = \\\n                tree.nodes[u].get('bundle', None)\n    _setup(root,\n           networkx.dfs_successors(tree, root))\n\n    def _add_node(u, stage, succ, pred):\n        node_name = node_to_name[u]\n        m.NodeStage[node_name] = m.Stages[stage]\n        if u == root:\n            m.ConditionalProbability[node_name] = 1.0\n        else:\n            assert u in pred\n            # prior to networkx ~2.0, we used a .edge attribute on DiGraph,\n            # which no longer exists.\n            if hasattr(tree, 'edge'):\n                edge = tree.edge[pred[u]][u]\n            else:\n                edge = tree.edges[pred[u],u]\n            probability = None\n            if edge_probability_attribute is not None:\n                if edge_probability_attribute not in edge:\n                    raise KeyError(\n                        \"edge '(%s, %s)' missing probability attribute: '%s'\"\n                        % (pred[u], u, edge_probability_attribute))\n                probability = edge[edge_probability_attribute]\n            else:\n                probability = 1.0/len(succ[pred[u]])\n            m.ConditionalProbability[node_name] = probability\n        # get node variables\n        if \"variables\" in tree.nodes[u]:\n            node_variables = tree.nodes[u][\"variables\"]\n            assert type(node_variables) in [tuple, list]\n            for varstring in node_variables:\n                m.NodeVariables[node_name].add(varstring)\n        if \"derived_variables\" in tree.nodes[u]:\n            node_derived_variables = tree.nodes[u][\"derived_variables\"]\n            assert type(node_derived_variables) in [tuple, list]\n            for varstring in node_derived_variables:\n                m.NodeDerivedVariables[node_name].add(varstring)\n        if \"cost\" in tree.nodes[u]:\n            assert isinstance(tree.nodes[u][\"cost\"], str)\n            m.NodeCost[node_name].value = tree.nodes[u][\"cost\"]\n        if u in succ:\n            child_names = []\n            for v in succ[u]:\n                child_names.append(\n                    _add_node(v, stage+1, succ, pred))\n            total_probability = 0.0\n            for child_name in child_names:\n                m.Children[node_name].add(child_name)\n                total_probability += \\\n                    pyomo.core.value(m.ConditionalProbability[child_name])\n            if abs(total_probability - 1.0) > 1e-5:\n                raise ValueError(\n                    \"edge probabilities leaving node '%s' \"\n                    \"do not sum to 1 (total=%r)\"\n                    % (u, total_probability))\n        else:\n            # a leaf node\n            scenario_name = node_to_scenario[u]\n            m.ScenarioLeafNode[scenario_name] = node_name\n            m.Children[node_name].clear()\n\n        return node_name\n\n    _add_node(root,\n              1,\n              networkx.dfs_successors(tree, root),\n              networkx.dfs_predecessors(tree, root))\n\n    if any(_b is not None for _b in scenario_bundle.values()):\n        if any(_b is None for _b in scenario_bundle.values()):\n            raise ValueError(\"Incomplete bundle specification. \"\n                             \"All scenarios require a bundle \"\n                             \"identifier.\")\n        m.Bundling.value = True\n        bundle_scenarios = {}\n        for bundle_name in sorted(set(scenario_bundle.values())):\n            m.Bundles.add(bundle_name)\n            bundle_scenarios[bundle_name] = []\n        for scenario_name in m.Scenarios:\n            bundle_scenarios[scenario_bundle[scenario_name]].\\\n                append(scenario_name)\n        for bundle_name in m.Bundles:\n            for scenario_name in sorted(bundle_scenarios[bundle_name]):\n                m.BundleScenarios[bundle_name].add(scenario_name)\n\n    return m",
  "def _setup(u, succ):\n        if node_name_attribute is not None:\n            if node_name_attribute not in tree.nodes[u]:\n                raise KeyError(\n                    \"node '%s' missing node name \"\n                    \"attribute: '%s'\"\n                    % (u, node_name_attribute))\n            node_name = tree.nodes[u][node_name_attribute]\n        else:\n            node_name = u\n        node_to_name[u] = node_name\n        m.Nodes.add(node_name)\n        if u in succ:\n            for v in succ[u]:\n                _setup(v, succ)\n        else:\n            # a leaf node\n            if scenario_name_attribute is not None:\n                if scenario_name_attribute not in tree.nodes[u]:\n                    raise KeyError(\n                        \"node '%s' missing scenario name \"\n                        \"attribute: '%s'\"\n                        % (u, scenario_name_attribute))\n                scenario_name = tree.nodes[u][scenario_name_attribute]\n            else:\n                scenario_name = u\n            node_to_scenario[u] = scenario_name\n            m.Scenarios.add(scenario_name)\n            scenario_bundle[scenario_name] = \\\n                tree.nodes[u].get('bundle', None)",
  "def _add_node(u, stage, succ, pred):\n        node_name = node_to_name[u]\n        m.NodeStage[node_name] = m.Stages[stage]\n        if u == root:\n            m.ConditionalProbability[node_name] = 1.0\n        else:\n            assert u in pred\n            # prior to networkx ~2.0, we used a .edge attribute on DiGraph,\n            # which no longer exists.\n            if hasattr(tree, 'edge'):\n                edge = tree.edge[pred[u]][u]\n            else:\n                edge = tree.edges[pred[u],u]\n            probability = None\n            if edge_probability_attribute is not None:\n                if edge_probability_attribute not in edge:\n                    raise KeyError(\n                        \"edge '(%s, %s)' missing probability attribute: '%s'\"\n                        % (pred[u], u, edge_probability_attribute))\n                probability = edge[edge_probability_attribute]\n            else:\n                probability = 1.0/len(succ[pred[u]])\n            m.ConditionalProbability[node_name] = probability\n        # get node variables\n        if \"variables\" in tree.nodes[u]:\n            node_variables = tree.nodes[u][\"variables\"]\n            assert type(node_variables) in [tuple, list]\n            for varstring in node_variables:\n                m.NodeVariables[node_name].add(varstring)\n        if \"derived_variables\" in tree.nodes[u]:\n            node_derived_variables = tree.nodes[u][\"derived_variables\"]\n            assert type(node_derived_variables) in [tuple, list]\n            for varstring in node_derived_variables:\n                m.NodeDerivedVariables[node_name].add(varstring)\n        if \"cost\" in tree.nodes[u]:\n            assert isinstance(tree.nodes[u][\"cost\"], str)\n            m.NodeCost[node_name].value = tree.nodes[u][\"cost\"]\n        if u in succ:\n            child_names = []\n            for v in succ[u]:\n                child_names.append(\n                    _add_node(v, stage+1, succ, pred))\n            total_probability = 0.0\n            for child_name in child_names:\n                m.Children[node_name].add(child_name)\n                total_probability += \\\n                    pyomo.core.value(m.ConditionalProbability[child_name])\n            if abs(total_probability - 1.0) > 1e-5:\n                raise ValueError(\n                    \"edge probabilities leaving node '%s' \"\n                    \"do not sum to 1 (total=%r)\"\n                    % (u, total_probability))\n        else:\n            # a leaf node\n            scenario_name = node_to_scenario[u]\n            m.ScenarioLeafNode[scenario_name] = node_name\n            m.Children[node_name].clear()\n\n        return node_name",
  "def worker_bee(synchronizer, buzz=1e6, sting=100.0):\n    \"\"\"\n    Do some busy work. Note that the synchronizer does not need to be an argument\n    to this function (it could be accessed some other way).\n\n    Args:\n        sychronizer (object): used to do asynchronous sums\n        buzz (int): Number of deviates to generate\n        sting (float): standard deviation\n\n    Returns:\n        nothing (these worker functions usually want to update an object\n                 but in this example we print).\n\n    Note: To make this fun, we are randomly distribute the load.\n          For even more fun, we occasionally try to \n          reduce the listner sleep time but the listener might not see it since\n          it might sleep through the whole thing. This is subtle.\n    \n    We are going to track the time that each rank last reported just to show\n    one way to do it. To spell out the way we will do it: \n    allocate n_proc of the vector to seconds_since_start and each rank \n    will put its seconds_since_start into its spot; hence, the sum will be \n    the report (since the others will have contributed zero).\n    The number of times a rank has contributed to the sum could be tracked\n    in an analogous way.\n    \"\"\"\n\n    local_sum_sofar = 0\n    local_iters_sofar = 0\n    global_iters_sofar = 0\n    old_sleep = np.zeros(1, dtype='d')\n    old_sleep[0] = synchronizer.sleep_secs[0]\n\n    # We will send/receive sum (1), iters (1) and secs (n_proc)\n    # and we are going to concatenat all \"vectors\" into one.\n    local_concat = {\"FirstReduce\": {\"ROOT\": np.zeros(2 + n_proc, dtype='d')}}\n    global_concat = {\"FirstReduce\": {\"ROOT\": \\\n                      np.zeros(len(local_concat[\"FirstReduce\"][\"ROOT\"]), \\\n                               dtype='d')}}\n\n    # In this trivial example, we are going enable the side gig and\n    # nothing will disable it. Normally, the listener side gig would\n    # be expected to disable it.\n    # Gratuitous call to enable the side_gig\n    synchronizer.compute_global_data(local_concat,\n                                     global_concat,\n                                     rednames=[\"FirstReduce\"],\n                                     enable_side_gig = True)\n\n    while global_iters_sofar < buzz:\n        # attempt at less sleep, maybe (but don't do it twice in a row)\n        if np.random.uniform() > 0.1 and old_sleep[0] \\\n           == synchronizer.sleep_secs[0]:\n            old_sleep[0] = synchronizer.sleep_secs[0]\n            synchronizer.sleep_secs[0] /= 10\n            logging.debug (\"TRYING to reduce sleep to {} from rank={}\".\\\n                   format(synchronizer.sleep_secs[0], rank))\n        elif old_sleep[0] != synchronizer.sleep_secs[0]:\n            synchronizer.sleep_secs[0] = old_sleep[0]\n            logging.debug (\"putting sleep back to {} from rank={}\".\\\n                   format(synchronizer.sleep_secs[0], rank))\n\n        localiterstodo = int(np.random.uniform() * buzz / n_proc)\n        if rank == 0:\n            logging.debug(\"**rank 0: iterstodo=\"+str(localiterstodo))\n        for i in range(localiterstodo):\n            local_sum_sofar += np.random.normal(0, sting)\n            \n        local_iters_sofar += localiterstodo\n        if rank == 0:\n            logging.debug(\"rank 0: iterstodo {} iters sofar {} sum_so_far {}=\"\\\n                          .format(localiterstodo, local_iters_sofar, local_sum_sofar))\n        local_concat[\"FirstReduce\"][\"ROOT\"][0] = local_iters_sofar\n        local_concat[\"FirstReduce\"][\"ROOT\"][1] = local_sum_sofar\n        local_concat[\"FirstReduce\"][\"ROOT\"][2+rank] \\\n            = (dt.datetime.now() - startdt).total_seconds()\n\n        # Only do \"FirstReduce\".\n        synchronizer.compute_global_data(local_concat,\n                                         global_concat,\n                                         rednames=[\"FirstReduce\"])\n\n        global_iters_sofar = global_concat[\"FirstReduce\"][\"ROOT\"][0]\n        global_sum = global_concat[\"FirstReduce\"][\"ROOT\"][1]\n        if rank == 0:\n            logging.debug(\"   rank 0: global_iters {} global_sum_so_far {}\"\\\n                          .format(global_iters_sofar, global_sum))\n\n    # tell the listener threads to shut down\n    synchronizer.quitting = 1\n\n    if rank == 0:\n        print (\"Rank 0 termination\")\n        print (\"Based on {} iterations, the average was {}\".\\\n               format(global_iters_sofar, global_sum / global_iters_sofar))\n        print (\"In case you are curious:\\n rank \\t last report *in* (sec)\")\n        for r in range(n_proc):\n            print (r, \"\\t\", global_concat[\"FirstReduce\"][\"ROOT\"][2+r])",
  "def side_gig(synchro, msg = None):\n    \"\"\" Demonstrate a listener side gig. This will be called by the listener.\n        This is a small, silly function. Usually, the side-gig will check\n        to see if it should do something or just pass.\n        dlw babble: we can *look* at synchronizer.global_data for\n        our redname because we know the reduction was just done.\n        We can then jump in and modify local_data for the next reduction.\n        This is either \"hackish\" or \"c-like\" depending on your point of view.\n    Args:\n        snchro (object): the Synchronizer object where the listener lives\n        msg (str): just to show keyword arg; normally, this would be \n                   something (e.g., an object) that is modified\n    \"\"\"\n    if synchro._rank == 0:\n        print (\"   (rank 0) ^*^* side_gig msg=\", str(msg))\n    logging.debug(\"enter side gig on rank %d\" % rank)\n\n    # Just to demonstrate how to do it, we will just return if nothing\n    # changed in the first reduce.\n    # NOTE: often the side will want to check and update enable_side_gig\n    #       on the synchro object. See aph.\n    # So this code needs to know the name of previous reduce.\n    # BTW: in this example, we are treating the function as an object,\n    #   but in most applications, this function will be in an object.\n    prevredname = \"FirstReduce\"\n    allthesame = True\n    if len(side_gig.prev_red_prev_concat) == 0: # first time\n        for cname, clen in synchro.Lens[prevredname].items():\n            side_gig.prev_red_prev_concat[cname] = np.zeros(clen, dtype='d')\n        allthesame = False # in case they are actually all zero\n\n    for cname in synchro.Lens[prevredname]:\n        if not np.array_equal(side_gig.prev_red_prev_concat[cname],\n                              synchro.global_data[prevredname][cname]):\n            allthesame = False\n            break\n    if allthesame:\n        logging.debug(\"Skipping intermediate side_gig on rank %d\" % rank)\n        return\n\n    logging.debug(\"Doing intermediate side_gig on rank %d\" % rank)\n    # It is OK for us to directly at global_data on the synchro because\n    # the side_gig is \"part of\" the listener, the listener has the lock,\n    # and only the listener updates global_data on the syncrho.\n    # Side gigs are a bit of a hack.\n\n    # this particular side_gig is very silly\n    lastsideresult = synchro.global_data[\"SecondReduce\"][\"ROOT\"]\n    logging.debug(\"In case you are curious, before this listener call to\"\\\n                  +\"side_gig, the value of the secondreduce was {} on rank {}\"\\\n                  .format(lastsideresult, rank))\n    \n    # dst, src\n    np.copyto(side_gig.prev_red_prev_concat[cname],\n              synchro.global_data[prevredname][cname])\n    \n    # For the second reduce, we are going to sum the ranks, which is silly.\n    \n    # Normally, the concats would be created once in an __init__, btw.\n    local_concat = {\"SecondReduce\": {\"ROOT\": np.zeros(1, dtype='d')}}\n    global_concat = {\"SecondReduce\": {\"ROOT\": np.zeros(1, dtype='d')}}\n    local_concat[\"SecondReduce\"][\"ROOT\"][0] = rank\n    # We can (and should) do a dangerous put in the side_gig because\n    # the listener will have the lock. If the worker gets to its compute_global\n    # then it will have to wait for the lock.\n    synchro._unsafe_put_local_data(\"SecondReduce\", local_concat)",
  "class Synchronizer(object):\n    \"\"\"\n    Manage both Async and Sync communcation.\n    Args:\n        comms (dict of mpi comms): keys are strings: mpi comms\n               There must be a key \"ROOT\" that is the global comm\n        Lens (sorted dict of dict of ints): \n                             the keys are reduction step names (defines them), \n                             then comms' keys. Contains length of vectors to sum\n        work_fct (function): the main worker function (args can be supplied\n                             when run is called.)\n        rank (int): the mpi rank\n        sleep_secs (float): Initial sleep seconds for the async listener.\n        async (boolean): True for async, False for asynchronous; default False\n        listener_gigs (dict of (fct, kwargs)): Optional. keys are step names,\n                                                  functions after steps\n                The functions are always passed this synchronizer object\n                as the first arg, then whatever is in this dictionary's\n                keyword args.\n    Attributes:\n        global_data (dict of np doubles): indices match comms (advanced)\n        sleep_secs (float): sleep seconds for the async listener\n                            The listener takes the min from the reduce.\n                            (not working as of March 2019)\n\n        quitting (int): assign zero to cause the listeners to stop (async)\n\n    Note:\n        As of Python 3.7 async is a reserved word. Using asynch instead.\n    \"\"\"\n\n    def __init__(self, comms, Lens, work_fct, rank,\n                 sleep_secs, asynch=False, listener_gigs = None):\n        self.asynch = asynch\n        self.comms = comms\n        self.Lens = Lens\n        self._rank = rank\n        self.sleep_secs = np.zeros(1, dtype='d')\n        self.sleep_secs[0] = sleep_secs\n        self.sleep_touse = np.zeros(1, dtype='d') # min over ranks\n        # The side gigs are very dangerous.\n        self.listener_gigs = listener_gigs\n        self.enable_side_gig = False # TBD should be gigs & indexed by reduction\n        \n        self.global_quitting = 0\n        self.quitting = 0\n        self.work_fct = work_fct\n        self.local_data = {}\n        self.global_data = {}\n        if not isinstance(self.Lens, collections.OrderedDict):\n            raise RuntimeError(\"listener_util: Lens must be an OrderedDict\")\n        for redname in self.Lens.keys():\n            self.local_data[redname] = {}\n            self.global_data[redname] = {}\n            for commname, ell in self.Lens[redname].items():\n                assert(commname in self.comms)\n                self.local_data[redname][commname] = np.zeros(ell, dtype='d')\n                self.global_data[redname][commname] = np.zeros(ell, dtype='d')\n        self.data_lock = threading.Lock()\n\n    def run(self, args, kwargs):\n        if self.asynch:\n            # THE WORKER\n            wthread = threading.Thread(name=self.work_fct.__name__,\n                                       target=self.work_fct,\n                                       args = args,\n                                       kwargs = kwargs)\n            ## ph = threading.Thread(name='ph_main', target=ph_main)\n            #ph.setDaemon(True)\n\n            listenargs = [self._rank, self]\n            l = threading.Thread(name='listener',\n                                 target=Synchronizer.listener_daemon,\n                                 args=listenargs)\n            #l.setDaemon(True)\n\n            l.start()\n            wthread.start()\n\n            l.join()\n        else:\n            self.work_fct(*args, **kwargs)\n\n    ###=====================###\n    def _check_Lens(self, local_data_in, global_data_out, redname, cname):\n        \"\"\" Essentially local to compute_global_data\n        \"\"\"\n        if len(local_data_in[redname][cname]) \\\n           != self.Lens[redname][cname]:\n            self.global_quitting = 1\n            print (\"\\nERROR listener_util:cname={} len in={} Lens={}\".\\\n                   format(cname, len(local_data_in[redname][cname]),\n                          self.Lens[redname][cname]))\n        if len(global_data_out[redname][cname]) \\\n           != self.Lens[redname][cname]:\n            self.global_quitting = 1\n            print (\"\\nERROR listener_util:cname={} len out={} Lens={}\".\\\n                   format(cname, len(global_data_out[redname][cname]),\n                          self.Lens[redname][cname]))\n\n            \n    #########################################################################\n    def compute_global_data(self, local_data_in,\n                            global_data_out,\n                            enable_side_gig = False,\n                            rednames=None,\n                            keep_up=False):\n        \"\"\"\n        Data processing. Cache local_data_in so it will be\n        reduced later by the listener. Copy out the data that\n        was the result of reductions the last time the listener reduced.\n\n        NOTE:\n            Do not call this from a listener side gig (lock issues).\n        Args:\n            local_data_in (dict): data computed locally\n            global_data_out (dict): global version (often a sum over locals)\n            enable_side_gig (boolean): sets a flag that allows the \n                   listener to run side gigs. It is intended to be a run once\n                   authorization and the side gig code itself disables it.\n            rednames (list of str): optional list of reductions to report\n            keep_up (boolean):indicates the data out should include this data in;\n                   otherwise, you will use a global that is \"one notch behind.\"\n        NOTE:\n            as of April 2019. keep_up should be used  very sparingly.\n\n        Note np.copy is dst, src\n        \"\"\"\n        if self.asynch:\n            logging.debug('Starting comp_glob update on Rank %d' % self._rank)\n            self.data_lock.acquire() \n            logging.debug('Lock aquired by comp_glob on Rank %d' % self._rank)\n            if rednames is None:\n                reds = self.Lens.keys()\n            else:\n                reds = rednames\n            for redname in reds:\n                for cname in self.Lens[redname].keys():\n                    if self.Lens[redname][cname] == 0:\n                        continue\n                    self._check_Lens(local_data_in, global_data_out,\n                                     redname, cname)\n                    if keep_up:\n                        \"\"\" Capture the new data that will be summed.\n                            Note: if you want this to be sparse, \n                            the caller should probably do it.\n                        \"\"\"\n                        global_data_out[redname][cname] \\\n                            = self.global_data[redname][cname] \\\n                            - self.local_data[redname][cname] \\\n                            + local_data_in[redname][cname]\n                        # The listener might sleep a long time, so update global\n                        np.copyto(self.global_data[redname][cname],  \n                                  global_data_out[redname][cname]) # dst, scr\n                    else:\n                        np.copyto(global_data_out[redname][cname],\n                                  self.global_data[redname][cname]) \n                    # The next copy is done after global,\n                    # so that keep_up can have the previous local_data.(dst, src)\n                    np.copyto(self.local_data[redname][cname],\n                              local_data_in[redname][cname]) \n            self.data_lock.release()\n            logging.debug('Lock released on Rank %d' % self._rank)\n            logging.debug('Ending update on Rank %d' % self._rank)\n            if enable_side_gig:\n                if self.enable_side_gig:\n                    raise RuntimeError(\"side gig already enabled.\")\n                else:\n                    self.enable_side_gig = True # the side_gig must disable\n        else: # synchronous\n            for redname in self.Lens.keys():\n                for cname in self.Lens[redname].keys():\n                    if self.Lens[redname][cname] == 0:\n                        continue\n                    comm = self.comms[cname]\n                    comm.Allreduce([local_data_in[cname], mpi.DOUBLE],\n                                   [global_data_out[cname], mpi.DOUBLE],\n                                   op=mpi.SUM)\n    ####################\n    def get_global_data(self, global_data_out):\n        \"\"\"\n        Copy and return the cached global data.\n\n        Args:\n            global_data_out (dict): global version (often a sum over locals)\n        NOTE:\n            As of March 2019, not used internally (duplicates code in compute)\n        \"\"\"\n        if self.asynch:\n            logging.debug('Enter get_global_data')\n            logging.debug(' get_glob wants lock on Rank %d' % self._rank)\n            self.data_lock.acquire() \n            logging.debug('Lock acquired by get_glob on Rank %d' % self._rank)\n            for redname in self.Lens.keys():\n                for cname in self.Lens[redname].keys():\n                    if self.Lens[redname][cname] == 0:\n                        continue\n                    # dst, src\n                    np.copyto(global_data_out[redname][cname],\n                              self.global_data[redname][cname])\n            self.data_lock.release()\n            logging.debug('Lock released on Rank %d' % self._rank)\n            logging.debug('Leave get_global_data')\n        else:\n            raise RuntimeError(\"get_global_data called for sycnhronous\")\n\n    ####################\n    def _unsafe_get_global_data(self, redname, global_data_out):\n        \"\"\"\n        NO LOCK. Copy and return the cached global data. Call only from\n        within a side_gig!!\n\n        Args:\n            redname (string): the particular reduction name to copy\n            global_data_out (dict): global version (often a sum over locals)\n        NOTE:\n            As of March 2019, not used internally (duplicates code in compute)\n        \"\"\"\n        if self.asynch:\n            logging.debug('Enter _usafe_get_global_data, redname={}'\\\n                          .format(redname))\n            for cname in self.Lens[redname].keys():\n                if self.Lens[redname][cname] == 0:\n                    continue\n                np.copyto(global_data_out[redname][cname],\n                          self.global_data[redname][cname]) # dst, src\n            logging.debug('Leave _unsafe_get_global_data')\n        else:\n            raise RuntimeError(\"_unsafe_get_global_data called for sycnhronous\")\n\n    ####################\n    def _unsafe_put_local_data(self, redname, local_data_in):\n        \"\"\"\n        NO LOCK. Copy and directly in to the local data cache. Call only from\n        within a side_gig!!\n\n        Args:\n            redname (string): the particular reduction name to copy into\n            local_data_in (dict): local data (often a summand over locals)\n        NOTE:\n            As of March 2019, not used internally (duplicates code in compute)\n        \"\"\"\n        if self.asynch:\n            logging.debug('Enter _usafe_put_local_data, redname={}'\\\n                          .format(redname))\n            for cname in self.Lens[redname].keys():\n                if self.Lens[redname][cname] == 0:\n                    continue\n                np.copyto(self.local_data[redname][cname],\n                          local_data_in[redname][cname],) # dst, src\n            logging.debug('Leave _unsafe_put_locobal_data')\n        else:\n            raise RuntimeError(\"_unsafe_put_local_data called for sycnhronous\")\n\n\n    @staticmethod\n    def listener_daemon(rank, synchronizer):\n        # both args added by DLW March 2019\n        # listener side_gigs added by DLW March 2019.\n        logging.debug('Starting Listener on Rank %d' % rank)\n        while synchronizer.global_quitting == 0:\n            # IDEA (Bill):  Add a Barrier here???\n            synchronizer.data_lock.acquire()\n            logging.debug('Locked; starting AllReduce on Rank %d' % rank)\n            for redname in synchronizer.Lens.keys():\n                for cname in synchronizer.Lens[redname].keys():\n                    if synchronizer.Lens[redname][cname] == 0:\n                        continue\n                    comm = synchronizer.comms[cname]\n                    logging.debug('  redname %s cname %s pre-reduce on rank %d' \\\n                                  % (redname, cname, rank))\n                    comm.Allreduce([synchronizer.local_data[redname][cname],\n                                    mpi.DOUBLE],\n                                   [synchronizer.global_data[redname][cname],\n                                    mpi.DOUBLE],\n                                   op=mpi.SUM)\n                    logging.debug(' post-reduce %s on rank %d' % (redname,rank))\n                    if synchronizer.enable_side_gig \\\n                       and synchronizer.listener_gigs is not None \\\n                       and synchronizer.listener_gigs[redname] is not None:\n                        args = [synchronizer]\n                        fct, kwargs = synchronizer.listener_gigs[redname]\n                        if kwargs is not None:\n                            fct(*args, **kwargs)\n                        else:\n                            fct(*args)\n            logging.debug('Still Locked; ending AllReduces on Rank %d' % rank)\n            synchronizer.global_quitting = synchronizer.comms[\"ROOT\"].allreduce(\n                synchronizer.quitting, op=mpi.SUM)\n            sleep_touse = np.zeros(1, dtype='d')\n            sleep_touse[0] = synchronizer.sleep_secs[0]\n\n            synchronizer.comms[\"ROOT\"].Allreduce([synchronizer.sleep_secs,\n                                                 mpi.DOUBLE],\n                                                 [sleep_touse, mpi.DOUBLE],\n                                                 op=mpi.MIN)\n\n            logging.debug('  releasing lock on Rank %d' % rank)\n            synchronizer.data_lock.release()\n            logging.debug('  sleep for %f on Rank %d' % (sleep_touse, rank))\n            try:\n                time.sleep(sleep_touse[0])\n            except:\n                print(\"sleep_touse={}\".format(sleep_touse))\n                raise\n        logging.debug('Exiting listener on Rank %d' % rank)",
  "def __init__(self, comms, Lens, work_fct, rank,\n                 sleep_secs, asynch=False, listener_gigs = None):\n        self.asynch = asynch\n        self.comms = comms\n        self.Lens = Lens\n        self._rank = rank\n        self.sleep_secs = np.zeros(1, dtype='d')\n        self.sleep_secs[0] = sleep_secs\n        self.sleep_touse = np.zeros(1, dtype='d') # min over ranks\n        # The side gigs are very dangerous.\n        self.listener_gigs = listener_gigs\n        self.enable_side_gig = False # TBD should be gigs & indexed by reduction\n        \n        self.global_quitting = 0\n        self.quitting = 0\n        self.work_fct = work_fct\n        self.local_data = {}\n        self.global_data = {}\n        if not isinstance(self.Lens, collections.OrderedDict):\n            raise RuntimeError(\"listener_util: Lens must be an OrderedDict\")\n        for redname in self.Lens.keys():\n            self.local_data[redname] = {}\n            self.global_data[redname] = {}\n            for commname, ell in self.Lens[redname].items():\n                assert(commname in self.comms)\n                self.local_data[redname][commname] = np.zeros(ell, dtype='d')\n                self.global_data[redname][commname] = np.zeros(ell, dtype='d')\n        self.data_lock = threading.Lock()",
  "def run(self, args, kwargs):\n        if self.asynch:\n            # THE WORKER\n            wthread = threading.Thread(name=self.work_fct.__name__,\n                                       target=self.work_fct,\n                                       args = args,\n                                       kwargs = kwargs)\n            ## ph = threading.Thread(name='ph_main', target=ph_main)\n            #ph.setDaemon(True)\n\n            listenargs = [self._rank, self]\n            l = threading.Thread(name='listener',\n                                 target=Synchronizer.listener_daemon,\n                                 args=listenargs)\n            #l.setDaemon(True)\n\n            l.start()\n            wthread.start()\n\n            l.join()\n        else:\n            self.work_fct(*args, **kwargs)",
  "def _check_Lens(self, local_data_in, global_data_out, redname, cname):\n        \"\"\" Essentially local to compute_global_data\n        \"\"\"\n        if len(local_data_in[redname][cname]) \\\n           != self.Lens[redname][cname]:\n            self.global_quitting = 1\n            print (\"\\nERROR listener_util:cname={} len in={} Lens={}\".\\\n                   format(cname, len(local_data_in[redname][cname]),\n                          self.Lens[redname][cname]))\n        if len(global_data_out[redname][cname]) \\\n           != self.Lens[redname][cname]:\n            self.global_quitting = 1\n            print (\"\\nERROR listener_util:cname={} len out={} Lens={}\".\\\n                   format(cname, len(global_data_out[redname][cname]),\n                          self.Lens[redname][cname]))",
  "def compute_global_data(self, local_data_in,\n                            global_data_out,\n                            enable_side_gig = False,\n                            rednames=None,\n                            keep_up=False):\n        \"\"\"\n        Data processing. Cache local_data_in so it will be\n        reduced later by the listener. Copy out the data that\n        was the result of reductions the last time the listener reduced.\n\n        NOTE:\n            Do not call this from a listener side gig (lock issues).\n        Args:\n            local_data_in (dict): data computed locally\n            global_data_out (dict): global version (often a sum over locals)\n            enable_side_gig (boolean): sets a flag that allows the \n                   listener to run side gigs. It is intended to be a run once\n                   authorization and the side gig code itself disables it.\n            rednames (list of str): optional list of reductions to report\n            keep_up (boolean):indicates the data out should include this data in;\n                   otherwise, you will use a global that is \"one notch behind.\"\n        NOTE:\n            as of April 2019. keep_up should be used  very sparingly.\n\n        Note np.copy is dst, src\n        \"\"\"\n        if self.asynch:\n            logging.debug('Starting comp_glob update on Rank %d' % self._rank)\n            self.data_lock.acquire() \n            logging.debug('Lock aquired by comp_glob on Rank %d' % self._rank)\n            if rednames is None:\n                reds = self.Lens.keys()\n            else:\n                reds = rednames\n            for redname in reds:\n                for cname in self.Lens[redname].keys():\n                    if self.Lens[redname][cname] == 0:\n                        continue\n                    self._check_Lens(local_data_in, global_data_out,\n                                     redname, cname)\n                    if keep_up:\n                        \"\"\" Capture the new data that will be summed.\n                            Note: if you want this to be sparse, \n                            the caller should probably do it.\n                        \"\"\"\n                        global_data_out[redname][cname] \\\n                            = self.global_data[redname][cname] \\\n                            - self.local_data[redname][cname] \\\n                            + local_data_in[redname][cname]\n                        # The listener might sleep a long time, so update global\n                        np.copyto(self.global_data[redname][cname],  \n                                  global_data_out[redname][cname]) # dst, scr\n                    else:\n                        np.copyto(global_data_out[redname][cname],\n                                  self.global_data[redname][cname]) \n                    # The next copy is done after global,\n                    # so that keep_up can have the previous local_data.(dst, src)\n                    np.copyto(self.local_data[redname][cname],\n                              local_data_in[redname][cname]) \n            self.data_lock.release()\n            logging.debug('Lock released on Rank %d' % self._rank)\n            logging.debug('Ending update on Rank %d' % self._rank)\n            if enable_side_gig:\n                if self.enable_side_gig:\n                    raise RuntimeError(\"side gig already enabled.\")\n                else:\n                    self.enable_side_gig = True # the side_gig must disable\n        else: # synchronous\n            for redname in self.Lens.keys():\n                for cname in self.Lens[redname].keys():\n                    if self.Lens[redname][cname] == 0:\n                        continue\n                    comm = self.comms[cname]\n                    comm.Allreduce([local_data_in[cname], mpi.DOUBLE],\n                                   [global_data_out[cname], mpi.DOUBLE],\n                                   op=mpi.SUM)",
  "def get_global_data(self, global_data_out):\n        \"\"\"\n        Copy and return the cached global data.\n\n        Args:\n            global_data_out (dict): global version (often a sum over locals)\n        NOTE:\n            As of March 2019, not used internally (duplicates code in compute)\n        \"\"\"\n        if self.asynch:\n            logging.debug('Enter get_global_data')\n            logging.debug(' get_glob wants lock on Rank %d' % self._rank)\n            self.data_lock.acquire() \n            logging.debug('Lock acquired by get_glob on Rank %d' % self._rank)\n            for redname in self.Lens.keys():\n                for cname in self.Lens[redname].keys():\n                    if self.Lens[redname][cname] == 0:\n                        continue\n                    # dst, src\n                    np.copyto(global_data_out[redname][cname],\n                              self.global_data[redname][cname])\n            self.data_lock.release()\n            logging.debug('Lock released on Rank %d' % self._rank)\n            logging.debug('Leave get_global_data')\n        else:\n            raise RuntimeError(\"get_global_data called for sycnhronous\")",
  "def _unsafe_get_global_data(self, redname, global_data_out):\n        \"\"\"\n        NO LOCK. Copy and return the cached global data. Call only from\n        within a side_gig!!\n\n        Args:\n            redname (string): the particular reduction name to copy\n            global_data_out (dict): global version (often a sum over locals)\n        NOTE:\n            As of March 2019, not used internally (duplicates code in compute)\n        \"\"\"\n        if self.asynch:\n            logging.debug('Enter _usafe_get_global_data, redname={}'\\\n                          .format(redname))\n            for cname in self.Lens[redname].keys():\n                if self.Lens[redname][cname] == 0:\n                    continue\n                np.copyto(global_data_out[redname][cname],\n                          self.global_data[redname][cname]) # dst, src\n            logging.debug('Leave _unsafe_get_global_data')\n        else:\n            raise RuntimeError(\"_unsafe_get_global_data called for sycnhronous\")",
  "def _unsafe_put_local_data(self, redname, local_data_in):\n        \"\"\"\n        NO LOCK. Copy and directly in to the local data cache. Call only from\n        within a side_gig!!\n\n        Args:\n            redname (string): the particular reduction name to copy into\n            local_data_in (dict): local data (often a summand over locals)\n        NOTE:\n            As of March 2019, not used internally (duplicates code in compute)\n        \"\"\"\n        if self.asynch:\n            logging.debug('Enter _usafe_put_local_data, redname={}'\\\n                          .format(redname))\n            for cname in self.Lens[redname].keys():\n                if self.Lens[redname][cname] == 0:\n                    continue\n                np.copyto(self.local_data[redname][cname],\n                          local_data_in[redname][cname],) # dst, src\n            logging.debug('Leave _unsafe_put_locobal_data')\n        else:\n            raise RuntimeError(\"_unsafe_put_local_data called for sycnhronous\")",
  "def listener_daemon(rank, synchronizer):\n        # both args added by DLW March 2019\n        # listener side_gigs added by DLW March 2019.\n        logging.debug('Starting Listener on Rank %d' % rank)\n        while synchronizer.global_quitting == 0:\n            # IDEA (Bill):  Add a Barrier here???\n            synchronizer.data_lock.acquire()\n            logging.debug('Locked; starting AllReduce on Rank %d' % rank)\n            for redname in synchronizer.Lens.keys():\n                for cname in synchronizer.Lens[redname].keys():\n                    if synchronizer.Lens[redname][cname] == 0:\n                        continue\n                    comm = synchronizer.comms[cname]\n                    logging.debug('  redname %s cname %s pre-reduce on rank %d' \\\n                                  % (redname, cname, rank))\n                    comm.Allreduce([synchronizer.local_data[redname][cname],\n                                    mpi.DOUBLE],\n                                   [synchronizer.global_data[redname][cname],\n                                    mpi.DOUBLE],\n                                   op=mpi.SUM)\n                    logging.debug(' post-reduce %s on rank %d' % (redname,rank))\n                    if synchronizer.enable_side_gig \\\n                       and synchronizer.listener_gigs is not None \\\n                       and synchronizer.listener_gigs[redname] is not None:\n                        args = [synchronizer]\n                        fct, kwargs = synchronizer.listener_gigs[redname]\n                        if kwargs is not None:\n                            fct(*args, **kwargs)\n                        else:\n                            fct(*args)\n            logging.debug('Still Locked; ending AllReduces on Rank %d' % rank)\n            synchronizer.global_quitting = synchronizer.comms[\"ROOT\"].allreduce(\n                synchronizer.quitting, op=mpi.SUM)\n            sleep_touse = np.zeros(1, dtype='d')\n            sleep_touse[0] = synchronizer.sleep_secs[0]\n\n            synchronizer.comms[\"ROOT\"].Allreduce([synchronizer.sleep_secs,\n                                                 mpi.DOUBLE],\n                                                 [sleep_touse, mpi.DOUBLE],\n                                                 op=mpi.MIN)\n\n            logging.debug('  releasing lock on Rank %d' % rank)\n            synchronizer.data_lock.release()\n            logging.debug('  sleep for %f on Rank %d' % (sleep_touse, rank))\n            try:\n                time.sleep(sleep_touse[0])\n            except:\n                print(\"sleep_touse={}\".format(sleep_touse))\n                raise\n        logging.debug('Exiting listener on Rank %d' % rank)",
  "class FWPH(mpisppy.phbase.PHBase):\n    \n    def __init__(\n        self,\n        PH_options,\n        FW_options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,\n        mpicomm=None,\n        scenario_creator_kwargs=None,\n        ph_converger=None,\n        rho_setter=None,\n    ):\n        super().__init__(\n            PH_options, \n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            extensions=None,\n            extension_kwargs=None,\n            ph_converger=ph_converger,\n            rho_setter=rho_setter,\n        )\n\n        self._init(FW_options)\n\n    def _init(self, FW_options):\n        self.FW_options = FW_options\n        self._options_checks_fw()\n        self.vb = True\n        if ('FW_verbose' in self.FW_options):\n            self.vb = self.FW_options['FW_verbose']\n\n    def fw_prep(self):\n        self.PH_Prep(attach_duals=True, attach_prox=False)\n        self.subproblem_creation(self.options['verbose'])\n        self._output_header()\n\n        if ('point_creator' in self.FW_options):\n            # The user cannot both specify and point_creator and use bundles.\n            # At this point, we have already checked for that possibility, so\n            # we can safely use the point_creator without further checks for\n            # bundles.\n            self._create_initial_points()\n            check = True if 'check_initial_points' not in self.FW_options \\\n                         else self.FW_options['check_initial_points']\n            if (check):\n                self._check_initial_points()\n            self._create_solvers()\n            self._use_rho_setter(verbose and self.cylinder_rank==0)\n            self._initialize_MIP_var_values()\n            best_bound = -np.inf if self.is_minimizing else np.inf\n        else:\n            trivial_bound = self.Iter0()\n            secs = time.time() - self.t0\n            self._output(0, trivial_bound, trivial_bound, np.nan, secs)\n            best_bound = trivial_bound\n\n        if ('mip_solver_options' in self.FW_options):\n            self._set_MIP_solver_options()\n\n        # Lines 2 and 3 of Algorithm 3 in Boland\n        self.Compute_Xbar(self.options['verbose'])\n        self.Update_W(self.options['verbose'])\n\n        # Necessary pre-processing steps\n        # We disable_W so they don't appear\n        # in the MIP objective when _set_QP_objective\n        # snarfs it for the QP\n        self._disable_W()\n        self._attach_MIP_vars()\n        self._initialize_QP_subproblems()\n        self._attach_indices()\n        self._attach_MIP_QP_maps()\n        self._set_QP_objective()\n        self._initialize_QP_var_values()\n        self._swap_nonant_vars()\n        self._reenable_W()\n\n        if (self.ph_converger):\n            self.convobject = self.ph_converger(self, self.cylinder_rank, self.n_proc)\n\n        return best_bound\n\n    def fwph_main(self):\n        self.t0 = time.time()\n        best_bound = self.fw_prep()\n\n        # FWPH takes some time to initialize\n        # If run as a spoke, check for convergence here\n        if self.spcomm and self.spcomm.is_converged():\n            return None, None, None\n\n        # The body of the algorithm\n        for itr in range(self.options['PHIterLimit']):\n            self._PHIter = itr\n            self._local_bound = 0\n            for name in self.local_subproblems:\n                dual_bound = self.SDM(name)\n                self._local_bound += self.local_subproblems[name]._mpisppy_probability * \\\n                                     dual_bound\n            self._compute_dual_bound()\n            if (self.is_minimizing):\n                best_bound = np.maximum(best_bound, self._local_bound)\n            else:\n                best_bound = np.minimum(best_bound, self._local_bound)\n\n            ## Hubs/spokes take precedence over convergers\n            if self.spcomm:\n                if self.spcomm.is_converged():\n                    secs = time.time() - self.t0\n                    self._output(itr+1, self._local_bound, \n                                 best_bound, np.nan, secs)\n                    if (self.cylinder_rank == 0 and self.vb):\n                        print('FWPH converged to user-specified criteria')\n                    break\n                self.spcomm.sync()\n            if (self.ph_converger):\n                self.Compute_Xbar(self.options['verbose'])\n                diff = self.convobject.convergence_value()\n                if (self.convobject.is_converged()):\n                    secs = time.time() - self.t0\n                    self._output(itr+1, self._local_bound, \n                                 best_bound, diff, secs)\n                    if (self.cylinder_rank == 0 and self.vb):\n                        print('FWPH converged to user-specified criteria')\n                    break\n            else: # Convergence check from Boland\n                diff = self._conv_diff()\n                self.Compute_Xbar(self.options['verbose'])\n                if (diff < self.options['convthresh']):\n                    secs = time.time() - self.t0\n                    self._output(itr+1, self._local_bound, \n                                 best_bound, diff, secs)\n                    if (self.cylinder_rank == 0 and self.vb):\n                        print('PH converged based on standard criteria')\n                    break\n\n            secs = time.time() - self.t0\n            self._output(itr+1, self._local_bound, best_bound, diff, secs)\n            self.Update_W(self.options['verbose'])\n            timed_out = self._is_timed_out()\n            if (self._is_timed_out()):\n                if (self.cylinder_rank == 0 and self.vb):\n                    print('Timeout.')\n                break\n\n        self._swap_nonant_vars_back()\n        weight_dict = self._gather_weight_dict() # None if rank != 0\n        xbars_dict  = self._get_xbars() # None if rank != 0\n        return itr+1, weight_dict, xbars_dict\n\n    def SDM(self, model_name):\n        '''  Algorithm 2 in Boland et al. (with small tweaks)\n        '''\n        mip = self.local_subproblems[model_name]\n        qp  = self.local_QP_subproblems[model_name]\n    \n        # Set the QP dual weights to the correct values If we are bundling, we\n        # initialize the QP dual weights to be the dual weights associated with\n        # the first scenario in the bundle (this should be okay, because each\n        # scenario in the bundle has the same dual weights, analytically--maybe\n        # a numerical problem).\n        arb_scen_mip = self.local_scenarios[mip.scen_list[0]] \\\n                       if self.bundling else mip\n        for (node_name, ix) in arb_scen_mip._mpisppy_data.nonant_indices:\n            qp._mpisppy_model.W[node_name, ix]._value = \\\n                arb_scen_mip._mpisppy_model.W[node_name, ix].value\n\n        alpha = self.FW_options['FW_weight']\n        # Algorithm 3 line 6\n        xt = {ndn_i:\n            (1 - alpha) * pyo.value(arb_scen_mip._mpisppy_model.xbars[ndn_i])\n            + alpha * pyo.value(xvar)\n            for ndn_i, xvar in arb_scen_mip._mpisppy_data.nonant_indices.items()\n            }\n\n        for itr in range(self.FW_options['FW_iter_limit']):\n            # Algorithm 2 line 4\n            mip_source = mip.scen_list if self.bundling else [model_name]\n            for scenario_name in mip_source:\n                scen_mip = self.local_scenarios[scenario_name]\n                for ndn_i, nonant in scen_mip._mpisppy_data.nonant_indices.items():\n                    x_source = xt[ndn_i] if itr==0 \\\n                               else nonant._value\n                    scen_mip._mpisppy_model.W[ndn_i]._value = (\n                        qp._mpisppy_model.W[ndn_i]._value\n                        + scen_mip._mpisppy_model.rho[ndn_i]._value\n                        * (x_source\n                        -  scen_mip._mpisppy_model.xbars[ndn_i]._value))\n\n            # Algorithm 2 line 5\n            if (sputils.is_persistent(mip._solver_plugin)):\n                mip_obj = find_active_objective(mip)\n                mip._solver_plugin.set_objective(mip_obj)\n            mip_results = mip._solver_plugin.solve(mip)\n            self._check_solve(mip_results, model_name + ' (MIP)')\n\n            # Algorithm 2 lines 6--8\n            obj = find_active_objective(mip)\n            if (itr == 0):\n                if (self.is_minimizing):\n                    dual_bound = mip_results.Problem[0].Lower_bound\n                else:\n                    dual_bound = mip_results.Problem[0].Upper_bound\n\n            # Algorithm 2 line 9 (compute \\Gamma^t)\n            val0 = pyo.value(obj)\n            new  = replace_expressions(obj.expr, mip._mpisppy_data.mip_to_qp)\n            val1 = pyo.value(new)\n            obj.expr = replace_expressions(new, qp._mpisppy_data.qp_to_mip)\n            if abs(val0) > 1e-9:\n                stop_check = (val1 - val0) / abs(val0) # \\Gamma^t in Boland, but normalized\n            else:\n                stop_check = val1 - val0 # \\Gamma^t in Boland\n            stop_check_tol = self.FW_options[\"stop_check_tol\"]\\\n                             if \"stop_check_tol\" in self.FW_options else 1e-4\n            if (self.is_minimizing and stop_check < -stop_check_tol):\n                print('Warning (fwph): convergence quantity Gamma^t = '\n                     '{sc:.2e} (should be non-negative)'.format(sc=stop_check))\n                print('Try decreasing the MIP gap tolerance and re-solving')\n            elif (not self.is_minimizing and stop_check > stop_check_tol):\n                print('Warning (fwph): convergence quantity Gamma^t = '\n                     '{sc:.2e} (should be non-positive)'.format(sc=stop_check))\n                print('Try decreasing the MIP gap tolerance and re-solving')\n\n            self._add_QP_column(model_name)\n            if (sputils.is_persistent(qp._QP_solver_plugin)):\n                qp_obj = find_active_objective(qp)\n                qp._QP_solver_plugin.set_objective(qp_obj)\n            qp_results = qp._QP_solver_plugin.solve(qp)\n            self._check_solve(qp_results, model_name + ' (QP)')\n\n            if (stop_check < self.FW_options['FW_conv_thresh']):\n                break\n\n        # Re-set the mip._mpisppy_model.W so that the QP objective \n        # is correct in the next major iteration\n        mip_source = mip.scen_list if self.bundling else [model_name]\n        for scenario_name in mip_source:\n            scen_mip = self.local_scenarios[scenario_name]\n            for (node_name, ix) in scen_mip._mpisppy_data.nonant_indices:\n                scen_mip._mpisppy_model.W[node_name, ix]._value = \\\n                    qp._mpisppy_model.W[node_name, ix]._value\n\n        return dual_bound\n\n    def _add_QP_column(self, model_name):\n        ''' Add a column to the QP, with values taken from the most recent MIP\n            solve.\n        '''\n        mip = self.local_subproblems[model_name]\n        qp  = self.local_QP_subproblems[model_name]\n        solver = qp._QP_solver_plugin\n        persistent = sputils.is_persistent(solver)\n\n        if hasattr(solver, 'add_column'):\n            new_var = qp.a.add()\n            coef_list = [1.]\n            constr_list = [qp.sum_one]\n            target = mip.ref_vars if self.bundling else mip.nonant_vars\n            for (node, ix) in qp.eqx.index_set():\n                coef_list.append(target[node, ix].value)\n                constr_list.append(qp.eqx[node, ix])\n            for key in mip._mpisppy_model.y_indices:\n                coef_list.append(mip.leaf_vars[key].value)\n                constr_list.append(qp.eqy[key])\n            solver.add_column(qp, new_var, 0, constr_list, coef_list)\n            return\n\n        # Add new variable and update \\sum a_i = 1 constraint\n        new_var = qp.a.add() # Add the new convex comb. variable\n        if (persistent):\n            solver.add_var(new_var)\n            solver.remove_constraint(qp.sum_one)\n            qp.sum_one._body += new_var\n            solver.add_constraint(qp.sum_one)\n        else:\n            qp.sum_one._body += new_var\n\n        target = mip.ref_vars if self.bundling else mip.nonant_vars\n        for (node, ix) in qp.eqx.index_set():\n            if (persistent):\n                solver.remove_constraint(qp.eqx[node, ix])\n                qp.eqx[node, ix]._body += new_var * target[node, ix].value\n                solver.add_constraint(qp.eqx[node, ix])\n            else:\n                qp.eqx[node, ix]._body += new_var * target[node,ix].value\n        for key in mip._mpisppy_model.y_indices:\n            if (persistent):\n                solver.remove_constraint(qp.eqy[key])\n                qp.eqy[key]._body += new_var * pyo.value(mip.leaf_vars[key])\n                solver.add_constraint(qp.eqy[key])\n            else:\n                qp.eqy[key]._body += new_var * pyo.value(mip.leaf_vars[key])\n\n    def _attach_indices(self):\n        ''' Attach the fields x_indices and y_indices to the model objects in\n            self.local_subproblems (not self.local_scenarios, nor\n            self.local_QP_subproblems).\n\n            x_indices is a list of tuples of the form...\n                (scenario name, node name, variable index) <-- bundling\n                (node name, variable index)                <-- no bundling\n            y_indices is a list of tuples of the form...\n                (scenario_name, \"LEAF\", variable index)    <-- bundling\n                (\"LEAF\", variable index)                   <-- no bundling\n            \n            Must be called after the subproblems (MIPs AND QPs) are created.\n        '''\n        for name in self.local_subproblems.keys():\n            mip = self.local_subproblems[name]\n            qp  = self.local_QP_subproblems[name]\n            if (self.bundling):\n                x_indices = [(scenario_name, node_name, ix)\n                    for scenario_name in mip.scen_list\n                    for (node_name, ix) in \n                        self.local_scenarios[scenario_name]._mpisppy_data.nonant_indices]\n                y_indices = [(scenario_name, 'LEAF', ix)\n                    for scenario_name in mip.scen_list\n                    for ix in range(mip.num_leaf_vars[scenario_name])]\n            else:\n                x_indices = mip._mpisppy_data.nonant_indices.keys()\n                y_indices = [('LEAF', ix) for ix in range(len(qp.y))]\n\n            y_indices = pyo.Set(initialize=y_indices)\n            y_indices.construct()\n            x_indices = pyo.Set(initialize=x_indices)\n            x_indices.construct()\n            mip._mpisppy_model.x_indices = x_indices\n            mip._mpisppy_model.y_indices = y_indices\n\n    def _attach_MIP_QP_maps(self):\n        ''' Create dictionaries that map MIP variable ids to their QP\n            counterparts, and vice versa.\n        '''\n        for name in self.local_subproblems.keys():\n            mip = self.local_subproblems[name]\n            qp  = self.local_QP_subproblems[name]\n\n            mip._mpisppy_data.mip_to_qp = {id(mip.nonant_vars[key]): qp.x[key]\n                                for key in mip._mpisppy_model.x_indices}\n            mip._mpisppy_data.mip_to_qp.update({id(mip.leaf_vars[key]): qp.y[key]\n                                for key in mip._mpisppy_model.y_indices})\n            qp._mpisppy_data.qp_to_mip = {id(qp.x[key]): mip.nonant_vars[key]\n                                for key in mip._mpisppy_model.x_indices}\n            qp._mpisppy_data.qp_to_mip.update({id(qp.y[key]): mip.leaf_vars[key]\n                                for key in mip._mpisppy_model.y_indices})\n\n    def _attach_MIP_vars(self):\n        ''' Create a list indexed (node_name, ix) for all the MIP\n            non-anticipative and leaf variables, so that they can be easily\n            accessed when adding columns to the QP.\n        '''\n        if (self.bundling):\n            for (bundle_name, EF) in self.local_subproblems.items():\n                EF.nonant_vars = dict()\n                EF.leaf_vars   = dict()\n                EF.num_leaf_vars = dict() # Keys are scenario names\n                for scenario_name in EF.scen_list:\n                    mip = self.local_scenarios[scenario_name]\n                    # Non-anticipative variables\n                    nonant_dict = {(scenario_name, ndn, ix): nonant\n                        for (ndn,ix), nonant in mip._mpisppy_data.nonant_indices.items()}\n                    EF.nonant_vars.update(nonant_dict)\n                    # Leaf variables\n                    leaf_var_dict = {(scenario_name, 'LEAF', ix):\n                        var for ix, var in enumerate(self._get_leaf_vars(mip))}\n                    EF.leaf_vars.update(leaf_var_dict)\n                    EF.num_leaf_vars[scenario_name] = len(leaf_vars_dict)\n                    # Reference variables are already attached: EF.ref_vars\n                    # indexed by (node_name, index)\n        else:\n            for (name, mip) in self.local_scenarios.items():\n                mip.nonant_vars = mip._mpisppy_data.nonant_indices\n                mip.leaf_vars = { ('LEAF', ix):\n                    var for ix, var in enumerate(self._get_leaf_vars(mip))\n                }\n\n    def _check_initial_points(self):\n        ''' If t_max (i.e. the inner iteration limit) is set to 1, then the\n            initial point set must satisfy the additional condition (17) in\n            Boland et al. This function verifies that condition (17) is\n            satisfied by solving a linear program (similar to the Phase I\n            auxiliary LP in two-phase simplex).\n\n            This function is only called by a single rank, which MUST be rank\n            0. The rank 0 check happens before this function is called.\n        '''\n        # Need to get the first-stage variable names (as supplied by the user)\n        # by picking them off of any random scenario that's laying around.\n        arb_scenario = list(self.local_scenarios.keys())[0]\n        arb_mip = self.local_scenarios[arb_scenario]\n        root = arb_mip._mpisppy_node_list[0]\n        stage_one_var_names = [var.name for var in root.nonant_vardata_list]\n\n        init_pts = self.comms['ROOT'].gather(self.local_initial_points, root=0)\n        if (self.cylinder_rank != 0):\n            return\n\n        print('Checking initial points...', end='', flush=True)\n        \n        points = {key: value for block in init_pts \n                             for (key, value) in block.items()}\n        scenario_names = points.keys()\n        num_scenarios = len(points)\n\n        # Some index sets we will need..\n        conv_ix = [(scenario_name, var_name) \n                    for scenario_name in scenario_names\n                    for var_name in stage_one_var_names]\n        conv_coeff = [(scenario_name, ix) \n                    for scenario_name in scenario_names\n                    for ix in range(len(points[scenario_name]))]\n\n        aux = pyo.ConcreteModel()\n        aux.x = pyo.Var(stage_one_var_names, within=pyo.Reals)\n        aux.slack_plus = pyo.Var(conv_ix, within=pyo.NonNegativeReals)\n        aux.slack_minus = pyo.Var(conv_ix, within=pyo.NonNegativeReals)\n        aux.conv = pyo.Var(conv_coeff, within=pyo.NonNegativeReals)\n\n        def sum_one_rule(model, scenario_name):\n            return pyo.quicksum(model.conv[scenario_name,ix] \\\n                for ix in range(len(points[scenario_name]))) == 1\n        aux.sum_one = pyo.Constraint(scenario_names, rule=sum_one_rule)\n\n        def conv_rule(model, scenario_name, var_name):\n            return model.x[var_name] \\\n                + model.slack_plus[scenario_name, var_name] \\\n                - model.slack_minus[scenario_name, var_name] \\\n                == pyo.quicksum(model.conv[scenario_name, ix] *\n                    points[scenario_name][ix][var_name] \n                    for ix in range(len(points[scenario_name])))\n        aux.comb = pyo.Constraint(conv_ix, rule=conv_rule)\n\n        obj_expr = pyo.quicksum(aux.slack_plus.values()) \\\n                   + pyo.quicksum(aux.slack_minus.values())\n        aux.obj = pyo.Objective(expr=obj_expr, sense=pyo.minimize)\n\n        solver = pyo.SolverFactory(self.FW_options['solver_name'])\n        results = solver.solve(aux)\n        self._check_solve(results, 'Auxiliary LP')\n\n        check_tol = self.FW_options['check_tol'] \\\n                        if 'check_tol' in self.FW_options.keys() else 1e-4\n        if (pyo.value(obj_expr) > check_tol):\n            print('error.')\n            raise ValueError('The specified initial points do not satisfy the '\n                'critera necessary for convergence. Please specify different '\n                'initial points, or increase FW_iter_limit')\n        print('done.')\n\n    def _check_solve(self, results, model_name):\n        ''' Verify that the solver solved to optimality '''\n        if (results.solver.status != pyo.SolverStatus.ok) or \\\n            (results.solver.termination_condition != pyo.TerminationCondition.optimal):\n            print('Solve failed on model', model_name)\n            print('Solver status:', results.solver.status)\n            print('Termination conditions:', results.solver.termination_condition)\n            raise RuntimeError()\n\n    def _compute_dual_bound(self):\n        ''' Compute the FWPH dual bound using self._local_bound from each rank\n        '''\n        send = np.array(self._local_bound)\n        recv = np.array(0.)\n        self.comms['ROOT'].Allreduce(\n            [send, MPI.DOUBLE], [recv, MPI.DOUBLE], op=MPI.SUM)\n        self._local_bound = recv\n\n    def _conv_diff(self):\n        ''' Perform the convergence check of Algorithm 3 in Boland et al. '''\n        diff = 0.\n        for name in self.local_subproblems.keys():\n            mip = self.local_subproblems[name]\n            arb_mip = self.local_scenarios[mip.scen_list[0]] \\\n                        if self.bundling else mip\n            qp  = self.local_QP_subproblems[name]\n            diff_s = 0.\n            for (node_name, ix) in arb_mip._mpisppy_data.nonant_indices:\n                qpx = qp.xr if self.bundling else qp.x\n                diff_s += np.power(pyo.value(qpx[node_name,ix]) - \n                        pyo.value(arb_mip._mpisppy_model.xbars[node_name,ix]), 2)\n            diff_s *= mip._mpisppy_probability\n            diff += diff_s\n        diff = np.array(diff)\n        recv = np.array(0.)\n        self.comms['ROOT'].Allreduce(\n            [diff, MPI.DOUBLE], [recv, MPI.DOUBLE], op=MPI.SUM)\n        return recv\n\n    def _create_initial_points(self):\n        pc = self.FW_options['point_creator']\n        if ('point_creator_data' in self.FW_options.keys()):\n            pd = self.FW_options['point_creator_data']\n        else:\n            pd = None\n        self.local_initial_points = dict()\n        for scenario_name in self.local_scenario_names:\n            pts = pc(scenario_name, point_creator_data=pd)\n            self.local_initial_points[scenario_name] = pts\n\n    def _extract_objective(self, mip):\n        ''' Extract the original part of the provided MIP's objective function\n            (no dual or prox terms), and create a copy containing the QP\n            variables in place of the MIP variables.\n\n            Args:\n                mip (Pyomo ConcreteModel): MIP model for a scenario or bundle.\n\n            Returns:\n                obj (Pyomo Objective): objective function extracted\n                    from the MIP\n                new (Pyomo Expression): expression from the MIP model\n                    objective with the MIP variables replaced by QP variables.\n                    Does not inculde dual or prox terms.\n\n            Notes:\n                Acts on either a single-scenario model or a bundle\n        '''\n        mip_to_qp = mip._mpisppy_data.mip_to_qp\n        obj = find_active_objective(mip)\n        repn = generate_standard_repn(obj.expr, quadratic=True)\n        if len(repn.nonlinear_vars) > 0:\n            raise ValueError(\"FWPH does not support models with nonlinear objective functions\")\n        linear_vars = [mip_to_qp[id(var)] for var in repn.linear_vars]\n        new = LinearExpression(\n            constant=repn.constant, linear_coefs=repn.linear_coefs, linear_vars=linear_vars\n        )\n        if repn.quadratic_vars:\n            quadratic_vars = (\n                (mip_to_qp[id(x)], mip_to_qp[id(y)]) for x,y in repn.quadratic_vars\n            )\n            new += pyo.quicksum(\n                (coef*x*y for coef,(x,y) in zip(repn.quadratic_coefs, quadratic_vars))\n            )\n        return obj, new\n\n    def _gather_weight_dict(self, strip_bundle_names=False):\n        ''' Compute a double nested dictionary of the form\n\n                weights[scenario name][variable name] = weight value\n\n            for FWPH to return to the user.\n        \n            Notes:\n                Must be called after the variables are swapped back.\n        '''\n        local_weights = dict()\n        for (name, scenario) in self.local_scenarios.items():\n            if (self.bundling and strip_bundle_names):\n                scenario_weights = dict()\n                for ndn_ix, var in scenario._mpisppy_data.nonant_indices.items():\n                    rexp = '^' + scenario.name + '\\.'\n                    var_name = re.sub(rexp, '', var.name)\n                    scenario_weights[var_name] = \\\n                                        scenario._mpisppy_model.W[ndn_ix].value\n            else:\n                scenario_weights = {nonant.name: scenario._mpisppy_model.W[ndn_ix].value\n                        for ndn_ix, nonant in scenario._mpisppy_data.nonant_indices.items()}\n            local_weights[name] = scenario_weights\n\n        weights = self.comms['ROOT'].gather(local_weights, root=0)\n        return weights\n\n    def _get_leaf_vars(self, scenario):\n        ''' This method simply needs to take an input scenario\n            (pyo.ConcreteModel) and yield the variable objects\n            corresponding to the leaf node variables for that scenario.\n\n            Functions by returning the complement of the set of\n            non-anticipative variables.\n        '''\n        nonant_var_ids = {id(var) for node in scenario._mpisppy_node_list\n                                  for var  in node.nonant_vardata_list}\n        for var in scenario.component_data_objects(pyo.Var):\n            if id(var) not in nonant_var_ids:\n                yield var\n\n    def _get_xbars(self, strip_bundle_names=False):\n        ''' Return the xbar vector if rank = 0 and None, otherwise\n            (Consistent with _gather_weight_dict).\n\n            Args:\n                TODO\n                \n            Notes:\n                Paralellism is not necessary since each rank already has an\n                identical copy of xbar, provided by Compute_Xbar().\n\n                Returned dictionary is indexed by variable name \n                (as provided by the user).\n\n                Must be called after variables are swapped back (I think).\n        '''\n        if (self.cylinder_rank != 0):\n            return None\n        else:\n            random_scenario_name = list(self.local_scenarios.keys())[0]\n            scenario = self.local_scenarios[random_scenario_name]\n            xbar_dict = {}\n            for node in scenario._mpisppy_node_list:\n                for (ix, var) in enumerate(node.nonant_vardata_list):\n                    var_name = var.name\n                    if (self.bundling and strip_bundle_names):\n                        rexp = '^' + random_scenario_name + '\\.'\n                        var_name = re.sub(rexp, '', var_name)\n                    xbar_dict[var_name] = scenario._mpisppy_model.xbars[node.name, ix].value\n            return xbar_dict\n\n    def _initialize_MIP_var_values(self):\n        ''' Initialize the MIP variable values to the user-specified point.\n\n            For now, arbitrarily choose the first point in each list.\n        '''\n        points = self.local_initial_points\n        for (name, mip) in self.local_subproblems.items():\n            pt = points[name][0] # Select the first point arbitrarily\n            mip_vars = list(mip.component_data_objects(pyo.Var))\n            for var in mip_vars:\n                try:\n                    var.set_value(pt[var.name])\n                except KeyError as e:\n                    raise KeyError('Found variable named' + var.name +\n                        ' in model ' + name + ' not contained in the '\n                        'specified initial point dictionary') from e\n\n    def _initialize_QP_subproblems(self):\n        ''' Instantiates the (convex) QP subproblems (eqn. (13) in the Boland\n            paper) for each scenario. Does not create/attach an objective.\n\n            Attachs a local_QP_subproblems dict to self. Keys are scenario\n            names (or bundle names), values are Pyomo ConcreteModel objects\n            corresponding to the QP subproblems. \n\n            QP subproblems are in their original form, without the x and y\n            variables eliminated. Rationale: pre-solve will get this, easier\n            bookkeeping (objective does not need to be changed at each inner\n            iteration this way).\n        '''\n        self.local_QP_subproblems = dict()\n        has_init_pts = hasattr(self, 'local_initial_points')\n        for (name, model) in self.local_subproblems.items():\n            if (self.bundling):\n                xr_indices = model.ref_vars.keys()\n                nonant_indices = model.nonant_vars.keys()\n                leaf_indices = model.leaf_vars.keys()\n                if (has_init_pts):\n                    raise RuntimeError('Cannot currently specify '\n                        'initial points while using bundles')\n            else:\n                nonant_indices = model._mpisppy_data.nonant_indices.keys()\n                leaf_indices = model.leaf_vars.keys()\n\n            ''' Convex comb. coefficients '''\n            QP = pyo.ConcreteModel()\n            QP.a = pyo.VarList(domain=pyo.NonNegativeReals)\n            if (has_init_pts):\n                for _ in range(len(self.local_initial_points[name])):\n                    QP.a.add()\n            else:\n                QP.a.add() # Just one variable (1-based index!) to start\n\n            ''' Other variables '''\n            QP.x = pyo.Var(nonant_indices, within=pyo.Reals)\n            QP.y = pyo.Var(leaf_indices, within=pyo.Reals)\n            if (self.bundling):\n                QP.xr = pyo.Var(xr_indices, within=pyo.Reals)\n\n            ''' Non-anticipativity constraint '''\n            if (self.bundling):\n                def nonant_rule(m, scenario_name, node_name, ix):\n                    return m.x[scenario_name, node_name, ix] == \\\n                            m.xr[node_name, ix]\n                QP.na = pyo.Constraint(nonant_indices, rule=nonant_rule)\n            \n            ''' (x,y) constraints '''\n            if (self.bundling):\n                def x_rule(m, node_name, ix):\n                    return -m.xr[node_name, ix] + m.a[1] * \\\n                            model.ref_vars[node_name, ix].value == 0\n                def y_rule(m, scenario_name, node_name, ix):\n                    return -m.y[scenario_name, node_name, ix] + m.a[1]\\\n                        * model.leaf_vars[scenario_name,node_name,ix].value == 0 \n                QP.eqx = pyo.Constraint(xr_indices, rule=x_rule)\n            else:\n                if (has_init_pts):\n                    pts = self.local_initial_points[name]\n                    def x_rule(m, node_name, ix):\n                        nm = model.nonant_vars[node_name, ix].name\n                        return -m.x[node_name, ix] + \\\n                            pyo.quicksum(m.a[i+1] * pts[i][nm] \n                                for i in range(len(pts))) == 0\n                    def y_rule(m, node_name, ix):\n                        nm = model.leaf_vars[node_name, ix].name\n                        return -m.y[node_name,ix] + \\\n                            pyo.quicksum(m.a[i+1] * pts[i][nm] \n                                for i in range(len(pts))) == 0\n                else:\n                    def x_rule(m, node_name, ix):\n                        return -m.x[node_name, ix] + m.a[1] * \\\n                                model.nonant_vars[node_name, ix].value == 0\n                    def y_rule(m, node_name, ix):\n                        return -m.y[node_name,ix] + m.a[1] * \\\n                                model.leaf_vars['LEAF', ix].value == 0\n                QP.eqx = pyo.Constraint(nonant_indices, rule=x_rule)\n\n            QP.eqy = pyo.Constraint(leaf_indices, rule=y_rule)\n            QP.sum_one = pyo.Constraint(expr=pyo.quicksum(QP.a.values())==1)\n\n            QP._mpisppy_data = pyo.Block(name=\"For non-Pyomo mpi-sppy data\")\n            QP._mpisppy_model = pyo.Block(name=\"For mpi-sppy Pyomo additions to the scenario model\")\n\n            self.local_QP_subproblems[name] = QP\n                \n    def _initialize_QP_var_values(self):\n        ''' Set the value of the QP variables to be equal to the values of the\n            corresponding MIP variables.\n\n            Notes:\n                Must be called before _swap_nonant_vars()\n\n                Must be called after _initialize_MIP_var_values(), if the user\n                specifies initial sets of points. Otherwise, it must be called\n                after Iter0().\n        '''\n        for name in self.local_subproblems.keys():\n            mip = self.local_subproblems[name]\n            qp  = self.local_QP_subproblems[name]\n\n            for key in mip._mpisppy_model.x_indices:\n                qp.x[key].set_value(mip.nonant_vars[key].value)\n            for key in mip._mpisppy_model.y_indices:\n                qp.y[key].set_value(mip.leaf_vars[key].value)\n\n            # Set the non-anticipative reference variables if we're bundling\n            if (self.bundling):\n                arb_scenario = mip.scen_list[0]\n                naix = self.local_scenarios[arb_scenario]._mpisppy_data.nonant_indices\n                for (node_name, ix) in naix:\n                    # Check that non-anticipativity is satisfied\n                    # within the bundle (for debugging)\n                    vals = [mip.nonant_vars[scenario_name, node_name, ix].value\n                            for scenario_name in mip.scen_list]\n                    assert(max(vals) - min(vals) < 1e-7)\n                    qp.xr[node_name, ix].set_value(\n                        mip.nonant_vars[arb_scenario, node_name, ix].value)\n\n    def _is_timed_out(self):\n        if (self.cylinder_rank == 0):\n            time_elapsed = time.time() - self.t0\n            status = 1 if (time_elapsed > self.FW_options['time_limit']) \\\n                       else 0\n        else:\n            status = None\n        status = self.comms['ROOT'].bcast(status, root=0)\n        return status != 0\n\n    def _options_checks_fw(self):\n        ''' Name                Boland notation (Algorithm 2)\n            -------------------------------------------------\n            FW_iter_limit       t_max\n            FW_weight           alpha\n            FW_conv_thresh      tau\n        '''\n        # 1. Check for required options\n        reqd_options = ['FW_iter_limit', 'FW_weight', 'FW_conv_thresh',\n                        'solver_name']\n        losers = [opt for opt in reqd_options if opt not in self.FW_options]\n        if (len(losers) > 0):\n            msg = \"FW_options is missing the following key(s): \" + \\\n                  \", \".join(losers)\n            raise RuntimeError(msg)\n\n        # 2. Check that bundles, pre-specified points and t_max play nice. This\n        #    is only checked on rank 0, because that is where the initial\n        #    points are supposed to be specified.\n        use_bundles = ('bundles_per_rank' in self.options \n                        and self.options['bundles_per_rank'] > 0)\n        t_max = self.FW_options['FW_iter_limit']\n        specd_init_pts = 'point_creator' in self.FW_options.keys() and \\\n                         self.FW_options['point_creator'] is not None\n\n        if (use_bundles and specd_init_pts):\n            if (t_max == 1):\n                raise RuntimeError('Cannot use bundles and specify initial '\n                    'points with t_max=1 at the same time.')\n            else:\n                if (self.cylinder_rank == 0):\n                    print('WARNING: Cannot specify initial points and use '\n                        'bundles at the same time. Ignoring specified initial '\n                        'points')\n                # Remove specified initial points\n                self.FW_options.pop('point_creator', None)\n\n        if (t_max == 1 and not specd_init_pts):\n            raise RuntimeError('FW_iter_limit set to 1. To ensure '\n                'convergence, provide initial points, or increase '\n                'FW_iter_limit')\n\n        # 3a. Check that the user did not specify the linearization of binary\n        #    proximal terms (no binary variables allowed in FWPH QPs)\n        if ('linearize_binary_proximal_terms' in self.options\n            and self.options['linearize_binary_proximal_terms']):\n            print('Warning: linearize_binary_proximal_terms cannot be used '\n                  'with the FWPH algorithm. Ignoring...')\n            self.options['linearize_binary_proximal_terms'] = False\n\n        # 3b. Check that the user did not specify the linearization of all\n        #    proximal terms (FWPH QPs should be QPs)\n        if ('linearize_proximal_terms' in self.options\n            and self.options['linearize_proximal_terms']):\n            print('Warning: linearize_proximal_terms cannot be used '\n                  'with the FWPH algorithm. Ignoring...')\n            self.options['linearize_proximal_terms'] = False\n\n        # 4. Provide a time limit of inf if the user did not specify\n        if ('time_limit' not in self.FW_options.keys()):\n            self.FW_options['time_limit'] = np.inf\n\n    def _output(self, itr, bound, best_bound, diff, secs):\n        if (self.cylinder_rank == 0 and self.vb):\n            print('{itr:3d} {bound:12.4f} {best_bound:12.4f} {diff:12.4e} {secs:11.1f}s'.format(\n                    itr=itr, bound=bound, best_bound=best_bound, \n                    diff=diff, secs=secs))\n        if (self.cylinder_rank == 0 and 'save_file' in self.FW_options.keys()):\n            fname = self.FW_options['save_file']\n            with open(fname, 'a') as f:\n                f.write('{itr:d},{bound:.16f},{best_bound:.16f},{diff:.16f},{secs:.16f}\\n'.format(\n                    itr=itr, bound=bound, best_bound=best_bound,\n                    diff=diff, secs=secs))\n\n    def _output_header(self):\n        if (self.cylinder_rank == 0 and self.vb):\n            print('itr {bound:>12s} {bb:>12s} {cd:>12s} {tm:>12s}'.format(\n                    bound=\"bound\", bb=\"best bound\", cd=\"conv diff\", tm=\"time\"))\n        if (self.cylinder_rank == 0 and 'save_file' in self.FW_options.keys()):\n            fname = self.FW_options['save_file']\n            with open(fname, 'a') as f:\n                f.write('{itr:s},{bound:s},{bb:s},{diff:s},{secs:s}\\n'.format(\n                    itr=\"Iteration\", bound=\"Bound\", bb=\"Best bound\",\n                    diff=\"Error\", secs=\"Time(s)\"))\n\n    def save_weights(self, fname):\n        ''' Save the computed weights to the specified file.\n\n            Notes:\n                Handles parallelism--only writes one copy of the file.\n\n                Rather \"fast-and-loose\", in that it doesn't enforce _when_ this\n                function can be called.\n        '''\n        weights = self._gather_weight_dict(strip_bundle_names=self.bundling) # None if rank != 0\n        if (self.cylinder_rank != 0):\n            return\n        with open(fname, 'w') as f:\n            for block in weights:\n                for (scenario_name, wts) in block.items():\n                    for (var_name, weight_val) in wts.items():\n                        row = '{sn},{vn},{wv:.16f}\\n'.format(\n                            sn=scenario_name, vn=var_name, wv=weight_val)\n                        f.write(row)\n\n    def save_xbars(self, fname):\n        ''' Save the computed xbar to the specified file.\n\n            Notes:\n                Handles parallelism--only writes one copy of the file.\n\n                Rather \"fast-and-loose\", in that it doesn't enforce _when_ this\n                function can be called.\n        '''\n        if (self.cylinder_rank != 0):\n            return\n        xbars = self._get_xbars(strip_bundle_names=self.bundling) # None if rank != 0\n        with open(fname, 'w') as f:\n            for (var_name, xbs) in xbars.items():\n                row = '{vn},{vv:.16f}\\n'.format(vn=var_name, vv=xbs)\n                f.write(row)\n\n    def _set_MIP_solver_options(self):\n        mip_opts = self.FW_options['mip_solver_options']\n        if (len(mip_opts) > 0):\n            for model in self.local_subproblems.values():\n                for (key, option) in mip_opts.items():\n                    model._solver_plugin.options[key] = option\n\n    def _set_QP_objective(self):\n        ''' Attach dual weights, objective function and solver to each QP.\n        \n            QP dual weights are initialized to the MIP dual weights.\n        '''\n\n        for name, mip in self.local_subproblems.items():\n            QP = self.local_QP_subproblems[name]\n\n            obj, new = self._extract_objective(mip)\n\n            ## Finish setting up objective for QP\n            if self.bundling:\n                m_source = self.local_scenarios[mip.scen_list[0]]\n                x_source = QP.xr\n            else:\n                m_source = mip\n                x_source = QP.x\n\n            QP._mpisppy_model.W = pyo.Param(\n                m_source._mpisppy_data.nonant_indices.keys(), mutable=True, initialize=m_source._mpisppy_model.W\n            )\n            # rhos are attached to each scenario, not each bundle (should they be?)\n            ph_term = pyo.quicksum((\n                QP._mpisppy_model.W[nni] * x_source[nni] +\n                (m_source._mpisppy_model.rho[nni] / 2.) * (x_source[nni] - m_source._mpisppy_model.xbars[nni]) * (x_source[nni] - m_source._mpisppy_model.xbars[nni])\n                for nni in m_source._mpisppy_data.nonant_indices\n            ))\n\n            if obj.is_minimizing():\n                QP.obj = pyo.Objective(expr=new+ph_term, sense=pyo.minimize)\n            else:\n                QP.obj = pyo.Objective(expr=-new+ph_term, sense=pyo.minimize)\n\n            ''' Attach a solver with various options '''\n            solver = pyo.SolverFactory(self.FW_options['solver_name'])\n            if sputils.is_persistent(solver):\n                solver.set_instance(QP)\n            if 'qp_solver_options' in self.FW_options:\n                qp_opts = self.FW_options['qp_solver_options']\n                if qp_opts:\n                    for (key, option) in qp_opts.items():\n                        solver.options[key] = option\n\n            self.local_QP_subproblems[name]._QP_solver_plugin = solver\n\n    def _swap_nonant_vars(self):\n        ''' Change the pointers in\n            scenario._mpisppy_node_list[i].nonant_vardata_list\n            to point to the QP variables, rather than the MIP variables.\n\n            Notes:\n                When computing xBar and updating the weights in the outer\n                iteration, the values of the x variables are pulled from\n                scenario._mpisppy_node_list[i].nonant_vardata_list. In the FWPH\n                algorithm, xBar should be computed using the QP values, not the\n                MIP values (like in normal PH).\n\n                Reruns SPBase._attach_nonant_indices so that the scenario \n                _nonant_indices dictionary has the correct variable pointers\n                \n                Updates nonant_vardata_list but NOT nonant_list.\n        '''\n        for (name, model) in self.local_subproblems.items():\n            scens = model.scen_list if self.bundling else [name]\n            for scenario_name in scens:\n                scenario = self.local_scenarios[scenario_name]\n                num_nonant_vars = scenario._mpisppy_data.nlens\n                node_list = scenario._mpisppy_node_list\n                for node in node_list:\n                    node.nonant_vardata_list = [\n                        self.local_QP_subproblems[name].xr[node.name,i]\n                        if self.bundling else\n                        self.local_QP_subproblems[name].x[node.name,i]\n                        for i in range(num_nonant_vars[node.name])]\n        self._attach_nonant_indices()\n\n    def _swap_nonant_vars_back(self):\n        ''' Swap variables back, in case they're needed somewhere else.\n        '''\n        for (name, model) in self.local_subproblems.items():\n            if (self.bundling):\n                EF = self.local_subproblems[name]\n                for scenario_name in EF.scen_list:\n                    scenario = self.local_scenarios[scenario_name]\n                    num_nonant_vars = scenario._mpisppy_data.nlens\n                    for node in scenario._mpisppy_node_list:\n                        node.nonant_vardata_list = [\n                            EF.nonant_vars[scenario_name,node.name,ix]\n                            for ix in range(num_nonant_vars[node.name])]\n            else:\n                scenario = self.local_scenarios[name]\n                num_nonant_vars = scenario._mpisppy_data.nlens\n                for node in scenario._mpisppy_node_list:\n                    node.nonant_vardata_list = [\n                        scenario.nonant_vars[node.name,ix]\n                        for ix in range(num_nonant_vars[node.name])]\n        self._attach_nonant_indices()",
  "def __init__(\n        self,\n        PH_options,\n        FW_options,\n        all_scenario_names,\n        scenario_creator,\n        scenario_denouement=None,\n        all_nodenames=None,\n        mpicomm=None,\n        scenario_creator_kwargs=None,\n        ph_converger=None,\n        rho_setter=None,\n    ):\n        super().__init__(\n            PH_options, \n            all_scenario_names,\n            scenario_creator,\n            scenario_denouement,\n            all_nodenames=all_nodenames,\n            mpicomm=mpicomm,\n            scenario_creator_kwargs=scenario_creator_kwargs,\n            extensions=None,\n            extension_kwargs=None,\n            ph_converger=ph_converger,\n            rho_setter=rho_setter,\n        )\n\n        self._init(FW_options)",
  "def _init(self, FW_options):\n        self.FW_options = FW_options\n        self._options_checks_fw()\n        self.vb = True\n        if ('FW_verbose' in self.FW_options):\n            self.vb = self.FW_options['FW_verbose']",
  "def fw_prep(self):\n        self.PH_Prep(attach_duals=True, attach_prox=False)\n        self.subproblem_creation(self.options['verbose'])\n        self._output_header()\n\n        if ('point_creator' in self.FW_options):\n            # The user cannot both specify and point_creator and use bundles.\n            # At this point, we have already checked for that possibility, so\n            # we can safely use the point_creator without further checks for\n            # bundles.\n            self._create_initial_points()\n            check = True if 'check_initial_points' not in self.FW_options \\\n                         else self.FW_options['check_initial_points']\n            if (check):\n                self._check_initial_points()\n            self._create_solvers()\n            self._use_rho_setter(verbose and self.cylinder_rank==0)\n            self._initialize_MIP_var_values()\n            best_bound = -np.inf if self.is_minimizing else np.inf\n        else:\n            trivial_bound = self.Iter0()\n            secs = time.time() - self.t0\n            self._output(0, trivial_bound, trivial_bound, np.nan, secs)\n            best_bound = trivial_bound\n\n        if ('mip_solver_options' in self.FW_options):\n            self._set_MIP_solver_options()\n\n        # Lines 2 and 3 of Algorithm 3 in Boland\n        self.Compute_Xbar(self.options['verbose'])\n        self.Update_W(self.options['verbose'])\n\n        # Necessary pre-processing steps\n        # We disable_W so they don't appear\n        # in the MIP objective when _set_QP_objective\n        # snarfs it for the QP\n        self._disable_W()\n        self._attach_MIP_vars()\n        self._initialize_QP_subproblems()\n        self._attach_indices()\n        self._attach_MIP_QP_maps()\n        self._set_QP_objective()\n        self._initialize_QP_var_values()\n        self._swap_nonant_vars()\n        self._reenable_W()\n\n        if (self.ph_converger):\n            self.convobject = self.ph_converger(self, self.cylinder_rank, self.n_proc)\n\n        return best_bound",
  "def fwph_main(self):\n        self.t0 = time.time()\n        best_bound = self.fw_prep()\n\n        # FWPH takes some time to initialize\n        # If run as a spoke, check for convergence here\n        if self.spcomm and self.spcomm.is_converged():\n            return None, None, None\n\n        # The body of the algorithm\n        for itr in range(self.options['PHIterLimit']):\n            self._PHIter = itr\n            self._local_bound = 0\n            for name in self.local_subproblems:\n                dual_bound = self.SDM(name)\n                self._local_bound += self.local_subproblems[name]._mpisppy_probability * \\\n                                     dual_bound\n            self._compute_dual_bound()\n            if (self.is_minimizing):\n                best_bound = np.maximum(best_bound, self._local_bound)\n            else:\n                best_bound = np.minimum(best_bound, self._local_bound)\n\n            ## Hubs/spokes take precedence over convergers\n            if self.spcomm:\n                if self.spcomm.is_converged():\n                    secs = time.time() - self.t0\n                    self._output(itr+1, self._local_bound, \n                                 best_bound, np.nan, secs)\n                    if (self.cylinder_rank == 0 and self.vb):\n                        print('FWPH converged to user-specified criteria')\n                    break\n                self.spcomm.sync()\n            if (self.ph_converger):\n                self.Compute_Xbar(self.options['verbose'])\n                diff = self.convobject.convergence_value()\n                if (self.convobject.is_converged()):\n                    secs = time.time() - self.t0\n                    self._output(itr+1, self._local_bound, \n                                 best_bound, diff, secs)\n                    if (self.cylinder_rank == 0 and self.vb):\n                        print('FWPH converged to user-specified criteria')\n                    break\n            else: # Convergence check from Boland\n                diff = self._conv_diff()\n                self.Compute_Xbar(self.options['verbose'])\n                if (diff < self.options['convthresh']):\n                    secs = time.time() - self.t0\n                    self._output(itr+1, self._local_bound, \n                                 best_bound, diff, secs)\n                    if (self.cylinder_rank == 0 and self.vb):\n                        print('PH converged based on standard criteria')\n                    break\n\n            secs = time.time() - self.t0\n            self._output(itr+1, self._local_bound, best_bound, diff, secs)\n            self.Update_W(self.options['verbose'])\n            timed_out = self._is_timed_out()\n            if (self._is_timed_out()):\n                if (self.cylinder_rank == 0 and self.vb):\n                    print('Timeout.')\n                break\n\n        self._swap_nonant_vars_back()\n        weight_dict = self._gather_weight_dict() # None if rank != 0\n        xbars_dict  = self._get_xbars() # None if rank != 0\n        return itr+1, weight_dict, xbars_dict",
  "def SDM(self, model_name):\n        '''  Algorithm 2 in Boland et al. (with small tweaks)\n        '''\n        mip = self.local_subproblems[model_name]\n        qp  = self.local_QP_subproblems[model_name]\n    \n        # Set the QP dual weights to the correct values If we are bundling, we\n        # initialize the QP dual weights to be the dual weights associated with\n        # the first scenario in the bundle (this should be okay, because each\n        # scenario in the bundle has the same dual weights, analytically--maybe\n        # a numerical problem).\n        arb_scen_mip = self.local_scenarios[mip.scen_list[0]] \\\n                       if self.bundling else mip\n        for (node_name, ix) in arb_scen_mip._mpisppy_data.nonant_indices:\n            qp._mpisppy_model.W[node_name, ix]._value = \\\n                arb_scen_mip._mpisppy_model.W[node_name, ix].value\n\n        alpha = self.FW_options['FW_weight']\n        # Algorithm 3 line 6\n        xt = {ndn_i:\n            (1 - alpha) * pyo.value(arb_scen_mip._mpisppy_model.xbars[ndn_i])\n            + alpha * pyo.value(xvar)\n            for ndn_i, xvar in arb_scen_mip._mpisppy_data.nonant_indices.items()\n            }\n\n        for itr in range(self.FW_options['FW_iter_limit']):\n            # Algorithm 2 line 4\n            mip_source = mip.scen_list if self.bundling else [model_name]\n            for scenario_name in mip_source:\n                scen_mip = self.local_scenarios[scenario_name]\n                for ndn_i, nonant in scen_mip._mpisppy_data.nonant_indices.items():\n                    x_source = xt[ndn_i] if itr==0 \\\n                               else nonant._value\n                    scen_mip._mpisppy_model.W[ndn_i]._value = (\n                        qp._mpisppy_model.W[ndn_i]._value\n                        + scen_mip._mpisppy_model.rho[ndn_i]._value\n                        * (x_source\n                        -  scen_mip._mpisppy_model.xbars[ndn_i]._value))\n\n            # Algorithm 2 line 5\n            if (sputils.is_persistent(mip._solver_plugin)):\n                mip_obj = find_active_objective(mip)\n                mip._solver_plugin.set_objective(mip_obj)\n            mip_results = mip._solver_plugin.solve(mip)\n            self._check_solve(mip_results, model_name + ' (MIP)')\n\n            # Algorithm 2 lines 6--8\n            obj = find_active_objective(mip)\n            if (itr == 0):\n                if (self.is_minimizing):\n                    dual_bound = mip_results.Problem[0].Lower_bound\n                else:\n                    dual_bound = mip_results.Problem[0].Upper_bound\n\n            # Algorithm 2 line 9 (compute \\Gamma^t)\n            val0 = pyo.value(obj)\n            new  = replace_expressions(obj.expr, mip._mpisppy_data.mip_to_qp)\n            val1 = pyo.value(new)\n            obj.expr = replace_expressions(new, qp._mpisppy_data.qp_to_mip)\n            if abs(val0) > 1e-9:\n                stop_check = (val1 - val0) / abs(val0) # \\Gamma^t in Boland, but normalized\n            else:\n                stop_check = val1 - val0 # \\Gamma^t in Boland\n            stop_check_tol = self.FW_options[\"stop_check_tol\"]\\\n                             if \"stop_check_tol\" in self.FW_options else 1e-4\n            if (self.is_minimizing and stop_check < -stop_check_tol):\n                print('Warning (fwph): convergence quantity Gamma^t = '\n                     '{sc:.2e} (should be non-negative)'.format(sc=stop_check))\n                print('Try decreasing the MIP gap tolerance and re-solving')\n            elif (not self.is_minimizing and stop_check > stop_check_tol):\n                print('Warning (fwph): convergence quantity Gamma^t = '\n                     '{sc:.2e} (should be non-positive)'.format(sc=stop_check))\n                print('Try decreasing the MIP gap tolerance and re-solving')\n\n            self._add_QP_column(model_name)\n            if (sputils.is_persistent(qp._QP_solver_plugin)):\n                qp_obj = find_active_objective(qp)\n                qp._QP_solver_plugin.set_objective(qp_obj)\n            qp_results = qp._QP_solver_plugin.solve(qp)\n            self._check_solve(qp_results, model_name + ' (QP)')\n\n            if (stop_check < self.FW_options['FW_conv_thresh']):\n                break\n\n        # Re-set the mip._mpisppy_model.W so that the QP objective \n        # is correct in the next major iteration\n        mip_source = mip.scen_list if self.bundling else [model_name]\n        for scenario_name in mip_source:\n            scen_mip = self.local_scenarios[scenario_name]\n            for (node_name, ix) in scen_mip._mpisppy_data.nonant_indices:\n                scen_mip._mpisppy_model.W[node_name, ix]._value = \\\n                    qp._mpisppy_model.W[node_name, ix]._value\n\n        return dual_bound",
  "def _add_QP_column(self, model_name):\n        ''' Add a column to the QP, with values taken from the most recent MIP\n            solve.\n        '''\n        mip = self.local_subproblems[model_name]\n        qp  = self.local_QP_subproblems[model_name]\n        solver = qp._QP_solver_plugin\n        persistent = sputils.is_persistent(solver)\n\n        if hasattr(solver, 'add_column'):\n            new_var = qp.a.add()\n            coef_list = [1.]\n            constr_list = [qp.sum_one]\n            target = mip.ref_vars if self.bundling else mip.nonant_vars\n            for (node, ix) in qp.eqx.index_set():\n                coef_list.append(target[node, ix].value)\n                constr_list.append(qp.eqx[node, ix])\n            for key in mip._mpisppy_model.y_indices:\n                coef_list.append(mip.leaf_vars[key].value)\n                constr_list.append(qp.eqy[key])\n            solver.add_column(qp, new_var, 0, constr_list, coef_list)\n            return\n\n        # Add new variable and update \\sum a_i = 1 constraint\n        new_var = qp.a.add() # Add the new convex comb. variable\n        if (persistent):\n            solver.add_var(new_var)\n            solver.remove_constraint(qp.sum_one)\n            qp.sum_one._body += new_var\n            solver.add_constraint(qp.sum_one)\n        else:\n            qp.sum_one._body += new_var\n\n        target = mip.ref_vars if self.bundling else mip.nonant_vars\n        for (node, ix) in qp.eqx.index_set():\n            if (persistent):\n                solver.remove_constraint(qp.eqx[node, ix])\n                qp.eqx[node, ix]._body += new_var * target[node, ix].value\n                solver.add_constraint(qp.eqx[node, ix])\n            else:\n                qp.eqx[node, ix]._body += new_var * target[node,ix].value\n        for key in mip._mpisppy_model.y_indices:\n            if (persistent):\n                solver.remove_constraint(qp.eqy[key])\n                qp.eqy[key]._body += new_var * pyo.value(mip.leaf_vars[key])\n                solver.add_constraint(qp.eqy[key])\n            else:\n                qp.eqy[key]._body += new_var * pyo.value(mip.leaf_vars[key])",
  "def _attach_indices(self):\n        ''' Attach the fields x_indices and y_indices to the model objects in\n            self.local_subproblems (not self.local_scenarios, nor\n            self.local_QP_subproblems).\n\n            x_indices is a list of tuples of the form...\n                (scenario name, node name, variable index) <-- bundling\n                (node name, variable index)                <-- no bundling\n            y_indices is a list of tuples of the form...\n                (scenario_name, \"LEAF\", variable index)    <-- bundling\n                (\"LEAF\", variable index)                   <-- no bundling\n            \n            Must be called after the subproblems (MIPs AND QPs) are created.\n        '''\n        for name in self.local_subproblems.keys():\n            mip = self.local_subproblems[name]\n            qp  = self.local_QP_subproblems[name]\n            if (self.bundling):\n                x_indices = [(scenario_name, node_name, ix)\n                    for scenario_name in mip.scen_list\n                    for (node_name, ix) in \n                        self.local_scenarios[scenario_name]._mpisppy_data.nonant_indices]\n                y_indices = [(scenario_name, 'LEAF', ix)\n                    for scenario_name in mip.scen_list\n                    for ix in range(mip.num_leaf_vars[scenario_name])]\n            else:\n                x_indices = mip._mpisppy_data.nonant_indices.keys()\n                y_indices = [('LEAF', ix) for ix in range(len(qp.y))]\n\n            y_indices = pyo.Set(initialize=y_indices)\n            y_indices.construct()\n            x_indices = pyo.Set(initialize=x_indices)\n            x_indices.construct()\n            mip._mpisppy_model.x_indices = x_indices\n            mip._mpisppy_model.y_indices = y_indices",
  "def _attach_MIP_QP_maps(self):\n        ''' Create dictionaries that map MIP variable ids to their QP\n            counterparts, and vice versa.\n        '''\n        for name in self.local_subproblems.keys():\n            mip = self.local_subproblems[name]\n            qp  = self.local_QP_subproblems[name]\n\n            mip._mpisppy_data.mip_to_qp = {id(mip.nonant_vars[key]): qp.x[key]\n                                for key in mip._mpisppy_model.x_indices}\n            mip._mpisppy_data.mip_to_qp.update({id(mip.leaf_vars[key]): qp.y[key]\n                                for key in mip._mpisppy_model.y_indices})\n            qp._mpisppy_data.qp_to_mip = {id(qp.x[key]): mip.nonant_vars[key]\n                                for key in mip._mpisppy_model.x_indices}\n            qp._mpisppy_data.qp_to_mip.update({id(qp.y[key]): mip.leaf_vars[key]\n                                for key in mip._mpisppy_model.y_indices})",
  "def _attach_MIP_vars(self):\n        ''' Create a list indexed (node_name, ix) for all the MIP\n            non-anticipative and leaf variables, so that they can be easily\n            accessed when adding columns to the QP.\n        '''\n        if (self.bundling):\n            for (bundle_name, EF) in self.local_subproblems.items():\n                EF.nonant_vars = dict()\n                EF.leaf_vars   = dict()\n                EF.num_leaf_vars = dict() # Keys are scenario names\n                for scenario_name in EF.scen_list:\n                    mip = self.local_scenarios[scenario_name]\n                    # Non-anticipative variables\n                    nonant_dict = {(scenario_name, ndn, ix): nonant\n                        for (ndn,ix), nonant in mip._mpisppy_data.nonant_indices.items()}\n                    EF.nonant_vars.update(nonant_dict)\n                    # Leaf variables\n                    leaf_var_dict = {(scenario_name, 'LEAF', ix):\n                        var for ix, var in enumerate(self._get_leaf_vars(mip))}\n                    EF.leaf_vars.update(leaf_var_dict)\n                    EF.num_leaf_vars[scenario_name] = len(leaf_vars_dict)\n                    # Reference variables are already attached: EF.ref_vars\n                    # indexed by (node_name, index)\n        else:\n            for (name, mip) in self.local_scenarios.items():\n                mip.nonant_vars = mip._mpisppy_data.nonant_indices\n                mip.leaf_vars = { ('LEAF', ix):\n                    var for ix, var in enumerate(self._get_leaf_vars(mip))\n                }",
  "def _check_initial_points(self):\n        ''' If t_max (i.e. the inner iteration limit) is set to 1, then the\n            initial point set must satisfy the additional condition (17) in\n            Boland et al. This function verifies that condition (17) is\n            satisfied by solving a linear program (similar to the Phase I\n            auxiliary LP in two-phase simplex).\n\n            This function is only called by a single rank, which MUST be rank\n            0. The rank 0 check happens before this function is called.\n        '''\n        # Need to get the first-stage variable names (as supplied by the user)\n        # by picking them off of any random scenario that's laying around.\n        arb_scenario = list(self.local_scenarios.keys())[0]\n        arb_mip = self.local_scenarios[arb_scenario]\n        root = arb_mip._mpisppy_node_list[0]\n        stage_one_var_names = [var.name for var in root.nonant_vardata_list]\n\n        init_pts = self.comms['ROOT'].gather(self.local_initial_points, root=0)\n        if (self.cylinder_rank != 0):\n            return\n\n        print('Checking initial points...', end='', flush=True)\n        \n        points = {key: value for block in init_pts \n                             for (key, value) in block.items()}\n        scenario_names = points.keys()\n        num_scenarios = len(points)\n\n        # Some index sets we will need..\n        conv_ix = [(scenario_name, var_name) \n                    for scenario_name in scenario_names\n                    for var_name in stage_one_var_names]\n        conv_coeff = [(scenario_name, ix) \n                    for scenario_name in scenario_names\n                    for ix in range(len(points[scenario_name]))]\n\n        aux = pyo.ConcreteModel()\n        aux.x = pyo.Var(stage_one_var_names, within=pyo.Reals)\n        aux.slack_plus = pyo.Var(conv_ix, within=pyo.NonNegativeReals)\n        aux.slack_minus = pyo.Var(conv_ix, within=pyo.NonNegativeReals)\n        aux.conv = pyo.Var(conv_coeff, within=pyo.NonNegativeReals)\n\n        def sum_one_rule(model, scenario_name):\n            return pyo.quicksum(model.conv[scenario_name,ix] \\\n                for ix in range(len(points[scenario_name]))) == 1\n        aux.sum_one = pyo.Constraint(scenario_names, rule=sum_one_rule)\n\n        def conv_rule(model, scenario_name, var_name):\n            return model.x[var_name] \\\n                + model.slack_plus[scenario_name, var_name] \\\n                - model.slack_minus[scenario_name, var_name] \\\n                == pyo.quicksum(model.conv[scenario_name, ix] *\n                    points[scenario_name][ix][var_name] \n                    for ix in range(len(points[scenario_name])))\n        aux.comb = pyo.Constraint(conv_ix, rule=conv_rule)\n\n        obj_expr = pyo.quicksum(aux.slack_plus.values()) \\\n                   + pyo.quicksum(aux.slack_minus.values())\n        aux.obj = pyo.Objective(expr=obj_expr, sense=pyo.minimize)\n\n        solver = pyo.SolverFactory(self.FW_options['solver_name'])\n        results = solver.solve(aux)\n        self._check_solve(results, 'Auxiliary LP')\n\n        check_tol = self.FW_options['check_tol'] \\\n                        if 'check_tol' in self.FW_options.keys() else 1e-4\n        if (pyo.value(obj_expr) > check_tol):\n            print('error.')\n            raise ValueError('The specified initial points do not satisfy the '\n                'critera necessary for convergence. Please specify different '\n                'initial points, or increase FW_iter_limit')\n        print('done.')",
  "def _check_solve(self, results, model_name):\n        ''' Verify that the solver solved to optimality '''\n        if (results.solver.status != pyo.SolverStatus.ok) or \\\n            (results.solver.termination_condition != pyo.TerminationCondition.optimal):\n            print('Solve failed on model', model_name)\n            print('Solver status:', results.solver.status)\n            print('Termination conditions:', results.solver.termination_condition)\n            raise RuntimeError()",
  "def _compute_dual_bound(self):\n        ''' Compute the FWPH dual bound using self._local_bound from each rank\n        '''\n        send = np.array(self._local_bound)\n        recv = np.array(0.)\n        self.comms['ROOT'].Allreduce(\n            [send, MPI.DOUBLE], [recv, MPI.DOUBLE], op=MPI.SUM)\n        self._local_bound = recv",
  "def _conv_diff(self):\n        ''' Perform the convergence check of Algorithm 3 in Boland et al. '''\n        diff = 0.\n        for name in self.local_subproblems.keys():\n            mip = self.local_subproblems[name]\n            arb_mip = self.local_scenarios[mip.scen_list[0]] \\\n                        if self.bundling else mip\n            qp  = self.local_QP_subproblems[name]\n            diff_s = 0.\n            for (node_name, ix) in arb_mip._mpisppy_data.nonant_indices:\n                qpx = qp.xr if self.bundling else qp.x\n                diff_s += np.power(pyo.value(qpx[node_name,ix]) - \n                        pyo.value(arb_mip._mpisppy_model.xbars[node_name,ix]), 2)\n            diff_s *= mip._mpisppy_probability\n            diff += diff_s\n        diff = np.array(diff)\n        recv = np.array(0.)\n        self.comms['ROOT'].Allreduce(\n            [diff, MPI.DOUBLE], [recv, MPI.DOUBLE], op=MPI.SUM)\n        return recv",
  "def _create_initial_points(self):\n        pc = self.FW_options['point_creator']\n        if ('point_creator_data' in self.FW_options.keys()):\n            pd = self.FW_options['point_creator_data']\n        else:\n            pd = None\n        self.local_initial_points = dict()\n        for scenario_name in self.local_scenario_names:\n            pts = pc(scenario_name, point_creator_data=pd)\n            self.local_initial_points[scenario_name] = pts",
  "def _extract_objective(self, mip):\n        ''' Extract the original part of the provided MIP's objective function\n            (no dual or prox terms), and create a copy containing the QP\n            variables in place of the MIP variables.\n\n            Args:\n                mip (Pyomo ConcreteModel): MIP model for a scenario or bundle.\n\n            Returns:\n                obj (Pyomo Objective): objective function extracted\n                    from the MIP\n                new (Pyomo Expression): expression from the MIP model\n                    objective with the MIP variables replaced by QP variables.\n                    Does not inculde dual or prox terms.\n\n            Notes:\n                Acts on either a single-scenario model or a bundle\n        '''\n        mip_to_qp = mip._mpisppy_data.mip_to_qp\n        obj = find_active_objective(mip)\n        repn = generate_standard_repn(obj.expr, quadratic=True)\n        if len(repn.nonlinear_vars) > 0:\n            raise ValueError(\"FWPH does not support models with nonlinear objective functions\")\n        linear_vars = [mip_to_qp[id(var)] for var in repn.linear_vars]\n        new = LinearExpression(\n            constant=repn.constant, linear_coefs=repn.linear_coefs, linear_vars=linear_vars\n        )\n        if repn.quadratic_vars:\n            quadratic_vars = (\n                (mip_to_qp[id(x)], mip_to_qp[id(y)]) for x,y in repn.quadratic_vars\n            )\n            new += pyo.quicksum(\n                (coef*x*y for coef,(x,y) in zip(repn.quadratic_coefs, quadratic_vars))\n            )\n        return obj, new",
  "def _gather_weight_dict(self, strip_bundle_names=False):\n        ''' Compute a double nested dictionary of the form\n\n                weights[scenario name][variable name] = weight value\n\n            for FWPH to return to the user.\n        \n            Notes:\n                Must be called after the variables are swapped back.\n        '''\n        local_weights = dict()\n        for (name, scenario) in self.local_scenarios.items():\n            if (self.bundling and strip_bundle_names):\n                scenario_weights = dict()\n                for ndn_ix, var in scenario._mpisppy_data.nonant_indices.items():\n                    rexp = '^' + scenario.name + '\\.'\n                    var_name = re.sub(rexp, '', var.name)\n                    scenario_weights[var_name] = \\\n                                        scenario._mpisppy_model.W[ndn_ix].value\n            else:\n                scenario_weights = {nonant.name: scenario._mpisppy_model.W[ndn_ix].value\n                        for ndn_ix, nonant in scenario._mpisppy_data.nonant_indices.items()}\n            local_weights[name] = scenario_weights\n\n        weights = self.comms['ROOT'].gather(local_weights, root=0)\n        return weights",
  "def _get_leaf_vars(self, scenario):\n        ''' This method simply needs to take an input scenario\n            (pyo.ConcreteModel) and yield the variable objects\n            corresponding to the leaf node variables for that scenario.\n\n            Functions by returning the complement of the set of\n            non-anticipative variables.\n        '''\n        nonant_var_ids = {id(var) for node in scenario._mpisppy_node_list\n                                  for var  in node.nonant_vardata_list}\n        for var in scenario.component_data_objects(pyo.Var):\n            if id(var) not in nonant_var_ids:\n                yield var",
  "def _get_xbars(self, strip_bundle_names=False):\n        ''' Return the xbar vector if rank = 0 and None, otherwise\n            (Consistent with _gather_weight_dict).\n\n            Args:\n                TODO\n                \n            Notes:\n                Paralellism is not necessary since each rank already has an\n                identical copy of xbar, provided by Compute_Xbar().\n\n                Returned dictionary is indexed by variable name \n                (as provided by the user).\n\n                Must be called after variables are swapped back (I think).\n        '''\n        if (self.cylinder_rank != 0):\n            return None\n        else:\n            random_scenario_name = list(self.local_scenarios.keys())[0]\n            scenario = self.local_scenarios[random_scenario_name]\n            xbar_dict = {}\n            for node in scenario._mpisppy_node_list:\n                for (ix, var) in enumerate(node.nonant_vardata_list):\n                    var_name = var.name\n                    if (self.bundling and strip_bundle_names):\n                        rexp = '^' + random_scenario_name + '\\.'\n                        var_name = re.sub(rexp, '', var_name)\n                    xbar_dict[var_name] = scenario._mpisppy_model.xbars[node.name, ix].value\n            return xbar_dict",
  "def _initialize_MIP_var_values(self):\n        ''' Initialize the MIP variable values to the user-specified point.\n\n            For now, arbitrarily choose the first point in each list.\n        '''\n        points = self.local_initial_points\n        for (name, mip) in self.local_subproblems.items():\n            pt = points[name][0] # Select the first point arbitrarily\n            mip_vars = list(mip.component_data_objects(pyo.Var))\n            for var in mip_vars:\n                try:\n                    var.set_value(pt[var.name])\n                except KeyError as e:\n                    raise KeyError('Found variable named' + var.name +\n                        ' in model ' + name + ' not contained in the '\n                        'specified initial point dictionary') from e",
  "def _initialize_QP_subproblems(self):\n        ''' Instantiates the (convex) QP subproblems (eqn. (13) in the Boland\n            paper) for each scenario. Does not create/attach an objective.\n\n            Attachs a local_QP_subproblems dict to self. Keys are scenario\n            names (or bundle names), values are Pyomo ConcreteModel objects\n            corresponding to the QP subproblems. \n\n            QP subproblems are in their original form, without the x and y\n            variables eliminated. Rationale: pre-solve will get this, easier\n            bookkeeping (objective does not need to be changed at each inner\n            iteration this way).\n        '''\n        self.local_QP_subproblems = dict()\n        has_init_pts = hasattr(self, 'local_initial_points')\n        for (name, model) in self.local_subproblems.items():\n            if (self.bundling):\n                xr_indices = model.ref_vars.keys()\n                nonant_indices = model.nonant_vars.keys()\n                leaf_indices = model.leaf_vars.keys()\n                if (has_init_pts):\n                    raise RuntimeError('Cannot currently specify '\n                        'initial points while using bundles')\n            else:\n                nonant_indices = model._mpisppy_data.nonant_indices.keys()\n                leaf_indices = model.leaf_vars.keys()\n\n            ''' Convex comb. coefficients '''\n            QP = pyo.ConcreteModel()\n            QP.a = pyo.VarList(domain=pyo.NonNegativeReals)\n            if (has_init_pts):\n                for _ in range(len(self.local_initial_points[name])):\n                    QP.a.add()\n            else:\n                QP.a.add() # Just one variable (1-based index!) to start\n\n            ''' Other variables '''\n            QP.x = pyo.Var(nonant_indices, within=pyo.Reals)\n            QP.y = pyo.Var(leaf_indices, within=pyo.Reals)\n            if (self.bundling):\n                QP.xr = pyo.Var(xr_indices, within=pyo.Reals)\n\n            ''' Non-anticipativity constraint '''\n            if (self.bundling):\n                def nonant_rule(m, scenario_name, node_name, ix):\n                    return m.x[scenario_name, node_name, ix] == \\\n                            m.xr[node_name, ix]\n                QP.na = pyo.Constraint(nonant_indices, rule=nonant_rule)\n            \n            ''' (x,y) constraints '''\n            if (self.bundling):\n                def x_rule(m, node_name, ix):\n                    return -m.xr[node_name, ix] + m.a[1] * \\\n                            model.ref_vars[node_name, ix].value == 0\n                def y_rule(m, scenario_name, node_name, ix):\n                    return -m.y[scenario_name, node_name, ix] + m.a[1]\\\n                        * model.leaf_vars[scenario_name,node_name,ix].value == 0 \n                QP.eqx = pyo.Constraint(xr_indices, rule=x_rule)\n            else:\n                if (has_init_pts):\n                    pts = self.local_initial_points[name]\n                    def x_rule(m, node_name, ix):\n                        nm = model.nonant_vars[node_name, ix].name\n                        return -m.x[node_name, ix] + \\\n                            pyo.quicksum(m.a[i+1] * pts[i][nm] \n                                for i in range(len(pts))) == 0\n                    def y_rule(m, node_name, ix):\n                        nm = model.leaf_vars[node_name, ix].name\n                        return -m.y[node_name,ix] + \\\n                            pyo.quicksum(m.a[i+1] * pts[i][nm] \n                                for i in range(len(pts))) == 0\n                else:\n                    def x_rule(m, node_name, ix):\n                        return -m.x[node_name, ix] + m.a[1] * \\\n                                model.nonant_vars[node_name, ix].value == 0\n                    def y_rule(m, node_name, ix):\n                        return -m.y[node_name,ix] + m.a[1] * \\\n                                model.leaf_vars['LEAF', ix].value == 0\n                QP.eqx = pyo.Constraint(nonant_indices, rule=x_rule)\n\n            QP.eqy = pyo.Constraint(leaf_indices, rule=y_rule)\n            QP.sum_one = pyo.Constraint(expr=pyo.quicksum(QP.a.values())==1)\n\n            QP._mpisppy_data = pyo.Block(name=\"For non-Pyomo mpi-sppy data\")\n            QP._mpisppy_model = pyo.Block(name=\"For mpi-sppy Pyomo additions to the scenario model\")\n\n            self.local_QP_subproblems[name] = QP",
  "def _initialize_QP_var_values(self):\n        ''' Set the value of the QP variables to be equal to the values of the\n            corresponding MIP variables.\n\n            Notes:\n                Must be called before _swap_nonant_vars()\n\n                Must be called after _initialize_MIP_var_values(), if the user\n                specifies initial sets of points. Otherwise, it must be called\n                after Iter0().\n        '''\n        for name in self.local_subproblems.keys():\n            mip = self.local_subproblems[name]\n            qp  = self.local_QP_subproblems[name]\n\n            for key in mip._mpisppy_model.x_indices:\n                qp.x[key].set_value(mip.nonant_vars[key].value)\n            for key in mip._mpisppy_model.y_indices:\n                qp.y[key].set_value(mip.leaf_vars[key].value)\n\n            # Set the non-anticipative reference variables if we're bundling\n            if (self.bundling):\n                arb_scenario = mip.scen_list[0]\n                naix = self.local_scenarios[arb_scenario]._mpisppy_data.nonant_indices\n                for (node_name, ix) in naix:\n                    # Check that non-anticipativity is satisfied\n                    # within the bundle (for debugging)\n                    vals = [mip.nonant_vars[scenario_name, node_name, ix].value\n                            for scenario_name in mip.scen_list]\n                    assert(max(vals) - min(vals) < 1e-7)\n                    qp.xr[node_name, ix].set_value(\n                        mip.nonant_vars[arb_scenario, node_name, ix].value)",
  "def _is_timed_out(self):\n        if (self.cylinder_rank == 0):\n            time_elapsed = time.time() - self.t0\n            status = 1 if (time_elapsed > self.FW_options['time_limit']) \\\n                       else 0\n        else:\n            status = None\n        status = self.comms['ROOT'].bcast(status, root=0)\n        return status != 0",
  "def _options_checks_fw(self):\n        ''' Name                Boland notation (Algorithm 2)\n            -------------------------------------------------\n            FW_iter_limit       t_max\n            FW_weight           alpha\n            FW_conv_thresh      tau\n        '''\n        # 1. Check for required options\n        reqd_options = ['FW_iter_limit', 'FW_weight', 'FW_conv_thresh',\n                        'solver_name']\n        losers = [opt for opt in reqd_options if opt not in self.FW_options]\n        if (len(losers) > 0):\n            msg = \"FW_options is missing the following key(s): \" + \\\n                  \", \".join(losers)\n            raise RuntimeError(msg)\n\n        # 2. Check that bundles, pre-specified points and t_max play nice. This\n        #    is only checked on rank 0, because that is where the initial\n        #    points are supposed to be specified.\n        use_bundles = ('bundles_per_rank' in self.options \n                        and self.options['bundles_per_rank'] > 0)\n        t_max = self.FW_options['FW_iter_limit']\n        specd_init_pts = 'point_creator' in self.FW_options.keys() and \\\n                         self.FW_options['point_creator'] is not None\n\n        if (use_bundles and specd_init_pts):\n            if (t_max == 1):\n                raise RuntimeError('Cannot use bundles and specify initial '\n                    'points with t_max=1 at the same time.')\n            else:\n                if (self.cylinder_rank == 0):\n                    print('WARNING: Cannot specify initial points and use '\n                        'bundles at the same time. Ignoring specified initial '\n                        'points')\n                # Remove specified initial points\n                self.FW_options.pop('point_creator', None)\n\n        if (t_max == 1 and not specd_init_pts):\n            raise RuntimeError('FW_iter_limit set to 1. To ensure '\n                'convergence, provide initial points, or increase '\n                'FW_iter_limit')\n\n        # 3a. Check that the user did not specify the linearization of binary\n        #    proximal terms (no binary variables allowed in FWPH QPs)\n        if ('linearize_binary_proximal_terms' in self.options\n            and self.options['linearize_binary_proximal_terms']):\n            print('Warning: linearize_binary_proximal_terms cannot be used '\n                  'with the FWPH algorithm. Ignoring...')\n            self.options['linearize_binary_proximal_terms'] = False\n\n        # 3b. Check that the user did not specify the linearization of all\n        #    proximal terms (FWPH QPs should be QPs)\n        if ('linearize_proximal_terms' in self.options\n            and self.options['linearize_proximal_terms']):\n            print('Warning: linearize_proximal_terms cannot be used '\n                  'with the FWPH algorithm. Ignoring...')\n            self.options['linearize_proximal_terms'] = False\n\n        # 4. Provide a time limit of inf if the user did not specify\n        if ('time_limit' not in self.FW_options.keys()):\n            self.FW_options['time_limit'] = np.inf",
  "def _output(self, itr, bound, best_bound, diff, secs):\n        if (self.cylinder_rank == 0 and self.vb):\n            print('{itr:3d} {bound:12.4f} {best_bound:12.4f} {diff:12.4e} {secs:11.1f}s'.format(\n                    itr=itr, bound=bound, best_bound=best_bound, \n                    diff=diff, secs=secs))\n        if (self.cylinder_rank == 0 and 'save_file' in self.FW_options.keys()):\n            fname = self.FW_options['save_file']\n            with open(fname, 'a') as f:\n                f.write('{itr:d},{bound:.16f},{best_bound:.16f},{diff:.16f},{secs:.16f}\\n'.format(\n                    itr=itr, bound=bound, best_bound=best_bound,\n                    diff=diff, secs=secs))",
  "def _output_header(self):\n        if (self.cylinder_rank == 0 and self.vb):\n            print('itr {bound:>12s} {bb:>12s} {cd:>12s} {tm:>12s}'.format(\n                    bound=\"bound\", bb=\"best bound\", cd=\"conv diff\", tm=\"time\"))\n        if (self.cylinder_rank == 0 and 'save_file' in self.FW_options.keys()):\n            fname = self.FW_options['save_file']\n            with open(fname, 'a') as f:\n                f.write('{itr:s},{bound:s},{bb:s},{diff:s},{secs:s}\\n'.format(\n                    itr=\"Iteration\", bound=\"Bound\", bb=\"Best bound\",\n                    diff=\"Error\", secs=\"Time(s)\"))",
  "def save_weights(self, fname):\n        ''' Save the computed weights to the specified file.\n\n            Notes:\n                Handles parallelism--only writes one copy of the file.\n\n                Rather \"fast-and-loose\", in that it doesn't enforce _when_ this\n                function can be called.\n        '''\n        weights = self._gather_weight_dict(strip_bundle_names=self.bundling) # None if rank != 0\n        if (self.cylinder_rank != 0):\n            return\n        with open(fname, 'w') as f:\n            for block in weights:\n                for (scenario_name, wts) in block.items():\n                    for (var_name, weight_val) in wts.items():\n                        row = '{sn},{vn},{wv:.16f}\\n'.format(\n                            sn=scenario_name, vn=var_name, wv=weight_val)\n                        f.write(row)",
  "def save_xbars(self, fname):\n        ''' Save the computed xbar to the specified file.\n\n            Notes:\n                Handles parallelism--only writes one copy of the file.\n\n                Rather \"fast-and-loose\", in that it doesn't enforce _when_ this\n                function can be called.\n        '''\n        if (self.cylinder_rank != 0):\n            return\n        xbars = self._get_xbars(strip_bundle_names=self.bundling) # None if rank != 0\n        with open(fname, 'w') as f:\n            for (var_name, xbs) in xbars.items():\n                row = '{vn},{vv:.16f}\\n'.format(vn=var_name, vv=xbs)\n                f.write(row)",
  "def _set_MIP_solver_options(self):\n        mip_opts = self.FW_options['mip_solver_options']\n        if (len(mip_opts) > 0):\n            for model in self.local_subproblems.values():\n                for (key, option) in mip_opts.items():\n                    model._solver_plugin.options[key] = option",
  "def _set_QP_objective(self):\n        ''' Attach dual weights, objective function and solver to each QP.\n        \n            QP dual weights are initialized to the MIP dual weights.\n        '''\n\n        for name, mip in self.local_subproblems.items():\n            QP = self.local_QP_subproblems[name]\n\n            obj, new = self._extract_objective(mip)\n\n            ## Finish setting up objective for QP\n            if self.bundling:\n                m_source = self.local_scenarios[mip.scen_list[0]]\n                x_source = QP.xr\n            else:\n                m_source = mip\n                x_source = QP.x\n\n            QP._mpisppy_model.W = pyo.Param(\n                m_source._mpisppy_data.nonant_indices.keys(), mutable=True, initialize=m_source._mpisppy_model.W\n            )\n            # rhos are attached to each scenario, not each bundle (should they be?)\n            ph_term = pyo.quicksum((\n                QP._mpisppy_model.W[nni] * x_source[nni] +\n                (m_source._mpisppy_model.rho[nni] / 2.) * (x_source[nni] - m_source._mpisppy_model.xbars[nni]) * (x_source[nni] - m_source._mpisppy_model.xbars[nni])\n                for nni in m_source._mpisppy_data.nonant_indices\n            ))\n\n            if obj.is_minimizing():\n                QP.obj = pyo.Objective(expr=new+ph_term, sense=pyo.minimize)\n            else:\n                QP.obj = pyo.Objective(expr=-new+ph_term, sense=pyo.minimize)\n\n            ''' Attach a solver with various options '''\n            solver = pyo.SolverFactory(self.FW_options['solver_name'])\n            if sputils.is_persistent(solver):\n                solver.set_instance(QP)\n            if 'qp_solver_options' in self.FW_options:\n                qp_opts = self.FW_options['qp_solver_options']\n                if qp_opts:\n                    for (key, option) in qp_opts.items():\n                        solver.options[key] = option\n\n            self.local_QP_subproblems[name]._QP_solver_plugin = solver",
  "def _swap_nonant_vars(self):\n        ''' Change the pointers in\n            scenario._mpisppy_node_list[i].nonant_vardata_list\n            to point to the QP variables, rather than the MIP variables.\n\n            Notes:\n                When computing xBar and updating the weights in the outer\n                iteration, the values of the x variables are pulled from\n                scenario._mpisppy_node_list[i].nonant_vardata_list. In the FWPH\n                algorithm, xBar should be computed using the QP values, not the\n                MIP values (like in normal PH).\n\n                Reruns SPBase._attach_nonant_indices so that the scenario \n                _nonant_indices dictionary has the correct variable pointers\n                \n                Updates nonant_vardata_list but NOT nonant_list.\n        '''\n        for (name, model) in self.local_subproblems.items():\n            scens = model.scen_list if self.bundling else [name]\n            for scenario_name in scens:\n                scenario = self.local_scenarios[scenario_name]\n                num_nonant_vars = scenario._mpisppy_data.nlens\n                node_list = scenario._mpisppy_node_list\n                for node in node_list:\n                    node.nonant_vardata_list = [\n                        self.local_QP_subproblems[name].xr[node.name,i]\n                        if self.bundling else\n                        self.local_QP_subproblems[name].x[node.name,i]\n                        for i in range(num_nonant_vars[node.name])]\n        self._attach_nonant_indices()",
  "def _swap_nonant_vars_back(self):\n        ''' Swap variables back, in case they're needed somewhere else.\n        '''\n        for (name, model) in self.local_subproblems.items():\n            if (self.bundling):\n                EF = self.local_subproblems[name]\n                for scenario_name in EF.scen_list:\n                    scenario = self.local_scenarios[scenario_name]\n                    num_nonant_vars = scenario._mpisppy_data.nlens\n                    for node in scenario._mpisppy_node_list:\n                        node.nonant_vardata_list = [\n                            EF.nonant_vars[scenario_name,node.name,ix]\n                            for ix in range(num_nonant_vars[node.name])]\n            else:\n                scenario = self.local_scenarios[name]\n                num_nonant_vars = scenario._mpisppy_data.nlens\n                for node in scenario._mpisppy_node_list:\n                    node.nonant_vardata_list = [\n                        scenario.nonant_vars[node.name,ix]\n                        for ix in range(num_nonant_vars[node.name])]\n        self._attach_nonant_indices()",
  "def sum_one_rule(model, scenario_name):\n            return pyo.quicksum(model.conv[scenario_name,ix] \\\n                for ix in range(len(points[scenario_name]))) == 1",
  "def conv_rule(model, scenario_name, var_name):\n            return model.x[var_name] \\\n                + model.slack_plus[scenario_name, var_name] \\\n                - model.slack_minus[scenario_name, var_name] \\\n                == pyo.quicksum(model.conv[scenario_name, ix] *\n                    points[scenario_name][ix][var_name] \n                    for ix in range(len(points[scenario_name])))",
  "def nonant_rule(m, scenario_name, node_name, ix):\n                    return m.x[scenario_name, node_name, ix] == \\\n                            m.xr[node_name, ix]",
  "def x_rule(m, node_name, ix):\n                    return -m.xr[node_name, ix] + m.a[1] * \\\n                            model.ref_vars[node_name, ix].value == 0",
  "def y_rule(m, scenario_name, node_name, ix):\n                    return -m.y[scenario_name, node_name, ix] + m.a[1]\\\n                        * model.leaf_vars[scenario_name,node_name,ix].value == 0",
  "def x_rule(m, node_name, ix):\n                        nm = model.nonant_vars[node_name, ix].name\n                        return -m.x[node_name, ix] + \\\n                            pyo.quicksum(m.a[i+1] * pts[i][nm] \n                                for i in range(len(pts))) == 0",
  "def y_rule(m, node_name, ix):\n                        nm = model.leaf_vars[node_name, ix].name\n                        return -m.y[node_name,ix] + \\\n                            pyo.quicksum(m.a[i+1] * pts[i][nm] \n                                for i in range(len(pts))) == 0",
  "def x_rule(m, node_name, ix):\n                        return -m.x[node_name, ix] + m.a[1] * \\\n                                model.nonant_vars[node_name, ix].value == 0",
  "def y_rule(m, node_name, ix):\n                        return -m.y[node_name,ix] + m.a[1] * \\\n                                model.leaf_vars['LEAF', ix].value == 0"
]