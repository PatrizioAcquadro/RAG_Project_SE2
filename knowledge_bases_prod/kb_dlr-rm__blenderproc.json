[
  "def cli():\n    \"\"\"\n    Command line function, parses the arguments given to BlenderProc.\n    \"\"\"\n    options = {\n        \"vis\": {\n            'hdf5': \"Visualizes the content of one or multiple .hdf5 files.\",\n            'coco': \"Visualizes the annotations written in coco format.\"\n        },\n        \"extract\": {\n            'hdf5': \"Extracts images out of an hdf5 file into separate image files.\"\n        },\n        \"download\": {\n            'blenderkit': \"Downloads materials and models from blenderkit.\",\n            'cc_textures': \"Downloads textures from cc0textures.com.\",\n            'haven': \"Downloads HDRIs, Textures and Models from polyhaven.com.\",\n            'ikea': \"Downloads the IKEA dataset.\",\n            'pix3d': \"Downloads the Pix3D dataset.\",\n            'scenenet': \"Downloads the scenenet dataset.\",\n            'matterport3d': \"Downloads the Matterport3D dataset.\"\n        },\n        \"pip\": {\n            'install': \"Installs package in the Blender python environment\",\n            'uninstall': \"Uninstalls package in the Blender python environment\"\n        },\n        \"quickstart\": {\n        }\n    }\n\n    parser = argparse.ArgumentParser(description=\"BlenderProc: A procedural Blender pipeline for \"\n                                                 \"photorealistic image generation.\",\n                                     formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument('-v', '--version', action='store_true', help='Version of BlenderProc')\n    subparsers = parser.add_subparsers(dest='mode', help=\"Select a BlenderProc command to run:\")\n\n    # Setup different modes\n    parser_run = subparsers.add_parser('run', help=\"Runs the BlenderProc pipeline in normal mode.\")\n    parser_quickstart = subparsers.add_parser('quickstart',\n                                              help=\"Runs a quickstart script blenderproc/scripts/quickstart.py\")\n    parser_debug = subparsers.add_parser('debug', help=\"Runs the BlenderProc pipeline in debug mode. This will open \"\n                                                       \"the Blender UI, so the 3D scene created by the pipeline \"\n                                                       \"can be visually inspected.\")\n    parser_vis = subparsers.add_parser('vis', help=f\"Visualize the content of BlenderProc output files. \\n\"\n                                                   f\"Options: {', '.join(options['vis'])}\",\n                                       formatter_class=argparse.RawTextHelpFormatter)\n    parser_download = subparsers.add_parser('download', help=\"Download datasets, materials or 3D models to run \"\n                                                             \"examples or your own pipeline. \\n\"\n                                                             \"Options: {', '.join(options['download'])}\",\n                                            formatter_class=argparse.RawTextHelpFormatter)\n    parser_extract = subparsers.add_parser('extract', help=\"Extract the raw images from generated containers such \"\n                                                           \"as hdf5. \\nOptions: {', '.join(options['extract'])}\",\n                                           formatter_class=argparse.RawTextHelpFormatter)\n    parser_pip = subparsers.add_parser('pip', help=\"Can be used to install/uninstall pip packages in the Blender \"\n                                                   \"python environment. \\nOptions: {', '.join(options['pip'])}\",\n                                       formatter_class=argparse.RawTextHelpFormatter)\n\n    sub_parser_vis = parser_vis.add_subparsers(dest='vis_mode')\n    for cmd, help_str in options['vis'].items():\n        sub_parser_vis.add_parser(cmd, help=help_str, add_help=False)\n\n    sub_parser_download = parser_download.add_subparsers(dest='download_mode')\n    for cmd, help_str in options['download'].items():\n        sub_parser_download.add_parser(cmd, help=help_str, add_help=False)\n\n    sub_parser_extract = parser_extract.add_subparsers(dest='extract_mode')\n    for cmd, help_str in options['extract'].items():\n        sub_parser_extract.add_parser(cmd, help=help_str, add_help=False)\n\n    parser_pip.add_argument('pip_mode', choices=options['pip'],\n                            help='\\n'.join(f\"{key}: {value}\" for key, value in options[\"pip\"].items()))\n    parser_pip.add_argument('pip_packages', metavar='pip_packages', nargs='*',\n                            help='A list of pip packages that should be installed/uninstalled. '\n                                 'Packages versions can be determined via the `==` notation.')\n    parser_pip.add_argument('--not-use-custom-package-path', dest='not_use_custom_package_path', action='store_true',\n                            help='If set, the pip packages will not be installed into the separate custom package '\n                                 'folder, but into blenders python site-packages folder. This should only be used, '\n                                 'if a specific pip package cannot be installed into a custom package path.')\n\n    # Setup all common arguments of run and debug mode\n    for subparser in [parser_run, parser_debug, parser_quickstart]:\n        if subparser != parser_quickstart:\n            subparser.add_argument('file', help='The path to a python file which uses BlenderProc via the API.')\n\n        subparser.add_argument('--reinstall-blender', dest='reinstall_blender', action='store_true',\n                               help='If given, the blender installation is deleted and reinstalled. Is ignored, if '\n                                    'a \"custom-blender-path\" is given.')\n        subparser.add_argument('--temp-dir', dest='temp_dir', default=None,\n                               help=\"The path to a directory where all temporary output files should be stored. \"\n                                    \"If it doesn't exist, it is created automatically. Type: string. Default: \"\n                                    \"\\\"/dev/shm\\\" or \\\"/tmp/\\\" depending on which is available.\")\n        subparser.add_argument('--keep-temp-dir', dest='keep_temp_dir', action='store_true',\n                               help=\"If set, the temporary directory is not removed in the end.\")\n        subparser.add_argument('--force-pip-update', dest='force_pip_update', action='store_true',\n                               help=\"If set, the cache of installed pip packages will be ignored and rebuild \"\n                                    \"based on pip freeze.\")\n\n    # Setup common arguments of run, debug and pip mode\n    for subparser in [parser_run, parser_debug, parser_pip, parser_quickstart]:\n        subparser.add_argument('--blender-install-path', dest='blender_install_path', default=None,\n                               help=\"Set path where blender should be installed. If None is given, \"\n                                    \"/home_local/<env:USER>/blender/ is used per default.\")\n        subparser.add_argument('--custom-blender-path', dest='custom_blender_path', default=None,\n                               help=\"Set, if you want to use a custom blender installation to run BlenderProc. \"\n                                    \"If None is given, blender is installed into the configured blender_install_path. \")\n\n    args, unknown_args = parser.parse_known_args()\n\n    if args.version:\n        # pylint: disable=import-outside-toplevel\n        from blenderproc import __version__\n        # pylint: enable=import-outside-toplevel\n        print(__version__)\n    elif args.mode in [\"run\", \"debug\", \"quickstart\"]:\n\n        # Install blender, if not already done\n        determine_result = InstallUtility.determine_blender_install_path(args)\n        custom_blender_path, blender_install_path = determine_result\n        blender_run_path, major_version = InstallUtility.make_sure_blender_is_installed(custom_blender_path,\n                                                                                        blender_install_path,\n                                                                                        args.reinstall_blender)\n\n        # Setup script path that should be executed\n        if args.mode == \"quickstart\":\n            path_src_run = os.path.join(repo_root_directory, \"blenderproc\", \"scripts\", \"quickstart.py\")\n            args.file = path_src_run\n            print(f\"'blenderproc quickstart' is an alias for 'blenderproc run {path_src_run}'\")\n        else:\n            path_src_run = args.file\n            SetupUtility.check_if_setup_utilities_are_at_the_top(path_src_run)\n\n        # Setup temp dir\n        temp_dir = SetupUtility.determine_temp_dir(args.temp_dir)\n\n        # Setup env vars\n        used_environment = dict(os.environ, PYTHONPATH=repo_root_directory, PYTHONNOUSERSITE=\"1\")\n        # this is done to enable the import of blenderproc inside the blender internal python environment\n        used_environment[\"INSIDE_OF_THE_INTERNAL_BLENDER_PYTHON_ENVIRONMENT\"] = \"1\"\n\n        # If pip update is forced, remove pip package cache\n        if args.force_pip_update:\n            SetupUtility.clean_installed_packages_cache(os.path.dirname(blender_run_path), major_version)\n\n        # Run either in debug or in normal mode\n        if args.mode == \"debug\":\n            # pylint: disable=consider-using-with\n            p = subprocess.Popen([blender_run_path, \"--python-use-system-env\", \"--python-exit-code\", \"0\", \"--python\",\n                                  os.path.join(repo_root_directory, \"blenderproc/debug_startup.py\"), \"--\",\n                                  path_src_run, temp_dir] + unknown_args,\n                                 env=used_environment)\n            # pylint: enable=consider-using-with\n        else:\n            # pylint: disable=consider-using-with\n            p = subprocess.Popen([blender_run_path, \"--background\", \"--python-use-system-env\", \"--python-exit-code\",\n                                  \"2\", \"--python\", path_src_run, \"--\", args.file, temp_dir] + unknown_args,\n                                 env=used_environment)\n            # pylint: enable=consider-using-with\n\n        def clean_temp_dir():\n            # If temp dir should not be kept and temp dir still exists => remove it\n            if not args.keep_temp_dir and os.path.exists(temp_dir):\n                print(\"Cleaning temporary directory\")\n                shutil.rmtree(temp_dir)\n\n        # Listen for SIGTERM signal, so we can properly clean up and terminate the child process\n        def handle_sigterm(_signum, _frame):\n            clean_temp_dir()\n            p.terminate()\n\n        signal.signal(signal.SIGTERM, handle_sigterm)\n\n        try:\n            p.wait()\n        except KeyboardInterrupt:\n            try:\n                p.terminate()\n            except OSError:\n                pass\n            p.wait()\n\n        # Clean up\n        clean_temp_dir()\n\n        sys.exit(p.returncode)\n    # Import the required entry point\n    elif args.mode in [\"vis\", \"extract\", \"download\"]:\n        # pylint: disable=import-outside-toplevel\n        if args.mode == \"vis\" and args.vis_mode == \"hdf5\":\n            from blenderproc.scripts.visHdf5Files import cli as current_cli\n        elif args.mode == \"vis\" and args.vis_mode == \"coco\":\n            from blenderproc.scripts.vis_coco_annotation import cli as current_cli\n        elif args.mode == \"extract\" and args.extract_mode == \"hdf5\":\n            from blenderproc.scripts.saveAsImg import cli as current_cli\n        elif args.mode == \"download\" and args.download_mode == \"blenderkit\":\n            from blenderproc.scripts.download_blenderkit import cli as current_cli\n        elif args.mode == \"download\" and args.download_mode == \"cc_textures\":\n            from blenderproc.scripts.download_cc_textures import cli as current_cli\n        elif args.mode == \"download\" and args.download_mode == \"haven\":\n            from blenderproc.scripts.download_haven import cli as current_cli\n        elif args.mode == \"download\" and args.download_mode == \"ikea\":\n            from blenderproc.scripts.download_ikea import cli as current_cli\n        elif args.mode == \"download\" and args.download_mode == \"pix3d\":\n            from blenderproc.scripts.download_pix3d import cli as current_cli\n        elif args.mode == \"download\" and args.download_mode == \"scenenet\":\n            from blenderproc.scripts.download_scenenet import cli as current_cli\n        elif args.mode == \"download\" and args.download_mode == \"matterport3d\":\n            from blenderproc.scripts.download_matterport3d import cli as current_cli\n        else:\n            raise RuntimeError(f\"There is no linked script for the command: {args.mode}. \"\n                               f\"Options are: {options[args.mode]}\")\n        # pylint: enable=import-outside-toplevel\n\n        # Remove the first argument (it's the script name)\n        sys.argv = sys.argv[:1] + unknown_args\n        # Call the script\n        current_cli()\n    elif args.mode == \"pip\":\n        # Install blender, if not already done\n        custom_blender_path, blender_install_path = InstallUtility.determine_blender_install_path(args)\n        blender_bin, major_version = InstallUtility.make_sure_blender_is_installed(custom_blender_path,\n                                                                                   blender_install_path)\n        blender_path = os.path.dirname(blender_bin)\n\n        if args.pip_mode == \"install\":\n            SetupUtility.setup_pip(user_required_packages=args.pip_packages, blender_path=blender_path,\n                                   major_version=major_version,\n                                   use_custom_package_path=not args.not_use_custom_package_path,\n                                   install_default_packages=False)\n        elif args.pip_mode == \"uninstall\":\n            SetupUtility.uninstall_pip_packages(args.pip_packages, blender_path=blender_path,\n                                                major_version=major_version)\n    else:\n        # If no command is given, print help\n        print(parser.format_help())\n        sys.exit(0)",
  "def clean_temp_dir():\n            # If temp dir should not be kept and temp dir still exists => remove it\n            if not args.keep_temp_dir and os.path.exists(temp_dir):\n                print(\"Cleaning temporary directory\")\n                shutil.rmtree(temp_dir)",
  "def handle_sigterm(_signum, _frame):\n            clean_temp_dir()\n            p.terminate()",
  "class RunBlenderProcOperator(bpy.types.Operator):\n    \"\"\" This operator adds the Run BlenderProc button to the GUI \"\"\"\n\n    bl_idname = \"wm.run_blenderproc\"\n    bl_label = \"Run BlenderProc\"\n    bl_description = \"This operator runs the loaded BlenderProc script and also makes sure to unload all \" \\\n                     \"modules before starting.\"\n    bl_options = {\"REGISTER\"}\n\n    def execute(self, _):\n        \"\"\"\n        Execute the button -> running BlenderProc\n        \"\"\"\n        # Delete all loaded models inside src/, as they are cached inside blender\n        for module in list(sys.modules.keys()):\n            if module.startswith(\"blenderproc\") and module not in [\"blenderproc.python.utility.SetupUtility\"]:\n                del sys.modules[module]\n\n        # Make sure the parent of the blenderproc folder is in sys.path\n        import_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n        if import_path not in sys.path:\n            sys.path.append(import_path)\n\n        # Run the script\n        try:\n            bpy.ops.text.run_script()\n        except RuntimeError:\n            # Skip irrelevant error messages (The relevant stacktrace+error has already been printed at this point)\n            pass\n        return {\"FINISHED\"}",
  "def draw(self, context):\n    \"\"\" Draws the newly defined Run BlenderProc button \"\"\"\n    layout = self.layout\n\n    st = context.space_data\n    context_text = st.text\n    is_syntax_highlight_supported = st.is_syntax_highlight_supported()\n    layout.template_header()\n\n    TEXT_MT_editor_menus.draw_collapsible(context, layout)\n\n    if context_text and context_text.is_modified:\n        row = layout.row(align=True)\n        row.alert = True\n        row.operator(\"text.resolve_conflict\", text=\"\", icon='HELP')\n\n    layout.separator_spacer()\n\n    row = layout.row(align=True)\n    row.template_ID(st, \"text\", new=\"text.new\",\n                    unlink=\"text.unlink\", open=\"text.open\")\n\n    if context_text:\n        is_osl = context_text.name.endswith((\".osl\", \".osl\"))\n        if is_osl:\n            row.operator(\"node.shader_script_update\",\n                         text=\"\", icon='FILE_REFRESH')\n        else:\n            row = layout.row()\n            row.active = is_syntax_highlight_supported\n            # The following line has changed compared to the orignal code,\n            # it starts our operator instead of text.run_script\n            row.operator(\"wm.run_blenderproc\", text=\"Run BlenderProc\")\n\n    layout.separator_spacer()\n\n    row = layout.row(align=True)\n    row.prop(st, \"show_line_numbers\", text=\"\")\n    row.prop(st, \"show_word_wrap\", text=\"\")\n\n    syntax = row.row(align=True)\n    syntax.active = is_syntax_highlight_supported\n    syntax.prop(st, \"show_syntax_highlight\", text=\"\")",
  "def execute(self, _):\n        \"\"\"\n        Execute the button -> running BlenderProc\n        \"\"\"\n        # Delete all loaded models inside src/, as they are cached inside blender\n        for module in list(sys.modules.keys()):\n            if module.startswith(\"blenderproc\") and module not in [\"blenderproc.python.utility.SetupUtility\"]:\n                del sys.modules[module]\n\n        # Make sure the parent of the blenderproc folder is in sys.path\n        import_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n        if import_path not in sys.path:\n            sys.path.append(import_path)\n\n        # Run the script\n        try:\n            bpy.ops.text.run_script()\n        except RuntimeError:\n            # Skip irrelevant error messages (The relevant stacktrace+error has already been printed at this point)\n            pass\n        return {\"FINISHED\"}",
  "def light_suncg_scene(lightbulb_emission_strength: float = 15, lampshade_emission_strength: float = 7,\n                      ceiling_emission_strength: float = 1.5):\n    \"\"\" Makes the lamps, windows and ceilings object emit light.\n\n    :param lightbulb_emission_strength: The emission strength that should be used for light bulbs. Default: 15\n    :param lampshade_emission_strength: The emission strength that should be used for lamp shades. Default: 7\n    :param ceiling_emission_strength: The emission strength that should be used for the ceiling. Default: 1.5\n    \"\"\"\n    # Read in the materials for lights and windows\n    lights, windows = Utility.read_suncg_lights_windows_materials()\n\n    collection_of_mats: Dict[str, Dict[str, Material]] = {\"lamp\": {}, \"window\": {}, \"ceiling\": {}}\n\n    # Make some objects emit lights\n    for obj in get_all_mesh_objects():\n        if obj.has_cp(\"modelId\"):\n            obj_id = obj.get_cp(\"modelId\")\n\n            # In the case of the lamp\n            if obj_id in lights:\n                _SuncgLighting.make_lamp_emissive(obj, lights[obj_id], collection_of_mats, lightbulb_emission_strength,\n                                                  lampshade_emission_strength)\n\n            # Make the windows emit light\n            if obj_id in windows:\n                _SuncgLighting.make_window_emissive(obj, collection_of_mats)\n\n            # Also make ceilings slightly emit light\n            if obj.get_name().startswith(\"Ceiling#\"):\n                _SuncgLighting.make_ceiling_emissive(obj, collection_of_mats, ceiling_emission_strength)",
  "class _SuncgLighting:\n    \"\"\" Adds light properties to the SUNCG scenes. \"\"\"\n\n    @staticmethod\n    def make_lamp_emissive(obj: MeshObject, light: Tuple[List[str], List[str]],\n                           collection_of_mats: Dict[str, Dict[str, Material]],\n                           lightbulb_emission_strength: float = 15,\n                           lampshade_emission_strength: float = 7):\n        \"\"\" Adds an emission shader to the object materials which are specified in the light dictionary\n\n        :param obj: The blender object.\n        :param light: A tuple of two lists. The first list specifies all materials which should act as a lightbulb,\n                      the second one lists all materials corresponding to lampshades.\n        :param collection_of_mats: A dictionary that contains materials for lamps, windows and ceilings.\n        :param lightbulb_emission_strength: The emission strength that should be used for light bulbs. Default: 15\n        :param lampshade_emission_strength: The emission strength that should be used for lamp shades. Default: 7\n        \"\"\"\n        for i, m in enumerate(obj.get_materials()):\n            if m is None:\n                continue\n            mat_name = m.get_name()\n            if \".\" in mat_name:\n                mat_name = mat_name[:mat_name.find(\".\")]\n            if mat_name in light[0] or mat_name in light[1]:\n                old_mat_name = m.get_name()\n                if old_mat_name in collection_of_mats[\"lamp\"]:\n                    # this material was used as a ceiling before use that one\n                    obj.set_material(i, collection_of_mats[\"lamp\"][old_mat_name])\n                    continue\n                # copy the material if more than one users is using it\n                if m.get_users() > 1:\n                    m = m.duplicate()\n                    obj.set_material(i, m)\n                # rename the material\n                m.set_name(m.get_name() + \"_emission\")\n\n                emission = m.get_nodes_with_type(\"Emission\")\n                if not emission:\n                    if mat_name in light[0]:\n                        # If the material corresponds to light bulb\n                        emission_strength = lightbulb_emission_strength\n                    else:\n                        # If the material corresponds to a lampshade\n                        emission_strength = lampshade_emission_strength\n\n                    m.make_emissive(emission_strength, emission_color=m.blender_obj.diffuse_color)\n                    collection_of_mats[\"lamp\"][old_mat_name] = m\n\n    @staticmethod\n    def make_window_emissive(obj: MeshObject, collection_of_mats: Dict[str, Dict[str, Material]]):\n        \"\"\" Makes the given window object emissive.\n\n        For each material with alpha < 1.\n        Uses a light path node to make it emit light, but at the same time look like a principle material.\n        Otherwise windows would be completely white.\n\n        :param obj: A window object.\n        :param collection_of_mats: A dictionary that contains materials for lamps, windows and ceilings.\n        \"\"\"\n        for i, m in enumerate(obj.get_materials()):\n            if m is None:\n                continue\n\n            # All parameters imported from the .mtl file are stored inside the principled bsdf node\n            principled_node = m.get_the_one_node_with_type(\"BsdfPrincipled\")\n            alpha = principled_node.inputs['Alpha'].default_value\n\n            if alpha < 1:\n                mat_name = m.get_name()\n                if mat_name in collection_of_mats[\"window\"]:\n                    # this material was used as a ceiling before use that one\n                    obj.set_material(i, collection_of_mats[\"window\"][mat_name])\n                    continue\n                # copy the material if more than one user is using it\n                if m.get_users() > 1:\n                    m = m.duplicate()\n                    obj.set_material(i, m)\n                # rename the material\n                m.set_name(m.get_name() + \"_emission\")\n                if not m.get_nodes_with_type('Emission'):\n                    transparent_node = m.new_node('ShaderNodeBsdfDiffuse')\n                    transparent_node.inputs['Color'].default_value[:3] = (0.285, 0.5, 0.48)\n\n                    m.make_emissive(emission_strength=10, emission_color=[1, 1, 1, 1],\n                                    non_emissive_color_socket=transparent_node.outputs['BSDF'])\n\n                collection_of_mats[\"window\"][mat_name] = m\n\n    @staticmethod\n    def make_ceiling_emissive(obj: MeshObject, collection_of_mats: Dict[str, Dict[str, Material]],\n                              ceiling_emission_strength: float = 1.5):\n        \"\"\" Makes the given ceiling object emissive, s.t. there is always a little bit ambient light.\n\n        :param obj: The ceiling object.\n        :param collection_of_mats: A dictionary that contains materials for lamps, windows and ceilings.\n        :param ceiling_emission_strength: The emission strength that should be used for the ceiling. Default: 1.5\n        \"\"\"\n        for i, material in enumerate(obj.get_materials()):\n            if material is None:\n                continue\n            mat_name = material.get_name()\n            if mat_name in collection_of_mats[\"ceiling\"]:\n                # this material was used as a ceiling before use that one\n                obj.set_material(i, collection_of_mats[\"ceiling\"][mat_name])\n                continue\n            # copy the material if more than one user is using it\n            if material.get_users() > 1:\n                material = material.duplicate()\n                obj.set_material(i, material)\n            # rename the material\n            material.set_name(material.get_name() + \"_emission\")\n\n            if not material.get_nodes_with_type(\"Emission\") and material.get_nodes_with_type(\"BsdfPrincipled\"):\n                material.make_emissive(emission_strength=ceiling_emission_strength, emission_color=[1, 1, 1, 1])\n                collection_of_mats[\"ceiling\"][mat_name] = material",
  "def make_lamp_emissive(obj: MeshObject, light: Tuple[List[str], List[str]],\n                           collection_of_mats: Dict[str, Dict[str, Material]],\n                           lightbulb_emission_strength: float = 15,\n                           lampshade_emission_strength: float = 7):\n        \"\"\" Adds an emission shader to the object materials which are specified in the light dictionary\n\n        :param obj: The blender object.\n        :param light: A tuple of two lists. The first list specifies all materials which should act as a lightbulb,\n                      the second one lists all materials corresponding to lampshades.\n        :param collection_of_mats: A dictionary that contains materials for lamps, windows and ceilings.\n        :param lightbulb_emission_strength: The emission strength that should be used for light bulbs. Default: 15\n        :param lampshade_emission_strength: The emission strength that should be used for lamp shades. Default: 7\n        \"\"\"\n        for i, m in enumerate(obj.get_materials()):\n            if m is None:\n                continue\n            mat_name = m.get_name()\n            if \".\" in mat_name:\n                mat_name = mat_name[:mat_name.find(\".\")]\n            if mat_name in light[0] or mat_name in light[1]:\n                old_mat_name = m.get_name()\n                if old_mat_name in collection_of_mats[\"lamp\"]:\n                    # this material was used as a ceiling before use that one\n                    obj.set_material(i, collection_of_mats[\"lamp\"][old_mat_name])\n                    continue\n                # copy the material if more than one users is using it\n                if m.get_users() > 1:\n                    m = m.duplicate()\n                    obj.set_material(i, m)\n                # rename the material\n                m.set_name(m.get_name() + \"_emission\")\n\n                emission = m.get_nodes_with_type(\"Emission\")\n                if not emission:\n                    if mat_name in light[0]:\n                        # If the material corresponds to light bulb\n                        emission_strength = lightbulb_emission_strength\n                    else:\n                        # If the material corresponds to a lampshade\n                        emission_strength = lampshade_emission_strength\n\n                    m.make_emissive(emission_strength, emission_color=m.blender_obj.diffuse_color)\n                    collection_of_mats[\"lamp\"][old_mat_name] = m",
  "def make_window_emissive(obj: MeshObject, collection_of_mats: Dict[str, Dict[str, Material]]):\n        \"\"\" Makes the given window object emissive.\n\n        For each material with alpha < 1.\n        Uses a light path node to make it emit light, but at the same time look like a principle material.\n        Otherwise windows would be completely white.\n\n        :param obj: A window object.\n        :param collection_of_mats: A dictionary that contains materials for lamps, windows and ceilings.\n        \"\"\"\n        for i, m in enumerate(obj.get_materials()):\n            if m is None:\n                continue\n\n            # All parameters imported from the .mtl file are stored inside the principled bsdf node\n            principled_node = m.get_the_one_node_with_type(\"BsdfPrincipled\")\n            alpha = principled_node.inputs['Alpha'].default_value\n\n            if alpha < 1:\n                mat_name = m.get_name()\n                if mat_name in collection_of_mats[\"window\"]:\n                    # this material was used as a ceiling before use that one\n                    obj.set_material(i, collection_of_mats[\"window\"][mat_name])\n                    continue\n                # copy the material if more than one user is using it\n                if m.get_users() > 1:\n                    m = m.duplicate()\n                    obj.set_material(i, m)\n                # rename the material\n                m.set_name(m.get_name() + \"_emission\")\n                if not m.get_nodes_with_type('Emission'):\n                    transparent_node = m.new_node('ShaderNodeBsdfDiffuse')\n                    transparent_node.inputs['Color'].default_value[:3] = (0.285, 0.5, 0.48)\n\n                    m.make_emissive(emission_strength=10, emission_color=[1, 1, 1, 1],\n                                    non_emissive_color_socket=transparent_node.outputs['BSDF'])\n\n                collection_of_mats[\"window\"][mat_name] = m",
  "def make_ceiling_emissive(obj: MeshObject, collection_of_mats: Dict[str, Dict[str, Material]],\n                              ceiling_emission_strength: float = 1.5):\n        \"\"\" Makes the given ceiling object emissive, s.t. there is always a little bit ambient light.\n\n        :param obj: The ceiling object.\n        :param collection_of_mats: A dictionary that contains materials for lamps, windows and ceilings.\n        :param ceiling_emission_strength: The emission strength that should be used for the ceiling. Default: 1.5\n        \"\"\"\n        for i, material in enumerate(obj.get_materials()):\n            if material is None:\n                continue\n            mat_name = material.get_name()\n            if mat_name in collection_of_mats[\"ceiling\"]:\n                # this material was used as a ceiling before use that one\n                obj.set_material(i, collection_of_mats[\"ceiling\"][mat_name])\n                continue\n            # copy the material if more than one user is using it\n            if material.get_users() > 1:\n                material = material.duplicate()\n                obj.set_material(i, material)\n            # rename the material\n            material.set_name(material.get_name() + \"_emission\")\n\n            if not material.get_nodes_with_type(\"Emission\") and material.get_nodes_with_type(\"BsdfPrincipled\"):\n                material.make_emissive(emission_strength=ceiling_emission_strength, emission_color=[1, 1, 1, 1])\n                collection_of_mats[\"ceiling\"][mat_name] = material",
  "def _default_light_pose_sampling(frustum_vertices: np.ndarray) -> np.ndarray:\n    \"\"\" Samples a new spotlight pose based on the camera frustum vertices.\n\n    :param frustum_vertices: The eight 3D coordinates of the camera frustum\n    :return: The newly sampled 3D spotlight pose\n    \"\"\"\n    middle_frustum_point = np.mean(frustum_vertices, axis=0)\n    cube_diag = np.linalg.norm(np.max(frustum_vertices, axis=0) - np.min(frustum_vertices, axis=0)) * 0.5\n    sampled_light_pose = part_sphere(middle_frustum_point,\n                                     radius=np.random.uniform(cube_diag * 0.01, cube_diag * 0.75), mode=\"SURFACE\")\n    return sampled_light_pose",
  "def _default_look_at_pose_sampling(frustum_vertices: np.ndarray, _sampled_light_pose: np.ndarray) -> np.ndarray:\n    \"\"\" This function samples the default look at location and is used inside\n    the `add_intersecting_spot_lights_to_camera_poses`.\n\n    :param frustum_vertices: The eight 3D coordinates of the camera frustum\n    :param _sampled_light_pose: The currently sampled light pose\n    :return: A new 3D look at pose\n    \"\"\"\n    middle_frustum_point = np.mean(frustum_vertices, axis=0)\n    return middle_frustum_point + np.random.normal(0, 1.0, 3)",
  "def add_intersecting_spot_lights_to_camera_poses(clip_start: float, clip_end: float,\n                                                 perform_look_at_intersection_check: bool = True,\n                                                 perform_look_at_pose_visibility_check: bool = True,\n                                                 light_pose_sampling: Optional[Callable[[np.ndarray],\n                                                                                        np.ndarray]] = None,\n                                                 look_at_pose_sampling: Optional[Callable[[np.ndarray, np.ndarray],\n                                                                                          np.ndarray]] = None,\n                                                 max_tries_per_cam_pose: int = 10000) -> Light:\n    \"\"\" This functions adds spotlights which intersect with the camera pose. This is useful to get a greater variety\n    in lighting situations then the general full illumination from all sides.\n\n    The spotlights location is defined by the `light_pose_sampling` parameter, it gets the eight coordinates of the\n    camera frustum vertices. This camera frustum starts at `clip_start` and ends at `clip_end`. It should return a\n    single 3D point. This point is then checked to not be in the camera frustum, if it is inside a new point will be\n    sampled.\n\n    After the defining a suitable light position, a look at pose is sampled via the `look_pose_sampling` function.\n    It uses the same eight coordinates of the camera frustum and the current sampled light position to return a look at\n    pose.\n\n    If the `perform_look_at_intersection_check` value is set an intersection check between the light position and\n    the look at location is done, which ensures that no object is between these two points.\n    Similarly, for the `perform_look_at_pose_visibility_check`, a newly sampled light pose does not have an intersecting\n    object between this sampled pose and the camera location.\n\n    :param clip_start: The distance between the camera pose and the near clipping plane, used for the sampling of a\n                       light and look at location\n    :param clip_end: The distance between the camera pose and the far clipping plane, used for the sampling of a\n                     light and look at location\n    :param perform_look_at_intersection_check: If this is True an intersection check between the light pose and the look\n                                               at pose is done, if an object is inbetween both poses are discarded.\n    :param perform_look_at_pose_visibility_check: If this is True, an intersection check between the look at pose and\n                                                  the camera location is done, to ensure that the light is visible and\n                                                  not hidden.\n    :param light_pose_sampling: This function samples a new 3D light pose based on the eight 3D coordinates of the\n                                camera frustum. If this is None, the `_default_light_pose_sampling` is used.\n    :param look_at_pose_sampling: This function samples a new 3D look at pose based on the eight 3D coordinates of the\n                                  camera frustum and the currently sampled light pose. If this is None,\n                                  the `_default_look_at_pose_sampling` is used.\n    :param max_tries_per_cam_pose: The amount of maximum tries per camera pose for finding a new light pose with\n                                   look at pose.\n    :return: The newly generated light\n    \"\"\"\n\n    if bpy.context.scene.frame_start == bpy.context.scene.frame_end:\n        raise RuntimeError(\"A camera poses has to be set first!\")\n\n    if light_pose_sampling is None:\n        light_pose_sampling = _default_light_pose_sampling\n\n    if look_at_pose_sampling is None:\n        look_at_pose_sampling = _default_look_at_pose_sampling\n\n    new_light = Light(light_type=\"SPOT\")\n    new_light.set_energy(10000)\n    # create a bvh tree to quickly check if an object is in the line of sight\n    bvh_tree = None\n    if perform_look_at_pose_visibility_check or perform_look_at_intersection_check:\n        bvh_tree = create_bvh_tree_multi_objects(get_all_mesh_objects())\n\n    # iterate over each camera pose\n    for frame_id in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n        # set the current camera frame for all functions\n        with KeyFrame(frame_id):\n            # get the vertices of the camera frustum\n            vertices = get_camera_frustum(clip_start=clip_start, clip_end=clip_end)\n            found_pose = False\n            for _ in range(max_tries_per_cam_pose):\n                # sample a new light position\n                sampled_pose = light_pose_sampling(vertices)\n                # sample a look at pose\n                look_at_point = look_at_pose_sampling(vertices, sampled_pose)\n                # check that the sampled pose is not inside the camera frustum and the look at point is\n                if not is_point_inside_camera_frustum(sampled_pose) and is_point_inside_camera_frustum(look_at_point):\n                    # check if an object is between the look at pose and the camera pose\n                    if perform_look_at_pose_visibility_check:\n                        cam_location = get_camera_pose()[:3, 3]\n                        look_dir = cam_location - look_at_point\n                        _, _, _, dist = bvh_tree.ray_cast(look_at_point, look_dir, np.linalg.norm(look_dir))\n                        if dist is not None:\n                            # if an object is between the light pose and the camera sample a new light pose\n                            continue\n                    # check if an object is between the sample point and the look at point\n                    if perform_look_at_intersection_check:\n                        look_dir = look_at_point - sampled_pose\n                        _, _, _, dist = bvh_tree.ray_cast(sampled_pose, look_dir, np.linalg.norm(look_dir))\n                        if dist is not None:\n                            # skip this light position as it collides with something\n                            continue\n\n                    # calculate the rotation matrix\n                    forward_vec = look_at_point - sampled_pose\n                    rotation_matrix = rotation_from_forward_vec(forward_vec)\n\n                    # save the pose and rotation\n                    new_light.set_location(sampled_pose)\n                    new_light.set_rotation_mat(rotation_matrix)\n                    found_pose = True\n                    break\n            if not found_pose:\n                raise RuntimeError(\"No pose found, increase the start and end clip or increase the amount of tries.\")\n    return new_light",
  "def light_surface(objects: List[MeshObject], emission_strength: float = 10.0, emission_color: list = None):\n    \"\"\" Add emission shader to the materials of the given objects.\n\n    :param objects: A list of mesh objects whose materials should emit light.\n    :param emission_strength: The strength of the emitted light.\n    :param emission_color: The color of the light to emit. Default: Color of the surface.\n    \"\"\"\n    empty_material = None\n\n    # for each object add a material\n    for obj in objects:\n        if not obj.has_materials():\n            # If this is the first object without any material\n            if empty_material is None:\n                # this object has no material so far -> create one\n                empty_material = obj.new_material(\"TextureLess\")\n            else:\n                # Just reuse the material that was already created for other objects with no material\n                obj.add_material(empty_material)\n                # Material has already been made emissive, so go to the next object\n                continue\n\n        for i, material in enumerate(obj.get_materials()):\n            if material is None:\n                continue\n            # if there is more than one user make a copy and then use the new one\n            if material.get_users() > 1:\n                material = material.duplicate()\n                obj.set_material(i, material)\n            # rename the material\n            material.set_name(material.get_name() + \"_emission\")\n            # add a custom property to later identify these materials\n            material.set_cp(\"is_lamp\", True)\n\n            material.make_emissive(emission_strength=emission_strength, emission_color=emission_color)",
  "def collect_all() -> List[Optional[Material]]:\n    \"\"\" Returns all existing materials.\n\n    :return: A list of all materials.\n    \"\"\"\n    return convert_to_materials(bpy.data.materials)",
  "def create(name: str) -> Material:\n    \"\"\" Creates a new empty material.\n\n    :param name: The name of the new material.\n    :return: The new material.\n    \"\"\"\n    new_mat = bpy.data.materials.new(name=name)\n    new_mat.use_nodes = True\n    return Material(new_mat)",
  "def convert_to_materials(blender_materials: List[Optional[bpy.types.Material]]) -> List[Optional[Material]]:\n    \"\"\" Converts the given list of blender materials to bproc.Material(s)\n\n    :param blender_materials: List of materials.\n    :return: The list of materials.\n    \"\"\"\n    return [(None if obj is None else Material(obj)) for obj in blender_materials]",
  "def find_cc_material_by_name(material_name: str, custom_properties: Dict[str, Any]) -> bpy.types.Material:\n    \"\"\"\n    Finds from all loaded materials the cc material, which has the given material_name and the given\n    custom_properties.\n\n    :param material_name: Name of the searched material\n    :param custom_properties: Custom properties, which have been assigned before\n    :return: bpy.types.Material: Return the searched material, if not found returns None\n    \"\"\"\n    # find used cc materials with this name\n    cond = {\"cp_is_cc_texture\": True, \"cp_asset_name\": material_name}\n    for key, value in custom_properties.items():\n        cond[key] = value\n    new_mats = MaterialGetter.perform_and_condition_check(cond, [])\n    if len(new_mats) == 1:\n        new_mat = new_mats[0]\n        return new_mat\n    if len(new_mats) > 1:\n        raise RuntimeError(\"There was more than one material found!\")\n    # the material was not even loaded\n    return None",
  "def is_material_used(material: bpy.types.Material):\n    \"\"\"\n    Checks if the given material is used on any object.\n\n    :param material: Material, which should be checked\n    :return: True if the material is used\n    \"\"\"\n    # check amount of usage of this material\n    return material.users != 0",
  "def create_new_cc_material(material_name: str, add_custom_properties: dict) -> bpy.types.Material:\n    \"\"\"\n    Creates a new material, which gets the given custom properties and the material name.\n\n    :param material_name: The name of the material\n    :param add_custom_properties: The custom properties, which should be added to the material\n    :return: bpy.types.Material: Return the newly created material\n    \"\"\"\n    # create a new material with the name of the asset\n    new_mat = bpy.data.materials.new(material_name)\n    new_mat[\"is_cc_texture\"] = True\n    new_mat[\"asset_name\"] = material_name\n    new_mat.use_nodes = True\n    for key, value in add_custom_properties.items():\n        if key.startswith(\"cp_\"):\n            cp_key = key[len(\"cp_\"):]\n        else:\n            raise ValueError(\"All cp have to start with cp_\")\n        new_mat[cp_key] = value\n    return new_mat",
  "def create_image_node(nodes: bpy.types.Nodes, image: Union[str, bpy.types.Image],\n                      non_color_mode: bool = False, x_location: float = 0.0,\n                      y_location: float = 0.0):\n    \"\"\"\n    Creates a texture image node inside a material.\n\n    :param nodes: Nodes from the current material\n    :param image: Either the path to the image which should be loaded or the bpy.types.Image\n    :param non_color_mode: If this True, the color mode of the image will be \"Non-Color\"\n    :param x_location: X Location in the node tree\n    :param y_location: Y Location in the node tree\n    :return: bpy.type.Node: Return the newly constructed image node\n    \"\"\"\n    image_node = nodes.new('ShaderNodeTexImage')\n    if isinstance(image, bpy.types.Image):\n        image_node.image = image\n    else:\n        image_node.image = bpy.data.images.load(image, check_existing=True)\n    if non_color_mode:\n        image_node.image.colorspace_settings.name = 'Non-Color'\n    image_node.location.x = x_location\n    image_node.location.y = y_location\n    return image_node",
  "def add_base_color(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, base_image_path,\n                   principled_bsdf: bpy.types.Node):\n    \"\"\"\n    Adds base color to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param base_image_path: Path to the base image\n    :param principled_bsdf: Principled BSDF node of the current material\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    if os.path.exists(base_image_path):\n        base_color = create_image_node(nodes, base_image_path, False,\n                                       _x_texture_node,\n                                       _y_texture_node)\n        links.new(base_color.outputs[\"Color\"], principled_bsdf.inputs[\"Base Color\"])\n        return base_color\n    return None",
  "def add_ambient_occlusion(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, ambient_occlusion_image_path,\n                          principled_bsdf: bpy.types.Node, base_color: bpy.types.Node):\n    \"\"\"\n    Adds ambient occlusion to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param ambient_occlusion_image_path: Path to the ambient occlusion image\n    :param principled_bsdf: Principled BSDF node of the current material\n    :param base_color: Base color node of the current material\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    if os.path.exists(ambient_occlusion_image_path):\n        ao_color = create_image_node(nodes, ambient_occlusion_image_path, True,\n                                     _x_texture_node,\n                                     _y_texture_node * 2)\n        math_node = nodes.new(type='ShaderNodeMixRGB')\n        math_node.blend_type = \"MULTIPLY\"\n        math_node.location.x = _x_texture_node * 0.5\n        math_node.location.y = _y_texture_node * 1.5\n        math_node.inputs[\"Fac\"].default_value = 0.333\n\n        links.new(base_color.outputs[\"Color\"], math_node.inputs[1])\n        links.new(ao_color.outputs[\"Color\"], math_node.inputs[2])\n        links.new(math_node.outputs[\"Color\"], principled_bsdf.inputs[\"Base Color\"])\n\n        return ao_color\n    return None",
  "def add_metal(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, metalness_image_path: str,\n              principled_bsdf: bpy.types.Node):\n    \"\"\"\n    Adds metal to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param metalness_image_path: Path to the metal image\n    :param principled_bsdf: Principled BSDF node of the current material\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    if os.path.exists(metalness_image_path):\n        metallic = create_image_node(nodes, metalness_image_path, True,\n                                     _x_texture_node, 0)\n        links.new(metallic.outputs[\"Color\"], principled_bsdf.inputs[\"Metallic\"])\n        return metallic\n    return None",
  "def add_roughness(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, roughness_image_path: str,\n                  principled_bsdf: bpy.types.Node):\n    \"\"\"\n    Adds roughness to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param roughness_image_path: Path to the metal image\n    :param principled_bsdf: Principled BSDF node of the current material\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    if os.path.exists(roughness_image_path):\n        roughness_texture = create_image_node(nodes, roughness_image_path, True,\n                                              _x_texture_node,\n                                              _y_texture_node * -1)\n        links.new(roughness_texture.outputs[\"Color\"], principled_bsdf.inputs[\"Roughness\"])\n        return roughness_texture\n    return None",
  "def add_specular(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, specular_image_path: str,\n                 principled_bsdf: bpy.types.Node):\n    \"\"\"\n    Adds specular to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param specular_image_path: Path to the metal image\n    :param principled_bsdf: Principled BSDF node of the current material\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    if os.path.exists(specular_image_path):\n        specular_texture = create_image_node(nodes, specular_image_path, True,\n                                             _x_texture_node, 0)\n        links.new(specular_texture.outputs[\"Color\"], principled_bsdf.inputs[\"Specular\"])\n        return specular_texture\n    return None",
  "def add_alpha(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, alpha_image_path: str,\n              principled_bsdf: bpy.types.Node):\n    \"\"\"\n    Adds alpha to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param alpha_image_path: Path to the metal image\n    :param principled_bsdf: Principled BSDF node of the current material\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    if os.path.exists(alpha_image_path):\n        alpha_texture = create_image_node(nodes, alpha_image_path, True,\n                                          _x_texture_node,\n                                          _y_texture_node * -2)\n        links.new(alpha_texture.outputs[\"Color\"], principled_bsdf.inputs[\"Alpha\"])\n        return alpha_texture\n    return None",
  "def add_normal(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, normal_image_path: str,\n               principled_bsdf: bpy.types.Node, invert_y_channel: bool):\n    \"\"\"\n    Adds normal to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param normal_image_path: Path to the metal image\n    :param principled_bsdf: Principled BSDF node of the current material\n    :param invert_y_channel: If this is True the Y Color Channel is inverted.\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    normal_y_value = _y_texture_node * -3\n    if os.path.exists(normal_image_path):\n        normal_texture = create_image_node(nodes, normal_image_path, True,\n                                           _x_texture_node,\n                                           normal_y_value)\n        if invert_y_channel:\n\n            separate_rgba = nodes.new('ShaderNodeSeparateRGB')\n            separate_rgba.location.x = 4.0 / 5.0 * _x_texture_node\n            separate_rgba.location.y = normal_y_value\n            links.new(normal_texture.outputs[\"Color\"], separate_rgba.inputs[\"Image\"])\n\n            invert_node = nodes.new(\"ShaderNodeInvert\")\n            invert_node.inputs[\"Fac\"].default_value = 1.0\n            invert_node.location.x = 3.0 / 5.0 * _x_texture_node\n            invert_node.location.y = normal_y_value\n\n            links.new(separate_rgba.outputs[\"G\"], invert_node.inputs[\"Color\"])\n\n            combine_rgba = nodes.new('ShaderNodeCombineRGB')\n            combine_rgba.location.x = 2.0 / 5.0 * _x_texture_node\n            combine_rgba.location.y = normal_y_value\n            links.new(separate_rgba.outputs[\"R\"], combine_rgba.inputs[\"R\"])\n            links.new(invert_node.outputs[\"Color\"], combine_rgba.inputs[\"G\"])\n            links.new(separate_rgba.outputs[\"B\"], combine_rgba.inputs[\"B\"])\n\n            current_output = combine_rgba.outputs[\"Image\"]\n        else:\n            current_output = normal_texture.outputs[\"Color\"]\n\n        normal_map = nodes.new(\"ShaderNodeNormalMap\")\n        normal_map.inputs[\"Strength\"].default_value = 1.0\n        normal_map.location.x = 1.0 / 5.0 * _x_texture_node\n        normal_map.location.y = normal_y_value\n        links.new(current_output, normal_map.inputs[\"Color\"])\n        links.new(normal_map.outputs[\"Normal\"], principled_bsdf.inputs[\"Normal\"])\n        return normal_texture\n    return None",
  "def add_bump(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, bump_image_path: str,\n             principled_bsdf: bpy.types.Node):\n    \"\"\"\n    Adds bump to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param bump_image_path: Path to the metal image\n    :param principled_bsdf: Principled BSDF node of the current material\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    bump_y_value = _y_texture_node * -3\n    if os.path.exists(bump_image_path):\n        bump_texture = create_image_node(nodes, bump_image_path, True,\n                                         _x_texture_node,\n                                         bump_y_value)\n        bump_map = nodes.new(\"ShaderNodeBumpMap\")\n        bump_map.inputs[\"Strength\"].default_value = 1.0\n        bump_map.location.x = 1.0 / 5.0 * _x_texture_node\n        bump_map.location.y = bump_y_value\n        links.new(bump_texture.outputs[\"Color\"], bump_map.inputs[\"Heights\"])\n        links.new(bump_map.outputs[\"Normal\"], principled_bsdf.inputs[\"Normal\"])\n        return bump_texture\n    return None",
  "def add_displacement(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, displacement_image_path: str,\n                     output_node: bpy.types.Node):\n    \"\"\"\n    Adds bump to the principled bsdf node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param displacement_image_path: Path to the metal image\n    :param output_node: Output node of the current material\n    :return: bpy.types.Node: The newly constructed texture node\n    \"\"\"\n    if os.path.exists(displacement_image_path):\n        displacement_texture = create_image_node(nodes, displacement_image_path, True,\n                                                 _x_texture_node,\n                                                 _y_texture_node * -4)\n        displacement_node = nodes.new(\"ShaderNodeDisplacement\")\n        displacement_node.inputs[\"Midlevel\"].default_value = 0.5\n        displacement_node.inputs[\"Scale\"].default_value = 0.15\n        displacement_node.location.x = _x_texture_node * 0.5\n        displacement_node.location.y = _y_texture_node * -4\n        links.new(displacement_texture.outputs[\"Color\"], displacement_node.inputs[\"Height\"])\n        links.new(displacement_node.outputs[\"Displacement\"], output_node.inputs[\"Displacement\"])\n        return displacement_texture\n    return None",
  "def connect_uv_maps(nodes: bpy.types.Nodes, links: bpy.types.NodeLinks, collection_of_texture_nodes: list):\n    \"\"\"\n    Connect all given texture nodes to a newly constructed UV node.\n\n    :param nodes: Nodes from the current material\n    :param links: Links from the current material\n    :param collection_of_texture_nodes: List of :class: `bpy.type.Node` of type :class: `ShaderNodeTexImage`\n    \"\"\"\n    if len(collection_of_texture_nodes) > 0:\n        texture_coords = nodes.new(\"ShaderNodeTexCoord\")\n        texture_coords.location.x = _x_texture_node * 1.4\n        mapping_node = nodes.new(\"ShaderNodeMapping\")\n        mapping_node.location.x = _x_texture_node * 1.2\n\n        links.new(texture_coords.outputs[\"UV\"], mapping_node.inputs[\"Vector\"])\n        for texture_node in collection_of_texture_nodes:\n            if texture_node is not None:\n                links.new(mapping_node.outputs[\"Vector\"], texture_node.inputs[\"Vector\"])",
  "def add_alpha_channel_to_textures(blurry_edges):\n    \"\"\"\n    Adds transparency to all textures, which contain an .png image as an image input\n\n    :param blurry_edges: If True, the edges of the alpha channel might be blurry,\n                            this causes errors if the alpha channel should only be 0 or 1\n\n    Be careful, when you replace the original texture with something else (Segmentation, ...),\n    the necessary texture node gets lost. By copying it into a new material as done in the SegMapRenderer, you\n    can keep the transparency even for those nodes.\n\n    \"\"\"\n    obj_with_mats = [obj for obj in bpy.context.scene.objects if hasattr(obj.data, 'materials')]\n    visited_materials = set()\n    # walk over all objects, which have materials\n    for obj in obj_with_mats:\n        for slot in obj.material_slots:\n            material = slot.material\n            if material is None:\n                # this can happen if a material slot was created but no material was assigned\n                continue\n            if material.name in visited_materials:\n                # skip a material if it has been used before\n                continue\n            visited_materials.add(material.name)\n            texture_node = None\n            # check each node of the material\n            for node in material.node_tree.nodes:\n                # if it is a texture image node\n                if 'TexImage' in node.bl_idname:\n                    if '.png' in node.image.name:  # contains an alpha channel\n                        texture_node = node\n            # this material contains an alpha png texture\n            if texture_node is not None:\n                nodes = material.node_tree.nodes\n                links = material.node_tree.links\n                node_connected_to_the_output, material_output = \\\n                    Utility.get_node_connected_to_the_output_and_unlink_it(material)\n\n                if node_connected_to_the_output is not None:\n                    mix_node = nodes.new(type='ShaderNodeMixShader')\n\n                    # avoid blurry edges on the edges important for Normal, SegMapRenderer and others\n                    if blurry_edges:\n                        # add the alpha channel of the image to the mix shader node as a factor\n                        links.new(texture_node.outputs['Alpha'], mix_node.inputs['Fac'])\n                    else:\n                        # Map all alpha values to 0 or 1 by applying the step function: 1 if x > 0.5 else 0\n                        step_function_node = nodes.new(\"ShaderNodeMath\")\n                        step_function_node.operation = \"GREATER_THAN\"\n                        links.new(texture_node.outputs['Alpha'], step_function_node.inputs['Value'])\n                        links.new(step_function_node.outputs['Value'], mix_node.inputs['Fac'])\n\n                    links.new(node_connected_to_the_output.outputs[0], mix_node.inputs[2])\n                    transparent_node = nodes.new(type='ShaderNodeBsdfTransparent')\n                    links.new(transparent_node.outputs['BSDF'], mix_node.inputs[1])\n                    # connect to material output\n                    links.new(mix_node.outputs['Shader'], material_output.inputs['Surface'])\n                else:\n                    raise RuntimeError(f\"Could not find shader node, which is connected to the material \"\n                                       f\"output for: {slot.name}\")",
  "def add_alpha_texture_node(used_material, new_material):\n    \"\"\"\n    Adds to a predefined new_material a texture node from an existing material (used_material)\n    This is necessary to connect it later on in the add_alpha_channel_to_textures\n\n    :param used_material: existing material, which might contain a texture node with a .png texture\n    :param new_material: a new material, which will get a copy of this texture node\n    :return: the modified new_material, if no texture node was found, the original new_material\n    \"\"\"\n    if used_material is None:\n        # this can happen if a material slot was created but no material was assigned\n        return used_material\n    # find out if there is an .png file used here\n    texture_node = None\n    for node in used_material.node_tree.nodes:\n        # if it is a texture image node\n        if 'TexImage' in node.bl_idname:\n            if '.png' in node.image.name:  # contains an alpha channel\n                texture_node = node\n    # this material contains an alpha png texture\n    if texture_node is not None:\n        new_mat_alpha = new_material.copy()  # copy the material\n        nodes = new_mat_alpha.node_tree.nodes\n        # copy the texture node into the new material to make sure it is used\n        new_tex_node = nodes.new(type='ShaderNodeTexImage')\n        new_tex_node.image = texture_node.image\n        # use the new material\n        return new_mat_alpha\n    return new_material",
  "def change_to_texture_less_render(use_alpha_channel):\n    \"\"\" Changes the materials, which do not contain a emission shader to a white slightly glossy texture\n\n    :param use_alpha_channel: If true, the alpha channel stored in .png textures is used.\n    \"\"\"\n    new_mat = bpy.data.materials.new(name=\"TextureLess\")\n    new_mat.use_nodes = True\n    nodes = new_mat.node_tree.nodes\n\n    principled_bsdf = Utility.get_the_one_node_with_type(nodes, \"BsdfPrincipled\")\n\n    # setting the color values for the shader\n    principled_bsdf.inputs['Specular'].default_value = 0.65  # specular\n    principled_bsdf.inputs['Roughness'].default_value = 0.2  # roughness\n\n    for used_object in [obj for obj in bpy.context.scene.objects if hasattr(obj.data, 'materials')]:\n        # replace all materials with the new texture less material\n        for slot in used_object.material_slots:\n            emission_shader = False\n            # check if the material contains an emission shader:\n            for node in slot.material.node_tree.nodes:\n                # check if one of the shader nodes is a Emission Shader\n                if 'Emission' in node.bl_idname:\n                    emission_shader = True\n                    break\n            # only replace materials, which do not contain any emission shader\n            if not emission_shader:\n                if use_alpha_channel:\n                    slot.material = add_alpha_texture_node(slot.material, new_mat)\n                else:\n                    slot.material = new_mat",
  "def create_procedural_texture(pattern_name: Optional[str] = None) -> bpy.types.Texture:\n    \"\"\" Creates a new procedural texture based on a specified pattern.\n\n    :param pattern_name: The name of the pattern. Available: [\"CLOUDS\", \"DISTORTED_NOISE\", \"MAGIC\", \"MARBLE\",\n                         \"MUSGRAVE\", \"NOISE\", \"STUCCI\", \"VORONOI\", \"WOOD\"]. If None is given, a random pattern is used.\n    :return: The created texture\n    \"\"\"\n    possible_patterns = [\"CLOUDS\", \"DISTORTED_NOISE\", \"MAGIC\", \"MARBLE\", \"MUSGRAVE\", \"NOISE\", \"STUCCI\",\n                         \"VORONOI\", \"WOOD\"]\n\n    # If no pattern has been given, use a random one, otherwise check whether the given pattern is valid.\n    if pattern_name is None:\n        pattern_name = random.choice(possible_patterns)\n    else:\n        pattern_name = pattern_name.upper()\n        if pattern_name not in possible_patterns:\n            raise RuntimeError(f\"There is no such pattern: {pattern_name}. Allowed patterns are: {possible_patterns}\")\n\n    return bpy.data.textures.new(f\"ct_{pattern_name}\", pattern_name)",
  "def create_material_from_texture(texture: Union[Path, str, bpy.types.Image], material_name: str) -> Material:\n    \"\"\"\n    Creates a material based on a given texture, the texture can either be a path to a texture file on disc or a\n    already loaded bpy.types.Image.\n\n    :param texture: either a path to an image, or a loaded bpy.types.Image texture\n    :param material_name: name of the newly created material\n    :return: the newly created material, which uses the texture as Base Color\n    \"\"\"\n    texture_path: Optional[Path] = None\n    if isinstance(texture, (Path, str)):\n        texture_path = Path(texture)\n    elif not isinstance(texture, bpy.types.Image):\n        raise TypeError(f\"The given type of texture must be either [str, Path, bpy.types.Image] \"\n                        f\"and not {type(texture)}.\")\n\n    # if a texture path was set, load the image\n    if texture_path:\n        if texture_path.exists():\n            texture = bpy.data.images.load(str(texture_path), check_existing=True)\n        else:\n            raise FileNotFoundError(f\"The given texture path could not be found: \\\"{texture_path}\\\"\")\n\n    if isinstance(texture, bpy.types.Image):\n        new_mat = bpy.data.materials.new(material_name)\n        new_mat.use_nodes = True\n        bp_mat = Material(new_mat)\n        bp_mat.set_principled_shader_value(\"Base Color\", texture)\n        return bp_mat\n    raise TypeError(\"The texture variable should be a bpy.types.Image at this point!\")",
  "def add_dust(material: Material, strength: float, texture_nodes: List[bpy.types.Texture] = None,\n             texture_scale: float = 0.1):\n    \"\"\" Adds a dust film to the material, where the strength determines how much dust is used.\n\n    This will be added right before the output of the material.\n\n    :param material: Used material\n    :param strength: This determines the strength of the dust, 0 means no dust 1.0 means full dust. Values above 1.0 are\n                        possible, but create a thick film out of dust, which hides the material completely.\n    :param texture_nodes: If a specific dust texture should be used, this can be specified.  If this is empty a random\n                          noise texture is generated.\n    :param texture_scale: This scale is used to scale down the used noise texture (even for the case where a random\n                          noise texture is used).\n    \"\"\"\n\n    group_node = material.new_node(\"ShaderNodeGroup\")\n    group_node.width = 250\n    group = bpy.data.node_groups.new(name=\"Dust Material\", type=\"ShaderNodeTree\")\n    group_node.node_tree = group\n    nodes, links = group.nodes, group.links\n\n    # define start locations and differences, to make the debugging easier\n    x_pos, x_diff = -(250 * 4), 250\n    y_pos, y_diff = (x_diff * 1), x_diff\n\n    # Extract the normal for the current material location\n    geometry_node = nodes.new('ShaderNodeNewGeometry')\n    geometry_node.location.x = x_pos + x_diff * 0\n    geometry_node.location.y = y_pos\n    # this node clips the values, to avoid negative values in the usage below\n    clip_mix_node = nodes.new(\"ShaderNodeMixRGB\")\n    clip_mix_node.inputs[\"Fac\"].default_value = 1.0\n    clip_mix_node.use_clamp = True\n    clip_mix_node.location.x = x_pos + x_diff * 1\n    clip_mix_node.location.y = y_pos\n    links.new(geometry_node.outputs[\"Normal\"], clip_mix_node.inputs[\"Color2\"])\n\n    # use only the z component\n    separate_z_normal = nodes.new(\"ShaderNodeSeparateRGB\")\n    separate_z_normal.location.x = x_pos + x_diff * 2\n    separate_z_normal.location.y = y_pos\n    links.new(clip_mix_node.outputs[\"Color\"], separate_z_normal.inputs[\"Image\"])\n\n    # this layer weight adds a small fresnel around the object, which makes it more realistic\n    layer_weight = nodes.new(\"ShaderNodeLayerWeight\")\n    layer_weight.location.x = x_pos + x_diff * 2\n    layer_weight.location.y = y_pos - y_diff * 1\n    layer_weight.inputs[\"Blend\"].default_value = 0.5\n    # combine it with the z component\n    mix_with_layer_weight = nodes.new(\"ShaderNodeMixRGB\")\n    mix_with_layer_weight.location.x = x_pos + x_diff * 3\n    mix_with_layer_weight.location.y = y_pos\n    mix_with_layer_weight.inputs[\"Fac\"].default_value = 0.2\n    links.new(separate_z_normal.outputs[\"B\"], mix_with_layer_weight.inputs[\"Color1\"])\n    links.new(layer_weight.outputs[\"Facing\"], mix_with_layer_weight.inputs[\"Color2\"])\n    # add a gamma node, to scale the colors correctly\n    gamma_node = nodes.new(\"ShaderNodeGamma\")\n    gamma_node.location.x = x_pos + x_diff * 4\n    gamma_node.location.y = y_pos\n    gamma_node.inputs[\"Gamma\"].default_value = 2.2\n    links.new(mix_with_layer_weight.outputs[\"Color\"], gamma_node.inputs[\"Color\"])\n\n    # use an overlay node to combine it with the texture result\n    overlay = nodes.new(\"ShaderNodeMixRGB\")\n    overlay.location.x = x_pos + x_diff * 5\n    overlay.location.y = y_pos\n    overlay.blend_type = \"OVERLAY\"\n    overlay.inputs[\"Fac\"].default_value = 1.0\n    links.new(gamma_node.outputs[\"Color\"], overlay.inputs[\"Color1\"])\n\n    # add a multiply node to scale down or up the strength of the dust\n    multiply_node = nodes.new(\"ShaderNodeMath\")\n    multiply_node.location.x = x_pos + x_diff * 6\n    multiply_node.location.y = y_pos\n    multiply_node.inputs[1].default_value = strength\n    multiply_node.operation = \"MULTIPLY\"\n    links.new(overlay.outputs[\"Color\"], multiply_node.inputs[0])\n\n    # add texture coords to make the scaling of the dust texture possible\n    texture_coords = nodes.new(\"ShaderNodeTexCoord\")\n    texture_coords.location.x = x_pos + x_diff * 0\n    texture_coords.location.y = y_pos - y_diff * 2\n    mapping_node = nodes.new(\"ShaderNodeMapping\")\n    mapping_node.location.x = x_pos + x_diff * 1\n    mapping_node.location.y = y_pos - y_diff * 2\n    mapping_node.vector_type = \"TEXTURE\"\n    scale_value = texture_scale\n    mapping_node.inputs[\"Scale\"].default_value = [scale_value] * 3\n    links.new(texture_coords.outputs[\"UV\"], mapping_node.inputs[\"Vector\"])\n    # check if a texture should be used\n    if texture_nodes is not None and texture_nodes:\n        texture_node = nodes.new(\"ShaderNodeTexImage\")\n        texture_node.location.x = x_pos + x_diff * 2\n        texture_node.location.y = y_pos - y_diff * 2\n        texture_node.image = random.choice(texture_nodes).image\n        links.new(mapping_node.outputs[\"Vector\"], texture_node.inputs[\"Vector\"])\n        links.new(texture_node.outputs[\"Color\"], overlay.inputs[\"Color2\"])\n    else:\n        if not texture_nodes:\n            warnings.warn(\"No texture was found, check the config. Random generated texture is used.\")\n        # if no texture is used, we great a random noise pattern, which is used instead\n        noise_node = nodes.new(\"ShaderNodeTexNoise\")\n        noise_node.location.x = x_pos + x_diff * 2\n        noise_node.location.y = y_pos - y_diff * 2\n        # this determines the pattern of the dust flakes, a high scale makes them small enough to look like dust\n        noise_node.inputs[\"Scale\"].default_value = 250.0\n        noise_node.inputs[\"Detail\"].default_value = 0.0\n        noise_node.inputs[\"Roughness\"].default_value = 0.0\n        noise_node.inputs[\"Distortion\"].default_value = 1.9\n        links.new(mapping_node.outputs[\"Vector\"], noise_node.inputs[\"Vector\"])\n        # this noise_node produces RGB colors, we only need one value in this instance red\n        separate_r_channel = nodes.new(\"ShaderNodeSeparateRGB\")\n        separate_r_channel.location.x = x_pos + x_diff * 3\n        separate_r_channel.location.y = y_pos - y_diff * 2\n        links.new(noise_node.outputs[\"Color\"], separate_r_channel.inputs[\"Image\"])\n        # as the produced noise image has a nice fading to it, we use a color ramp to create dust flakes\n        color_ramp = nodes.new(\"ShaderNodeValToRGB\")\n        color_ramp.location.x = x_pos + x_diff * 4\n        color_ramp.location.y = y_pos - y_diff * 2\n        color_ramp.color_ramp.elements[0].position = 0.4\n        color_ramp.color_ramp.elements[0].color = [1, 1, 1, 1]\n        color_ramp.color_ramp.elements[1].position = 0.46\n        color_ramp.color_ramp.elements[1].color = [0, 0, 0, 1]\n        links.new(separate_r_channel.outputs[\"R\"], color_ramp.inputs[\"Fac\"])\n        links.new(color_ramp.outputs[\"Color\"], overlay.inputs[\"Color2\"])\n\n    # combine the previous color with the new dust mode\n    mix_shader = nodes.new(\"ShaderNodeMixShader\")\n    mix_shader.location = (x_pos + x_diff * 8, y_pos)\n    links.new(multiply_node.outputs[\"Value\"], mix_shader.inputs[\"Fac\"])\n\n    # add a bsdf node for the dust, this will be used to actually give the dust a color\n    dust_color = nodes.new(\"ShaderNodeBsdfPrincipled\")\n    dust_color.location = (x_pos + x_diff * 6, y_pos - y_diff)\n    # the used dust color is a grey with a tint in orange\n    dust_color.inputs[\"Base Color\"].default_value = [0.8, 0.773, 0.7, 1.0]\n    dust_color.inputs[\"Roughness\"].default_value = 1.0\n    dust_color.inputs[\"Specular\"].default_value = 0.0\n    links.new(dust_color.outputs[\"BSDF\"], mix_shader.inputs[2])\n\n    # create the input and output nodes inside of the group\n    group_output = nodes.new(\"NodeGroupOutput\")\n    group_output.location = (x_pos + x_diff * 9, y_pos)\n    group_input = nodes.new(\"NodeGroupInput\")\n    group_input.location = (x_pos + x_diff * 7, y_pos - y_diff * 0.5)\n\n    # create sockets for the outside of the group match them to the mix shader\n    group.outputs.new(mix_shader.outputs[0].bl_idname, mix_shader.outputs[0].name)\n    group.inputs.new(mix_shader.inputs[1].bl_idname, mix_shader.inputs[1].name)\n    group.inputs.new(multiply_node.inputs[1].bl_idname, \"Dust strength\")\n    group.inputs.new(mapping_node.inputs[\"Scale\"].bl_idname, \"Texture scale\")\n\n    # link the input and output to the mix shader\n    links.new(group_input.outputs[0], mix_shader.inputs[1])\n    links.new(mix_shader.outputs[0], group_output.inputs[0])\n    links.new(group_input.outputs[\"Dust strength\"], multiply_node.inputs[1])\n    links.new(group_input.outputs[\"Texture scale\"], mapping_node.inputs[\"Scale\"])\n\n    # remove the connection between the output and the last node and put the mix shader in between\n    node_connected_to_the_output, material_output = material.get_node_connected_to_the_output_and_unlink_it()\n\n    # place the group node above the material output\n    group_node.location = (material_output.location.x - x_diff, material_output.location.y + y_diff)\n\n    # connect the dust group\n    material.link(node_connected_to_the_output.outputs[0], group_node.inputs[0])\n    material.link(group_node.outputs[0], material_output.inputs[\"Surface\"])\n\n    # set the default values\n    group_node.inputs[\"Dust strength\"].default_value = strength\n    group_node.inputs[\"Texture scale\"].default_value = [texture_scale] * 3",
  "class Entity(Struct):\n    \"\"\"\n    The entity class of all objects which can be placed inside the scene. They have a 6D pose consisting of location\n    and rotation.\n    \"\"\"\n\n    def update_blender_ref(self, name: str):\n        \"\"\" Updates the contained blender reference using the given name of the instance.\n\n        :param name: The name of the instance which will be used to update its blender reference.\n        \"\"\"\n        self.blender_obj = bpy.data.objects[name]\n\n    def set_location(self, location: Union[list, Vector, np.ndarray], frame: Optional[int] = None):\n        \"\"\" Sets the location of the entity in 3D world coordinates.\n\n        :param location: The location to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.location = location\n        Utility.insert_keyframe(self.blender_obj, \"location\", frame)\n\n    def set_rotation_euler(self, rotation_euler: Union[list, Euler, np.ndarray], frame: Optional[int] = None):\n        \"\"\" Sets the rotation of the entity in euler angles.\n\n        :param rotation_euler: The euler angles to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.rotation_euler = rotation_euler\n        Utility.insert_keyframe(self.blender_obj, \"rotation_euler\", frame)\n\n    def set_rotation_mat(self, rotation_mat: Union[Matrix, np.ndarray], frame: Optional[int] = None):\n        \"\"\" Sets the rotation of the entity using a rotation matrix.\n\n        :param rotation_mat: The 3x3 local to world rotation matrix.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.set_rotation_euler(Matrix(rotation_mat).to_euler(), frame)\n\n    def set_scale(self, scale: Union[list, np.ndarray, Vector], frame: Optional[int] = None):\n        \"\"\" Sets the scale of the entity along all three axes.\n\n        :param scale: The scale to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.scale = scale\n        Utility.insert_keyframe(self.blender_obj, \"scale\", frame)\n\n    def get_location(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Returns the location of the entity in 3D world coordinates.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The location at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return np.array(self.blender_obj.location)\n\n    def get_rotation(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Returns the rotation of the entity in euler angles.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The rotation at the specified frame.\n        \"\"\"\n        warnings.warn(\"This function will be deprecated. Use get_rotation_euler() instead.\")\n        return self.get_rotation_euler(frame)\n\n    def get_rotation_euler(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Returns the rotation of the entity in euler angles.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The rotation at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return np.array(self.blender_obj.rotation_euler)\n\n    def get_rotation_mat(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Gets the rotation matrix of the entity.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The 3x3 local2world rotation matrix.\n        \"\"\"\n        return np.array(Euler(self.get_rotation_euler(frame)).to_matrix())\n\n    def get_scale(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Returns the scale of the entity along all three axes.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The scale at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return np.array(self.blender_obj.scale)\n\n    def apply_T(self, transform: Union[np.ndarray, Matrix]):\n        \"\"\" Apply the given transformation to the pose of the entity.\n\n        :param transform: A 4x4 matrix representing the transformation.\n        \"\"\"\n        self.blender_obj.matrix_world = Matrix(self.get_local2world_mat()) @ Matrix(transform)\n\n    def set_local2world_mat(self, matrix_world: Union[np.ndarray, Matrix]):\n        \"\"\" Sets the pose of the object in the form of a local2world matrix.\n\n        :param matrix_world: A 4x4 matrix.\n        \"\"\"\n        # To make sure matrices are always interpreted row-wise, we first convert them to a mathutils matrix.\n        self.blender_obj.matrix_world = Matrix(matrix_world)\n\n    def get_local2world_mat(self) -> np.ndarray:\n        \"\"\" Returns the pose of the object in the form of a local2world matrix.\n\n        :return: The 4x4 local2world matrix.\n        \"\"\"\n        obj = self.blender_obj\n        # Start with local2parent matrix (if obj has no parent, that equals local2world)\n        matrix_world = obj.matrix_basis\n\n        # Go up the scene graph along all parents\n        while obj.parent is not None:\n            # Add transformation to parent frame\n            matrix_world = obj.parent.matrix_basis @ obj.matrix_parent_inverse @ matrix_world\n            obj = obj.parent\n\n        return np.array(matrix_world)\n\n    def select(self):\n        \"\"\" Selects the entity. \"\"\"\n        self.blender_obj.select_set(True)\n\n    def deselect(self):\n        \"\"\" Deselects the entity. \"\"\"\n        self.blender_obj.select_set(False)\n\n    def clear_parent(self):\n        \"\"\" Removes the object's parent and moves the object into the root level of the scene graph. \"\"\"\n        # Remember original object pose\n        obj_pose = self.get_local2world_mat()\n        self.blender_obj.parent = None\n        # Make sure the object pose stays the same\n        self.set_local2world_mat(obj_pose)\n\n    def set_parent(self, new_parent: \"Entity\"):\n        \"\"\" Sets the parent of this object.\n\n        :param new_parent: The parent entity to set.\n        \"\"\"\n        # If the object has already a parent object, remove it first.\n        if self.blender_obj.parent is not None:\n            self.clear_parent()\n        self.blender_obj.parent = new_parent.blender_obj\n        # Make sure the object pose stays the same => add inverse of new parent's pose to transformation chain\n        self.blender_obj.matrix_parent_inverse = Matrix(new_parent.get_local2world_mat()).inverted()\n\n    def get_parent(self) -> Optional[\"Entity\"]:\n        \"\"\" Returns the parent of the entity.\n\n        :return: The parent.\n        \"\"\"\n        return convert_to_entity_subclass(self.blender_obj.parent) if self.blender_obj.parent is not None else None\n\n    def get_children(self, return_all_offspring: bool = False) -> List[\"Entity\"]:\n        \"\"\" Returns the children objects.\n\n        :param return_all_offspring: If this is True all children and their children are recursively found and returned\n        :return: A list of all children objects.\n        \"\"\"\n\n        def collect_offspring(entity: bpy.types.Object) -> List[bpy.types.Object]:\n            \"\"\"\n            Recursively collects the offspring for an entity\n            \"\"\"\n            offspring = []\n            for child in entity.children:\n                offspring.append(child)\n                offspring.extend(collect_offspring(child))\n            return offspring\n\n        if return_all_offspring:\n            used_children = collect_offspring(self.blender_obj)\n        else:\n            used_children = self.blender_obj.children\n        return convert_to_entities(used_children, convert_to_subclasses=True)\n\n    def delete(self, remove_all_offspring: bool = False):\n        \"\"\" Deletes the entity and maybe all of its offspring\n\n        :param remove_all_offspring: If this is True all children and their children are recursively deleted\n        \"\"\"\n        selected_objects = [self]\n        if remove_all_offspring:\n            selected_objects.extend(self.get_children(return_all_offspring=True))\n        bpy.ops.object.delete({\"selected_objects\": [e.blender_obj for e in selected_objects]})\n\n    def is_empty(self) -> bool:\n        \"\"\" Returns whether the entity is from type \"EMPTY\".\n\n        :return: True, if its an empty.\n        \"\"\"\n        return self.blender_obj.type == \"EMPTY\"\n\n    def __setattr__(self, key, value):\n        if key != \"blender_obj\":\n            raise RuntimeError(\"The entity class does not allow setting any attribute. Use the corresponding \"\n                               \"method or directly access the blender attribute via entity.blender_obj.attribute_name\")\n        object.__setattr__(self, key, value)\n\n    def __eq__(self, other):\n        if isinstance(other, Entity):\n            return self.blender_obj == other.blender_obj\n        return False\n\n    def __hash__(self):\n        return hash(self.blender_obj)",
  "def create_empty(entity_name: str, empty_type: str = \"plain_axes\") -> \"Entity\":\n    \"\"\" Creates an empty entity.\n\n    :param entity_name: The name of the new entity.\n    :param empty_type: Type of the newly created empty entity. Available: [\"plain_axes\", \"arrows\", \"single_arrow\", \\\n                       \"circle\", \"cube\", \"sphere\", \"cone\"]\n    :return: The new Mesh entity.\n    \"\"\"\n    if empty_type.lower() in [\"plain_axes\", \"arrows\", \"single_arrow\", \"circle\", \"cube\", \"sphere\", \"cone\"]:\n        bpy.ops.object.empty_add(type=empty_type.upper(), align=\"WORLD\")\n    else:\n        raise RuntimeError(f'Unknown basic empty type \"{empty_type}\"! Available types: \"plain_axes\".')\n\n    new_entity = Entity(bpy.context.object)\n    new_entity.set_name(entity_name)\n    return new_entity",
  "def convert_to_entities(blender_objects: list, convert_to_subclasses: bool = False) -> List[\"Entity\"]:\n    \"\"\" Converts the given list of blender objects to entities\n\n    :param blender_objects: List of blender objects.\n    :param convert_to_subclasses: If True, each blender object will be wrapped into an entity subclass based\n                                  on the type of object.\n    :return: The list of entities.\n    \"\"\"\n    if not convert_to_subclasses:\n        return [Entity(obj) for obj in blender_objects]\n    return [convert_to_entity_subclass(obj) for obj in blender_objects]",
  "def convert_to_entity_subclass(blender_object: bpy.types.Object) -> \"Entity\":\n    \"\"\" Converts the given blender object into our respective wrapper class.\n\n    :param blender_object: The blender object.\n    :return: The wrapped object.\n    \"\"\"\n    if blender_object.type == 'MESH':\n        # pylint: disable=import-outside-toplevel,cyclic-import\n        from blenderproc.python.types.MeshObjectUtility import MeshObject\n        # pylint: enable=import-outside-toplevel,cyclic-import\n        return MeshObject(blender_object)\n    if blender_object.type == 'LIGHT':\n        # pylint: disable=import-outside-toplevel,cyclic-import\n        from blenderproc.python.types.LightUtility import Light\n        # pylint: enable=import-outside-toplevel,cyclic-import\n        return Light(blender_obj=blender_object)\n    return Entity(blender_object)",
  "def delete_multiple(entities: List[Union[\"Entity\"]], remove_all_offspring: bool = False):\n    \"\"\" Deletes multiple entities at once\n\n    :param entities: A list of entities that should be deleted\n    :param remove_all_offspring: If this is True all children and their children are recursively deleted\n    \"\"\"\n\n    if remove_all_offspring:\n        all_nodes = []\n        for entity in entities:\n            all_nodes.append(entity)\n            all_nodes.extend(entity.get_children(return_all_offspring=True))\n        # avoid doubles\n        all_nodes = set(all_nodes)\n        bpy.ops.object.delete({\"selected_objects\": [e.blender_obj for e in all_nodes]})\n    else:\n        bpy.ops.object.delete({\"selected_objects\": [e.blender_obj for e in entities]})",
  "def update_blender_ref(self, name: str):\n        \"\"\" Updates the contained blender reference using the given name of the instance.\n\n        :param name: The name of the instance which will be used to update its blender reference.\n        \"\"\"\n        self.blender_obj = bpy.data.objects[name]",
  "def set_location(self, location: Union[list, Vector, np.ndarray], frame: Optional[int] = None):\n        \"\"\" Sets the location of the entity in 3D world coordinates.\n\n        :param location: The location to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.location = location\n        Utility.insert_keyframe(self.blender_obj, \"location\", frame)",
  "def set_rotation_euler(self, rotation_euler: Union[list, Euler, np.ndarray], frame: Optional[int] = None):\n        \"\"\" Sets the rotation of the entity in euler angles.\n\n        :param rotation_euler: The euler angles to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.rotation_euler = rotation_euler\n        Utility.insert_keyframe(self.blender_obj, \"rotation_euler\", frame)",
  "def set_rotation_mat(self, rotation_mat: Union[Matrix, np.ndarray], frame: Optional[int] = None):\n        \"\"\" Sets the rotation of the entity using a rotation matrix.\n\n        :param rotation_mat: The 3x3 local to world rotation matrix.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.set_rotation_euler(Matrix(rotation_mat).to_euler(), frame)",
  "def set_scale(self, scale: Union[list, np.ndarray, Vector], frame: Optional[int] = None):\n        \"\"\" Sets the scale of the entity along all three axes.\n\n        :param scale: The scale to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.scale = scale\n        Utility.insert_keyframe(self.blender_obj, \"scale\", frame)",
  "def get_location(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Returns the location of the entity in 3D world coordinates.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The location at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return np.array(self.blender_obj.location)",
  "def get_rotation(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Returns the rotation of the entity in euler angles.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The rotation at the specified frame.\n        \"\"\"\n        warnings.warn(\"This function will be deprecated. Use get_rotation_euler() instead.\")\n        return self.get_rotation_euler(frame)",
  "def get_rotation_euler(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Returns the rotation of the entity in euler angles.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The rotation at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return np.array(self.blender_obj.rotation_euler)",
  "def get_rotation_mat(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Gets the rotation matrix of the entity.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The 3x3 local2world rotation matrix.\n        \"\"\"\n        return np.array(Euler(self.get_rotation_euler(frame)).to_matrix())",
  "def get_scale(self, frame: Optional[int] = None) -> np.ndarray:\n        \"\"\" Returns the scale of the entity along all three axes.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The scale at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return np.array(self.blender_obj.scale)",
  "def apply_T(self, transform: Union[np.ndarray, Matrix]):\n        \"\"\" Apply the given transformation to the pose of the entity.\n\n        :param transform: A 4x4 matrix representing the transformation.\n        \"\"\"\n        self.blender_obj.matrix_world = Matrix(self.get_local2world_mat()) @ Matrix(transform)",
  "def set_local2world_mat(self, matrix_world: Union[np.ndarray, Matrix]):\n        \"\"\" Sets the pose of the object in the form of a local2world matrix.\n\n        :param matrix_world: A 4x4 matrix.\n        \"\"\"\n        # To make sure matrices are always interpreted row-wise, we first convert them to a mathutils matrix.\n        self.blender_obj.matrix_world = Matrix(matrix_world)",
  "def get_local2world_mat(self) -> np.ndarray:\n        \"\"\" Returns the pose of the object in the form of a local2world matrix.\n\n        :return: The 4x4 local2world matrix.\n        \"\"\"\n        obj = self.blender_obj\n        # Start with local2parent matrix (if obj has no parent, that equals local2world)\n        matrix_world = obj.matrix_basis\n\n        # Go up the scene graph along all parents\n        while obj.parent is not None:\n            # Add transformation to parent frame\n            matrix_world = obj.parent.matrix_basis @ obj.matrix_parent_inverse @ matrix_world\n            obj = obj.parent\n\n        return np.array(matrix_world)",
  "def select(self):\n        \"\"\" Selects the entity. \"\"\"\n        self.blender_obj.select_set(True)",
  "def deselect(self):\n        \"\"\" Deselects the entity. \"\"\"\n        self.blender_obj.select_set(False)",
  "def clear_parent(self):\n        \"\"\" Removes the object's parent and moves the object into the root level of the scene graph. \"\"\"\n        # Remember original object pose\n        obj_pose = self.get_local2world_mat()\n        self.blender_obj.parent = None\n        # Make sure the object pose stays the same\n        self.set_local2world_mat(obj_pose)",
  "def set_parent(self, new_parent: \"Entity\"):\n        \"\"\" Sets the parent of this object.\n\n        :param new_parent: The parent entity to set.\n        \"\"\"\n        # If the object has already a parent object, remove it first.\n        if self.blender_obj.parent is not None:\n            self.clear_parent()\n        self.blender_obj.parent = new_parent.blender_obj\n        # Make sure the object pose stays the same => add inverse of new parent's pose to transformation chain\n        self.blender_obj.matrix_parent_inverse = Matrix(new_parent.get_local2world_mat()).inverted()",
  "def get_parent(self) -> Optional[\"Entity\"]:\n        \"\"\" Returns the parent of the entity.\n\n        :return: The parent.\n        \"\"\"\n        return convert_to_entity_subclass(self.blender_obj.parent) if self.blender_obj.parent is not None else None",
  "def get_children(self, return_all_offspring: bool = False) -> List[\"Entity\"]:\n        \"\"\" Returns the children objects.\n\n        :param return_all_offspring: If this is True all children and their children are recursively found and returned\n        :return: A list of all children objects.\n        \"\"\"\n\n        def collect_offspring(entity: bpy.types.Object) -> List[bpy.types.Object]:\n            \"\"\"\n            Recursively collects the offspring for an entity\n            \"\"\"\n            offspring = []\n            for child in entity.children:\n                offspring.append(child)\n                offspring.extend(collect_offspring(child))\n            return offspring\n\n        if return_all_offspring:\n            used_children = collect_offspring(self.blender_obj)\n        else:\n            used_children = self.blender_obj.children\n        return convert_to_entities(used_children, convert_to_subclasses=True)",
  "def delete(self, remove_all_offspring: bool = False):\n        \"\"\" Deletes the entity and maybe all of its offspring\n\n        :param remove_all_offspring: If this is True all children and their children are recursively deleted\n        \"\"\"\n        selected_objects = [self]\n        if remove_all_offspring:\n            selected_objects.extend(self.get_children(return_all_offspring=True))\n        bpy.ops.object.delete({\"selected_objects\": [e.blender_obj for e in selected_objects]})",
  "def is_empty(self) -> bool:\n        \"\"\" Returns whether the entity is from type \"EMPTY\".\n\n        :return: True, if its an empty.\n        \"\"\"\n        return self.blender_obj.type == \"EMPTY\"",
  "def __setattr__(self, key, value):\n        if key != \"blender_obj\":\n            raise RuntimeError(\"The entity class does not allow setting any attribute. Use the corresponding \"\n                               \"method or directly access the blender attribute via entity.blender_obj.attribute_name\")\n        object.__setattr__(self, key, value)",
  "def __eq__(self, other):\n        if isinstance(other, Entity):\n            return self.blender_obj == other.blender_obj\n        return False",
  "def __hash__(self):\n        return hash(self.blender_obj)",
  "def collect_offspring(entity: bpy.types.Object) -> List[bpy.types.Object]:\n            \"\"\"\n            Recursively collects the offspring for an entity\n            \"\"\"\n            offspring = []\n            for child in entity.children:\n                offspring.append(child)\n                offspring.extend(collect_offspring(child))\n            return offspring",
  "def get_instances() -> List[Tuple[str, \"Struct\"]]:\n    \"\"\" Returns a list containing all existing struct instances.\n\n    :return: A list of tuples, each containing a struct and its name.\n    \"\"\"\n    # this can only be imported here, else it causes a circle import\n    #pylint: disable=import-outside-toplevel,cyclic-import\n    from blenderproc.python.types.StructUtility import Struct\n    #pylint: enable=import-outside-toplevel,cyclic-import\n\n    instances = []\n    # Iterate over all still existing instances\n    for instance in Struct.__refs__:\n        # Check that the referenced blender_obj inside is valid\n        if instance.is_valid():\n            # Collect instance and its name (its unique identifier)\n            instances.append((instance.get_name(), instance))\n    return instances",
  "class Link(Entity):\n    \"\"\"\n    Every instance of this class is a link which is usually part of an URDFObject. It can have objects attached to it,\n    and different types of armature bones for manipulation.\n    \"\"\"\n    def __init__(self, bpy_object: bpy.types.Object):\n        super().__init__(bpy_object)\n\n        object.__setattr__(self, 'visuals', [])\n        object.__setattr__(self, 'inertial', None)\n        object.__setattr__(self, 'collisions', [])\n        object.__setattr__(self, 'joint_type', None)\n        object.__setattr__(self, 'bone', None)\n        object.__setattr__(self, 'fk_bone', None)\n        object.__setattr__(self, 'ik_bone', None)\n        object.__setattr__(self, 'ik_bone_controller', None)\n        object.__setattr__(self, 'ik_bone_constraint', None)\n        object.__setattr__(self, 'armature', None)\n        object.__setattr__(self, 'child', None)\n        object.__setattr__(self, 'fk_ik_mode', \"fk\")\n        object.__setattr__(self, 'link2bone_mat', None)\n        object.__setattr__(self, 'visual_local2link_mats', [])\n        object.__setattr__(self, 'collision_local2link_mats', [])\n        object.__setattr__(self, 'inertial_local2link_mat', None)\n\n    def set_link_parent(self, parent: \"Link\"):\n        \"\"\" Sets the parent of this link.\n\n        :param parent: Parent link.\n        \"\"\"\n        assert isinstance(parent, Link)\n        object.__setattr__(self, \"parent\", parent)\n\n    def get_link_parent(self) -> \"Link\":\n        \"\"\" Returns this link's parent.\n\n        :return: Parent link.\n        \"\"\"\n        return self.parent\n\n    def set_link_child(self, child: \"Link\"):\n        \"\"\" Sets the child of this link.\n\n        :param child: Child link.\n        \"\"\"\n        assert isinstance(child, Link)\n        object.__setattr__(self, \"child\", child)\n\n    def get_link_child(self) -> \"Link\":\n        \"\"\" Returns this link's child.\n\n        :return: Child link.\n        \"\"\"\n        return self.child\n\n    def _set_rotation_euler(self, bone: bpy.types.PoseBone,\n                            rotation_euler: Union[float, List[float], Euler, np.ndarray], mode: str = \"absolute\"):\n        \"\"\" Rotates the bone based on euler angles. Validates values with given constraints.\n\n        :param bone: The bone to be rotated.\n        :param rotation_euler: The amount of rotation (in radians). Either three floats for x, y and z axes, or a\n                               single float. In the latter case, the axis of rotation is derived based on the rotation\n                               constraint. If these are not properly set (i.e., two axes must have equal min/max\n                               values) an exception will be thrown.\n        :param mode: One of [\"absolute\", \"relative\"]. For absolute rotations we clip the rotation value based on the\n                     constraints. For relative we don't - this will result in inverse motion after the constraint's\n                     limits have been reached.\n        \"\"\"\n        assert mode in [\"absolute\", \"relative\"]\n\n        bpy.ops.object.select_all(action='DESELECT')\n\n        bone.bone.select = True\n        bone.rotation_mode = 'XYZ'\n\n        # in absolute mode we overwrite the rotation values of the armature\n        if mode == \"absolute\":\n            if isinstance(rotation_euler, float):\n                axis = self._determine_rotation_axis(bone=bone)\n                rotation_euler = self._clip_value_from_constraint(bone=bone, value=rotation_euler,\n                                                                  constraint_name=\"Limit Rotation\", axis=axis)\n                current_rotation_euler = bone.rotation_euler\n                current_rotation_euler[[\"X\", \"Y\", \"Z\"].index(axis)] = rotation_euler\n                bone.rotation_euler = current_rotation_euler\n                print(f\"Set rotation_euler of bone {bone.name} to {rotation_euler}\")\n            else:\n                bone.rotation_euler = Vector(\n                    [self._clip_value_from_constraint(bone=bone, value=rot_euler, constraint_name=\"Limit Rotation\",\n                                                      axis=axis)\n                     for rot_euler, axis in zip(rotation_euler, [\"X\", \"Y\", \"Z\"])])\n                print(f\"Set rotation_euler of bone {bone.name} to {rotation_euler}\")\n        # in relative mode we add the rotation to the current value\n        elif mode == \"relative\":\n            if isinstance(rotation_euler, float):\n                axis = self._determine_rotation_axis(bone=bone)\n                if axis is not None:\n                    bone.rotation_euler.rotate_axis(axis, rotation_euler)\n                else:\n                    for axis in ['X', 'Y', 'Z']:\n                        bone.rotation_euler.rotate_axis(axis, rotation_euler)\n                print(f\"Relatively rotated bone {bone.name} around axis {axis} for {rotation_euler} radians\")\n            else:\n                for axis, rotation in zip([\"X\", \"Y\", \"Z\"], rotation_euler):\n                    bone.rotation_euler.rotate_axis(axis, rotation)\n                print(f\"Relatively rotated {bone.name} for {rotation_euler} radians\")\n\n    def set_rotation_euler(self, *args, **kwargs):\n        raise NotImplementedError(\"Please use 'set_rotation_euler_fk()' or set_rotation_euler_ik()'!\")\n\n    def set_location(self, *args, **kwargs):\n        raise NotImplementedError(\"Please use 'set_location_ik()'!\")\n\n    def set_rotation_euler_fk(self, *args, **kwargs):\n        \"\"\" Sets the rotation for this link in forward kinematics mode. See self._set_rotation_euler() for details. \"\"\"\n        if self.get_fk_ik_mode() != \"fk\":\n            self.switch_fk_ik_mode(mode=\"fk\")\n        self._set_rotation_euler(bone=self.fk_bone, *args, **kwargs)\n\n    def set_rotation_euler_ik(self, *args, **kwargs):\n        \"\"\" Sets the rotation for this link in inverse kinematics mode. See self._set_rotation_euler() for details. \"\"\"\n        if self.get_fk_ik_mode() != \"ik\":\n            self.switch_fk_ik_mode(mode=\"ik\")\n        self._set_rotation_euler(bone=self.ik_bone_controller, *args, **kwargs)\n\n    def set_location_ik(self, location: Union[List[float], np.array, Vector]):\n        \"\"\" Sets the location of the ik bone controller in inverse kinematics mode.\n\n        :param location: Location vector.\n        \"\"\"\n        if self.get_fk_ik_mode() != \"ik\":\n            self.switch_fk_ik_mode(mode=\"ik\")\n\n        if not isinstance(location, Vector):\n            location = Vector(location)\n        assert self.ik_bone_controller is not None, \"No ik bone chain created. Please run \" \\\n                                                    \"'urdf_object.create_ik_bone_controller()' first!\"\n\n        # first: determine offset to base frame\n        self.ik_bone_controller.location = Vector([0., 0., 0.])\n        self.ik_bone_controller.rotation_mode = \"XYZ\"\n        self.ik_bone_controller.rotation_euler = Vector([0., 0., 0.])\n        bpy.context.view_layer.update()\n        offset_mat = self.ik_bone_controller.matrix\n\n        # offset location by ik offset to base bones\n        location += (self.ik_bone.head - self.bone.head)\n        location = Vector([location[0], location[1], location[2], 1.])\n\n        # move to current frame\n        location = offset_mat.inverted() @ location\n        self.ik_bone_controller.location = location[:3]\n\n    def _determine_rotation_axis(self, bone: bpy.types.PoseBone) -> Optional[str]:\n        \"\"\" Determines the single rotation axis and checks if the constraints are set well to have only one axis of\n            freedom.\n\n        :param bone: Bone of which the rotation axis will be determined.\n        :return: The single rotation axis ('X', 'Y' or 'Z') or None if no constraint is set..\n        \"\"\"\n        c = get_constraint(bone=bone, constraint_name=\"Limit Rotation\")\n        if c is None:\n            print(\"WARNING: No rotation constraint set. Will rotate all axes relatively!\")\n            return None\n\n        axes = ['X', 'Y', 'Z']\n        if c.use_limit_x and c.min_x == c.max_x:\n            axes.pop(axes.index('X'))\n        if c.use_limit_y and c.min_y == c.max_y:\n            axes.pop(axes.index('Y'))\n        if c.use_limit_z and c.min_z == c.max_z:\n            axes.pop(axes.index('Z'))\n        assert len(axes) == 1, f\"Constraints are set wrong for a rotation around a single axis. Only one axis should \" \\\n                               f\"be allowed to move, but found freedom in {len(axes)} axes of armature \" \\\n                               f\"{self.get_name()} (constraint: {c}, uses limits (xyz): \" \\\n                               f\"{c.use_limit_x, c.use_limit_y, c.use_limit_z}, \" \\\n                               f\"values: {c.min_x, c.max_x, c.min_y, c.max_y, c.min_z, c.max_z}).\"\n\n        return axes[0]\n\n    def _clip_value_from_constraint(self, bone: bpy.types.PoseBone, value: float = 0, constraint_name: str = \"\",\n                                    axis: str = \"X\") -> float:\n        \"\"\" Checks if an axis is constraint, and clips the value to the min/max of this constraint. If the constraint\n            does not exist, nothing is done.\n\n        :param bone: The bone from which the constraints will be determined.\n        :param value: Value to be clipped.\n        :param constraint_name: Name of the constraint.\n        :param axis: Axis to check.\n        :return: Clipped value if a constraint is set, else the initial value.\n        \"\"\"\n        c = get_constraint(bone=bone, constraint_name=constraint_name)\n        if c is not None:\n            min_value = {\"x\": c.min_x, \"y\": c.min_y, \"z\": c.min_z}[axis.lower()]\n            max_value = {\"x\": c.max_x, \"y\": c.max_y, \"z\": c.max_z}[axis.lower()]\n            print(f\"Clipping {value} to be in range {min_value}, {max_value}\")\n            if value < min_value:\n                return min_value\n            if value > max_value:\n                return max_value\n        return value\n\n    def set_visuals(self, visuals: List[MeshObject]):\n        \"\"\" Sets the visual meshes for this link.\n\n        :param visuals: List of visual meshes.\n        \"\"\"\n        object.__setattr__(self, \"visuals\", visuals)\n\n    def get_visuals(self) -> List[MeshObject]:\n        \"\"\" Returns the visual meshes for this link.\n\n        :return: List of visual meshes.\n        \"\"\"\n        return self.visuals\n\n    def set_collisions(self, collisions: List[MeshObject]):\n        \"\"\" Sets the collision meshes for this link.\n\n        :param collisions: List of collision meshes.\n        \"\"\"\n        object.__setattr__(self, \"collisions\", collisions)\n\n    def get_collisions(self) -> List[MeshObject]:\n        \"\"\" Returns the collision meshes for this link.\n\n        :return: List of collision meshes.\n        \"\"\"\n        return self.collisions\n\n    def set_inertial(self, inertial: Inertial):\n        \"\"\" Sets the inertial meshes for this link.\n\n        :param inertial: List of inertial meshes.\n        \"\"\"\n        object.__setattr__(self, \"inertial\", inertial)\n\n    def get_inertial(self) -> Inertial:\n        \"\"\" Returns the inertial meshes for this link.\n\n        :return: List of inertial meshes.\n        \"\"\"\n        return self.inertial\n\n    def set_bone(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the bone controlling the visuals / collisions / inertial of the link.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"bone\", bone)\n\n    def set_fk_bone(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the bone controlling the forward kinematic motion of this link.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"fk_bone\", bone)\n\n    def set_ik_bone(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the bone controlling the inverse kinematic motion of this link.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone\", bone)\n\n    def set_ik_bone_controller(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the control bone controlling the inverse kinematic motion for this link.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_controller\", bone)\n\n    def set_ik_bone_constraint(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the constraint bone responsible for constraining to inverse kinematic motion.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_constraint\", bone)\n\n    def set_armature(self, armature: bpy.types.Armature):\n        \"\"\" Sets the armature which holds all the bones of all links.\n\n        :param armature: The armature.\n        \"\"\"\n        object.__setattr__(self, \"armature\", armature)\n\n    def _set_fk_ik_mode(self, mode=\"fk\"):\n        \"\"\" Sets the mode of the link.\n\n        :param mode: One of [\"fk\", \"ik\"] denoting forward or inverse kinematics mode.\n        \"\"\"\n        object.__setattr__(self, \"fk_ik_mode\", mode)\n\n    def set_link2bone_mat(self, matrix: Matrix):\n        \"\"\" Sets the transformation matrix from bone to link.\n\n        :param matrix: The transformation from bone to link.\n        \"\"\"\n        object.__setattr__(self, \"link2bone_mat\", matrix)\n\n    def set_visual_local2link_mats(self, matrix_list: List[Matrix]):\n        \"\"\" Sets the transformation matrices from link to the visual parts.\n\n        :param matrix_list: List of transformation matrices.\n        \"\"\"\n        object.__setattr__(self, \"visual_local2link_mats\", matrix_list)\n\n    def set_collision_local2link_mats(self, matrix_list: List[Matrix]):\n        \"\"\" Sets the transformation matrices from link to the collision parts.\n\n        :param matrix_list: List of transformation matrices.\n        \"\"\"\n        object.__setattr__(self, \"collision_local2link_mats\", matrix_list)\n\n    def set_inertial_local2link_mat(self, matrix: Matrix):\n        \"\"\" Sets the transformation matrix from link to inertial.\n\n        :param matrix: The transformation matrix from link to inertial.\n        \"\"\"\n        object.__setattr__(self, \"inertial_local2link_mat\", matrix)\n\n    def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object and all visual, collision and inertial parts.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.hide(hide_object=hide_object)\n        for obj in self.get_all_objs():\n            obj.hide(hide_object=hide_object)\n\n    def get_visual_local2world_mats(self, parent2world_matrix: Optional[Matrix] = None) -> Optional[List[Matrix]]:\n        \"\"\"Returns the transformation matrices from world to the visual parts.\n\n        :param parent2world_matrix: The transformation from the link's armature to the world frame.\n        :return: List of transformation matrices.\n        \"\"\"\n        bpy.context.view_layer.update()\n        if parent2world_matrix is None:\n            parent2world_matrix = Matrix(Entity(self.armature).get_local2world_mat())\n        bone_mat = Matrix.Identity(4)\n        if self.bone is not None:\n            bone_mat = self.bone.matrix\n        if self.visuals:\n            link2base_mats = [bone_mat @ (self.link2bone_mat.inverted() @ visual_local2link_mat) for\n                              visual_local2link_mat in self.visual_local2link_mats]\n            return [parent2world_matrix @ mat for mat in link2base_mats]\n        return None\n\n    def get_collision_local2world_mats(self, parent2world_matrix: Optional[Matrix] = None) -> Optional[List[Matrix]]:\n        \"\"\"Returns the transformation matrices from world to the collision parts.\n\n        :param parent2world_matrix: The transformation from the link's armature to the world frame.\n        :return: List of transformation matrices.\n        \"\"\"\n        bpy.context.view_layer.update()\n        if parent2world_matrix is None:\n            parent2world_matrix = Matrix(Entity(self.armature).get_local2world_mat())\n        bone_mat = Matrix.Identity(4)\n        if self.bone is not None:\n            bone_mat = self.bone.matrix\n        if self.collisions:\n            link2base_mats = [bone_mat @ (self.link2bone_mat.inverted() @ collision_local2link_mat) for\n                              collision_local2link_mat in self.collision_local2link_mats]\n            return [parent2world_matrix @ mat for mat in link2base_mats]\n        return None\n\n    def get_inertial_local2world_mat(self, parent2world_matrix: Optional[Matrix] = None) -> Optional[Matrix]:\n        \"\"\"Returns the transformation matrix from world to the inertial part.\n\n        :param parent2world_matrix: The transformation from the link's armature to the world frame.\n        :return: The transformation matrix.\n        \"\"\"\n        bpy.context.view_layer.update()\n        if parent2world_matrix is None:\n            parent2world_matrix = Matrix(Entity(self.armature).get_local2world_mat())\n        bone_mat = Matrix.Identity(4)\n        if self.bone is not None:\n            bone_mat = self.bone.matrix\n        if self.inertial is not None:\n            link2base_mat = bone_mat @ (self.link2bone_mat.inverted() @ self.inertial_local2link_mat)\n            return parent2world_matrix @ link2base_mat\n        return None\n\n    def get_all_objs(self):\n        \"\"\" Returns all meshes of this link.\n\n        :return: List of all meshes (visual, collision, inertial) for this link.\n        \"\"\"\n        return self.visuals + self.collisions + ([self.inertial] if self.inertial is not None else [])\n\n    def set_joint_type(self, joint_type: Optional[str]):\n        \"\"\" Sets the joint type of the link which specifies the connection to its parent.\n\n        :param joint_type: One of ['fixed', 'prismatic', 'revolute', 'continuous', 'planar', 'floating' or None].\n        \"\"\"\n        object.__setattr__(self, \"joint_type\", joint_type)\n\n    def get_joint_type(self) -> Optional[str]:\n        \"\"\" Returns the joint type.\n\n        :return: The joint type of the armature.\n        \"\"\"\n        return self.joint_type\n\n    def parent_with_bone(self, weight_distribution='rigid'):\n        \"\"\" Parents all objects of the link to the bone.\n\n        :param weight_distribution: One of ['envelope', 'automatic', 'rigid']. For more information please see\n                                    https://docs.blender.org/manual/en/latest/animation/armatures/skinning/parenting.html.\n        \"\"\"\n        assert weight_distribution in ['envelope', 'automatic', 'rigid']\n\n        if self.bone is None:\n            # only set parent\n            print(f\"WARNING: Link {self.get_name()} does not have a bone to be parented with (usually because it's the \"\n                  f\"first link)!\")\n            self.blender_obj.parent = self.armature\n            return\n\n        if weight_distribution in ['envelope', 'automatic']:\n            bpy.ops.object.mode_set(mode='OBJECT')\n            bpy.ops.object.select_all(action=\"DESELECT\")\n            self.select()\n            for obj in self.get_all_objs():\n                obj.select()\n            self.armature.select_set(True)\n            bpy.context.view_layer.objects.active = self.armature\n            bpy.ops.object.mode_set(mode='POSE')\n            if weight_distribution == 'envelope':\n                bpy.ops.object.parent_set(type='ARMATURE_ENVELOPE')\n            else:\n                bpy.ops.object.parent_set(type='ARMATURE_AUTO')\n            bpy.ops.object.mode_set(mode='OBJECT')\n        elif weight_distribution == 'rigid':\n            for obj in self.get_all_objs() + [self]:\n                bpy.ops.object.select_all(action='DESELECT')\n                obj.blender_obj.parent = self.armature\n                bpy.context.view_layer.objects.active = obj.blender_obj\n                mod = obj.blender_obj.modifiers.new(\"Armature\", \"ARMATURE\")\n                mod.object = self.armature\n                obj.blender_obj.vertex_groups.new(name=self.bone.name)\n                vertices = [v.index for v in obj.blender_obj.data.vertices]\n                obj.blender_obj.vertex_groups[0].add(vertices, 1.0, 'REPLACE')\n        bpy.ops.object.select_all(action='DESELECT')\n\n    def create_ik_bone_controller(self, relative_location: Optional[Union[List[float], Vector]] = None,\n                                   use_rotation: bool = True,\n                                   chain_length: int = 0) -> Tuple[bpy.types.PoseBone, bpy.types.PoseBone, Matrix]:\n        \"\"\" Creates an ik bone controller and a corresponding constraint bone for the respective link.\n\n        :param relative_location: Relative location of the ik bone controller w.r.t. the bone's location. This can be\n                                  used to shift the point of control further away from the end effector.\n        :param use_rotation: Whether to rotate the child links as well. Defaults to True.\n        :param chain_length: The number of parent links which are influenced by this ik bone. Defaults to 0 for all\n                             parents.\n        :return: Constraint and control bone.\n        \"\"\"\n\n        if relative_location is None:\n            relative_location = [0., 0., 0.]\n\n        bpy.ops.object.select_all(action='DESELECT')\n        bpy.context.view_layer.objects.active = self.armature\n        bpy.ops.object.mode_set(mode='EDIT')\n        self.armature.select_set(True)\n        edit_bones = self.armature.data.edit_bones\n\n        # we need two bones: a controll bone and a constraint bone\n        # the controll bone will be placed exactly where the current ik bone is\n        ik_bone_controller = edit_bones.new(self.ik_bone.name + '.controller')\n        ik_bone_controller.head = self.ik_bone.head + Vector(relative_location)\n        ik_bone_controller.tail = self.ik_bone.tail + Vector(relative_location)\n        ik_bone_controller.parent = edit_bones[self.bone.name].parent_recursive[-1]\n\n        # the constraint bone will be placed at the head of the ik bone\n        ik_bone_constraint = edit_bones.new(self.ik_bone.name + '.constraint')\n        ik_bone_constraint.tail = self.ik_bone.head + Vector(relative_location)\n        ik_bone_constraint.head = ik_bone_constraint.tail - (self.ik_bone.tail - self.ik_bone.head)\n        # we need to re-parent the pre- and successor of the constraint bone\n        ik_bone_constraint.parent = edit_bones[self.ik_bone.name].parent_recursive[0]\n        edit_bones[self.ik_bone.name].parent = edit_bones[ik_bone_constraint.name]\n\n        # add the bones to the link\n        bpy.ops.object.mode_set(mode='POSE')\n        self.set_ik_bone_constraint(self.armature.pose.bones.get(self.ik_bone.name + '.constraint'))\n        self.set_ik_bone_controller(self.armature.pose.bones.get(self.ik_bone.name + '.controller'))\n\n        # add ik constraint\n        set_ik_constraint(self.ik_bone_constraint, self.armature, self.ik_bone_controller.name,\n                          use_rotation=use_rotation, chain_length=chain_length)\n\n        if self.get_joint_type() == \"revolute\":\n            set_ik_limits_from_rotation_constraint(self.ik_bone_constraint,\n                                                   constraint=self.bone.constraints[\"Limit Rotation\"])\n\n        bpy.ops.object.mode_set(mode='OBJECT')\n\n        # determine offset\n        bpy.context.view_layer.update()\n        offset = self.ik_bone_constraint.matrix.inverted() @ self.ik_bone_controller.matrix\n\n        return self.ik_bone_constraint, self.ik_bone_controller, offset\n\n    def get_fk_ik_mode(self) -> str:\n        \"\"\" Returns the currently selected mode.\n\n        :return: One of [\"fk\", \"ik\"] denoting forward or inverse kinematics mode.\n        \"\"\"\n        return self.fk_ik_mode\n\n    def switch_fk_ik_mode(self, mode: str = \"fk\", keep_pose: bool = True):\n        \"\"\" Switches between forward and inverse kinematics mode. Will do this automatically when switching between e.g.\n            `set_rotation_euler_fk()` and `set_rotation_euler_ik()`.\n\n        :param mode: One of [\"fk\", \"ik\"] denoting forward or inverse kinematics mode.\n        :param keep_pose: If specified, will keep the pose when switching modes. Otherwise, will return to the old pose\n                          of the previously selected mode.\n        \"\"\"\n        if self.bone is None:\n            return None\n        assert mode in [\"fk\", \"ik\"]\n        if mode == \"fk\":  # turn off copy rotation constraints of fk bone and base bone\n            if self.get_fk_ik_mode() == \"fk\":\n                return None\n            bpy.context.view_layer.update()\n\n            if keep_pose:\n                self.fk_bone.matrix = self.ik_bone.matrix\n            if self.joint_type == \"revolute\":\n                self.bone.constraints[\"copy_rotation.fk\"].influence = 1.\n                self.bone.constraints[\"copy_rotation.ik\"].influence = 0.\n\n            if \"copy_rotation.ik\" in self.bone.constraints.keys():\n                self.bone.constraints[\"copy_rotation.ik\"].influence = 0.  # otherwise skip\n\n            if self.ik_bone_controller is not None:  # switch off ik constraint\n                self.ik_bone_constraint.constraints[\"IK\"].influence = 0.\n            self._set_fk_ik_mode(mode=\"fk\")\n\n        else:  # turn off copy rotation constraints of ik bone and base bone\n            if self.get_fk_ik_mode() == \"ik\":\n                return None\n            bpy.context.view_layer.update()\n\n            if keep_pose:\n                self.ik_bone.matrix = self.fk_bone.matrix\n\n            if self.joint_type == \"revolute\":\n                self.bone.constraints[\"copy_rotation.fk\"].influence = 0.\n                self.bone.constraints[\"copy_rotation.ik\"].influence = 1.\n\n            self._set_fk_ik_mode(mode=\"ik\")\n            if self.ik_bone_controller is not None:\n                bpy.context.view_layer.update()\n                location = np.array(self.armature.pose.bones[self.bone.name].head)  # or matrix?\n                print(\"desired ik location\", location)\n                # location = Vector(np.array(mat)[:3, -1])\n                self.set_location_ik(location)\n\n            if self.ik_bone_constraint is not None:\n                self.ik_bone_constraint.constraints[\"IK\"].influence = 1.\n                fk_bone_mat = np.array(self.fk_bone.matrix)\n                fk_bone_mat[:3, -1] = np.array(self.ik_bone_controller.matrix)[:3, -1]\n                self.ik_bone_controller.matrix = Matrix(fk_bone_mat)\n        return None\n\n    def get_joint_rotation(self, frame: int = None) -> float:\n        \"\"\" Get current joint rotation based on euler angles.\n\n        :param frame: The desired frame.\n        :return: Current joint rotation in radians.\n        \"\"\"\n\n        if self.bone is None:\n            return 0.0\n\n        pose_bone = self.armature.pose.bones[self.bone.name]\n        data_bone = self.armature.data.bones[self.bone.name]\n\n        with KeyFrame(frame):\n            M_pose = pose_bone.matrix\n            M_data = data_bone.matrix_local\n\n        # grab the parent's world pose and rest matrices\n        if data_bone.parent:\n            M_parent_data = data_bone.parent.matrix_local.copy()\n            M_parent_pose = pose_bone.parent.matrix.copy()\n        else:\n            M_parent_data = Matrix()\n            M_parent_pose = Matrix()\n\n        M1 = M_data.copy()\n        M1.invert()\n\n        M2 = M_parent_pose.copy()\n        M2.invert()\n\n        visual_matrix = M1 @ M_parent_data @ M2 @ M_pose\n\n        return visual_matrix.to_quaternion().angle\n\n    def mesh_as_trimesh(self) -> Optional[Trimesh]:\n        \"\"\" Returns a trimesh.Trimesh instance of the link's first visual object, if it exists.\n\n        :return: The link's first visual object as trimesh.Trimesh if the link has one or more visuals, else None.\n        \"\"\"\n        # get mesh data\n        if self.visuals:\n            return self.visuals[0].mesh_as_trimesh()\n\n        return None",
  "def __init__(self, bpy_object: bpy.types.Object):\n        super().__init__(bpy_object)\n\n        object.__setattr__(self, 'visuals', [])\n        object.__setattr__(self, 'inertial', None)\n        object.__setattr__(self, 'collisions', [])\n        object.__setattr__(self, 'joint_type', None)\n        object.__setattr__(self, 'bone', None)\n        object.__setattr__(self, 'fk_bone', None)\n        object.__setattr__(self, 'ik_bone', None)\n        object.__setattr__(self, 'ik_bone_controller', None)\n        object.__setattr__(self, 'ik_bone_constraint', None)\n        object.__setattr__(self, 'armature', None)\n        object.__setattr__(self, 'child', None)\n        object.__setattr__(self, 'fk_ik_mode', \"fk\")\n        object.__setattr__(self, 'link2bone_mat', None)\n        object.__setattr__(self, 'visual_local2link_mats', [])\n        object.__setattr__(self, 'collision_local2link_mats', [])\n        object.__setattr__(self, 'inertial_local2link_mat', None)",
  "def set_link_parent(self, parent: \"Link\"):\n        \"\"\" Sets the parent of this link.\n\n        :param parent: Parent link.\n        \"\"\"\n        assert isinstance(parent, Link)\n        object.__setattr__(self, \"parent\", parent)",
  "def get_link_parent(self) -> \"Link\":\n        \"\"\" Returns this link's parent.\n\n        :return: Parent link.\n        \"\"\"\n        return self.parent",
  "def set_link_child(self, child: \"Link\"):\n        \"\"\" Sets the child of this link.\n\n        :param child: Child link.\n        \"\"\"\n        assert isinstance(child, Link)\n        object.__setattr__(self, \"child\", child)",
  "def get_link_child(self) -> \"Link\":\n        \"\"\" Returns this link's child.\n\n        :return: Child link.\n        \"\"\"\n        return self.child",
  "def _set_rotation_euler(self, bone: bpy.types.PoseBone,\n                            rotation_euler: Union[float, List[float], Euler, np.ndarray], mode: str = \"absolute\"):\n        \"\"\" Rotates the bone based on euler angles. Validates values with given constraints.\n\n        :param bone: The bone to be rotated.\n        :param rotation_euler: The amount of rotation (in radians). Either three floats for x, y and z axes, or a\n                               single float. In the latter case, the axis of rotation is derived based on the rotation\n                               constraint. If these are not properly set (i.e., two axes must have equal min/max\n                               values) an exception will be thrown.\n        :param mode: One of [\"absolute\", \"relative\"]. For absolute rotations we clip the rotation value based on the\n                     constraints. For relative we don't - this will result in inverse motion after the constraint's\n                     limits have been reached.\n        \"\"\"\n        assert mode in [\"absolute\", \"relative\"]\n\n        bpy.ops.object.select_all(action='DESELECT')\n\n        bone.bone.select = True\n        bone.rotation_mode = 'XYZ'\n\n        # in absolute mode we overwrite the rotation values of the armature\n        if mode == \"absolute\":\n            if isinstance(rotation_euler, float):\n                axis = self._determine_rotation_axis(bone=bone)\n                rotation_euler = self._clip_value_from_constraint(bone=bone, value=rotation_euler,\n                                                                  constraint_name=\"Limit Rotation\", axis=axis)\n                current_rotation_euler = bone.rotation_euler\n                current_rotation_euler[[\"X\", \"Y\", \"Z\"].index(axis)] = rotation_euler\n                bone.rotation_euler = current_rotation_euler\n                print(f\"Set rotation_euler of bone {bone.name} to {rotation_euler}\")\n            else:\n                bone.rotation_euler = Vector(\n                    [self._clip_value_from_constraint(bone=bone, value=rot_euler, constraint_name=\"Limit Rotation\",\n                                                      axis=axis)\n                     for rot_euler, axis in zip(rotation_euler, [\"X\", \"Y\", \"Z\"])])\n                print(f\"Set rotation_euler of bone {bone.name} to {rotation_euler}\")\n        # in relative mode we add the rotation to the current value\n        elif mode == \"relative\":\n            if isinstance(rotation_euler, float):\n                axis = self._determine_rotation_axis(bone=bone)\n                if axis is not None:\n                    bone.rotation_euler.rotate_axis(axis, rotation_euler)\n                else:\n                    for axis in ['X', 'Y', 'Z']:\n                        bone.rotation_euler.rotate_axis(axis, rotation_euler)\n                print(f\"Relatively rotated bone {bone.name} around axis {axis} for {rotation_euler} radians\")\n            else:\n                for axis, rotation in zip([\"X\", \"Y\", \"Z\"], rotation_euler):\n                    bone.rotation_euler.rotate_axis(axis, rotation)\n                print(f\"Relatively rotated {bone.name} for {rotation_euler} radians\")",
  "def set_rotation_euler(self, *args, **kwargs):\n        raise NotImplementedError(\"Please use 'set_rotation_euler_fk()' or set_rotation_euler_ik()'!\")",
  "def set_location(self, *args, **kwargs):\n        raise NotImplementedError(\"Please use 'set_location_ik()'!\")",
  "def set_rotation_euler_fk(self, *args, **kwargs):\n        \"\"\" Sets the rotation for this link in forward kinematics mode. See self._set_rotation_euler() for details. \"\"\"\n        if self.get_fk_ik_mode() != \"fk\":\n            self.switch_fk_ik_mode(mode=\"fk\")\n        self._set_rotation_euler(bone=self.fk_bone, *args, **kwargs)",
  "def set_rotation_euler_ik(self, *args, **kwargs):\n        \"\"\" Sets the rotation for this link in inverse kinematics mode. See self._set_rotation_euler() for details. \"\"\"\n        if self.get_fk_ik_mode() != \"ik\":\n            self.switch_fk_ik_mode(mode=\"ik\")\n        self._set_rotation_euler(bone=self.ik_bone_controller, *args, **kwargs)",
  "def set_location_ik(self, location: Union[List[float], np.array, Vector]):\n        \"\"\" Sets the location of the ik bone controller in inverse kinematics mode.\n\n        :param location: Location vector.\n        \"\"\"\n        if self.get_fk_ik_mode() != \"ik\":\n            self.switch_fk_ik_mode(mode=\"ik\")\n\n        if not isinstance(location, Vector):\n            location = Vector(location)\n        assert self.ik_bone_controller is not None, \"No ik bone chain created. Please run \" \\\n                                                    \"'urdf_object.create_ik_bone_controller()' first!\"\n\n        # first: determine offset to base frame\n        self.ik_bone_controller.location = Vector([0., 0., 0.])\n        self.ik_bone_controller.rotation_mode = \"XYZ\"\n        self.ik_bone_controller.rotation_euler = Vector([0., 0., 0.])\n        bpy.context.view_layer.update()\n        offset_mat = self.ik_bone_controller.matrix\n\n        # offset location by ik offset to base bones\n        location += (self.ik_bone.head - self.bone.head)\n        location = Vector([location[0], location[1], location[2], 1.])\n\n        # move to current frame\n        location = offset_mat.inverted() @ location\n        self.ik_bone_controller.location = location[:3]",
  "def _determine_rotation_axis(self, bone: bpy.types.PoseBone) -> Optional[str]:\n        \"\"\" Determines the single rotation axis and checks if the constraints are set well to have only one axis of\n            freedom.\n\n        :param bone: Bone of which the rotation axis will be determined.\n        :return: The single rotation axis ('X', 'Y' or 'Z') or None if no constraint is set..\n        \"\"\"\n        c = get_constraint(bone=bone, constraint_name=\"Limit Rotation\")\n        if c is None:\n            print(\"WARNING: No rotation constraint set. Will rotate all axes relatively!\")\n            return None\n\n        axes = ['X', 'Y', 'Z']\n        if c.use_limit_x and c.min_x == c.max_x:\n            axes.pop(axes.index('X'))\n        if c.use_limit_y and c.min_y == c.max_y:\n            axes.pop(axes.index('Y'))\n        if c.use_limit_z and c.min_z == c.max_z:\n            axes.pop(axes.index('Z'))\n        assert len(axes) == 1, f\"Constraints are set wrong for a rotation around a single axis. Only one axis should \" \\\n                               f\"be allowed to move, but found freedom in {len(axes)} axes of armature \" \\\n                               f\"{self.get_name()} (constraint: {c}, uses limits (xyz): \" \\\n                               f\"{c.use_limit_x, c.use_limit_y, c.use_limit_z}, \" \\\n                               f\"values: {c.min_x, c.max_x, c.min_y, c.max_y, c.min_z, c.max_z}).\"\n\n        return axes[0]",
  "def _clip_value_from_constraint(self, bone: bpy.types.PoseBone, value: float = 0, constraint_name: str = \"\",\n                                    axis: str = \"X\") -> float:\n        \"\"\" Checks if an axis is constraint, and clips the value to the min/max of this constraint. If the constraint\n            does not exist, nothing is done.\n\n        :param bone: The bone from which the constraints will be determined.\n        :param value: Value to be clipped.\n        :param constraint_name: Name of the constraint.\n        :param axis: Axis to check.\n        :return: Clipped value if a constraint is set, else the initial value.\n        \"\"\"\n        c = get_constraint(bone=bone, constraint_name=constraint_name)\n        if c is not None:\n            min_value = {\"x\": c.min_x, \"y\": c.min_y, \"z\": c.min_z}[axis.lower()]\n            max_value = {\"x\": c.max_x, \"y\": c.max_y, \"z\": c.max_z}[axis.lower()]\n            print(f\"Clipping {value} to be in range {min_value}, {max_value}\")\n            if value < min_value:\n                return min_value\n            if value > max_value:\n                return max_value\n        return value",
  "def set_visuals(self, visuals: List[MeshObject]):\n        \"\"\" Sets the visual meshes for this link.\n\n        :param visuals: List of visual meshes.\n        \"\"\"\n        object.__setattr__(self, \"visuals\", visuals)",
  "def get_visuals(self) -> List[MeshObject]:\n        \"\"\" Returns the visual meshes for this link.\n\n        :return: List of visual meshes.\n        \"\"\"\n        return self.visuals",
  "def set_collisions(self, collisions: List[MeshObject]):\n        \"\"\" Sets the collision meshes for this link.\n\n        :param collisions: List of collision meshes.\n        \"\"\"\n        object.__setattr__(self, \"collisions\", collisions)",
  "def get_collisions(self) -> List[MeshObject]:\n        \"\"\" Returns the collision meshes for this link.\n\n        :return: List of collision meshes.\n        \"\"\"\n        return self.collisions",
  "def set_inertial(self, inertial: Inertial):\n        \"\"\" Sets the inertial meshes for this link.\n\n        :param inertial: List of inertial meshes.\n        \"\"\"\n        object.__setattr__(self, \"inertial\", inertial)",
  "def get_inertial(self) -> Inertial:\n        \"\"\" Returns the inertial meshes for this link.\n\n        :return: List of inertial meshes.\n        \"\"\"\n        return self.inertial",
  "def set_bone(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the bone controlling the visuals / collisions / inertial of the link.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"bone\", bone)",
  "def set_fk_bone(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the bone controlling the forward kinematic motion of this link.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"fk_bone\", bone)",
  "def set_ik_bone(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the bone controlling the inverse kinematic motion of this link.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone\", bone)",
  "def set_ik_bone_controller(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the control bone controlling the inverse kinematic motion for this link.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_controller\", bone)",
  "def set_ik_bone_constraint(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the constraint bone responsible for constraining to inverse kinematic motion.\n\n        :param bone: The bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_constraint\", bone)",
  "def set_armature(self, armature: bpy.types.Armature):\n        \"\"\" Sets the armature which holds all the bones of all links.\n\n        :param armature: The armature.\n        \"\"\"\n        object.__setattr__(self, \"armature\", armature)",
  "def _set_fk_ik_mode(self, mode=\"fk\"):\n        \"\"\" Sets the mode of the link.\n\n        :param mode: One of [\"fk\", \"ik\"] denoting forward or inverse kinematics mode.\n        \"\"\"\n        object.__setattr__(self, \"fk_ik_mode\", mode)",
  "def set_link2bone_mat(self, matrix: Matrix):\n        \"\"\" Sets the transformation matrix from bone to link.\n\n        :param matrix: The transformation from bone to link.\n        \"\"\"\n        object.__setattr__(self, \"link2bone_mat\", matrix)",
  "def set_visual_local2link_mats(self, matrix_list: List[Matrix]):\n        \"\"\" Sets the transformation matrices from link to the visual parts.\n\n        :param matrix_list: List of transformation matrices.\n        \"\"\"\n        object.__setattr__(self, \"visual_local2link_mats\", matrix_list)",
  "def set_collision_local2link_mats(self, matrix_list: List[Matrix]):\n        \"\"\" Sets the transformation matrices from link to the collision parts.\n\n        :param matrix_list: List of transformation matrices.\n        \"\"\"\n        object.__setattr__(self, \"collision_local2link_mats\", matrix_list)",
  "def set_inertial_local2link_mat(self, matrix: Matrix):\n        \"\"\" Sets the transformation matrix from link to inertial.\n\n        :param matrix: The transformation matrix from link to inertial.\n        \"\"\"\n        object.__setattr__(self, \"inertial_local2link_mat\", matrix)",
  "def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object and all visual, collision and inertial parts.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.hide(hide_object=hide_object)\n        for obj in self.get_all_objs():\n            obj.hide(hide_object=hide_object)",
  "def get_visual_local2world_mats(self, parent2world_matrix: Optional[Matrix] = None) -> Optional[List[Matrix]]:\n        \"\"\"Returns the transformation matrices from world to the visual parts.\n\n        :param parent2world_matrix: The transformation from the link's armature to the world frame.\n        :return: List of transformation matrices.\n        \"\"\"\n        bpy.context.view_layer.update()\n        if parent2world_matrix is None:\n            parent2world_matrix = Matrix(Entity(self.armature).get_local2world_mat())\n        bone_mat = Matrix.Identity(4)\n        if self.bone is not None:\n            bone_mat = self.bone.matrix\n        if self.visuals:\n            link2base_mats = [bone_mat @ (self.link2bone_mat.inverted() @ visual_local2link_mat) for\n                              visual_local2link_mat in self.visual_local2link_mats]\n            return [parent2world_matrix @ mat for mat in link2base_mats]\n        return None",
  "def get_collision_local2world_mats(self, parent2world_matrix: Optional[Matrix] = None) -> Optional[List[Matrix]]:\n        \"\"\"Returns the transformation matrices from world to the collision parts.\n\n        :param parent2world_matrix: The transformation from the link's armature to the world frame.\n        :return: List of transformation matrices.\n        \"\"\"\n        bpy.context.view_layer.update()\n        if parent2world_matrix is None:\n            parent2world_matrix = Matrix(Entity(self.armature).get_local2world_mat())\n        bone_mat = Matrix.Identity(4)\n        if self.bone is not None:\n            bone_mat = self.bone.matrix\n        if self.collisions:\n            link2base_mats = [bone_mat @ (self.link2bone_mat.inverted() @ collision_local2link_mat) for\n                              collision_local2link_mat in self.collision_local2link_mats]\n            return [parent2world_matrix @ mat for mat in link2base_mats]\n        return None",
  "def get_inertial_local2world_mat(self, parent2world_matrix: Optional[Matrix] = None) -> Optional[Matrix]:\n        \"\"\"Returns the transformation matrix from world to the inertial part.\n\n        :param parent2world_matrix: The transformation from the link's armature to the world frame.\n        :return: The transformation matrix.\n        \"\"\"\n        bpy.context.view_layer.update()\n        if parent2world_matrix is None:\n            parent2world_matrix = Matrix(Entity(self.armature).get_local2world_mat())\n        bone_mat = Matrix.Identity(4)\n        if self.bone is not None:\n            bone_mat = self.bone.matrix\n        if self.inertial is not None:\n            link2base_mat = bone_mat @ (self.link2bone_mat.inverted() @ self.inertial_local2link_mat)\n            return parent2world_matrix @ link2base_mat\n        return None",
  "def get_all_objs(self):\n        \"\"\" Returns all meshes of this link.\n\n        :return: List of all meshes (visual, collision, inertial) for this link.\n        \"\"\"\n        return self.visuals + self.collisions + ([self.inertial] if self.inertial is not None else [])",
  "def set_joint_type(self, joint_type: Optional[str]):\n        \"\"\" Sets the joint type of the link which specifies the connection to its parent.\n\n        :param joint_type: One of ['fixed', 'prismatic', 'revolute', 'continuous', 'planar', 'floating' or None].\n        \"\"\"\n        object.__setattr__(self, \"joint_type\", joint_type)",
  "def get_joint_type(self) -> Optional[str]:\n        \"\"\" Returns the joint type.\n\n        :return: The joint type of the armature.\n        \"\"\"\n        return self.joint_type",
  "def parent_with_bone(self, weight_distribution='rigid'):\n        \"\"\" Parents all objects of the link to the bone.\n\n        :param weight_distribution: One of ['envelope', 'automatic', 'rigid']. For more information please see\n                                    https://docs.blender.org/manual/en/latest/animation/armatures/skinning/parenting.html.\n        \"\"\"\n        assert weight_distribution in ['envelope', 'automatic', 'rigid']\n\n        if self.bone is None:\n            # only set parent\n            print(f\"WARNING: Link {self.get_name()} does not have a bone to be parented with (usually because it's the \"\n                  f\"first link)!\")\n            self.blender_obj.parent = self.armature\n            return\n\n        if weight_distribution in ['envelope', 'automatic']:\n            bpy.ops.object.mode_set(mode='OBJECT')\n            bpy.ops.object.select_all(action=\"DESELECT\")\n            self.select()\n            for obj in self.get_all_objs():\n                obj.select()\n            self.armature.select_set(True)\n            bpy.context.view_layer.objects.active = self.armature\n            bpy.ops.object.mode_set(mode='POSE')\n            if weight_distribution == 'envelope':\n                bpy.ops.object.parent_set(type='ARMATURE_ENVELOPE')\n            else:\n                bpy.ops.object.parent_set(type='ARMATURE_AUTO')\n            bpy.ops.object.mode_set(mode='OBJECT')\n        elif weight_distribution == 'rigid':\n            for obj in self.get_all_objs() + [self]:\n                bpy.ops.object.select_all(action='DESELECT')\n                obj.blender_obj.parent = self.armature\n                bpy.context.view_layer.objects.active = obj.blender_obj\n                mod = obj.blender_obj.modifiers.new(\"Armature\", \"ARMATURE\")\n                mod.object = self.armature\n                obj.blender_obj.vertex_groups.new(name=self.bone.name)\n                vertices = [v.index for v in obj.blender_obj.data.vertices]\n                obj.blender_obj.vertex_groups[0].add(vertices, 1.0, 'REPLACE')\n        bpy.ops.object.select_all(action='DESELECT')",
  "def create_ik_bone_controller(self, relative_location: Optional[Union[List[float], Vector]] = None,\n                                   use_rotation: bool = True,\n                                   chain_length: int = 0) -> Tuple[bpy.types.PoseBone, bpy.types.PoseBone, Matrix]:\n        \"\"\" Creates an ik bone controller and a corresponding constraint bone for the respective link.\n\n        :param relative_location: Relative location of the ik bone controller w.r.t. the bone's location. This can be\n                                  used to shift the point of control further away from the end effector.\n        :param use_rotation: Whether to rotate the child links as well. Defaults to True.\n        :param chain_length: The number of parent links which are influenced by this ik bone. Defaults to 0 for all\n                             parents.\n        :return: Constraint and control bone.\n        \"\"\"\n\n        if relative_location is None:\n            relative_location = [0., 0., 0.]\n\n        bpy.ops.object.select_all(action='DESELECT')\n        bpy.context.view_layer.objects.active = self.armature\n        bpy.ops.object.mode_set(mode='EDIT')\n        self.armature.select_set(True)\n        edit_bones = self.armature.data.edit_bones\n\n        # we need two bones: a controll bone and a constraint bone\n        # the controll bone will be placed exactly where the current ik bone is\n        ik_bone_controller = edit_bones.new(self.ik_bone.name + '.controller')\n        ik_bone_controller.head = self.ik_bone.head + Vector(relative_location)\n        ik_bone_controller.tail = self.ik_bone.tail + Vector(relative_location)\n        ik_bone_controller.parent = edit_bones[self.bone.name].parent_recursive[-1]\n\n        # the constraint bone will be placed at the head of the ik bone\n        ik_bone_constraint = edit_bones.new(self.ik_bone.name + '.constraint')\n        ik_bone_constraint.tail = self.ik_bone.head + Vector(relative_location)\n        ik_bone_constraint.head = ik_bone_constraint.tail - (self.ik_bone.tail - self.ik_bone.head)\n        # we need to re-parent the pre- and successor of the constraint bone\n        ik_bone_constraint.parent = edit_bones[self.ik_bone.name].parent_recursive[0]\n        edit_bones[self.ik_bone.name].parent = edit_bones[ik_bone_constraint.name]\n\n        # add the bones to the link\n        bpy.ops.object.mode_set(mode='POSE')\n        self.set_ik_bone_constraint(self.armature.pose.bones.get(self.ik_bone.name + '.constraint'))\n        self.set_ik_bone_controller(self.armature.pose.bones.get(self.ik_bone.name + '.controller'))\n\n        # add ik constraint\n        set_ik_constraint(self.ik_bone_constraint, self.armature, self.ik_bone_controller.name,\n                          use_rotation=use_rotation, chain_length=chain_length)\n\n        if self.get_joint_type() == \"revolute\":\n            set_ik_limits_from_rotation_constraint(self.ik_bone_constraint,\n                                                   constraint=self.bone.constraints[\"Limit Rotation\"])\n\n        bpy.ops.object.mode_set(mode='OBJECT')\n\n        # determine offset\n        bpy.context.view_layer.update()\n        offset = self.ik_bone_constraint.matrix.inverted() @ self.ik_bone_controller.matrix\n\n        return self.ik_bone_constraint, self.ik_bone_controller, offset",
  "def get_fk_ik_mode(self) -> str:\n        \"\"\" Returns the currently selected mode.\n\n        :return: One of [\"fk\", \"ik\"] denoting forward or inverse kinematics mode.\n        \"\"\"\n        return self.fk_ik_mode",
  "def switch_fk_ik_mode(self, mode: str = \"fk\", keep_pose: bool = True):\n        \"\"\" Switches between forward and inverse kinematics mode. Will do this automatically when switching between e.g.\n            `set_rotation_euler_fk()` and `set_rotation_euler_ik()`.\n\n        :param mode: One of [\"fk\", \"ik\"] denoting forward or inverse kinematics mode.\n        :param keep_pose: If specified, will keep the pose when switching modes. Otherwise, will return to the old pose\n                          of the previously selected mode.\n        \"\"\"\n        if self.bone is None:\n            return None\n        assert mode in [\"fk\", \"ik\"]\n        if mode == \"fk\":  # turn off copy rotation constraints of fk bone and base bone\n            if self.get_fk_ik_mode() == \"fk\":\n                return None\n            bpy.context.view_layer.update()\n\n            if keep_pose:\n                self.fk_bone.matrix = self.ik_bone.matrix\n            if self.joint_type == \"revolute\":\n                self.bone.constraints[\"copy_rotation.fk\"].influence = 1.\n                self.bone.constraints[\"copy_rotation.ik\"].influence = 0.\n\n            if \"copy_rotation.ik\" in self.bone.constraints.keys():\n                self.bone.constraints[\"copy_rotation.ik\"].influence = 0.  # otherwise skip\n\n            if self.ik_bone_controller is not None:  # switch off ik constraint\n                self.ik_bone_constraint.constraints[\"IK\"].influence = 0.\n            self._set_fk_ik_mode(mode=\"fk\")\n\n        else:  # turn off copy rotation constraints of ik bone and base bone\n            if self.get_fk_ik_mode() == \"ik\":\n                return None\n            bpy.context.view_layer.update()\n\n            if keep_pose:\n                self.ik_bone.matrix = self.fk_bone.matrix\n\n            if self.joint_type == \"revolute\":\n                self.bone.constraints[\"copy_rotation.fk\"].influence = 0.\n                self.bone.constraints[\"copy_rotation.ik\"].influence = 1.\n\n            self._set_fk_ik_mode(mode=\"ik\")\n            if self.ik_bone_controller is not None:\n                bpy.context.view_layer.update()\n                location = np.array(self.armature.pose.bones[self.bone.name].head)  # or matrix?\n                print(\"desired ik location\", location)\n                # location = Vector(np.array(mat)[:3, -1])\n                self.set_location_ik(location)\n\n            if self.ik_bone_constraint is not None:\n                self.ik_bone_constraint.constraints[\"IK\"].influence = 1.\n                fk_bone_mat = np.array(self.fk_bone.matrix)\n                fk_bone_mat[:3, -1] = np.array(self.ik_bone_controller.matrix)[:3, -1]\n                self.ik_bone_controller.matrix = Matrix(fk_bone_mat)\n        return None",
  "def get_joint_rotation(self, frame: int = None) -> float:\n        \"\"\" Get current joint rotation based on euler angles.\n\n        :param frame: The desired frame.\n        :return: Current joint rotation in radians.\n        \"\"\"\n\n        if self.bone is None:\n            return 0.0\n\n        pose_bone = self.armature.pose.bones[self.bone.name]\n        data_bone = self.armature.data.bones[self.bone.name]\n\n        with KeyFrame(frame):\n            M_pose = pose_bone.matrix\n            M_data = data_bone.matrix_local\n\n        # grab the parent's world pose and rest matrices\n        if data_bone.parent:\n            M_parent_data = data_bone.parent.matrix_local.copy()\n            M_parent_pose = pose_bone.parent.matrix.copy()\n        else:\n            M_parent_data = Matrix()\n            M_parent_pose = Matrix()\n\n        M1 = M_data.copy()\n        M1.invert()\n\n        M2 = M_parent_pose.copy()\n        M2.invert()\n\n        visual_matrix = M1 @ M_parent_data @ M2 @ M_pose\n\n        return visual_matrix.to_quaternion().angle",
  "def mesh_as_trimesh(self) -> Optional[Trimesh]:\n        \"\"\" Returns a trimesh.Trimesh instance of the link's first visual object, if it exists.\n\n        :return: The link's first visual object as trimesh.Trimesh if the link has one or more visuals, else None.\n        \"\"\"\n        # get mesh data\n        if self.visuals:\n            return self.visuals[0].mesh_as_trimesh()\n\n        return None",
  "class Material(Struct):\n    \"\"\"\n    The material class containing the texture and material properties, which are assigned to the surfaces\n    of MeshObjects.\n    \"\"\"\n\n\n    def __init__(self, material: bpy.types.Material):\n        super().__init__(material)\n        if not material.use_nodes:\n            raise RuntimeError(f\"The given material {material.name} does not have nodes enabled and can \"\n                               f\"therefore not be handled by BlenderProc's Material wrapper class.\")\n\n        self.nodes = material.node_tree.nodes\n        self.links = material.node_tree.links\n\n    def update_blender_ref(self, name):\n        \"\"\" Updates the contained blender reference using the given name of the instance.\n\n        :param name: The name of the instance which will be used to update its blender reference.\n        \"\"\"\n        self.blender_obj = bpy.data.materials[name]\n        self.nodes = bpy.data.materials[name].node_tree.nodes\n        self.links = bpy.data.materials[name].node_tree.links\n\n    def get_users(self) -> int:\n        \"\"\" Returns the number of users of the material.\n\n        :return: The number of users.\n        \"\"\"\n        return self.blender_obj.users\n\n    def duplicate(self) -> \"Material\":\n        \"\"\" Duplicates the material.\n\n        :return: The new material which is a copy of this one.\n        \"\"\"\n        return Material(self.blender_obj.copy())\n\n    def get_the_one_node_with_type(self, node_type: str, created_in_func: str = \"\") -> bpy.types.Node:\n        \"\"\" Returns the one node which is of the given node_type\n\n        This function will only work if there is only one of the nodes of this type.\n\n        :param node_type: The node type to look for.\n        :param created_in_func: only return node created by the specified function\n        :return: The node.\n        \"\"\"\n        return Utility.get_the_one_node_with_type(self.nodes, node_type, created_in_func)\n\n    def get_nodes_with_type(self, node_type: str, created_in_func: str = \"\") -> List[bpy.types.Node]:\n        \"\"\" Returns all nodes which are of the given node_type\n\n        :param node_type: The note type to look for.\n        :param created_in_func: only return nodes created by the specified function\n        :return: The list of nodes with the given type.\n        \"\"\"\n        return Utility.get_nodes_with_type(self.nodes, node_type, created_in_func)\n\n    def get_nodes_created_in_func(self, created_in_func: str) -> List[bpy.types.Node]:\n        \"\"\" Returns all nodes which are of the given node_type\n\n        :param created_in_func: return all nodes created in the given function\n        :return: The list of nodes with the given type.\n        \"\"\"\n        return Utility.get_nodes_created_in_func(self.nodes, created_in_func)\n\n    def new_node(self, node_type: str, created_in_func: str = \"\") -> bpy.types.Node:\n        \"\"\" Creates a new node in the material's node tree.\n\n        :param node_type: The desired type of the new node.\n        :param created_in_func: Save the function name in which this node was created as a custom property.\n                                Allows to later retrieve and delete specific nodes again.\n        :return: The new node.\n        \"\"\"\n        new_node = self.nodes.new(node_type)\n        if created_in_func:\n            new_node[\"created_in_func\"] = created_in_func\n        return new_node\n\n    def remove_node(self, node: bpy.types.Node):\n        \"\"\" Removes the node from the material's node tree.\n\n        :param node: The node to remove.\n        \"\"\"\n        self.nodes.remove(node)\n\n    def insert_node_instead_existing_link(self, source_socket: bpy.types.NodeSocket,\n                                          new_node_dest_socket: bpy.types.NodeSocket,\n                                          new_node_src_socket: bpy.types.NodeSocket,\n                                          dest_socket: bpy.types.NodeSocket):\n        \"\"\" Replaces the node between source_socket and dest_socket with a new node.\n\n        Before: source_socket -> dest_socket\n        After: source_socket -> new_node_dest_socket and new_node_src_socket -> dest_socket\n\n        :param source_socket: The source socket.\n        :param new_node_dest_socket: The new destination for the link starting from source_socket.\n        :param new_node_src_socket: The new source for the link towards dest_socket.\n        :param dest_socket: The destination socket\n        \"\"\"\n        Utility.insert_node_instead_existing_link(self.links, source_socket, new_node_dest_socket, new_node_src_socket,\n                                                  dest_socket)\n\n    def link(self, source_socket: bpy.types.NodeSocket, dest_socket: bpy.types.NodeSocket):\n        \"\"\" Creates a new link between the two given sockets.\n\n        :param source_socket: The source socket.\n        :param dest_socket: The destination socket\n        \"\"\"\n        self.links.new(source_socket, dest_socket)\n\n    def unlink(self, source_socket: bpy.types.NodeSocket, dest_socket: bpy.types.NodeSocket):\n        \"\"\" Removes the link between the two given sockets.\n\n        :param source_socket: The source socket.\n        :param dest_socket: The destination socket\n        \"\"\"\n        self.links.remove(source_socket, dest_socket)\n\n    def map_vertex_color(self, layer_name: str = 'Col', active_shading: bool = True):\n        \"\"\" Maps existing vertex color to the base color of the principled bsdf node or a new background color node.\n\n        :param layer_name: Name of the vertex color layer. Type: string.\n        :param active_shading: Whether to keep the principled bsdf shader. If True, the material properties\n                               influence light reflections such as specularity, roughness, etc. alter the\n                               object's appearance. Type: bool.\n        \"\"\"\n\n        if active_shading:\n            # create new shader node attribute\n            attr_node = self.nodes.new(type='ShaderNodeAttribute')\n            attr_node.attribute_name = layer_name\n            # connect it to base color of principled bsdf\n            principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n            self.links.new(attr_node.outputs['Color'], principled_bsdf.inputs['Base Color'])\n        else:\n            # create new vertex color shade node\n            vcol = self.nodes.new(type=\"ShaderNodeVertexColor\")\n            vcol.layer_name = layer_name\n            result = Utility.get_node_connected_to_the_output_and_unlink_it(self.blender_obj)\n            node_connected_to_output, material_output = result\n            # remove principled bsdf\n            self.nodes.remove(node_connected_to_output)\n            background_color_node = self.nodes.new(type=\"ShaderNodeBackground\")\n            if 'Color' in background_color_node.inputs:\n                self.links.new(vcol.outputs['Color'], background_color_node.inputs['Color'])\n                self.links.new(background_color_node.outputs[\"Background\"], material_output.inputs[\"Surface\"])\n            else:\n                raise RuntimeError(f\"Material '{self.blender_obj.name}' has no node connected to the output, \"\n                                   f\"which has as a 'Base Color' input.\")\n\n    def remove_emissive(self):\n        \"\"\" Remove emissive part of the material.\n        \"\"\"\n        for node in self.get_nodes_created_in_func(self.make_emissive.__name__):\n            self.remove_node(node)\n\n    def make_emissive(self, emission_strength: float, replace: bool = False, emission_color: List[float] = None,\n                      non_emissive_color_socket: bpy.types.NodeSocket = None):\n        \"\"\" Makes the material emit light.\n\n        :param emission_strength: The strength of the emitted light.\n        :param replace: When replace is set to True, the existing material will be completely replaced by the emission\n                        shader, otherwise it still looks the same, while emitting light.\n        :param emission_color: The color of the light to emit. Default: Color of the original object.\n        :param non_emissive_color_socket: An output socket that defines how the material should look like. By default,\n                                          that is the output of the principled shader node. Has no effect if replace\n                                          is set to True.\n        \"\"\"\n        self.remove_emissive()\n\n        output_node = self.get_the_one_node_with_type(\"OutputMaterial\")\n\n        if not replace:\n            mix_node = self.new_node('ShaderNodeMixShader', self.make_emissive.__name__)\n            if non_emissive_color_socket is None:\n                principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n                non_emissive_color_socket = principled_bsdf.outputs['BSDF']\n            self.insert_node_instead_existing_link(non_emissive_color_socket, mix_node.inputs[2],\n                                                   mix_node.outputs['Shader'], output_node.inputs['Surface'])\n\n            # The light path node returns 1, if the material is hit by a ray coming from the camera, else it\n            # returns 0. In this way the mix shader will use the principled shader for rendering the color of\n            # the emitting surface itself, while using the emission shader for lighting the scene.\n            light_path_node = self.new_node('ShaderNodeLightPath', self.make_emissive.__name__)\n            self.link(light_path_node.outputs['Is Camera Ray'], mix_node.inputs['Fac'])\n            output_socket = mix_node.inputs[1]\n        else:\n            output_socket = output_node.inputs['Surface']\n\n        emission_node = self.new_node('ShaderNodeEmission', self.make_emissive.__name__)\n\n        if emission_color is None:\n            principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n            if len(principled_bsdf.inputs[\"Base Color\"].links) == 1:\n                # get the node connected to the Base Color\n                socket_connected_to_the_base_color = principled_bsdf.inputs[\"Base Color\"].links[0].from_socket\n                self.link(socket_connected_to_the_base_color, emission_node.inputs[\"Color\"])\n            else:\n                emission_node.inputs[\"Color\"].default_value = principled_bsdf.inputs[\"Base Color\"].default_value\n        else:\n            emission_node.inputs[\"Color\"].default_value = emission_color\n\n        # set the emission strength of the shader\n        emission_node.inputs['Strength'].default_value = emission_strength\n\n        self.link(emission_node.outputs[\"Emission\"], output_socket)\n\n    def set_principled_shader_value(self, input_name: str, value: Union[float, bpy.types.Image, bpy.types.NodeSocket]):\n        \"\"\" Sets value of an input to the principled shader node.\n\n        :param input_name: The name of the input socket of the principled shader node.\n        :param value: The value to set. Can be a simple value to use as default_value, a socket which will be\n                      connected to the input or an image which will be used for a new TextureNode connected to\n                      the input.\n        \"\"\"\n        principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n\n        if isinstance(value, bpy.types.Image):\n            node = self.new_node('ShaderNodeTexImage')\n            node.label = input_name\n            node.image = value\n            self.link(node.outputs['Color'], principled_bsdf.inputs[input_name])\n        elif isinstance(value, bpy.types.NodeSocket):\n            self.link(value, principled_bsdf.inputs[input_name])\n        else:\n            if principled_bsdf.inputs[input_name].links:\n                self.links.remove(principled_bsdf.inputs[input_name].links[0])\n            principled_bsdf.inputs[input_name].default_value = value\n\n    def get_principled_shader_value(self, input_name: str) -> Union[float, bpy.types.NodeSocket]:\n        \"\"\"\n        Gets the default value or the connected node socket to an input socket of the principled shader\n        node of the material.\n\n        :param input_name: The name of the input socket of the principled shader node.\n        :return: the connected socket to the input socket or the default_value of the given input_name\n        \"\"\"\n        # get the one node from type Principled BSDF\n        principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n        # check if the input name is a valid input\n        if input_name in principled_bsdf.inputs:\n            # check if there are any connections to this input socket\n            if principled_bsdf.inputs[input_name].links:\n                if len(principled_bsdf.inputs[input_name].links) == 1:\n                    # return the connected node\n                    return principled_bsdf.inputs[input_name].links[0].from_socket\n                raise RuntimeError(f\"The input socket has more than one input link: \"\n                                   f\"{[link.from_node.name for link in principled_bsdf.inputs[input_name].links]}\")\n            # else return the default value\n            return principled_bsdf.inputs[input_name].default_value\n        raise RuntimeError(f\"The input name could not be found in the inputs: {input_name}\")\n\n    def get_node_connected_to_the_output_and_unlink_it(self):\n        \"\"\"\n        Searches for the OutputMaterial in the material and finds the connected node to it,\n        removes the connection between this node and the output and returns this node and the material_output\n        \"\"\"\n        material_output = self.get_the_one_node_with_type('OutputMaterial')\n        # find the node, which is connected to the output\n        node_connected_to_the_output = None\n        for link in self.links:\n            if link.to_node == material_output:\n                node_connected_to_the_output = link.from_node\n                # remove this link\n                self.links.remove(link)\n                break\n        return node_connected_to_the_output, material_output\n\n    def infuse_texture(self, texture: bpy.types.Texture, mode: str = \"overlay\", connection: str = \"Base Color\",\n                       texture_scale: float = 0.05, strength: float = 0.5, invert_texture: bool = False):\n        \"\"\" Overlays the selected material with a texture, this can be either a color texture like for example dirt or\n        it can be a texture, which is used as an input to the Principled BSDF of the given material.\n\n        :param texture: A texture which should be infused in the material.\n        :param mode: The mode determines how the texture is used. There are three options: \"overlay\" in which\n                     the selected texture is overlayed over a preexisting one. If there is none, nothing happens.\n                     The second option: \"mix\" is similar to overlay, just that the textures are mixed there.\n                     The last option: \"set\" replaces any existing texture and is even added if there was none before.\n        :param connection: By default the \"Base Color\" input of the principled shader will be used. This can be\n                           changed to any valid input of a principled shader. Default: \"Base Color\". For available\n                           check the blender documentation.\n        :param texture_scale: The used texture can be scaled down or up by a factor, to make it match the\n                              preexisting UV mapping. Make sure that the object has a UV mapping beforehand.\n        :param strength: The strength determines how much the newly generated texture is going to be used.\n        :param invert_texture: It might be sometimes useful to invert the input texture, this can be done by\n                               setting this to True.\n        \"\"\"\n        used_mode = mode.lower()\n        if used_mode not in [\"overlay\", \"mix\", \"set\"]:\n            raise Exception(f'This mode is unknown here: {used_mode}, only [\"overlay\", \"mix\", \"set\"]!')\n\n        used_connector = connection.title()\n\n        principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n        if used_connector not in principled_bsdf.inputs:\n            raise Exception(f\"The {used_connector} is not an input to Principled BSDF!\")\n\n        node_socket_connected_to_the_connector = None\n        for link in principled_bsdf.inputs[used_connector].links:\n            node_socket_connected_to_the_connector = link.from_socket\n            # remove this connection\n            self.links.remove(link)\n        if node_socket_connected_to_the_connector is not None or used_mode == \"set\":\n            texture_node = self.new_node(\"ShaderNodeTexImage\")\n            texture_node.image = texture.image\n            # add texture coords to make the scaling of the dust texture possible\n            texture_coords = self.new_node(\"ShaderNodeTexCoord\")\n            mapping_node = self.new_node(\"ShaderNodeMapping\")\n            mapping_node.vector_type = \"TEXTURE\"\n            mapping_node.inputs[\"Scale\"].default_value = [texture_scale] * 3\n            self.link(texture_coords.outputs[\"UV\"], mapping_node.inputs[\"Vector\"])\n            self.link(mapping_node.outputs[\"Vector\"], texture_node.inputs[\"Vector\"])\n            texture_node_output = texture_node.outputs[\"Color\"]\n            if invert_texture:\n                invert_node = self.new_node(\"ShaderNodeInvert\")\n                invert_node.inputs[\"Fac\"].default_value = 1.0\n                self.link(texture_node_output, invert_node.inputs[\"Color\"])\n                texture_node_output = invert_node.outputs[\"Color\"]\n            if node_socket_connected_to_the_connector is not None and used_mode != \"set\":\n                mix_node = self.new_node(\"ShaderNodeMixRGB\")\n                if used_mode in \"mix_node\":\n                    mix_node.blend_type = \"OVERLAY\"\n                elif used_mode in \"mix\":\n                    mix_node.blend_type = \"MIX\"\n                mix_node.inputs[\"Fac\"].default_value = strength\n                self.link(texture_node_output, mix_node.inputs[\"Color2\"])\n                # hopefully 0 is the color node!\n                self.link(node_socket_connected_to_the_connector, mix_node.inputs[\"Color1\"])\n                self.link(mix_node.outputs[\"Color\"], principled_bsdf.inputs[used_connector])\n            elif used_mode == \"set\":\n                self.link(texture_node_output, principled_bsdf.inputs[used_connector])\n\n    def infuse_material(self, material: \"Material\", mode: str = \"mix\", mix_strength: float = 0.5):\n        \"\"\"\n        Infuse a material inside another material. The given material, will be adapted and the used material, will\n        be added, depending on the mode either as add or as mix. This change is applied to all outputs of the material,\n        this includes the Surface (Color) and also the displacement and volume. For displacement mix means multiply.\n\n        :param material: Material to infuse.\n        :param mode: The mode determines how the two materials are mixed. There are two options \"mix\" in which the\n                     preexisting material is mixed with the selected one in \"used_material\" or \"add\" in which\n                     they are just added on top of each other. Available: [\"mix\", \"add\"]\n        :param mix_strength: In the \"mix\" mode a strength can be set to determine how much of each material is\n                             going to be used. A strength of 1.0 means that the new material is going to be used\n                             completely.\n        \"\"\"\n        # determine the mode\n        used_mode = mode.lower()\n        if used_mode not in [\"add\", \"mix\"]:\n            raise Exception(f'This mode is unknown here: {used_mode}, only [\"mix\", \"add\"]!')\n\n        # move the copied material inside of a group\n        group_node = self.new_node(\"ShaderNodeGroup\")\n        group = BlenderUtility.add_nodes_to_group(material.nodes,\n                                                  f\"{used_mode.title()}_{material.get_name()}\")\n        group_node.node_tree = group\n        # get the current material output and put the used material in between the last node and the material output\n        material_output = self.get_the_one_node_with_type(\"OutputMaterial\")\n        for mat_output_input in material_output.inputs:\n            if len(mat_output_input.links) > 0:\n                if \"Float\" in mat_output_input.bl_idname or \"Vector\" in mat_output_input.bl_idname:\n                    # For displacement\n                    infuse_node = self.new_node(\"ShaderNodeMixRGB\")\n                    if used_mode == \"mix\":\n                        # as there is no mix mode, we use multiply here, which is similar\n                        infuse_node.blend_type = \"MULTIPLY\"\n                        infuse_node.inputs[\"Fac\"].default_value = mix_strength\n                        input_offset = 1\n                    elif used_mode == \"add\":\n                        infuse_node.blend_type = \"ADD\"\n                        input_offset = 0\n                    else:\n                        raise Exception(f\"This mode is not supported here: {used_mode}!\")\n                    infuse_output = infuse_node.outputs[\"Color\"]\n                else:\n                    # for the normal surface output (Color)\n                    if used_mode == \"mix\":\n                        infuse_node = self.new_node('ShaderNodeMixShader')\n                        infuse_node.inputs[0].default_value = mix_strength\n                        input_offset = 1\n                    elif used_mode == \"add\":\n                        infuse_node = self.new_node('ShaderNodeMixShader')\n                        input_offset = 0\n                    else:\n                        raise Exception(f\"This mode is not supported here: {used_mode}!\")\n                    infuse_output = infuse_node.outputs[\"Shader\"]\n\n                # link the infuse node with the correct group node and the material output\n                for link in mat_output_input.links:\n                    self.link(link.from_socket, infuse_node.inputs[input_offset])\n                self.link(group_node.outputs[mat_output_input.name], infuse_node.inputs[input_offset + 1])\n                self.link(infuse_output, mat_output_input)\n\n    def set_displacement_from_principled_shader_value(self, input_name: str, multiply_factor: float):\n        \"\"\" Connects the node that is connected to the specified input of the principled shader node\n        with the displacement output of the material.\n\n        :param input_name: The name of the input socket of the principled shader node.\n        :param multiply_factor: A factor by which the displacement should be multiplied.\n        \"\"\"\n        # Find socket that is connected with the specified input of the principled shader node.\n        input_socket = self.get_principled_shader_value(input_name)\n        if not isinstance(input_socket, bpy.types.NodeSocket):\n            raise Exception(f\"The input {input_name} of the principled shader does not have any incoming connection.\")\n\n        # Create multiplication node and connect with retrieved socket\n        math_node = self.new_node('ShaderNodeMath')\n        math_node.operation = \"MULTIPLY\"\n        math_node.inputs[1].default_value = multiply_factor\n        self.link(input_socket, math_node.inputs[0])\n\n        # Connect multiplication node with displacement output\n        output = self.get_the_one_node_with_type(\"OutputMaterial\")\n        self.link(math_node.outputs[\"Value\"], output.inputs[\"Displacement\"])\n\n    def __setattr__(self, key, value):\n        if key not in [\"links\", \"nodes\", \"blender_obj\"]:\n            raise RuntimeError(\"The API class does not allow setting any attribute. Use the corresponding method or \"\n                               \"directly access the blender attribute via entity.blender_obj.attribute_name\")\n        object.__setattr__(self, key, value)",
  "def __init__(self, material: bpy.types.Material):\n        super().__init__(material)\n        if not material.use_nodes:\n            raise RuntimeError(f\"The given material {material.name} does not have nodes enabled and can \"\n                               f\"therefore not be handled by BlenderProc's Material wrapper class.\")\n\n        self.nodes = material.node_tree.nodes\n        self.links = material.node_tree.links",
  "def update_blender_ref(self, name):\n        \"\"\" Updates the contained blender reference using the given name of the instance.\n\n        :param name: The name of the instance which will be used to update its blender reference.\n        \"\"\"\n        self.blender_obj = bpy.data.materials[name]\n        self.nodes = bpy.data.materials[name].node_tree.nodes\n        self.links = bpy.data.materials[name].node_tree.links",
  "def get_users(self) -> int:\n        \"\"\" Returns the number of users of the material.\n\n        :return: The number of users.\n        \"\"\"\n        return self.blender_obj.users",
  "def duplicate(self) -> \"Material\":\n        \"\"\" Duplicates the material.\n\n        :return: The new material which is a copy of this one.\n        \"\"\"\n        return Material(self.blender_obj.copy())",
  "def get_the_one_node_with_type(self, node_type: str, created_in_func: str = \"\") -> bpy.types.Node:\n        \"\"\" Returns the one node which is of the given node_type\n\n        This function will only work if there is only one of the nodes of this type.\n\n        :param node_type: The node type to look for.\n        :param created_in_func: only return node created by the specified function\n        :return: The node.\n        \"\"\"\n        return Utility.get_the_one_node_with_type(self.nodes, node_type, created_in_func)",
  "def get_nodes_with_type(self, node_type: str, created_in_func: str = \"\") -> List[bpy.types.Node]:\n        \"\"\" Returns all nodes which are of the given node_type\n\n        :param node_type: The note type to look for.\n        :param created_in_func: only return nodes created by the specified function\n        :return: The list of nodes with the given type.\n        \"\"\"\n        return Utility.get_nodes_with_type(self.nodes, node_type, created_in_func)",
  "def get_nodes_created_in_func(self, created_in_func: str) -> List[bpy.types.Node]:\n        \"\"\" Returns all nodes which are of the given node_type\n\n        :param created_in_func: return all nodes created in the given function\n        :return: The list of nodes with the given type.\n        \"\"\"\n        return Utility.get_nodes_created_in_func(self.nodes, created_in_func)",
  "def new_node(self, node_type: str, created_in_func: str = \"\") -> bpy.types.Node:\n        \"\"\" Creates a new node in the material's node tree.\n\n        :param node_type: The desired type of the new node.\n        :param created_in_func: Save the function name in which this node was created as a custom property.\n                                Allows to later retrieve and delete specific nodes again.\n        :return: The new node.\n        \"\"\"\n        new_node = self.nodes.new(node_type)\n        if created_in_func:\n            new_node[\"created_in_func\"] = created_in_func\n        return new_node",
  "def remove_node(self, node: bpy.types.Node):\n        \"\"\" Removes the node from the material's node tree.\n\n        :param node: The node to remove.\n        \"\"\"\n        self.nodes.remove(node)",
  "def insert_node_instead_existing_link(self, source_socket: bpy.types.NodeSocket,\n                                          new_node_dest_socket: bpy.types.NodeSocket,\n                                          new_node_src_socket: bpy.types.NodeSocket,\n                                          dest_socket: bpy.types.NodeSocket):\n        \"\"\" Replaces the node between source_socket and dest_socket with a new node.\n\n        Before: source_socket -> dest_socket\n        After: source_socket -> new_node_dest_socket and new_node_src_socket -> dest_socket\n\n        :param source_socket: The source socket.\n        :param new_node_dest_socket: The new destination for the link starting from source_socket.\n        :param new_node_src_socket: The new source for the link towards dest_socket.\n        :param dest_socket: The destination socket\n        \"\"\"\n        Utility.insert_node_instead_existing_link(self.links, source_socket, new_node_dest_socket, new_node_src_socket,\n                                                  dest_socket)",
  "def link(self, source_socket: bpy.types.NodeSocket, dest_socket: bpy.types.NodeSocket):\n        \"\"\" Creates a new link between the two given sockets.\n\n        :param source_socket: The source socket.\n        :param dest_socket: The destination socket\n        \"\"\"\n        self.links.new(source_socket, dest_socket)",
  "def unlink(self, source_socket: bpy.types.NodeSocket, dest_socket: bpy.types.NodeSocket):\n        \"\"\" Removes the link between the two given sockets.\n\n        :param source_socket: The source socket.\n        :param dest_socket: The destination socket\n        \"\"\"\n        self.links.remove(source_socket, dest_socket)",
  "def map_vertex_color(self, layer_name: str = 'Col', active_shading: bool = True):\n        \"\"\" Maps existing vertex color to the base color of the principled bsdf node or a new background color node.\n\n        :param layer_name: Name of the vertex color layer. Type: string.\n        :param active_shading: Whether to keep the principled bsdf shader. If True, the material properties\n                               influence light reflections such as specularity, roughness, etc. alter the\n                               object's appearance. Type: bool.\n        \"\"\"\n\n        if active_shading:\n            # create new shader node attribute\n            attr_node = self.nodes.new(type='ShaderNodeAttribute')\n            attr_node.attribute_name = layer_name\n            # connect it to base color of principled bsdf\n            principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n            self.links.new(attr_node.outputs['Color'], principled_bsdf.inputs['Base Color'])\n        else:\n            # create new vertex color shade node\n            vcol = self.nodes.new(type=\"ShaderNodeVertexColor\")\n            vcol.layer_name = layer_name\n            result = Utility.get_node_connected_to_the_output_and_unlink_it(self.blender_obj)\n            node_connected_to_output, material_output = result\n            # remove principled bsdf\n            self.nodes.remove(node_connected_to_output)\n            background_color_node = self.nodes.new(type=\"ShaderNodeBackground\")\n            if 'Color' in background_color_node.inputs:\n                self.links.new(vcol.outputs['Color'], background_color_node.inputs['Color'])\n                self.links.new(background_color_node.outputs[\"Background\"], material_output.inputs[\"Surface\"])\n            else:\n                raise RuntimeError(f\"Material '{self.blender_obj.name}' has no node connected to the output, \"\n                                   f\"which has as a 'Base Color' input.\")",
  "def remove_emissive(self):\n        \"\"\" Remove emissive part of the material.\n        \"\"\"\n        for node in self.get_nodes_created_in_func(self.make_emissive.__name__):\n            self.remove_node(node)",
  "def make_emissive(self, emission_strength: float, replace: bool = False, emission_color: List[float] = None,\n                      non_emissive_color_socket: bpy.types.NodeSocket = None):\n        \"\"\" Makes the material emit light.\n\n        :param emission_strength: The strength of the emitted light.\n        :param replace: When replace is set to True, the existing material will be completely replaced by the emission\n                        shader, otherwise it still looks the same, while emitting light.\n        :param emission_color: The color of the light to emit. Default: Color of the original object.\n        :param non_emissive_color_socket: An output socket that defines how the material should look like. By default,\n                                          that is the output of the principled shader node. Has no effect if replace\n                                          is set to True.\n        \"\"\"\n        self.remove_emissive()\n\n        output_node = self.get_the_one_node_with_type(\"OutputMaterial\")\n\n        if not replace:\n            mix_node = self.new_node('ShaderNodeMixShader', self.make_emissive.__name__)\n            if non_emissive_color_socket is None:\n                principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n                non_emissive_color_socket = principled_bsdf.outputs['BSDF']\n            self.insert_node_instead_existing_link(non_emissive_color_socket, mix_node.inputs[2],\n                                                   mix_node.outputs['Shader'], output_node.inputs['Surface'])\n\n            # The light path node returns 1, if the material is hit by a ray coming from the camera, else it\n            # returns 0. In this way the mix shader will use the principled shader for rendering the color of\n            # the emitting surface itself, while using the emission shader for lighting the scene.\n            light_path_node = self.new_node('ShaderNodeLightPath', self.make_emissive.__name__)\n            self.link(light_path_node.outputs['Is Camera Ray'], mix_node.inputs['Fac'])\n            output_socket = mix_node.inputs[1]\n        else:\n            output_socket = output_node.inputs['Surface']\n\n        emission_node = self.new_node('ShaderNodeEmission', self.make_emissive.__name__)\n\n        if emission_color is None:\n            principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n            if len(principled_bsdf.inputs[\"Base Color\"].links) == 1:\n                # get the node connected to the Base Color\n                socket_connected_to_the_base_color = principled_bsdf.inputs[\"Base Color\"].links[0].from_socket\n                self.link(socket_connected_to_the_base_color, emission_node.inputs[\"Color\"])\n            else:\n                emission_node.inputs[\"Color\"].default_value = principled_bsdf.inputs[\"Base Color\"].default_value\n        else:\n            emission_node.inputs[\"Color\"].default_value = emission_color\n\n        # set the emission strength of the shader\n        emission_node.inputs['Strength'].default_value = emission_strength\n\n        self.link(emission_node.outputs[\"Emission\"], output_socket)",
  "def set_principled_shader_value(self, input_name: str, value: Union[float, bpy.types.Image, bpy.types.NodeSocket]):\n        \"\"\" Sets value of an input to the principled shader node.\n\n        :param input_name: The name of the input socket of the principled shader node.\n        :param value: The value to set. Can be a simple value to use as default_value, a socket which will be\n                      connected to the input or an image which will be used for a new TextureNode connected to\n                      the input.\n        \"\"\"\n        principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n\n        if isinstance(value, bpy.types.Image):\n            node = self.new_node('ShaderNodeTexImage')\n            node.label = input_name\n            node.image = value\n            self.link(node.outputs['Color'], principled_bsdf.inputs[input_name])\n        elif isinstance(value, bpy.types.NodeSocket):\n            self.link(value, principled_bsdf.inputs[input_name])\n        else:\n            if principled_bsdf.inputs[input_name].links:\n                self.links.remove(principled_bsdf.inputs[input_name].links[0])\n            principled_bsdf.inputs[input_name].default_value = value",
  "def get_principled_shader_value(self, input_name: str) -> Union[float, bpy.types.NodeSocket]:\n        \"\"\"\n        Gets the default value or the connected node socket to an input socket of the principled shader\n        node of the material.\n\n        :param input_name: The name of the input socket of the principled shader node.\n        :return: the connected socket to the input socket or the default_value of the given input_name\n        \"\"\"\n        # get the one node from type Principled BSDF\n        principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n        # check if the input name is a valid input\n        if input_name in principled_bsdf.inputs:\n            # check if there are any connections to this input socket\n            if principled_bsdf.inputs[input_name].links:\n                if len(principled_bsdf.inputs[input_name].links) == 1:\n                    # return the connected node\n                    return principled_bsdf.inputs[input_name].links[0].from_socket\n                raise RuntimeError(f\"The input socket has more than one input link: \"\n                                   f\"{[link.from_node.name for link in principled_bsdf.inputs[input_name].links]}\")\n            # else return the default value\n            return principled_bsdf.inputs[input_name].default_value\n        raise RuntimeError(f\"The input name could not be found in the inputs: {input_name}\")",
  "def get_node_connected_to_the_output_and_unlink_it(self):\n        \"\"\"\n        Searches for the OutputMaterial in the material and finds the connected node to it,\n        removes the connection between this node and the output and returns this node and the material_output\n        \"\"\"\n        material_output = self.get_the_one_node_with_type('OutputMaterial')\n        # find the node, which is connected to the output\n        node_connected_to_the_output = None\n        for link in self.links:\n            if link.to_node == material_output:\n                node_connected_to_the_output = link.from_node\n                # remove this link\n                self.links.remove(link)\n                break\n        return node_connected_to_the_output, material_output",
  "def infuse_texture(self, texture: bpy.types.Texture, mode: str = \"overlay\", connection: str = \"Base Color\",\n                       texture_scale: float = 0.05, strength: float = 0.5, invert_texture: bool = False):\n        \"\"\" Overlays the selected material with a texture, this can be either a color texture like for example dirt or\n        it can be a texture, which is used as an input to the Principled BSDF of the given material.\n\n        :param texture: A texture which should be infused in the material.\n        :param mode: The mode determines how the texture is used. There are three options: \"overlay\" in which\n                     the selected texture is overlayed over a preexisting one. If there is none, nothing happens.\n                     The second option: \"mix\" is similar to overlay, just that the textures are mixed there.\n                     The last option: \"set\" replaces any existing texture and is even added if there was none before.\n        :param connection: By default the \"Base Color\" input of the principled shader will be used. This can be\n                           changed to any valid input of a principled shader. Default: \"Base Color\". For available\n                           check the blender documentation.\n        :param texture_scale: The used texture can be scaled down or up by a factor, to make it match the\n                              preexisting UV mapping. Make sure that the object has a UV mapping beforehand.\n        :param strength: The strength determines how much the newly generated texture is going to be used.\n        :param invert_texture: It might be sometimes useful to invert the input texture, this can be done by\n                               setting this to True.\n        \"\"\"\n        used_mode = mode.lower()\n        if used_mode not in [\"overlay\", \"mix\", \"set\"]:\n            raise Exception(f'This mode is unknown here: {used_mode}, only [\"overlay\", \"mix\", \"set\"]!')\n\n        used_connector = connection.title()\n\n        principled_bsdf = self.get_the_one_node_with_type(\"BsdfPrincipled\")\n        if used_connector not in principled_bsdf.inputs:\n            raise Exception(f\"The {used_connector} is not an input to Principled BSDF!\")\n\n        node_socket_connected_to_the_connector = None\n        for link in principled_bsdf.inputs[used_connector].links:\n            node_socket_connected_to_the_connector = link.from_socket\n            # remove this connection\n            self.links.remove(link)\n        if node_socket_connected_to_the_connector is not None or used_mode == \"set\":\n            texture_node = self.new_node(\"ShaderNodeTexImage\")\n            texture_node.image = texture.image\n            # add texture coords to make the scaling of the dust texture possible\n            texture_coords = self.new_node(\"ShaderNodeTexCoord\")\n            mapping_node = self.new_node(\"ShaderNodeMapping\")\n            mapping_node.vector_type = \"TEXTURE\"\n            mapping_node.inputs[\"Scale\"].default_value = [texture_scale] * 3\n            self.link(texture_coords.outputs[\"UV\"], mapping_node.inputs[\"Vector\"])\n            self.link(mapping_node.outputs[\"Vector\"], texture_node.inputs[\"Vector\"])\n            texture_node_output = texture_node.outputs[\"Color\"]\n            if invert_texture:\n                invert_node = self.new_node(\"ShaderNodeInvert\")\n                invert_node.inputs[\"Fac\"].default_value = 1.0\n                self.link(texture_node_output, invert_node.inputs[\"Color\"])\n                texture_node_output = invert_node.outputs[\"Color\"]\n            if node_socket_connected_to_the_connector is not None and used_mode != \"set\":\n                mix_node = self.new_node(\"ShaderNodeMixRGB\")\n                if used_mode in \"mix_node\":\n                    mix_node.blend_type = \"OVERLAY\"\n                elif used_mode in \"mix\":\n                    mix_node.blend_type = \"MIX\"\n                mix_node.inputs[\"Fac\"].default_value = strength\n                self.link(texture_node_output, mix_node.inputs[\"Color2\"])\n                # hopefully 0 is the color node!\n                self.link(node_socket_connected_to_the_connector, mix_node.inputs[\"Color1\"])\n                self.link(mix_node.outputs[\"Color\"], principled_bsdf.inputs[used_connector])\n            elif used_mode == \"set\":\n                self.link(texture_node_output, principled_bsdf.inputs[used_connector])",
  "def infuse_material(self, material: \"Material\", mode: str = \"mix\", mix_strength: float = 0.5):\n        \"\"\"\n        Infuse a material inside another material. The given material, will be adapted and the used material, will\n        be added, depending on the mode either as add or as mix. This change is applied to all outputs of the material,\n        this includes the Surface (Color) and also the displacement and volume. For displacement mix means multiply.\n\n        :param material: Material to infuse.\n        :param mode: The mode determines how the two materials are mixed. There are two options \"mix\" in which the\n                     preexisting material is mixed with the selected one in \"used_material\" or \"add\" in which\n                     they are just added on top of each other. Available: [\"mix\", \"add\"]\n        :param mix_strength: In the \"mix\" mode a strength can be set to determine how much of each material is\n                             going to be used. A strength of 1.0 means that the new material is going to be used\n                             completely.\n        \"\"\"\n        # determine the mode\n        used_mode = mode.lower()\n        if used_mode not in [\"add\", \"mix\"]:\n            raise Exception(f'This mode is unknown here: {used_mode}, only [\"mix\", \"add\"]!')\n\n        # move the copied material inside of a group\n        group_node = self.new_node(\"ShaderNodeGroup\")\n        group = BlenderUtility.add_nodes_to_group(material.nodes,\n                                                  f\"{used_mode.title()}_{material.get_name()}\")\n        group_node.node_tree = group\n        # get the current material output and put the used material in between the last node and the material output\n        material_output = self.get_the_one_node_with_type(\"OutputMaterial\")\n        for mat_output_input in material_output.inputs:\n            if len(mat_output_input.links) > 0:\n                if \"Float\" in mat_output_input.bl_idname or \"Vector\" in mat_output_input.bl_idname:\n                    # For displacement\n                    infuse_node = self.new_node(\"ShaderNodeMixRGB\")\n                    if used_mode == \"mix\":\n                        # as there is no mix mode, we use multiply here, which is similar\n                        infuse_node.blend_type = \"MULTIPLY\"\n                        infuse_node.inputs[\"Fac\"].default_value = mix_strength\n                        input_offset = 1\n                    elif used_mode == \"add\":\n                        infuse_node.blend_type = \"ADD\"\n                        input_offset = 0\n                    else:\n                        raise Exception(f\"This mode is not supported here: {used_mode}!\")\n                    infuse_output = infuse_node.outputs[\"Color\"]\n                else:\n                    # for the normal surface output (Color)\n                    if used_mode == \"mix\":\n                        infuse_node = self.new_node('ShaderNodeMixShader')\n                        infuse_node.inputs[0].default_value = mix_strength\n                        input_offset = 1\n                    elif used_mode == \"add\":\n                        infuse_node = self.new_node('ShaderNodeMixShader')\n                        input_offset = 0\n                    else:\n                        raise Exception(f\"This mode is not supported here: {used_mode}!\")\n                    infuse_output = infuse_node.outputs[\"Shader\"]\n\n                # link the infuse node with the correct group node and the material output\n                for link in mat_output_input.links:\n                    self.link(link.from_socket, infuse_node.inputs[input_offset])\n                self.link(group_node.outputs[mat_output_input.name], infuse_node.inputs[input_offset + 1])\n                self.link(infuse_output, mat_output_input)",
  "def set_displacement_from_principled_shader_value(self, input_name: str, multiply_factor: float):\n        \"\"\" Connects the node that is connected to the specified input of the principled shader node\n        with the displacement output of the material.\n\n        :param input_name: The name of the input socket of the principled shader node.\n        :param multiply_factor: A factor by which the displacement should be multiplied.\n        \"\"\"\n        # Find socket that is connected with the specified input of the principled shader node.\n        input_socket = self.get_principled_shader_value(input_name)\n        if not isinstance(input_socket, bpy.types.NodeSocket):\n            raise Exception(f\"The input {input_name} of the principled shader does not have any incoming connection.\")\n\n        # Create multiplication node and connect with retrieved socket\n        math_node = self.new_node('ShaderNodeMath')\n        math_node.operation = \"MULTIPLY\"\n        math_node.inputs[1].default_value = multiply_factor\n        self.link(input_socket, math_node.inputs[0])\n\n        # Connect multiplication node with displacement output\n        output = self.get_the_one_node_with_type(\"OutputMaterial\")\n        self.link(math_node.outputs[\"Value\"], output.inputs[\"Displacement\"])",
  "def __setattr__(self, key, value):\n        if key not in [\"links\", \"nodes\", \"blender_obj\"]:\n            raise RuntimeError(\"The API class does not allow setting any attribute. Use the corresponding method or \"\n                               \"directly access the blender attribute via entity.blender_obj.attribute_name\")\n        object.__setattr__(self, key, value)",
  "def get_armature_from_bone(bone_name: str) -> Optional[MeshObject]:\n    \"\"\" Returns the armature that holds a specified bone.\n\n    :param bone_name: Name of the bone.\n    :return: The respective armature.\n    \"\"\"\n    for obj in bpy.data.objects:\n        if obj.type == \"ARMATURE\":\n            if obj.pose.bones.get(bone_name) is not None:\n                return obj\n    return None",
  "def add_constraint_if_not_existing(bone: bpy.types.PoseBone, constraint_name: str = \"\",\n                                   custom_constraint_name: Optional[str] = None,\n                                   add_to_existing: bool = False) -> Optional[bpy.types.Constraint]:\n    \"\"\" Adds a new constraint.\n\n    :param bone: The bone to add the constraint to.\n    :param constraint_name: Name of the desired constraint.\n    :param custom_constraint_name: Custom name for the constraint. If not specified will use the default name.\n    :param add_to_existing: If true, will add a new constraint even if a constraint of the same type already exists.\n    :return: The created constraint or None if it already exists and `add_to_existing=False`.\n    \"\"\"\n    if custom_constraint_name is None:\n        custom_constraint_name = constraint_name\n    if constraint_name not in bone.constraints.keys() or add_to_existing:\n        c = bone.constraints.new(constraint_name.upper().replace(' ', '_'))\n        c.name = custom_constraint_name\n        return c\n    return None",
  "def set_rotation_constraint(bone: bpy.types.PoseBone, x_limits: Optional[List[float]] = None,\n                            y_limits: Optional[List[float]] = None, z_limits: Optional[List[float]] = None,\n                            set_ik_limits: bool = True):\n    \"\"\" Sets rotation constraints on the armature's bone.\n\n    :param bone: The bone to set the constraint to.\n    :param x_limits: A list of two float values specifying min/max radiant values along the x-axis or None if no\n                     constraint should be applied.\n    :param y_limits: A list of two float values specifying min/max radiant values along the y-axis or None if no\n                     constraint should be applied.\n    :param z_limits: A list of two float values specifying min/max radiant values along the z-axis or None if no\n                     constraint should be applied.\n    :param set_ik_limits: If true will set inverse kinematics constraints based on the allowed rotation axis.\n    \"\"\"\n    if x_limits is None and y_limits is None and z_limits is None:\n        return\n\n    # add new constraint if it doesn't exist\n    constraint = add_constraint_if_not_existing(bone, constraint_name=\"Limit Rotation\")\n\n    if x_limits is not None:\n        constraint.use_limit_x = True\n        constraint.min_x, constraint.max_x = x_limits\n    if y_limits is not None:\n        constraint.use_limit_y = True\n        constraint.min_y, constraint.max_y = y_limits\n    if z_limits is not None:\n        constraint.use_limit_z = True\n        constraint.min_z, constraint.max_z = z_limits\n    constraint.owner_space = \"LOCAL\"\n\n    if set_ik_limits:\n        set_ik_limits_from_rotation_constraint(bone)",
  "def set_ik_limits_from_rotation_constraint(bone: bpy.types.PoseBone,\n                                           constraint: Optional[bpy.types.Constraint] = None):\n    \"\"\" Sets inverse kinematics limits based on a given rotation constraint.\n\n    :param bone: The bone to set the inverse kinematics limits to.\n    :param constraint: The rotation constraint. If None tries to determine it automatically from the bone.\n    \"\"\"\n    if constraint is None:\n        constraint = get_rotation_constraint(bone=bone)\n\n    if constraint is not None:\n        if constraint.use_limit_x:\n            if constraint.min_x == constraint.max_x == 0:\n                bone.lock_ik_x = True\n            else:\n                bone.use_ik_limit_x = True\n                bone.ik_min_x = constraint.min_x\n                bone.ik_max_x = constraint.max_x\n        if constraint.use_limit_y:\n            if constraint.min_y == constraint.max_y == 0:\n                bone.lock_ik_y = True\n            else:\n                bone.use_ik_limit_y = True\n                bone.ik_min_y = constraint.min_y\n                bone.ik_max_y = constraint.max_y\n        if constraint.use_limit_z:\n            if constraint.min_z == constraint.max_z == 0:\n                bone.lock_ik_z = True\n            else:\n                bone.use_ik_limit_z = True\n                bone.ik_min_z = constraint.min_z\n                bone.ik_max_z = constraint.max_z",
  "def copy_constraints(source_bone: bpy.types.PoseBone, target_bone: bpy.types.PoseBone,\n                     constraints_to_be_copied: Optional[List[str]] = None):\n    \"\"\" Copies constraints from one bone to another.\n\n    :param source_bone: The bone holding the constraints to be copied.\n    :param target_bone: The bone where the constraints should be copied to.\n    :param constraints_to_be_copied: A list of constraints to copy if not all constraints should be copied.\n    \"\"\"\n    for c in source_bone.constraints:\n        if constraints_to_be_copied is not None and c.name not in constraints_to_be_copied:\n            continue\n        c_copy = add_constraint_if_not_existing(target_bone, constraint_name=c.name)\n        for prop in dir(c):\n            setattr(c_copy, prop, getattr(c, prop))",
  "def set_ik_constraint(bone: bpy.types.PoseBone, target: bpy.types.Armature, target_bone: str, influence: float = 1.,\n                      use_rotation: bool = True, chain_length: int = 0):\n    \"\"\" Sets an inverse kinematics constraint.\n\n    :param bone: The bone to set the constraint to.\n    :param target: The armature holding the bone.\n    :param target_bone: Name of the target bone which movements shall influence this bone.\n    :param influence: Influence of the constraint.\n    :param use_rotation: Whether to rotate the child links as well. Defaults to True.\n    :param chain_length: The number of parent links which are influenced by this ik bone. Defaults to 0 for all\n                         parents.\n    \"\"\"\n    c = add_constraint_if_not_existing(bone, constraint_name=\"IK\")\n    c.target = target\n    c.subtarget = target_bone\n    c.influence = influence\n    c.use_rotation = use_rotation\n    c.chain_count = chain_length",
  "def set_copy_rotation_constraint(bone: bpy.types.PoseBone, target: bpy.types.PoseBone, target_bone: str,\n                                 custom_constraint_name: Optional[str] = None, influence: float = 1.):\n    \"\"\" Sets a copy_rotation constraint.\n\n    :param bone: The bone to set the constraint to.\n    :param target: The armature holding the bone.\n    :param target_bone: Name of the target bone which rotations shall influence this bone.\n    :param custom_constraint_name: Custom name for the constraint. If not specified will use the default name.\n    :param influence: Influence of the constraint.\n     \"\"\"\n    c = add_constraint_if_not_existing(bone, constraint_name=\"Copy Rotation\",\n                                       custom_constraint_name=custom_constraint_name, add_to_existing=True)\n    c.target = target\n    c.subtarget = target_bone\n    c.influence = influence",
  "def set_copy_location_constraint(bone: bpy.types.PoseBone, target: bpy.types.Armature, target_bone: str,\n                                 custom_constraint_name: Optional[str] = None, influence: float = 1.,\n                                 target_space: str = 'LOCAL', owner_space: str = 'LOCAL'):\n    \"\"\" Sets a copy_location constraint.\n\n    :param bone: The bone to set the constraint to.\n    :param target: The armature holding the bone.\n    :param target_bone: Name of the target bone which locations shall influence this bone.\n    :param custom_constraint_name: Custom name for the constraint. If not specified will use the default name.\n    :param influence: Influence of the constraint.\n    :param target_space: Target space of the constraint.\n    :param owner_space: Owner space of the constraint.\n     \"\"\"\n    c = add_constraint_if_not_existing(bone, constraint_name=\"Copy Location\",\n                                       custom_constraint_name=custom_constraint_name, add_to_existing=True)\n    c.target = target\n    c.subtarget = target_bone\n    c.influence = influence\n    c.target_space = target_space\n    c.owner_space = owner_space",
  "def set_location_constraint(bone: bpy.types.PoseBone, x_limits: Optional[List[float]] = None,\n                            y_limits: Optional[List[float]] = None, z_limits: Optional[List[float]] = None):\n    \"\"\" Sets a location constraint.\n\n    :param bone: The bone to set the constraint to.\n    :param x_limits: A list of two float values specifying min/max values along the x-axis or None if no constraint\n                     should be applied.\n    :param y_limits: A list of two float values specifying min/max values along the y-axis or None if no constraint\n                     should be applied.\n    :param z_limits: A list of two float values specifying min/max values along the z-axis or None if no constraint\n                     should be applied.\n    \"\"\"\n    if x_limits is None and y_limits is None and z_limits is None:\n        return\n\n    # add new constraint if it doesn't exist\n    constraint = add_constraint_if_not_existing(bone, constraint_name=\"Limit Location\")\n\n    if x_limits is not None:\n        constraint.use_min_x = True\n        constraint.use_max_x = True\n        constraint.min_x, constraint.max_x = x_limits\n    if y_limits is not None:\n        constraint.use_min_y = True\n        constraint.use_max_y = True\n        constraint.min_y, constraint.max_y = y_limits\n    if z_limits is not None:\n        constraint.use_min_z = True\n        constraint.use_max_z = True\n        constraint.min_z, constraint.max_z = z_limits\n    constraint.owner_space = \"LOCAL\"",
  "def get_constraint(bone: bpy.types.PoseBone, constraint_name: str = \"\") -> Optional[bpy.types.Constraint]:\n    \"\"\" Returns the desired constraint if existing; otherwise None.\n\n    :param bone: The bone to set the constraint to.\n    :param constraint_name: Name of the constraint.\n    :return: Constraint if it exists; else None.\n    \"\"\"\n    if constraint_name in bone.constraints.keys():\n        return bone.constraints[constraint_name]\n    return None",
  "def get_location_constraint(bone: bpy.types.PoseBone) -> Optional[bpy.types.Constraint]:\n    \"\"\" Returns the location constraint if existing; otherwise None.\n\n    :param bone: The bone to set the constraint to.\n    :return: Location constraint if it exists; else None.\n    \"\"\"\n    return get_constraint(bone, constraint_name=\"Limit Location\")",
  "def get_rotation_constraint(bone: bpy.types.PoseBone) -> Optional[bpy.types.Constraint]:\n    \"\"\" Returns the rotation constraint if existing; otherwise None.\n\n    :param bone: The bone to set the constraint to.\n    :return: Rotation constraint if it exists; else None.\n    \"\"\"\n    return get_constraint(bone, constraint_name=\"Limit Rotation\")",
  "def remove_constraint(bone: bpy.types.PoseBone, constraint_key: str = \"\"):\n    \"\"\" Removes a specified constraint.\n\n    :param bone: The bone to set the constraint to.\n    :param constraint_key: Key to be removed.\n    \"\"\"\n    bone.constraints.remove(bone.constraints[constraint_key])",
  "def remove_constraints(bone: bpy.types.PoseBone):\n    \"\"\" Removes all constraints of the armature.\n\n    :param bone: The bone to set the constraint to.\n    \"\"\"\n    for constraint_key in bone.constraints.keys():\n        remove_constraint(bone=bone, constraint_key=constraint_key)",
  "class Struct:\n    \"\"\"\n    The base class of all things in BlenderProc, this can be an Entity in the scene or a Material which is only applied\n    to a MeshObject.\n    \"\"\"\n\n    # Contains weak refs to all struct instances\n    # As it only uses weak references, instances can still be removed by GC when all other references are gone.\n    # If that happens, the instances' weak ref is also automatically removed from the set\n    __refs__: weakref.WeakSet = weakref.WeakSet()\n\n    def __init__(self, bpy_object: bpy.types.Object):\n        self.blender_obj = bpy_object\n        # Remember that this instance exists\n        Struct.__refs__.add(self)\n\n    def is_valid(self):\n        \"\"\" Check whether the contained blender reference is valid.\n\n        The reference might become invalid after an undo operation or when the referenced struct is deleted.\n\n        :return: True, if it is valid.\n        \"\"\"\n        return str(self.blender_obj) != \"<bpy_struct, Object invalid>\"\n\n    def set_name(self, name: str):\n        \"\"\" Sets the name of the struct.\n\n        :param name: The new name.\n        \"\"\"\n        self.blender_obj.name = name\n\n    def get_name(self) -> str:\n        \"\"\" Returns the name of the struct.\n\n        :return: The name.\n        \"\"\"\n        return self.blender_obj.name\n\n    def get_cp(self, key: str, frame: int = None) -> Any:\n        \"\"\" Returns the custom property with the given key.\n\n        :param key: The key of the custom property.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The value of the custom property.\n        \"\"\"\n        with KeyFrame(frame):\n            value = self.blender_obj[key]\n            if isinstance(value, (Vector, Euler, Color, Matrix, Quaternion)):\n                value = np.array(value)\n            return value\n\n    def set_cp(self, key: str, value: Any, frame: int = None):\n        \"\"\" Sets the custom property with the given key. The key can not be the same as any member over the stored\n        blender object.\n\n        Keyframes can be only set for custom properties for the types int, float or bool.\n\n        :param key: The key of the custom property.\n        :param value: The value to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        if hasattr(self.blender_obj, key):\n            raise ValueError(f\"The given key: {key} is already an attribute of the blender object and can not be \"\n                             f\"used as an custom property, please change the custom property name.\")\n        self.blender_obj[key] = value\n        if isinstance(self.blender_obj[key], (float, int)):\n            Utility.insert_keyframe(self.blender_obj, \"[\\\"\" + key + \"\\\"]\", frame)\n\n    def del_cp(self, key: str):\n        \"\"\" Removes the custom property with the given key.\n\n        :param key: The key of the custom property to remove.\n        \"\"\"\n        del self.blender_obj[key]\n\n    def has_cp(self, key: str) -> bool:\n        \"\"\" Return whether a custom property with the given key exists.\n\n        :param key: The key of the custom property to check.\n        :return: True, if the custom property exists.\n        \"\"\"\n        return key in self.blender_obj\n\n    def get_all_cps(self) -> Dict[str, Any]:\n        \"\"\" Returns all custom properties as key, value pairs.\n\n        :return: A dictionary of custom properties as key, value pairs\n        \"\"\"\n        return dict(self.blender_obj.items())\n\n    def clear_all_cps(self):\n        \"\"\" Removes all existing custom properties the struct has. \"\"\"\n        # iterating over the keys is not possible as deleting them changes the structure of the\n        # underlying blender object -> to solve this we always remove only the first element until no element is left\n        while len(self.blender_obj.keys()) > 0:\n            # extract first element of the keys\n            key = list(self.blender_obj.keys())[0]\n            # delete this first element\n            del self.blender_obj[key]\n\n    def get_attr(self, attr_name: str) -> Any:\n        \"\"\" Returns the value of the attribute with the given name.\n\n        :param attr_name: The name of the attribute.\n        :return: The value of the attribute\n        \"\"\"\n        if hasattr(self.blender_obj, attr_name):\n            value = getattr(self.blender_obj, attr_name)\n            if isinstance(value, (Vector, Euler, Color, Matrix, Quaternion)):\n                value = np.array(value)\n            return value\n        raise ValueError(f\"This element does not have an attribute {attr_name}\")\n\n    def __setattr__(self, key: str, value: Any):\n        if key != \"blender_obj\":\n            raise RuntimeError(\"The API class does not allow setting any attribute. Use the corresponding method or \"\n                               \"directly access the blender attribute via entity.blender_obj.attribute_name\")\n        object.__setattr__(self, key, value)",
  "def __init__(self, bpy_object: bpy.types.Object):\n        self.blender_obj = bpy_object\n        # Remember that this instance exists\n        Struct.__refs__.add(self)",
  "def is_valid(self):\n        \"\"\" Check whether the contained blender reference is valid.\n\n        The reference might become invalid after an undo operation or when the referenced struct is deleted.\n\n        :return: True, if it is valid.\n        \"\"\"\n        return str(self.blender_obj) != \"<bpy_struct, Object invalid>\"",
  "def set_name(self, name: str):\n        \"\"\" Sets the name of the struct.\n\n        :param name: The new name.\n        \"\"\"\n        self.blender_obj.name = name",
  "def get_name(self) -> str:\n        \"\"\" Returns the name of the struct.\n\n        :return: The name.\n        \"\"\"\n        return self.blender_obj.name",
  "def get_cp(self, key: str, frame: int = None) -> Any:\n        \"\"\" Returns the custom property with the given key.\n\n        :param key: The key of the custom property.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The value of the custom property.\n        \"\"\"\n        with KeyFrame(frame):\n            value = self.blender_obj[key]\n            if isinstance(value, (Vector, Euler, Color, Matrix, Quaternion)):\n                value = np.array(value)\n            return value",
  "def set_cp(self, key: str, value: Any, frame: int = None):\n        \"\"\" Sets the custom property with the given key. The key can not be the same as any member over the stored\n        blender object.\n\n        Keyframes can be only set for custom properties for the types int, float or bool.\n\n        :param key: The key of the custom property.\n        :param value: The value to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        if hasattr(self.blender_obj, key):\n            raise ValueError(f\"The given key: {key} is already an attribute of the blender object and can not be \"\n                             f\"used as an custom property, please change the custom property name.\")\n        self.blender_obj[key] = value\n        if isinstance(self.blender_obj[key], (float, int)):\n            Utility.insert_keyframe(self.blender_obj, \"[\\\"\" + key + \"\\\"]\", frame)",
  "def del_cp(self, key: str):\n        \"\"\" Removes the custom property with the given key.\n\n        :param key: The key of the custom property to remove.\n        \"\"\"\n        del self.blender_obj[key]",
  "def has_cp(self, key: str) -> bool:\n        \"\"\" Return whether a custom property with the given key exists.\n\n        :param key: The key of the custom property to check.\n        :return: True, if the custom property exists.\n        \"\"\"\n        return key in self.blender_obj",
  "def get_all_cps(self) -> Dict[str, Any]:\n        \"\"\" Returns all custom properties as key, value pairs.\n\n        :return: A dictionary of custom properties as key, value pairs\n        \"\"\"\n        return dict(self.blender_obj.items())",
  "def clear_all_cps(self):\n        \"\"\" Removes all existing custom properties the struct has. \"\"\"\n        # iterating over the keys is not possible as deleting them changes the structure of the\n        # underlying blender object -> to solve this we always remove only the first element until no element is left\n        while len(self.blender_obj.keys()) > 0:\n            # extract first element of the keys\n            key = list(self.blender_obj.keys())[0]\n            # delete this first element\n            del self.blender_obj[key]",
  "def get_attr(self, attr_name: str) -> Any:\n        \"\"\" Returns the value of the attribute with the given name.\n\n        :param attr_name: The name of the attribute.\n        :return: The value of the attribute\n        \"\"\"\n        if hasattr(self.blender_obj, attr_name):\n            value = getattr(self.blender_obj, attr_name)\n            if isinstance(value, (Vector, Euler, Color, Matrix, Quaternion)):\n                value = np.array(value)\n            return value\n        raise ValueError(f\"This element does not have an attribute {attr_name}\")",
  "def __setattr__(self, key: str, value: Any):\n        if key != \"blender_obj\":\n            raise RuntimeError(\"The API class does not allow setting any attribute. Use the corresponding method or \"\n                               \"directly access the blender attribute via entity.blender_obj.attribute_name\")\n        object.__setattr__(self, key, value)",
  "class MeshObject(Entity):\n    \"\"\"\n    Every instance of this class is a mesh which can be rendered in the scene. It can have multiple materials and\n    different configurations of vertices with faces and edges.\n    \"\"\"\n\n    def get_materials(self) -> List[Optional[Material]]:\n        \"\"\" Returns the materials used by the mesh.\n\n        :return: A list of materials.\n        \"\"\"\n        return MaterialLoaderUtility.convert_to_materials(self.blender_obj.data.materials)\n\n    def has_materials(self) -> bool:\n        \"\"\"\n        Returns True if the object has material slots. This does not necessarily mean any `Material` is assigned to it.\n\n        :return: True if the object has material slots.\n        \"\"\"\n        return len(self.blender_obj.data.materials) > 0\n\n    def set_material(self, index: int, material: Material):\n        \"\"\" Sets the given material at the given index of the objects material list.\n\n        :param index: The index to set the material to.\n        :param material: The material to set.\n        \"\"\"\n        self.blender_obj.data.materials[index] = material.blender_obj\n\n    def add_material(self, material: Material):\n        \"\"\" Adds a new material to the object.\n\n        :param material: The material to add.\n        \"\"\"\n        self.blender_obj.data.materials.append(material.blender_obj)\n\n    def new_material(self, name: str) -> Material:\n        \"\"\" Creates a new material and adds it to the object.\n\n        :param name: The name of the new material.\n        \"\"\"\n        new_mat = MaterialLoaderUtility.create(name)\n        self.add_material(new_mat)\n        return new_mat\n\n    def clear_materials(self):\n        \"\"\" Removes all materials from the object. \"\"\"\n        self.blender_obj.data.materials.clear()\n\n    def replace_materials(self, material: bpy.types.Material):\n        \"\"\" Replaces all materials of the object with the given new material.\n\n        :param material: A material that should exclusively be used as new material for the object.\n        \"\"\"\n        # first remove all existing\n        self.clear_materials()\n        # add the new one\n        self.add_material(material)\n\n    def duplicate(self, duplicate_children: bool = True) -> \"MeshObject\":\n        \"\"\" Duplicates the object.\n\n        :param duplicate_children: If True, also all children objects are recursively duplicated.\n        :return: A new mesh object, which is a duplicate of this object.\n        \"\"\"\n        new_entity = self.blender_obj.copy()\n        new_entity.data = self.blender_obj.data.copy()\n        bpy.context.collection.objects.link(new_entity)\n\n        duplicate_obj = MeshObject(new_entity)\n\n        if duplicate_children:\n            for child in self.get_children():\n                duplicate_child = child.duplicate(duplicate_children=duplicate_children)\n                duplicate_child.set_parent(duplicate_obj)\n\n        return duplicate_obj\n\n    def get_mesh(self) -> bpy.types.Mesh:\n        \"\"\" Returns the blender mesh of the object.\n\n        :return: The mesh.\n        \"\"\"\n        return self.blender_obj.data\n\n    def set_shading_mode(self, mode: str, angle_value: float = 30):\n        \"\"\" Sets the shading mode of all faces of the object.\n\n        :param mode: Desired mode of the shading. Available: [\"FLAT\", \"SMOOTH\", \"AUTO\"]. Type: str\n        :param angle_value: Angle in degrees at which flat shading is activated in `AUTO` mode. Type: float\n        \"\"\"\n        if mode.lower() == \"flat\":\n            is_smooth = False\n            self.blender_obj.data.use_auto_smooth = False\n        elif mode.lower() == \"smooth\":\n            is_smooth = True\n            self.blender_obj.data.use_auto_smooth = False\n        elif mode.lower() == \"auto\":\n            is_smooth = True\n            self.blender_obj.data.use_auto_smooth = True\n            self.blender_obj.data.auto_smooth_angle = np.deg2rad(angle_value)\n        else:\n            raise RuntimeError(f\"This shading mode is unknown: {mode}\")\n\n        for face in self.get_mesh().polygons:\n            face.use_smooth = is_smooth\n\n    def move_origin_to_bottom_mean_point(self):\n        \"\"\"\n        Moves the object center to bottom of the bounding box in Z direction and also in the middle of the X and Y\n        plane, which then makes the placement easier.\n        \"\"\"\n        bpy.ops.object.select_all(action='DESELECT')\n        self.select()\n        bpy.context.view_layer.objects.active = self.blender_obj\n        bb = self.get_bound_box()\n        bb_center = np.mean(bb, axis=0)\n        bb_min_z_value = np.min(bb, axis=0)[2]\n        bpy.ops.object.mode_set(mode='EDIT')\n        bpy.ops.mesh.select_all(action='SELECT')\n        bpy.ops.transform.translate(value=[-bb_center[0], -bb_center[1], -bb_min_z_value])\n        bpy.ops.object.mode_set(mode='OBJECT')\n        self.deselect()\n\n    def get_bound_box(self, local_coords: bool = False) -> np.ndarray:\n        \"\"\"\n        :return: 8x3 array describing the object aligned bounding box coordinates in world coordinates\n        \"\"\"\n        if not local_coords:\n            local2world = Matrix(self.get_local2world_mat())\n            return np.array([local2world @ Vector(cord) for cord in self.blender_obj.bound_box])\n        return np.array([Vector(cord) for cord in self.blender_obj.bound_box])\n\n    def persist_transformation_into_mesh(self, location: bool = True, rotation: bool = True, scale: bool = True):\n        \"\"\"\n        Apply the current transformation of the object, which are saved in the location, scale or rotation attributes\n        to the mesh and sets them to their init values.\n\n        :param location: Determines whether the object's location should be persisted.\n        :param rotation: Determines whether the object's rotation should be persisted.\n        :param scale: Determines whether the object's scale should be persisted.\n        \"\"\"\n        bpy.ops.object.transform_apply({\"selected_editable_objects\": [self.blender_obj]}, location=location,\n                                       rotation=rotation, scale=scale)\n\n    def get_origin(self) -> np.ndarray:\n        \"\"\" Returns the origin of the object.\n\n        :return: The origin in world coordinates.\n        \"\"\"\n        return np.array(self.blender_obj.location.copy())\n\n    def set_origin(self, point: Union[list, np.ndarray, Vector] = None, mode: str = \"POINT\") -> np.ndarray:\n        \"\"\" Sets the origin of the object.\n\n        This will not change the appearing pose of the object, as the vertex locations experience the inverse\n        transformation applied to the origin.\n\n        :param point: The point in world coordinates to which the origin should be set. This parameter is only\n                      relevant if mode is set to \"POINT\".\n        :param mode: The mode specifying how the origin should be set. Available options are: [\"POINT\",\n                     \"CENTER_OF_MASS\", \"CENTER_OF_VOLUME\"]\n        :return: The new origin in world coordinates.\n        \"\"\"\n        context = {\"selected_editable_objects\": [self.blender_obj]}\n\n        if mode == \"POINT\":\n            if point is None:\n                raise Exception(\"The parameter point is not given even though the mode is set to POINT.\")\n            prev_cursor_location = bpy.context.scene.cursor.location.copy()\n            bpy.context.scene.cursor.location = point\n            bpy.ops.object.origin_set(context, type='ORIGIN_CURSOR')\n            bpy.context.scene.cursor.location = prev_cursor_location.copy()\n        elif mode == \"CENTER_OF_MASS\":\n            bpy.ops.object.origin_set(context, type='ORIGIN_CENTER_OF_MASS')\n        elif mode == \"CENTER_OF_VOLUME\":\n            bpy.ops.object.origin_set(context, type='ORIGIN_CENTER_OF_VOLUME')\n        else:\n            raise Exception(\"No such mode: \" + mode)\n\n        return self.get_origin()\n\n    def enable_rigidbody(self, active: bool, collision_shape: str = 'CONVEX_HULL', collision_margin: float = 0.001,\n                         collision_mesh_source: str = \"FINAL\", mass: Optional[float] = None, mass_factor: float = 1,\n                         friction: float = 0.5, angular_damping: float = 0.1, linear_damping: float = 0.04):\n        \"\"\" Enables the rigidbody component of the object which makes it participate in physics simulations.\n\n        :param active: If True, the object actively participates in the simulation and its key frames are ignored.\n                       If False, the object still follows its keyframes and only acts as an obstacle, but is not\n                       influenced by the simulation.\n        :param collision_shape: Collision shape of object in simulation. Default: 'CONVEX_HULL'. Available: 'BOX',\n                                'SPHERE', 'CAPSULE', 'CYLINDER', 'CONE', 'CONVEX_HULL', 'MESH', 'COMPOUND'.\n        :param collision_margin: The margin around objects where collisions are already recognized. Higher values\n                                 improve stability, but also make objects hover a bit.\n        :param collision_mesh_source: Source of the mesh used to create collision shape. Default: 'FINAL'. Available:\n                                      ['BASE', 'DEFORM', 'FINAL'].\n        :param mass: The mass in kilogram the object should have. If None is given the mass is calculated based on\n                     its bounding box volume and the given `mass_factor`.\n        :param mass_factor: Scaling factor for mass. This is only considered if the given `mass` is None. Defines the\n                            linear function mass=bounding_box_volume*mass_factor (defines material density).\n        :param friction: Resistance of object to movement.\n        :param angular_damping: Amount of angular velocity that is lost over time.\n        :param linear_damping: Amount of linear velocity that is lost over time.\n        \"\"\"\n        # Enable rigid body component\n        bpy.ops.rigidbody.object_add({'object': self.blender_obj})\n        # Sett attributes\n        rigid_body = self.blender_obj.rigid_body\n        rigid_body.type = \"ACTIVE\" if active else \"PASSIVE\"\n        rigid_body.collision_shape = collision_shape\n        rigid_body.collision_margin = collision_margin\n        rigid_body.use_margin = True\n        rigid_body.mesh_source = collision_mesh_source\n        rigid_body.friction = friction\n        rigid_body.angular_damping = angular_damping\n        rigid_body.linear_damping = linear_damping\n\n        if mass is None:\n            rigid_body.mass = self.get_bound_box_volume() * mass_factor\n        else:\n            rigid_body.mass = mass\n\n    def build_convex_decomposition_collision_shape(self, vhacd_path: str, temp_dir: Optional[str] = None,\n                                                   cache_dir: str = \"blenderproc_resources/decomposition_cache\"):\n        \"\"\" Builds a collision shape of the object by decomposing it into near convex parts using V-HACD\n\n        :param vhacd_path: The directory in which vhacd should be installed or is already installed.\n        :param temp_dir: The temp dir to use for storing the object files created by v-hacd.\n        :param cache_dir: If a directory is given, convex decompositions are stored there named after the meshes hash.\n                          If the same mesh is decomposed a second time, the result is loaded from the cache and the\n                          actual decomposition is skipped.\n        \"\"\"\n        if platform == \"win32\":\n            raise Exception(\"This is currently not supported under Windows\")\n\n        if temp_dir is None:\n            temp_dir = Utility.get_temporary_directory()\n\n        # Decompose the object\n        parts = convex_decomposition(self, temp_dir, resolve_path(vhacd_path), cache_dir=resolve_path(cache_dir))\n        parts = [MeshObject(p) for p in parts]\n\n        # Make the convex parts children of this object, enable their rigid body component and hide them\n        for part in parts:\n            part.set_parent(self)\n            part.enable_rigidbody(True, \"CONVEX_HULL\")\n            part.hide()\n\n    def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.blender_obj.hide_render = hide_object\n\n    def disable_rigidbody(self):\n        \"\"\" Disables the rigidbody element of the object \"\"\"\n        if self.has_rigidbody_enabled():\n            bpy.ops.rigidbody.object_remove({'object': self.blender_obj})\n        else:\n            warnings.warn(f\"MeshObject {self.get_name()} has no rigid_body component enabled\")\n\n    def has_rigidbody_enabled(self) -> bool:\n        \"\"\" Checks whether object has rigidbody element enabled\n\n        :return: True if object has rigidbody element enabled\n        \"\"\"\n        return self.get_rigidbody() is not None\n\n    def get_rigidbody(self) -> Optional[bpy.types.RigidBodyObject]:\n        \"\"\" Returns the rigid body component\n\n        :return: The rigid body component of the object.\n        \"\"\"\n        return self.blender_obj.rigid_body\n\n    def get_bound_box_volume(self) -> float:\n        \"\"\" Gets the volume of the object aligned bounding box.\n\n        :return: volume of a bounding box.\n        \"\"\"\n        bb = self.get_bound_box()\n        # Search for the point which is the maximum distance away from the first point\n        # we call this first point min and the furthest away point max\n        # the vector between the two is a diagonal of the bounding box\n        min_point, max_point = bb[0], None\n        max_dist = -1\n        for point in bb:\n            dist = np.linalg.norm(point - min_point)\n            if dist > max_dist:\n                max_point = point\n                max_dist = dist\n        diag = max_point - min_point\n        # use the diagonal to calculate the volume of the box\n        return abs(diag[0]) * abs(diag[1]) * abs(diag[2])\n\n    def mesh_as_bmesh(self, return_copy=False) -> bmesh.types.BMesh:\n        \"\"\" Returns a bmesh based on the object's mesh.\n\n        Independent of return_copy, changes to the returned bmesh only take into effect after calling\n        update_from_bmesh().\n\n        :param return_copy: If True, a copy of the objects bmesh will be returned, otherwise the bmesh owned by\n                            blender is returned (the object has to be in edit mode for that).\n        :return: The bmesh\n        \"\"\"\n        if return_copy:\n            bm = bmesh.new()\n            bm.from_mesh(self.get_mesh())\n        else:\n            if bpy.context.mode != \"EDIT_MESH\":\n                raise Exception(f\"The object: {self.get_name()} is not in EDIT mode before calling mesh_as_bmesh()\")\n            bm = bmesh.from_edit_mesh(self.get_mesh())\n        return bm\n\n    def update_from_bmesh(self, bm: bmesh.types.BMesh, free_bm_mesh=True) -> bmesh.types.BMesh:\n        \"\"\" Updates the object's mesh based on the given bmesh.\n\n        :param bm: The bmesh to set.\n        :param free_bm_mesh: If True and the given bmesh is not owned by blender, it will be deleted in the end.\n        \"\"\"\n        # If the bmesh is owned by blender\n        if bm.is_wrapped:\n            # Just tell the mesh to update itself based on its bmesh\n            bmesh.update_edit_mesh(self.get_mesh())\n        else:\n            # Set mesh from bmesh\n            bm.to_mesh(self.get_mesh())\n            # Optional: Free the bmesh\n            if free_bm_mesh:\n                bm.free()\n        # Make sure the mesh is updated\n        self.get_mesh().update()\n\n    def join_with_other_objects(self, objects: List[\"MeshObject\"]):\n        \"\"\"\n            Joins the given list of objects with this object.\n\n            Does not change the global selection.\n            The given object-references become invalid after the join operation.\n\n        :param objects: List of objects which will be merged with this object\n        \"\"\"\n        context = {}\n        # save selection\n        context[\"object\"] = context[\"active_object\"] = self.blender_obj\n        # select all objects which will be merged with the target\n        context[\"selected_objects\"] = context[\"selected_editable_objects\"] = [obj.blender_obj for obj in objects] + \\\n                                                                             [self.blender_obj]\n        # execute the joining operation\n        bpy.ops.object.join(context)\n\n    def edit_mode(self):\n        \"\"\" Switch into edit mode of this mesh object \"\"\"\n        # Make sure we are in object mode\n        if bpy.context.mode != \"OBJECT\":\n            self.object_mode()\n\n        # Set object active (Context overriding does not work for bpy.ops.object.mode_set)\n        bpy.ops.object.select_all(action='DESELECT')\n        bpy.context.view_layer.objects.active = self.blender_obj\n        self.blender_obj.select_set(True)\n        bpy.ops.object.mode_set(mode='EDIT')\n\n    def object_mode(self):\n        \"\"\" Switch back into object mode \"\"\"\n        bpy.ops.object.mode_set(mode='OBJECT')\n\n    def create_bvh_tree(self) -> mathutils.bvhtree.BVHTree:\n        \"\"\" Builds a bvh tree based on the object's mesh.\n\n        :return: The new bvh tree\n        \"\"\"\n        bm = bmesh.new()\n        bm.from_mesh(self.get_mesh())\n        bm.transform(Matrix(self.get_local2world_mat()))\n        bvh_tree = mathutils.bvhtree.BVHTree.FromBMesh(bm)\n        bm.free()\n        return bvh_tree\n\n    def position_is_above_object(self, position: Union[Vector, np.ndarray],\n                                 down_direction: Union[Vector, np.ndarray] = None, check_no_objects_in_between=True):\n        \"\"\" Make sure the given position is straight above the given object.\n\n        If check_no_objects_in_between is True, this also checks that there are no other objects in between.\n\n        :param position: The position to check.\n        :param down_direction: A vector specifying the direction straight down. If None is given, a vector\n                               into -Z direction is used.\n        :param check_no_objects_in_between: If True, it is also checked that no other objects are in between\n                                            position and object.\n        :return: True, if a ray sent into negative z-direction starting from the position hits the object first.\n        \"\"\"\n        if down_direction is None:\n            down_direction = [0, 0, -1]\n\n        if check_no_objects_in_between:\n            # Send a ray straight down and check if the first hit object is the query object\n            hit, _, _, _, hit_object, _ = scene_ray_cast(position, down_direction)\n            return hit and hit_object == self\n        # Compute world-to-local matrix, so we can bring position and down vector into the local coordinate system\n        world2local = Matrix(np.linalg.inv(self.get_local2world_mat()))\n        # Send raycast on object (this will ignore all other objects, so we only need to check whether the ray hit)\n        hit, _, _, _ = self.blender_obj.ray_cast(world2local @ Vector(position),\n                                                 world2local.to_3x3() @ Vector(down_direction))\n        return hit\n\n    def ray_cast(self, origin: Union[Vector, list, np.ndarray], direction: Union[Vector, list, np.ndarray],\n                 max_distance: float = 1.70141e+38) -> Tuple[bool, np.ndarray, np.ndarray, int]:\n        \"\"\" Cast a ray onto evaluated geometry, in object space.\n\n        :param origin: Origin of the ray, in object space.\n        :param direction: Direction of the ray, in object space.\n        :param max_distance: Maximum distance.\n        :return: Whether the ray successfully hit the geometry\n                 The hit location of this ray cast, float array of 3 items in [-inf, inf]\n                 The face normal at the ray cast hit location, float array of 3 items in [-inf, inf]\n                 The face index, -1 when original data isn\u2019t available, int in [-inf, inf]\n        \"\"\"\n        result, location, normal, index = self.blender_obj.ray_cast(Vector(origin), Vector(direction),\n                                                                    distance=max_distance)\n        return result, np.array(location), np.array(normal), index\n\n    def add_uv_mapping(self, projection: str, overwrite: bool = False):\n        \"\"\" Adds a UV mapping to the object based on the given projection type.\n\n        :param projection: The kind of projection to use. Available: [\"cube, \"cylinder\", \"smart\", \"sphere\"].\n        :param overwrite: If True, the uv mapping will be changed, even if the object already has an uv mapping.\n        \"\"\"\n        if not self.has_uv_mapping() or overwrite:\n            self.edit_mode()\n            if projection == \"cube\":\n                bpy.ops.uv.cube_project()\n            elif projection == \"cylinder\":\n                bpy.ops.uv.cylinder_project()\n            elif projection == \"smart\":\n                bpy.ops.uv.smart_project()\n            elif projection == \"sphere\":\n                bpy.ops.uv.sphere_project()\n            else:\n                raise RuntimeError(f\"Unknown projection: '{projection}'. Please use 'cube', 'cylinder', \"\n                                   f\"'smart' or 'sphere'.\")\n            self.object_mode()\n\n    def has_uv_mapping(self):\n        \"\"\" Returns whether the mesh object has a valid uv mapping. \"\"\"\n        if len(self.blender_obj.data.uv_layers) > 1:\n            raise Exception(\"This only support objects which only have one uv layer.\")\n        for layer in self.blender_obj.data.uv_layers:\n            max_val = np.max([list(uv_coords.uv) for uv_coords in layer.data])\n            return max_val > 1e-7\n        return False\n\n    def scale_uv_coordinates(self, factor: float):\n        \"\"\"Scales the UV coordinates of an object by a given factor. Scaling with a factor greater than one has the\n        effect of making the texture look smaller on the object.\n\n        :param factor: The amount the UV coordinates will be scaled.\n        :type factor: float\n        \"\"\"\n        if not self.has_uv_mapping():\n            raise Exception(\"Cannot scale UV coordinates of a MeshObject that has no UV mapping.\")\n\n        mesh = self.blender_obj.data\n        uv_layer = mesh.uv_layers.active\n        for loop in mesh.loops:\n            uv_layer.data[loop.index].uv *= factor\n\n    def add_displace_modifier(self, texture: bpy.types.Texture, mid_level: float = 0.5, strength: float = 0.1,\n                              min_vertices_for_subdiv: int = 10000, subdiv_level: int = 2):\n        \"\"\" Adds a displace modifier with a texture to an object.\n\n        If the mesh has less than min_vertices_for_subdiv vertices, also a subdivision modifier is added.\n\n        :param texture: The texture that will be used to displace the vertices.\n        :param mid_level: Texture value that gives no displacement. Parameter of displace modifier.\n        :param strength: Amount to displace geometry. Parameter of displace modifier.\n        :param min_vertices_for_subdiv: Checks if a subdivision is necessary. If the vertices of a object are less than\n                                        'min_vertices_for_subdiv' a Subdivision modifier will be add to the object.\n        :param subdiv_level:  Numbers of Subdivisions to perform when rendering. Parameter of Subdivision modifier.\n        \"\"\"\n        # Add a subdivision modifier, if the mesh has too less vertices.\n        if not len(self.get_mesh().vertices) > min_vertices_for_subdiv:\n            self.add_modifier(\"SUBSURF\", render_levels=subdiv_level)\n\n        # Add the displacement modifier\n        self.add_modifier(\"DISPLACE\", texture=texture, mid_level=mid_level, strength=strength)\n\n    def add_modifier(self, name: str, **kwargs):\n        \"\"\" Adds a new modifier to the object.\n\n        :param name: The name/type of the modifier to add.\n        :param kwargs: Additional attributes that should be set to the modifier.\n        \"\"\"\n        # Create the new modifier\n        bpy.ops.object.modifier_add({\"object\": self.blender_obj}, type=name)\n        # Set the attributes\n        modifier = self.blender_obj.modifiers[-1]\n        for key, value in kwargs.items():\n            setattr(modifier, key, value)\n\n    def mesh_as_trimesh(self) -> Trimesh:\n        \"\"\" Returns a trimesh.Trimesh instance of the MeshObject.\n\n        :return: The object as trimesh.Trimesh.\n        \"\"\"\n        # get mesh data\n        mesh = self.get_mesh()\n\n        # get vertices\n        verts = np.array([[v.co[0], v.co[1], v.co[2]] for v in mesh.vertices])\n\n        # check if faces are pure tris or quads\n        if not all(len(f.vertices[:]) == len(mesh.polygons[0].vertices[:]) for f in mesh.polygons):\n            raise Exception(f\"The mesh {self.get_name()} must have pure triangular or pure quad faces\")\n\n        # re-scale the vertices since scale operations doesn't apply to the mesh data\n        verts *= self.blender_obj.scale\n\n        # get faces\n        faces = np.array([f.vertices[:] for f in mesh.polygons if len(f.vertices[:]) in [3, 4]])\n\n        return Trimesh(vertices=verts, faces=faces)",
  "def create_from_blender_mesh(blender_mesh: bpy.types.Mesh, object_name: str = None) -> \"MeshObject\":\n    \"\"\" Creates a new Mesh object using the given blender mesh.\n\n    :param blender_mesh: The blender mesh.\n    :param object_name: The name of the new object. If None is given, the name of the given mesh is used.\n    :return: The new Mesh object.\n    \"\"\"\n    # link this mesh inside of a new object\n    obj = bpy.data.objects.new(blender_mesh.name if object_name is None else object_name, blender_mesh)\n    # link the object in the collection\n    bpy.context.collection.objects.link(obj)\n    return MeshObject(obj)",
  "def create_with_empty_mesh(object_name: str, mesh_name: str = None) -> \"MeshObject\":\n    \"\"\" Creates an object with an empty mesh.\n    :param object_name: The name of the new object.\n    :param mesh_name: The name of the contained blender mesh. If None is given, the object name is used.\n    :return: The new Mesh object.\n    \"\"\"\n    if mesh_name is None:\n        mesh_name = object_name\n    return create_from_blender_mesh(bpy.data.meshes.new(mesh_name), object_name)",
  "def create_primitive(shape: str, **kwargs) -> \"MeshObject\":\n    \"\"\" Creates a new primitive mesh object.\n\n    :param shape: The name of the primitive to create. Available: [\"CUBE\", \"CYLINDER\", \"CONE\", \"PLANE\",\n                  \"SPHERE\", \"MONKEY\"]\n    :return: The newly created MeshObject\n    \"\"\"\n    if shape == \"CUBE\":\n        bpy.ops.mesh.primitive_cube_add(**kwargs)\n    elif shape == \"CYLINDER\":\n        bpy.ops.mesh.primitive_cylinder_add(**kwargs)\n    elif shape == \"CONE\":\n        bpy.ops.mesh.primitive_cone_add(**kwargs)\n    elif shape == \"PLANE\":\n        bpy.ops.mesh.primitive_plane_add(**kwargs)\n    elif shape == \"SPHERE\":\n        bpy.ops.mesh.primitive_uv_sphere_add(**kwargs)\n    elif shape == \"MONKEY\":\n        bpy.ops.mesh.primitive_monkey_add(**kwargs)\n    else:\n        raise Exception(\"No such shape: \" + shape)\n\n    primitive = MeshObject(bpy.context.object)\n    # Blender bug: Scale is ignored by default for planes and monkeys.\n    # See https://developer.blender.org/T88047\n    if 'scale' in kwargs and shape in [\"MONKEY\", \"PLANE\"]:\n        primitive.set_scale(kwargs['scale'])\n\n    return primitive",
  "def convert_to_meshes(blender_objects: list) -> List[MeshObject]:\n    \"\"\" Converts the given list of blender objects to mesh objects\n\n    :param blender_objects: List of blender objects.\n    :return: The list of meshes.\n    \"\"\"\n    return [MeshObject(obj) for obj in blender_objects]",
  "def get_all_mesh_objects() -> List[MeshObject]:\n    \"\"\"\n    Returns all mesh objects in scene\n\n    :return: List of all MeshObjects\n    \"\"\"\n    return convert_to_meshes(get_all_blender_mesh_objects())",
  "def disable_all_rigid_bodies():\n    \"\"\" Disables the rigidbody element of all objects \"\"\"\n    for obj in get_all_mesh_objects():\n        if obj.has_rigidbody_enabled():\n            obj.disable_rigidbody()",
  "def create_bvh_tree_multi_objects(mesh_objects: List[MeshObject]) -> mathutils.bvhtree.BVHTree:\n    \"\"\" Creates a bvh tree which contains multiple mesh objects.\n\n    Such a tree is later used for fast raycasting.\n\n    :param mesh_objects: The list of mesh objects that should be put into the BVH tree.\n    :return: The built BVH tree.\n    \"\"\"\n    # Create bmesh which will contain the meshes of all objects\n    bm = bmesh.new()\n    # Go through all mesh objects\n    for obj in mesh_objects:\n        # Add object mesh to bmesh (the newly added vertices will be automatically selected)\n        bm.from_mesh(obj.get_mesh())\n        # Apply world matrix to all selected vertices\n        bm.transform(Matrix(obj.get_local2world_mat()), filter={\"SELECT\"})\n        # Deselect all vertices\n        for v in bm.verts:\n            v.select = False\n\n    # Create tree from bmesh\n    bvh_tree = mathutils.bvhtree.BVHTree.FromBMesh(bm)\n    bm.free()\n    return bvh_tree",
  "def compute_poi(objects: List[MeshObject]) -> np.ndarray:\n    \"\"\" Computes a point of interest in the scene. Point is defined as a location of the one of the selected objects\n    that is the closest one to the mean location of the bboxes of the selected objects.\n\n    :param objects: The list of mesh objects that should be considered.\n    :return: Point of interest in the scene.\n    \"\"\"\n    # Init matrix for all points of all bounding boxes\n    mean_bb_points = []\n\n    for obj in objects:\n        # Get bounding box corners\n        bb_points = obj.get_bound_box()\n        # Compute mean coords of bounding box\n        mean_bb_points.append(np.mean(bb_points, axis=0))\n    # Query point - mean of means\n    mean_bb_point = np.mean(mean_bb_points, axis=0)\n    # Closest point (from means) to query point (mean of means)\n    poi = mean_bb_points[np.argmin(np.linalg.norm(mean_bb_points - mean_bb_point, axis=1))]\n\n    return poi",
  "def scene_ray_cast(origin: Union[Vector, list, np.ndarray], direction: Union[Vector, list, np.ndarray],\n                   max_distance: float = 1.70141e+38) -> Tuple[\n    bool, np.ndarray, np.ndarray, int, MeshObject, np.ndarray]:\n    \"\"\" Cast a ray onto all geometry from the scene, in world space.\n\n   :param origin: Origin of the ray, in world space.\n   :param direction: Direction of the ray, in world space.\n   :param max_distance: Maximum distance.\n   :return: Whether the ray successfully hit any geometry\n            The hit location of this ray cast, float array of 3 items in [-inf, inf]\n            The face normal at the ray cast hit location, float array of 3 items in [-inf, inf]\n            The face index, -1 when original data isn\u2019t available, int in [-inf, inf]\n            If any object has been hit, the MeshObject otherwise None.\n            Some 4x4 matrix.\n   \"\"\"\n    hit, location, normal, index, hit_object, matrix = bpy.context.scene.ray_cast(bpy.context.evaluated_depsgraph_get(),\n                                                                                  Vector(origin), Vector(direction),\n                                                                                  distance=max_distance)\n    if hit_object is not None:\n        hit_object = MeshObject(hit_object)\n    return hit, np.array(location), np.array(normal), index, hit_object, np.array(matrix)",
  "def get_materials(self) -> List[Optional[Material]]:\n        \"\"\" Returns the materials used by the mesh.\n\n        :return: A list of materials.\n        \"\"\"\n        return MaterialLoaderUtility.convert_to_materials(self.blender_obj.data.materials)",
  "def has_materials(self) -> bool:\n        \"\"\"\n        Returns True if the object has material slots. This does not necessarily mean any `Material` is assigned to it.\n\n        :return: True if the object has material slots.\n        \"\"\"\n        return len(self.blender_obj.data.materials) > 0",
  "def set_material(self, index: int, material: Material):\n        \"\"\" Sets the given material at the given index of the objects material list.\n\n        :param index: The index to set the material to.\n        :param material: The material to set.\n        \"\"\"\n        self.blender_obj.data.materials[index] = material.blender_obj",
  "def add_material(self, material: Material):\n        \"\"\" Adds a new material to the object.\n\n        :param material: The material to add.\n        \"\"\"\n        self.blender_obj.data.materials.append(material.blender_obj)",
  "def new_material(self, name: str) -> Material:\n        \"\"\" Creates a new material and adds it to the object.\n\n        :param name: The name of the new material.\n        \"\"\"\n        new_mat = MaterialLoaderUtility.create(name)\n        self.add_material(new_mat)\n        return new_mat",
  "def clear_materials(self):\n        \"\"\" Removes all materials from the object. \"\"\"\n        self.blender_obj.data.materials.clear()",
  "def replace_materials(self, material: bpy.types.Material):\n        \"\"\" Replaces all materials of the object with the given new material.\n\n        :param material: A material that should exclusively be used as new material for the object.\n        \"\"\"\n        # first remove all existing\n        self.clear_materials()\n        # add the new one\n        self.add_material(material)",
  "def duplicate(self, duplicate_children: bool = True) -> \"MeshObject\":\n        \"\"\" Duplicates the object.\n\n        :param duplicate_children: If True, also all children objects are recursively duplicated.\n        :return: A new mesh object, which is a duplicate of this object.\n        \"\"\"\n        new_entity = self.blender_obj.copy()\n        new_entity.data = self.blender_obj.data.copy()\n        bpy.context.collection.objects.link(new_entity)\n\n        duplicate_obj = MeshObject(new_entity)\n\n        if duplicate_children:\n            for child in self.get_children():\n                duplicate_child = child.duplicate(duplicate_children=duplicate_children)\n                duplicate_child.set_parent(duplicate_obj)\n\n        return duplicate_obj",
  "def get_mesh(self) -> bpy.types.Mesh:\n        \"\"\" Returns the blender mesh of the object.\n\n        :return: The mesh.\n        \"\"\"\n        return self.blender_obj.data",
  "def set_shading_mode(self, mode: str, angle_value: float = 30):\n        \"\"\" Sets the shading mode of all faces of the object.\n\n        :param mode: Desired mode of the shading. Available: [\"FLAT\", \"SMOOTH\", \"AUTO\"]. Type: str\n        :param angle_value: Angle in degrees at which flat shading is activated in `AUTO` mode. Type: float\n        \"\"\"\n        if mode.lower() == \"flat\":\n            is_smooth = False\n            self.blender_obj.data.use_auto_smooth = False\n        elif mode.lower() == \"smooth\":\n            is_smooth = True\n            self.blender_obj.data.use_auto_smooth = False\n        elif mode.lower() == \"auto\":\n            is_smooth = True\n            self.blender_obj.data.use_auto_smooth = True\n            self.blender_obj.data.auto_smooth_angle = np.deg2rad(angle_value)\n        else:\n            raise RuntimeError(f\"This shading mode is unknown: {mode}\")\n\n        for face in self.get_mesh().polygons:\n            face.use_smooth = is_smooth",
  "def move_origin_to_bottom_mean_point(self):\n        \"\"\"\n        Moves the object center to bottom of the bounding box in Z direction and also in the middle of the X and Y\n        plane, which then makes the placement easier.\n        \"\"\"\n        bpy.ops.object.select_all(action='DESELECT')\n        self.select()\n        bpy.context.view_layer.objects.active = self.blender_obj\n        bb = self.get_bound_box()\n        bb_center = np.mean(bb, axis=0)\n        bb_min_z_value = np.min(bb, axis=0)[2]\n        bpy.ops.object.mode_set(mode='EDIT')\n        bpy.ops.mesh.select_all(action='SELECT')\n        bpy.ops.transform.translate(value=[-bb_center[0], -bb_center[1], -bb_min_z_value])\n        bpy.ops.object.mode_set(mode='OBJECT')\n        self.deselect()",
  "def get_bound_box(self, local_coords: bool = False) -> np.ndarray:\n        \"\"\"\n        :return: 8x3 array describing the object aligned bounding box coordinates in world coordinates\n        \"\"\"\n        if not local_coords:\n            local2world = Matrix(self.get_local2world_mat())\n            return np.array([local2world @ Vector(cord) for cord in self.blender_obj.bound_box])\n        return np.array([Vector(cord) for cord in self.blender_obj.bound_box])",
  "def persist_transformation_into_mesh(self, location: bool = True, rotation: bool = True, scale: bool = True):\n        \"\"\"\n        Apply the current transformation of the object, which are saved in the location, scale or rotation attributes\n        to the mesh and sets them to their init values.\n\n        :param location: Determines whether the object's location should be persisted.\n        :param rotation: Determines whether the object's rotation should be persisted.\n        :param scale: Determines whether the object's scale should be persisted.\n        \"\"\"\n        bpy.ops.object.transform_apply({\"selected_editable_objects\": [self.blender_obj]}, location=location,\n                                       rotation=rotation, scale=scale)",
  "def get_origin(self) -> np.ndarray:\n        \"\"\" Returns the origin of the object.\n\n        :return: The origin in world coordinates.\n        \"\"\"\n        return np.array(self.blender_obj.location.copy())",
  "def set_origin(self, point: Union[list, np.ndarray, Vector] = None, mode: str = \"POINT\") -> np.ndarray:\n        \"\"\" Sets the origin of the object.\n\n        This will not change the appearing pose of the object, as the vertex locations experience the inverse\n        transformation applied to the origin.\n\n        :param point: The point in world coordinates to which the origin should be set. This parameter is only\n                      relevant if mode is set to \"POINT\".\n        :param mode: The mode specifying how the origin should be set. Available options are: [\"POINT\",\n                     \"CENTER_OF_MASS\", \"CENTER_OF_VOLUME\"]\n        :return: The new origin in world coordinates.\n        \"\"\"\n        context = {\"selected_editable_objects\": [self.blender_obj]}\n\n        if mode == \"POINT\":\n            if point is None:\n                raise Exception(\"The parameter point is not given even though the mode is set to POINT.\")\n            prev_cursor_location = bpy.context.scene.cursor.location.copy()\n            bpy.context.scene.cursor.location = point\n            bpy.ops.object.origin_set(context, type='ORIGIN_CURSOR')\n            bpy.context.scene.cursor.location = prev_cursor_location.copy()\n        elif mode == \"CENTER_OF_MASS\":\n            bpy.ops.object.origin_set(context, type='ORIGIN_CENTER_OF_MASS')\n        elif mode == \"CENTER_OF_VOLUME\":\n            bpy.ops.object.origin_set(context, type='ORIGIN_CENTER_OF_VOLUME')\n        else:\n            raise Exception(\"No such mode: \" + mode)\n\n        return self.get_origin()",
  "def enable_rigidbody(self, active: bool, collision_shape: str = 'CONVEX_HULL', collision_margin: float = 0.001,\n                         collision_mesh_source: str = \"FINAL\", mass: Optional[float] = None, mass_factor: float = 1,\n                         friction: float = 0.5, angular_damping: float = 0.1, linear_damping: float = 0.04):\n        \"\"\" Enables the rigidbody component of the object which makes it participate in physics simulations.\n\n        :param active: If True, the object actively participates in the simulation and its key frames are ignored.\n                       If False, the object still follows its keyframes and only acts as an obstacle, but is not\n                       influenced by the simulation.\n        :param collision_shape: Collision shape of object in simulation. Default: 'CONVEX_HULL'. Available: 'BOX',\n                                'SPHERE', 'CAPSULE', 'CYLINDER', 'CONE', 'CONVEX_HULL', 'MESH', 'COMPOUND'.\n        :param collision_margin: The margin around objects where collisions are already recognized. Higher values\n                                 improve stability, but also make objects hover a bit.\n        :param collision_mesh_source: Source of the mesh used to create collision shape. Default: 'FINAL'. Available:\n                                      ['BASE', 'DEFORM', 'FINAL'].\n        :param mass: The mass in kilogram the object should have. If None is given the mass is calculated based on\n                     its bounding box volume and the given `mass_factor`.\n        :param mass_factor: Scaling factor for mass. This is only considered if the given `mass` is None. Defines the\n                            linear function mass=bounding_box_volume*mass_factor (defines material density).\n        :param friction: Resistance of object to movement.\n        :param angular_damping: Amount of angular velocity that is lost over time.\n        :param linear_damping: Amount of linear velocity that is lost over time.\n        \"\"\"\n        # Enable rigid body component\n        bpy.ops.rigidbody.object_add({'object': self.blender_obj})\n        # Sett attributes\n        rigid_body = self.blender_obj.rigid_body\n        rigid_body.type = \"ACTIVE\" if active else \"PASSIVE\"\n        rigid_body.collision_shape = collision_shape\n        rigid_body.collision_margin = collision_margin\n        rigid_body.use_margin = True\n        rigid_body.mesh_source = collision_mesh_source\n        rigid_body.friction = friction\n        rigid_body.angular_damping = angular_damping\n        rigid_body.linear_damping = linear_damping\n\n        if mass is None:\n            rigid_body.mass = self.get_bound_box_volume() * mass_factor\n        else:\n            rigid_body.mass = mass",
  "def build_convex_decomposition_collision_shape(self, vhacd_path: str, temp_dir: Optional[str] = None,\n                                                   cache_dir: str = \"blenderproc_resources/decomposition_cache\"):\n        \"\"\" Builds a collision shape of the object by decomposing it into near convex parts using V-HACD\n\n        :param vhacd_path: The directory in which vhacd should be installed or is already installed.\n        :param temp_dir: The temp dir to use for storing the object files created by v-hacd.\n        :param cache_dir: If a directory is given, convex decompositions are stored there named after the meshes hash.\n                          If the same mesh is decomposed a second time, the result is loaded from the cache and the\n                          actual decomposition is skipped.\n        \"\"\"\n        if platform == \"win32\":\n            raise Exception(\"This is currently not supported under Windows\")\n\n        if temp_dir is None:\n            temp_dir = Utility.get_temporary_directory()\n\n        # Decompose the object\n        parts = convex_decomposition(self, temp_dir, resolve_path(vhacd_path), cache_dir=resolve_path(cache_dir))\n        parts = [MeshObject(p) for p in parts]\n\n        # Make the convex parts children of this object, enable their rigid body component and hide them\n        for part in parts:\n            part.set_parent(self)\n            part.enable_rigidbody(True, \"CONVEX_HULL\")\n            part.hide()",
  "def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.blender_obj.hide_render = hide_object",
  "def disable_rigidbody(self):\n        \"\"\" Disables the rigidbody element of the object \"\"\"\n        if self.has_rigidbody_enabled():\n            bpy.ops.rigidbody.object_remove({'object': self.blender_obj})\n        else:\n            warnings.warn(f\"MeshObject {self.get_name()} has no rigid_body component enabled\")",
  "def has_rigidbody_enabled(self) -> bool:\n        \"\"\" Checks whether object has rigidbody element enabled\n\n        :return: True if object has rigidbody element enabled\n        \"\"\"\n        return self.get_rigidbody() is not None",
  "def get_rigidbody(self) -> Optional[bpy.types.RigidBodyObject]:\n        \"\"\" Returns the rigid body component\n\n        :return: The rigid body component of the object.\n        \"\"\"\n        return self.blender_obj.rigid_body",
  "def get_bound_box_volume(self) -> float:\n        \"\"\" Gets the volume of the object aligned bounding box.\n\n        :return: volume of a bounding box.\n        \"\"\"\n        bb = self.get_bound_box()\n        # Search for the point which is the maximum distance away from the first point\n        # we call this first point min and the furthest away point max\n        # the vector between the two is a diagonal of the bounding box\n        min_point, max_point = bb[0], None\n        max_dist = -1\n        for point in bb:\n            dist = np.linalg.norm(point - min_point)\n            if dist > max_dist:\n                max_point = point\n                max_dist = dist\n        diag = max_point - min_point\n        # use the diagonal to calculate the volume of the box\n        return abs(diag[0]) * abs(diag[1]) * abs(diag[2])",
  "def mesh_as_bmesh(self, return_copy=False) -> bmesh.types.BMesh:\n        \"\"\" Returns a bmesh based on the object's mesh.\n\n        Independent of return_copy, changes to the returned bmesh only take into effect after calling\n        update_from_bmesh().\n\n        :param return_copy: If True, a copy of the objects bmesh will be returned, otherwise the bmesh owned by\n                            blender is returned (the object has to be in edit mode for that).\n        :return: The bmesh\n        \"\"\"\n        if return_copy:\n            bm = bmesh.new()\n            bm.from_mesh(self.get_mesh())\n        else:\n            if bpy.context.mode != \"EDIT_MESH\":\n                raise Exception(f\"The object: {self.get_name()} is not in EDIT mode before calling mesh_as_bmesh()\")\n            bm = bmesh.from_edit_mesh(self.get_mesh())\n        return bm",
  "def update_from_bmesh(self, bm: bmesh.types.BMesh, free_bm_mesh=True) -> bmesh.types.BMesh:\n        \"\"\" Updates the object's mesh based on the given bmesh.\n\n        :param bm: The bmesh to set.\n        :param free_bm_mesh: If True and the given bmesh is not owned by blender, it will be deleted in the end.\n        \"\"\"\n        # If the bmesh is owned by blender\n        if bm.is_wrapped:\n            # Just tell the mesh to update itself based on its bmesh\n            bmesh.update_edit_mesh(self.get_mesh())\n        else:\n            # Set mesh from bmesh\n            bm.to_mesh(self.get_mesh())\n            # Optional: Free the bmesh\n            if free_bm_mesh:\n                bm.free()\n        # Make sure the mesh is updated\n        self.get_mesh().update()",
  "def join_with_other_objects(self, objects: List[\"MeshObject\"]):\n        \"\"\"\n            Joins the given list of objects with this object.\n\n            Does not change the global selection.\n            The given object-references become invalid after the join operation.\n\n        :param objects: List of objects which will be merged with this object\n        \"\"\"\n        context = {}\n        # save selection\n        context[\"object\"] = context[\"active_object\"] = self.blender_obj\n        # select all objects which will be merged with the target\n        context[\"selected_objects\"] = context[\"selected_editable_objects\"] = [obj.blender_obj for obj in objects] + \\\n                                                                             [self.blender_obj]\n        # execute the joining operation\n        bpy.ops.object.join(context)",
  "def edit_mode(self):\n        \"\"\" Switch into edit mode of this mesh object \"\"\"\n        # Make sure we are in object mode\n        if bpy.context.mode != \"OBJECT\":\n            self.object_mode()\n\n        # Set object active (Context overriding does not work for bpy.ops.object.mode_set)\n        bpy.ops.object.select_all(action='DESELECT')\n        bpy.context.view_layer.objects.active = self.blender_obj\n        self.blender_obj.select_set(True)\n        bpy.ops.object.mode_set(mode='EDIT')",
  "def object_mode(self):\n        \"\"\" Switch back into object mode \"\"\"\n        bpy.ops.object.mode_set(mode='OBJECT')",
  "def create_bvh_tree(self) -> mathutils.bvhtree.BVHTree:\n        \"\"\" Builds a bvh tree based on the object's mesh.\n\n        :return: The new bvh tree\n        \"\"\"\n        bm = bmesh.new()\n        bm.from_mesh(self.get_mesh())\n        bm.transform(Matrix(self.get_local2world_mat()))\n        bvh_tree = mathutils.bvhtree.BVHTree.FromBMesh(bm)\n        bm.free()\n        return bvh_tree",
  "def position_is_above_object(self, position: Union[Vector, np.ndarray],\n                                 down_direction: Union[Vector, np.ndarray] = None, check_no_objects_in_between=True):\n        \"\"\" Make sure the given position is straight above the given object.\n\n        If check_no_objects_in_between is True, this also checks that there are no other objects in between.\n\n        :param position: The position to check.\n        :param down_direction: A vector specifying the direction straight down. If None is given, a vector\n                               into -Z direction is used.\n        :param check_no_objects_in_between: If True, it is also checked that no other objects are in between\n                                            position and object.\n        :return: True, if a ray sent into negative z-direction starting from the position hits the object first.\n        \"\"\"\n        if down_direction is None:\n            down_direction = [0, 0, -1]\n\n        if check_no_objects_in_between:\n            # Send a ray straight down and check if the first hit object is the query object\n            hit, _, _, _, hit_object, _ = scene_ray_cast(position, down_direction)\n            return hit and hit_object == self\n        # Compute world-to-local matrix, so we can bring position and down vector into the local coordinate system\n        world2local = Matrix(np.linalg.inv(self.get_local2world_mat()))\n        # Send raycast on object (this will ignore all other objects, so we only need to check whether the ray hit)\n        hit, _, _, _ = self.blender_obj.ray_cast(world2local @ Vector(position),\n                                                 world2local.to_3x3() @ Vector(down_direction))\n        return hit",
  "def ray_cast(self, origin: Union[Vector, list, np.ndarray], direction: Union[Vector, list, np.ndarray],\n                 max_distance: float = 1.70141e+38) -> Tuple[bool, np.ndarray, np.ndarray, int]:\n        \"\"\" Cast a ray onto evaluated geometry, in object space.\n\n        :param origin: Origin of the ray, in object space.\n        :param direction: Direction of the ray, in object space.\n        :param max_distance: Maximum distance.\n        :return: Whether the ray successfully hit the geometry\n                 The hit location of this ray cast, float array of 3 items in [-inf, inf]\n                 The face normal at the ray cast hit location, float array of 3 items in [-inf, inf]\n                 The face index, -1 when original data isn\u2019t available, int in [-inf, inf]\n        \"\"\"\n        result, location, normal, index = self.blender_obj.ray_cast(Vector(origin), Vector(direction),\n                                                                    distance=max_distance)\n        return result, np.array(location), np.array(normal), index",
  "def add_uv_mapping(self, projection: str, overwrite: bool = False):\n        \"\"\" Adds a UV mapping to the object based on the given projection type.\n\n        :param projection: The kind of projection to use. Available: [\"cube, \"cylinder\", \"smart\", \"sphere\"].\n        :param overwrite: If True, the uv mapping will be changed, even if the object already has an uv mapping.\n        \"\"\"\n        if not self.has_uv_mapping() or overwrite:\n            self.edit_mode()\n            if projection == \"cube\":\n                bpy.ops.uv.cube_project()\n            elif projection == \"cylinder\":\n                bpy.ops.uv.cylinder_project()\n            elif projection == \"smart\":\n                bpy.ops.uv.smart_project()\n            elif projection == \"sphere\":\n                bpy.ops.uv.sphere_project()\n            else:\n                raise RuntimeError(f\"Unknown projection: '{projection}'. Please use 'cube', 'cylinder', \"\n                                   f\"'smart' or 'sphere'.\")\n            self.object_mode()",
  "def has_uv_mapping(self):\n        \"\"\" Returns whether the mesh object has a valid uv mapping. \"\"\"\n        if len(self.blender_obj.data.uv_layers) > 1:\n            raise Exception(\"This only support objects which only have one uv layer.\")\n        for layer in self.blender_obj.data.uv_layers:\n            max_val = np.max([list(uv_coords.uv) for uv_coords in layer.data])\n            return max_val > 1e-7\n        return False",
  "def scale_uv_coordinates(self, factor: float):\n        \"\"\"Scales the UV coordinates of an object by a given factor. Scaling with a factor greater than one has the\n        effect of making the texture look smaller on the object.\n\n        :param factor: The amount the UV coordinates will be scaled.\n        :type factor: float\n        \"\"\"\n        if not self.has_uv_mapping():\n            raise Exception(\"Cannot scale UV coordinates of a MeshObject that has no UV mapping.\")\n\n        mesh = self.blender_obj.data\n        uv_layer = mesh.uv_layers.active\n        for loop in mesh.loops:\n            uv_layer.data[loop.index].uv *= factor",
  "def add_displace_modifier(self, texture: bpy.types.Texture, mid_level: float = 0.5, strength: float = 0.1,\n                              min_vertices_for_subdiv: int = 10000, subdiv_level: int = 2):\n        \"\"\" Adds a displace modifier with a texture to an object.\n\n        If the mesh has less than min_vertices_for_subdiv vertices, also a subdivision modifier is added.\n\n        :param texture: The texture that will be used to displace the vertices.\n        :param mid_level: Texture value that gives no displacement. Parameter of displace modifier.\n        :param strength: Amount to displace geometry. Parameter of displace modifier.\n        :param min_vertices_for_subdiv: Checks if a subdivision is necessary. If the vertices of a object are less than\n                                        'min_vertices_for_subdiv' a Subdivision modifier will be add to the object.\n        :param subdiv_level:  Numbers of Subdivisions to perform when rendering. Parameter of Subdivision modifier.\n        \"\"\"\n        # Add a subdivision modifier, if the mesh has too less vertices.\n        if not len(self.get_mesh().vertices) > min_vertices_for_subdiv:\n            self.add_modifier(\"SUBSURF\", render_levels=subdiv_level)\n\n        # Add the displacement modifier\n        self.add_modifier(\"DISPLACE\", texture=texture, mid_level=mid_level, strength=strength)",
  "def add_modifier(self, name: str, **kwargs):\n        \"\"\" Adds a new modifier to the object.\n\n        :param name: The name/type of the modifier to add.\n        :param kwargs: Additional attributes that should be set to the modifier.\n        \"\"\"\n        # Create the new modifier\n        bpy.ops.object.modifier_add({\"object\": self.blender_obj}, type=name)\n        # Set the attributes\n        modifier = self.blender_obj.modifiers[-1]\n        for key, value in kwargs.items():\n            setattr(modifier, key, value)",
  "def mesh_as_trimesh(self) -> Trimesh:\n        \"\"\" Returns a trimesh.Trimesh instance of the MeshObject.\n\n        :return: The object as trimesh.Trimesh.\n        \"\"\"\n        # get mesh data\n        mesh = self.get_mesh()\n\n        # get vertices\n        verts = np.array([[v.co[0], v.co[1], v.co[2]] for v in mesh.vertices])\n\n        # check if faces are pure tris or quads\n        if not all(len(f.vertices[:]) == len(mesh.polygons[0].vertices[:]) for f in mesh.polygons):\n            raise Exception(f\"The mesh {self.get_name()} must have pure triangular or pure quad faces\")\n\n        # re-scale the vertices since scale operations doesn't apply to the mesh data\n        verts *= self.blender_obj.scale\n\n        # get faces\n        faces = np.array([f.vertices[:] for f in mesh.polygons if len(f.vertices[:]) in [3, 4]])\n\n        return Trimesh(vertices=verts, faces=faces)",
  "class Armature(Entity):\n    \"\"\"\n    An armature object, which can be connected with Link objects to other armature objects.\n\n    It enables the transformation of different objects.\n    \"\"\"\n\n    def __init__(self, bpy_object: bpy.types.Object):\n        super().__init__(bpy_object=bpy_object)\n\n    def set_rotation_euler(self, rotation_euler: Union[float, list, Euler, np.ndarray], frame: Optional[int] = None,\n                           mode: str = \"absolute\"):\n        \"\"\" Rotates the armature based on euler angles. Validate values with given constraints.\n\n        :param rotation_euler: The amount of rotation (in radians). Either three floats for x, y and z axes, or\n                               a single float. In the latter case, the axis of rotation is derived based on the\n                               rotation constraint. If these are not properly set (i.e., two axes must have equal\n                               min/max values) an exception will be thrown.\n        :param frame: Keyframe where to insert the respective rotations.\n        :param mode: One of [\"absolute\", \"relative\"]. For absolute rotations we clip the rotation value based on\n                     the constraints. For relative, we don't - this will result in inverse motion after the\n                     constraint's limits have been reached.\n        \"\"\"\n        assert mode in [\"absolute\", \"relative\"]\n        bpy.ops.object.select_all(action='DESELECT')\n        bone = self.blender_obj.pose.bones.get('Bone')\n        bone.bone.select = True\n        bone.rotation_mode = 'XYZ'\n\n        # in absolute mode we overwrite the rotation values of the armature\n        if mode == \"absolute\":\n            if isinstance(rotation_euler, float):\n                axis = self._determine_rotation_axis()\n                rotation_euler = self._clip_value_from_constraint(value=rotation_euler,\n                                                                  constraint_name=\"Limit Rotation\", axis=axis)\n                current_rotation_euler = bone.rotation_euler\n                current_rotation_euler[[\"X\", \"Y\", \"Z\"].index(axis)] = rotation_euler\n                bone.rotation_euler = current_rotation_euler\n                print(f\"Set rotation_euler of armature {self.get_name()} to {rotation_euler}\")\n            else:\n                bone.rotation_euler = Vector([self._clip_value_from_constraint(value=rot_euler,\n                                                                               constraint_name=\"Limit Rotation\",\n                                                                               axis=axis)\n                                              for rot_euler, axis in zip(rotation_euler, [\"X\", \"Y\", \"Z\"])])\n                print(f\"Set rotation_euler of armature {self.get_name()} to {rotation_euler}\")\n        # in relative mode we add the rotation to the current value\n        elif mode == \"relative\":\n            if isinstance(rotation_euler, float):\n                axis = self._determine_rotation_axis()\n                bone.rotation_euler.rotate_axis(axis, rotation_euler)\n                print(f\"Relatively rotated armature {self.get_name()} around axis {axis} for {rotation_euler} radians\")\n            else:\n                for axis, rotation in zip([\"X\", \"Y\", \"Z\"], rotation_euler):\n                    bone.rotation_euler.rotate_axis(axis, rotation)\n                print(f\"Relatively rotated armature {self.get_name()} for {rotation_euler} radians\")\n\n        Utility.insert_keyframe(bone, \"rotation_euler\", frame)\n        if frame is not None and frame > bpy.context.scene.frame_end:\n            bpy.context.scene.frame_end += 1\n\n    def _determine_rotation_axis(self):\n        \"\"\"\n        Determines the single rotation axis and checks if the constraints are set well to have\n        only one axis of freedom.\n\n        :return: The single rotation axis ('X', 'Y' or 'Z').\n        \"\"\"\n        c = self.get_constraint(constraint_name=\"Limit Rotation\")\n        assert c is not None, \"Tried to determine the single rotation axis but no rotation constraints are set!\"\n\n        axes = ['X', 'Y', 'Z']\n        if c.use_limit_x and c.min_x == c.max_x:\n            axes.pop(axes.index('X'))\n        if c.use_limit_y and c.min_y == c.max_y:\n            axes.pop(axes.index('Y'))\n        if c.use_limit_z and c.min_z == c.max_z:\n            axes.pop(axes.index('Z'))\n        assert len(axes) == 1, f\"Constraints are set wrong for a rotation around a single axis. Only one axis should \" \\\n                               f\"be allowed to move, but found freedom in {len(axes)} axes of armature \" \\\n                               f\"{self.get_name()} (constraint: {c}, uses limits (xyz): \" \\\n                               f\"{c.use_limit_x, c.use_limit_y, c.use_limit_z}, \" \\\n                               f\"values: {c.min_x, c.max_x, c.min_y, c.max_y, c.min_z, c.max_z}).\"\n\n        return axes[0]\n\n    def _clip_value_from_constraint(self, value: float, constraint_name: str, axis: str) -> float:\n        \"\"\"\n        Checks if an axis is constraint, and clips the value to the min/max of this constraint.\n        If the constraint does not exist, nothing is done.\n\n        :param value: Value to be clipped.\n        :param constraint_name: Name of the constraint.\n        :param axis: Axis to check.\n        :return: Clipped value if a constraint is set, else the initial value.\n        \"\"\"\n        c = self.get_constraint(constraint_name=constraint_name)\n        if c is not None:\n            min_value = {\"x\": c.min_x, \"y\": c.min_y, \"z\": c.min_z}[axis.lower()]\n            max_value = {\"x\": c.max_x, \"y\": c.max_y, \"z\": c.max_z}[axis.lower()]\n            print(f\"Clipping {value} to be in range {min_value}, {max_value}\")\n            if value < min_value:\n                return min_value\n            if value > max_value:\n                return max_value\n        return value\n\n    def add_constraint_if_not_existing(self, constraint_name: str) -> bpy.types.Constraint:\n        \"\"\" Adds a new constraint if it doesn't exist, and returns the specified constraint.\n\n        :param constraint_name: Name of the desired constraint.\n        \"\"\"\n        if constraint_name not in self.blender_obj.pose.bones[\"Bone\"].constraints.keys():\n            self.blender_obj.pose.bones[\"Bone\"].constraints.new(constraint_name.upper().replace(' ', '_'))\n        return self.blender_obj.pose.bones[\"Bone\"].constraints[constraint_name]\n\n    def set_rotation_constraint(self, x_limits: Optional[List[float]] = None,\n                                y_limits: Optional[List[float]] = None, z_limits: Optional[List[float]] = None):\n        \"\"\" Sets rotation constraints on the armature's bone.\n\n        :param x_limits: A list of two float values specifying min/max radiant values along the x-axis or\n                         None if no constraint should be applied.\n        :param y_limits: A list of two float values specifying min/max radiant values along the y-axis or\n                         None if no constraint should be applied.\n        :param z_limits: A list of two float values specifying min/max radiant values along the z-axis or\n                         None if no constraint should be applied.\n        \"\"\"\n        if x_limits is None and y_limits is None and z_limits is None:\n            return\n\n        # add new constraint if it doesn't exist\n        constraint = self.add_constraint_if_not_existing(constraint_name=\"Limit Rotation\")\n\n        if x_limits is not None:\n            constraint.use_limit_x = True\n            constraint.min_x, constraint.max_x = x_limits\n        if y_limits is not None:\n            constraint.use_limit_y = True\n            constraint.min_y, constraint.max_y = y_limits\n        if z_limits is not None:\n            constraint.use_limit_z = True\n            constraint.min_z, constraint.max_z = z_limits\n        constraint.owner_space = \"LOCAL\"\n\n    def set_location_constraint(self, x_limits: Optional[List[float]] = None,\n                                y_limits: Optional[List[float]] = None, z_limits: Optional[List[float]] = None):\n        \"\"\" Sets location constraints on the armature's bone.\n\n        :param x_limits: A list of two float values specifying min/max values along the x-axis or\n                         None if no constraint should be applied.\n        :param y_limits: A list of two float values specifying min/max values along the y-axis or\n                         None if no constraint should be applied.\n        :param z_limits: A list of two float values specifying min/max values along the z-axis or\n                         None if no constraint should be applied.\n        \"\"\"\n        if x_limits is None and y_limits is None and z_limits is None:\n            return\n\n        # add new constraint if it doesn't exist\n        constraint = self.add_constraint_if_not_existing(constraint_name=\"Limit Location\")\n\n        if x_limits is not None:\n            constraint.use_min_x = True\n            constraint.use_max_x = True\n            constraint.min_x, constraint.max_x = x_limits\n        if y_limits is not None:\n            constraint.use_min_y = True\n            constraint.use_max_y = True\n            constraint.min_y, constraint.max_y = y_limits\n        if z_limits is not None:\n            constraint.use_min_z = True\n            constraint.use_max_z = True\n            constraint.min_z, constraint.max_z = z_limits\n        constraint.owner_space = \"LOCAL\"\n\n    def get_constraint(self, constraint_name: str) -> Optional[bpy.types.Constraint]:\n        \"\"\" Returns the desired constraint if existing; otherwise None.\n\n        :param constraint_name: Name of the constraint.\n        :return: Constraint if it exists; else None.\n        \"\"\"\n        if constraint_name in self.blender_obj.pose.bones[\"Bone\"].constraints.keys():\n            return self.blender_obj.pose.bones[\"Bone\"].constraints[constraint_name]\n        return None\n\n    def get_location_constraint(self) -> Optional[bpy.types.Constraint]:\n        \"\"\" Returns the location constraint if existing; otherwise None.\n\n        :return: Location constraint if it exists; else None.\n        \"\"\"\n        return self.get_constraint(constraint_name=\"Limit Location\")\n\n    def get_rotation_constraint(self) -> Optional[bpy.types.Constraint]:\n        \"\"\" Returns the rotation constraint if existing; otherwise None.\n\n        :return: Rotation constraint if it exists; else None.\n        \"\"\"\n        return self.get_constraint(constraint_name=\"Limit Rotation\")\n\n    def remove_constraint(self, constraint_key: str):\n        \"\"\" Removes a specified constraint.\n\n        :param constraint_key: Key to be removed.\n        \"\"\"\n        bone = self.blender_obj.pose.bones[\"Bone\"]\n        bone.constraints.remove(bone.constraints[constraint_key])\n\n    def remove_constraints(self):\n        \"\"\" Removes all constraints of the armature. \"\"\"\n        bone = self.blender_obj.pose.bones[\"Bone\"]\n        for constraint_key in bone.constraints.keys():\n            self.remove_constraint(constraint_key=constraint_key)\n\n    def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.blender_obj.hide_render = hide_object",
  "def __init__(self, bpy_object: bpy.types.Object):\n        super().__init__(bpy_object=bpy_object)",
  "def set_rotation_euler(self, rotation_euler: Union[float, list, Euler, np.ndarray], frame: Optional[int] = None,\n                           mode: str = \"absolute\"):\n        \"\"\" Rotates the armature based on euler angles. Validate values with given constraints.\n\n        :param rotation_euler: The amount of rotation (in radians). Either three floats for x, y and z axes, or\n                               a single float. In the latter case, the axis of rotation is derived based on the\n                               rotation constraint. If these are not properly set (i.e., two axes must have equal\n                               min/max values) an exception will be thrown.\n        :param frame: Keyframe where to insert the respective rotations.\n        :param mode: One of [\"absolute\", \"relative\"]. For absolute rotations we clip the rotation value based on\n                     the constraints. For relative, we don't - this will result in inverse motion after the\n                     constraint's limits have been reached.\n        \"\"\"\n        assert mode in [\"absolute\", \"relative\"]\n        bpy.ops.object.select_all(action='DESELECT')\n        bone = self.blender_obj.pose.bones.get('Bone')\n        bone.bone.select = True\n        bone.rotation_mode = 'XYZ'\n\n        # in absolute mode we overwrite the rotation values of the armature\n        if mode == \"absolute\":\n            if isinstance(rotation_euler, float):\n                axis = self._determine_rotation_axis()\n                rotation_euler = self._clip_value_from_constraint(value=rotation_euler,\n                                                                  constraint_name=\"Limit Rotation\", axis=axis)\n                current_rotation_euler = bone.rotation_euler\n                current_rotation_euler[[\"X\", \"Y\", \"Z\"].index(axis)] = rotation_euler\n                bone.rotation_euler = current_rotation_euler\n                print(f\"Set rotation_euler of armature {self.get_name()} to {rotation_euler}\")\n            else:\n                bone.rotation_euler = Vector([self._clip_value_from_constraint(value=rot_euler,\n                                                                               constraint_name=\"Limit Rotation\",\n                                                                               axis=axis)\n                                              for rot_euler, axis in zip(rotation_euler, [\"X\", \"Y\", \"Z\"])])\n                print(f\"Set rotation_euler of armature {self.get_name()} to {rotation_euler}\")\n        # in relative mode we add the rotation to the current value\n        elif mode == \"relative\":\n            if isinstance(rotation_euler, float):\n                axis = self._determine_rotation_axis()\n                bone.rotation_euler.rotate_axis(axis, rotation_euler)\n                print(f\"Relatively rotated armature {self.get_name()} around axis {axis} for {rotation_euler} radians\")\n            else:\n                for axis, rotation in zip([\"X\", \"Y\", \"Z\"], rotation_euler):\n                    bone.rotation_euler.rotate_axis(axis, rotation)\n                print(f\"Relatively rotated armature {self.get_name()} for {rotation_euler} radians\")\n\n        Utility.insert_keyframe(bone, \"rotation_euler\", frame)\n        if frame is not None and frame > bpy.context.scene.frame_end:\n            bpy.context.scene.frame_end += 1",
  "def _determine_rotation_axis(self):\n        \"\"\"\n        Determines the single rotation axis and checks if the constraints are set well to have\n        only one axis of freedom.\n\n        :return: The single rotation axis ('X', 'Y' or 'Z').\n        \"\"\"\n        c = self.get_constraint(constraint_name=\"Limit Rotation\")\n        assert c is not None, \"Tried to determine the single rotation axis but no rotation constraints are set!\"\n\n        axes = ['X', 'Y', 'Z']\n        if c.use_limit_x and c.min_x == c.max_x:\n            axes.pop(axes.index('X'))\n        if c.use_limit_y and c.min_y == c.max_y:\n            axes.pop(axes.index('Y'))\n        if c.use_limit_z and c.min_z == c.max_z:\n            axes.pop(axes.index('Z'))\n        assert len(axes) == 1, f\"Constraints are set wrong for a rotation around a single axis. Only one axis should \" \\\n                               f\"be allowed to move, but found freedom in {len(axes)} axes of armature \" \\\n                               f\"{self.get_name()} (constraint: {c}, uses limits (xyz): \" \\\n                               f\"{c.use_limit_x, c.use_limit_y, c.use_limit_z}, \" \\\n                               f\"values: {c.min_x, c.max_x, c.min_y, c.max_y, c.min_z, c.max_z}).\"\n\n        return axes[0]",
  "def _clip_value_from_constraint(self, value: float, constraint_name: str, axis: str) -> float:\n        \"\"\"\n        Checks if an axis is constraint, and clips the value to the min/max of this constraint.\n        If the constraint does not exist, nothing is done.\n\n        :param value: Value to be clipped.\n        :param constraint_name: Name of the constraint.\n        :param axis: Axis to check.\n        :return: Clipped value if a constraint is set, else the initial value.\n        \"\"\"\n        c = self.get_constraint(constraint_name=constraint_name)\n        if c is not None:\n            min_value = {\"x\": c.min_x, \"y\": c.min_y, \"z\": c.min_z}[axis.lower()]\n            max_value = {\"x\": c.max_x, \"y\": c.max_y, \"z\": c.max_z}[axis.lower()]\n            print(f\"Clipping {value} to be in range {min_value}, {max_value}\")\n            if value < min_value:\n                return min_value\n            if value > max_value:\n                return max_value\n        return value",
  "def add_constraint_if_not_existing(self, constraint_name: str) -> bpy.types.Constraint:\n        \"\"\" Adds a new constraint if it doesn't exist, and returns the specified constraint.\n\n        :param constraint_name: Name of the desired constraint.\n        \"\"\"\n        if constraint_name not in self.blender_obj.pose.bones[\"Bone\"].constraints.keys():\n            self.blender_obj.pose.bones[\"Bone\"].constraints.new(constraint_name.upper().replace(' ', '_'))\n        return self.blender_obj.pose.bones[\"Bone\"].constraints[constraint_name]",
  "def set_rotation_constraint(self, x_limits: Optional[List[float]] = None,\n                                y_limits: Optional[List[float]] = None, z_limits: Optional[List[float]] = None):\n        \"\"\" Sets rotation constraints on the armature's bone.\n\n        :param x_limits: A list of two float values specifying min/max radiant values along the x-axis or\n                         None if no constraint should be applied.\n        :param y_limits: A list of two float values specifying min/max radiant values along the y-axis or\n                         None if no constraint should be applied.\n        :param z_limits: A list of two float values specifying min/max radiant values along the z-axis or\n                         None if no constraint should be applied.\n        \"\"\"\n        if x_limits is None and y_limits is None and z_limits is None:\n            return\n\n        # add new constraint if it doesn't exist\n        constraint = self.add_constraint_if_not_existing(constraint_name=\"Limit Rotation\")\n\n        if x_limits is not None:\n            constraint.use_limit_x = True\n            constraint.min_x, constraint.max_x = x_limits\n        if y_limits is not None:\n            constraint.use_limit_y = True\n            constraint.min_y, constraint.max_y = y_limits\n        if z_limits is not None:\n            constraint.use_limit_z = True\n            constraint.min_z, constraint.max_z = z_limits\n        constraint.owner_space = \"LOCAL\"",
  "def set_location_constraint(self, x_limits: Optional[List[float]] = None,\n                                y_limits: Optional[List[float]] = None, z_limits: Optional[List[float]] = None):\n        \"\"\" Sets location constraints on the armature's bone.\n\n        :param x_limits: A list of two float values specifying min/max values along the x-axis or\n                         None if no constraint should be applied.\n        :param y_limits: A list of two float values specifying min/max values along the y-axis or\n                         None if no constraint should be applied.\n        :param z_limits: A list of two float values specifying min/max values along the z-axis or\n                         None if no constraint should be applied.\n        \"\"\"\n        if x_limits is None and y_limits is None and z_limits is None:\n            return\n\n        # add new constraint if it doesn't exist\n        constraint = self.add_constraint_if_not_existing(constraint_name=\"Limit Location\")\n\n        if x_limits is not None:\n            constraint.use_min_x = True\n            constraint.use_max_x = True\n            constraint.min_x, constraint.max_x = x_limits\n        if y_limits is not None:\n            constraint.use_min_y = True\n            constraint.use_max_y = True\n            constraint.min_y, constraint.max_y = y_limits\n        if z_limits is not None:\n            constraint.use_min_z = True\n            constraint.use_max_z = True\n            constraint.min_z, constraint.max_z = z_limits\n        constraint.owner_space = \"LOCAL\"",
  "def get_constraint(self, constraint_name: str) -> Optional[bpy.types.Constraint]:\n        \"\"\" Returns the desired constraint if existing; otherwise None.\n\n        :param constraint_name: Name of the constraint.\n        :return: Constraint if it exists; else None.\n        \"\"\"\n        if constraint_name in self.blender_obj.pose.bones[\"Bone\"].constraints.keys():\n            return self.blender_obj.pose.bones[\"Bone\"].constraints[constraint_name]\n        return None",
  "def get_location_constraint(self) -> Optional[bpy.types.Constraint]:\n        \"\"\" Returns the location constraint if existing; otherwise None.\n\n        :return: Location constraint if it exists; else None.\n        \"\"\"\n        return self.get_constraint(constraint_name=\"Limit Location\")",
  "def get_rotation_constraint(self) -> Optional[bpy.types.Constraint]:\n        \"\"\" Returns the rotation constraint if existing; otherwise None.\n\n        :return: Rotation constraint if it exists; else None.\n        \"\"\"\n        return self.get_constraint(constraint_name=\"Limit Rotation\")",
  "def remove_constraint(self, constraint_key: str):\n        \"\"\" Removes a specified constraint.\n\n        :param constraint_key: Key to be removed.\n        \"\"\"\n        bone = self.blender_obj.pose.bones[\"Bone\"]\n        bone.constraints.remove(bone.constraints[constraint_key])",
  "def remove_constraints(self):\n        \"\"\" Removes all constraints of the armature. \"\"\"\n        bone = self.blender_obj.pose.bones[\"Bone\"]\n        for constraint_key in bone.constraints.keys():\n            self.remove_constraint(constraint_key=constraint_key)",
  "def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.blender_obj.hide_render = hide_object",
  "class URDFObject(Entity):\n    \"\"\"\n    This class represents an URDF object, which is comprised of an armature and one or multiple links. Among others, it\n    serves as an interface for manipulation of the URDF model.\n    \"\"\"\n    def __init__(self, armature: bpy.types.Armature, links: List[Link], xml_tree: Optional[\"urdfpy.URDF\"] = None):\n        super().__init__(bpy_object=armature)\n\n        object.__setattr__(self, \"links\", links)\n        object.__setattr__(self, \"xml_tree\", xml_tree)\n        object.__setattr__(self, \"ik_bone_constraint\", None)\n        object.__setattr__(self, \"ik_bone_controller\", None)\n        object.__setattr__(self, \"fk_ik_mode\", None)\n        object.__setattr__(self, \"ik_link\", None)\n        object.__setattr__(self, 'ik_bone_offset', None)\n\n    def get_all_urdf_objs(self) -> List[Union[Link, Inertial, MeshObject]]:\n        \"\"\" Returns a list of all urdf-related objects.\n\n        :return: List of all urdf-related objects.\n        \"\"\"\n        objs = []\n        for link in self.links:\n            objs.append(link)\n            objs.extend(link.get_all_objs())\n        return objs\n\n    def get_all_collision_objs(self) -> List[MeshObject]:\n        \"\"\" Returns a list of all collision objects.\n\n        :return: List of all collision objects.\n        \"\"\"\n        return [obj for obj in self.get_all_urdf_objs() if 'collision' in obj.get_name()]\n\n    def get_all_inertial_objs(self) -> List[Inertial]:\n        \"\"\" Returns a list of all inertial objects.\n\n        :return: List of all inertial objects.\n        \"\"\"\n        return [obj for obj in self.get_all_urdf_objs() if isinstance(obj, Inertial)]\n\n    def get_all_visual_objs(self) -> List[MeshObject]:\n        \"\"\" Returns a list of all visual objects.\n\n        :return: List of all visual objects.\n        \"\"\"\n        return [obj for obj in self.get_all_urdf_objs() if 'visual' in obj.get_name()]\n\n    def hide_links_and_collision_inertial_objs(self):\n        \"\"\" Hides links and their respective collision and inertial objects from rendering. \"\"\"\n        self.blender_obj.hide_set(True)\n        for link in self.links:\n            for obj in link.get_all_objs():\n                if \"collision\" in obj.get_name() or \"inertial\" in obj.get_name():\n                    obj.hide()\n\n    def set_ascending_category_ids(self, category_ids: Optional[List[int]] = None):\n        \"\"\" Sets semantic categories to the links and their associated objects.\n\n        :param category_ids: List of 'category_id's for every link. If None, will create a list from [1 ... len(links)].\n        \"\"\"\n        if category_ids is None:\n            category_ids = list(range(1, len(self.links) + 1))\n\n        assert len(category_ids) == len(self.links), f\"Need equal amount of category ids for links. Got \" \\\n                                                     f\"{len(category_ids)} and {len(self.links)}, respectively.\"\n        for link, category_id in zip(self.links, category_ids):\n            link.set_cp(key=\"category_id\", value=category_id)\n            for obj in link.get_all_objs():\n                obj.set_cp(key=\"category_id\", value=category_id)\n\n    def remove_link_by_index(self, index: int = 0):\n        \"\"\" Removes a link and all its associated objects given an index. Also handles relationship of the link's child\n            with its parent. This is useful for removing a 'world link' which could be a simple flat surface, or if\n            someone wants to shorten the whole urdf object.\n\n        :param index: Index of the joint to be removed.\n        \"\"\"\n        assert index < len(self.links), f\"Invalid index {index}. Index must be in range 0, {len(self.links)} (no. \" \\\n                                        f\"links: {len(self.links)}).\"\n\n        # remove link from the urdf instance and determine child / parent\n        link_to_be_removed = self.links.pop(index)\n        child = link_to_be_removed.get_link_child()\n\n        # remove bones and assign old bone pose to child bone\n        if child is not None and link_to_be_removed.bone is not None:\n            print(f'Trying to put {child.get_name()} to position of {link_to_be_removed.get_name()}')\n            bpy.context.view_layer.update()\n            bpy.ops.object.select_all(action='DESELECT')\n            bpy.context.view_layer.objects.active = self.blender_obj\n            bpy.ops.object.mode_set(mode='EDIT', toggle=False)\n            edit_bones = self.blender_obj.data.edit_bones\n            offset = edit_bones[child.bone.name].head - edit_bones[link_to_be_removed.bone.name].head\n\n            edit_bones[child.bone.name].head -= offset\n            edit_bones[child.bone.name].tail -= offset\n            edit_bones[child.fk_bone.name].head -= offset\n            edit_bones[child.fk_bone.name].tail -= offset\n            edit_bones[child.ik_bone.name].head -= offset\n            edit_bones[child.ik_bone.name].tail -= offset\n\n            grand_child = child.get_link_child()\n            while grand_child is not None:\n                edit_bones[grand_child.bone.name].head -= offset\n                edit_bones[grand_child.bone.name].tail -= offset\n                edit_bones[grand_child.fk_bone.name].head -= offset\n                edit_bones[grand_child.fk_bone.name].tail -= offset\n                edit_bones[grand_child.ik_bone.name].head -= offset\n                edit_bones[grand_child.ik_bone.name].tail -= offset\n                grand_child = grand_child.get_link_child()\n\n            bpy.ops.object.mode_set(mode='OBJECT')\n            bpy.context.view_layer.update()\n\n            # do the same for the link objects\n            for obj in child.get_all_objs():\n                obj.set_location(location=obj.get_location() - offset)\n            grand_child = child.get_link_child()\n            while grand_child is not None:\n                for obj in grand_child.get_all_objs():\n                    obj.set_location(location=obj.get_location() - offset)\n                grand_child = grand_child.get_link_child()\n\n            if link_to_be_removed == self.ik_link:\n                self._set_ik_link(None)\n\n            for obj in link_to_be_removed.get_all_objs():\n                obj.delete()\n            link_to_be_removed.delete()\n\n    def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.hide()\n        for link in self.links:\n            link.hide(hide_object=hide_object)\n\n    def get_all_local2world_mats(self) -> np.array:\n        \"\"\" Returns all matrix_world matrices from every joint.\n\n        :return: Numpy array of shape (num_bones, 4, 4).\n        \"\"\"\n\n        bpy.context.view_layer.update()\n        matrices = []\n        for link in self.links:\n            if link.bone is not None:\n                matrices.append(Matrix(self.get_local2world_mat()) @ link.bone.matrix)\n        return np.stack(matrices)\n\n    def get_all_visual_local2world_mats(self) -> np.array:\n        \"\"\" Returns all transformations from world frame to the visual objects.\n\n        :return: Numpy array of shape (num_bones, 4, 4).\n        \"\"\"\n        return np.stack([link.get_visual_local2world_mats(Matrix(self.get_local2world_mat())) for link in self.links])\n\n    def get_all_collision_local2world_mats(self) -> np.array:\n        \"\"\" Returns all transformations from the world frame to the collision objects.\n\n        :return: Numpy array of shape (num_bones, 4, 4).\n        \"\"\"\n        return np.stack([\n            link.get_collision_local2world_mats(Matrix(self.get_local2world_mat())) for link in self.links\n        ])\n\n    def get_all_inertial_local2world_mats(self) -> np.array:\n        \"\"\" Returns all transformations from the world frame to the inertial objects.\n\n        :return: Numpy array of shape (num_bones, 4, 4).\n        \"\"\"\n        return np.stack([link.get_inertial_local2world_mat(Matrix(self.get_local2world_mat())) for link in self.links])\n\n    def _set_ik_bone_controller(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the ik bone controller.\n\n        :param bone: Bone to set as ik control bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_controller\", bone)\n\n    def _set_ik_bone_constraint(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the ik bone constraint.\n\n        :param bone: Bone to set as ik constraint bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_constraint\", bone)\n\n    def _set_fk_ik_mode(self, mode: str = \"fk\"):\n        \"\"\" Sets the mode of the bone chain.\n\n        :param mode: One of \"fk\" or \"ik\" for forward / inverse kinematic.\n        \"\"\"\n        object.__setattr__(self, \"fk_ik_mode\", mode)\n\n    def _set_ik_link(self, ik_link: Optional[Link]):\n        \"\"\" Sets the ik link constraint.\n\n        :param ik_link: Link to set as ik link.\n        \"\"\"\n        object.__setattr__(self, \"ik_link\", ik_link)\n\n    def create_ik_bone_controller(self, link: Optional[Link] = None,\n                                  relative_location: Optional[Union[List[float], Vector]] = None,\n                                  use_rotation: bool = True,\n                                  chain_length: int = 0):\n        \"\"\" Creates an ik bone controller and a corresponding constraint bone for the respective link.\n\n        :param link: The link to create an ik bone for. If None, will use the last link.\n        :param relative_location: Relative location of the ik bone controller w.r.t. the bone's location. This can be\n                                  used to shift the point of control further away from the end effector.\n        :param use_rotation: Whether to rotate the child links as well. Defaults to True.\n        :param chain_length: The number of parent links which are influenced by this ik bone. Defaults to 0 for all\n                             parents.\n        \"\"\"\n        if self.ik_bone_controller is not None:\n            raise NotImplementedError(\"URDFObject already has an ik bone controller. More than one ik controllers are \"\n                                      \"currently not supported!\")\n        if relative_location is None:\n            relative_location = [0., 0., 0.]\n        if link is None:\n            link = self.links[-1]\n        ik_bone_controller, ik_bone_constraint, offset = link.create_ik_bone_controller(\n            relative_location=relative_location, use_rotation=use_rotation, chain_length=chain_length)\n        self._set_ik_bone_controller(ik_bone_controller)\n        self._set_ik_bone_constraint(ik_bone_constraint)\n        self._set_ik_bone_offset(offset=offset)\n        self._set_ik_link(link)\n        self._switch_fk_ik_mode(mode=\"ik\")\n\n    def _switch_fk_ik_mode(self, mode: str = \"fk\", keep_pose: bool = True):\n        \"\"\" Switches between forward and inverse kinematics mode. Will do this automatically when switching between e.g.\n            `set_rotation_euler_fk()` and `set_rotation_euler_ik()`.\n\n        :param mode: One of  for forward / inverse kinematik.\n        :param keep_pose: If specified, will keep the pose when switching modes. Otherwise, will return to the old pose\n                          of the previously selected mode.\n        \"\"\"\n        if mode == \"ik\" and self.ik_bone_controller is None:\n            raise NotImplementedError(\"URDFObject doesn't have an ik bone controller. Please set up an ik bone first \"\n                                      \"with 'urdf_object.create_ik_bone_controller()'\")\n        if self.fk_ik_mode != mode:\n            for link in self.links:\n                link.switch_fk_ik_mode(mode=mode, keep_pose=keep_pose)\n            self._set_fk_ik_mode(mode=mode)\n\n    def get_links_with_revolute_joints(self) -> List[Link]:\n        \"\"\" Returns all revolute joints.\n\n        :return: List of revolute joints.\n        \"\"\"\n        return [link for link in self.links if link.joint_type == \"revolute\"]\n\n    def _set_keyframe(self, name: str, frame: int = 0):\n        \"\"\" Sets a keyframe for a specific name for all bones of all links, as well as the copy_rotation constraint for\n            revolute joints.\n\n        :param name: Name of the keyframe to be inserted.\n        :param frame: Where to insert the keyframe.\n        \"\"\"\n        bpy.context.view_layer.update()\n        Utility.insert_keyframe(self.blender_obj, name, frame)\n        for link in self.links:\n            if link.bone is not None:\n                Utility.insert_keyframe(link.bone, name, frame)\n                Utility.insert_keyframe(link.fk_bone, name, frame)\n                Utility.insert_keyframe(link.ik_bone, name, frame)\n                if link.joint_type == 'revolute':\n                    Utility.insert_keyframe(link.bone.constraints['copy_rotation.fk'], \"influence\", frame=frame)\n                    Utility.insert_keyframe(link.bone.constraints['copy_rotation.ik'], \"influence\", frame=frame)\n        if self.ik_bone_controller is not None:\n            Utility.insert_keyframe(self.ik_bone_controller, name, frame)\n        if self.ik_bone_constraint is not None:\n            Utility.insert_keyframe(self.ik_bone_constraint, name, frame)\n\n        if frame > bpy.context.scene.frame_end:\n            bpy.context.scene.frame_end += 1\n\n    def set_rotation_euler_fk(self, link: Optional[Link], rotation_euler: Union[float, List[float], Euler, np.ndarray],\n                              mode: str = \"absolute\", frame: int = 0):\n        \"\"\" Rotates one specific link or all links based on euler angles in forward kinematic mode. Validates values\n            with given constraints.\n\n        :param link: The link to be rotated. If None, will perform the rotation on all revolute joints.\n        :param rotation_euler: The amount of rotation (in radians). Either three floats for x, y and z axes, or a\n                               single float. In the latter case, the axis of rotation is derived based on the rotation\n                               constraint. If these are not properly set (i.e., two axes must have equal min/max\n                               values) an exception will be thrown.\n        :param mode: One of [\"absolute\", \"relative\"]. For absolute rotations we clip the rotation value based on the\n                     constraints. For relative we don't - this will result in inverse motion after the constraint's\n                     limits have been reached.\n        :param frame: The keyframe where to insert the rotation.\n        \"\"\"\n        self._switch_fk_ik_mode(mode=\"fk\")\n        if link is not None:\n            link.set_rotation_euler_fk(rotation_euler=rotation_euler, mode=mode)\n        else:\n            revolute_joints = self.get_links_with_revolute_joints()\n            if isinstance(rotation_euler, list) and len(revolute_joints) == len(rotation_euler):\n                for revolute_joint, rotation in zip(revolute_joints, rotation_euler):\n                    revolute_joint.set_rotation_euler_fk(rotation_euler=rotation, mode=mode)\n            else:\n                for revolute_joint in revolute_joints:\n                    revolute_joint.set_rotation_euler_fk(rotation_euler=rotation_euler, mode=mode)\n        self._set_keyframe(name=\"rotation_euler\", frame=frame)\n\n    def set_rotation_euler_ik(self, rotation_euler: Union[float, List[float], Euler, np.ndarray],\n                              mode: str = \"absolute\", frame: int = 0):\n        \"\"\" Performs rotation in inverse kinematics mode.\n\n        :param rotation_euler: The amount of rotation (in radians). Either three floats for x, y and z axes, or a\n                               single float. In the latter case, the axis of rotation is derived based on the rotation\n                               constraint. If these are not properly set (i.e., two axes must have equal min/max\n                               values) an exception will be thrown.\n        :param mode: One of [\"absolute\", \"relative\"]. For absolute rotations we clip the rotation value based on the\n                     constraints. For relative we don't - this will result in inverse motion after the constraint's\n                     limits have been reached.\n        :param frame: The keyframe where to insert the rotation.\n        \"\"\"\n        self._switch_fk_ik_mode(mode=\"ik\")\n        assert self.ik_link is not None\n        self.ik_link.set_rotation_euler_ik(rotation_euler=rotation_euler, mode=mode)\n        self._set_keyframe(name=\"rotation_euler\", frame=frame)\n\n    def set_location_ik(self, location: Union[List[float], np.array, Vector], frame: int = 0):\n        \"\"\" Performs location change in inverse kinematics mode.\n\n        :param location: Location vector.\n        :param frame: The keyframe where to insert the rotation.\n        \"\"\"\n        self._switch_fk_ik_mode(mode=\"ik\")\n        assert self.ik_link is not None\n        self.ik_link.set_location_ik(location=location)\n        self._set_keyframe(name=\"location\", frame=frame)\n\n    def has_reached_ik_pose(self, location_error: float = 0.01, rotation_error: float = 0.01) -> bool:\n        \"\"\" Checks whether the urdf object was able to move to the currently set pose.\n\n        :param location_error: Tolerable location error in m.\n        :param rotation_error: Tolerable rotation error in radians.\n        :return: True if the link is at the desired ik pose; else False.\n        \"\"\"\n        curr_offset = self.ik_bone_controller.matrix.inverted() @ self.ik_bone_constraint.matrix\n        t_curr, q_curr, _ = curr_offset.decompose()\n        t_orig, q_orig, _ = self.ik_bone_offset.decompose()\n\n        t_diff = (t_curr - t_orig).length\n        q_diff = q_curr.rotation_difference(q_orig).angle\n\n        if t_diff < location_error and q_diff < rotation_error:\n            print(f'Pose is within in given constraints:\\n'\n                  f'  translation difference: {t_diff:.4f} (max: {location_error})\\n'\n                  f'  rotation difference: {q_diff:.4f} (max: {rotation_error})')\n            return True\n        print(f'Pose is not within given constraints:\\n'\n              f'  translation difference: {t_diff:.4f} (max: {location_error})\\n'\n              f'  rotation difference: {q_diff:.4f} (max: {rotation_error})')\n        return False\n\n    def _set_ik_bone_offset(self, offset: Matrix):\n        \"\"\" Sets the location offset between the control and constraint bone.\n\n        :param offset: The location offset.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_offset\", offset)",
  "def __init__(self, armature: bpy.types.Armature, links: List[Link], xml_tree: Optional[\"urdfpy.URDF\"] = None):\n        super().__init__(bpy_object=armature)\n\n        object.__setattr__(self, \"links\", links)\n        object.__setattr__(self, \"xml_tree\", xml_tree)\n        object.__setattr__(self, \"ik_bone_constraint\", None)\n        object.__setattr__(self, \"ik_bone_controller\", None)\n        object.__setattr__(self, \"fk_ik_mode\", None)\n        object.__setattr__(self, \"ik_link\", None)\n        object.__setattr__(self, 'ik_bone_offset', None)",
  "def get_all_urdf_objs(self) -> List[Union[Link, Inertial, MeshObject]]:\n        \"\"\" Returns a list of all urdf-related objects.\n\n        :return: List of all urdf-related objects.\n        \"\"\"\n        objs = []\n        for link in self.links:\n            objs.append(link)\n            objs.extend(link.get_all_objs())\n        return objs",
  "def get_all_collision_objs(self) -> List[MeshObject]:\n        \"\"\" Returns a list of all collision objects.\n\n        :return: List of all collision objects.\n        \"\"\"\n        return [obj for obj in self.get_all_urdf_objs() if 'collision' in obj.get_name()]",
  "def get_all_inertial_objs(self) -> List[Inertial]:\n        \"\"\" Returns a list of all inertial objects.\n\n        :return: List of all inertial objects.\n        \"\"\"\n        return [obj for obj in self.get_all_urdf_objs() if isinstance(obj, Inertial)]",
  "def get_all_visual_objs(self) -> List[MeshObject]:\n        \"\"\" Returns a list of all visual objects.\n\n        :return: List of all visual objects.\n        \"\"\"\n        return [obj for obj in self.get_all_urdf_objs() if 'visual' in obj.get_name()]",
  "def hide_links_and_collision_inertial_objs(self):\n        \"\"\" Hides links and their respective collision and inertial objects from rendering. \"\"\"\n        self.blender_obj.hide_set(True)\n        for link in self.links:\n            for obj in link.get_all_objs():\n                if \"collision\" in obj.get_name() or \"inertial\" in obj.get_name():\n                    obj.hide()",
  "def set_ascending_category_ids(self, category_ids: Optional[List[int]] = None):\n        \"\"\" Sets semantic categories to the links and their associated objects.\n\n        :param category_ids: List of 'category_id's for every link. If None, will create a list from [1 ... len(links)].\n        \"\"\"\n        if category_ids is None:\n            category_ids = list(range(1, len(self.links) + 1))\n\n        assert len(category_ids) == len(self.links), f\"Need equal amount of category ids for links. Got \" \\\n                                                     f\"{len(category_ids)} and {len(self.links)}, respectively.\"\n        for link, category_id in zip(self.links, category_ids):\n            link.set_cp(key=\"category_id\", value=category_id)\n            for obj in link.get_all_objs():\n                obj.set_cp(key=\"category_id\", value=category_id)",
  "def remove_link_by_index(self, index: int = 0):\n        \"\"\" Removes a link and all its associated objects given an index. Also handles relationship of the link's child\n            with its parent. This is useful for removing a 'world link' which could be a simple flat surface, or if\n            someone wants to shorten the whole urdf object.\n\n        :param index: Index of the joint to be removed.\n        \"\"\"\n        assert index < len(self.links), f\"Invalid index {index}. Index must be in range 0, {len(self.links)} (no. \" \\\n                                        f\"links: {len(self.links)}).\"\n\n        # remove link from the urdf instance and determine child / parent\n        link_to_be_removed = self.links.pop(index)\n        child = link_to_be_removed.get_link_child()\n\n        # remove bones and assign old bone pose to child bone\n        if child is not None and link_to_be_removed.bone is not None:\n            print(f'Trying to put {child.get_name()} to position of {link_to_be_removed.get_name()}')\n            bpy.context.view_layer.update()\n            bpy.ops.object.select_all(action='DESELECT')\n            bpy.context.view_layer.objects.active = self.blender_obj\n            bpy.ops.object.mode_set(mode='EDIT', toggle=False)\n            edit_bones = self.blender_obj.data.edit_bones\n            offset = edit_bones[child.bone.name].head - edit_bones[link_to_be_removed.bone.name].head\n\n            edit_bones[child.bone.name].head -= offset\n            edit_bones[child.bone.name].tail -= offset\n            edit_bones[child.fk_bone.name].head -= offset\n            edit_bones[child.fk_bone.name].tail -= offset\n            edit_bones[child.ik_bone.name].head -= offset\n            edit_bones[child.ik_bone.name].tail -= offset\n\n            grand_child = child.get_link_child()\n            while grand_child is not None:\n                edit_bones[grand_child.bone.name].head -= offset\n                edit_bones[grand_child.bone.name].tail -= offset\n                edit_bones[grand_child.fk_bone.name].head -= offset\n                edit_bones[grand_child.fk_bone.name].tail -= offset\n                edit_bones[grand_child.ik_bone.name].head -= offset\n                edit_bones[grand_child.ik_bone.name].tail -= offset\n                grand_child = grand_child.get_link_child()\n\n            bpy.ops.object.mode_set(mode='OBJECT')\n            bpy.context.view_layer.update()\n\n            # do the same for the link objects\n            for obj in child.get_all_objs():\n                obj.set_location(location=obj.get_location() - offset)\n            grand_child = child.get_link_child()\n            while grand_child is not None:\n                for obj in grand_child.get_all_objs():\n                    obj.set_location(location=obj.get_location() - offset)\n                grand_child = grand_child.get_link_child()\n\n            if link_to_be_removed == self.ik_link:\n                self._set_ik_link(None)\n\n            for obj in link_to_be_removed.get_all_objs():\n                obj.delete()\n            link_to_be_removed.delete()",
  "def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.hide()\n        for link in self.links:\n            link.hide(hide_object=hide_object)",
  "def get_all_local2world_mats(self) -> np.array:\n        \"\"\" Returns all matrix_world matrices from every joint.\n\n        :return: Numpy array of shape (num_bones, 4, 4).\n        \"\"\"\n\n        bpy.context.view_layer.update()\n        matrices = []\n        for link in self.links:\n            if link.bone is not None:\n                matrices.append(Matrix(self.get_local2world_mat()) @ link.bone.matrix)\n        return np.stack(matrices)",
  "def get_all_visual_local2world_mats(self) -> np.array:\n        \"\"\" Returns all transformations from world frame to the visual objects.\n\n        :return: Numpy array of shape (num_bones, 4, 4).\n        \"\"\"\n        return np.stack([link.get_visual_local2world_mats(Matrix(self.get_local2world_mat())) for link in self.links])",
  "def get_all_collision_local2world_mats(self) -> np.array:\n        \"\"\" Returns all transformations from the world frame to the collision objects.\n\n        :return: Numpy array of shape (num_bones, 4, 4).\n        \"\"\"\n        return np.stack([\n            link.get_collision_local2world_mats(Matrix(self.get_local2world_mat())) for link in self.links\n        ])",
  "def get_all_inertial_local2world_mats(self) -> np.array:\n        \"\"\" Returns all transformations from the world frame to the inertial objects.\n\n        :return: Numpy array of shape (num_bones, 4, 4).\n        \"\"\"\n        return np.stack([link.get_inertial_local2world_mat(Matrix(self.get_local2world_mat())) for link in self.links])",
  "def _set_ik_bone_controller(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the ik bone controller.\n\n        :param bone: Bone to set as ik control bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_controller\", bone)",
  "def _set_ik_bone_constraint(self, bone: bpy.types.PoseBone):\n        \"\"\" Sets the ik bone constraint.\n\n        :param bone: Bone to set as ik constraint bone.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_constraint\", bone)",
  "def _set_fk_ik_mode(self, mode: str = \"fk\"):\n        \"\"\" Sets the mode of the bone chain.\n\n        :param mode: One of \"fk\" or \"ik\" for forward / inverse kinematic.\n        \"\"\"\n        object.__setattr__(self, \"fk_ik_mode\", mode)",
  "def _set_ik_link(self, ik_link: Optional[Link]):\n        \"\"\" Sets the ik link constraint.\n\n        :param ik_link: Link to set as ik link.\n        \"\"\"\n        object.__setattr__(self, \"ik_link\", ik_link)",
  "def create_ik_bone_controller(self, link: Optional[Link] = None,\n                                  relative_location: Optional[Union[List[float], Vector]] = None,\n                                  use_rotation: bool = True,\n                                  chain_length: int = 0):\n        \"\"\" Creates an ik bone controller and a corresponding constraint bone for the respective link.\n\n        :param link: The link to create an ik bone for. If None, will use the last link.\n        :param relative_location: Relative location of the ik bone controller w.r.t. the bone's location. This can be\n                                  used to shift the point of control further away from the end effector.\n        :param use_rotation: Whether to rotate the child links as well. Defaults to True.\n        :param chain_length: The number of parent links which are influenced by this ik bone. Defaults to 0 for all\n                             parents.\n        \"\"\"\n        if self.ik_bone_controller is not None:\n            raise NotImplementedError(\"URDFObject already has an ik bone controller. More than one ik controllers are \"\n                                      \"currently not supported!\")\n        if relative_location is None:\n            relative_location = [0., 0., 0.]\n        if link is None:\n            link = self.links[-1]\n        ik_bone_controller, ik_bone_constraint, offset = link.create_ik_bone_controller(\n            relative_location=relative_location, use_rotation=use_rotation, chain_length=chain_length)\n        self._set_ik_bone_controller(ik_bone_controller)\n        self._set_ik_bone_constraint(ik_bone_constraint)\n        self._set_ik_bone_offset(offset=offset)\n        self._set_ik_link(link)\n        self._switch_fk_ik_mode(mode=\"ik\")",
  "def _switch_fk_ik_mode(self, mode: str = \"fk\", keep_pose: bool = True):\n        \"\"\" Switches between forward and inverse kinematics mode. Will do this automatically when switching between e.g.\n            `set_rotation_euler_fk()` and `set_rotation_euler_ik()`.\n\n        :param mode: One of  for forward / inverse kinematik.\n        :param keep_pose: If specified, will keep the pose when switching modes. Otherwise, will return to the old pose\n                          of the previously selected mode.\n        \"\"\"\n        if mode == \"ik\" and self.ik_bone_controller is None:\n            raise NotImplementedError(\"URDFObject doesn't have an ik bone controller. Please set up an ik bone first \"\n                                      \"with 'urdf_object.create_ik_bone_controller()'\")\n        if self.fk_ik_mode != mode:\n            for link in self.links:\n                link.switch_fk_ik_mode(mode=mode, keep_pose=keep_pose)\n            self._set_fk_ik_mode(mode=mode)",
  "def get_links_with_revolute_joints(self) -> List[Link]:\n        \"\"\" Returns all revolute joints.\n\n        :return: List of revolute joints.\n        \"\"\"\n        return [link for link in self.links if link.joint_type == \"revolute\"]",
  "def _set_keyframe(self, name: str, frame: int = 0):\n        \"\"\" Sets a keyframe for a specific name for all bones of all links, as well as the copy_rotation constraint for\n            revolute joints.\n\n        :param name: Name of the keyframe to be inserted.\n        :param frame: Where to insert the keyframe.\n        \"\"\"\n        bpy.context.view_layer.update()\n        Utility.insert_keyframe(self.blender_obj, name, frame)\n        for link in self.links:\n            if link.bone is not None:\n                Utility.insert_keyframe(link.bone, name, frame)\n                Utility.insert_keyframe(link.fk_bone, name, frame)\n                Utility.insert_keyframe(link.ik_bone, name, frame)\n                if link.joint_type == 'revolute':\n                    Utility.insert_keyframe(link.bone.constraints['copy_rotation.fk'], \"influence\", frame=frame)\n                    Utility.insert_keyframe(link.bone.constraints['copy_rotation.ik'], \"influence\", frame=frame)\n        if self.ik_bone_controller is not None:\n            Utility.insert_keyframe(self.ik_bone_controller, name, frame)\n        if self.ik_bone_constraint is not None:\n            Utility.insert_keyframe(self.ik_bone_constraint, name, frame)\n\n        if frame > bpy.context.scene.frame_end:\n            bpy.context.scene.frame_end += 1",
  "def set_rotation_euler_fk(self, link: Optional[Link], rotation_euler: Union[float, List[float], Euler, np.ndarray],\n                              mode: str = \"absolute\", frame: int = 0):\n        \"\"\" Rotates one specific link or all links based on euler angles in forward kinematic mode. Validates values\n            with given constraints.\n\n        :param link: The link to be rotated. If None, will perform the rotation on all revolute joints.\n        :param rotation_euler: The amount of rotation (in radians). Either three floats for x, y and z axes, or a\n                               single float. In the latter case, the axis of rotation is derived based on the rotation\n                               constraint. If these are not properly set (i.e., two axes must have equal min/max\n                               values) an exception will be thrown.\n        :param mode: One of [\"absolute\", \"relative\"]. For absolute rotations we clip the rotation value based on the\n                     constraints. For relative we don't - this will result in inverse motion after the constraint's\n                     limits have been reached.\n        :param frame: The keyframe where to insert the rotation.\n        \"\"\"\n        self._switch_fk_ik_mode(mode=\"fk\")\n        if link is not None:\n            link.set_rotation_euler_fk(rotation_euler=rotation_euler, mode=mode)\n        else:\n            revolute_joints = self.get_links_with_revolute_joints()\n            if isinstance(rotation_euler, list) and len(revolute_joints) == len(rotation_euler):\n                for revolute_joint, rotation in zip(revolute_joints, rotation_euler):\n                    revolute_joint.set_rotation_euler_fk(rotation_euler=rotation, mode=mode)\n            else:\n                for revolute_joint in revolute_joints:\n                    revolute_joint.set_rotation_euler_fk(rotation_euler=rotation_euler, mode=mode)\n        self._set_keyframe(name=\"rotation_euler\", frame=frame)",
  "def set_rotation_euler_ik(self, rotation_euler: Union[float, List[float], Euler, np.ndarray],\n                              mode: str = \"absolute\", frame: int = 0):\n        \"\"\" Performs rotation in inverse kinematics mode.\n\n        :param rotation_euler: The amount of rotation (in radians). Either three floats for x, y and z axes, or a\n                               single float. In the latter case, the axis of rotation is derived based on the rotation\n                               constraint. If these are not properly set (i.e., two axes must have equal min/max\n                               values) an exception will be thrown.\n        :param mode: One of [\"absolute\", \"relative\"]. For absolute rotations we clip the rotation value based on the\n                     constraints. For relative we don't - this will result in inverse motion after the constraint's\n                     limits have been reached.\n        :param frame: The keyframe where to insert the rotation.\n        \"\"\"\n        self._switch_fk_ik_mode(mode=\"ik\")\n        assert self.ik_link is not None\n        self.ik_link.set_rotation_euler_ik(rotation_euler=rotation_euler, mode=mode)\n        self._set_keyframe(name=\"rotation_euler\", frame=frame)",
  "def set_location_ik(self, location: Union[List[float], np.array, Vector], frame: int = 0):\n        \"\"\" Performs location change in inverse kinematics mode.\n\n        :param location: Location vector.\n        :param frame: The keyframe where to insert the rotation.\n        \"\"\"\n        self._switch_fk_ik_mode(mode=\"ik\")\n        assert self.ik_link is not None\n        self.ik_link.set_location_ik(location=location)\n        self._set_keyframe(name=\"location\", frame=frame)",
  "def has_reached_ik_pose(self, location_error: float = 0.01, rotation_error: float = 0.01) -> bool:\n        \"\"\" Checks whether the urdf object was able to move to the currently set pose.\n\n        :param location_error: Tolerable location error in m.\n        :param rotation_error: Tolerable rotation error in radians.\n        :return: True if the link is at the desired ik pose; else False.\n        \"\"\"\n        curr_offset = self.ik_bone_controller.matrix.inverted() @ self.ik_bone_constraint.matrix\n        t_curr, q_curr, _ = curr_offset.decompose()\n        t_orig, q_orig, _ = self.ik_bone_offset.decompose()\n\n        t_diff = (t_curr - t_orig).length\n        q_diff = q_curr.rotation_difference(q_orig).angle\n\n        if t_diff < location_error and q_diff < rotation_error:\n            print(f'Pose is within in given constraints:\\n'\n                  f'  translation difference: {t_diff:.4f} (max: {location_error})\\n'\n                  f'  rotation difference: {q_diff:.4f} (max: {rotation_error})')\n            return True\n        print(f'Pose is not within given constraints:\\n'\n              f'  translation difference: {t_diff:.4f} (max: {location_error})\\n'\n              f'  rotation difference: {q_diff:.4f} (max: {rotation_error})')\n        return False",
  "def _set_ik_bone_offset(self, offset: Matrix):\n        \"\"\" Sets the location offset between the control and constraint bone.\n\n        :param offset: The location offset.\n        \"\"\"\n        object.__setattr__(self, \"ik_bone_offset\", offset)",
  "class Inertial(Entity):\n    \"\"\"\n    Every instance of this class is an inertial object which is usually part of an URDFObject.\n    \"\"\"\n    def __init__(self, bpy_object: bpy.types.Object):\n        super().__init__(bpy_object=bpy_object)\n\n        object.__setattr__(self, \"inertia\", None)\n        object.__setattr__(self, \"mass\", None)\n        object.__setattr__(self, \"origin\", None)\n\n    def set_inertia(self, inertia: np.ndarray):\n        \"\"\" Sets inertia value.\n\n        :param inertia: 3x3 symmetric rotational inertia matrix.\n        \"\"\"\n        assert inertia.shape == (3, 3)\n        object.__setattr__(self, \"inertia\", inertia)\n\n    def get_inertia(self) -> np.ndarray:\n        \"\"\" Returns the inertia.\n\n        :return: The inertia matrix.\n        \"\"\"\n        return self.inertia\n\n    def set_mass(self, mass: float):\n        \"\"\" Sets the mass.\n\n        :param mass: Mass of the link in kilograms.\n        \"\"\"\n        object.__setattr__(self, \"mass\", mass)\n\n    def get_mass(self) -> float:\n        \"\"\" Returns the mass of the link.\n\n        :return: The mass.\n        \"\"\"\n        return self.mass\n\n    def set_origin(self, origin: Union[np.ndarray, Matrix]):\n        \"\"\" Sets the origin and the world matrix of the inertia.\n\n        :param origin: 4x4 matrix of the inertials relative to the link frame.\n        \"\"\"\n        object.__setattr__(self, \"origin\", Matrix(origin))\n        self.blender_obj.matrix_world = Matrix(origin)\n\n    def get_origin(self) -> Matrix:\n        \"\"\" Returns the origin of the inertia.\n\n        :return: The pose relative to the link frame.\n        \"\"\"\n        return self.origin\n\n    def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.blender_obj.hide_render = hide_object",
  "def __init__(self, bpy_object: bpy.types.Object):\n        super().__init__(bpy_object=bpy_object)\n\n        object.__setattr__(self, \"inertia\", None)\n        object.__setattr__(self, \"mass\", None)\n        object.__setattr__(self, \"origin\", None)",
  "def set_inertia(self, inertia: np.ndarray):\n        \"\"\" Sets inertia value.\n\n        :param inertia: 3x3 symmetric rotational inertia matrix.\n        \"\"\"\n        assert inertia.shape == (3, 3)\n        object.__setattr__(self, \"inertia\", inertia)",
  "def get_inertia(self) -> np.ndarray:\n        \"\"\" Returns the inertia.\n\n        :return: The inertia matrix.\n        \"\"\"\n        return self.inertia",
  "def set_mass(self, mass: float):\n        \"\"\" Sets the mass.\n\n        :param mass: Mass of the link in kilograms.\n        \"\"\"\n        object.__setattr__(self, \"mass\", mass)",
  "def get_mass(self) -> float:\n        \"\"\" Returns the mass of the link.\n\n        :return: The mass.\n        \"\"\"\n        return self.mass",
  "def set_origin(self, origin: Union[np.ndarray, Matrix]):\n        \"\"\" Sets the origin and the world matrix of the inertia.\n\n        :param origin: 4x4 matrix of the inertials relative to the link frame.\n        \"\"\"\n        object.__setattr__(self, \"origin\", Matrix(origin))\n        self.blender_obj.matrix_world = Matrix(origin)",
  "def get_origin(self) -> Matrix:\n        \"\"\" Returns the origin of the inertia.\n\n        :return: The pose relative to the link frame.\n        \"\"\"\n        return self.origin",
  "def hide(self, hide_object: bool = True):\n        \"\"\" Sets the visibility of the object.\n\n        :param hide_object: Determines whether the object should be hidden in rendering.\n        \"\"\"\n        self.blender_obj.hide_render = hide_object",
  "class Light(Entity):\n    \"\"\"\n    This class allows the creation and management of lights in the scene.\n    However, we advise to use emissive materials on objects to light a scene as these produce more realistic light\n    scenarios as the lighting does not directly start from a small point in space.\n    \"\"\"\n\n    def __init__(self, light_type: str = \"POINT\", name: str = \"light\", blender_obj: Optional[bpy.types.Object] = None):\n        \"\"\"\n        Constructs a new light if no blender_obj is given, else the params type and name are used to construct a new\n        light.\n\n        :param light_type: The initial type of light, can be one of [POINT, SUN, SPOT, AREA].\n        :param name: The name of the new light\n        :param blender_obj: A bpy.types.Light, this is then used instead of the type and name.\n        \"\"\"\n        if blender_obj is None:\n            # this creates a light object and sets is as the used entity inside the super class\n            light_data = bpy.data.lights.new(name=name, type=light_type)\n            light_obj = bpy.data.objects.new(name=name, object_data=light_data)\n            bpy.context.collection.objects.link(light_obj)\n            super().__init__(light_obj)\n        else:\n            super().__init__(blender_obj)\n\n    def set_energy(self, energy: float, frame: Optional[int] = None):\n        \"\"\" Sets the energy of the light.\n\n        :param energy: The energy to set. If the type is SUN this value is interpreted as Watt per square meter,\n                       otherwise it is interpreted as Watt.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.data.energy = energy\n        Utility.insert_keyframe(self.blender_obj.data, \"energy\", frame)\n\n    def set_color(self, color: Union[list, Color], frame: Optional[int] = None):\n        \"\"\" Sets the color of the light.\n\n        :param color: The rgb color to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.data.color = color\n        Utility.insert_keyframe(self.blender_obj.data, \"color\", frame)\n\n    def set_distance(self, distance: float, frame: Optional[int] = None):\n        \"\"\" Sets the falloff distance of the light = point where light is half the original intensity.\n\n        :param distance: The falloff distance to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.data.distance = distance\n        Utility.insert_keyframe(self.blender_obj.data, \"distance\", frame)\n\n    def set_type(self, light_type: str, frame: Optional[int] = None):\n        \"\"\" Sets the type of the light.\n\n        :param light_type: The type to set, can be one of [POINT, SUN, SPOT, AREA].\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.data.type = light_type\n        Utility.insert_keyframe(self.blender_obj.data, \"type\", frame)\n\n    def setup_as_projector(self, pattern: np.ndarray, frame: Optional[int] = None):\n        r\"\"\" Sets a spotlight source as projector of a pattern image. Sets location and angle of projector to current\n        camera. Adjusts scale of pattern image to fit field-of-view of camera:\n        :math:`(0.5 + \\frac{X}{Z \\cdot F}, 0.5 + \\frac{X}{Z \\cdot F \\cdot r}, 0)`\n        where $F$ is focal length and $r$ aspect ratio.\n        WARNING: This should be done after the camera parameters are set!\n\n        :param pattern: pattern image to be projected onto scene as np.ndarray.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        cam_ob = bpy.context.scene.camera\n        fov = cam_ob.data.angle     # field of view of current camera in radians\n\n        focal_length = 2 * np.tan(fov / 2)\n        # Image aspect ratio = height / width\n        aspect_ratio = bpy.context.scene.render.resolution_y / bpy.context.scene.render.resolution_x\n\n        # Set location of light source to camera -- COPY TRANSFORMS\n        self.blender_obj.constraints.new('COPY_TRANSFORMS')\n        self.blender_obj.constraints['Copy Transforms'].target = cam_ob\n\n        # Setup nodes for projecting image\n        self.blender_obj.data.use_nodes = True\n        self.blender_obj.data.shadow_soft_size = 0\n        self.blender_obj.data.spot_size = 3.14159  # 180deg in rad\n        self.blender_obj.data.cycles.cast_shadow = False\n\n        nodes = self.blender_obj.data.node_tree.nodes\n        links = self.blender_obj.data.node_tree.links\n\n        node_ox = nodes.get('Emission')\n\n        image_data = bpy.data.images.new('pattern', width=pattern.shape[1], height=pattern.shape[0], alpha=True)\n        image_data.pixels = pattern.ravel()\n\n        # Set Up Nodes\n        node_pattern = nodes.new(type=\"ShaderNodeTexImage\")  # Texture Image\n        node_pattern.label = 'Texture Image'\n        node_pattern.image = bpy.data.images['pattern']\n        node_pattern.extension = 'CLIP'\n\n        node_coord = nodes.new(type=\"ShaderNodeTexCoord\")  # Texture Coordinate\n        node_coord.label = 'Texture Coordinate'\n\n        f_value = nodes.new(type=\"ShaderNodeValue\")\n        f_value.label = 'Focal Length'\n        f_value.outputs[0].default_value = focal_length\n\n        fr_value = nodes.new(type=\"ShaderNodeValue\")\n        fr_value.label = 'Focal Length * Ratio'\n        fr_value.outputs[0].default_value = focal_length * aspect_ratio\n\n        divide1 = nodes.new(type=\"ShaderNodeMath\")\n        divide1.label = 'X / ZF'\n        divide1.operation = 'DIVIDE'\n\n        divide2 = nodes.new(type=\"ShaderNodeMath\")\n        divide2.label = 'Y / ZFr'\n        divide2.operation = 'DIVIDE'\n\n        multiply1 = nodes.new(type=\"ShaderNodeMath\")\n        multiply1.label = 'Z * F'\n        multiply1.operation = 'MULTIPLY'\n\n        multiply2 = nodes.new(type=\"ShaderNodeMath\")\n        multiply2.label = 'Z * Fr'\n        multiply2.operation = 'MULTIPLY'\n\n        center_image = nodes.new(type=\"ShaderNodeVectorMath\")\n        center_image.operation = 'ADD'\n        center_image.label = 'Offset'\n        center_image.inputs[1].default_value[0] = 0.5\n        center_image.inputs[1].default_value[1] = 0.5\n\n        xyz_components = nodes.new(type=\"ShaderNodeSeparateXYZ\")\n\n        combine_xyz = nodes.new(type=\"ShaderNodeCombineXYZ\")\n\n        # Set Up Links\n        links.new(node_pattern.outputs[\"Color\"], node_ox.inputs[\"Color\"])  # Link Image Texture to Emission\n        links.new(node_coord.outputs[\"Normal\"], xyz_components.inputs[\"Vector\"])\n        # ZF\n        links.new(f_value.outputs[0], multiply1.inputs[1])\n        links.new(xyz_components.outputs[\"Z\"], multiply1.inputs[0])\n        # ZFr\n        links.new(fr_value.outputs[0], multiply2.inputs[1])\n        links.new(xyz_components.outputs[\"Z\"], multiply2.inputs[0])\n        # X / ZF\n        links.new(xyz_components.outputs[\"X\"], divide1.inputs[0])\n        links.new(multiply1.outputs[0], divide1.inputs[1])\n        # Y / ZFr\n        links.new(xyz_components.outputs[\"Y\"], divide2.inputs[0])\n        links.new(multiply2.outputs[0], divide2.inputs[1])\n        # Combine (X/ZF, Y/ZFr, 0)\n        links.new(divide1.outputs[0], combine_xyz.inputs[\"X\"])\n        links.new(divide2.outputs[0], combine_xyz.inputs[\"Y\"])\n        # Center image by offset\n        links.new(combine_xyz.outputs[\"Vector\"], center_image.inputs[0])\n        # Link Mapping to Image Texture\n        links.new(center_image.outputs[\"Vector\"], node_pattern.inputs[\"Vector\"])\n\n        Utility.insert_keyframe(self.blender_obj.data, \"use_projector\", frame)\n\n\n    def get_energy(self, frame: Optional[int] = None) -> float:\n        \"\"\" Returns the energy of the light.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The energy at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return self.blender_obj.data.energy\n\n    def get_color(self, frame: Optional[int] = None) -> Color:\n        \"\"\" Returns the RGB color of the light.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The color at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return self.blender_obj.data.color\n\n    def get_distance(self, frame: Optional[int] = None) -> float:\n        \"\"\" Returns the falloff distance of the light (point where light is half the original intensity).\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The falloff distance at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return self.blender_obj.data.distance\n\n    def get_type(self, frame: Optional[int] = None) -> str:\n        \"\"\" Returns the type of the light.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The type at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return self.blender_obj.data.type",
  "def __init__(self, light_type: str = \"POINT\", name: str = \"light\", blender_obj: Optional[bpy.types.Object] = None):\n        \"\"\"\n        Constructs a new light if no blender_obj is given, else the params type and name are used to construct a new\n        light.\n\n        :param light_type: The initial type of light, can be one of [POINT, SUN, SPOT, AREA].\n        :param name: The name of the new light\n        :param blender_obj: A bpy.types.Light, this is then used instead of the type and name.\n        \"\"\"\n        if blender_obj is None:\n            # this creates a light object and sets is as the used entity inside the super class\n            light_data = bpy.data.lights.new(name=name, type=light_type)\n            light_obj = bpy.data.objects.new(name=name, object_data=light_data)\n            bpy.context.collection.objects.link(light_obj)\n            super().__init__(light_obj)\n        else:\n            super().__init__(blender_obj)",
  "def set_energy(self, energy: float, frame: Optional[int] = None):\n        \"\"\" Sets the energy of the light.\n\n        :param energy: The energy to set. If the type is SUN this value is interpreted as Watt per square meter,\n                       otherwise it is interpreted as Watt.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.data.energy = energy\n        Utility.insert_keyframe(self.blender_obj.data, \"energy\", frame)",
  "def set_color(self, color: Union[list, Color], frame: Optional[int] = None):\n        \"\"\" Sets the color of the light.\n\n        :param color: The rgb color to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.data.color = color\n        Utility.insert_keyframe(self.blender_obj.data, \"color\", frame)",
  "def set_distance(self, distance: float, frame: Optional[int] = None):\n        \"\"\" Sets the falloff distance of the light = point where light is half the original intensity.\n\n        :param distance: The falloff distance to set.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.data.distance = distance\n        Utility.insert_keyframe(self.blender_obj.data, \"distance\", frame)",
  "def set_type(self, light_type: str, frame: Optional[int] = None):\n        \"\"\" Sets the type of the light.\n\n        :param light_type: The type to set, can be one of [POINT, SUN, SPOT, AREA].\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        self.blender_obj.data.type = light_type\n        Utility.insert_keyframe(self.blender_obj.data, \"type\", frame)",
  "def setup_as_projector(self, pattern: np.ndarray, frame: Optional[int] = None):\n        r\"\"\" Sets a spotlight source as projector of a pattern image. Sets location and angle of projector to current\n        camera. Adjusts scale of pattern image to fit field-of-view of camera:\n        :math:`(0.5 + \\frac{X}{Z \\cdot F}, 0.5 + \\frac{X}{Z \\cdot F \\cdot r}, 0)`\n        where $F$ is focal length and $r$ aspect ratio.\n        WARNING: This should be done after the camera parameters are set!\n\n        :param pattern: pattern image to be projected onto scene as np.ndarray.\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        \"\"\"\n        cam_ob = bpy.context.scene.camera\n        fov = cam_ob.data.angle     # field of view of current camera in radians\n\n        focal_length = 2 * np.tan(fov / 2)\n        # Image aspect ratio = height / width\n        aspect_ratio = bpy.context.scene.render.resolution_y / bpy.context.scene.render.resolution_x\n\n        # Set location of light source to camera -- COPY TRANSFORMS\n        self.blender_obj.constraints.new('COPY_TRANSFORMS')\n        self.blender_obj.constraints['Copy Transforms'].target = cam_ob\n\n        # Setup nodes for projecting image\n        self.blender_obj.data.use_nodes = True\n        self.blender_obj.data.shadow_soft_size = 0\n        self.blender_obj.data.spot_size = 3.14159  # 180deg in rad\n        self.blender_obj.data.cycles.cast_shadow = False\n\n        nodes = self.blender_obj.data.node_tree.nodes\n        links = self.blender_obj.data.node_tree.links\n\n        node_ox = nodes.get('Emission')\n\n        image_data = bpy.data.images.new('pattern', width=pattern.shape[1], height=pattern.shape[0], alpha=True)\n        image_data.pixels = pattern.ravel()\n\n        # Set Up Nodes\n        node_pattern = nodes.new(type=\"ShaderNodeTexImage\")  # Texture Image\n        node_pattern.label = 'Texture Image'\n        node_pattern.image = bpy.data.images['pattern']\n        node_pattern.extension = 'CLIP'\n\n        node_coord = nodes.new(type=\"ShaderNodeTexCoord\")  # Texture Coordinate\n        node_coord.label = 'Texture Coordinate'\n\n        f_value = nodes.new(type=\"ShaderNodeValue\")\n        f_value.label = 'Focal Length'\n        f_value.outputs[0].default_value = focal_length\n\n        fr_value = nodes.new(type=\"ShaderNodeValue\")\n        fr_value.label = 'Focal Length * Ratio'\n        fr_value.outputs[0].default_value = focal_length * aspect_ratio\n\n        divide1 = nodes.new(type=\"ShaderNodeMath\")\n        divide1.label = 'X / ZF'\n        divide1.operation = 'DIVIDE'\n\n        divide2 = nodes.new(type=\"ShaderNodeMath\")\n        divide2.label = 'Y / ZFr'\n        divide2.operation = 'DIVIDE'\n\n        multiply1 = nodes.new(type=\"ShaderNodeMath\")\n        multiply1.label = 'Z * F'\n        multiply1.operation = 'MULTIPLY'\n\n        multiply2 = nodes.new(type=\"ShaderNodeMath\")\n        multiply2.label = 'Z * Fr'\n        multiply2.operation = 'MULTIPLY'\n\n        center_image = nodes.new(type=\"ShaderNodeVectorMath\")\n        center_image.operation = 'ADD'\n        center_image.label = 'Offset'\n        center_image.inputs[1].default_value[0] = 0.5\n        center_image.inputs[1].default_value[1] = 0.5\n\n        xyz_components = nodes.new(type=\"ShaderNodeSeparateXYZ\")\n\n        combine_xyz = nodes.new(type=\"ShaderNodeCombineXYZ\")\n\n        # Set Up Links\n        links.new(node_pattern.outputs[\"Color\"], node_ox.inputs[\"Color\"])  # Link Image Texture to Emission\n        links.new(node_coord.outputs[\"Normal\"], xyz_components.inputs[\"Vector\"])\n        # ZF\n        links.new(f_value.outputs[0], multiply1.inputs[1])\n        links.new(xyz_components.outputs[\"Z\"], multiply1.inputs[0])\n        # ZFr\n        links.new(fr_value.outputs[0], multiply2.inputs[1])\n        links.new(xyz_components.outputs[\"Z\"], multiply2.inputs[0])\n        # X / ZF\n        links.new(xyz_components.outputs[\"X\"], divide1.inputs[0])\n        links.new(multiply1.outputs[0], divide1.inputs[1])\n        # Y / ZFr\n        links.new(xyz_components.outputs[\"Y\"], divide2.inputs[0])\n        links.new(multiply2.outputs[0], divide2.inputs[1])\n        # Combine (X/ZF, Y/ZFr, 0)\n        links.new(divide1.outputs[0], combine_xyz.inputs[\"X\"])\n        links.new(divide2.outputs[0], combine_xyz.inputs[\"Y\"])\n        # Center image by offset\n        links.new(combine_xyz.outputs[\"Vector\"], center_image.inputs[0])\n        # Link Mapping to Image Texture\n        links.new(center_image.outputs[\"Vector\"], node_pattern.inputs[\"Vector\"])\n\n        Utility.insert_keyframe(self.blender_obj.data, \"use_projector\", frame)",
  "def get_energy(self, frame: Optional[int] = None) -> float:\n        \"\"\" Returns the energy of the light.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The energy at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return self.blender_obj.data.energy",
  "def get_color(self, frame: Optional[int] = None) -> Color:\n        \"\"\" Returns the RGB color of the light.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The color at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return self.blender_obj.data.color",
  "def get_distance(self, frame: Optional[int] = None) -> float:\n        \"\"\" Returns the falloff distance of the light (point where light is half the original intensity).\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The falloff distance at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return self.blender_obj.data.distance",
  "def get_type(self, frame: Optional[int] = None) -> str:\n        \"\"\" Returns the type of the light.\n\n        :param frame: The frame number which the value should be set to. If None is given, the current\n                      frame number is used.\n        :return: The type at the specified frame.\n        \"\"\"\n        with KeyFrame(frame):\n            return self.blender_obj.data.type",
  "def write_coco_annotations(output_dir: str, instance_segmaps: List[np.ndarray],\n                           instance_attribute_maps: List[dict],\n                           colors: List[np.ndarray], color_file_format: str = \"PNG\",\n                           mask_encoding_format: str = \"rle\", supercategory: str = \"coco_annotations\",\n                           append_to_existing_output: bool = True,\n                           jpg_quality: int = 95, label_mapping: Optional[LabelIdMapping] = None,\n                           file_prefix: str = \"\", indent: Optional[Union[int, str]] = None):\n    \"\"\" Writes coco annotations in the following steps:\n    1. Locate the seg images\n    2. Locate the rgb maps\n    3. Locate the seg mappings\n    4. Read color mappings\n    5. For each frame write the coco annotation\n\n    :param output_dir: Output directory to write the coco annotations\n    :param instance_segmaps: List of instance segmentation maps\n    :param instance_attribute_maps: per-frame mappings with idx, class and optionally supercategory/bop_dataset_name\n    :param colors: List of color images. Does not support stereo images, enter left and right inputs subsequently.\n    :param color_file_format: Format to save color images in\n    :param mask_encoding_format: Encoding format of the binary masks. Default: 'rle'. Available: 'rle', 'polygon'.\n    :param supercategory: name of the dataset/supercategory to filter for, e.g. a specific BOP dataset set\n                          by 'bop_dataset_name' or any loaded object with specified 'cp_supercategory'\n    :param append_to_existing_output: If true and if there is already a coco_annotations.json file in the output\n                                      directory, the new coco annotations will be appended to the existing file.\n                                      Also, the rgb images will be named such that there are no collisions.\n    :param jpg_quality: The desired quality level of the jpg encoding\n    :param label_mapping: The label mapping which should be used to label the categories based on their ids.\n                          If None, is given then the `name` field in the csv files is used or - if not existing -\n                          the category id itself is used.\n    :param file_prefix: Optional prefix for image file names\n    :param indent: If indent is a non-negative integer or string, then the annotation output\n                   will be pretty-printed with that indent level. An indent level of 0, negative, or \"\" will\n                   only insert newlines. None (the default) selects the most compact representation.\n                   Using a positive integer indent indents that many spaces per level.\n                   If indent is a string (such as \"\\t\"), that string is used to indent each level.\n    \"\"\"\n\n    if len(colors) > 0 and len(colors[0].shape) == 4:\n        raise ValueError(\"BlenderProc currently does not support writing coco annotations for stereo images. \"\n                         \"However, you can enter left and right images / segmaps separately.\")\n\n    # Create output directory\n    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n\n    coco_annotations_path = os.path.join(output_dir, \"coco_annotations.json\")\n    # Calculate image numbering offset, if append_to_existing_output is activated and coco data exists\n    if append_to_existing_output and os.path.exists(coco_annotations_path):\n        with open(coco_annotations_path, 'r', encoding=\"utf-8\") as fp:\n            existing_coco_annotations = json.load(fp)\n        image_offset = max(image[\"id\"] for image in existing_coco_annotations[\"images\"]) + 1\n    else:\n        image_offset = 0\n        existing_coco_annotations = None\n\n    # collect all RGB paths\n    new_coco_image_paths = []\n\n    # for each rendered frame\n    for frame in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n        color_rgb = colors[frame - bpy.context.scene.frame_start]\n\n        # Reverse channel order for opencv\n        color_bgr = color_rgb.copy()\n        color_bgr[..., :3] = color_bgr[..., :3][..., ::-1]\n\n        if color_file_format == 'PNG':\n            target_base_path = f'images/{file_prefix}{frame + image_offset:06d}.png'\n            target_path = os.path.join(output_dir, target_base_path)\n            cv2.imwrite(target_path, color_bgr)\n        elif color_file_format == 'JPEG':\n            target_base_path = f'images/{file_prefix}{frame + image_offset:06d}.jpg'\n            target_path = os.path.join(output_dir, target_base_path)\n            cv2.imwrite(target_path, color_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n        else:\n            raise RuntimeError(f'Unknown color_file_format={color_file_format}. Try \"PNG\" or \"JPEG\"')\n\n\n        new_coco_image_paths.append(target_base_path)\n\n    coco_output = _CocoWriterUtility.generate_coco_annotations(instance_segmaps,\n                                                               instance_attribute_maps,\n                                                               new_coco_image_paths,\n                                                               supercategory,\n                                                               mask_encoding_format,\n                                                               existing_coco_annotations,\n                                                               label_mapping)\n\n    print(\"Writing coco annotations to \" + coco_annotations_path)\n    with open(coco_annotations_path, 'w', encoding=\"utf-8\") as fp:\n        json.dump(coco_output, fp, indent=indent)",
  "def binary_mask_to_rle(binary_mask: np.ndarray) -> Dict[str, List[int]]:\n    \"\"\"Converts a binary mask to COCOs run-length encoding (RLE) format. Instead of outputting\n    a mask image, you give a list of start pixels and how many pixels after each of those\n    starts are included in the mask.\n    :param binary_mask: a 2D binary numpy array where '1's represent the object\n    :return: Mask in RLE format\n    \"\"\"\n    rle: Dict[str, List[int]] = {'counts': [], 'size': list(binary_mask.shape)}\n    counts = rle.get('counts')\n    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n        if i == 0 and value == 1:\n            counts.append(0)\n        counts.append(len(list(elements)))\n    return rle",
  "def rle_to_binary_mask(rle: Dict[str, List[int]]) -> np.ndarray:\n    \"\"\"Converts a COCOs run-length encoding (RLE) to binary mask.\n    :param rle: Mask in RLE format\n    :return: a 2D binary numpy array where '1's represent the object\n    \"\"\"\n    binary_array = np.zeros(np.prod(rle.get('size')), dtype=bool)\n    counts: List[int] = rle.get('counts')\n\n    start = 0\n    for i in range(len(counts) - 1):\n        start += counts[i]\n        end = start + counts[i + 1]\n        binary_array[start:end] = (i + 1) % 2\n\n    binary_mask = binary_array.reshape(*rle.get('size'), order='F')\n\n    return binary_mask",
  "class _CocoWriterUtility:\n\n    @staticmethod\n    def generate_coco_annotations(inst_segmaps, inst_attribute_maps, image_paths, supercategory,\n                                  mask_encoding_format, existing_coco_annotations=None,\n                                  label_mapping: LabelIdMapping = None):\n        \"\"\"Generates coco annotations for images\n\n        :param inst_segmaps: List of instance segmentation maps\n        :param inst_attribute_maps: per-frame mappings with idx, class and optionally supercategory/bop_dataset_name\n        :param image_paths: A list of paths which points to the rendered images.\n        :param supercategory: name of the dataset/supercategory to filter for, e.g. a specific BOP dataset\n        :param mask_encoding_format: Encoding format of the binary mask. Type: string.\n        :param existing_coco_annotations: If given, the new coco annotations will be appended to the given\n                                          coco annotations dict.\n        :param label_mapping: The label mapping which should be used to label the categories based on their ids.\n                              If None, is given then the `name` field in the csv files is used or - if not existing -\n                              the category id itself is used.\n        :return: dict containing coco annotations\n        \"\"\"\n\n        categories = []\n        visited_categories = []\n        instance_2_category_maps = []\n\n        for inst_attribute_map in inst_attribute_maps:\n            instance_2_category_map = {}\n            for inst in inst_attribute_map:\n                # skip background\n                if int(inst[\"category_id\"]) != 0:\n                    # take all objects or objects from specified supercategory is defined\n                    inst_supercategory = \"coco_annotations\"\n                    if \"bop_dataset_name\" in inst:\n                        inst_supercategory = inst[\"bop_dataset_name\"]\n                    elif \"supercategory\" in inst:\n                        inst_supercategory = inst[\"supercategory\"]\n\n                    if supercategory in [inst_supercategory, 'coco_annotations']:\n                        if int(inst[\"category_id\"]) not in visited_categories:\n                            cat_dict: Dict[str, Union[str, int]] = {'id': int(inst[\"category_id\"]),\n                                                                    'supercategory': inst_supercategory}\n                            # Determine name of category based on label_mapping, name or category_id\n                            if label_mapping is not None:\n                                cat_dict[\"name\"] = label_mapping.label_from_id(cat_dict['id'])\n                            elif \"name\" in inst:\n                                cat_dict[\"name\"] = inst[\"name\"]\n                            else:\n                                cat_dict[\"name\"] = inst[\"category_id\"]\n\n                            categories.append(cat_dict)\n                            visited_categories.append(cat_dict['id'])\n                        instance_2_category_map[int(inst[\"idx\"])] = int(inst[\"category_id\"])\n            instance_2_category_maps.append(instance_2_category_map)\n\n        licenses = [{\n            \"id\": 1,\n            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n        }]\n        info = {\n            \"description\": supercategory,\n            \"url\": \"https://github.com/waspinator/pycococreator\",\n            \"version\": \"0.1.0\",\n            \"year\": 2020,\n            \"contributor\": \"Unknown\",\n            \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n        }\n\n        images: List[Dict[str, Union[str, int]]] = []\n        annotations: List[Dict[str, Union[str, int]]] = []\n\n        for inst_segmap, image_path, instance_2_category_map in zip(inst_segmaps, image_paths,\n                                                                    instance_2_category_maps):\n\n            # Add coco info for image\n            image_id = len(images)\n            images.append(_CocoWriterUtility.create_image_info(image_id, image_path, inst_segmap.shape))\n\n            # Go through all objects visible in this image\n            instances = np.unique(inst_segmap)\n            # Remove background\n            instances = np.delete(instances, np.where(instances == 0))\n            for inst in instances:\n                if inst in instance_2_category_map:\n                    # Calc object mask\n                    binary_inst_mask = np.where(inst_segmap == inst, 1, 0)\n                    # Add coco info for object in this image\n                    annotation = _CocoWriterUtility.create_annotation_info(len(annotations) + 1,\n                                                                           image_id,\n                                                                           instance_2_category_map[inst],\n                                                                           binary_inst_mask,\n                                                                           mask_encoding_format)\n                    if annotation is not None:\n                        annotations.append(annotation)\n\n        new_coco_annotations = {\n            \"info\": info,\n            \"licenses\": licenses,\n            \"categories\": categories,\n            \"images\": images,\n            \"annotations\": annotations\n        }\n\n        if existing_coco_annotations is not None:\n            new_coco_annotations = _CocoWriterUtility.merge_coco_annotations(existing_coco_annotations,\n                                                                             new_coco_annotations)\n\n        return new_coco_annotations\n\n    @staticmethod\n    def merge_coco_annotations(existing_coco_annotations, new_coco_annotations):\n        \"\"\" Merges the two given coco annotation dicts into one.\n\n        Currently, this requires both coco annotations to have the exact same categories/objects.\n        The \"images\" and \"annotations\" sections are concatenated and respective ids are adjusted.\n\n        :param existing_coco_annotations: A dict describing the first coco annotations.\n        :param new_coco_annotations: A dict describing the second coco annotations.\n        :return: A dict containing the merged coco annotations.\n        \"\"\"\n\n        # Concatenate category sections\n        for cat_dict in new_coco_annotations[\"categories\"]:\n            if cat_dict not in existing_coco_annotations[\"categories\"]:\n                existing_coco_annotations[\"categories\"].append(cat_dict)\n\n        # Concatenate images sections\n        image_id_offset = max(image[\"id\"] for image in existing_coco_annotations[\"images\"]) + 1\n        for image in new_coco_annotations[\"images\"]:\n            image[\"id\"] += image_id_offset\n        existing_coco_annotations[\"images\"].extend(new_coco_annotations[\"images\"])\n\n        # Concatenate annotations sections\n        if len(existing_coco_annotations[\"annotations\"]) > 0:\n            annotation_id_offset = max(annotation[\"id\"] for annotation in existing_coco_annotations[\"annotations\"]) + 1\n        else:\n            annotation_id_offset = 0\n        for annotation in new_coco_annotations[\"annotations\"]:\n            annotation[\"id\"] += annotation_id_offset\n            annotation[\"image_id\"] += image_id_offset\n        existing_coco_annotations[\"annotations\"].extend(new_coco_annotations[\"annotations\"])\n\n        return existing_coco_annotations\n\n    @staticmethod\n    def create_image_info(image_id: int, file_name: str, image_size: Tuple[int, int]) -> Dict[str, Union[str, int]]:\n        \"\"\"Creates image info section of coco annotation\n\n        :param image_id: integer to uniquly identify image\n        :param file_name: filename for image\n        :param image_size: The size of the image, given as [W, H]\n        \"\"\"\n        image_info: Dict[str, Union[str, int]] = {\n            \"id\": image_id,\n            \"file_name\": file_name,\n            \"width\": image_size[1],\n            \"height\": image_size[0],\n            \"date_captured\": datetime.datetime.utcnow().isoformat(' '),\n            \"license\": 1,\n            \"coco_url\": \"\",\n            \"flickr_url\": \"\"\n        }\n\n        return image_info\n\n    @staticmethod\n    def create_annotation_info(annotation_id: int, image_id: int, category_id: int, binary_mask: np.ndarray,\n                               mask_encoding_format: str, tolerance: int = 2) -> Optional[Dict[str, Union[str, int]]]:\n        \"\"\"Creates info section of coco annotation\n\n        :param annotation_id: integer to uniquly identify the annotation\n        :param image_id: integer to uniquly identify image\n        :param category_id: Id of the category\n        :param binary_mask: A binary image mask of the object with the shape [H, W].\n        :param mask_encoding_format: Encoding format of the mask. Type: string.\n        :param tolerance: The tolerance for fitting polygons to the objects mask.\n        \"\"\"\n\n        area = _CocoWriterUtility.calc_binary_mask_area(binary_mask)\n        if area < 1:\n            return None\n\n        bounding_box = _CocoWriterUtility.bbox_from_binary_mask(binary_mask)\n\n        if mask_encoding_format == 'rle':\n            segmentation = binary_mask_to_rle(binary_mask)\n        elif mask_encoding_format == 'polygon':\n            segmentation = _CocoWriterUtility.binary_mask_to_polygon(binary_mask, tolerance)\n            if not segmentation:\n                return None\n        else:\n            raise RuntimeError(f\"Unknown encoding format: {mask_encoding_format}\")\n\n        annotation_info: Dict[str, Union[str, int]] = {\n            \"id\": annotation_id,\n            \"image_id\": image_id,\n            \"category_id\": category_id,\n            \"iscrowd\": 0,\n            \"area\": area,\n            \"bbox\": bounding_box,\n            \"segmentation\": segmentation,\n            \"width\": binary_mask.shape[1],\n            \"height\": binary_mask.shape[0],\n        }\n        return annotation_info\n\n    @staticmethod\n    def bbox_from_binary_mask(binary_mask: np.ndarray) -> List[int]:\n        \"\"\" Returns the smallest bounding box containing all pixels marked \"1\" in the given image mask.\n\n        :param binary_mask: A binary image mask with the shape [H, W].\n        :return: The bounding box represented as [x, y, width, height]\n        \"\"\"\n        # Find all columns and rows that contain 1s\n        rows = np.any(binary_mask, axis=1)\n        cols = np.any(binary_mask, axis=0)\n        # Find the min and max col/row index that contain 1s\n        rmin, rmax = np.where(rows)[0][[0, -1]]\n        cmin, cmax = np.where(cols)[0][[0, -1]]\n        # Calc height and width\n        h = rmax - rmin + 1\n        w = cmax - cmin + 1\n        return [int(cmin), int(rmin), int(w), int(h)]\n\n    @staticmethod\n    def calc_binary_mask_area(binary_mask: np.ndarray) -> int:\n        \"\"\" Returns the area of the given binary mask which is defined as the number of 1s in the mask.\n\n        :param binary_mask: A binary image mask with the shape [H, W].\n        :return: The computed area\n        \"\"\"\n        return binary_mask.sum().tolist()\n\n    @staticmethod\n    def close_contour(contour: np.ndarray) -> np.ndarray:\n        \"\"\" Makes sure the given contour is closed.\n\n        :param contour: The contour to close.\n        :return: The closed contour.\n        \"\"\"\n        # If first != last point => add first point to end of contour to close it\n        if not np.array_equal(contour[0], contour[-1]):\n            contour = np.vstack((contour, contour[0]))\n        return contour\n\n    @staticmethod\n    def binary_mask_to_polygon(binary_mask: np.ndarray, tolerance: int = 0) -> List[np.ndarray]:\n        \"\"\"Converts a binary mask to COCO polygon representation\n\n         :param binary_mask: a 2D binary numpy array where '1's represent the object\n         :param tolerance: Maximum distance from original points of polygon to approximated polygonal chain. If\n                           tolerance is 0, the original coordinate array is returned.\n        \"\"\"\n        polygons = []\n        # pad mask to close contours of shapes which start and end at an edge\n        padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n        contours = np.array(measure.find_contours(padded_binary_mask, 0.5))\n        # Reverse padding\n        contours -= 1\n        for contour in contours:\n            # Make sure contour is closed\n            contour = _CocoWriterUtility.close_contour(contour)\n            # Approximate contour by polygon\n            polygon = measure.approximate_polygon(contour, tolerance)\n            # Skip invalid polygons\n            if len(polygon) < 3:\n                continue\n            # Flip xy to yx point representation\n            polygon = np.flip(polygon, axis=1)\n            # Flatten\n            polygon = polygon.ravel()\n            # after padding and subtracting 1 we may get -0.5 points in our segmentation\n            polygon[polygon < 0] = 0\n            polygons.append(polygon.tolist())\n\n        return polygons",
  "def generate_coco_annotations(inst_segmaps, inst_attribute_maps, image_paths, supercategory,\n                                  mask_encoding_format, existing_coco_annotations=None,\n                                  label_mapping: LabelIdMapping = None):\n        \"\"\"Generates coco annotations for images\n\n        :param inst_segmaps: List of instance segmentation maps\n        :param inst_attribute_maps: per-frame mappings with idx, class and optionally supercategory/bop_dataset_name\n        :param image_paths: A list of paths which points to the rendered images.\n        :param supercategory: name of the dataset/supercategory to filter for, e.g. a specific BOP dataset\n        :param mask_encoding_format: Encoding format of the binary mask. Type: string.\n        :param existing_coco_annotations: If given, the new coco annotations will be appended to the given\n                                          coco annotations dict.\n        :param label_mapping: The label mapping which should be used to label the categories based on their ids.\n                              If None, is given then the `name` field in the csv files is used or - if not existing -\n                              the category id itself is used.\n        :return: dict containing coco annotations\n        \"\"\"\n\n        categories = []\n        visited_categories = []\n        instance_2_category_maps = []\n\n        for inst_attribute_map in inst_attribute_maps:\n            instance_2_category_map = {}\n            for inst in inst_attribute_map:\n                # skip background\n                if int(inst[\"category_id\"]) != 0:\n                    # take all objects or objects from specified supercategory is defined\n                    inst_supercategory = \"coco_annotations\"\n                    if \"bop_dataset_name\" in inst:\n                        inst_supercategory = inst[\"bop_dataset_name\"]\n                    elif \"supercategory\" in inst:\n                        inst_supercategory = inst[\"supercategory\"]\n\n                    if supercategory in [inst_supercategory, 'coco_annotations']:\n                        if int(inst[\"category_id\"]) not in visited_categories:\n                            cat_dict: Dict[str, Union[str, int]] = {'id': int(inst[\"category_id\"]),\n                                                                    'supercategory': inst_supercategory}\n                            # Determine name of category based on label_mapping, name or category_id\n                            if label_mapping is not None:\n                                cat_dict[\"name\"] = label_mapping.label_from_id(cat_dict['id'])\n                            elif \"name\" in inst:\n                                cat_dict[\"name\"] = inst[\"name\"]\n                            else:\n                                cat_dict[\"name\"] = inst[\"category_id\"]\n\n                            categories.append(cat_dict)\n                            visited_categories.append(cat_dict['id'])\n                        instance_2_category_map[int(inst[\"idx\"])] = int(inst[\"category_id\"])\n            instance_2_category_maps.append(instance_2_category_map)\n\n        licenses = [{\n            \"id\": 1,\n            \"name\": \"Attribution-NonCommercial-ShareAlike License\",\n            \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\"\n        }]\n        info = {\n            \"description\": supercategory,\n            \"url\": \"https://github.com/waspinator/pycococreator\",\n            \"version\": \"0.1.0\",\n            \"year\": 2020,\n            \"contributor\": \"Unknown\",\n            \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n        }\n\n        images: List[Dict[str, Union[str, int]]] = []\n        annotations: List[Dict[str, Union[str, int]]] = []\n\n        for inst_segmap, image_path, instance_2_category_map in zip(inst_segmaps, image_paths,\n                                                                    instance_2_category_maps):\n\n            # Add coco info for image\n            image_id = len(images)\n            images.append(_CocoWriterUtility.create_image_info(image_id, image_path, inst_segmap.shape))\n\n            # Go through all objects visible in this image\n            instances = np.unique(inst_segmap)\n            # Remove background\n            instances = np.delete(instances, np.where(instances == 0))\n            for inst in instances:\n                if inst in instance_2_category_map:\n                    # Calc object mask\n                    binary_inst_mask = np.where(inst_segmap == inst, 1, 0)\n                    # Add coco info for object in this image\n                    annotation = _CocoWriterUtility.create_annotation_info(len(annotations) + 1,\n                                                                           image_id,\n                                                                           instance_2_category_map[inst],\n                                                                           binary_inst_mask,\n                                                                           mask_encoding_format)\n                    if annotation is not None:\n                        annotations.append(annotation)\n\n        new_coco_annotations = {\n            \"info\": info,\n            \"licenses\": licenses,\n            \"categories\": categories,\n            \"images\": images,\n            \"annotations\": annotations\n        }\n\n        if existing_coco_annotations is not None:\n            new_coco_annotations = _CocoWriterUtility.merge_coco_annotations(existing_coco_annotations,\n                                                                             new_coco_annotations)\n\n        return new_coco_annotations",
  "def merge_coco_annotations(existing_coco_annotations, new_coco_annotations):\n        \"\"\" Merges the two given coco annotation dicts into one.\n\n        Currently, this requires both coco annotations to have the exact same categories/objects.\n        The \"images\" and \"annotations\" sections are concatenated and respective ids are adjusted.\n\n        :param existing_coco_annotations: A dict describing the first coco annotations.\n        :param new_coco_annotations: A dict describing the second coco annotations.\n        :return: A dict containing the merged coco annotations.\n        \"\"\"\n\n        # Concatenate category sections\n        for cat_dict in new_coco_annotations[\"categories\"]:\n            if cat_dict not in existing_coco_annotations[\"categories\"]:\n                existing_coco_annotations[\"categories\"].append(cat_dict)\n\n        # Concatenate images sections\n        image_id_offset = max(image[\"id\"] for image in existing_coco_annotations[\"images\"]) + 1\n        for image in new_coco_annotations[\"images\"]:\n            image[\"id\"] += image_id_offset\n        existing_coco_annotations[\"images\"].extend(new_coco_annotations[\"images\"])\n\n        # Concatenate annotations sections\n        if len(existing_coco_annotations[\"annotations\"]) > 0:\n            annotation_id_offset = max(annotation[\"id\"] for annotation in existing_coco_annotations[\"annotations\"]) + 1\n        else:\n            annotation_id_offset = 0\n        for annotation in new_coco_annotations[\"annotations\"]:\n            annotation[\"id\"] += annotation_id_offset\n            annotation[\"image_id\"] += image_id_offset\n        existing_coco_annotations[\"annotations\"].extend(new_coco_annotations[\"annotations\"])\n\n        return existing_coco_annotations",
  "def create_image_info(image_id: int, file_name: str, image_size: Tuple[int, int]) -> Dict[str, Union[str, int]]:\n        \"\"\"Creates image info section of coco annotation\n\n        :param image_id: integer to uniquly identify image\n        :param file_name: filename for image\n        :param image_size: The size of the image, given as [W, H]\n        \"\"\"\n        image_info: Dict[str, Union[str, int]] = {\n            \"id\": image_id,\n            \"file_name\": file_name,\n            \"width\": image_size[1],\n            \"height\": image_size[0],\n            \"date_captured\": datetime.datetime.utcnow().isoformat(' '),\n            \"license\": 1,\n            \"coco_url\": \"\",\n            \"flickr_url\": \"\"\n        }\n\n        return image_info",
  "def create_annotation_info(annotation_id: int, image_id: int, category_id: int, binary_mask: np.ndarray,\n                               mask_encoding_format: str, tolerance: int = 2) -> Optional[Dict[str, Union[str, int]]]:\n        \"\"\"Creates info section of coco annotation\n\n        :param annotation_id: integer to uniquly identify the annotation\n        :param image_id: integer to uniquly identify image\n        :param category_id: Id of the category\n        :param binary_mask: A binary image mask of the object with the shape [H, W].\n        :param mask_encoding_format: Encoding format of the mask. Type: string.\n        :param tolerance: The tolerance for fitting polygons to the objects mask.\n        \"\"\"\n\n        area = _CocoWriterUtility.calc_binary_mask_area(binary_mask)\n        if area < 1:\n            return None\n\n        bounding_box = _CocoWriterUtility.bbox_from_binary_mask(binary_mask)\n\n        if mask_encoding_format == 'rle':\n            segmentation = binary_mask_to_rle(binary_mask)\n        elif mask_encoding_format == 'polygon':\n            segmentation = _CocoWriterUtility.binary_mask_to_polygon(binary_mask, tolerance)\n            if not segmentation:\n                return None\n        else:\n            raise RuntimeError(f\"Unknown encoding format: {mask_encoding_format}\")\n\n        annotation_info: Dict[str, Union[str, int]] = {\n            \"id\": annotation_id,\n            \"image_id\": image_id,\n            \"category_id\": category_id,\n            \"iscrowd\": 0,\n            \"area\": area,\n            \"bbox\": bounding_box,\n            \"segmentation\": segmentation,\n            \"width\": binary_mask.shape[1],\n            \"height\": binary_mask.shape[0],\n        }\n        return annotation_info",
  "def bbox_from_binary_mask(binary_mask: np.ndarray) -> List[int]:\n        \"\"\" Returns the smallest bounding box containing all pixels marked \"1\" in the given image mask.\n\n        :param binary_mask: A binary image mask with the shape [H, W].\n        :return: The bounding box represented as [x, y, width, height]\n        \"\"\"\n        # Find all columns and rows that contain 1s\n        rows = np.any(binary_mask, axis=1)\n        cols = np.any(binary_mask, axis=0)\n        # Find the min and max col/row index that contain 1s\n        rmin, rmax = np.where(rows)[0][[0, -1]]\n        cmin, cmax = np.where(cols)[0][[0, -1]]\n        # Calc height and width\n        h = rmax - rmin + 1\n        w = cmax - cmin + 1\n        return [int(cmin), int(rmin), int(w), int(h)]",
  "def calc_binary_mask_area(binary_mask: np.ndarray) -> int:\n        \"\"\" Returns the area of the given binary mask which is defined as the number of 1s in the mask.\n\n        :param binary_mask: A binary image mask with the shape [H, W].\n        :return: The computed area\n        \"\"\"\n        return binary_mask.sum().tolist()",
  "def close_contour(contour: np.ndarray) -> np.ndarray:\n        \"\"\" Makes sure the given contour is closed.\n\n        :param contour: The contour to close.\n        :return: The closed contour.\n        \"\"\"\n        # If first != last point => add first point to end of contour to close it\n        if not np.array_equal(contour[0], contour[-1]):\n            contour = np.vstack((contour, contour[0]))\n        return contour",
  "def binary_mask_to_polygon(binary_mask: np.ndarray, tolerance: int = 0) -> List[np.ndarray]:\n        \"\"\"Converts a binary mask to COCO polygon representation\n\n         :param binary_mask: a 2D binary numpy array where '1's represent the object\n         :param tolerance: Maximum distance from original points of polygon to approximated polygonal chain. If\n                           tolerance is 0, the original coordinate array is returned.\n        \"\"\"\n        polygons = []\n        # pad mask to close contours of shapes which start and end at an edge\n        padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n        contours = np.array(measure.find_contours(padded_binary_mask, 0.5))\n        # Reverse padding\n        contours -= 1\n        for contour in contours:\n            # Make sure contour is closed\n            contour = _CocoWriterUtility.close_contour(contour)\n            # Approximate contour by polygon\n            polygon = measure.approximate_polygon(contour, tolerance)\n            # Skip invalid polygons\n            if len(polygon) < 3:\n                continue\n            # Flip xy to yx point representation\n            polygon = np.flip(polygon, axis=1)\n            # Flatten\n            polygon = polygon.ravel()\n            # after padding and subtracting 1 we may get -0.5 points in our segmentation\n            polygon[polygon < 0] = 0\n            polygons.append(polygon.tolist())\n\n        return polygons",
  "def write_gif_animation(\n        output_dir_path: str,\n        output_data_dict: Dict[str, List[Union[np.ndarray, list, dict]]],\n        append_to_existing_output: bool = False,\n        frame_duration_in_ms: int = 50,\n        reverse_animation: bool = False):\n    \"\"\"\n    Generates a .gif file animation out of rendered frames\n\n    :param output_dir_path: The directory path in which the gif animation folder will be saved\n    :param output_data_dict: The data dictionary which was produced by the render method.\n    :param append_to_existing_output: If this is True, the output_dir_path folder will be scanned for pre-existing\n                            files of the name #_animation.gif and the number of newly added files will\n                            start right where the last run left off.\n    :param frame_duration_in_ms: Duration of each frame in the animation in milliseconds.\n    :param reverse_animation: If this is True, the order of the frames will be reversed.\n    \"\"\"\n\n    # Generates subdirectory for .gif files\n    output_dir_path = _GifWriterUtility.provide_directory(output_dir_path)\n\n    # From WriterUtility.py\n    amount_of_frames = 0\n    for data_block in output_data_dict.values():\n        if isinstance(data_block, list):\n            amount_of_frames = max([amount_of_frames, len(data_block)])\n\n    # From WriterUtility.py\n    if amount_of_frames != bpy.context.scene.frame_end - bpy.context.scene.frame_start:\n        raise RuntimeError(\"The amount of images stored in the output_data_dict does not correspond with the amount\"\n                           \"of images specified by frame_start to frame_end.\")\n\n    # Sorts out keys which are just metadata and not plottable\n    keys_to_use = _GifWriterUtility.select_keys(output_data_dict)\n\n    # Build temporary folders with .png collections\n    to_animate = _GifWriterUtility.cache_png(keys_to_use, output_data_dict)\n\n    # Write the cache .png files to .gif files and delete cache\n    _GifWriterUtility.write_to_gif(to_animate, output_dir_path,\n                                   append_to_existing_output,\n                                   frame_duration_in_ms,\n                                   reverse_animation)",
  "class _GifWriterUtility:\n\n    @staticmethod\n    def provide_directory(output_dir_path: str) -> str:\n        \"\"\" Generates subdirectory for .gif files if not existent \"\"\"\n        output_dir_path = os.path.join(output_dir_path, 'gif_animations')\n        if not os.path.exists(output_dir_path):\n            print(f\"\\n Generate output folder: {output_dir_path}\")\n            os.makedirs(output_dir_path)\n        return output_dir_path\n\n    @staticmethod\n    def select_keys(output_data_dict: Dict[str, List[Union[np.ndarray, list, dict]]]) -> List[str]:\n        \"\"\" Sorts out keys which are just metadata and not plottable \"\"\"\n\n        def is_image(x: Union[np.ndarray, list, dict]) -> bool:\n            \"\"\" Checks if the input x is not a string and is not a vector \"\"\"\n            x = np.array(x)\n            return not np.issubdtype(x.dtype, np.string_) and len(x.shape) != 1\n\n        return [key for key, value in output_data_dict.items()\n                if len(value) > 0 and is_image(value[0])]\n\n    @staticmethod\n    def cache_png(keys_to_use: List[str],\n                  output_data_dict: Dict[str, List[Union[np.ndarray, list, dict]]]) -> Dict[str, List[str]]:\n        \"\"\"\n        Builds temporary folders with .png collections\n        and returns the locations as dictionary.\n        \"\"\"\n\n        if not set(keys_to_use) <= set(output_data_dict.keys()):\n            raise ValueError(\"The keys_to_use list must be contained in the list of keys from the output_data_dict!\")\n\n        to_animate = {}\n        for key in keys_to_use:\n            value = output_data_dict[key][0]\n            value = np.array(value)\n            # Check if frames are rendered in stereo vision\n            if value.shape[0] == 2:\n                # stereo images\n                for index, perspective in enumerate(['_L', '_R']):\n                    to_animate[key + perspective] = []\n                    for number, frame in enumerate(output_data_dict[key]):\n                        file_path = os.path.join(\n                            Utility.get_temporary_directory(),\n                            f'{number}_{key}_{perspective}.png')\n                        to_animate[key + perspective].append(file_path)\n                        vis_data(key=key,\n                                 data=frame[index],\n                                 save_to_file=file_path)\n            else:\n                # non stereo images\n                to_animate[key] = []\n                for number, frame in enumerate(output_data_dict[key]):\n                    file_path = os.path.join(\n                        Utility.get_temporary_directory(),\n                        f'{number}_{key}.png')\n                    to_animate[key].append(file_path)\n                    vis_data(key=key,\n                             data=frame,\n                             save_to_file=file_path)\n        return to_animate\n\n    @staticmethod\n    def look_for_existing_output(output_dir_path: str, append_to_existing_output: bool, name_ending: str) -> int:\n        \"\"\"\n        Looks for the highest existing #.gif number and adapts respectively\n        \"\"\"\n        if append_to_existing_output:\n            gif_number = 0\n            for path in os.listdir(output_dir_path):\n                if path.endswith(name_ending):\n                    index = path[:-len(name_ending)]\n                    if index.isdigit():\n                        gif_number = max(gif_number, int(index) + 1)\n        else:\n            gif_number = 0\n        return gif_number\n\n    @staticmethod\n    def write_to_gif(to_animate: Dict[str, list],\n                     output_dir_path: str,\n                     append_to_existing_output: bool,\n                     frame_duration_in_ms: int,\n                     reverse_animation: bool) -> None:\n        \"\"\"\n        Loads all .png files from each specific temporary folder and concatenates them to a single gif file respectively\n        \"\"\"\n        for key, frame_list in to_animate.items():\n            print(f'gif for {key}')\n            if reverse_animation:\n                frame_list.reverse()\n            # loads actual picture data as frames\n            frames = [Image.open(path) for path in frame_list]\n\n            gif_number = _GifWriterUtility.look_for_existing_output(output_dir_path, append_to_existing_output,\n                                                                    f\"_{key}_animation.gif\")\n            file_name = f\"{gif_number}_{key}_animation.gif\"\n            file = os.path.join(output_dir_path, file_name)\n            frames[0].save(file, format='GIF', append_images=frames[1:], save_all=True, duration=frame_duration_in_ms,\n                           loop=0)",
  "def provide_directory(output_dir_path: str) -> str:\n        \"\"\" Generates subdirectory for .gif files if not existent \"\"\"\n        output_dir_path = os.path.join(output_dir_path, 'gif_animations')\n        if not os.path.exists(output_dir_path):\n            print(f\"\\n Generate output folder: {output_dir_path}\")\n            os.makedirs(output_dir_path)\n        return output_dir_path",
  "def select_keys(output_data_dict: Dict[str, List[Union[np.ndarray, list, dict]]]) -> List[str]:\n        \"\"\" Sorts out keys which are just metadata and not plottable \"\"\"\n\n        def is_image(x: Union[np.ndarray, list, dict]) -> bool:\n            \"\"\" Checks if the input x is not a string and is not a vector \"\"\"\n            x = np.array(x)\n            return not np.issubdtype(x.dtype, np.string_) and len(x.shape) != 1\n\n        return [key for key, value in output_data_dict.items()\n                if len(value) > 0 and is_image(value[0])]",
  "def cache_png(keys_to_use: List[str],\n                  output_data_dict: Dict[str, List[Union[np.ndarray, list, dict]]]) -> Dict[str, List[str]]:\n        \"\"\"\n        Builds temporary folders with .png collections\n        and returns the locations as dictionary.\n        \"\"\"\n\n        if not set(keys_to_use) <= set(output_data_dict.keys()):\n            raise ValueError(\"The keys_to_use list must be contained in the list of keys from the output_data_dict!\")\n\n        to_animate = {}\n        for key in keys_to_use:\n            value = output_data_dict[key][0]\n            value = np.array(value)\n            # Check if frames are rendered in stereo vision\n            if value.shape[0] == 2:\n                # stereo images\n                for index, perspective in enumerate(['_L', '_R']):\n                    to_animate[key + perspective] = []\n                    for number, frame in enumerate(output_data_dict[key]):\n                        file_path = os.path.join(\n                            Utility.get_temporary_directory(),\n                            f'{number}_{key}_{perspective}.png')\n                        to_animate[key + perspective].append(file_path)\n                        vis_data(key=key,\n                                 data=frame[index],\n                                 save_to_file=file_path)\n            else:\n                # non stereo images\n                to_animate[key] = []\n                for number, frame in enumerate(output_data_dict[key]):\n                    file_path = os.path.join(\n                        Utility.get_temporary_directory(),\n                        f'{number}_{key}.png')\n                    to_animate[key].append(file_path)\n                    vis_data(key=key,\n                             data=frame,\n                             save_to_file=file_path)\n        return to_animate",
  "def look_for_existing_output(output_dir_path: str, append_to_existing_output: bool, name_ending: str) -> int:\n        \"\"\"\n        Looks for the highest existing #.gif number and adapts respectively\n        \"\"\"\n        if append_to_existing_output:\n            gif_number = 0\n            for path in os.listdir(output_dir_path):\n                if path.endswith(name_ending):\n                    index = path[:-len(name_ending)]\n                    if index.isdigit():\n                        gif_number = max(gif_number, int(index) + 1)\n        else:\n            gif_number = 0\n        return gif_number",
  "def write_to_gif(to_animate: Dict[str, list],\n                     output_dir_path: str,\n                     append_to_existing_output: bool,\n                     frame_duration_in_ms: int,\n                     reverse_animation: bool) -> None:\n        \"\"\"\n        Loads all .png files from each specific temporary folder and concatenates them to a single gif file respectively\n        \"\"\"\n        for key, frame_list in to_animate.items():\n            print(f'gif for {key}')\n            if reverse_animation:\n                frame_list.reverse()\n            # loads actual picture data as frames\n            frames = [Image.open(path) for path in frame_list]\n\n            gif_number = _GifWriterUtility.look_for_existing_output(output_dir_path, append_to_existing_output,\n                                                                    f\"_{key}_animation.gif\")\n            file_name = f\"{gif_number}_{key}_animation.gif\"\n            file = os.path.join(output_dir_path, file_name)\n            frames[0].save(file, format='GIF', append_images=frames[1:], save_all=True, duration=frame_duration_in_ms,\n                           loop=0)",
  "def is_image(x: Union[np.ndarray, list, dict]) -> bool:\n            \"\"\" Checks if the input x is not a string and is not a vector \"\"\"\n            x = np.array(x)\n            return not np.issubdtype(x.dtype, np.string_) and len(x.shape) != 1",
  "def write_bop(output_dir: str, target_objects: Optional[List[MeshObject]] = None,\n              depths: List[np.ndarray] = None, colors: List[np.ndarray] = None,\n              color_file_format: str = \"PNG\", dataset: str = \"\", append_to_existing_output: bool = True,\n              depth_scale: float = 1.0, jpg_quality: int = 95, save_world2cam: bool = True,\n              ignore_dist_thres: float = 100., m2mm: Optional[bool] = None, annotation_unit: str = 'mm',\n              frames_per_chunk: int = 1000, calc_mask_info_coco: bool = True, delta: float = 0.015):\n    \"\"\"Write the BOP data\n\n    :param output_dir: Path to the output directory.\n    :param target_objects: Objects for which to save ground truth poses in BOP format. Default: Save all objects or\n                           from specified dataset\n    :param depths: List of depth images in m to save\n    :param colors: List of color images to save\n    :param color_file_format: File type to save color images. Available: \"PNG\", \"JPEG\"\n    :param jpg_quality: If color_file_format is \"JPEG\", save with the given quality.\n    :param dataset: Only save annotations for objects of the specified bop dataset. Saves all object poses if undefined.\n    :param append_to_existing_output: If true, the new frames will be appended to the existing ones.\n    :param depth_scale: Multiply the uint16 output depth image with this factor to get depth in mm. Used to trade-off\n                        between depth accuracy and maximum depth value. Default corresponds to 65.54m maximum depth\n                        and 1mm accuracy.\n    :param save_world2cam: If true, camera to world transformations \"cam_R_w2c\", \"cam_t_w2c\" are saved\n                           in scene_camera.json\n    :param ignore_dist_thres: Distance between camera and object after which object is ignored. Mostly due to\n                              failed physics.\n    :param m2mm: Original bop annotations and models are in mm. If true, we convert the gt annotations to mm here. This\n                 is needed if BopLoader option mm2m is used (deprecated).\n    :param annotation_unit: The unit in which the annotations are saved. Available: 'm', 'dm', 'cm', 'mm'.\n    :param frames_per_chunk: Number of frames saved in each chunk (called scene in BOP)\n    :param calc_mask_info_coco: Whether to calculate gt masks, gt info and gt coco annotations.\n    :param delta: Tolerance used for estimation of the visibility masks (in [m]).\n    \"\"\"\n\n    # Output paths.\n    dataset_dir = os.path.join(output_dir, dataset)\n    chunks_dir = os.path.join(dataset_dir, 'train_pbr')\n    camera_path = os.path.join(dataset_dir, 'camera.json')\n\n    # Create the output directory structure.\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n        os.makedirs(chunks_dir)\n    elif not append_to_existing_output:\n        raise FileExistsError(f\"The output folder already exists: {dataset_dir}\")\n\n    # Select target objects or objects from the specified dataset or all objects\n    if target_objects is not None:\n        dataset_objects = target_objects\n    elif dataset:\n        dataset_objects = []\n        for obj in get_all_mesh_objects():\n            if \"bop_dataset_name\" in obj.blender_obj and not obj.blender_obj.hide_render:\n                if obj.blender_obj[\"bop_dataset_name\"] == dataset:\n                    dataset_objects.append(obj)\n    else:\n        dataset_objects = get_all_mesh_objects()\n\n    # Check if there is any object from the specified dataset.\n    if not dataset_objects:\n        raise RuntimeError(f\"The scene does not contain any object from the specified dataset: {dataset}. \"\n                           f\"Either remove the dataset parameter or assign custom property 'bop_dataset_name'\"\n                           f\" to selected objects\")\n\n    if calc_mask_info_coco:\n        # It might be that a chunk dir already exists where the writer appends frames.\n        # If one (or multiple) more chunk dirs are created to save the rendered frames to,\n        # mask/info/coco annotations need to be calculated for all of them\n        chunk_dirs = sorted(glob.glob(os.path.join(chunks_dir, '*')))\n        chunk_dirs = [d for d in chunk_dirs if os.path.isdir(d)]\n        last_chunk_dir = sorted(chunk_dirs)[-1] if chunk_dirs else None\n\n        starting_chunk_id = 0\n        starting_frame_id = 0\n        if last_chunk_dir:\n            last_chunk_gt_fpath = os.path.join(last_chunk_dir, 'scene_gt.json')\n            chunk_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n\n            # Current chunk and frame ID's.\n            starting_chunk_id = int(os.path.basename(last_chunk_dir))\n            starting_frame_id = int(sorted(chunk_gt.keys())[-1]) + 1\n\n            if starting_frame_id % frames_per_chunk == 0:\n                starting_chunk_id += 1\n                starting_frame_id = 0\n\n    # Save the data.\n    _BopWriterUtility.write_camera(camera_path, depth_scale=depth_scale)\n    assert annotation_unit in ['m', 'dm', 'cm', 'mm'], (f\"Invalid annotation unit: `{annotation_unit}`. Supported \"\n                                                        f\"are 'm', 'dm', 'cm', 'mm'\")\n    annotation_scale = {'m': 1., 'dm': 10., 'cm': 100., 'mm': 1000.}[annotation_unit]\n    if m2mm is not None:\n        warnings.warn(\"WARNING: `m2mm` is deprecated, please use `annotation_scale='mm'` instead!\")\n        annotation_scale = 1000.\n    _BopWriterUtility.write_frames(chunks_dir, dataset_objects=dataset_objects, depths=depths, colors=colors,\n                                   color_file_format=color_file_format, frames_per_chunk=frames_per_chunk,\n                                   annotation_scale=annotation_scale, ignore_dist_thres=ignore_dist_thres,\n                                   save_world2cam=save_world2cam, depth_scale=depth_scale, jpg_quality=jpg_quality)\n\n    if calc_mask_info_coco:\n        # Set up the bop toolkit\n        SetupUtility.setup_pip([\"git+https://github.com/thodan/bop_toolkit\", \"PyOpenGL==3.1.0\"])\n\n        # determine which objects to add to the vsipy renderer\n        # for numpy>=1.20, np.float is deprecated: https://numpy.org/doc/stable/release/1.20.0-notes.html#deprecations\n        np.float = float\n\n        # Determine for which directories mask_info_coco has to be calculated\n        chunk_dirs = sorted(glob.glob(os.path.join(chunks_dir, '*')))\n        chunk_dirs = [d for d in chunk_dirs if os.path.isdir(d)]\n        chunk_dir_ids = [d.split('/')[-1] for d in chunk_dirs]\n        chunk_dirs = chunk_dirs[chunk_dir_ids.index(f\"{starting_chunk_id:06d}\"):]\n\n        # convert all objects to trimesh objects\n        trimesh_objects = {}\n        for obj in dataset_objects:\n            if obj.get_cp('category_id') in trimesh_objects:\n                continue\n            if isinstance(obj, Link):\n                if not obj.visuals:\n                    continue\n                if len(obj.visuals) > 1:\n                    warnings.warn('BOP Writer only supports saving annotations of one visual mesh per Link')\n            trimesh_obj = obj.mesh_as_trimesh()\n            # we need to create a double-sided material to be able to render non-watertight meshes\n            # the other parameters are defaults, see\n            # https://github.com/mmatl/pyrender/blob/master/pyrender/mesh.py#L216-L223\n            material = pyrender.MetallicRoughnessMaterial(alphaMode='BLEND', baseColorFactor=[0.3, 0.3, 0.3, 1.0],\n                                                          metallicFactor=0.2, roughnessFactor=0.8, doubleSided=True)\n            # here we also add the scale factor of the objects. the position of the pyrender camera will change based\n            # on the initial scale factor of the objects and the saved annotation format\n            if not np.all(np.isclose(np.array(obj.blender_obj.scale), obj.blender_obj.scale[0])):\n                print(\"WARNING: the scale is not the same across all dimensions, writing bop_toolkit annotations with \"\n                      \"the bop writer will fail!\")\n            trimesh_objects[obj.get_cp('category_id')] = pyrender.Mesh.from_trimesh(mesh=trimesh_obj, material=material)\n\n        _BopWriterUtility.calc_gt_masks(chunk_dirs=chunk_dirs, starting_frame_id=starting_frame_id,\n                                        dataset_objects=trimesh_objects, annotation_scale=annotation_scale,\n                                        delta=delta)\n        _BopWriterUtility.calc_gt_info(chunk_dirs=chunk_dirs, starting_frame_id=starting_frame_id,\n                                       dataset_objects=trimesh_objects, annotation_scale=annotation_scale,\n                                       delta=delta)\n        _BopWriterUtility.calc_gt_coco(chunk_dirs=chunk_dirs, dataset_objects=dataset_objects,\n                                       starting_frame_id=starting_frame_id)",
  "def bop_pose_to_pyrender_coordinate_system(cam_R_m2c: np.ndarray, cam_t_m2c: np.ndarray) -> np.ndarray:\n    \"\"\" Converts an object pose in bop format to pyrender camera coordinate system\n        (https://pyrender.readthedocs.io/en/latest/examples/cameras.html).\n\n    :param cam_R_m2c: 3x3 Rotation matrix.\n    :param cam_t_m2c: Translation vector.\n    :return: Pose in pyrender coordinate system.\n    \"\"\"\n    # create homogeneous transformation matrix\n    bop_pose = np.eye(4)\n    bop_pose[:3, :3] = cam_R_m2c\n    bop_pose[:3, 3] = cam_t_m2c\n\n    return change_target_coordinate_frame_of_transformation_matrix(bop_pose, [\"X\", \"-Y\", \"-Z\"])",
  "class _BopWriterUtility:\n    \"\"\" Saves the synthesized dataset in the BOP format. The dataset is split\n        into chunks which are saved as individual \"scenes\". For more details\n        about the BOP format, visit the BOP toolkit docs:\n        https://github.com/thodan/bop_toolkit/blob/master/docs/bop_datasets_format.md\n\n    \"\"\"\n\n    @staticmethod\n    def load_json(path, keys_to_int=False):\n        \"\"\"Loads content of a JSON file.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit).\n\n        :param path: Path to the JSON file.\n        :param keys_to_int: Convert digit dict keys to integers. Default: False\n        :return: Content of the loaded JSON file.\n        \"\"\"\n\n        # Keys to integers.\n        def convert_keys_to_int(x):\n            return {int(k) if k.lstrip('-').isdigit() else k: v for k, v in x.items()}\n\n        with open(path, 'r', encoding=\"utf-8\") as f:\n            if keys_to_int:\n                content = json.load(f, object_hook=convert_keys_to_int)\n            else:\n                content = json.load(f)\n\n        return content\n\n    @staticmethod\n    def save_json(path, content):\n        \"\"\" Saves the content to a JSON file in a human-friendly format.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit).\n\n        :param path: Path to the output JSON file.\n        :param content: Dictionary/list to save.\n        \"\"\"\n        text = \"\"\n        with open(path, 'w', encoding=\"utf-8\") as file:\n            if isinstance(content, dict):\n                text += '{\\n'\n                content_sorted = sorted(content.items(), key=lambda x: x[0])\n                for elem_id, (k, v) in enumerate(content_sorted):\n                    text += f'  \"{k}\": {json.dumps(v, sort_keys=True)}'\n                    if elem_id != len(content) - 1:\n                        text += ','\n                    text += '\\n'\n                text += '}'\n                file.write(text)\n            elif isinstance(content, list):\n                text += '[\\n'\n                for elem_id, elem in enumerate(content):\n                    text += f'  {json.dumps(elem, sort_keys=True)}'\n                    if elem_id != len(content) - 1:\n                        text += ','\n                    text += '\\n'\n                text += ']'\n                file.write(text)\n            else:\n                json.dump(content, file, sort_keys=True)\n\n    @staticmethod\n    def save_depth(path: str, im: np.ndarray):\n        \"\"\"Saves a depth image (16-bit) to a PNG file.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit).\n\n        :param path: Path to the output depth image file.\n        :param im: ndarray with the depth image to save.\n        \"\"\"\n        if not path.endswith(\".png\"):\n            raise ValueError('Only PNG format is currently supported.')\n\n        im[im > 65535] = 65535\n        im_uint16 = np.round(im).astype(np.uint16)\n\n        # PyPNG library can save 16-bit PNG and is faster than imageio.imwrite().\n        w_depth = png.Writer(im.shape[1], im.shape[0], greyscale=True, bitdepth=16)\n        with open(path, 'wb') as f:\n            w_depth.write(f, np.reshape(im_uint16, (-1, im.shape[1])))\n\n    @staticmethod\n    def write_camera(camera_path: str, depth_scale: float = 1.0):\n        \"\"\" Writes camera.json into dataset_dir.\n        :param camera_path: Path to camera.json\n        :param depth_scale: Multiply the uint16 output depth image with this factor to get depth in mm.\n        \"\"\"\n        # Use second frame for reading intrinsics (due to backwards compatibility)\n        bpy.context.scene.frame_set(1)\n        cam_K = _WriterUtility.get_cam_attribute(bpy.context.scene.camera, 'cam_K')\n        camera = {'cx': cam_K[0][2],\n                  'cy': cam_K[1][2],\n                  'depth_scale': depth_scale,\n                  'fx': cam_K[0][0],\n                  'fy': cam_K[1][1],\n                  'height': bpy.context.scene.render.resolution_y,\n                  'width': bpy.context.scene.render.resolution_x}\n\n        _BopWriterUtility.save_json(camera_path, camera)\n\n    @staticmethod\n    def get_frame_gt(dataset_objects: List[bpy.types.Mesh], unit_scaling: float, ignore_dist_thres: float,\n                     destination_frame: Optional[List[str]] = None):\n        \"\"\" Returns GT pose annotations between active camera and objects.\n        :param dataset_objects: Save annotations for these objects.\n        :param unit_scaling: 1000. for outputting poses in mm\n        :param ignore_dist_thres: Distance between camera and object after which object is ignored.\n                                  Mostly due to failed physics.\n        :param destination_frame: Transform poses from Blender internal coordinates to OpenCV coordinates\n        :return: A list of GT camera-object pose annotations for scene_gt.json\n        \"\"\"\n        if destination_frame is None:\n            destination_frame = [\"X\", \"-Y\", \"-Z\"]\n\n        H_c2w_opencv = Matrix(_WriterUtility.get_cam_attribute(bpy.context.scene.camera, 'cam2world_matrix',\n                                                               local_frame_change=destination_frame))\n\n        frame_gt = []\n        for obj in dataset_objects:\n            if isinstance(obj, Link):\n                if not obj.visuals:\n                    continue\n                if len(obj.visuals) > 1:\n                    warnings.warn('BOP Writer only supports saving poses of one visual mesh per Link')\n                H_m2w = Matrix(obj.get_visual_local2world_mats()[0])\n            else:\n                H_m2w = Matrix(obj.get_local2world_mat())\n                assert obj.has_cp(\"category_id\"), f\"{obj.get_name()} object has no custom property 'category_id'\"\n\n            cam_H_m2c = H_c2w_opencv.inverted() @ H_m2w\n            cam_R_m2c = cam_H_m2c.to_quaternion().to_matrix()\n            cam_t_m2c = cam_H_m2c.to_translation()\n\n            # ignore examples that fell through the plane\n            if not np.linalg.norm(list(cam_t_m2c)) > ignore_dist_thres:\n                cam_t_m2c = list(cam_t_m2c * unit_scaling)\n                frame_gt.append({\n                    'cam_R_m2c': list(cam_R_m2c[0]) + list(cam_R_m2c[1]) + list(cam_R_m2c[2]),\n                    'cam_t_m2c': cam_t_m2c,\n                    'obj_id': obj.get_cp(\"category_id\") if not isinstance(obj, Link) else obj.visuals[0].get_cp(\n                        'category_id')\n                })\n            else:\n                print('ignored obj, ', obj.get_cp(\"category_id\"), 'because either ')\n                print('(1) it is further away than parameter \"ignore_dist_thres: \",', ignore_dist_thres)\n                print('(e.g. because it fell through a plane during physics sim)')\n                print('or')\n                print('(2) the object pose has not been given in meters')\n\n        return frame_gt\n\n    @staticmethod\n    def get_frame_camera(save_world2cam: bool, depth_scale: float = 1.0, unit_scaling: float = 1000.,\n                         destination_frame: Optional[List[str]] = None):\n        \"\"\" Returns camera parameters for the active camera.\n        :param save_world2cam: If true, camera to world transformations \"cam_R_w2c\", \"cam_t_w2c\" are saved\n                               in scene_camera.json\n        :param depth_scale: Multiply the uint16 output depth image with this factor to get depth in mm.\n        :param unit_scaling: 1000. for outputting poses in mm\n        :param destination_frame: Transform poses from Blender internal coordinates to OpenCV coordinates\n        :return: dict containing info for scene_camera.json\n        \"\"\"\n        if destination_frame is None:\n            destination_frame = [\"X\", \"-Y\", \"-Z\"]\n\n        cam_K = _WriterUtility.get_cam_attribute(bpy.context.scene.camera, 'cam_K')\n\n        frame_camera_dict = {\n            'cam_K': cam_K[0] + cam_K[1] + cam_K[2],\n            'depth_scale': depth_scale\n        }\n\n        if save_world2cam:\n            H_c2w_opencv = Matrix(_WriterUtility.get_cam_attribute(bpy.context.scene.camera, 'cam2world_matrix',\n                                                                   local_frame_change=destination_frame))\n\n            H_w2c_opencv = H_c2w_opencv.inverted()\n            R_w2c_opencv = H_w2c_opencv.to_quaternion().to_matrix()\n            t_w2c_opencv = H_w2c_opencv.to_translation() * unit_scaling\n\n            frame_camera_dict['cam_R_w2c'] = list(R_w2c_opencv[0]) + list(R_w2c_opencv[1]) + list(R_w2c_opencv[2])\n            frame_camera_dict['cam_t_w2c'] = list(t_w2c_opencv)\n\n        return frame_camera_dict\n\n    @staticmethod\n    def write_frames(chunks_dir: str, dataset_objects: list, depths: List[np.ndarray],\n                     colors: List[np.ndarray], color_file_format: str = \"PNG\",\n                     depth_scale: float = 1.0, frames_per_chunk: int = 1000, annotation_scale: float = 1000.,\n                     ignore_dist_thres: float = 100., save_world2cam: bool = True, jpg_quality: int = 95):\n        \"\"\"Write each frame's ground truth into chunk directory in BOP format\n\n        :param chunks_dir: Path to the output directory of the current chunk.\n        :param dataset_objects: Save annotations for these objects.\n        :param depths: List of depth images in m to save\n        :param colors: List of color images to save\n        :param color_file_format: File type to save color images. Available: \"PNG\", \"JPEG\"\n        :param jpg_quality: If color_file_format is \"JPEG\", save with the given quality.\n        :param depth_scale: Multiply the uint16 output depth image with this factor to get depth in mm. Used to\n                            trade-off between depth accuracy and maximum depth value. Default corresponds to\n                            65.54m maximum depth and 1mm accuracy.\n        :param ignore_dist_thres: Distance between camera and object after which object is ignored.\n                                  Mostly due to failed physics.\n        :param annotation_scale: The scale factor applied to the calculated annotations (in [m]) to get them into the\n                                 specified format (see `annotation_format` in `write_bop` for further details).\n        :param frames_per_chunk: Number of frames saved in each chunk (called scene in BOP)\n        \"\"\"\n\n        # Format of the depth images.\n        depth_ext = '.png'\n\n        rgb_tpath = os.path.join(chunks_dir, '{chunk_id:06d}', 'rgb', '{im_id:06d}' + '{im_type}')\n        depth_tpath = os.path.join(chunks_dir, '{chunk_id:06d}', 'depth', '{im_id:06d}' + depth_ext)\n        chunk_camera_tpath = os.path.join(chunks_dir, '{chunk_id:06d}', 'scene_camera.json')\n        chunk_gt_tpath = os.path.join(chunks_dir, '{chunk_id:06d}', 'scene_gt.json')\n\n        # Paths to the already existing chunk folders (such folders may exist\n        # when appending to an existing dataset).\n        chunk_dirs = sorted(glob.glob(os.path.join(chunks_dir, '*')))\n        chunk_dirs = [d for d in chunk_dirs if os.path.isdir(d)]\n\n        # Get ID's of the last already existing chunk and frame.\n        curr_chunk_id = 0\n        curr_frame_id = 0\n        if len(chunk_dirs):\n            last_chunk_dir = sorted(chunk_dirs)[-1]\n            last_chunk_gt_fpath = os.path.join(last_chunk_dir, 'scene_gt.json')\n            chunk_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n\n            # Last chunk and frame ID's.\n            last_chunk_id = int(os.path.basename(last_chunk_dir))\n            last_frame_id = int(sorted(chunk_gt.keys())[-1])\n\n            # Current chunk and frame ID's.\n            curr_chunk_id = last_chunk_id\n            curr_frame_id = last_frame_id + 1\n            if curr_frame_id % frames_per_chunk == 0:\n                curr_chunk_id += 1\n                curr_frame_id = 0\n\n        # Initialize structures for the GT annotations and camera info.\n        chunk_gt = {}\n        chunk_camera = {}\n        if curr_frame_id != 0:\n            # Load GT and camera info of the chunk we are appending to.\n            chunk_gt = _BopWriterUtility.load_json(\n                chunk_gt_tpath.format(chunk_id=curr_chunk_id), keys_to_int=True)\n            chunk_camera = _BopWriterUtility.load_json(\n                chunk_camera_tpath.format(chunk_id=curr_chunk_id), keys_to_int=True)\n\n        # Go through all frames.\n        num_new_frames = bpy.context.scene.frame_end - bpy.context.scene.frame_start\n\n        if len(depths) != len(colors) != num_new_frames:\n            raise Exception(\"The amount of images stored in the depths/colors does not correspond to the amount\"\n                            \"of images specified by frame_start to frame_end.\")\n\n        for frame_id in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n            # Activate frame.\n            bpy.context.scene.frame_set(frame_id)\n\n            # Reset data structures and prepare folders for a new chunk.\n            if curr_frame_id == 0:\n                chunk_gt = {}\n                chunk_camera = {}\n                os.makedirs(os.path.dirname(\n                    rgb_tpath.format(chunk_id=curr_chunk_id, im_id=0, im_type='PNG')))\n                os.makedirs(os.path.dirname(\n                    depth_tpath.format(chunk_id=curr_chunk_id, im_id=0)))\n\n            # Get GT annotations and camera info for the current frame.\n            chunk_gt[curr_frame_id] = _BopWriterUtility.get_frame_gt(dataset_objects, annotation_scale,\n                                                                     ignore_dist_thres)\n            chunk_camera[curr_frame_id] = _BopWriterUtility.get_frame_camera(save_world2cam, depth_scale,\n                                                                             annotation_scale)\n\n            color_rgb = colors[frame_id]\n            color_bgr = color_rgb.copy()\n            color_bgr[..., :3] = color_bgr[..., :3][..., ::-1]\n            if color_file_format == 'PNG':\n                rgb_fpath = rgb_tpath.format(chunk_id=curr_chunk_id, im_id=curr_frame_id, im_type='.png')\n                cv2.imwrite(rgb_fpath, color_bgr)\n            elif color_file_format == 'JPEG':\n                rgb_fpath = rgb_tpath.format(chunk_id=curr_chunk_id, im_id=curr_frame_id, im_type='.jpg')\n                cv2.imwrite(rgb_fpath, color_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n\n            depth = depths[frame_id]\n\n            # Scale the depth to retain a higher precision (the depth is saved\n            # as a 16-bit PNG image with range 0-65535).\n            depth_mm = 1000.0 * depth  # [m] -> [mm]\n            depth_mm_scaled = depth_mm / float(depth_scale)\n\n            # Save the scaled depth image.\n            depth_fpath = depth_tpath.format(chunk_id=curr_chunk_id, im_id=curr_frame_id)\n            _BopWriterUtility.save_depth(depth_fpath, depth_mm_scaled)\n\n            # Save the chunk info if we are at the end of a chunk or at the last new frame.\n            if ((curr_frame_id + 1) % frames_per_chunk == 0) or \\\n                    (frame_id == num_new_frames - 1):\n\n                # Save GT annotations.\n                _BopWriterUtility.save_json(chunk_gt_tpath.format(chunk_id=curr_chunk_id), chunk_gt)\n\n                # Save camera info.\n                _BopWriterUtility.save_json(chunk_camera_tpath.format(chunk_id=curr_chunk_id), chunk_camera)\n\n                # Update ID's.\n                curr_chunk_id += 1\n                curr_frame_id = 0\n            else:\n                curr_frame_id += 1\n\n    @staticmethod\n    def calc_gt_masks(chunk_dirs: List[str], dataset_objects: Dict[int, pyrender.Mesh], starting_frame_id: int = 0,\n                      annotation_scale: float = 1000., delta: float = 0.015):\n        \"\"\" Calculates the ground truth masks.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit), with the difference of using pyrender for depth\n        rendering.\n\n        :param chunk_dirs: List of directories to calculate the gt masks for.\n        :param dataset_objects: Dict containing all objects to save the annotations for.\n        :param starting_frame_id: The first frame id the writer has written during this run.\n        :param annotation_scale: The scale factor applied to the calculated annotations (in [m]) to get them into the\n                                 specified format (see `annotation_format` in `write_bop` for further details).\n        :param delta: Tolerance used for estimation of the visibility masks.\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n        # pylint: disable=import-outside-toplevel\n        from bop_toolkit_lib import inout, misc, visibility\n        # pylint: enable=import-outside-toplevel\n\n        width = bpy.context.scene.render.resolution_x\n        height = bpy.context.scene.render.resolution_y\n\n        renderer = pyrender.OffscreenRenderer(viewport_width=width, viewport_height=height)\n\n        for dir_counter, chunk_dir in enumerate(chunk_dirs):\n            last_chunk_gt_fpath = os.path.join(chunk_dir, 'scene_gt.json')\n            last_chunk_camera_fpath = os.path.join(chunk_dir, 'scene_camera.json')\n            scene_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n            scene_camera = _BopWriterUtility.load_json(last_chunk_camera_fpath, keys_to_int=True)\n\n            # Create folders for the output masks (if they do not exist yet).\n            mask_dir_path = os.path.dirname(os.path.join(chunk_dir, 'mask', '000000_000000.png'))\n            misc.ensure_dir(mask_dir_path)\n\n            mask_visib_dir_path = os.path.dirname(os.path.join(chunk_dir, 'mask_visib', '000000_000000.png'))\n            misc.ensure_dir(mask_visib_dir_path)\n\n            im_ids = sorted(scene_gt.keys())\n\n            # append to existing output\n            if dir_counter == 0:\n                im_ids = im_ids[starting_frame_id:]\n\n            for im_counter, im_id in enumerate(im_ids):\n                if im_counter % 100 == 0:\n                    misc.log(f'Calculating GT masks - {chunk_dir}, {im_counter}')\n\n                K = np.array(scene_camera[im_id]['cam_K']).reshape(3, 3)\n                fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n                camera = pyrender.IntrinsicsCamera(fx=fx, fy=fy, cx=cx, cy=cy, znear=0.1, zfar=100000)\n\n                # Load depth image.\n                depth_path = os.path.join(\n                    chunk_dir, 'depth', '{im_id:06d}.png').format(im_id=im_id)\n                depth_im = inout.load_depth(depth_path)\n                depth_im *= scene_camera[im_id]['depth_scale']  # to [mm]\n                depth_im /= 1000.  # to [m]\n                dist_im = misc.depth_im_to_dist_im_fast(depth_im, K)\n\n                for gt_id, gt in enumerate(scene_gt[im_id]):\n                    # create a new scene\n                    scene = pyrender.Scene()\n\n                    # add camera and current object\n                    scene.add(camera)\n                    t = np.array(gt['cam_t_m2c'])\n                    # rescale translation depending on initial saving format\n                    t /= annotation_scale\n\n                    pose = bop_pose_to_pyrender_coordinate_system(cam_R_m2c=np.array(gt['cam_R_m2c']).reshape(3, 3),\n                                                                  cam_t_m2c=t)\n                    scene.add(dataset_objects[gt['obj_id']], pose=pose)\n\n                    # Render the depth image.\n                    _, depth_gt = renderer.render(scene=scene)\n\n                    # Convert depth image to distance image.\n                    dist_gt = misc.depth_im_to_dist_im_fast(depth_gt, K)\n\n                    # Mask of the full object silhouette.\n                    mask = dist_gt > 0\n\n                    # Mask of the visible part of the object silhouette.\n                    mask_visib = visibility.estimate_visib_mask_gt(\n                        dist_im, dist_gt, delta, visib_mode='bop19')\n\n                    # Save the calculated masks.\n                    mask_path = os.path.join(\n                        chunk_dir, 'mask', '{im_id:06d}_{gt_id:06d}.png').format(im_id=im_id, gt_id=gt_id)\n                    inout.save_im(mask_path, 255 * mask.astype(np.uint8))\n\n                    mask_visib_path = os.path.join(\n                        chunk_dir, 'mask_visib',\n                        '{im_id:06d}_{gt_id:06d}.png').format(im_id=im_id, gt_id=gt_id)\n                    inout.save_im(mask_visib_path, 255 * mask_visib.astype(np.uint8))\n\n    @staticmethod\n    def calc_gt_info(chunk_dirs: List[str], dataset_objects: Dict[int, pyrender.Mesh], starting_frame_id: int = 0,\n                     annotation_scale: float = 1000., delta: float = 0.015):\n        \"\"\" Calculates the ground truth masks.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit), with the difference of using pyrender for depth\n        rendering.\n\n        :param chunk_dirs: List of directories to calculate the gt info for.\n        :param dataset_objects: Dict containing all objects to save the annotations for.\n        :param starting_frame_id: The first frame id the writer has written during this run.\n        :param annotation_scale: The scale factor applied to the calculated annotations (in [m]) to get them into the\n                                 specified format (see `annotation_format` in `write_bop` for further details).\n        :param delta: Tolerance used for estimation of the visibility masks.\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n        # pylint: disable=import-outside-toplevel\n        from bop_toolkit_lib import inout, misc, visibility\n        # pylint: enable=import-outside-toplevel\n\n        im_width, im_height = bpy.context.scene.render.resolution_x, bpy.context.scene.render.resolution_y\n        ren_width, ren_height = 3 * im_width, 3 * im_height\n        ren_cx_offset, ren_cy_offset = im_width, im_height\n        renderer = pyrender.OffscreenRenderer(viewport_width=ren_width, viewport_height=ren_height)\n\n        for dir_counter, chunk_dir in enumerate(chunk_dirs):\n            last_chunk_gt_fpath = os.path.join(chunk_dir, 'scene_gt.json')\n            last_chunk_camera_fpath = os.path.join(chunk_dir, 'scene_camera.json')\n            scene_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n            scene_camera = _BopWriterUtility.load_json(last_chunk_camera_fpath, keys_to_int=True)\n\n            # load existing gt info\n            if dir_counter == 0 and starting_frame_id > 0:\n                misc.log(f\"Loading gt info from existing chunk dir - {chunk_dir}\")\n                scene_gt_info = _BopWriterUtility.load_json(os.path.join(chunk_dir, 'scene_gt_info.json'),\n                                                            keys_to_int=True)\n            else:\n                scene_gt_info = {}\n\n            im_ids = sorted(scene_gt.keys())\n\n            # append to existing output\n            if dir_counter == 0:\n                im_ids = im_ids[starting_frame_id:]\n\n            for im_counter, im_id in enumerate(im_ids):\n                if im_counter % 100 == 0:\n                    misc.log(f'Calculating GT info - {chunk_dir}, {im_counter}')\n\n                # Load depth image.\n                depth_fpath = os.path.join(chunk_dir, 'depth', '{im_id:06d}.png').format(im_id=im_id)\n                assert os.path.isfile(depth_fpath)\n                depth = inout.load_depth(depth_fpath)\n                depth *= scene_camera[im_id]['depth_scale']  # Convert to [mm].\n                depth /= 1000.  # to [m]\n\n                K = np.array(scene_camera[im_id]['cam_K']).reshape(3, 3)\n                fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n                im_size = (depth.shape[1], depth.shape[0])\n                camera = pyrender.IntrinsicsCamera(fx=fx, fy=fy, cx=cx+ren_cx_offset, cy=cy+ren_cy_offset, znear=0.1,\n                                                   zfar=100000)\n\n                scene_gt_info[im_id] = []\n                for gt in scene_gt[im_id]:\n                    # create a new scene\n                    scene = pyrender.Scene()\n\n                    # add camera and current object\n                    scene.add(camera)\n                    t = np.array(gt['cam_t_m2c'])\n                    # rescale translation depending on initial saving format\n                    t /= annotation_scale\n                    pose = bop_pose_to_pyrender_coordinate_system(cam_R_m2c=np.array(gt['cam_R_m2c']).reshape(3, 3),\n                                                                  cam_t_m2c=t)\n                    scene.add(dataset_objects[gt['obj_id']], pose=pose)\n\n                    # render the depth image\n                    _, depth_gt_large = renderer.render(scene=scene)\n                    depth_gt = depth_gt_large[\n                               ren_cy_offset:(ren_cy_offset + im_height),\n                               ren_cx_offset:(ren_cx_offset + im_width)]\n\n                    # Convert depth images to distance images.\n                    dist_gt = misc.depth_im_to_dist_im_fast(depth_gt, K)\n                    dist_im = misc.depth_im_to_dist_im_fast(depth, K)\n\n                    # Estimation of the visibility mask.\n                    visib_gt = visibility.estimate_visib_mask_gt(\n                        dist_im, dist_gt, delta, visib_mode='bop19')\n\n                    # Mask of the object in the GT pose.\n                    obj_mask_gt_large = depth_gt_large > 0\n                    obj_mask_gt = dist_gt > 0\n\n                    # Number of pixels in the whole object silhouette\n                    # (even in the truncated part).\n                    px_count_all = np.sum(obj_mask_gt_large)\n\n                    # Number of pixels in the object silhouette with a valid depth measurement\n                    # (i.e. with a non-zero value in the depth image).\n                    px_count_valid = np.sum(dist_im[obj_mask_gt] > 0)\n\n                    # Number of pixels in the visible part of the object silhouette.\n                    px_count_visib = visib_gt.sum()\n\n                    # Visible surface fraction.\n                    if px_count_all > 0:\n                        visib_fract = px_count_visib / float(px_count_all)\n                    else:\n                        visib_fract = 0.0\n\n                    # Bounding box of the whole object silhouette\n                    # (including the truncated part).\n                    bbox = [-1, -1, -1, -1]\n                    if px_count_visib > 0:\n                        ys, xs = obj_mask_gt_large.nonzero()\n                        ys -= ren_cy_offset\n                        xs -= ren_cx_offset\n                        bbox = misc.calc_2d_bbox(xs, ys, im_size)\n\n                    # Bounding box of the visible surface part.\n                    bbox_visib = [-1, -1, -1, -1]\n                    if px_count_visib > 0:\n                        ys, xs = visib_gt.nonzero()\n                        bbox_visib = misc.calc_2d_bbox(xs, ys, im_size)\n\n                    # Store the calculated info.\n                    scene_gt_info[im_id].append({\n                        'px_count_all': int(px_count_all),\n                        'px_count_valid': int(px_count_valid),\n                        'px_count_visib': int(px_count_visib),\n                        'visib_fract': float(visib_fract),\n                        'bbox_obj': [int(e) for e in bbox],\n                        'bbox_visib': [int(e) for e in bbox_visib]\n                    })\n\n            # Save the info for the current scene.\n            scene_gt_info_path = os.path.join(chunk_dir, 'scene_gt_info.json')\n            misc.ensure_dir(os.path.dirname(scene_gt_info_path))\n            inout.save_json(scene_gt_info_path, scene_gt_info)\n\n    @staticmethod\n    def calc_gt_coco(chunk_dirs: List[str], dataset_objects: List[MeshObject], starting_frame_id: int = 0):\n        \"\"\" Calculates the COCO annotations.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit).\n\n        :param chunk_dirs: List of directories to calculate the gt coco annotations for.\n        :param dataset_objects: List containing all objects to save the annotations for.\n        :param starting_frame_id: The first frame id the writer has written during this run.\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n        # pylint: disable=import-outside-toplevel\n        from bop_toolkit_lib import inout, misc, pycoco_utils\n        # pylint: enable=import-outside-toplevel\n\n        for dir_counter, chunk_dir in enumerate(chunk_dirs):\n            dataset_name = chunk_dir.split('/')[-3]\n\n            CATEGORIES = [{'id': obj.get_cp('category_id'), 'name': str(obj.get_cp('category_id')), 'supercategory':\n                          dataset_name} for obj in dataset_objects]\n\n            # Remove all duplicate dicts from list.\n            # Ref: https://stackoverflow.com/questions/9427163/remove-duplicate-dict-in-list-in-python\n            CATEGORIES = list({frozenset(item.items()):item for item in CATEGORIES}.values())\n\n            INFO = {\n                \"description\": dataset_name + '_train',\n                \"url\": \"https://github.com/thodan/bop_toolkit\",\n                \"version\": \"0.1.0\",\n                \"year\": datetime.date.today().year,\n                \"contributor\": \"\",\n                \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n            }\n\n            # load existing coco annotations\n            if dir_counter == 0 and starting_frame_id > 0:\n                misc.log(f\"Loading coco annotations from existing chunk dir - {chunk_dir}\")\n                coco_scene_output = _BopWriterUtility.load_json(os.path.join(chunk_dir, 'scene_gt_coco.json'))\n                if coco_scene_output[\"annotations\"]:\n                    segmentation_id = coco_scene_output[\"annotations\"][-1]['id'] + 1\n                else:\n                    segmentation_id = 1\n            else:\n                coco_scene_output = {\n                    \"info\": INFO,\n                    \"licenses\": [],\n                    \"categories\": CATEGORIES,\n                    \"images\": [],\n                    \"annotations\": []\n                }\n                segmentation_id = 1\n\n            # Load info about the GT poses (e.g. visibility) for the current scene.\n            last_chunk_gt_fpath = os.path.join(chunk_dir, 'scene_gt.json')\n            scene_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n            last_chunk_gt_info_fpath = os.path.join(chunk_dir, 'scene_gt_info.json')\n            scene_gt_info = inout.load_json(last_chunk_gt_info_fpath, keys_to_int=True)\n            # Output coco path\n            coco_gt_path = os.path.join(chunk_dir, 'scene_gt_coco.json')\n            misc.log(f'Calculating COCO annotations - {chunk_dir}')\n\n            # Go through each view in scene_gt\n            for scene_view, inst_list in scene_gt.items():\n                im_id = int(scene_view)\n\n                # skip already existing annotations\n                if dir_counter == 0 and im_id < starting_frame_id:\n                    continue\n\n                img_path = os.path.join(chunk_dir, 'rgb', '{im_id:06d}.jpg').format(im_id=im_id)\n                relative_img_path = os.path.relpath(img_path, os.path.dirname(coco_gt_path))\n                im_size = (bpy.context.scene.render.resolution_x, bpy.context.scene.render.resolution_y)\n                image_info = pycoco_utils.create_image_info(im_id, relative_img_path, im_size)\n                coco_scene_output[\"images\"].append(image_info)\n                gt_info = scene_gt_info[scene_view]\n\n                # Go through each instance in view\n                for idx, inst in enumerate(inst_list):\n                    category_info = inst['obj_id']\n                    visibility = gt_info[idx]['visib_fract']\n                    # Add ignore flag for objects smaller than 10% visible\n                    ignore_gt = visibility < 0.1\n                    mask_visib_p = os.path.join(\n                        chunk_dir, 'mask_visib',\n                        '{im_id:06d}_{gt_id:06d}.png').format(im_id=im_id, gt_id=idx)\n                    mask_full_p = os.path.join(\n                        chunk_dir, 'mask', '{im_id:06d}_{gt_id:06d}.png').format(im_id=im_id, gt_id=idx)\n\n                    binary_inst_mask_visib = inout.load_depth(mask_visib_p).astype(bool)\n                    if binary_inst_mask_visib.sum() < 1:\n                        continue\n\n                    # use `amodal` bbox type per default\n                    binary_inst_mask_full = inout.load_depth(mask_full_p).astype(bool)\n                    if binary_inst_mask_full.sum() < 1:\n                        continue\n                    bounding_box = pycoco_utils.bbox_from_binary_mask(binary_inst_mask_full)\n\n                    annotation_info = pycoco_utils.create_annotation_info(\n                        segmentation_id, im_id, category_info, binary_inst_mask_visib, bounding_box, tolerance=2,\n                        ignore=ignore_gt)\n\n                    if annotation_info is not None:\n                        coco_scene_output[\"annotations\"].append(annotation_info)\n\n                    segmentation_id += 1\n\n            with open(coco_gt_path, 'w', encoding='utf-8') as output_json_file:\n                json.dump(coco_scene_output, output_json_file)",
  "def load_json(path, keys_to_int=False):\n        \"\"\"Loads content of a JSON file.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit).\n\n        :param path: Path to the JSON file.\n        :param keys_to_int: Convert digit dict keys to integers. Default: False\n        :return: Content of the loaded JSON file.\n        \"\"\"\n\n        # Keys to integers.\n        def convert_keys_to_int(x):\n            return {int(k) if k.lstrip('-').isdigit() else k: v for k, v in x.items()}\n\n        with open(path, 'r', encoding=\"utf-8\") as f:\n            if keys_to_int:\n                content = json.load(f, object_hook=convert_keys_to_int)\n            else:\n                content = json.load(f)\n\n        return content",
  "def save_json(path, content):\n        \"\"\" Saves the content to a JSON file in a human-friendly format.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit).\n\n        :param path: Path to the output JSON file.\n        :param content: Dictionary/list to save.\n        \"\"\"\n        text = \"\"\n        with open(path, 'w', encoding=\"utf-8\") as file:\n            if isinstance(content, dict):\n                text += '{\\n'\n                content_sorted = sorted(content.items(), key=lambda x: x[0])\n                for elem_id, (k, v) in enumerate(content_sorted):\n                    text += f'  \"{k}\": {json.dumps(v, sort_keys=True)}'\n                    if elem_id != len(content) - 1:\n                        text += ','\n                    text += '\\n'\n                text += '}'\n                file.write(text)\n            elif isinstance(content, list):\n                text += '[\\n'\n                for elem_id, elem in enumerate(content):\n                    text += f'  {json.dumps(elem, sort_keys=True)}'\n                    if elem_id != len(content) - 1:\n                        text += ','\n                    text += '\\n'\n                text += ']'\n                file.write(text)\n            else:\n                json.dump(content, file, sort_keys=True)",
  "def save_depth(path: str, im: np.ndarray):\n        \"\"\"Saves a depth image (16-bit) to a PNG file.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit).\n\n        :param path: Path to the output depth image file.\n        :param im: ndarray with the depth image to save.\n        \"\"\"\n        if not path.endswith(\".png\"):\n            raise ValueError('Only PNG format is currently supported.')\n\n        im[im > 65535] = 65535\n        im_uint16 = np.round(im).astype(np.uint16)\n\n        # PyPNG library can save 16-bit PNG and is faster than imageio.imwrite().\n        w_depth = png.Writer(im.shape[1], im.shape[0], greyscale=True, bitdepth=16)\n        with open(path, 'wb') as f:\n            w_depth.write(f, np.reshape(im_uint16, (-1, im.shape[1])))",
  "def write_camera(camera_path: str, depth_scale: float = 1.0):\n        \"\"\" Writes camera.json into dataset_dir.\n        :param camera_path: Path to camera.json\n        :param depth_scale: Multiply the uint16 output depth image with this factor to get depth in mm.\n        \"\"\"\n        # Use second frame for reading intrinsics (due to backwards compatibility)\n        bpy.context.scene.frame_set(1)\n        cam_K = _WriterUtility.get_cam_attribute(bpy.context.scene.camera, 'cam_K')\n        camera = {'cx': cam_K[0][2],\n                  'cy': cam_K[1][2],\n                  'depth_scale': depth_scale,\n                  'fx': cam_K[0][0],\n                  'fy': cam_K[1][1],\n                  'height': bpy.context.scene.render.resolution_y,\n                  'width': bpy.context.scene.render.resolution_x}\n\n        _BopWriterUtility.save_json(camera_path, camera)",
  "def get_frame_gt(dataset_objects: List[bpy.types.Mesh], unit_scaling: float, ignore_dist_thres: float,\n                     destination_frame: Optional[List[str]] = None):\n        \"\"\" Returns GT pose annotations between active camera and objects.\n        :param dataset_objects: Save annotations for these objects.\n        :param unit_scaling: 1000. for outputting poses in mm\n        :param ignore_dist_thres: Distance between camera and object after which object is ignored.\n                                  Mostly due to failed physics.\n        :param destination_frame: Transform poses from Blender internal coordinates to OpenCV coordinates\n        :return: A list of GT camera-object pose annotations for scene_gt.json\n        \"\"\"\n        if destination_frame is None:\n            destination_frame = [\"X\", \"-Y\", \"-Z\"]\n\n        H_c2w_opencv = Matrix(_WriterUtility.get_cam_attribute(bpy.context.scene.camera, 'cam2world_matrix',\n                                                               local_frame_change=destination_frame))\n\n        frame_gt = []\n        for obj in dataset_objects:\n            if isinstance(obj, Link):\n                if not obj.visuals:\n                    continue\n                if len(obj.visuals) > 1:\n                    warnings.warn('BOP Writer only supports saving poses of one visual mesh per Link')\n                H_m2w = Matrix(obj.get_visual_local2world_mats()[0])\n            else:\n                H_m2w = Matrix(obj.get_local2world_mat())\n                assert obj.has_cp(\"category_id\"), f\"{obj.get_name()} object has no custom property 'category_id'\"\n\n            cam_H_m2c = H_c2w_opencv.inverted() @ H_m2w\n            cam_R_m2c = cam_H_m2c.to_quaternion().to_matrix()\n            cam_t_m2c = cam_H_m2c.to_translation()\n\n            # ignore examples that fell through the plane\n            if not np.linalg.norm(list(cam_t_m2c)) > ignore_dist_thres:\n                cam_t_m2c = list(cam_t_m2c * unit_scaling)\n                frame_gt.append({\n                    'cam_R_m2c': list(cam_R_m2c[0]) + list(cam_R_m2c[1]) + list(cam_R_m2c[2]),\n                    'cam_t_m2c': cam_t_m2c,\n                    'obj_id': obj.get_cp(\"category_id\") if not isinstance(obj, Link) else obj.visuals[0].get_cp(\n                        'category_id')\n                })\n            else:\n                print('ignored obj, ', obj.get_cp(\"category_id\"), 'because either ')\n                print('(1) it is further away than parameter \"ignore_dist_thres: \",', ignore_dist_thres)\n                print('(e.g. because it fell through a plane during physics sim)')\n                print('or')\n                print('(2) the object pose has not been given in meters')\n\n        return frame_gt",
  "def get_frame_camera(save_world2cam: bool, depth_scale: float = 1.0, unit_scaling: float = 1000.,\n                         destination_frame: Optional[List[str]] = None):\n        \"\"\" Returns camera parameters for the active camera.\n        :param save_world2cam: If true, camera to world transformations \"cam_R_w2c\", \"cam_t_w2c\" are saved\n                               in scene_camera.json\n        :param depth_scale: Multiply the uint16 output depth image with this factor to get depth in mm.\n        :param unit_scaling: 1000. for outputting poses in mm\n        :param destination_frame: Transform poses from Blender internal coordinates to OpenCV coordinates\n        :return: dict containing info for scene_camera.json\n        \"\"\"\n        if destination_frame is None:\n            destination_frame = [\"X\", \"-Y\", \"-Z\"]\n\n        cam_K = _WriterUtility.get_cam_attribute(bpy.context.scene.camera, 'cam_K')\n\n        frame_camera_dict = {\n            'cam_K': cam_K[0] + cam_K[1] + cam_K[2],\n            'depth_scale': depth_scale\n        }\n\n        if save_world2cam:\n            H_c2w_opencv = Matrix(_WriterUtility.get_cam_attribute(bpy.context.scene.camera, 'cam2world_matrix',\n                                                                   local_frame_change=destination_frame))\n\n            H_w2c_opencv = H_c2w_opencv.inverted()\n            R_w2c_opencv = H_w2c_opencv.to_quaternion().to_matrix()\n            t_w2c_opencv = H_w2c_opencv.to_translation() * unit_scaling\n\n            frame_camera_dict['cam_R_w2c'] = list(R_w2c_opencv[0]) + list(R_w2c_opencv[1]) + list(R_w2c_opencv[2])\n            frame_camera_dict['cam_t_w2c'] = list(t_w2c_opencv)\n\n        return frame_camera_dict",
  "def write_frames(chunks_dir: str, dataset_objects: list, depths: List[np.ndarray],\n                     colors: List[np.ndarray], color_file_format: str = \"PNG\",\n                     depth_scale: float = 1.0, frames_per_chunk: int = 1000, annotation_scale: float = 1000.,\n                     ignore_dist_thres: float = 100., save_world2cam: bool = True, jpg_quality: int = 95):\n        \"\"\"Write each frame's ground truth into chunk directory in BOP format\n\n        :param chunks_dir: Path to the output directory of the current chunk.\n        :param dataset_objects: Save annotations for these objects.\n        :param depths: List of depth images in m to save\n        :param colors: List of color images to save\n        :param color_file_format: File type to save color images. Available: \"PNG\", \"JPEG\"\n        :param jpg_quality: If color_file_format is \"JPEG\", save with the given quality.\n        :param depth_scale: Multiply the uint16 output depth image with this factor to get depth in mm. Used to\n                            trade-off between depth accuracy and maximum depth value. Default corresponds to\n                            65.54m maximum depth and 1mm accuracy.\n        :param ignore_dist_thres: Distance between camera and object after which object is ignored.\n                                  Mostly due to failed physics.\n        :param annotation_scale: The scale factor applied to the calculated annotations (in [m]) to get them into the\n                                 specified format (see `annotation_format` in `write_bop` for further details).\n        :param frames_per_chunk: Number of frames saved in each chunk (called scene in BOP)\n        \"\"\"\n\n        # Format of the depth images.\n        depth_ext = '.png'\n\n        rgb_tpath = os.path.join(chunks_dir, '{chunk_id:06d}', 'rgb', '{im_id:06d}' + '{im_type}')\n        depth_tpath = os.path.join(chunks_dir, '{chunk_id:06d}', 'depth', '{im_id:06d}' + depth_ext)\n        chunk_camera_tpath = os.path.join(chunks_dir, '{chunk_id:06d}', 'scene_camera.json')\n        chunk_gt_tpath = os.path.join(chunks_dir, '{chunk_id:06d}', 'scene_gt.json')\n\n        # Paths to the already existing chunk folders (such folders may exist\n        # when appending to an existing dataset).\n        chunk_dirs = sorted(glob.glob(os.path.join(chunks_dir, '*')))\n        chunk_dirs = [d for d in chunk_dirs if os.path.isdir(d)]\n\n        # Get ID's of the last already existing chunk and frame.\n        curr_chunk_id = 0\n        curr_frame_id = 0\n        if len(chunk_dirs):\n            last_chunk_dir = sorted(chunk_dirs)[-1]\n            last_chunk_gt_fpath = os.path.join(last_chunk_dir, 'scene_gt.json')\n            chunk_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n\n            # Last chunk and frame ID's.\n            last_chunk_id = int(os.path.basename(last_chunk_dir))\n            last_frame_id = int(sorted(chunk_gt.keys())[-1])\n\n            # Current chunk and frame ID's.\n            curr_chunk_id = last_chunk_id\n            curr_frame_id = last_frame_id + 1\n            if curr_frame_id % frames_per_chunk == 0:\n                curr_chunk_id += 1\n                curr_frame_id = 0\n\n        # Initialize structures for the GT annotations and camera info.\n        chunk_gt = {}\n        chunk_camera = {}\n        if curr_frame_id != 0:\n            # Load GT and camera info of the chunk we are appending to.\n            chunk_gt = _BopWriterUtility.load_json(\n                chunk_gt_tpath.format(chunk_id=curr_chunk_id), keys_to_int=True)\n            chunk_camera = _BopWriterUtility.load_json(\n                chunk_camera_tpath.format(chunk_id=curr_chunk_id), keys_to_int=True)\n\n        # Go through all frames.\n        num_new_frames = bpy.context.scene.frame_end - bpy.context.scene.frame_start\n\n        if len(depths) != len(colors) != num_new_frames:\n            raise Exception(\"The amount of images stored in the depths/colors does not correspond to the amount\"\n                            \"of images specified by frame_start to frame_end.\")\n\n        for frame_id in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n            # Activate frame.\n            bpy.context.scene.frame_set(frame_id)\n\n            # Reset data structures and prepare folders for a new chunk.\n            if curr_frame_id == 0:\n                chunk_gt = {}\n                chunk_camera = {}\n                os.makedirs(os.path.dirname(\n                    rgb_tpath.format(chunk_id=curr_chunk_id, im_id=0, im_type='PNG')))\n                os.makedirs(os.path.dirname(\n                    depth_tpath.format(chunk_id=curr_chunk_id, im_id=0)))\n\n            # Get GT annotations and camera info for the current frame.\n            chunk_gt[curr_frame_id] = _BopWriterUtility.get_frame_gt(dataset_objects, annotation_scale,\n                                                                     ignore_dist_thres)\n            chunk_camera[curr_frame_id] = _BopWriterUtility.get_frame_camera(save_world2cam, depth_scale,\n                                                                             annotation_scale)\n\n            color_rgb = colors[frame_id]\n            color_bgr = color_rgb.copy()\n            color_bgr[..., :3] = color_bgr[..., :3][..., ::-1]\n            if color_file_format == 'PNG':\n                rgb_fpath = rgb_tpath.format(chunk_id=curr_chunk_id, im_id=curr_frame_id, im_type='.png')\n                cv2.imwrite(rgb_fpath, color_bgr)\n            elif color_file_format == 'JPEG':\n                rgb_fpath = rgb_tpath.format(chunk_id=curr_chunk_id, im_id=curr_frame_id, im_type='.jpg')\n                cv2.imwrite(rgb_fpath, color_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n\n            depth = depths[frame_id]\n\n            # Scale the depth to retain a higher precision (the depth is saved\n            # as a 16-bit PNG image with range 0-65535).\n            depth_mm = 1000.0 * depth  # [m] -> [mm]\n            depth_mm_scaled = depth_mm / float(depth_scale)\n\n            # Save the scaled depth image.\n            depth_fpath = depth_tpath.format(chunk_id=curr_chunk_id, im_id=curr_frame_id)\n            _BopWriterUtility.save_depth(depth_fpath, depth_mm_scaled)\n\n            # Save the chunk info if we are at the end of a chunk or at the last new frame.\n            if ((curr_frame_id + 1) % frames_per_chunk == 0) or \\\n                    (frame_id == num_new_frames - 1):\n\n                # Save GT annotations.\n                _BopWriterUtility.save_json(chunk_gt_tpath.format(chunk_id=curr_chunk_id), chunk_gt)\n\n                # Save camera info.\n                _BopWriterUtility.save_json(chunk_camera_tpath.format(chunk_id=curr_chunk_id), chunk_camera)\n\n                # Update ID's.\n                curr_chunk_id += 1\n                curr_frame_id = 0\n            else:\n                curr_frame_id += 1",
  "def calc_gt_masks(chunk_dirs: List[str], dataset_objects: Dict[int, pyrender.Mesh], starting_frame_id: int = 0,\n                      annotation_scale: float = 1000., delta: float = 0.015):\n        \"\"\" Calculates the ground truth masks.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit), with the difference of using pyrender for depth\n        rendering.\n\n        :param chunk_dirs: List of directories to calculate the gt masks for.\n        :param dataset_objects: Dict containing all objects to save the annotations for.\n        :param starting_frame_id: The first frame id the writer has written during this run.\n        :param annotation_scale: The scale factor applied to the calculated annotations (in [m]) to get them into the\n                                 specified format (see `annotation_format` in `write_bop` for further details).\n        :param delta: Tolerance used for estimation of the visibility masks.\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n        # pylint: disable=import-outside-toplevel\n        from bop_toolkit_lib import inout, misc, visibility\n        # pylint: enable=import-outside-toplevel\n\n        width = bpy.context.scene.render.resolution_x\n        height = bpy.context.scene.render.resolution_y\n\n        renderer = pyrender.OffscreenRenderer(viewport_width=width, viewport_height=height)\n\n        for dir_counter, chunk_dir in enumerate(chunk_dirs):\n            last_chunk_gt_fpath = os.path.join(chunk_dir, 'scene_gt.json')\n            last_chunk_camera_fpath = os.path.join(chunk_dir, 'scene_camera.json')\n            scene_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n            scene_camera = _BopWriterUtility.load_json(last_chunk_camera_fpath, keys_to_int=True)\n\n            # Create folders for the output masks (if they do not exist yet).\n            mask_dir_path = os.path.dirname(os.path.join(chunk_dir, 'mask', '000000_000000.png'))\n            misc.ensure_dir(mask_dir_path)\n\n            mask_visib_dir_path = os.path.dirname(os.path.join(chunk_dir, 'mask_visib', '000000_000000.png'))\n            misc.ensure_dir(mask_visib_dir_path)\n\n            im_ids = sorted(scene_gt.keys())\n\n            # append to existing output\n            if dir_counter == 0:\n                im_ids = im_ids[starting_frame_id:]\n\n            for im_counter, im_id in enumerate(im_ids):\n                if im_counter % 100 == 0:\n                    misc.log(f'Calculating GT masks - {chunk_dir}, {im_counter}')\n\n                K = np.array(scene_camera[im_id]['cam_K']).reshape(3, 3)\n                fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n                camera = pyrender.IntrinsicsCamera(fx=fx, fy=fy, cx=cx, cy=cy, znear=0.1, zfar=100000)\n\n                # Load depth image.\n                depth_path = os.path.join(\n                    chunk_dir, 'depth', '{im_id:06d}.png').format(im_id=im_id)\n                depth_im = inout.load_depth(depth_path)\n                depth_im *= scene_camera[im_id]['depth_scale']  # to [mm]\n                depth_im /= 1000.  # to [m]\n                dist_im = misc.depth_im_to_dist_im_fast(depth_im, K)\n\n                for gt_id, gt in enumerate(scene_gt[im_id]):\n                    # create a new scene\n                    scene = pyrender.Scene()\n\n                    # add camera and current object\n                    scene.add(camera)\n                    t = np.array(gt['cam_t_m2c'])\n                    # rescale translation depending on initial saving format\n                    t /= annotation_scale\n\n                    pose = bop_pose_to_pyrender_coordinate_system(cam_R_m2c=np.array(gt['cam_R_m2c']).reshape(3, 3),\n                                                                  cam_t_m2c=t)\n                    scene.add(dataset_objects[gt['obj_id']], pose=pose)\n\n                    # Render the depth image.\n                    _, depth_gt = renderer.render(scene=scene)\n\n                    # Convert depth image to distance image.\n                    dist_gt = misc.depth_im_to_dist_im_fast(depth_gt, K)\n\n                    # Mask of the full object silhouette.\n                    mask = dist_gt > 0\n\n                    # Mask of the visible part of the object silhouette.\n                    mask_visib = visibility.estimate_visib_mask_gt(\n                        dist_im, dist_gt, delta, visib_mode='bop19')\n\n                    # Save the calculated masks.\n                    mask_path = os.path.join(\n                        chunk_dir, 'mask', '{im_id:06d}_{gt_id:06d}.png').format(im_id=im_id, gt_id=gt_id)\n                    inout.save_im(mask_path, 255 * mask.astype(np.uint8))\n\n                    mask_visib_path = os.path.join(\n                        chunk_dir, 'mask_visib',\n                        '{im_id:06d}_{gt_id:06d}.png').format(im_id=im_id, gt_id=gt_id)\n                    inout.save_im(mask_visib_path, 255 * mask_visib.astype(np.uint8))",
  "def calc_gt_info(chunk_dirs: List[str], dataset_objects: Dict[int, pyrender.Mesh], starting_frame_id: int = 0,\n                     annotation_scale: float = 1000., delta: float = 0.015):\n        \"\"\" Calculates the ground truth masks.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit), with the difference of using pyrender for depth\n        rendering.\n\n        :param chunk_dirs: List of directories to calculate the gt info for.\n        :param dataset_objects: Dict containing all objects to save the annotations for.\n        :param starting_frame_id: The first frame id the writer has written during this run.\n        :param annotation_scale: The scale factor applied to the calculated annotations (in [m]) to get them into the\n                                 specified format (see `annotation_format` in `write_bop` for further details).\n        :param delta: Tolerance used for estimation of the visibility masks.\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n        # pylint: disable=import-outside-toplevel\n        from bop_toolkit_lib import inout, misc, visibility\n        # pylint: enable=import-outside-toplevel\n\n        im_width, im_height = bpy.context.scene.render.resolution_x, bpy.context.scene.render.resolution_y\n        ren_width, ren_height = 3 * im_width, 3 * im_height\n        ren_cx_offset, ren_cy_offset = im_width, im_height\n        renderer = pyrender.OffscreenRenderer(viewport_width=ren_width, viewport_height=ren_height)\n\n        for dir_counter, chunk_dir in enumerate(chunk_dirs):\n            last_chunk_gt_fpath = os.path.join(chunk_dir, 'scene_gt.json')\n            last_chunk_camera_fpath = os.path.join(chunk_dir, 'scene_camera.json')\n            scene_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n            scene_camera = _BopWriterUtility.load_json(last_chunk_camera_fpath, keys_to_int=True)\n\n            # load existing gt info\n            if dir_counter == 0 and starting_frame_id > 0:\n                misc.log(f\"Loading gt info from existing chunk dir - {chunk_dir}\")\n                scene_gt_info = _BopWriterUtility.load_json(os.path.join(chunk_dir, 'scene_gt_info.json'),\n                                                            keys_to_int=True)\n            else:\n                scene_gt_info = {}\n\n            im_ids = sorted(scene_gt.keys())\n\n            # append to existing output\n            if dir_counter == 0:\n                im_ids = im_ids[starting_frame_id:]\n\n            for im_counter, im_id in enumerate(im_ids):\n                if im_counter % 100 == 0:\n                    misc.log(f'Calculating GT info - {chunk_dir}, {im_counter}')\n\n                # Load depth image.\n                depth_fpath = os.path.join(chunk_dir, 'depth', '{im_id:06d}.png').format(im_id=im_id)\n                assert os.path.isfile(depth_fpath)\n                depth = inout.load_depth(depth_fpath)\n                depth *= scene_camera[im_id]['depth_scale']  # Convert to [mm].\n                depth /= 1000.  # to [m]\n\n                K = np.array(scene_camera[im_id]['cam_K']).reshape(3, 3)\n                fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n                im_size = (depth.shape[1], depth.shape[0])\n                camera = pyrender.IntrinsicsCamera(fx=fx, fy=fy, cx=cx+ren_cx_offset, cy=cy+ren_cy_offset, znear=0.1,\n                                                   zfar=100000)\n\n                scene_gt_info[im_id] = []\n                for gt in scene_gt[im_id]:\n                    # create a new scene\n                    scene = pyrender.Scene()\n\n                    # add camera and current object\n                    scene.add(camera)\n                    t = np.array(gt['cam_t_m2c'])\n                    # rescale translation depending on initial saving format\n                    t /= annotation_scale\n                    pose = bop_pose_to_pyrender_coordinate_system(cam_R_m2c=np.array(gt['cam_R_m2c']).reshape(3, 3),\n                                                                  cam_t_m2c=t)\n                    scene.add(dataset_objects[gt['obj_id']], pose=pose)\n\n                    # render the depth image\n                    _, depth_gt_large = renderer.render(scene=scene)\n                    depth_gt = depth_gt_large[\n                               ren_cy_offset:(ren_cy_offset + im_height),\n                               ren_cx_offset:(ren_cx_offset + im_width)]\n\n                    # Convert depth images to distance images.\n                    dist_gt = misc.depth_im_to_dist_im_fast(depth_gt, K)\n                    dist_im = misc.depth_im_to_dist_im_fast(depth, K)\n\n                    # Estimation of the visibility mask.\n                    visib_gt = visibility.estimate_visib_mask_gt(\n                        dist_im, dist_gt, delta, visib_mode='bop19')\n\n                    # Mask of the object in the GT pose.\n                    obj_mask_gt_large = depth_gt_large > 0\n                    obj_mask_gt = dist_gt > 0\n\n                    # Number of pixels in the whole object silhouette\n                    # (even in the truncated part).\n                    px_count_all = np.sum(obj_mask_gt_large)\n\n                    # Number of pixels in the object silhouette with a valid depth measurement\n                    # (i.e. with a non-zero value in the depth image).\n                    px_count_valid = np.sum(dist_im[obj_mask_gt] > 0)\n\n                    # Number of pixels in the visible part of the object silhouette.\n                    px_count_visib = visib_gt.sum()\n\n                    # Visible surface fraction.\n                    if px_count_all > 0:\n                        visib_fract = px_count_visib / float(px_count_all)\n                    else:\n                        visib_fract = 0.0\n\n                    # Bounding box of the whole object silhouette\n                    # (including the truncated part).\n                    bbox = [-1, -1, -1, -1]\n                    if px_count_visib > 0:\n                        ys, xs = obj_mask_gt_large.nonzero()\n                        ys -= ren_cy_offset\n                        xs -= ren_cx_offset\n                        bbox = misc.calc_2d_bbox(xs, ys, im_size)\n\n                    # Bounding box of the visible surface part.\n                    bbox_visib = [-1, -1, -1, -1]\n                    if px_count_visib > 0:\n                        ys, xs = visib_gt.nonzero()\n                        bbox_visib = misc.calc_2d_bbox(xs, ys, im_size)\n\n                    # Store the calculated info.\n                    scene_gt_info[im_id].append({\n                        'px_count_all': int(px_count_all),\n                        'px_count_valid': int(px_count_valid),\n                        'px_count_visib': int(px_count_visib),\n                        'visib_fract': float(visib_fract),\n                        'bbox_obj': [int(e) for e in bbox],\n                        'bbox_visib': [int(e) for e in bbox_visib]\n                    })\n\n            # Save the info for the current scene.\n            scene_gt_info_path = os.path.join(chunk_dir, 'scene_gt_info.json')\n            misc.ensure_dir(os.path.dirname(scene_gt_info_path))\n            inout.save_json(scene_gt_info_path, scene_gt_info)",
  "def calc_gt_coco(chunk_dirs: List[str], dataset_objects: List[MeshObject], starting_frame_id: int = 0):\n        \"\"\" Calculates the COCO annotations.\n        From the BOP toolkit (https://github.com/thodan/bop_toolkit).\n\n        :param chunk_dirs: List of directories to calculate the gt coco annotations for.\n        :param dataset_objects: List containing all objects to save the annotations for.\n        :param starting_frame_id: The first frame id the writer has written during this run.\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n        # pylint: disable=import-outside-toplevel\n        from bop_toolkit_lib import inout, misc, pycoco_utils\n        # pylint: enable=import-outside-toplevel\n\n        for dir_counter, chunk_dir in enumerate(chunk_dirs):\n            dataset_name = chunk_dir.split('/')[-3]\n\n            CATEGORIES = [{'id': obj.get_cp('category_id'), 'name': str(obj.get_cp('category_id')), 'supercategory':\n                          dataset_name} for obj in dataset_objects]\n\n            # Remove all duplicate dicts from list.\n            # Ref: https://stackoverflow.com/questions/9427163/remove-duplicate-dict-in-list-in-python\n            CATEGORIES = list({frozenset(item.items()):item for item in CATEGORIES}.values())\n\n            INFO = {\n                \"description\": dataset_name + '_train',\n                \"url\": \"https://github.com/thodan/bop_toolkit\",\n                \"version\": \"0.1.0\",\n                \"year\": datetime.date.today().year,\n                \"contributor\": \"\",\n                \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n            }\n\n            # load existing coco annotations\n            if dir_counter == 0 and starting_frame_id > 0:\n                misc.log(f\"Loading coco annotations from existing chunk dir - {chunk_dir}\")\n                coco_scene_output = _BopWriterUtility.load_json(os.path.join(chunk_dir, 'scene_gt_coco.json'))\n                if coco_scene_output[\"annotations\"]:\n                    segmentation_id = coco_scene_output[\"annotations\"][-1]['id'] + 1\n                else:\n                    segmentation_id = 1\n            else:\n                coco_scene_output = {\n                    \"info\": INFO,\n                    \"licenses\": [],\n                    \"categories\": CATEGORIES,\n                    \"images\": [],\n                    \"annotations\": []\n                }\n                segmentation_id = 1\n\n            # Load info about the GT poses (e.g. visibility) for the current scene.\n            last_chunk_gt_fpath = os.path.join(chunk_dir, 'scene_gt.json')\n            scene_gt = _BopWriterUtility.load_json(last_chunk_gt_fpath, keys_to_int=True)\n            last_chunk_gt_info_fpath = os.path.join(chunk_dir, 'scene_gt_info.json')\n            scene_gt_info = inout.load_json(last_chunk_gt_info_fpath, keys_to_int=True)\n            # Output coco path\n            coco_gt_path = os.path.join(chunk_dir, 'scene_gt_coco.json')\n            misc.log(f'Calculating COCO annotations - {chunk_dir}')\n\n            # Go through each view in scene_gt\n            for scene_view, inst_list in scene_gt.items():\n                im_id = int(scene_view)\n\n                # skip already existing annotations\n                if dir_counter == 0 and im_id < starting_frame_id:\n                    continue\n\n                img_path = os.path.join(chunk_dir, 'rgb', '{im_id:06d}.jpg').format(im_id=im_id)\n                relative_img_path = os.path.relpath(img_path, os.path.dirname(coco_gt_path))\n                im_size = (bpy.context.scene.render.resolution_x, bpy.context.scene.render.resolution_y)\n                image_info = pycoco_utils.create_image_info(im_id, relative_img_path, im_size)\n                coco_scene_output[\"images\"].append(image_info)\n                gt_info = scene_gt_info[scene_view]\n\n                # Go through each instance in view\n                for idx, inst in enumerate(inst_list):\n                    category_info = inst['obj_id']\n                    visibility = gt_info[idx]['visib_fract']\n                    # Add ignore flag for objects smaller than 10% visible\n                    ignore_gt = visibility < 0.1\n                    mask_visib_p = os.path.join(\n                        chunk_dir, 'mask_visib',\n                        '{im_id:06d}_{gt_id:06d}.png').format(im_id=im_id, gt_id=idx)\n                    mask_full_p = os.path.join(\n                        chunk_dir, 'mask', '{im_id:06d}_{gt_id:06d}.png').format(im_id=im_id, gt_id=idx)\n\n                    binary_inst_mask_visib = inout.load_depth(mask_visib_p).astype(bool)\n                    if binary_inst_mask_visib.sum() < 1:\n                        continue\n\n                    # use `amodal` bbox type per default\n                    binary_inst_mask_full = inout.load_depth(mask_full_p).astype(bool)\n                    if binary_inst_mask_full.sum() < 1:\n                        continue\n                    bounding_box = pycoco_utils.bbox_from_binary_mask(binary_inst_mask_full)\n\n                    annotation_info = pycoco_utils.create_annotation_info(\n                        segmentation_id, im_id, category_info, binary_inst_mask_visib, bounding_box, tolerance=2,\n                        ignore=ignore_gt)\n\n                    if annotation_info is not None:\n                        coco_scene_output[\"annotations\"].append(annotation_info)\n\n                    segmentation_id += 1\n\n            with open(coco_gt_path, 'w', encoding='utf-8') as output_json_file:\n                json.dump(coco_scene_output, output_json_file)",
  "def convert_keys_to_int(x):\n            return {int(k) if k.lstrip('-').isdigit() else k: v for k, v in x.items()}",
  "def write_hdf5(output_dir_path: str, output_data_dict: Dict[str, List[Union[np.ndarray, list, dict]]],\n               append_to_existing_output: bool = False, stereo_separate_keys: bool = False):\n    \"\"\"\n    Saves the information provided inside of the output_data_dict into a .hdf5 container\n\n    :param output_dir_path: The folder path in which the .hdf5 containers will be generated\n    :param output_data_dict: The container, which keeps the different images, which should be saved to disc.\n                             Each key will be saved as its own key in the .hdf5 container.\n    :param append_to_existing_output: If this is True, the output_dir_path folder will be scanned for pre-existing\n                                      .hdf5 containers and the numbering of the newly added containers, will start\n                                      right where the last run left off.\n    :param stereo_separate_keys: If this is True and the rendering was done in stereo mode, than the stereo images\n                                 won't be saved in one tensor [2, img_x, img_y, channels], where the img[0] is the\n                                 left image and img[1] the right. They will be saved in separate keys: for example\n                                 for colors in colors_0 and colors_1.\n    \"\"\"\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    amount_of_frames = 0\n    for data_block in output_data_dict.values():\n        if isinstance(data_block, list):\n            amount_of_frames = max([amount_of_frames, len(data_block)])\n\n    # if append to existing output is turned on the existing folder is searched for the highest occurring\n    # index, which is then used as starting point for this run\n    if append_to_existing_output:\n        frame_offset = 0\n        # Look for hdf5 file with highest index\n        for path in os.listdir(output_dir_path):\n            if path.endswith(\".hdf5\"):\n                index = path[:-len(\".hdf5\")]\n                if index.isdigit():\n                    frame_offset = max(frame_offset, int(index) + 1)\n    else:\n        frame_offset = 0\n\n    if amount_of_frames != bpy.context.scene.frame_end - bpy.context.scene.frame_start:\n        raise Exception(\"The amount of images stored in the output_data_dict does not correspond with the amount\"\n                        \"of images specified by frame_start to frame_end.\")\n\n    for frame in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n        # for each frame a new .hdf5 file is generated\n        hdf5_path = os.path.join(output_dir_path, str(frame + frame_offset) + \".hdf5\")\n        with h5py.File(hdf5_path, \"w\") as file:\n            # Go through all the output types\n            print(f\"Merging data for frame {frame} into {hdf5_path}\")\n\n            adjusted_frame = frame - bpy.context.scene.frame_start\n            for key, data_block in output_data_dict.items():\n                if adjusted_frame < len(data_block):\n                    # get the current data block for the current frame\n                    used_data_block = data_block[adjusted_frame]\n                    if stereo_separate_keys and (bpy.context.scene.render.use_multiview or\n                                                 used_data_block.shape[0] == 2):\n                        # stereo mode was activated\n                        _WriterUtility.write_to_hdf_file(file, key + \"_0\", data_block[adjusted_frame][0])\n                        _WriterUtility.write_to_hdf_file(file, key + \"_1\", data_block[adjusted_frame][1])\n                    else:\n                        _WriterUtility.write_to_hdf_file(file, key, data_block[adjusted_frame])\n                else:\n                    raise Exception(f\"There are more frames {adjusted_frame} then there are blocks of information \"\n                                    f\" {len(data_block)} in the given list for key {key}.\")\n            blender_proc_version = Utility.get_current_version()\n            if blender_proc_version is not None:\n                _WriterUtility.write_to_hdf_file(file, \"blender_proc_version\", np.string_(blender_proc_version))",
  "class _WriterUtility:\n\n    @staticmethod\n    def load_registered_outputs(keys: Set[str], keys_with_alpha_channel: Set[str] = None) -> \\\n            Dict[str, Union[np.ndarray, List[np.ndarray]]]:\n        \"\"\"\n        Loads registered outputs with specified keys\n\n        :param keys: set of output_key types to load\n        :param keys_with_alpha_channel: A set containing all keys whose alpha channels should be loaded.\n        :return: dict of lists of raw loaded outputs. Keys are e.g. 'distance', 'colors', 'normals', 'segmap'\n        \"\"\"\n        output_data_dict: Dict[str, Union[np.ndarray, List[np.ndarray]]] = {}\n        reg_outputs = Utility.get_registered_outputs()\n        for reg_out in reg_outputs:\n            if reg_out['key'] in keys:\n                key_has_alpha_channel = keys_with_alpha_channel is not None and reg_out[\n                    'key'] in keys_with_alpha_channel\n                if '%' in reg_out['path']:\n                    # per frame outputs\n                    for frame_id in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n                        output_path = resolve_path(reg_out['path'] % frame_id)\n                        if os.path.exists(output_path):\n                            output_file = _WriterUtility.load_output_file(output_path, key_has_alpha_channel)\n                        else:\n                            # check for stereo files\n                            output_paths = _WriterUtility.get_stereo_path_pair(output_path)\n                            # convert to a tensor of shape [2, img_x, img_y, channels]\n                            # output_file[0] is the left image and output_file[1] the right image\n                            output_file = np.array(\n                                [_WriterUtility.load_output_file(path, key_has_alpha_channel) for path in\n                                 output_paths])\n                        # For outputs like distance or depth, we automatically trim the last channel here\n                        if \"trim_redundant_channels\" in reg_out and reg_out[\"trim_redundant_channels\"]:\n                            output_file = trim_redundant_channels(output_file)\n                        if \"convert_to_depth\" in reg_out and reg_out[\"convert_to_depth\"]:\n                            output_file = dist2depth(output_file)\n                        if \"convert_to_distance\" in reg_out and reg_out[\"convert_to_distance\"]:\n                            output_file = depth2dist(output_file)\n\n                        # semantic seg must be last\n                        if \"is_semantic_segmentation\" in reg_out and reg_out[\"is_semantic_segmentation\"]\\\n                                and \"semantic_segmentation_mapping\" in reg_out \\\n                                and \"semantic_segmentation_default_values\" in reg_out:\n                            output_file = segmentation_mapping(output_file,\n                                                               reg_out[\"semantic_segmentation_mapping\"],\n                                                               reg_out[\"semantic_segmentation_default_values\"])\n                            for key, output_info in output_file.items():\n                                output_data_dict.setdefault(key, []).append(output_info)\n                        else:\n                            output_data_dict.setdefault(reg_out['key'], []).append(output_file)\n                else:\n                    # per run outputs\n                    output_path = resolve_path(reg_out['path'])\n                    output_file = _WriterUtility.load_output_file(output_path, key_has_alpha_channel)\n                    output_data_dict[reg_out['key']] = output_file\n\n        return output_data_dict\n\n    @staticmethod\n    def get_stereo_path_pair(file_path: str) -> Tuple[str, str]:\n        \"\"\"\n        Returns stereoscopic file path pair for a given \"normal\" image file path.\n\n        :param file_path: The file path of a single image.\n        :return: The pair of file paths corresponding to the stereo images,\n        \"\"\"\n        path_split = file_path.split(\".\")\n        path_l = f\"{path_split[0]}_L.{path_split[1]}\"\n        path_r = f\"{path_split[0]}_R.{path_split[1]}\"\n\n        return path_l, path_r\n\n    @staticmethod\n    def load_output_file(file_path: str, load_alpha_channel: bool = False,\n                         remove: bool = True) -> Union[np.ndarray, List[Any]]:\n        \"\"\" Tries to read in the file with the given path into a numpy array.\n\n        :param file_path: The file path. Type: string.\n        :param load_alpha_channel: Whether to load the alpha channel as well. Type: bool. Default: False\n        :param remove: Whether to delete file after loading.\n        :return: Loaded data from the file as numpy array if possible.\n        \"\"\"\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(\"File not found: \" + file_path)\n\n        file_ending = file_path[file_path.rfind(\".\") + 1:].lower()\n\n        if file_ending in [\"exr\", \"png\", \"jpg\"]:\n            # num_channels is 4 if transparent_background is true in config\n            output = load_image(file_path, num_channels=3 + (1 if load_alpha_channel else 0))\n        elif file_ending in [\"npy\", \"npz\"]:\n            output = np.load(file_path)\n        elif file_ending in [\"csv\"]:\n            output = _WriterUtility.load_csv(file_path)\n        else:\n            raise NotImplementedError(\"File with ending \" + file_ending + \" cannot be loaded.\")\n\n        if remove:\n            os.remove(file_path)\n        return output\n\n    @staticmethod\n    def load_csv(file_path: str) -> List[Any]:\n        \"\"\" Load the csv file at the given path.\n\n        :param file_path: The path. Type: string.\n        :return: The content of the file\n        \"\"\"\n        rows = []\n        with open(file_path, mode='r', encoding=\"utf-8\") as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            for row in csv_reader:\n                rows.append(row)\n        return rows\n\n    @staticmethod\n    def get_common_attribute(item: bpy.types.Object, attribute_name: str,\n                             local_frame_change: Union[None, List[str]] = None,\n                             world_frame_change: Union[None, List[str]] = None) -> Any:\n        \"\"\" Returns the value of the requested attribute for the given item.\n\n        This method covers all general attributes that blender objects have.\n\n        :param item: The item. Type: blender object.\n        :param attribute_name: The attribute name. Type: string.\n        :param local_frame_change: Can be used to change the local coordinate frame of matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :param world_frame_change: Can be used to change the world coordinate frame of points and matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :return: The attribute value.\n        \"\"\"\n\n        if local_frame_change is None:\n            local_frame_change = [\"X\", \"Y\", \"Z\"]\n        if world_frame_change is None:\n            world_frame_change = [\"X\", \"Y\", \"Z\"]\n\n        # Print warning if local_frame_change is used with other attributes than matrix_world\n        if local_frame_change != [\"X\", \"Y\", \"Z\"] and attribute_name in [\"location\", \"rotation_euler\",\n                                                                        \"rotation_forward_vec\", \"rotation_up_vec\"]:\n            print(\"Warning: The local_frame_change parameter is at the moment only supported by \"\n                  \"the matrix_world attribute.\")\n\n        if attribute_name == \"name\":\n            return item.name\n        if attribute_name == \"location\":\n            return change_coordinate_frame_of_point(item.location, world_frame_change)\n        if attribute_name == \"rotation_euler\":\n            return change_coordinate_frame_of_point(item.rotation_euler, world_frame_change)\n        if attribute_name == \"rotation_forward_vec\":\n            # Calc forward vector from rotation matrix\n            rot_mat = item.rotation_euler.to_matrix()\n            forward = rot_mat @ mathutils.Vector([0, 0, -1])\n            return change_coordinate_frame_of_point(forward, world_frame_change)\n        if attribute_name == \"rotation_up_vec\":\n            # Calc up vector from rotation matrix\n            rot_mat = item.rotation_euler.to_matrix()\n            up = rot_mat @ mathutils.Vector([0, 1, 0])\n            return change_coordinate_frame_of_point(up, world_frame_change)\n        if attribute_name == \"matrix_world\":\n            # Transform matrix_world to given destination frame\n            matrix_world = change_source_coordinate_frame_of_transformation_matrix(Entity(item).get_local2world_mat(),\n                                                                                   local_frame_change)\n            matrix_world = change_target_coordinate_frame_of_transformation_matrix(matrix_world, world_frame_change)\n            return [list(c) for c in matrix_world]\n        if attribute_name.startswith(\"customprop_\"):\n            custom_property_name = attribute_name[len(\"customprop_\"):]\n            # Make sure the requested custom property exist\n            if custom_property_name in item:\n                return item[custom_property_name]\n            raise ValueError(f\"No such custom property: {custom_property_name}\")\n        raise ValueError(f\"No such attribute: {attribute_name}\")\n\n    @staticmethod\n    def get_cam_attribute(cam_ob: bpy.context.scene.camera, attribute_name: str,\n                          local_frame_change: Union[None, List[str]] = None,\n                          world_frame_change: Union[None, List[str]] = None) -> Any:\n        \"\"\" Returns the value of the requested attribute for the given object.\n\n        :param cam_ob: The camera object.\n        :param attribute_name: The attribute name.\n        :param local_frame_change: Can be used to change the local coordinate frame of matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :param world_frame_change: Can be used to change the world coordinate frame of points and matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :return: The attribute value.\n        \"\"\"\n\n        if attribute_name == \"fov_x\":\n            return CameraUtility.get_fov()[0]\n        if attribute_name == \"fov_y\":\n            return CameraUtility.get_fov()[1]\n        if attribute_name == \"shift_x\":\n            return cam_ob.data.shift_x\n        if attribute_name == \"shift_y\":\n            return cam_ob.data.shift_y\n        if attribute_name == \"half_fov_x\":\n            return CameraUtility.get_fov()[0] * 0.5\n        if attribute_name == \"half_fov_y\":\n            return CameraUtility.get_fov()[1] * 0.5\n        if attribute_name == \"cam_K\":\n            return [list(c) for c in CameraUtility.get_intrinsics_as_K_matrix()]\n        if attribute_name == \"cam2world_matrix\":\n            return _WriterUtility.get_common_attribute(cam_ob, \"matrix_world\", local_frame_change,\n                                                       world_frame_change)\n        return _WriterUtility.get_common_attribute(cam_ob, attribute_name, local_frame_change,\n                                                   world_frame_change)\n\n    @staticmethod\n    def get_light_attribute(light: bpy.types.Light, attribute_name: str,\n                            local_frame_change: Union[None, List[str]] = None,\n                            world_frame_change: Union[None, List[str]] = None) -> Any:\n        \"\"\" Returns the value of the requested attribute for the given light.\n\n        :param light: The light. Type: blender scene object of type light.\n        :param attribute_name: The attribute name.\n        :param local_frame_change: Can be used to change the local coordinate frame of matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :param world_frame_change: Can be used to change the world coordinate frame of points and matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :return: The attribute value.\n        \"\"\"\n        if attribute_name == \"energy\":\n            return light.data.energy\n        return _WriterUtility.get_common_attribute(light, attribute_name, local_frame_change, world_frame_change)\n\n    @staticmethod\n    def _get_shapenet_attribute(shapenet_obj: bpy.types.Object, attribute_name: str,\n                                local_frame_change: Union[None, List[str]] = None,\n                                world_frame_change: Union[None, List[str]] = None):\n        \"\"\" Returns the value of the requested attribute for the given object.\n\n        :param shapenet_obj: The ShapeNet object.\n        :param attribute_name: The attribute name.\n        :param local_frame_change: Can be used to change the local coordinate frame of matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :param world_frame_change: Can be used to change the world coordinate frame of points and matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :return: The attribute value.\n        \"\"\"\n\n        if attribute_name == \"used_synset_id\":\n            return shapenet_obj.get(\"used_synset_id\", \"\")\n        if attribute_name == \"used_source_id\":\n            return shapenet_obj.get(\"used_source_id\", \"\")\n        return _WriterUtility.get_common_attribute(shapenet_obj, attribute_name, local_frame_change,\n                                                   world_frame_change)\n\n    @staticmethod\n    def write_to_hdf_file(file, key: str, data: Union[np.ndarray, list, dict], compression: str = \"gzip\"):\n        \"\"\" Adds the given data as a new entry to the given hdf5 file.\n\n        :param file: The hdf5 file handle. Type: hdf5.File\n        :param key: The key at which the data should be stored in the hdf5 file.\n        :param data: The data to store.\n        \"\"\"\n        if not isinstance(data, np.ndarray) and not isinstance(data, np.bytes_):\n            if isinstance(data, (list, dict)):\n                # If the data contains one or multiple dicts that contain e.q. object states\n                if isinstance(data, dict) or len(data) > 0 and isinstance(data[0], dict):\n                    # Serialize them into json (automatically convert numpy arrays to lists)\n                    data = np.string_(json.dumps(data, cls=NumpyEncoder))\n                data = np.array(data)\n            else:\n                raise Exception(\n                    f\"This fct. expects the data for key {key} to be a np.ndarray, list or dict not a {type(data)}!\")\n\n        if data.dtype.char == 'S':\n            file.create_dataset(key, data=data, dtype=data.dtype)\n        else:\n            file.create_dataset(key, data=data, compression=compression)",
  "def load_registered_outputs(keys: Set[str], keys_with_alpha_channel: Set[str] = None) -> \\\n            Dict[str, Union[np.ndarray, List[np.ndarray]]]:\n        \"\"\"\n        Loads registered outputs with specified keys\n\n        :param keys: set of output_key types to load\n        :param keys_with_alpha_channel: A set containing all keys whose alpha channels should be loaded.\n        :return: dict of lists of raw loaded outputs. Keys are e.g. 'distance', 'colors', 'normals', 'segmap'\n        \"\"\"\n        output_data_dict: Dict[str, Union[np.ndarray, List[np.ndarray]]] = {}\n        reg_outputs = Utility.get_registered_outputs()\n        for reg_out in reg_outputs:\n            if reg_out['key'] in keys:\n                key_has_alpha_channel = keys_with_alpha_channel is not None and reg_out[\n                    'key'] in keys_with_alpha_channel\n                if '%' in reg_out['path']:\n                    # per frame outputs\n                    for frame_id in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n                        output_path = resolve_path(reg_out['path'] % frame_id)\n                        if os.path.exists(output_path):\n                            output_file = _WriterUtility.load_output_file(output_path, key_has_alpha_channel)\n                        else:\n                            # check for stereo files\n                            output_paths = _WriterUtility.get_stereo_path_pair(output_path)\n                            # convert to a tensor of shape [2, img_x, img_y, channels]\n                            # output_file[0] is the left image and output_file[1] the right image\n                            output_file = np.array(\n                                [_WriterUtility.load_output_file(path, key_has_alpha_channel) for path in\n                                 output_paths])\n                        # For outputs like distance or depth, we automatically trim the last channel here\n                        if \"trim_redundant_channels\" in reg_out and reg_out[\"trim_redundant_channels\"]:\n                            output_file = trim_redundant_channels(output_file)\n                        if \"convert_to_depth\" in reg_out and reg_out[\"convert_to_depth\"]:\n                            output_file = dist2depth(output_file)\n                        if \"convert_to_distance\" in reg_out and reg_out[\"convert_to_distance\"]:\n                            output_file = depth2dist(output_file)\n\n                        # semantic seg must be last\n                        if \"is_semantic_segmentation\" in reg_out and reg_out[\"is_semantic_segmentation\"]\\\n                                and \"semantic_segmentation_mapping\" in reg_out \\\n                                and \"semantic_segmentation_default_values\" in reg_out:\n                            output_file = segmentation_mapping(output_file,\n                                                               reg_out[\"semantic_segmentation_mapping\"],\n                                                               reg_out[\"semantic_segmentation_default_values\"])\n                            for key, output_info in output_file.items():\n                                output_data_dict.setdefault(key, []).append(output_info)\n                        else:\n                            output_data_dict.setdefault(reg_out['key'], []).append(output_file)\n                else:\n                    # per run outputs\n                    output_path = resolve_path(reg_out['path'])\n                    output_file = _WriterUtility.load_output_file(output_path, key_has_alpha_channel)\n                    output_data_dict[reg_out['key']] = output_file\n\n        return output_data_dict",
  "def get_stereo_path_pair(file_path: str) -> Tuple[str, str]:\n        \"\"\"\n        Returns stereoscopic file path pair for a given \"normal\" image file path.\n\n        :param file_path: The file path of a single image.\n        :return: The pair of file paths corresponding to the stereo images,\n        \"\"\"\n        path_split = file_path.split(\".\")\n        path_l = f\"{path_split[0]}_L.{path_split[1]}\"\n        path_r = f\"{path_split[0]}_R.{path_split[1]}\"\n\n        return path_l, path_r",
  "def load_output_file(file_path: str, load_alpha_channel: bool = False,\n                         remove: bool = True) -> Union[np.ndarray, List[Any]]:\n        \"\"\" Tries to read in the file with the given path into a numpy array.\n\n        :param file_path: The file path. Type: string.\n        :param load_alpha_channel: Whether to load the alpha channel as well. Type: bool. Default: False\n        :param remove: Whether to delete file after loading.\n        :return: Loaded data from the file as numpy array if possible.\n        \"\"\"\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(\"File not found: \" + file_path)\n\n        file_ending = file_path[file_path.rfind(\".\") + 1:].lower()\n\n        if file_ending in [\"exr\", \"png\", \"jpg\"]:\n            # num_channels is 4 if transparent_background is true in config\n            output = load_image(file_path, num_channels=3 + (1 if load_alpha_channel else 0))\n        elif file_ending in [\"npy\", \"npz\"]:\n            output = np.load(file_path)\n        elif file_ending in [\"csv\"]:\n            output = _WriterUtility.load_csv(file_path)\n        else:\n            raise NotImplementedError(\"File with ending \" + file_ending + \" cannot be loaded.\")\n\n        if remove:\n            os.remove(file_path)\n        return output",
  "def load_csv(file_path: str) -> List[Any]:\n        \"\"\" Load the csv file at the given path.\n\n        :param file_path: The path. Type: string.\n        :return: The content of the file\n        \"\"\"\n        rows = []\n        with open(file_path, mode='r', encoding=\"utf-8\") as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            for row in csv_reader:\n                rows.append(row)\n        return rows",
  "def get_common_attribute(item: bpy.types.Object, attribute_name: str,\n                             local_frame_change: Union[None, List[str]] = None,\n                             world_frame_change: Union[None, List[str]] = None) -> Any:\n        \"\"\" Returns the value of the requested attribute for the given item.\n\n        This method covers all general attributes that blender objects have.\n\n        :param item: The item. Type: blender object.\n        :param attribute_name: The attribute name. Type: string.\n        :param local_frame_change: Can be used to change the local coordinate frame of matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :param world_frame_change: Can be used to change the world coordinate frame of points and matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :return: The attribute value.\n        \"\"\"\n\n        if local_frame_change is None:\n            local_frame_change = [\"X\", \"Y\", \"Z\"]\n        if world_frame_change is None:\n            world_frame_change = [\"X\", \"Y\", \"Z\"]\n\n        # Print warning if local_frame_change is used with other attributes than matrix_world\n        if local_frame_change != [\"X\", \"Y\", \"Z\"] and attribute_name in [\"location\", \"rotation_euler\",\n                                                                        \"rotation_forward_vec\", \"rotation_up_vec\"]:\n            print(\"Warning: The local_frame_change parameter is at the moment only supported by \"\n                  \"the matrix_world attribute.\")\n\n        if attribute_name == \"name\":\n            return item.name\n        if attribute_name == \"location\":\n            return change_coordinate_frame_of_point(item.location, world_frame_change)\n        if attribute_name == \"rotation_euler\":\n            return change_coordinate_frame_of_point(item.rotation_euler, world_frame_change)\n        if attribute_name == \"rotation_forward_vec\":\n            # Calc forward vector from rotation matrix\n            rot_mat = item.rotation_euler.to_matrix()\n            forward = rot_mat @ mathutils.Vector([0, 0, -1])\n            return change_coordinate_frame_of_point(forward, world_frame_change)\n        if attribute_name == \"rotation_up_vec\":\n            # Calc up vector from rotation matrix\n            rot_mat = item.rotation_euler.to_matrix()\n            up = rot_mat @ mathutils.Vector([0, 1, 0])\n            return change_coordinate_frame_of_point(up, world_frame_change)\n        if attribute_name == \"matrix_world\":\n            # Transform matrix_world to given destination frame\n            matrix_world = change_source_coordinate_frame_of_transformation_matrix(Entity(item).get_local2world_mat(),\n                                                                                   local_frame_change)\n            matrix_world = change_target_coordinate_frame_of_transformation_matrix(matrix_world, world_frame_change)\n            return [list(c) for c in matrix_world]\n        if attribute_name.startswith(\"customprop_\"):\n            custom_property_name = attribute_name[len(\"customprop_\"):]\n            # Make sure the requested custom property exist\n            if custom_property_name in item:\n                return item[custom_property_name]\n            raise ValueError(f\"No such custom property: {custom_property_name}\")\n        raise ValueError(f\"No such attribute: {attribute_name}\")",
  "def get_cam_attribute(cam_ob: bpy.context.scene.camera, attribute_name: str,\n                          local_frame_change: Union[None, List[str]] = None,\n                          world_frame_change: Union[None, List[str]] = None) -> Any:\n        \"\"\" Returns the value of the requested attribute for the given object.\n\n        :param cam_ob: The camera object.\n        :param attribute_name: The attribute name.\n        :param local_frame_change: Can be used to change the local coordinate frame of matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :param world_frame_change: Can be used to change the world coordinate frame of points and matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :return: The attribute value.\n        \"\"\"\n\n        if attribute_name == \"fov_x\":\n            return CameraUtility.get_fov()[0]\n        if attribute_name == \"fov_y\":\n            return CameraUtility.get_fov()[1]\n        if attribute_name == \"shift_x\":\n            return cam_ob.data.shift_x\n        if attribute_name == \"shift_y\":\n            return cam_ob.data.shift_y\n        if attribute_name == \"half_fov_x\":\n            return CameraUtility.get_fov()[0] * 0.5\n        if attribute_name == \"half_fov_y\":\n            return CameraUtility.get_fov()[1] * 0.5\n        if attribute_name == \"cam_K\":\n            return [list(c) for c in CameraUtility.get_intrinsics_as_K_matrix()]\n        if attribute_name == \"cam2world_matrix\":\n            return _WriterUtility.get_common_attribute(cam_ob, \"matrix_world\", local_frame_change,\n                                                       world_frame_change)\n        return _WriterUtility.get_common_attribute(cam_ob, attribute_name, local_frame_change,\n                                                   world_frame_change)",
  "def get_light_attribute(light: bpy.types.Light, attribute_name: str,\n                            local_frame_change: Union[None, List[str]] = None,\n                            world_frame_change: Union[None, List[str]] = None) -> Any:\n        \"\"\" Returns the value of the requested attribute for the given light.\n\n        :param light: The light. Type: blender scene object of type light.\n        :param attribute_name: The attribute name.\n        :param local_frame_change: Can be used to change the local coordinate frame of matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :param world_frame_change: Can be used to change the world coordinate frame of points and matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :return: The attribute value.\n        \"\"\"\n        if attribute_name == \"energy\":\n            return light.data.energy\n        return _WriterUtility.get_common_attribute(light, attribute_name, local_frame_change, world_frame_change)",
  "def _get_shapenet_attribute(shapenet_obj: bpy.types.Object, attribute_name: str,\n                                local_frame_change: Union[None, List[str]] = None,\n                                world_frame_change: Union[None, List[str]] = None):\n        \"\"\" Returns the value of the requested attribute for the given object.\n\n        :param shapenet_obj: The ShapeNet object.\n        :param attribute_name: The attribute name.\n        :param local_frame_change: Can be used to change the local coordinate frame of matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :param world_frame_change: Can be used to change the world coordinate frame of points and matrices.\n                                   Default: [\"X\", \"Y\", \"Z\"]\n        :return: The attribute value.\n        \"\"\"\n\n        if attribute_name == \"used_synset_id\":\n            return shapenet_obj.get(\"used_synset_id\", \"\")\n        if attribute_name == \"used_source_id\":\n            return shapenet_obj.get(\"used_source_id\", \"\")\n        return _WriterUtility.get_common_attribute(shapenet_obj, attribute_name, local_frame_change,\n                                                   world_frame_change)",
  "def write_to_hdf_file(file, key: str, data: Union[np.ndarray, list, dict], compression: str = \"gzip\"):\n        \"\"\" Adds the given data as a new entry to the given hdf5 file.\n\n        :param file: The hdf5 file handle. Type: hdf5.File\n        :param key: The key at which the data should be stored in the hdf5 file.\n        :param data: The data to store.\n        \"\"\"\n        if not isinstance(data, np.ndarray) and not isinstance(data, np.bytes_):\n            if isinstance(data, (list, dict)):\n                # If the data contains one or multiple dicts that contain e.q. object states\n                if isinstance(data, dict) or len(data) > 0 and isinstance(data[0], dict):\n                    # Serialize them into json (automatically convert numpy arrays to lists)\n                    data = np.string_(json.dumps(data, cls=NumpyEncoder))\n                data = np.array(data)\n            else:\n                raise Exception(\n                    f\"This fct. expects the data for key {key} to be a np.ndarray, list or dict not a {type(data)}!\")\n\n        if data.dtype.char == 'S':\n            file.create_dataset(key, data=data, dtype=data.dtype)\n        else:\n            file.create_dataset(key, data=data, compression=compression)",
  "def all_with_type(elements: List[Struct], filtered_data_type: Type[Struct] = None) -> List[Struct]:\n    \"\"\" Returns all elements from the given list having a given type.\n\n    :param elements: A list of elements.\n    :param filtered_data_type: If not None, only elements from the given type are returned.\n    :return: All mesh objects from the given list.\n    \"\"\"\n    if filtered_data_type is not None:\n        return list(filter(lambda x: isinstance(x, filtered_data_type), elements))\n    return elements",
  "def by_attr(elements: List[Struct], attr_name: str, value: Any, filtered_data_type: Type[Struct] = None,\n            regex: bool = False) -> List[Struct]:\n    \"\"\" Returns all elements from the given list whose specified attribute has the given value.\n\n    :param elements: A list of elements.\n    :param attr_name: The name of the attribute to look for.\n    :param value: The value the attribute should have.\n    :param filtered_data_type: If not None, only elements from the given type are returned.\n    :param regex: If True, string values will be matched via regex.\n    :return: The elements from the given list that match the given value at the specified attribute.\n    \"\"\"\n    elements = all_with_type(elements, filtered_data_type)\n    return list(filter(lambda struct: _Filter.check_equality(struct.get_attr(attr_name), value, regex), elements))",
  "def one_by_attr(elements: List[Struct], attr_name: str, value: Any, filtered_data_type: Type[Struct] = None,\n                regex: bool = False) -> Struct:\n    \"\"\" Returns the one element from the given list whose specified attribute has the given value.\n\n    An error is thrown is more than one or no element has been found.\n\n    :param elements: A list of elements.\n    :param attr_name: The name of the attribute to look for.\n    :param value: The value the attribute should have.\n    :param filtered_data_type: If not None, only elements from the given type are returned.\n    :param regex: If True, string values will be matched via regex.\n    :return: The one element from the given list that matches the given value at the specified attribute.\n    \"\"\"\n    elements = by_attr(elements, attr_name, value, filtered_data_type, regex)\n    return _Filter.check_list_has_length_one(elements)",
  "def by_cp(elements: List[Struct], cp_name: str, value: Any, filtered_data_type: Type[Struct] = None,\n          regex: bool = False) -> List[Struct]:\n    \"\"\"  Returns all elements from the given list whose specified custom property has the given value.\n\n    :param elements: A list of elements.\n    :param cp_name: The name of the custom property to look for.\n    :param value: The value the custom property should have.\n    :param filtered_data_type: If not None, only elements from the given type are returned.\n    :param regex: If True, string values will be matched via regex.\n    :return: The elements from the given list that match the given value at the specified custom property.\n    \"\"\"\n    elements = all_with_type(elements, filtered_data_type)\n    return list(\n        filter(lambda struct: struct.has_cp(cp_name) and _Filter.check_equality(struct.get_cp(cp_name), value, regex),\n               elements))",
  "def one_by_cp(elements: List[Struct], cp_name: str, value: Any, filtered_data_type: Type[Struct] = None,\n              regex: bool = False) -> Struct:\n    \"\"\" Returns the one element from the given list whose specified custom property has the given value.\n\n    An error is thrown is more than one or no element has been found.\n\n    :param elements: A list of elements.\n    :param cp_name: The name of the custom property to look for.\n    :param value: The value the custom property should have.\n    :param filtered_data_type: If not None, only elements from the given type are returned.\n    :param regex: If True, string values will be matched via regex.\n    :return: The one element from the given list that matches the given value at the specified custom property.\n    \"\"\"\n    elements = by_cp(elements, cp_name, value, filtered_data_type, regex)\n    return _Filter.check_list_has_length_one(elements)",
  "def by_attr_in_interval(elements: List[Struct], attr_name: str, min_value: Any = None, max_value: Any = None,\n                        filtered_data_type: Type[Struct] = None) -> List[Struct]:\n    \"\"\" Returns all elements from the given list whose specified attribute has a value in the given interval\n        (including the boundaries).\n\n    :param elements: A list of elements.\n    :param attr_name: The name of the attribute to look for.\n    :param min_value: The minimum value of the interval.\n    :param max_value: The maximum value of the interval.\n    :param filtered_data_type: If not None, only elements from the given type are returned.\n    :return: The elements from the given list that match the given value at the specified attribute.\n    \"\"\"\n    elements = all_with_type(elements, filtered_data_type)\n    return list(filter(lambda struct: (min_value is None or min_value <= struct.get_attr(attr_name)) and (\n            max_value is None or max_value >= struct.get_attr(attr_name)), elements))",
  "def by_attr_outside_interval(elements: List[Struct], attr_name: str, min_value: Any = None, max_value: Any = None,\n                             filtered_data_type: Type[Struct] = None) -> List[Struct]:\n    \"\"\" Returns all elements from the given list whose specified attribute has a value outside the given interval.\n\n    :param elements: A list of elements.\n    :param attr_name: The name of the attribute to look for.\n    :param min_value: The minimum value of the interval.\n    :param max_value: The maximum value of the interval.\n    :param filtered_data_type: If not None, only elements from the given type are returned.\n    :return: The elements from the given list that match the given value at the specified attribute.\n    \"\"\"\n    elements = all_with_type(elements, filtered_data_type)\n    in_interval = by_attr_in_interval(elements, attr_name, min_value, max_value)\n    return [e for e in elements if e not in in_interval]",
  "class _Filter:\n    \"\"\"Static class for filtering elements based on different elements. \"\"\"\n\n    @staticmethod\n    def check_list_has_length_one(elements: List[Any]) -> Any:\n        \"\"\" Checks if the given list only contains one element and returns it.\n\n        :param elements: The list of elements to check.\n        :return: The one element of the list.\n        \"\"\"\n        if len(elements) > 1:\n            raise Exception(\"More than one element with the given condition has been found.\")\n        if len(elements) == 0:\n            raise Exception(\"No element with the given condition has been found.\")\n        return elements[0]\n\n    @staticmethod\n    def check_equality(attr_value: Any, filter_value: Any, regex: bool = False) -> bool:\n        \"\"\" Checks whether the two values are equal. If the values have multiple elements,\n            they must all match (uses broadcasting).\n\n        :param attr_value: The first value.\n        :param filter_value: The second value.\n        :param regex: If True, string values will be matched via regex.\n        :return: True, if the two values are equal.\n        \"\"\"\n\n        # If desired do regex matching for strings\n        if isinstance(attr_value, str) and regex:\n            return re.fullmatch(filter_value, attr_value)\n        try:\n            return np.all(np.array(filter_value) == np.array(attr_value))\n        except Exception as e:\n            raise RuntimeError(f'Could not broadcast attribute {attr_value} with shape {np.array(attr_value).shape} '\n                               f'to filter_value {filter_value} with shape {np.array(filter_value).shape}!') from e",
  "def check_list_has_length_one(elements: List[Any]) -> Any:\n        \"\"\" Checks if the given list only contains one element and returns it.\n\n        :param elements: The list of elements to check.\n        :return: The one element of the list.\n        \"\"\"\n        if len(elements) > 1:\n            raise Exception(\"More than one element with the given condition has been found.\")\n        if len(elements) == 0:\n            raise Exception(\"No element with the given condition has been found.\")\n        return elements[0]",
  "def check_equality(attr_value: Any, filter_value: Any, regex: bool = False) -> bool:\n        \"\"\" Checks whether the two values are equal. If the values have multiple elements,\n            they must all match (uses broadcasting).\n\n        :param attr_value: The first value.\n        :param filter_value: The second value.\n        :param regex: If True, string values will be matched via regex.\n        :return: True, if the two values are equal.\n        \"\"\"\n\n        # If desired do regex matching for strings\n        if isinstance(attr_value, str) and regex:\n            return re.fullmatch(filter_value, attr_value)\n        try:\n            return np.all(np.array(filter_value) == np.array(attr_value))\n        except Exception as e:\n            raise RuntimeError(f'Could not broadcast attribute {attr_value} with shape {np.array(attr_value).shape} '\n                               f'to filter_value {filter_value} with shape {np.array(filter_value).shape}!') from e",
  "def dist2depth(dist: Union[List[np.ndarray], np.ndarray]) -> Union[List[np.ndarray], np.ndarray]:\n    \"\"\"\n    Maps a distance image to depth image, also works with a list of images.\n\n    :param dist: The distance data.\n    :return: The depth data\n    \"\"\"\n\n    dist = trim_redundant_channels(dist)\n\n    if isinstance(dist, list) or hasattr(dist, \"shape\") and len(dist.shape) > 2:\n        return [dist2depth(img) for img in dist]\n\n    K = CameraUtility.get_intrinsics_as_K_matrix()\n    f, cx, cy = K[0, 0], K[0, 2], K[1, 2]\n\n    xs, ys = np.meshgrid(np.arange(dist.shape[1]), np.arange(dist.shape[0]))\n\n    # coordinate distances to principal point\n    x_opt = np.abs(xs - cx)\n    y_opt = np.abs(ys - cy)\n\n    # Solve 3 equations in Wolfram Alpha:\n    # Solve[{X == (x-c0)/f0*Z, Y == (y-c1)/f0*Z, X*X + Y*Y + Z*Z = d*d}, {X,Y,Z}]\n    depth = dist * f / np.sqrt(x_opt ** 2 + y_opt ** 2 + f ** 2)\n\n    return depth",
  "def depth2dist(depth: Union[List[np.ndarray], np.ndarray]) -> Union[List[np.ndarray], np.ndarray]:\n    \"\"\"\n    Maps a depth image to distance image, also works with a list of images.\n\n    :param depth: The depth data.\n    :return: The distance data\n    \"\"\"\n\n    depth = trim_redundant_channels(depth)\n\n    if isinstance(depth, list) or hasattr(depth, \"shape\") and len(depth.shape) > 2:\n        return [depth2dist(img) for img in depth]\n\n    K = CameraUtility.get_intrinsics_as_K_matrix()\n    f, cx, cy = K[0, 0], K[0, 2], K[1, 2]\n\n    xs, ys = np.meshgrid(np.arange(depth.shape[1]), np.arange(depth.shape[0]))\n\n    # coordinate distances to principal point\n    x_opt = np.abs(xs - cx)\n    y_opt = np.abs(ys - cy)\n\n    # Solve 3 equations in Wolfram Alpha:\n    # Solve[{X == (x-c0)/f0*Z, Y == (y-c1)/f0*Z, X*X + Y*Y + Z*Z = d*d}, {X,Y,Z}]\n    dist = depth * np.sqrt(x_opt ** 2 + y_opt ** 2 + f ** 2) / f\n\n    return dist",
  "def remove_segmap_noise(image: Union[list, np.ndarray]) -> Union[list, np.ndarray]:\n    \"\"\"\n    A function that takes an image and a few 2D indices, where these indices correspond to pixel values in\n    segmentation maps, where these values are not real labels, but some deviations from the real labels, that were\n    generated as a result of Blender doing some interpolation, smoothing, or other numerical operations.\n\n    Assumes that noise pixel values won't occur more than 100 times.\n\n    :param image: ndarray of the .exr segmap\n    :return: The denoised segmap image\n    \"\"\"\n\n    if isinstance(image, list) or hasattr(image, \"shape\") and len(image.shape) > 3:\n        return [remove_segmap_noise(img) for img in image]\n\n    noise_indices = _PostProcessingUtility.determine_noisy_pixels(image)\n\n    for index in noise_indices:\n        neighbors = _PostProcessingUtility.get_pixel_neighbors(image, index[0], index[\n            1])  # Extracting the indices surrounding 3x3 neighbors\n        curr_val = image[index[0]][index[1]][0]  # Current value of the noisy pixel\n\n        neighbor_vals = [image[neighbor[0]][neighbor[1]] for neighbor in\n                         neighbors]  # Getting the values of the neighbors\n        # Getting the unique values only\n        neighbor_vals = np.unique(np.array([np.array(index) for index in neighbor_vals]))\n\n        min_val = 10000000000\n        min_idx = 0\n\n        # Here we iterate through the unique values of the neighbor and find the one closest to the current noisy value\n        for idx, n in enumerate(neighbor_vals):\n            # Is this closer than the current closest value?\n            if n - curr_val <= min_val:\n                # If so, update\n                min_val = n - curr_val\n                min_idx = idx\n\n        # Now that we have found the closest value, assign it to the noisy value\n        new_val = neighbor_vals[min_idx]\n        image[index[0]][index[1]] = np.array([new_val, new_val, new_val])\n\n    return image",
  "def oil_paint_filter(image: Union[list, np.ndarray], filter_size: int = 5, edges_only: bool = True,\n                     rgb: bool = False) -> Union[list, np.ndarray]:\n    \"\"\" Applies the oil paint filter on a single channel image (or more than one channel, where each channel is a\n        replica of the other). This could be desired for corrupting rendered depth maps to appear more realistic.\n        Also trims the redundant channels if they exist.\n\n        :param image: Input image or list of images\n        :param filter_size: Filter size, should be an odd number.\n        :param edges_only: If true, applies the filter on the edges only.\n        :param rgb: Apply the filter on an RGB image (if the image has 3 channels, they're assumed to not be \\\n                    replicated).\n        :return: filtered image\n    \"\"\"\n\n    if rgb:\n        if isinstance(image, list) or hasattr(image, \"shape\") and len(image.shape) > 3:\n            return [oil_paint_filter(img, filter_size, edges_only, rgb) for img in image]\n\n        intensity_img = np.sum(image, axis=2) / 3.0\n\n        neighbors = np.array(\n            _PostProcessingUtility.get_pixel_neighbors_stacked(image, filter_size, return_list=True))\n        neighbors_intensity = _PostProcessingUtility.get_pixel_neighbors_stacked(intensity_img, filter_size)\n\n        mode_intensity = stats.mode(neighbors_intensity, axis=2)[0].reshape(image.shape[0], image.shape[1])\n\n        # keys here would match all instances of the mode value\n        mode_keys = np.argwhere(neighbors_intensity == np.expand_dims(mode_intensity, axis=3))\n        # Remove the duplicate keys, since they point to the same value, and to be able to use them for indexing\n        _, unique_indices = np.unique(mode_keys[:, 0:2], axis=0, return_index=True)\n        unique_keys = mode_keys[unique_indices]\n\n        filtered_img = neighbors[unique_keys[:, 2], unique_keys[:, 0], unique_keys[:, 1], :] \\\n            .reshape(image.shape[0], image.shape[1], image.shape[2])\n\n        if edges_only:\n            edges = cv2.Canny(image, 0, np.max(image))  # Assuming \"image\" is an uint8 array.\n            image[edges > 0] = filtered_img[edges > 0]\n            filtered_img = image\n    else:\n        image = trim_redundant_channels(image)\n        if isinstance(image, list) or hasattr(image, \"shape\") and len(image.shape) > 2:\n            return [oil_paint_filter(img, filter_size, edges_only, rgb) for img in image]\n\n        if len(image.shape) == 3 and image.shape[2] > 1:\n            image = image[:, :, 0]\n\n        filtered_img = stats.mode(_PostProcessingUtility.get_pixel_neighbors_stacked(image, filter_size), axis=2)[0]\n        filtered_img = filtered_img.reshape(filtered_img.shape[0], filtered_img.shape[1])\n\n        if edges_only:\n            # Handle inf and map input to the range: 0-255\n            _image = np.copy(image)\n            _max = np.max(_image) if np.max(_image) != np.inf else np.unique(_image)[-2]\n            _image[_image > _max] = _max\n            _image = (_image / _max) * 255.0\n\n            __img = np.uint8(_image)\n            edges = cv2.Canny(__img, 0, np.max(__img))\n\n            image[edges > 0] = filtered_img[edges > 0]\n            filtered_img = image\n\n    return filtered_img",
  "def add_kinect_azure_noise(depth: Union[list, np.ndarray], color: Optional[Union[list, np.ndarray]] = None,\n                           missing_depth_darkness_thres: int = 15) -> Union[list, np.ndarray]:\n    \"\"\"\n    Add noise, holes and smooth depth maps according to the noise characteristics of the Kinect Azure sensor.\n    https://www.mdpi.com/1424-8220/21/2/413\n\n    For further realism, consider to use the projection from depth to color image in the Azure Kinect SDK:\n    https://docs.microsoft.com/de-de/azure/kinect-dk/use-image-transformation\n\n    :param depth: Input depth image(s) in meters\n    :param color: Optional color image(s) to add missing depth at close to black surfaces\n    :param missing_depth_darkness_thres: uint8 gray value threshold at which depth becomes invalid, i.e. 0\n    :return: Noisy depth image(s)\n    \"\"\"\n\n    if isinstance(depth, list) or hasattr(depth, \"shape\") and len(depth.shape) > 2:\n        if color is None:\n            color = len(depth) * [None]\n        assert len(color) == len(depth), \"Enter same number of depth and color images\"\n        return [add_kinect_azure_noise(d, c, missing_depth_darkness_thres) for d,c in zip(depth, color)]\n\n    # smoothing at borders\n    depth = add_gaussian_shifts(depth, 0.25)\n\n    # 0.5mm base noise, 1mm std noise @ 1m, 3.6mm std noise @ 3m\n    depth += (5/10000 + np.maximum((depth-0.5) * 1/1000, 0)) * np.random.normal(size=depth.shape)\n\n    # Creates the shape of the kernel\n    shape = cv2.MORPH_RECT\n    kernel = cv2.getStructuringElement(shape, (3,3))\n\n    # Applies the minimum filter with kernel NxN\n    min_depth = cv2.erode(depth, kernel)\n    max_depth = cv2.dilate(depth, kernel)\n\n    # missing depth at 0.8m min/max difference\n    depth[abs(min_depth-max_depth) > 0.8] = 0\n\n    # create missing depth at dark surfaces\n    if color is not None:\n        gray = cv2.cvtColor(color, cv2.COLOR_RGB2GRAY)\n        depth[gray<missing_depth_darkness_thres] = 0\n\n    return depth",
  "def add_gaussian_shifts(image: Union[list, np.ndarray], std: float = 0.5) -> Union[list, np.ndarray]:\n    \"\"\"\n    Randomly shifts the pixels of the input depth image in x and y direction.\n\n    :param image: Input depth image(s)\n    :param std: Standard deviation of pixel shifts, defaults to 0.5\n    :return: Augmented images\n    \"\"\"\n\n    if isinstance(image, list) or hasattr(image, \"shape\") and len(image.shape) > 2:\n        return [add_gaussian_shifts(img, std=std) for img in image]\n\n    rows, cols = image.shape\n    gaussian_shifts = np.random.normal(0, std, size=(rows, cols, 2))\n    gaussian_shifts = gaussian_shifts.astype(np.float32)\n\n    # creating evenly spaced coordinates\n    xx = np.linspace(0, cols-1, cols)\n    yy = np.linspace(0, rows-1, rows)\n\n    # get xpixels and ypixels\n    xp, yp = np.meshgrid(xx, yy)\n\n    xp = xp.astype(np.float32)\n    yp = yp.astype(np.float32)\n\n    xp_interp = np.minimum(np.maximum(xp + gaussian_shifts[:, :, 0], 0.0), cols)\n    yp_interp = np.minimum(np.maximum(yp + gaussian_shifts[:, :, 1], 0.0), rows)\n\n    depth_interp = cv2.remap(image, xp_interp, yp_interp, cv2.INTER_LINEAR)\n\n    return depth_interp",
  "def trim_redundant_channels(image: Union[list, np.ndarray]) -> Union[list, np.ndarray]:\n    \"\"\"\n    Remove redundant channels, this is useful to remove the two of the three channels created for a\n    depth or distance image. This also works on a list of images. Be aware that there is no check performed,\n    to ensure that all channels are really equal.\n\n    :param image: Input image or list of images\n    :return: The trimmed image data with preserved input type\n    \"\"\"\n\n    if isinstance(image, list):\n        return [trim_redundant_channels(ele) for ele in image]\n\n    if hasattr(image, \"shape\") and len(image.shape) > 3:\n        return np.array([trim_redundant_channels(ele) for ele in image])\n\n    if hasattr(image, \"shape\") and len(image.shape) == 3 and image.shape[2] == 3:\n        image = image[:, :, 0]  # All channels have the same value, so just extract any single channel\n\n    return image",
  "def segmentation_mapping(image: Union[List[np.ndarray], np.ndarray],\n                         map_by: Union[str, List[str]],\n                         default_values: Optional[Dict[str, int]]) \\\n        -> Dict[str, Union[np.ndarray, List[np.ndarray], List[Dict[str, Any]]]]:\n    \"\"\" Maps an image or a list of images to the desired segmentation images plus segmentation dictionary for keys,\n    which can not be stored in an image (e.g. `name`).\n\n    :param image: A list or single image of a scene, must contain the pass indices defined in\n                 `enable_segmentation_output`.\n    :param map_by: The keys which will be extracted from the objects, either a single key or a list of keys.\n    :param default_values: If an object does not provide a key a default key must be provided.\n    :return: A dict mapping each key in map_by to an output list of images or a dictionary containing the information\n    \"\"\"\n\n    return_dict: Dict[str, Union[np.ndarray, List[np.ndarray], List[Dict[str, Any]]]] = {}\n    is_stereo_case = bpy.context.scene.render.use_multiview\n    # convert a single image to a list of stereo images\n    if isinstance(image, list):\n        if len(image) == 0:\n            raise RuntimeError(\"The given image list is empty\")\n        if hasattr(image[0], \"shape\") and len(image[0].shape) == 2 and not is_stereo_case:\n            # convert list of images to np.ndarray\n            image = np.array(image)[:, np.newaxis, :, :]  # reshape for the stereo case\n    elif hasattr(image, \"shape\") and len(image.shape) == 3:\n        if not is_stereo_case:\n            image = np.array(image)[:, np.newaxis, :, :]  # reshape for stereo case\n        else:\n            # this is a single image in stereo mode -> make a list out of it\n            image = np.array(image)[np.newaxis, :, :, :]\n    elif hasattr(image, \"shape\") and len(image.shape) == 2:\n        if is_stereo_case:\n            raise RuntimeError(\"The amount of dimensions for an image must be higher than two in stereo mode!\")\n        # add stereo case and make a list out of it\n        image = np.array(image)[np.newaxis, np.newaxis, :, :]\n\n    # convert to int, to avoid rounding errors\n    image = np.array(image).astype(np.int64)\n\n    # convert map by to a list\n    if not isinstance(map_by, list):\n        map_by = [map_by]\n\n    for frame_image in image:\n        non_image_attributes: Dict[int, Dict[str, Any]] = {}\n        mapped_results_stereo_dict: Dict[str, List[np.ndarray]] = {}\n        for stereo_image in frame_image:\n\n            # map object ids in the image to the used objects\n            object_ids = np.unique(stereo_image).astype(int)\n            object_ids_to_object = {}\n            for obj in get_all_blender_mesh_objects():\n                if obj.pass_index in object_ids:\n                    object_ids_to_object[obj.pass_index] = obj\n            object_ids_to_object[0] = bpy.context.scene.world\n\n            for map_by_attribute in map_by:\n\n                # create result map\n                resulting_map = np.zeros((stereo_image.shape[0], stereo_image.shape[1]), dtype=np.float64)\n\n                # save the type of the stored variable in the resulting map\n                found_dtype = None\n\n                map_by_attribute = map_by_attribute.lower()\n                current_attribute = map_by_attribute\n                if map_by_attribute in [\"class\", \"category_id\"]:\n                    # class mode\n                    current_attribute = \"category_id\"\n                if map_by_attribute == \"instance\":\n                    mapped_results_stereo_dict.setdefault(f\"{map_by_attribute}_segmaps\", []).append(stereo_image)\n                else:\n                    # check if a default value was specified\n                    default_value_set = False\n                    default_value = None\n                    if default_values and current_attribute in default_values:\n                        default_value_set = True\n                        default_value = default_values[current_attribute]\n                    elif default_values and current_attribute in default_values:\n                        default_value_set = True\n                        default_value = default_values[current_attribute]\n\n                    for object_id in object_ids:\n                        # get current object\n                        current_obj = object_ids_to_object[object_id]\n\n                        # if the current obj has an attribute with that name -> get it\n                        if hasattr(current_obj, current_attribute):\n                            value = getattr(current_obj, current_attribute)\n                        # if the current object has a custom property with that name -> get it\n                        elif current_attribute in current_obj:\n                            value = current_obj[current_attribute]\n                        elif current_attribute.startswith(\"cf_\"):\n                            if current_attribute == \"cf_basename\":\n                                value = current_obj.name\n                                if \".\" in value:\n                                    value = value[:value.rfind(\".\")]\n                            else:\n                                raise ValueError(f\"The given attribute is a custom function: \\\"cf_\\\", but it is not \"\n                                                 f\"defined here: {current_attribute}\")\n                        elif default_value_set:\n                            # if none of the above applies use the default value\n                            value = default_value\n                        else:\n                            # if the requested current_attribute is not a custom property or an attribute\n                            # or there is a default value stored\n                            # it throws an exception\n                            d_error = {current_attribute: None}\n                            raise RuntimeError(f\"The object \\\"{current_obj.name}\\\" does not have the \"\n                                               f\"attribute: \\\"{current_attribute}\\\". Either set the attribute for \"\n                                               f\"every object or pass a default value to \"\n                                               f\"bproc.renderer.enable_segmentation_output(default_values={d_error}).\")\n\n                        # save everything which is not instance also in the .csv\n                        if isinstance(value, (int, float, np.integer, np.floating)):\n                            resulting_map[stereo_image == object_id] = value\n                            found_dtype = type(value)\n\n                        if isinstance(value, (mathutils.Vector, mathutils.Matrix)):\n                            value = np.array(value)\n\n                        if object_id in non_image_attributes:\n                            non_image_attributes[object_id][current_attribute] = value\n                        else:\n                            non_image_attributes[object_id] = {current_attribute: value}\n\n                    # if a value was found the resulting map should be stored\n                    if found_dtype is not None:\n                        resulting_map = resulting_map.astype(found_dtype)\n                        mapped_results_stereo_dict.setdefault(f\"{map_by_attribute}_segmaps\", []).append(resulting_map)\n                    elif \"instance\" not in map_by:\n                        raise ValueError(f\"The map_by key \\\"{map_by_attribute}\\\" requires that the instance map is \"\n                                         f\"stored as well in the output. Change it to: {map_by + ['instance']}\")\n        # combine stereo image and add to output\n        for key, list_of_stereo_images in mapped_results_stereo_dict.items():\n            if len(list_of_stereo_images) == 1:\n                return_dict.setdefault(key, []).append(list_of_stereo_images[0])\n            else:\n                stereo_image = np.stack(list_of_stereo_images, axis=0)\n                return_dict.setdefault(key, []).append(stereo_image)\n\n        # combine non image attributes\n        mappings = []\n        for object_id, attribute_dict in non_image_attributes.items():\n            # converting to int to being able to save it to a hdf5 container\n            mappings.append({\"idx\": int(object_id), **attribute_dict})\n        return_dict.setdefault(\"instance_attribute_maps\", []).append(mappings)\n\n    # check if only one image was provided as input\n    if image.shape[0] == 1:\n        # remove the list in the return dict, as there was only a single input image\n        # this still works with stereo image as they are fused together in here\n        return {key: value[0] for key, value in return_dict.items()}\n    return return_dict",
  "class _PostProcessingUtility:\n\n    @staticmethod\n    def get_pixel_neighbors(data: np.ndarray, i: int, j: int) -> np.ndarray:\n        \"\"\" Returns the valid neighbor pixel indices of the given pixel.\n\n        :param data: The whole image data.\n        :param i: The row index of the pixel\n        :param j: The col index of the pixel.\n        :return: A list of neighbor point indices.\n        \"\"\"\n        neighbors = []\n        for p in range(max(0, i - 1), min(data.shape[0], i + 2)):\n            for q in range(max(0, j - 1), min(data.shape[1], j + 2)):\n                if not (p == i and q == j):  # We don't want the current pixel, just the neighbors\n                    neighbors.append([p, q])\n\n        return np.array(neighbors)\n\n    @staticmethod\n    def get_pixel_neighbors_stacked(img: np.ndarray, filter_size: int = 3,\n                                    return_list: bool = False) -> Union[list, np.ndarray]:\n        \"\"\"\n        Stacks the neighbors of each pixel according to a square filter around each given pixel in the depth dimensions.\n        The neighbors are represented by shifting the input image in all directions required to simulate the filter.\n\n        :param img: Input image. Type: blender object of type image.\n        :param filter_size: Filter size. Type: int. Default: 5..\n        :param return_list: Instead of stacking in the output array, just return a list of the \"neighbor\" \\\n                            images along with the input image.\n        :return: Either a tensor with the \"neighbor\" images stacked in a separate additional dimension, or a list of \\\n                 images of the same shape as the input image, containing the shifted images (simulating the neighbors) \\\n                 and the input image.\n        \"\"\"\n        _min = -int(filter_size / 2)\n        _max = _min + filter_size\n\n        rows, cols = img.shape[0], img.shape[1]\n\n        channels = [img]\n        for p in range(_min, _max):\n            for q in range(_min, _max):\n                if p == 0 and q == 0:\n                    continue\n                shifted = np.zeros_like(img)\n                shifted[max(p, 0):min(rows, rows + p), max(q, 0):min(cols, cols + q)] = img[\n                                                                                        max(-p, 0):min(rows - p, rows),\n                                                                                        max(-q, 0):min(cols - q, cols)]\n\n                channels.append(shifted)\n\n        if return_list:\n            return channels\n        return np.dstack(tuple(channels))\n\n    @staticmethod\n    def is_in(element, test_elements, assume_unique=False, invert=False):\n        \"\"\" As np.isin is only available after v1.13 and blender is using 1.10.1 we have to implement it manually. \"\"\"\n        element = np.asarray(element)\n        return np.in1d(element, test_elements, assume_unique=assume_unique, invert=invert).reshape(element.shape)\n\n    @staticmethod\n    def determine_noisy_pixels(image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        :param image: The image data.\n        :return: a list of 2D indices that correspond to the noisy pixels. One criterion of finding \\\n                              these pixels is to use a histogram and find the pixels with frequencies lower than \\\n                              a threshold, e.g. 100.\n        \"\"\"\n        # The map was scaled to be ranging along the entire 16-bit color depth, and this is the scaling down operation\n        # that should remove some noise or deviations\n        image = (image * 37) / (65536)  # assuming 16 bit color depth\n        image = image.astype(np.int32)\n        b, counts = np.unique(image.flatten(), return_counts=True)\n\n        # Removing further noise where there are some stray pixel values with very small counts, by assigning them to\n        # their closest (numerically, since this deviation is a\n        # result of some numerical operation) neighbor.\n        hist = sorted((np.asarray((b, counts)).T), key=lambda x: x[1])\n        # Assuming the stray pixels wouldn't have a count of more than 100\n        noise_vals = [h[0] for h in hist if h[1] <= 100]\n        noise_indices = np.argwhere(_PostProcessingUtility.is_in(image, noise_vals))\n\n        return noise_indices",
  "def get_pixel_neighbors(data: np.ndarray, i: int, j: int) -> np.ndarray:\n        \"\"\" Returns the valid neighbor pixel indices of the given pixel.\n\n        :param data: The whole image data.\n        :param i: The row index of the pixel\n        :param j: The col index of the pixel.\n        :return: A list of neighbor point indices.\n        \"\"\"\n        neighbors = []\n        for p in range(max(0, i - 1), min(data.shape[0], i + 2)):\n            for q in range(max(0, j - 1), min(data.shape[1], j + 2)):\n                if not (p == i and q == j):  # We don't want the current pixel, just the neighbors\n                    neighbors.append([p, q])\n\n        return np.array(neighbors)",
  "def get_pixel_neighbors_stacked(img: np.ndarray, filter_size: int = 3,\n                                    return_list: bool = False) -> Union[list, np.ndarray]:\n        \"\"\"\n        Stacks the neighbors of each pixel according to a square filter around each given pixel in the depth dimensions.\n        The neighbors are represented by shifting the input image in all directions required to simulate the filter.\n\n        :param img: Input image. Type: blender object of type image.\n        :param filter_size: Filter size. Type: int. Default: 5..\n        :param return_list: Instead of stacking in the output array, just return a list of the \"neighbor\" \\\n                            images along with the input image.\n        :return: Either a tensor with the \"neighbor\" images stacked in a separate additional dimension, or a list of \\\n                 images of the same shape as the input image, containing the shifted images (simulating the neighbors) \\\n                 and the input image.\n        \"\"\"\n        _min = -int(filter_size / 2)\n        _max = _min + filter_size\n\n        rows, cols = img.shape[0], img.shape[1]\n\n        channels = [img]\n        for p in range(_min, _max):\n            for q in range(_min, _max):\n                if p == 0 and q == 0:\n                    continue\n                shifted = np.zeros_like(img)\n                shifted[max(p, 0):min(rows, rows + p), max(q, 0):min(cols, cols + q)] = img[\n                                                                                        max(-p, 0):min(rows - p, rows),\n                                                                                        max(-q, 0):min(cols - q, cols)]\n\n                channels.append(shifted)\n\n        if return_list:\n            return channels\n        return np.dstack(tuple(channels))",
  "def is_in(element, test_elements, assume_unique=False, invert=False):\n        \"\"\" As np.isin is only available after v1.13 and blender is using 1.10.1 we have to implement it manually. \"\"\"\n        element = np.asarray(element)\n        return np.in1d(element, test_elements, assume_unique=assume_unique, invert=invert).reshape(element.shape)",
  "def determine_noisy_pixels(image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        :param image: The image data.\n        :return: a list of 2D indices that correspond to the noisy pixels. One criterion of finding \\\n                              these pixels is to use a histogram and find the pixels with frequencies lower than \\\n                              a threshold, e.g. 100.\n        \"\"\"\n        # The map was scaled to be ranging along the entire 16-bit color depth, and this is the scaling down operation\n        # that should remove some noise or deviations\n        image = (image * 37) / (65536)  # assuming 16 bit color depth\n        image = image.astype(np.int32)\n        b, counts = np.unique(image.flatten(), return_counts=True)\n\n        # Removing further noise where there are some stray pixel values with very small counts, by assigning them to\n        # their closest (numerically, since this deviation is a\n        # result of some numerical operation) neighbor.\n        hist = sorted((np.asarray((b, counts)).T), key=lambda x: x[1])\n        # Assuming the stray pixels wouldn't have a count of more than 100\n        noise_vals = [h[0] for h in hist if h[1] <= 100]\n        noise_indices = np.argwhere(_PostProcessingUtility.is_in(image, noise_vals))\n\n        return noise_indices",
  "def stereo_global_matching(color_images: List[np.ndarray], depth_max: Optional[float] = None, window_size: int = 7,\n                           num_disparities: int = 32, min_disparity: int = 0, disparity_filter: bool = True,\n                           depth_completion: bool = True) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n    \"\"\" Does the stereo global matching in the following steps:\n    1. Collect camera object and its state,\n    2. For each frame, load left and right images and call the `sgm()` methode.\n    3. Write the results to a numpy file.\n\n    :param color_images: A list of stereo images, where each entry has the shape [2, height, width, 3].\n    :param depth_max: The maximum depth value for clipping the resulting depth values. If None,\n                      distance_start + distance_range that were configured for distance rendering are used.\n    :param window_size: Semi-global matching kernel size. Should be an odd number.\n    :param num_disparities: Semi-global matching number of disparities. Should be > 0 and divisible by 16.\n    :param min_disparity: Semi-global matching minimum disparity.\n    :param disparity_filter: Applies post-processing of the generated disparity map using WLS filter.\n    :param depth_completion: Applies basic depth completion using image processing techniques.\n    :return: Returns the computed depth and disparity images for all given frames.\n    \"\"\"\n    # Collect camera and camera object\n    cam_ob = bpy.context.scene.camera\n    cam = cam_ob.data\n\n    baseline = cam.stereo.interocular_distance\n    if not baseline:\n        raise Exception(\"Stereo parameters are not set. Make sure to enable RGB stereo rendering before this module.\")\n\n    if depth_max is None:\n        depth_max = bpy.context.scene.world.mist_settings.start + bpy.context.scene.world.mist_settings.depth\n\n    baseline = cam.stereo.interocular_distance\n    if not baseline:\n        raise Exception(\"Stereo parameters are not set. Make sure to enable RGB stereo rendering before this module.\")\n\n    focal_length = CameraUtility.get_intrinsics_as_K_matrix()[0, 0]\n\n    depth_frames = []\n    disparity_frames = []\n    for color_image in color_images:\n        depth, disparity = _StereoGlobalMatching.stereo_global_matching(color_image[0], color_image[1], baseline,\n                                                                        depth_max, focal_length, window_size,\n                                                                        num_disparities, min_disparity,\n                                                                        disparity_filter, depth_completion)\n\n        depth_frames.append(depth)\n        disparity_frames.append(disparity)\n\n    return depth_frames, disparity_frames",
  "class _StereoGlobalMatching:\n\n    @staticmethod\n    def stereo_global_matching(left_color_image: np.ndarray, right_color_image: np.ndarray, baseline: float,\n                               depth_max: float, focal_length: float, window_size: int = 7, num_disparities: int = 32,\n                               min_disparity: int = 0, disparity_filter: bool = True,\n                               depth_completion: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\" Semi global matching funciton, for more details on what this function does check the original paper\n        https://elib.dlr.de/73119/1/180Hirschmueller.pdf\n\n        :param left_color_image: The left color image.\n        :param right_color_image: The right color image.\n        :param baseline: The baseline that was used for rendering the two images.\n        :param depth_max: The maximum depth value for clipping the resulting depth values.\n        :param focal_length: The focal length that was used for rendering the two images.\n        :param window_size: Semi-global matching kernel size. Should be an odd number.\n        :param num_disparities: Semi-global matching number of disparities. Should be > 0 and divisible by 16.\n        :param min_disparity: Semi-global matching minimum disparity.\n        :param disparity_filter: Applies post-processing of the generated disparity map using WLS filter.\n        :param depth_completion: Applies basic depth completion using image processing techniques.\n        :return: depth, disparity\n         \"\"\"\n        if window_size % 2 == 0:\n            raise ValueError(\"Window size must be an odd number\")\n\n        if not (num_disparities > 0 and num_disparities % 16 == 0):\n            raise ValueError(\"Number of disparities must be > 0 and divisible by 16\")\n\n        left_matcher = cv2.StereoSGBM_create(\n            minDisparity=min_disparity,\n            numDisparities=num_disparities,\n            blockSize=5,\n            P1=8 * 3 * window_size ** 2,\n            P2=32 * 3 * window_size ** 2,\n            disp12MaxDiff=-1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            # mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n            mode=cv2.StereoSGBM_MODE_HH\n        )\n\n        if disparity_filter:\n            right_matcher = cv2.ximgproc.createRightMatcher(left_matcher)\n\n            lmbda = 80000\n            sigma = 1.2\n\n            wls_filter = cv2.ximgproc.createDisparityWLSFilter(matcher_left=left_matcher)\n            wls_filter.setLambda(lmbda)\n            wls_filter.setSigmaColor(sigma)\n\n            dispr = right_matcher.compute(right_color_image, left_color_image)\n\n        displ = left_matcher.compute(left_color_image, right_color_image)\n\n        filteredImg = None\n        if disparity_filter:\n            filteredImg = wls_filter.filter(displ, left_color_image, None, dispr).astype(np.float32)\n            filteredImg = cv2.normalize(src=filteredImg, dst=filteredImg, beta=0, alpha=255, norm_type=cv2.NORM_MINMAX)\n\n        disparity_to_be_written = filteredImg if disparity_filter else displ\n        disparity = np.float32(np.copy(disparity_to_be_written)) / 16.0\n\n        # Triangulation\n        depth = (1.0 / disparity) * baseline * focal_length\n\n        # Clip from depth map to 25 meters\n        depth[depth > depth_max] = depth_max\n        depth[depth < 0] = 0.0\n\n        if depth_completion:\n            depth = _StereoGlobalMatching.fill_in_fast(depth, depth_max)\n\n        return depth, disparity_to_be_written\n\n    @staticmethod\n    # https://github.com/kujason/ip_basic/blob/master/ip_basic/depth_map_utils.py\n    def fill_in_fast(depth_map: np.ndarray, max_depth: float = 100.0, custom_kernel: Optional[np.ndarray] = None,\n                     extrapolate: bool = False, blur_type: str = 'bilateral'):\n        \"\"\"Fast, in-place depth completion.\n\n        :param depth_map: projected depths\n        :param max_depth: max depth value for inversion\n        :param custom_kernel: kernel to apply initial dilation\n        :param extrapolate: whether to extrapolate by extending depths to top of the frame, and applying a 31x31 \\\n                            full kernel dilation\n        :param blur_type: 'bilateral' - preserves local structure (recommended), 'gaussian' - provides lower RMSE\n        :return: depth_map: dense depth map\n        \"\"\"\n\n        # Full kernels\n        FULL_KERNEL_5 = np.ones((5, 5), np.uint8)\n        FULL_KERNEL_7 = np.ones((7, 7), np.uint8)\n        FULL_KERNEL_31 = np.ones((31, 31), np.uint8)\n\n        if custom_kernel is None:\n            custom_kernel = FULL_KERNEL_5\n\n        # Invert\n        valid_pixels = depth_map > 0.1\n        depth_map[valid_pixels] = max_depth - depth_map[valid_pixels]\n\n        # Dilate\n        depth_map = cv2.dilate(depth_map, custom_kernel)\n\n        # Hole closing\n        depth_map = cv2.morphologyEx(depth_map, cv2.MORPH_CLOSE, FULL_KERNEL_5)\n\n        # Fill empty spaces with dilated values\n        empty_pixels = depth_map < 0.1\n        dilated = cv2.dilate(depth_map, FULL_KERNEL_7)\n        depth_map[empty_pixels] = dilated[empty_pixels]\n\n        # Extend the highest pixel to top of image\n        if extrapolate:\n            top_row_pixels = np.argmax(depth_map > 0.1, axis=0)\n            top_pixel_values = depth_map[top_row_pixels, range(depth_map.shape[1])]\n\n            for pixel_col_idx in range(depth_map.shape[1]):\n                depth_map[0:top_row_pixels[pixel_col_idx], pixel_col_idx] = \\\n                    top_pixel_values[pixel_col_idx]\n\n            # Large Fill\n            empty_pixels = depth_map < 0.1\n            dilated = cv2.dilate(depth_map, FULL_KERNEL_31)\n            depth_map[empty_pixels] = dilated[empty_pixels]\n\n        # Median blur\n        depth_map = cv2.medianBlur(depth_map, 5)\n\n        # Bilateral or Gaussian blur\n        if blur_type == 'bilateral':\n            # Bilateral blur\n            depth_map = cv2.bilateralFilter(depth_map, 5, 1.5, 2.0)\n        elif blur_type == 'gaussian':\n            # Gaussian blur\n            valid_pixels = depth_map > 0.1\n            blurred = cv2.GaussianBlur(depth_map, (5, 5), 0)\n            depth_map[valid_pixels] = blurred[valid_pixels]\n\n        # Invert\n        valid_pixels = depth_map > 0.1\n        depth_map[valid_pixels] = max_depth - depth_map[valid_pixels]\n\n        return depth_map",
  "def stereo_global_matching(left_color_image: np.ndarray, right_color_image: np.ndarray, baseline: float,\n                               depth_max: float, focal_length: float, window_size: int = 7, num_disparities: int = 32,\n                               min_disparity: int = 0, disparity_filter: bool = True,\n                               depth_completion: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\" Semi global matching funciton, for more details on what this function does check the original paper\n        https://elib.dlr.de/73119/1/180Hirschmueller.pdf\n\n        :param left_color_image: The left color image.\n        :param right_color_image: The right color image.\n        :param baseline: The baseline that was used for rendering the two images.\n        :param depth_max: The maximum depth value for clipping the resulting depth values.\n        :param focal_length: The focal length that was used for rendering the two images.\n        :param window_size: Semi-global matching kernel size. Should be an odd number.\n        :param num_disparities: Semi-global matching number of disparities. Should be > 0 and divisible by 16.\n        :param min_disparity: Semi-global matching minimum disparity.\n        :param disparity_filter: Applies post-processing of the generated disparity map using WLS filter.\n        :param depth_completion: Applies basic depth completion using image processing techniques.\n        :return: depth, disparity\n         \"\"\"\n        if window_size % 2 == 0:\n            raise ValueError(\"Window size must be an odd number\")\n\n        if not (num_disparities > 0 and num_disparities % 16 == 0):\n            raise ValueError(\"Number of disparities must be > 0 and divisible by 16\")\n\n        left_matcher = cv2.StereoSGBM_create(\n            minDisparity=min_disparity,\n            numDisparities=num_disparities,\n            blockSize=5,\n            P1=8 * 3 * window_size ** 2,\n            P2=32 * 3 * window_size ** 2,\n            disp12MaxDiff=-1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            # mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n            mode=cv2.StereoSGBM_MODE_HH\n        )\n\n        if disparity_filter:\n            right_matcher = cv2.ximgproc.createRightMatcher(left_matcher)\n\n            lmbda = 80000\n            sigma = 1.2\n\n            wls_filter = cv2.ximgproc.createDisparityWLSFilter(matcher_left=left_matcher)\n            wls_filter.setLambda(lmbda)\n            wls_filter.setSigmaColor(sigma)\n\n            dispr = right_matcher.compute(right_color_image, left_color_image)\n\n        displ = left_matcher.compute(left_color_image, right_color_image)\n\n        filteredImg = None\n        if disparity_filter:\n            filteredImg = wls_filter.filter(displ, left_color_image, None, dispr).astype(np.float32)\n            filteredImg = cv2.normalize(src=filteredImg, dst=filteredImg, beta=0, alpha=255, norm_type=cv2.NORM_MINMAX)\n\n        disparity_to_be_written = filteredImg if disparity_filter else displ\n        disparity = np.float32(np.copy(disparity_to_be_written)) / 16.0\n\n        # Triangulation\n        depth = (1.0 / disparity) * baseline * focal_length\n\n        # Clip from depth map to 25 meters\n        depth[depth > depth_max] = depth_max\n        depth[depth < 0] = 0.0\n\n        if depth_completion:\n            depth = _StereoGlobalMatching.fill_in_fast(depth, depth_max)\n\n        return depth, disparity_to_be_written",
  "def fill_in_fast(depth_map: np.ndarray, max_depth: float = 100.0, custom_kernel: Optional[np.ndarray] = None,\n                     extrapolate: bool = False, blur_type: str = 'bilateral'):\n        \"\"\"Fast, in-place depth completion.\n\n        :param depth_map: projected depths\n        :param max_depth: max depth value for inversion\n        :param custom_kernel: kernel to apply initial dilation\n        :param extrapolate: whether to extrapolate by extending depths to top of the frame, and applying a 31x31 \\\n                            full kernel dilation\n        :param blur_type: 'bilateral' - preserves local structure (recommended), 'gaussian' - provides lower RMSE\n        :return: depth_map: dense depth map\n        \"\"\"\n\n        # Full kernels\n        FULL_KERNEL_5 = np.ones((5, 5), np.uint8)\n        FULL_KERNEL_7 = np.ones((7, 7), np.uint8)\n        FULL_KERNEL_31 = np.ones((31, 31), np.uint8)\n\n        if custom_kernel is None:\n            custom_kernel = FULL_KERNEL_5\n\n        # Invert\n        valid_pixels = depth_map > 0.1\n        depth_map[valid_pixels] = max_depth - depth_map[valid_pixels]\n\n        # Dilate\n        depth_map = cv2.dilate(depth_map, custom_kernel)\n\n        # Hole closing\n        depth_map = cv2.morphologyEx(depth_map, cv2.MORPH_CLOSE, FULL_KERNEL_5)\n\n        # Fill empty spaces with dilated values\n        empty_pixels = depth_map < 0.1\n        dilated = cv2.dilate(depth_map, FULL_KERNEL_7)\n        depth_map[empty_pixels] = dilated[empty_pixels]\n\n        # Extend the highest pixel to top of image\n        if extrapolate:\n            top_row_pixels = np.argmax(depth_map > 0.1, axis=0)\n            top_pixel_values = depth_map[top_row_pixels, range(depth_map.shape[1])]\n\n            for pixel_col_idx in range(depth_map.shape[1]):\n                depth_map[0:top_row_pixels[pixel_col_idx], pixel_col_idx] = \\\n                    top_pixel_values[pixel_col_idx]\n\n            # Large Fill\n            empty_pixels = depth_map < 0.1\n            dilated = cv2.dilate(depth_map, FULL_KERNEL_31)\n            depth_map[empty_pixels] = dilated[empty_pixels]\n\n        # Median blur\n        depth_map = cv2.medianBlur(depth_map, 5)\n\n        # Bilateral or Gaussian blur\n        if blur_type == 'bilateral':\n            # Bilateral blur\n            depth_map = cv2.bilateralFilter(depth_map, 5, 1.5, 2.0)\n        elif blur_type == 'gaussian':\n            # Gaussian blur\n            valid_pixels = depth_map > 0.1\n            blurred = cv2.GaussianBlur(depth_map, (5, 5), 0)\n            depth_map[valid_pixels] = blurred[valid_pixels]\n\n        # Invert\n        valid_pixels = depth_map > 0.1\n        depth_map[valid_pixels] = max_depth - depth_map[valid_pixels]\n\n        return depth_map",
  "def replace_objects(objects_to_be_replaced: List[MeshObject], objects_to_replace_with: List[MeshObject],\n                    ignore_collision_with: Optional[List[MeshObject]] = None, replace_ratio: float = 1,\n                    copy_properties: bool = True, max_tries: int = 100,\n                    relative_pose_sampler: Callable[[MeshObject], None] = None):\n    \"\"\"\n    Replaces mesh objects with another mesh objects and scales them accordingly, the replaced objects and the\n    objects to replace with in following steps:\n\n    1. Randomly select a subset of objects_to_be_replaced.\n    2. For each of these objects, sample other objects from objects_to_replace_with and try to replace them.\n    3. In each try, the poses of the objects are aligned and a check for collisions with other objects is done.\n    4. An object is skipped if max_tries is reached.\n\n    :param objects_to_be_replaced: Objects, which should be removed from the scene.\n    :param objects_to_replace_with: Objects, which will be tried to be added to the scene.\n    :param ignore_collision_with: Objects, which are not checked for collisions with.\n    :param replace_ratio: Ratio of objects in the original scene, which will be replaced.\n    :param copy_properties: Copies the custom properties of the objects_to_be_replaced to the objects_to_replace_with.\n    :param max_tries: Maximum number of tries to replace one object.\n    :param relative_pose_sampler: A function that randomly perturbs the pose of the object to replace with\n                                  (after it has been aligned to the object to replace).\n    \"\"\"\n    if ignore_collision_with is None:\n        ignore_collision_with = []\n\n    # Hide new objects from renderers until they are added\n    for obj in objects_to_replace_with:\n        obj.hide()\n\n    check_collision_with = []\n    for obj in get_all_mesh_objects():\n        if obj not in ignore_collision_with:\n            check_collision_with.append(obj)\n\n    # amount of replacements depends on the amount of objects and the replace ratio\n    objects_to_be_replaced = random.sample(objects_to_be_replaced, k=int(len(objects_to_be_replaced) * replace_ratio))\n    if len(objects_to_be_replaced) == 0:\n        print(\"Warning: The amount of objects, which should be replace is zero!\")\n\n    # Go over all objects we should replace\n    for current_object_to_be_replaced in objects_to_be_replaced:\n        print(current_object_to_be_replaced.get_name())\n        # Do at most max_tries to replace the object with a random object from  objects_to_replace_with\n        tries = 0\n        while tries < max_tries:\n            current_object_to_replace_with = np.random.choice(objects_to_replace_with)\n            if _ObjectReplacer.replace(current_object_to_be_replaced, current_object_to_replace_with,\n                                       check_collision_with, relative_pose_sampler=relative_pose_sampler):\n\n                # Duplicate the added object to be able to add it again\n                duplicate_new_object = current_object_to_replace_with.duplicate()\n\n                # Copy properties to the newly duplicated object\n                if copy_properties:\n                    for key, value in current_object_to_be_replaced.get_all_cps().items():\n                        duplicate_new_object.set_cp(key, value)\n\n                duplicate_new_object.hide(False)\n\n                print('Replaced ', current_object_to_be_replaced.get_name(), ' by ', duplicate_new_object.get_name())\n\n                # Delete the original object and remove it from the list\n                check_collision_with.remove(current_object_to_be_replaced)\n                current_object_to_be_replaced.delete()\n                break\n            tries += 1\n\n        if tries == max_tries:\n            print(\"Could not replace \" + current_object_to_be_replaced.get_name())",
  "class _ObjectReplacer:\n    \"\"\" Replaces mesh objects with another mesh objects and scales them accordingly, the replaced objects and the\n        objects to replace with, can be selected over Selectors (getter.Entity).\n    \"\"\"\n\n    @staticmethod\n    def bb_ratio(bb1: np.ndarray, bb2: np.ndarray) -> list:\n        \"\"\" Rough estimation of the ratios between two bounding boxes sides, not axis aligned\n\n        :param bb1: bounding box 1. Type: float multi-dimensional array of 8 * 3.\n        :param bb2: bounding box 2. Type: float multi-dimensional array of 8 * 3.\n        returns the ratio between each side of the bounding box. Type: a list of floats.\n        \"\"\"\n        ratio_a = (bb1[0, 0] - bb1[4, 0]) / (bb2[0, 0] - bb2[4, 0])\n        ratio_b = (bb1[0, 1] - bb1[3, 1]) / (bb2[0, 1] - bb2[3, 1])\n        ratio_c = (bb1[0, 2] - bb1[1, 2]) / (bb2[0, 2] - bb2[1, 2])\n        return [ratio_a, ratio_b, ratio_c]\n\n    @staticmethod\n    def replace(obj_to_remove: MeshObject, obj_to_add: MeshObject,\n                check_collision_with: Optional[List[MeshObject]] = None, scale: bool = True,\n                relative_pose_sampler: Callable[[MeshObject], None] = None):\n        \"\"\" Scale, translate, rotate obj_to_add to match obj_to_remove and check if there is a bounding box collision\n        returns a boolean.\n\n        :param obj_to_remove: An object to remove from the scene.\n        :param obj_to_add: An object to put in the scene instead of obj_to_remove.\n        :param check_collision_with: A list of objects, which are not checked for collisions with.\n        :param scale: Scales obj_to_add to match obj_to_remove dimensions.\n        :param relative_pose_sampler: A function that randomly perturbs the pose of the object to replace with\n                                      (after it has been aligned to the object to replace).\n        \"\"\"\n        if check_collision_with is None:\n            check_collision_with = []\n        # New object takes location, rotation and rough scale of original object\n        obj_to_add.set_location(obj_to_remove.get_location())\n        obj_to_add.set_rotation_euler(obj_to_remove.get_rotation_euler())\n        if scale:\n            obj_to_add.set_scale(_ObjectReplacer.bb_ratio(obj_to_remove.get_bound_box(True),\n                                                          obj_to_add.get_bound_box(True)))\n        if relative_pose_sampler is not None:\n            relative_pose_sampler(obj_to_add)\n\n        # Check for collision between the new object and other objects in the scene\n        objects_to_check_against = [obj for obj in check_collision_with if obj not in (obj_to_add, obj_to_remove)]\n        return CollisionUtility.check_intersections(obj_to_add, None, objects_to_check_against, [])",
  "def bb_ratio(bb1: np.ndarray, bb2: np.ndarray) -> list:\n        \"\"\" Rough estimation of the ratios between two bounding boxes sides, not axis aligned\n\n        :param bb1: bounding box 1. Type: float multi-dimensional array of 8 * 3.\n        :param bb2: bounding box 2. Type: float multi-dimensional array of 8 * 3.\n        returns the ratio between each side of the bounding box. Type: a list of floats.\n        \"\"\"\n        ratio_a = (bb1[0, 0] - bb1[4, 0]) / (bb2[0, 0] - bb2[4, 0])\n        ratio_b = (bb1[0, 1] - bb1[3, 1]) / (bb2[0, 1] - bb2[3, 1])\n        ratio_c = (bb1[0, 2] - bb1[1, 2]) / (bb2[0, 2] - bb2[1, 2])\n        return [ratio_a, ratio_b, ratio_c]",
  "def replace(obj_to_remove: MeshObject, obj_to_add: MeshObject,\n                check_collision_with: Optional[List[MeshObject]] = None, scale: bool = True,\n                relative_pose_sampler: Callable[[MeshObject], None] = None):\n        \"\"\" Scale, translate, rotate obj_to_add to match obj_to_remove and check if there is a bounding box collision\n        returns a boolean.\n\n        :param obj_to_remove: An object to remove from the scene.\n        :param obj_to_add: An object to put in the scene instead of obj_to_remove.\n        :param check_collision_with: A list of objects, which are not checked for collisions with.\n        :param scale: Scales obj_to_add to match obj_to_remove dimensions.\n        :param relative_pose_sampler: A function that randomly perturbs the pose of the object to replace with\n                                      (after it has been aligned to the object to replace).\n        \"\"\"\n        if check_collision_with is None:\n            check_collision_with = []\n        # New object takes location, rotation and rough scale of original object\n        obj_to_add.set_location(obj_to_remove.get_location())\n        obj_to_add.set_rotation_euler(obj_to_remove.get_rotation_euler())\n        if scale:\n            obj_to_add.set_scale(_ObjectReplacer.bb_ratio(obj_to_remove.get_bound_box(True),\n                                                          obj_to_add.get_bound_box(True)))\n        if relative_pose_sampler is not None:\n            relative_pose_sampler(obj_to_add)\n\n        # Check for collision between the new object and other objects in the scene\n        objects_to_check_against = [obj for obj in check_collision_with if obj not in (obj_to_add, obj_to_remove)]\n        return CollisionUtility.check_intersections(obj_to_add, None, objects_to_check_against, [])",
  "def sample_poses_on_surface(objects_to_sample: List[MeshObject], surface: MeshObject,\n                            sample_pose_func: Callable[[MeshObject], None], max_tries: int = 100,\n                            min_distance: float = 0.25, max_distance: float = 0.6,\n                            up_direction: Optional[np.ndarray] = None,\n                            check_all_bb_corners_over_surface: bool = True) -> List[MeshObject]:\n    \"\"\" Samples objects poses on a surface.\n\n    The objects are positioned slightly above the surface due to the non-axis aligned nature of used bounding boxes\n    and possible non-alignment of the sampling surface (i.e. on the X-Y hyperplane, can be somewhat mitigated with\n    precise \"up_direction\" value), which leads to the objects hovering slightly above the surface. So it is\n    recommended to use the PhysicsPositioning module afterwards for realistically looking placements of objects on\n    the sampling surface. If placing fails due to collisions, the object will be moved back to the intial pose\n    and hidden from rendering.\n\n    :param objects_to_sample: A list of objects that should be sampled above the surface.\n    :param surface: Object to place objects_to_sample on.\n    :param sample_pose_func: The function to use for sampling the pose of a given object.\n    :param max_tries: Amount of tries before giving up on an object (deleting it) and moving to the next one.\n    :param min_distance: Minimum distance to the closest other object from objects_to_sample. Center to center.\n    :param max_distance: Maximum distance to the closest other object from objects_to_sample. Center to center.\n    :param up_direction: Normal vector of the side of surface the objects should be placed on.\n    :param check_all_bb_corners_over_surface: If this is True all bounding box corners have to be above the surface,\n                                              else only the center of the object has to be above the surface\n    :return: The list of placed objects.\n    \"\"\"\n    if up_direction is None:\n        up_direction = np.array([0., 0., 1.])\n    else:\n        up_direction /= np.linalg.norm(up_direction)\n\n    surface_bounds = surface.get_bound_box()\n    surface_height = max(up_direction.dot(corner) for corner in surface_bounds)\n\n    # cache to fasten collision detection\n    bvh_cache: Dict[str, mathutils.bvhtree.BVHTree] = {}\n\n    placed_objects: List[MeshObject] = []\n    for obj in objects_to_sample:\n        print(f\"Trying to put {obj.get_name()}\")\n        initial_pose = obj.get_local2world_mat()\n        placed_successfully = False\n\n        for i in range(max_tries):\n            sample_pose_func(obj)\n            # Remove bvh cache, as object has changed\n            if obj.get_name() in bvh_cache:\n                del bvh_cache[obj.get_name()]\n\n            if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):\n                print(\"Collision detected, retrying!\")\n                continue\n\n            if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):\n                print(\"Not above surface, retrying!\")\n                continue\n\n            _OnSurfaceSampler.drop(obj, up_direction, surface_height)\n            # Remove bvh cache, as object has changed\n            if obj.get_name() in bvh_cache:\n                del bvh_cache[obj.get_name()]\n\n            if not _OnSurfaceSampler.check_above_surface(obj, surface, up_direction, check_all_bb_corners_over_surface):\n                print(\"Not above surface after drop, retrying!\")\n                continue\n\n            if not _OnSurfaceSampler.check_spacing(obj, placed_objects, min_distance, max_distance):\n                print(\"Bad spacing after drop, retrying!\")\n                continue\n\n            if not CollisionUtility.check_intersections(obj, bvh_cache, placed_objects, []):\n                print(\"Collision detected after drop, retrying!\")\n                continue\n\n            print(f\"Placed object \\\"{obj.get_name()}\\\" successfully at {obj.get_location()} after {i + 1} iterations!\")\n            placed_objects.append(obj)\n\n            placed_successfully = True\n            break\n\n        if not placed_successfully:\n            print(f\"Giving up on {obj.get_name()}, hiding...\")\n            obj.hide(True)\n            obj.set_local2world_mat(initial_pose)\n\n    return placed_objects",
  "class _OnSurfaceSampler:\n\n    @staticmethod\n    def check_above_surface(obj: MeshObject, surface: MeshObject, up_direction: np.ndarray,\n                            check_all_bb_corners_over_surface: bool = True) -> bool:\n        \"\"\" Check if all corners of the bounding box are \"above\" the surface\n\n        :param obj: Object for which the check is carried out. Type: blender object.\n        :param surface: The surface object.\n        :param up_direction: The direction that indicates \"above\" direction.\n        :param check_all_bb_corners_over_surface: If this is True all bounding box corners have to be above the surface,\n                                                  else only the center of the object has to be above the surface\n        :return: True if the bounding box is above the surface, False - if not.\n        \"\"\"\n        if check_all_bb_corners_over_surface:\n            for point in obj.get_bound_box():\n                if not surface.position_is_above_object(point + up_direction, -up_direction,\n                                                        check_no_objects_in_between=False):\n                    return False\n            return True\n        center = np.mean(obj.get_bound_box(), axis=0)\n        return surface.position_is_above_object(center + up_direction, -up_direction, check_no_objects_in_between=False)\n\n    @staticmethod\n    def check_spacing(obj: MeshObject, placed_objects: List[MeshObject], min_distance: float, max_distance: float) \\\n            -> bool:\n        \"\"\" Check if object is not too close or too far from previous objects.\n\n        :param obj: Object for which the check is carried out.\n        :param placed_objects: A list of already placed objects that should be used for checking spacing.\n        :param min_distance: Minimum distance to the closest other object from placed_objects. Center to center.\n        :param max_distance: Maximum distance to the closest other object from placed_objects. Center to center.\n        :return: True, if the spacing is correct\n        \"\"\"\n        closest_distance = None\n\n        for already_placed in placed_objects:\n            distance = np.linalg.norm(already_placed.get_location() - obj.get_location())\n            if closest_distance is None or distance < closest_distance:\n                closest_distance = distance\n\n        return closest_distance is None or (min_distance <= closest_distance <= max_distance)\n\n    @staticmethod\n    def drop(obj: MeshObject, up_direction: np.ndarray, surface_height: float):\n        \"\"\" Moves object \"down\" until its bounding box touches the bounding box of the surface. This uses bounding boxes\n            which are not aligned optimally, this will cause objects to be placed slightly to high.\n\n        :param obj: Object to move. Type: blender object.\n        :param up_direction: Vector which points into the opposite drop direction.\n        :param surface_height: Height of the surface above its origin.\n        \"\"\"\n        obj_bounds = obj.get_bound_box()\n        obj_height = min(up_direction.dot(corner) for corner in obj_bounds)\n\n        obj.set_location(obj.get_location() - up_direction * (obj_height - surface_height))",
  "def check_above_surface(obj: MeshObject, surface: MeshObject, up_direction: np.ndarray,\n                            check_all_bb_corners_over_surface: bool = True) -> bool:\n        \"\"\" Check if all corners of the bounding box are \"above\" the surface\n\n        :param obj: Object for which the check is carried out. Type: blender object.\n        :param surface: The surface object.\n        :param up_direction: The direction that indicates \"above\" direction.\n        :param check_all_bb_corners_over_surface: If this is True all bounding box corners have to be above the surface,\n                                                  else only the center of the object has to be above the surface\n        :return: True if the bounding box is above the surface, False - if not.\n        \"\"\"\n        if check_all_bb_corners_over_surface:\n            for point in obj.get_bound_box():\n                if not surface.position_is_above_object(point + up_direction, -up_direction,\n                                                        check_no_objects_in_between=False):\n                    return False\n            return True\n        center = np.mean(obj.get_bound_box(), axis=0)\n        return surface.position_is_above_object(center + up_direction, -up_direction, check_no_objects_in_between=False)",
  "def check_spacing(obj: MeshObject, placed_objects: List[MeshObject], min_distance: float, max_distance: float) \\\n            -> bool:\n        \"\"\" Check if object is not too close or too far from previous objects.\n\n        :param obj: Object for which the check is carried out.\n        :param placed_objects: A list of already placed objects that should be used for checking spacing.\n        :param min_distance: Minimum distance to the closest other object from placed_objects. Center to center.\n        :param max_distance: Maximum distance to the closest other object from placed_objects. Center to center.\n        :return: True, if the spacing is correct\n        \"\"\"\n        closest_distance = None\n\n        for already_placed in placed_objects:\n            distance = np.linalg.norm(already_placed.get_location() - obj.get_location())\n            if closest_distance is None or distance < closest_distance:\n                closest_distance = distance\n\n        return closest_distance is None or (min_distance <= closest_distance <= max_distance)",
  "def drop(obj: MeshObject, up_direction: np.ndarray, surface_height: float):\n        \"\"\" Moves object \"down\" until its bounding box touches the bounding box of the surface. This uses bounding boxes\n            which are not aligned optimally, this will cause objects to be placed slightly to high.\n\n        :param obj: Object to move. Type: blender object.\n        :param up_direction: Vector which points into the opposite drop direction.\n        :param surface_height: Height of the surface above its origin.\n        \"\"\"\n        obj_bounds = obj.get_bound_box()\n        obj_height = min(up_direction.dot(corner) for corner in obj_bounds)\n\n        obj.set_location(obj.get_location() - up_direction * (obj_height - surface_height))",
  "def slice_faces_with_normals(mesh_object: MeshObject, compare_angle_degrees: float = 7.5,\n                             up_vector_upwards: Optional[np.array] = None,\n                             new_name_for_object: str = \"Surface\") -> Optional[MeshObject]:\n    \"\"\" Extracts normal faces like floors in the following steps:\n    1. Searchs for the specified object.\n    2. Splits the surfaces which point upwards at a specified level away.\n\n    :param mesh_object: Object to which all polygons will be extracted.\n    :param compare_angle_degrees: Maximum difference between the up vector and the current polygon normal in degrees.\n    :param up_vector_upwards: If this is True the `up_vec` points upwards -> [0, 0, 1] if not it points downwards:\n                              [0, 0, -1] in world coordinates. This vector is used for the\n                              `compare_angle_degrees` option.\n    :param new_name_for_object: Name for the newly created object, which faces fulfill the given parameters.\n\n    :return: The extracted surface of the object.\n    \"\"\"\n    # set the up_vector\n    if up_vector_upwards is None:\n        up_vector_upwards = np.array([0.0, 0.0, 1.0])\n\n    # the up vector has to have unit length\n    up_vector_upwards /= np.linalg.norm(up_vector_upwards)\n\n    mesh_object.edit_mode()\n    bm = mesh_object.mesh_as_bmesh()\n    bpy.ops.mesh.select_all(action='DESELECT')\n\n    list_of_median_poses: List[Tuple[float, bmesh.types.BMFace]] = [\n        (FaceSlicer.get_median_face_pose(f, mesh_object.get_local2world_mat())[2], f) for f in\n        bm.faces if FaceSlicer.check_face_angle(f, mesh_object.get_local2world_mat(), up_vector_upwards,\n                                                np.deg2rad(compare_angle_degrees))]\n\n    list_of_median_poses_only_z_value = [value for value, face in list_of_median_poses]\n\n    bandwidth_in_meter = 0.005\n    ms = MeanShift(bandwidth=bandwidth_in_meter, bin_seeding=True)\n    ms.fit(np.array(list_of_median_poses_only_z_value).reshape((-1, 1)))\n\n    all_labels = {}\n    for l in np.unique(ms.labels_):\n        all_labels[l] = 0.0\n\n    faces = [face for value, face in list_of_median_poses]\n    for label, face in zip(ms.labels_, faces):\n        all_labels[label] += face.calc_area()\n    max_label = max(all_labels, key=all_labels.get)\n\n    bpy.ops.mesh.select_all(action='DESELECT')\n    for f, label in zip(faces, ms.labels_):\n        if label == max_label:\n            f.select = True\n    bpy.ops.mesh.separate(type='SELECTED')\n\n    selected_objects = bpy.context.selected_objects\n    if selected_objects:\n        if len(selected_objects) == 2:\n            selected_objects = [o for o in selected_objects\n                                if o != bpy.context.view_layer.objects.active]\n            selected_objects[0].name = new_name_for_object\n            newly_created_object = MeshObject(selected_objects[0])\n        else:\n            raise RuntimeError(\"There is more than one selection after splitting, this should not happen!\")\n    else:\n        raise RuntimeError(\"No surface object was constructed!\")\n\n    mesh_object.object_mode()\n\n    return newly_created_object",
  "def extract_floor(mesh_objects: List[MeshObject], compare_angle_degrees: float = 7.5, compare_height: float = 0.15,\n                  up_vector_upwards: bool = True, height_list_path: str = None, new_name_for_object: str = \"Floor\",\n                  should_skip_if_object_is_already_there: bool = False) -> List[MeshObject]:\n    \"\"\" Extracts floors in the following steps:\n    1. Searchs for the specified object.\n    2. Splits the surfaces which point upwards at a specified level away.\n\n    :param mesh_objects: Objects to where all polygons will be extracted.\n    :param compare_angle_degrees: Maximum difference between the up vector and the current polygon normal in degrees.\n    :param compare_height: Maximum difference in Z direction between the polygons median point and the specified\n                           height of the room.\n    :param up_vector_upwards: If this is True the `up_vec` points upwards -> [0, 0, 1] if not it points\n                              downwards: [0, 0, -1] in world coordinates. This vector is used for the\n                              `compare_angle_degrees` option.\n    :param height_list_path: Path to a file with height values. If none is provided, a ceiling and floor is\n                             automatically detected. This might fail. The height_list_values can be specified in a\n                             list like fashion in the file: [0.0, 2.0]. These values are in the same size the dataset\n                             is in, which is usually meters. The content must always be a list, e.g. [0.0].\n    :param new_name_for_object: Name for the newly created object, which faces fulfill the given parameters.\n    :param should_skip_if_object_is_already_there: If this is true no extraction will be done, if an object is there,\n                                                   which has the same name as name_for_split_obj, which would be used\n                                                   for the newly created object.\n    :return: The extracted floor objects.\n    \"\"\"\n    # set the up_vector\n    up_vec = mathutils.Vector([0, 0, 1])\n    if not up_vector_upwards:\n        up_vec *= -1.0\n\n    height_list = []\n    if height_list_path is not None:\n        height_file_path = resolve_path(height_list_path)\n        with open(height_file_path, \"r\", encoding=\"utf-8\") as file:\n            height_list = [float(val) for val in ast.literal_eval(file.read())]\n\n    object_names = [obj.name for obj in bpy.context.scene.objects if obj.type == \"MESH\"]\n\n    def clean_up_name(name: str):\n        \"\"\"\n        Clean up the given name from Floor1 to floor\n\n        :param name: given name\n        :return: str: cleaned up name\n        \"\"\"\n        name = ''.join([i for i in name if not i.isdigit()])  # remove digits\n        name = name.lower().replace(\".\", \"\").strip()  # remove dots and whitespace\n        return name\n\n    object_names = [clean_up_name(name) for name in object_names]\n    if should_skip_if_object_is_already_there and new_name_for_object.lower() in object_names:\n        # if should_skip is True and if there is an object, which name is the same as the one for the newly\n        # split object, than the execution is skipped\n        return []\n\n    newly_created_objects = []\n    for obj in mesh_objects:\n        obj.edit_mode()\n        bm = obj.mesh_as_bmesh()\n        # this makes sure all normals are calculated\n        bm.normal_update()\n        bpy.ops.mesh.select_all(action='DESELECT')\n\n        if height_list:\n            counter = 0\n            for height_val in height_list:\n                counter = FaceSlicer.select_at_height_value(bm, height_val, compare_height, up_vec,\n                                                            np.deg2rad(compare_angle_degrees),\n                                                            obj.get_local2world_mat())\n\n            if counter:\n                obj.update_from_bmesh(bm)\n                bpy.ops.mesh.separate(type='SELECTED')\n        else:\n            # no height list was provided, try to estimate them on its own\n\n            # first get a list of all height values of the median points, which are inside of the defined\n            # compare angle range\n            list_of_median_poses: Union[List[float], np.ndarray] = [\n                FaceSlicer.get_median_face_pose(f, obj.get_local2world_mat())[2] for f in\n                bm.faces if\n                FaceSlicer.check_face_angle(f, obj.get_local2world_mat(), up_vec,\n                                            np.deg2rad(compare_angle_degrees))]\n            if not list_of_median_poses:\n                print(f\"Object with name: {obj.get_name()} is skipped no faces were relevant, try with \"\n                      f\"flipped up_vec\")\n                list_of_median_poses = [FaceSlicer.get_median_face_pose(f, obj.get_local2world_mat())[2] for f in\n                                        bm.faces if FaceSlicer.check_face_angle(f, obj.get_local2world_mat(),\n                                                                                -up_vec,\n                                                                                np.deg2rad(compare_angle_degrees))]\n                if not list_of_median_poses:\n                    print(f\"Still no success for: {obj.get_name()} skip object.\")\n                    bpy.ops.object.mode_set(mode='OBJECT')\n                    bpy.ops.object.select_all(action='DESELECT')\n                    continue\n\n                successful_up_vec = -up_vec\n            else:\n                successful_up_vec = up_vec\n\n            list_of_median_poses = np.reshape(list_of_median_poses, (-1, 1))\n            if np.var(list_of_median_poses) < 1e-4:\n                # All faces are already correct\n                height_value = np.mean(list_of_median_poses)\n            else:\n                ms = MeanShift(bandwidth=0.2, bin_seeding=True)\n                ms.fit(list_of_median_poses)\n\n                # if the up vector is negative the maximum value is searched\n                if up_vector_upwards:\n                    height_value = np.min(ms.cluster_centers_)\n                else:\n                    height_value = np.max(ms.cluster_centers_)\n\n            counter = FaceSlicer.select_at_height_value(bm, height_value, compare_height, successful_up_vec,\n                                                        np.deg2rad(compare_angle_degrees), obj.get_local2world_mat())\n\n            if counter:\n                obj.update_from_bmesh(bm)\n                bpy.ops.mesh.separate(type='SELECTED')\n            selected_objects = bpy.context.selected_objects\n            if selected_objects:\n                if len(selected_objects) == 2:\n                    selected_objects = [o for o in selected_objects\n                                        if o != bpy.context.view_layer.objects.active]\n                    selected_objects[0].name = new_name_for_object\n                    newly_created_objects.append(MeshObject(selected_objects[0]))\n                else:\n                    raise RuntimeError(\"There is more than one selection after splitting, this should not happen!\")\n            else:\n                raise RuntimeError(\"No floor object was constructed!\")\n\n        obj.object_mode()\n\n    return newly_created_objects",
  "class FaceSlicer:\n    \"\"\"\n    Slicing the faces from an object away.\n    \"\"\"\n\n    @staticmethod\n    def select_at_height_value(bm: bmesh.types.BMesh, height_value: float, compare_height: float,\n                               up_vector: Union[mathutils.Vector, np.ndarray], cmp_angle: float,\n                               matrix_world: Union[mathutils.Matrix, np.ndarray]) -> int:\n        \"\"\"\n        Selects for a given `height_value` all faces, which are inside the given `compare_height` band and also face\n        upwards. This is done by comparing the face.normal in world coordinates to the `up_vector` and the resulting\n        angle must be smaller than `compare_angle`.\n\n        :param bm: The object as BMesh in edit mode. The face should be structured, meaning a lookup was performed on \\\n                   them before.\n        :param height_value: Height value which is used for comparing the faces median point against\n        :param compare_height: Defines the range in which the face median is compared to the height value.\n        :param up_vector: Vector, which is used for comparing the face.normal against\n        :param cmp_angle: Angle, which is used to compare against the up_vec in radians.\n        :param matrix_world: The matrix_world of the object, to which the face belongs\n        \"\"\"\n        # deselect all faces\n        counter = 0\n        for f in bm.faces:\n            if FaceSlicer.check_face_with(f, matrix_world, height_value, compare_height, up_vector, cmp_angle):\n                counter += 1\n                f.select = True\n        print(f\"Selected {counter} polygons as floor\")\n        return counter\n\n    @staticmethod\n    def get_median_face_pose(face: bmesh.types.BMFace,\n                             matrix_world: Union[mathutils.Matrix, np.ndarray]) -> mathutils.Vector:\n        \"\"\"\n        Returns the median face pose of all its vertices in the world coordinate frame.\n\n        :param face: Current selected frame, its vertices are used to calculate the median\n        :param matrix_world: The matrix of the current object to which this face belongs\n        :return: mathutils.Vector(): The current median point of the vertices in world coordinates\n        \"\"\"\n        # calculate the median position of the current face\n        median_pose = face.calc_center_median().to_4d()\n        median_pose[3] = 1.0\n        median_pose = mathutils.Matrix(matrix_world) @ median_pose\n        return median_pose\n\n    @staticmethod\n    def check_face_angle(face: bmesh.types.BMFace, matrix_world: Union[mathutils.Matrix, np.ndarray],\n                         up_vector: Union[mathutils.Vector, np.ndarray], cmp_angle: float) -> bool:\n        \"\"\"\n        Checks if a face.normal in world coordinates angular difference to the `up_vec` is closer as\n        `cmp_anlge`.\n\n        :param face: The face, which will be checked\n        :param matrix_world: The matrix_world of the object, to which the face belongs\n        :param up_vector: Vector, which is used for comparing the face.normal against\n        :param cmp_angle: Angle, which is used to compare against the up_vec in radians.\n        :return: bool: Returns true if the face is close the height_value and is inside of the cmp_angle range\n        \"\"\"\n        # if the face has no surface the face angle is always bigger than the compare angle\n        if face.calc_area() == 0.0:\n            return False\n        # calculate the normal\n        normal_face = face.normal.to_4d()\n        normal_face[3] = 0.0\n        normal_face = (mathutils.Matrix(matrix_world) @ normal_face).to_3d()\n        normal_face_length = np.linalg.norm(normal_face)\n        if normal_face_length < 1e-7:\n            return False\n        normal_face /= np.linalg.norm(normal_face)\n        # compare the normal to the current up_vec\n        return acos(normal_face @ mathutils.Vector(up_vector)) < cmp_angle\n\n    @staticmethod\n    def check_face_with(face: bmesh.types.BMFace, matrix_world: Union[mathutils.Matrix, np.ndarray],\n                        height_value: float,\n                        cmp_height: float, up_vector: Union[mathutils.Vector, np.ndarray], cmp_angle: float) -> bool:\n        \"\"\"\n        Check if the face is on a certain `height_value` by checking if it is inside of the band spanned by\n        `cmp_height` -> [`height_value` - `cmp_height`, `height_value` + `cmp_height`] and then if the face\n        has a similar angle to the given `up_vec`, the difference must be smaller than `cmp_angle`.\n\n        :param face: The face, which will be checked\n        :param matrix_world: The matrix_world of the object, to which the face belongs\n        :param height_value: Height value which is used for comparing the faces median point against\n        :param cmp_height: Defines the range in which the face median is compared to the height value.\n        :param up_vector: Vector, which is used for comparing the face.normal against\n        :param cmp_angle: Angle, which is used to compare against the up_vec in radians.\n        :return: bool: Returns true if the face is close the height_value and is inside of the cmp_angle range\n        \"\"\"\n        median_pose = FaceSlicer.get_median_face_pose(face, matrix_world)\n\n        # compare that pose to the current height_band\n        if fabs(median_pose[2] - height_value) < cmp_height:\n            return FaceSlicer.check_face_angle(face, matrix_world, up_vector, cmp_angle)\n        return False",
  "def clean_up_name(name: str):\n        \"\"\"\n        Clean up the given name from Floor1 to floor\n\n        :param name: given name\n        :return: str: cleaned up name\n        \"\"\"\n        name = ''.join([i for i in name if not i.isdigit()])  # remove digits\n        name = name.lower().replace(\".\", \"\").strip()  # remove dots and whitespace\n        return name",
  "def select_at_height_value(bm: bmesh.types.BMesh, height_value: float, compare_height: float,\n                               up_vector: Union[mathutils.Vector, np.ndarray], cmp_angle: float,\n                               matrix_world: Union[mathutils.Matrix, np.ndarray]) -> int:\n        \"\"\"\n        Selects for a given `height_value` all faces, which are inside the given `compare_height` band and also face\n        upwards. This is done by comparing the face.normal in world coordinates to the `up_vector` and the resulting\n        angle must be smaller than `compare_angle`.\n\n        :param bm: The object as BMesh in edit mode. The face should be structured, meaning a lookup was performed on \\\n                   them before.\n        :param height_value: Height value which is used for comparing the faces median point against\n        :param compare_height: Defines the range in which the face median is compared to the height value.\n        :param up_vector: Vector, which is used for comparing the face.normal against\n        :param cmp_angle: Angle, which is used to compare against the up_vec in radians.\n        :param matrix_world: The matrix_world of the object, to which the face belongs\n        \"\"\"\n        # deselect all faces\n        counter = 0\n        for f in bm.faces:\n            if FaceSlicer.check_face_with(f, matrix_world, height_value, compare_height, up_vector, cmp_angle):\n                counter += 1\n                f.select = True\n        print(f\"Selected {counter} polygons as floor\")\n        return counter",
  "def get_median_face_pose(face: bmesh.types.BMFace,\n                             matrix_world: Union[mathutils.Matrix, np.ndarray]) -> mathutils.Vector:\n        \"\"\"\n        Returns the median face pose of all its vertices in the world coordinate frame.\n\n        :param face: Current selected frame, its vertices are used to calculate the median\n        :param matrix_world: The matrix of the current object to which this face belongs\n        :return: mathutils.Vector(): The current median point of the vertices in world coordinates\n        \"\"\"\n        # calculate the median position of the current face\n        median_pose = face.calc_center_median().to_4d()\n        median_pose[3] = 1.0\n        median_pose = mathutils.Matrix(matrix_world) @ median_pose\n        return median_pose",
  "def check_face_angle(face: bmesh.types.BMFace, matrix_world: Union[mathutils.Matrix, np.ndarray],\n                         up_vector: Union[mathutils.Vector, np.ndarray], cmp_angle: float) -> bool:\n        \"\"\"\n        Checks if a face.normal in world coordinates angular difference to the `up_vec` is closer as\n        `cmp_anlge`.\n\n        :param face: The face, which will be checked\n        :param matrix_world: The matrix_world of the object, to which the face belongs\n        :param up_vector: Vector, which is used for comparing the face.normal against\n        :param cmp_angle: Angle, which is used to compare against the up_vec in radians.\n        :return: bool: Returns true if the face is close the height_value and is inside of the cmp_angle range\n        \"\"\"\n        # if the face has no surface the face angle is always bigger than the compare angle\n        if face.calc_area() == 0.0:\n            return False\n        # calculate the normal\n        normal_face = face.normal.to_4d()\n        normal_face[3] = 0.0\n        normal_face = (mathutils.Matrix(matrix_world) @ normal_face).to_3d()\n        normal_face_length = np.linalg.norm(normal_face)\n        if normal_face_length < 1e-7:\n            return False\n        normal_face /= np.linalg.norm(normal_face)\n        # compare the normal to the current up_vec\n        return acos(normal_face @ mathutils.Vector(up_vector)) < cmp_angle",
  "def check_face_with(face: bmesh.types.BMFace, matrix_world: Union[mathutils.Matrix, np.ndarray],\n                        height_value: float,\n                        cmp_height: float, up_vector: Union[mathutils.Vector, np.ndarray], cmp_angle: float) -> bool:\n        \"\"\"\n        Check if the face is on a certain `height_value` by checking if it is inside of the band spanned by\n        `cmp_height` -> [`height_value` - `cmp_height`, `height_value` + `cmp_height`] and then if the face\n        has a similar angle to the given `up_vec`, the difference must be smaller than `cmp_angle`.\n\n        :param face: The face, which will be checked\n        :param matrix_world: The matrix_world of the object, to which the face belongs\n        :param height_value: Height value which is used for comparing the faces median point against\n        :param cmp_height: Defines the range in which the face median is compared to the height value.\n        :param up_vector: Vector, which is used for comparing the face.normal against\n        :param cmp_angle: Angle, which is used to compare against the up_vec in radians.\n        :return: bool: Returns true if the face is close the height_value and is inside of the cmp_angle range\n        \"\"\"\n        median_pose = FaceSlicer.get_median_face_pose(face, matrix_world)\n\n        # compare that pose to the current height_band\n        if fabs(median_pose[2] - height_value) < cmp_height:\n            return FaceSlicer.check_face_angle(face, matrix_world, up_vector, cmp_angle)\n        return False",
  "def merge_objects(objects: List[Entity], merged_object_name: str = 'merged_object') -> Entity:\n    \"\"\" Generates an empty object and sets this as parent object for all objects in the list which do not already\n        have a parent set.\n\n    :param objects: A list of objects to be merged.\n    :param merged_object_name: The name of the parent object.\n    \"\"\"\n    assert merged_object_name != \"\", \"Parent object name cannot be empty!\"\n    print('name', merged_object_name)\n\n    # create new empty object which acts as parent, and link it to the collection\n    parent_obj = create_empty(merged_object_name)\n\n    # select all relevant objects\n    for obj in objects:\n        # objects with a parent will be skipped, as this relationship will otherwise be broken\n        # if a parent exists this object should be grandchild of parent_obj (or grandgrand...)\n        if obj.get_parent() is not None:\n            continue\n        # if the object doesn't have a parent we can set its parent\n        obj.set_parent(parent_obj)\n\n    return parent_obj",
  "def sample_poses(objects_to_sample: List[MeshObject], sample_pose_func: Callable[[MeshObject], None],\n                 objects_to_check_collisions: List[MeshObject] = None, max_tries: int = 1000,\n                 mode_on_failure: str = \"last_pose\") -> Dict[Entity, Tuple[int, bool]]:\n    \"\"\"\n    Samples positions and rotations of selected object inside the sampling volume while performing mesh and\n    bounding box collision checks.\n\n\n    :param objects_to_sample: A list of mesh objects whose poses are sampled based on the given function.\n    :param sample_pose_func: The function to use for sampling the pose of a given object.\n    :param objects_to_check_collisions: A list of mesh objects who should not be considered when checking for\n                                        collisions.\n    :param max_tries: Amount of tries before giving up on an object and moving to the next one.\n    :param mode_on_failure: Define final state of objects that could not be placed without collisions within max_tries\n                            attempts. Options: 'last_pose', 'initial_pose'\n\n    :return: A dict with the objects to sample as keys and a Tuple with the number of executed attempts to place the\n             object as first element, and a bool whether it has been successfully placed without collisions.\n    \"\"\"\n    # Check if mode on failure is allowed\n    allowed_modes_on_failure = [\"last_pose\", \"initial_pose\"]\n    if mode_on_failure not in allowed_modes_on_failure:\n        raise ValueError(f\"{mode_on_failure} is not an allowed mode_on_failure.\")\n\n    # After this many tries we give up on current object and continue with the rest\n    if objects_to_check_collisions is None:\n        objects_to_check_collisions = get_all_mesh_objects()\n\n    # Among objects_to_sample only check collisions against already placed objects\n    cur_objects_to_check_collisions = list(set(objects_to_check_collisions) - set(objects_to_sample))\n\n    if max_tries <= 0:\n        raise ValueError(f\"The value of max_tries must be greater than zero: {max_tries}\")\n\n    if not objects_to_sample:\n        raise RuntimeError(\"The list of objects_to_sample can not be empty!\")\n\n    # cache to fasten collision detection\n    bvh_cache: Dict[str, mathutils.bvhtree.BVHTree] = {}\n\n    sample_results: Dict[Entity, Tuple[int, bool]] = {}\n\n    # for every selected object\n    for obj in objects_to_sample:\n\n        # Store the obejct's initial pose in case we need to place it back\n        if mode_on_failure == 'initial_pose':\n            initial_location = obj.get_location()\n            initial_rotation = obj.get_rotation_euler()\n\n        no_collision = True\n\n        amount_of_tries_done = -1\n\n        # Try max_iter amount of times\n        for i in range(max_tries):\n\n            # Put the top object in queue at the sampled point in space\n            sample_pose_func(obj)\n\n            # Remove bvh cache, as object has changed\n            if obj.get_name() in bvh_cache:\n                del bvh_cache[obj.get_name()]\n\n            no_collision = CollisionUtility.check_intersections(obj, bvh_cache, cur_objects_to_check_collisions, [])\n\n            # If no collision then keep the position\n            if no_collision:\n                amount_of_tries_done = i\n                break\n\n        # After placing an object, we will check collisions with it\n        cur_objects_to_check_collisions.append(obj)\n\n        if no_collision:\n            print(f\"It took {amount_of_tries_done + 1} tries to place {obj.get_name()}\")\n        else:\n            amount_of_tries_done = max_tries\n            print(f\"Could not place {obj.get_name()} without a collision.\")\n\n            if mode_on_failure == 'initial_pose':\n                obj.set_location(initial_location)\n                obj.set_rotation_euler(initial_rotation)\n\n        sample_results[obj] = (amount_of_tries_done, no_collision)\n\n    return sample_results",
  "def simulate_physics_and_fix_final_poses(min_simulation_time: float = 4.0, max_simulation_time: float = 40.0,\n                                         check_object_interval: float = 2.0,\n                                         object_stopped_location_threshold: float = 0.01,\n                                         object_stopped_rotation_threshold: float = 0.1, substeps_per_frame: int = 10,\n                                         solver_iters: int = 10, verbose: bool = False):\n    \"\"\" Simulates the current scene and in the end fixes the final poses of all active objects.\n\n    The simulation is run for at least `min_simulation_time` seconds and at a maximum `max_simulation_time` seconds.\n    Every `check_object_interval` seconds, it is checked if the maximum object movement in the last second is below a\n    given threshold. If that is the case, the simulation is stopped.\n\n    After performing the simulation, the simulation cache is removed, the rigid body components are disabled and the\n    pose of the active objects is set to their final pose in the simulation.\n\n    :param min_simulation_time: The minimum number of seconds to simulate.\n    :param max_simulation_time: The maximum number of seconds to simulate.\n    :param check_object_interval: The interval in seconds at which all objects should be checked if they are still\n                                  moving. If all objects have stopped moving, then the simulation will be stopped.\n    :param object_stopped_location_threshold: The maximum difference per second and per coordinate in the rotation\n                                              Euler vector that is allowed such that an object is still recognized\n                                              as 'stopped moving'.\n    :param object_stopped_rotation_threshold: The maximum difference per second and per coordinate in the rotation\n                                              Euler vector that is allowed such that an object is still recognized\n                                              as 'stopped moving'.\n    :param substeps_per_frame: Number of simulation steps taken per frame.\n    :param solver_iters: Number of constraint solver iterations made per simulation step.\n    :param verbose: If True, more details during the physics simulation are printed.\n    \"\"\"\n    # Undo changes made in the simulation like origin adjustment and persisting the object's scale\n    with UndoAfterExecution():\n        # Run simulation and remember poses before and after\n        obj_poses_before_sim = _PhysicsSimulation.get_pose()\n        origin_shifts = simulate_physics(min_simulation_time, max_simulation_time, check_object_interval,\n                                         object_stopped_location_threshold, object_stopped_rotation_threshold,\n                                         substeps_per_frame, solver_iters, verbose)\n        obj_poses_after_sim = _PhysicsSimulation.get_pose()\n\n        # Make sure to remove the simulation cache as we are only interested in the final poses\n        bpy.ops.ptcache.free_bake({\"point_cache\": bpy.context.scene.rigidbody_world.point_cache})\n\n    # Fix the pose of all objects to their pose at the end of the simulation (also revert origin shift)\n    for obj in get_all_mesh_objects():\n        if obj.has_rigidbody_enabled():\n            # Skip objects that have parents with compound rigid body component\n            has_compound_parent = obj.get_parent() is not None and isinstance(obj.get_parent(), MeshObject) \\\n                                  and obj.get_parent().get_rigidbody() is not None \\\n                                  and obj.get_parent().get_rigidbody().collision_shape == \"COMPOUND\"\n            if obj.get_rigidbody().type == \"ACTIVE\" and not has_compound_parent:\n                # compute relative object rotation before and after simulation\n                R_obj_before_sim = mathutils.Euler(obj_poses_before_sim[obj.get_name()]['rotation']).to_matrix()\n                R_obj_after = mathutils.Euler(obj_poses_after_sim[obj.get_name()]['rotation']).to_matrix()\n                R_obj_rel = R_obj_before_sim @ R_obj_after.transposed()\n                # Apply relative rotation to origin shift\n                origin_shift = R_obj_rel.transposed() @ mathutils.Vector(origin_shifts[obj.get_name()])\n\n                # Fix pose of object to the one it had at the end of the simulation\n                obj.set_location(obj_poses_after_sim[obj.get_name()]['location'] - origin_shift)\n                obj.set_rotation_euler(obj_poses_after_sim[obj.get_name()]['rotation'])\n\n    # Deactivate the simulation so it does not influence object positions\n    bpy.context.scene.rigidbody_world.enabled = False\n    bpy.context.view_layer.update()",
  "def simulate_physics(min_simulation_time: float = 4.0, max_simulation_time: float = 40.0,\n                     check_object_interval: float = 2.0, object_stopped_location_threshold: float = 0.01,\n                     object_stopped_rotation_threshold: float = 0.1, substeps_per_frame: int = 10,\n                     solver_iters: int = 10, verbose: bool = False) -> dict:\n    \"\"\" Simulates the current scene.\n\n    The simulation is run for at least `min_simulation_time` seconds and at a maximum `max_simulation_time` seconds.\n    Every `check_object_interval` seconds, it is checked if the maximum object movement in the last second is below\n    a given threshold. If that is the case, the simulation is stopped.\n\n    The origin of all objects is set to their center of mass in this function which is necessary to achieve a realistic\n    simulation in blender (see https://blender.stackexchange.com/questions/167488/physics-not-working-as-expected)\n    Also the scale of each participating object is persisted as scale != 1 can make the simulation unstable.\n\n    :param min_simulation_time: The minimum number of seconds to simulate.\n    :param max_simulation_time: The maximum number of seconds to simulate.\n    :param check_object_interval: The interval in seconds at which all objects should be checked if they are still\n                                  moving. If all objects have stopped moving, then the simulation will be stopped.\n    :param object_stopped_location_threshold: The maximum difference per second and per coordinate in the rotation\n                                              Euler vector that is allowed such that an object is still recognized\n                                              as 'stopped moving'.\n    :param object_stopped_rotation_threshold: The maximum difference per second and per coordinate in the rotation\n                                              Euler vector that is allowed such that an object is still recognized\n                                              as 'stopped moving'.\n    :param substeps_per_frame: Number of simulation steps taken per frame.\n    :param solver_iters: Number of constraint solver iterations made per simulation step.\n    :param verbose: If True, more details during the physics simulation are printed.\n    :return: A dict containing for every active object the shift that was added to their origins.\n    \"\"\"\n    # Shift the origin of all objects to their center of mass to make the simulation more realistic\n    origin_shift = {}\n    for obj in get_all_mesh_objects():\n        if obj.has_rigidbody_enabled():\n            prev_origin = obj.get_origin()\n            new_origin = obj.set_origin(mode=\"CENTER_OF_VOLUME\")\n            origin_shift[obj.get_name()] = new_origin - prev_origin\n\n            # Persist mesh scaling as having a scale != 1 can make the simulation unstable\n            obj.persist_transformation_into_mesh(location=False, rotation=False, scale=True)\n\n    # Configure simulator\n    bpy.context.scene.rigidbody_world.substeps_per_frame = substeps_per_frame\n    bpy.context.scene.rigidbody_world.solver_iterations = solver_iters\n\n    # Perform simulation\n    _PhysicsSimulation.do_simulation(min_simulation_time, max_simulation_time, check_object_interval,\n                                     object_stopped_location_threshold, object_stopped_rotation_threshold,\n                                     verbose)\n\n    return origin_shift",
  "class _PhysicsSimulation:\n\n    @staticmethod\n    def seconds_to_frames(seconds: float) -> int:\n        \"\"\" Converts the given number of seconds into the corresponding number of blender animation frames.\n\n        :param seconds: The number of seconds.\n        :return: The number of frames.\n        \"\"\"\n        return int(seconds * bpy.context.scene.render.fps)\n\n    @staticmethod\n    def frames_to_seconds(frames: int) -> float:\n        \"\"\" Converts the given number of frames into the corresponding number of seconds.\n\n        :param frames: The number of frames.\n        :return: The number of seconds:\n        \"\"\"\n        return float(frames) / bpy.context.scene.render.fps\n\n    @staticmethod\n    def do_simulation(min_simulation_time: float, max_simulation_time: float, check_object_interval: float,\n                      object_stopped_location_threshold: float, object_stopped_rotation_threshold: float,\n                      verbose: bool = False):\n        \"\"\" Perform the simulation.\n\n        This method bakes the simulation for the configured number of iterations and returns all object positions\n        at the last frame.\n\n        :param min_simulation_time: The minimum number of seconds to simulate.\n        :param max_simulation_time: The maximum number of seconds to simulate.\n        :param check_object_interval: The interval in seconds at which all objects should be checked if they are still\n                                      moving. If all objects have stopped moving, then the simulation will be stopped.\n        :param object_stopped_location_threshold: The maximum difference per second and per coordinate in the rotation\n                                                  Euler vector that is allowed such that an object is still recognized\n                                                  as 'stopped moving'.\n        :param object_stopped_rotation_threshold: The maximum difference per second and per coordinate in the rotation\n                                                  Euler vector that is allowed such that an object is still recognized\n                                                  as 'stopped moving'.\n        :param verbose: If True, more details during the physics simulation are printed.\n        \"\"\"\n        # Make sure the RigidBody world is active\n        bpy.context.scene.rigidbody_world.enabled = True\n\n        # Run simulation\n        point_cache = bpy.context.scene.rigidbody_world.point_cache\n        point_cache.frame_start = 1\n\n        if min_simulation_time >= max_simulation_time:\n            raise Exception(\"max_simulation_iterations has to be bigger than min_simulation_iterations\")\n\n        # Run simulation starting from min to max in the configured steps\n        for current_time in np.arange(min_simulation_time, max_simulation_time, check_object_interval):\n            current_frame = _PhysicsSimulation.seconds_to_frames(current_time)\n            print(\"Running simulation up to \" + str(current_time) + \" seconds (\" + str(current_frame) + \" frames)\")\n\n            # Simulate current interval\n            point_cache.frame_end = current_frame\n            with stdout_redirected(enabled=not verbose):\n                bpy.ops.ptcache.bake({\"point_cache\": point_cache}, bake=True)\n\n            # Go to second last frame and get poses\n            bpy.context.scene.frame_set(current_frame - _PhysicsSimulation.seconds_to_frames(1))\n            old_poses = _PhysicsSimulation.get_pose()\n\n            # Go to last frame of simulation and get poses\n            bpy.context.scene.frame_set(current_frame)\n            new_poses = _PhysicsSimulation.get_pose()\n\n            # If objects have stopped moving between the last two frames, then stop here\n            if _PhysicsSimulation.have_objects_stopped_moving(old_poses, new_poses, object_stopped_location_threshold,\n                                                              object_stopped_rotation_threshold):\n                print(\"Objects have stopped moving after \" + str(current_time) + \"  seconds (\" + str(\n                    current_frame) + \" frames)\")\n                break\n            if current_time + check_object_interval >= max_simulation_time:\n                print(\"Stopping simulation as configured max_simulation_time has been reached\")\n            else:\n                # Free bake (this will not completely remove the simulation cache, so further simulations can\n                # reuse the already calculated frames)\n                bpy.ops.ptcache.free_bake({\"point_cache\": point_cache})\n\n    @staticmethod\n    def get_pose() -> dict:\n        \"\"\" Returns position and rotation values of all objects in the scene with ACTIVE rigid_body type.\n\n        :return: Dict of form {obj_name:{'location':[x, y, z], 'rotation':[x_rot, y_rot, z_rot]}}.\n        \"\"\"\n        objects_poses = {}\n        objects_with_physics = [obj for obj in get_all_blender_mesh_objects() if obj.rigid_body is not None]\n\n        for obj in objects_with_physics:\n            if obj.rigid_body.type == 'ACTIVE':\n                location = bpy.context.scene.objects[obj.name].matrix_world.translation.copy()\n                rotation = mathutils.Vector(bpy.context.scene.objects[obj.name].matrix_world.to_euler())\n                objects_poses.update({obj.name: {'location': location, 'rotation': rotation}})\n\n        return objects_poses\n\n    @staticmethod\n    def have_objects_stopped_moving(last_poses: dict, new_poses: dict, object_stopped_location_threshold: float,\n                                    object_stopped_rotation_threshold: float) -> bool:\n        \"\"\" Check if the difference between the two given poses per object is smaller than the configured threshold.\n\n        :param last_poses: Dict of form {obj_name:{'location':[x, y, z], 'rotation':[x_rot, y_rot, z_rot]}}.\n        :param new_poses: Dict of form {obj_name:{'location':[x, y, z], 'rotation':[x_rot, y_rot, z_rot]}}.\n        :param object_stopped_location_threshold: The maximum difference per second and per coordinate in the rotation\n                                                  Euler vector that is allowed such that an object is still recognized\n                                                  as 'stopped moving'.\n        :param object_stopped_rotation_threshold: The maximum difference per second and per coordinate in the rotation\n                                                  Euler vector that is allowed such that an object is still recognized\n                                                  as 'stopped moving'.\n        :return: True, if no objects are moving anymore.\n        \"\"\"\n        stopped = True\n        for obj_name in last_poses:\n            # Check location difference\n            location_diff = last_poses[obj_name]['location'] - new_poses[obj_name]['location']\n            stopped = stopped and not any(location_diff[i] > object_stopped_location_threshold for i in range(3))\n\n            # Check rotation difference\n            rotation_diff = last_poses[obj_name]['rotation'] - new_poses[obj_name]['rotation']\n            stopped = stopped and not any(rotation_diff[i] > object_stopped_rotation_threshold for i in range(3))\n\n            if not stopped:\n                break\n\n        return stopped",
  "def seconds_to_frames(seconds: float) -> int:\n        \"\"\" Converts the given number of seconds into the corresponding number of blender animation frames.\n\n        :param seconds: The number of seconds.\n        :return: The number of frames.\n        \"\"\"\n        return int(seconds * bpy.context.scene.render.fps)",
  "def frames_to_seconds(frames: int) -> float:\n        \"\"\" Converts the given number of frames into the corresponding number of seconds.\n\n        :param frames: The number of frames.\n        :return: The number of seconds:\n        \"\"\"\n        return float(frames) / bpy.context.scene.render.fps",
  "def do_simulation(min_simulation_time: float, max_simulation_time: float, check_object_interval: float,\n                      object_stopped_location_threshold: float, object_stopped_rotation_threshold: float,\n                      verbose: bool = False):\n        \"\"\" Perform the simulation.\n\n        This method bakes the simulation for the configured number of iterations and returns all object positions\n        at the last frame.\n\n        :param min_simulation_time: The minimum number of seconds to simulate.\n        :param max_simulation_time: The maximum number of seconds to simulate.\n        :param check_object_interval: The interval in seconds at which all objects should be checked if they are still\n                                      moving. If all objects have stopped moving, then the simulation will be stopped.\n        :param object_stopped_location_threshold: The maximum difference per second and per coordinate in the rotation\n                                                  Euler vector that is allowed such that an object is still recognized\n                                                  as 'stopped moving'.\n        :param object_stopped_rotation_threshold: The maximum difference per second and per coordinate in the rotation\n                                                  Euler vector that is allowed such that an object is still recognized\n                                                  as 'stopped moving'.\n        :param verbose: If True, more details during the physics simulation are printed.\n        \"\"\"\n        # Make sure the RigidBody world is active\n        bpy.context.scene.rigidbody_world.enabled = True\n\n        # Run simulation\n        point_cache = bpy.context.scene.rigidbody_world.point_cache\n        point_cache.frame_start = 1\n\n        if min_simulation_time >= max_simulation_time:\n            raise Exception(\"max_simulation_iterations has to be bigger than min_simulation_iterations\")\n\n        # Run simulation starting from min to max in the configured steps\n        for current_time in np.arange(min_simulation_time, max_simulation_time, check_object_interval):\n            current_frame = _PhysicsSimulation.seconds_to_frames(current_time)\n            print(\"Running simulation up to \" + str(current_time) + \" seconds (\" + str(current_frame) + \" frames)\")\n\n            # Simulate current interval\n            point_cache.frame_end = current_frame\n            with stdout_redirected(enabled=not verbose):\n                bpy.ops.ptcache.bake({\"point_cache\": point_cache}, bake=True)\n\n            # Go to second last frame and get poses\n            bpy.context.scene.frame_set(current_frame - _PhysicsSimulation.seconds_to_frames(1))\n            old_poses = _PhysicsSimulation.get_pose()\n\n            # Go to last frame of simulation and get poses\n            bpy.context.scene.frame_set(current_frame)\n            new_poses = _PhysicsSimulation.get_pose()\n\n            # If objects have stopped moving between the last two frames, then stop here\n            if _PhysicsSimulation.have_objects_stopped_moving(old_poses, new_poses, object_stopped_location_threshold,\n                                                              object_stopped_rotation_threshold):\n                print(\"Objects have stopped moving after \" + str(current_time) + \"  seconds (\" + str(\n                    current_frame) + \" frames)\")\n                break\n            if current_time + check_object_interval >= max_simulation_time:\n                print(\"Stopping simulation as configured max_simulation_time has been reached\")\n            else:\n                # Free bake (this will not completely remove the simulation cache, so further simulations can\n                # reuse the already calculated frames)\n                bpy.ops.ptcache.free_bake({\"point_cache\": point_cache})",
  "def get_pose() -> dict:\n        \"\"\" Returns position and rotation values of all objects in the scene with ACTIVE rigid_body type.\n\n        :return: Dict of form {obj_name:{'location':[x, y, z], 'rotation':[x_rot, y_rot, z_rot]}}.\n        \"\"\"\n        objects_poses = {}\n        objects_with_physics = [obj for obj in get_all_blender_mesh_objects() if obj.rigid_body is not None]\n\n        for obj in objects_with_physics:\n            if obj.rigid_body.type == 'ACTIVE':\n                location = bpy.context.scene.objects[obj.name].matrix_world.translation.copy()\n                rotation = mathutils.Vector(bpy.context.scene.objects[obj.name].matrix_world.to_euler())\n                objects_poses.update({obj.name: {'location': location, 'rotation': rotation}})\n\n        return objects_poses",
  "def have_objects_stopped_moving(last_poses: dict, new_poses: dict, object_stopped_location_threshold: float,\n                                    object_stopped_rotation_threshold: float) -> bool:\n        \"\"\" Check if the difference between the two given poses per object is smaller than the configured threshold.\n\n        :param last_poses: Dict of form {obj_name:{'location':[x, y, z], 'rotation':[x_rot, y_rot, z_rot]}}.\n        :param new_poses: Dict of form {obj_name:{'location':[x, y, z], 'rotation':[x_rot, y_rot, z_rot]}}.\n        :param object_stopped_location_threshold: The maximum difference per second and per coordinate in the rotation\n                                                  Euler vector that is allowed such that an object is still recognized\n                                                  as 'stopped moving'.\n        :param object_stopped_rotation_threshold: The maximum difference per second and per coordinate in the rotation\n                                                  Euler vector that is allowed such that an object is still recognized\n                                                  as 'stopped moving'.\n        :return: True, if no objects are moving anymore.\n        \"\"\"\n        stopped = True\n        for obj_name in last_poses:\n            # Check location difference\n            location_diff = last_poses[obj_name]['location'] - new_poses[obj_name]['location']\n            stopped = stopped and not any(location_diff[i] > object_stopped_location_threshold for i in range(3))\n\n            # Check rotation difference\n            rotation_diff = last_poses[obj_name]['rotation'] - new_poses[obj_name]['rotation']\n            stopped = stopped and not any(rotation_diff[i] > object_stopped_rotation_threshold for i in range(3))\n\n            if not stopped:\n                break\n\n        return stopped",
  "def construct_random_room(used_floor_area: float, interior_objects: List[MeshObject], materials: List[Material],\n                          amount_of_extrusions: int = 0, fac_from_square_room: float = 0.3, corridor_width: float = 0.9,\n                          wall_height: float = 2.5, amount_of_floor_cuts: int = 2, only_use_big_edges: bool = True,\n                          create_ceiling: bool = True, assign_material_to_ceiling: bool = False,\n                          placement_tries_per_face: int = 3,\n                          amount_of_objects_per_sq_meter: float = 3.0):\n    \"\"\"\n    Constructs a random room based on the given parameters, each room gets filled with the objects in the\n    `interior_objects` list.\n\n    :param used_floor_area: The amount of square meters used for this room (e.g. 25 qm)\n    :param interior_objects: List of interior objects, which are sampled inside this room\n    :param materials: List of materials, which will be used for the floor, ceiling, and the walls\n    :param amount_of_extrusions: Amount of extrusions performed on the basic floor shape, zero equals a rectangular room\n    :param fac_from_square_room: Maximum allowed factor between the length of two main sides of a rectangular room\n    :param corridor_width: Minimum corridor width in meters, is used for the extrusions\n    :param wall_height: Height of the walls of the room\n    :param amount_of_floor_cuts: The floor plan gets cut with each iteration, allowing for the finding of new edges\n                                 which are used to create extrusions.\n    :param only_use_big_edges: If this is all edges are sorted by length and only the bigger half is used\n    :param create_ceiling: If this is true a ceiling is created for the room\n    :param assign_material_to_ceiling: If this is True the ceiling also gets a material assigned\n    :param placement_tries_per_face: How many tries should be performed per face to place an object, a higher amount\n                                     will ensure that the amount of objects per sq meter are closer to the desired value\n    :param amount_of_objects_per_sq_meter: How many objects should be placed on each square meter of room\n    \"\"\"\n    # internally the first basic rectangular is counted as one\n    amount_of_extrusions += 1\n\n    bvh_cache_for_intersection: Dict[str, mathutils.bvhtree.BVHTree] = {}\n    placed_objects = []\n\n    # construct a random room\n    floor_obj, wall_obj, ceiling_obj = _construct_random_room(used_floor_area, amount_of_extrusions,\n                                                             fac_from_square_room, corridor_width,\n                                                             wall_height, amount_of_floor_cuts,\n                                                             only_use_big_edges, create_ceiling)\n    placed_objects.append(wall_obj)\n    if ceiling_obj is not None:\n        placed_objects.append(ceiling_obj)\n\n    # assign materials to all existing objects\n    _assign_materials_to_floor_wall_ceiling(floor_obj, wall_obj, ceiling_obj,\n                                           assign_material_to_ceiling, materials)\n\n    # get all floor faces and save their size and bounding box for the round robin\n    floor_obj.edit_mode()\n    bm = floor_obj.mesh_as_bmesh()\n    bm.faces.ensure_lookup_table()\n\n    list_of_face_sizes = []\n    list_of_face_bb = []\n    for face in bm.faces:\n        list_of_face_sizes.append(face.calc_area())\n        list_of_verts = [v.co for v in face.verts]\n        bb_min_point, bb_max_point = np.min(list_of_verts, axis=0), np.max(list_of_verts, axis=0)\n        list_of_face_bb.append((bb_min_point, bb_max_point))\n    floor_obj.update_from_bmesh(bm)\n    floor_obj.object_mode()\n    bpy.ops.object.select_all(action='DESELECT')\n    total_face_size = sum(list_of_face_sizes)\n\n    # sort them after size\n    interior_objects.sort(key=lambda obj: obj.get_bound_box_volume())\n    interior_objects.reverse()\n\n    list_of_deleted_objects = []\n\n    step_size = 1.0 / amount_of_objects_per_sq_meter * float(len(interior_objects))\n    current_step_size_counter = random.uniform(-step_size, step_size)\n    for selected_obj in interior_objects:\n        current_obj = selected_obj\n        is_duplicated = False\n\n        # if the step size is bigger than the room size, certain objects need to be skipped\n        if step_size > total_face_size:\n            current_step_size_counter += total_face_size\n            if current_step_size_counter > step_size:\n                current_step_size_counter = random.uniform(-step_size, step_size)\n                continue\n\n        # walk over all faces in a round robin fashion\n        total_acc_size = 0\n        # select a random start point\n        current_i = random.randrange(len(list_of_face_sizes))\n        current_accumulated_face_size = random.uniform(0, step_size + 1e-7)\n        # check if the accumulation of all visited faces is bigger than the sum of all of them\n        while total_acc_size < total_face_size:\n            face_size = list_of_face_sizes[current_i]\n            face_bb = list_of_face_bb[current_i]\n            if face_size < step_size:\n                # face size is bigger than one step\n                current_accumulated_face_size += face_size\n                if current_accumulated_face_size > step_size:\n                    for _ in range(placement_tries_per_face):\n                        found_spot = _sample_new_object_poses_on_face(current_obj, face_bb,\n                                                                     bvh_cache_for_intersection,\n                                                                     placed_objects, wall_obj)\n                        if found_spot:\n                            placed_objects.append(current_obj)\n                            current_obj = current_obj.duplicate()\n                            is_duplicated = True\n                            break\n                    current_accumulated_face_size -= step_size\n            else:\n                # face size is bigger than one step\n                amount_of_steps = int((face_size + current_accumulated_face_size) / step_size)\n                for _ in range(amount_of_steps):\n                    for _ in range(placement_tries_per_face):\n                        found_spot = _sample_new_object_poses_on_face(current_obj, face_bb,\n                                                                     bvh_cache_for_intersection,\n                                                                     placed_objects, wall_obj)\n                        if found_spot:\n                            placed_objects.append(current_obj)\n                            current_obj = current_obj.duplicate()\n                            is_duplicated = True\n                            break\n                # left over value is used in next round\n                current_accumulated_face_size = face_size - (amount_of_steps * step_size)\n            current_i = (current_i + 1) % len(list_of_face_sizes)\n            total_acc_size += face_size\n\n        # remove current obj from the bvh cache\n        if current_obj.get_name() in bvh_cache_for_intersection:\n            del bvh_cache_for_intersection[current_obj.get_name()]\n        # if there was no collision save the object in the placed list\n        if is_duplicated:\n            # delete the duplicated object\n            list_of_deleted_objects.append(current_obj)\n\n    # Add the loaded objects, which couldn't be placed\n    list_of_deleted_objects.extend([obj for obj in interior_objects if obj not in placed_objects])\n    # Delete them all\n    delete_multiple(list_of_deleted_objects, remove_all_offspring=True)\n\n    if floor_obj is not None:\n        placed_objects.append(floor_obj)\n    return placed_objects",
  "def _construct_random_room(used_floor_area: float, amount_of_extrusions: int, fac_from_square_room: float,\n                           corridor_width: float, wall_height: float, amount_of_floor_cuts: int,\n                           only_use_big_edges: bool, create_ceiling: bool) -> Tuple[MeshObject, MeshObject, MeshObject]:\n    \"\"\"\n    This function constructs the floor plan and builds up the wall. This can be more than just a rectangular shape.\n\n    If `amount_of_extrusions` is bigger than zero, the basic rectangular shape is extended, by first performing\n    random cuts in this base rectangular shape along the axis. Then one of the edges is randomly selected and\n    from there it is extruded outwards to get to the desired `floor_area`. This process is repeated\n    `amount_of_extrusions` times. It might be that a room has less than the desired `amount_of_extrusions` if\n    the random splitting reaches the `floor_area` beforehand.\n    \"\"\"\n    floor_obj = None\n    wall_obj = None\n    ceiling_obj = None\n\n    # if there is more than one extrusions, the used floor area must be split over all sections\n    # the first section should be at least 50% - 80% big, after that the size depends on the amount of left\n    # floor values\n    if amount_of_extrusions > 1:\n        size_sequence = []\n        running_sum = 0.0\n        start_minimum = 0.0\n        for i in range(amount_of_extrusions - 1):\n            if i == 0:\n                size_sequence.append(random.uniform(0.4, 0.8))\n                start_minimum = (1.0 - size_sequence[-1]) / amount_of_extrusions\n            else:\n                if start_minimum < 1.0 - running_sum:\n                    size_sequence.append(random.uniform(start_minimum, 1.0 - running_sum))\n                else:\n                    break\n            running_sum += size_sequence[-1]\n        if 1.0 - running_sum > 1e-7:\n            size_sequence.append(1.0 - running_sum)\n        if amount_of_extrusions != len(size_sequence):\n            print(f\"Amount of extrusions was reduced to: {len(size_sequence)}. To avoid rooms, \"\n                  f\"which are smaller than 1e-7\")\n            amount_of_extrusions = len(size_sequence)\n    else:\n        size_sequence = [1.0]\n    # this list of areas is then used to calculate the extrusions\n    # if there is only one element in there, it will create a rectangle\n    used_floor_areas = [size * used_floor_area for size in size_sequence]\n\n    # calculate the squared room length for the base room\n    squared_room_length = np.sqrt(used_floor_areas[0])\n    # create a new plane and rename it to Wall\n    wall_obj = create_primitive(\"PLANE\")\n    wall_obj.set_name(\"Wall\")\n\n    # calculate the side length of the base room, for that the `fac_from_square_room` is used\n    room_length_x = fac_from_square_room * random.uniform(-1, 1) * squared_room_length + squared_room_length\n    # make sure that the floor area is still used\n    room_length_y = used_floor_areas[0] / room_length_x\n    # change the plane to this size\n    wall_obj.edit_mode()\n    bpy.ops.transform.resize(value=(room_length_x * 0.5, room_length_y * 0.5, 1))\n    wall_obj.object_mode()\n\n    def cut_plane(plane: MeshObject):\n        \"\"\"\n        Cuts the floor plane in several pieces randomly. This is used for selecting random edges for the extrusions\n        later on. This function assumes the current `plane` object is already selected and no other object is\n        selected.\n\n        :param plane: The object, which should be split in edit mode.\n        \"\"\"\n\n        # save the size of the plane to determine a best split value\n        x_size = plane.get_scale()[0]\n        y_size = plane.get_scale()[1]\n\n        # switch to edit mode and select all faces\n        bpy.ops.object.mode_set(mode='EDIT')\n        bpy.ops.mesh.select_all(action='SELECT')\n        bpy.ops.object.mode_set(mode='OBJECT')\n\n        # convert plane to BMesh object\n        bm = plane.mesh_as_bmesh(True)\n        bm.faces.ensure_lookup_table()\n        # find all selected edges\n        edges = [e for e in bm.edges if e.select]\n\n        biggest_face_id = np.argmax([f.calc_area() for f in bm.faces])\n        biggest_face = bm.faces[biggest_face_id]\n        # find the biggest face\n        faces = [f for f in bm.faces if f == biggest_face]\n        geom = []\n        geom.extend(edges)\n        geom.extend(faces)\n\n        # calculate cutting point\n        cutting_point = [x_size * random.uniform(-1, 1), y_size * random.uniform(-1, 1), 0]\n        # select a random axis to specify in which direction to cut\n        direction_axis = [1, 0, 0] if random.uniform(0, 1) < 0.5 else [0, 1, 0]\n\n        # cut the plane and update the final mesh\n        bmesh.ops.bisect_plane(bm, dist=0.01, geom=geom, plane_co=cutting_point, plane_no=direction_axis)\n        plane.update_from_bmesh(bm)\n\n    # for each floor cut perform one cut_plane\n    for i in range(amount_of_floor_cuts):\n        cut_plane(wall_obj)\n\n    # do several extrusions of the basic floor plan, the first one is always the basic one\n    for i in range(1, amount_of_extrusions):\n        # Change to edit mode of the selected floor\n        wall_obj.edit_mode()\n        bpy.ops.mesh.select_all(action='DESELECT')\n        bm = wall_obj.mesh_as_bmesh()\n        bm.faces.ensure_lookup_table()\n        bm.edges.ensure_lookup_table()\n        # calculate the size of all edges and find all edges, which are wider than the minimum corridor_width\n        # to avoid that super small, super long pieces are created\n        boundary_edges = [e for e in bm.edges if e.is_boundary]\n        boundary_sizes = [(e, e.calc_length()) for e in boundary_edges]\n        boundary_sizes = [(e, s) for e, s in boundary_sizes if s > corridor_width]\n\n        if len(boundary_sizes) > 0:\n            # sort the boundaries to focus only on the big ones\n            boundary_sizes.sort(key=lambda e: e[1])\n            if only_use_big_edges:\n                # only select the bigger half of the selected boundaries\n                half_size = len(boundary_sizes) // 2\n            else:\n                # use any of the selected boundaries\n                half_size = 0\n            used_edges = [e for e, s in boundary_sizes[half_size:]]\n\n            random_edge = None\n            shift_vec = None\n            edge_counter = 0\n            random_index = random.randrange(len(used_edges))\n            while edge_counter < len(used_edges):\n                # select a random edge from the choose edges\n                random_edge = used_edges[random_index]\n                # get the direction of the current edge\n                direction = np.abs(random_edge.verts[0].co - random_edge.verts[1].co)\n                # the shift value depends on the used_floor_area size\n                shift_value = used_floor_areas[i] / random_edge.calc_length()\n\n                # depending if the random edge is aligned with the x-axis or the y-axis,\n                # the shift is the opposite direction\n                if direction[0] == 0:\n                    x_shift, y_shift = shift_value, 0\n                else:\n                    x_shift, y_shift = 0, shift_value\n                # calculate the vertices for the new face\n                shift_vec = mathutils.Vector([x_shift, y_shift, 0])\n                dir_found = False\n                for tested_dir in [1, -1]:\n                    shift_vec *= tested_dir\n                    new_verts = [e.co for e in random_edge.verts]\n                    new_verts.extend([e + shift_vec for e in new_verts])\n                    new_verts = np.array(new_verts)\n\n                    # check if the newly constructed face is colliding with one of the others\n                    # if so generate a new face\n                    collision_face_found = False\n                    for existing_face in bm.faces:\n                        existing_verts = np.array([v.co for v in existing_face.verts])\n                        if CollisionUtility.check_bb_intersection_on_values(np.min(existing_verts, axis=0)[:2],\n                                                           np.max(existing_verts, axis=0)[:2],\n                                                           np.min(new_verts, axis=0)[:2],\n                                                           np.max(new_verts, axis=0)[:2],\n                                                           # by using this check an edge collision is ignored\n                                                           used_check=lambda a, b: a > b):\n                            collision_face_found = True\n                            break\n                    if not collision_face_found:\n                        dir_found = True\n                        break\n                if dir_found:\n                    break\n                random_index = (random_index + 1) % len(used_edges)\n                edge_counter += 1\n                random_edge = None\n\n            if random_edge is None:\n                for e in used_edges:\n                    e.select = True\n                raise Exception(\"No edge found to extrude up on! The reason might be that there are to many cuts\"\n                                \"in the basic room or that the corridor width is too high.\")\n            # extrude this edge with the calculated shift\n            random_edge.select = True\n            bpy.ops.mesh.extrude_region_move(MESH_OT_extrude_region={\"use_normal_flip\": False,\n                                                                     \"use_dissolve_ortho_edges\": False,\n                                                                     \"mirror\": False},\n                                             TRANSFORM_OT_translate={\"value\": shift_vec,\n                                                                     \"orient_type\": 'GLOBAL'})\n        else:\n            raise Exception(\"The corridor width is so big that no edge could be selected, \"\n                            \"reduce the corridor width or reduce the amount of floor cuts.\")\n        # remove all doubles vertices, which might occur\n        bpy.ops.mesh.select_all(action='SELECT')\n        bpy.ops.mesh.remove_doubles()\n        bpy.ops.mesh.select_all(action='DESELECT')\n        wall_obj.update_from_bmesh(bm)\n        wall_obj.object_mode()\n\n    # create walls based on the outer shell\n    wall_obj.edit_mode()\n    bpy.ops.mesh.normals_make_consistent(inside=False)\n    bm = wall_obj.mesh_as_bmesh()\n    bm.edges.ensure_lookup_table()\n\n    # select all edges\n    boundary_edges = [e for e in bm.edges if e.is_boundary]\n    for e in boundary_edges:\n        e.select = True\n    # extrude all boundary edges to create the walls\n    bpy.ops.mesh.extrude_region_move(TRANSFORM_OT_translate={\"value\": (0, 0, wall_height)})\n    wall_obj.update_from_bmesh(bm)\n    wall_obj.object_mode()\n\n    def extract_plane_from_room(obj: MeshObject, used_split_height: float, up_vec: mathutils.Vector,\n                                new_name_for_obj: str):\n        \"\"\"\n        Extract a plane from the current room object. This uses the FaceSlicer Module functions\n\n        :param obj: The current room object\n        :param used_split_height: The height at which the split should be performed. Usually 0 or wall_height\n        :param up_vec: The up_vec corresponds to the face.normal of the selected faces\n        :param new_name_for_obj: This will be the new name of the created object\n        :return: (bool, bpy.types.Object): Returns True if the object was split and also returns the object. \\\n                                           Else it returns (False, None).\n        \"\"\"\n        compare_height = 0.15\n        compare_angle = math.radians(7.5)\n        obj.edit_mode()\n        bpy.ops.mesh.select_all(action='DESELECT')\n        bm = obj.mesh_as_bmesh()\n        bm.faces.ensure_lookup_table()\n        # Select faces at given height that should be separate from the mesh\n        counter = FaceSlicer.select_at_height_value(bm, used_split_height, compare_height,\n                                                    mathutils.Vector(up_vec), compare_angle,\n                                                    obj.get_local2world_mat())\n        # if any faces are selected split them up\n        if counter:\n            bpy.ops.mesh.separate(type='SELECTED')\n            obj.update_from_bmesh(bm)\n            obj.object_mode()\n            cur_selected_objects = bpy.context.selected_objects\n            if cur_selected_objects:\n                if len(cur_selected_objects) == 2:\n                    cur_selected_objects = [o for o in cur_selected_objects\n                                            if o != bpy.context.view_layer.objects.active]\n                    cur_selected_objects[0].name = new_name_for_obj\n                    cur_created_obj = MeshObject(cur_selected_objects[0])\n                else:\n                    raise Exception(\"There is more than one selection after splitting, this should not happen!\")\n            else:\n                raise Exception(\"No floor object was constructed!\")\n            bpy.ops.object.select_all(action='DESELECT')\n            return True, cur_created_obj\n        obj.object_mode()\n        bpy.ops.object.select_all(action='DESELECT')\n        return False, None\n\n    # if only one rectangle was created, the wall extrusion creates a full room with ceiling and floor, if not\n    # only the floor gets created and the ceiling is missing\n    only_rectangle_mode = False\n    for used_split_height in [(0, \"Floor\", [0, 0, 1]), (wall_height, \"Ceiling\", [0, 0, -1])]:\n        created, created_obj = extract_plane_from_room(wall_obj, used_split_height[0], used_split_height[2],\n                                                       used_split_height[1])\n        if not created and used_split_height[1] == \"Floor\":\n            only_rectangle_mode = True\n            break\n        if created and created_obj is not None:\n            if \"Floor\" == used_split_height[1]:\n                floor_obj = created_obj\n            elif \"Ceiling\" == used_split_height[1]:\n                ceiling_obj = created_obj\n\n    if only_rectangle_mode:\n        # in this case the floor and ceiling are pointing outwards, so that normals have to be flipped\n        for used_split_height in [(0, \"Floor\", [0, 0, -1]), (wall_height, \"Ceiling\", [0, 0, 1])]:\n            created, created_obj = extract_plane_from_room(wall_obj, used_split_height[0],\n                                                           used_split_height[2],\n                                                           used_split_height[1])\n            # save the result accordingly\n            if created and created_obj is not None:\n                if \"Floor\" == used_split_height[1]:\n                    floor_obj = created_obj\n                elif \"Ceiling\" == used_split_height[1]:\n                    ceiling_obj = created_obj\n    elif create_ceiling:\n        # there is no ceiling -> create one\n        wall_obj.edit_mode()\n        bpy.ops.mesh.select_all(action='DESELECT')\n        bm = wall_obj.mesh_as_bmesh()\n        bm.edges.ensure_lookup_table()\n        # select all upper edges and create a ceiling\n        for e in bm.edges:\n            if ((e.verts[0].co + e.verts[1].co) * 0.5)[2] >= wall_height - 1e-4:\n                e.select = True\n        bpy.ops.mesh.edge_face_add()\n        # split the ceiling away\n        bpy.ops.mesh.separate(type='SELECTED')\n        wall_obj.update_from_bmesh(bm)\n        wall_obj.object_mode()\n        selected_objects = bpy.context.selected_objects\n        if selected_objects:\n            if len(selected_objects) == 2:\n                selected_objects = [o for o in selected_objects\n                                    if o != bpy.context.view_layer.objects.active]\n                selected_objects[0].name = \"Ceiling\"\n                ceiling_obj = MeshObject(selected_objects[0])\n            else:\n                raise Exception(\"There is more than one selection after splitting, this should not happen!\")\n        else:\n            raise Exception(\"No floor object was constructed!\")\n        bpy.ops.object.select_all(action='DESELECT')\n\n    return floor_obj, wall_obj, ceiling_obj",
  "def _assign_materials_to_floor_wall_ceiling(floor_obj: MeshObject, wall_obj: MeshObject, ceiling_obj: MeshObject,\n                                            assign_material_to_ceiling: bool, materials: List[Material]):\n    \"\"\"\n    Assigns materials to the floor, wall and ceiling. These are randomly selected from the CCMaterials. This means\n    it is required that the CCMaterialLoader has been executed before, this module is run.\n    \"\"\"\n\n    # first create a uv mapping for each of the three objects\n    for obj in [floor_obj, wall_obj, ceiling_obj]:\n        if obj is not None:\n            obj.edit_mode()\n            bpy.ops.mesh.select_all(action='SELECT')\n            bpy.ops.uv.cube_project(cube_size=1.0)\n            obj.object_mode()\n\n    if materials:\n        floor_obj.replace_materials(random.choice(materials))\n        wall_obj.replace_materials(random.choice(materials))\n        if ceiling_obj is not None and assign_material_to_ceiling:\n            ceiling_obj.replace_materials(random.choice(materials))\n    else:\n        warnings.warn(\"There were no CCMaterials found, which means the CCMaterialLoader was not executed first!\"\n                      \"No materials have been assigned to the walls, floors and possible ceiling.\")",
  "def _sample_new_object_poses_on_face(current_obj: MeshObject, face_bb, bvh_cache_for_intersection: dict,\n                                     placed_objects: List[MeshObject], wall_obj: MeshObject):\n    \"\"\"\n    Sample new object poses on the current `floor_obj`.\n\n    :param face_bb:\n    :return: True, if there is no collision\n    \"\"\"\n    random_placed_value = [random.uniform(face_bb[0][i], face_bb[1][i]) for i in range(2)]\n    random_placed_value.append(0.0)  # floor z value\n\n    random_placed_rotation = [0, 0, random.uniform(0, np.pi * 2.0)]\n\n    current_obj.set_location(random_placed_value)\n    current_obj.set_rotation_euler(random_placed_rotation)\n\n    # Remove bvh cache, as object has changed\n    if current_obj.get_name() in bvh_cache_for_intersection:\n        del bvh_cache_for_intersection[current_obj.get_name()]\n\n    # perform check if object can be placed there\n    no_collision = CollisionUtility.check_intersections(current_obj,\n                                                        bvh_cache=bvh_cache_for_intersection,\n                                                        objects_to_check_against=placed_objects,\n                                                        list_of_objects_with_no_inside_check=[wall_obj])\n    return no_collision",
  "def cut_plane(plane: MeshObject):\n        \"\"\"\n        Cuts the floor plane in several pieces randomly. This is used for selecting random edges for the extrusions\n        later on. This function assumes the current `plane` object is already selected and no other object is\n        selected.\n\n        :param plane: The object, which should be split in edit mode.\n        \"\"\"\n\n        # save the size of the plane to determine a best split value\n        x_size = plane.get_scale()[0]\n        y_size = plane.get_scale()[1]\n\n        # switch to edit mode and select all faces\n        bpy.ops.object.mode_set(mode='EDIT')\n        bpy.ops.mesh.select_all(action='SELECT')\n        bpy.ops.object.mode_set(mode='OBJECT')\n\n        # convert plane to BMesh object\n        bm = plane.mesh_as_bmesh(True)\n        bm.faces.ensure_lookup_table()\n        # find all selected edges\n        edges = [e for e in bm.edges if e.select]\n\n        biggest_face_id = np.argmax([f.calc_area() for f in bm.faces])\n        biggest_face = bm.faces[biggest_face_id]\n        # find the biggest face\n        faces = [f for f in bm.faces if f == biggest_face]\n        geom = []\n        geom.extend(edges)\n        geom.extend(faces)\n\n        # calculate cutting point\n        cutting_point = [x_size * random.uniform(-1, 1), y_size * random.uniform(-1, 1), 0]\n        # select a random axis to specify in which direction to cut\n        direction_axis = [1, 0, 0] if random.uniform(0, 1) < 0.5 else [0, 1, 0]\n\n        # cut the plane and update the final mesh\n        bmesh.ops.bisect_plane(bm, dist=0.01, geom=geom, plane_co=cutting_point, plane_no=direction_axis)\n        plane.update_from_bmesh(bm)",
  "def extract_plane_from_room(obj: MeshObject, used_split_height: float, up_vec: mathutils.Vector,\n                                new_name_for_obj: str):\n        \"\"\"\n        Extract a plane from the current room object. This uses the FaceSlicer Module functions\n\n        :param obj: The current room object\n        :param used_split_height: The height at which the split should be performed. Usually 0 or wall_height\n        :param up_vec: The up_vec corresponds to the face.normal of the selected faces\n        :param new_name_for_obj: This will be the new name of the created object\n        :return: (bool, bpy.types.Object): Returns True if the object was split and also returns the object. \\\n                                           Else it returns (False, None).\n        \"\"\"\n        compare_height = 0.15\n        compare_angle = math.radians(7.5)\n        obj.edit_mode()\n        bpy.ops.mesh.select_all(action='DESELECT')\n        bm = obj.mesh_as_bmesh()\n        bm.faces.ensure_lookup_table()\n        # Select faces at given height that should be separate from the mesh\n        counter = FaceSlicer.select_at_height_value(bm, used_split_height, compare_height,\n                                                    mathutils.Vector(up_vec), compare_angle,\n                                                    obj.get_local2world_mat())\n        # if any faces are selected split them up\n        if counter:\n            bpy.ops.mesh.separate(type='SELECTED')\n            obj.update_from_bmesh(bm)\n            obj.object_mode()\n            cur_selected_objects = bpy.context.selected_objects\n            if cur_selected_objects:\n                if len(cur_selected_objects) == 2:\n                    cur_selected_objects = [o for o in cur_selected_objects\n                                            if o != bpy.context.view_layer.objects.active]\n                    cur_selected_objects[0].name = new_name_for_obj\n                    cur_created_obj = MeshObject(cur_selected_objects[0])\n                else:\n                    raise Exception(\"There is more than one selection after splitting, this should not happen!\")\n            else:\n                raise Exception(\"No floor object was constructed!\")\n            bpy.ops.object.select_all(action='DESELECT')\n            return True, cur_created_obj\n        obj.object_mode()\n        bpy.ops.object.select_all(action='DESELECT')\n        return False, None",
  "def render_segmap(output_dir: Optional[str] = None, temp_dir: Optional[str] = None,\n                  map_by: Union[str, List[str]] = \"class\",\n                  default_values: Optional[Dict[str, int]] = None, file_prefix: str = \"segmap_\",\n                  output_key: str = \"segmap\", segcolormap_output_file_prefix: str = \"instance_attribute_map_\",\n                  segcolormap_output_key: str = \"segcolormap\", use_alpha_channel: bool = False,\n                  render_colorspace_size_per_dimension: int = 2048) -> Dict[str, Union[np.ndarray, List[np.ndarray]]]:\n    \"\"\" Renders segmentation maps for all frames\n\n    :param output_dir: The directory to write images to.\n    :param temp_dir: The directory to write intermediate data to.\n    :param map_by: The attributes to be used for color mapping.\n    :param default_values: The default values used for the keys used in attributes, if None is {\"class\": 0}.\n    :param file_prefix: The prefix to use for writing the images.\n    :param output_key: The key to use for registering the output.\n    :param segcolormap_output_file_prefix: The prefix to use for writing the segmentation-color map csv.\n    :param segcolormap_output_key: The key to use for registering the segmentation-color map output.\n    :param use_alpha_channel: If true, the alpha channel stored in .png textures is used.\n    :param render_colorspace_size_per_dimension: As we use float16 for storing the rendering, the interval of \\\n                                                 integers which can be precisely stored is [-2048, 2048]. As \\\n                                                 blender does not allow negative values for colors, we use \\\n                                                 [0, 2048] ** 3 as our color space which allows ~8 billion \\\n                                                 different colors/objects. This should be enough.\n    :return: dict of lists of segmaps and (for instance segmentation) segcolormaps\n    \"\"\"\n\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n    if temp_dir is None:\n        temp_dir = Utility.get_temporary_directory()\n    if default_values is None:\n        default_values = {\"class\": 0}\n\n    with UndoAfterExecution():\n        RendererUtility.render_init()\n        # the amount of samples must be one and there can not be any noise threshold\n        RendererUtility.set_max_amount_of_samples(1)\n        RendererUtility.set_noise_threshold(0)\n        RendererUtility.set_denoiser(None)\n        RendererUtility.set_light_bounces(1, 0, 0, 1, 0, 8, 0)\n\n        attributes = map_by\n        if 'class' in default_values:\n            default_values['cp_category_id'] = default_values['class']\n\n        # Get objects with meshes (i.e. not lights or cameras)\n        objs_with_mats = get_all_blender_mesh_objects()\n\n        result = _colorize_objects_for_instance_segmentation(objs_with_mats, use_alpha_channel,\n                                                             render_colorspace_size_per_dimension)\n        colors, num_splits_per_dimension, objects = result\n\n        bpy.context.scene.cycles.filter_width = 0.0\n\n        if use_alpha_channel:\n            MaterialLoaderUtility.add_alpha_channel_to_textures(blurry_edges=False)\n\n        # Determine path for temporary and for final output\n        temporary_segmentation_file_path = os.path.join(temp_dir, \"seg_\")\n        final_segmentation_file_path = os.path.join(output_dir, file_prefix)\n\n        RendererUtility.set_output_format(\"OPEN_EXR\", 16)\n        RendererUtility.render(temp_dir, \"seg_\", None, return_data=False)\n\n        # Find optimal dtype of output based on max index\n        for dtype in [np.uint8, np.uint16, np.uint32]:\n            optimal_dtype = dtype\n            if np.iinfo(optimal_dtype).max >= len(colors) - 1:\n                break\n\n        if isinstance(attributes, str):\n            # only one result is requested\n            result_channels = 1\n            attributes = [attributes]\n        elif isinstance(attributes, list):\n            result_channels = len(attributes)\n        else:\n            raise RuntimeError(f\"The type of this is not supported here: {attributes}\")\n\n        # define them for the avoid rendering case\n        there_was_an_instance_rendering = False\n        list_of_attributes: List[str] = []\n\n        # Check if stereo is enabled\n        if bpy.context.scene.render.use_multiview:\n            suffixes = [\"_L\", \"_R\"]\n        else:\n            suffixes = [\"\"]\n\n        return_dict: Dict[str, Union[np.ndarray, List[np.ndarray]]] = {}\n\n        # After rendering\n        for frame in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):  # for each rendered frame\n            save_in_csv_attributes: Dict[int, Dict[str, Any]] = {}\n\n            there_was_an_instance_rendering = False\n            for suffix in suffixes:\n                file_path = temporary_segmentation_file_path + f\"{frame:04d}\" + suffix + \".exr\"\n                segmentation = load_image(file_path)\n                print(file_path, segmentation.shape)\n\n                segmap = Utility.map_back_from_equally_spaced_equidistant_values(segmentation,\n                                                                                 num_splits_per_dimension,\n                                                                                 render_colorspace_size_per_dimension)\n                segmap = segmap.astype(optimal_dtype)\n\n                object_ids = np.unique(segmap)\n                max_id = np.max(object_ids)\n                if max_id >= len(objects):\n                    raise Exception(\"There are more object colors than there are objects\")\n                combined_result_map = []\n                list_of_attributes = []\n                channels = []\n                for channel_id in range(result_channels):\n                    num_default_values = 0\n                    resulting_map = np.zeros((segmap.shape[0], segmap.shape[1]), dtype=optimal_dtype)\n                    was_used = False\n                    current_attribute = attributes[channel_id]\n                    org_attribute = current_attribute\n\n                    # if the class is used the category_id attribute is evaluated\n                    if current_attribute == \"class\":\n                        current_attribute = \"cp_category_id\"\n                    # in the instance case the resulting ids are directly used\n                    if current_attribute == \"instance\":\n                        there_was_an_instance_rendering = True\n                        resulting_map = segmap\n                        was_used = True\n                    else:\n                        if current_attribute != \"cp_category_id\":\n                            list_of_attributes.append(current_attribute)\n                        # for the current attribute remove cp_ and _csv, if present\n                        attribute = current_attribute\n                        if attribute.startswith(\"cp_\"):\n                            attribute = attribute[len(\"cp_\"):]\n                        # check if a default value was specified\n                        default_value_set = False\n                        if current_attribute in default_values or attribute in default_values:\n                            default_value_set = True\n                            if current_attribute in default_values:\n                                default_value = default_values[current_attribute]\n                            elif attribute in default_values:\n                                default_value = default_values[attribute]\n                        # iterate over all object ids\n                        for object_id in object_ids:\n                            # Convert np.uint8 to int, such that the save_in_csv_attributes dict can later be serialized\n                            object_id = int(object_id)\n                            # get the corresponding object via the id\n                            current_obj = objects[object_id]\n                            # if the current obj has a attribute with that name -> get it\n                            if hasattr(current_obj, attribute):\n                                value = getattr(current_obj, attribute)\n                            # if the current object has a custom property with that name -> get it\n                            elif current_attribute.startswith(\"cp_\") and attribute in current_obj:\n                                value = current_obj[attribute]\n                            elif current_attribute.startswith(\"cf_\"):\n                                if current_attribute == \"cf_basename\":\n                                    value = current_obj.name\n                                    if \".\" in value:\n                                        value = value[:value.rfind(\".\")]\n                            elif default_value_set:\n                                # if none of the above applies use the default value\n                                value = default_value\n                                num_default_values += 1\n                            else:\n                                # if the requested current_attribute is not a custom property or an attribute\n                                # or there is a default value stored\n                                # it throws an exception\n                                raise RuntimeError(f\"The obj: {current_obj.name} does not have the \"\n                                                   f\"attribute: {current_attribute}, striped: {attribute}. \"\n                                                   f\"Maybe try a default value.\")\n\n                            # save everything which is not instance also in the .csv\n                            if isinstance(value, (int, float, np.integer, np.floating)):\n                                was_used = True\n                                resulting_map[segmap == object_id] = value\n\n                            if object_id in save_in_csv_attributes:\n                                save_in_csv_attributes[object_id][attribute] = value\n                            else:\n                                save_in_csv_attributes[object_id] = {attribute: value}\n\n                    if was_used and num_default_values < len(object_ids):\n                        channels.append(org_attribute)\n                        combined_result_map.append(resulting_map)\n                        return_dict.setdefault(f\"{org_attribute}_segmaps{suffix}\", []).append(resulting_map)\n\n                fname = final_segmentation_file_path + f\"{frame:04d}\" + suffix\n                # combine all resulting images to one image\n                resulting_map = np.stack(combined_result_map, axis=2)\n                # remove the unneeded third dimension\n                if resulting_map.shape[2] == 1:\n                    resulting_map = resulting_map[:, :, 0]\n                # TODO: Remove unnecessary save when we give up backwards compatibility\n                np.save(fname, resulting_map)\n\n            if there_was_an_instance_rendering:\n                mappings = []\n                for object_id, attribute_dict in save_in_csv_attributes.items():\n                    mappings.append({\"idx\": object_id, **attribute_dict})\n                return_dict.setdefault(\"instance_attribute_maps\", []).append(mappings)\n\n                # write color mappings to file\n                # TODO: Remove unnecessary csv file when we give up backwards compatibility\n                csv_file_path = os.path.join(output_dir, segcolormap_output_file_prefix + f\"{frame:04d}.csv\")\n                with open(csv_file_path, 'w', newline='', encoding=\"utf-8\") as csvfile:\n                    # get from the first element the used field names\n                    fieldnames = [\"idx\"]\n                    # get all used object element keys\n                    for object_element in save_in_csv_attributes.values():\n                        fieldnames.extend(list(object_element.keys()))\n                        break\n                    for channel_name in channels:\n                        fieldnames.append(f\"channel_{channel_name}\")\n                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                    writer.writeheader()\n                    # save for each object all values in one row\n                    for obj_idx, object_element in save_in_csv_attributes.items():\n                        object_element[\"idx\"] = obj_idx\n                        for i, channel_name in enumerate(channels):\n                            object_element[f\"channel_{channel_name}\"] = i\n                        writer.writerow(object_element)\n            else:\n                if len(list_of_attributes) > 0:\n                    raise RuntimeError(f\"There were attributes specified in the may_by, which could not be saved as \"\n                                       f\"there was no \\\"instance\\\" may_by key used. This is true for this/these \"\n                                       f\"keys: {', '.join(list_of_attributes)}\")\n                # if there was no instance rendering no .csv file is generated!\n                # delete all saved info about .csv\n                save_in_csv_attributes = {}\n\n    Utility.register_output(output_dir, file_prefix, output_key, \".npy\", \"2.0.0\")\n\n    if save_in_csv_attributes:\n        Utility.register_output(output_dir,\n                                segcolormap_output_file_prefix,\n                                segcolormap_output_key,\n                                \".csv\",\n                                \"2.0.0\")\n    return return_dict",
  "def _colorize_object(obj: bpy.types.Object, color: mathutils.Vector, use_alpha_channel: bool):\n    \"\"\" Adjusts the materials of the given object, s.t. they are ready for rendering the seg map.\n\n    This is done by replacing all nodes just with an emission node, which emits the color corresponding to the\n    category of the object.\n\n    :param obj: The object to use.\n    :param color: RGB array of a color in the range of [0, self.render_colorspace_size_per_dimension].\n    :param use_alpha_channel: If true, the alpha channel stored in .png textures is used.\n    \"\"\"\n    # Create new material emitting the given color\n    new_mat = bpy.data.materials.new(name=\"segmentation\")\n    new_mat.use_nodes = True\n    # sampling as light,conserves memory, by not keeping a reference to it for multiple importance sampling.\n    # This shouldn't change the results because with an emission of 1 the colorized objects aren't emitting light.\n    # Also, BlenderProc's segmap render settings are configured so that there is only a single sample to distribute,\n    # multiple importance shouldn't affect the noise of the render anyway.\n    # This fixes issue #530\n    new_mat.cycles.sample_as_light = False\n    nodes = new_mat.node_tree.nodes\n    links = new_mat.node_tree.links\n    emission_node = nodes.new(type='ShaderNodeEmission')\n    output = Utility.get_the_one_node_with_type(nodes, 'OutputMaterial')\n\n    emission_node.inputs['Color'].default_value[:3] = color\n    links.new(emission_node.outputs['Emission'], output.inputs['Surface'])\n\n    # Set material to be used for coloring all faces of the given object\n    if len(obj.material_slots) > 0:\n        for i, material_slot in enumerate(obj.material_slots):\n            if use_alpha_channel:\n                obj.data.materials[i] = MaterialLoaderUtility.add_alpha_texture_node(material_slot.material,\n                                                                                     new_mat)\n            else:\n                obj.data.materials[i] = new_mat\n    else:\n        obj.data.materials.append(new_mat)",
  "def _set_world_background_color(color: List[float]):\n    \"\"\" Set the background color of the blender world object.\n\n    :param color: A 3-dim list containing the background color.\n    \"\"\"\n    if len(color) != 3:\n        raise Exception(\"The given color has to be three dimensional.\")\n\n    nodes = bpy.context.scene.world.node_tree.nodes\n    links = bpy.context.scene.world.node_tree.links\n\n    background_node = Utility.get_the_one_node_with_type(nodes, \"Background\")\n\n    # Unlink any incoming link that would overwrite the default value\n    if len(background_node.inputs['Color'].links) > 0:\n        links.remove(background_node.inputs['Color'].links[0])\n    # Set strength to 1 as it would act as a multiplier\n    background_node.inputs['Strength'].default_value = 1\n    background_node.inputs['Color'].default_value = color + [1]\n\n    # Make sure the background node is connected to the output node\n    output_node = Utility.get_the_one_node_with_type(nodes, \"Output\")\n    links.new(background_node.outputs[\"Background\"], output_node.inputs[\"Surface\"])",
  "def _colorize_objects_for_instance_segmentation(objects: List[bpy.types.Object], use_alpha_channel: bool,\n                                                render_colorspace_size_per_dimension: int) \\\n        -> Tuple[List[List[int]], int, List[bpy.types.Object]]:\n    \"\"\" Sets a different color to each object.\n\n    :param objects: A list of objects.\n    :param use_alpha_channel: If true, the alpha channel stored in .png textures is used.\n    :param render_colorspace_size_per_dimension: The limit of the colorspace to use per dimension for generating colors.\n    :return: The num_splits_per_dimension of the spanned color space, the color map\n    \"\"\"\n    # + 1 for the background\n    colors, num_splits_per_dimension = Utility.generate_equidistant_values(len(objects) + 1,\n                                                                           render_colorspace_size_per_dimension)\n    # this list maps ids in the image back to the objects\n    color_map = []\n\n    # Set world background label, which is always label zero\n    _set_world_background_color(colors[0])\n    color_map.append(bpy.context.scene.world)  # add the world background as an object to this list\n\n    for idx, obj in enumerate(objects):\n        _colorize_object(obj, colors[idx + 1], use_alpha_channel)\n        color_map.append(obj)\n\n    return colors, num_splits_per_dimension, color_map",
  "def render_optical_flow(output_dir: str = None, temp_dir: str = None, get_forward_flow: bool = True,\n                        get_backward_flow: bool = True, blender_image_coordinate_style: bool = False,\n                        forward_flow_output_file_prefix: str = \"forward_flow_\",\n                        forward_flow_output_key: str = \"forward_flow\",\n                        backward_flow_output_file_prefix: str = \"backward_flow_\",\n                        backward_flow_output_key: str = \"backward_flow\", return_data: bool = True,\n                        verbose: bool = False) -> \\\n        Dict[str, Union[np.ndarray, List[np.ndarray]]]:\n    \"\"\" Renders the optical flow (forward and backward) for all frames.\n\n    :param output_dir: The directory to write images to.\n    :param temp_dir: The directory to write intermediate data to.\n    :param get_forward_flow: Whether to render forward optical flow.\n    :param get_backward_flow: Whether to render backward optical flow.\n    :param blender_image_coordinate_style: Whether to specify the image coordinate system at the bottom left\n                                           (blender default; True) or top left (standard convention; False).\n    :param forward_flow_output_file_prefix: The file prefix that should be used when writing forward flow to a file.\n    :param forward_flow_output_key: The key which should be used for storing forward optical flow values.\n    :param backward_flow_output_file_prefix: The file prefix that should be used when writing backward flow to a file.\n    :param backward_flow_output_key: The key which should be used for storing backward optical flow values.\n    :param return_data: Whether to load and return generated data.\n    :param verbose: If True, more details about the rendering process are printed.\n    :return: dict of lists of raw renderer outputs. Keys can be 'forward_flow', 'backward_flow'\n    \"\"\"\n    if get_forward_flow is False and get_backward_flow is False:\n        raise RuntimeError(\"Take the FlowRenderer Module out of the config if both forward and \"\n                           \"backward flow are set to False!\")\n\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n    if temp_dir is None:\n        temp_dir = Utility.get_temporary_directory()\n\n    with UndoAfterExecution():\n        RendererUtility.render_init()\n        # the amount of samples must be one and there can not be any noise threshold\n        RendererUtility.set_max_amount_of_samples(1)\n        RendererUtility.set_noise_threshold(0)\n        RendererUtility.set_denoiser(None)\n        RendererUtility.set_light_bounces(1, 0, 0, 1, 0, 8, 0)\n\n        _FlowRendererUtility.output_vector_field(get_forward_flow, get_backward_flow, output_dir)\n\n        # only need to render once; both fwd and bwd flow will be saved\n        temporary_fwd_flow_file_path = os.path.join(temp_dir, 'fwd_flow_')\n        temporary_bwd_flow_file_path = os.path.join(temp_dir, 'bwd_flow_')\n        print(f\"Rendering {bpy.context.scene.frame_end - bpy.context.scene.frame_start} frames of optical flow...\")\n        RendererUtility.render(temp_dir, \"img_flow_temp_ignore_me_\", None, load_keys=set(), verbose=verbose)\n\n        # After rendering: convert to optical flow or calculate hsv visualization, if desired\n        for frame in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n            # temporarily save respective vector fields\n            if get_forward_flow:\n                file_path = temporary_fwd_flow_file_path + f\"{frame:04d}\" + \".exr\"\n                fwd_flow_field = load_image(file_path, num_channels=4).astype(np.float32)\n\n                if not blender_image_coordinate_style:\n                    fwd_flow_field[:, :, 1] = fwd_flow_field[:, :, 1] * -1\n\n                file_name = os.path.join(output_dir, forward_flow_output_file_prefix) + f\"{frame:04d}\"\n                forward_flow = fwd_flow_field * -1  # invert forward flow to point at next frame\n                np.save(file_name + '.npy', forward_flow[:, :, :2])\n\n            if get_backward_flow:\n                file_path = temporary_bwd_flow_file_path + f\"{frame:04d}\" + \".exr\"\n                bwd_flow_field = load_image(file_path, num_channels=4).astype(np.float32)\n\n                if not blender_image_coordinate_style:\n                    bwd_flow_field[:, :, 1] = bwd_flow_field[:, :, 1] * -1\n\n                file_name = os.path.join(output_dir, backward_flow_output_file_prefix) + f\"{frame:04d}\"\n                np.save(file_name + '.npy', bwd_flow_field[:, :, :2])\n\n    load_keys = set()\n    # register desired outputs\n    if get_forward_flow:\n        Utility.register_output(output_dir, forward_flow_output_file_prefix, forward_flow_output_key, '.npy', '2.0.0')\n        load_keys.add(forward_flow_output_key)\n    if get_backward_flow:\n        Utility.register_output(output_dir, backward_flow_output_file_prefix, backward_flow_output_key, '.npy', '2.0.0')\n        load_keys.add(backward_flow_output_key)\n\n    return _WriterUtility.load_registered_outputs(load_keys) if return_data else {}",
  "class _FlowRendererUtility():\n\n    @staticmethod\n    def output_vector_field(forward_flow: bool, backward_flow: bool, output_dir: str):\n        \"\"\" Configures compositor to output speed vectors.\n\n        :param forward_flow: Whether to render forward optical flow.\n        :param backward_flow: Whether to render backward optical flow.\n        :param output_dir: The directory to write images to.\n        \"\"\"\n\n        # Flow settings (is called \"vector\" in blender)\n        bpy.context.scene.render.use_compositing = True\n        bpy.context.scene.use_nodes = True\n        bpy.context.view_layer.use_pass_vector = True\n\n        # Adapt compositor to output vector field\n        tree = bpy.context.scene.node_tree\n        links = tree.links\n\n        # Use existing render layer\n        render_layer_node = tree.nodes.get('Render Layers')\n\n        separate_rgba = tree.nodes.new('CompositorNodeSepRGBA')\n        links.new(render_layer_node.outputs['Vector'], separate_rgba.inputs['Image'])\n\n        if forward_flow:\n            combine_fwd_flow = tree.nodes.new('CompositorNodeCombRGBA')\n            links.new(separate_rgba.outputs['B'], combine_fwd_flow.inputs['R'])\n            links.new(separate_rgba.outputs['A'], combine_fwd_flow.inputs['G'])\n            fwd_flow_output_file = tree.nodes.new('CompositorNodeOutputFile')\n            fwd_flow_output_file.base_path = output_dir\n            fwd_flow_output_file.format.file_format = \"OPEN_EXR\"\n            fwd_flow_output_file.file_slots.values()[0].path = \"fwd_flow_\"\n            links.new(combine_fwd_flow.outputs['Image'], fwd_flow_output_file.inputs['Image'])\n\n        if backward_flow:\n            # actually need to split - otherwise the A channel of the image is getting weird, no idea why\n            combine_bwd_flow = tree.nodes.new('CompositorNodeCombRGBA')\n            links.new(separate_rgba.outputs['R'], combine_bwd_flow.inputs['R'])\n            links.new(separate_rgba.outputs['G'], combine_bwd_flow.inputs['G'])\n            bwd_flow_output_file = tree.nodes.new('CompositorNodeOutputFile')\n            bwd_flow_output_file.base_path = output_dir\n            bwd_flow_output_file.format.file_format = \"OPEN_EXR\"\n            bwd_flow_output_file.file_slots.values()[0].path = \"bwd_flow_\"\n            links.new(combine_bwd_flow.outputs['Image'], bwd_flow_output_file.inputs['Image'])",
  "def output_vector_field(forward_flow: bool, backward_flow: bool, output_dir: str):\n        \"\"\" Configures compositor to output speed vectors.\n\n        :param forward_flow: Whether to render forward optical flow.\n        :param backward_flow: Whether to render backward optical flow.\n        :param output_dir: The directory to write images to.\n        \"\"\"\n\n        # Flow settings (is called \"vector\" in blender)\n        bpy.context.scene.render.use_compositing = True\n        bpy.context.scene.use_nodes = True\n        bpy.context.view_layer.use_pass_vector = True\n\n        # Adapt compositor to output vector field\n        tree = bpy.context.scene.node_tree\n        links = tree.links\n\n        # Use existing render layer\n        render_layer_node = tree.nodes.get('Render Layers')\n\n        separate_rgba = tree.nodes.new('CompositorNodeSepRGBA')\n        links.new(render_layer_node.outputs['Vector'], separate_rgba.inputs['Image'])\n\n        if forward_flow:\n            combine_fwd_flow = tree.nodes.new('CompositorNodeCombRGBA')\n            links.new(separate_rgba.outputs['B'], combine_fwd_flow.inputs['R'])\n            links.new(separate_rgba.outputs['A'], combine_fwd_flow.inputs['G'])\n            fwd_flow_output_file = tree.nodes.new('CompositorNodeOutputFile')\n            fwd_flow_output_file.base_path = output_dir\n            fwd_flow_output_file.format.file_format = \"OPEN_EXR\"\n            fwd_flow_output_file.file_slots.values()[0].path = \"fwd_flow_\"\n            links.new(combine_fwd_flow.outputs['Image'], fwd_flow_output_file.inputs['Image'])\n\n        if backward_flow:\n            # actually need to split - otherwise the A channel of the image is getting weird, no idea why\n            combine_bwd_flow = tree.nodes.new('CompositorNodeCombRGBA')\n            links.new(separate_rgba.outputs['R'], combine_bwd_flow.inputs['R'])\n            links.new(separate_rgba.outputs['G'], combine_bwd_flow.inputs['G'])\n            bwd_flow_output_file = tree.nodes.new('CompositorNodeOutputFile')\n            bwd_flow_output_file.base_path = output_dir\n            bwd_flow_output_file.format.file_format = \"OPEN_EXR\"\n            bwd_flow_output_file.file_slots.values()[0].path = \"bwd_flow_\"\n            links.new(combine_bwd_flow.outputs['Image'], bwd_flow_output_file.inputs['Image'])",
  "def set_denoiser(denoiser: Optional[str]):\n    \"\"\" Enables the specified denoiser.\n\n    Automatically disables all previously activated denoiser.\n\n    :param denoiser: The name of the denoiser which should be enabled. Options are \"INTEL\", \"OPTIX\" and None. \\\n                     If None is given, then no denoiser will be active.\n    \"\"\"\n    # Make sure there is no denoiser active\n    disable_all_denoiser()\n    if denoiser is None:\n        pass\n    elif denoiser.upper() == \"OPTIX\":\n        bpy.context.scene.cycles.use_denoising = True\n        bpy.context.view_layer.cycles.use_denoising = True\n        bpy.context.scene.cycles.denoiser = \"OPTIX\"\n    elif denoiser.upper() == \"INTEL\":\n        # The intel denoiser is activated via the compositor\n        bpy.context.scene.use_nodes = True\n        nodes = bpy.context.scene.node_tree.nodes\n        links = bpy.context.scene.node_tree.links\n\n        # The denoiser gets normal and diffuse color as input\n        bpy.context.view_layer.use_pass_normal = True\n        bpy.context.view_layer.use_pass_diffuse_color = True\n\n        # Add denoiser node\n        denoise_node = nodes.new(\"CompositorNodeDenoise\")\n\n        # Link nodes\n        render_layer_node = Utility.get_the_one_node_with_type(nodes, 'CompositorNodeRLayers')\n        composite_node = Utility.get_the_one_node_with_type(nodes, 'CompositorNodeComposite')\n        Utility.insert_node_instead_existing_link(links,\n                                                  render_layer_node.outputs['Image'],\n                                                  denoise_node.inputs['Image'],\n                                                  denoise_node.outputs['Image'],\n                                                  composite_node.inputs['Image'])\n\n        links.new(render_layer_node.outputs['DiffCol'], denoise_node.inputs['Albedo'])\n        links.new(render_layer_node.outputs['Normal'], denoise_node.inputs['Normal'])\n    else:\n        raise Exception(\"No such denoiser: \" + denoiser)",
  "def set_light_bounces(diffuse_bounces: Optional[int] = None, glossy_bounces: Optional[int] = None,\n                      ao_bounces_render: Optional[int] = None, max_bounces: Optional[int] = None,\n                      transmission_bounces: Optional[int] = None, transparent_max_bounces: Optional[int] = None,\n                      volume_bounces: Optional[int] = None):\n    \"\"\"\n    Sets the number of light bounces that should be used by the raytracing renderer.\n    Default values are defined in DefaultConfig.py\n\n    :param diffuse_bounces: Maximum number of diffuse reflection bounces, bounded by total maximum.\n    :param glossy_bounces: Maximum number of glossy reflection bounces, bounded by total maximum.\n    :param ao_bounces_render: Approximate indirect light with background tinted ambient occlusion at the \\\n                              specified bounce, 0 disables this feature.\n    :param max_bounces: Total maximum number of bounces.\n    :param transmission_bounces: Maximum number of transmission bounces, bounded by total maximum.\n    :param transparent_max_bounces: Maximum number of transparent bounces.\n    :param volume_bounces: Maximum number of volumetric scattering events.\n    \"\"\"\n    if diffuse_bounces is not None:\n        bpy.context.scene.cycles.diffuse_bounces = diffuse_bounces\n    if glossy_bounces is not None:\n        bpy.context.scene.cycles.glossy_bounces = glossy_bounces\n    if ao_bounces_render is not None:\n        bpy.context.scene.cycles.ao_bounces_render = ao_bounces_render\n    if max_bounces is not None:\n        bpy.context.scene.cycles.max_bounces = max_bounces\n    if transmission_bounces is not None:\n        bpy.context.scene.cycles.transmission_bounces = transmission_bounces\n    if transparent_max_bounces is not None:\n        bpy.context.scene.cycles.transparent_max_bounces = transparent_max_bounces\n    if volume_bounces is not None:\n        bpy.context.scene.cycles.volume_bounces = volume_bounces",
  "def set_cpu_threads(num_threads: int):\n    \"\"\" Sets the number of CPU cores to use simultaneously while rendering.\n\n    :param num_threads: The number of threads to use. If 0 is given the number is automatically detected based\n                        on the cpu cores.\n    \"\"\"\n    # If set to 0, use number of cores (default)\n    if num_threads > 0:\n        bpy.context.scene.render.threads_mode = \"FIXED\"\n        bpy.context.scene.render.threads = num_threads\n    else:\n        bpy.context.scene.render.threads_mode = \"AUTO\"",
  "def toggle_stereo(enable: bool):\n    \"\"\" Enables/Disables stereoscopy.\n\n    :param enable: True, if stereoscopy should be enabled.\n    \"\"\"\n    bpy.context.scene.render.use_multiview = enable\n    if enable:\n        bpy.context.scene.render.views_format = \"STEREO_3D\"",
  "def set_simplify_subdivision_render(simplify_subdivision_render: int):\n    \"\"\" Sets global maximum subdivision level during rendering to speedup rendering.\n\n    :param simplify_subdivision_render: The maximum subdivision level. If 0 is given, simplification of scene\n                                        is disabled.\n    \"\"\"\n    if simplify_subdivision_render > 0:\n        bpy.context.scene.render.use_simplify = True\n        bpy.context.scene.render.simplify_subdivision_render = simplify_subdivision_render\n    else:\n        bpy.context.scene.render.use_simplify = False",
  "def set_noise_threshold(noise_threshold: float):\n    \"\"\" Configures the adaptive sampling, the noise threshold is typically between 0.1 and 0.001.\n    Adaptive sampling automatically decreases the number of samples per pixel based on estimated level of noise.\n\n    We do not recommend setting the noise threshold value to zero and therefore turning off the adaptive sampling.\n\n    For more information see the official documentation:\n    https://docs.blender.org/manual/en/latest/render/cycles/render_settings/sampling.html#adaptive-sampling\n\n    :param noise_threshold: Noise level to stop sampling at. If 0 is given, adaptive sampling is disabled and only the\n                            max amount of samples is used.\n    \"\"\"\n    if noise_threshold > 0:\n        bpy.context.scene.cycles.use_adaptive_sampling = True\n        bpy.context.scene.cycles.adaptive_threshold = noise_threshold\n    else:\n        bpy.context.scene.cycles.use_adaptive_sampling = False",
  "def set_max_amount_of_samples(samples: int):\n    \"\"\" Sets the maximum number of samples to render for each pixel.\n    This maximum amount is usually not reached if the noise threshold is low enough.\n    If the noise threshold was set to 0, then only the maximum number of samples is used (We do not recommend this).\n\n    :param samples: The maximum number of samples per pixel\n    \"\"\"\n    bpy.context.scene.cycles.samples = samples",
  "def enable_distance_output(activate_antialiasing: bool, output_dir: Optional[str] = None,\n                           file_prefix: str = \"distance_\",\n                           output_key: str = \"distance\", antialiasing_distance_max: float = None,\n                           convert_to_depth: bool = False):\n    \"\"\" Enables writing distance images.\n\n\n    :param activate_antialiasing: If this is True the final image will be anti-aliased\n    :param output_dir: The directory to write files to, if this is None the temporary directory is used.\n    :param file_prefix: The prefix to use for writing the files.\n    :param output_key: The key to use for registering the distance output.\n    :param antialiasing_distance_max: Max distance in which the distance is measured. Resolution decreases\n                                      antiproportionally. Only if activate_antialiasing is True.\n    :param convert_to_depth: If this is true, while loading a postprocessing step is executed to convert this distance\n                             image to a depth image\n    \"\"\"\n    if not activate_antialiasing:\n        return enable_depth_output(activate_antialiasing, output_dir, file_prefix, output_key, convert_to_distance=True)\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n    if antialiasing_distance_max is None:\n        antialiasing_distance_max = DefaultConfig.antialiasing_distance_max\n\n    if GlobalStorage.is_in_storage(\"distance_output_is_enabled\"):\n        msg = \"The distance enable function can not be called twice. Either you called it twice or you used the \" \\\n              \"enable_depth_output with activate_antialiasing=True, which internally calls this function. This is \" \\\n              \"currently not supported, but there is an easy way to solve this, you can use the \" \\\n              \"bproc.postprocessing.dist2depth and depth2dist function on the output of the renderer and generate \" \\\n              \"the antialiased depth image yourself.\"\n        raise RuntimeError(msg)\n    GlobalStorage.add(\"distance_output_is_enabled\", True)\n\n    bpy.context.scene.render.use_compositing = True\n    bpy.context.scene.use_nodes = True\n    GlobalStorage.add(\"renderer_distance_end\", antialiasing_distance_max)\n\n    tree = bpy.context.scene.node_tree\n    links = tree.links\n    # Use existing render layer\n    render_layer_node = Utility.get_the_one_node_with_type(tree.nodes, 'CompositorNodeRLayers')\n\n    # Set mist pass limits\n    bpy.context.scene.world.mist_settings.start = 0\n    bpy.context.scene.world.mist_settings.depth = antialiasing_distance_max\n    bpy.context.scene.world.mist_settings.falloff = \"LINEAR\"\n\n    bpy.context.view_layer.use_pass_mist = True  # Enable distance pass\n    # Create a mapper node to map from 0-1 to SI units\n    mapper_node = tree.nodes.new(\"CompositorNodeMapRange\")\n    links.new(render_layer_node.outputs[\"Mist\"], mapper_node.inputs['Value'])\n    # map the values 0-1 to range distance_start to distance_range\n    mapper_node.inputs['From Max'].default_value = 1.0\n    mapper_node.inputs['To Min'].default_value = 0\n    mapper_node.inputs['To Max'].default_value = antialiasing_distance_max\n    final_output = mapper_node.outputs['Value']\n\n    # Build output node\n    output_file = tree.nodes.new(\"CompositorNodeOutputFile\")\n    output_file.base_path = output_dir\n    output_file.format.file_format = \"OPEN_EXR\"\n    output_file.file_slots.values()[0].path = file_prefix\n\n    # Feed the Z-Buffer or Mist output of the render layer to the input of the file IO layer\n    links.new(final_output, output_file.inputs['Image'])\n\n    Utility.add_output_entry({\n        \"key\": output_key,\n        \"path\": os.path.join(output_dir, file_prefix) + \"%04d\" + \".exr\",\n        \"version\": \"2.0.0\",\n        \"trim_redundant_channels\": True,\n        \"convert_to_depth\": convert_to_depth\n    })\n    return None",
  "def enable_depth_output(activate_antialiasing: bool, output_dir: Optional[str] = None, file_prefix: str = \"depth_\",\n                        output_key: str = \"depth\", antialiasing_distance_max: float = None,\n                        convert_to_distance: bool = False):\n    \"\"\" Enables writing depth images.\n\n    Depth images will be written in the form of .exr files during the next rendering.\n\n    :param activate_antialiasing: If this is True the final image will be antialiased\n    :param output_dir: The directory to write files to, if this is None the temporary directory is used.\n    :param file_prefix: The prefix to use for writing the files.\n    :param output_key: The key to use for registering the depth output.\n    :param antialiasing_distance_max: Max distance in which the distance is measured. \\\n                                      Only if activate_antialiasing is True.\n    :param convert_to_distance: If this is true, while loading a postprocessing step is executed to convert this depth \\\n                                image to a distance image\n    \"\"\"\n    if activate_antialiasing:\n        return enable_distance_output(activate_antialiasing, output_dir, file_prefix, output_key,\n                                      antialiasing_distance_max, convert_to_depth=True)\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n\n    if GlobalStorage.is_in_storage(\"depth_output_is_enabled\"):\n        msg = \"The depth enable function can not be called twice. Either you called it twice or you used the \" \\\n              \"enable_distance_output with activate_antialiasing=False, which internally calls this function. This \" \\\n              \"is currently not supported, but there is an easy way to solve this, you can use the \" \\\n              \"bproc.postprocessing.dist2depth and depth2dist function on the output of the renderer and generate \" \\\n              \"the antialiased distance image yourself.\"\n        raise RuntimeError(msg)\n    GlobalStorage.add(\"depth_output_is_enabled\", True)\n\n    bpy.context.scene.render.use_compositing = True\n    bpy.context.scene.use_nodes = True\n\n    tree = bpy.context.scene.node_tree\n    links = tree.links\n    # Use existing render layer\n    render_layer_node = Utility.get_the_one_node_with_type(tree.nodes, 'CompositorNodeRLayers')\n\n    # Enable z-buffer pass\n    bpy.context.view_layer.use_pass_z = True\n\n    # Build output node\n    output_file = tree.nodes.new(\"CompositorNodeOutputFile\")\n    output_file.base_path = output_dir\n    output_file.format.file_format = \"OPEN_EXR\"\n    output_file.file_slots.values()[0].path = file_prefix\n\n    # Feed the Z-Buffer output of the render layer to the input of the file IO layer\n    links.new(render_layer_node.outputs[\"Depth\"], output_file.inputs['Image'])\n\n    Utility.add_output_entry({\n        \"key\": output_key,\n        \"path\": os.path.join(output_dir, file_prefix) + \"%04d\" + \".exr\",\n        \"version\": \"2.0.0\",\n        \"trim_redundant_channels\": True,\n        \"convert_to_distance\": convert_to_distance\n    })\n    return None",
  "def enable_normals_output(output_dir: Optional[str] = None, file_prefix: str = \"normals_\",\n                          output_key: str = \"normals\"):\n    \"\"\" Enables writing normal images.\n\n    Normal images will be written in the form of .exr files during the next rendering.\n\n    :param output_dir: The directory to write files to, if this is None the temporary directory is used.\n    :param file_prefix: The prefix to use for writing the files.\n    :param output_key: The key to use for registering the normal output.\n    \"\"\"\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n\n    bpy.context.view_layer.use_pass_normal = True\n    bpy.context.scene.render.use_compositing = True\n    bpy.context.scene.use_nodes = True\n    tree = bpy.context.scene.node_tree\n    links = tree.links\n\n    # Use existing render layer\n    render_layer_node = Utility.get_the_one_node_with_type(tree.nodes, 'CompositorNodeRLayers')\n\n    separate_rgba = tree.nodes.new(\"CompositorNodeSepRGBA\")\n    space_between_nodes_x = 200\n    space_between_nodes_y = -300\n    separate_rgba.location.x = space_between_nodes_x\n    separate_rgba.location.y = space_between_nodes_y\n    links.new(render_layer_node.outputs[\"Normal\"], separate_rgba.inputs[\"Image\"])\n\n    combine_rgba = tree.nodes.new(\"CompositorNodeCombRGBA\")\n    combine_rgba.location.x = space_between_nodes_x * 14\n\n    c_channels = [\"R\", \"G\", \"B\"]\n    offset = space_between_nodes_x * 2\n    multiplication_values: List[List[bpy.types.Node]] = [[], [], []]\n    channel_results = {}\n    for row_index, channel in enumerate(c_channels):\n        # matrix multiplication\n        mulitpliers = []\n        for column in range(3):\n            multiply = tree.nodes.new(\"CompositorNodeMath\")\n            multiply.operation = \"MULTIPLY\"\n            multiply.inputs[1].default_value = 0  # setting at the end for all frames\n            multiply.location.x = column * space_between_nodes_x + offset\n            multiply.location.y = row_index * space_between_nodes_y\n            links.new(separate_rgba.outputs[c_channels[column]], multiply.inputs[0])\n            mulitpliers.append(multiply)\n            multiplication_values[row_index].append(multiply)\n\n        first_add = tree.nodes.new(\"CompositorNodeMath\")\n        first_add.operation = \"ADD\"\n        first_add.location.x = space_between_nodes_x * 5 + offset\n        first_add.location.y = row_index * space_between_nodes_y\n        links.new(mulitpliers[0].outputs[\"Value\"], first_add.inputs[0])\n        links.new(mulitpliers[1].outputs[\"Value\"], first_add.inputs[1])\n\n        second_add = tree.nodes.new(\"CompositorNodeMath\")\n        second_add.operation = \"ADD\"\n        second_add.location.x = space_between_nodes_x * 6 + offset\n        second_add.location.y = row_index * space_between_nodes_y\n        links.new(first_add.outputs[\"Value\"], second_add.inputs[0])\n        links.new(mulitpliers[2].outputs[\"Value\"], second_add.inputs[1])\n\n        channel_results[channel] = second_add\n\n    # set the matrix accordingly\n    rot_around_x_axis = mathutils.Matrix.Rotation(math.radians(-90.0), 4, 'X')\n    for frame in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n        used_rotation_matrix = CameraUtility.get_camera_pose(frame) @ rot_around_x_axis\n        for row_index in range(3):\n            for column_index in range(3):\n                current_multiply = multiplication_values[row_index][column_index]\n                current_multiply.inputs[1].default_value = used_rotation_matrix[column_index][row_index]\n                current_multiply.inputs[1].keyframe_insert(data_path='default_value', frame=frame)\n    offset = 8 * space_between_nodes_x\n    for index, channel in enumerate(c_channels):\n        multiply = tree.nodes.new(\"CompositorNodeMath\")\n        multiply.operation = \"MULTIPLY\"\n        multiply.location.x = space_between_nodes_x * 2 + offset\n        multiply.location.y = index * space_between_nodes_y\n        links.new(channel_results[channel].outputs[\"Value\"], multiply.inputs[0])\n        if channel == \"G\":\n            multiply.inputs[1].default_value = -0.5\n        else:\n            multiply.inputs[1].default_value = 0.5\n        add = tree.nodes.new(\"CompositorNodeMath\")\n        add.operation = \"ADD\"\n        add.location.x = space_between_nodes_x * 3 + offset\n        add.location.y = index * space_between_nodes_y\n        links.new(multiply.outputs[\"Value\"], add.inputs[0])\n        add.inputs[1].default_value = 0.5\n        output_channel = channel\n        if channel == \"G\":\n            output_channel = \"B\"\n        elif channel == \"B\":\n            output_channel = \"G\"\n        links.new(add.outputs[\"Value\"], combine_rgba.inputs[output_channel])\n\n    output_file = tree.nodes.new(\"CompositorNodeOutputFile\")\n    output_file.base_path = output_dir\n    output_file.format.file_format = \"OPEN_EXR\"\n    output_file.file_slots.values()[0].path = file_prefix\n    output_file.location.x = space_between_nodes_x * 15\n    links.new(combine_rgba.outputs[\"Image\"], output_file.inputs[\"Image\"])\n\n    Utility.add_output_entry({\n        \"key\": output_key,\n        \"path\": os.path.join(output_dir, file_prefix) + \"%04d\" + \".exr\",\n        \"version\": \"2.0.0\"\n    })",
  "def enable_segmentation_output(map_by: Union[str, List[str]] = \"category_id\",\n                               default_values: Optional[Dict[str, Any]] = None,\n                               pass_alpha_threshold: float = 0.05,\n                               output_dir: Optional[str] = None,\n                               file_prefix: str = \"segmap_\", output_key: str = \"segmap\"):\n    \"\"\" Enables segmentation output by certain keys.\n\n    The key instances is used, if a mapping of every object in the scene to an integer is requested. These integers\n    are assigned randomly and do not follow any system. They are consisted for one rendering call.\n\n    By default, the custom property `category_id` is used. It has to be set for every visible object in the scene,\n    including the background (world). One can provide a `default_value` for it to avoid errors: `{\"category_id\": 0}`.\n\n    Map by keys can be all custom properties or the attributes of an object such as `location` or `name`. If the value\n    can not be stored in the image itself an instance image has to be generated. The output then will contain a\n    dictionary mapping the instance ids to the attributes of the objects.\n\n    :param map_by: Map by keys, either a single str or a list of str.\n    :param default_values: A dictionary offering a default value for objects which do not provide a value\n                           for a certain key\n    :param pass_alpha_threshold: This alpha threshold is used to decide which object to use a low value means that an\n                                 object has to be nearly completely transparent to be considered transparent, while\n                                 materials such as frosted class with an alpha value of 0.5 would be considered opaque\n    :param output_dir: The temporary output dir in which the resulting .exr images are saved\n    :param file_prefix: The prefix to use for writing the files.\n    :param output_key: The key to use for registering the segmentation output.\n    \"\"\"\n    # give all objects an id, background is always zero\n    for index, obj in enumerate(get_all_blender_mesh_objects()):\n        obj.pass_index = index + 1\n\n    # add the pass object index id to the rendering output\n    bpy.context.scene.render.use_compositing = True\n    bpy.context.scene.use_nodes = True\n    bpy.context.scene.view_layers[\"ViewLayer\"].use_pass_object_index = True\n\n    tree = bpy.context.scene.node_tree\n    links = tree.links\n\n    render_layer_node = tree.nodes.get('Render Layers')\n\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n\n    output_node = tree.nodes.new('CompositorNodeOutputFile')\n    output_node.base_path = output_dir\n    output_node.format.file_format = \"OPEN_EXR\"\n    output_node.file_slots.values()[0].path = file_prefix\n    Utility.add_output_entry({\n        \"key\": output_key,\n        \"path\": os.path.join(output_dir, file_prefix) + \"%04d\" + \".exr\",\n        \"version\": \"3.0.0\",\n        \"trim_redundant_channels\": True,\n        \"is_semantic_segmentation\": True,\n        \"semantic_segmentation_mapping\": map_by,\n        \"semantic_segmentation_default_values\": default_values\n    })\n\n    links.new(render_layer_node.outputs[\"IndexOB\"], output_node.inputs[\"Image\"])\n\n    # set the threshold low to avoid noise in alpha materials\n    bpy.context.scene.view_layers[\"ViewLayer\"].pass_alpha_threshold = pass_alpha_threshold",
  "def enable_diffuse_color_output(output_dir: Optional[str] = None, file_prefix: str = \"diffuse_\",\n                                output_key: str = \"diffuse\"):\n    \"\"\" Enables writing diffuse color (albedo) images.\n\n    Diffuse color images will be written in the form of .png files during the next rendering.\n\n    :param output_dir: The directory to write files to, if this is None the temporary directory is used.\n    :param file_prefix: The prefix to use for writing the files.\n    :param output_key: The key to use for registering the diffuse color output.\n    \"\"\"\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n\n    bpy.context.scene.render.use_compositing = True\n    bpy.context.scene.use_nodes = True\n    tree = bpy.context.scene.node_tree\n    links = tree.links\n\n    bpy.context.view_layer.use_pass_diffuse_color = True\n    render_layer_node = Utility.get_the_one_node_with_type(tree.nodes, 'CompositorNodeRLayers')\n    final_output = render_layer_node.outputs[\"DiffCol\"]\n\n    output_file = tree.nodes.new('CompositorNodeOutputFile')\n    output_file.base_path = output_dir\n    output_file.format.file_format = \"PNG\"\n    output_file.file_slots.values()[0].path = file_prefix\n    links.new(final_output, output_file.inputs['Image'])\n\n    Utility.add_output_entry({\n        \"key\": output_key,\n        \"path\": os.path.join(output_dir, file_prefix) + \"%04d\" + \".png\",\n        \"version\": \"2.0.0\"\n    })",
  "def map_file_format_to_file_ending(file_format: str) -> str:\n    \"\"\" Returns the files endings for a given blender output format.\n\n    :param file_format: The blender file format.\n    :return: The file ending.\n    \"\"\"\n    if file_format == 'PNG':\n        return \".png\"\n    if file_format == 'JPEG':\n        return \".jpg\"\n    if file_format == 'OPEN_EXR':\n        return \".exr\"\n    raise RuntimeError(f\"Unknown Image Type {file_format}\")",
  "def _progress_bar_thread(pipe_out: int, stdout: IO, total_frames: int, num_samples: int):\n    \"\"\" The thread rendering the progress bar\n\n    :param pipe_out: The pipe output delivering blenders debug messages.\n    :param stdout: The stdout to which the progress bar should be written.\n    :param total_frames: The number of frames that should be rendered.\n    :param num_samples: The number of samples used to render each frame.\n    \"\"\"\n    # Define columns for progress bar\n    columns = [\n        TextColumn(\"[progress.description]{task.description}\"),\n        BarColumn(),\n        TimeRemainingColumn(),\n        TextColumn(\"[progress.description]{task.fields[status]}\"),\n    ]\n    # Initializes progress bar using given stdout\n    with Progress(*columns, console=Console(file=stdout), transient=True) as progress:\n        complete_task = progress.add_task(\"[green]Total\", total=total_frames, status=\"\")\n        frame_task = progress.add_task(\"[yellow]Current frame\", total=num_samples, status=\"\")\n\n        # Continuously read blenders debug messages\n        current_line = \"\"\n        while True:\n            # Read the next character\n            char = os.read(pipe_out, 1)\n            if not char:\n                break\n            char = chr(char[0])\n            # If its the ending character, stop\n            if not char or \"\\b\" == char:\n                break\n            # If the current line has ended\n            if char == \"\\n\":\n                # Check if its a line we can use (starts with \"Fra:\")\n                if current_line.startswith(\"Fra:\"):\n                    # Extract current frame number and set to progress bar\n                    frame_number = int(current_line.split()[0][len(\"Fra:\"):])\n                    progress.update(complete_task, completed=frame_number)\n                    progress.update(complete_task, status=f\"Rendering frame {frame_number + 1} of {total_frames}\")\n\n                    # Split line into columns\n                    status_columns = [col.strip() for col in current_line.split(\"|\")]\n                    if \"Scene, ViewLayer\" in status_columns:\n                        # If we are currently at \"Scene, ViewLayer\", use everything afterwards\n                        status = \" | \".join(status_columns[status_columns.index(\"Scene, ViewLayer\") + 1:])\n                        # If we are currently rendering, update the progress\n                        if status.startswith(\"Sample\"):\n                            progress.update(frame_task, completed=int(status[len(\"Sample\"):].split(\"/\", maxsplit=1)[0]))\n                    elif \"Compositing\" in status_columns:\n                        # If we are at \"Compositing\", use everything afterwards including \"Compositing\"\n                        status = \" | \".join(status_columns[status_columns.index(\"Compositing\"):])\n                        # Set render progress to complete\n                        progress.update(frame_task, completed=num_samples)\n                    else:\n                        # In every other case, use last column\n                        status = status_columns[-1]\n                    # Set status to progress bar\n                    progress.update(frame_task, status=status)\n                # Start with next line\n                current_line = \"\"\n            else:\n                # Append char to current line\n                current_line += char",
  "def _render_progress_bar(pipe_out: int, pipe_in: int, stdout: IO, total_frames: int, enabled: bool = True):\n    \"\"\" Shows a progress bar visualizing the render progress.\n\n    :param pipe_out: The pipe output delivering blenders debug messages.\n    :param pipe_in: The input of the pipe, necessary to send the end character.\n    :param stdout: The stdout to which the progress bar should be written.\n    :param total_frames: The number of frames that should be rendered.\n    :param enabled: If False, no progress bar is shown.\n    \"\"\"\n    if enabled:\n        thread = threading.Thread(target=_progress_bar_thread,\n                                  args=(pipe_out, stdout, total_frames, bpy.context.scene.cycles.samples))\n        thread.start()\n        try:\n            yield\n        finally:\n            # Send final character, so the thread knows to stop\n            w = os.fdopen(pipe_in, 'w')\n            w.write(\"\\b\")\n            w.close()\n            thread.join()\n    else:\n        yield",
  "def render(output_dir: Optional[str] = None, file_prefix: str = \"rgb_\", output_key: Optional[str] = \"colors\",\n           load_keys: Optional[Set[str]] = None, return_data: bool = True,\n           keys_with_alpha_channel: Optional[Set[str]] = None,\n           verbose: bool = False) -> Dict[str, Union[np.ndarray, List[np.ndarray]]]:\n    \"\"\" Render all frames.\n\n    This will go through all frames from scene.frame_start to scene.frame_end and render each of them.\n\n    :param output_dir: The directory to write files to, if this is None the temporary directory is used. \\\n                       The temporary directory is usually in the shared memory (only true for linux).\n    :param file_prefix: The prefix to use for writing the images.\n    :param output_key: The key to use for registering the output.\n    :param load_keys: Set of output keys to load when available\n    :param return_data: Whether to load and return generated data.\n    :param keys_with_alpha_channel: A set containing all keys whose alpha channels should be loaded.\n    :param verbose: If True, more details about the rendering process are printed.\n    :return: dict of lists of raw renderer output. Keys can be 'distance', 'colors', 'normals'\n    \"\"\"\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n    if load_keys is None:\n        load_keys = {'colors', 'distance', 'normals', 'diffuse', 'depth', 'segmap'}\n        keys_with_alpha_channel = {'colors'} if bpy.context.scene.render.film_transparent else None\n\n    if output_key is not None:\n        Utility.add_output_entry({\n            \"key\": output_key,\n            \"path\": os.path.join(output_dir, file_prefix) + \"%04d\" +\n                    map_file_format_to_file_ending(bpy.context.scene.render.image_settings.file_format),\n            \"version\": \"2.0.0\"\n        })\n        load_keys.add(output_key)\n\n    bpy.context.scene.render.filepath = os.path.join(output_dir, file_prefix)\n\n    # Skip if there is nothing to render\n    if bpy.context.scene.frame_end != bpy.context.scene.frame_start:\n        if len(get_all_blender_mesh_objects()) == 0:\n            raise Exception(\"There are no mesh-objects to render, \"\n                            \"please load an object before invoking the renderer.\")\n        # Print what is rendered\n        total_frames = bpy.context.scene.frame_end - bpy.context.scene.frame_start\n        if load_keys:\n            registered_output_keys = [output[\"key\"] for output in Utility.get_registered_outputs()]\n            keys_to_render = sorted([key for key in load_keys if key in registered_output_keys])\n            print(f\"Rendering {total_frames} frames of {', '.join(keys_to_render)}...\")\n\n        # As frame_end is pointing to the next free frame, decrease it by one, as\n        # blender will render all frames in [frame_start, frame_ned]\n        bpy.context.scene.frame_end -= 1\n\n        # Define pipe to communicate blenders debug messages to progress bar\n        pipe_out, pipe_in = os.pipe()\n        begin = time.time()\n        with stdout_redirected(pipe_in, enabled=not verbose) as stdout:\n            with _render_progress_bar(pipe_out, pipe_in, stdout, total_frames, enabled=not verbose):\n                bpy.ops.render.render(animation=True, write_still=True)\n\n        # Close Pipes to prevent having unclosed file handles\n        try:\n            os.close(pipe_out)\n        except OSError:\n            pass\n        try:\n            os.close(pipe_in)\n        except OSError:\n            pass\n\n        print(f\"Finished rendering after {time.time() - begin:.3f} seconds\")\n        # Revert changes\n        bpy.context.scene.frame_end += 1\n    else:\n        raise RuntimeError(\"No camera poses have been registered, therefore nothing can be rendered. A camera \"\n                           \"pose can be registered via bproc.camera.add_camera_pose().\")\n\n    return _WriterUtility.load_registered_outputs(load_keys, keys_with_alpha_channel) if return_data else {}",
  "def set_output_format(file_format: Optional[str] = None, color_depth: Optional[int] = None,\n                      enable_transparency: Optional[bool] = None, jpg_quality: Optional[int] = None):\n    \"\"\" Sets the output format to use for rendering. Default values defined in DefaultConfig.py.\n\n    :param file_format: The file format to use, e.q. \"PNG\", \"JPEG\" or \"OPEN_EXR\".\n    :param color_depth: The color depth.\n    :param enable_transparency: If true, the output will contain a alpha channel and the background will be\n                                set transparent.\n    :param jpg_quality: The quality to use, if file format is set to \"JPEG\".\n    \"\"\"\n    if enable_transparency is not None:\n        # In case a previous renderer changed these settings\n        # Store as RGB by default unless the user specifies store_alpha as true in yaml\n        bpy.context.scene.render.image_settings.color_mode = \"RGBA\" if enable_transparency else \"RGB\"\n        # set the background as transparent if transparent_background is true in yaml\n        bpy.context.scene.render.film_transparent = enable_transparency\n    if file_format is not None:\n        bpy.context.scene.render.image_settings.file_format = file_format\n    if color_depth is not None:\n        bpy.context.scene.render.image_settings.color_depth = str(color_depth)\n    if jpg_quality is not None:\n        # only influences jpg quality\n        bpy.context.scene.render.image_settings.quality = jpg_quality",
  "def enable_motion_blur(motion_blur_length: float = 0.5, rolling_shutter_type: str = \"NONE\",\n                       rolling_shutter_length: float = 0.1):\n    \"\"\" Enables motion blur and sets rolling shutter.\n\n    :param motion_blur_length: Time taken in frames between shutter open and close.\n    :param rolling_shutter_type: Type of rolling shutter effect. If \"NONE\", rolling shutter is disabled.\n    :param rolling_shutter_length: Scanline \"exposure\" time for the rolling shutter effect.\n    \"\"\"\n    bpy.context.scene.render.use_motion_blur = True\n    bpy.context.scene.render.motion_blur_shutter = motion_blur_length\n\n    bpy.context.scene.cycles.rolling_shutter_type = rolling_shutter_type\n    bpy.context.scene.cycles.rolling_shutter_duration = rolling_shutter_length",
  "def render_init():\n    \"\"\" Initializes the renderer.\n\n    This enables the cycles renderer and sets some options to speedup rendering.\n    \"\"\"\n    bpy.context.scene.render.resolution_percentage = 100\n    # Lightning settings to reduce training time\n    bpy.context.scene.render.engine = 'CYCLES'\n\n    bpy.context.scene.cycles.debug_bvh_type = \"STATIC_BVH\"\n    bpy.context.scene.cycles.debug_use_spatial_splits = True\n    # Setting use_persistent_data to True makes the rendering getting slower and slower (probably a blender bug)\n    bpy.context.scene.render.use_persistent_data = True",
  "def disable_all_denoiser():\n    \"\"\" Disables all denoiser.\n\n    At the moment this includes the cycles and the intel denoiser.\n    \"\"\"\n    # Disable cycles denoiser\n    bpy.context.view_layer.cycles.use_denoising = False\n    bpy.context.scene.cycles.use_denoising = False\n\n    # Disable intel denoiser\n    if bpy.context.scene.use_nodes:\n        nodes = bpy.context.scene.node_tree.nodes\n        links = bpy.context.scene.node_tree.links\n\n        # Go through all existing denoiser nodes\n        for denoiser_node in Utility.get_nodes_with_type(nodes, 'CompositorNodeDenoise'):\n            in_node = denoiser_node.inputs['Image']\n            out_node = denoiser_node.outputs['Image']\n\n            # If it is fully included into the node tree\n            if in_node.is_linked and out_node.is_linked:\n                # There is always only one input link\n                in_link = in_node.links[0]\n                # Connect from_socket of the incoming link with all to_sockets of the out going links\n                for link in out_node.links:\n                    links.new(in_link.from_socket, link.to_socket)\n\n            # Finally remove the denoiser node\n            nodes.remove(denoiser_node)",
  "def set_world_background(color: List[float], strength: float = 1):\n    \"\"\" Sets the color of blenders world background\n\n    :param color: A three-dimensional list specifying the new color in floats.\n    :param strength: The strength of the emitted background light.\n    \"\"\"\n    world = bpy.context.scene.world\n    world.use_nodes = True\n    nodes = world.node_tree.nodes\n    links = world.node_tree.links\n\n    # Unlink any incoming link that would overwrite the default value\n    if len(nodes.get(\"Background\").inputs['Color'].links) > 0:\n        links.remove(nodes.get(\"Background\").inputs['Color'].links[0])\n\n    nodes.get(\"Background\").inputs['Strength'].default_value = strength\n    nodes.get(\"Background\").inputs['Color'].default_value = color + [1]",
  "def enable_experimental_features():\n    \"\"\" Enables experimental cycles features. \"\"\"\n    bpy.context.scene.cycles.feature_set = 'EXPERIMENTAL'",
  "def set_render_devices(use_only_cpu: bool = False, desired_gpu_device_type: Union[str, List[str]] = None,\n                       desired_gpu_ids: Union[int, List[int]] = None):\n    \"\"\" Configures the devices to use for rendering.\n\n    :param use_only_cpu: If True, only the cpu is used for rendering.\n    :param desired_gpu_device_type: One or multiple GPU device types to consider. If multiple are given,\n                                    the first available is used. Possible choices are [\"OPTIX\", \"CUDA\",\n                                    \"METAL\", \"HIP\"]. Default is [\"OPTIX\", \"CUDA\", \"HIP\"] on linux/windows and\n                                    [\"METAL\"] on supported Mac devices.\n    :param desired_gpu_ids: One or multiple GPU ids to specifically use. If none is given, all suitable GPUs are used.\n    \"\"\"\n    # Mark beginning of selection to avoid confusion when calling set_render_devices multiple times:\n    print(\"Selecting render devices...\")\n\n    if desired_gpu_device_type is None:\n        # If no gpu types are specified, use the default types based on the OS\n        if sys.platform == \"darwin\":\n            mac_version = platform.mac_ver()[0]\n            mac_version_numbers = [int(ele) for ele in mac_version.split(\".\")]\n            # On recent macs, use METAL, otherwise use cpu only\n            if (mac_version_numbers[0] == 12 and mac_version_numbers[1] >= 3) or mac_version_numbers[0] > 12:\n                desired_gpu_device_type = [\"METAL\"]\n            else:\n                desired_gpu_device_type = []\n        else:\n            # Define default for linux and windows\n            desired_gpu_device_type = [\"OPTIX\", \"CUDA\", \"HIP\"]\n    elif not isinstance(desired_gpu_device_type, list):\n        # Make sure it's a list\n        desired_gpu_device_type = [desired_gpu_device_type]\n\n    # Make sure desired_gpu_device_type is a list\n    if desired_gpu_ids is not None and not isinstance(desired_gpu_ids, list):\n        desired_gpu_ids = [desired_gpu_ids]\n\n    # Decide between gpu and cpu rendering\n    if not desired_gpu_device_type or use_only_cpu:\n        # Use only CPU\n        bpy.context.scene.cycles.device = \"CPU\"\n        bpy.context.preferences.addons['cycles'].preferences.compute_device_type = \"NONE\"\n        print(\"Using only the CPU for rendering\")\n    else:\n        # Use GPU\n        bpy.context.scene.cycles.device = \"GPU\"\n        preferences = bpy.context.preferences.addons['cycles'].preferences\n\n        # Go over all specified device types\n        found = False\n        for device_type in desired_gpu_device_type:\n            # Check if there are devices that support that type\n            devices = preferences.get_devices_for_type(device_type)\n            if devices:\n                # Set device type\n                bpy.context.preferences.addons['cycles'].preferences.compute_device_type = device_type\n                # Go over all devices with that type\n                found = False\n                for i, device in enumerate(devices):\n                    # Only use gpus with specified ids\n                    if desired_gpu_ids is None or i in desired_gpu_ids:\n                        print(f\"Device {device.name} of type {device.type} found and used.\")\n                        device.use = True\n                        found = True\n                    else:\n                        device.use = False\n\n                if not found:\n                    raise RuntimeError(f\"The specified gpu ids lead to no selected gpu at all. Valid gpu ids are \"\n                                       f\"{list(range(len(devices)))}\")\n\n                break\n\n        if not found:\n            bpy.context.scene.cycles.device = \"CPU\"\n            bpy.context.preferences.addons['cycles'].preferences.compute_device_type = \"NONE\"\n            print(\"Using only the CPU for rendering\")",
  "def render_nocs(output_dir: Optional[str] = None, file_prefix: str = \"nocs_\", output_key: str = \"nocs\",\n                return_data: bool = True, verbose: bool = False) -> Dict[str, List[np.ndarray]]:\n    \"\"\" Renders the Normalized Object Coordinate Space (NOCS).\n\n    Colors each object based on its local coordinates.\n    The coordinates [-1, 1] are mapped into the [0, 1] colorspace.\n    It is therefore, recommended that all local vertex coordinates are in range [-1, 1].\n    The world background is rendered transparent.\n\n    :param output_dir: The directory to write images to. If None is given, the temp dir is used.\n    :param file_prefix: The prefix to use for writing the images.\n    :param output_key: The key to use for registering the output.\n    :param return_data: Whether to load and return generated data.\n    :param verbose: If True, more details about the rendering process are printed.\n    :return: A dict containing one entry \"nocs\" which points to the list of rendered frames.\n    \"\"\"\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n\n    with UndoAfterExecution():\n        nocs_material = _NOCSRendererUtility.create_nocs_material()\n\n        # Set the NOCS material to all objects\n        for obj in get_all_blender_mesh_objects():\n            if len(obj.material_slots) > 0:\n                for i in range(len(obj.material_slots)):\n                    obj.data.materials[i] = nocs_material.blender_obj\n            else:\n                obj.data.materials.append(nocs_material.blender_obj)\n\n        # Make sure the background is black\n        set_world_background([0, 0, 0])\n\n        # Set all fast rendering parameters with only one ray per pixel\n        RendererUtility.render_init()\n        # the amount of samples must be one and there can not be any noise threshold\n        RendererUtility.set_max_amount_of_samples(1)\n        RendererUtility.set_noise_threshold(0)\n        RendererUtility.set_denoiser(None)\n        RendererUtility.set_light_bounces(1, 0, 0, 1, 0, 8, 0)\n        bpy.context.scene.cycles.filter_width = 0.0\n\n        # Use exr as output format, as it uses a linear colorspace and uses float16\n        RendererUtility.set_output_format(\"OPEN_EXR\", 16, enable_transparency=True)\n        # Render and ret\n        return RendererUtility.render(output_dir, file_prefix, output_key, load_keys={output_key},\n                                      return_data=return_data, keys_with_alpha_channel={output_key}, verbose=verbose)",
  "class _NOCSRendererUtility:\n\n    @staticmethod\n    def create_nocs_material() -> Material:\n        \"\"\" Creates the material which visualizes the NOCS.\n\n        :return: The created material.\n        \"\"\"\n        nocs_material: Material = MaterialLoaderUtility.create(\"nocs\")\n        tex_coords_node = nocs_material.new_node(\"ShaderNodeTexCoord\")\n\n        # Scale [-1, 1] to [-0.5, 0.5]\n        scale_node = nocs_material.new_node(\"ShaderNodeVectorMath\")\n        scale_node.operation = \"SCALE\"\n        scale_node.inputs[3].default_value = 0.5\n\n        # Move [-0.5, 0.5] to [0, 1]\n        add_node = nocs_material.new_node(\"ShaderNodeVectorMath\")\n        add_node.operation = \"ADD\"\n        add_node.inputs[1].default_value = [0.5, 0.5, 0.5]\n\n        # Link the three nodes\n        nocs_material.link(tex_coords_node.outputs[\"Object\"], scale_node.inputs[0])\n        nocs_material.link(scale_node.outputs[\"Vector\"], add_node.inputs[0])\n\n        # Link to output node\n        output_node = nocs_material.get_the_one_node_with_type('OutputMaterial')\n        nocs_material.link(add_node.outputs[\"Vector\"], output_node.inputs['Surface'])\n        return nocs_material",
  "def create_nocs_material() -> Material:\n        \"\"\" Creates the material which visualizes the NOCS.\n\n        :return: The created material.\n        \"\"\"\n        nocs_material: Material = MaterialLoaderUtility.create(\"nocs\")\n        tex_coords_node = nocs_material.new_node(\"ShaderNodeTexCoord\")\n\n        # Scale [-1, 1] to [-0.5, 0.5]\n        scale_node = nocs_material.new_node(\"ShaderNodeVectorMath\")\n        scale_node.operation = \"SCALE\"\n        scale_node.inputs[3].default_value = 0.5\n\n        # Move [-0.5, 0.5] to [0, 1]\n        add_node = nocs_material.new_node(\"ShaderNodeVectorMath\")\n        add_node.operation = \"ADD\"\n        add_node.inputs[1].default_value = [0.5, 0.5, 0.5]\n\n        # Link the three nodes\n        nocs_material.link(tex_coords_node.outputs[\"Object\"], scale_node.inputs[0])\n        nocs_material.link(scale_node.outputs[\"Vector\"], add_node.inputs[0])\n\n        # Link to output node\n        output_node = nocs_material.get_the_one_node_with_type('OutputMaterial')\n        nocs_material.link(add_node.outputs[\"Vector\"], output_node.inputs['Surface'])\n        return nocs_material",
  "def perform_obstacle_in_view_check(cam2world_matrix: Union[Matrix, np.ndarray], proximity_checks: dict,\n                                   bvh_tree: BVHTree, sqrt_number_of_rays: int = 10) -> bool:\n    \"\"\" Check if there are obstacles in front of the camera which are too far or too close based on the given\n        proximity_checks.\n\n    :param cam2world_matrix: Transformation matrix that transforms from the camera space to the world space.\n    :param proximity_checks: A dictionary containing operators (e.g. avg, min) as keys and as values dictionaries\n                             containing thresholds in the form of {\"min\": 1.0, \"max\":4.0} or just the numerical\n                             threshold in case of max or min. The operators are combined in conjunction\n                             (i.e boolean AND). This can also be used to avoid the background in images, with the\n                             no_background: True option.\n    :param bvh_tree: A bvh tree containing all objects that should be considered here.\n    :param sqrt_number_of_rays: The square root of the number of rays which will be used to determine the\n                                visible objects.\n    :return: True, if the given camera pose does not violate any of the specified proximity_checks.\n    \"\"\"\n    if not proximity_checks:  # if no checks are in the settings all positions are accepted\n        return True\n\n    cam2world_matrix = Matrix(cam2world_matrix)\n\n    cam_ob = bpy.context.scene.camera\n    cam = cam_ob.data\n    # Get position of the corners of the near plane\n    frame = cam.view_frame(scene=bpy.context.scene)\n    # Bring to world space\n    frame = [cam2world_matrix @ v for v in frame]\n\n    # Compute vectors along both sides of the plane\n    vec_x = frame[1] - frame[0]\n    vec_y = frame[3] - frame[0]\n\n    sum_value = 0.0\n    sum_sq = 0.0\n\n    range_distance = sys.float_info.max\n\n    # Input validation\n    for operator in proximity_checks:\n        if operator in [\"min\", \"max\"] and not isinstance(proximity_checks[operator], numbers.Number):\n            raise RuntimeError(\"Threshold must be a number in perform_obstacle_in_view_check\")\n        if operator in [\"avg\", \"var\"]:\n            if \"min\" not in proximity_checks[operator] or \"max\" not in proximity_checks[operator]:\n                raise RuntimeError(\"Please specify the accepted interval for the avg and var operators \"\n                                   \"in perform_obstacle_in_view_check\")\n            if not isinstance(proximity_checks[operator][\"min\"], numbers.Number) or \\\n                    not isinstance(proximity_checks[operator][\"max\"], numbers.Number):\n                raise ValueError(\"Threshold must be a number in perform_obstacle_in_view_check\")\n\n    # If there are no average or variance operators, we can decrease the ray range distance for efficiency\n    if \"avg\" not in proximity_checks and \"var\" not in proximity_checks:\n        if \"max\" in proximity_checks:\n            # Cap distance values at a value slightly higher than the max threshold\n            range_distance = proximity_checks[\"max\"] + 1.0\n        else:\n            range_distance = proximity_checks[\"min\"]\n\n    no_range_distance = False\n    if \"no_background\" in proximity_checks and proximity_checks[\"no_background\"]:\n        # when no background is on, it can not be combined with a reduced range distance\n        no_range_distance = True\n\n    # Go in discrete grid-like steps over plane\n    position = cam2world_matrix.to_translation()\n    for x in range(0, sqrt_number_of_rays):\n        for y in range(0, sqrt_number_of_rays):\n            # Compute current point on plane\n            end = frame[0] + vec_x * x / float(sqrt_number_of_rays - 1) + vec_y * y / float(sqrt_number_of_rays - 1)\n            # Send ray from the camera position through the current point on the plane\n            if no_range_distance:\n                _, _, _, dist = bvh_tree.ray_cast(position, end - position)\n            else:\n                _, _, _, dist = bvh_tree.ray_cast(position, end - position, range_distance)\n\n            # Check if something was hit and how far it is away\n            if dist is not None:\n                if \"min\" in proximity_checks and dist <= proximity_checks[\"min\"]:\n                    return False\n                if \"max\" in proximity_checks and dist >= proximity_checks[\"max\"]:\n                    return False\n                if \"avg\" in proximity_checks:\n                    sum_value += dist\n                if \"var\" in proximity_checks:\n                    if not \"avg\" in proximity_checks:\n                        sum_value += dist\n                    sum_sq += dist * dist\n            elif \"no_background\" in proximity_checks and proximity_checks[\"no_background\"]:\n                return False\n\n    if \"avg\" in proximity_checks:\n        avg = sum_value / (sqrt_number_of_rays * sqrt_number_of_rays)\n        # Check that the average distance is not within the accepted interval\n        if avg >= proximity_checks[\"avg\"][\"max\"] or avg <= proximity_checks[\"avg\"][\"min\"]:\n            return False\n\n    if \"var\" in proximity_checks:\n        if not \"avg\" in proximity_checks:\n            avg = sum_value / (sqrt_number_of_rays * sqrt_number_of_rays)\n        sq_avg = avg * avg\n\n        avg_sq = sum_sq / (sqrt_number_of_rays * sqrt_number_of_rays)\n\n        var = avg_sq - sq_avg\n        # Check that the variance value of the distance is not within the accepted interval\n        if var >= proximity_checks[\"var\"][\"max\"] or var <= proximity_checks[\"var\"][\"min\"]:\n            return False\n\n    return True",
  "def visible_objects(cam2world_matrix: Union[Matrix, np.ndarray], sqrt_number_of_rays: int = 10) -> Set[MeshObject]:\n    \"\"\" Returns a set of objects visible from the given camera pose.\n\n    Sends a grid of rays through the camera frame and returns all objects hit by at least one ray.\n\n    :param cam2world_matrix: The world matrix which describes the camera orientation to check.\n    :param sqrt_number_of_rays: The square root of the number of rays which will be used to determine the\n                                visible objects.\n    :return: A set of objects visible hit by the sent rays.\n    \"\"\"\n    cam2world_matrix = Matrix(cam2world_matrix)\n\n    visible_objects_set = set()\n    cam_ob = bpy.context.scene.camera\n    cam = cam_ob.data\n\n    # Get position of the corners of the near plane\n    frame = cam.view_frame(scene=bpy.context.scene)\n    # Bring to world space\n    frame = [cam2world_matrix @ v for v in frame]\n\n    # Compute vectors along both sides of the plane\n    vec_x = frame[1] - frame[0]\n    vec_y = frame[3] - frame[0]\n\n    # Go in discrete grid-like steps over plane\n    position = cam2world_matrix.to_translation()\n    for x in range(0, sqrt_number_of_rays):\n        for y in range(0, sqrt_number_of_rays):\n            # Compute current point on plane\n            end = frame[0] + vec_x * x / float(sqrt_number_of_rays - 1) + vec_y * y / float(sqrt_number_of_rays - 1)\n            # Send ray from the camera position through the current point on the plane\n            _, _, _, _, hit_object, _ = bpy.context.scene.ray_cast(bpy.context.evaluated_depsgraph_get(),\n                                                                   position, end - position)\n            # Add hit object to set\n            if hit_object:\n                visible_objects_set.add(MeshObject(hit_object))\n\n    return visible_objects_set",
  "def scene_coverage_score(cam2world_matrix: Union[Matrix, np.ndarray], special_objects: list = None,\n                         special_objects_weight: float = 2, sqrt_number_of_rays: int = 10) -> float:\n    \"\"\" Evaluate the interestingness/coverage of the scene.\n\n    This module tries to look at as many objects at possible, this might lead to\n    a focus on the same objects from similar angles.\n\n    Only for SUNCG and 3D Front:\n        The least interesting objects: walls, ceilings, floors.\n\n    :param cam2world_matrix: The world matrix which describes the camera pose to check.\n    :param special_objects: Objects that weights differently in calculating whether the scene is interesting or not,\n                            uses the coarse_grained_class or if not SUNCG, 3D Front, the category_id.\n    :param special_objects_weight: Weighting factor for more special objects, used to estimate how interesting the\n                                   scene is. Default: 2.0.\n    :param sqrt_number_of_rays: The square root of the number of rays which will be used to determine the\n                                visible objects.\n    :return: the scoring of the scene.\n    \"\"\"\n    cam2world_matrix = Matrix(cam2world_matrix)\n\n    if special_objects is None:\n        special_objects = []\n    cam_ob = bpy.context.scene.camera\n    cam = cam_ob.data\n\n    num_of_rays = sqrt_number_of_rays * sqrt_number_of_rays\n    score = 0.0\n    objects_hit: defaultdict = defaultdict(int)\n\n    # Get position of the corners of the near plane\n    frame = cam.view_frame(scene=bpy.context.scene)\n    # Bring to world space\n    frame = [cam2world_matrix @ v for v in frame]\n\n    # Compute vectors along both sides of the plane\n    vec_x = frame[1] - frame[0]\n    vec_y = frame[3] - frame[0]\n\n    # Go in discrete grid-like steps over plane\n    position = cam2world_matrix.to_translation()\n    for x in range(0, sqrt_number_of_rays):\n        for y in range(0, sqrt_number_of_rays):\n            # Compute current point on plane\n            end = frame[0] + vec_x * x / float(sqrt_number_of_rays - 1) + vec_y * y / float(sqrt_number_of_rays - 1)\n            # Send ray from the camera position through the current point on the plane\n            hit, _, _, _, hit_object, _ = bpy.context.scene.ray_cast(bpy.context.evaluated_depsgraph_get(),\n                                                                     position, end - position)\n\n            if hit:\n                is_of_special_dataset = \"is_suncg\" in hit_object or \"is_3d_front\" in hit_object\n                is_suncg_object = \"suncg_type\" in hit_object and hit_object[\"suncg_type\"] == \"Object\"\n                is_front_3d_object = \"3D_future_type\" in hit_object and hit_object[\"3D_future_type\"] == \"Object\"\n                if is_of_special_dataset and is_suncg_object or is_of_special_dataset and is_front_3d_object:\n                    # calculate the score based on the type of the object,\n                    # wall, floor and ceiling objects have 0 score\n                    if \"coarse_grained_class\" in hit_object:\n                        object_class = hit_object[\"coarse_grained_class\"]\n                        objects_hit[object_class] += 1\n                        if object_class in special_objects:\n                            score += special_objects_weight\n                        else:\n                            score += 1\n                    else:\n                        score += 1\n                elif \"category_id\" in hit_object:\n                    object_class = hit_object[\"category_id\"]\n                    if object_class in special_objects:\n                        score += special_objects_weight\n                    else:\n                        score += 1\n                    objects_hit[object_class] += 1\n                else:\n                    objects_hit[hit_object] += 1\n                    score += 1\n    # For a scene with three different objects, the starting variance is 1.0, increases/decreases by '1/3' for\n    # each object more/less, excluding floor, ceiling and walls\n    scene_variance = len(objects_hit) / 3.0\n    for object_hit_value in objects_hit.values():\n        # For an object taking half of the scene, the scene_variance is halved, this penalizes non-even\n        # distribution of the objects in the scene\n        scene_variance *= 1.0 - object_hit_value / float(num_of_rays)\n    score = scene_variance * (score / float(num_of_rays))\n    return score",
  "def decrease_interest_score(interest_score: float, min_interest_score: float, interest_score_step: float):\n    \"\"\" Decreases the interest scores in the given interval\n\n    :param interest_score: The current interest score.\n    :param min_interest_score: The minimum desired interest scores.\n    :param interest_score_step: The step size in which the interest score should be reduced.\n    :return: Returns the new interest score, and True/False if minimum has not been reached.\n    \"\"\"\n    if interest_score <= min_interest_score:\n        return False, interest_score\n    return True, interest_score - interest_score_step",
  "def check_novel_pose(cam2world_matrix: Union[Matrix, np.ndarray], existing_poses: List[Union[Matrix, np.ndarray]],\n                     check_pose_novelty_rot: bool, check_pose_novelty_translation: bool,\n                     min_var_diff_rot: float = -1, min_var_diff_translation: float = -1):\n    \"\"\" Checks if a newly sampled pose is novel based on variance checks.\n\n    :param cam2world_matrix: The world matrix which describes the camera pose to check.\n    :param existing_poses: The list of already sampled valid poses.\n    :param check_pose_novelty_rot: Checks that a sampled new pose is novel with respect to the rotation component.\n    :param check_pose_novelty_translation: Checks that a sampled new pose is novel with respect to the\n                                           translation component.\n    :param min_var_diff_rot: Considers a pose novel if it increases the variance of the rotation component of all\n                             poses sampled by this parameter's value in percentage. If set to -1, then it would only\n                             check that the variance is increased. Default: sys.float_info.min.\n    :param min_var_diff_translation: Same as min_var_diff_rot but for translation. If set to -1, then it would only\n                                     check that the variance is increased. Default: sys.float_info.min.\n    :return: True, if the given pose is novel.\n    \"\"\"\n    def _variance_constraint(array, new_val, old_var, diff_threshold, mode):\n        array.append(new_val)\n        var = np.var(array)\n\n        if var < old_var:\n            array.pop()\n            return False\n\n        diff = ((var - old_var) / old_var) * 100.0\n        print(f\"Variance difference {mode}: {diff}\")\n        if diff < diff_threshold:  # Check if the variance increased sufficiently\n            array.pop()\n            return False\n\n        return True\n\n    if len(existing_poses) > 0:  # First pose is always novel\n        cam2world_matrix = Matrix(cam2world_matrix)\n        if check_pose_novelty_rot:\n            rotations = [Matrix(pose).to_euler() for pose in existing_poses]\n            var_rot = np.var(rotations)\n\n            if not _variance_constraint(rotations, cam2world_matrix.to_euler(), var_rot, min_var_diff_rot, \"rotation\"):\n                return False\n\n        if check_pose_novelty_translation:\n            translations = [Matrix(pose).to_translation() for pose in existing_poses]\n            var_translation = np.var(translations)\n\n            if not _variance_constraint(translations, cam2world_matrix.to_translation(), var_translation,\n                                        min_var_diff_translation, \"translation\"):\n                return False\n\n    return True",
  "def _variance_constraint(array, new_val, old_var, diff_threshold, mode):\n        array.append(new_val)\n        var = np.var(array)\n\n        if var < old_var:\n            array.pop()\n            return False\n\n        diff = ((var - old_var) / old_var) * 100.0\n        print(f\"Variance difference {mode}: {diff}\")\n        if diff < diff_threshold:  # Check if the variance increased sufficiently\n            array.pop()\n            return False\n\n        return True",
  "def set_lens_distortion(k1: float, k2: float, k3: float = 0.0, p1: float = 0.0, p2: float = 0.0,\n                        use_global_storage: bool = False) -> np.ndarray:\n    \"\"\"\n    This function applies the lens distortion parameters to obtain an distorted-to-undistorted mapping for all\n    natural pixels coordinates of the goal distorted image into the real pixel coordinates of the undistorted\n    Blender image. Since such a mapping usually yields void image areas, this function suggests a different\n    (usually higher) image resolution for the generated Blender image. Eventually, the function\n    `apply_lens_distortion` will make us of this image to fill in the goal distorted image with valid color\n    values by interpolation. Note that when adapting the internal image resolution demanded from Blender, the\n    camera main point (cx,cy) of the K intrinsic matrix is (internally and temporarily) shifted.\n\n    This function has to be used together with bproc.postprocessing.apply_lens_distortion(), else only the\n    resolution is increased but the image(s) will not be distorted.\n\n    :param k1: First radial distortion parameter (of 3rd degree in radial distance) as defined\n            by the undistorted-to-distorted Brown-Conrady lens distortion model, which is conform to\n            the current DLR CalLab/OpenCV/Bouguet/Kalibr implementations.\n            Note that undistorted-to-distorted means that the distortion parameters are multiplied\n            by undistorted, normalized camera projections to yield distorted projections, that are in\n            turn digitized by the intrinsic camera matrix.\n    :param k2: Second radial distortion parameter (of 5th degree in radial distance) as defined\n            by the undistorted-to-distorted Brown-Conrady lens distortion model, which is conform\n            to the current DLR CalLab/OpenCV/Bouguet/Kalibr implementations.\n    :param k3: Third radial distortion parameter (of 7th degree in radial distance) as defined\n            by the undistorted-to-distorted Brown-Conrady lens distortion model, which is conform to\n            the current DLR CalLab/OpenCV/Bouguet/Kalibr implementations.\n            The use of this parameter is discouraged unless the angular field of view is too high,\n            rendering it necessary, and the parameter allows for a distorted projection in the whole\n            sensor size (which isn't always given by features-driven camera calibration).\n    :param p1: First decentering distortion parameter as defined by the undistorted-to-distorted\n            Brown-Conrady lens distortion model in (Brown, 1965; Brown, 1971; Weng et al., 1992) and is\n            comform to the current DLR CalLab implementation. Note that OpenCV/Bouguet/Kalibr permute them.\n            This parameter shares one degree of freedom (j1) with p2; as a consequence, either both\n            parameters are given or none. The use of these parameters is discouraged since either current\n            cameras do not need them or their potential accuracy gain is negligible w.r.t. image processing.\n    :param p2: Second decentering distortion parameter as defined by the undistorted-to-distorted\n            Brown-Conrady lens distortion model in (Brown, 1965; Brown, 1971; Weng et al., 1992) and is\n            comform to the current DLR CalLab implementation. Note that OpenCV/Bouguet/Kalibr permute them.\n            This parameter shares one degree of freedom (j1) with p1; as a consequence, either both\n            parameters are given or none. The use of these parameters is discouraged since either current\n            cameras do not need them or their potential accuracy gain is negligible w.r.t. image processing.\n    :use_global_storage: Whether to save the mapping coordinates and original image resolution in a global\n                         storage (backward compat for configs)\n    :return: mapping coordinates from distorted to undistorted image pixels\n    \"\"\"\n    if all(v == 0.0 for v in [k1, k2, k3, p1, p2]):\n        raise Exception(\"All given lens distortion parameters (k1, k2, k3, p1, p2) are zero.\")\n\n    # save the original image resolution (desired output resolution)\n    original_image_resolution = (bpy.context.scene.render.resolution_y, bpy.context.scene.render.resolution_x)\n\n    # get the current K matrix (skew==0 in Blender)\n    camera_K_matrix = CameraUtility.get_intrinsics_as_K_matrix()\n    fx, fy = camera_K_matrix[0][0], camera_K_matrix[1][1]\n    cx, cy = camera_K_matrix[0][2], camera_K_matrix[1][2]\n\n    # Get row,column image coordinates for all pixels for row-wise image flattening\n    # The center of the upper-left pixel has coordinates [0,0] both in DLR CalDe and python/scipy\n    row = np.repeat(np.arange(0, original_image_resolution[0]), original_image_resolution[1])\n    column = np.tile(np.arange(0, original_image_resolution[1]), original_image_resolution[0])\n\n    # P_und is the undistorted pinhole projection at z==1 of all image pixels\n    P_und = np.linalg.inv(camera_K_matrix) @ np.vstack((column, row, np.ones(np.prod(original_image_resolution[:2]))))\n\n    # P_und are then distorted by the lens, i.e. P_dis = dis(P_und)\n    # => Find mapping I_dis(row,column) -> I_und(float,float)\n    #\n    # We aim at finding the brightness for every discrete pixel of the\n    # generated distorted image. In the original undistorted image these\n    # are located at real coordinates to be calculated. After that we can\n    # interpolate on the original undistorted image.\n    # Since dis() cannot be inverted, we iterate (up to ~10 times\n    # depending on the AOV and the distortion):\n    # 1) assume P_und~=P_dis\n    # 2) distort()\n    # 3) estimate distance between dist(P_und) and P_dis\n    # 4) subtract this distance from the estimated P_und,\n    #    perhaps with a factor (>1 for accel, <1 for stability at unstable distortion regions)\n    # 5) repeat until P_dis ~ dist(P_und)\n    # This works because translations in _dis and _und are approx. equivariant\n    # and the mapping is (hopefully) injective (1:1).\n    #\n    # An alternative, non-iterative approach is P_dis(float,float)=dis(P_und(row,column))\n    # and then interpolate on an irregular grid of distorted points. This is faster\n    # when generating the mapping matrix but much slower in inference.\n\n    # Init dist at undist\n    x = P_und[0, :].copy()\n    y = P_und[1, :].copy()\n    res = [1e3]\n    it = 0\n    while res[-1] > 0.15:\n        r2 = np.square(x) + np.square(y)\n        radial_part = 1 + k1 * r2 + k2 * r2 * r2 + k3 * r2 * r2 * r2\n        x_ = x * radial_part + 2 * p2 * x * y + p1 * (r2 + 2 * np.square(x))\n        y_ = y * radial_part + 2 * p1 * x * y + p2 * (r2 + 2 * np.square(y))\n\n        error = np.max(np.hypot(fx * (x_ - P_und[0, :]), fy * (y_ - P_und[1, :])))\n        res.append(error)\n        it += 1\n\n        # Take action if the optimization stalls or gets unstable\n        # (distortion models are tricky if badly parameterized, especially in outer regions)\n        if (it > 1) and (res[-1] > res[-2] * .99999):\n            print(\"The residual for the worst distorted pixel got unstable/stalled.\")\n            # factor *= .5\n            if it > 1e3:\n                raise Exception(\n                    \"The iterative distortion algorithm is unstable/stalled after 1000 iterations.\")\n            if error > 1e9:\n                print(\"Some (corner) pixels of the desired image are not defined by the used lens distortion model.\")\n                print(\"We invite you to double-check your distortion model.\")\n                print(\"The parameters k3,p1,p2 can easily overshoot for regions where the calibration \"\n                      \"software had no datapoints.\")\n                print(\"You can either:\")\n                print(\"- take more projections (ideally image-filling) at the image corners and repeat calibration,\")\n                print(\"- reduce the # of released parameters to calibrate to k1,k2, or\")\n                print(\"- reduce the target image size (subtract some lines and columns from the desired resolution\")\n                print(\"  and subtract at most that number of lines and columns from the main point location).\")\n                print(\"BlenderProc will not generate incomplete images with void regions since these are not \"\n                      \"useful for ML (data leakage).\")\n                print(\"For that, you can use the Matlab code in robotic.de/callab, which robustifies against \"\n                      \"these unstable pixels.\")\n                raise Exception(\"The iterative distortion algorithm is unstable.\")\n\n        # update undistorted projection\n        x -= x_ - P_und[0, :]  # * factor\n        y -= y_ - P_und[1, :]  # * factor\n\n    # u and v are now the pixel coordinates on the undistorted image that\n    # will distort into the row,column coordinates of the distorted image\n    u = fx * x + cx\n    v = fy * y + cy\n\n    # Stacking this way for the interpolation in the undistorted image array\n    mapping_coords = np.vstack([v, u])\n\n    # Find out the image resolution needed from Blender to generate filled-in distorted images of the desired resolution\n    min_und_column_needed = np.floor(np.min(u))\n    max_und_column_needed = np.ceil(np.max(u))\n    min_und_row_needed = np.floor(np.min(v))\n    max_und_row_needed = np.ceil(np.max(v))\n    columns_needed = max_und_column_needed + 1 - min_und_column_needed\n    rows_needed = max_und_row_needed + 1 - min_und_row_needed\n    cx_new = cx - min_und_column_needed\n    cy_new = cy - min_und_row_needed\n    # To avoid spline boundary approximations at the border pixels ('mode' in map_coordinates() )\n    columns_needed += 2\n    rows_needed += 2\n    cx_new += 1\n    cy_new += 1\n    # suggested resolution for Blender image generation\n    new_image_resolution = np.array([columns_needed, rows_needed], dtype=int)\n\n    # Adapt/shift the mapping function coordinates to the new_image_resolution resolution\n    # (if we didn't, the mapping would only be valid for same resolution mapping)\n    # (same resolution mapping yields undesired void image areas)\n    mapping_coords[0, :] += cy_new - cy\n    mapping_coords[1, :] += cx_new - cx\n\n    camera_changed_K_matrix = CameraUtility.get_intrinsics_as_K_matrix()\n    # update cx and cy in the K matrix\n    camera_changed_K_matrix[0, 2] = cx_new\n    camera_changed_K_matrix[1, 2] = cy_new\n\n    # reuse the values, which have been set before\n    clip_start = bpy.context.scene.camera.data.clip_start\n    clip_end = bpy.context.scene.camera.data.clip_end\n\n    CameraUtility.set_intrinsics_from_K_matrix(camera_changed_K_matrix, new_image_resolution[0],\n                                               new_image_resolution[1], clip_start, clip_end)\n\n    if use_global_storage:\n        GlobalStorage.set(\"_lens_distortion_is_used\", {\"mapping_coords\": mapping_coords,\n                                                    \"original_image_res\": original_image_resolution})\n    return mapping_coords",
  "def apply_lens_distortion(image: Union[List[np.ndarray], np.ndarray],\n                          mapping_coords: Optional[np.ndarray] = None,\n                          orig_res_x: Optional[int] = None,\n                          orig_res_y: Optional[int] = None,\n                          use_interpolation: bool = True) -> Union[List[np.ndarray], np.ndarray]:\n    \"\"\"\n    This functions applies the lens distortion mapping that needs to be precalculated by\n    `bproc.camera.set_lens_distortion()`.\n\n    Without calling this function the `set_lens_distortion` fct. only increases the image resolution and\n    changes the K matrix of the camera.\n\n    :param image: a list of images or an image to be distorted\n    :param mapping_coords: an array of pixel mappings from undistorted to distorted image\n    :param orig_res_x: original and output width resolution of the image\n    :param orig_res_y: original and output height resolution of the image\n    :param use_interpolation: if this is True, for each pixel an interpolation will be performed, if this is false\n                              the nearest pixel will be used\n    :return: a list of images or an image that have been distorted, now in the desired (original) resolution\n    \"\"\"\n\n    if mapping_coords is None or orig_res_x is None or orig_res_y is None:\n        # if lens distortion was used apply it now\n        if GlobalStorage.is_in_storage(\"_lens_distortion_is_used\"):\n            # extract the necessary params from the GlobalStorage\n            content = GlobalStorage.get(\"_lens_distortion_is_used\")\n            mapping_coords = content[\"mapping_coords\"]\n            orig_res_y, orig_res_x = content[\"original_image_res\"]\n        else:\n            raise Exception(\"Applying of a lens distortion is only possible after calling \"\n                            \"bproc.camera.set_lens_distortion(...) and pass 'mapping_coords' and \"\n                            \"'orig_res_x' + 'orig_res_x' to bproc.postprocessing.apply_lens_distortion(...). \"\n                            \"Previously this could also have been done via the CameraInterface module, \"\n                            \"see the example on lens_distortion.\")\n    interpolation_order = 2 if use_interpolation else 0\n\n    def _internal_apply(input_image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Applies the distortion to the input image\n        :param input_image: input image, which will be distorted\n        :return: distorted input image\n        \"\"\"\n        amount_of_output_channels = 1\n        if len(input_image.shape) == 3:\n            amount_of_output_channels = input_image.shape[2]\n        image_distorted = np.zeros((orig_res_y, orig_res_x, amount_of_output_channels))\n        used_dtpye = input_image.dtype\n        data = input_image.astype(np.float)\n        # Forward mapping in order to distort the undistorted image coordinates\n        # and reshape the arrays into the image shape grid.\n        # The reference frame for coords is as in DLR CalDe etc. (the upper-left pixel center is at [0,0])\n        for i in range(image_distorted.shape[2]):\n            if len(input_image.shape) == 3:\n                image_distorted[:, :, i] = np.reshape(map_coordinates(data[:, :, i], mapping_coords,\n                                                                      order=interpolation_order,\n                                                                      mode='nearest'), image_distorted[:, :, i].shape)\n            else:\n                image_distorted[:, :, i] = np.reshape(map_coordinates(data, mapping_coords, order=interpolation_order,\n                                                                      mode='nearest'),\n                                                      image_distorted[:, :, i].shape)\n        # Other options are:\n        # - map_coordinates() in all channels at the same time (turns out to be slower)\n        # - use torch.nn.functional.grid_sample() instead to do it on the GPU (even in batches)\n\n        if used_dtpye == np.uint8:\n            image_distorted = np.clip(image_distorted, 0, 255)\n        data = image_distorted.astype(used_dtpye)\n        if len(input_image.shape) == 2:\n            return data[:, :, 0]\n        return data\n    if isinstance(image, list):\n        return [_internal_apply(img) for img in image]\n    if isinstance(image, np.ndarray):\n        return _internal_apply(image)\n    raise Exception(f\"This type can not be worked with here: {type(image)}, only \"\n                    f\"np.ndarray or list of np.ndarray are supported\")",
  "def set_camera_parameters_from_config_file(camera_intrinsics_file_path: str, read_the_extrinsics: bool = False,\n                                           camera_index: int = 0) -> Tuple[int, int, np.ndarray]:\n    \"\"\"\n    This function sets the camera intrinsic parameters based on a config file, currently it only supports the\n    DLR-RMC camera calibration file format used in the \"DLR CalDe and DLR CalLab\" camera calibration toolbox.\n    The calibration file allows to use multiple cameras, but only one can be used inside of BlenderProc per run.\n\n    :param camera_intrinsics_file_path: Path to the calibration file\n    :param camera_index: Used camera index\n    :return: mapping coordinates from distorted to undistorted image pixels, as returned from set_lens_distortion()\n    \"\"\"\n    if not os.path.exists(camera_intrinsics_file_path):\n        raise Exception(f\"The camera intrinsics file does not exist: {camera_intrinsics_file_path}\")\n\n    def _is_number(value: str) -> bool:\n        # check if the given string value is a digit (float or int)\n        if value.isnumeric():\n            return True\n        try:\n            float(value)\n            return True\n        except ValueError:\n            return False\n\n    with open(camera_intrinsics_file_path, \"r\", encoding=\"utf-8\") as file:\n        final_lines = []\n        for line in file.readlines():\n            line = line.strip()\n            if \"#\" in line:\n                line = line[:line.find(\"#\")].strip()\n            if \"[\" in line and \"]\" in line:\n                # add commas in between the numbers, which are seperated by spaces\n                line = line.replace(\";\", \" ; \")\n                line = line.replace(\"[\", \"[ \")\n                line = line.replace(\"]\", \" ]\")\n                for i in range(15):\n                    line = line.replace(\"  \", \" \")\n                elements = line.split(\" \")\n                if elements:\n                    final_elements = []\n                    # add in commas between two elements if the current and the next element are numbers\n                    for i in range(len(elements) - 1):\n                        final_elements.append(elements[i])\n                        if _is_number(elements[i]) and _is_number(elements[i + 1]):\n                            final_elements.append(\",\")\n                    final_elements.append(elements[-1])\n                    line = \" \".join(final_elements)\n            if \";\" in line and \"[\" in line and \"]\" in line:\n                # this line contains a matrix\n                line = line.replace(\"[\", \"[[\").replace(\"]\", \"]]\").replace(\";\", \"], [\")\n            # convert it to yaml format\n            if line.startswith(\"camera.\"):\n                current_nr = line[len(\"camera.\"):line.find(\".\", len(\"camera.\"))]\n                if _is_number(current_nr):\n                    # remove all lines which are not focused around the selected camera index\n                    if int(current_nr) != camera_index:\n                        line = \"\"\n                else:\n                    # remove lines which are not specified to a certain camera\n                    line = \"\"\n            line = line.replace(f\"camera.{camera_index}.\", \"\")\n            if line.count(\"=\") == 1:\n                line = f'\"{line.split(\"=\")[0]}\"= {line.split(\"=\")[1]}'\n            else:\n                line = \"\"\n            line = line.replace(\"=\", \":\")\n            if line:\n                final_lines.append(line)\n\n    extracted_camera_parameters = yaml.safe_load(\"\\n\".join(final_lines))\n    print(f\"Interpreted intrinsics from DLR-RMC camera calibration file: {extracted_camera_parameters}\")\n    # check version and origin parameters\n    if extracted_camera_parameters.get(\"version\") is None or extracted_camera_parameters[\"version\"] != 2:\n        if extracted_camera_parameters.get(\"version\") is None:\n            raise RuntimeError(\"The version tag is not set in the DLR-RMC camera calibration file!\")\n        raise RuntimeError(f\"Only version 2 is supported for the DLR-RMC camera calibration \"\n                           f\"file, not {extracted_camera_parameters['version']}\")\n    if extracted_camera_parameters.get(\"origin\") is None or extracted_camera_parameters[\"origin\"] != \"center\":\n        raise RuntimeError(\"The origin in the DLR-RMC camera calibration file has to be defined and \"\n                           \"set to center for BlenderProc distortion to work.\")\n    # set intrinsics based on the yaml-read matrix called A and the yaml-read camera image size\n    CameraUtility.set_intrinsics_from_K_matrix(extracted_camera_parameters.get(\"A\"),\n                                               extracted_camera_parameters[\"width\"],\n                                               extracted_camera_parameters[\"height\"])\n    # setup the lens distortion and adapt intrinsics so that it can be later used in the PostProcessing\n    mapping_coords = set_lens_distortion(extracted_camera_parameters[\"k1\"], extracted_camera_parameters.get(\"k2\", 0.0),\n                                         extracted_camera_parameters.get(\"k3\", 0.0),\n                                         extracted_camera_parameters.get(\"p1\", 0.0),\n                                         extracted_camera_parameters.get(\"p2\", 0.0))\n    if read_the_extrinsics:\n        cam2world = np.eye(4)\n        cam2world[:3, :3] = np.array(extracted_camera_parameters[\"R\"])\n        cam2world[:3, 3] = np.array(extracted_camera_parameters[\"T\"])\n        cam2world = change_source_coordinate_frame_of_transformation_matrix(cam2world, [\"X\", \"-Y\", \"-Z\"])\n        CameraUtility.add_camera_pose(cam2world)\n    return extracted_camera_parameters[\"width\"], extracted_camera_parameters[\"height\"], mapping_coords",
  "def _internal_apply(input_image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Applies the distortion to the input image\n        :param input_image: input image, which will be distorted\n        :return: distorted input image\n        \"\"\"\n        amount_of_output_channels = 1\n        if len(input_image.shape) == 3:\n            amount_of_output_channels = input_image.shape[2]\n        image_distorted = np.zeros((orig_res_y, orig_res_x, amount_of_output_channels))\n        used_dtpye = input_image.dtype\n        data = input_image.astype(np.float)\n        # Forward mapping in order to distort the undistorted image coordinates\n        # and reshape the arrays into the image shape grid.\n        # The reference frame for coords is as in DLR CalDe etc. (the upper-left pixel center is at [0,0])\n        for i in range(image_distorted.shape[2]):\n            if len(input_image.shape) == 3:\n                image_distorted[:, :, i] = np.reshape(map_coordinates(data[:, :, i], mapping_coords,\n                                                                      order=interpolation_order,\n                                                                      mode='nearest'), image_distorted[:, :, i].shape)\n            else:\n                image_distorted[:, :, i] = np.reshape(map_coordinates(data, mapping_coords, order=interpolation_order,\n                                                                      mode='nearest'),\n                                                      image_distorted[:, :, i].shape)\n        # Other options are:\n        # - map_coordinates() in all channels at the same time (turns out to be slower)\n        # - use torch.nn.functional.grid_sample() instead to do it on the GPU (even in batches)\n\n        if used_dtpye == np.uint8:\n            image_distorted = np.clip(image_distorted, 0, 255)\n        data = image_distorted.astype(used_dtpye)\n        if len(input_image.shape) == 2:\n            return data[:, :, 0]\n        return data",
  "def _is_number(value: str) -> bool:\n        # check if the given string value is a digit (float or int)\n        if value.isnumeric():\n            return True\n        try:\n            float(value)\n            return True\n        except ValueError:\n            return False",
  "def add_camera_pose(cam2world_matrix: Union[np.ndarray, Matrix], frame: Optional[int] = None) -> int:\n    \"\"\" Sets a new camera pose to a new or existing frame\n\n    :param cam2world_matrix: The transformation matrix from camera to world coordinate system\n    :param frame: Optional, the frame to set the camera pose to.\n    :return: The frame to which the pose has been set.\n    \"\"\"\n    if not isinstance(cam2world_matrix, Matrix):\n        cam2world_matrix = Matrix(cam2world_matrix)\n\n    # Set cam2world_matrix\n    cam_ob = bpy.context.scene.camera\n    cam_ob.matrix_world = cam2world_matrix\n\n    # Add new frame if no frame is given\n    if frame is None:\n        frame = bpy.context.scene.frame_end\n    if bpy.context.scene.frame_end < frame + 1:\n        bpy.context.scene.frame_end = frame + 1\n\n    # Persist camera pose\n    cam_ob.keyframe_insert(data_path='location', frame=frame)\n    cam_ob.keyframe_insert(data_path='rotation_euler', frame=frame)\n\n    return frame",
  "def get_camera_pose(frame: Optional[int] = None) -> np.ndarray:\n    \"\"\" Returns the camera pose in the form of a 4x4 cam2world transformation matrx.\n\n    :param frame: The frame number whose assigned camera pose should be returned. If None is give, the current frame\n                  is used.\n    :return: The 4x4 cam2world transformation matrix.\n    \"\"\"\n    with KeyFrame(frame):\n        return np.array(Entity(bpy.context.scene.camera).get_local2world_mat())",
  "def get_camera_frustum(clip_start: Optional[float] = None, clip_end: Optional[float] = None,\n                       frame: Optional[int] = None) -> np.ndarray:\n    \"\"\" Get the current camera frustum as eight 3D coordinates.\n\n    :param clip_start: The distance between the camera pose and the near clipping plane.\n    :param clip_end: The distance between the camera pose and the far clipping plane.\n    :param frame: The frame number whose assigned camera pose should be used. If None is give, the current frame\n                  is used.\n    :return: The eight 3D coordinates of the camera frustum\n    \"\"\"\n    poses = np.array(list(product((-1, 1), repeat=3)))\n    poses = np.concatenate([poses, np.ones((8, 1))], axis=1)\n\n    # get the current camera pose\n    camera_pose = get_camera_pose(frame)\n\n    # get the projection matrix\n    projection_matrix = get_projection_matrix(clip_start, clip_end)\n\n    # bring the camera frustum points into the world frame\n    poses = poses @ np.linalg.inv(projection_matrix).transpose()\n    poses /= poses[:, 3:]\n    poses = poses @ camera_pose.transpose()\n    poses /= poses[:, 3:]\n    return poses[:, :3]",
  "def is_point_inside_camera_frustum(point: Union[List[float], Vector, np.ndarray],\n                                   clip_start: Optional[float] = None, clip_end: Optional[float] = None,\n                                   frame: Optional[int] = None) -> bool:\n    \"\"\" Checks if a given 3D point lies inside the camera frustum.\n\n    :param point: The point, which should be checked\n    :param clip_start: The distance between the camera pose and the near clipping plane.\n    :param clip_end: The distance between the camera pose and the far clipping plane.\n    :param frame: The frame number whose assigned camera pose should be used. If None is give, the current frame\n                  is used.\n    :return: True, if the point lies inside the camera frustum, else False\n    \"\"\"\n    camera_pose = get_camera_pose(frame)\n\n    point4d = np.insert(np.array(point), 3, 1, axis=0)\n    point4d = point4d @ np.linalg.inv(camera_pose).transpose()\n    point4d /= point4d[3]\n    projection_matrix = get_projection_matrix(clip_start, clip_end)\n    point4d = point4d @ projection_matrix.transpose()\n    point4d /= point4d[3]\n    point4d = point4d[:3]\n\n    return np.all([point4d < 1, -1 < point4d])",
  "def get_camera_frustum_as_object(clip_start: Optional[float] = None, clip_end: Optional[float] = None,\n                                 frame: Optional[int] = None) -> MeshObject:\n    \"\"\" Get the current camera frustum as deformed cube\n\n    :param clip_start: The distance between the camera pose and the near clipping plane.\n    :param clip_end: The distance between the camera pose and the far clipping plane.\n    :param frame: The frame number whose assigned camera pose should be used. If None is give, the current frame\n                  is used.\n    :return: The newly created MeshObject\n    \"\"\"\n    points = get_camera_frustum(clip_start, clip_end, frame)\n    cube = create_primitive(\"CUBE\")\n    cube.set_name(\"Camera Frustum\")\n    for i in range(8):\n        cube.get_mesh().vertices[i].co = points[i]\n    return cube",
  "def rotation_from_forward_vec(forward_vec: Union[np.ndarray, Vector], up_axis: str = 'Y',\n                              inplane_rot: Optional[float] = None) -> np.ndarray:\n    \"\"\" Returns a camera rotation matrix for the given forward vector and up axis\n\n    :param forward_vec: The forward vector which specifies the direction the camera should look.\n    :param up_axis: The up axis, usually Y.\n    :param inplane_rot: The inplane rotation in radians. If None is given, the inplane rotation is determined only\n                        based on the up vector.\n    :return: The corresponding rotation matrix.\n    \"\"\"\n    rotation_matrix = Vector(forward_vec).to_track_quat('-Z', up_axis).to_matrix()\n    if inplane_rot is not None:\n        rotation_matrix @= Euler((0.0, 0.0, inplane_rot)).to_matrix()\n    return np.array(rotation_matrix)",
  "def set_resolution(image_width: int = None, image_height: int = None):\n    \"\"\" Sets the camera resolution.\n\n    :param image_width: The image width in pixels.\n    :param image_height: The image height in pixels.\n    \"\"\"\n    set_intrinsics_from_blender_params(None, image_width, image_height)",
  "def set_intrinsics_from_blender_params(lens: float = None, image_width: int = None, image_height: int = None,\n                                       clip_start: float = None, clip_end: float = None,\n                                       pixel_aspect_x: float = None, pixel_aspect_y: float = None, shift_x: int = None,\n                                       shift_y: int = None, lens_unit: str = None):\n    \"\"\" Sets the camera intrinsics using blenders represenation.\n\n    :param lens: Either the focal length in millimeters or the FOV in radians, depending on the given lens_unit.\n    :param image_width: The image width in pixels.\n    :param image_height: The image height in pixels.\n    :param clip_start: Clipping start.\n    :param clip_end: Clipping end.\n    :param pixel_aspect_x: The pixel aspect ratio along x.\n    :param pixel_aspect_y: The pixel aspect ratio along y.\n    :param shift_x: The shift in x direction.\n    :param shift_y: The shift in y direction.\n    :param lens_unit: Either FOV or MILLIMETERS depending on whether the lens is defined as focal length in\n                      millimeters or as FOV in radians.\n    \"\"\"\n\n    cam_ob = bpy.context.scene.camera\n    cam = cam_ob.data\n\n    if lens_unit is not None:\n        cam.lens_unit = lens_unit\n\n    if lens is not None:\n        # Set focal length\n        if cam.lens_unit == 'MILLIMETERS':\n            if lens < 1:\n                raise Exception(\"The focal length is smaller than 1mm which is not allowed in blender: \" + str(lens))\n            cam.lens = lens\n        elif cam.lens_unit == \"FOV\":\n            cam.angle = lens\n        else:\n            raise Exception(\"No such lens unit: \" + lens_unit)\n\n    # Set resolution\n    if image_width is not None:\n        bpy.context.scene.render.resolution_x = image_width\n    if image_height is not None:\n        bpy.context.scene.render.resolution_y = image_height\n\n    # Set clipping\n    if clip_start is not None:\n        cam.clip_start = clip_start\n    if clip_end is not None:\n        cam.clip_end = clip_end\n\n    # Set aspect ratio\n    if pixel_aspect_x is not None:\n        bpy.context.scene.render.pixel_aspect_x = pixel_aspect_x\n    if pixel_aspect_y is not None:\n        bpy.context.scene.render.pixel_aspect_y = pixel_aspect_y\n\n    # Set shift\n    if shift_x is not None:\n        cam.shift_x = shift_x\n    if shift_y is not None:\n        cam.shift_y = shift_y",
  "def set_stereo_parameters(convergence_mode: str, convergence_distance: float, interocular_distance: float):\n    \"\"\" Sets the stereo parameters of the camera.\n\n    :param convergence_mode: How the two cameras converge (e.g. Off-Axis where both cameras are shifted inwards to\n                             converge in the convergence plane, or parallel where they do not converge and are\n                             parallel). Available: [\"OFFAXIS\", \"PARALLEL\", \"TOE\"]\n    :param convergence_distance: The convergence point for the stereo cameras (i.e. distance from the projector\n                                 to the projection screen)\n    :param interocular_distance: Distance between the camera pair\n    \"\"\"\n    cam_ob = bpy.context.scene.camera\n    cam = cam_ob.data\n\n    cam.stereo.convergence_mode = convergence_mode\n    cam.stereo.convergence_distance = convergence_distance\n    cam.stereo.interocular_distance = interocular_distance",
  "def set_intrinsics_from_K_matrix(K: Union[np.ndarray, Matrix], image_width: int, image_height: int,\n                                 clip_start: float = None, clip_end: float = None):\n    \"\"\" Set the camera intrinsics via a K matrix.\n\n    The K matrix should have the format:\n        [[fx, 0, cx],\n         [0, fy, cy],\n         [0, 0,  1]]\n\n    This method is based on https://blender.stackexchange.com/a/120063.\n\n    :param K: The 3x3 K matrix.\n    :param image_width: The image width in pixels.\n    :param image_height: The image height in pixels.\n    :param clip_start: Clipping start.\n    :param clip_end: Clipping end.\n    \"\"\"\n\n    K = Matrix(K)\n\n    cam = bpy.context.scene.camera.data\n\n    if abs(K[0][1]) > 1e-7:\n        raise ValueError(f\"Skew is not supported by blender and therefore \"\n                         f\"not by BlenderProc, set this to zero: {K[0][1]} and recalibrate\")\n\n    fx, fy = K[0][0], K[1][1]\n    cx, cy = K[0][2], K[1][2]\n\n    # If fx!=fy change pixel aspect ratio\n    pixel_aspect_x = pixel_aspect_y = 1\n    if fx > fy:\n        pixel_aspect_y = fx / fy\n    elif fx < fy:\n        pixel_aspect_x = fy / fx\n\n    # Compute sensor size in mm and view in px\n    pixel_aspect_ratio = pixel_aspect_y / pixel_aspect_x\n    view_fac_in_px = get_view_fac_in_px(cam, pixel_aspect_x, pixel_aspect_y, image_width, image_height)\n    sensor_size_in_mm = get_sensor_size(cam)\n\n    # Convert focal length in px to focal length in mm\n    f_in_mm = fx * sensor_size_in_mm / view_fac_in_px\n\n    # Convert principal point in px to blenders internal format\n    shift_x = (cx - (image_width - 1) / 2) / -view_fac_in_px\n    shift_y = (cy - (image_height - 1) / 2) / view_fac_in_px * pixel_aspect_ratio\n\n    # Finally set all intrinsics\n    set_intrinsics_from_blender_params(f_in_mm, image_width, image_height, clip_start, clip_end, pixel_aspect_x,\n                                       pixel_aspect_y, shift_x, shift_y, \"MILLIMETERS\")",
  "def get_sensor_size(cam: bpy.types.Camera) -> float:\n    \"\"\" Returns the sensor size in millimeters based on the configured sensor_fit.\n\n    :param cam: The camera object.\n    :return: The sensor size in millimeters.\n    \"\"\"\n    if cam.sensor_fit == 'VERTICAL':\n        sensor_size_in_mm = cam.sensor_height\n    else:\n        sensor_size_in_mm = cam.sensor_width\n    return sensor_size_in_mm",
  "def get_view_fac_in_px(cam: bpy.types.Camera, pixel_aspect_x: float, pixel_aspect_y: float,\n                       resolution_x_in_px: int, resolution_y_in_px: int) -> int:\n    \"\"\" Returns the camera view in pixels.\n\n    :param cam: The camera object.\n    :param pixel_aspect_x: The pixel aspect ratio along x.\n    :param pixel_aspect_y: The pixel aspect ratio along y.\n    :param resolution_x_in_px: The image width in pixels.\n    :param resolution_y_in_px: The image height in pixels.\n    :return: The camera view in pixels.\n    \"\"\"\n    # Determine the sensor fit mode to use\n    if cam.sensor_fit == 'AUTO':\n        if pixel_aspect_x * resolution_x_in_px >= pixel_aspect_y * resolution_y_in_px:\n            sensor_fit = 'HORIZONTAL'\n        else:\n            sensor_fit = 'VERTICAL'\n    else:\n        sensor_fit = cam.sensor_fit\n\n    # Based on the sensor fit mode, determine the view in pixels\n    pixel_aspect_ratio = pixel_aspect_y / pixel_aspect_x\n    if sensor_fit == 'HORIZONTAL':\n        view_fac_in_px = resolution_x_in_px\n    else:\n        view_fac_in_px = pixel_aspect_ratio * resolution_y_in_px\n\n    return view_fac_in_px",
  "def get_projection_matrix(clip_start: Optional[float] = None, clip_end: Optional[float] = None) -> np.ndarray:\n    \"\"\" Returns the projection matrix, it allows to overwrite the current used values for the near and far\n    clipping plane.\n\n    :param clip_start: The distance between the camera pose and the near clipping plane.\n    :param clip_end: The distance between the camera pose and the far clipping plane.\n    :return: The 4x4 projection matrix of the current camera\n    \"\"\"\n    if clip_start is None:\n        near = bpy.context.scene.camera.data.clip_start\n    else:\n        near = clip_start\n    if clip_end is None:\n        far = bpy.context.scene.camera.data.clip_end\n    else:\n        far = clip_end\n    # get the field of view\n    x_fov, y_fov = get_fov()\n    # convert them to height and width values\n    height, width = 1.0 / np.tan(y_fov * 0.5), 1. / np.tan(x_fov * 0.5)\n    # build up the projection matrix\n    return np.array([[width, 0, 0, 0], [0, height, 0, 0],\n                     [0, 0, -(near + far) / (far - near), -(2 * near * far) / (far - near)],\n                     [0, 0, -1, 0]])",
  "def get_intrinsics_as_K_matrix() -> np.ndarray:\n    \"\"\" Returns the current set intrinsics in the form of a K matrix.\n\n    This is basically the inverse of the the set_intrinsics_from_K_matrix() function.\n\n    :return: The 3x3 K matrix\n    \"\"\"\n    cam_ob = bpy.context.scene.camera\n    cam = cam_ob.data\n\n    f_in_mm = cam.lens\n    resolution_x_in_px = bpy.context.scene.render.resolution_x\n    resolution_y_in_px = bpy.context.scene.render.resolution_y\n\n    # Compute sensor size in mm and view in px\n    pixel_aspect_ratio = bpy.context.scene.render.pixel_aspect_y / bpy.context.scene.render.pixel_aspect_x\n    view_fac_in_px = get_view_fac_in_px(cam, bpy.context.scene.render.pixel_aspect_x,\n                                        bpy.context.scene.render.pixel_aspect_y, resolution_x_in_px, resolution_y_in_px)\n    sensor_size_in_mm = get_sensor_size(cam)\n\n    # Convert focal length in mm to focal length in px\n    fx = f_in_mm / sensor_size_in_mm * view_fac_in_px\n    fy = fx / pixel_aspect_ratio\n\n    # Convert principal point in blenders format to px\n    cx = (resolution_x_in_px - 1) / 2 - cam.shift_x * view_fac_in_px\n    cy = (resolution_y_in_px - 1) / 2 + cam.shift_y * view_fac_in_px / pixel_aspect_ratio\n\n    # Build K matrix\n    K = np.array([[fx, 0, cx],\n                  [0, fy, cy],\n                  [0, 0, 1]])\n    return K",
  "def get_fov() -> Tuple[float, float]:\n    \"\"\" Returns the horizontal and vertical FOV of the current camera.\n\n    Blender also offers the current FOV as direct attributes of the camera object, however\n    at least the vertical FOV heavily differs from how it would usually be defined.\n\n    :return: The horizontal and vertical FOV in radians.\n    \"\"\"\n    # Get focal length\n    K = get_intrinsics_as_K_matrix()\n    # Convert focal length to FOV\n    fov_x = 2 * np.arctan(bpy.context.scene.render.resolution_x / 2 / K[0, 0])\n    fov_y = 2 * np.arctan(bpy.context.scene.render.resolution_y / 2 / K[1, 1])\n    return fov_x, fov_y",
  "def add_depth_of_field(focal_point_obj: Entity, fstop_value: float,\n                       aperture_blades: int = 0, aperture_rotation: float = 0.0, aperture_ratio: float = 1.0,\n                       focal_distance: float = -1.0):\n    \"\"\"\n    Adds depth of field to the given camera, the focal point will be set by the focal_point_obj, ideally an empty\n    instance is used for this see `bproc.object.create_empty()` on how to init one of those. A higher fstop value\n    makes the resulting image look sharper, while a low value decreases the sharpness.\n\n    Check the documentation on\n    https://docs.blender.org/manual/en/latest/render/cameras.html#depth-of-field\n\n    :param focal_point_obj: The used focal point, if the object moves the focal point will move with it\n    :param fstop_value: A higher fstop value, will increase the sharpness of the scene\n    :param aperture_blades: Amount of blades used in the camera\n    :param aperture_rotation: Rotation of the blades in the camera in radiant\n    :param aperture_ratio: Ratio of the anamorphic bokeh effect, below 1.0 will give a horizontal one, above one a \\\n                           vertical one.\n    :param focal_distance: Sets the distance to the focal point when no focal_point_obj is given.\n    \"\"\"\n    cam_ob = bpy.context.scene.camera\n    camera = cam_ob.data\n\n    # activate depth of field rendering for this camera\n    camera.dof.use_dof = True\n    if focal_point_obj is not None:\n        # set the focus point of the camera\n        camera.dof.focus_object = focal_point_obj.blender_obj\n    elif focal_distance >= 0.0:\n        camera.dof.focus_distance = focal_distance\n    else:\n        raise RuntimeError(\"Either a focal_point_obj have to be given or the focal_distance has to be higher \"\n                           \"than zero.\")\n    # set the aperture of the camera, lower values make the scene more out of focus, higher values make them look\n    # sharper\n    camera.dof.aperture_fstop = fstop_value\n    # set the amount of blades\n    camera.dof.aperture_blades = aperture_blades\n    # setting the rotation of the aperture in radiant\n    camera.dof.aperture_rotation = aperture_rotation\n    # Change the amount of distortion to simulate the anamorphic bokeh effect. A setting of 1.0 shows no\n    # distortion, where a number below 1.0 will cause a horizontal distortion, and a higher number will\n    # cause a vertical distortion.\n    camera.dof.aperture_ratio = aperture_ratio",
  "class Front3DPointInRoomSampler:\n    \"\"\"\n    Allows the sampling 3D Front scenes\n    \"\"\"\n\n    def __init__(self, front3d_objects: List[MeshObject], amount_of_objects_needed_per_room: int = 2):\n        \"\"\" Collects the floors of all rooms with at least N objects.\n\n        :param front3d_objects: The list of front3d objects that should be considered.\n        :param amount_of_objects_needed_per_room: The number of objects a rooms needs to have, such that it is\n                                                  considered for sampling.\n        \"\"\"\n        front3d_objects = [obj for obj in front3d_objects if obj.has_cp(\"is_3D_future\")]\n\n        floor_objs = [obj for obj in front3d_objects if obj.get_name().lower().startswith(\"floor\")]\n\n        # count objects per floor -> room\n        floor_obj_counters = {obj.get_name(): 0 for obj in floor_objs}\n        counter = 0\n        for obj in front3d_objects:\n            name = obj.get_name().lower()\n            if \"wall\" in name or \"ceiling\" in name:\n                continue\n            counter += 1\n\n            for floor_obj in floor_objs:\n                is_above = floor_obj.position_is_above_object(obj.get_location())\n                if is_above:\n                    floor_obj_counters[floor_obj.get_name()] += 1\n        self.used_floors = [obj for obj in floor_objs if\n                            floor_obj_counters[obj.get_name()] > amount_of_objects_needed_per_room]\n\n    def sample(self, height: float, max_tries: int = 1000) -> np.ndarray:\n        \"\"\" Samples a point inside one of the loaded Front3d rooms.\n\n        The points are uniformly sampled along x/y over all rooms.\n        The z-coordinate is set based on the given height value.\n\n        :param height: The height above the floor to use for the z-component of the point.\n        :param max_tries: The maximum number of times sampling above the floor should be tried.\n        :return: The sampled point.\n        \"\"\"\n        for _ in range(max_tries):\n            # Sample room via floor objects\n            floor_obj = random.choice(self.used_floors)\n\n            # Get min/max along x/y-axis from bounding box of room\n            bounding_box = floor_obj.get_bound_box()\n            min_corner = np.min(bounding_box, axis=0)\n            max_corner = np.max(bounding_box, axis=0)\n\n            # Sample uniformly inside bounding box\n            point = np.array([\n                random.uniform(min_corner[0], max_corner[0]),\n                random.uniform(min_corner[1], max_corner[1]),\n                floor_obj.get_location()[2] + height\n            ])\n\n            # Check if sampled pose is above the floor to make sure its really inside the room\n            if floor_obj.position_is_above_object(point):\n                return point\n\n        raise RuntimeError(\"Cannot sample any point inside the loaded front3d rooms.\")",
  "def __init__(self, front3d_objects: List[MeshObject], amount_of_objects_needed_per_room: int = 2):\n        \"\"\" Collects the floors of all rooms with at least N objects.\n\n        :param front3d_objects: The list of front3d objects that should be considered.\n        :param amount_of_objects_needed_per_room: The number of objects a rooms needs to have, such that it is\n                                                  considered for sampling.\n        \"\"\"\n        front3d_objects = [obj for obj in front3d_objects if obj.has_cp(\"is_3D_future\")]\n\n        floor_objs = [obj for obj in front3d_objects if obj.get_name().lower().startswith(\"floor\")]\n\n        # count objects per floor -> room\n        floor_obj_counters = {obj.get_name(): 0 for obj in floor_objs}\n        counter = 0\n        for obj in front3d_objects:\n            name = obj.get_name().lower()\n            if \"wall\" in name or \"ceiling\" in name:\n                continue\n            counter += 1\n\n            for floor_obj in floor_objs:\n                is_above = floor_obj.position_is_above_object(obj.get_location())\n                if is_above:\n                    floor_obj_counters[floor_obj.get_name()] += 1\n        self.used_floors = [obj for obj in floor_objs if\n                            floor_obj_counters[obj.get_name()] > amount_of_objects_needed_per_room]",
  "def sample(self, height: float, max_tries: int = 1000) -> np.ndarray:\n        \"\"\" Samples a point inside one of the loaded Front3d rooms.\n\n        The points are uniformly sampled along x/y over all rooms.\n        The z-coordinate is set based on the given height value.\n\n        :param height: The height above the floor to use for the z-component of the point.\n        :param max_tries: The maximum number of times sampling above the floor should be tried.\n        :return: The sampled point.\n        \"\"\"\n        for _ in range(max_tries):\n            # Sample room via floor objects\n            floor_obj = random.choice(self.used_floors)\n\n            # Get min/max along x/y-axis from bounding box of room\n            bounding_box = floor_obj.get_bound_box()\n            min_corner = np.min(bounding_box, axis=0)\n            max_corner = np.max(bounding_box, axis=0)\n\n            # Sample uniformly inside bounding box\n            point = np.array([\n                random.uniform(min_corner[0], max_corner[0]),\n                random.uniform(min_corner[1], max_corner[1]),\n                floor_obj.get_location()[2] + height\n            ])\n\n            # Check if sampled pose is above the floor to make sure its really inside the room\n            if floor_obj.position_is_above_object(point):\n                return point\n\n        raise RuntimeError(\"Cannot sample any point inside the loaded front3d rooms.\")",
  "def shell(center: Union[Vector, np.ndarray, List[float]], radius_min: float, radius_max: float,\n          elevation_min: float = -90, elevation_max: float = 90, azimuth_min: float = -180,\n          azimuth_max: float = 180, uniform_volume: bool = False) -> np.ndarray:\n    \"\"\"\n    Samples a point from the volume between two spheres (radius_min, radius_max). Optionally the spheres can\n    be constraint by setting elevation and azimuth angles. E.g. if you only want to sample in the upper\n    hemisphere set elevation_min = 0.\n\n    :param center: Center shared by both spheres.\n    :param radius_min: Radius of the smaller sphere.\n    :param radius_max: Radius of the bigger sphere.\n    :param elevation_min: Minimum angle of elevation in degrees. Range: [-90, 90].\n    :param elevation_max: Maximum angle of elevation in degrees. Range: [-90, 90].\n    :param azimuth_min: Minimum angle of azimuth in degrees. Range: [-180, 180].\n    :param azimuth_max: Maximum angle of azimuth in degrees. Range: [-180, 180].\n    :param uniform_volume: Instead of sampling the angles and radius uniformly, sample the shell volume uniformly.\n                           As a result, there will be more samples at larger radii.\n    :return: A sampled point.\n    \"\"\"\n\n    center = np.array(center)\n\n    assert -180 <= azimuth_min <= 180, \"azimuth_min must be in range [-180, 180]\"\n    assert -180 <= azimuth_max <= 180, \"azimuth_max must be in range [-180, 180]\"\n    assert -90 <= elevation_min <= 90, \"elevation_min must be in range [-90, 90]\"\n    assert -90 <= elevation_min <= 90, \"elevation_max must be in range [-90, 90]\"\n    assert azimuth_min < azimuth_max, \"azimuth_min must be smaller than azimuth_max\"\n    assert elevation_min < elevation_max, \"elevation_min must be smaller than elevation_max\"\n\n    if uniform_volume:\n\n        radius = radius_min + (radius_max - radius_min) * np.cbrt(np.random.rand())\n\n        # rejection sampling\n        constr_fulfilled = False\n        while not constr_fulfilled:\n            direction_vector = np.random.randn(3)\n            direction_vector /= np.linalg.norm(direction_vector)\n\n            # https://stackoverflow.com/questions/4116658/faster-numpy-cartesian-to-spherical-coordinate-conversion\n            xy = direction_vector[0] * direction_vector[0] + direction_vector[1] * direction_vector[1]\n            elevation = np.arctan2(direction_vector[2], np.sqrt(xy))\n            azimuth = np.arctan2(direction_vector[1], direction_vector[0])\n\n            elev_constraint = np.deg2rad(elevation_min) < elevation < np.deg2rad(elevation_max)\n            azim_constraint = np.deg2rad(azimuth_min) < azimuth < np.deg2rad(azimuth_max)\n            constr_fulfilled = elev_constraint and azim_constraint\n    else:\n        el_sampled = np.deg2rad(elevation_min + (elevation_max - elevation_min) * np.random.rand())\n        az_sampled = np.deg2rad(azimuth_min + (azimuth_max - azimuth_min) * np.random.rand())\n        # spherical to cartesian coordinates\n        direction_vector = np.array([np.sin(np.pi / 2 - el_sampled) * np.cos(az_sampled),\n                                     np.sin(np.pi / 2 - el_sampled) * np.sin(az_sampled),\n                                     np.cos(np.pi / 2 - el_sampled)])\n\n        # Calculate the uniform radius\n        radius = np.random.uniform(radius_min, radius_max)\n\n    # Get the coordinates of a sampled point inside the shell\n    position = direction_vector * radius + center\n\n    return position",
  "class ReplicaPointInRoomSampler:\n    \"\"\"\n    Allows the sampling in Replica scenes\n    \"\"\"\n\n    def __init__(self, room_bounding_box: Dict[str, np.ndarray], replica_floor: Union[MeshObject, List[MeshObject]],\n                 height_list_file_path: str):\n        \"\"\" Collect object containing all floors of all rooms and read in text file containing possible height values.\n\n        :param room_bounding_box: The bounding box of the room, needs a min key and max key, representing the edges of\n                                  the room bounding box\n        :param replica_floor: The replica floor object.\n        :param height_list_file_path: The path to the file containing possible height values.\n        \"\"\"\n        self.bounding_box = room_bounding_box\n        self.floor_object = replica_floor\n        if isinstance(self.floor_object, list) and not self.floor_object:\n            raise Exception(\"The floor object list can not be empty!\")\n\n        with open(height_list_file_path, \"r\", encoding=\"utf-8\") as file:\n            self.floor_height_values = [float(val) for val in ast.literal_eval(file.read())]\n\n    def sample(self, height: float, max_tries: int = 1000) -> np.ndarray:\n        \"\"\" Samples a point inside one of the loaded replica rooms.\n\n        The points are uniformly sampled along x/y over all rooms.\n        The z-coordinate is set based on the given height value.\n\n        :param height: The height above the floor to use for the z-component of the point.\n        :param max_tries: The maximum number of times sampling above the floor should be tried.\n        :return: The sampled point.\n        \"\"\"\n        for _ in range(max_tries):\n\n            # Sample uniformly inside bounding box\n            point = np.array([\n                random.uniform(self.bounding_box[\"min\"][0], self.bounding_box[\"max\"][0]),\n                random.uniform(self.bounding_box[\"min\"][1], self.bounding_box[\"max\"][1]),\n                self.floor_height_values[random.randrange(0, len(self.floor_height_values))] + height\n            ])\n\n            if isinstance(self.floor_object, list):\n                for floor_object in self.floor_object:\n                    # Check if sampled pose is above the floor to make sure it is really inside the room\n                    if floor_object.position_is_above_object(point):\n                        return point\n            else:\n                # Check if sampled pose is above the floor to make sure it is really inside the room\n                if self.floor_object.position_is_above_object(point):\n                    return point\n\n        raise Exception(\"Cannot sample any point inside the loaded replica rooms.\")",
  "def __init__(self, room_bounding_box: Dict[str, np.ndarray], replica_floor: Union[MeshObject, List[MeshObject]],\n                 height_list_file_path: str):\n        \"\"\" Collect object containing all floors of all rooms and read in text file containing possible height values.\n\n        :param room_bounding_box: The bounding box of the room, needs a min key and max key, representing the edges of\n                                  the room bounding box\n        :param replica_floor: The replica floor object.\n        :param height_list_file_path: The path to the file containing possible height values.\n        \"\"\"\n        self.bounding_box = room_bounding_box\n        self.floor_object = replica_floor\n        if isinstance(self.floor_object, list) and not self.floor_object:\n            raise Exception(\"The floor object list can not be empty!\")\n\n        with open(height_list_file_path, \"r\", encoding=\"utf-8\") as file:\n            self.floor_height_values = [float(val) for val in ast.literal_eval(file.read())]",
  "def sample(self, height: float, max_tries: int = 1000) -> np.ndarray:\n        \"\"\" Samples a point inside one of the loaded replica rooms.\n\n        The points are uniformly sampled along x/y over all rooms.\n        The z-coordinate is set based on the given height value.\n\n        :param height: The height above the floor to use for the z-component of the point.\n        :param max_tries: The maximum number of times sampling above the floor should be tried.\n        :return: The sampled point.\n        \"\"\"\n        for _ in range(max_tries):\n\n            # Sample uniformly inside bounding box\n            point = np.array([\n                random.uniform(self.bounding_box[\"min\"][0], self.bounding_box[\"max\"][0]),\n                random.uniform(self.bounding_box[\"min\"][1], self.bounding_box[\"max\"][1]),\n                self.floor_height_values[random.randrange(0, len(self.floor_height_values))] + height\n            ])\n\n            if isinstance(self.floor_object, list):\n                for floor_object in self.floor_object:\n                    # Check if sampled pose is above the floor to make sure it is really inside the room\n                    if floor_object.position_is_above_object(point):\n                        return point\n            else:\n                # Check if sampled pose is above the floor to make sure it is really inside the room\n                if self.floor_object.position_is_above_object(point):\n                    return point\n\n        raise Exception(\"Cannot sample any point inside the loaded replica rooms.\")",
  "def disk(center: Union[Vector, np.ndarray, List[float]], radius: float,\n         rotation: Optional[Union[Vector, np.ndarray, List[float]]] = None,\n         sample_from: str = \"disk\", start_angle: float = 0, end_angle: float = 180) -> np.ndarray:\n    \"\"\" Samples a point on a 1-sphere (circle), or on a 2-ball (disk, i.e. circle + interior space), or on an arc/sector\n        with an inner angle less or equal than 180 degrees. Returns a 3d mathutils.Vector sampled point.\n\n    Example 1: Sample a point from a 1-sphere.\n\n    .. code-block:: python\n\n        Disk.sample(\n            center=[0, 0, 4],\n            radius=7,\n            sample_from=\"circle\"\n        )\n\n    Example 2: Sample a point from a sector.\n\n    .. code-block:: python\n\n        Disk.sample(\n            center=[0, 0, 4],\n            radius=7,\n            sample_from=\"sector\",\n            start_angle=0,\n            end_angle=90\n        )\n\n    :param center: Center (in 3d space) of a 2d geometrical shape to sample from.\n    :param radius: The radius of the disk.\n    :param rotation: List of three (XYZ) Euler angles that represent the rotation of the 2d geometrical structure\n                     used for sampling in 3d space.\n    :param sample_from: Mode of sampling. Defines the geometrical structure used for sampling, i.e. the shape to\n                        sample from.\n    :param start_angle: Start angle in degrees that is used to define a sector/arc to sample from. Must be smaller than\n                        end_angle. Arc's/sector's inner angle (between start and end) must be less or equal than\n                        180 degrees. Angle increases in the counterclockwise direction from the positive direction\n                        of X axis.\n    :param end_angle: End angle in degrees that is used to define a sector/arc to sample from. Must be bigger than\n                      start_angle. Arc's/sector's inner angle (between start and end) must be less or equal\n                      than 180 degrees. Angle increases in the counterclockwise direction from the positive\n                      direction of X axis.\n    :return: A random point sampled point on a circle/disk/arc/sector.\n    \"\"\"\n    if rotation is None:\n        rotation = [0, 0, 0]\n\n    # check if the mode/sampling structure is supported\n    if sample_from not in [\"disk\", \"circle\", \"sector\", \"arc\"]:\n        raise Exception(\"Unknown mode of operation: \" + sample_from)\n    # if mode is sampling from sector or arc\n    if sample_from in [\"arc\", \"sector\"]:\n        # check if start and end angles comply to boundaries\n        if not all([start_angle < end_angle, abs(start_angle - end_angle) <= 180]):\n            raise Exception(\"Sector's/arch's start and end points are defined wrong! Boundaries to comply with:\"\n                            \"1. start_angle < end_angle; 2. abs(start_angle - end_angle) <= 180.\")\n        # transform to 2d vectors\n        start_vec = [np.cos(np.deg2rad(start_angle)), np.sin(np.deg2rad(start_angle))]\n        end_vec = [np.cos(np.deg2rad(end_angle)), np.sin(np.deg2rad(end_angle))]\n\n    # if sampling from the circle or arc set magnitude to radius, if not - to the scaled radius\n    if sample_from.lower() in [\"circle\", \"arc\"]:\n        magnitude = radius\n    elif sample_from.lower() in [\"disk\", \"sector\"]:\n        magnitude = radius * np.sqrt(np.random.uniform())\n    else:\n        raise Exception(\"Unknown mode of operation: \" + sample_from)\n\n    sampled_point = _Disk.sample_point(magnitude)\n\n    # sample a point until it falls into the defined sector/arc\n    if sample_from in [\"arc\", \"sector\"]:\n        while not all([not _Disk.is_clockwise(start_vec, sampled_point), _Disk.is_clockwise(end_vec, sampled_point)]):\n            sampled_point = _Disk.sample_point(magnitude)\n\n    # get rotation\n    rot_mat = mathutils.Euler(rotation, 'XYZ').to_matrix()\n    # apply rotation and add center\n    location = np.array(rot_mat) @ sampled_point + np.array(center)\n\n    return location",
  "class _Disk:\n\n    @staticmethod\n    def sample_point(magnitude: float) -> np.ndarray:\n        \"\"\" Samples a 3d point from a two-dimensional normal distribution with the third dim equal to 0.\n\n        :param magnitude: Scaling factor of a radius.\n        :return: Sampled 3d point. Type: numpy.array.\n        \"\"\"\n        direction = np.random.normal(loc=0.0, scale=1.0, size=2)\n        if np.count_nonzero(direction) == 0:\n            direction[0] = 1e-5\n        norm = np.sqrt(direction.dot(direction))\n        sampled_point = np.append(list(map(lambda x: magnitude * x / norm, direction)), 0)\n\n        return sampled_point\n\n    @staticmethod\n    def is_clockwise(rel_point: Union[Vector, np.ndarray, List[float]],\n                     sampled_point: Union[Vector, np.ndarray, List[float]]) -> bool:\n        \"\"\" Checks if the sampled_point is in the clockwise direction in relation to the rel_point.\n\n        :param rel_point: Point relative to which the test is performed.\n        :param sampled_point: Point for which test is performed.\n        :return: True if the sampled_point lies in the clockwise direction in relation to the rel_point, False if not.\n        \"\"\"\n        return (-rel_point[0] * sampled_point[1] + rel_point[1] * sampled_point[0]) > 0",
  "def sample_point(magnitude: float) -> np.ndarray:\n        \"\"\" Samples a 3d point from a two-dimensional normal distribution with the third dim equal to 0.\n\n        :param magnitude: Scaling factor of a radius.\n        :return: Sampled 3d point. Type: numpy.array.\n        \"\"\"\n        direction = np.random.normal(loc=0.0, scale=1.0, size=2)\n        if np.count_nonzero(direction) == 0:\n            direction[0] = 1e-5\n        norm = np.sqrt(direction.dot(direction))\n        sampled_point = np.append(list(map(lambda x: magnitude * x / norm, direction)), 0)\n\n        return sampled_point",
  "def is_clockwise(rel_point: Union[Vector, np.ndarray, List[float]],\n                     sampled_point: Union[Vector, np.ndarray, List[float]]) -> bool:\n        \"\"\" Checks if the sampled_point is in the clockwise direction in relation to the rel_point.\n\n        :param rel_point: Point relative to which the test is performed.\n        :param sampled_point: Point for which test is performed.\n        :return: True if the sampled_point lies in the clockwise direction in relation to the rel_point, False if not.\n        \"\"\"\n        return (-rel_point[0] * sampled_point[1] + rel_point[1] * sampled_point[0]) > 0",
  "def uniformSO3(around_x: bool = True, around_y: bool = True, around_z: bool = True) -> np.ndarray:\n    \"\"\" Uniformly samples rotations from SO(3). Allows to limit the rotation around Blender World coordinate axes.\n\n    :param around_x: Whether to rotate around X-axis.\n    :param around_y: Whether to rotate around Y-axis.\n    :param around_z: Whether to rotate around Z-axis.\n    :return: Sampled rotation in euler angles.\n    \"\"\"\n    # Uniform sampling in full SO3.\n    if around_x and around_y and around_z:\n        quat_rand = _UniformSO3.random_quaternion()\n        euler_rand = mathutils.Quaternion(quat_rand).to_euler()\n\n    # Uniform sampling of angles around the selected axes.\n    else:\n        def random_angle():\n            return random.uniform(0, 2 * np.pi)\n\n        mat_rand = mathutils.Matrix.Identity(3)\n        if around_x:\n            mat_rand @= mathutils.Matrix.Rotation(random_angle(), 3, 'X')\n        if around_y:\n            mat_rand @= mathutils.Matrix.Rotation(random_angle(), 3, 'Y')\n        if around_z:\n            mat_rand @= mathutils.Matrix.Rotation(random_angle(), 3, 'Z')\n        euler_rand = mat_rand.to_euler()\n\n    return np.array(euler_rand)",
  "class _UniformSO3:\n    @staticmethod\n    def random_quaternion(rand: Optional[Union[List[float], np.ndarray]] = None) -> np.ndarray:\n        \"\"\" Return uniform random unit quaternion.\n\n        https://github.com/thodan/bop_toolkit/blob/master/bop_toolkit_lib/transform.py\n\n        :param rand: Three independent random variables that are uniformly distributed between 0 and 1.\n        :return: Unit quaternion.\n        \"\"\"\n        if rand is None:\n            rand = np.random.rand(3)\n        else:\n            assert len(rand) == 3\n\n        r1 = np.sqrt(1.0 - rand[0])\n        r2 = np.sqrt(rand[0])\n        pi2 = np.pi * 2.0\n        t1 = pi2 * rand[1]\n        t2 = pi2 * rand[2]\n\n        return np.array([np.cos(t2) * r2, np.sin(t1) * r1, np.cos(t1) * r1, np.sin(t2) * r2])",
  "def random_quaternion(rand: Optional[Union[List[float], np.ndarray]] = None) -> np.ndarray:\n        \"\"\" Return uniform random unit quaternion.\n\n        https://github.com/thodan/bop_toolkit/blob/master/bop_toolkit_lib/transform.py\n\n        :param rand: Three independent random variables that are uniformly distributed between 0 and 1.\n        :return: Unit quaternion.\n        \"\"\"\n        if rand is None:\n            rand = np.random.rand(3)\n        else:\n            assert len(rand) == 3\n\n        r1 = np.sqrt(1.0 - rand[0])\n        r2 = np.sqrt(rand[0])\n        pi2 = np.pi * 2.0\n        t1 = pi2 * rand[1]\n        t2 = pi2 * rand[2]\n\n        return np.array([np.cos(t2) * r2, np.sin(t1) * r1, np.cos(t1) * r1, np.sin(t2) * r2])",
  "def random_angle():\n            return random.uniform(0, 2 * np.pi)",
  "def part_sphere(center: Union[Vector, np.ndarray, List[float]], radius: float, mode: str,\n                dist_above_center: float = 0.0,\n                part_sphere_dir_vector: Optional[Union[Vector, np.ndarray, List[float]]] = None) -> np.ndarray:\n    \"\"\" Samples a point from the surface or from the interior of solid sphere which is split in two parts.\n\n    https://math.stackexchange.com/a/87238\n    https://math.stackexchange.com/a/1585996\n\n    Example 1: Sample a point from the surface of the sphere that is split by a plane with displacement of 0.5\n    above center and a normal of [1, 0, 0].\n\n    .. code-block:: python\n\n        PartSphere.sample(\n            center=[0, 0, 0],\n            part_sphere_vector=[1, 0, 0],\n            mode=\"SURFACE\",\n            distance_above_center=0.5\n        )\n\n    :param center: Location of the center of the sphere.\n    :param radius: The radius of the sphere.\n    :param mode: Mode of sampling. Determines the geometrical structure used for sampling. Available: SURFACE (sampling\n                 from the 2-sphere), INTERIOR (sampling from the 3-ball).\n    :param dist_above_center: The distance above the center, which should be used. Default: 0.0 (half of the sphere).\n    :param part_sphere_dir_vector: The direction in which the sphere should be split, the end point of the vector,\n                                   will be in the middle of the sphere pointing towards the middle of the\n                                   resulting surface. Default: [0, 0, 1].\n    :return: A random point lying inside or on the surface of a solid sphere.\n    \"\"\"\n    if part_sphere_dir_vector is None:\n        part_sphere_dir_vector = np.array([0, 0, 1], np.float32)\n    else:\n        part_sphere_dir_vector = np.array(part_sphere_dir_vector).astype(np.float32)\n    part_sphere_dir_vector /= np.linalg.norm(part_sphere_dir_vector)\n\n    if dist_above_center >= radius:\n        raise ValueError(\"The dist_above_center value is bigger or as big as the radius!\")\n\n    while True:\n        location = sphere(center, radius, mode)\n        # project the location onto the part_sphere_dir_vector and get the length\n        loc_in_sphere = location - np.array(center)\n        length = loc_in_sphere.dot(part_sphere_dir_vector)\n        if length > dist_above_center:\n            return location",
  "class SuncgPointInRoomSampler:\n    \"\"\"\n    Allows the sampling in the SUNCG scenes\n    \"\"\"\n\n    def __init__(self, suncg_objects: List[MeshObject]):\n        \"\"\"\n        :param suncg_objects: The list of suncg objects to consider.\n        \"\"\"\n        # Collect all valid room objects\n        self.rooms = []\n        for room_obj in suncg_objects:\n            # Check if object is from type room and has bbox\n            if room_obj.has_cp(\"suncg_type\") and room_obj.get_cp(\"suncg_type\") == \"Room\" and room_obj.has_cp(\"bbox\"):\n                # Make sure the room has a floor which is required for sampling\n                floor_obj = self._find_floor(suncg_objects, room_obj)\n                if floor_obj is not None:\n                    self.rooms.append((room_obj, floor_obj))\n\n    def sample(self, height: float, max_tries: int = 1000) -> Tuple[np.ndarray, int]:\n        \"\"\" Samples a point inside one of the loaded suncg rooms.\n\n        The points are uniformly sampled along x/y over all rooms.\n        The z-coordinate is set based on the given height value.\n\n        :param height: The height above the floor to use for the z-component of the point.\n        :param max_tries: The maximum number of times sampling above the floor should be tried.\n        :return: The sampled point and the id of the room it was sampled in.\n        \"\"\"\n        for _ in range(max_tries):\n            # Sample room\n            room_id = random.randrange(len(self.rooms))\n            room_obj, floor_obj = self.rooms[room_id]\n\n            point = np.array([\n                random.uniform(room_obj.get_cp(\"bbox\")[\"min\"][0], room_obj.get_cp(\"bbox\")[\"max\"][0]),\n                random.uniform(room_obj.get_cp(\"bbox\")[\"min\"][1], room_obj.get_cp(\"bbox\")[\"max\"][1]),\n                room_obj.get_cp(\"bbox\")[\"min\"][2] + height\n            ])\n\n            # Check if sampled pose is valid\n            if floor_obj.position_is_above_object(point):\n                return point, room_id\n\n        raise Exception(\"Cannot sample any point inside the loaded suncg rooms.\")\n\n    def _find_floor(self, suncg_objects: List[MeshObject], room_obj: MeshObject) -> Optional[MeshObject]:\n        \"\"\" Returns the floor object of the given room object.\n\n        Goes through all children and returns the first one with type \"Floor\".\n\n        :param suncg_objects:\n        :param room_obj: The room object.\n        :return: The found floor object or None if none has been found.\n        \"\"\"\n        for obj in suncg_objects:\n            if obj.get_parent() == room_obj and obj.has_cp(\"suncg_type\") and obj.get_cp(\"suncg_type\") == \"Floor\":\n                return obj\n        return None",
  "def __init__(self, suncg_objects: List[MeshObject]):\n        \"\"\"\n        :param suncg_objects: The list of suncg objects to consider.\n        \"\"\"\n        # Collect all valid room objects\n        self.rooms = []\n        for room_obj in suncg_objects:\n            # Check if object is from type room and has bbox\n            if room_obj.has_cp(\"suncg_type\") and room_obj.get_cp(\"suncg_type\") == \"Room\" and room_obj.has_cp(\"bbox\"):\n                # Make sure the room has a floor which is required for sampling\n                floor_obj = self._find_floor(suncg_objects, room_obj)\n                if floor_obj is not None:\n                    self.rooms.append((room_obj, floor_obj))",
  "def sample(self, height: float, max_tries: int = 1000) -> Tuple[np.ndarray, int]:\n        \"\"\" Samples a point inside one of the loaded suncg rooms.\n\n        The points are uniformly sampled along x/y over all rooms.\n        The z-coordinate is set based on the given height value.\n\n        :param height: The height above the floor to use for the z-component of the point.\n        :param max_tries: The maximum number of times sampling above the floor should be tried.\n        :return: The sampled point and the id of the room it was sampled in.\n        \"\"\"\n        for _ in range(max_tries):\n            # Sample room\n            room_id = random.randrange(len(self.rooms))\n            room_obj, floor_obj = self.rooms[room_id]\n\n            point = np.array([\n                random.uniform(room_obj.get_cp(\"bbox\")[\"min\"][0], room_obj.get_cp(\"bbox\")[\"max\"][0]),\n                random.uniform(room_obj.get_cp(\"bbox\")[\"min\"][1], room_obj.get_cp(\"bbox\")[\"max\"][1]),\n                room_obj.get_cp(\"bbox\")[\"min\"][2] + height\n            ])\n\n            # Check if sampled pose is valid\n            if floor_obj.position_is_above_object(point):\n                return point, room_id\n\n        raise Exception(\"Cannot sample any point inside the loaded suncg rooms.\")",
  "def _find_floor(self, suncg_objects: List[MeshObject], room_obj: MeshObject) -> Optional[MeshObject]:\n        \"\"\" Returns the floor object of the given room object.\n\n        Goes through all children and returns the first one with type \"Floor\".\n\n        :param suncg_objects:\n        :param room_obj: The room object.\n        :return: The found floor object or None if none has been found.\n        \"\"\"\n        for obj in suncg_objects:\n            if obj.get_parent() == room_obj and obj.has_cp(\"suncg_type\") and obj.get_cp(\"suncg_type\") == \"Floor\":\n                return obj\n        return None",
  "def sphere(center: Union[Vector, np.ndarray, list], radius: float, mode: str) -> np.ndarray:\n    \"\"\" Samples a point from the surface or from the interior of solid sphere.\n\n    https://math.stackexchange.com/a/87238\n    https://math.stackexchange.com/a/1585996\n\n    Example 1: Sample a point from the surface of the solid sphere of a defined radius and center location.\n\n    .. code-block:: python\n\n        Sphere.sample(\n            center=Vector([0, 0, 0]),\n            radius=2,\n            mode=\"SURFACE\"\n        )\n\n    :param center: Location of the center of the sphere.\n    :param radius: The radius of the sphere.\n    :param mode: Mode of sampling. Determines the geometrical structure used for sampling. Available: SURFACE (sampling\n                 from the 2-sphere), INTERIOR (sampling from the 3-ball).\n    \"\"\"\n    center = np.array(center)\n\n    # Sample\n    direction = np.random.normal(loc=0.0, scale=1.0, size=3)\n\n    if np.count_nonzero(direction) == 0:  # Check no division by zero\n        direction[0] = 1e-5\n\n    # For normalization\n    norm = np.sqrt(direction.dot(direction))\n\n    # If sampling from the surface set magnitude to radius of the sphere\n    if mode == \"SURFACE\":\n        magnitude = radius\n    # If sampling from the interior set it to scaled radius\n    elif mode == \"INTERIOR\":\n        magnitude = radius * np.cbrt(np.random.uniform())\n    else:\n        raise Exception(\"Unknown sampling mode: \" + mode)\n\n    # Normalize\n    sampled_point = list(map(lambda x: magnitude * x / norm, direction))\n\n    # Add center\n    location = np.array(sampled_point) + center\n\n    return location",
  "def random_walk(total_length: int, dims: int, step_magnitude: float = 1.0, window_size: int = 1,\n                interval: Optional[List[np.ndarray]] = None, distribution: str = 'uniform',\n                order: float = 1.0) -> np.ndarray:\n    \"\"\"\n    Creates a random walk with the specified properties. Can be used to simulate camera shaking or POI drift.\n    steps ~ step_magnitude * U[-1,1]^order\n\n    :param total_length: length of the random walk\n    :param dims: In how many dimensions the random walk should happen\n    :param step_magnitude: Maximum magnitude of any coordinate in a single step\n    :param window_size: Convolve the final trajectory with an average filter that smoothens the trajectory with a\n                        given filter size.\n    :param interval: Constrain the random walk to an interval and mirror steps if they go beyond. List of arrays\n                     with dimension dims.\n    :param distribution: Distribution to sample steps from. Choose from ['normal', 'uniform'].\n    :param order: Sample from higher order distribution instead of the uniform. Higher order leads to steps being\n                  less frequently close to step_magnitude and thus overall decreased variance.\n    :return: The random walk trajectory (total_length, dims)\n    \"\"\"\n    # Set sampling distribution\n    if distribution == 'uniform':\n        dist_fun = np.random.rand\n    elif distribution == 'normal':\n        dist_fun = np.random.randn\n    else:\n        raise RuntimeError(f'Unknown distribution: {distribution}. Choose between \"normal\" and \"uniform\"')\n\n        # Sample random steps\n    random_steps = step_magnitude * np.random.choice([-1, 1], (total_length, dims)) * dist_fun(total_length,\n                                                                                               dims) ** order\n\n    # Cumulate the random steps to a random walk trajectory\n    cumulative_steps = np.cumsum(random_steps, axis=0)\n\n    # Keep the steps within the predefined interval\n    if interval is not None:\n        assert len(interval) == 2, \"interval must have length of two\"\n        left_bound = np.array(interval[0])\n        size = np.abs(interval[1] - left_bound)\n        cumulative_steps = np.abs((cumulative_steps - left_bound + size) % (2 * size) - size) + left_bound\n\n    # Smooth the random walk trajectory using a sliding window of size window_size\n    if window_size > 1:\n        initial_padding = np.ones((window_size - 1, dims)) * cumulative_steps[:1, :]\n        cumulative_steps_padded = np.vstack((initial_padding, cumulative_steps))\n        for i in range(dims):\n            cumulative_steps[:, i] = np.convolve(cumulative_steps_padded[:, i], np.ones(window_size) / window_size,\n                                                 'valid')\n\n    return cumulative_steps",
  "def upper_region(objects_to_sample_on: Union[MeshObject, List[MeshObject]],\n                 face_sample_range: Optional[Union[Vector, np.ndarray, List[float]]] = None, min_height: float = 0.0,\n                 max_height: float = 1.0, use_ray_trace_check: bool = False,\n                 upper_dir: Optional[Union[Vector, np.ndarray, List[float]]] = None,\n                 use_upper_dir: bool = True) -> np.ndarray:\n    \"\"\"\n    Uniformly samples 3-dimensional value over the bounding box of the specified objects (can be just a plane) in the\n    defined upper direction. If \"use_upper_dir\" is False, samples along the face normal closest to \"upper_dir\". The\n    sampling volume results in a parallelepiped. \"min_height\" and \"max_height\" define the sampling distance from\n    the face.\n\n    Example 1: Sample a location on the surface of the given objects with height above this\n    surface in range of [1.5, 1.8].\n\n    .. code-block:: python\n\n        UpperRegionSampler.sample(\n            objects_to_sample_on=objs,\n            min_height=1.5,\n            max_height=1.8\n        )\n\n    :param objects_to_sample_on: Objects, on which to sample on.\n    :param face_sample_range: Restricts the area on the face where objects are sampled. Specifically describes\n                              relative lengths of both face vectors between which points are sampled.\n                              Default: [0.0, 1.0]\n    :param min_height: Minimum distance to the bounding box that a point is sampled on.\n    :param max_height: Maximum distance to the bounding box that a point is sampled on.\n    :param use_ray_trace_check: Toggles using a ray casting towards the sampled object (if the object is directly\n                                below the sampled position is the position accepted).\n    :param upper_dir: The 'up' direction of the sampling box. Default: [0.0, 0.0, 1.0].\n    :param use_upper_dir: Toggles using a ray casting towards the sampled object (if the object is directly\n                          below the sampled position is the position accepted).\n    :return: Sampled value.\n    \"\"\"\n    if face_sample_range is None:\n        face_sample_range = [0.0, 1.0]\n    if upper_dir is None:\n        upper_dir = [0.0, 0.0, 1.0]\n\n    face_sample_range = np.array(face_sample_range)\n    upper_dir = np.array(upper_dir)\n    upper_dir /= np.linalg.norm(upper_dir)\n    if not isinstance(objects_to_sample_on, list):\n        objects_to_sample_on = [objects_to_sample_on]\n\n    if max_height < min_height:\n        raise RuntimeError(f\"The minimum height ({min_height}) must be smaller than the maximum height ({max_height})!\")\n\n    regions = []\n\n    def calc_vec_and_normals(face: List[np.ndarray]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n        \"\"\" Calculates the two vectors, which lie in the plane of the face and the normal of the face.\n\n        :param face: Four corner coordinates of a face. Type: [4x[3xfloat]].\n        :return: (two vectors in the plane), and the normal.\n        \"\"\"\n        vec1 = face[1] - face[0]\n        vec2 = face[3] - face[0]\n        normal = np.cross(vec1, vec2)\n        normal /= np.linalg.norm(normal)\n        return (vec1, vec2), normal\n\n    # determine for each object in objects the region, where to sample on\n    for obj in objects_to_sample_on:\n        bb = obj.get_bound_box()\n        faces = []\n        faces.append([bb[0], bb[1], bb[2], bb[3]])\n        faces.append([bb[0], bb[4], bb[5], bb[1]])\n        faces.append([bb[1], bb[5], bb[6], bb[2]])\n        faces.append([bb[6], bb[7], bb[3], bb[2]])\n        faces.append([bb[3], bb[7], bb[4], bb[0]])\n        faces.append([bb[7], bb[6], bb[5], bb[4]])\n        # select the face, which has the smallest angle to the upper direction\n        min_diff_angle = 2 * math.pi\n        selected_face = None\n        for face in faces:\n            # calc the normal of all faces\n            _, normal = calc_vec_and_normals(face)\n            diff_angle = math.acos(normal.dot(upper_dir))\n            if diff_angle < min_diff_angle:\n                min_diff_angle = diff_angle\n                selected_face = face\n        # save the selected face values\n        if selected_face is not None:\n            vectors, normal = calc_vec_and_normals(selected_face)\n            base_point = selected_face[0]\n            regions.append(Region2D(vectors, normal, base_point))\n        else:\n            raise RuntimeError(f\"Couldn't find a face, for this obj: {obj.get_name()}\")\n\n    if regions and len(regions) == len(objects_to_sample_on):\n        selected_region_id = random.randint(0, len(regions) - 1)\n        selected_region, obj = regions[selected_region_id], objects_to_sample_on[selected_region_id]\n        if use_ray_trace_check:\n            inv_world_matrix = np.linalg.inv(obj.get_local2world_mat())\n        while True:\n            ret = selected_region.sample_point(face_sample_range)\n            dir_val = upper_dir if use_upper_dir else selected_region.normal()\n            ret += dir_val * random.uniform(min_height, max_height)\n            if use_ray_trace_check:\n                # transform the coords into the reference frame of the object\n                c_ret = inv_world_matrix @ np.concatenate((ret, [1]), 0)\n                c_dir = inv_world_matrix @ np.concatenate((dir_val * -1.0, [0]), 0)\n                # check if the object was hit\n                hit, _, _, _ = obj.ray_cast(c_ret[:3], c_dir[:3])\n                if hit:  # if the object was hit return\n                    break\n            else:\n                break\n        return np.array(ret)\n    raise RuntimeError(\"The amount of regions is either zero or does not match the amount of objects!\")",
  "class Region2D:\n    \"\"\" Helper class for UpperRegionSampler: Defines a 2D region in 3D.\n    \"\"\"\n\n    def __init__(self, vectors: Tuple[np.ndarray, np.ndarray], normal: np.ndarray, base_point: np.ndarray):\n        self._vectors = vectors  # the two vectors which lie in the selected face\n        self._normal = normal  # the normal of the selected face\n        self._base_point = base_point  # the base point of the selected face\n\n    def sample_point(self, face_sample_range: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Samples a point in the 2D Region\n\n        :param face_sample_range: relative lengths of both face vectors between which points are sampled\n        :return:\n        \"\"\"\n        ret = self._base_point.copy()\n        # walk over both vectors in the plane and determine a distance in both direction\n        for vec in self._vectors:\n            ret += vec * random.uniform(face_sample_range[0], face_sample_range[1])\n        return ret\n\n    def normal(self):\n        \"\"\"\n        :return: the normal of the region\n        \"\"\"\n        return self._normal",
  "def calc_vec_and_normals(face: List[np.ndarray]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n        \"\"\" Calculates the two vectors, which lie in the plane of the face and the normal of the face.\n\n        :param face: Four corner coordinates of a face. Type: [4x[3xfloat]].\n        :return: (two vectors in the plane), and the normal.\n        \"\"\"\n        vec1 = face[1] - face[0]\n        vec2 = face[3] - face[0]\n        normal = np.cross(vec1, vec2)\n        normal /= np.linalg.norm(normal)\n        return (vec1, vec2), normal",
  "def __init__(self, vectors: Tuple[np.ndarray, np.ndarray], normal: np.ndarray, base_point: np.ndarray):\n        self._vectors = vectors  # the two vectors which lie in the selected face\n        self._normal = normal  # the normal of the selected face\n        self._base_point = base_point",
  "def sample_point(self, face_sample_range: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Samples a point in the 2D Region\n\n        :param face_sample_range: relative lengths of both face vectors between which points are sampled\n        :return:\n        \"\"\"\n        ret = self._base_point.copy()\n        # walk over both vectors in the plane and determine a distance in both direction\n        for vec in self._vectors:\n            ret += vec * random.uniform(face_sample_range[0], face_sample_range[1])\n        return ret",
  "def normal(self):\n        \"\"\"\n        :return: the normal of the region\n        \"\"\"\n        return self._normal",
  "def load_replica_segmented_mesh(data_path: Union[str, Path], data_set_name: str,\n                                use_smooth_shading: bool = False) -> List[MeshObject]:\n    \"\"\"\n    Loads a segmented replica file\n\n    :param data_path: The path to the data folder, where all rooms are saved.\n    :param data_set_name: Name of the room (for example: apartment_0).\n    :param use_smooth_shading: if set to True all objects loaded, will have smooth shading activated\n    :return: The list of loaded and separated mesh objects.\n    \"\"\"\n    try:\n        # This import is done inside to avoid having the requirement that BlenderProc depends on plyfile\n        #pylint: disable=import-outside-toplevel\n        import plyfile\n        #pylint: enable=import-outside-toplevel\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"This function needs the plyfile lib, install it via:\"\n                                  \"\\n\\tblenderproc pip install plyfile\") from e\n\n    if isinstance(data_path, str):\n        data_path = Path(data_path)\n\n    current_folder = data_path / data_set_name\n\n    if not current_folder.exists():\n        raise Exception(f\"The dataset folder: \\\"{current_folder}\\\" does not exist!\")\n\n    json_file_path = current_folder / \"habitat\" / \"info_semantic.json\"\n\n    class_mapping = {}\n    with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n        data = json.load(file)\n\n    for ele in data[\"classes\"]:\n        if \"id\" in ele:\n            class_mapping[ele[\"id\"]] = ele[\"name\"]\n    for ele in data[\"objects\"]:\n        if \"id\" in ele:\n            class_mapping[ele[\"id\"]] = ele[\"class_name\"]\n\n    ply_segmented_path = current_folder / \"habitat\" / \"mesh_semantic.ply\"\n    if not ply_segmented_path.exists():\n        raise Exception(f\"Could not find \\\"{ply_segmented_path}\\\", the path was created automatically.\")\n\n    plydata = plyfile.PlyData.read(str(ply_segmented_path))\n\n    vertex_data = np.array([e.tolist() for e in plydata[\"vertex\"]])\n    vertices = vertex_data[:, :3]\n    normals = vertex_data[:, 3:6]\n    # add alpha channel\n    colors = np.concatenate([vertex_data[:, 6:].astype(float) / 255.0, np.ones((vertex_data.shape[0], 1))], axis=-1)\n\n    # extract the face indices and the class ids\n    face_indices = np.array([e.tolist()[0] for e in plydata[\"face\"]])\n    class_face_ids = np.array([e.tolist()[1] for e in plydata[\"face\"]])\n    used_class_ids = np.unique(class_face_ids)\n\n    objs = []\n    for current_class_id in used_class_ids:\n        used_obj_name = class_mapping.get(current_class_id, \"undefined\")\n        obj = create_with_empty_mesh(used_obj_name, used_obj_name + \"_mesh\")\n        # add this new data to the mesh object\n        mesh = obj.get_mesh()\n\n        # first select all currently used faces, based on the object id\n        current_face_indices = face_indices[class_face_ids == current_class_id]\n        amount_of_vertices = current_face_indices.shape[1]\n        current_face_indices = current_face_indices.reshape(-1)\n        # as we add all vertices used for the current object, the face indices order is just from 0\n        # to amount of vertices\n        vertex_indices = np.arange(0, current_face_indices.shape[0])\n\n        # add vertices\n        mesh.vertices.add(current_face_indices.shape[0])\n        mesh.vertices.foreach_set(\"co\", vertices[current_face_indices].reshape(-1))\n        mesh.vertices.foreach_set(\"normal\", normals[current_face_indices].reshape(-1))\n\n        # add faces\n        num_vertex_indicies = len(vertex_indices)\n        mesh.loops.add(num_vertex_indicies)\n        mesh.loops.foreach_set(\"vertex_index\", vertex_indices)\n\n        # the loops are set based on how the faces are a ranged\n        num_loops = int(num_vertex_indicies // amount_of_vertices)\n        mesh.polygons.add(num_loops)\n        # always amount_of_vertices vertices form one polygon\n        loop_start = np.arange(0, num_vertex_indicies, amount_of_vertices)\n        # the total size of each triangle is therefore amount_of_vertices\n        loop_total = [amount_of_vertices] * num_loops\n        mesh.polygons.foreach_set(\"loop_start\", loop_start)\n        mesh.polygons.foreach_set(\"loop_total\", loop_total)\n\n        # this update is needed else the vertex colors can't be set\n        mesh.update()\n\n        # check if the mesh already has some vertex colors\n        if not mesh.vertex_colors:\n            mesh.vertex_colors.new()\n        # get the newly created vertex colors\n        color_layer = mesh.vertex_colors[\"Col\"]\n        color_layer.data.foreach_set(\"color\", colors[current_face_indices].reshape(-1))\n        # one final update to integrate the vertex colors\n        mesh.update()\n\n        objs.append(obj)\n\n    # add smoothing if requested\n    if use_smooth_shading:\n        for obj in objs:\n            obj.set_shading_mode(\"SMOOTH\")\n\n    return objs",
  "def load_replica(data_path: str, data_set_name: str, use_smooth_shading: bool = False) -> List[MeshObject]:\n    \"\"\" Just imports the configured .ply file straight into blender for the replica case.\n\n    :param data_path: The path to the data folder, where all rooms are saved.\n    :param data_set_name: Name of the room (for example: apartment_0).\n    :param use_smooth_shading: Enable smooth shading on all surfaces, instead of flat shading.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    file_path = os.path.join(data_path, data_set_name, 'mesh.ply')\n    loaded_objects = load_obj(file_path)\n\n    if use_smooth_shading:\n        for obj in loaded_objects:\n            obj.set_shading_mode(\"SMOOTH\")\n\n    return loaded_objects",
  "def load_blend(path: str, obj_types: Optional[Union[List[str], str]] = None, name_regrex: Optional[str] = None,\n               data_blocks: Union[List[str], str] = \"objects\", link: bool = False) -> List[Entity]:\n    \"\"\"\n    Loads entities (everything that can be stored in a .blend file's folders, see Blender's documentation for\n    bpy.types.ID for more info) that match a name pattern from a specified .blend file's section/data_block.\n\n    :param path: Path to a .blend file.\n    :param obj_types: The type of objects to load. This parameter is only relevant when `data_blocks`\n                      is set to `\"objects\"`. Available options are: ['mesh', 'curve', 'hair', 'armature',\n                      'empty', 'light', 'camera']\n    :param name_regrex: Regular expression representing a name pattern of entities' (everything that can be\n                        stored in a .blend file's folders, see Blender's documentation for bpy.types.ID\n                        for more info) names.\n    :param data_blocks: The data block or a list of data blocks which should be loaded from the given .blend file.\n                        Available options are: ['armatures', 'cameras', 'curves', 'hairs', 'images', 'lights',\n                        'materials', 'meshes', 'objects', 'textures']\n    :param link: whether to link instead of append data blocks from .blend file. Linked objects can not be modified.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    if obj_types is None:\n        obj_types = [\"mesh\", \"empty\"]\n    # get a path to a .blend file\n    path = resolve_path(path)\n    data_blocks = _BlendLoader.validate_and_standardizes_configured_list(data_blocks, _BlendLoader.valid_data_blocks,\n                                                                         \"data block\")\n    obj_types = _BlendLoader.validate_and_standardizes_configured_list(obj_types, _BlendLoader.valid_object_types,\n                                                                       \"object type\")\n\n    # Remember which orphans existed beforehand\n    orphans_before = collect_all_orphan_data_blocks()\n\n    # Start importing blend file. All objects that should be imported need to be copied from \"data_from\" to \"data_to\"\n    with bpy.data.libraries.load(path, link=link) as (data_from, data_to):\n        for data_block in data_blocks:\n            # Verify that the given data block is valid\n            if hasattr(data_from, data_block):\n                # Find all entities of this data block that match the specified pattern\n                data_to_entities = []\n                for entity_name in getattr(data_from, data_block):\n                    if not name_regrex or re.fullmatch(name_regrex, entity_name) is not None:\n                        data_to_entities.append(entity_name)\n                # Import them\n                setattr(data_to, data_block, data_to_entities)\n                print(\"Imported \" + str(len(data_to_entities)) + \" \" + data_block)\n            else:\n                raise Exception(\"No such data block: \" + data_block)\n\n    # Go over all imported objects again\n    loaded_objects: List[Entity] = []\n    for data_block in data_blocks:\n        # Some adjustments that only affect objects\n        if data_block == \"objects\":\n            for obj in getattr(data_to, data_block):\n                # Check that the object type is desired\n                if obj.type.lower() in obj_types:\n                    # Link objects to the scene\n                    bpy.context.collection.objects.link(obj)\n                    loaded_objects.append(convert_to_entity_subclass(obj))\n\n                    # If a camera was imported\n                    if obj.type == 'CAMERA':\n                        # Make it the active camera in the scene\n                        bpy.context.scene.camera = obj\n\n                        # Find the maximum frame number of its key frames\n                        max_keyframe = -1\n                        if obj.animation_data is not None:\n                            fcurves = obj.animation_data.action.fcurves\n                            for curve in fcurves:\n                                keyframe_points = curve.keyframe_points\n                                for keyframe in keyframe_points:\n                                    max_keyframe = max(max_keyframe, keyframe.co[0])\n\n                        # Set frame_end to the next free keyframe\n                        bpy.context.scene.frame_end = max_keyframe + 1\n                else:\n                    # Remove object again if its type is not desired\n                    bpy.data.objects.remove(obj, do_unlink=True)\n            print(\"Selected \" + str(len(loaded_objects)) + \" of the loaded objects by type\")\n        else:\n            loaded_objects.extend(getattr(data_to, data_block))\n\n    # As some loaded objects were deleted again due to their type, we need also to remove the dependent\n    # data blocks that were also loaded and are now orphans\n    _BlendLoader.purge_added_orphans(orphans_before, data_to)\n    return loaded_objects",
  "class _BlendLoader:\n    valid_data_blocks = [collection.lower() for collection in dir(bpy.data) if\n                        isinstance(getattr(bpy.data, collection), bpy.types.bpy_prop_collection)]\n    valid_object_types = ['mesh', 'curve', 'surface', 'meta', 'font', 'hair', 'pointcloud', 'volume', 'gpencil',\n                          'armature', 'lattice', 'empty', 'light', 'light_probe', 'camera', 'speaker']\n\n    @staticmethod\n    def validate_and_standardizes_configured_list(config_value: Union[list, str], allowed_elements: list,\n                                                  element_name: str) -> list:\n        \"\"\" Makes sure the given config value is a list, is lower case and only consists of valid elements.\n\n        :param config_value: The configured value that should be standardized and validated.\n        :param allowed_elements: A list of valid elements. If one configured element is not contained in this list\n                                 an exception is thrown.\n        :param element_name: How one element is called. Used to create an error message.\n        :return: The standardized and validated config value.\n        \"\"\"\n        # Make sure we have a list\n        if not isinstance(config_value, list):\n            config_value = [config_value]\n        config_value = [element.lower() for element in config_value]\n\n        # Check that the given elements are valid\n        for element in config_value:\n            if element not in allowed_elements:\n                raise Exception(\"No such \" + element_name + \": \" + element)\n\n        return config_value\n\n    @staticmethod\n    def purge_added_orphans(orphans_before, data_to):\n        \"\"\" Removes all orphans that did not exists before loading the blend file.\n\n        :param orphans_before: A dict of sets containing orphans of all kind of data_blocks that existed before\n                               loading the blend file.\n        :param data_to: The list of objects that were loaded on purpose and should not be removed, even when\n                        they are orphans.\n        \"\"\"\n        purge_orphans = True\n        while purge_orphans:\n            purge_orphans = False\n            orphans_after = collect_all_orphan_data_blocks()\n            # Go over all data_block types\n            for collection_name, orphan_block in orphans_after.items():\n                # Go over all orphans of that type that were added due to this loader\n                for orphan in orphan_block.difference(orphans_before[collection_name]):\n                    # Check whether this orphan was loaded on purpose\n                    if orphan not in getattr(data_to, collection_name):\n                        # Remove the orphan\n                        getattr(bpy.data, collection_name).remove(orphan)\n                        # Make sure to run the loop again, so we can detect newly created orphans\n                        purge_orphans = True",
  "def validate_and_standardizes_configured_list(config_value: Union[list, str], allowed_elements: list,\n                                                  element_name: str) -> list:\n        \"\"\" Makes sure the given config value is a list, is lower case and only consists of valid elements.\n\n        :param config_value: The configured value that should be standardized and validated.\n        :param allowed_elements: A list of valid elements. If one configured element is not contained in this list\n                                 an exception is thrown.\n        :param element_name: How one element is called. Used to create an error message.\n        :return: The standardized and validated config value.\n        \"\"\"\n        # Make sure we have a list\n        if not isinstance(config_value, list):\n            config_value = [config_value]\n        config_value = [element.lower() for element in config_value]\n\n        # Check that the given elements are valid\n        for element in config_value:\n            if element not in allowed_elements:\n                raise Exception(\"No such \" + element_name + \": \" + element)\n\n        return config_value",
  "def purge_added_orphans(orphans_before, data_to):\n        \"\"\" Removes all orphans that did not exists before loading the blend file.\n\n        :param orphans_before: A dict of sets containing orphans of all kind of data_blocks that existed before\n                               loading the blend file.\n        :param data_to: The list of objects that were loaded on purpose and should not be removed, even when\n                        they are orphans.\n        \"\"\"\n        purge_orphans = True\n        while purge_orphans:\n            purge_orphans = False\n            orphans_after = collect_all_orphan_data_blocks()\n            # Go over all data_block types\n            for collection_name, orphan_block in orphans_after.items():\n                # Go over all orphans of that type that were added due to this loader\n                for orphan in orphan_block.difference(orphans_before[collection_name]):\n                    # Check whether this orphan was loaded on purpose\n                    if orphan not in getattr(data_to, collection_name):\n                        # Remove the orphan\n                        getattr(bpy.data, collection_name).remove(orphan)\n                        # Make sure to run the loop again, so we can detect newly created orphans\n                        purge_orphans = True",
  "def load_suncg(house_path: str, label_mapping: LabelIdMapping,\n               suncg_dir: Optional[str] = None) -> List[Union[Entity, MeshObject]]:\n    \"\"\" Loads a house.json file into blender.\n\n    - Loads all objects files specified in the house.json file.\n    - Orders them hierarchically (level -> room -> object)\n    - Writes metadata into the custom properties of each object\n\n    :param house_path: The path to the house.json file which should be loaded.\n    :param label_mapping: A dict which maps the names of the objects to ids.\n    :param suncg_dir: The path to the suncg root directory which should be used for loading objects,\n                      rooms, textures etc.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    # If not suncg root directory has been given, determine it via the given house directory.\n    if suncg_dir is None:\n        suncg_dir = os.path.join(os.path.dirname(house_path), \"..\", \"..\")\n\n    _SuncgLoader.suncg_dir = suncg_dir\n    _SuncgLoader.collection_of_loaded_objs = {}\n    # there are only two types of materials, textures and diffuse\n    _SuncgLoader.collection_of_loaded_mats = {\"texture\": {}, \"diffuse\": {}}\n\n    with open(resolve_path(house_path), \"r\", encoding=\"utf-8\") as f:\n        config = json.load(f)\n\n    model_category_mapping_path = resolve_resource(os.path.join('suncg', 'Better_labeling_for_NYU.csv'))\n    result = _SuncgLoader.read_model_category_mapping(model_category_mapping_path)\n    object_label_map, object_fine_grained_label_map, object_coarse_grained_label_map = result\n\n    house_id = config[\"id\"]\n    loaded_objects = []\n\n    for level in config[\"levels\"]:\n        # Build empty level object which acts as a parent for all rooms on the level\n        level_obj = create_empty(\"Level#\" + level[\"id\"])\n        level_obj.set_cp(\"suncg_type\", \"Level\")\n        if \"bbox\" in level:\n            level_obj.set_cp(\"bbox\", _SuncgLoader.correct_bbox_frame(level[\"bbox\"]))\n        else:\n            print(\"Warning: The level with id \" + level[\n                \"id\"] + \" is missing the bounding box attribute in the given house.json file!\")\n        loaded_objects.append(level_obj)\n\n        room_per_object: Dict[int, Entity] = {}\n\n        for node in level[\"nodes\"]:\n            # Skip invalid nodes (This is the same behavior as in the SUNCG Toolbox)\n            if \"valid\" in node and node[\"valid\"] == 0:\n                continue\n\n            # Metadata is directly stored in the objects custom data\n            metadata = {\n                \"type\": node[\"type\"],\n                \"is_suncg\": True\n            }\n\n            if \"modelId\" in node:\n                metadata[\"modelId\"] = node[\"modelId\"]\n\n                if node[\"modelId\"] in object_fine_grained_label_map:\n                    metadata[\"fine_grained_class\"] = object_fine_grained_label_map[node[\"modelId\"]]\n                    metadata[\"coarse_grained_class\"] = object_coarse_grained_label_map[node[\"modelId\"]]\n                    metadata[\"category_id\"] = label_mapping.id_from_label(object_label_map[node[\"modelId\"]])\n\n            if \"bbox\" in node:\n                metadata[\"bbox\"] = _SuncgLoader.correct_bbox_frame(node[\"bbox\"])\n\n            if \"transform\" in node:\n                transform = Matrix([node[\"transform\"][i * 4:(i + 1) * 4] for i in range(4)])\n                # Transpose, as given transform matrix was col-wise, but blender expects row-wise\n                transform.transpose()\n            else:\n                transform = None\n\n            if \"materials\" in node:\n                material_adjustments = node[\"materials\"]\n            else:\n                material_adjustments = []\n\n            # Lookup if the object belongs to a room\n            object_id = int(node[\"id\"].split(\"_\")[-1])\n            parent = room_per_object.get(object_id, level_obj)\n\n            if node[\"type\"] == \"Room\":\n                loaded_objects += _SuncgLoader.load_room(node, metadata, material_adjustments, transform, house_id,\n                                                         level_obj, room_per_object, label_mapping)\n            elif node[\"type\"] == \"Ground\":\n                loaded_objects += _SuncgLoader.load_ground(node, metadata, material_adjustments, transform, house_id,\n                                                           parent, label_mapping)\n            elif node[\"type\"] == \"Object\":\n                loaded_objects += _SuncgLoader.load_object(node, metadata, material_adjustments, transform, parent)\n            elif node[\"type\"] == \"Box\":\n                loaded_objects += _SuncgLoader.load_box(node, material_adjustments, transform, parent, label_mapping)\n    _SuncgLoader.rename_materials()\n    return loaded_objects",
  "class _SuncgLoader:\n    suncg_dir: Optional[str] = None\n    collection_of_loaded_objs: Dict[str, List[MeshObject]] = {}\n    collection_of_loaded_mats: Dict[str, Dict[str, Material]] = {}\n\n    @staticmethod\n    def rename_materials():\n        \"\"\"\n        Rename all materials based on their texture if they have one\n\n        This makes the accessing later on easier\n        \"\"\"\n        # TODO: should only be done to suncg materials\n        for material in bpy.data.materials:\n            if material.use_nodes:\n                nodes = material.node_tree.nodes\n                textures = Utility.get_nodes_with_type(nodes, \"ShaderNodeTexImage\")\n                if len(textures) == 1:\n                    material.name = textures[0].image.name\n\n    @staticmethod\n    def load_room(node: Dict[str, Any], metadata: Dict[str, Union[str, int]],\n                  material_adjustments: List[Dict[str, str]], transform: Matrix, house_id: str,\n                  parent: Entity, room_per_object: Dict[int, Entity], label_mapping: LabelIdMapping) \\\n            -> List[Union[Entity, MeshObject]]:\n        \"\"\" Load the room specified in the given node.\n\n        :param node: The node dict which contains information from house.json..\n        :param metadata: A dict of metadata which will be written into the object's custom data.\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param house_id: The id of the current house.\n        :param parent: The parent object to which the room should be linked\n        :param room_per_object: A dict for object -> room lookup (Will be written into)\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # Build empty room object which acts as a parent for all objects inside\n        room_obj = create_empty(\"Room#\" + node[\"id\"])\n        room_obj.set_cp(\"suncg_type\", \"Room\")\n        room_obj.set_cp(\"bbox\", _SuncgLoader.correct_bbox_frame(node[\"bbox\"]))\n        room_obj.set_cp(\"roomTypes\", node[\"roomTypes\"])\n        room_obj.set_parent(parent)\n        loaded_objects = [room_obj]\n\n        # Store indices of all contained objects in\n        if \"nodeIndices\" in node:\n            for child_id in node[\"nodeIndices\"]:\n                room_per_object[child_id] = room_obj\n\n        if \"hideFloor\" not in node or node[\"hideFloor\"] != 1:\n            metadata[\"type\"] = \"Floor\"\n            metadata[\"category_id\"] = label_mapping.id_from_label(\"floor\")\n            metadata[\"fine_grained_class\"] = \"floor\"\n            loaded_objects += _SuncgLoader.load_obj(\n                os.path.join(_SuncgLoader.suncg_dir, \"room\", house_id, node[\"modelId\"] + \"f.obj\"), metadata,\n                material_adjustments, transform, room_obj)\n\n        if \"hideCeiling\" not in node or node[\"hideCeiling\"] != 1:\n            metadata[\"type\"] = \"Ceiling\"\n            metadata[\"category_id\"] = label_mapping.id_from_label(\"ceiling\")\n            metadata[\"fine_grained_class\"] = \"ceiling\"\n            loaded_objects += _SuncgLoader.load_obj(\n                os.path.join(_SuncgLoader.suncg_dir, \"room\", house_id, node[\"modelId\"] + \"c.obj\"), metadata,\n                material_adjustments, transform, room_obj)\n\n        if \"hideWalls\" not in node or node[\"hideWalls\"] != 1:\n            metadata[\"type\"] = \"Wall\"\n            metadata[\"category_id\"] = label_mapping.id_from_label(\"wall\")\n            metadata[\"fine_grained_class\"] = \"wall\"\n            loaded_objects += _SuncgLoader.load_obj(\n                os.path.join(_SuncgLoader.suncg_dir, \"room\", house_id, node[\"modelId\"] + \"w.obj\"), metadata,\n                material_adjustments, transform, room_obj)\n\n        return loaded_objects\n\n    @staticmethod\n    def load_ground(node: Dict[str, Any], metadata: Dict[str, Union[str, int]],\n                    material_adjustments: List[Dict[str, str]],\n                    transform: Matrix, house_id: str, parent: Entity, label_mapping: LabelIdMapping) -> List[\n        MeshObject]:\n        \"\"\" Load the ground specified in the given node.\n\n        :param node: The node dict which contains information from house.json..\n        :param metadata: A dict of metadata which will be written into the object's custom data.\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param house_id: The id of the current house.\n        :param parent: The parent object to which the ground should be linked\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        metadata[\"type\"] = \"Ground\"\n        metadata[\"category_id\"] = label_mapping.id_from_label(\"floor\")\n        metadata[\"fine_grained_class\"] = \"ground\"\n        return _SuncgLoader.load_obj(os.path.join(_SuncgLoader.suncg_dir, \"room\", house_id, node[\"modelId\"] + \"f.obj\"),\n                                     metadata, material_adjustments, transform, parent)\n\n    @staticmethod\n    def load_object(node: Dict[str, Any], metadata: Dict[str, Union[str, int]],\n                    material_adjustments: List[Dict[str, str]],\n                    transform: Matrix, parent: Entity) -> List[MeshObject]:\n        \"\"\" Load the object specified in the given node.\n\n        :param node: The node dict which contains information from house.json..\n        :param metadata: A dict of metadata which will be written into the object's custom data.\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param parent: The parent object to which the ground should be linked\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        if \"state\" not in node or node[\"state\"] == 0:\n            file_path = os.path.join(_SuncgLoader.suncg_dir, \"object\", node[\"modelId\"], node[\"modelId\"] + \".obj\")\n            return _SuncgLoader.load_obj(file_path, metadata, material_adjustments, transform, parent)\n        file_path = os.path.join(_SuncgLoader.suncg_dir, \"object\", node[\"modelId\"], node[\"modelId\"] + \"_0.obj\")\n        return _SuncgLoader.load_obj(file_path, metadata, material_adjustments, transform, parent)\n\n    @staticmethod\n    def correct_bbox_frame(bbox: Dict[str, Union[np.ndarray, list, Vector]]) -> Dict[str, np.ndarray]:\n        \"\"\" Corrects the coordinate frame of the given bbox.\n\n        :param bbox: The bbox.\n        :return: The corrected bbox.\n        \"\"\"\n        return {\n            \"min\": change_coordinate_frame_of_point(bbox[\"min\"], [\"X\", \"-Z\", \"Y\"]),\n            \"max\": change_coordinate_frame_of_point(bbox[\"max\"], [\"X\", \"-Z\", \"Y\"])\n        }\n\n    @staticmethod\n    def load_box(node: Dict[str, Any], material_adjustments: List[Dict[str, str]], transform: Matrix, parent: Entity,\n                 label_mapping: LabelIdMapping) -> List[MeshObject]:\n        \"\"\" Creates a cube inside blender which follows the specifications of the given node.\n\n        :param node: The node dict which contains information from house.json..\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param parent: The parent object to which the ground should be linked\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        box = create_primitive(\"CUBE\")\n        box.set_name(\"Box#\" + node[\"id\"])\n        # Scale the cube to the required dimensions\n        local2world_mat = Matrix.Scale(node[\"dimensions\"][0] / 2, 4, (1.0, 0.0, 0.0)) \\\n                          @ Matrix.Scale(node[\"dimensions\"][1] / 2, 4, (0.0, 1.0, 0.0)) \\\n                          @ Matrix.Scale(node[\"dimensions\"][2] / 2, 4, (0.0, 0.0, 1.0))\n        box.set_local2world_mat(local2world_mat)\n        bpy.ops.object.editmode_toggle()\n        bpy.ops.uv.cube_project()\n        bpy.ops.object.editmode_toggle()\n\n        # Create an empty material which is filled in the next step\n        box.new_material(\"material_0\")\n\n        _SuncgLoader.transform_and_colorize_object(box, material_adjustments, transform, parent)\n        # set class to void\n        box.set_cp(\"category_id\", label_mapping.id_from_label(\"void\"))\n        # Rotate cube to match objects loaded from .obj, has to be done after transformations have been applied\n        box.set_local2world_mat(Matrix.Rotation(math.radians(90), 4, \"X\") @ Matrix(box.get_local2world_mat()))\n\n        return [box]\n\n    @staticmethod\n    def load_obj(path: str, metadata: Dict[str, Union[str, int]], material_adjustments: List[Dict[str, str]],\n                 transform: Optional[Matrix] = None, parent: Optional[Entity] = None) -> List[MeshObject]:\n        \"\"\" Load the wavefront object file from the given path and adjust according to the given arguments.\n\n        :param path: The path to the .obj file.\n        :param metadata: A dict of metadata which will be written into the object's custom data.\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param parent: The parent object to which the object should be linked\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        if not os.path.exists(path):\n            print(f\"Warning: {path} is missing!\")\n            return []\n        object_already_loaded = path in _SuncgLoader.collection_of_loaded_objs\n        loaded_objects = load_obj(filepath=path, cached_objects=_SuncgLoader.collection_of_loaded_objs)\n        if object_already_loaded:\n            print(f\"Duplicate object: {path}\")\n            for obj in loaded_objects:\n                # the original object matrix from the .obj loader -> is not an identity matrix\n                obj.set_local2world_mat(Matrix([[1, 0, 0, 0], [0, 0, -1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]))\n                # remove all custom properties\n                obj.clear_all_cps()\n        # Go through all imported objects\n        for obj in loaded_objects:\n            for key in metadata.keys():\n                used_key = key\n                if key == \"type\":\n                    used_key = \"suncg_type\"\n                obj.set_cp(used_key, metadata[key])\n\n            _SuncgLoader.transform_and_colorize_object(obj, material_adjustments, transform, parent)\n\n        return loaded_objects\n\n    @staticmethod\n    def transform_and_colorize_object(obj: MeshObject, material_adjustments: List[Dict[str, str]],\n                                      transform: Optional[Matrix] = None, parent: Optional[Entity] = None):\n        \"\"\" Applies the given transformation to the object and refactors its materials.\n\n        Material is replaced with an existing material if possible or is changed according to the material_adjustments\n\n        :param obj: The object to use.\n        :param material_adjustments: A list of adjustments to make. (Each element i corresponds to material_i)\n        :param transform: The transformation matrix to apply\n        :param parent: The parent object to which the object should be linked\n        \"\"\"\n        if parent is not None:\n            obj.set_parent(parent)\n\n        if transform is not None:\n            # Apply transformation\n            obj.apply_T(transform)\n\n        for i, mat in enumerate(obj.get_materials()):\n            if mat is None:\n                continue\n            # the material name of an object contains a nr, which is mentioned in the material_adjustments\n            index = mat.get_name()[mat.get_name().find(\"_\") + 1:]\n            if \".\" in index:\n                index = index[:index.find(\".\")]\n            index = int(index)\n\n            # check if this index is mentioned in material_adjustments and if a texture is necessary\n            force_texture = index < len(material_adjustments) and \"texture\" in material_adjustments[index]\n            _SuncgLoader.recreate_material_nodes(mat, force_texture)\n\n            if index < len(material_adjustments):\n                _SuncgLoader.adjust_material_nodes(mat, material_adjustments[index])\n            mat_type, value = _SuncgLoader.get_type_and_value_from_mat(mat)\n            current_mats = _SuncgLoader.collection_of_loaded_mats[mat_type]\n            if value in current_mats:\n                obj.set_material(i, current_mats[value])\n            else:\n                # save the current material for later\n                current_mats[value] = mat\n\n    @staticmethod\n    def get_type_and_value_from_mat(mat: Material) -> Tuple[str, str]:\n        \"\"\"\n        Returns the type of the material -> either diffuse or with texture (there are only two in SUNCG)\n\n        :param mat: the material where the type and value should be determined\n        :return: mat_type, value: mat_type is either \"diffuse\" or \"texture\", the value contains either name of the \\\n                                 image or the color mapped to an RGB string of the values\n        \"\"\"\n        image_node = mat.get_nodes_with_type('TexImage')\n        if len(image_node) == 1:\n            # there is an image node -> type texture\n            mat_type = \"texture\"\n            image_node = image_node[0]\n            if image_node.image is None:\n                raise RuntimeError(f\"The image does not have a texture for material: {mat.get_name()}\")\n            value = image_node.image.name\n            if \".\" in value:\n                value = value[:value.find(\".\")]\n        else:\n            mat_type = \"diffuse\"\n            principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n            used_keys = list(principled_node.inputs[\"Base Color\"].default_value)\n            alpha = principled_node.inputs['Alpha'].default_value\n            used_keys.append(alpha)\n            value = \"_\".join([str(int(255. * ele)) for ele in used_keys])\n        return mat_type, value\n\n    @staticmethod\n    def recreate_material_nodes(mat: Material, force_texture: bool):\n        \"\"\" Remove all nodes and recreate a diffuse node, optionally with texture.\n\n        This will replace all material nodes with only a diffuse and a texturing node (to speedup rendering).\n\n        :param mat: The blender material\n        :param force_texture: True, if a texture node should always be created even if the material\n                              has no texture at the moment\n        \"\"\"\n        image_node = mat.get_nodes_with_type('TexImage')\n        # if there is no image no create one\n        if force_texture and len(image_node) == 0:\n            # The principled BSDF node contains all imported material properties\n            principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n\n            uv_node = mat.new_node('ShaderNodeTexCoord')\n            # create an image node and link it\n            image_node = mat.new_node('ShaderNodeTexImage')\n            mat.link(uv_node.outputs['UV'], image_node.inputs['Vector'])\n            mat.link(image_node.outputs['Color'], principled_node.inputs['Base Color'])\n\n    @staticmethod\n    def adjust_material_nodes(mat: Material, adjustments: Dict[str, str]):\n        \"\"\" Adjust the material node of the given material according to the given adjustments.\n\n        Textures or diffuse colors will be changed according to the given material_adjustments.\n\n        :param mat: The blender material.\n        :param adjustments: A dict containing a new \"diffuse\" color or a new \"texture\" path\n        \"\"\"\n\n        if \"diffuse\" in adjustments:\n            principle_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n            principle_node.inputs['Base Color'].default_value = Utility.hex_to_rgba(adjustments[\"diffuse\"])\n\n        if \"texture\" in adjustments:\n            image_path = os.path.join(_SuncgLoader.suncg_dir, \"texture\", adjustments[\"texture\"])\n            image_path = resolve_path(image_path)\n\n            if os.path.exists(image_path + \".png\"):\n                image_path += \".png\"\n            else:\n                image_path += \".jpg\"\n\n            image_node = mat.get_the_one_node_with_type(\"ShaderNodeTexImage\")\n            if os.path.exists(image_path):\n                image_node.image = bpy.data.images.load(image_path, check_existing=True)\n            else:\n                print(f\"Warning: Cannot load texture, path does not exist: {image_path}, remove image node again\")\n                mat.remove_node(image_node)\n\n    @staticmethod\n    def read_model_category_mapping(path: str):\n        \"\"\" Reads in the model category mapping csv.\n\n        :param path: The path to the csv file.\n        \"\"\"\n        object_label_map = {}\n        object_fine_grained_label_map = {}\n        object_coarse_grained_label_map = {}\n\n        with open(resolve_path(path), 'r', encoding=\"utf-8\") as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                object_label_map[row[\"model_id\"]] = row[\"nyuv2_40class\"]\n                object_fine_grained_label_map[row[\"model_id\"]] = row[\"fine_grained_class\"]\n                object_coarse_grained_label_map[row[\"model_id\"]] = row[\"coarse_grained_class\"]\n\n        return object_label_map, object_fine_grained_label_map, object_coarse_grained_label_map",
  "def rename_materials():\n        \"\"\"\n        Rename all materials based on their texture if they have one\n\n        This makes the accessing later on easier\n        \"\"\"\n        # TODO: should only be done to suncg materials\n        for material in bpy.data.materials:\n            if material.use_nodes:\n                nodes = material.node_tree.nodes\n                textures = Utility.get_nodes_with_type(nodes, \"ShaderNodeTexImage\")\n                if len(textures) == 1:\n                    material.name = textures[0].image.name",
  "def load_room(node: Dict[str, Any], metadata: Dict[str, Union[str, int]],\n                  material_adjustments: List[Dict[str, str]], transform: Matrix, house_id: str,\n                  parent: Entity, room_per_object: Dict[int, Entity], label_mapping: LabelIdMapping) \\\n            -> List[Union[Entity, MeshObject]]:\n        \"\"\" Load the room specified in the given node.\n\n        :param node: The node dict which contains information from house.json..\n        :param metadata: A dict of metadata which will be written into the object's custom data.\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param house_id: The id of the current house.\n        :param parent: The parent object to which the room should be linked\n        :param room_per_object: A dict for object -> room lookup (Will be written into)\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # Build empty room object which acts as a parent for all objects inside\n        room_obj = create_empty(\"Room#\" + node[\"id\"])\n        room_obj.set_cp(\"suncg_type\", \"Room\")\n        room_obj.set_cp(\"bbox\", _SuncgLoader.correct_bbox_frame(node[\"bbox\"]))\n        room_obj.set_cp(\"roomTypes\", node[\"roomTypes\"])\n        room_obj.set_parent(parent)\n        loaded_objects = [room_obj]\n\n        # Store indices of all contained objects in\n        if \"nodeIndices\" in node:\n            for child_id in node[\"nodeIndices\"]:\n                room_per_object[child_id] = room_obj\n\n        if \"hideFloor\" not in node or node[\"hideFloor\"] != 1:\n            metadata[\"type\"] = \"Floor\"\n            metadata[\"category_id\"] = label_mapping.id_from_label(\"floor\")\n            metadata[\"fine_grained_class\"] = \"floor\"\n            loaded_objects += _SuncgLoader.load_obj(\n                os.path.join(_SuncgLoader.suncg_dir, \"room\", house_id, node[\"modelId\"] + \"f.obj\"), metadata,\n                material_adjustments, transform, room_obj)\n\n        if \"hideCeiling\" not in node or node[\"hideCeiling\"] != 1:\n            metadata[\"type\"] = \"Ceiling\"\n            metadata[\"category_id\"] = label_mapping.id_from_label(\"ceiling\")\n            metadata[\"fine_grained_class\"] = \"ceiling\"\n            loaded_objects += _SuncgLoader.load_obj(\n                os.path.join(_SuncgLoader.suncg_dir, \"room\", house_id, node[\"modelId\"] + \"c.obj\"), metadata,\n                material_adjustments, transform, room_obj)\n\n        if \"hideWalls\" not in node or node[\"hideWalls\"] != 1:\n            metadata[\"type\"] = \"Wall\"\n            metadata[\"category_id\"] = label_mapping.id_from_label(\"wall\")\n            metadata[\"fine_grained_class\"] = \"wall\"\n            loaded_objects += _SuncgLoader.load_obj(\n                os.path.join(_SuncgLoader.suncg_dir, \"room\", house_id, node[\"modelId\"] + \"w.obj\"), metadata,\n                material_adjustments, transform, room_obj)\n\n        return loaded_objects",
  "def load_ground(node: Dict[str, Any], metadata: Dict[str, Union[str, int]],\n                    material_adjustments: List[Dict[str, str]],\n                    transform: Matrix, house_id: str, parent: Entity, label_mapping: LabelIdMapping) -> List[\n        MeshObject]:\n        \"\"\" Load the ground specified in the given node.\n\n        :param node: The node dict which contains information from house.json..\n        :param metadata: A dict of metadata which will be written into the object's custom data.\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param house_id: The id of the current house.\n        :param parent: The parent object to which the ground should be linked\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        metadata[\"type\"] = \"Ground\"\n        metadata[\"category_id\"] = label_mapping.id_from_label(\"floor\")\n        metadata[\"fine_grained_class\"] = \"ground\"\n        return _SuncgLoader.load_obj(os.path.join(_SuncgLoader.suncg_dir, \"room\", house_id, node[\"modelId\"] + \"f.obj\"),\n                                     metadata, material_adjustments, transform, parent)",
  "def load_object(node: Dict[str, Any], metadata: Dict[str, Union[str, int]],\n                    material_adjustments: List[Dict[str, str]],\n                    transform: Matrix, parent: Entity) -> List[MeshObject]:\n        \"\"\" Load the object specified in the given node.\n\n        :param node: The node dict which contains information from house.json..\n        :param metadata: A dict of metadata which will be written into the object's custom data.\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param parent: The parent object to which the ground should be linked\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        if \"state\" not in node or node[\"state\"] == 0:\n            file_path = os.path.join(_SuncgLoader.suncg_dir, \"object\", node[\"modelId\"], node[\"modelId\"] + \".obj\")\n            return _SuncgLoader.load_obj(file_path, metadata, material_adjustments, transform, parent)\n        file_path = os.path.join(_SuncgLoader.suncg_dir, \"object\", node[\"modelId\"], node[\"modelId\"] + \"_0.obj\")\n        return _SuncgLoader.load_obj(file_path, metadata, material_adjustments, transform, parent)",
  "def correct_bbox_frame(bbox: Dict[str, Union[np.ndarray, list, Vector]]) -> Dict[str, np.ndarray]:\n        \"\"\" Corrects the coordinate frame of the given bbox.\n\n        :param bbox: The bbox.\n        :return: The corrected bbox.\n        \"\"\"\n        return {\n            \"min\": change_coordinate_frame_of_point(bbox[\"min\"], [\"X\", \"-Z\", \"Y\"]),\n            \"max\": change_coordinate_frame_of_point(bbox[\"max\"], [\"X\", \"-Z\", \"Y\"])\n        }",
  "def load_box(node: Dict[str, Any], material_adjustments: List[Dict[str, str]], transform: Matrix, parent: Entity,\n                 label_mapping: LabelIdMapping) -> List[MeshObject]:\n        \"\"\" Creates a cube inside blender which follows the specifications of the given node.\n\n        :param node: The node dict which contains information from house.json..\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param parent: The parent object to which the ground should be linked\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        box = create_primitive(\"CUBE\")\n        box.set_name(\"Box#\" + node[\"id\"])\n        # Scale the cube to the required dimensions\n        local2world_mat = Matrix.Scale(node[\"dimensions\"][0] / 2, 4, (1.0, 0.0, 0.0)) \\\n                          @ Matrix.Scale(node[\"dimensions\"][1] / 2, 4, (0.0, 1.0, 0.0)) \\\n                          @ Matrix.Scale(node[\"dimensions\"][2] / 2, 4, (0.0, 0.0, 1.0))\n        box.set_local2world_mat(local2world_mat)\n        bpy.ops.object.editmode_toggle()\n        bpy.ops.uv.cube_project()\n        bpy.ops.object.editmode_toggle()\n\n        # Create an empty material which is filled in the next step\n        box.new_material(\"material_0\")\n\n        _SuncgLoader.transform_and_colorize_object(box, material_adjustments, transform, parent)\n        # set class to void\n        box.set_cp(\"category_id\", label_mapping.id_from_label(\"void\"))\n        # Rotate cube to match objects loaded from .obj, has to be done after transformations have been applied\n        box.set_local2world_mat(Matrix.Rotation(math.radians(90), 4, \"X\") @ Matrix(box.get_local2world_mat()))\n\n        return [box]",
  "def load_obj(path: str, metadata: Dict[str, Union[str, int]], material_adjustments: List[Dict[str, str]],\n                 transform: Optional[Matrix] = None, parent: Optional[Entity] = None) -> List[MeshObject]:\n        \"\"\" Load the wavefront object file from the given path and adjust according to the given arguments.\n\n        :param path: The path to the .obj file.\n        :param metadata: A dict of metadata which will be written into the object's custom data.\n        :param material_adjustments: Adjustments to the materials which were specified inside house.json.\n        :param transform: The transformation that should be applied to the loaded objects.\n        :param parent: The parent object to which the object should be linked\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        if not os.path.exists(path):\n            print(f\"Warning: {path} is missing!\")\n            return []\n        object_already_loaded = path in _SuncgLoader.collection_of_loaded_objs\n        loaded_objects = load_obj(filepath=path, cached_objects=_SuncgLoader.collection_of_loaded_objs)\n        if object_already_loaded:\n            print(f\"Duplicate object: {path}\")\n            for obj in loaded_objects:\n                # the original object matrix from the .obj loader -> is not an identity matrix\n                obj.set_local2world_mat(Matrix([[1, 0, 0, 0], [0, 0, -1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]))\n                # remove all custom properties\n                obj.clear_all_cps()\n        # Go through all imported objects\n        for obj in loaded_objects:\n            for key in metadata.keys():\n                used_key = key\n                if key == \"type\":\n                    used_key = \"suncg_type\"\n                obj.set_cp(used_key, metadata[key])\n\n            _SuncgLoader.transform_and_colorize_object(obj, material_adjustments, transform, parent)\n\n        return loaded_objects",
  "def transform_and_colorize_object(obj: MeshObject, material_adjustments: List[Dict[str, str]],\n                                      transform: Optional[Matrix] = None, parent: Optional[Entity] = None):\n        \"\"\" Applies the given transformation to the object and refactors its materials.\n\n        Material is replaced with an existing material if possible or is changed according to the material_adjustments\n\n        :param obj: The object to use.\n        :param material_adjustments: A list of adjustments to make. (Each element i corresponds to material_i)\n        :param transform: The transformation matrix to apply\n        :param parent: The parent object to which the object should be linked\n        \"\"\"\n        if parent is not None:\n            obj.set_parent(parent)\n\n        if transform is not None:\n            # Apply transformation\n            obj.apply_T(transform)\n\n        for i, mat in enumerate(obj.get_materials()):\n            if mat is None:\n                continue\n            # the material name of an object contains a nr, which is mentioned in the material_adjustments\n            index = mat.get_name()[mat.get_name().find(\"_\") + 1:]\n            if \".\" in index:\n                index = index[:index.find(\".\")]\n            index = int(index)\n\n            # check if this index is mentioned in material_adjustments and if a texture is necessary\n            force_texture = index < len(material_adjustments) and \"texture\" in material_adjustments[index]\n            _SuncgLoader.recreate_material_nodes(mat, force_texture)\n\n            if index < len(material_adjustments):\n                _SuncgLoader.adjust_material_nodes(mat, material_adjustments[index])\n            mat_type, value = _SuncgLoader.get_type_and_value_from_mat(mat)\n            current_mats = _SuncgLoader.collection_of_loaded_mats[mat_type]\n            if value in current_mats:\n                obj.set_material(i, current_mats[value])\n            else:\n                # save the current material for later\n                current_mats[value] = mat",
  "def get_type_and_value_from_mat(mat: Material) -> Tuple[str, str]:\n        \"\"\"\n        Returns the type of the material -> either diffuse or with texture (there are only two in SUNCG)\n\n        :param mat: the material where the type and value should be determined\n        :return: mat_type, value: mat_type is either \"diffuse\" or \"texture\", the value contains either name of the \\\n                                 image or the color mapped to an RGB string of the values\n        \"\"\"\n        image_node = mat.get_nodes_with_type('TexImage')\n        if len(image_node) == 1:\n            # there is an image node -> type texture\n            mat_type = \"texture\"\n            image_node = image_node[0]\n            if image_node.image is None:\n                raise RuntimeError(f\"The image does not have a texture for material: {mat.get_name()}\")\n            value = image_node.image.name\n            if \".\" in value:\n                value = value[:value.find(\".\")]\n        else:\n            mat_type = \"diffuse\"\n            principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n            used_keys = list(principled_node.inputs[\"Base Color\"].default_value)\n            alpha = principled_node.inputs['Alpha'].default_value\n            used_keys.append(alpha)\n            value = \"_\".join([str(int(255. * ele)) for ele in used_keys])\n        return mat_type, value",
  "def recreate_material_nodes(mat: Material, force_texture: bool):\n        \"\"\" Remove all nodes and recreate a diffuse node, optionally with texture.\n\n        This will replace all material nodes with only a diffuse and a texturing node (to speedup rendering).\n\n        :param mat: The blender material\n        :param force_texture: True, if a texture node should always be created even if the material\n                              has no texture at the moment\n        \"\"\"\n        image_node = mat.get_nodes_with_type('TexImage')\n        # if there is no image no create one\n        if force_texture and len(image_node) == 0:\n            # The principled BSDF node contains all imported material properties\n            principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n\n            uv_node = mat.new_node('ShaderNodeTexCoord')\n            # create an image node and link it\n            image_node = mat.new_node('ShaderNodeTexImage')\n            mat.link(uv_node.outputs['UV'], image_node.inputs['Vector'])\n            mat.link(image_node.outputs['Color'], principled_node.inputs['Base Color'])",
  "def adjust_material_nodes(mat: Material, adjustments: Dict[str, str]):\n        \"\"\" Adjust the material node of the given material according to the given adjustments.\n\n        Textures or diffuse colors will be changed according to the given material_adjustments.\n\n        :param mat: The blender material.\n        :param adjustments: A dict containing a new \"diffuse\" color or a new \"texture\" path\n        \"\"\"\n\n        if \"diffuse\" in adjustments:\n            principle_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n            principle_node.inputs['Base Color'].default_value = Utility.hex_to_rgba(adjustments[\"diffuse\"])\n\n        if \"texture\" in adjustments:\n            image_path = os.path.join(_SuncgLoader.suncg_dir, \"texture\", adjustments[\"texture\"])\n            image_path = resolve_path(image_path)\n\n            if os.path.exists(image_path + \".png\"):\n                image_path += \".png\"\n            else:\n                image_path += \".jpg\"\n\n            image_node = mat.get_the_one_node_with_type(\"ShaderNodeTexImage\")\n            if os.path.exists(image_path):\n                image_node.image = bpy.data.images.load(image_path, check_existing=True)\n            else:\n                print(f\"Warning: Cannot load texture, path does not exist: {image_path}, remove image node again\")\n                mat.remove_node(image_node)",
  "def read_model_category_mapping(path: str):\n        \"\"\" Reads in the model category mapping csv.\n\n        :param path: The path to the csv file.\n        \"\"\"\n        object_label_map = {}\n        object_fine_grained_label_map = {}\n        object_coarse_grained_label_map = {}\n\n        with open(resolve_path(path), 'r', encoding=\"utf-8\") as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                object_label_map[row[\"model_id\"]] = row[\"nyuv2_40class\"]\n                object_fine_grained_label_map[row[\"model_id\"]] = row[\"fine_grained_class\"]\n                object_coarse_grained_label_map[row[\"model_id\"]] = row[\"coarse_grained_class\"]\n\n        return object_label_map, object_fine_grained_label_map, object_coarse_grained_label_map",
  "def load_urdf(urdf_file: str, weight_distribution: str = 'rigid',\n              fk_offset: Optional[Union[List[float], Vector, np.array]] = None,\n              ik_offset: Optional[Union[List[float], Vector, np.array]] = None) -> URDFObject:\n    \"\"\" Loads an urdf object from an URDF file.\n\n    :param urdf_file: Path to the URDF file.\n    :param weight_distribution: One of ['envelope', 'automatic', 'rigid']. For more information please see\n                                https://docs.blender.org/manual/en/latest/animation/armatures/skinning/parenting.html.\n    :param fk_offset: Offset between fk (forward kinematic) bone chain and link bone chain. This does not have any\n                      effect on the transformations, but can be useful for visualization in blender.\n    :param ik_offset: Offset between ik (inverse kinematic) bone chain and link bone chain. Effects on the\n                      transformation (e.g. `urdf_object.set_location_ik()`) are being handled internally. Useful for\n                      visualization in blender.\n    :return: URDF object instance.\n    \"\"\"\n    # install urdfpy\n    SetupUtility.setup_pip(user_required_packages=[\"git+https://github.com/wboerdijk/urdfpy.git\"])\n    # This import is done inside to avoid having the requirement that BlenderProc depends on urdfpy\n    # pylint: disable=import-outside-toplevel\n    from urdfpy import URDF\n    # pylint: enable=import-outside-toplevel\n\n    if fk_offset is None:\n        fk_offset = [0., -1., 0.]\n\n    if ik_offset is None:\n        ik_offset = [0., 1., 0.]\n\n    # load urdf tree representation\n    urdf_tree = URDF.load(urdf_file)\n\n    # create new empty armature\n    bpy.ops.object.select_all(action='DESELECT')\n    bpy.ops.object.armature_add()\n    armature = bpy.context.active_object\n    armature.name = urdf_tree.name\n    # remove initially created bone\n    bpy.ops.object.mode_set(mode='EDIT')\n    armature.data.edit_bones.remove(armature.data.edit_bones.values()[0])\n    bpy.ops.object.mode_set(mode='OBJECT')\n\n    # recursively create bones, starting at the base bone(s)\n    base_joints = get_joints_which_have_link_as_parent(urdf_tree.base_link.name, urdf_tree.joints)\n    for base_joint in base_joints:\n        create_bone(armature, base_joint, urdf_tree.joints, parent_bone_name=None, create_recursive=True,\n                    fk_offset=fk_offset, ik_offset=ik_offset)\n\n    # load links\n    links = load_links(urdf_tree.links, urdf_tree.joints, armature, urdf_path=urdf_file)\n\n    # propagate poses through the links\n    for base_joint in base_joints:\n        propagate_pose(links, base_joint, urdf_tree.joints, armature)\n\n    # parent links with the bone\n    for link in links:\n        link.parent_with_bone(weight_distribution=weight_distribution)\n\n    # set to forward kinematics per default\n    for link in links:\n        link.switch_fk_ik_mode(mode=\"fk\")\n\n    urdf_object = URDFObject(armature, links=links, xml_tree=urdf_tree)\n\n    # hide all inertial and collision objects per default\n    urdf_object.hide_links_and_collision_inertial_objs()\n\n    return urdf_object",
  "def get_joints_which_have_link_as_parent(link_name: str, joint_trees: List[\"urdfpy.Joint\"]) -> List[\"urdfpy.Joint\"]:\n    \"\"\" Returns a list of joints which have a specific link as parent.\n\n    :param link_name: Name of the link.\n    :param joint_trees: List of urdfpy.Joint objects.\n    :return: List of urdfpy.Joint objects.\n    \"\"\"\n    return [joint_tree for i, joint_tree in enumerate(joint_trees) if joint_tree.parent == link_name]",
  "def get_joints_which_have_link_as_child(link_name: str, joint_trees: List[\"urdfpy.Joint\"]) -> Optional[\"urdfpy.Joint\"]:\n    \"\"\" Returns the joint which is the parent of a specific link.\n\n    :param link_name: Name of the link.\n    :param joint_trees: List of urdfpy.Joint objects.\n    :return: List of urdfpy.Joint objects, or None if no joint is defined as parent for the respective link.\n    \"\"\"\n    valid_joint_trees = [joint_tree for i, joint_tree in enumerate(joint_trees) if joint_tree.child == link_name]\n    if not valid_joint_trees:  # happens for the very first link\n        warnings.warn(f\"WARNING: There is no joint defined for the link {link_name}!\")\n        return None\n    if len(valid_joint_trees) == 1:\n        return valid_joint_trees[0]\n    raise NotImplementedError(f\"More than one ({len(valid_joint_trees)}) joints map onto a single link with name \"\n                              f\"{link_name}\")",
  "def create_bone(armature: bpy.types.Armature, joint_tree: \"urdfpy.Joint\", all_joint_trees: List[\"urdfpy.Joint\"],\n                parent_bone_name: Optional[str] = None, create_recursive: bool = True,\n                parent_origin: Optional[Matrix] = None,\n                fk_offset: Union[List[float], Vector, np.array] = None,\n                ik_offset: Union[List[float], Vector, np.array] = None):\n    \"\"\" Creates deform, fk and ik bone for a specific joint. Can loop recursively through the child(ren).\n\n    :param armature: The armature which encapsulates all bones.\n    :param joint_tree: The urdf definition for the joint.\n    :param all_joint_trees: List of urdf definitions for all joints.\n    :param parent_bone_name: Name of the parent bone.\n    :param create_recursive: Whether to recursively create bones for the child(ren) of the link.\n    :param parent_origin: Pose of the parent.\n    :param fk_offset: Offset between fk bone chain and link bone chain. This does not have any effect on the\n                      transformations, but can be useful for visualization in blender.\n    :param ik_offset: Offset between fk bone chain and link bone chain. Effects on the transformation (e.g.\n                      `urdf_object.set_location_ik()`) are being handled internally. Useful for visualization in\n                      blender.\n    \"\"\"\n    if fk_offset is None:\n        fk_offset = [0., -1., 0.]\n\n    if ik_offset is None:\n        ik_offset = [0., 1., 0.]\n\n    bpy.ops.object.select_all(action='DESELECT')\n    bpy.context.view_layer.objects.active = armature\n    bpy.ops.object.mode_set(mode='EDIT', toggle=False)\n\n    # create initial bone\n    edit_bones = armature.data.edit_bones\n    editbone = edit_bones.new(joint_tree.name)\n\n    origin = joint_tree.origin\n    if parent_origin is not None:\n        origin = parent_origin @ origin\n\n    axis = Matrix(origin[:3, :3]) @ Vector(joint_tree.axis)\n    editbone.head = Vector(origin[:3, -1])\n    editbone.tail = editbone.head + axis.normalized() * 0.2\n\n    if parent_bone_name is not None:\n        parent_bone = edit_bones.get(parent_bone_name)\n        editbone.parent = parent_bone\n\n    # create fk bone\n    fk_editbone = edit_bones.new(joint_tree.name + '.fk')\n    axis = Matrix(origin[:3, :3]) @ Vector(joint_tree.axis)\n    fk_editbone.head = Vector(origin[:3, -1]) + Vector(fk_offset)\n    fk_editbone.tail = fk_editbone.head + axis.normalized() * 0.2\n\n    if parent_bone_name is not None:\n        parent_bone = edit_bones.get(parent_bone_name + '.fk')\n        fk_editbone.parent = parent_bone\n\n    # create ik bone\n    ik_editbone = edit_bones.new(joint_tree.name + '.ik')\n    axis = Matrix(origin[:3, :3]) @ Vector(joint_tree.axis)\n    ik_editbone.head = Vector(origin[:3, -1]) + Vector(ik_offset)\n    ik_editbone.tail = ik_editbone.head + axis.normalized() * 0.2\n\n    if parent_bone_name is not None:\n        parent_bone = edit_bones.get(parent_bone_name + '.ik')\n        ik_editbone.parent = parent_bone\n\n    bone_name = editbone.name\n    fk_bone_name = fk_editbone.name\n    ik_bone_name = ik_editbone.name\n\n    bpy.ops.object.mode_set(mode='OBJECT', toggle=False)\n\n    # derive posebones for constraints\n    bone = armature.pose.bones[bone_name]\n    fk_bone = armature.pose.bones[fk_bone_name]\n    ik_bone = armature.pose.bones[ik_bone_name]\n\n    # set rotation mode\n    bone.rotation_mode = \"XYZ\"\n    fk_bone.rotation_mode = \"XYZ\"\n    ik_bone.rotation_mode = \"XYZ\"\n\n    # manage constraints\n    if joint_tree.joint_type == \"fixed\":\n        set_location_constraint(bone=bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n        set_location_constraint(bone=fk_bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n        set_location_constraint(bone=ik_bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n        set_rotation_constraint(bone=bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n        set_rotation_constraint(bone=fk_bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n        set_rotation_constraint(bone=ik_bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n    elif joint_tree.joint_type == \"revolute\":\n        limits = None\n        if joint_tree.limit is not None:\n            limits = np.array([joint_tree.limit.lower, joint_tree.limit.upper])\n\n        set_location_constraint(bone=bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n        set_location_constraint(bone=fk_bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n        set_location_constraint(bone=ik_bone, x_limits=[0., 0.], y_limits=[0., 0.], z_limits=[0., 0.])\n        set_rotation_constraint(bone=bone, x_limits=[0, 0], y_limits=limits, z_limits=[0, 0])\n        set_rotation_constraint(bone=fk_bone, x_limits=[0, 0], y_limits=limits, z_limits=[0, 0])\n        set_rotation_constraint(bone=ik_bone, x_limits=[0, 0], y_limits=limits, z_limits=[0, 0])\n        set_copy_rotation_constraint(bone=bone, target=armature, target_bone=fk_bone.name,\n                                     custom_constraint_name=\"copy_rotation.fk\")\n        set_copy_rotation_constraint(bone=bone, target=armature, target_bone=ik_bone.name,\n                                     custom_constraint_name=\"copy_rotation.ik\",\n                                     influence=0.)  # start in fk mode per default\n    else:\n        warnings.warn(f\"WARNING: No constraint implemented for joint type '{joint_tree.joint_type}'!\")\n\n    if create_recursive:\n        child_joints = get_joints_which_have_link_as_parent(link_name=joint_tree.child, joint_trees=all_joint_trees)\n\n        for child_joint in child_joints:\n            create_bone(armature, child_joint, all_joint_trees, parent_bone_name=bone.name, create_recursive=True,\n                        parent_origin=origin, fk_offset=fk_offset, ik_offset=ik_offset)",
  "def load_links(link_trees: List[\"urdfpy.Link\"], joint_trees: List[\"urdfpy.Joint\"], armature: bpy.types.Armature,\n               urdf_path: str) -> List[Link]:\n    \"\"\" Loads links and their visual, collision and inertial objects from a list of urdfpy.Link objects.\n\n    :param link_trees: List of urdf definitions for all links.\n    :param joint_trees: List of urdf definitions for all joints.\n    :param armature: The armature which encapsulates all bones.\n    :param urdf_path: Path to the URDF file.\n    :return: List of links.\n    \"\"\"\n    links = []\n    for link_tree in link_trees:\n        visuals, collisions, inertial = [], [], None\n\n        if link_tree.visuals:\n            visuals = [load_visual_collision_obj(visual_tree, name=f\"{link_tree.name}_visual\", urdf_path=urdf_path)\n                       for visual_tree in link_tree.visuals]\n\n        if link_tree.collisions:\n            collisions = [load_visual_collision_obj(collision_tree, name=f\"{link_tree.name}_collision\",\n                                                    urdf_path=urdf_path)\n                          for collision_tree in link_tree.collisions]\n\n        if link_tree.inertial:\n            inertial = load_inertial(link_tree.inertial, name=f\"{link_tree.name}_inertial\")\n\n        # determine bone name\n        corresponding_joint = get_joints_which_have_link_as_child(link_tree.name, joint_trees)\n\n        # create link and set attributes\n        link = Link(bpy_object=create_with_empty_mesh(link_tree.name).blender_obj)\n        link.set_armature(armature)\n        link.set_visuals(visuals)\n        link.set_visual_local2link_mats([Matrix(obj.get_local2world_mat()) for obj in visuals])\n        link.set_collisions(collisions)\n        link.set_collision_local2link_mats([Matrix(obj.get_local2world_mat()) for obj in collisions])\n        link.set_inertial(inertial)\n        link.set_inertial_local2link_mat(Matrix(inertial.get_local2world_mat()) if inertial is not None else None)\n        link.set_name(name=link_tree.name)\n\n        # if there exists a joint also set the corresponding bones to the link\n        if corresponding_joint is not None:\n            link.set_bone(armature.pose.bones.get(corresponding_joint.name))\n            link.set_fk_bone(armature.pose.bones.get(corresponding_joint.name + '.fk'))\n            link.set_ik_bone(armature.pose.bones.get(corresponding_joint.name + '.ik'))\n            link.set_joint_type(corresponding_joint.joint_type)\n\n        links.append(link)\n    return links",
  "def propagate_pose(links: List[Link], joint_tree: \"urdfpy.Joint\", joint_trees: List[\"urdfpy.Joint\"],\n                   armature: bpy.types.Armature, recursive: bool = True):\n    \"\"\" Loads links and their visual, collision and inertial objects from a list of urdfpy.Link objects.\n\n    :param links: List of links.\n    :param joint_tree: The urdf definition for the joint.\n    :param joint_trees: List of urdf definitions for all joints.\n    :param armature: The armature which encapsulates all bones.\n    :param recursive: Whether to recursively create bones for the child(ren) of the link.\n    \"\"\"\n    child_link = one_by_attr(elements=links, attr_name=\"name\", value=joint_tree.child)\n    parent_link = one_by_attr(elements=links, attr_name=\"name\", value=joint_tree.parent)\n\n    # determine full transformation matrix\n    mat = Matrix(parent_link.get_local2world_mat()) @ Matrix(joint_tree.origin)\n    child_link.set_local2world_mat(mat)\n    child_link.set_link_parent(parent=parent_link)\n    parent_link.set_link_child(child=child_link)\n\n    for obj in child_link.get_all_objs():\n        obj.set_local2world_mat(Matrix(child_link.get_local2world_mat()) @ Matrix(obj.get_local2world_mat()))\n\n    # set link2bone mat\n    if child_link.bone is not None:\n        child_link.set_link2bone_mat(mat.inverted() @ child_link.bone.matrix)\n\n    if recursive:\n        child_joint_trees = get_joints_which_have_link_as_parent(child_link.get_name(), joint_trees)\n        for child_joint_tree in child_joint_trees:\n            propagate_pose(links, child_joint_tree, joint_trees, armature, recursive=True)",
  "def load_geometry(geometry_tree: \"urdfpy.Geometry\", urdf_path: Optional[str] = None) -> MeshObject:\n    \"\"\" Loads a geometric element from an urdf tree.\n\n    :param geometry_tree: The urdf representation of the geometric element.\n    :param urdf_path: Optional path of the urdf file for relative geometry files.\n    :return: The respective MeshObject.\n    \"\"\"\n    if geometry_tree.mesh is not None:\n        if os.path.isfile(geometry_tree.mesh.filename):\n            obj = load_obj(filepath=geometry_tree.mesh.filename)[0]\n        elif urdf_path is not None and os.path.isfile(urdf_path):\n            relative_path = os.path.join('/'.join(urdf_path.split('/')[:-1]), geometry_tree.mesh.filename)\n            if os.path.isfile(relative_path):\n                # load in default coordinate system\n                obj = load_obj(filepath=relative_path, forward_axis='Y', up_axis='Z')[0]\n            else:\n                warnings.warn(f\"Couldn't load mesh file for {geometry_tree} (filename: {geometry_tree.mesh.filename}; \"\n                              f\"urdf filename: {urdf_path})\")\n        else:\n            raise NotImplementedError(f\"Couldn't load mesh file for {geometry_tree} (filename: \"\n                                      f\"{geometry_tree.mesh.filename})\")\n    elif geometry_tree.box is not None:\n        obj = create_primitive(shape=\"CUBE\")\n        obj.blender_obj.dimensions = Vector(geometry_tree.box.size)\n    elif geometry_tree.cylinder is not None:\n        obj = create_primitive(shape=\"CYLINDER\", radius=geometry_tree.cylinder.radius,\n                               depth=geometry_tree.cylinder.length)\n    elif geometry_tree.sphere is not None:\n        obj = create_primitive(shape=\"SPHERE\", radius=geometry_tree.sphere.radius)\n    else:\n        raise NotImplementedError(f\"Unknown geometry in urdf_tree {geometry_tree}\")\n    obj.persist_transformation_into_mesh(location=True, rotation=True, scale=True)\n    return obj",
  "def load_visual_collision_obj(viscol_tree: Union[\"urdfpy.Visual\", \"urdfpy.Collision\"], name: str,\n                              urdf_path: Optional[str] = None) -> MeshObject:\n    \"\"\" Loads a visual / collision element from an urdf tree.\n\n    :param viscol_tree: The urdf representation of the visual / collision element.\n    :param name: Name of the visual / collision element.\n    :param urdf_path: Optional path of the urdf file for relative geometry files.\n    :return: The respective MeshObject.\n    \"\"\"\n    obj = load_geometry(viscol_tree.geometry, urdf_path=urdf_path)\n    if hasattr(viscol_tree, \"name\") and viscol_tree.name is not None:\n        name = viscol_tree.name\n    obj.set_name(name=name)\n\n    # load material - only valid for visuals\n    if hasattr(viscol_tree, \"material\") and viscol_tree.material is not None:\n        # clear all existing materials\n        obj.clear_materials()\n\n        # check if material exists\n        if viscol_tree.material.name in [m.name for m in get_all_materials()]:\n            mat = bpy.data.materials[viscol_tree.material.name]\n            mat = Material(mat)\n        else:\n            # create a new material\n            mat = MaterialLoaderUtility.create(name=viscol_tree.material.name)\n            # create a principled node and set the default color\n            principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n            color = viscol_tree.material.color\n            if color is None:\n                color = Vector([1., 1., 1., 1.])\n            principled_node.inputs[\"Base Color\"].default_value = color\n        obj.replace_materials(mat)\n\n        # check for textures\n        if viscol_tree.material.texture is not None:\n            # image should've been loaded automatically\n            mat = MaterialLoaderUtility.create(name=viscol_tree.material.name + \"_texture\")\n            nodes = mat.nodes\n            links = mat.links\n\n            color_image = nodes.new('ShaderNodeTexImage')\n\n            if not os.path.exists(viscol_tree.material.texture.filename):\n                raise Exception(f\"Couldn't load texture image for {viscol_tree} from \"\n                                f\"{viscol_tree.material.texture.filename}\")\n            color_image.image = bpy.data.images.load(viscol_tree.material.texture.filename, check_existing=True)\n\n            principled = Utility.get_the_one_node_with_type(nodes, \"BsdfPrincipled\")\n            links.new(color_image.outputs[\"Color\"], principled.inputs[\"Base Color\"])\n\n            obj.replace_materials(mat)\n\n    # set the pose of the object\n    origin = Matrix.Identity(4)\n    if hasattr(viscol_tree, \"origin\"):\n        origin = Matrix(viscol_tree.origin)\n    obj.set_local2world_mat(Matrix(origin))\n\n    # set scale of the mesh\n    scale = get_size_from_geometry(viscol_tree.geometry)\n    if scale is not None:\n        obj.set_scale([scale, scale, scale])\n        obj.persist_transformation_into_mesh(location=False, rotation=False, scale=True)\n\n    return obj",
  "def load_inertial(inertial_tree: \"urdfpy.Inertial\", name: str) -> Inertial:\n    \"\"\" Loads an inertial element from an urdf tree.\n\n    :param inertial_tree: The urdf representation of the inertial element.\n    :param name: Name if the inertial element.\n    :return: The respective Inertial object.\n    \"\"\"\n    # create new primitive\n    primitive = create_primitive(shape=\"CUBE\")\n    inertial = Inertial(primitive.blender_obj)\n    inertial.set_name(name=name)\n\n    # set inertial-specific attributes\n    inertial.set_origin(origin=inertial_tree.origin)\n    inertial.set_mass(mass=inertial_tree.mass)\n    inertial.set_inertia(inertia=inertial_tree.inertia)\n    primitive.blender_obj.dimensions = Vector([0.03, 0.03, 0.03])  # just small values to keep cubes small for debugging\n    primitive.persist_transformation_into_mesh(location=True, rotation=True, scale=True)\n    return inertial",
  "def get_size_from_geometry(geometry: \"urdfpy.Geometry\") -> Optional[float]:\n    \"\"\" Helper to derive the link size from the largest geometric element.\n\n    :param geometry: The urdf representation of the geometric element.\n    :return: A single float representing the link's size.\n    \"\"\"\n    if geometry.box is not None:\n        return max(geometry.geometry.size)\n    if geometry.cylinder is not None:\n        return max(geometry.geometry.radius, geometry.geometry.length)\n    if geometry.mesh is not None:\n        if hasattr(geometry.geometry, \"scale\") and geometry.geometry.scale is not None:\n            return max(geometry.geometry.scale)\n        if hasattr(geometry.geometry, \"size\") and geometry.geometry.size is not None:\n            return max(geometry.geometry.size)\n        return None\n    if geometry.sphere is not None:\n        return geometry.geometry.radius\n    print(f\"Warning: Failed to derive size from geometry model {geometry}. Setting scale to 0.2!\")\n    return None",
  "def load_matterport3d(data_folder: Union[Path, str], used_house_id: Optional[str] = None,\n                      compare_floor_angle_in_degrees: float = 15.0) -> Tuple[MeshObject, MeshObject]:\n    \"\"\"\n    Load a scene from the Matterport3D dataset.\n\n    :param data_folder: Path to the downloaded Matterport3D dataset, please use `blenderproc download matterport`\n    :param used_house_id: A possible used_house_id for example: \"X7HyMhZNoso\" or \"Z6MFQCViBuw\", if None is given a\n                          random one is selected\n    :param compare_floor_angle_in_degrees: The angle which is used to check if a face is pointing upwards, all faces\n                                           pointing upwards are used to extract the floor object\n    :return: The general scene and the floor object as a tuple of `MeshObject`\n    \"\"\"\n\n    data_folder = Path(data_folder)\n    if not data_folder.exists():\n        raise FileNotFoundError(\"The Matterport3D data folder must exist!\")\n\n    all_object_files = list(data_folder.glob(\"**/*.obj\"))\n\n    # the aayBHfsNo7d id can not be read by blender and is therefore remove\n    all_object_files = [object_file for object_file in all_object_files if \"aayBHfsNo7d\" not in str(object_file)]\n\n    if used_house_id is not None:\n        loaded_house_file = [object_file for object_file in all_object_files if used_house_id in str(object_file)]\n        if not loaded_house_file:\n            raise ValueError(f\"The used house id: {used_house_id} does not appear in the downloaded .obj files!\")\n        if len(loaded_house_file) == 1:\n            loaded_house_file = loaded_house_file[0]\n        else:\n            raise ValueError(f\"The used house id: {used_house_id} does appear more than once in the downloaded \"\n                             f\".obj files!\")\n    else:\n        loaded_house_file = random.choice(all_object_files)\n\n    loaded_obj = load_obj(str(loaded_house_file), forward_axis=\"Y\")\n    if len(loaded_obj) == 1:\n        obj = loaded_obj[0]\n    else:\n        raise RuntimeError(f\"At this point only one object should be loaded, not more or less: {len(loaded_obj)}\")\n\n    # replace all BSDF materials with background materials, to avoid that any light is cast on the objects\n    for material in obj.get_materials():\n        principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n        material.remove_node(principled_bsdf)\n\n        textures = material.get_the_one_node_with_type(\"ShaderNodeTexImage\")\n        background_color_node = material.new_node(\"ShaderNodeBackground\")\n\n        material.link(textures.outputs[\"Color\"], background_color_node.inputs[\"Color\"])\n        output_node = material.get_the_one_node_with_type(\"OutputMaterial\")\n        material.link(background_color_node.outputs[\"Background\"], output_node.inputs[\"Surface\"])\n\n    # split away the floor object\n    floor_obj = slice_faces_with_normals(obj, compare_angle_degrees=compare_floor_angle_in_degrees)\n\n    return obj, floor_obj",
  "def load_ccmaterials(folder_path: str = \"resources/cctextures\", used_assets: list = None, preload: bool = False,\n                     fill_used_empty_materials: bool = False, add_custom_properties: dict = None,\n                     use_all_materials: bool = False) -> List[Material]:\n    \"\"\" This method loads all textures obtained from https://ambientCG.com, use the script\n    (scripts/download_cc_textures.py) to download all the textures to your pc.\n\n    All textures here support Physically based rendering (PBR), which makes the textures more realistic.\n\n    All materials will have the custom property \"is_cc_texture\": True, which will make the selection later on easier.\n\n    :param folder_path: The path to the downloaded cc0textures.\n    :param used_assets: A list of all asset names, you want to use. The asset-name must not be typed in completely,\n                        only the beginning the name starts with. By default, all assets will be loaded,\n                        specified by an empty list.\n    :param preload: If set true, only the material names are loaded and not the complete material.\n    :param fill_used_empty_materials: If set true, the preloaded materials, which are used are now loaded completely.\n    :param add_custom_properties:  A dictionary of materials and the respective properties.\n    :param use_all_materials: If this is false only a selection of probably useful textures is used. This excludes \\\n                              some see through texture and non tileable texture.\n    :return a list of all loaded materials, if preload is active these materials do not contain any textures yet\n            and have to be filled before rendering (by calling this function again, no need to save the prior\n            returned list)\n    \"\"\"\n    folder_path = resolve_path(folder_path)\n    # this selected textures are probably useful for random selection\n    probably_useful_texture = [\"paving stones\", \"tiles\", \"wood\", \"fabric\", \"bricks\", \"metal\", \"wood floor\",\n                               \"ground\", \"rock\", \"concrete\", \"leather\", \"planks\", \"rocks\", \"gravel\",\n                               \"asphalt\", \"painted metal\", \"painted plaster\", \"marble\", \"carpet\",\n                               \"plastic\", \"roofing tiles\", \"bark\", \"metal plates\", \"wood siding\",\n                               \"terrazzo\", \"plaster\", \"paint\", \"corrugated steel\", \"painted wood\",\n                               \"lava cardboard\", \"clay\", \"diamond plate\", \"ice\", \"moss\", \"pipe\", \"candy\",\n                               \"chipboard\", \"rope\", \"sponge\", \"tactile paving\", \"paper\", \"cork\",\n                               \"wood chips\"]\n    if not use_all_materials and used_assets is None:\n        used_assets = probably_useful_texture\n    elif used_assets is not None:\n        used_assets = [asset.lower() for asset in used_assets]\n\n    if add_custom_properties is None:\n        add_custom_properties = {}\n\n    if preload and fill_used_empty_materials:\n        raise Exception(\"Preload and fill used empty materials can not be done at the same time, check config!\")\n\n    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n        materials = []\n        for asset in os.listdir(folder_path):\n            if used_assets:\n                skip_this_one = True\n                for used_asset in used_assets:\n                    # lower is necessary here, as all used assets are made that that way\n                    if asset.lower().startswith(used_asset.replace(\" \", \"\")):\n                        skip_this_one = False\n                        break\n                if skip_this_one:\n                    continue\n            current_path = os.path.join(folder_path, asset)\n            if os.path.isdir(current_path):\n                base_image_path = os.path.join(current_path, f\"{asset}_2K_Color.jpg\")\n                if not os.path.exists(base_image_path):\n                    continue\n\n                # construct all image paths\n                ambient_occlusion_image_path = base_image_path.replace(\"Color\", \"AmbientOcclusion\")\n                metallic_image_path = base_image_path.replace(\"Color\", \"Metalness\")\n                roughness_image_path = base_image_path.replace(\"Color\", \"Roughness\")\n                alpha_image_path = base_image_path.replace(\"Color\", \"Opacity\")\n                normal_image_path = base_image_path.replace(\"Color\", \"Normal\")\n                displacement_image_path = base_image_path.replace(\"Color\", \"Displacement\")\n\n                # if the material was already created it only has to be searched\n                if fill_used_empty_materials:\n                    new_mat = MaterialLoaderUtility.find_cc_material_by_name(asset, add_custom_properties)\n                else:\n                    new_mat = MaterialLoaderUtility.create_new_cc_material(asset, add_custom_properties)\n\n                # if preload then the material is only created but not filled\n                if preload:\n                    # Set alpha to 0 if the material has an alpha texture, so it can be detected\n                    # e.q. in the material getter.\n                    nodes = new_mat.node_tree.nodes\n                    principled_bsdf = Utility.get_the_one_node_with_type(nodes, \"BsdfPrincipled\")\n                    principled_bsdf.inputs[\"Alpha\"].default_value = 0 if os.path.exists(alpha_image_path) else 1\n                    # add it here for the preload case\n                    materials.append(Material(new_mat))\n                    continue\n                if fill_used_empty_materials and not MaterialLoaderUtility.is_material_used(new_mat):\n                    # now only the materials, which have been used should be filled\n                    continue\n\n                # create material based on these image paths\n                _CCMaterialLoader.create_material(new_mat, base_image_path, ambient_occlusion_image_path,\n                                                  metallic_image_path, roughness_image_path, alpha_image_path,\n                                                  normal_image_path, displacement_image_path)\n\n                materials.append(Material(new_mat))\n        return materials\n    raise FileNotFoundError(f\"The folder path does not exist: {folder_path}\")",
  "class _CCMaterialLoader:\n\n    @staticmethod\n    def create_material(new_mat: bpy.types.Material, base_image_path: str, ambient_occlusion_image_path: str,\n                        metallic_image_path: str, roughness_image_path: str, alpha_image_path: str,\n                        normal_image_path: str, displacement_image_path: str):\n        \"\"\"\n        Create a material for the cctexture datatset, the combination used here is calibrated to this.\n\n        :param new_mat: The new material, which will get all the given textures\n        :param base_image_path: The path to the color image\n        :param ambient_occlusion_image_path: The path to the ambient occlusion image\n        :param metallic_image_path: The path to the metallic image\n        :param roughness_image_path: The path to the roughness image\n        :param alpha_image_path: The path to the alpha image (when this was written there was no alpha image provided \\\n                                 in the haven dataset)\n        :param normal_image_path: The path to the normal image\n        :param displacement_image_path: The path to the displacement image\n        \"\"\"\n        nodes = new_mat.node_tree.nodes\n        links = new_mat.node_tree.links\n\n        principled_bsdf = Utility.get_the_one_node_with_type(nodes, \"BsdfPrincipled\")\n        output_node = Utility.get_the_one_node_with_type(nodes, \"OutputMaterial\")\n\n        collection_of_texture_nodes = []\n        base_color = MaterialLoaderUtility.add_base_color(nodes, links, base_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(base_color)\n\n        principled_bsdf.inputs[\"Specular\"].default_value = 0.333\n\n        ao_node = MaterialLoaderUtility.add_ambient_occlusion(nodes, links, ambient_occlusion_image_path,\n                                                              principled_bsdf, base_color)\n        collection_of_texture_nodes.append(ao_node)\n\n        metallic_node = MaterialLoaderUtility.add_metal(nodes, links, metallic_image_path,\n                                                        principled_bsdf)\n        collection_of_texture_nodes.append(metallic_node)\n\n        roughness_node = MaterialLoaderUtility.add_roughness(nodes, links, roughness_image_path,\n                                                             principled_bsdf)\n        collection_of_texture_nodes.append(roughness_node)\n\n        alpha_node = MaterialLoaderUtility.add_alpha(nodes, links, alpha_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(alpha_node)\n\n        normal_node = MaterialLoaderUtility.add_normal(nodes, links, normal_image_path, principled_bsdf,\n                                                       invert_y_channel=True)\n        collection_of_texture_nodes.append(normal_node)\n\n        displacement_node = MaterialLoaderUtility.add_displacement(nodes, links, displacement_image_path,\n                                                                   output_node)\n        collection_of_texture_nodes.append(displacement_node)\n\n        collection_of_texture_nodes = [node for node in collection_of_texture_nodes if node is not None]\n\n        MaterialLoaderUtility.connect_uv_maps(nodes, links, collection_of_texture_nodes)",
  "def create_material(new_mat: bpy.types.Material, base_image_path: str, ambient_occlusion_image_path: str,\n                        metallic_image_path: str, roughness_image_path: str, alpha_image_path: str,\n                        normal_image_path: str, displacement_image_path: str):\n        \"\"\"\n        Create a material for the cctexture datatset, the combination used here is calibrated to this.\n\n        :param new_mat: The new material, which will get all the given textures\n        :param base_image_path: The path to the color image\n        :param ambient_occlusion_image_path: The path to the ambient occlusion image\n        :param metallic_image_path: The path to the metallic image\n        :param roughness_image_path: The path to the roughness image\n        :param alpha_image_path: The path to the alpha image (when this was written there was no alpha image provided \\\n                                 in the haven dataset)\n        :param normal_image_path: The path to the normal image\n        :param displacement_image_path: The path to the displacement image\n        \"\"\"\n        nodes = new_mat.node_tree.nodes\n        links = new_mat.node_tree.links\n\n        principled_bsdf = Utility.get_the_one_node_with_type(nodes, \"BsdfPrincipled\")\n        output_node = Utility.get_the_one_node_with_type(nodes, \"OutputMaterial\")\n\n        collection_of_texture_nodes = []\n        base_color = MaterialLoaderUtility.add_base_color(nodes, links, base_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(base_color)\n\n        principled_bsdf.inputs[\"Specular\"].default_value = 0.333\n\n        ao_node = MaterialLoaderUtility.add_ambient_occlusion(nodes, links, ambient_occlusion_image_path,\n                                                              principled_bsdf, base_color)\n        collection_of_texture_nodes.append(ao_node)\n\n        metallic_node = MaterialLoaderUtility.add_metal(nodes, links, metallic_image_path,\n                                                        principled_bsdf)\n        collection_of_texture_nodes.append(metallic_node)\n\n        roughness_node = MaterialLoaderUtility.add_roughness(nodes, links, roughness_image_path,\n                                                             principled_bsdf)\n        collection_of_texture_nodes.append(roughness_node)\n\n        alpha_node = MaterialLoaderUtility.add_alpha(nodes, links, alpha_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(alpha_node)\n\n        normal_node = MaterialLoaderUtility.add_normal(nodes, links, normal_image_path, principled_bsdf,\n                                                       invert_y_channel=True)\n        collection_of_texture_nodes.append(normal_node)\n\n        displacement_node = MaterialLoaderUtility.add_displacement(nodes, links, displacement_image_path,\n                                                                   output_node)\n        collection_of_texture_nodes.append(displacement_node)\n\n        collection_of_texture_nodes = [node for node in collection_of_texture_nodes if node is not None]\n\n        MaterialLoaderUtility.connect_uv_maps(nodes, links, collection_of_texture_nodes)",
  "def load_shapenet(data_path: str, used_synset_id: str, used_source_id: str = \"\",\n                  move_object_origin: bool = True) -> MeshObject:\n    \"\"\" This loads an object from ShapeNet based on the given synset_id, which specifies the category of objects to use.\n\n    From these objects one is randomly sampled and loaded.\n\n    Todo: not good:\n    Note: if this module is used with another loader that loads objects with semantic mapping, make sure the other\n    module is loaded first in the config file.\n\n    :param data_path: The path to the ShapeNetCore.v2 folder.\n    :param used_synset_id: The synset id for example: '02691156', check the data_path folder for more ids.\n    :param used_source_id: Object identifier of the a particular ShapeNet category, see inside any ShapeNet category\n                           for identifiers\n    :param move_object_origin: Moves the object center to the bottom of the bounding box in Z direction and also in the\n                               middle of the X and Y plane, this does not change the `.location` of the object.\n                               Default: True\n    :return: The loaded mesh object.\n    \"\"\"\n    data_path = resolve_path(data_path)\n    taxonomy_file_path = os.path.join(data_path, \"taxonomy.json\")\n\n    files_with_fitting_synset = _ShapeNetLoader.get_files_with_synset(used_synset_id, used_source_id,\n                                                                      taxonomy_file_path, data_path)\n    selected_obj = random.choice(files_with_fitting_synset)\n    # with the new version the textures are all wrong\n    loaded_objects = load_obj(selected_obj, use_legacy_obj_import=True)\n\n    # In shapenet every .obj file only contains one object, make sure that is the case\n    if len(loaded_objects) != 1:\n        raise RuntimeError(\"The ShapeNetLoader expects every .obj file to contain exactly one object, \"\n                           \"however the file \" + selected_obj + \" contained \" + str(len(loaded_objects)) + \" objects.\")\n    obj = loaded_objects[0]\n\n    obj.set_cp(\"used_synset_id\", used_synset_id)\n    obj.set_cp(\"used_source_id\", pathlib.PurePath(selected_obj).parts[-3])\n\n    _ShapeNetLoader.correct_materials(obj)\n\n    # removes the x axis rotation found in all ShapeNet objects, this is caused by importing .obj files\n    # the object has the same pose as before, just that the rotation_euler is now [0, 0, 0]\n    obj.persist_transformation_into_mesh(location=False, rotation=True, scale=False)\n\n    # check if the move_to_world_origin flag is set\n    if move_object_origin:\n        # move the origin of the object to the world origin and on top of the X-Y plane\n        # makes it easier to place them later on, this does not change the `.location`\n        obj.move_origin_to_bottom_mean_point()\n    bpy.ops.object.select_all(action='DESELECT')\n\n    return obj",
  "class _ShapeNetLoader:\n\n    @staticmethod\n    def get_files_with_synset(used_synset_id: str, used_source_id: str, path_to_taxonomy_file: str,\n                              data_path: str) -> list:\n        \"\"\" Returns a list of a .obj file for the given synset_id\n\n        :param used_synset_id: the id of the category something like: '02691156', see the data_path folder for more ids\n        :param used_source_id: object identifier of a particular ShapeNet category, see inside any ShapeNet\n                               category for identifiers\n        :param path_to_taxonomy_file: path to the taxonomy.json file, should be in the data_path, too\n        :param data_path: path to the ShapeNetCore.v2 folder\n        :return: list of .obj files, which are in the synset_id folder, based on the given taxonomy\n        \"\"\"\n        if os.path.exists(path_to_taxonomy_file):\n            files = []\n            with open(path_to_taxonomy_file, \"r\", encoding=\"utf-8\") as f:\n                loaded_data = json.load(f)\n                parent_synset_id = _ShapeNetLoader.find_parent_synset_id(data_path, used_synset_id, loaded_data)\n                id_path = os.path.join(data_path, parent_synset_id)\n\n                if not used_source_id:\n                    files.extend(glob.glob(os.path.join(id_path, \"*\", \"models\", \"model_normalized.obj\")))\n                else:\n                    if not os.path.exists(os.path.join(id_path, used_source_id)):\n                        raise ValueError(f\"The used_source_id {used_source_id} is not correct\")\n\n                    # Using both the used_synset_id and used_source_id\n                    files.append(os.path.join(id_path, used_source_id, \"models\", \"model_normalized.obj\"))\n\n            # Sort files to make random choice deterministic for the case when used_source_id is not specified\n            files.sort()\n            return files\n        raise FileNotFoundError(f\"The taxonomy file could not be found: {path_to_taxonomy_file}\")\n\n    @staticmethod\n    def find_parent_synset_id(data_path, synset_id, json_data):\n        \"\"\"\n        Returns the parent synset_id if it exists. If the synset_id is already parent synset_id, it is just returned\n        :param data_path: path to the ShapeNetCore.v2 folder\n        :param synset_id: the id of the category something like: '02691156', see the data_path folder for more ids\n        :param json_data: loaded data from the ShapeNet taxonomy.json file\n        :return: parent synset_id\n        \"\"\"\n        id_path = os.path.join(data_path, synset_id)\n\n        # Check if the synset_id is alreay a parent synset_id\n        if os.path.exists(id_path):\n            return synset_id\n\n        for block in json_data:\n            if synset_id in block[\"children\"]:\n                parent_synset_id = block[\"synsetId\"]\n                return _ShapeNetLoader.find_parent_synset_id(data_path, parent_synset_id, json_data)\n\n        raise ValueError(f\"The used_synset_id {synset_id} does not exists in the taxonomy file\")\n\n    @staticmethod\n    def correct_materials(obj: MeshObject):\n        \"\"\" If the used material contains an alpha texture, the alpha texture has to be flipped to be correct\n\n        :param obj: object where the material maybe wrong\n        \"\"\"\n        for material in obj.get_materials():\n            if material is None:\n                continue\n            texture_nodes = material.get_nodes_with_type(\"ShaderNodeTexImage\")\n            if texture_nodes and len(texture_nodes) > 1:\n                principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n                # find the image texture node which is connected to alpha\n                node_connected_to_the_alpha = None\n                for node_links in principled_bsdf.inputs[\"Alpha\"].links:\n                    if \"ShaderNodeTexImage\" in node_links.from_node.bl_idname:\n                        node_connected_to_the_alpha = node_links.from_node\n                # if a node was found which is connected to the alpha node, add an invert between the two\n                if node_connected_to_the_alpha is not None:\n                    invert_node = material.new_node(\"ShaderNodeInvert\")\n                    invert_node.inputs[\"Fac\"].default_value = 1.0\n                    material.insert_node_instead_existing_link(node_connected_to_the_alpha.outputs[\"Color\"],\n                                                               invert_node.inputs[\"Color\"],\n                                                               invert_node.outputs[\"Color\"],\n                                                               principled_bsdf.inputs[\"Alpha\"])",
  "def get_files_with_synset(used_synset_id: str, used_source_id: str, path_to_taxonomy_file: str,\n                              data_path: str) -> list:\n        \"\"\" Returns a list of a .obj file for the given synset_id\n\n        :param used_synset_id: the id of the category something like: '02691156', see the data_path folder for more ids\n        :param used_source_id: object identifier of a particular ShapeNet category, see inside any ShapeNet\n                               category for identifiers\n        :param path_to_taxonomy_file: path to the taxonomy.json file, should be in the data_path, too\n        :param data_path: path to the ShapeNetCore.v2 folder\n        :return: list of .obj files, which are in the synset_id folder, based on the given taxonomy\n        \"\"\"\n        if os.path.exists(path_to_taxonomy_file):\n            files = []\n            with open(path_to_taxonomy_file, \"r\", encoding=\"utf-8\") as f:\n                loaded_data = json.load(f)\n                parent_synset_id = _ShapeNetLoader.find_parent_synset_id(data_path, used_synset_id, loaded_data)\n                id_path = os.path.join(data_path, parent_synset_id)\n\n                if not used_source_id:\n                    files.extend(glob.glob(os.path.join(id_path, \"*\", \"models\", \"model_normalized.obj\")))\n                else:\n                    if not os.path.exists(os.path.join(id_path, used_source_id)):\n                        raise ValueError(f\"The used_source_id {used_source_id} is not correct\")\n\n                    # Using both the used_synset_id and used_source_id\n                    files.append(os.path.join(id_path, used_source_id, \"models\", \"model_normalized.obj\"))\n\n            # Sort files to make random choice deterministic for the case when used_source_id is not specified\n            files.sort()\n            return files\n        raise FileNotFoundError(f\"The taxonomy file could not be found: {path_to_taxonomy_file}\")",
  "def find_parent_synset_id(data_path, synset_id, json_data):\n        \"\"\"\n        Returns the parent synset_id if it exists. If the synset_id is already parent synset_id, it is just returned\n        :param data_path: path to the ShapeNetCore.v2 folder\n        :param synset_id: the id of the category something like: '02691156', see the data_path folder for more ids\n        :param json_data: loaded data from the ShapeNet taxonomy.json file\n        :return: parent synset_id\n        \"\"\"\n        id_path = os.path.join(data_path, synset_id)\n\n        # Check if the synset_id is alreay a parent synset_id\n        if os.path.exists(id_path):\n            return synset_id\n\n        for block in json_data:\n            if synset_id in block[\"children\"]:\n                parent_synset_id = block[\"synsetId\"]\n                return _ShapeNetLoader.find_parent_synset_id(data_path, parent_synset_id, json_data)\n\n        raise ValueError(f\"The used_synset_id {synset_id} does not exists in the taxonomy file\")",
  "def correct_materials(obj: MeshObject):\n        \"\"\" If the used material contains an alpha texture, the alpha texture has to be flipped to be correct\n\n        :param obj: object where the material maybe wrong\n        \"\"\"\n        for material in obj.get_materials():\n            if material is None:\n                continue\n            texture_nodes = material.get_nodes_with_type(\"ShaderNodeTexImage\")\n            if texture_nodes and len(texture_nodes) > 1:\n                principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n                # find the image texture node which is connected to alpha\n                node_connected_to_the_alpha = None\n                for node_links in principled_bsdf.inputs[\"Alpha\"].links:\n                    if \"ShaderNodeTexImage\" in node_links.from_node.bl_idname:\n                        node_connected_to_the_alpha = node_links.from_node\n                # if a node was found which is connected to the alpha node, add an invert between the two\n                if node_connected_to_the_alpha is not None:\n                    invert_node = material.new_node(\"ShaderNodeInvert\")\n                    invert_node.inputs[\"Fac\"].default_value = 1.0\n                    material.insert_node_instead_existing_link(node_connected_to_the_alpha.outputs[\"Color\"],\n                                                               invert_node.inputs[\"Color\"],\n                                                               invert_node.outputs[\"Color\"],\n                                                               principled_bsdf.inputs[\"Alpha\"])",
  "def load_AMASS(data_path: str, sub_dataset_id: str, temp_dir: str = None, body_model_gender: str = None,\n               subject_id: str = \"\", sequence_id: int = -1, frame_id: int = -1, num_betas: int = 10,\n               num_dmpls: int = 8) -> List[MeshObject]:\n    \"\"\"\n    use the pose parameters to generate the mesh and loads it to the scene.\n\n    :param data_path: The path to the AMASS Dataset folder in resources folder.\n    :param sub_dataset_id: Identifier for the sub dataset, the dataset which the human pose object should be extracted\n                           from. Available: ['CMU', 'Transitions_mocap', 'MPI_Limits', 'SSM_synced', 'TotalCapture',\n                           'Eyes_Japan_Dataset', 'MPI_mosh', 'MPI_HDM05', 'HumanEva', 'ACCAD', 'EKUT', 'SFU', 'KIT',\n                           'H36M', 'TCD_handMocap', 'BML']\n    :param temp_dir: A temp directory which is used for writing the temporary .obj file.\n    :param body_model_gender: The model gender pose is represented by either using male, female or neutral body shape.\n                              Available:[male, female, neutral]. If None is selected a random one is chosen.\n    :param subject_id: Type of motion from which the pose should be extracted, this is dataset dependent parameter.\n                       If left empty a random subject id is picked.\n    :param sequence_id: Sequence id in the dataset, sequences are the motion recorded to represent certain action.\n                        If set to -1 a random sequence id is selected.\n    :param frame_id: Frame id in a selected motion sequence. If none is selected a random one is picked\n    :param num_betas: Number of body parameters\n    :param num_dmpls: Number of DMPL parameters\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    if body_model_gender is None:\n        body_model_gender = random.choice([\"male\", \"female\", \"neutral\"])\n    if temp_dir is None:\n        temp_dir = Utility.get_temporary_directory()\n\n    # Install required additonal packages\n    SetupUtility.setup_pip([\"git+https://github.com/abahnasy/smplx\",\n                            \"git+https://github.com/abahnasy/human_body_prior\"])\n\n    # Get the currently supported mocap datasets by this loader\n    taxonomy_file_path = resolve_path(os.path.join(data_path, \"taxonomy.json\"))\n    supported_mocap_datasets = _AMASSLoader.get_supported_mocap_datasets(taxonomy_file_path, data_path)\n\n    # selected_obj = self._files_with_fitting_ids\n    pose_body, betas = _AMASSLoader.get_pose_parameters(supported_mocap_datasets, num_betas,\n                                                        sub_dataset_id, subject_id, sequence_id, frame_id)\n    # load parametric Model\n    body_model, faces = _AMASSLoader.load_parametric_body_model(data_path, body_model_gender, num_betas, num_dmpls)\n    # Generate Body representations using SMPL model\n    body_repr = body_model(pose_body=pose_body, betas=betas)\n    # Generate .obj file represents the selected pose\n    generated_obj = _AMASSLoader.write_body_mesh_to_obj_file(body_repr, faces, temp_dir)\n\n    loaded_obj = load_obj(generated_obj)\n\n    _AMASSLoader.correct_materials(loaded_obj)\n\n    # set the shading mode explicitly to smooth\n    for obj in loaded_obj:\n        obj.set_shading_mode(\"SMOOTH\")\n\n    # removes the x axis rotation found in all ShapeNet objects, this is caused by importing .obj files\n    # the object has the same pose as before, just that the rotation_euler is now [0, 0, 0]\n    for obj in loaded_obj:\n        obj.persist_transformation_into_mesh(location=False, rotation=True, scale=False)\n\n    # move the origin of the object to the world origin and on top of the X-Y plane\n    # makes it easier to place them later on, this does not change the `.location`\n    for obj in loaded_obj:\n        obj.move_origin_to_bottom_mean_point()\n    bpy.ops.object.select_all(action='DESELECT')\n\n    return loaded_obj",
  "class _AMASSLoader:\n    \"\"\"\n    AMASS is a large database of human motion unifying 15 different optical marker-based motion capture datasets\n    by representing them within a common framework and parameterization. All the mocap data is converted into\n    realistic 3D human meshes represented by a rigged body model called SMPL, which provides a standard skeletal\n    representation as well as a fully rigged surface mesh. Warning: Only one part of the AMASS database is currently\n    supported by the loader! Please refer to the AMASSLoader example for more information about the currently\n    supported datasets.\n\n    Any human pose recorded in these motions could be reconstructed using the following parameters:\n    `\"sub_dataset_identifier\"`, `\"sequence id\"`, `\"frame id\"` and `\"model gender\"` which will represent the pose,\n    these parameters specify the exact pose to be generated based on the selected mocap dataset and motion\n    category recorded in this dataset.\n\n    Note: if this module is used with another loader that loads objects with semantic mapping, make sure the other\n    module is loaded first in the config file.\n    \"\"\"\n\n    # hex values for human skin tone to sample from\n    human_skin_colors = ['2D221E', '3C2E28', '4B3932', '5A453C', '695046', '785C50', '87675A', '967264', 'A57E6E',\n                         'B48A78', 'C39582', 'D2A18C', 'E1AC96', 'F0B8A0', 'FFC3AA', 'FFCEB4', 'FFDABE', 'FFE5C8']\n\n\n    @staticmethod\n    def get_pose_parameters(supported_mocap_datasets: dict, num_betas: int, used_sub_dataset_id: str,\n                            used_subject_id: str, used_sequence_id: int,\n                            used_frame_id: int) -> Tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n        \"\"\" Extract pose and shape parameters corresponding to the requested pose from the database to be\n        processed by the parametric model\n\n        :param supported_mocap_datasets: A dict which maps sub dataset names to their paths.\n        :param num_betas: Number of body parameters\n        :param used_sub_dataset_id: Identifier for the sub dataset, the dataset which the human pose object\n                                    should be extracted from.\n        :param used_subject_id: Type of motion from which the pose should be extracted, this is dataset\n                                dependent parameter.\n        :param used_sequence_id: Sequence id in the dataset, sequences are the motion recorded to represent\n                                 certain action.\n        :param used_frame_id: Frame id in a selected motion sequence. If none is selected a random one is picked\n        :return: tuple of arrays contains the parameters. Type: tuple\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on torch\n        #pylint: disable=import-outside-toplevel\n        import torch\n        #pylint: enable=import-outside-toplevel\n\n        # check if the sub_dataset is supported\n        if used_sub_dataset_id in supported_mocap_datasets:\n            # get path from dictionary\n            sub_dataset_path = supported_mocap_datasets[used_sub_dataset_id]\n            # concatenate path to specific\n            if not used_subject_id:\n                # if none was selected\n                possible_subject_ids = glob.glob(os.path.join(sub_dataset_path, \"*\"))\n                possible_subject_ids.sort()\n                if len(possible_subject_ids) > 0:\n                    used_subject_id_str = os.path.basename(random.choice(possible_subject_ids))\n                else:\n                    raise FileNotFoundError(f\"No subjects found in folder: {sub_dataset_path}\")\n            else:\n                used_subject_id_str = f\"{int(used_sequence_id):02d}\"\n\n            if used_sequence_id < 0:\n                # if no sequence id was selected\n                possible_sequence_ids = glob.glob(os.path.join(sub_dataset_path, used_subject_id_str, \"*\"))\n                possible_sequence_ids.sort()\n                if len(possible_sequence_ids) > 0:\n                    used_sequence_id = os.path.basename(random.choice(possible_sequence_ids))\n                    used_sequence_id = used_sequence_id[used_sequence_id.find(\"_\")+1:used_sequence_id.rfind(\"_\")]\n                else:\n                    raise FileNotFoundError(f\"No sequences found in folder: \"\n                                            f\"{os.path.join(sub_dataset_path, used_subject_id_str)}\")\n            subject_path = os.path.join(sub_dataset_path, used_subject_id_str)\n            used_subject_id_str_reduced = used_subject_id_str[:used_subject_id_str.find(\"_\")] \\\n                if \"_\" in used_subject_id_str else used_subject_id_str\n            sequence_path = os.path.join(subject_path, used_subject_id_str_reduced +\n                                         f\"_{int(used_sequence_id):02d}_poses.npz\")\n            if os.path.exists(sequence_path):\n                # load AMASS dataset sequence file which contains the coefficients for the whole motion sequence\n                sequence_body_data = np.load(sequence_path)\n                # get the number of supported frames\n                no_of_frames_per_sequence = sequence_body_data['poses'].shape[0]\n                if used_frame_id < 0:\n                    frame_id = random.randint(0, no_of_frames_per_sequence)  # pick a random id\n                else:\n                    frame_id = used_frame_id\n                # Extract Body Model coefficients\n                if frame_id in range(0, no_of_frames_per_sequence):\n                    # use GPU to accelerate mesh calculations\n                    comp_device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n                    # parameters that control the body pose\n                    # refer to http://files.is.tue.mpg.de/black/papers/amass.pdf, Section 3.1 for more\n                    # information about the parameter representation and the below chosen values\n                    pose_body = torch.Tensor(sequence_body_data['poses'][frame_id:frame_id + 1, 3:66]).to(comp_device)\n                    # parameters that control the body shape\n                    betas = torch.Tensor(sequence_body_data['betas'][:num_betas][np.newaxis]).to(comp_device)\n                    return pose_body, betas\n                raise RuntimeError(f\"Requested frame id is beyond sequence range, for the selected sequence, choose \"\n                                   f\"frame id within the following range: [0, {no_of_frames_per_sequence}]\")\n            raise RuntimeError(f\"Invalid sequence/subject: {used_subject_id} category identifiers, please choose a \"\n                               f\"valid one. Used path: {sequence_path}\")\n        raise RuntimeError(f\"The requested mocap dataset is not yest supported, please choose another one \"\n                           f\"from the following supported datasets: {list(supported_mocap_datasets.keys())}\")\n\n    @staticmethod\n    def load_parametric_body_model(data_path: str, used_body_model_gender: str, num_betas: int,\n                                   num_dmpls: int) -> Tuple[\"BodyModel\", np.array]:\n        \"\"\" loads the parametric model that is used to generate the mesh object\n\n        :return:  parametric model. Type: tuple.\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on torch\n        #pylint: disable=import-outside-toplevel\n        import torch\n        from human_body_prior.body_model.body_model import BodyModel\n        #pylint: enable=import-outside-toplevel\n\n        # body model\n        bm_path = os.path.join(data_path, 'body_models', 'smplh', used_body_model_gender, 'model.npz')\n        # deformation model\n        dmpl_path = os.path.join(data_path, 'body_models', 'dmpls', used_body_model_gender, 'model.npz')\n        if not os.path.exists(bm_path) or not os.path.exists(dmpl_path):\n            raise FileNotFoundError(\"Parametric Body model doesn't exist, please follow download \"\n                                    \"instructions section in AMASS Example\")\n        # pylint: disable=no-member\n        comp_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        #pylint: enable=no-member\n        body_model = BodyModel(bm_path=bm_path, num_betas=num_betas, num_dmpls=num_dmpls,\n                               path_dmpl=dmpl_path).to(comp_device)\n        faces = body_model.f.detach().cpu().numpy()\n        return body_model, faces\n\n    @staticmethod\n    def get_supported_mocap_datasets(taxonomy_file_path: str, data_path: str) -> dict:\n        \"\"\" Get latest updated list from taxonomoy json file about the supported mocap datasets supported in the\n            loader module and update.supported_mocap_datasets list\n\n        :param taxonomy_file_path: path to taxomomy.json file which contains the supported datasets and their\n                                   respective paths. Type: string.\n        :param data_path: path to the AMASS dataset root folder. Type: string.\n        \"\"\"\n\n        # dictionary contains mocap dataset name and path to its sub folder within the main dataset, dictionary will\n        # be filled from taxonomy.json file which indicates the supported datastests\n        supported_mocap_datasets = {}\n        if os.path.exists(taxonomy_file_path):\n            with open(taxonomy_file_path, \"r\", encoding=\"utf-8\") as f:\n                loaded_data = json.load(f)\n                for block in loaded_data:\n                    if \"sub_data_id\" in block:\n                        sub_dataset_id = block[\"sub_data_id\"]\n                        supported_mocap_datasets[sub_dataset_id] = os.path.join(data_path, block[\"path\"])\n        else:\n            raise FileNotFoundError(f\"The taxonomy file could not be found: {taxonomy_file_path}\")\n\n        return supported_mocap_datasets\n\n\n    @staticmethod\n    def write_body_mesh_to_obj_file(body_representation: \"torch.Tensor\", faces: np.array, temp_dir: str) -> str:\n        \"\"\" Write the generated pose as obj file on the desk.\n\n        :param body_representation: parameters generated from the BodyModel model which represent the obj\n                                     pose and shape. Type: torch.Tensor\n        :param faces: face parametric model which is used to generate the face mesh. Type: numpy.array\n        :param temp_dir: Path to the folder in which the generated pose as obj will be stored\n        :return: path to generated obj file. Type: string.\n        \"\"\"\n        # Generate temp object with name = timestamp\n        starttime = datetime.now().replace(microsecond=0)\n        obj_file_name = datetime.strftime(starttime, '%Y%m%d_%H%M') + \".obj\"\n        # Write to an .obj file\n        outmesh_path = os.path.join(temp_dir, obj_file_name)\n        with open(outmesh_path, 'w', encoding=\"utf-8\") as fp:\n            fp.write(\"\".join([f'v {v[0]:f} {v[1]:f} {v[2]:f}\\n' for v in\n                              body_representation.v[0].detach().cpu().numpy()]))\n            fp.write(\"\".join([f'f {f[0]} {f[1]} {f[2]}\\n' for f in faces + 1]))\n        return outmesh_path\n\n    @staticmethod\n    def correct_materials(objects: List[MeshObject]):\n        \"\"\" If the used material contains an alpha texture, the alpha texture has to be flipped to be correct\n\n        :param objects: Mesh objects where the material might be wrong.\n        \"\"\"\n        for obj in objects:\n            if not obj.get_materials():\n                obj.new_material(\"Skin material\")\n            for material in obj.get_materials():\n                if material is None:\n                    continue\n                # Create a principled node and set the default color\n                principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n                # Pick random skin color value\n                skin_tone_hex = np.random.choice(_AMASSLoader.human_skin_colors)\n                skin_tone_rgb = Utility.hex_to_rgba(skin_tone_hex)[:3]\n\n                # this is done to make the chance higher that the representation of skin tones is more diverse\n                skin_tone_fac = random.uniform(0.0, 1)\n                skin_tone_rgb = [value * skin_tone_fac for value in skin_tone_rgb]\n                principled_bsdf.inputs[\"Base Color\"].default_value = mathutils.Vector([*skin_tone_rgb, 1.0])\n                principled_bsdf.inputs[\"Subsurface\"].default_value = 0.2\n                principled_bsdf.inputs[\"Subsurface Color\"].default_value = mathutils.Vector([*skin_tone_rgb, 1.0])\n                principled_bsdf.inputs[\"Subsurface Radius\"].default_value = mathutils.Vector([1.0, 0.2, 0.1])\n                principled_bsdf.inputs[\"Subsurface IOR\"].default_value = 2.5\n\n                # darker skin looks better when made less specular\n                principled_bsdf.inputs[\"Specular\"].default_value = np.mean(skin_tone_rgb) / 255.0\n\n                texture_nodes = material.get_nodes_with_type(\"ShaderNodeTexImage\")\n                if texture_nodes and len(texture_nodes) > 1:\n                    # find the image texture node which is connected to alpha\n                    node_connected_to_the_alpha = None\n                    for node_links in principled_bsdf.inputs[\"Alpha\"].links:\n                        if \"ShaderNodeTexImage\" in node_links.from_node.bl_idname:\n                            node_connected_to_the_alpha = node_links.from_node\n                    # if a node was found which is connected to the alpha node, add an invert between the two\n                    if node_connected_to_the_alpha is not None:\n                        invert_node = material.new_node(\"ShaderNodeInvert\")\n                        invert_node.inputs[\"Fac\"].default_value = 1.0\n                        material.insert_node_instead_existing_link(node_connected_to_the_alpha.outputs[\"Color\"],\n                                                                   invert_node.inputs[\"Color\"],\n                                                                   invert_node.outputs[\"Color\"],\n                                                                   principled_bsdf.inputs[\"Alpha\"])",
  "def get_pose_parameters(supported_mocap_datasets: dict, num_betas: int, used_sub_dataset_id: str,\n                            used_subject_id: str, used_sequence_id: int,\n                            used_frame_id: int) -> Tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n        \"\"\" Extract pose and shape parameters corresponding to the requested pose from the database to be\n        processed by the parametric model\n\n        :param supported_mocap_datasets: A dict which maps sub dataset names to their paths.\n        :param num_betas: Number of body parameters\n        :param used_sub_dataset_id: Identifier for the sub dataset, the dataset which the human pose object\n                                    should be extracted from.\n        :param used_subject_id: Type of motion from which the pose should be extracted, this is dataset\n                                dependent parameter.\n        :param used_sequence_id: Sequence id in the dataset, sequences are the motion recorded to represent\n                                 certain action.\n        :param used_frame_id: Frame id in a selected motion sequence. If none is selected a random one is picked\n        :return: tuple of arrays contains the parameters. Type: tuple\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on torch\n        #pylint: disable=import-outside-toplevel\n        import torch\n        #pylint: enable=import-outside-toplevel\n\n        # check if the sub_dataset is supported\n        if used_sub_dataset_id in supported_mocap_datasets:\n            # get path from dictionary\n            sub_dataset_path = supported_mocap_datasets[used_sub_dataset_id]\n            # concatenate path to specific\n            if not used_subject_id:\n                # if none was selected\n                possible_subject_ids = glob.glob(os.path.join(sub_dataset_path, \"*\"))\n                possible_subject_ids.sort()\n                if len(possible_subject_ids) > 0:\n                    used_subject_id_str = os.path.basename(random.choice(possible_subject_ids))\n                else:\n                    raise FileNotFoundError(f\"No subjects found in folder: {sub_dataset_path}\")\n            else:\n                used_subject_id_str = f\"{int(used_sequence_id):02d}\"\n\n            if used_sequence_id < 0:\n                # if no sequence id was selected\n                possible_sequence_ids = glob.glob(os.path.join(sub_dataset_path, used_subject_id_str, \"*\"))\n                possible_sequence_ids.sort()\n                if len(possible_sequence_ids) > 0:\n                    used_sequence_id = os.path.basename(random.choice(possible_sequence_ids))\n                    used_sequence_id = used_sequence_id[used_sequence_id.find(\"_\")+1:used_sequence_id.rfind(\"_\")]\n                else:\n                    raise FileNotFoundError(f\"No sequences found in folder: \"\n                                            f\"{os.path.join(sub_dataset_path, used_subject_id_str)}\")\n            subject_path = os.path.join(sub_dataset_path, used_subject_id_str)\n            used_subject_id_str_reduced = used_subject_id_str[:used_subject_id_str.find(\"_\")] \\\n                if \"_\" in used_subject_id_str else used_subject_id_str\n            sequence_path = os.path.join(subject_path, used_subject_id_str_reduced +\n                                         f\"_{int(used_sequence_id):02d}_poses.npz\")\n            if os.path.exists(sequence_path):\n                # load AMASS dataset sequence file which contains the coefficients for the whole motion sequence\n                sequence_body_data = np.load(sequence_path)\n                # get the number of supported frames\n                no_of_frames_per_sequence = sequence_body_data['poses'].shape[0]\n                if used_frame_id < 0:\n                    frame_id = random.randint(0, no_of_frames_per_sequence)  # pick a random id\n                else:\n                    frame_id = used_frame_id\n                # Extract Body Model coefficients\n                if frame_id in range(0, no_of_frames_per_sequence):\n                    # use GPU to accelerate mesh calculations\n                    comp_device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n                    # parameters that control the body pose\n                    # refer to http://files.is.tue.mpg.de/black/papers/amass.pdf, Section 3.1 for more\n                    # information about the parameter representation and the below chosen values\n                    pose_body = torch.Tensor(sequence_body_data['poses'][frame_id:frame_id + 1, 3:66]).to(comp_device)\n                    # parameters that control the body shape\n                    betas = torch.Tensor(sequence_body_data['betas'][:num_betas][np.newaxis]).to(comp_device)\n                    return pose_body, betas\n                raise RuntimeError(f\"Requested frame id is beyond sequence range, for the selected sequence, choose \"\n                                   f\"frame id within the following range: [0, {no_of_frames_per_sequence}]\")\n            raise RuntimeError(f\"Invalid sequence/subject: {used_subject_id} category identifiers, please choose a \"\n                               f\"valid one. Used path: {sequence_path}\")\n        raise RuntimeError(f\"The requested mocap dataset is not yest supported, please choose another one \"\n                           f\"from the following supported datasets: {list(supported_mocap_datasets.keys())}\")",
  "def load_parametric_body_model(data_path: str, used_body_model_gender: str, num_betas: int,\n                                   num_dmpls: int) -> Tuple[\"BodyModel\", np.array]:\n        \"\"\" loads the parametric model that is used to generate the mesh object\n\n        :return:  parametric model. Type: tuple.\n        \"\"\"\n        # This import is done inside to avoid having the requirement that BlenderProc depends on torch\n        #pylint: disable=import-outside-toplevel\n        import torch\n        from human_body_prior.body_model.body_model import BodyModel\n        #pylint: enable=import-outside-toplevel\n\n        # body model\n        bm_path = os.path.join(data_path, 'body_models', 'smplh', used_body_model_gender, 'model.npz')\n        # deformation model\n        dmpl_path = os.path.join(data_path, 'body_models', 'dmpls', used_body_model_gender, 'model.npz')\n        if not os.path.exists(bm_path) or not os.path.exists(dmpl_path):\n            raise FileNotFoundError(\"Parametric Body model doesn't exist, please follow download \"\n                                    \"instructions section in AMASS Example\")\n        # pylint: disable=no-member\n        comp_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        #pylint: enable=no-member\n        body_model = BodyModel(bm_path=bm_path, num_betas=num_betas, num_dmpls=num_dmpls,\n                               path_dmpl=dmpl_path).to(comp_device)\n        faces = body_model.f.detach().cpu().numpy()\n        return body_model, faces",
  "def get_supported_mocap_datasets(taxonomy_file_path: str, data_path: str) -> dict:\n        \"\"\" Get latest updated list from taxonomoy json file about the supported mocap datasets supported in the\n            loader module and update.supported_mocap_datasets list\n\n        :param taxonomy_file_path: path to taxomomy.json file which contains the supported datasets and their\n                                   respective paths. Type: string.\n        :param data_path: path to the AMASS dataset root folder. Type: string.\n        \"\"\"\n\n        # dictionary contains mocap dataset name and path to its sub folder within the main dataset, dictionary will\n        # be filled from taxonomy.json file which indicates the supported datastests\n        supported_mocap_datasets = {}\n        if os.path.exists(taxonomy_file_path):\n            with open(taxonomy_file_path, \"r\", encoding=\"utf-8\") as f:\n                loaded_data = json.load(f)\n                for block in loaded_data:\n                    if \"sub_data_id\" in block:\n                        sub_dataset_id = block[\"sub_data_id\"]\n                        supported_mocap_datasets[sub_dataset_id] = os.path.join(data_path, block[\"path\"])\n        else:\n            raise FileNotFoundError(f\"The taxonomy file could not be found: {taxonomy_file_path}\")\n\n        return supported_mocap_datasets",
  "def write_body_mesh_to_obj_file(body_representation: \"torch.Tensor\", faces: np.array, temp_dir: str) -> str:\n        \"\"\" Write the generated pose as obj file on the desk.\n\n        :param body_representation: parameters generated from the BodyModel model which represent the obj\n                                     pose and shape. Type: torch.Tensor\n        :param faces: face parametric model which is used to generate the face mesh. Type: numpy.array\n        :param temp_dir: Path to the folder in which the generated pose as obj will be stored\n        :return: path to generated obj file. Type: string.\n        \"\"\"\n        # Generate temp object with name = timestamp\n        starttime = datetime.now().replace(microsecond=0)\n        obj_file_name = datetime.strftime(starttime, '%Y%m%d_%H%M') + \".obj\"\n        # Write to an .obj file\n        outmesh_path = os.path.join(temp_dir, obj_file_name)\n        with open(outmesh_path, 'w', encoding=\"utf-8\") as fp:\n            fp.write(\"\".join([f'v {v[0]:f} {v[1]:f} {v[2]:f}\\n' for v in\n                              body_representation.v[0].detach().cpu().numpy()]))\n            fp.write(\"\".join([f'f {f[0]} {f[1]} {f[2]}\\n' for f in faces + 1]))\n        return outmesh_path",
  "def correct_materials(objects: List[MeshObject]):\n        \"\"\" If the used material contains an alpha texture, the alpha texture has to be flipped to be correct\n\n        :param objects: Mesh objects where the material might be wrong.\n        \"\"\"\n        for obj in objects:\n            if not obj.get_materials():\n                obj.new_material(\"Skin material\")\n            for material in obj.get_materials():\n                if material is None:\n                    continue\n                # Create a principled node and set the default color\n                principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n                # Pick random skin color value\n                skin_tone_hex = np.random.choice(_AMASSLoader.human_skin_colors)\n                skin_tone_rgb = Utility.hex_to_rgba(skin_tone_hex)[:3]\n\n                # this is done to make the chance higher that the representation of skin tones is more diverse\n                skin_tone_fac = random.uniform(0.0, 1)\n                skin_tone_rgb = [value * skin_tone_fac for value in skin_tone_rgb]\n                principled_bsdf.inputs[\"Base Color\"].default_value = mathutils.Vector([*skin_tone_rgb, 1.0])\n                principled_bsdf.inputs[\"Subsurface\"].default_value = 0.2\n                principled_bsdf.inputs[\"Subsurface Color\"].default_value = mathutils.Vector([*skin_tone_rgb, 1.0])\n                principled_bsdf.inputs[\"Subsurface Radius\"].default_value = mathutils.Vector([1.0, 0.2, 0.1])\n                principled_bsdf.inputs[\"Subsurface IOR\"].default_value = 2.5\n\n                # darker skin looks better when made less specular\n                principled_bsdf.inputs[\"Specular\"].default_value = np.mean(skin_tone_rgb) / 255.0\n\n                texture_nodes = material.get_nodes_with_type(\"ShaderNodeTexImage\")\n                if texture_nodes and len(texture_nodes) > 1:\n                    # find the image texture node which is connected to alpha\n                    node_connected_to_the_alpha = None\n                    for node_links in principled_bsdf.inputs[\"Alpha\"].links:\n                        if \"ShaderNodeTexImage\" in node_links.from_node.bl_idname:\n                            node_connected_to_the_alpha = node_links.from_node\n                    # if a node was found which is connected to the alpha node, add an invert between the two\n                    if node_connected_to_the_alpha is not None:\n                        invert_node = material.new_node(\"ShaderNodeInvert\")\n                        invert_node.inputs[\"Fac\"].default_value = 1.0\n                        material.insert_node_instead_existing_link(node_connected_to_the_alpha.outputs[\"Color\"],\n                                                                   invert_node.inputs[\"Color\"],\n                                                                   invert_node.outputs[\"Color\"],\n                                                                   principled_bsdf.inputs[\"Alpha\"])",
  "def load_pix3d(used_category: str, data_path: str = 'resources/pix3d') -> List[MeshObject]:\n    \"\"\" Loads one random Pix3D object from the given category.\n\n    :param used_category: The category to use for example: 'bed', check the data_path/model folder for more categories.\n                          Available: ['bed', 'bookcase', 'chair', 'desk', 'misc', 'sofa', 'table', 'tool', 'wardrobe']\n    :param data_path: The path to the Pix3D folder.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    data_path = resolve_path(data_path)\n    files_with_fitting_category = _Pix3DLoader.get_files_with_category(used_category, data_path)\n\n    selected_obj = random.choice(files_with_fitting_category)\n    loaded_obj = load_obj(selected_obj)\n\n    _Pix3DLoader.correct_materials(loaded_obj)\n\n    # removes the x axis rotation found in all ShapeNet objects, this is caused by importing .obj files\n    # the object has the same pose as before, just that the rotation_euler is now [0, 0, 0]\n    for obj in loaded_obj:\n        obj.persist_transformation_into_mesh(location=False, rotation=True, scale=False)\n\n    # move the origin of the object to the world origin and on top of the X-Y plane\n    # makes it easier to place them later on, this does not change the `.location`\n    for obj in loaded_obj:\n        obj.move_origin_to_bottom_mean_point()\n    bpy.ops.object.select_all(action='DESELECT')\n\n    return loaded_obj",
  "class _Pix3DLoader:\n    \"\"\"\n    This loads an object from Pix3D based on the given category of objects to use.\n\n    From these objects one is randomly sampled and loaded.\n\n    Note: if this class is used with another loader that loads objects with semantic mapping, make sure the other\n    module is loaded first. TODO: Really?\n    \"\"\"\n\n    @staticmethod\n    def get_files_with_category(used_category: str, data_path: str) -> list:\n        \"\"\"\n        Returns a list of a .obj file for the given category. This function creates a category path file for each used\n        category. This will speed up the usage the next time the category is used.\n\n        :param used_category: the category something like: 'bed', see the data_path folder for categories\n        :param data_path: path to the Pix3D folder\n        :return: list of .obj files, which are in the data_path folder, based on the given category\n        \"\"\"\n\n        path_to_annotation_file = os.path.join(data_path, \"pix3d.json\")\n        if os.path.exists(path_to_annotation_file):\n            files = []\n            path_to_category_file = os.path.join(data_path, f\"category_{used_category.strip()}_paths.txt\")\n            if os.path.exists(path_to_category_file):\n                with open(path_to_category_file, \"r\", encoding=\"utf-8\") as f:\n                    files = f.read().split(\"\\n\")\n            else:\n                with open(path_to_annotation_file, \"r\", encoding=\"utf-8\") as f:\n                    loaded_data = json.load(f)\n                    for block in loaded_data:\n                        if \"category\" in block:\n                            category = block[\"category\"]\n                            if category == used_category:\n                                files.append(block[\"model\"])\n                # remove doubles\n                files = list(set(files))\n                with open(path_to_category_file, \"w\", encoding=\"utf-8\") as f:\n                    f.write(\"\\n\".join(files))\n            files = [os.path.join(data_path, file) for file in files]\n            return files\n        raise FileNotFoundError(f\"The annotation file could not be found: {path_to_annotation_file}\")\n\n    @staticmethod\n    def correct_materials(objects: List[MeshObject]):\n        \"\"\" If the used material contains an alpha texture, the alpha texture has to be flipped to be correct\n\n        :param objects: The list of mesh objects where the material maybe wrong.\n        \"\"\"\n\n        for obj in objects:\n            for material in obj.get_materials():\n                if material is None:\n                    continue\n                texture_nodes = material.get_nodes_with_type(\"ShaderNodeTexImage\")\n                if texture_nodes and len(texture_nodes) > 1:\n                    principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n                    # find the image texture node which is connected to alpha\n                    node_connected_to_the_alpha = None\n                    for node_links in principled_bsdf.inputs[\"Alpha\"].links:\n                        if \"ShaderNodeTexImage\" in node_links.from_node.bl_idname:\n                            node_connected_to_the_alpha = node_links.from_node\n                    # if a node was found which is connected to the alpha node, add an invert between the two\n                    if node_connected_to_the_alpha is not None:\n                        invert_node = material.new_node(\"ShaderNodeInvert\")\n                        invert_node.inputs[\"Fac\"].default_value = 1.0\n                        material.insert_node_instead_existing_link(node_connected_to_the_alpha.outputs[\"Color\"],\n                                                                   invert_node.inputs[\"Color\"],\n                                                                   invert_node.outputs[\"Color\"],\n                                                                   principled_bsdf.inputs[\"Alpha\"])",
  "def get_files_with_category(used_category: str, data_path: str) -> list:\n        \"\"\"\n        Returns a list of a .obj file for the given category. This function creates a category path file for each used\n        category. This will speed up the usage the next time the category is used.\n\n        :param used_category: the category something like: 'bed', see the data_path folder for categories\n        :param data_path: path to the Pix3D folder\n        :return: list of .obj files, which are in the data_path folder, based on the given category\n        \"\"\"\n\n        path_to_annotation_file = os.path.join(data_path, \"pix3d.json\")\n        if os.path.exists(path_to_annotation_file):\n            files = []\n            path_to_category_file = os.path.join(data_path, f\"category_{used_category.strip()}_paths.txt\")\n            if os.path.exists(path_to_category_file):\n                with open(path_to_category_file, \"r\", encoding=\"utf-8\") as f:\n                    files = f.read().split(\"\\n\")\n            else:\n                with open(path_to_annotation_file, \"r\", encoding=\"utf-8\") as f:\n                    loaded_data = json.load(f)\n                    for block in loaded_data:\n                        if \"category\" in block:\n                            category = block[\"category\"]\n                            if category == used_category:\n                                files.append(block[\"model\"])\n                # remove doubles\n                files = list(set(files))\n                with open(path_to_category_file, \"w\", encoding=\"utf-8\") as f:\n                    f.write(\"\\n\".join(files))\n            files = [os.path.join(data_path, file) for file in files]\n            return files\n        raise FileNotFoundError(f\"The annotation file could not be found: {path_to_annotation_file}\")",
  "def correct_materials(objects: List[MeshObject]):\n        \"\"\" If the used material contains an alpha texture, the alpha texture has to be flipped to be correct\n\n        :param objects: The list of mesh objects where the material maybe wrong.\n        \"\"\"\n\n        for obj in objects:\n            for material in obj.get_materials():\n                if material is None:\n                    continue\n                texture_nodes = material.get_nodes_with_type(\"ShaderNodeTexImage\")\n                if texture_nodes and len(texture_nodes) > 1:\n                    principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n                    # find the image texture node which is connected to alpha\n                    node_connected_to_the_alpha = None\n                    for node_links in principled_bsdf.inputs[\"Alpha\"].links:\n                        if \"ShaderNodeTexImage\" in node_links.from_node.bl_idname:\n                            node_connected_to_the_alpha = node_links.from_node\n                    # if a node was found which is connected to the alpha node, add an invert between the two\n                    if node_connected_to_the_alpha is not None:\n                        invert_node = material.new_node(\"ShaderNodeInvert\")\n                        invert_node.inputs[\"Fac\"].default_value = 1.0\n                        material.insert_node_instead_existing_link(node_connected_to_the_alpha.outputs[\"Color\"],\n                                                                   invert_node.inputs[\"Color\"],\n                                                                   invert_node.outputs[\"Color\"],\n                                                                   principled_bsdf.inputs[\"Alpha\"])",
  "def set_world_background_hdr_img(path_to_hdr_file: str, strength: float = 1.0,\n                                 rotation_euler: Union[list, Euler, np.ndarray] = None):\n    \"\"\"\n    Sets the world background to the given hdr_file.\n\n    :param path_to_hdr_file: Path to the .hdr file\n    :param strength: The brightness of the background.\n    :param rotation_euler: The euler angles of the background.\n    \"\"\"\n    if rotation_euler is None:\n        rotation_euler = [0.0, 0.0, 0.0]\n\n    if not os.path.exists(path_to_hdr_file):\n        raise FileNotFoundError(f\"The given path does not exists: {path_to_hdr_file}\")\n\n    world = bpy.context.scene.world\n    nodes = world.node_tree.nodes\n    links = world.node_tree.links\n\n    # add a texture node and load the image and link it\n    texture_node = nodes.new(type=\"ShaderNodeTexEnvironment\")\n    texture_node.image = bpy.data.images.load(path_to_hdr_file, check_existing=True)\n\n    # get the one background node of the world shader\n    background_node = Utility.get_the_one_node_with_type(nodes, \"Background\")\n\n    # link the new texture node to the background\n    links.new(texture_node.outputs[\"Color\"], background_node.inputs[\"Color\"])\n\n    # Set the brightness of the background\n    background_node.inputs[\"Strength\"].default_value = strength\n\n    # add a mapping node and a texture coordinate node\n    mapping_node = nodes.new(\"ShaderNodeMapping\")\n    tex_coords_node = nodes.new(\"ShaderNodeTexCoord\")\n\n    #link the texture coordinate node to mapping node\n    links.new(tex_coords_node.outputs[\"Generated\"], mapping_node.inputs[\"Vector\"])\n\n    #link the mapping node to the texture node\n    links.new(mapping_node.outputs[\"Vector\"], texture_node.inputs[\"Vector\"])\n\n    mapping_node.inputs[\"Rotation\"].default_value = rotation_euler",
  "def get_random_world_background_hdr_img_path_from_haven(data_path: str) -> str:\n    \"\"\" Sets the world background to a random .hdr file from the given directory.\n\n    :param data_path: A path pointing to a directory containing .hdr files.\n    :return The path to a random selected path\n    \"\"\"\n\n    if os.path.exists(data_path):\n        data_path = os.path.join(data_path, \"hdris\")\n        if not os.path.exists(data_path):\n            raise FileNotFoundError(f\"The folder: {data_path} does not contain a folder name hdfris. \"\n                                    f\"Please use the download script.\")\n    else:\n        raise FileNotFoundError(f\"The data path does not exists: {data_path}\")\n\n    hdr_files = glob.glob(os.path.join(data_path, \"*\", \"*.hdr\"))\n    # this will be ensure that the call is deterministic\n    hdr_files.sort()\n\n    # this file be used\n    random_hdr_file = random.choice(hdr_files)\n\n    return random_hdr_file",
  "def load_ikea(data_dir: str = 'resources/IKEA', obj_categories: Optional[Union[List[str], str]] = None,\n              obj_style: Optional[str] = None) -> List[MeshObject]:\n    \"\"\" Loads ikea objects based on selected type and style.\n\n    If there are multiple options it picks one randomly or if style or type is None it picks one randomly.\n\n    :param data_dir: The directory with all the IKEA models.\n    :param obj_categories: The category to use for example: 'bookcase'. This can also be a list of elements.\n                           Available: ['bed', 'bookcase', 'chair', 'desk', 'sofa', 'table', 'wardrobe']\n    :param obj_style: The IKEA style to use for example: 'hemnes'. See data_dir for other options.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    obj_dict = _IKEALoader.generate_object_dict(data_dir)\n\n    # If obj_categories is given, make sure it is a list\n    if obj_categories is not None and not isinstance(obj_categories, list):\n        obj_categories = [obj_categories]\n\n    if obj_categories is not None and obj_style is not None:\n        object_lst = []\n        for obj_category in obj_categories:\n            object_lst.extend([obj[0] for (key, obj) in obj_dict.items() \\\n                               if obj_style in key.lower() and obj_category in key])\n        if not object_lst:\n            selected_obj = random.choice(obj_dict.get(random.choice(list(obj_dict.keys()))))\n            warnings.warn(f\"Could not find object of type: {obj_categories}, and style: \"\n                          f\"{obj_style}. Selecting random object...\", category=Warning)\n        else:\n            # Multiple objects with same type and style are possible: select randomly from list.\n            selected_obj = random.choice(object_lst)\n    elif obj_categories is not None:\n        object_lst = []\n        for obj_category in obj_categories:\n            object_lst.extend(_IKEALoader.get_object_by_type(obj_category, obj_dict))\n        selected_obj = random.choice(object_lst)\n    elif obj_style is not None:\n        object_lst = _IKEALoader.get_object_by_style(obj_style, obj_dict)\n        selected_obj = random.choice(object_lst)\n    else:\n        random_key = random.choice(list(obj_dict.keys()))\n        # One key can have multiple object files as value: select randomly from list.\n        selected_obj = random.choice(obj_dict.get(random_key))\n\n    print(\"Selected object: \", os.path.basename(selected_obj))\n    loaded_obj = load_obj(selected_obj)\n\n    # extract the name from the path:\n    selected_dir_name = os.path.dirname(selected_obj)\n    selected_name = \"\"\n    if os.path.basename(selected_dir_name).startswith(\"IKEA_\"):\n        selected_name = os.path.basename(selected_dir_name)\n    else:\n        selected_dir_name = os.path.dirname(selected_dir_name)\n        if os.path.basename(selected_dir_name).startswith(\"IKEA_\"):\n            selected_name = os.path.basename(selected_dir_name)\n    if selected_name:\n        for obj in loaded_obj:\n            obj.set_name(selected_name)\n\n    # extract the file unit from the .obj file to convert every object to meters\n    file_unit = \"\"\n    with open(selected_obj, \"r\", encoding=\"utf-8\") as file:\n        first_lines = [next(file) for x in range(5)]\n        for line in first_lines:\n            if \"File units\" in line:\n                file_unit = line.strip().split(\" \")[-1]\n                if file_unit not in [\"inches\", \"meters\", \"centimeters\", \"millimeters\"]:\n                    raise RuntimeError(f\"The file unit type could not be found, check the selected \"\n                                       f\"file: {selected_obj}\")\n                break\n\n    for obj in loaded_obj:\n        # convert all objects to meters\n        if file_unit == \"inches\":\n            scale = 0.0254\n        elif file_unit == \"centimeters\":\n            scale = 0.01\n        elif file_unit == \"millimeters\":\n            scale = 0.001\n        elif file_unit == \"meters\":\n            scale = 1.0\n        else:\n            raise RuntimeError(f\"The file unit type: {file_unit} is not defined\")\n        if scale != 1.0:\n            # move all object centers to the world origin and set the bounding box correctly\n            bpy.ops.object.select_all(action='DESELECT')\n            obj.select()\n            bpy.context.view_layer.objects.active = obj.blender_obj\n            # scale object down\n            bpy.ops.object.mode_set(mode='EDIT')\n            bpy.ops.mesh.select_all(action='SELECT')\n            bpy.ops.transform.resize(value=(scale, scale, scale))\n            bpy.ops.object.mode_set(mode='OBJECT')\n            bpy.ops.object.select_all(action='DESELECT')\n\n    # removes the x-axis rotation found in all ShapeNet objects, this is caused by importing .obj files\n    # the object has the same pose as before, just that the rotation_euler is now [0, 0, 0]\n    for obj in loaded_obj:\n        obj.persist_transformation_into_mesh(location=False, rotation=True, scale=False)\n\n    # move the origin of the object to the world origin and on top of the X-Y plane\n    # makes it easier to place them later on, this does not change the `.location`\n    for obj in loaded_obj:\n        obj.move_origin_to_bottom_mean_point()\n    bpy.ops.object.select_all(action='DESELECT')\n    return loaded_obj",
  "class _IKEALoader:\n    \"\"\"\n    This class loads objects from the IKEA dataset.\n\n    Objects can be selected randomly, based on object type, object style, or both.\n    \"\"\"\n\n    @staticmethod\n    def generate_object_dict(data_dir: str) -> Dict[str, List[str]]:\n        \"\"\"\n        Generates a dictionary of all available objects, i.e. all .obj files that have an associated .mtl file.\n\n        :param data_dir: The directory with all the IKEA models.\n        :return: dict: {IKEA_<type>_<style> : [<path_to_obj_file>, ...]}\n        \"\"\"\n        obj_dict = {}\n        counter = 0\n        obj_files = glob.glob(os.path.join(data_dir, \"IKEA\", \"*\", \"*.obj\"))\n        # this will ensure that the call is deterministic\n        obj_files.sort()\n        for obj_file in obj_files:\n            category = [s for s in obj_file.split('/') if 'IKEA_' in s][0]\n            if _IKEALoader.check_material_file(obj_file):\n                obj_dict.setdefault(category, []).append(obj_file)\n                counter += 1\n\n        print(f'Found {counter} object files in dataset belonging to {len(obj_dict)} categories')\n        if not obj_dict:\n            raise RuntimeError(\"No obj file was found, check if the correct folder is provided!\")\n        # to avoid randomness while accessing the dict\n        obj_dict = OrderedDict(obj_dict)\n\n        return obj_dict\n\n    @staticmethod\n    def check_material_file(path: str) -> bool:\n        \"\"\"\n        Checks whether there is a texture file (.mtl) associated to the object available.\n\n        :param path: path to object\n        :return: texture file exists\n        \"\"\"\n        name = os.path.basename(path).split(\".\")[0]\n        obj_dir = os.path.dirname(path)\n        mtl_path = os.path.join(obj_dir, name + \".mtl\")\n        return os.path.exists(mtl_path)\n\n    @staticmethod\n    def get_object_by_type(obj_type: str, obj_dict: Dict[str, List[str]]) -> List[str]:\n        \"\"\"\n        Finds all available objects with a specific type.\n\n        :param obj_type: type of object e.g. 'table'\n        :param obj_dict: mapping of the style and type to the path {IKEA_<type>_<style> : [<path_to_obj_file>, ...]}\n        :return: list of available objects with specified type\n        \"\"\"\n        object_lst = [obj[0] for (key, obj) in obj_dict.items() if obj_type in key]\n        if not object_lst:\n            warnings.warn(f\"There were no objects found matching the type: {obj_type}.\", category=Warning)\n        return object_lst\n\n    @staticmethod\n    def get_object_by_style(obj_style: str, obj_dict: Dict[str, List[str]]) -> List[str]:\n        \"\"\"\n        Finds all available objects with a specific style, i.e. IKEA product series.\n\n        :param obj_style: Style of the object\n        :param obj_dict: mapping of the style and type to the path {IKEA_<type>_<style> : [<path_to_obj_file>, ...]}\n        :return: (list) list of available objects with specified style\n        \"\"\"\n        object_lst = [obj[0] for (key, obj) in obj_dict.items() if obj_style in key.lower()]\n        if not object_lst:\n            warnings.warn(f\"There were no objects found matching the style: {obj_style}.\", category=Warning)\n        return object_lst",
  "def generate_object_dict(data_dir: str) -> Dict[str, List[str]]:\n        \"\"\"\n        Generates a dictionary of all available objects, i.e. all .obj files that have an associated .mtl file.\n\n        :param data_dir: The directory with all the IKEA models.\n        :return: dict: {IKEA_<type>_<style> : [<path_to_obj_file>, ...]}\n        \"\"\"\n        obj_dict = {}\n        counter = 0\n        obj_files = glob.glob(os.path.join(data_dir, \"IKEA\", \"*\", \"*.obj\"))\n        # this will ensure that the call is deterministic\n        obj_files.sort()\n        for obj_file in obj_files:\n            category = [s for s in obj_file.split('/') if 'IKEA_' in s][0]\n            if _IKEALoader.check_material_file(obj_file):\n                obj_dict.setdefault(category, []).append(obj_file)\n                counter += 1\n\n        print(f'Found {counter} object files in dataset belonging to {len(obj_dict)} categories')\n        if not obj_dict:\n            raise RuntimeError(\"No obj file was found, check if the correct folder is provided!\")\n        # to avoid randomness while accessing the dict\n        obj_dict = OrderedDict(obj_dict)\n\n        return obj_dict",
  "def check_material_file(path: str) -> bool:\n        \"\"\"\n        Checks whether there is a texture file (.mtl) associated to the object available.\n\n        :param path: path to object\n        :return: texture file exists\n        \"\"\"\n        name = os.path.basename(path).split(\".\")[0]\n        obj_dir = os.path.dirname(path)\n        mtl_path = os.path.join(obj_dir, name + \".mtl\")\n        return os.path.exists(mtl_path)",
  "def get_object_by_type(obj_type: str, obj_dict: Dict[str, List[str]]) -> List[str]:\n        \"\"\"\n        Finds all available objects with a specific type.\n\n        :param obj_type: type of object e.g. 'table'\n        :param obj_dict: mapping of the style and type to the path {IKEA_<type>_<style> : [<path_to_obj_file>, ...]}\n        :return: list of available objects with specified type\n        \"\"\"\n        object_lst = [obj[0] for (key, obj) in obj_dict.items() if obj_type in key]\n        if not object_lst:\n            warnings.warn(f\"There were no objects found matching the type: {obj_type}.\", category=Warning)\n        return object_lst",
  "def get_object_by_style(obj_style: str, obj_dict: Dict[str, List[str]]) -> List[str]:\n        \"\"\"\n        Finds all available objects with a specific style, i.e. IKEA product series.\n\n        :param obj_style: Style of the object\n        :param obj_dict: mapping of the style and type to the path {IKEA_<type>_<style> : [<path_to_obj_file>, ...]}\n        :return: (list) list of available objects with specified style\n        \"\"\"\n        object_lst = [obj[0] for (key, obj) in obj_dict.items() if obj_style in key.lower()]\n        if not object_lst:\n            warnings.warn(f\"There were no objects found matching the style: {obj_style}.\", category=Warning)\n        return object_lst",
  "def load_scenenet(file_path: str, texture_folder: str, label_mapping: LabelIdMapping,\n                  unknown_texture_folder: Optional[str] = None) -> List[MeshObject]:\n    \"\"\" Loads all SceneNet objects at the given \"file_path\".\n\n    The textures for each object are sampled based on the name of the object, if the name is not represented in the\n    texture folder the unknown folder is used. This folder does not exist, after downloading the texture dataset.\n    Make sure to create and put some textures, you want to use for these instances there.\n\n    All objects get \"category_id\" set based on the data in the \"resources/id_mappings/nyu_idset.csv\"\n\n    Each object will have the custom property \"is_scene_net_obj\".\n\n    :param file_path: The path to the .obj file from SceneNet.\n    :param label_mapping: A dict which maps the names of the objects to ids.\n    :param texture_folder: The path to the texture folder used to sample the textures.\n    :param unknown_texture_folder: The path to the textures, which are used if the texture type is unknown.\n                                   The default path does not exist if the dataset was just downloaded, it has to\n                                   be created manually.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    if unknown_texture_folder is None:\n        unknown_texture_folder = os.path.join(texture_folder, \"unknown\")\n\n    # blender search the whole root directory recursively!\n    loaded_objects = load_obj(filepath=file_path)\n    loaded_objects.sort(key=lambda ele: ele.get_name())\n    # sample materials for each object\n    _SceneNetLoader.random_sample_materials_for_each_obj(loaded_objects, texture_folder, unknown_texture_folder)\n\n    # set the category ids for each object\n    _SceneNetLoader.set_category_ids(loaded_objects, label_mapping)\n\n    for obj in loaded_objects:\n        obj.set_cp(\"is_scene_net_obj\", True)\n\n    return loaded_objects",
  "class _SceneNetLoader:\n\n    @staticmethod\n    def random_sample_materials_for_each_obj(loaded_objects: List[MeshObject], texture_folder: str,\n                                             unknown_texture_folder: str):\n        \"\"\"\n        Random sample materials for each of the loaded objects\n\n        Based on the name the textures from the texture_folder will be selected\n\n        :param loaded_objects: objects loaded from the .obj file\n        :param texture_folder: The path to the texture folder used to sample the textures.\n        :param unknown_texture_folder: The path to the textures, which are used if the the texture type is unknown.\n        \"\"\"\n        # for each object add a material\n        for obj in loaded_objects:\n            for material in obj.get_materials():\n                if material is None:\n                    continue\n                principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n                texture_nodes = material.get_nodes_with_type(\"ShaderNodeTexImage\")\n                if not texture_nodes or len(texture_nodes) == 1:\n                    if len(texture_nodes) == 1:\n                        # these materials do not exist they are just named in the .mtl files\n                        texture_node = texture_nodes[0]\n                    else:\n                        texture_node = material.new_node(\"ShaderNodeTexImage\")\n                    mat_name = material.get_name()\n                    if \".\" in mat_name:\n                        mat_name = mat_name[:mat_name.find(\".\")]\n                    mat_name = mat_name.replace(\"_\", \"\")\n                    # remove all digits from the string\n                    mat_name = ''.join([i for i in mat_name if not i.isdigit()])\n                    image_paths = glob.glob(os.path.join(texture_folder, mat_name, \"*\"))\n                    if not image_paths:\n                        if not os.path.exists(unknown_texture_folder):\n                            raise FileNotFoundError(f\"The unknown texture folder does not exist: \"\n                                                    f\"{unknown_texture_folder}, check if it was set correctly \"\n                                                    f\"via the config.\")\n                        image_paths = glob.glob(os.path.join(unknown_texture_folder, \"*\"))\n                        if not image_paths:\n                            raise FileNotFoundError(f\"The unknown texture folder did not contain any \"\n                                                    f\"textures: {unknown_texture_folder}\")\n                    image_paths.sort()\n                    image_path = random.choice(image_paths)\n                    if os.path.exists(image_path):\n                        texture_node.image = bpy.data.images.load(image_path, check_existing=True)\n                    else:\n                        raise FileNotFoundError(f\"No image was found for this entity: {obj.get_name()}, \"\n                                                f\"material name: {mat_name}\")\n                    material.link(texture_node.outputs[\"Color\"], principled_bsdf.inputs[\"Base Color\"])\n        for obj in loaded_objects:\n            obj_name = obj.get_name()\n            if \".\" in obj_name:\n                obj_name = obj_name[:obj_name.find(\".\")]\n            obj_name = obj_name.lower()\n            if \"wall\" in obj_name or \"floor\" in obj_name or \"ceiling\" in obj_name:\n                # set the shading of all polygons to flat\n                obj.set_shading_mode(\"FLAT\")\n\n    @staticmethod\n    def set_category_ids(loaded_objects: List[MeshObject], label_mapping: LabelIdMapping):\n        \"\"\"\n        Set the category ids for the objs based on the .csv file loaded in LabelIdMapping\n\n        Each object will have a custom property with a label, can be used by the SegMapRenderer.\n\n        :param loaded_objects: objects loaded from the .obj file\n        :param label_mapping: A dict which maps the names of the objects to ids.\n        \"\"\"\n\n        #  Some category names in scenenet objects are written differently than in nyu_idset.csv\n        normalize_name = {\"floor-mat\": \"floor_mat\", \"refrigerator\": \"refridgerator\", \"shower-curtain\": \"shower_curtain\",\n                          \"nightstand\": \"night_stand\", \"Other-structure\": \"otherstructure\",\n                          \"Other-furniture\": \"otherfurniture\", \"Other-prop\": \"otherprop\",\n                          \"floor_tiles_floor_tiles_0125\": \"floor\", \"ground\": \"floor\",\n                          \"floor_enclose\": \"floor\", \"floor_enclose2\": \"floor\",\n                          \"floor_base_object01_56\": \"floor\", \"walls1_line01_12\": \"wall\", \"room_skeleton\": \"wall\",\n                          \"ceilingwall\": \"ceiling\"}\n\n        for obj in loaded_objects:\n            obj_name = obj.get_name().lower().split(\".\")[0]\n\n            # If it's one of the cases that the category have different names in both idsets.\n            obj_name = normalize_name.get(obj_name, obj_name)  # Then normalize it.\n\n            if label_mapping.has_label(obj_name):\n                obj.set_cp(\"category_id\", label_mapping.id_from_label(obj_name))\n            # Check whether the object's name without suffixes like 's', '1' or '2' exist in the mapping.\n            elif label_mapping.has_label(obj_name[:-1]):\n                obj.set_cp(\"category_id\", label_mapping.id_from_label(obj_name[:-1]))\n            elif \"painting\" in obj_name:\n                obj.set_cp(\"category_id\", label_mapping.id_from_label(\"picture\"))\n            else:\n                print(f\"This object was not specified: {obj_name} use objects for it.\")\n                obj.set_cp(\"category_id\", label_mapping.id_from_label(\"otherstructure\".lower()))\n\n            # Correct names of floor and ceiling objects to make them later\n            # easier to identify (e.g. by the FloorExtractor)\n            if obj.get_cp(\"category_id\") == label_mapping.id_from_label(\"floor\"):\n                obj.set_name(\"floor\")\n            elif obj.get_cp(\"category_id\") == label_mapping.id_from_label(\"ceiling\"):\n                obj.set_name(\"ceiling\")",
  "def random_sample_materials_for_each_obj(loaded_objects: List[MeshObject], texture_folder: str,\n                                             unknown_texture_folder: str):\n        \"\"\"\n        Random sample materials for each of the loaded objects\n\n        Based on the name the textures from the texture_folder will be selected\n\n        :param loaded_objects: objects loaded from the .obj file\n        :param texture_folder: The path to the texture folder used to sample the textures.\n        :param unknown_texture_folder: The path to the textures, which are used if the the texture type is unknown.\n        \"\"\"\n        # for each object add a material\n        for obj in loaded_objects:\n            for material in obj.get_materials():\n                if material is None:\n                    continue\n                principled_bsdf = material.get_the_one_node_with_type(\"BsdfPrincipled\")\n                texture_nodes = material.get_nodes_with_type(\"ShaderNodeTexImage\")\n                if not texture_nodes or len(texture_nodes) == 1:\n                    if len(texture_nodes) == 1:\n                        # these materials do not exist they are just named in the .mtl files\n                        texture_node = texture_nodes[0]\n                    else:\n                        texture_node = material.new_node(\"ShaderNodeTexImage\")\n                    mat_name = material.get_name()\n                    if \".\" in mat_name:\n                        mat_name = mat_name[:mat_name.find(\".\")]\n                    mat_name = mat_name.replace(\"_\", \"\")\n                    # remove all digits from the string\n                    mat_name = ''.join([i for i in mat_name if not i.isdigit()])\n                    image_paths = glob.glob(os.path.join(texture_folder, mat_name, \"*\"))\n                    if not image_paths:\n                        if not os.path.exists(unknown_texture_folder):\n                            raise FileNotFoundError(f\"The unknown texture folder does not exist: \"\n                                                    f\"{unknown_texture_folder}, check if it was set correctly \"\n                                                    f\"via the config.\")\n                        image_paths = glob.glob(os.path.join(unknown_texture_folder, \"*\"))\n                        if not image_paths:\n                            raise FileNotFoundError(f\"The unknown texture folder did not contain any \"\n                                                    f\"textures: {unknown_texture_folder}\")\n                    image_paths.sort()\n                    image_path = random.choice(image_paths)\n                    if os.path.exists(image_path):\n                        texture_node.image = bpy.data.images.load(image_path, check_existing=True)\n                    else:\n                        raise FileNotFoundError(f\"No image was found for this entity: {obj.get_name()}, \"\n                                                f\"material name: {mat_name}\")\n                    material.link(texture_node.outputs[\"Color\"], principled_bsdf.inputs[\"Base Color\"])\n        for obj in loaded_objects:\n            obj_name = obj.get_name()\n            if \".\" in obj_name:\n                obj_name = obj_name[:obj_name.find(\".\")]\n            obj_name = obj_name.lower()\n            if \"wall\" in obj_name or \"floor\" in obj_name or \"ceiling\" in obj_name:\n                # set the shading of all polygons to flat\n                obj.set_shading_mode(\"FLAT\")",
  "def set_category_ids(loaded_objects: List[MeshObject], label_mapping: LabelIdMapping):\n        \"\"\"\n        Set the category ids for the objs based on the .csv file loaded in LabelIdMapping\n\n        Each object will have a custom property with a label, can be used by the SegMapRenderer.\n\n        :param loaded_objects: objects loaded from the .obj file\n        :param label_mapping: A dict which maps the names of the objects to ids.\n        \"\"\"\n\n        #  Some category names in scenenet objects are written differently than in nyu_idset.csv\n        normalize_name = {\"floor-mat\": \"floor_mat\", \"refrigerator\": \"refridgerator\", \"shower-curtain\": \"shower_curtain\",\n                          \"nightstand\": \"night_stand\", \"Other-structure\": \"otherstructure\",\n                          \"Other-furniture\": \"otherfurniture\", \"Other-prop\": \"otherprop\",\n                          \"floor_tiles_floor_tiles_0125\": \"floor\", \"ground\": \"floor\",\n                          \"floor_enclose\": \"floor\", \"floor_enclose2\": \"floor\",\n                          \"floor_base_object01_56\": \"floor\", \"walls1_line01_12\": \"wall\", \"room_skeleton\": \"wall\",\n                          \"ceilingwall\": \"ceiling\"}\n\n        for obj in loaded_objects:\n            obj_name = obj.get_name().lower().split(\".\")[0]\n\n            # If it's one of the cases that the category have different names in both idsets.\n            obj_name = normalize_name.get(obj_name, obj_name)  # Then normalize it.\n\n            if label_mapping.has_label(obj_name):\n                obj.set_cp(\"category_id\", label_mapping.id_from_label(obj_name))\n            # Check whether the object's name without suffixes like 's', '1' or '2' exist in the mapping.\n            elif label_mapping.has_label(obj_name[:-1]):\n                obj.set_cp(\"category_id\", label_mapping.id_from_label(obj_name[:-1]))\n            elif \"painting\" in obj_name:\n                obj.set_cp(\"category_id\", label_mapping.id_from_label(\"picture\"))\n            else:\n                print(f\"This object was not specified: {obj_name} use objects for it.\")\n                obj.set_cp(\"category_id\", label_mapping.id_from_label(\"otherstructure\".lower()))\n\n            # Correct names of floor and ceiling objects to make them later\n            # easier to identify (e.g. by the FloorExtractor)\n            if obj.get_cp(\"category_id\") == label_mapping.id_from_label(\"floor\"):\n                obj.set_name(\"floor\")\n            elif obj.get_cp(\"category_id\") == label_mapping.id_from_label(\"ceiling\"):\n                obj.set_name(\"ceiling\")",
  "class RockEssentialsRockLoader:\n    \"\"\" Loads rocks/cliffs from a specified .blend Rocks Essentials file. \"\"\"\n\n    @staticmethod\n    def load_rocks(path: str, subsec_num: int, objects: Optional[List[str]] = None,\n                   sample_objects: bool = False, amount: int = None) -> List[MeshObject]:\n        \"\"\" Loads rocks from the given blend file.\n\n        :param path: Path to a .blend file containing desired rock/cliff objects in //Object// section.\n        :param subsec_num: Number of a corresponding cell (batch) in `rocks` list in configuration.\n                           Used for name generation.\n        :param objects: List of rock-/cliff-object names to be loaded. If not specified then `amount` property\n                        is used for consequential loading.\n        :param sample_objects: Toggles the uniform sampling of objects to load. Takes into account `objects` and\n                               `amount` parameters. Requires 'amount' param to be defined.\n        :param amount: Amount of rock-/cliff-object to load. If not specified, the amount will be set to\n                       the amount of suitable objects in the current section of a blend file. Must be bigger than 0.\n        :return: List of loaded objects.\n        \"\"\"\n        loaded_objects = []\n        obj_types = [\"Rock\", \"Cliff\"]\n        amount_defined = False\n        if objects is None:\n            objects = []\n\n        obj_list = []\n        with bpy.data.libraries.load(path) as (data_from, _):\n            # if list of names is empty\n            if not objects:\n                # get list of rocks suitable for loading - objects that are rocks or cliffs\n                for obj_type in obj_types:\n                    obj_list += [obj for obj in data_from.objects if obj_type in obj]\n            else:\n                # if names are defined - get those that are available in this .blend file\n                obj_list = [obj for obj in data_from.objects if obj in objects]\n\n        # get amount of rocks in this batch, set to all suitable if not defined\n        if amount is not None:\n            amount_defined = True\n            if amount == 0:\n                raise RuntimeError(\"Amount param can't be equal to zero!\")\n        else:\n            amount = len(obj_list)\n\n        for i in range(amount):\n            # load rock: choose random from the list if sampling is True, go through list if not\n            if sample_objects and amount_defined:\n                obj = choice(obj_list)\n            else:\n                obj = obj_list[i % len(obj_list)]\n            bpy.ops.wm.append(filepath=os.path.join(path, \"Object\", obj), filename=obj,\n                              directory=os.path.join(path, \"Object\"))\n            loaded_obj = MeshObject(bpy.context.scene.objects[obj])\n            # set custom name for easier tracking in the scene\n            loaded_obj.set_name(obj + \"_\" + str(subsec_num) + \"_\" + str(i))\n            # append to return list\n            loaded_objects.append(loaded_obj)\n\n        return loaded_objects\n\n    @staticmethod\n    def set_rocks_properties(objects: List[MeshObject], physics: bool = False, render_levels: int = 3,\n                             high_detail_mode: bool = False, scale: Optional[Union[Vector, np.ndarray, list]] = None,\n                             reflection_amount: Optional[float] = None, reflection_roughness: Optional[float] = None,\n                             hsv: Optional[List[float]] = None):\n        \"\"\" Sets rocks properties in accordance to the given parameters.\n\n        :param objects: List of loaded rock mesh objects.\n        :param physics: Custom property for physics/rigidbody state.\n        :param render_levels: Number of subdivisions to perform when rendering.\n        :param high_detail_mode: Flag for enabling HDM when possible.\n        :param scale: Scale of a rock as a 3d-vector with each value as a scaling factor per according dimension.\n        :param reflection_amount: Reflection texture value. Default: rock-specific. Range: [0,1]\n        :param reflection_roughness: Roughness texture value. Default: rock-specific. Range: [0,1]\n        :param hsv: Hue-Saturation-Value parameters of the HSV node. (3 values).\n                    Range: H: [0, 1], S: [0, 2], V: [0, 2]. Default: rock-specific.\n        \"\"\"\n\n        if scale is None:\n            scale = [1.0, 1.0, 1.0]\n\n        for obj in objects:\n            # set physics parameter\n            obj.set_cp(\"physics\", physics)\n            # set category id\n            obj.set_cp(\"category_id\", 1)\n            # set render value\n            obj.blender_obj.modifiers[\"Subsurf\"].render_levels = render_levels\n            # set scale\n            obj.set_scale(scale)\n            # set HDM if enabled\n            if obj.has_cp(\"01) High Detail Mode\"):\n                obj.set_cp(\"01) High Detail Mode\", high_detail_mode)\n            else:\n                print(\"High Detail Mode is unavailable for \" + str(obj.get_name()) + \", omitting.\")\n            if reflection_amount is not None:\n                obj.set_cp(\"05) Reflection Amount\", reflection_amount)\n            if reflection_roughness is not None:\n                obj.set_cp(\"06) Reflection Roughness\", reflection_roughness)\n            if hsv is not None:\n                obj.set_cp(\"02) Saturation\", hsv[1])\n                obj.set_cp(\"03) Hue\", hsv[0])\n                obj.set_cp(\"04) Value\", hsv[2])",
  "def load_rocks(path: str, subsec_num: int, objects: Optional[List[str]] = None,\n                   sample_objects: bool = False, amount: int = None) -> List[MeshObject]:\n        \"\"\" Loads rocks from the given blend file.\n\n        :param path: Path to a .blend file containing desired rock/cliff objects in //Object// section.\n        :param subsec_num: Number of a corresponding cell (batch) in `rocks` list in configuration.\n                           Used for name generation.\n        :param objects: List of rock-/cliff-object names to be loaded. If not specified then `amount` property\n                        is used for consequential loading.\n        :param sample_objects: Toggles the uniform sampling of objects to load. Takes into account `objects` and\n                               `amount` parameters. Requires 'amount' param to be defined.\n        :param amount: Amount of rock-/cliff-object to load. If not specified, the amount will be set to\n                       the amount of suitable objects in the current section of a blend file. Must be bigger than 0.\n        :return: List of loaded objects.\n        \"\"\"\n        loaded_objects = []\n        obj_types = [\"Rock\", \"Cliff\"]\n        amount_defined = False\n        if objects is None:\n            objects = []\n\n        obj_list = []\n        with bpy.data.libraries.load(path) as (data_from, _):\n            # if list of names is empty\n            if not objects:\n                # get list of rocks suitable for loading - objects that are rocks or cliffs\n                for obj_type in obj_types:\n                    obj_list += [obj for obj in data_from.objects if obj_type in obj]\n            else:\n                # if names are defined - get those that are available in this .blend file\n                obj_list = [obj for obj in data_from.objects if obj in objects]\n\n        # get amount of rocks in this batch, set to all suitable if not defined\n        if amount is not None:\n            amount_defined = True\n            if amount == 0:\n                raise RuntimeError(\"Amount param can't be equal to zero!\")\n        else:\n            amount = len(obj_list)\n\n        for i in range(amount):\n            # load rock: choose random from the list if sampling is True, go through list if not\n            if sample_objects and amount_defined:\n                obj = choice(obj_list)\n            else:\n                obj = obj_list[i % len(obj_list)]\n            bpy.ops.wm.append(filepath=os.path.join(path, \"Object\", obj), filename=obj,\n                              directory=os.path.join(path, \"Object\"))\n            loaded_obj = MeshObject(bpy.context.scene.objects[obj])\n            # set custom name for easier tracking in the scene\n            loaded_obj.set_name(obj + \"_\" + str(subsec_num) + \"_\" + str(i))\n            # append to return list\n            loaded_objects.append(loaded_obj)\n\n        return loaded_objects",
  "def set_rocks_properties(objects: List[MeshObject], physics: bool = False, render_levels: int = 3,\n                             high_detail_mode: bool = False, scale: Optional[Union[Vector, np.ndarray, list]] = None,\n                             reflection_amount: Optional[float] = None, reflection_roughness: Optional[float] = None,\n                             hsv: Optional[List[float]] = None):\n        \"\"\" Sets rocks properties in accordance to the given parameters.\n\n        :param objects: List of loaded rock mesh objects.\n        :param physics: Custom property for physics/rigidbody state.\n        :param render_levels: Number of subdivisions to perform when rendering.\n        :param high_detail_mode: Flag for enabling HDM when possible.\n        :param scale: Scale of a rock as a 3d-vector with each value as a scaling factor per according dimension.\n        :param reflection_amount: Reflection texture value. Default: rock-specific. Range: [0,1]\n        :param reflection_roughness: Roughness texture value. Default: rock-specific. Range: [0,1]\n        :param hsv: Hue-Saturation-Value parameters of the HSV node. (3 values).\n                    Range: H: [0, 1], S: [0, 2], V: [0, 2]. Default: rock-specific.\n        \"\"\"\n\n        if scale is None:\n            scale = [1.0, 1.0, 1.0]\n\n        for obj in objects:\n            # set physics parameter\n            obj.set_cp(\"physics\", physics)\n            # set category id\n            obj.set_cp(\"category_id\", 1)\n            # set render value\n            obj.blender_obj.modifiers[\"Subsurf\"].render_levels = render_levels\n            # set scale\n            obj.set_scale(scale)\n            # set HDM if enabled\n            if obj.has_cp(\"01) High Detail Mode\"):\n                obj.set_cp(\"01) High Detail Mode\", high_detail_mode)\n            else:\n                print(\"High Detail Mode is unavailable for \" + str(obj.get_name()) + \", omitting.\")\n            if reflection_amount is not None:\n                obj.set_cp(\"05) Reflection Amount\", reflection_amount)\n            if reflection_roughness is not None:\n                obj.set_cp(\"06) Reflection Roughness\", reflection_roughness)\n            if hsv is not None:\n                obj.set_cp(\"02) Saturation\", hsv[1])\n                obj.set_cp(\"03) Hue\", hsv[0])\n                obj.set_cp(\"04) Value\", hsv[2])",
  "def load_bop_objs(bop_dataset_path: str, model_type: str = \"\", obj_ids: Optional[List[int]] = None,\n                  sample_objects: bool = False, num_of_objs_to_sample: Optional[int] = None,\n                  obj_instances_limit: int = -1, mm2m: Optional[bool] = None, object_model_unit: str = 'm',\n                  move_origin_to_x_y_plane: bool = False) -> List[MeshObject]:\n    \"\"\" Loads all or a subset of 3D models of any BOP dataset\n\n    :param bop_dataset_path: Full path to a specific bop dataset e.g. /home/user/bop/tless.\n    :param model_type: Optionally, specify type of BOP model. Available: [reconst, cad or eval].\n    :param obj_ids: List of object ids to load. Default: [] (load all objects from the given BOP dataset)\n    :param sample_objects: Toggles object sampling from the specified dataset.\n    :param num_of_objs_to_sample: Amount of objects to sample from the specified dataset. If this amount is bigger\n                                  than the dataset actually contains, then all objects will be loaded.\n    :param obj_instances_limit: Limits the amount of object copies when sampling. Default: -1 (no limit).\n    :param mm2m: Specify whether to convert poses and models to meters (deprecated).\n    :param object_model_unit: The unit the object model is in. Object model will be scaled to meters. This does not\n                              affect the annotation units. Available: ['m', 'dm', 'cm', 'mm'].\n    :param move_origin_to_x_y_plane: Move center of the object to the lower side of the object, this will not work\n                                     when used in combination with pose estimation tasks! This is designed for the\n                                     use-case where BOP objects are used as filler objects in the background.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n\n    bop_path, bop_dataset_name = _BopLoader.setup_bop_toolkit(bop_dataset_path)\n\n    # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n    # pylint: disable=import-outside-toplevel\n    from bop_toolkit_lib import dataset_params\n\n    # pylint: enable=import-outside-toplevel\n\n    model_p = dataset_params.get_model_params(bop_path, bop_dataset_name, model_type=model_type if model_type else None)\n\n    assert object_model_unit in ['m', 'dm', 'cm', 'mm'], (f\"Invalid object model unit: `{object_model_unit}`. \"\n                                                          f\"Supported are 'm', 'dm', 'cm', 'mm'\")\n    scale = {'m': 1., 'dm': 0.1, 'cm': 0.01, 'mm': 0.001}[object_model_unit]\n    if mm2m is not None:\n        warnings.warn(\"WARNING: `mm2m` is deprecated, please use `object_model_unit='mm'` instead!\")\n        scale = 0.001\n\n    if obj_ids is None:\n        obj_ids = []\n\n    obj_ids = obj_ids if obj_ids else model_p['obj_ids']\n\n    loaded_objects = []\n    # if sampling is enabled\n    if sample_objects:\n        loaded_ids = {}\n        loaded_amount = 0\n        if obj_instances_limit != -1 and len(obj_ids) * obj_instances_limit < num_of_objs_to_sample:\n            raise RuntimeError(f\"{bop_dataset_path}'s contains {len(obj_ids)} objects, {num_of_objs_to_sample} object \"\n                               f\"where requested to sample with an instances limit of {obj_instances_limit}. Raise \"\n                               f\"the limit amount or decrease the requested amount of objects.\")\n        while loaded_amount != num_of_objs_to_sample:\n            random_id = choice(obj_ids)\n            if random_id not in loaded_ids:\n                loaded_ids.update({random_id: 0})\n            # if there is no limit or if there is one, but it is not reached for this particular object\n            if obj_instances_limit == -1 or loaded_ids[random_id] < obj_instances_limit:\n                cur_obj = _BopLoader.load_mesh(random_id, model_p, bop_dataset_name, scale)\n                loaded_ids[random_id] += 1\n                loaded_amount += 1\n                loaded_objects.append(cur_obj)\n            else:\n                print(f\"ID {random_id} was loaded {loaded_ids[random_id]} times with limit of {obj_instances_limit}. \"\n                      f\"Total loaded amount {loaded_amount} while {num_of_objs_to_sample} are being requested\")\n    else:\n        for obj_id in obj_ids:\n            cur_obj = _BopLoader.load_mesh(obj_id, model_p, bop_dataset_name, scale)\n            loaded_objects.append(cur_obj)\n    # move the origin of the object to the world origin and on top of the X-Y plane\n    # makes it easier to place them later on, this does not change the `.location`\n    # This is only useful if the BOP objects are not used in a pose estimation scenario.\n    if move_origin_to_x_y_plane:\n        for obj in loaded_objects:\n            obj.move_origin_to_bottom_mean_point()\n\n    return loaded_objects",
  "def load_bop_scene(bop_dataset_path: str, scene_id: int, model_type: str = \"\", cam_type: str = \"\",\n                   split: str = \"test\", source_frame: Optional[List[str]] = None,\n                   mm2m: Optional[bool] = None, object_model_unit: str = 'm') -> List[MeshObject]:\n    \"\"\" Replicate a BOP scene from the given dataset: load scene objects, object poses, camera intrinsics and\n        extrinsics\n\n    - Interfaces with the bob_toolkit, allows loading of train, val and test splits\n    - Relative camera poses are loaded/computed with respect to a reference model\n    - Sets real camera intrinsics\n\n    :param bop_dataset_path: Full path to a specific bop dataset e.g. /home/user/bop/tless.\n    :param scene_id: Specify BOP dataset scene to synthetically replicate. Default: -1 (no scene is replicated,\n                     only BOP Objects are loaded).\n    :param model_type: Optionally, specify type of BOP model.  Available: [reconst, cad or eval].\n    :param cam_type: Camera type. If not defined, dataset-specific default camera type is used.\n    :param split: Optionally, test or val split depending on BOP dataset.\n    :param source_frame: Can be used if the given positions and rotations are specified in frames different from the\n                         blender frame. Has to be a list of three strings. Example: ['X', '-Z', 'Y']:\n                         Point (1,2,3) will be transformed to (1, -3, 2). Default: [\"X\", \"-Y\", \"-Z\"],\n                         Available: ['X', 'Y', 'Z', '-X', '-Y', '-Z'].\n    :param mm2m: Specify whether to convert poses and models to meters (deprecated).\n    :param object_model_unit: The unit the object model is in. Object model will be scaled to meters. This does not\n                              affect the annotation units. Available: ['m', 'dm', 'cm', 'mm'].\n    :return: The list of loaded mesh objects.\n    \"\"\"\n\n    bop_path, bop_dataset_name = _BopLoader.setup_bop_toolkit(bop_dataset_path)\n\n    # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n    # pylint: disable=import-outside-toplevel\n    from bop_toolkit_lib import dataset_params, inout\n\n    # pylint: enable=import-outside-toplevel\n\n    if source_frame is None:\n        source_frame = [\"X\", \"-Y\", \"-Z\"]\n\n    model_p = dataset_params.get_model_params(bop_path, bop_dataset_name, model_type=model_type if model_type else None)\n    try:\n        split_p = dataset_params.get_split_params(bop_path, bop_dataset_name, split=split,\n                                                  split_type=cam_type if cam_type else None)\n    except ValueError as e:\n        raise RuntimeError(f\"Wrong path or {split} split does not exist in {bop_dataset_path}.\") from e\n    sc_gt = inout.load_scene_gt(split_p['scene_gt_tpath'].format(**{'scene_id': scene_id}))\n    sc_camera = inout.load_json(split_p['scene_camera_tpath'].format(**{'scene_id': scene_id}))\n\n    assert object_model_unit in ['m', 'dm', 'cm', 'mm'], (f\"Invalid object model unit: `{object_model_unit}`. \"\n                                                          f\"Supported are 'm', 'dm', 'cm', 'mm'\")\n    scale = {'m': 1., 'dm': 0.1, 'cm': 0.01, 'mm': 0.001}[object_model_unit]\n    if mm2m is not None:\n        warnings.warn(\"WARNING: `mm2m` is deprecated, please use `object_model_unit='mm'` instead!\")\n        scale = 0.001\n\n    for i, (cam_id, insts) in enumerate(sc_gt.items()):\n        cam_K, cam_H_m2c_ref = _BopLoader.get_ref_cam_extrinsics_intrinsics(sc_camera, cam_id, insts, scale)\n\n        if i == 0:\n            # define world = first camera\n            cam_H_m2w_ref = cam_H_m2c_ref.copy()\n\n            cur_objs = []\n            # load scene objects and set their poses\n            for inst in insts:\n                cur_objs.append(_BopLoader.load_mesh(inst['obj_id'], model_p, bop_dataset_name, scale))\n                _BopLoader.set_object_pose(cur_objs[-1], inst, scale)\n\n        cam_H_c2w = _BopLoader.compute_camera_to_world_trafo(cam_H_m2w_ref, cam_H_m2c_ref, source_frame)\n        # set camera intrinsics\n        CameraUtility.set_intrinsics_from_K_matrix(cam_K, split_p['im_size'][0], split_p['im_size'][1])\n\n        # set camera extrinsics as next frame\n        frame_id = CameraUtility.add_camera_pose(cam_H_c2w)\n\n        # Add key frame for camera shift, as it changes from frame to frame in the tless replication\n        cam = bpy.context.scene.camera.data\n        cam.keyframe_insert(data_path='shift_x', frame=frame_id)\n        cam.keyframe_insert(data_path='shift_y', frame=frame_id)\n\n        # Copy object poses to key frame (to be sure)\n        for cur_obj in cur_objs:\n            _BopLoader.insert_key_frames(cur_obj, frame_id)\n\n    return cur_objs",
  "def load_bop_intrinsics(bop_dataset_path: str, split: str = \"test\", cam_type: str = \"\") -> Tuple[np.ndarray, int, int]:\n    \"\"\"\n    Load and set the camera matrix and image resolution of a specified BOP dataset\n\n    :param bop_dataset_path: Full path to a specific bop dataset e.g. /home/user/bop/tless.\n    :param split: Optionally, train, test or val split depending on BOP dataset, defaults to \"test\"\n    :param cam_type: Camera type. If not defined, dataset-specific default camera type is used.\n    :returns: camera matrix K, W, H\n    \"\"\"\n\n    bop_path, bop_dataset_name = _BopLoader.setup_bop_toolkit(bop_dataset_path)\n\n    # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n    # pylint: disable=import-outside-toplevel\n    from bop_toolkit_lib import dataset_params\n\n    # pylint: enable=import-outside-toplevel\n\n    cam_p = dataset_params.get_camera_params(bop_path, bop_dataset_name, cam_type=cam_type if cam_type else None)\n\n    try:\n        split_p = dataset_params.get_split_params(bop_path, bop_dataset_name, split=split,\n                                                  split_type=cam_type if cam_type else None)\n    except ValueError as e:\n        raise RuntimeError(f\"Wrong path or {split} split does not exist in {bop_dataset_path}.\") from e\n\n    # TLESS exception because images are cropped\n    if bop_dataset_name in ['tless']:\n        cam_p['K'][0, 2] = split_p['im_size'][0] / 2\n        cam_p['K'][1, 2] = split_p['im_size'][1] / 2\n\n    # set camera intrinsics\n    CameraUtility.set_intrinsics_from_K_matrix(cam_p['K'], split_p['im_size'][0], split_p['im_size'][1])\n\n    return cam_p['K'], split_p['im_size'][0], split_p['im_size'][1]",
  "class _BopLoader:\n    CACHED_OBJECTS = {}\n    @staticmethod\n    def setup_bop_toolkit(bop_dataset_path: str) -> Tuple[str, str]:\n        \"\"\"\n        Install the bop_toolkit from Github and set an environment variable pointing to the BOP datasets\n\n        :param bop_dataset_path: Path to the bop dataset\n        :return (bop_path, bop_dataset_name): Path to BOP datasets and BOP dataset name\n        \"\"\"\n\n        bop_dataset_name = os.path.basename(bop_dataset_path)\n        bop_path = os.path.dirname(bop_dataset_path)\n\n        print(f\"bob: {bop_dataset_path}, dataset_path: {bop_path}\")\n        print(f\"dataset: {bop_dataset_name}\")\n\n        if not os.path.exists(bop_path):\n            raise FileNotFoundError(f\"It seems the BOP dataset does not exist under the given path {bop_dataset_path}\")\n\n        # Install bop_toolkit_lib\n        SetupUtility.setup_pip([\"git+https://github.com/thodan/bop_toolkit\"])\n        os.environ[\"BOP_PATH\"] = bop_path\n\n        return bop_path, bop_dataset_name\n\n    @staticmethod\n    def compute_camera_to_world_trafo(cam_H_m2w_ref: np.array, cam_H_m2c_ref: np.array,\n                                      source_frame: List[str]) -> np.ndarray:\n        \"\"\" Returns camera to world transformation in blender coords.\n\n        :param cam_H_m2c_ref: (4x4) Homog trafo from object to camera coords.\n        :param cam_H_m2w_ref: (4x4) Homog trafo from object to world coords.\n        :param source_frame: Can be used if the given positions and rotations are specified in frames different\n                             from the blender frame.\n        :return: cam_H_c2w: (4x4) Homog trafo from camera to world coords.\n        \"\"\"\n\n        cam_H_c2w = np.dot(cam_H_m2w_ref, np.linalg.inv(cam_H_m2c_ref))\n\n        print('-----------------------------')\n        print(f\"Cam: {cam_H_c2w}\")\n        print('-----------------------------')\n\n        # transform from OpenCV to blender coords\n        cam_H_c2w = change_source_coordinate_frame_of_transformation_matrix(cam_H_c2w, source_frame)\n\n        return cam_H_c2w\n\n    @staticmethod\n    def set_object_pose(cur_obj: bpy.types.Object, inst: dict, scale: float):\n        \"\"\" Set object pose for current obj\n\n        :param cur_obj: Current object.\n        :param inst: instance from BOP scene_gt file.\n        :param scale: factor to transform set pose in mm or meters.\n        \"\"\"\n\n        cam_H_m2c = np.eye(4)\n        cam_H_m2c[:3, :3] = np.array(inst['cam_R_m2c']).reshape(3, 3)\n        cam_H_m2c[:3, 3] = np.array(inst['cam_t_m2c']).reshape(3) * scale\n\n        # world = camera @ i=0\n        cam_H_m2w = cam_H_m2c\n\n        print('-----------------------------')\n        print(f\"Cam: {cam_H_m2w}\")\n        print('-----------------------------')\n\n        cur_obj.set_local2world_mat(Matrix(cam_H_m2w))\n        cur_obj.set_scale(Vector((scale, scale, scale)))\n\n    @staticmethod\n    def insert_key_frames(obj: bpy.types.Object, frame_id: int):\n        \"\"\" Insert key frames for given object pose.\n\n        :param obj: Loaded object.\n        :param frame_id: The frame number where key frames should be inserted.\n        \"\"\"\n\n        obj.set_location(obj.get_location(), frame_id)\n        obj.set_rotation_euler(obj.get_rotation_euler(), frame_id)\n\n    @staticmethod\n    def get_ref_cam_extrinsics_intrinsics(sc_camera: dict, cam_id: int, insts: dict,\n                                          scale: float) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\" Get camK and transformation from object instance 0 to camera cam_id as reference.\n\n        :param sc_camera: BOP scene_camera file.\n        :param cam_id: BOP camera id.\n        :param insts: Instance from BOP scene_gt file.\n        :param scale: Factor to transform get pose in mm or meters.\n        :return (camK, cam_H_m2c_ref): loaded camera matrix. Loaded object to camera transformation.\n        \"\"\"\n\n        cam_K = np.array(sc_camera[str(cam_id)]['cam_K']).reshape(3, 3)\n        cam_H_m2c_ref = np.eye(4)\n        cam_H_m2c_ref[:3, :3] = np.array(insts[0]['cam_R_m2c']).reshape(3, 3)\n        cam_H_m2c_ref[:3, 3] = np.array(insts[0]['cam_t_m2c']).reshape(3) * scale\n\n        return cam_K, cam_H_m2c_ref\n\n    @staticmethod\n    def get_loaded_obj(model_path: str) -> Optional[bpy.types.Object]:\n        \"\"\" Returns the object if it has already been loaded.\n\n        :param model_path: Model path of the new object.\n        :return: Object if found, else return None.\n        \"\"\"\n        for loaded_obj in bpy.context.scene.objects:\n            if 'model_path' in loaded_obj and loaded_obj['model_path'] == model_path:\n                return loaded_obj\n        return None\n\n    @staticmethod\n    def load_mesh(obj_id: int, model_p: dict, bop_dataset_name: str, scale: float = 1) -> MeshObject:\n        \"\"\" Loads BOP mesh and sets category_id.\n\n        :param obj_id: The obj_id of the BOP Object.\n        :param model_p: model parameters defined in dataset_params.py in bop_toolkit.\n        :param bop_dataset_name: The name of the used bop dataset.\n        :param scale: factor to transform set pose in mm or meters.\n        :return: Loaded mesh object.\n        \"\"\"\n\n        model_path = model_p[\"model_tpath\"].format(**{\"obj_id\": obj_id})\n\n        # if the object was not previously loaded - load it, if duplication is allowed - duplicate it\n        duplicated = model_path in _BopLoader.CACHED_OBJECTS\n        objs = load_obj(model_path, cached_objects=_BopLoader.CACHED_OBJECTS)\n        assert (\n            len(objs) == 1\n        ), f\"Loading object from '{model_path}' returned more than one mesh\"\n        cur_obj = objs[0]\n\n        if duplicated:\n            # See issue https://github.com/DLR-RM/BlenderProc/issues/590\n            for i, material in enumerate(cur_obj.get_materials()):\n                material_dup = material.duplicate()\n                cur_obj.set_material(i, material_dup)\n\n        # Change Material name to be backward compatible\n        cur_obj.get_materials()[-1].set_name(\"bop_\" + bop_dataset_name + \"_vertex_col_material\")\n        cur_obj.set_scale(Vector((scale, scale, scale)))\n        cur_obj.set_cp(\"category_id\", obj_id)\n        cur_obj.set_cp(\"model_path\", model_path)\n        cur_obj.set_cp(\"is_bop_object\", True)\n        cur_obj.set_cp(\"bop_dataset_name\", bop_dataset_name)\n\n        return cur_obj",
  "def setup_bop_toolkit(bop_dataset_path: str) -> Tuple[str, str]:\n        \"\"\"\n        Install the bop_toolkit from Github and set an environment variable pointing to the BOP datasets\n\n        :param bop_dataset_path: Path to the bop dataset\n        :return (bop_path, bop_dataset_name): Path to BOP datasets and BOP dataset name\n        \"\"\"\n\n        bop_dataset_name = os.path.basename(bop_dataset_path)\n        bop_path = os.path.dirname(bop_dataset_path)\n\n        print(f\"bob: {bop_dataset_path}, dataset_path: {bop_path}\")\n        print(f\"dataset: {bop_dataset_name}\")\n\n        if not os.path.exists(bop_path):\n            raise FileNotFoundError(f\"It seems the BOP dataset does not exist under the given path {bop_dataset_path}\")\n\n        # Install bop_toolkit_lib\n        SetupUtility.setup_pip([\"git+https://github.com/thodan/bop_toolkit\"])\n        os.environ[\"BOP_PATH\"] = bop_path\n\n        return bop_path, bop_dataset_name",
  "def compute_camera_to_world_trafo(cam_H_m2w_ref: np.array, cam_H_m2c_ref: np.array,\n                                      source_frame: List[str]) -> np.ndarray:\n        \"\"\" Returns camera to world transformation in blender coords.\n\n        :param cam_H_m2c_ref: (4x4) Homog trafo from object to camera coords.\n        :param cam_H_m2w_ref: (4x4) Homog trafo from object to world coords.\n        :param source_frame: Can be used if the given positions and rotations are specified in frames different\n                             from the blender frame.\n        :return: cam_H_c2w: (4x4) Homog trafo from camera to world coords.\n        \"\"\"\n\n        cam_H_c2w = np.dot(cam_H_m2w_ref, np.linalg.inv(cam_H_m2c_ref))\n\n        print('-----------------------------')\n        print(f\"Cam: {cam_H_c2w}\")\n        print('-----------------------------')\n\n        # transform from OpenCV to blender coords\n        cam_H_c2w = change_source_coordinate_frame_of_transformation_matrix(cam_H_c2w, source_frame)\n\n        return cam_H_c2w",
  "def set_object_pose(cur_obj: bpy.types.Object, inst: dict, scale: float):\n        \"\"\" Set object pose for current obj\n\n        :param cur_obj: Current object.\n        :param inst: instance from BOP scene_gt file.\n        :param scale: factor to transform set pose in mm or meters.\n        \"\"\"\n\n        cam_H_m2c = np.eye(4)\n        cam_H_m2c[:3, :3] = np.array(inst['cam_R_m2c']).reshape(3, 3)\n        cam_H_m2c[:3, 3] = np.array(inst['cam_t_m2c']).reshape(3) * scale\n\n        # world = camera @ i=0\n        cam_H_m2w = cam_H_m2c\n\n        print('-----------------------------')\n        print(f\"Cam: {cam_H_m2w}\")\n        print('-----------------------------')\n\n        cur_obj.set_local2world_mat(Matrix(cam_H_m2w))\n        cur_obj.set_scale(Vector((scale, scale, scale)))",
  "def insert_key_frames(obj: bpy.types.Object, frame_id: int):\n        \"\"\" Insert key frames for given object pose.\n\n        :param obj: Loaded object.\n        :param frame_id: The frame number where key frames should be inserted.\n        \"\"\"\n\n        obj.set_location(obj.get_location(), frame_id)\n        obj.set_rotation_euler(obj.get_rotation_euler(), frame_id)",
  "def get_ref_cam_extrinsics_intrinsics(sc_camera: dict, cam_id: int, insts: dict,\n                                          scale: float) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\" Get camK and transformation from object instance 0 to camera cam_id as reference.\n\n        :param sc_camera: BOP scene_camera file.\n        :param cam_id: BOP camera id.\n        :param insts: Instance from BOP scene_gt file.\n        :param scale: Factor to transform get pose in mm or meters.\n        :return (camK, cam_H_m2c_ref): loaded camera matrix. Loaded object to camera transformation.\n        \"\"\"\n\n        cam_K = np.array(sc_camera[str(cam_id)]['cam_K']).reshape(3, 3)\n        cam_H_m2c_ref = np.eye(4)\n        cam_H_m2c_ref[:3, :3] = np.array(insts[0]['cam_R_m2c']).reshape(3, 3)\n        cam_H_m2c_ref[:3, 3] = np.array(insts[0]['cam_t_m2c']).reshape(3) * scale\n\n        return cam_K, cam_H_m2c_ref",
  "def get_loaded_obj(model_path: str) -> Optional[bpy.types.Object]:\n        \"\"\" Returns the object if it has already been loaded.\n\n        :param model_path: Model path of the new object.\n        :return: Object if found, else return None.\n        \"\"\"\n        for loaded_obj in bpy.context.scene.objects:\n            if 'model_path' in loaded_obj and loaded_obj['model_path'] == model_path:\n                return loaded_obj\n        return None",
  "def load_mesh(obj_id: int, model_p: dict, bop_dataset_name: str, scale: float = 1) -> MeshObject:\n        \"\"\" Loads BOP mesh and sets category_id.\n\n        :param obj_id: The obj_id of the BOP Object.\n        :param model_p: model parameters defined in dataset_params.py in bop_toolkit.\n        :param bop_dataset_name: The name of the used bop dataset.\n        :param scale: factor to transform set pose in mm or meters.\n        :return: Loaded mesh object.\n        \"\"\"\n\n        model_path = model_p[\"model_tpath\"].format(**{\"obj_id\": obj_id})\n\n        # if the object was not previously loaded - load it, if duplication is allowed - duplicate it\n        duplicated = model_path in _BopLoader.CACHED_OBJECTS\n        objs = load_obj(model_path, cached_objects=_BopLoader.CACHED_OBJECTS)\n        assert (\n            len(objs) == 1\n        ), f\"Loading object from '{model_path}' returned more than one mesh\"\n        cur_obj = objs[0]\n\n        if duplicated:\n            # See issue https://github.com/DLR-RM/BlenderProc/issues/590\n            for i, material in enumerate(cur_obj.get_materials()):\n                material_dup = material.duplicate()\n                cur_obj.set_material(i, material_dup)\n\n        # Change Material name to be backward compatible\n        cur_obj.get_materials()[-1].set_name(\"bop_\" + bop_dataset_name + \"_vertex_col_material\")\n        cur_obj.set_scale(Vector((scale, scale, scale)))\n        cur_obj.set_cp(\"category_id\", obj_id)\n        cur_obj.set_cp(\"model_path\", model_path)\n        cur_obj.set_cp(\"is_bop_object\", True)\n        cur_obj.set_cp(\"bop_dataset_name\", bop_dataset_name)\n\n        return cur_obj",
  "def load_front3d(json_path: str, future_model_path: str, front_3D_texture_path: str, label_mapping: LabelIdMapping,\n                 ceiling_light_strength: float = 0.8, lamp_light_strength: float = 7.0) -> List[MeshObject]:\n    \"\"\" Loads the 3D-Front scene specified by the given json file.\n\n    :param json_path: Path to the json file, where the house information is stored.\n    :param future_model_path: Path to the models used in the 3D-Front dataset.\n    :param front_3D_texture_path: Path to the 3D-FRONT-texture folder.\n    :param label_mapping: A dict which maps the names of the objects to ids.\n    :param ceiling_light_strength: Strength of the emission shader used in the ceiling.\n    :param lamp_light_strength: Strength of the emission shader used in each lamp.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    json_path = resolve_path(json_path)\n    future_model_path = resolve_path(future_model_path)\n    front_3D_texture_path = resolve_path(front_3D_texture_path)\n\n    if not os.path.exists(json_path):\n        raise FileNotFoundError(f\"The given path does not exists: {json_path}\")\n    if not json_path.endswith(\".json\"):\n        raise FileNotFoundError(f\"The given path does not point to a .json file: {json_path}\")\n    if not os.path.exists(future_model_path):\n        raise FileNotFoundError(f\"The 3D future model path does not exist: {future_model_path}\")\n\n    # load data from json file\n    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n        data = json.load(json_file)\n\n    if \"scene\" not in data:\n        raise ValueError(f\"There is no scene data in this json file: {json_path}\")\n\n    created_objects = _Front3DLoader.create_mesh_objects_from_file(data, front_3D_texture_path,\n                                                                   ceiling_light_strength, label_mapping, json_path)\n\n    all_loaded_furniture = _Front3DLoader.load_furniture_objs(data, future_model_path,\n                                                              lamp_light_strength, label_mapping)\n\n    created_objects += _Front3DLoader.move_and_duplicate_furniture(data, all_loaded_furniture)\n\n    # add an identifier to the obj\n    for obj in created_objects:\n        obj.set_cp(\"is_3d_front\", True)\n\n    return created_objects",
  "class _Front3DLoader:\n    \"\"\" Loads the 3D-Front dataset.\n\n    https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset\n\n    Each object gets the name based on the category/type, on top of that you can use a mapping specified in the\n    resources/front_3D folder.\n\n    The dataset already supports semantic segmentation with either the 3D-Front classes or the nyu classes.\n    As we have created this mapping ourselves it might be faulty.\n\n    The Front3DLoader creates automatically lights in the scene, by adding emission shaders to the ceiling and lamps.\n    \"\"\"\n\n    @staticmethod\n    def extract_hash_nr_for_texture(given_url: str, front_3D_texture_path: str) -> str:\n        \"\"\"\n        Constructs the path of the hash folder and checks if the texture is available if not it is downloaded\n\n        :param given_url: The url of the texture\n        :param front_3D_texture_path: The path to where the texture are saved\n        :return: The hash id, which is used in the url\n        \"\"\"\n        # extract the hash nr from the given url\n        hash_nr = given_url.split(\"/\")[-2]\n        hash_folder = os.path.join(front_3D_texture_path, hash_nr)\n        if not os.path.exists(hash_folder):\n            # download the file\n            os.makedirs(hash_folder)\n            warnings.warn(f\"This texture: {hash_nr} could not be found it will be downloaded.\")\n            # replace https with http as ssl connection out of blender are difficult\n            urlretrieve(given_url.replace(\"https://\", \"http://\"), os.path.join(hash_folder, \"texture.png\"))\n            if not os.path.exists(os.path.join(hash_folder, \"texture.png\")):\n                raise Exception(f\"The texture could not be found, the following url was used: \"\n                                f\"{front_3D_texture_path}, this is the extracted hash: {hash_nr}, \"\n                                f\"given url: {given_url}\")\n        return hash_folder\n\n    @staticmethod\n    def get_used_image(hash_folder_path: str, saved_image_dict: Mapping[str, bpy.types.Texture]) -> bpy.types.Texture:\n        \"\"\"\n        Returns a texture object for the given hash_folder_path, the textures are stored in the saved_image_dict,\n        to avoid that texture are loaded multiple times\n\n        :param hash_folder_path: Path to the hash folder\n        :param saved_image_dict: Dict which maps the hash_folder_paths to bpy.types.Texture\n        :return: The loaded texture bpy.types.Texture\n        \"\"\"\n        if hash_folder_path in saved_image_dict:\n            ret_used_image = saved_image_dict[hash_folder_path]\n        else:\n            textures = load_texture(hash_folder_path)\n            if len(textures) != 1:\n                raise Exception(f\"There is not just one texture: {len(textures)}\")\n            ret_used_image = textures[0].image\n            saved_image_dict[hash_folder_path] = ret_used_image\n        return ret_used_image\n\n    @staticmethod\n    def create_mesh_objects_from_file(data: dict, front_3D_texture_path: str, ceiling_light_strength: float,\n                                      label_mapping: LabelIdMapping, json_path: str) -> List[MeshObject]:\n        \"\"\"\n        This creates for a given data json block all defined meshes and assigns the correct materials.\n        This means that the json file contains some mesh, like walls and floors, which have to built up manually.\n\n        It also already adds the lighting for the ceiling\n\n        :param data: json data dir. Must contain \"material\" and \"mesh\"\n        :param front_3D_texture_path: Path to the 3D-FRONT-texture folder.\n        :param ceiling_light_strength: Strength of the emission shader used in the ceiling.\n        :param label_mapping: A dict which maps the names of the objects to ids.\n        :param json_path: Path to the json file, where the house information is stored.\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # extract all used materials -> there are more materials defined than used\n        used_materials = []\n        for mat in data[\"material\"]:\n            used_materials.append({\"uid\": mat[\"uid\"], \"texture\": mat[\"texture\"],\n                                   \"normaltexture\": mat[\"normaltexture\"], \"color\": mat[\"color\"]})\n\n        created_objects = []\n        # maps loaded images from image file path to bpy.type.image\n        saved_images = {}\n        saved_normal_images = {}\n        # materials based on colors to avoid recreating the same material over and over\n        used_materials_based_on_color = {}\n        # materials based on texture to avoid recreating the same material over and over\n        used_materials_based_on_texture = {}\n        for mesh_data in data[\"mesh\"]:\n            # extract the obj name, which also is used as the category_id name\n            used_obj_name = mesh_data[\"type\"].strip()\n            if used_obj_name == \"\":\n                used_obj_name = \"void\"\n            if \"material\" not in mesh_data:\n                warnings.warn(f\"Material is not defined for {used_obj_name} in this file: {json_path}\")\n                continue\n            # create a new mesh\n            obj = create_with_empty_mesh(used_obj_name, used_obj_name + \"_mesh\")\n            created_objects.append(obj)\n\n            # set two custom properties, first that it is a 3D_future object and second the category_id\n            obj.set_cp(\"is_3D_future\", True)\n            obj.set_cp(\"category_id\", label_mapping.id_from_label(used_obj_name.lower()))\n\n            # get the material uid of the current mesh data\n            current_mat = mesh_data[\"material\"]\n            used_mat = None\n            # search in the used materials after this uid\n            for u_mat in used_materials:\n                if u_mat[\"uid\"] == current_mat:\n                    used_mat = u_mat\n                    break\n            # If there should be a material used\n            if used_mat:\n                if used_mat[\"texture\"]:\n                    # extract the has folder is from the url and download it if necessary\n                    hash_folder = _Front3DLoader.extract_hash_nr_for_texture(used_mat[\"texture\"], front_3D_texture_path)\n                    if hash_folder in used_materials_based_on_texture and \"ceiling\" not in used_obj_name.lower():\n                        mat = used_materials_based_on_texture[hash_folder]\n                        obj.add_material(mat)\n                    else:\n                        # Create a new material\n                        mat = MaterialLoaderUtility.create(name=used_obj_name + \"_material\")\n                        principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n                        if used_mat[\"color\"]:\n                            principled_node.inputs[\"Base Color\"].default_value = mathutils.Vector(\n                                used_mat[\"color\"]) / 255.0\n\n                        used_image = _Front3DLoader.get_used_image(hash_folder, saved_images)\n                        mat.set_principled_shader_value(\"Base Color\", used_image)\n\n                        if \"ceiling\" in used_obj_name.lower():\n                            mat.make_emissive(ceiling_light_strength,\n                                              emission_color=mathutils.Vector(used_mat[\"color\"]) / 255.0)\n\n                        if used_mat[\"normaltexture\"]:\n                            # get the used image based on the normal texture path\n                            # extract the has folder is from the url and download it if necessary\n                            hash_folder = _Front3DLoader.extract_hash_nr_for_texture(used_mat[\"normaltexture\"],\n                                                                                     front_3D_texture_path)\n                            used_image = _Front3DLoader.get_used_image(hash_folder, saved_normal_images)\n\n                            # create normal texture\n                            normal_texture = MaterialLoaderUtility.create_image_node(mat.nodes, used_image, True)\n                            normal_map = mat.nodes.new(\"ShaderNodeNormalMap\")\n                            normal_map.inputs[\"Strength\"].default_value = 1.0\n                            mat.links.new(normal_texture.outputs[\"Color\"], normal_map.inputs[\"Color\"])\n                            # connect normal texture to principled shader\n                            mat.set_principled_shader_value(\"Normal\", normal_map.outputs[\"Normal\"])\n\n                        obj.add_material(mat)\n                        used_materials_based_on_texture[hash_folder] = mat\n                # if there is a normal color used\n                elif used_mat[\"color\"]:\n                    used_hash = tuple(used_mat[\"color\"])\n                    if used_hash in used_materials_based_on_color and \"ceiling\" not in used_obj_name.lower():\n                        mat = used_materials_based_on_color[used_hash]\n                    else:\n                        # Create a new material\n                        mat = MaterialLoaderUtility.create(name=used_obj_name + \"_material\")\n                        # create a principled node and set the default color\n                        principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n                        principled_node.inputs[\"Base Color\"].default_value = mathutils.Vector(used_mat[\"color\"]) / 255.0\n                        # if the object is a ceiling add some light output\n                        if \"ceiling\" in used_obj_name.lower():\n                            mat.make_emissive(ceiling_light_strength,\n                                              emission_color=mathutils.Vector(used_mat[\"color\"]) / 255.0)\n                        else:\n                            used_materials_based_on_color[used_hash] = mat\n\n                    # as this material was just created the material is just append it to the empty list\n                    obj.add_material(mat)\n\n            # extract the vertices from the mesh_data\n            vert = [float(ele) for ele in mesh_data[\"xyz\"]]\n            # extract the faces from the mesh_data\n            faces = mesh_data[\"faces\"]\n            # extract the normals from the mesh_data\n            normal = [float(ele) for ele in mesh_data[\"normal\"]]\n\n            # map those to the blender coordinate system\n            num_vertices = int(len(vert) / 3)\n            vertices = np.reshape(np.array(vert), [num_vertices, 3])\n            normal = np.reshape(np.array(normal), [num_vertices, 3])\n            # flip the first and second value\n            vertices[:, 1], vertices[:, 2] = vertices[:, 2], vertices[:, 1].copy()\n            normal[:, 1], normal[:, 2] = normal[:, 2], normal[:, 1].copy()\n            # reshape back to a long list\n            vertices = np.reshape(vertices, [num_vertices * 3])\n            normal = np.reshape(normal, [num_vertices * 3])\n\n            # add this new data to the mesh object\n            mesh = obj.get_mesh()\n            mesh.vertices.add(num_vertices)\n            mesh.vertices.foreach_set(\"co\", vertices)\n            mesh.vertices.foreach_set(\"normal\", normal)\n\n            # link the faces as vertex indices\n            num_vertex_indicies = len(faces)\n            mesh.loops.add(num_vertex_indicies)\n            mesh.loops.foreach_set(\"vertex_index\", faces)\n\n            # the loops are set based on how the faces are a ranged\n            num_loops = int(num_vertex_indicies / 3)\n            mesh.polygons.add(num_loops)\n            # always 3 vertices form one triangle\n            loop_start = np.arange(0, num_vertex_indicies, 3)\n            # the total size of each triangle is therefore 3\n            loop_total = [3] * num_loops\n            mesh.polygons.foreach_set(\"loop_start\", loop_start)\n            mesh.polygons.foreach_set(\"loop_total\", loop_total)\n\n            # the uv coordinates are reshaped then the face coords are extracted\n            uv_mesh_data = [float(ele) for ele in mesh_data[\"uv\"] if ele is not None]\n            # bb1737bf-dae6-4215-bccf-fab6f584046b.json includes one mesh which only has no UV mapping\n            if uv_mesh_data:\n                uv = np.reshape(np.array(uv_mesh_data), [num_vertices, 2])\n                used_uvs = uv[faces, :]\n                # and again reshaped back to the long list\n                used_uvs = np.reshape(used_uvs, [2 * num_vertex_indicies])\n\n                mesh.uv_layers.new(name=\"new_uv_layer\")\n                mesh.uv_layers[-1].data.foreach_set(\"uv\", used_uvs)\n            else:\n                warnings.warn(f\"This mesh {obj.get_name()} does not have a specified uv map!\")\n\n            # this update converts the upper data into a mesh\n            mesh.update()\n\n            # the generation might fail if the data does not line up\n            # this is not used as even if the data does not line up it is still able to render the objects\n            # We assume that not all meshes in the dataset do conform with the mesh standards set in blender\n            # result = mesh.validate(verbose=False)\n            # if result:\n            #    raise Exception(\"The generation of the mesh: {} failed!\".format(used_obj_name))\n\n        return created_objects\n\n    @staticmethod\n    def load_furniture_objs(data: dict, future_model_path: str, lamp_light_strength: float,\n                            label_mapping: LabelIdMapping) -> List[MeshObject]:\n        \"\"\"\n        Load all furniture objects specified in the json file, these objects are stored as \"raw_model.obj\" in the\n        3D_future_model_path. For lamp the lamp_light_strength value can be changed via the config.\n\n        :param data: json data dir. Should contain \"furniture\"\n        :param future_model_path: Path to the models used in the 3D-Front dataset.\n        :param lamp_light_strength: Strength of the emission shader used in each lamp.\n        :param label_mapping: A dict which maps the names of the objects to ids.\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # collect all loaded furniture objects\n        all_objs = []\n        # for each furniture element\n        for ele in data[\"furniture\"]:\n            # create the paths based on the \"jid\"\n            folder_path = os.path.join(future_model_path, ele[\"jid\"])\n            obj_file = os.path.join(folder_path, \"raw_model.obj\")\n            # if the object exists load it -> a lot of object do not exist\n            # we are unsure why this is -> we assume that not all objects have been made public\n            if os.path.exists(obj_file) and not \"7e101ef3-7722-4af8-90d5-7c562834fabd\" in obj_file:\n                # load all objects from this .obj file\n                objs = load_obj(filepath=obj_file)\n                # extract the name, which serves as category id\n                used_obj_name = \"\"\n                if \"category\" in ele:\n                    used_obj_name = ele[\"category\"]\n                elif \"title\" in ele:\n                    used_obj_name = ele[\"title\"]\n                    if \"/\" in used_obj_name:\n                        used_obj_name = used_obj_name.split(\"/\")[0]\n                if used_obj_name == \"\":\n                    used_obj_name = \"others\"\n                for obj in objs:\n                    obj.set_name(used_obj_name)\n                    # add some custom properties\n                    obj.set_cp(\"uid\", ele[\"uid\"])\n                    # this custom property determines if the object was used before\n                    # is needed to only clone the second appearance of this object\n                    obj.set_cp(\"is_used\", False)\n                    obj.set_cp(\"is_3D_future\", True)\n                    obj.set_cp(\"3D_future_type\", \"Non-Object\")  # is an non object used for the interesting score\n                    # set the category id based on the used obj name\n                    obj.set_cp(\"category_id\", label_mapping.id_from_label(used_obj_name.lower()))\n                    # walk over all materials\n                    for mat in obj.get_materials():\n                        if mat is None:\n                            continue\n                        principled_node = mat.get_nodes_with_type(\"BsdfPrincipled\")\n                        if \"bed\" in used_obj_name.lower() or \"sofa\" in used_obj_name.lower():\n                            if len(principled_node) == 1:\n                                principled_node[0].inputs[\"Roughness\"].default_value = 0.5\n                        is_lamp = \"lamp\" in used_obj_name.lower()\n                        if len(principled_node) == 0 and is_lamp:\n                            # this material has already been transformed\n                            continue\n                        if len(principled_node) == 1:\n                            principled_node = principled_node[0]\n                        else:\n                            raise ValueError(f\"The amount of principle nodes can not be more than 1, \"\n                                             f\"for obj: {obj.get_name()}!\")\n\n                        # Front3d .mtl files contain emission color which make the object mistakenly emissive\n                        # => Reset the emission color\n                        principled_node.inputs[\"Emission\"].default_value[:3] = [0, 0, 0]\n\n                        # For each a texture node\n                        image_node = mat.new_node('ShaderNodeTexImage')\n                        # and load the texture.png\n                        base_image_path = os.path.join(folder_path, \"texture.png\")\n                        image_node.image = bpy.data.images.load(base_image_path, check_existing=True)\n                        mat.link(image_node.outputs['Color'], principled_node.inputs['Base Color'])\n                        # if the object is a lamp, do the same as for the ceiling and add an emission shader\n                        if is_lamp:\n                            mat.make_emissive(lamp_light_strength)\n\n                all_objs.extend(objs)\n            elif \"7e101ef3-7722-4af8-90d5-7c562834fabd\" in obj_file:\n                warnings.warn(f\"This file {obj_file} was skipped as it can not be read by blender.\")\n        return all_objs\n\n    @staticmethod\n    def move_and_duplicate_furniture(data: dict, all_loaded_furniture: list) -> List[MeshObject]:\n        \"\"\"\n        Move and duplicate the furniture depending on the data in the data json dir.\n        After loading each object gets a location based on the data in the json file. Some objects are used more than\n        once these are duplicated and then placed.\n\n        :param data: json data dir. Should contain \"scene\", which should contain \"room\"\n        :param all_loaded_furniture: all objects which have been loaded in load_furniture_objs\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # this rotation matrix rotates the given quaternion into the blender coordinate system\n        blender_rot_mat = mathutils.Matrix.Rotation(radians(-90), 4, 'X')\n        created_objects = []\n        # for each room\n        for room_id, room in enumerate(data[\"scene\"][\"room\"]):\n            # for each object in that room\n            for child in room[\"children\"]:\n                if \"furniture\" in child[\"instanceid\"]:\n                    # find the object where the uid matches the child ref id\n                    for obj in all_loaded_furniture:\n                        if obj.get_cp(\"uid\") == child[\"ref\"]:\n                            # if the object was used before, duplicate the object and move that duplicated obj\n                            if obj.get_cp(\"is_used\"):\n                                new_obj = obj.duplicate()\n                            else:\n                                # if it is the first time use the object directly\n                                new_obj = obj\n                            created_objects.append(new_obj)\n                            new_obj.set_cp(\"is_used\", True)\n                            new_obj.set_cp(\"room_id\", room_id)\n                            new_obj.set_cp(\"3D_future_type\", \"Object\")  # is an object used for the interesting score\n                            new_obj.set_cp(\"coarse_grained_class\", new_obj.get_cp(\"category_id\"))\n                            # this flips the y and z coordinate to bring it to the blender coordinate system\n                            new_obj.set_location(mathutils.Vector(child[\"pos\"]).xzy)\n                            new_obj.set_scale(child[\"scale\"])\n                            # extract the quaternion and convert it to a rotation matrix\n                            rotation_mat = mathutils.Quaternion(child[\"rot\"]).to_euler().to_matrix().to_4x4()\n                            # transform it into the blender coordinate system and then to an euler\n                            new_obj.set_rotation_euler((blender_rot_mat @ rotation_mat).to_euler())\n        return created_objects",
  "def extract_hash_nr_for_texture(given_url: str, front_3D_texture_path: str) -> str:\n        \"\"\"\n        Constructs the path of the hash folder and checks if the texture is available if not it is downloaded\n\n        :param given_url: The url of the texture\n        :param front_3D_texture_path: The path to where the texture are saved\n        :return: The hash id, which is used in the url\n        \"\"\"\n        # extract the hash nr from the given url\n        hash_nr = given_url.split(\"/\")[-2]\n        hash_folder = os.path.join(front_3D_texture_path, hash_nr)\n        if not os.path.exists(hash_folder):\n            # download the file\n            os.makedirs(hash_folder)\n            warnings.warn(f\"This texture: {hash_nr} could not be found it will be downloaded.\")\n            # replace https with http as ssl connection out of blender are difficult\n            urlretrieve(given_url.replace(\"https://\", \"http://\"), os.path.join(hash_folder, \"texture.png\"))\n            if not os.path.exists(os.path.join(hash_folder, \"texture.png\")):\n                raise Exception(f\"The texture could not be found, the following url was used: \"\n                                f\"{front_3D_texture_path}, this is the extracted hash: {hash_nr}, \"\n                                f\"given url: {given_url}\")\n        return hash_folder",
  "def get_used_image(hash_folder_path: str, saved_image_dict: Mapping[str, bpy.types.Texture]) -> bpy.types.Texture:\n        \"\"\"\n        Returns a texture object for the given hash_folder_path, the textures are stored in the saved_image_dict,\n        to avoid that texture are loaded multiple times\n\n        :param hash_folder_path: Path to the hash folder\n        :param saved_image_dict: Dict which maps the hash_folder_paths to bpy.types.Texture\n        :return: The loaded texture bpy.types.Texture\n        \"\"\"\n        if hash_folder_path in saved_image_dict:\n            ret_used_image = saved_image_dict[hash_folder_path]\n        else:\n            textures = load_texture(hash_folder_path)\n            if len(textures) != 1:\n                raise Exception(f\"There is not just one texture: {len(textures)}\")\n            ret_used_image = textures[0].image\n            saved_image_dict[hash_folder_path] = ret_used_image\n        return ret_used_image",
  "def create_mesh_objects_from_file(data: dict, front_3D_texture_path: str, ceiling_light_strength: float,\n                                      label_mapping: LabelIdMapping, json_path: str) -> List[MeshObject]:\n        \"\"\"\n        This creates for a given data json block all defined meshes and assigns the correct materials.\n        This means that the json file contains some mesh, like walls and floors, which have to built up manually.\n\n        It also already adds the lighting for the ceiling\n\n        :param data: json data dir. Must contain \"material\" and \"mesh\"\n        :param front_3D_texture_path: Path to the 3D-FRONT-texture folder.\n        :param ceiling_light_strength: Strength of the emission shader used in the ceiling.\n        :param label_mapping: A dict which maps the names of the objects to ids.\n        :param json_path: Path to the json file, where the house information is stored.\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # extract all used materials -> there are more materials defined than used\n        used_materials = []\n        for mat in data[\"material\"]:\n            used_materials.append({\"uid\": mat[\"uid\"], \"texture\": mat[\"texture\"],\n                                   \"normaltexture\": mat[\"normaltexture\"], \"color\": mat[\"color\"]})\n\n        created_objects = []\n        # maps loaded images from image file path to bpy.type.image\n        saved_images = {}\n        saved_normal_images = {}\n        # materials based on colors to avoid recreating the same material over and over\n        used_materials_based_on_color = {}\n        # materials based on texture to avoid recreating the same material over and over\n        used_materials_based_on_texture = {}\n        for mesh_data in data[\"mesh\"]:\n            # extract the obj name, which also is used as the category_id name\n            used_obj_name = mesh_data[\"type\"].strip()\n            if used_obj_name == \"\":\n                used_obj_name = \"void\"\n            if \"material\" not in mesh_data:\n                warnings.warn(f\"Material is not defined for {used_obj_name} in this file: {json_path}\")\n                continue\n            # create a new mesh\n            obj = create_with_empty_mesh(used_obj_name, used_obj_name + \"_mesh\")\n            created_objects.append(obj)\n\n            # set two custom properties, first that it is a 3D_future object and second the category_id\n            obj.set_cp(\"is_3D_future\", True)\n            obj.set_cp(\"category_id\", label_mapping.id_from_label(used_obj_name.lower()))\n\n            # get the material uid of the current mesh data\n            current_mat = mesh_data[\"material\"]\n            used_mat = None\n            # search in the used materials after this uid\n            for u_mat in used_materials:\n                if u_mat[\"uid\"] == current_mat:\n                    used_mat = u_mat\n                    break\n            # If there should be a material used\n            if used_mat:\n                if used_mat[\"texture\"]:\n                    # extract the has folder is from the url and download it if necessary\n                    hash_folder = _Front3DLoader.extract_hash_nr_for_texture(used_mat[\"texture\"], front_3D_texture_path)\n                    if hash_folder in used_materials_based_on_texture and \"ceiling\" not in used_obj_name.lower():\n                        mat = used_materials_based_on_texture[hash_folder]\n                        obj.add_material(mat)\n                    else:\n                        # Create a new material\n                        mat = MaterialLoaderUtility.create(name=used_obj_name + \"_material\")\n                        principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n                        if used_mat[\"color\"]:\n                            principled_node.inputs[\"Base Color\"].default_value = mathutils.Vector(\n                                used_mat[\"color\"]) / 255.0\n\n                        used_image = _Front3DLoader.get_used_image(hash_folder, saved_images)\n                        mat.set_principled_shader_value(\"Base Color\", used_image)\n\n                        if \"ceiling\" in used_obj_name.lower():\n                            mat.make_emissive(ceiling_light_strength,\n                                              emission_color=mathutils.Vector(used_mat[\"color\"]) / 255.0)\n\n                        if used_mat[\"normaltexture\"]:\n                            # get the used image based on the normal texture path\n                            # extract the has folder is from the url and download it if necessary\n                            hash_folder = _Front3DLoader.extract_hash_nr_for_texture(used_mat[\"normaltexture\"],\n                                                                                     front_3D_texture_path)\n                            used_image = _Front3DLoader.get_used_image(hash_folder, saved_normal_images)\n\n                            # create normal texture\n                            normal_texture = MaterialLoaderUtility.create_image_node(mat.nodes, used_image, True)\n                            normal_map = mat.nodes.new(\"ShaderNodeNormalMap\")\n                            normal_map.inputs[\"Strength\"].default_value = 1.0\n                            mat.links.new(normal_texture.outputs[\"Color\"], normal_map.inputs[\"Color\"])\n                            # connect normal texture to principled shader\n                            mat.set_principled_shader_value(\"Normal\", normal_map.outputs[\"Normal\"])\n\n                        obj.add_material(mat)\n                        used_materials_based_on_texture[hash_folder] = mat\n                # if there is a normal color used\n                elif used_mat[\"color\"]:\n                    used_hash = tuple(used_mat[\"color\"])\n                    if used_hash in used_materials_based_on_color and \"ceiling\" not in used_obj_name.lower():\n                        mat = used_materials_based_on_color[used_hash]\n                    else:\n                        # Create a new material\n                        mat = MaterialLoaderUtility.create(name=used_obj_name + \"_material\")\n                        # create a principled node and set the default color\n                        principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n                        principled_node.inputs[\"Base Color\"].default_value = mathutils.Vector(used_mat[\"color\"]) / 255.0\n                        # if the object is a ceiling add some light output\n                        if \"ceiling\" in used_obj_name.lower():\n                            mat.make_emissive(ceiling_light_strength,\n                                              emission_color=mathutils.Vector(used_mat[\"color\"]) / 255.0)\n                        else:\n                            used_materials_based_on_color[used_hash] = mat\n\n                    # as this material was just created the material is just append it to the empty list\n                    obj.add_material(mat)\n\n            # extract the vertices from the mesh_data\n            vert = [float(ele) for ele in mesh_data[\"xyz\"]]\n            # extract the faces from the mesh_data\n            faces = mesh_data[\"faces\"]\n            # extract the normals from the mesh_data\n            normal = [float(ele) for ele in mesh_data[\"normal\"]]\n\n            # map those to the blender coordinate system\n            num_vertices = int(len(vert) / 3)\n            vertices = np.reshape(np.array(vert), [num_vertices, 3])\n            normal = np.reshape(np.array(normal), [num_vertices, 3])\n            # flip the first and second value\n            vertices[:, 1], vertices[:, 2] = vertices[:, 2], vertices[:, 1].copy()\n            normal[:, 1], normal[:, 2] = normal[:, 2], normal[:, 1].copy()\n            # reshape back to a long list\n            vertices = np.reshape(vertices, [num_vertices * 3])\n            normal = np.reshape(normal, [num_vertices * 3])\n\n            # add this new data to the mesh object\n            mesh = obj.get_mesh()\n            mesh.vertices.add(num_vertices)\n            mesh.vertices.foreach_set(\"co\", vertices)\n            mesh.vertices.foreach_set(\"normal\", normal)\n\n            # link the faces as vertex indices\n            num_vertex_indicies = len(faces)\n            mesh.loops.add(num_vertex_indicies)\n            mesh.loops.foreach_set(\"vertex_index\", faces)\n\n            # the loops are set based on how the faces are a ranged\n            num_loops = int(num_vertex_indicies / 3)\n            mesh.polygons.add(num_loops)\n            # always 3 vertices form one triangle\n            loop_start = np.arange(0, num_vertex_indicies, 3)\n            # the total size of each triangle is therefore 3\n            loop_total = [3] * num_loops\n            mesh.polygons.foreach_set(\"loop_start\", loop_start)\n            mesh.polygons.foreach_set(\"loop_total\", loop_total)\n\n            # the uv coordinates are reshaped then the face coords are extracted\n            uv_mesh_data = [float(ele) for ele in mesh_data[\"uv\"] if ele is not None]\n            # bb1737bf-dae6-4215-bccf-fab6f584046b.json includes one mesh which only has no UV mapping\n            if uv_mesh_data:\n                uv = np.reshape(np.array(uv_mesh_data), [num_vertices, 2])\n                used_uvs = uv[faces, :]\n                # and again reshaped back to the long list\n                used_uvs = np.reshape(used_uvs, [2 * num_vertex_indicies])\n\n                mesh.uv_layers.new(name=\"new_uv_layer\")\n                mesh.uv_layers[-1].data.foreach_set(\"uv\", used_uvs)\n            else:\n                warnings.warn(f\"This mesh {obj.get_name()} does not have a specified uv map!\")\n\n            # this update converts the upper data into a mesh\n            mesh.update()\n\n            # the generation might fail if the data does not line up\n            # this is not used as even if the data does not line up it is still able to render the objects\n            # We assume that not all meshes in the dataset do conform with the mesh standards set in blender\n            # result = mesh.validate(verbose=False)\n            # if result:\n            #    raise Exception(\"The generation of the mesh: {} failed!\".format(used_obj_name))\n\n        return created_objects",
  "def load_furniture_objs(data: dict, future_model_path: str, lamp_light_strength: float,\n                            label_mapping: LabelIdMapping) -> List[MeshObject]:\n        \"\"\"\n        Load all furniture objects specified in the json file, these objects are stored as \"raw_model.obj\" in the\n        3D_future_model_path. For lamp the lamp_light_strength value can be changed via the config.\n\n        :param data: json data dir. Should contain \"furniture\"\n        :param future_model_path: Path to the models used in the 3D-Front dataset.\n        :param lamp_light_strength: Strength of the emission shader used in each lamp.\n        :param label_mapping: A dict which maps the names of the objects to ids.\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # collect all loaded furniture objects\n        all_objs = []\n        # for each furniture element\n        for ele in data[\"furniture\"]:\n            # create the paths based on the \"jid\"\n            folder_path = os.path.join(future_model_path, ele[\"jid\"])\n            obj_file = os.path.join(folder_path, \"raw_model.obj\")\n            # if the object exists load it -> a lot of object do not exist\n            # we are unsure why this is -> we assume that not all objects have been made public\n            if os.path.exists(obj_file) and not \"7e101ef3-7722-4af8-90d5-7c562834fabd\" in obj_file:\n                # load all objects from this .obj file\n                objs = load_obj(filepath=obj_file)\n                # extract the name, which serves as category id\n                used_obj_name = \"\"\n                if \"category\" in ele:\n                    used_obj_name = ele[\"category\"]\n                elif \"title\" in ele:\n                    used_obj_name = ele[\"title\"]\n                    if \"/\" in used_obj_name:\n                        used_obj_name = used_obj_name.split(\"/\")[0]\n                if used_obj_name == \"\":\n                    used_obj_name = \"others\"\n                for obj in objs:\n                    obj.set_name(used_obj_name)\n                    # add some custom properties\n                    obj.set_cp(\"uid\", ele[\"uid\"])\n                    # this custom property determines if the object was used before\n                    # is needed to only clone the second appearance of this object\n                    obj.set_cp(\"is_used\", False)\n                    obj.set_cp(\"is_3D_future\", True)\n                    obj.set_cp(\"3D_future_type\", \"Non-Object\")  # is an non object used for the interesting score\n                    # set the category id based on the used obj name\n                    obj.set_cp(\"category_id\", label_mapping.id_from_label(used_obj_name.lower()))\n                    # walk over all materials\n                    for mat in obj.get_materials():\n                        if mat is None:\n                            continue\n                        principled_node = mat.get_nodes_with_type(\"BsdfPrincipled\")\n                        if \"bed\" in used_obj_name.lower() or \"sofa\" in used_obj_name.lower():\n                            if len(principled_node) == 1:\n                                principled_node[0].inputs[\"Roughness\"].default_value = 0.5\n                        is_lamp = \"lamp\" in used_obj_name.lower()\n                        if len(principled_node) == 0 and is_lamp:\n                            # this material has already been transformed\n                            continue\n                        if len(principled_node) == 1:\n                            principled_node = principled_node[0]\n                        else:\n                            raise ValueError(f\"The amount of principle nodes can not be more than 1, \"\n                                             f\"for obj: {obj.get_name()}!\")\n\n                        # Front3d .mtl files contain emission color which make the object mistakenly emissive\n                        # => Reset the emission color\n                        principled_node.inputs[\"Emission\"].default_value[:3] = [0, 0, 0]\n\n                        # For each a texture node\n                        image_node = mat.new_node('ShaderNodeTexImage')\n                        # and load the texture.png\n                        base_image_path = os.path.join(folder_path, \"texture.png\")\n                        image_node.image = bpy.data.images.load(base_image_path, check_existing=True)\n                        mat.link(image_node.outputs['Color'], principled_node.inputs['Base Color'])\n                        # if the object is a lamp, do the same as for the ceiling and add an emission shader\n                        if is_lamp:\n                            mat.make_emissive(lamp_light_strength)\n\n                all_objs.extend(objs)\n            elif \"7e101ef3-7722-4af8-90d5-7c562834fabd\" in obj_file:\n                warnings.warn(f\"This file {obj_file} was skipped as it can not be read by blender.\")\n        return all_objs",
  "def move_and_duplicate_furniture(data: dict, all_loaded_furniture: list) -> List[MeshObject]:\n        \"\"\"\n        Move and duplicate the furniture depending on the data in the data json dir.\n        After loading each object gets a location based on the data in the json file. Some objects are used more than\n        once these are duplicated and then placed.\n\n        :param data: json data dir. Should contain \"scene\", which should contain \"room\"\n        :param all_loaded_furniture: all objects which have been loaded in load_furniture_objs\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # this rotation matrix rotates the given quaternion into the blender coordinate system\n        blender_rot_mat = mathutils.Matrix.Rotation(radians(-90), 4, 'X')\n        created_objects = []\n        # for each room\n        for room_id, room in enumerate(data[\"scene\"][\"room\"]):\n            # for each object in that room\n            for child in room[\"children\"]:\n                if \"furniture\" in child[\"instanceid\"]:\n                    # find the object where the uid matches the child ref id\n                    for obj in all_loaded_furniture:\n                        if obj.get_cp(\"uid\") == child[\"ref\"]:\n                            # if the object was used before, duplicate the object and move that duplicated obj\n                            if obj.get_cp(\"is_used\"):\n                                new_obj = obj.duplicate()\n                            else:\n                                # if it is the first time use the object directly\n                                new_obj = obj\n                            created_objects.append(new_obj)\n                            new_obj.set_cp(\"is_used\", True)\n                            new_obj.set_cp(\"room_id\", room_id)\n                            new_obj.set_cp(\"3D_future_type\", \"Object\")  # is an object used for the interesting score\n                            new_obj.set_cp(\"coarse_grained_class\", new_obj.get_cp(\"category_id\"))\n                            # this flips the y and z coordinate to bring it to the blender coordinate system\n                            new_obj.set_location(mathutils.Vector(child[\"pos\"]).xzy)\n                            new_obj.set_scale(child[\"scale\"])\n                            # extract the quaternion and convert it to a rotation matrix\n                            rotation_mat = mathutils.Quaternion(child[\"rot\"]).to_euler().to_matrix().to_4x4()\n                            # transform it into the blender coordinate system and then to an euler\n                            new_obj.set_rotation_euler((blender_rot_mat @ rotation_mat).to_euler())\n        return created_objects",
  "def identify_base_color_image_path(texture_map_paths: List[str]) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"Finds the path to the base color image in a list of texture map paths.\n    We do this by looking for any of the \"base color\" identifiers in each path.\n    We also make sure to account for different capitalization of the identifier.\n\n    :param texture_map_paths: paths to check\n    :type texture_map_paths: list of strings\n    :return: path to the base color image and the specific identifier\n    :rtype: tuple of 2 strings\n    \"\"\"\n    for texture_map_path in texture_map_paths:\n        for identifier_lowercase in _texture_map_identifiers[\"base color\"]:\n            search_string = f\"_{identifier_lowercase}_\"\n            search_start = texture_map_path.lower().find(search_string)\n            if search_start != -1:\n                identifier_start = search_start + 1\n                identifier_end = identifier_start + len(identifier_lowercase)\n                identifier = texture_map_path[identifier_start:identifier_end]\n                return texture_map_path, identifier\n    return None, None",
  "def identify_texture_maps(texture_folder_path: Union[str, Path]) -> Optional[Dict[str, str]]:\n    \"\"\"Finds the paths of the different textures maps in a texture folder.\n\n    :param texture_folder_path: path to the texture folder\n    :return: dictionary that maps texture map types to their path when found, else it maps to an empty string\n    \"\"\"\n    if isinstance(texture_folder_path, str):\n        texture_folder_path = Path(texture_folder_path)\n    texture_map_paths = [str(path.absolute()) for path in texture_folder_path.glob(\"*.jpg\")]\n    color_path, color_identifier = identify_base_color_image_path(texture_map_paths)\n\n    if not color_path:\n        return None\n\n    texture_map_types = _texture_map_identifiers.keys()\n    texture_map_paths_by_type = {type: \"\" for type in texture_map_types}\n    texture_map_paths_by_type[\"base color\"] = color_path\n\n    # To find the other texture maps, we replace the color identifier, with the identifiers of the other texture map\n    # types. By comparing lowercase paths, we also account for different capitalization's e.g. Nor, NOR, NoR, ...\n    for type_val in texture_map_types:\n        for identifier in _texture_map_identifiers[type_val]:\n            texture_map_path_lowercase = color_path.replace(color_identifier, identifier).lower()\n            for path in texture_map_paths:\n                if path.lower() == texture_map_path_lowercase:\n                    texture_map_paths_by_type[type_val] = path\n                    break\n\n    return texture_map_paths_by_type",
  "def load_haven_mat(folder_path: Union[str, Path] = \"resources/haven\", used_assets: Optional[List[str]] = None,\n                   preload: bool = False, fill_used_empty_materials: bool = False,\n                   add_cp: Optional[Dict[str, Any]] = None, return_random_element: bool = False) \\\n        -> Union[List[Material], Material]:\n    \"\"\" Loads all specified haven textures from the given directory.\n\n    :param folder_path: The path to the downloaded haven.\n    :param used_assets: A list of all asset names, you want to use. The asset-name must not be typed in completely,\n                        only the beginning the name starts with. By default, all assets will be loaded, specified\n                        by an empty list or None.\n    :param preload: If set true, only the material names are loaded and not the complete material.\n    :param fill_used_empty_materials: If set true, the preloaded materials, which are used are now loaded completely.\n    :param add_cp: A dictionary of materials and the respective properties.\n    :param return_random_element: If this is True only a single Material is loaded and returned, if you want to sample\n                                  many materials load them all with the preload option, use them and then fill the used\n                                  empty materials instead of calling this function multiple times.\n    :return a list of all loaded materials, if preload is active these materials do not contain any textures yet\n            and have to be filled before rendering (by calling this function again, there is no need to save the prior\n            returned list) or if return_random_element is True only a single Material is returned\n    \"\"\"\n    # set default value\n    if add_cp is None:\n        add_cp = {}\n    if used_assets is None:\n        used_assets = []\n\n    # makes the integration of complex materials easier\n    addon_utils.enable(\"node_wrangler\")\n\n    haven_folder = Path(resolve_path(str(folder_path)))\n\n    if preload and fill_used_empty_materials:\n        raise RuntimeError(\"Preload and fill used empty materials can not be done at the same time, check config!\")\n\n    if not haven_folder.exists():\n        raise FileNotFoundError(f\"The given haven folder does not exist: {haven_folder}\")\n\n    # add the \"textures\" folder, if that is not already the case\n    if haven_folder.name != \"textures\" and (haven_folder / \"textures\").exists():\n        haven_folder /= \"textures\"\n\n    texture_names: List[str] = os.listdir(haven_folder)\n    texture_names.sort()\n    if not texture_names:\n        raise FileNotFoundError(f\"No texture folders found in {haven_folder}.\")\n\n    # filter texture names based on used assets:\n    if used_assets:\n        texture_names = [texture_name for texture_name in texture_names if\n                         any(texture_name.startswith(asset) for asset in used_assets)]\n        if not texture_names:\n            raise FileNotFoundError(f\"No texture folders found in {haven_folder} for which used_assets \"\n                                    f\"can be meet: {used_assets}.\")\n\n    # if only one element is returned\n    if return_random_element:\n        texture_names = [random.choice(texture_names)]\n\n    materials: List[Material] = []\n    for texture_name in texture_names:\n        texture_folder_path = haven_folder / texture_name\n        if not texture_folder_path.is_dir():\n            print(f\"Ignoring {texture_folder_path}, must be a folder.\")\n            continue\n\n        texture_map_paths_by_type = identify_texture_maps(str(texture_folder_path))\n        if texture_map_paths_by_type is None:\n            print(f\"Ignoring {texture_name}, could not identify texture maps.\")\n            continue\n\n        # if the material was already created it only has to be searched\n        if fill_used_empty_materials:\n            new_mat = MaterialLoaderUtility.find_cc_material_by_name(texture_name, add_cp)\n        else:\n            new_mat = MaterialLoaderUtility.create_new_cc_material(texture_name, add_cp)\n        # append newly created material\n        materials.append(Material(new_mat))\n        if preload:\n            # if preload then the material is only created but not filled\n            continue\n        if fill_used_empty_materials and not MaterialLoaderUtility.is_material_used(new_mat):\n            # now only the materials, which have been used should be filled\n            continue\n\n        # create material based on the found image paths\n        HavenMaterialLoader.create_material(new_mat,\n                                            texture_map_paths_by_type[\"base color\"],\n                                            texture_map_paths_by_type[\"ambient occlusion\"],\n                                            texture_map_paths_by_type[\"specular\"],\n                                            texture_map_paths_by_type[\"roughness\"],\n                                            texture_map_paths_by_type[\"transparency\"],\n                                            texture_map_paths_by_type[\"normal\"],\n                                            texture_map_paths_by_type[\"displacement\"],\n                                            texture_map_paths_by_type[\"bump\"])\n    if return_random_element:\n        if len(materials) != 1:\n            raise RuntimeError(f\"The amount of loaded materials is not one: {materials}, this should not happen!\")\n        return materials[0]\n\n    return materials",
  "class HavenMaterialLoader:\n    \"\"\"\n    This class loads all textures obtained from https://texturehaven.com, use 'blenderproc download haven'\n    to download all the textures to your pc.\n\n    All textures here support Physically based rendering (PBR), which makes the textures more realistic.\n\n    There is a preload option, in which you only load empty materials, without any loaded textures, these are than\n    later filled, when an object really uses them. This saves on loading times.\n    \"\"\"\n\n    @staticmethod\n    def create_material(new_mat: bpy.types.Material, base_image_path: str, ambient_occlusion_image_path: str,\n                        specular_image_path: str,\n                        roughness_image_path: str, alpha_image_path: str, normal_image_path: str,\n                        displacement_image_path: str,\n                        bump_image_path: str):\n        \"\"\"\n        Create a material for the haven datatset, the combination used here is calibrated to the haven dataset format.\n\n        :param new_mat: The new material, which will get all the given textures\n        :param base_image_path: The path to the color image\n        :param ambient_occlusion_image_path: The path to the ambient occlusion image\n        :param specular_image_path: The path to the specular image\n        :param roughness_image_path: The path to the roughness image\n        :param alpha_image_path: The path to the alpha image (when this was written there was no alpha image provided \\\n                                 in the haven dataset)\n        :param normal_image_path: The path to the normal image\n        :param displacement_image_path: The path to the displacement image\n        :param bump_image_path: The path to the bump image\n        \"\"\"\n        nodes = new_mat.node_tree.nodes\n        links = new_mat.node_tree.links\n\n        principled_bsdf = Utility.get_the_one_node_with_type(nodes, \"BsdfPrincipled\")\n        output_node = Utility.get_the_one_node_with_type(nodes, \"OutputMaterial\")\n\n        collection_of_texture_nodes = []\n        base_color = MaterialLoaderUtility.add_base_color(nodes, links, base_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(base_color)\n\n        specular_color = MaterialLoaderUtility.add_specular(nodes, links, specular_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(specular_color)\n\n        ao_node = MaterialLoaderUtility.add_ambient_occlusion(nodes, links, ambient_occlusion_image_path,\n                                                              principled_bsdf, base_color)\n        collection_of_texture_nodes.append(ao_node)\n\n        roughness_node = MaterialLoaderUtility.add_roughness(nodes, links, roughness_image_path,\n                                                             principled_bsdf)\n        collection_of_texture_nodes.append(roughness_node)\n\n        alpha_node = MaterialLoaderUtility.add_alpha(nodes, links, alpha_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(alpha_node)\n\n        # only add a bump map if no normal map was found\n        if not os.path.exists(normal_image_path):\n            bump_node = MaterialLoaderUtility.add_bump(nodes, links, bump_image_path, principled_bsdf)\n            collection_of_texture_nodes.append(bump_node)\n        else:\n            normal_node = MaterialLoaderUtility.add_normal(nodes, links, normal_image_path, principled_bsdf,\n                                                           invert_y_channel=False)\n            collection_of_texture_nodes.append(normal_node)\n\n        displacement_node = MaterialLoaderUtility.add_displacement(nodes, links, displacement_image_path,\n                                                                   output_node)\n        collection_of_texture_nodes.append(displacement_node)\n\n        collection_of_texture_nodes = [node for node in collection_of_texture_nodes if node is not None]\n\n        MaterialLoaderUtility.connect_uv_maps(nodes, links, collection_of_texture_nodes)",
  "def create_material(new_mat: bpy.types.Material, base_image_path: str, ambient_occlusion_image_path: str,\n                        specular_image_path: str,\n                        roughness_image_path: str, alpha_image_path: str, normal_image_path: str,\n                        displacement_image_path: str,\n                        bump_image_path: str):\n        \"\"\"\n        Create a material for the haven datatset, the combination used here is calibrated to the haven dataset format.\n\n        :param new_mat: The new material, which will get all the given textures\n        :param base_image_path: The path to the color image\n        :param ambient_occlusion_image_path: The path to the ambient occlusion image\n        :param specular_image_path: The path to the specular image\n        :param roughness_image_path: The path to the roughness image\n        :param alpha_image_path: The path to the alpha image (when this was written there was no alpha image provided \\\n                                 in the haven dataset)\n        :param normal_image_path: The path to the normal image\n        :param displacement_image_path: The path to the displacement image\n        :param bump_image_path: The path to the bump image\n        \"\"\"\n        nodes = new_mat.node_tree.nodes\n        links = new_mat.node_tree.links\n\n        principled_bsdf = Utility.get_the_one_node_with_type(nodes, \"BsdfPrincipled\")\n        output_node = Utility.get_the_one_node_with_type(nodes, \"OutputMaterial\")\n\n        collection_of_texture_nodes = []\n        base_color = MaterialLoaderUtility.add_base_color(nodes, links, base_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(base_color)\n\n        specular_color = MaterialLoaderUtility.add_specular(nodes, links, specular_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(specular_color)\n\n        ao_node = MaterialLoaderUtility.add_ambient_occlusion(nodes, links, ambient_occlusion_image_path,\n                                                              principled_bsdf, base_color)\n        collection_of_texture_nodes.append(ao_node)\n\n        roughness_node = MaterialLoaderUtility.add_roughness(nodes, links, roughness_image_path,\n                                                             principled_bsdf)\n        collection_of_texture_nodes.append(roughness_node)\n\n        alpha_node = MaterialLoaderUtility.add_alpha(nodes, links, alpha_image_path, principled_bsdf)\n        collection_of_texture_nodes.append(alpha_node)\n\n        # only add a bump map if no normal map was found\n        if not os.path.exists(normal_image_path):\n            bump_node = MaterialLoaderUtility.add_bump(nodes, links, bump_image_path, principled_bsdf)\n            collection_of_texture_nodes.append(bump_node)\n        else:\n            normal_node = MaterialLoaderUtility.add_normal(nodes, links, normal_image_path, principled_bsdf,\n                                                           invert_y_channel=False)\n            collection_of_texture_nodes.append(normal_node)\n\n        displacement_node = MaterialLoaderUtility.add_displacement(nodes, links, displacement_image_path,\n                                                                   output_node)\n        collection_of_texture_nodes.append(displacement_node)\n\n        collection_of_texture_nodes = [node for node in collection_of_texture_nodes if node is not None]\n\n        MaterialLoaderUtility.connect_uv_maps(nodes, links, collection_of_texture_nodes)",
  "def load_obj(filepath: str, cached_objects: Optional[Dict[str, List[MeshObject]]] = None,\n             use_legacy_obj_import: bool = False, **kwargs) -> List[MeshObject]:\n    \"\"\" Import all objects for the given file and returns the loaded objects\n\n    In .obj files a list of objects can be saved in.\n    In .ply files only one object can be saved so the list has always at most one element\n\n    :param filepath: the filepath to the location where the data is stored\n    :param cached_objects: a dict of filepath to objects, which have been loaded before, to avoid reloading\n                           (the dict is updated in this function)\n    :param use_legacy_obj_import: If this is true the old legacy obj importer in python is used. It is slower, but\n                                  it correctly imports the textures in the ShapeNet dataset.\n    :param kwargs: all other params are handed directly to the bpy loading fct. check the corresponding documentation\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The given filepath does not exist: {filepath}\")\n\n    if cached_objects is not None and isinstance(cached_objects, dict):\n        if filepath in cached_objects.keys():\n            created_obj = []\n            for obj in cached_objects[filepath]:\n                # duplicate the object\n                created_obj.append(obj.duplicate())\n            return created_obj\n        loaded_objects = load_obj(filepath, cached_objects=None, **kwargs)\n        cached_objects[filepath] = loaded_objects\n        return loaded_objects\n    # save all selected objects\n    previously_selected_objects = bpy.context.selected_objects\n    if filepath.endswith(\".obj\"):\n        # load an .obj file:\n        if use_legacy_obj_import:\n            bpy.ops.import_scene.obj(filepath=filepath, **kwargs)\n        else:\n            bpy.ops.wm.obj_import(filepath=filepath, **kwargs)\n    elif filepath.endswith(\".ply\"):\n        PLY_TEXTURE_FILE_COMMENT = \"comment TextureFile \"\n        model_name = os.path.basename(filepath)\n\n        # Read file\n        with open(filepath, \"r\", encoding=\"latin-1\") as file:\n            ply_file_content = file.read()\n\n        # Check if texture file is given\n        if PLY_TEXTURE_FILE_COMMENT in ply_file_content:\n            # Find name of texture file\n            texture_file_name = re.search(f\"{PLY_TEXTURE_FILE_COMMENT}(.*)\\n\", ply_file_content).group(1)\n\n            # Determine full texture file path\n            texture_file_path = os.path.join(os.path.dirname(filepath), texture_file_name)\n            material = create_material_from_texture(\n                texture_file_path, material_name=f\"ply_{model_name}_texture_model\"\n            )\n\n            # Change content of ply file to work with blender ply importer\n            new_ply_file_content = ply_file_content\n            new_ply_file_content = new_ply_file_content.replace(\"property float texture_u\", \"property float s\")\n            new_ply_file_content = new_ply_file_content.replace(\"property float texture_v\", \"property float t\")\n\n            # Create temporary .ply file\n            tmp_ply_file = os.path.join(Utility.get_temporary_directory(), model_name)\n            with open(tmp_ply_file, \"w\", encoding=\"latin-1\") as file:\n                file.write(new_ply_file_content)\n\n            # Load .ply mesh\n            bpy.ops.import_mesh.ply(filepath=tmp_ply_file, **kwargs)\n\n        else:  # If no texture was given\n            # load a .ply mesh\n            bpy.ops.import_mesh.ply(filepath=filepath, **kwargs)\n            # Create default material\n            material = create_material('ply_material')\n            material.map_vertex_color()\n        selected_objects = [obj for obj in bpy.context.selected_objects if obj not in previously_selected_objects]\n        for obj in selected_objects:\n            obj.data.materials.append(material.blender_obj)\n    elif filepath.endswith('.dae'):\n        bpy.ops.wm.collada_import(filepath=filepath)\n    elif filepath.lower().endswith('.stl'):\n        # load a .stl file\n        bpy.ops.wm.stl_import(filepath=filepath, **kwargs)\n        # add a default material to stl file\n        mat = bpy.data.materials.new(name=\"stl_material\")\n        mat.use_nodes = True\n        selected_objects = [obj for obj in bpy.context.selected_objects if\n                                obj not in previously_selected_objects]\n        for obj in selected_objects:\n            obj.data.materials.append(mat)\n    elif filepath.lower().endswith('.fbx'):\n        bpy.ops.import_scene.fbx(filepath=filepath)\n    elif filepath.lower().endswith('.glb') or filepath.lower().endswith('.gltf'):\n        bpy.ops.import_scene.gltf(filepath=filepath)\n\n    mesh_objects = convert_to_meshes([obj for obj in bpy.context.selected_objects\n                                  if obj not in previously_selected_objects])\n    # Add model_path cp to all objects\n    for obj in mesh_objects:\n        obj.set_cp(\"model_path\", filepath)\n    return mesh_objects",
  "def load_texture(path: str, colorspace: str = \"sRGB\") -> List[bpy.types.Texture]:\n    \"\"\" Loads images and creates image textures.\n\n    Depending on the form of the provided path:\n    1. Loads an image, creates an image texture, and assigns the loaded image to the texture, when a path to an\n    image is provided.\n    2. Load images and for each creates a texture, and assigns an image to this texture, if a path to a\n    folder with images is provided.\n\n    NOTE: Same image file can be loaded once to avoid unnecessary overhead. If you really need the same image in\n    different colorspaces, then have a copy per desired colorspace and load them in different instances of this Loader.\n\n    :param path: The path to the folder with assets/to the asset.\n    :param colorspace: Colorspace type to assign to loaded assets. Available: ['Filmic Log', 'Linear', 'Linear ACES',\n                       'Non-Color', 'Raw', 'sRGB', 'XYZ'].\n    :return: The list of created textures.\n    \"\"\"\n    path = resolve_path(path)\n    image_paths = _TextureLoader.resolve_paths(path)\n    textures = _TextureLoader.load_and_create(image_paths, colorspace)\n\n    return textures",
  "class _TextureLoader:\n\n    @staticmethod\n    def resolve_paths(path: str) -> list:\n        \"\"\" Resolves absolute paths to assets in the provided folder, or to an asset if an appropriate path is provided.\n\n        :param path: Path to folder containing assets or path to an asset. Type: string.\n        :return: List of absolute paths to assets. Type: list.\n        \"\"\"\n        image_paths = []\n        if os.path.exists(path):\n            if os.path.isdir(path):\n                image_paths = glob.glob(os.path.join(path, \"*\"))\n            else:\n                image_paths.append(path)\n        else:\n            raise RuntimeError(f\"Invalid path: {path}\")\n\n        return image_paths\n\n    @staticmethod\n    def load_and_create(image_paths: list, colorspace: str) -> List[bpy.types.Texture]:\n        \"\"\" Loads an image, creates an image texture and assigns an image to this texture per each provided path.\n\n        :param image_paths: List of absolute paths to assets. Type: list.\n        :param colorspace: Colorspace type of the assets. Type: string.\n        :return: Created textures. Type: list.\n        \"\"\"\n        existing = [image.filepath for image in bpy.data.images]\n        textures = []\n        for image_path in image_paths:\n            if image_path not in existing:\n                loaded_image = bpy.data.images.load(filepath=image_path)\n                existing.append(image_path)\n                loaded_image.colorspace_settings.name = colorspace\n                texture_name = f\"ct_{loaded_image.name}\"\n                tex = bpy.data.textures.new(name=texture_name, type=\"IMAGE\")\n                tex.image = loaded_image\n                tex.use_nodes = True\n                tex.type = \"IMAGE\"\n                textures.append(tex)\n            else:\n                warnings.warn(f\"Image {image_path} has been already loaded and a corresponding texture was created. \"\n                              f\"Following the save behaviour of reducing the overhead, it is skipped. So, if you \"\n                              f\"really need to load the same image again (for example, in a different or in the \"\n                              f\"same colorspace), use the copy of the file.\")\n        return textures",
  "def resolve_paths(path: str) -> list:\n        \"\"\" Resolves absolute paths to assets in the provided folder, or to an asset if an appropriate path is provided.\n\n        :param path: Path to folder containing assets or path to an asset. Type: string.\n        :return: List of absolute paths to assets. Type: list.\n        \"\"\"\n        image_paths = []\n        if os.path.exists(path):\n            if os.path.isdir(path):\n                image_paths = glob.glob(os.path.join(path, \"*\"))\n            else:\n                image_paths.append(path)\n        else:\n            raise RuntimeError(f\"Invalid path: {path}\")\n\n        return image_paths",
  "def load_and_create(image_paths: list, colorspace: str) -> List[bpy.types.Texture]:\n        \"\"\" Loads an image, creates an image texture and assigns an image to this texture per each provided path.\n\n        :param image_paths: List of absolute paths to assets. Type: list.\n        :param colorspace: Colorspace type of the assets. Type: string.\n        :return: Created textures. Type: list.\n        \"\"\"\n        existing = [image.filepath for image in bpy.data.images]\n        textures = []\n        for image_path in image_paths:\n            if image_path not in existing:\n                loaded_image = bpy.data.images.load(filepath=image_path)\n                existing.append(image_path)\n                loaded_image.colorspace_settings.name = colorspace\n                texture_name = f\"ct_{loaded_image.name}\"\n                tex = bpy.data.textures.new(name=texture_name, type=\"IMAGE\")\n                tex.image = loaded_image\n                tex.use_nodes = True\n                tex.type = \"IMAGE\"\n                textures.append(tex)\n            else:\n                warnings.warn(f\"Image {image_path} has been already loaded and a corresponding texture was created. \"\n                              f\"Following the save behaviour of reducing the overhead, it is skipped. So, if you \"\n                              f\"really need to load the same image again (for example, in a different or in the \"\n                              f\"same colorspace), use the copy of the file.\")\n        return textures",
  "class GlobalStorage:\n    \"\"\"\n    The GlobalStorage has two functions:\n        1. It can store data over the boundaries of modules with the add(), set() and get() functions\n        2. It keeps a global config, which can be used as a fallback strategy in the case a config value is used\n           in many modules, for example the \"output_dir\".\n\n    To 1. you can save your own keys in the GlobalStorage to access them in a later module.\n        For example you have a personal renderer or loader, which has attributes, which are independent of the scene and\n        the objects so custom properties for those are not the way to go. In these instances you can use these \n        functions.\n\n    Here is a list of all used global_storage_keys to avoid that your key is clashing with existing keys:\n\n    .. list-table:: \n        :widths: 25 100 10\n        :header-rows: 1\n\n        * - Parameter\n          - Description\n          - Type\n        * - renderer_distance_end\n          - This key is saved by the Renderer during distance rendering and is used in the\n            StereoGlobalMatchingWriter. \n          - string\n         \n    Please add all new keys you create to this list.\n    \n    To 2. the global config is inited during the main.Initializer module, this means before that it is not possible to\n    access keys from the global config, but it is possible to add keys, which can then be later accessed for that check:\n    add_to_config_before_init(). It is usually not necessary that you will access the global config yourself as each\n    Config checks automatically if the key is stored in the global config, if it was not defined in the current module.\n    The checking order:\n    Local module then the global config if both fail the default value is used, if there is none an Exception is thrown.\n    \"\"\"\n\n    # holds variables which are created during the execution to get information over module boundaries\n    _storage_dict: Dict[str, Any] = {}\n\n    # defines all variables which are stored globally and are set by the config\n    _global_config: Optional[\"Config\"] = None\n\n    # internal variables defined before the global config exists, will be copied into the global config on init\n    # and then deleted, should not be used after init of the GlobalStorage\n    _add_to_global_config_at_init: Dict[str, Any] = {}\n\n    @staticmethod\n    def init_global(global_config: \"Config\"):\n        \"\"\"\n        Inits the global config with the given config, global_config should be of type blenderproc.python.Config\n\n        Adds a key value pairs from add_to_global_config_at_init\n\n        :param global_config: the config to use\n        \"\"\"\n        GlobalStorage._global_config = global_config\n        for key, value in GlobalStorage._add_to_global_config_at_init.items():\n            if not GlobalStorage._global_config.has_param(key):\n                GlobalStorage._global_config.data[key] = value\n            else:\n                raise RuntimeError(f\"This key was already found in the global config: {key} it is \"\n                                   \"also used internally, please use another key!\")\n\n    @staticmethod\n    def add_to_config_before_init(key: str, value: Any):\n        \"\"\"\n        Adds values to the global config before the GlobalStorage was inited, these value can only be accessed\n        after the GlobalStorage was inited.\n\n        :param key: the key which is used in the global config to identify the value\n        :param value: the value which can be identified over the key\n        \"\"\"\n        if GlobalStorage._global_config is None:\n            if key not in GlobalStorage._add_to_global_config_at_init:\n                GlobalStorage._add_to_global_config_at_init[key] = value\n            else:\n                raise RuntimeError(f\"This key: {key} was added before to the list of \"\n                                   \"add_to_global_config_at_init!\")\n        else:\n            raise RuntimeError(\"This fct. should only be called before the GlobalStorage was inited!\")\n\n    @staticmethod\n    def add(key: str, value: Any):\n        \"\"\"\n        Adds a key to the GlobalStorage this is independent of the global config, this can be used to store values\n        over Module boundaries. Adding only works if there is not already a key like this in the GlobalStorage.\n\n        For example the distance renderer sets the value \"distance_end\" during the rendering process, a module which is\n        executed afterwards can then with get() access this value.\n\n        These values can be added before the global config was inited as they do not depend on each other.\n\n        :param key: which is added to the GlobalStorage\n        :param value: which can be accessed by this key over the get() fct.\n        \"\"\"\n        if key not in GlobalStorage._storage_dict:\n            GlobalStorage._storage_dict[key] = value\n        else:\n            raise RuntimeError(f\"The key: {key} was already set before with \"\n                               f\"this value: {GlobalStorage._storage_dict[key]}\")\n\n    @staticmethod\n    def set(key: str, value: Any):\n        \"\"\"\n        Sets a key in the GlobalStorage this is independent of the global config, this can be used to store values\n        over Module boundaries. Setting always works and overwrites existing keys\n\n        For example the distance renderer sets the value \"renderer_distance_end\" during the rendering process, a module\n        which is executed afterwards can then with get() access this value.\n\n        These values can be added before the global config was inited as they do not depend on each other.\n\n        :param key: which is added to the GlobalStorage\n        :param value: which can be accessed by this key over the get() fct.\n        \"\"\"\n        GlobalStorage._storage_dict[key] = value\n\n    @staticmethod\n    def get(key: str) -> Any:\n        \"\"\"\n        Returns a value from the GlobalStorage, please check add() and set() for more information\n\n        :param key: for which a value is searched\n        :return: value for the key\n        \"\"\"\n        if key in GlobalStorage._storage_dict:\n            return GlobalStorage._storage_dict[key]\n        raise RuntimeError(f\"The key: {key} is not in the global storage!\")\n\n    @staticmethod\n    def is_in_storage(key: str) -> bool:\n        \"\"\"\n        Checks if a key is in the GlobalStorage\n\n        :param key: for which a value is searched\n        :return: True if the key is in the storage\n        \"\"\"\n        return key in GlobalStorage._storage_dict\n\n    @staticmethod\n    def has_param(key: str) -> bool:\n        \"\"\"\n        Checks if this key is in the global config not in the GlobalStorage!\n\n        :param key: which should be checked\n        :return: True if the key is in the global config\n        \"\"\"\n        if GlobalStorage._global_config is not None:\n            return GlobalStorage._global_config.has_param(key)\n        return False\n\n    @staticmethod\n    def get_global_config() -> \"Config\":\n        \"\"\"\n        Returns the global config, this function should be used with care!\n\n        There are only a few cases where this function should be called, please read the description at the top and\n        make sure you have to call this function.\n\n        :return: the global config as a utility.Config object\n        \"\"\"\n        if GlobalStorage._global_config is not None:\n            return GlobalStorage._global_config\n        raise RuntimeError(\"The global config was not initialized!\")",
  "def init_global(global_config: \"Config\"):\n        \"\"\"\n        Inits the global config with the given config, global_config should be of type blenderproc.python.Config\n\n        Adds a key value pairs from add_to_global_config_at_init\n\n        :param global_config: the config to use\n        \"\"\"\n        GlobalStorage._global_config = global_config\n        for key, value in GlobalStorage._add_to_global_config_at_init.items():\n            if not GlobalStorage._global_config.has_param(key):\n                GlobalStorage._global_config.data[key] = value\n            else:\n                raise RuntimeError(f\"This key was already found in the global config: {key} it is \"\n                                   \"also used internally, please use another key!\")",
  "def add_to_config_before_init(key: str, value: Any):\n        \"\"\"\n        Adds values to the global config before the GlobalStorage was inited, these value can only be accessed\n        after the GlobalStorage was inited.\n\n        :param key: the key which is used in the global config to identify the value\n        :param value: the value which can be identified over the key\n        \"\"\"\n        if GlobalStorage._global_config is None:\n            if key not in GlobalStorage._add_to_global_config_at_init:\n                GlobalStorage._add_to_global_config_at_init[key] = value\n            else:\n                raise RuntimeError(f\"This key: {key} was added before to the list of \"\n                                   \"add_to_global_config_at_init!\")\n        else:\n            raise RuntimeError(\"This fct. should only be called before the GlobalStorage was inited!\")",
  "def add(key: str, value: Any):\n        \"\"\"\n        Adds a key to the GlobalStorage this is independent of the global config, this can be used to store values\n        over Module boundaries. Adding only works if there is not already a key like this in the GlobalStorage.\n\n        For example the distance renderer sets the value \"distance_end\" during the rendering process, a module which is\n        executed afterwards can then with get() access this value.\n\n        These values can be added before the global config was inited as they do not depend on each other.\n\n        :param key: which is added to the GlobalStorage\n        :param value: which can be accessed by this key over the get() fct.\n        \"\"\"\n        if key not in GlobalStorage._storage_dict:\n            GlobalStorage._storage_dict[key] = value\n        else:\n            raise RuntimeError(f\"The key: {key} was already set before with \"\n                               f\"this value: {GlobalStorage._storage_dict[key]}\")",
  "def set(key: str, value: Any):\n        \"\"\"\n        Sets a key in the GlobalStorage this is independent of the global config, this can be used to store values\n        over Module boundaries. Setting always works and overwrites existing keys\n\n        For example the distance renderer sets the value \"renderer_distance_end\" during the rendering process, a module\n        which is executed afterwards can then with get() access this value.\n\n        These values can be added before the global config was inited as they do not depend on each other.\n\n        :param key: which is added to the GlobalStorage\n        :param value: which can be accessed by this key over the get() fct.\n        \"\"\"\n        GlobalStorage._storage_dict[key] = value",
  "def get(key: str) -> Any:\n        \"\"\"\n        Returns a value from the GlobalStorage, please check add() and set() for more information\n\n        :param key: for which a value is searched\n        :return: value for the key\n        \"\"\"\n        if key in GlobalStorage._storage_dict:\n            return GlobalStorage._storage_dict[key]\n        raise RuntimeError(f\"The key: {key} is not in the global storage!\")",
  "def is_in_storage(key: str) -> bool:\n        \"\"\"\n        Checks if a key is in the GlobalStorage\n\n        :param key: for which a value is searched\n        :return: True if the key is in the storage\n        \"\"\"\n        return key in GlobalStorage._storage_dict",
  "def has_param(key: str) -> bool:\n        \"\"\"\n        Checks if this key is in the global config not in the GlobalStorage!\n\n        :param key: which should be checked\n        :return: True if the key is in the global config\n        \"\"\"\n        if GlobalStorage._global_config is not None:\n            return GlobalStorage._global_config.has_param(key)\n        return False",
  "def get_global_config() -> \"Config\":\n        \"\"\"\n        Returns the global config, this function should be used with care!\n\n        There are only a few cases where this function should be called, please read the description at the top and\n        make sure you have to call this function.\n\n        :return: the global config as a utility.Config object\n        \"\"\"\n        if GlobalStorage._global_config is not None:\n            return GlobalStorage._global_config\n        raise RuntimeError(\"The global config was not initialized!\")",
  "def resolve_path(path: Union[str, Path]) -> str:\n    \"\"\" Returns an absolute path. If given path is relative, current working directory is put in front.\n\n    :param path: The path to resolve.\n    :return: The absolute path.\n    \"\"\"\n    if isinstance(path, Path):\n        path = str(path.absolute())\n    path = path.strip()\n\n    if path.startswith(\"/\"):\n        return path\n    if path.startswith(\"~\"):\n        return path.replace(\"~\", os.getenv(\"HOME\"))\n    return os.path.join(os.getcwd(), path)",
  "def resolve_resource(relative_resource_path: str) -> str:\n    \"\"\" Returns an absolute path to the given BlenderProc resource.\n\n    :param relative_resource_path: The relative path inside the BlenderProc resource folder.\n    :return: The absolute path.\n    \"\"\"\n    return resolve_path(os.path.join(Utility.blenderproc_root, \"blenderproc\", \"resources\", relative_resource_path))",
  "def num_frames() -> int:\n    \"\"\" Returns the currently total number of registered frames.\n\n    :return: The number of frames.\n    \"\"\"\n    return bpy.context.scene.frame_end - bpy.context.scene.frame_start",
  "def reset_keyframes() -> None:\n    \"\"\" Removes registered keyframes from all objects and resets frame_start and frame_end \"\"\"\n    bpy.context.scene.frame_start = 0\n    bpy.context.scene.frame_end = 0\n    for a in bpy.data.actions:\n        bpy.data.actions.remove(a)",
  "def set_keyframe_render_interval(frame_start: Optional[int] = None, frame_end: Optional[int] = None):\n    \"\"\" Sets frame_start and/or frame_end which determine the frames that will be rendered.\n\n    :param frame_start: The new frame_start value. If None, it will be ignored.\n    :param frame_end: The new frame_end value. If None, it will be ignored.\n    \"\"\"\n    if frame_start is not None:\n        bpy.context.scene.frame_start = frame_start\n    if frame_end is not None:\n        bpy.context.scene.frame_end = frame_end",
  "class Utility:\n    \"\"\"\n    The main utility class, helps with different BlenderProc functions.\n    \"\"\"\n    blenderproc_root = os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\")\n    temp_dir = \"\"\n    used_temp_id = None\n\n    @staticmethod\n    def get_current_version() -> Optional[str]:\n        \"\"\" Gets the current blenderproc version.\n\n        :return: a string, the BlenderProc version\n        \"\"\"\n        return __version__\n\n    @staticmethod\n    def get_temporary_directory() -> str:\n        \"\"\"\n        :return: default temporary directory, shared memory if it exists\n        \"\"\"\n        return Utility.temp_dir\n\n    @staticmethod\n    def merge_dicts(source: Dict[Any, Any], destination: Dict[Any, Any]) -> Dict[Any, Any]:\n        \"\"\" Recursively copies all key value pairs from src to dest (Overwrites existing)\n\n        :param source: The source dict.\n        :param destination: The destination dict\n        :return: The modified destination dict.\n        \"\"\"\n        for key, value in source.items():\n            if isinstance(value, dict):\n                # get node or create one\n                node = destination.setdefault(key, {})\n                Utility.merge_dicts(value, node)\n            else:\n                destination[key] = value\n\n        return destination\n\n    @staticmethod\n    def hex_to_rgba(hex_value: str) -> List[float]:\n        \"\"\" Converts the given hex string to rgba color values.\n\n        :param hex_value: The hex string, describing rgb.\n        :return: The rgba color, in form of a list. Values between 0 and 1.\n        \"\"\"\n        return [x / 255 for x in bytes.fromhex(hex_value[-6:])] + [1.0]\n\n    @staticmethod\n    def rgb_to_hex(rgb: Tuple[int, int, int]) -> str:\n        \"\"\" Converts the given rgb to hex values.\n\n        :param rgb: tuple of three with rgb integers.\n        :return: Hex string.\n        \"\"\"\n        if len(rgb) != 3:\n            raise ValueError(f\"The given rgb has to have 3 values: {rgb}\")\n\n        return f\"#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}\"\n\n    @staticmethod\n    def insert_node_instead_existing_link(links: bpy.types.NodeLinks, source_socket: bpy.types.NodeSocket,\n                                          new_node_dest_socket: bpy.types.NodeSocket,\n                                          new_node_src_socket: bpy.types.NodeSocket, dest_socket: bpy.types.NodeSocket):\n        \"\"\" Replaces the node between source_socket and dest_socket with a new node.\n\n        Before: source_socket -> dest_socket\n        After: source_socket -> new_node_dest_socket and new_node_src_socket -> dest_socket\n\n        :param links: The collection of all links.\n        :param source_socket: The source socket.\n        :param new_node_dest_socket: The new destination for the link starting from source_socket.\n        :param new_node_src_socket: The new source for the link towards dest_socket.\n        :param dest_socket: The destination socket\n        \"\"\"\n        for l in links:\n            if l.from_socket == source_socket or l.to_socket == dest_socket:\n                links.remove(l)\n\n        links.new(source_socket, new_node_dest_socket)\n        links.new(new_node_src_socket, dest_socket)\n\n    @staticmethod\n    def get_node_connected_to_the_output_and_unlink_it(material: bpy.types.Material) \\\n            -> Tuple[Optional[bpy.types.Node], bpy.types.Node]:\n        \"\"\"\n        Searches for the OutputMaterial in the given material and finds the connected node to it,\n        removes the connection between this node and the output and returns this node and the material_output\n\n        :param material: Material on which this operation should be performed\n        \"\"\"\n        nodes = material.node_tree.nodes\n        links = material.node_tree.links\n\n        material_output = Utility.get_the_one_node_with_type(nodes, 'OutputMaterial')\n        # find the node, which is connected to the output\n        node_connected_to_the_output = None\n        for link in links:\n            if link.to_node == material_output:\n                node_connected_to_the_output = link.from_node\n                # remove this link\n                links.remove(link)\n                break\n        return node_connected_to_the_output, material_output\n\n    @staticmethod\n    def get_nodes_with_type(nodes: List[bpy.types.Node], node_type: str,\n                            created_in_func: Optional[str] = None) -> List[bpy.types.Node]:\n        \"\"\"\n        Returns all nodes which are of the given node_type\n\n        :param nodes: list of nodes of the current material\n        :param node_type: node types\n        :param created_in_func: Only return nodes created by the specified function\n        :return: list of nodes, which belong to the type\n        \"\"\"\n        nodes_with_type = [node for node in nodes if node_type in node.bl_idname]\n        if created_in_func:\n            nodes_with_type = Utility.get_nodes_created_in_func(nodes_with_type, created_in_func)\n        return nodes_with_type\n\n    @staticmethod\n    def get_the_one_node_with_type(nodes: List[bpy.types.Node], node_type: str,\n                                   created_in_func: str = \"\") -> bpy.types.Node:\n        \"\"\"\n        Returns the one node which is of the given node_type\n\n        This function will only work if there is only one of the nodes of this type.\n\n        :param nodes: list of nodes of the current material\n        :param node_type: node types\n        :param created_in_func: only return node created by the specified function\n        :return: node of the node type\n        \"\"\"\n        node = Utility.get_nodes_with_type(nodes, node_type, created_in_func)\n        if node and len(node) == 1:\n            return node[0]\n        raise RuntimeError(f\"There is not only one node of this type: {node_type}, there are: {len(node)}\")\n\n    @staticmethod\n    def get_nodes_created_in_func(nodes: List[bpy.types.Node], created_in_func: str) -> List[bpy.types.Node]:\n        \"\"\" Returns all nodes which are created in the given function\n\n        :param nodes: list of nodes of the current material\n        :param created_in_func: return all nodes created in the given function\n        :return: The list of nodes with the given type.\n        \"\"\"\n        return [node for node in nodes if \"created_in_func\" in node and node[\"created_in_func\"] == created_in_func]\n\n    @staticmethod\n    def read_suncg_lights_windows_materials() -> Tuple[Dict[str, Tuple[List[str], List[str]]], List[str]]:\n        \"\"\"\n        Returns the lights dictionary and windows list which contains their respective materials\n\n        :return: dictionary of lights' and list of windows' materials\n        \"\"\"\n        # Read in lights\n        lights: Dict[str, Tuple[List[str], List[str]]] = {}\n        # File format: <obj id> <number of lightbulb materials> <lightbulb material names>\n        #              <number of lampshade materials> <lampshade material names>\n        with open(resolve_resource(os.path.join(\"suncg\", \"light_geometry_compact.txt\")), \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n            for row in lines:\n                row = row.strip().split()\n                lights[row[0]] = ([], [])\n\n                index = 1\n\n                # Read in lightbulb materials\n                number = int(row[index])\n                index += 1\n                for _ in range(number):\n                    lights[row[0]][0].append(row[index])\n                    index += 1\n\n                # Read in lampshade materials\n                number = int(row[index])\n                index += 1\n                for _ in range(number):\n                    lights[row[0]][1].append(row[index])\n                    index += 1\n\n        # Read in windows\n        windows = []\n        with open(resolve_resource(os.path.join('suncg', 'ModelCategoryMapping.csv')), 'r',\n                  encoding=\"utf-8\") as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                if row[\"coarse_grained_class\"] == \"window\":\n                    windows.append(row[\"model_id\"])\n\n        return lights, windows\n\n    @staticmethod\n    def generate_equidistant_values(num: int, space_size_per_dimension: int) -> Tuple[List[List[int]], int]:\n        \"\"\" This function generates N equidistant values in a 3-dim space and returns num of them.\n\n        Every dimension of the space is limited by [0, K], where K is the given space_size_per_dimension.\n        Basically it splits a cube of shape K x K x K in to N smaller blocks. Where, N = cube_length^3\n        and cube_length is the smallest integer for which N >= num.\n\n        If K is not a multiple of N, then the sum of all blocks might\n        not fill up the whole K ** 3 cube.\n\n        :param num: The total number of values required.\n        :param space_size_per_dimension: The side length of cube.\n        \"\"\"\n        num_splits_per_dimension = 1\n        values = []\n        # find cube_length bound of cubes to be made\n        while num_splits_per_dimension ** 3 < num:\n            num_splits_per_dimension += 1\n\n        # Calc the side length of a block. We do a integer division here, s.t. we get blocks with the exact same size,\n        # even though we are then not using the full space of [0, 255] ** 3\n        block_length = space_size_per_dimension // num_splits_per_dimension\n\n        # Calculate the center of each block and use them as equidistant values\n        r_mid_point = block_length // 2\n        for _ in range(num_splits_per_dimension):\n            g_mid_point = block_length // 2\n            for _ in range(num_splits_per_dimension):\n                b_mid_point = block_length // 2\n                for _ in range(num_splits_per_dimension):\n                    values.append([r_mid_point, g_mid_point, b_mid_point])\n                    b_mid_point += block_length\n                g_mid_point += block_length\n            r_mid_point += block_length\n        return values[:num], num_splits_per_dimension\n\n    @staticmethod\n    def map_back_from_equally_spaced_equidistant_values(values: np.ndarray, num_splits_per_dimension: int,\n                                                        space_size_per_dimension: int) -> np.ndarray:\n        \"\"\" Maps the given values back to their original indices.\n\n        This function calculates for each given value the corresponding index in the list of values created by the\n        generate_equidistant_values() method.\n\n        :param values: An array of shape [M, N, 3];\n        :param num_splits_per_dimension: The number of splits per dimension that were made when building up the\n                                         equidistant values.\n        :param space_size_per_dimension: The space size used for the 3D cube.\n        :return: A 2-dim array of indices corresponding to the given values.\n        \"\"\"\n        # Calc the side length of a block.\n        block_length = space_size_per_dimension // num_splits_per_dimension\n        # Subtract a half of a block from all values, s.t. now every value points to the lower corner of a block\n        values -= block_length // 2\n        # this clipping is necessary to avoid that numbers below zero are than used in an uint16\n        values = np.clip(values, 0, space_size_per_dimension)\n        # Calculate the block indices per dimension\n        values /= block_length\n        # Compute the global index of the block (corresponds to the three nested for loops inside\n        # generate_equidistant_values())\n        values = values[:, :, 0] * num_splits_per_dimension * num_splits_per_dimension + \\\n                 values[:, :, 1] * num_splits_per_dimension + values[:, :, 2]\n        # Round the values, s.t. derivations are put back to their closest index.\n        return np.round(values)\n\n    @staticmethod\n    def replace_output_entry(output: Dict[str, str]):\n        \"\"\" Replaces the output in the scene's custom properties with the given one\n\n        :param output: A dict containing key and path of the new output type.\n        \"\"\"\n        registered_outputs = Utility.get_registered_outputs()\n        for i, reg_out in enumerate(registered_outputs):\n            if output[\"key\"] == reg_out[\"key\"]:\n                registered_outputs[i] = output\n        GlobalStorage.set(\"output\", registered_outputs)\n\n    @staticmethod\n    def add_output_entry(output: Dict[str, str]):\n        \"\"\" Registers the given output in the scene's custom properties\n\n        :param output: A dict containing key and path of the new output type.\n        \"\"\"\n        if GlobalStorage.is_in_storage(\"output\"):\n            # E.g. multiple camera samplers\n            if Utility.output_already_registered(output, GlobalStorage.get(\"output\")):\n                Utility.replace_output_entry(output)\n            else:\n                GlobalStorage.get(\"output\").append(output)\n        else:\n            GlobalStorage.set(\"output\", [output])\n\n    @staticmethod\n    def register_output(output_dir: str, prefix: str, key: str, suffix: str, version: str,\n                        unique_for_camposes: bool = True):\n        \"\"\" Registers new output type using configured key and file prefix.\n\n        :param output_dir: The output directory containing the generated files.\n        :param prefix: The default prefix of the generated files.\n        :param key: The default key which should be used for storing the output in merged file.\n        :param suffix: The suffix of the generated files.\n        :param version: The version number which will be stored at key_version in the final merged file.\n        :param unique_for_camposes: True if the output to be registered is unique for all the camera poses\n        \"\"\"\n\n        Utility.add_output_entry({\n            \"key\": key,\n            \"path\": os.path.join(output_dir, prefix) + (\"%04d\" if unique_for_camposes else \"\") + suffix,\n            \"version\": version\n        })\n\n    @staticmethod\n    def find_registered_output_by_key(key: str) -> Optional[Any]:\n        \"\"\" Returns the output which was registered with the given key.\n\n        :param key: The output key to look for.\n        :return: The dict containing all information registered for that output. If no output with the given\n                 key exists, None is returned.\n        \"\"\"\n        for output in Utility.get_registered_outputs():\n            if output[\"key\"] == key:\n                return output\n\n        return None\n\n    @staticmethod\n    def get_registered_outputs() -> List[Dict[str, Any]]:\n        \"\"\" Returns a list of outputs which were registered.\n\n        :return: A list of dicts containing all information registered for the outputs.\n        \"\"\"\n        outputs = []\n        if GlobalStorage.is_in_storage(\"output\"):\n            outputs = GlobalStorage.get(\"output\")\n\n        return outputs\n\n    @staticmethod\n    def output_already_registered(output: Dict[str, Any], output_list: List[Dict[str, Any]]) -> bool:\n        \"\"\" Checks if the given output entry already exists in the list of outputs, by checking on the key and path.\n        Also throws an error if it detects an entry having the same key but not the same path and vice versa since this\n        is ambiguous.\n\n        :param output: The output dict entry.\n        :param output_list: The list of output entries.\n        :return: bool indicating whether it already exists.\n        \"\"\"\n        for _output in output_list:\n            if output[\"key\"] == _output[\"key\"] and output[\"path\"] == _output[\"path\"]:\n                print(\"Warning! Detected output entries with duplicate keys and paths\")\n                return True\n            if output[\"key\"] == _output[\"key\"] or output[\"path\"] == _output[\"path\"]:\n                raise RuntimeError(\"Can not have two output entries with the same key/path but not same path/key.\" +\n                                   f\"Original entry's data: key:{_output['key']} path:{_output['path']}, Entry to be \"\n                                   f\"registered: key:{output['key']} path:{output['path']}\")\n\n        return False\n\n    @staticmethod\n    def insert_keyframe(obj: bpy.types.Object, data_path: str, frame: Optional[int] = None):\n        \"\"\" Inserts a keyframe for the given object and data path at the specified frame number:\n\n        :param obj: The blender object to use.\n        :param data_path: The data path of the attribute.\n        :param frame: The frame number to use. If None is given, the current frame number is used.\n        \"\"\"\n        # If no frame is given use the current frame specified by the surrounding KeyFrame context manager\n        if frame is None and KeyFrame.is_any_active():\n            frame = bpy.context.scene.frame_current\n        # If no frame is given and no KeyFrame context manager surrounds us => do nothing\n        if frame is not None:\n            obj.keyframe_insert(data_path=data_path, frame=frame)",
  "class BlockStopWatch:\n    \"\"\" Calls a print statement to mark the start and end of this block and also measures execution time.\n\n    Usage: with BlockStopWatch('text'):\n    \"\"\"\n\n    def __init__(self, block_name: str):\n        self.block_name = block_name\n        self.start: float = 0.0\n\n    def __enter__(self):\n        print(f\"#### Start - {self.block_name} ####\")\n        self.start = time.time()\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_value: Optional[BaseException],\n                 traceback: Optional[TracebackType]):\n        print(f\"#### Finished - {self.block_name} (took {time.time() - self.start:.3f} seconds) ####\")",
  "class UndoAfterExecution:\n    \"\"\" Reverts all changes done to the blender project inside this block.\n\n    Usage: with UndoAfterExecution():\n    \"\"\"\n\n    def __init__(self, check_point_name: Optional[str] = None, perform_undo_op: bool = True):\n        if check_point_name is None:\n            check_point_name = inspect.stack()[1].filename + \" - \" + inspect.stack()[1].function\n        self.check_point_name = check_point_name\n        self._perform_undo_op = perform_undo_op\n        self.struct_instances: List[Tuple[str, \"Struct\"]] = []\n\n    def __enter__(self):\n        if self._perform_undo_op:\n            # Collect all existing struct instances\n            self.struct_instances = get_instances()\n            bpy.ops.ed.undo_push(message=\"before \" + self.check_point_name)\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_value: Optional[BaseException],\n                 traceback: Optional[TracebackType]):\n        if self._perform_undo_op:\n            bpy.ops.ed.undo_push(message=\"after \" + self.check_point_name)\n            # The current state points to \"after\", now by calling undo we go back to \"before\"\n            bpy.ops.ed.undo()\n            # After applying undo, all references to blender objects are invalid.\n            # Therefore, we now go over all instances and update their references using their name as unique identifier.\n            for name, struct in self.struct_instances:\n                struct.update_blender_ref(name)",
  "class _KeyFrameState(threading.local):\n    \"\"\"\n    This class is only used in the KeyFrame class\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.depth = 0",
  "class KeyFrame:\n    \"\"\"\n    A content manager for setting the frame correctly.\n    \"\"\"\n    # Remember how many KeyFrame context manager have been applied around the current execution point\n    state = _KeyFrameState()\n\n    def __init__(self, frame: int):\n        \"\"\" Sets the frame number for its complete block.\n\n        :param frame: The frame number to set. If None is given, nothing is changed.\n        \"\"\"\n        self._frame = frame\n        self._prev_frame = None\n\n    def __enter__(self):\n        KeyFrame.state.depth += 1\n        if self._frame is not None:\n            self._prev_frame = bpy.context.scene.frame_current\n            bpy.context.scene.frame_set(self._frame)\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_value: Optional[BaseException],\n                 traceback: Optional[TracebackType]):\n        KeyFrame.state.depth -= 1\n        if self._prev_frame is not None:\n            bpy.context.scene.frame_set(self._prev_frame)\n\n    @staticmethod\n    def is_any_active() -> bool:\n        \"\"\" Returns whether the current execution point is surrounded by a KeyFrame context manager.\n\n        :return: True, if there is at least one surrounding KeyFrame context manager\n        \"\"\"\n        return KeyFrame.state.depth > 0",
  "class NumpyEncoder(json.JSONEncoder):\n    \"\"\" A json encoder that is also capable of serializing numpy arrays \"\"\"\n\n    def default(self, o: Any):\n        # If its a numpy array\n        if isinstance(o, np.ndarray):\n            # Convert it to a list\n            return o.tolist()\n        return json.JSONEncoder.default(self, o)",
  "def get_file_descriptor(file_or_fd: Union[int, IO]) -> int:\n    \"\"\" Returns the file descriptor of the given file.\n\n    :param file_or_fd: Either a file or a file descriptor. If a file descriptor is given, it is returned directly.\n    :return: The file descriptor of the given file.\n    \"\"\"\n    if hasattr(file_or_fd, 'fileno'):\n        fd = file_or_fd.fileno()\n    else:\n        fd = file_or_fd\n    if not isinstance(fd, int):\n        raise AttributeError(\"Expected a file (`.fileno()`) or a file descriptor\")\n    return fd",
  "def stdout_redirected(to: Union[int, IO, str] = os.devnull, enabled: bool = True) -> IO:\n    \"\"\" Redirects all stdout to the given file.\n\n    From https://stackoverflow.com/a/22434262.\n\n    :param to: The file which should be the new target for stdout. Can be a path, file or file descriptor.\n    :param enabled: If False, then this context manager does nothing.\n    :return: The old stdout output.\n    \"\"\"\n    if enabled:\n        stdout = sys.stdout\n        stdout_fd = get_file_descriptor(stdout)\n        # copy stdout_fd before it is overwritten\n        # NOTE: `copied` is inheritable on Windows when duplicating a standard stream\n        with os.fdopen(os.dup(stdout_fd), 'w') as copied:\n            stdout.flush()  # flush library buffers that dup2 knows nothing about\n            try:\n                os.dup2(get_file_descriptor(to), stdout_fd)  # $ exec >&to\n            except AttributeError:  # filename\n                with open(to, 'wb') as to_file:\n                    os.dup2(to_file.fileno(), stdout_fd)  # $ exec > to\n            try:\n                yield copied\n            finally:\n                # restore stdout to its previous value\n                # NOTE: dup2 makes stdout_fd inheritable unconditionally\n                stdout.flush()\n                os.dup2(copied.fileno(), stdout_fd)  # $ exec >&copied\n    else:\n        yield sys.stdout",
  "def get_current_version() -> Optional[str]:\n        \"\"\" Gets the current blenderproc version.\n\n        :return: a string, the BlenderProc version\n        \"\"\"\n        return __version__",
  "def get_temporary_directory() -> str:\n        \"\"\"\n        :return: default temporary directory, shared memory if it exists\n        \"\"\"\n        return Utility.temp_dir",
  "def merge_dicts(source: Dict[Any, Any], destination: Dict[Any, Any]) -> Dict[Any, Any]:\n        \"\"\" Recursively copies all key value pairs from src to dest (Overwrites existing)\n\n        :param source: The source dict.\n        :param destination: The destination dict\n        :return: The modified destination dict.\n        \"\"\"\n        for key, value in source.items():\n            if isinstance(value, dict):\n                # get node or create one\n                node = destination.setdefault(key, {})\n                Utility.merge_dicts(value, node)\n            else:\n                destination[key] = value\n\n        return destination",
  "def hex_to_rgba(hex_value: str) -> List[float]:\n        \"\"\" Converts the given hex string to rgba color values.\n\n        :param hex_value: The hex string, describing rgb.\n        :return: The rgba color, in form of a list. Values between 0 and 1.\n        \"\"\"\n        return [x / 255 for x in bytes.fromhex(hex_value[-6:])] + [1.0]",
  "def rgb_to_hex(rgb: Tuple[int, int, int]) -> str:\n        \"\"\" Converts the given rgb to hex values.\n\n        :param rgb: tuple of three with rgb integers.\n        :return: Hex string.\n        \"\"\"\n        if len(rgb) != 3:\n            raise ValueError(f\"The given rgb has to have 3 values: {rgb}\")\n\n        return f\"#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}\"",
  "def insert_node_instead_existing_link(links: bpy.types.NodeLinks, source_socket: bpy.types.NodeSocket,\n                                          new_node_dest_socket: bpy.types.NodeSocket,\n                                          new_node_src_socket: bpy.types.NodeSocket, dest_socket: bpy.types.NodeSocket):\n        \"\"\" Replaces the node between source_socket and dest_socket with a new node.\n\n        Before: source_socket -> dest_socket\n        After: source_socket -> new_node_dest_socket and new_node_src_socket -> dest_socket\n\n        :param links: The collection of all links.\n        :param source_socket: The source socket.\n        :param new_node_dest_socket: The new destination for the link starting from source_socket.\n        :param new_node_src_socket: The new source for the link towards dest_socket.\n        :param dest_socket: The destination socket\n        \"\"\"\n        for l in links:\n            if l.from_socket == source_socket or l.to_socket == dest_socket:\n                links.remove(l)\n\n        links.new(source_socket, new_node_dest_socket)\n        links.new(new_node_src_socket, dest_socket)",
  "def get_node_connected_to_the_output_and_unlink_it(material: bpy.types.Material) \\\n            -> Tuple[Optional[bpy.types.Node], bpy.types.Node]:\n        \"\"\"\n        Searches for the OutputMaterial in the given material and finds the connected node to it,\n        removes the connection between this node and the output and returns this node and the material_output\n\n        :param material: Material on which this operation should be performed\n        \"\"\"\n        nodes = material.node_tree.nodes\n        links = material.node_tree.links\n\n        material_output = Utility.get_the_one_node_with_type(nodes, 'OutputMaterial')\n        # find the node, which is connected to the output\n        node_connected_to_the_output = None\n        for link in links:\n            if link.to_node == material_output:\n                node_connected_to_the_output = link.from_node\n                # remove this link\n                links.remove(link)\n                break\n        return node_connected_to_the_output, material_output",
  "def get_nodes_with_type(nodes: List[bpy.types.Node], node_type: str,\n                            created_in_func: Optional[str] = None) -> List[bpy.types.Node]:\n        \"\"\"\n        Returns all nodes which are of the given node_type\n\n        :param nodes: list of nodes of the current material\n        :param node_type: node types\n        :param created_in_func: Only return nodes created by the specified function\n        :return: list of nodes, which belong to the type\n        \"\"\"\n        nodes_with_type = [node for node in nodes if node_type in node.bl_idname]\n        if created_in_func:\n            nodes_with_type = Utility.get_nodes_created_in_func(nodes_with_type, created_in_func)\n        return nodes_with_type",
  "def get_the_one_node_with_type(nodes: List[bpy.types.Node], node_type: str,\n                                   created_in_func: str = \"\") -> bpy.types.Node:\n        \"\"\"\n        Returns the one node which is of the given node_type\n\n        This function will only work if there is only one of the nodes of this type.\n\n        :param nodes: list of nodes of the current material\n        :param node_type: node types\n        :param created_in_func: only return node created by the specified function\n        :return: node of the node type\n        \"\"\"\n        node = Utility.get_nodes_with_type(nodes, node_type, created_in_func)\n        if node and len(node) == 1:\n            return node[0]\n        raise RuntimeError(f\"There is not only one node of this type: {node_type}, there are: {len(node)}\")",
  "def get_nodes_created_in_func(nodes: List[bpy.types.Node], created_in_func: str) -> List[bpy.types.Node]:\n        \"\"\" Returns all nodes which are created in the given function\n\n        :param nodes: list of nodes of the current material\n        :param created_in_func: return all nodes created in the given function\n        :return: The list of nodes with the given type.\n        \"\"\"\n        return [node for node in nodes if \"created_in_func\" in node and node[\"created_in_func\"] == created_in_func]",
  "def read_suncg_lights_windows_materials() -> Tuple[Dict[str, Tuple[List[str], List[str]]], List[str]]:\n        \"\"\"\n        Returns the lights dictionary and windows list which contains their respective materials\n\n        :return: dictionary of lights' and list of windows' materials\n        \"\"\"\n        # Read in lights\n        lights: Dict[str, Tuple[List[str], List[str]]] = {}\n        # File format: <obj id> <number of lightbulb materials> <lightbulb material names>\n        #              <number of lampshade materials> <lampshade material names>\n        with open(resolve_resource(os.path.join(\"suncg\", \"light_geometry_compact.txt\")), \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n            for row in lines:\n                row = row.strip().split()\n                lights[row[0]] = ([], [])\n\n                index = 1\n\n                # Read in lightbulb materials\n                number = int(row[index])\n                index += 1\n                for _ in range(number):\n                    lights[row[0]][0].append(row[index])\n                    index += 1\n\n                # Read in lampshade materials\n                number = int(row[index])\n                index += 1\n                for _ in range(number):\n                    lights[row[0]][1].append(row[index])\n                    index += 1\n\n        # Read in windows\n        windows = []\n        with open(resolve_resource(os.path.join('suncg', 'ModelCategoryMapping.csv')), 'r',\n                  encoding=\"utf-8\") as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                if row[\"coarse_grained_class\"] == \"window\":\n                    windows.append(row[\"model_id\"])\n\n        return lights, windows",
  "def generate_equidistant_values(num: int, space_size_per_dimension: int) -> Tuple[List[List[int]], int]:\n        \"\"\" This function generates N equidistant values in a 3-dim space and returns num of them.\n\n        Every dimension of the space is limited by [0, K], where K is the given space_size_per_dimension.\n        Basically it splits a cube of shape K x K x K in to N smaller blocks. Where, N = cube_length^3\n        and cube_length is the smallest integer for which N >= num.\n\n        If K is not a multiple of N, then the sum of all blocks might\n        not fill up the whole K ** 3 cube.\n\n        :param num: The total number of values required.\n        :param space_size_per_dimension: The side length of cube.\n        \"\"\"\n        num_splits_per_dimension = 1\n        values = []\n        # find cube_length bound of cubes to be made\n        while num_splits_per_dimension ** 3 < num:\n            num_splits_per_dimension += 1\n\n        # Calc the side length of a block. We do a integer division here, s.t. we get blocks with the exact same size,\n        # even though we are then not using the full space of [0, 255] ** 3\n        block_length = space_size_per_dimension // num_splits_per_dimension\n\n        # Calculate the center of each block and use them as equidistant values\n        r_mid_point = block_length // 2\n        for _ in range(num_splits_per_dimension):\n            g_mid_point = block_length // 2\n            for _ in range(num_splits_per_dimension):\n                b_mid_point = block_length // 2\n                for _ in range(num_splits_per_dimension):\n                    values.append([r_mid_point, g_mid_point, b_mid_point])\n                    b_mid_point += block_length\n                g_mid_point += block_length\n            r_mid_point += block_length\n        return values[:num], num_splits_per_dimension",
  "def map_back_from_equally_spaced_equidistant_values(values: np.ndarray, num_splits_per_dimension: int,\n                                                        space_size_per_dimension: int) -> np.ndarray:\n        \"\"\" Maps the given values back to their original indices.\n\n        This function calculates for each given value the corresponding index in the list of values created by the\n        generate_equidistant_values() method.\n\n        :param values: An array of shape [M, N, 3];\n        :param num_splits_per_dimension: The number of splits per dimension that were made when building up the\n                                         equidistant values.\n        :param space_size_per_dimension: The space size used for the 3D cube.\n        :return: A 2-dim array of indices corresponding to the given values.\n        \"\"\"\n        # Calc the side length of a block.\n        block_length = space_size_per_dimension // num_splits_per_dimension\n        # Subtract a half of a block from all values, s.t. now every value points to the lower corner of a block\n        values -= block_length // 2\n        # this clipping is necessary to avoid that numbers below zero are than used in an uint16\n        values = np.clip(values, 0, space_size_per_dimension)\n        # Calculate the block indices per dimension\n        values /= block_length\n        # Compute the global index of the block (corresponds to the three nested for loops inside\n        # generate_equidistant_values())\n        values = values[:, :, 0] * num_splits_per_dimension * num_splits_per_dimension + \\\n                 values[:, :, 1] * num_splits_per_dimension + values[:, :, 2]\n        # Round the values, s.t. derivations are put back to their closest index.\n        return np.round(values)",
  "def replace_output_entry(output: Dict[str, str]):\n        \"\"\" Replaces the output in the scene's custom properties with the given one\n\n        :param output: A dict containing key and path of the new output type.\n        \"\"\"\n        registered_outputs = Utility.get_registered_outputs()\n        for i, reg_out in enumerate(registered_outputs):\n            if output[\"key\"] == reg_out[\"key\"]:\n                registered_outputs[i] = output\n        GlobalStorage.set(\"output\", registered_outputs)",
  "def add_output_entry(output: Dict[str, str]):\n        \"\"\" Registers the given output in the scene's custom properties\n\n        :param output: A dict containing key and path of the new output type.\n        \"\"\"\n        if GlobalStorage.is_in_storage(\"output\"):\n            # E.g. multiple camera samplers\n            if Utility.output_already_registered(output, GlobalStorage.get(\"output\")):\n                Utility.replace_output_entry(output)\n            else:\n                GlobalStorage.get(\"output\").append(output)\n        else:\n            GlobalStorage.set(\"output\", [output])",
  "def register_output(output_dir: str, prefix: str, key: str, suffix: str, version: str,\n                        unique_for_camposes: bool = True):\n        \"\"\" Registers new output type using configured key and file prefix.\n\n        :param output_dir: The output directory containing the generated files.\n        :param prefix: The default prefix of the generated files.\n        :param key: The default key which should be used for storing the output in merged file.\n        :param suffix: The suffix of the generated files.\n        :param version: The version number which will be stored at key_version in the final merged file.\n        :param unique_for_camposes: True if the output to be registered is unique for all the camera poses\n        \"\"\"\n\n        Utility.add_output_entry({\n            \"key\": key,\n            \"path\": os.path.join(output_dir, prefix) + (\"%04d\" if unique_for_camposes else \"\") + suffix,\n            \"version\": version\n        })",
  "def find_registered_output_by_key(key: str) -> Optional[Any]:\n        \"\"\" Returns the output which was registered with the given key.\n\n        :param key: The output key to look for.\n        :return: The dict containing all information registered for that output. If no output with the given\n                 key exists, None is returned.\n        \"\"\"\n        for output in Utility.get_registered_outputs():\n            if output[\"key\"] == key:\n                return output\n\n        return None",
  "def get_registered_outputs() -> List[Dict[str, Any]]:\n        \"\"\" Returns a list of outputs which were registered.\n\n        :return: A list of dicts containing all information registered for the outputs.\n        \"\"\"\n        outputs = []\n        if GlobalStorage.is_in_storage(\"output\"):\n            outputs = GlobalStorage.get(\"output\")\n\n        return outputs",
  "def output_already_registered(output: Dict[str, Any], output_list: List[Dict[str, Any]]) -> bool:\n        \"\"\" Checks if the given output entry already exists in the list of outputs, by checking on the key and path.\n        Also throws an error if it detects an entry having the same key but not the same path and vice versa since this\n        is ambiguous.\n\n        :param output: The output dict entry.\n        :param output_list: The list of output entries.\n        :return: bool indicating whether it already exists.\n        \"\"\"\n        for _output in output_list:\n            if output[\"key\"] == _output[\"key\"] and output[\"path\"] == _output[\"path\"]:\n                print(\"Warning! Detected output entries with duplicate keys and paths\")\n                return True\n            if output[\"key\"] == _output[\"key\"] or output[\"path\"] == _output[\"path\"]:\n                raise RuntimeError(\"Can not have two output entries with the same key/path but not same path/key.\" +\n                                   f\"Original entry's data: key:{_output['key']} path:{_output['path']}, Entry to be \"\n                                   f\"registered: key:{output['key']} path:{output['path']}\")\n\n        return False",
  "def insert_keyframe(obj: bpy.types.Object, data_path: str, frame: Optional[int] = None):\n        \"\"\" Inserts a keyframe for the given object and data path at the specified frame number:\n\n        :param obj: The blender object to use.\n        :param data_path: The data path of the attribute.\n        :param frame: The frame number to use. If None is given, the current frame number is used.\n        \"\"\"\n        # If no frame is given use the current frame specified by the surrounding KeyFrame context manager\n        if frame is None and KeyFrame.is_any_active():\n            frame = bpy.context.scene.frame_current\n        # If no frame is given and no KeyFrame context manager surrounds us => do nothing\n        if frame is not None:\n            obj.keyframe_insert(data_path=data_path, frame=frame)",
  "def __init__(self, block_name: str):\n        self.block_name = block_name\n        self.start: float = 0.0",
  "def __enter__(self):\n        print(f\"#### Start - {self.block_name} ####\")\n        self.start = time.time()",
  "def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_value: Optional[BaseException],\n                 traceback: Optional[TracebackType]):\n        print(f\"#### Finished - {self.block_name} (took {time.time() - self.start:.3f} seconds) ####\")",
  "def __init__(self, check_point_name: Optional[str] = None, perform_undo_op: bool = True):\n        if check_point_name is None:\n            check_point_name = inspect.stack()[1].filename + \" - \" + inspect.stack()[1].function\n        self.check_point_name = check_point_name\n        self._perform_undo_op = perform_undo_op\n        self.struct_instances: List[Tuple[str, \"Struct\"]] = []",
  "def __enter__(self):\n        if self._perform_undo_op:\n            # Collect all existing struct instances\n            self.struct_instances = get_instances()\n            bpy.ops.ed.undo_push(message=\"before \" + self.check_point_name)",
  "def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_value: Optional[BaseException],\n                 traceback: Optional[TracebackType]):\n        if self._perform_undo_op:\n            bpy.ops.ed.undo_push(message=\"after \" + self.check_point_name)\n            # The current state points to \"after\", now by calling undo we go back to \"before\"\n            bpy.ops.ed.undo()\n            # After applying undo, all references to blender objects are invalid.\n            # Therefore, we now go over all instances and update their references using their name as unique identifier.\n            for name, struct in self.struct_instances:\n                struct.update_blender_ref(name)",
  "def __init__(self):\n        super().__init__()\n        self.depth = 0",
  "def __init__(self, frame: int):\n        \"\"\" Sets the frame number for its complete block.\n\n        :param frame: The frame number to set. If None is given, nothing is changed.\n        \"\"\"\n        self._frame = frame\n        self._prev_frame = None",
  "def __enter__(self):\n        KeyFrame.state.depth += 1\n        if self._frame is not None:\n            self._prev_frame = bpy.context.scene.frame_current\n            bpy.context.scene.frame_set(self._frame)",
  "def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_value: Optional[BaseException],\n                 traceback: Optional[TracebackType]):\n        KeyFrame.state.depth -= 1\n        if self._prev_frame is not None:\n            bpy.context.scene.frame_set(self._prev_frame)",
  "def is_any_active() -> bool:\n        \"\"\" Returns whether the current execution point is surrounded by a KeyFrame context manager.\n\n        :return: True, if there is at least one surrounding KeyFrame context manager\n        \"\"\"\n        return KeyFrame.state.depth > 0",
  "def default(self, o: Any):\n        # If its a numpy array\n        if isinstance(o, np.ndarray):\n            # Convert it to a list\n            return o.tolist()\n        return json.JSONEncoder.default(self, o)",
  "def change_coordinate_frame_of_point(point: Union[np.ndarray, List[float], Vector],\n                                     new_frame: List[str]) -> np.ndarray:\n    \"\"\" Transforms the given point into another coordinate frame.\n\n    Example: [1, 2, 3] and [\"X\", \"-Z\", \"Y\"] => [1, -3, 2]\n\n    :param point: The point to convert in form of a np.ndarray, list or mathutils.Vector.\n    :param new_frame: An array containing three elements, describing each axis of the new coordinate frame\n                      based on the axes of the current frame. Available: [\"X\", \"Y\", \"Z\", \"-X\", \"-Y\", \"-Z\"].\n    :return: The converted point also in form of a np.ndarray\n    \"\"\"\n    assert len(new_frame) == 3, f\"The specified coordinate frame has more or less than tree axes: {new_frame}\"\n    point = np.array(point)\n\n    output = []\n    for axis in new_frame:\n        axis = axis.upper()\n\n        if axis.endswith(\"X\"):\n            output.append(point[0])\n        elif axis.endswith(\"Y\"):\n            output.append(point[1])\n        elif axis.endswith(\"Z\"):\n            output.append(point[2])\n        else:\n            raise ValueError(f\"Invalid axis: {axis}\")\n\n        if axis.startswith(\"-\"):\n            output[-1] *= -1\n\n    return np.array(output)",
  "def change_target_coordinate_frame_of_transformation_matrix(matrix: Union[np.ndarray, Matrix],\n                                                            new_frame: List[str]) -> np.ndarray:\n    \"\"\" Changes the coordinate frame the given transformation matrix is mapping to.\n\n    Given a matrix $T_A^B$ that maps from A to B, this function can be used\n    to change the axes of B into B' and therefore end up with $T_A^B'$.\n\n    :param matrix: The matrix to convert in form of a np.ndarray or mathutils.Matrix\n    :param new_frame: An array containing three elements, describing each axis of the new coordinate frame\n                      based on the axes of the current frame. Available: [\"X\", \"Y\", \"Z\", \"-X\", \"-Y\", \"-Z\"].\n    :return: The converted matrix is in form of a np.ndarray\n    \"\"\"\n    tmat = MathUtility.build_coordinate_frame_changing_transformation_matrix(new_frame)\n\n    # Apply transformation matrix\n    output = np.matmul(tmat, matrix)\n    return output",
  "def change_source_coordinate_frame_of_transformation_matrix(matrix: Union[np.ndarray, Matrix],\n                                                            new_frame: list) -> np.ndarray:\n    \"\"\" Changes the coordinate frame the given transformation matrix is mapping from.\n\n    Given a matrix $T_A^B$ that maps from A to B, this function can be used\n    to change the axes of A into A' and therefore end up with $T_A'^B$.\n\n    :param matrix: The matrix to convert in form of a np.ndarray or mathutils.Matrix\n    :param new_frame: An array containing three elements, describing each axis of the new coordinate frame\n                      based on the axes of the current frame. Available: [\"X\", \"Y\", \"Z\", \"-X\", \"-Y\", \"-Z\"].\n    :return: The converted matrix is in form of a np.ndarray\n    \"\"\"\n    tmat = MathUtility.build_coordinate_frame_changing_transformation_matrix(new_frame)\n    tmat = np.linalg.inv(tmat)\n\n    # Apply transformation matrix\n    output = np.matmul(matrix, tmat)\n    return output",
  "def build_transformation_mat(translation: Union[np.ndarray, List[float], Vector],\n                             rotation: Union[np.ndarray, List[List[float]], Matrix]) -> np.ndarray:\n    \"\"\" Build a transformation matrix from translation and rotation parts.\n\n    :param translation: A (3,) vector representing the translation part.\n    :param rotation: A 3x3 rotation matrix or Euler angles of shape (3,).\n    :return: The 4x4 transformation matrix.\n    \"\"\"\n    translation = np.array(translation)\n    rotation = np.array(rotation)\n\n    mat = np.eye(4)\n    if translation.shape[0] == 3:\n        mat[:3, 3] = translation\n    else:\n        raise RuntimeError(f\"Translation has invalid shape: {translation.shape}. Must be (3,) or (3,1) vector.\")\n    if rotation.shape == (3, 3):\n        mat[:3, :3] = rotation\n    elif rotation.shape[0] == 3:\n        mat[:3, :3] = np.array(Euler(rotation).to_matrix())\n    else:\n        raise RuntimeError(f\"Rotation has invalid shape: {rotation.shape}. Must be rotation matrix of shape \"\n                           f\"(3,3) or Euler angles of shape (3,) or (3,1).\")\n\n    return mat",
  "class MathUtility:\n    \"\"\"\n    Math utility class\n    \"\"\"\n\n    @staticmethod\n    def build_coordinate_frame_changing_transformation_matrix(destination_frame: List[str]) -> np.ndarray:\n        \"\"\" Builds a transformation matrix that switches the coordinate frame.\n\n        :param destination_frame: An array containing three elements, describing each axis of the destination\n                                  coordinate frame based on the axes of the source frame.\n                                  Available: [\"X\", \"Y\", \"Z\", \"-X\", \"-Y\", \"-Z\"].\n        :return: The transformation matrix\n        \"\"\"\n        assert len(destination_frame) == 3, f\"The specified coordinate frame has more or less than \" \\\n                                            f\"tree axes: {destination_frame}\"\n\n        # Build transformation matrix that maps the given matrix to the specified coordinate frame.\n        tmat = np.zeros((4, 4))\n        for i, axis in enumerate(destination_frame):\n            axis = axis.upper()\n\n            if axis.endswith(\"X\"):\n                tmat[i, 0] = 1\n            elif axis.endswith(\"Y\"):\n                tmat[i, 1] = 1\n            elif axis.endswith(\"Z\"):\n                tmat[i, 2] = 1\n            else:\n                raise Exception(\"Invalid axis: \" + axis)\n\n            if axis.startswith(\"-\"):\n                tmat[i] *= -1\n        tmat[3, 3] = 1\n        return tmat",
  "def build_coordinate_frame_changing_transformation_matrix(destination_frame: List[str]) -> np.ndarray:\n        \"\"\" Builds a transformation matrix that switches the coordinate frame.\n\n        :param destination_frame: An array containing three elements, describing each axis of the destination\n                                  coordinate frame based on the axes of the source frame.\n                                  Available: [\"X\", \"Y\", \"Z\", \"-X\", \"-Y\", \"-Z\"].\n        :return: The transformation matrix\n        \"\"\"\n        assert len(destination_frame) == 3, f\"The specified coordinate frame has more or less than \" \\\n                                            f\"tree axes: {destination_frame}\"\n\n        # Build transformation matrix that maps the given matrix to the specified coordinate frame.\n        tmat = np.zeros((4, 4))\n        for i, axis in enumerate(destination_frame):\n            axis = axis.upper()\n\n            if axis.endswith(\"X\"):\n                tmat[i, 0] = 1\n            elif axis.endswith(\"Y\"):\n                tmat[i, 1] = 1\n            elif axis.endswith(\"Z\"):\n                tmat[i, 2] = 1\n            else:\n                raise Exception(\"Invalid axis: \" + axis)\n\n            if axis.startswith(\"-\"):\n                tmat[i] *= -1\n        tmat[3, 3] = 1\n        return tmat",
  "class CollisionUtility:\n    \"\"\"\n    This class provides utility functions to check if two objects intersect with each other.\n    \"\"\"\n\n    @staticmethod\n    def check_intersections(obj: MeshObject, bvh_cache: Optional[Dict[str, mathutils.bvhtree.BVHTree]],\n                            objects_to_check_against: List[MeshObject],\n                            list_of_objects_with_no_inside_check: List[MeshObject]):\n        \"\"\" Checks if an object intersects with any object given in the list.\n\n        The bvh_cache adds all current objects to the bvh tree, which increases the speed.\n\n        If an object is already in the cache it is removed, before performing the check.\n\n        :param obj: Object which should be checked. Type: :class:`bpy.types.Object`\n        :param bvh_cache: Dict of all the bvh trees, removes the `obj` from the cache before adding it again. \\\n                          Type: :class:`dict`\n        :param objects_to_check_against: List of objects which the object is checked again \\\n                                         Type: :class:`list`\n        :param list_of_objects_with_no_inside_check: List of objects on which no inside check is performed. \\\n                                                     This check is only done for the objects in \\\n                                                     `objects_to_check_against`. Type: :class:`list`\n        :return: Type: :class:`bool`, True if no collision was found, false if at least one collision was found\n        \"\"\"\n\n        no_collision = True\n        # Now check for collisions\n        for collision_obj in objects_to_check_against:\n            # Do not check collisions with yourself\n            if collision_obj == obj:\n                continue\n            # First check if bounding boxes collides\n            intersection = CollisionUtility.check_bb_intersection(obj, collision_obj)\n            # if they do\n            if intersection:\n                skip_inside_check = collision_obj in list_of_objects_with_no_inside_check\n                # then check for more refined collisions\n                intersection, bvh_cache = CollisionUtility.check_mesh_intersection(obj, collision_obj,\n                                                                                   bvh_cache=bvh_cache,\n                                                                                   skip_inside_check=skip_inside_check)\n            if intersection:\n                no_collision = False\n                break\n        return no_collision\n\n\n    @staticmethod\n    def check_bb_intersection(obj1: MeshObject, obj2: MeshObject):\n        \"\"\"\n        Checks if there is a bounding box collision, these don't have to be axis-aligned, but if they are not:\n        The surrounding/including axis-aligned bounding box is calculated and used to check the intersection.\n\n        :param obj1: object 1  to check for intersection, must be a mesh\n        :param obj2: object 2  to check for intersection, must be a mesh\n        :return: True if the two bounding boxes intersect with each other\n        \"\"\"\n        b1w = obj1.get_bound_box()\n\n        def min_and_max_point(bb):\n            \"\"\"\n            Find the minimum and maximum point of the bounding box\n            :param bb: bounding box\n            :return: min, max\n            \"\"\"\n            values = np.array(bb)\n            return np.min(values, axis=0), np.max(values, axis=0)\n\n        # get min and max point of the axis-aligned bounding box\n        min_b1, max_b1 = min_and_max_point(b1w)\n        b2w = obj2.get_bound_box()\n        # get min and max point of the axis-aligned bounding box\n        min_b2, max_b2 = min_and_max_point(b2w)\n        return CollisionUtility.check_bb_intersection_on_values(min_b1, max_b1, min_b2, max_b2)\n\n    @staticmethod\n    def check_bb_intersection_on_values(min_b1: List[float], max_b1: List[float], min_b2: List[float],\n                                        max_b2: List[float],\n                                        used_check: Callable[[float, float], bool] = lambda a, b: a >= b):\n        \"\"\"\n        Checks if there is an intersection of the given bounding box values. Here we use two different bounding boxes,\n        namely b1 and b2. Each of them has a corresponding set of min and max values, this works for 2 and 3 dimensional\n        problems.\n\n        :param min_b1: List of minimum bounding box points for b1.\n        :param max_b1: List of maximum bounding box points for b1.\n        :param min_b2: List of minimum bounding box points for b2.\n        :param max_b2: List of maximum bounding box points for b2.\n        :param used_check: The operation used inside the is_overlapping1D. With that it possible to change the \\\n                           collision check from volume and surface check to pure surface or volume checks.\n        :return: True if the two bounding boxes intersect with each other\n        \"\"\"\n        collide = True\n        for min_b1_val, max_b1_val, min_b2_val, max_b2_val in zip(min_b1, max_b1, min_b2, max_b2):\n            # inspired by this:\n            # https://stackoverflow.com/questions/20925818/algorithm-to-check-if-two-boxes-overlap\n            # Checks in each dimension, if there is an overlap if this happens it must be an overlap in 3D, too.\n            def is_overlapping_1D(x_min_1, x_max_1, x_min_2, x_max_2):\n                # returns true if the min and max values are overlapping\n                return used_check(x_max_1, x_min_2) and used_check(x_max_2, x_min_1)\n\n            collide = collide and is_overlapping_1D(min_b1_val, max_b1_val, min_b2_val, max_b2_val)\n        return collide\n\n    @staticmethod\n    def check_mesh_intersection(obj1: MeshObject, obj2: MeshObject, skip_inside_check: bool = False,\n                                bvh_cache: Optional[Dict[str, mathutils.bvhtree.BVHTree]] = None) \\\n            -> Tuple[bool, Dict[str, mathutils.bvhtree.BVHTree]]:\n        \"\"\"\n        Checks if the two objects are intersecting.\n\n        This will use BVH trees to check whether the objects are overlapping.\n\n        It is further also checked if one object is completely inside the other.\n        This check requires that both objects are watertight, have correct normals and are coherent.\n        If this is not the case it can be disabled via the parameter skip_inside_check.\n\n        :param obj1: object 1 to check for intersection, must be a mesh\n        :param obj2: object 2 to check for intersection, must be a mesh\n        :param skip_inside_check: Disables checking whether one object is completely inside the other.\n        :param bvh_cache: Dict of all the bvh trees, removes the `obj` from the cache before adding it again.\n        :return: True, if they are intersecting\n        \"\"\"\n\n        if bvh_cache is None:\n            bvh_cache = {}\n\n        # If one of the objects has no vertices, collision is impossible\n        if len(obj1.get_mesh().vertices) == 0 or len(obj2.get_mesh().vertices) == 0:\n            return False, bvh_cache\n\n        # create bvhtree for obj1\n        if obj1.get_name() not in bvh_cache:\n            obj1_BVHtree = obj1.create_bvh_tree()\n            bvh_cache[obj1.get_name()] = obj1_BVHtree\n        else:\n            obj1_BVHtree = bvh_cache[obj1.get_name()]\n\n        # create bvhtree for obj2\n        if obj2.get_name() not in bvh_cache:\n            obj2_BVHtree = obj2.create_bvh_tree()\n            bvh_cache[obj2.get_name()] = obj2_BVHtree\n        else:\n            obj2_BVHtree = bvh_cache[obj2.get_name()]\n\n        # Check whether both meshes intersect\n        inter = len(obj1_BVHtree.overlap(obj2_BVHtree)) > 0\n\n        # Optionally check whether obj2 is contained in obj1\n        if not inter and not skip_inside_check:\n            inter = CollisionUtility.is_point_inside_object(obj1, obj1_BVHtree,\n                                                            Matrix(obj2.get_local2world_mat()) @\n                                                            obj2.get_mesh().vertices[0].co)\n            if inter:\n                print(\"Warning: Detected that \" + obj2.get_name() + \" is completely inside \" + obj1.get_name() +\n                      \". This might be wrong, if \" + obj1.get_name() +\n                      \" is not water tight or has incorrect normals. If that is the case, consider setting \"\n                      \"skip_inside_check to True.\")\n\n        # Optionally check whether obj1 is contained in obj2\n        if not inter and not skip_inside_check:\n            inter = CollisionUtility.is_point_inside_object(obj2, obj2_BVHtree, Matrix(obj1.get_local2world_mat())\n                                                            @ obj1.get_mesh().vertices[0].co)\n            if inter:\n                print(\"Warning: Detected that \" + obj1.get_name() + \" is completely inside \" + obj2.get_name() +\n                      \". This might be wrong, if \" + obj2.get_name() + \" is not water tight or has incorrect \"\n                                                                       \"normals. If that is the case, consider \"\n                                                                       \"setting skip_inside_check to True.\")\n\n        return inter, bvh_cache\n\n    @staticmethod\n    def is_point_inside_object(obj: MeshObject, obj_bvh_tree: mathutils.bvhtree.BVHTree,\n                               point: Union[Vector, np.ndarray]) -> bool:\n        \"\"\" Checks whether the given point is inside the given object.\n\n        This only works if the given object is watertight and has correct normals\n\n        :param obj: The object\n        :param obj_bvh_tree: A bvh tree of the object\n        :param point: The point to check\n        :return: True, if the point is inside the object\n        \"\"\"\n        point = Vector(point)\n        # Look for closest point on object\n        nearest, normal, _, _ = obj_bvh_tree.find_nearest(point)\n        # Compute direction\n        p2 = nearest - point\n        # Compute dot product between direction and normal vector\n        a = p2.normalized().dot((Euler(obj.get_rotation_euler()).to_matrix() @ normal).normalized())\n        return a >= 0.0",
  "def check_intersections(obj: MeshObject, bvh_cache: Optional[Dict[str, mathutils.bvhtree.BVHTree]],\n                            objects_to_check_against: List[MeshObject],\n                            list_of_objects_with_no_inside_check: List[MeshObject]):\n        \"\"\" Checks if an object intersects with any object given in the list.\n\n        The bvh_cache adds all current objects to the bvh tree, which increases the speed.\n\n        If an object is already in the cache it is removed, before performing the check.\n\n        :param obj: Object which should be checked. Type: :class:`bpy.types.Object`\n        :param bvh_cache: Dict of all the bvh trees, removes the `obj` from the cache before adding it again. \\\n                          Type: :class:`dict`\n        :param objects_to_check_against: List of objects which the object is checked again \\\n                                         Type: :class:`list`\n        :param list_of_objects_with_no_inside_check: List of objects on which no inside check is performed. \\\n                                                     This check is only done for the objects in \\\n                                                     `objects_to_check_against`. Type: :class:`list`\n        :return: Type: :class:`bool`, True if no collision was found, false if at least one collision was found\n        \"\"\"\n\n        no_collision = True\n        # Now check for collisions\n        for collision_obj in objects_to_check_against:\n            # Do not check collisions with yourself\n            if collision_obj == obj:\n                continue\n            # First check if bounding boxes collides\n            intersection = CollisionUtility.check_bb_intersection(obj, collision_obj)\n            # if they do\n            if intersection:\n                skip_inside_check = collision_obj in list_of_objects_with_no_inside_check\n                # then check for more refined collisions\n                intersection, bvh_cache = CollisionUtility.check_mesh_intersection(obj, collision_obj,\n                                                                                   bvh_cache=bvh_cache,\n                                                                                   skip_inside_check=skip_inside_check)\n            if intersection:\n                no_collision = False\n                break\n        return no_collision",
  "def check_bb_intersection(obj1: MeshObject, obj2: MeshObject):\n        \"\"\"\n        Checks if there is a bounding box collision, these don't have to be axis-aligned, but if they are not:\n        The surrounding/including axis-aligned bounding box is calculated and used to check the intersection.\n\n        :param obj1: object 1  to check for intersection, must be a mesh\n        :param obj2: object 2  to check for intersection, must be a mesh\n        :return: True if the two bounding boxes intersect with each other\n        \"\"\"\n        b1w = obj1.get_bound_box()\n\n        def min_and_max_point(bb):\n            \"\"\"\n            Find the minimum and maximum point of the bounding box\n            :param bb: bounding box\n            :return: min, max\n            \"\"\"\n            values = np.array(bb)\n            return np.min(values, axis=0), np.max(values, axis=0)\n\n        # get min and max point of the axis-aligned bounding box\n        min_b1, max_b1 = min_and_max_point(b1w)\n        b2w = obj2.get_bound_box()\n        # get min and max point of the axis-aligned bounding box\n        min_b2, max_b2 = min_and_max_point(b2w)\n        return CollisionUtility.check_bb_intersection_on_values(min_b1, max_b1, min_b2, max_b2)",
  "def check_bb_intersection_on_values(min_b1: List[float], max_b1: List[float], min_b2: List[float],\n                                        max_b2: List[float],\n                                        used_check: Callable[[float, float], bool] = lambda a, b: a >= b):\n        \"\"\"\n        Checks if there is an intersection of the given bounding box values. Here we use two different bounding boxes,\n        namely b1 and b2. Each of them has a corresponding set of min and max values, this works for 2 and 3 dimensional\n        problems.\n\n        :param min_b1: List of minimum bounding box points for b1.\n        :param max_b1: List of maximum bounding box points for b1.\n        :param min_b2: List of minimum bounding box points for b2.\n        :param max_b2: List of maximum bounding box points for b2.\n        :param used_check: The operation used inside the is_overlapping1D. With that it possible to change the \\\n                           collision check from volume and surface check to pure surface or volume checks.\n        :return: True if the two bounding boxes intersect with each other\n        \"\"\"\n        collide = True\n        for min_b1_val, max_b1_val, min_b2_val, max_b2_val in zip(min_b1, max_b1, min_b2, max_b2):\n            # inspired by this:\n            # https://stackoverflow.com/questions/20925818/algorithm-to-check-if-two-boxes-overlap\n            # Checks in each dimension, if there is an overlap if this happens it must be an overlap in 3D, too.\n            def is_overlapping_1D(x_min_1, x_max_1, x_min_2, x_max_2):\n                # returns true if the min and max values are overlapping\n                return used_check(x_max_1, x_min_2) and used_check(x_max_2, x_min_1)\n\n            collide = collide and is_overlapping_1D(min_b1_val, max_b1_val, min_b2_val, max_b2_val)\n        return collide",
  "def check_mesh_intersection(obj1: MeshObject, obj2: MeshObject, skip_inside_check: bool = False,\n                                bvh_cache: Optional[Dict[str, mathutils.bvhtree.BVHTree]] = None) \\\n            -> Tuple[bool, Dict[str, mathutils.bvhtree.BVHTree]]:\n        \"\"\"\n        Checks if the two objects are intersecting.\n\n        This will use BVH trees to check whether the objects are overlapping.\n\n        It is further also checked if one object is completely inside the other.\n        This check requires that both objects are watertight, have correct normals and are coherent.\n        If this is not the case it can be disabled via the parameter skip_inside_check.\n\n        :param obj1: object 1 to check for intersection, must be a mesh\n        :param obj2: object 2 to check for intersection, must be a mesh\n        :param skip_inside_check: Disables checking whether one object is completely inside the other.\n        :param bvh_cache: Dict of all the bvh trees, removes the `obj` from the cache before adding it again.\n        :return: True, if they are intersecting\n        \"\"\"\n\n        if bvh_cache is None:\n            bvh_cache = {}\n\n        # If one of the objects has no vertices, collision is impossible\n        if len(obj1.get_mesh().vertices) == 0 or len(obj2.get_mesh().vertices) == 0:\n            return False, bvh_cache\n\n        # create bvhtree for obj1\n        if obj1.get_name() not in bvh_cache:\n            obj1_BVHtree = obj1.create_bvh_tree()\n            bvh_cache[obj1.get_name()] = obj1_BVHtree\n        else:\n            obj1_BVHtree = bvh_cache[obj1.get_name()]\n\n        # create bvhtree for obj2\n        if obj2.get_name() not in bvh_cache:\n            obj2_BVHtree = obj2.create_bvh_tree()\n            bvh_cache[obj2.get_name()] = obj2_BVHtree\n        else:\n            obj2_BVHtree = bvh_cache[obj2.get_name()]\n\n        # Check whether both meshes intersect\n        inter = len(obj1_BVHtree.overlap(obj2_BVHtree)) > 0\n\n        # Optionally check whether obj2 is contained in obj1\n        if not inter and not skip_inside_check:\n            inter = CollisionUtility.is_point_inside_object(obj1, obj1_BVHtree,\n                                                            Matrix(obj2.get_local2world_mat()) @\n                                                            obj2.get_mesh().vertices[0].co)\n            if inter:\n                print(\"Warning: Detected that \" + obj2.get_name() + \" is completely inside \" + obj1.get_name() +\n                      \". This might be wrong, if \" + obj1.get_name() +\n                      \" is not water tight or has incorrect normals. If that is the case, consider setting \"\n                      \"skip_inside_check to True.\")\n\n        # Optionally check whether obj1 is contained in obj2\n        if not inter and not skip_inside_check:\n            inter = CollisionUtility.is_point_inside_object(obj2, obj2_BVHtree, Matrix(obj1.get_local2world_mat())\n                                                            @ obj1.get_mesh().vertices[0].co)\n            if inter:\n                print(\"Warning: Detected that \" + obj1.get_name() + \" is completely inside \" + obj2.get_name() +\n                      \". This might be wrong, if \" + obj2.get_name() + \" is not water tight or has incorrect \"\n                                                                       \"normals. If that is the case, consider \"\n                                                                       \"setting skip_inside_check to True.\")\n\n        return inter, bvh_cache",
  "def is_point_inside_object(obj: MeshObject, obj_bvh_tree: mathutils.bvhtree.BVHTree,\n                               point: Union[Vector, np.ndarray]) -> bool:\n        \"\"\" Checks whether the given point is inside the given object.\n\n        This only works if the given object is watertight and has correct normals\n\n        :param obj: The object\n        :param obj_bvh_tree: A bvh tree of the object\n        :param point: The point to check\n        :return: True, if the point is inside the object\n        \"\"\"\n        point = Vector(point)\n        # Look for closest point on object\n        nearest, normal, _, _ = obj_bvh_tree.find_nearest(point)\n        # Compute direction\n        p2 = nearest - point\n        # Compute dot product between direction and normal vector\n        a = p2.normalized().dot((Euler(obj.get_rotation_euler()).to_matrix() @ normal).normalized())\n        return a >= 0.0",
  "def min_and_max_point(bb):\n            \"\"\"\n            Find the minimum and maximum point of the bounding box\n            :param bb: bounding box\n            :return: min, max\n            \"\"\"\n            values = np.array(bb)\n            return np.min(values, axis=0), np.max(values, axis=0)",
  "def is_overlapping_1D(x_min_1, x_max_1, x_min_2, x_max_2):\n                # returns true if the min and max values are overlapping\n                return used_check(x_max_1, x_min_2) and used_check(x_max_2, x_min_1)",
  "def generate_random_pattern_img(width: int, height: int, n_points: int) -> np.ndarray:\n    \"\"\"Generate transparent image with random pattern.\n\n    :param width: width of image to be generated.\n    :param height: height of image to be generated.\n    :param n_points: number of white points uniformly placed on image.\n    \"\"\"\n    pattern_img = np.zeros((height, width, 4), dtype=np.uint8)\n\n    m_width = int(width // np.sqrt(n_points))\n    m_height = int(height // np.sqrt(n_points))\n\n    for i, j in itertools.product(range(width // m_width), range(height // m_height)):\n        x_idx = random.randint(i * m_width, (i + 1) * m_width - 1)\n        y_idx = random.randint(j * m_height, (j + 1) * m_height - 1)\n        pattern_img = cv2.circle(pattern_img, (x_idx, y_idx), 1, (255, 255, 255, 255), -1)\n\n    return pattern_img",
  "class SetupUtility:\n    \"\"\"\n    Setup class, ensures that all necessary pip packages are there\n    \"\"\"\n    # Remember already installed packages, so we do not have to call pip freeze multiple times\n    installed_packages: Optional[Dict[str, str]] = None\n    package_list_is_from_cache = False\n    main_setup_called = False\n\n    @staticmethod\n    def setup(user_required_packages: Optional[List[str]] = None, blender_path: Optional[str] = None,\n              major_version: Optional[str] = None, reinstall_packages: bool = False,\n              debug_args: Optional[List[str]] = None) -> List[str]:\n        \"\"\" Sets up the python environment.\n\n        - Makes sure all required pip packages are installed\n        - Prepares the given sys.argv\n\n        :param user_required_packages: A list of python packages that are additionally necessary to execute the\n                                       python script.\n        :param blender_path: The path to the blender installation. If None, it is determined automatically based on\n                             the current python env.\n        :param major_version: The version number of the blender installation. If None, it is determined automatically\n                              based on the current python env.\n        :param reinstall_packages: Set to true, if all python packages should be reinstalled.\n        :param debug_args: Can be used to overwrite sys.argv in debug mode.\n        :return: List of sys.argv after removing blender specific commands\n        \"\"\"\n        packages_path = SetupUtility.setup_pip(user_required_packages, blender_path, major_version, reinstall_packages)\n\n        if not SetupUtility.main_setup_called:\n            SetupUtility.main_setup_called = True\n            sys.path.append(packages_path)\n            is_debug_mode = \"--background\" not in sys.argv\n\n            # Setup temporary directory\n            if is_debug_mode:\n                SetupUtility.setup_utility_paths(\"examples/debugging/temp\")\n            else:\n                SetupUtility.setup_utility_paths(sys.argv[sys.argv.index(\"--\") + 2])\n\n            # Only prepare args in non-debug mode (In debug mode the arguments are already ready to use)\n            if not is_debug_mode:\n                # Cut off blender specific arguments\n                sys.argv = sys.argv[sys.argv.index(\"--\") + 1:sys.argv.index(\"--\") + 2] + \\\n                           sys.argv[sys.argv.index(\"--\") + 3:]\n            elif debug_args is not None:\n                sys.argv = [\"debug\"] + debug_args\n\n        return sys.argv\n\n    @staticmethod\n    def setup_utility_paths(temp_dir: str):\n        \"\"\" Set utility paths: Temp dir and working dir.\n\n        :param temp_dir: Path to temporary directory where Blender saves output. Default is shared memory.\n        \"\"\"\n        # pylint: disable=import-outside-toplevel,cyclic-import\n        from blenderproc.python.utility.Utility import Utility, resolve_path\n        # pylint: enable=import-outside-toplevel,cyclic-import\n\n        Utility.temp_dir = resolve_path(temp_dir)\n        os.makedirs(Utility.temp_dir, exist_ok=True)\n\n    @staticmethod\n    def determine_python_paths(blender_path: Optional[str], major_version: Optional[str]) -> Union[str, str, str, str]:\n        \"\"\" Determines python binary, custom pip packages and the blender pip packages path.\n\n        :param blender_path: The path to the blender main folder.\n        :param major_version: The major version string of the blender installation.\n        :return:\n              - The path to the python binary of the blender installation\n              - The path to the directory containing custom pip packages installed by BlenderProc\n              - The path to the directory containing pip packages installed by blender.\n        \"\"\"\n        # If no bleneder path is given, determine it based on sys.executable\n        if blender_path is None:\n            blender_path = os.path.abspath(os.path.join(os.path.dirname(sys.executable), \"..\", \"..\", \"..\"))\n            major_version = os.path.basename(os.path.abspath(os.path.join(os.path.dirname(sys.executable), \"..\", \"..\")))\n\n        # Based on the OS determined the three paths\n        current_python_version = \"python3.10\"\n        if platform in [\"linux\", \"linux2\"]:\n            python_bin_folder = os.path.join(blender_path, major_version, \"python\", \"bin\")\n            python_bin = os.path.join(python_bin_folder, current_python_version)\n            packages_path = os.path.abspath(os.path.join(blender_path, \"custom-python-packages\"))\n            packages_import_path = os.path.join(packages_path, \"lib\", current_python_version, \"site-packages\")\n            pre_python_package_path = os.path.join(blender_path, major_version, \"python\", \"lib\",\n                                                   current_python_version, \"site-packages\")\n        elif platform == \"darwin\":\n            python_bin_folder = os.path.join(blender_path, \"..\", \"Resources\", major_version, \"python\", \"bin\")\n            python_bin = os.path.join(python_bin_folder, current_python_version)\n            packages_path = os.path.abspath(os.path.join(blender_path, \"custom-python-packages\"))\n            packages_import_path = os.path.join(packages_path, \"lib\", current_python_version, \"site-packages\")\n            pre_python_package_path = os.path.join(blender_path, \"..\", \"Resources\", major_version, \"python\",\n                                                   \"lib\", current_python_version, \"site-packages\")\n        elif platform == \"win32\":\n            python_bin_folder = os.path.join(blender_path, major_version, \"python\", \"bin\")\n            python_bin = os.path.join(python_bin_folder, \"python\")\n            packages_path = os.path.abspath(os.path.join(blender_path, \"custom-python-packages\"))\n            packages_import_path = os.path.join(packages_path, current_python_version.replace(\".\", \"\").capitalize(),\n                                                \"site-packages\")\n            pre_python_package_path = os.path.join(blender_path, major_version, \"python\", \"lib\", \"site-packages\")\n        else:\n            raise RuntimeError(f\"This system is not supported yet: {platform}\")\n\n        return python_bin, packages_path, packages_import_path, pre_python_package_path\n\n    @staticmethod\n    def setup_pip(user_required_packages: Optional[List[str]] = None, blender_path: Optional[str] = None,\n                  major_version: Optional[str] = None, reinstall_packages: bool = False,\n                  use_custom_package_path: bool = True, install_default_packages: bool = True) -> str:\n        \"\"\"\n        Makes sure the given user required and the general required python packages are installed in the BlenderProc env\n\n        At the first run all installed packages are collected via pip freeze.\n        If a pip packages is already installed, it is skipped.\n\n        :param user_required_packages: A list of pip packages that should be installed. The version number can be\n                                       specified via the usual == notation.\n        :param blender_path: The path to the blender installation.\n        :param major_version: The version number of the blender installation.\n        :param reinstall_packages: Set to true, if all python packages should be reinstalled.\n        :param use_custom_package_path: If True, the python packages are installed into a custom folder, separate\n                                        from blenders own python packages.\n        :param install_default_packages: If True, general required python packages are made sure to be installed.\n        :return: Returns the path to the directory which contains all custom installed pip packages.\n        \"\"\"\n        required_packages = []\n        # Only install general required packages on first setup_pip call\n        if SetupUtility.installed_packages is None and install_default_packages:\n            required_packages += DefaultConfig.default_pip_packages\n        if user_required_packages is not None:\n            required_packages += user_required_packages\n\n        if reinstall_packages:\n            raise ValueError(\"The reinstall package mode is not supported right now!\")\n\n        result = SetupUtility.determine_python_paths(blender_path, major_version)\n        python_bin, packages_path, packages_import_path, pre_python_package_path = result\n\n        # Init pip\n        SetupUtility._ensure_pip(python_bin, packages_path, packages_import_path, pre_python_package_path)\n\n        # If the list of installed packages was read from cache\n        if SetupUtility.package_list_is_from_cache:\n            # Check if there would be any pip package updates based on the cache\n            found_package_to_install = SetupUtility._pip_install_packages(required_packages, python_bin,\n                                                                          packages_path, dry_run=True)\n            # If yes, reload the list of installed packages\n            if found_package_to_install:\n                SetupUtility._ensure_pip(python_bin, packages_path, packages_import_path,\n                                         pre_python_package_path, force_update=True)\n\n        packages_were_installed = SetupUtility._pip_install_packages(required_packages, python_bin,\n                                                                     packages_path,\n                                                                     use_custom_package_path=use_custom_package_path)\n\n        # Make sure to update the pip package list cache, if it does not exist or changes have been made\n        cache_path = os.path.join(packages_path, \"installed_packages_cache_v2.json\")\n        if packages_were_installed or not os.path.exists(cache_path):\n            with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(SetupUtility.installed_packages, f)\n\n        # If packages were installed, invalidate the module cache, s.t. the new modules can be imported right away\n        if packages_were_installed:\n            importlib.invalidate_caches()\n        return packages_import_path\n\n    @staticmethod\n    def _pip_install_packages(required_packages, python_bin, packages_path, reinstall_packages: bool = False,\n                              dry_run: bool = False, use_custom_package_path: bool = True) -> bool:\n        \"\"\" Installs the list of given pip packages in the given python environment.\n\n        :param required_packages: A list of pip packages that should be installed. The version number can be\n                                  specified via the usual == notation.\n        :param python_bin: Path to python binary.\n        :param packages_path: Path where our pip packages should be installed\n        :param reinstall_packages: Set to true, if all python packages should be reinstalled.\n        :param dry_run: If true, nothing will be installed, and it will only be checked whether there are\n                        any potential packages to update/install.\n        :param use_custom_package_path: If True, the python packages are installed into a custom folder,\n                                        separate from blenders own python packages.\n        :return: Returns True, if any packages were update/installed or - if dry_run=True - if there are any potential\n                 packages to update/install.\n        \"\"\"\n        # Install all packages\n        packages_were_installed = False\n        for package in required_packages:\n            # If -f (find_links) flag for pip install in required package set find_link = link to parse\n            find_link = None\n\n            # Extract name and target version\n            if \"==\" in package:\n                package_name, package_version = package.lower().split('==')\n                if ' -f ' in package_version:\n                    find_link = package_version.split(' -f ')[1].strip()\n                    package_version = package_version.split(' -f ')[0].strip()\n            else:\n                package_name, package_version = package.lower(), None\n\n            if package_name == \"opencv-python\":\n                raise RuntimeError(\"Please use opencv-contrib-python instead of opencv-python, as having both \"\n                                   \"packages installed in the same environment can lead to complications.\")\n\n            # If the package is given via git, extract package name from url\n            if package_name.startswith(\"git+\"):\n                # Extract part after last slash\n                package_name = package_name[package_name.rfind(\"/\") + 1:]\n                # Replace underscores with dashes as it's done by pip\n                package_name = package_name.replace(\"_\", \"-\")\n\n            # Check if package is installed\n            # pylint: disable=unsupported-membership-test\n            already_installed = package_name in SetupUtility.installed_packages\n            # pylint: enable=unsupported-membership-test\n\n            # If version check is necessary\n            if package_version is not None and already_installed:\n                # Check if the correct version is installed\n                # pylint: disable=unsubscriptable-object\n                already_installed = package_version == SetupUtility.installed_packages[package_name]\n                # pylint: enable=unsubscriptable-object\n\n            # Only install if it's not already installed (pip would check this itself, but at first downloads the\n            # requested package which of course always takes a while)\n            if not already_installed or reinstall_packages:\n                print(f\"Installing pip package {package_name} {package_version}\")\n                extra_args = []\n                # Set find link flag, if required\n                if find_link:\n                    extra_args.extend([\"-f\", find_link])\n                    package = package_name + \"==\" + package_version\n                # If the env var is set, disable pip cache\n                if os.getenv(\"BLENDER_PROC_NO_PIP_CACHE\", 'False').lower() in ('true', '1', 't'):\n                    extra_args.append(\"--no-cache-dir\")\n\n                if not dry_run:\n                    if use_custom_package_path:\n                        extra_args.extend([\"--user\"])\n                    # Run pip install\n                    # pylint: disable=consider-using-with\n                    subprocess.Popen([python_bin, \"-m\", \"pip\", \"install\", package, \"--upgrade\"] + extra_args,\n                                     env=dict(os.environ, PYTHONNOUSERSITE=\"0\", PYTHONUSERBASE=packages_path)).wait()\n                    # pylint: enable=consider-using-with\n                    # pylint: disable=unsupported-assignment-operation\n                    SetupUtility.installed_packages[package_name] = package_version\n                    # pylint: enable=unsupported-assignment-operation\n                    packages_were_installed = True\n                else:\n                    return True\n\n        return packages_were_installed\n\n    @staticmethod\n    def uninstall_pip_packages(package_names: List[str], blender_path: str, major_version: str):\n        \"\"\" Uninstalls the given pip packages in blenders python environment.\n\n        :param package_names: A list of pip packages that should be uninstalled.\n        :param blender_path: The path to the blender main folder.\n        :param major_version: The major version string of the blender installation.\n        \"\"\"\n        # Determine python and packages paths\n        python_bin, _, packages_import_path, _ = SetupUtility.determine_python_paths(blender_path, major_version)\n\n        # Run pip uninstall\n        # pylint: disable=consider-using-with\n        subprocess.Popen([python_bin, \"-m\", \"pip\", \"uninstall\"] + package_names,\n                         env=dict(os.environ, PYTHONPATH=packages_import_path)).wait()\n        # pylint: enable=consider-using-with\n\n        # Clear installed packages cache\n        SetupUtility.clean_installed_packages_cache(blender_path, major_version)\n\n    @staticmethod\n    def _ensure_pip(python_bin: str, packages_path: str, packages_import_path: str,\n                    pre_python_package_path: str, force_update: bool = False):\n        \"\"\" Make sure pip is installed and read in the already installed packages\n\n        :param python_bin: Path to python binary.\n        :param packages_path: Path where our pip packages should be installed\n        :param packages_import_path: Path to site-packages in packages_path which contains the installed packages\n        :param pre_python_package_path: Path that contains blender's default pip packages\n        :param force_update: If True, the installed-packages-cache will be ignored and will be recollected based\n                             on the actually installed packages.\n        \"\"\"\n        if SetupUtility.installed_packages is None:\n            if not force_update:\n                cache_path = os.path.join(packages_path, \"installed_packages_cache_v2.json\")\n                if os.path.exists(cache_path):\n                    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n                        SetupUtility.installed_packages = json.load(f)\n                        SetupUtility.package_list_is_from_cache = True\n                    return\n\n            SetupUtility.installed_packages = {}\n            # pylint: disable=consider-using-with\n            subprocess.Popen([python_bin, \"-m\", \"ensurepip\"], env=dict(os.environ, PYTHONPATH=\"\")).wait()\n            # Make sure pip is up-to-date\n            subprocess.Popen([python_bin, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"],\n                             env=dict(os.environ, PYTHONPATH=\"\")).wait()\n            # pylint: enable=consider-using-with\n\n            # Make sure to not install into the default site-packages path, as this would overwrite\n            # already pre-installed packages\n            if not os.path.exists(packages_path):\n                os.mkdir(packages_path)\n\n            # Collect already installed packages by calling pip list (outputs: <package name>==<version>)\n            installed_packages = subprocess.check_output([python_bin, \"-m\", \"pip\", \"list\", \"--format=freeze\",\n                                                          f\"--path={pre_python_package_path}\"])\n            installed_packages += subprocess.check_output([python_bin, \"-m\", \"pip\", \"list\", \"--format=freeze\",\n                                                           f\"--path={packages_import_path}\"])\n\n            # Split up strings into two lists (names and versions)\n            installed_packages_name, installed_packages_versions = zip(*[str(line).lower().split('==')\n                                                                         for line in installed_packages.splitlines()])\n            installed_packages_name = [ele[2:] if ele.startswith(\"b'\") else ele\n                                       for ele in installed_packages_name]\n            installed_packages_versions = [ele[:-1] if ele.endswith(\"'\") else ele\n                                           for ele in installed_packages_versions]\n            SetupUtility.installed_packages = dict(zip(installed_packages_name, installed_packages_versions))\n            SetupUtility.package_list_is_from_cache = False\n\n    @staticmethod\n    def clean_installed_packages_cache(blender_path, major_version):\n        \"\"\" Removes the json file containing a list of all installed pip packages (if it exists).\n\n        :param blender_path: The path to the blender main folder.\n        :param major_version: The major version string of the blender installation.\n        \"\"\"\n        _, packages_path, _, _ = SetupUtility.determine_python_paths(blender_path, major_version)\n        cache_path = os.path.join(packages_path, \"installed_packages_cache_v2.json\")\n        if os.path.exists(cache_path):\n            os.remove(cache_path)\n\n    @staticmethod\n    def extract_file(output_dir: str, file: Union[str, BytesIO], mode: str = \"ZIP\"):\n        \"\"\" Extract all members from the archive into output_dir.\n\n        :param output_dir: The output directory that should contain the extracted files.\n        :param file: The path to the archive which should be extracted.\n        :param mode: The type of the given file, has to be in [\"TAR\", \"ZIP\"]\n        \"\"\"\n        try:\n            if mode.lower() == \"zip\":\n                with zipfile.ZipFile(file) as tar:\n                    tar.extractall(str(output_dir))\n            elif mode.lower() == \"tar\":\n                with tarfile.open(file) as tar:\n                    tar.extractall(str(output_dir))\n            else:\n                raise RuntimeError(f\"No such mode: {mode}\")\n\n        except (IOError, zipfile.BadZipfile) as e:\n            print(f\"Bad zip file given as input. {e}\")\n            raise e\n\n    @staticmethod\n    def extract_from_response(output_dir: str, response: requests.Response):\n        \"\"\" Extract all members from the archive to output_dir\n\n        :param output_dir: the dir to zip file extract to\n        :param response: the response to a requested url that contains a zip file\n        \"\"\"\n        file = BytesIO(response.content)\n        SetupUtility.extract_file(output_dir, file)\n\n    @staticmethod\n    def check_if_setup_utilities_are_at_the_top(path_to_run_file: str):\n        \"\"\"\n        Checks if the given python scripts has at the top an import to SetupUtility, if not an\n        exception is thrown. With an explanation that each python script has to start with SetupUtility.\n\n        :param path_to_run_file: path to the used python script\n        \"\"\"\n        if os.path.exists(path_to_run_file):\n            with open(path_to_run_file, \"r\", encoding=\"utf-8\") as file:\n                text = file.read()\n                lines = [l.strip() for l in text.split(\"\\n\")]\n                lines = [l for l in lines if l and not l.startswith(\"#\")]\n                for index, line in enumerate(lines):\n                    if \"import blenderproc\" in line or \"from blenderproc\" in line:\n                        return\n                    code = \"\\n\".join(lines[:index + 2])\n                    raise RuntimeError(f'The given script \"{path_to_run_file}\" does not have a blenderproc '\n                                       f'import at the top! Make sure that is the first thing you import, as '\n                                       f'otherwise the import of third-party packages installed in the '\n                                       f'blender environment will fail.\\n'\n                                       f'Your code:\\n#####################\\n{code}\\n\"'\n                                       f'\"####################\\nReplaces this with:\\n\"'\n                                       f'\"import blenderproc as bproc\"')\n        else:\n            raise RuntimeError(f\"The given run script does not exist: {path_to_run_file}\")\n\n    @staticmethod\n    def determine_temp_dir(given_temp_dir: str) -> str:\n        \"\"\" Finds and creates a temporary directory.\n\n        On linux the temp dir is per default placed in /dev/shm or /tmp.\n        The name of the created temp dir contains a uuid, so multiple BlenderProc processes\n        can run on one system.\n\n        :param given_temp_dir: A directory inside which the temp dir should be created\n        :return: The path to the created temp dir.\n        \"\"\"\n        # Determine perfect temp dir\n        if given_temp_dir is None:\n            if sys.platform != \"win32\":\n                if os.path.exists(\"/dev/shm\"):\n                    temp_dir = \"/dev/shm\"\n                else:\n                    temp_dir = \"/tmp\"\n            else:\n                temp_dir = os.getenv(\"TEMP\")\n        else:\n            temp_dir = given_temp_dir\n        # Generate unique directory name in temp dir\n        temp_dir = os.path.join(temp_dir, \"blender_proc_\" + str(uuid.uuid4().hex))\n        # Create the temp dir\n        print(\"Using temporary directory: \" + temp_dir)\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir)\n        return temp_dir",
  "def setup(user_required_packages: Optional[List[str]] = None, blender_path: Optional[str] = None,\n              major_version: Optional[str] = None, reinstall_packages: bool = False,\n              debug_args: Optional[List[str]] = None) -> List[str]:\n        \"\"\" Sets up the python environment.\n\n        - Makes sure all required pip packages are installed\n        - Prepares the given sys.argv\n\n        :param user_required_packages: A list of python packages that are additionally necessary to execute the\n                                       python script.\n        :param blender_path: The path to the blender installation. If None, it is determined automatically based on\n                             the current python env.\n        :param major_version: The version number of the blender installation. If None, it is determined automatically\n                              based on the current python env.\n        :param reinstall_packages: Set to true, if all python packages should be reinstalled.\n        :param debug_args: Can be used to overwrite sys.argv in debug mode.\n        :return: List of sys.argv after removing blender specific commands\n        \"\"\"\n        packages_path = SetupUtility.setup_pip(user_required_packages, blender_path, major_version, reinstall_packages)\n\n        if not SetupUtility.main_setup_called:\n            SetupUtility.main_setup_called = True\n            sys.path.append(packages_path)\n            is_debug_mode = \"--background\" not in sys.argv\n\n            # Setup temporary directory\n            if is_debug_mode:\n                SetupUtility.setup_utility_paths(\"examples/debugging/temp\")\n            else:\n                SetupUtility.setup_utility_paths(sys.argv[sys.argv.index(\"--\") + 2])\n\n            # Only prepare args in non-debug mode (In debug mode the arguments are already ready to use)\n            if not is_debug_mode:\n                # Cut off blender specific arguments\n                sys.argv = sys.argv[sys.argv.index(\"--\") + 1:sys.argv.index(\"--\") + 2] + \\\n                           sys.argv[sys.argv.index(\"--\") + 3:]\n            elif debug_args is not None:\n                sys.argv = [\"debug\"] + debug_args\n\n        return sys.argv",
  "def setup_utility_paths(temp_dir: str):\n        \"\"\" Set utility paths: Temp dir and working dir.\n\n        :param temp_dir: Path to temporary directory where Blender saves output. Default is shared memory.\n        \"\"\"\n        # pylint: disable=import-outside-toplevel,cyclic-import\n        from blenderproc.python.utility.Utility import Utility, resolve_path\n        # pylint: enable=import-outside-toplevel,cyclic-import\n\n        Utility.temp_dir = resolve_path(temp_dir)\n        os.makedirs(Utility.temp_dir, exist_ok=True)",
  "def determine_python_paths(blender_path: Optional[str], major_version: Optional[str]) -> Union[str, str, str, str]:\n        \"\"\" Determines python binary, custom pip packages and the blender pip packages path.\n\n        :param blender_path: The path to the blender main folder.\n        :param major_version: The major version string of the blender installation.\n        :return:\n              - The path to the python binary of the blender installation\n              - The path to the directory containing custom pip packages installed by BlenderProc\n              - The path to the directory containing pip packages installed by blender.\n        \"\"\"\n        # If no bleneder path is given, determine it based on sys.executable\n        if blender_path is None:\n            blender_path = os.path.abspath(os.path.join(os.path.dirname(sys.executable), \"..\", \"..\", \"..\"))\n            major_version = os.path.basename(os.path.abspath(os.path.join(os.path.dirname(sys.executable), \"..\", \"..\")))\n\n        # Based on the OS determined the three paths\n        current_python_version = \"python3.10\"\n        if platform in [\"linux\", \"linux2\"]:\n            python_bin_folder = os.path.join(blender_path, major_version, \"python\", \"bin\")\n            python_bin = os.path.join(python_bin_folder, current_python_version)\n            packages_path = os.path.abspath(os.path.join(blender_path, \"custom-python-packages\"))\n            packages_import_path = os.path.join(packages_path, \"lib\", current_python_version, \"site-packages\")\n            pre_python_package_path = os.path.join(blender_path, major_version, \"python\", \"lib\",\n                                                   current_python_version, \"site-packages\")\n        elif platform == \"darwin\":\n            python_bin_folder = os.path.join(blender_path, \"..\", \"Resources\", major_version, \"python\", \"bin\")\n            python_bin = os.path.join(python_bin_folder, current_python_version)\n            packages_path = os.path.abspath(os.path.join(blender_path, \"custom-python-packages\"))\n            packages_import_path = os.path.join(packages_path, \"lib\", current_python_version, \"site-packages\")\n            pre_python_package_path = os.path.join(blender_path, \"..\", \"Resources\", major_version, \"python\",\n                                                   \"lib\", current_python_version, \"site-packages\")\n        elif platform == \"win32\":\n            python_bin_folder = os.path.join(blender_path, major_version, \"python\", \"bin\")\n            python_bin = os.path.join(python_bin_folder, \"python\")\n            packages_path = os.path.abspath(os.path.join(blender_path, \"custom-python-packages\"))\n            packages_import_path = os.path.join(packages_path, current_python_version.replace(\".\", \"\").capitalize(),\n                                                \"site-packages\")\n            pre_python_package_path = os.path.join(blender_path, major_version, \"python\", \"lib\", \"site-packages\")\n        else:\n            raise RuntimeError(f\"This system is not supported yet: {platform}\")\n\n        return python_bin, packages_path, packages_import_path, pre_python_package_path",
  "def setup_pip(user_required_packages: Optional[List[str]] = None, blender_path: Optional[str] = None,\n                  major_version: Optional[str] = None, reinstall_packages: bool = False,\n                  use_custom_package_path: bool = True, install_default_packages: bool = True) -> str:\n        \"\"\"\n        Makes sure the given user required and the general required python packages are installed in the BlenderProc env\n\n        At the first run all installed packages are collected via pip freeze.\n        If a pip packages is already installed, it is skipped.\n\n        :param user_required_packages: A list of pip packages that should be installed. The version number can be\n                                       specified via the usual == notation.\n        :param blender_path: The path to the blender installation.\n        :param major_version: The version number of the blender installation.\n        :param reinstall_packages: Set to true, if all python packages should be reinstalled.\n        :param use_custom_package_path: If True, the python packages are installed into a custom folder, separate\n                                        from blenders own python packages.\n        :param install_default_packages: If True, general required python packages are made sure to be installed.\n        :return: Returns the path to the directory which contains all custom installed pip packages.\n        \"\"\"\n        required_packages = []\n        # Only install general required packages on first setup_pip call\n        if SetupUtility.installed_packages is None and install_default_packages:\n            required_packages += DefaultConfig.default_pip_packages\n        if user_required_packages is not None:\n            required_packages += user_required_packages\n\n        if reinstall_packages:\n            raise ValueError(\"The reinstall package mode is not supported right now!\")\n\n        result = SetupUtility.determine_python_paths(blender_path, major_version)\n        python_bin, packages_path, packages_import_path, pre_python_package_path = result\n\n        # Init pip\n        SetupUtility._ensure_pip(python_bin, packages_path, packages_import_path, pre_python_package_path)\n\n        # If the list of installed packages was read from cache\n        if SetupUtility.package_list_is_from_cache:\n            # Check if there would be any pip package updates based on the cache\n            found_package_to_install = SetupUtility._pip_install_packages(required_packages, python_bin,\n                                                                          packages_path, dry_run=True)\n            # If yes, reload the list of installed packages\n            if found_package_to_install:\n                SetupUtility._ensure_pip(python_bin, packages_path, packages_import_path,\n                                         pre_python_package_path, force_update=True)\n\n        packages_were_installed = SetupUtility._pip_install_packages(required_packages, python_bin,\n                                                                     packages_path,\n                                                                     use_custom_package_path=use_custom_package_path)\n\n        # Make sure to update the pip package list cache, if it does not exist or changes have been made\n        cache_path = os.path.join(packages_path, \"installed_packages_cache_v2.json\")\n        if packages_were_installed or not os.path.exists(cache_path):\n            with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(SetupUtility.installed_packages, f)\n\n        # If packages were installed, invalidate the module cache, s.t. the new modules can be imported right away\n        if packages_were_installed:\n            importlib.invalidate_caches()\n        return packages_import_path",
  "def _pip_install_packages(required_packages, python_bin, packages_path, reinstall_packages: bool = False,\n                              dry_run: bool = False, use_custom_package_path: bool = True) -> bool:\n        \"\"\" Installs the list of given pip packages in the given python environment.\n\n        :param required_packages: A list of pip packages that should be installed. The version number can be\n                                  specified via the usual == notation.\n        :param python_bin: Path to python binary.\n        :param packages_path: Path where our pip packages should be installed\n        :param reinstall_packages: Set to true, if all python packages should be reinstalled.\n        :param dry_run: If true, nothing will be installed, and it will only be checked whether there are\n                        any potential packages to update/install.\n        :param use_custom_package_path: If True, the python packages are installed into a custom folder,\n                                        separate from blenders own python packages.\n        :return: Returns True, if any packages were update/installed or - if dry_run=True - if there are any potential\n                 packages to update/install.\n        \"\"\"\n        # Install all packages\n        packages_were_installed = False\n        for package in required_packages:\n            # If -f (find_links) flag for pip install in required package set find_link = link to parse\n            find_link = None\n\n            # Extract name and target version\n            if \"==\" in package:\n                package_name, package_version = package.lower().split('==')\n                if ' -f ' in package_version:\n                    find_link = package_version.split(' -f ')[1].strip()\n                    package_version = package_version.split(' -f ')[0].strip()\n            else:\n                package_name, package_version = package.lower(), None\n\n            if package_name == \"opencv-python\":\n                raise RuntimeError(\"Please use opencv-contrib-python instead of opencv-python, as having both \"\n                                   \"packages installed in the same environment can lead to complications.\")\n\n            # If the package is given via git, extract package name from url\n            if package_name.startswith(\"git+\"):\n                # Extract part after last slash\n                package_name = package_name[package_name.rfind(\"/\") + 1:]\n                # Replace underscores with dashes as it's done by pip\n                package_name = package_name.replace(\"_\", \"-\")\n\n            # Check if package is installed\n            # pylint: disable=unsupported-membership-test\n            already_installed = package_name in SetupUtility.installed_packages\n            # pylint: enable=unsupported-membership-test\n\n            # If version check is necessary\n            if package_version is not None and already_installed:\n                # Check if the correct version is installed\n                # pylint: disable=unsubscriptable-object\n                already_installed = package_version == SetupUtility.installed_packages[package_name]\n                # pylint: enable=unsubscriptable-object\n\n            # Only install if it's not already installed (pip would check this itself, but at first downloads the\n            # requested package which of course always takes a while)\n            if not already_installed or reinstall_packages:\n                print(f\"Installing pip package {package_name} {package_version}\")\n                extra_args = []\n                # Set find link flag, if required\n                if find_link:\n                    extra_args.extend([\"-f\", find_link])\n                    package = package_name + \"==\" + package_version\n                # If the env var is set, disable pip cache\n                if os.getenv(\"BLENDER_PROC_NO_PIP_CACHE\", 'False').lower() in ('true', '1', 't'):\n                    extra_args.append(\"--no-cache-dir\")\n\n                if not dry_run:\n                    if use_custom_package_path:\n                        extra_args.extend([\"--user\"])\n                    # Run pip install\n                    # pylint: disable=consider-using-with\n                    subprocess.Popen([python_bin, \"-m\", \"pip\", \"install\", package, \"--upgrade\"] + extra_args,\n                                     env=dict(os.environ, PYTHONNOUSERSITE=\"0\", PYTHONUSERBASE=packages_path)).wait()\n                    # pylint: enable=consider-using-with\n                    # pylint: disable=unsupported-assignment-operation\n                    SetupUtility.installed_packages[package_name] = package_version\n                    # pylint: enable=unsupported-assignment-operation\n                    packages_were_installed = True\n                else:\n                    return True\n\n        return packages_were_installed",
  "def uninstall_pip_packages(package_names: List[str], blender_path: str, major_version: str):\n        \"\"\" Uninstalls the given pip packages in blenders python environment.\n\n        :param package_names: A list of pip packages that should be uninstalled.\n        :param blender_path: The path to the blender main folder.\n        :param major_version: The major version string of the blender installation.\n        \"\"\"\n        # Determine python and packages paths\n        python_bin, _, packages_import_path, _ = SetupUtility.determine_python_paths(blender_path, major_version)\n\n        # Run pip uninstall\n        # pylint: disable=consider-using-with\n        subprocess.Popen([python_bin, \"-m\", \"pip\", \"uninstall\"] + package_names,\n                         env=dict(os.environ, PYTHONPATH=packages_import_path)).wait()\n        # pylint: enable=consider-using-with\n\n        # Clear installed packages cache\n        SetupUtility.clean_installed_packages_cache(blender_path, major_version)",
  "def _ensure_pip(python_bin: str, packages_path: str, packages_import_path: str,\n                    pre_python_package_path: str, force_update: bool = False):\n        \"\"\" Make sure pip is installed and read in the already installed packages\n\n        :param python_bin: Path to python binary.\n        :param packages_path: Path where our pip packages should be installed\n        :param packages_import_path: Path to site-packages in packages_path which contains the installed packages\n        :param pre_python_package_path: Path that contains blender's default pip packages\n        :param force_update: If True, the installed-packages-cache will be ignored and will be recollected based\n                             on the actually installed packages.\n        \"\"\"\n        if SetupUtility.installed_packages is None:\n            if not force_update:\n                cache_path = os.path.join(packages_path, \"installed_packages_cache_v2.json\")\n                if os.path.exists(cache_path):\n                    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n                        SetupUtility.installed_packages = json.load(f)\n                        SetupUtility.package_list_is_from_cache = True\n                    return\n\n            SetupUtility.installed_packages = {}\n            # pylint: disable=consider-using-with\n            subprocess.Popen([python_bin, \"-m\", \"ensurepip\"], env=dict(os.environ, PYTHONPATH=\"\")).wait()\n            # Make sure pip is up-to-date\n            subprocess.Popen([python_bin, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"],\n                             env=dict(os.environ, PYTHONPATH=\"\")).wait()\n            # pylint: enable=consider-using-with\n\n            # Make sure to not install into the default site-packages path, as this would overwrite\n            # already pre-installed packages\n            if not os.path.exists(packages_path):\n                os.mkdir(packages_path)\n\n            # Collect already installed packages by calling pip list (outputs: <package name>==<version>)\n            installed_packages = subprocess.check_output([python_bin, \"-m\", \"pip\", \"list\", \"--format=freeze\",\n                                                          f\"--path={pre_python_package_path}\"])\n            installed_packages += subprocess.check_output([python_bin, \"-m\", \"pip\", \"list\", \"--format=freeze\",\n                                                           f\"--path={packages_import_path}\"])\n\n            # Split up strings into two lists (names and versions)\n            installed_packages_name, installed_packages_versions = zip(*[str(line).lower().split('==')\n                                                                         for line in installed_packages.splitlines()])\n            installed_packages_name = [ele[2:] if ele.startswith(\"b'\") else ele\n                                       for ele in installed_packages_name]\n            installed_packages_versions = [ele[:-1] if ele.endswith(\"'\") else ele\n                                           for ele in installed_packages_versions]\n            SetupUtility.installed_packages = dict(zip(installed_packages_name, installed_packages_versions))\n            SetupUtility.package_list_is_from_cache = False",
  "def clean_installed_packages_cache(blender_path, major_version):\n        \"\"\" Removes the json file containing a list of all installed pip packages (if it exists).\n\n        :param blender_path: The path to the blender main folder.\n        :param major_version: The major version string of the blender installation.\n        \"\"\"\n        _, packages_path, _, _ = SetupUtility.determine_python_paths(blender_path, major_version)\n        cache_path = os.path.join(packages_path, \"installed_packages_cache_v2.json\")\n        if os.path.exists(cache_path):\n            os.remove(cache_path)",
  "def extract_file(output_dir: str, file: Union[str, BytesIO], mode: str = \"ZIP\"):\n        \"\"\" Extract all members from the archive into output_dir.\n\n        :param output_dir: The output directory that should contain the extracted files.\n        :param file: The path to the archive which should be extracted.\n        :param mode: The type of the given file, has to be in [\"TAR\", \"ZIP\"]\n        \"\"\"\n        try:\n            if mode.lower() == \"zip\":\n                with zipfile.ZipFile(file) as tar:\n                    tar.extractall(str(output_dir))\n            elif mode.lower() == \"tar\":\n                with tarfile.open(file) as tar:\n                    tar.extractall(str(output_dir))\n            else:\n                raise RuntimeError(f\"No such mode: {mode}\")\n\n        except (IOError, zipfile.BadZipfile) as e:\n            print(f\"Bad zip file given as input. {e}\")\n            raise e",
  "def extract_from_response(output_dir: str, response: requests.Response):\n        \"\"\" Extract all members from the archive to output_dir\n\n        :param output_dir: the dir to zip file extract to\n        :param response: the response to a requested url that contains a zip file\n        \"\"\"\n        file = BytesIO(response.content)\n        SetupUtility.extract_file(output_dir, file)",
  "def check_if_setup_utilities_are_at_the_top(path_to_run_file: str):\n        \"\"\"\n        Checks if the given python scripts has at the top an import to SetupUtility, if not an\n        exception is thrown. With an explanation that each python script has to start with SetupUtility.\n\n        :param path_to_run_file: path to the used python script\n        \"\"\"\n        if os.path.exists(path_to_run_file):\n            with open(path_to_run_file, \"r\", encoding=\"utf-8\") as file:\n                text = file.read()\n                lines = [l.strip() for l in text.split(\"\\n\")]\n                lines = [l for l in lines if l and not l.startswith(\"#\")]\n                for index, line in enumerate(lines):\n                    if \"import blenderproc\" in line or \"from blenderproc\" in line:\n                        return\n                    code = \"\\n\".join(lines[:index + 2])\n                    raise RuntimeError(f'The given script \"{path_to_run_file}\" does not have a blenderproc '\n                                       f'import at the top! Make sure that is the first thing you import, as '\n                                       f'otherwise the import of third-party packages installed in the '\n                                       f'blender environment will fail.\\n'\n                                       f'Your code:\\n#####################\\n{code}\\n\"'\n                                       f'\"####################\\nReplaces this with:\\n\"'\n                                       f'\"import blenderproc as bproc\"')\n        else:\n            raise RuntimeError(f\"The given run script does not exist: {path_to_run_file}\")",
  "def determine_temp_dir(given_temp_dir: str) -> str:\n        \"\"\" Finds and creates a temporary directory.\n\n        On linux the temp dir is per default placed in /dev/shm or /tmp.\n        The name of the created temp dir contains a uuid, so multiple BlenderProc processes\n        can run on one system.\n\n        :param given_temp_dir: A directory inside which the temp dir should be created\n        :return: The path to the created temp dir.\n        \"\"\"\n        # Determine perfect temp dir\n        if given_temp_dir is None:\n            if sys.platform != \"win32\":\n                if os.path.exists(\"/dev/shm\"):\n                    temp_dir = \"/dev/shm\"\n                else:\n                    temp_dir = \"/tmp\"\n            else:\n                temp_dir = os.getenv(\"TEMP\")\n        else:\n            temp_dir = given_temp_dir\n        # Generate unique directory name in temp dir\n        temp_dir = os.path.join(temp_dir, \"blender_proc_\" + str(uuid.uuid4().hex))\n        # Create the temp dir\n        print(\"Using temporary directory: \" + temp_dir)\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir)\n        return temp_dir",
  "def add_object_only_with_vertices(vertices: List[List[float]], name: str = 'NewVertexObject') -> bpy.types.Object:\n    \"\"\"\n    Generates a new object with the given vertices, no edges or faces are generated.\n\n    :param vertices: [[float, float, float]] list of vertices\n    :param name: str name of the new object\n    :return: the generated obj\n    \"\"\"\n    mesh = bpy.data.meshes.new('mesh')\n    # create new object\n    obj = bpy.data.objects.new(name, mesh)\n    # TODO check if this always works?\n    col = bpy.data.collections.get('Collection')\n    # link object in collection\n    col.objects.link(obj)\n\n    # convert vertices to mesh\n    bm = bmesh.new()\n    for v in vertices:\n        bm.verts.new(v)\n    bm.to_mesh(mesh)\n    bm.free()\n    return obj",
  "def add_object_only_with_direction_vectors(vertices: List[List[float]], normals: List[List[float]],\n                                           radius: float = 1.0, name: str = 'NewDirectionObject') -> bpy.types.Object:\n    \"\"\"\n    Generates a new object with the given vertices and normals, there will be an edge between each point and the\n    point plus the normal times the radius. No faces are generated.\n\n    :param vertices: [[float, float, float]] list of vertices\n    :param normals: [[float, float, float]] list of normals\n    :param radius: Determines the size of the edge generated\n    :param name: str name of the new object\n    :return: the generated obj\n    \"\"\"\n    if len(vertices) != len(normals):\n        raise Exception(\"The lenght of the vertices and normals is not equal!\")\n\n    mesh = bpy.data.meshes.new('mesh')\n    # create new object\n    obj = bpy.data.objects.new(name, mesh)\n    # TODO check if this always works?\n    col = bpy.data.collections.get('Collection')\n    # link object in collection\n    col.objects.link(obj)\n\n    # convert vertices to mesh\n    bm = bmesh.new()\n    for v, n in zip(vertices, normals):\n        v1 = bm.verts.new(v)\n        new_vert = v + n * radius\n        v2 = bm.verts.new(new_vert)\n        bm.edges.new([v1, v2])\n    bm.to_mesh(mesh)\n    bm.free()\n    return obj",
  "def add_cube_based_on_bb(bouding_box: List[Vector], name: str = 'NewCube') -> bpy.types.Object:\n    \"\"\"\n    Generates a cube based on the given bounding box, the bounding_box can be generated with our get_bounds(obj) fct.\n\n    :param bounding_box: bound_box [8x[3xfloat]], with 8 vertices for each corner\n    :param name: name of the new cube\n    :return: the generated object\n    \"\"\"\n    if len(bouding_box) != 8:\n        raise Exception(\"The amount of vertices is wrong for this bounding box!\")\n    mesh = bpy.data.meshes.new('mesh')\n    # create new object\n    obj = bpy.data.objects.new(name, mesh)\n    # TODO check if this always works?\n    col = bpy.data.collections.get('Collection')\n    # link object in collection\n    col.objects.link(obj)\n\n    # convert vertices to mesh\n    new_vertices = []\n    bm = bmesh.new()\n    for v in bouding_box:\n        new_vertices.append(bm.verts.new(v))\n    # create all 6 surfaces, the ordering is depending on the ordering of the vertices in the bounding box\n    bm.faces.new([new_vertices[0], new_vertices[1], new_vertices[2], new_vertices[3]])\n    bm.faces.new([new_vertices[0], new_vertices[4], new_vertices[5], new_vertices[1]])\n    bm.faces.new([new_vertices[1], new_vertices[5], new_vertices[6], new_vertices[2]])\n    bm.faces.new([new_vertices[2], new_vertices[3], new_vertices[7], new_vertices[6]])\n    bm.faces.new([new_vertices[0], new_vertices[4], new_vertices[7], new_vertices[3]])\n    bm.faces.new([new_vertices[4], new_vertices[5], new_vertices[6], new_vertices[7]])\n    bm.to_mesh(mesh)\n    bm.free()\n    return obj",
  "def get_all_blender_mesh_objects() -> List[bpy.types.Object]:\n    \"\"\"\n    Returns a list of all mesh objects in the scene\n    :return: a list of all mesh objects\n    \"\"\"\n    return [obj for obj in bpy.context.scene.objects if obj.type == 'MESH']",
  "def get_all_materials() -> List[bpy.types.Material]:\n    \"\"\"\n    Returns a list of all materials used and unused\n    :return: a list of all materials\n    \"\"\"\n    return list(bpy.data.materials)",
  "def get_all_textures() -> List[bpy.types.Texture]:\n    \"\"\"\n    Returns a list of all textures.\n    :return: All textures. Type: list.\n    \"\"\"\n    return list(bpy.data.textures)",
  "def load_image(file_path: str, num_channels: int = 3) -> np.ndarray:\n    \"\"\" Load the image at the given path returns its pixels as a numpy array.\n\n    The alpha channel is neglected.\n\n    :param file_path: The path to the image.\n    :param num_channels: Number of channels to return.\n    :return: The numpy array\n    \"\"\"\n    file_ending = file_path[file_path.rfind(\".\") + 1:].lower()\n    if file_ending in [\"exr\", \"png\"]:\n        try:\n            return imageio.imread(file_path)[:, :, :num_channels]\n        except ValueError:\n            print(\"It seems the freeimage library which is necessary to read .exr files cannot \"\n                  \"be found on your computer.\")\n            print(\"Gonna try to download it automatically.\")\n\n            # Since PEP 476 the certificate of https connections is verified per default.\n            # However, in the blender python env no local certificates seem to be found which makes\n            # certification impossible.\n            # Therefore, we have to switch certificate verification off for now.\n            if hasattr(ssl, '_create_unverified_context'):\n                # pylint: disable=protected-access\n                prev_context = ssl._create_default_https_context\n                ssl._create_default_https_context = ssl._create_unverified_context\n                # pylint: enable=protected-access\n\n            # Download free image library\n            imageio.plugins.freeimage.download()\n\n            # Undo certificate check changes\n            if hasattr(ssl, '_create_unverified_context'):\n                # pylint: disable=protected-access\n                ssl._create_default_https_context = prev_context\n                # pylint: enable=protected-access\n\n            try:\n                # Try again\n                return imageio.imread(file_path)[:, :, :num_channels]\n            except ValueError as e2:\n                error = \"The automatic installation of the freeimage library failed, so you need to install \" \\\n                        \"the imageio .exr extension manually. This is quite simple: \\n\"\n                error += \"Use a different python environment (not blenders internal environment), \" \\\n                         \"`pip install imageio`.\\n\"\n                error += 'And then execute the following command in this env: \\n'\n                error += '`python -c \"import imageio; imageio.plugins.freeimage.download()\"`\\n'\n                error += \"Now everything should work -> run the pipeline again.\"\n                raise RuntimeError(error) from e2\n    elif file_ending in [\"jpg\"]:\n        img = cv2.imread(file_path)  # reads an image in the BGR format\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        return img\n    else:\n        raise NotImplementedError(\"File with ending \" + file_ending + \" cannot be loaded.\")",
  "def collect_all_orphan_data_blocks() -> Dict[str, Any]:\n    \"\"\" Returns all orphan data blocks grouped by their type\n\n    :return: A dict of sets\n    \"\"\"\n    orphans = defaultdict(set)\n    # Go over all datablock types\n    for collection_name in dir(bpy.data):\n        collection = getattr(bpy.data, collection_name)\n        if isinstance(collection, bpy.types.bpy_prop_collection):\n            # Go over all datablocks of that type\n            for datablock in collection:\n                # Add them to the list if they are orphan\n                if datablock.users == 0:\n                    orphans[collection_name].add(datablock)\n\n    return orphans",
  "def copy_attributes(attributes: list, old_prop: str, new_prop: str):\n    \"\"\"\n    Copies the list of attributes from the old to the new prop if the attribute exists.\n\n    :param: attributes: Current selected attributes\n    :param: old_prop: Old property\n    :param: new_prop: New property\n    \"\"\"\n\n    # check if the attribute exists and copy it\n    for attr in attributes:\n        if hasattr(new_prop, attr):\n            setattr(new_prop, attr, getattr(old_prop, attr))",
  "def get_node_attributes(node: bpy.types.Node) -> List[str]:\n    \"\"\"\n    Returns a list of all properties identifiers if they should not be ignored\n\n    :param: node: the node which attributes should be returned\n    :return: list of attributes of the given node\n    \"\"\"\n\n    # all attributes that shouldn't be copied\n    ignore_attributes = (\"rna_type\", \"type\", \"dimensions\", \"inputs\", \"outputs\", \"internal_links\", \"select\",\n                         \"texture_mapping\", \"color_mapping\", \"image_user\", \"interface\")\n\n    attributes = []\n    for attr in node.bl_rna.properties:\n        # check if the attribute should be copied and add it to the list of attributes to copy\n        if not attr.identifier in ignore_attributes and not attr.identifier.split(\"_\")[0] == \"bl\":\n            attributes.append(attr.identifier)\n\n    return attributes",
  "def copy_nodes(nodes: bpy.types.Nodes, goal_nodes: bpy.types.Nodes):\n    \"\"\"\n    Copies all nodes from the given list into the group with their attributes\n\n    :param: node: the nodes which should be copied\n    :param: goal_nodes: the nodes where they should be copied too\n    \"\"\"\n\n    if len(goal_nodes) > 0:\n        raise Exception(f\"This function only works if goal_nodes was empty before, has {len(goal_nodes)} elements.\")\n\n    # the attributes that should be copied for every link\n    input_attributes = [\"default_value\", \"name\"]\n    output_attributes = [\"default_value\", \"name\"]\n\n    for node in nodes:\n        # create a new node in the goal_nodes and find and copy its attributes\n        new_node = goal_nodes.new(node.bl_idname)\n        node_attributes = get_node_attributes(node)\n        copy_attributes(node_attributes, node, new_node)\n\n        # copy the attributes for all inputs\n        for inp, new_inp in zip(node.inputs, new_node.inputs):\n            copy_attributes(input_attributes, inp, new_inp)\n\n        # copy the attributes for all outputs\n        for out, new_out in zip(node.outputs, new_node.outputs):\n            copy_attributes(output_attributes, out, new_out)",
  "def copy_links(nodes: bpy.types.Nodes, goal_nodes: bpy.types.Nodes, goal_links: bpy.types.NodeLinks):\n    \"\"\"\n    Copies all links between the nodes to goal_links with the goal_nodes.\n\n    :param nodes: Nodes, which are used as base for the copying\n    :param goal_nodes: Nodes, which are will be newly connected\n    :param goal_links: Links, where all the newly generated links are saved\n    \"\"\"\n\n    for node in nodes:\n        # find the corresponding node\n        new_node = goal_nodes[node.name]\n\n        # enumerate over every link in the nodes inputs\n        for i, inp in enumerate(node.inputs):\n            for link in inp.links:\n                # find the connected node for the link\n                connected_node = goal_nodes[link.from_node.name]\n                # connect the goal nodes\n                goal_links.new(connected_node.outputs[link.from_socket.name], new_node.inputs[i])",
  "def add_group_nodes(group: bpy.types.ShaderNodeTree) -> Tuple[bpy.types.Node, bpy.types.Node]:\n    \"\"\"\n    Adds the group input and output node and positions them correctly.\n\n    :param group: the group which will get an output and input node\n    :return bpy.types.NodeGroupInput, bpy.types.NodeGroupOutput: the input and output to the given group\n    \"\"\"\n\n    # add group input and output\n    group_input = group.nodes.new(\"NodeGroupInput\")\n    group_output = group.nodes.new(\"NodeGroupOutput\")\n\n    # if there are any nodes in the group, find the min and maxi x position of all nodes and position the group nodes\n    if len(group.nodes) > 0:\n        min_pos = 9999999\n        max_pos = -9999999\n\n        for node in group.nodes:\n            if node.location[0] < min_pos:\n                min_pos = node.location[0]\n            elif node.location[0] + node.width > max_pos:\n                max_pos = node.location[0]\n\n        group_input.location = (min_pos - 250, 0)\n        group_output.location = (max_pos + 250, 0)\n    return group_input, group_output",
  "def copy_nodes_from_mat_to_material(from_material: bpy.types.Material, to_material: bpy.types.Material):\n    \"\"\"\n    Copy nodes from one material to another material\n\n    :param from_material: The material from which the nodes are selected\n    :param to_material: The material to which the nodes will be copied\n    \"\"\"\n    # get the list of all selected nodes from the active objects active material\n    nodes = from_material.node_tree.nodes\n\n    # copy all nodes from from_material to the to_material with all their attributes\n    copy_nodes(nodes, to_material.node_tree.nodes)\n\n    # copy the links between the nodes to the to_material\n    copy_links(nodes, to_material.node_tree.nodes, to_material.node_tree.links)",
  "def add_nodes_to_group(nodes: bpy.types.Node, group_name: str) -> bpy.types.ShaderNodeTree:\n    \"\"\"\n    Creates the node group, copies all attributes and links and adds the group input and output\n    https://blender.stackexchange.com/a/175604\n\n    :param nodes: Nodes, which should be used\n    :param group_name: Name of the group\n    :return bpy.types.ShaderNodeTree: the group which can be used inside of a bpy.types.ShaderNodeGroup\n    \"\"\"\n    # create new node group\n    group = bpy.data.node_groups.new(name=group_name, type=\"ShaderNodeTree\")\n\n    # copy all nodes from the list to the created group with all their attributes\n    copy_nodes(nodes, group.nodes)\n\n    # copy the links between the nodes to the created groups nodes\n    copy_links(nodes, group.nodes, group.links)\n\n    # add the group input and output node to the created group\n    _, group_output = add_group_nodes(group)\n\n    # check if the selection of nodes goes over a material, if so replace the material output with the output of\n    # the group\n    material_outputs = Utility.get_nodes_with_type(group.nodes, \"OutputMaterial\")\n    if len(material_outputs) == 1:\n        for input_node in material_outputs[0].inputs:\n            group.outputs.new(input_node.bl_idname, input_node.name)\n            for link in input_node.links:\n                group.links.new(link.from_socket, group_output.inputs[input_node.name])\n        # remove the material output, the material output should never be inside of a group\n        group.nodes.remove(material_outputs[0])\n    return group",
  "class LabelIdMapping:\n    \"\"\" Handles category id mapping for semantic segmentation maps. \"\"\"\n\n    def __init__(self):\n        # maps from an id to its name. E.g. id_label_map[0] = \"void\"\n        self._id_label_map = {}\n        # maps a class/category name to its id. E.g. label_id_map[\"void\"] = 0\n        self._label_id_map = {}\n        self._num_ids = 0\n\n    @staticmethod\n    def from_csv(path, label_col_name=\"name\", id_col_name=\"id\") -> \"LabelIdMapping\":\n        \"\"\" Builds a label-id mapping based on the given csv file.\n\n        :param path: The path to a csv file.\n        :param label_col_name: The name of the column which should be used as label.\n        :param id_col_name: The name of the column which should be used as id.\n        :return: The built label mapping object.\n        \"\"\"\n        with open(path, 'r', encoding=\"utf-8\") as csv_file:\n            reader = csv.DictReader(csv_file)\n            mapping = LabelIdMapping()\n\n            for row in reader:\n                mapping.add(row[label_col_name], int(row[id_col_name]))\n\n            return mapping\n\n    @staticmethod\n    def from_dict(label_to_id: dict) -> \"LabelIdMapping\":\n        \"\"\" Builds a label-id mapping based on the given dict.\n\n        :param label_to_id: A dict where keys are labels and values are ids.\n        :return: The built label mapping object.\n        \"\"\"\n        mapping = LabelIdMapping()\n        for label, id_value in label_to_id.items():\n            mapping.add(label, id_value)\n        return mapping\n\n    def add(self, label: str, id_value: int):\n        \"\"\" Inserts the given label-id pair into the mapping.\n\n        :param label: The label of the pair.\n        :param id_value: The id of the pair\n        \"\"\"\n        if self.has_id(id_value):\n            raise Exception(\"There already exists a label-id mapping for the id \" + str(id_value))\n        if self.has_label(label):\n            raise Exception(\"There already exists a label-id mapping for the label \" + label)\n\n        self._id_label_map[id_value] = label\n        self._label_id_map[label] = id_value\n        self._num_ids = max(self._num_ids, id_value + 1)\n\n    def id_from_label(self, label: str) -> int:\n        \"\"\" Returns the id assigned to the given label.\n\n        :param label: The label to look for.\n        :return: The id with the given label.\n        \"\"\"\n        return self._label_id_map[label]\n\n    def label_from_id(self, id_value: int) -> str:\n        \"\"\" Returns the label assigned to the given id.\n\n        :param id_value: The id to look for.\n        :return: The label with the given id.\n        \"\"\"\n        return self._id_label_map[id_value]\n\n    def has_label(self, label: str) -> bool:\n        \"\"\" Checks if the mapping contains the given label.\n\n        :param label: The label to look for.\n        :return: True, if the label is already in use.\n        \"\"\"\n        return label in self._label_id_map\n\n    def has_id(self, id_value: int) -> bool:\n        \"\"\" Checks if the mapping contains the given id.\n\n        :param id_value: The id to look for.\n        :return: True, if the id is already in use.\n        \"\"\"\n        return id_value in self._id_label_map",
  "def __init__(self):\n        # maps from an id to its name. E.g. id_label_map[0] = \"void\"\n        self._id_label_map = {}\n        # maps a class/category name to its id. E.g. label_id_map[\"void\"] = 0\n        self._label_id_map = {}\n        self._num_ids = 0",
  "def from_csv(path, label_col_name=\"name\", id_col_name=\"id\") -> \"LabelIdMapping\":\n        \"\"\" Builds a label-id mapping based on the given csv file.\n\n        :param path: The path to a csv file.\n        :param label_col_name: The name of the column which should be used as label.\n        :param id_col_name: The name of the column which should be used as id.\n        :return: The built label mapping object.\n        \"\"\"\n        with open(path, 'r', encoding=\"utf-8\") as csv_file:\n            reader = csv.DictReader(csv_file)\n            mapping = LabelIdMapping()\n\n            for row in reader:\n                mapping.add(row[label_col_name], int(row[id_col_name]))\n\n            return mapping",
  "def from_dict(label_to_id: dict) -> \"LabelIdMapping\":\n        \"\"\" Builds a label-id mapping based on the given dict.\n\n        :param label_to_id: A dict where keys are labels and values are ids.\n        :return: The built label mapping object.\n        \"\"\"\n        mapping = LabelIdMapping()\n        for label, id_value in label_to_id.items():\n            mapping.add(label, id_value)\n        return mapping",
  "def add(self, label: str, id_value: int):\n        \"\"\" Inserts the given label-id pair into the mapping.\n\n        :param label: The label of the pair.\n        :param id_value: The id of the pair\n        \"\"\"\n        if self.has_id(id_value):\n            raise Exception(\"There already exists a label-id mapping for the id \" + str(id_value))\n        if self.has_label(label):\n            raise Exception(\"There already exists a label-id mapping for the label \" + label)\n\n        self._id_label_map[id_value] = label\n        self._label_id_map[label] = id_value\n        self._num_ids = max(self._num_ids, id_value + 1)",
  "def id_from_label(self, label: str) -> int:\n        \"\"\" Returns the id assigned to the given label.\n\n        :param label: The label to look for.\n        :return: The id with the given label.\n        \"\"\"\n        return self._label_id_map[label]",
  "def label_from_id(self, id_value: int) -> str:\n        \"\"\" Returns the label assigned to the given id.\n\n        :param id_value: The id to look for.\n        :return: The label with the given id.\n        \"\"\"\n        return self._id_label_map[id_value]",
  "def has_label(self, label: str) -> bool:\n        \"\"\" Checks if the mapping contains the given label.\n\n        :param label: The label to look for.\n        :return: True, if the label is already in use.\n        \"\"\"\n        return label in self._label_id_map",
  "def has_id(self, id_value: int) -> bool:\n        \"\"\" Checks if the mapping contains the given id.\n\n        :param id_value: The id to look for.\n        :return: True, if the id is already in use.\n        \"\"\"\n        return id_value in self._id_label_map",
  "def init(clean_up_scene: bool = True):\n    \"\"\" Initializes BlenderProc.\n\n    Cleans up the whole scene at first and then initializes basic blender settings, the world, the renderer and\n    the camera. This method should only be called once in the beginning. If you want to clean up the scene afterwards,\n    use bproc.clean_up()\n\n    :param clean_up_scene: Set to False, if you want to keep all scene data.\n    \"\"\"\n    # Check if init has already been run\n    if GlobalStorage.is_in_storage(\"bproc_init_complete\") and GlobalStorage.get(\"bproc_init_complete\"):\n        raise RuntimeError(\"BlenderProc has already been initialized via bproc.init(), this should not be done twice. \"\n                           \"If you want to clean up the scene, use bproc.clean_up().\")\n\n    if clean_up_scene:\n        clean_up(clean_up_camera=True)\n\n    # Set language if necessary\n    if bpy.context.preferences.view.language != \"en_US\":\n        print(\"Setting blender language settings to english during this run\")\n        bpy.context.preferences.view.language = \"en_US\"\n\n    # Use cycles\n    bpy.context.scene.render.engine = 'CYCLES'\n\n    # Set default render devices\n    RendererUtility.set_render_devices()\n\n    # Set default parameters\n    _Initializer.set_default_parameters()\n\n    random_seed = os.getenv(\"BLENDER_PROC_RANDOM_SEED\")\n    if random_seed:\n        print(f\"Got random seed: {random_seed}\")\n        try:\n            random_seed = int(random_seed)\n        except ValueError as e:\n            raise e\n        random.seed(random_seed)\n        np_random.seed(random_seed)\n\n    # Remember init was completed\n    GlobalStorage.add(\"bproc_init_complete\", True)",
  "def clean_up(clean_up_camera: bool = False):\n    \"\"\" Resets the scene to its clean state.\n\n    This method removes all objects, camera poses and cleans up the world background.\n    All (renderer) settings and the UI are kept as they are.\n\n    :param clean_up_camera: If True, also the camera is set back to its clean state.\n    \"\"\"\n    # Switch to right context\n    if bpy.context.object is not None and bpy.context.object.mode != \"OBJECT\":\n        bpy.ops.object.mode_set(mode='OBJECT')\n\n    # Clean up\n    _Initializer.remove_all_data(clean_up_camera)\n    _Initializer.remove_custom_properties()\n\n    # Create new world\n    new_world = bpy.data.worlds.new(\"World\")\n    bpy.context.scene.world = new_world\n    new_world[\"category_id\"] = 0\n\n    if clean_up_camera:\n        # Create the camera\n        cam = bpy.data.cameras.new(\"Camera\")\n        cam_ob = bpy.data.objects.new(\"Camera\", cam)\n        bpy.context.scene.collection.objects.link(cam_ob)\n        bpy.context.scene.camera = cam_ob\n\n    # Make sure keyframes are cleaned up\n    reset_keyframes()",
  "class _Initializer:\n    \"\"\"\n    This is the initializer class used to init a BlenderProc scene.\n    \"\"\"\n\n    @staticmethod\n    def set_default_parameters():\n        \"\"\" Loads and sets default parameters defined in DefaultConfig.py \"\"\"\n        # Set default intrinsics\n        CameraUtility.set_intrinsics_from_blender_params(DefaultConfig.fov, DefaultConfig.resolution_x,\n                                                         DefaultConfig.resolution_y, DefaultConfig.clip_start,\n                                                         DefaultConfig.clip_end, DefaultConfig.pixel_aspect_x,\n                                                         DefaultConfig.pixel_aspect_y, DefaultConfig.shift_x,\n                                                         DefaultConfig.shift_y, DefaultConfig.lens_unit)\n        CameraUtility.set_stereo_parameters(DefaultConfig.stereo_convergence_mode,\n                                            DefaultConfig.stereo_convergence_distance,\n                                            DefaultConfig.stereo_interocular_distance)\n\n        # Init renderer\n        RendererUtility.render_init()\n        RendererUtility.set_world_background(DefaultConfig.world_background)\n        RendererUtility.set_max_amount_of_samples(DefaultConfig.samples)\n        RendererUtility.set_noise_threshold(DefaultConfig.sampling_noise_threshold)\n\n        # Set number of cpu cores used for rendering (1 thread is always used for coordination => 1\n        # cpu thread means GPU-only rendering)\n        RendererUtility.set_cpu_threads(0)\n        RendererUtility.set_denoiser(DefaultConfig.denoiser)\n\n        RendererUtility.set_simplify_subdivision_render(DefaultConfig.simplify_subdivision_render)\n\n        RendererUtility.set_light_bounces(DefaultConfig.diffuse_bounces,\n                                          DefaultConfig.glossy_bounces,\n                                          DefaultConfig.ao_bounces_render,\n                                          DefaultConfig.max_bounces,\n                                          DefaultConfig.transmission_bounces,\n                                          DefaultConfig.transparency_bounces,\n                                          DefaultConfig.volume_bounces)\n\n        RendererUtility.set_output_format(DefaultConfig.file_format,\n                                          DefaultConfig.color_depth,\n                                          DefaultConfig.enable_transparency,\n                                          DefaultConfig.jpg_quality)\n\n    @staticmethod\n    def remove_all_data(remove_camera: bool = True):\n        \"\"\" Remove all data blocks except opened scripts, the default scene and the camera.\n\n        :param remove_camera: If True, also the default camera is removed.\n        \"\"\"\n        # Go through all attributes of bpy.data\n        for collection in dir(bpy.data):\n            data_structure = getattr(bpy.data, collection)\n            # Check that it is a data collection\n            if isinstance(data_structure, bpy.types.bpy_prop_collection) and hasattr(data_structure, \"remove\") \\\n                    and collection not in [\"texts\"]:\n                # Go over all entities in that collection\n                for block in data_structure:\n                    # Skip the default scene\n                    if isinstance(block, bpy.types.Scene) and block.name == \"Scene\":\n                        continue\n                    # If desired, skip camera\n                    if not remove_camera and isinstance(block, (bpy.types.Object, bpy.types.Camera)) \\\n                            and block.name == \"Camera\":\n                        continue\n                    data_structure.remove(block)\n\n    @staticmethod\n    def remove_custom_properties():\n        \"\"\" Remove all custom properties registered at global entities like the scene. \"\"\"\n        for key in bpy.context.scene.keys():\n            del bpy.context.scene[key]",
  "def set_default_parameters():\n        \"\"\" Loads and sets default parameters defined in DefaultConfig.py \"\"\"\n        # Set default intrinsics\n        CameraUtility.set_intrinsics_from_blender_params(DefaultConfig.fov, DefaultConfig.resolution_x,\n                                                         DefaultConfig.resolution_y, DefaultConfig.clip_start,\n                                                         DefaultConfig.clip_end, DefaultConfig.pixel_aspect_x,\n                                                         DefaultConfig.pixel_aspect_y, DefaultConfig.shift_x,\n                                                         DefaultConfig.shift_y, DefaultConfig.lens_unit)\n        CameraUtility.set_stereo_parameters(DefaultConfig.stereo_convergence_mode,\n                                            DefaultConfig.stereo_convergence_distance,\n                                            DefaultConfig.stereo_interocular_distance)\n\n        # Init renderer\n        RendererUtility.render_init()\n        RendererUtility.set_world_background(DefaultConfig.world_background)\n        RendererUtility.set_max_amount_of_samples(DefaultConfig.samples)\n        RendererUtility.set_noise_threshold(DefaultConfig.sampling_noise_threshold)\n\n        # Set number of cpu cores used for rendering (1 thread is always used for coordination => 1\n        # cpu thread means GPU-only rendering)\n        RendererUtility.set_cpu_threads(0)\n        RendererUtility.set_denoiser(DefaultConfig.denoiser)\n\n        RendererUtility.set_simplify_subdivision_render(DefaultConfig.simplify_subdivision_render)\n\n        RendererUtility.set_light_bounces(DefaultConfig.diffuse_bounces,\n                                          DefaultConfig.glossy_bounces,\n                                          DefaultConfig.ao_bounces_render,\n                                          DefaultConfig.max_bounces,\n                                          DefaultConfig.transmission_bounces,\n                                          DefaultConfig.transparency_bounces,\n                                          DefaultConfig.volume_bounces)\n\n        RendererUtility.set_output_format(DefaultConfig.file_format,\n                                          DefaultConfig.color_depth,\n                                          DefaultConfig.enable_transparency,\n                                          DefaultConfig.jpg_quality)",
  "def remove_all_data(remove_camera: bool = True):\n        \"\"\" Remove all data blocks except opened scripts, the default scene and the camera.\n\n        :param remove_camera: If True, also the default camera is removed.\n        \"\"\"\n        # Go through all attributes of bpy.data\n        for collection in dir(bpy.data):\n            data_structure = getattr(bpy.data, collection)\n            # Check that it is a data collection\n            if isinstance(data_structure, bpy.types.bpy_prop_collection) and hasattr(data_structure, \"remove\") \\\n                    and collection not in [\"texts\"]:\n                # Go over all entities in that collection\n                for block in data_structure:\n                    # Skip the default scene\n                    if isinstance(block, bpy.types.Scene) and block.name == \"Scene\":\n                        continue\n                    # If desired, skip camera\n                    if not remove_camera and isinstance(block, (bpy.types.Object, bpy.types.Camera)) \\\n                            and block.name == \"Camera\":\n                        continue\n                    data_structure.remove(block)",
  "def remove_custom_properties():\n        \"\"\" Remove all custom properties registered at global entities like the scene. \"\"\"\n        for key in bpy.context.scene.keys():\n            del bpy.context.scene[key]",
  "class InstallUtility:\n    \"\"\"\n    This class provides functions to install BlenderProc and set up the correct environment\n    \"\"\"\n\n    @staticmethod\n    def determine_blender_install_path(used_args: \"argparse.NameSpace\") -> Union[str, str]:\n        \"\"\" Determines the path of the blender installation\n\n        :param used_args: The given command line arguments.\n        :return:\n               - The path to an already existing blender installation that should be used, otherwise None\n               - The path to where blender should be installed.\n        \"\"\"\n        custom_blender_path = used_args.custom_blender_path\n        blender_install_path = used_args.blender_install_path\n\n        # If no blender install path is given set it to /home_local/<env:USER>/blender/ per default\n        if blender_install_path is None:\n            user_name = getpass.getuser()\n            blender_install_path = os.path.join(\"/home_local\", user_name, \"blender\")\n        return custom_blender_path, blender_install_path\n\n    @staticmethod\n    def make_sure_blender_is_installed(custom_blender_path: str, blender_install_path: str,\n                                       reinstall_blender: bool = False) -> Tuple[str, str]:\n        \"\"\" Make sure blender is installed.\n\n        :param custom_blender_path: The path to an already existing blender installation that should\n                                    be used, otherwise None.\n        :param blender_install_path: The path to where blender should be installed.\n        :param reinstall_blender: If True, blender will be forced to reinstall.\n        :return:\n               - The path to the blender binary.\n               - The major version of the blender installation.\n        \"\"\"\n        # If blender should be downloaded automatically\n        if custom_blender_path is None:\n            # Determine path where blender should be installed\n            if blender_install_path is not None:\n                blender_install_path = os.path.expanduser(blender_install_path)\n                if blender_install_path.startswith(\"/home_local\") and not os.path.exists(\"/home_local\"):\n                    user_name = getpass.getuser()\n                    home_path = os.getenv(\"USERPROFILE\") if platform == \"win32\" else os.getenv(\"HOME\")\n                    print(f\"Warning: Changed install path from {join('/home_local', user_name)}... to {home_path}..., \"\n                          f\"there is no /home_local/ on this machine.\")\n                    # Replace the seperator from '/' to the os-specific one\n                    # Since all example config files use '/' as seperator\n                    blender_install_path = blender_install_path.replace(os.path.join(\"/home_local\", user_name),\n                                                                        home_path, 1)\n                    blender_install_path = blender_install_path.replace('/', os.path.sep)\n            else:\n                blender_install_path = \"blender\"\n\n            # Determine configured version\n            # right new only support blender-3.3.1\n            major_version = \"3.3\"\n            minor_version = \"1\"\n            blender_version = f\"blender-{major_version}.{minor_version}\"\n            if platform in [\"linux\", \"linux2\"]:\n                blender_version += \"-linux-x64\"\n                blender_path = os.path.join(blender_install_path, blender_version)\n            elif platform == \"darwin\":\n                # check if the current mac uses an Intel x86 processor\n                if \"x86\" in machine():\n                    blender_version += \"-macos-x64\"\n                else:\n                    # or an Apple Silicon\n                    blender_version += \"-macos-arm64\"\n                blender_install_path = os.path.join(blender_install_path, blender_version)\n                blender_path = os.path.join(blender_install_path, \"Blender.app\")\n            elif platform == \"win32\":\n                blender_version += \"-windows-x64\"\n                blender_install_path = os.path.join(blender_install_path, blender_version)\n                blender_path = blender_install_path\n            else:\n                raise RuntimeError(f\"This system is not supported yet: {platform}\")\n\n            # If forced reinstall is demanded, remove existing files\n            if os.path.exists(blender_path) and reinstall_blender:\n                print(\"Removing existing blender installation\")\n                shutil.rmtree(blender_path)\n\n            # Download blender if it not already exists\n            if not os.path.exists(blender_path):\n                if version_info.major != 3:\n                    try:\n                        # pylint: disable=import-outside-toplevel\n                        import lzma\n                        # pylint: enable=import-outside-toplevel\n                    except ImportError as e:\n                        print(\"For decompressing \\\".xz\\\" files in python 2.x is it necessary to use lzma\")\n                        raise e  # from import lzma -> pip install --user pyliblzma\n                used_url = \"https://download.blender.org/release/Blender\" + major_version + \"/\" + blender_version\n                if platform in [\"linux\", \"linux2\"]:\n                    url = used_url + \".tar.xz\"\n                elif platform == \"darwin\":\n                    url = used_url + \".dmg\"\n                elif platform == \"win32\":\n                    url = used_url + \".zip\"\n                else:\n                    raise RuntimeError(f\"This system is not supported yet: {platform}\")\n                try:\n                    try:\n                        # pylint: disable=import-outside-toplevel\n                        import progressbar\n                        # pylint: enable=import-outside-toplevel\n                        class DownloadProgressBar:\n                            \"\"\"\n                            Download progress bar, uses the progressbar library to display a progressbar during download\n                            \"\"\"\n                            def __init__(self):\n                                self.pbar = None\n\n                            def __call__(self, block_num, block_size, total_size):\n                                if not self.pbar:\n                                    self.pbar = progressbar.ProgressBar(maxval=total_size)\n                                    self.pbar.start()\n                                downloaded = block_num * block_size\n                                if downloaded < total_size:\n                                    self.pbar.update(downloaded)\n                                else:\n                                    self.pbar.finish()\n\n                        print(\"Downloading blender from \" + url)\n                        file_tmp = urlretrieve(url, None, DownloadProgressBar())[0]\n                    except ImportError:\n                        print(\"Progressbar for downloading, can only be shown, \"\n                              \"when the python package \\\"progressbar\\\" is installed\")\n                        file_tmp = urlretrieve(url, None)[0]\n                except URLError as e:\n                    if platform == \"win32\":\n                        # on windows this is a known problem that the ssl certificates doesn't properly work\n                        # deactivate the ssl check\n                        if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n                                getattr(ssl, '_create_unverified_context', None)):\n                            # pylint: disable=protected-access\n                            ssl._create_default_https_context = ssl._create_unverified_context\n                            # pylint: enable=protected-access\n                        file_tmp = urlretrieve(url, None)[0]\n                    else:\n                        raise e\n                if platform in [\"linux\", \"linux2\"]:\n                    if version_info.major == 3:\n                        SetupUtility.extract_file(blender_install_path, file_tmp, \"TAR\")\n                    else:\n                        with contextlib.closing(lzma.LZMAFile(file_tmp)) as xz:\n                            with tarfile.open(fileobj=xz) as f:\n                                f.extractall(blender_install_path)\n                elif platform == \"darwin\":\n                    if not os.path.exists(blender_install_path):\n                        os.makedirs(blender_install_path)\n                    os.rename(file_tmp, os.path.join(blender_install_path, blender_version + \".dmg\"))\n                    # pylint: disable=consider-using-with\n                    # installing the blender app by mounting it and extracting the information\n                    subprocess.Popen([f\"hdiutil attach {os.path.join(blender_install_path, blender_version + '.dmg')}\"],\n                                     shell=True).wait()\n                    subprocess.Popen([f'cp -r {os.path.join(\"/\", \"Volumes\", \"Blender\", \"Blender.app\")} '\n                                      f'{blender_install_path}'], shell=True).wait()\n                    subprocess.Popen([f'diskutil unmount {os.path.join(\"/\", \"Volumes\", \"Blender\")}'], shell=True)\n                    # removing the downloaded image again\n                    subprocess.Popen([f'rm {os.path.join(blender_install_path, blender_version + \".dmg\")}'],\n                                     shell=True).wait()\n                    # pylint: enable=consider-using-with\n                    # add Blender.app path to it\n                elif platform == \"win32\":\n                    SetupUtility.extract_file(blender_install_path, file_tmp)\n                # rename the blender folder to better fit our existing scheme\n                for folder in os.listdir(blender_install_path):\n                    if os.path.isdir(os.path.join(blender_install_path, folder)) and \\\n                            folder.startswith(f\"blender-{major_version}.{minor_version}\"):\n                        os.rename(os.path.join(blender_install_path, folder),\n                                  os.path.join(blender_install_path, blender_version))\n        else:\n            blender_path = os.path.expanduser(custom_blender_path)\n\n            # Try to get major version of given blender installation\n            major_version = None\n            for sub_dir in os.listdir(blender_path):\n                # Search for the subdirectory which has the major version as its name\n                if os.path.isdir(os.path.join(blender_path, sub_dir)) and sub_dir.replace(\".\", \"\").isdigit():\n                    major_version = sub_dir\n                    break\n\n            if major_version is None:\n                raise RuntimeError(\"Could not determine major blender version\")\n\n        print(\"Using blender in \" + blender_path)\n\n        # Run script\n        if platform in [\"linux\", \"linux2\"]:\n            blender_run_path = os.path.join(blender_path, \"blender\")\n        elif platform == \"darwin\":\n            blender_run_path = os.path.join(blender_path, \"Contents\", \"MacOS\", \"Blender\")\n        elif platform == \"win32\":\n            blender_run_path = os.path.join(blender_install_path, blender_version, \"blender\")\n        else:\n            raise RuntimeError(f\"This system is not supported yet: {platform}\")\n\n        return blender_run_path, major_version",
  "def determine_blender_install_path(used_args: \"argparse.NameSpace\") -> Union[str, str]:\n        \"\"\" Determines the path of the blender installation\n\n        :param used_args: The given command line arguments.\n        :return:\n               - The path to an already existing blender installation that should be used, otherwise None\n               - The path to where blender should be installed.\n        \"\"\"\n        custom_blender_path = used_args.custom_blender_path\n        blender_install_path = used_args.blender_install_path\n\n        # If no blender install path is given set it to /home_local/<env:USER>/blender/ per default\n        if blender_install_path is None:\n            user_name = getpass.getuser()\n            blender_install_path = os.path.join(\"/home_local\", user_name, \"blender\")\n        return custom_blender_path, blender_install_path",
  "def make_sure_blender_is_installed(custom_blender_path: str, blender_install_path: str,\n                                       reinstall_blender: bool = False) -> Tuple[str, str]:\n        \"\"\" Make sure blender is installed.\n\n        :param custom_blender_path: The path to an already existing blender installation that should\n                                    be used, otherwise None.\n        :param blender_install_path: The path to where blender should be installed.\n        :param reinstall_blender: If True, blender will be forced to reinstall.\n        :return:\n               - The path to the blender binary.\n               - The major version of the blender installation.\n        \"\"\"\n        # If blender should be downloaded automatically\n        if custom_blender_path is None:\n            # Determine path where blender should be installed\n            if blender_install_path is not None:\n                blender_install_path = os.path.expanduser(blender_install_path)\n                if blender_install_path.startswith(\"/home_local\") and not os.path.exists(\"/home_local\"):\n                    user_name = getpass.getuser()\n                    home_path = os.getenv(\"USERPROFILE\") if platform == \"win32\" else os.getenv(\"HOME\")\n                    print(f\"Warning: Changed install path from {join('/home_local', user_name)}... to {home_path}..., \"\n                          f\"there is no /home_local/ on this machine.\")\n                    # Replace the seperator from '/' to the os-specific one\n                    # Since all example config files use '/' as seperator\n                    blender_install_path = blender_install_path.replace(os.path.join(\"/home_local\", user_name),\n                                                                        home_path, 1)\n                    blender_install_path = blender_install_path.replace('/', os.path.sep)\n            else:\n                blender_install_path = \"blender\"\n\n            # Determine configured version\n            # right new only support blender-3.3.1\n            major_version = \"3.3\"\n            minor_version = \"1\"\n            blender_version = f\"blender-{major_version}.{minor_version}\"\n            if platform in [\"linux\", \"linux2\"]:\n                blender_version += \"-linux-x64\"\n                blender_path = os.path.join(blender_install_path, blender_version)\n            elif platform == \"darwin\":\n                # check if the current mac uses an Intel x86 processor\n                if \"x86\" in machine():\n                    blender_version += \"-macos-x64\"\n                else:\n                    # or an Apple Silicon\n                    blender_version += \"-macos-arm64\"\n                blender_install_path = os.path.join(blender_install_path, blender_version)\n                blender_path = os.path.join(blender_install_path, \"Blender.app\")\n            elif platform == \"win32\":\n                blender_version += \"-windows-x64\"\n                blender_install_path = os.path.join(blender_install_path, blender_version)\n                blender_path = blender_install_path\n            else:\n                raise RuntimeError(f\"This system is not supported yet: {platform}\")\n\n            # If forced reinstall is demanded, remove existing files\n            if os.path.exists(blender_path) and reinstall_blender:\n                print(\"Removing existing blender installation\")\n                shutil.rmtree(blender_path)\n\n            # Download blender if it not already exists\n            if not os.path.exists(blender_path):\n                if version_info.major != 3:\n                    try:\n                        # pylint: disable=import-outside-toplevel\n                        import lzma\n                        # pylint: enable=import-outside-toplevel\n                    except ImportError as e:\n                        print(\"For decompressing \\\".xz\\\" files in python 2.x is it necessary to use lzma\")\n                        raise e  # from import lzma -> pip install --user pyliblzma\n                used_url = \"https://download.blender.org/release/Blender\" + major_version + \"/\" + blender_version\n                if platform in [\"linux\", \"linux2\"]:\n                    url = used_url + \".tar.xz\"\n                elif platform == \"darwin\":\n                    url = used_url + \".dmg\"\n                elif platform == \"win32\":\n                    url = used_url + \".zip\"\n                else:\n                    raise RuntimeError(f\"This system is not supported yet: {platform}\")\n                try:\n                    try:\n                        # pylint: disable=import-outside-toplevel\n                        import progressbar\n                        # pylint: enable=import-outside-toplevel\n                        class DownloadProgressBar:\n                            \"\"\"\n                            Download progress bar, uses the progressbar library to display a progressbar during download\n                            \"\"\"\n                            def __init__(self):\n                                self.pbar = None\n\n                            def __call__(self, block_num, block_size, total_size):\n                                if not self.pbar:\n                                    self.pbar = progressbar.ProgressBar(maxval=total_size)\n                                    self.pbar.start()\n                                downloaded = block_num * block_size\n                                if downloaded < total_size:\n                                    self.pbar.update(downloaded)\n                                else:\n                                    self.pbar.finish()\n\n                        print(\"Downloading blender from \" + url)\n                        file_tmp = urlretrieve(url, None, DownloadProgressBar())[0]\n                    except ImportError:\n                        print(\"Progressbar for downloading, can only be shown, \"\n                              \"when the python package \\\"progressbar\\\" is installed\")\n                        file_tmp = urlretrieve(url, None)[0]\n                except URLError as e:\n                    if platform == \"win32\":\n                        # on windows this is a known problem that the ssl certificates doesn't properly work\n                        # deactivate the ssl check\n                        if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n                                getattr(ssl, '_create_unverified_context', None)):\n                            # pylint: disable=protected-access\n                            ssl._create_default_https_context = ssl._create_unverified_context\n                            # pylint: enable=protected-access\n                        file_tmp = urlretrieve(url, None)[0]\n                    else:\n                        raise e\n                if platform in [\"linux\", \"linux2\"]:\n                    if version_info.major == 3:\n                        SetupUtility.extract_file(blender_install_path, file_tmp, \"TAR\")\n                    else:\n                        with contextlib.closing(lzma.LZMAFile(file_tmp)) as xz:\n                            with tarfile.open(fileobj=xz) as f:\n                                f.extractall(blender_install_path)\n                elif platform == \"darwin\":\n                    if not os.path.exists(blender_install_path):\n                        os.makedirs(blender_install_path)\n                    os.rename(file_tmp, os.path.join(blender_install_path, blender_version + \".dmg\"))\n                    # pylint: disable=consider-using-with\n                    # installing the blender app by mounting it and extracting the information\n                    subprocess.Popen([f\"hdiutil attach {os.path.join(blender_install_path, blender_version + '.dmg')}\"],\n                                     shell=True).wait()\n                    subprocess.Popen([f'cp -r {os.path.join(\"/\", \"Volumes\", \"Blender\", \"Blender.app\")} '\n                                      f'{blender_install_path}'], shell=True).wait()\n                    subprocess.Popen([f'diskutil unmount {os.path.join(\"/\", \"Volumes\", \"Blender\")}'], shell=True)\n                    # removing the downloaded image again\n                    subprocess.Popen([f'rm {os.path.join(blender_install_path, blender_version + \".dmg\")}'],\n                                     shell=True).wait()\n                    # pylint: enable=consider-using-with\n                    # add Blender.app path to it\n                elif platform == \"win32\":\n                    SetupUtility.extract_file(blender_install_path, file_tmp)\n                # rename the blender folder to better fit our existing scheme\n                for folder in os.listdir(blender_install_path):\n                    if os.path.isdir(os.path.join(blender_install_path, folder)) and \\\n                            folder.startswith(f\"blender-{major_version}.{minor_version}\"):\n                        os.rename(os.path.join(blender_install_path, folder),\n                                  os.path.join(blender_install_path, blender_version))\n        else:\n            blender_path = os.path.expanduser(custom_blender_path)\n\n            # Try to get major version of given blender installation\n            major_version = None\n            for sub_dir in os.listdir(blender_path):\n                # Search for the subdirectory which has the major version as its name\n                if os.path.isdir(os.path.join(blender_path, sub_dir)) and sub_dir.replace(\".\", \"\").isdigit():\n                    major_version = sub_dir\n                    break\n\n            if major_version is None:\n                raise RuntimeError(\"Could not determine major blender version\")\n\n        print(\"Using blender in \" + blender_path)\n\n        # Run script\n        if platform in [\"linux\", \"linux2\"]:\n            blender_run_path = os.path.join(blender_path, \"blender\")\n        elif platform == \"darwin\":\n            blender_run_path = os.path.join(blender_path, \"Contents\", \"MacOS\", \"Blender\")\n        elif platform == \"win32\":\n            blender_run_path = os.path.join(blender_install_path, blender_version, \"blender\")\n        else:\n            raise RuntimeError(f\"This system is not supported yet: {platform}\")\n\n        return blender_run_path, major_version",
  "class DownloadProgressBar:\n                            \"\"\"\n                            Download progress bar, uses the progressbar library to display a progressbar during download\n                            \"\"\"\n                            def __init__(self):\n                                self.pbar = None\n\n                            def __call__(self, block_num, block_size, total_size):\n                                if not self.pbar:\n                                    self.pbar = progressbar.ProgressBar(maxval=total_size)\n                                    self.pbar.start()\n                                downloaded = block_num * block_size\n                                if downloaded < total_size:\n                                    self.pbar.update(downloaded)\n                                else:\n                                    self.pbar.finish()",
  "def __init__(self):\n                                self.pbar = None",
  "def __call__(self, block_num, block_size, total_size):\n                                if not self.pbar:\n                                    self.pbar = progressbar.ProgressBar(maxval=total_size)\n                                    self.pbar.start()\n                                downloaded = block_num * block_size\n                                if downloaded < total_size:\n                                    self.pbar.update(downloaded)\n                                else:\n                                    self.pbar.finish()",
  "class DefaultConfig:\n    \"\"\"\n    All the default config values are specified in this class.\n    \"\"\"\n    # Camera\n    resolution_x = 512\n    resolution_y = 512\n    clip_start = 0.1\n    clip_end = 1000\n    fov = 0.691111\n    pixel_aspect_x = 1\n    pixel_aspect_y = 1\n    shift_x = 0\n    shift_y = 0\n    lens_unit = \"FOV\"\n\n    # Stereo\n    stereo_convergence_mode = \"PARALLEL\"\n    stereo_convergence_distance = 0.00001\n    stereo_interocular_distance = 0.065\n\n    # Renderer\n    file_format = \"PNG\"\n    color_depth = 8\n    enable_transparency = False\n    jpg_quality = 95\n    samples = 1024\n    sampling_noise_threshold = 0.01\n    cpu_threads = 1\n    denoiser = \"INTEL\"\n    simplify_subdivision_render = 3\n    diffuse_bounces = 3\n    glossy_bounces = 0\n    ao_bounces_render = 3\n    max_bounces = 3\n    transmission_bounces = 0\n    transparency_bounces = 8\n    volume_bounces = 0\n    antialiasing_distance_max = 10000\n    world_background = [0.05, 0.05, 0.05]\n\n    # Setup\n    default_pip_packages = [\"wheel\", \"pyyaml==5.1.2\", \"imageio==2.9.0\", \"gitpython==3.1.18\",\n                            \"scikit-image==0.19.2\", \"pypng==0.0.20\", \"scipy==1.7.3\", \"matplotlib==3.5.1\",\n                            \"pytz==2021.1\", \"h5py==3.6.0\", \"Pillow==8.3.2\", \"opencv-contrib-python==4.5.5.64\",\n                            \"scikit-learn==1.0.2\", \"python-dateutil==2.8.2\", \"rich==12.6.0\", \"trimesh==3.21.5\",\n                            \"pyrender==0.1.45\"]",
  "class MaterialGetter:\n    \"\"\" Filters materials. \"\"\"\n\n    @staticmethod\n    def perform_and_condition_check(and_condition, materials, used_materials_to_check=None):\n        \"\"\" Checks for all materials in the scene if all given conditions are true, collects them in the return list.\n\n        :param and_condition: Given conditions. Type: dict.\n        :param materials: Materials, that are already in the return list. Type: list.\n        :param used_materials_to_check: a list of materials to perform the check on. Type: list. Default: all materials\n        :return: Materials that fulfilled given conditions. Type: list.\n        \"\"\"\n        new_materials = []\n        if used_materials_to_check is None:\n            used_materials_to_check = get_all_materials()\n\n        # through every material\n        for material in used_materials_to_check:\n            if material in new_materials or material in materials or material is None:\n                continue\n\n            select_material = True\n            for key, value in and_condition.items():\n                # check if the key is a requested custom property\n                requested_custom_property = False\n                requested_custom_function = False\n                if key.startswith('cp_'):\n                    requested_custom_property = True\n                    key = key[3:]\n                if key.startswith('cf_'):\n                    requested_custom_function = True\n                    key = key[3:]\n                if hasattr(material, key) and not requested_custom_property and not requested_custom_function:\n                    # check if the type of the value of attribute matches desired\n                    if isinstance(getattr(material, key), type(value)):\n                        new_value = value\n                    # if not, try to enforce some mathutils-specific type\n                    else:\n                        if isinstance(getattr(material, key), mathutils.Vector):\n                            new_value = mathutils.Vector(value)\n                        elif isinstance(getattr(material, key), mathutils.Euler):\n                            new_value = mathutils.Euler(value)\n                        elif isinstance(getattr(material, key), mathutils.Color):\n                            new_value = mathutils.Color(value)\n                        # raise an exception if it is none of them\n                        else:\n                            raise Exception(f\"Types are not matching: {type(getattr(material, key))} \"\n                                            \"and {type(value)} !\")\n                    # or check for equality\n                    if not ((isinstance(getattr(material, key), str) and\n                            re.fullmatch(value, getattr(material, key)) is not None)\n                            or getattr(material, key) == new_value):\n                        select_material = False\n                        break\n                # check if a custom property with this name exists\n                elif key in material and requested_custom_property:\n                    # check if the type of the value of such custom property matches desired\n                    if isinstance(material[key], type(value)) or (isinstance(material[key], int)\n                                                                  and isinstance(value, bool)):\n                        # if it is a string and if the whole string matches the given pattern\n                        if not ((isinstance(material[key], str) and re.fullmatch(value, material[key]) is not None) or\n                                material[key] == value):\n                            select_material = False\n                            break\n                    else:\n                        # raise an exception if not\n                        raise Exception(f\"Types are not matching: {type(material[key])} and {type(value)} !\")\n                elif requested_custom_function:\n                    if key.startswith(\"texture_amount_\"):\n                        if material.use_nodes:\n                            value = int(value)\n                            nodes = material.node_tree.nodes\n                            texture_nodes = Utility.get_nodes_with_type(nodes, \"TexImage\")\n                            amount_of_texture_nodes = len(texture_nodes) if texture_nodes is not None else 0\n                            if \"min\" in key:\n                                if not amount_of_texture_nodes >= value:\n                                    select_material = False\n                                    break\n                            elif \"max\" in key:\n                                if not amount_of_texture_nodes <= value:\n                                    select_material = False\n                                    break\n                            elif \"eq\" in key:\n                                if not amount_of_texture_nodes == value:\n                                    select_material = False\n                                    break\n                            else:\n                                raise Exception(f\"This type of key is unknown: {key}\")\n                        else:\n                            select_material = False\n                            break\n                    elif key.startswith(\"principled_bsdf_amount_\"):\n                        if material.use_nodes:\n                            value = int(value)\n                            nodes = material.node_tree.nodes\n                            principled = Utility.get_nodes_with_type(nodes, \"BsdfPrincipled\")\n                            amount_of_principled_bsdf_nodes = len(principled) if principled is not None else 0\n                            if \"min\" in key:\n                                if not amount_of_principled_bsdf_nodes >= value:\n                                    select_material = False\n                                    break\n                            elif \"max\" in key:\n                                if not amount_of_principled_bsdf_nodes <= value:\n                                    select_material = False\n                                    break\n                            elif \"eq\" in key:\n                                if not amount_of_principled_bsdf_nodes == value:\n                                    select_material = False\n                                    break\n                            else:\n                                raise Exception(f\"This type of key is unknown: {key}\")\n                        else:\n                            select_material = False\n                            break\n                    elif key.startswith(\"principled_bsdf_\"):  # must be after the amount check\n                        # This custom function can check the value of a certain Principled BSDF shader input.\n                        # For example this can be used to avoid using materials, which have an Alpha Texture by\n                        # adding they key: `\"cf_principled_bsdf_Alpha_eq\": 1.0`\n                        if material.use_nodes:\n                            value = float(value)\n                            # first check if there is only one Principled BSDF node in the material\n                            nodes = material.node_tree.nodes\n                            principled = Utility.get_nodes_with_type(nodes, \"BsdfPrincipled\")\n                            amount_of_principled_bsdf_nodes = len(principled) if principled is not None else 0\n                            if amount_of_principled_bsdf_nodes != 1:\n                                select_material = False\n                                break\n                            principled = principled[0]\n                            # then extract the input name from the key, for the Alpha example: `Alpha`\n                            extracted_input_name = key[len(\"principled_bsdf_\"):key.rfind(\"_\")]\n                            # check if this key exists, else throw an error\n                            if extracted_input_name not in principled.inputs:\n                                raise Exception(\"Only valid inputs of a principled node are allowed: \"\n                                                f\"{extracted_input_name} in: {key}\")\n                            # extract this input value\n                            used_value = principled.inputs[extracted_input_name]\n                            # if this input value is not a default value it will be connected via the links\n                            if len(used_value.links) > 0:\n                                select_material = False\n                                break\n                            # if no link is found check the default value\n                            used_value = used_value.default_value\n                            # compare the given value to the default value\n                            if key.endswith(\"min\"):\n                                if not used_value >= value:\n                                    select_material = False\n                                    break\n                            elif key.endswith(\"max\"):\n                                if not used_value <= value:\n                                    select_material = False\n                                    break\n                            elif key.endswith(\"eq\"):\n                                if not used_value == value:\n                                    select_material = False\n                                    break\n                            else:\n                                raise Exception(f\"This type of key is unknown: {key}\")\n                        else:\n                            select_material = False\n                            break\n                    elif key == \"use_materials_of_objects\":\n                        objects = Utility.build_provider_based_on_config(value).run()\n                        found_material = False\n                        # iterate over all selected objects\n                        for obj in objects:\n                            # check if they have materials\n                            if hasattr(obj, \"material_slots\"):\n                                for mat_slot in obj.material_slots:\n                                    # if the material is the same as the currently checked one\n                                    if mat_slot.material == material:\n                                        found_material = True\n                                        break\n                            if found_material:\n                                break\n                        if not found_material:\n                            select_material = False\n                            break\n                    else:\n                        select_material = False\n                        break\n                else:\n                    select_material = False\n                    break\n            if select_material:\n                new_materials.append(material)\n        return new_materials",
  "def perform_and_condition_check(and_condition, materials, used_materials_to_check=None):\n        \"\"\" Checks for all materials in the scene if all given conditions are true, collects them in the return list.\n\n        :param and_condition: Given conditions. Type: dict.\n        :param materials: Materials, that are already in the return list. Type: list.\n        :param used_materials_to_check: a list of materials to perform the check on. Type: list. Default: all materials\n        :return: Materials that fulfilled given conditions. Type: list.\n        \"\"\"\n        new_materials = []\n        if used_materials_to_check is None:\n            used_materials_to_check = get_all_materials()\n\n        # through every material\n        for material in used_materials_to_check:\n            if material in new_materials or material in materials or material is None:\n                continue\n\n            select_material = True\n            for key, value in and_condition.items():\n                # check if the key is a requested custom property\n                requested_custom_property = False\n                requested_custom_function = False\n                if key.startswith('cp_'):\n                    requested_custom_property = True\n                    key = key[3:]\n                if key.startswith('cf_'):\n                    requested_custom_function = True\n                    key = key[3:]\n                if hasattr(material, key) and not requested_custom_property and not requested_custom_function:\n                    # check if the type of the value of attribute matches desired\n                    if isinstance(getattr(material, key), type(value)):\n                        new_value = value\n                    # if not, try to enforce some mathutils-specific type\n                    else:\n                        if isinstance(getattr(material, key), mathutils.Vector):\n                            new_value = mathutils.Vector(value)\n                        elif isinstance(getattr(material, key), mathutils.Euler):\n                            new_value = mathutils.Euler(value)\n                        elif isinstance(getattr(material, key), mathutils.Color):\n                            new_value = mathutils.Color(value)\n                        # raise an exception if it is none of them\n                        else:\n                            raise Exception(f\"Types are not matching: {type(getattr(material, key))} \"\n                                            \"and {type(value)} !\")\n                    # or check for equality\n                    if not ((isinstance(getattr(material, key), str) and\n                            re.fullmatch(value, getattr(material, key)) is not None)\n                            or getattr(material, key) == new_value):\n                        select_material = False\n                        break\n                # check if a custom property with this name exists\n                elif key in material and requested_custom_property:\n                    # check if the type of the value of such custom property matches desired\n                    if isinstance(material[key], type(value)) or (isinstance(material[key], int)\n                                                                  and isinstance(value, bool)):\n                        # if it is a string and if the whole string matches the given pattern\n                        if not ((isinstance(material[key], str) and re.fullmatch(value, material[key]) is not None) or\n                                material[key] == value):\n                            select_material = False\n                            break\n                    else:\n                        # raise an exception if not\n                        raise Exception(f\"Types are not matching: {type(material[key])} and {type(value)} !\")\n                elif requested_custom_function:\n                    if key.startswith(\"texture_amount_\"):\n                        if material.use_nodes:\n                            value = int(value)\n                            nodes = material.node_tree.nodes\n                            texture_nodes = Utility.get_nodes_with_type(nodes, \"TexImage\")\n                            amount_of_texture_nodes = len(texture_nodes) if texture_nodes is not None else 0\n                            if \"min\" in key:\n                                if not amount_of_texture_nodes >= value:\n                                    select_material = False\n                                    break\n                            elif \"max\" in key:\n                                if not amount_of_texture_nodes <= value:\n                                    select_material = False\n                                    break\n                            elif \"eq\" in key:\n                                if not amount_of_texture_nodes == value:\n                                    select_material = False\n                                    break\n                            else:\n                                raise Exception(f\"This type of key is unknown: {key}\")\n                        else:\n                            select_material = False\n                            break\n                    elif key.startswith(\"principled_bsdf_amount_\"):\n                        if material.use_nodes:\n                            value = int(value)\n                            nodes = material.node_tree.nodes\n                            principled = Utility.get_nodes_with_type(nodes, \"BsdfPrincipled\")\n                            amount_of_principled_bsdf_nodes = len(principled) if principled is not None else 0\n                            if \"min\" in key:\n                                if not amount_of_principled_bsdf_nodes >= value:\n                                    select_material = False\n                                    break\n                            elif \"max\" in key:\n                                if not amount_of_principled_bsdf_nodes <= value:\n                                    select_material = False\n                                    break\n                            elif \"eq\" in key:\n                                if not amount_of_principled_bsdf_nodes == value:\n                                    select_material = False\n                                    break\n                            else:\n                                raise Exception(f\"This type of key is unknown: {key}\")\n                        else:\n                            select_material = False\n                            break\n                    elif key.startswith(\"principled_bsdf_\"):  # must be after the amount check\n                        # This custom function can check the value of a certain Principled BSDF shader input.\n                        # For example this can be used to avoid using materials, which have an Alpha Texture by\n                        # adding they key: `\"cf_principled_bsdf_Alpha_eq\": 1.0`\n                        if material.use_nodes:\n                            value = float(value)\n                            # first check if there is only one Principled BSDF node in the material\n                            nodes = material.node_tree.nodes\n                            principled = Utility.get_nodes_with_type(nodes, \"BsdfPrincipled\")\n                            amount_of_principled_bsdf_nodes = len(principled) if principled is not None else 0\n                            if amount_of_principled_bsdf_nodes != 1:\n                                select_material = False\n                                break\n                            principled = principled[0]\n                            # then extract the input name from the key, for the Alpha example: `Alpha`\n                            extracted_input_name = key[len(\"principled_bsdf_\"):key.rfind(\"_\")]\n                            # check if this key exists, else throw an error\n                            if extracted_input_name not in principled.inputs:\n                                raise Exception(\"Only valid inputs of a principled node are allowed: \"\n                                                f\"{extracted_input_name} in: {key}\")\n                            # extract this input value\n                            used_value = principled.inputs[extracted_input_name]\n                            # if this input value is not a default value it will be connected via the links\n                            if len(used_value.links) > 0:\n                                select_material = False\n                                break\n                            # if no link is found check the default value\n                            used_value = used_value.default_value\n                            # compare the given value to the default value\n                            if key.endswith(\"min\"):\n                                if not used_value >= value:\n                                    select_material = False\n                                    break\n                            elif key.endswith(\"max\"):\n                                if not used_value <= value:\n                                    select_material = False\n                                    break\n                            elif key.endswith(\"eq\"):\n                                if not used_value == value:\n                                    select_material = False\n                                    break\n                            else:\n                                raise Exception(f\"This type of key is unknown: {key}\")\n                        else:\n                            select_material = False\n                            break\n                    elif key == \"use_materials_of_objects\":\n                        objects = Utility.build_provider_based_on_config(value).run()\n                        found_material = False\n                        # iterate over all selected objects\n                        for obj in objects:\n                            # check if they have materials\n                            if hasattr(obj, \"material_slots\"):\n                                for mat_slot in obj.material_slots:\n                                    # if the material is the same as the currently checked one\n                                    if mat_slot.material == material:\n                                        found_material = True\n                                        break\n                            if found_material:\n                                break\n                        if not found_material:\n                            select_material = False\n                            break\n                    else:\n                        select_material = False\n                        break\n                else:\n                    select_material = False\n                    break\n            if select_material:\n                new_materials.append(material)\n        return new_materials",
  "def convex_decomposition(obj: \"MeshObject\", temp_dir: str, vhacd_path: str, resolution: int = 1000000,\n                         name_template: str = \"?_hull_#\", remove_doubles: bool = True, apply_modifiers: bool = True,\n                         apply_transforms: str = \"NONE\", depth: int = 20, max_num_vertices_per_ch: int = 64,\n                         cache_dir: Optional[str] = None):\n    \"\"\" Uses V-HACD to decompose the given object.\n\n    You can turn of the usage of OpenCL by setting the environment variable NO_OPENCL to \"1\".\n\n    :param obj: The blender object to decompose.\n    :param temp_dir: The temp directory where to store the convex parts.\n    :param vhacd_path: The directory in which vhacd should be installed or is already installed.\n    :param resolution: maximum number of voxels generated during the voxelization stage\n    :param name_template: The template how to name the convex parts.\n    :param remove_doubles: Remove double vertices before decomposition.\n    :param apply_modifiers: Apply modifiers before decomposition.\n    :param apply_transforms: Apply transforms before decomposition.\n    :param depth: maximum number of clipping stages. During each split stage, all the model parts (with a concavity\n                  higher than the user defined threshold) are clipped according the \"best\" clipping plane\n    :param max_num_vertices_per_ch: controls the maximum number of triangles per convex-hull\n    :param cache_dir: If a directory is given, convex decompositions are stored there named after the meshes hash.\n                      If the same mesh is decomposed a second time, the result is loaded from the cache and the actual\n                      decomposition is skipped.\n    :return: The list of convex parts composing the given object.\n    \"\"\"\n    if platform not in [\"linux\", \"linux2\"]:\n        raise RuntimeError(f\"Convex decomposition is at the moment only available on linux: {platform}\")\n\n    # Download v-hacd library if necessary\n    if not os.path.exists(os.path.join(vhacd_path, \"v-hacd\")):\n        os.makedirs(vhacd_path, exist_ok=True)\n        print(\"Downloading v-hacd library into \" + str(vhacd_path))\n        git.Git(vhacd_path).clone(\"https://github.com/kmammou/v-hacd.git\")\n\n        print(\"Building v-hacd\")\n        if \"NO_OPENCL\" in os.environ and os.environ[\"NO_OPENCL\"] == \"1\":\n            os.system(os.path.join(os.path.dirname(__file__), \"build_linux.sh\") + \" \" +\n                      os.path.join(vhacd_path, \"v-hacd\") + \" -DNO_OPENCL=ON\")\n        else:\n            os.system(os.path.join(os.path.dirname(__file__), \"build_linux.sh\") + \" \" +\n                      os.path.join(vhacd_path, \"v-hacd\") + \" -DNO_OPENCL=OFF\")\n\n    off_filename = os.path.join(temp_dir, \"vhacd.obj\")\n    log_file_name = os.path.join(temp_dir, \"vhacd_log.txt\")\n\n    # Apply modifiers\n    bpy.ops.object.select_all(action=\"DESELECT\")\n    if apply_modifiers:\n        mesh = obj.blender_obj.evaluated_get(bpy.context.evaluated_depsgraph_get()).data.copy()\n    else:\n        mesh = obj.blender_obj.data.copy()\n\n    # Apply transforms\n    translation, quaternion, scale = Matrix(obj.get_local2world_mat()).decompose()\n    scale_matrix = Matrix(((scale.x, 0, 0, 0), (0, scale.y, 0, 0), (0, 0, scale.z, 0), (0, 0, 0, 1)))\n    if apply_transforms in [\"S\", \"RS\", \"LRS\"]:\n        pre_matrix = scale_matrix\n        post_matrix = Matrix()\n    else:\n        pre_matrix = Matrix()\n        post_matrix = scale_matrix\n    if apply_transforms in [\"RS\", \"LRS\"]:\n        pre_matrix = quaternion.to_matrix().to_4x4() @ pre_matrix\n    else:\n        post_matrix = quaternion.to_matrix().to_4x4() @ post_matrix\n    if apply_transforms == \"LRS\":\n        pre_matrix = Matrix.Translation(translation) @ pre_matrix\n    else:\n        post_matrix = Matrix.Translation(translation) @ post_matrix\n\n    mesh.transform(pre_matrix)\n\n    # Create bmesh\n    bm = bmesh.new()\n    bm.from_mesh(mesh)\n    if remove_doubles:\n        bmesh.ops.remove_doubles(bm, verts=bm.verts, dist=0.0001)\n    bmesh.ops.triangulate(bm, faces=bm.faces)\n    bm.to_mesh(mesh)\n    bm.free()\n\n    # Build a hash for the given mesh\n    mesh_hash = 0\n    for vert in mesh.vertices:\n        # Combine the hashes of the local coordinates of all vertices\n        mesh_hash = hash((mesh_hash, hash(vert.co[:])))\n    mesh_hash = abs(mesh_hash)\n\n    if cache_dir is None or not os.path.exists(os.path.join(cache_dir, str(mesh_hash) + \".obj\")):\n        vhacd_binary = os.path.join(vhacd_path, \"v-hacd\", \"app\", \"TestVHACD\")\n        if not os.path.exists(vhacd_binary):\n            raise FileNotFoundError(\"The vhacd binary was not found, the build script probably failed!\")\n\n        # Run V-HACD\n        print(f\"\\nExporting mesh for V-HACD: {off_filename}...\")\n        obj_export(mesh, off_filename)\n        bpy.data.meshes.remove(mesh)\n        cmd_line = f'\"{vhacd_binary}\" {off_filename} -r {resolution} -v {max_num_vertices_per_ch} -d {depth}'\n        if os.path.exists(os.path.basename(log_file_name)):\n            cmd_line += f\"2>&1 > {log_file_name}\"\n        print(f\"Running V-HACD...\\n{cmd_line}\\n\")\n        with Popen(cmd_line, bufsize=-1, close_fds=True, shell=True, cwd=temp_dir) as vhacd_process:\n            vhacd_process.wait()\n        out_file_name = os.path.join(temp_dir, \"decomp.obj\")\n\n        # Import convex parts\n        if not os.path.exists(out_file_name):\n            raise RuntimeError(f\"No output produced by convex decomposition of object {obj.get_name()}\")\n\n        if cache_dir is not None:\n            # Create cache dir, if it not exists yet\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir, exist_ok=True)\n            # Copy decomposition into cache dir\n            shutil.copyfile(out_file_name, os.path.join(cache_dir, str(mesh_hash) + \".obj\"))\n    else:\n        out_file_name = os.path.join(cache_dir, str(mesh_hash) + \".obj\")\n\n    bpy.ops.import_scene.obj(filepath=out_file_name, axis_forward=\"Y\", axis_up=\"Z\")\n    imported = bpy.context.selected_objects\n\n    # Name and transform the loaded parts\n    for index, hull in enumerate(imported):\n        hull.select_set(False)\n        hull.matrix_basis = post_matrix\n        name = name_template.replace(\"?\", obj.get_name(), 1)\n        name = name.replace(\"#\", str(index + 1), 1)\n        if name == name_template:\n            name += str(index + 1)\n        hull.name = name\n        hull.data.name = name\n        hull.display_type = \"WIRE\"\n\n    return imported",
  "def obj_export(mesh, fullpath):\n    \"\"\" Export triangulated mesh to Object File Format \"\"\"\n    with open(fullpath, \"wb\") as off:\n        # pylint: disable=consider-using-f-string\n        for vert in mesh.vertices:\n            off.write(str.encode(\"v {:g} {:g} {:g}\\n\".format(*vert.co)))\n        for face in mesh.polygons:\n            vertices = np.array(face.vertices) + 1\n            off.write(str.encode(\"f {} {} {}\\n\".format(*vertices)))",
  "def split_object_according_to_groups(file_path, folder):\n    \"\"\"\n    Splits the given .obj file into different objects, assuming these objects have been separated via groups before.\n\n    :param file_path: Path to the .obj file\n    :param folder: Folder in which the resulting split .obj files we be saved\n    \"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n        lines = text.split(\"\\n\")\n        start_info = \"\"\n        for line in lines:\n            if line.strip().startswith(\"g \"):\n                break\n            start_info += line + \"\\n\"\n\n        list_of_split_ids = [i for i, line in enumerate(lines) if line.strip().startswith(\"g \")]\n        last_i = list_of_split_ids[0]\n        group_counter = 0\n        for current_i in list_of_split_ids[1:]:\n            current_text = start_info\n            current_lines = lines[last_i: current_i]\n            face_lines = [l[len(\"f \"):].strip().split(\" \") for l in current_lines if l.strip().startswith(\"f \")]\n            face_lines = np.array([[[int(e) for e in eles.split(\"/\")] for eles in l] for l in face_lines])\n            face_offset = np.min(face_lines, axis=0)\n            face_offset = np.min(face_offset, axis=0) - 1\n\n            final_lins = []\n            for line in current_lines:\n                if line.strip().startswith(\"f \"):\n                    blocks = line[len(\"f \"):].strip().split(\" \")\n                    values = [np.array([int(e) for e in eles.split(\"/\")]) - face_offset for eles in blocks]\n                    f_line = \"f \" + \" \".join([\"/\".join([str(int(e)) for e in eles]) for eles in values])\n                    final_lins.append(f_line)\n                else:\n                    final_lins.append(line)\n            last_i = current_i\n\n            amount_of_faces = sum(1 for l in final_lins if l.startswith(\"f \"))\n            if amount_of_faces > 10:\n                current_text += \"\\n\".join(final_lins)\n                file_path_obj = os.path.join(folder, f\"{os.path.basename(folder)}_{group_counter}.obj\")\n                with open(file_path_obj, \"w\", encoding=\"utf-8\") as file:\n                    file.write(current_text)\n                group_counter += 1",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Downloads the IKEA dataset\")\n    parser.add_argument('output_dir', help=\"Determines where the data is going to be saved.\")\n    args = parser.parse_args()\n\n    # setting the default header, else the server does not allow the download\n    opener = build_opener()\n    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n    install_opener(opener)\n\n    ikea_dir = args.output_dir\n    if not os.path.exists(ikea_dir):\n        os.makedirs(ikea_dir)\n\n    # download the zip file, which contains all the model files.\n    print(\"Downloading the zip file (166mb)...\")\n    ikea_url = \"http://ikea.csail.mit.edu/zip/IKEA_models.zip\"\n    zip_file_path = os.path.join(ikea_dir, \"IKEA_models.zip\")\n    urlretrieve(ikea_url, zip_file_path)\n    print(\"Download complete.\")\n\n    # unzip the zip file\n    print(\"Unzipping the zip file...\")\n    ikea_dir = os.path.join(ikea_dir, \"IKEA\")\n    SetupUtility.extract_file(ikea_dir, zip_file_path)\n    os.remove(zip_file_path)\n\n    subprocess.call(\"chmod -R a+rw *\", shell=True, cwd=ikea_dir)\n\n    print(\"The IKEA dataset has some weird bugs, these are fixed now.\")\n    if os.path.exists(os.path.join(ikea_dir, \"IKEA_bed_BEDDINGE\")):\n        shutil.rmtree(os.path.join(ikea_dir, \"IKEA_bed_BEDDINGE\"))\n\n    nils_folder = os.path.join(ikea_dir, \"IKEA_chair_NILS\")\n    if os.path.exists(nils_folder):\n        shutil.rmtree(nils_folder)\n\n    # delete all no double .obj\n    for folder in glob.glob(os.path.join(ikea_dir, \"*\")):\n        no_jitter_folders = glob.glob(os.path.join(folder, \"nojitter*\"))\n        org_obj_files = glob.glob(os.path.join(folder, \".obj\"))\n        for org_obj_file in org_obj_files:\n            os.remove(org_obj_file)\n        if no_jitter_folders:\n            for no_jitter_folder in no_jitter_folders:\n                obj_files = glob.glob(os.path.join(no_jitter_folder, \"*.obj\"))\n                # first remove all the ones without mtl\n                for obj_file in obj_files:\n                    new_name = obj_file.replace(os.path.basename(os.path.dirname(obj_file)), \"\")\n                    os.rename(obj_file, new_name)\n                    os.rename(obj_file.replace(\".obj\", \".mtl\"), new_name.replace(\".obj\", \".mtl\"))\n                jpg_files = glob.glob(os.path.join(no_jitter_folder, \"*.jpg\"))\n                for jpg_file in jpg_files:\n                    new_name = jpg_file.replace(os.path.basename(os.path.dirname(jpg_file)), \"\")\n                    os.rename(jpg_file, new_name)\n                folders_in_no_jitter = [f for f in glob.glob(os.path.join(no_jitter_folder, \"*\")) if os.path.isdir(f)]\n                for mv_folder in folders_in_no_jitter:\n                    new_name = mv_folder.replace(os.path.basename(os.path.dirname(mv_folder)), \"\")\n                    os.rename(mv_folder, new_name)\n\n    # delete no jitter\n    for folder in glob.glob(os.path.join(ikea_dir, \"*\", \"nojitter*\")):\n        shutil.rmtree(folder)\n\n    # delete all skp files\n    skp_files = glob.glob(os.path.join(ikea_dir, \"*\", \"*.skp\"))\n    for skp_file in skp_files:\n        os.remove(skp_file)\n\n    # delete all json files\n    js_files = glob.glob(os.path.join(ikea_dir, \"*\", \"*.js\"))\n    for js_file in js_files:\n        os.remove(js_file)\n\n    # delete specific files which need to broken up:\n    def delete_obj_file(path):\n        os.remove(path)\n        if os.path.exists(path.replace(\".obj\", \".mtl\")):\n            os.remove(path.replace(\".obj\", \".mtl\"))\n\n    # directly remove these files:\n    path = os.path.join(ikea_dir, \"IKEA_wardrobe_PAX\", \"4b91d887fd34890a35d389147630ded_obj0_object.obj\")\n    delete_obj_file(path)\n    path = os.path.join(ikea_dir, \"IKEA_chair_JOKKMOKK\", \"221da64f5789c4bfcf7d397dd220c7e2_obj0_object.obj\")\n    delete_obj_file(path)\n    path = os.path.join(ikea_dir, \"IKEA_table_JOKKMOKK\", \"jokkmokk_table_2_obj0_object.obj\")\n    delete_obj_file(path)\n    path = os.path.join(ikea_dir, \"IKEA_table_UTBY\", \"cfcd08bbf590325e7b190cd56debb387_obj0_object.obj\")\n    delete_obj_file(path)\n    path = os.path.join(ikea_dir, \"IKEA_chair_STEFAN\", \"7e44c6d0933417ace05f257fa4ec4037_obj0_object.obj\")\n    delete_obj_file(path)\n    shutil.rmtree(os.path.join(ikea_dir, \"IKEA_chair_URBAN\"))\n\n    # this are several couches in one object file\n    first_path = os.path.join(ikea_dir, \"IKEA_sofa_VRETA\", \"3b58f55ed32ceef86315023d0bef39b6_obj0_object.obj\")\n    split_object_according_to_groups(first_path, os.path.join(ikea_dir, \"IKEA_sofa_VRETA\"))\n    os.remove(first_path)\n\n    for ele in [\"d1748541564ade6cfe63adf1a76042f0_obj0_object.obj\", \"c5e1449fc0ee6833f072f21dd9a7251_obj0_object.obj\"]:\n        path = os.path.join(ikea_dir, \"IKEA_wardrobe_PAX\", ele)\n        delete_obj_file(path)",
  "def delete_obj_file(path):\n        os.remove(path)\n        if os.path.exists(path.replace(\".obj\", \".mtl\")):\n            os.remove(path.replace(\".obj\", \".mtl\"))",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Downloads the scenenet dataset\")\n    parser.add_argument('output_dir', help=\"Determines where the data is going to be saved.\")\n    args = parser.parse_args()\n\n    # setting the default header, else the server does not allow the download\n    opener = build_opener()\n    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n    install_opener(opener)\n\n    scenenet_dir = args.output_dir\n    if not os.path.exists(scenenet_dir):\n        os.makedirs(scenenet_dir)\n\n    # download the zip file, which contains all the obj files\n    print(\"Download the zip file, may take a while:\")\n    scenenet_url = \"https://bitbucket.org/robotvault/downloadscenenet/get/cfe5ab85ddcc.zip\"\n    zip_file_path = os.path.join(scenenet_dir, \"scene_net.zip\")\n    urlretrieve(scenenet_url, zip_file_path)\n\n    # unzip the zip file\n    print(\"Unzip the zip file.\")\n    SetupUtility.extract_file(scenenet_dir, zip_file_path)\n\n    os.remove(zip_file_path)\n    os.rename(os.path.join(scenenet_dir, \"robotvault-downloadscenenet-cfe5ab85ddcc\"),\n              os.path.join(scenenet_dir, \"SceneNetData\"))\n\n    print(\"Please also download the texture library from here: http://tinyurl.com/zpc9ppb\")\n    print(\"This is a google drive folder downloading via script is tedious.\")",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-c', '--conf', dest='conf', default='coco_annotations.json', help='coco annotation json file')\n    parser.add_argument('-i', '--image_index', dest='image_index', default=0,\n                        help='image over which to annotate, uses the rgb rendering', type=int)\n    parser.add_argument('-b', '--base_path', dest='base_path',\n                        default='examples/advanced/coco_annotations/output/coco_data',\n                        help='path to folder with coco_annotation.json and images', type=str)\n    parser.add_argument('--save', '-s', action='store_true',\n                        help='saves visualization of coco annotations under base_path/coco_annotated_x.png ')\n\n    args = parser.parse_args()\n\n    conf = args.conf\n    image_idx = args.image_index\n    base_path = args.base_path\n    save = args.save\n\n    # Read coco_annotations config\n    with open(os.path.join(base_path, conf), \"r\", encoding=\"utf-8\") as f:\n        annotations = json.load(f)\n        categories = annotations['categories']\n        images = annotations['images']\n        annotations = annotations['annotations']\n\n    im = Image.open(os.path.join(base_path, images[image_idx]['file_name']))\n\n    def get_category(_id):\n        category = [category[\"name\"] for category in categories if category[\"id\"] == _id]\n        if category:\n            return str(category[0])\n        raise RuntimeError(f\"Category {_id} is not defined in {os.path.join(base_path, conf)}\")\n\n    def rle_to_binary_mask(rle):\n        \"\"\"Converts a COCOs run-length encoding (RLE) to binary mask.\n        :param rle: Mask in RLE format\n        :return: a 2D binary numpy array where '1's represent the object\n        \"\"\"\n        binary_array = np.zeros(np.prod(rle.get('size')), dtype=bool)\n        counts = rle.get('counts')\n\n        start = 0\n        for i in range(len(counts) - 1):\n            start += counts[i]\n            end = start + counts[i + 1]\n            binary_array[start:end] = (i + 1) % 2\n\n        binary_mask = binary_array.reshape(*rle.get('size'), order='F')\n\n        return binary_mask\n\n    font = ImageFont.load_default()\n    # Add bounding boxes and masks\n    for annotation in annotations:\n        if annotation[\"image_id\"] == image_idx:\n            draw = ImageDraw.Draw(im)\n            bb = annotation['bbox']\n            draw.rectangle(((bb[0], bb[1]), (bb[0] + bb[2], bb[1] + bb[3])), fill=None, outline=\"red\")\n            draw.text((bb[0] + 2, bb[1] + 2), get_category(annotation[\"category_id\"]), font=font)\n            if isinstance(annotation[\"segmentation\"], dict):\n                im.putalpha(255)\n                rle_seg = annotation[\"segmentation\"]\n                item = rle_to_binary_mask(rle_seg).astype(np.uint8) * 255\n                item = Image.fromarray(item, mode='L')\n                overlay = Image.new('RGBA', im.size)\n                draw_ov = ImageDraw.Draw(overlay)\n                rand_color = np.random.randint(0, 256, 3)\n                draw_ov.bitmap((0, 0), item, fill=(rand_color[0], rand_color[1], rand_color[2], 128))\n                im = Image.alpha_composite(im, overlay)\n            else:\n                # go through all polygons and plot them\n                for item in annotation['segmentation']:\n                    poly = Image.new('RGBA', im.size)\n                    pdraw = ImageDraw.Draw(poly)\n                    rand_color = np.random.randint(0, 256, 3)\n                    pdraw.polygon(item, fill=(rand_color[0], rand_color[1], rand_color[2], 127),\n                                  outline=(255, 255, 255, 255))\n                    im.paste(poly, mask=poly)\n    if save:\n        im.save(os.path.join(base_path, f'coco_annotated_{image_idx}.png'), \"PNG\")\n    im.show()",
  "def get_category(_id):\n        category = [category[\"name\"] for category in categories if category[\"id\"] == _id]\n        if category:\n            return str(category[0])\n        raise RuntimeError(f\"Category {_id} is not defined in {os.path.join(base_path, conf)}\")",
  "def rle_to_binary_mask(rle):\n        \"\"\"Converts a COCOs run-length encoding (RLE) to binary mask.\n        :param rle: Mask in RLE format\n        :return: a 2D binary numpy array where '1's represent the object\n        \"\"\"\n        binary_array = np.zeros(np.prod(rle.get('size')), dtype=bool)\n        counts = rle.get('counts')\n\n        start = 0\n        for i in range(len(counts) - 1):\n            start += counts[i]\n            end = start + counts[i + 1]\n            binary_array[start:end] = (i + 1) % 2\n\n        binary_mask = binary_array.reshape(*rle.get('size'), order='F')\n\n        return binary_mask",
  "def save_array_as_image(array, key, file_path):\n    \"\"\" Save array as an image, using the vis_data function\"\"\"\n    vis_data(key, array, None, \"\", save_to_file=file_path)",
  "def convert_hdf(base_file_path: str, output_folder: Optional[str] = None):\n    \"\"\" Convert a hdf5 file to images \"\"\"\n    if os.path.exists(base_file_path):\n        if os.path.isfile(base_file_path):\n            base_name = str(os.path.basename(base_file_path)).split('.', maxsplit=1)[0]\n            if output_folder is not None:\n                base_name = os.path.join(output_folder, base_name)\n            with h5py.File(base_file_path, 'r') as data:\n                print(f\"{base_file_path}:\")\n                for key, val in data.items():\n                    val = np.array(val)\n                    if np.issubdtype(val.dtype, np.string_) or len(val.shape) == 1:\n                        pass  # metadata\n                    else:\n                        print(f\"key: {key} {val.shape} {val.dtype.name}\")\n\n                        if val.shape[0] != 2:\n                            # mono image\n                            file_path = f'{base_name}_{key}.png'\n                            save_array_as_image(val, key, file_path)\n                        else:\n                            # stereo image\n                            for image_index, image_value in enumerate(val):\n                                file_path = f'{base_name}_{key}_{image_index}.png'\n                                save_array_as_image(image_value, key, file_path)\n        else:\n            print(\"The path is not a file\")\n    else:\n        print(f\"The file does not exist: {base_file_path}\")",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Script to save images out of a hdf5 files.\")\n    parser.add_argument('hdf5', nargs='+', help='Path to hdf5 file/s')\n    parser.add_argument('--output_dir', default=None,\n                        help=\"Determines where the data is going to be saved. Default: Current directory\")\n\n    args = parser.parse_args()\n\n    if isinstance(args.hdf5, str):\n        convert_hdf(args.hdf5, args.output_dir)\n    elif isinstance(args.hdf5, list):\n        for file in args.hdf5:\n            convert_hdf(file, args.output_dir)\n    else:\n        print(\"Input must be a path\")",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Downloads the Matterport3D dataset\")\n    parser.add_argument('download_mp_script', help=\"Path to the download script, this script is only available after \"\n                                                   \"accepting the Terms of Use.\")\n    parser.add_argument('scans_txt_path', help=\"Path to the scans.txt data, this file is only available after \"\n                                               \"accepting the Terms of Use.\")\n    parser.add_argument('output_dir', help=\"Determines where the data is going to be saved.\")\n    args = parser.parse_args()\n\n    data_path = Path(args.output_dir)\n    data_path.mkdir(exist_ok=True)\n\n    scan_file = Path(args.scans_txt_path)\n    if not scan_file.exists():\n        raise FileNotFoundError(\"The scans.txt file could not be found check argument.\")\n\n    with scan_file.open(\"r\", encoding=\"utf-8\") as file:\n        current_ids = [id_val for id_val in file.read().split(\"\\n\") if id_val.strip()]\n\n    download_mp_file = Path(args.download_mp_script).absolute()\n    if not download_mp_file.exists():\n        raise FileNotFoundError(f\"The download_mp script could not be found: {download_mp_file}\")\n\n    for current_id in current_ids:\n        # the script only works with python2, and it only downloads the matterport_mesh nothing else\n        cmd = f\"python2 -u {download_mp_file} -o {data_path} --id {current_id} --type matterport_mesh\"\n        with subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE) as pipe:\n            # agree to terms of use -> was already done via e-mail\n            pipe.communicate(input=\"agree\".encode())\n\n    for zip_file in (data_path / \"v1\" / \"scans\").glob(\"*/*.zip\"):\n        with zipfile.ZipFile(zip_file) as tar:\n            tar.extractall(str(Path(zip_file.absolute()).parent))\n        os.remove(zip_file)",
  "def flow_to_rgb(flow):\n    \"\"\"\n    Visualizes optical flow in hsv space and converts it to rgb space.\n    :param flow: (np.array (h, w, c)) optical flow\n    :return: (np.array (h, w, c)) rgb data\n    \"\"\"\n    # pylint: disable=import-outside-toplevel\n    import cv2\n    # pylint: enable=import-outside-toplevel\n\n    im1 = flow[:, :, 0]\n    im2 = flow[:, :, 1]\n\n    h, w = flow.shape[:2]\n\n    # Use Hue, Saturation, Value colour model\n    hsv = np.zeros((h, w, 3), dtype=np.float32)\n    hsv[..., 1] = 1\n\n    mag, ang = cv2.cartToPolar(im1, im2)\n    hsv[..., 0] = ang * 180 / np.pi\n    hsv[..., 2] = cv2.normalize(mag, None, 0, 1, cv2.NORM_MINMAX)\n\n    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)",
  "def key_matches(key, patterns, return_index=False):\n    \"\"\"\n    Match the key to the patterns\n    \"\"\"\n    for p, pattern in enumerate(patterns):\n        if re.fullmatch(pattern, key):\n            return (True, p) if return_index else True\n\n    return (False, None) if return_index else False",
  "def vis_data(key, data, full_hdf5_data=None, file_label=\"\", rgb_keys=None, flow_keys=None, segmap_keys=None,\n             segcolormap_keys=None, depth_keys=None, depth_max=default_depth_max, save_to_file=None):\n    \"\"\"\n    Visualize the data\n    \"\"\"\n    if rgb_keys is None:\n        rgb_keys = default_rgb_keys[:]\n    if flow_keys is None:\n        flow_keys = default_flow_keys[:]\n    if segmap_keys is None:\n        segmap_keys = default_segmap_keys[:]\n    if segcolormap_keys is None:\n        segcolormap_keys = default_segcolormap_keys[:]\n    if depth_keys is None:\n        depth_keys = default_depth_keys[:]\n\n    # If key is valid and does not contain segmentation data, create figure and add title\n    if key_matches(key, flow_keys + rgb_keys + depth_keys):\n        plt.figure()\n        plt.title(f\"{key} in {file_label}\")\n\n    if key_matches(key, flow_keys):\n        try:\n            # Visualize optical flow\n            if save_to_file is None:\n                plt.imshow(flow_to_rgb(data), cmap='jet')\n            else:\n                plt.imsave(save_to_file, flow_to_rgb(data), cmap='jet')\n                plt.close()\n        except ImportError as e:\n            raise ImportError(\"Using .hdf5 containers, which contain flow images needs opencv-python to be \"\n                              \"installed!\") from e\n    elif key_matches(key, segmap_keys):\n        # Try to find labels for each channel in the segcolormap\n        channel_labels = {}\n        _, key_index = key_matches(key, segmap_keys, return_index=True)\n        if key_index < len(segcolormap_keys):\n            # Check if segcolormap_key for the current segmap key is configured and exists\n            segcolormap_key = segcolormap_keys[key_index]\n            if full_hdf5_data is not None and segcolormap_key in full_hdf5_data:\n                # Extract segcolormap data\n                segcolormap = json.loads(np.array(full_hdf5_data[segcolormap_key]).tostring())\n                if len(segcolormap) > 0:\n                    # Go through all columns, we are looking for channel_* ones\n                    for colormap_key, colormap_value in segcolormap[0].items():\n                        if colormap_key.startswith(\"channel_\") and colormap_value.isdigit():\n                            channel_labels[int(colormap_value)] = colormap_key[len(\"channel_\"):]\n\n        # Make sure we have three dimensions\n        if len(data.shape) == 2:\n            data = data[:, :, None]\n        # Go through all channels\n        for i in range(data.shape[2]):\n            # Try to determine label\n            channel_label = channel_labels.get(i, i)\n\n            # Visualize channel\n            if save_to_file is None:\n                plt.figure()\n                plt.title(f\"{key} / {channel_label} in {file_label}\")\n                plt.imshow(data[:, :, i], cmap='jet')\n            else:\n                if data.shape[2] > 1:\n                    filename = save_to_file.replace(\".png\", f\"_{channel_label}.png\")\n                else:\n                    filename = save_to_file\n                plt.imsave(filename, data[:, :, i], cmap='jet')\n                plt.close()\n\n    elif key_matches(key, depth_keys):\n        # Make sure the data has only one channel, otherwise matplotlib will treat it as a rgb image\n        if len(data.shape) == 3:\n            if data.shape[2] != 1:\n                print(f\"Warning: The data with key '{key}' has more than one channel which would not allow using \"\n                      f\"a jet color map. Therefore only the first channel is visualized.\")\n            data = data[:, :, 0]\n\n        if save_to_file is None:\n            plt.imshow(data, cmap='summer', vmax=depth_max)\n            plt.colorbar()\n        else:\n            plt.imsave(save_to_file, data, cmap='summer', vmax=depth_max)\n            plt.close()\n    elif key_matches(key, rgb_keys):\n        if save_to_file is None:\n            plt.imshow(data)\n        else:\n            plt.imsave(save_to_file, data)\n            plt.close()\n    else:\n        if save_to_file is None:\n            plt.imshow(data)\n        else:\n            plt.imsave(save_to_file, data)\n            plt.close()",
  "def vis_file(path, keys_to_visualize=None, rgb_keys=None, flow_keys=None, segmap_keys=None, segcolormap_keys=None,\n             depth_keys=None, depth_max=default_depth_max, save_to_path=None):\n    \"\"\" Visualize a file \"\"\"\n    if save_to_path is not None and not os.path.exists(save_to_path):\n        os.makedirs(save_to_path)\n\n    # Check if file exists\n    if os.path.exists(path):\n        if os.path.isfile(path):\n            with h5py.File(path, 'r') as data:\n                print(path + \": \")\n\n                # Select only a subset of keys if args.keys is given\n                if keys_to_visualize is not None:\n                    keys = [key for key in data.keys() if key_matches(key, keys_to_visualize)]\n                else:\n                    keys = list(data.keys())\n\n                # Visualize every key\n                res = []\n                for key in keys:\n                    value = np.array(data[key])\n\n                    if sum(ele for ele in value.shape) < 5 or \"version\" in key:\n                        if value.dtype == \"|S5\":\n                            res.append(\n                                (key, str(value).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"b'\", \"\").replace(\"'\", \"\")))\n                        else:\n                            res.append((key, value))\n                    else:\n                        res.append((key, value.shape))\n\n                if res:\n                    res = [f\"'{key}': {key_res}\" for key, key_res in res]\n                    print(\"Keys: \" + ', '.join(res))\n\n                for key in keys:\n                    value = np.array(data[key])\n                    if save_to_path is not None:\n                        save_to_file = os.path.join(save_to_path,\n                                                    str(os.path.basename(path)).split('.', maxsplit=1)[0] +\n                                                    f\"_{key}.png\")\n                    else:\n                        save_to_file = None\n\n                    # Check if it is a stereo image\n                    if len(value.shape) >= 3 and value.shape[0] == 2:\n                        # Visualize both eyes separately\n                        for i, img in enumerate(value):\n                            if save_to_file:\n                                save_to_file = str(Path(save_to_file).with_suffix(\"\")) + (\n                                    \"_left\" if i == 0 else \"_right\") + Path(save_to_file).suffix\n                            vis_data(key, img, data, os.path.basename(path) + (\" (left)\" if i == 0 else \" (right)\"),\n                                     rgb_keys, flow_keys, segmap_keys, segcolormap_keys, depth_keys, depth_max,\n                                     save_to_file)\n                    else:\n                        vis_data(key, value, data, os.path.basename(path), rgb_keys, flow_keys, segmap_keys,\n                                 segcolormap_keys, depth_keys, depth_max, save_to_file)\n        else:\n            print(\"The path is not a file\")\n    else:\n        print(f\"The file does not exist: {path}\")",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Script to visualize hdf5 files\")\n\n    parser.add_argument('hdf5_paths', nargs='+', help='Path to hdf5 file/s')\n    parser.add_argument('--keys', nargs='+', help='Keys that should be visualized. If none is given, '\n                                                  'all keys are visualized.', default=all_default_keys)\n    parser.add_argument('--rgb_keys', nargs='+', help='Keys that should be interpreted as rgb data.',\n                        default=default_rgb_keys)\n    parser.add_argument('--flow_keys', nargs='+', help='Keys that should be interpreted as optical flow data.',\n                        default=default_flow_keys)\n    parser.add_argument('--segmap_keys', nargs='+', help='Keys that should be interpreted as segmentation data.',\n                        default=default_segmap_keys)\n    parser.add_argument('--segcolormap_keys', nargs='+', help='Keys that point to the segmentation color maps '\n                                                              'corresponding to the configured segmap_keys.',\n                        default=default_segcolormap_keys)\n    parser.add_argument('--depth_keys', nargs='+', help='Keys that contain additional non-RGB data which should be '\n                                                        'visualized using a jet color map.', default=default_depth_keys)\n    parser.add_argument('--depth_max', type=float, default=default_depth_max)\n    parser.add_argument('--save', default=None, type=str, help='Saves visualizations to file.')\n\n    args = parser.parse_args()\n\n    # Visualize all given files\n    for path in args.hdf5_paths:\n        vis_file(\n            path=path,\n            keys_to_visualize=args.keys,\n            rgb_keys=args.rgb_keys,\n            flow_keys=args.flow_keys,\n            segmap_keys=args.segmap_keys,\n            segcolormap_keys=args.segcolormap_keys,\n            depth_keys=args.depth_keys,\n            depth_max=args.depth_max,\n            save_to_path=args.save\n        )\n    if args.save is None:\n        plt.show()",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Downloads the Pix3D dataset\")\n    parser.add_argument('output_dir', help=\"Determines where the data is going to be saved.\")\n    args = parser.parse_args()\n\n    # setting the default header, else the server does not allow the download\n    opener = build_opener()\n    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n    install_opener(opener)\n\n    pix3d_dir = args.output_dir\n    if not os.path.exists(pix3d_dir):\n        os.makedirs(pix3d_dir)\n\n    # download the zip file, which contains all the obj files. Size ~3.5 GB\n    print(\"Download the zip file, may take a while:\")\n    pix3d_url = \"http://pix3d.csail.mit.edu/data/pix3d.zip\"\n    zip_file_path = os.path.join(pix3d_dir, \"pix3d.zip\")\n    urlretrieve(pix3d_url, zip_file_path)\n\n    # unzip the zip file\n    print(\"Unzip the zip file.\")\n    SetupUtility.extract_file(pix3d_dir, zip_file_path)\n\n    os.remove(zip_file_path)\n    shutil.rmtree(os.path.join(pix3d_dir, \"img\"))\n    shutil.rmtree(os.path.join(pix3d_dir, \"mask\"))",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Downloads textures from ambientCG.com\")\n    parser.add_argument('output_dir', help=\"Determines where the data is going to be saved.\")\n    args = parser.parse_args()\n\n    # setting the default header, else the server does not allow the download\n    headers = {\n        'User-Agent': 'Mozilla/5.0'\n    }\n\n    cc_texture_dir = Path(args.output_dir)\n    cc_texture_dir.mkdir(parents=True, exist_ok=True)\n\n    # until all download files have been found\n    # this loop is necessary as the server only allows downloading the info for 100 materials at once\n    current_offset = 0\n    data = {}\n    while True:\n        offset_size = 100\n        # download the json file, which contains all information\n        json_url = f\"https://ambientcg.com/api/v2/full_json?include=downloadData&limit={offset_size}\" \\\n                   f\"&offset={current_offset}&type=material\"\n        request = requests.get(json_url, headers=headers, timeout=30)\n        json_data = request.json()\n        current_offset += offset_size\n        if \"foundAssets\" in json_data and len(json_data[\"foundAssets\"]) > 0:\n            for asset in json_data[\"foundAssets\"]:\n                if \"downloadFolders\" in asset and \"default\" in asset[\"downloadFolders\"] and \\\n                        \"downloadFiletypeCategories\" in asset[\"downloadFolders\"][\"default\"]:\n                    current_download_dict = asset[\"downloadFolders\"][\"default\"][\"downloadFiletypeCategories\"]\n                    if \"zip\" in current_download_dict and \"downloads\" in current_download_dict[\"zip\"]:\n                        for download_attr in current_download_dict[\"zip\"][\"downloads\"]:\n                            if \"attribute\" in download_attr and download_attr[\"attribute\"] == \"2K-JPG\":\n                                data[asset[\"assetId\"]] = (\n                                    download_attr[\"fullDownloadPath\"], download_attr[\"zipContent\"])\n                    else:\n                        print(f\"No zip or downloads found for asset: {asset['assetId']}\")\n                else:\n                    print(f\"No downloadFolders or default or downloadFiletypeCategories found for asset: \"\n                          f\"{asset['assetId']}\")\n        else:\n            break\n    excluding_list = [\"sign\", \"roadlines\", \"manhole\", \"backdrop\", \"foliage\", \"TreeEnd\", \"TreeStump\",\n                      \"3DBread\", \"3DApple\", \"FlowerSet\", \"FoodSteps\", \"PineNeedles\", \"Grate\",\n                      \"PavingEdge\", \"Painting\", \"RockBrush\", \"WrinklesBrush\", \"Sticker\", \"3DRock\"]\n\n    # download each asset and create a folder for it (unpacking + deleting the zip included)\n    for index, (asset, content) in enumerate(data.items()):\n        # first check if the element should be skipped\n        do_not_use = False\n        for exclude_element in excluding_list:\n            if asset.lower().startswith(exclude_element.lower()):\n                do_not_use = True\n                break\n        if do_not_use:\n            continue\n\n        link, zip_assets = content\n        # check if the download has already happened\n        download_assets = True\n        current_folder = cc_texture_dir / asset\n        if not current_folder.exists():\n            current_folder.mkdir(parents=True)\n        else:\n            files_in_asset_folder = [file_path.name for file_path in current_folder.iterdir()]\n            delete_folder = False\n            for zip_asset in zip_assets:\n                if zip_asset not in files_in_asset_folder:\n                    delete_folder = True\n                    break\n            if delete_folder:\n                print(f\"Redownload the asset: {asset}, not all files are present after download\")\n                # remove folder and create it again\n                shutil.rmtree(current_folder)\n                current_folder.mkdir(parents=True)\n            else:\n                download_assets = False\n\n        if download_assets:\n            # the asset should be downloaded and has not been downloaded yet\n            print(f\"Download asset: {asset} of {index}/{len(data)}\")\n            response = requests.get(link, headers=headers, timeout=30)\n            SetupUtility.extract_from_response(current_folder, response)\n\n    print(f\"Done downloading textures, saved in {cc_texture_dir}\")",
  "def download_blendkit_assets(asset_types: List[str], output_dir: str):\n    \"\"\"\n    Download the blenderkit dataset\n    \"\"\"\n    output_dir = Path(output_dir)\n    assets = {}\n    for asset_type in asset_types:\n        page = 1\n        print(f\"Downloading {asset_type} assets\")\n        while True:\n            print(f\"Download metadata: page {page}\")\n            try:\n                url_path = f\"https://www.blenderkit.com/api/v1/search/?query=asset_type:{asset_type}+order:_score+\" \\\n                           f\"is_free:True&addon_version=1.0.30&page={page}\"\n                with urllib.request.urlopen(url_path) as url:\n                    data = json.loads(url.read().decode())\n                    # Extract results\n                    assets.setdefault(asset_type, []).extend(data[\"results\"])\n            except HTTPError as e:\n                if e.code == 404:\n                    # We reached the end\n                    break\n                raise e\n            # Goto next page\n            page += 1\n\n        total_assets = len(assets.setdefault(asset_type, []))\n        print(f\"Retrieved metadata for {total_assets} assets\")\n\n        # Create output directory\n        blenderkit_mat_dir = output_dir / ''.join([asset_type, 's'])\n        blenderkit_mat_dir.mkdir(exist_ok=True, parents=True)\n        # Create a random scene uuid which is necessary for downloading files\n        scene_uuid = str(uuid.uuid4())\n        # Set temp path for downloading. This allows clean stop and continue of the script\n        temp_path = blenderkit_mat_dir / \"temp.blend\"\n\n        # setting the default header, else the server does not allow the download\n        opener = build_opener()\n        opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n        install_opener(opener)\n\n        for i, asset in enumerate(assets.setdefault(asset_type, [])):\n            # Check if asset has already been downloaded\n            output_path = blenderkit_mat_dir / (asset[\"id\"] + \".blend\")\n            if output_path.exists():\n                print(f\"Skipping asset: {asset['id']} of {i}/{total_assets}\")\n                continue\n\n            print(f\"Download asset: {asset['id']} of {i}/{total_assets}\")\n\n            # Try to find url to blend file\n            download_url = None\n            for file in asset[\"files\"]:\n                if file[\"fileType\"] == \"blend\":\n                    if download_url is not None:\n                        print(f\"Warning: asset {asset['id']} has more than one blend file in downloads.\")\n                    download_url = file[\"downloadUrl\"]\n\n            if download_url is None:\n                print(f\"Warning: asset {asset['id']} has no blend file in downloads.\")\n                continue\n\n            # Download metadata for blend file\n            with urllib.request.urlopen(download_url + \"?scene_uuid=\" + scene_uuid) as url:\n                data = json.loads(url.read().decode())\n                # Extract actual download path\n                file_path = data[\"filePath\"]\n                # Download the file\n                urlretrieve(file_path, str(temp_path))\n                temp_path.rename(output_path)",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Downloads materials and models from blenderkit\")\n    parser.add_argument('output_dir', help=\"Determines where the data is going to be saved\")\n    parser.add_argument('--asset_types', nargs=\"+\", help=\"Which type of assets to download\",\n                        default=[\"material\", \"model\"], choices=[\"material\", \"model\"])\n    args = parser.parse_args()\n\n    download_blendkit_assets(args.asset_types, args.output_dir)",
  "def cli():\n    \"\"\"\n    Command line function\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('output_folder', help=\"Determines where the data is going to be saved.\")\n    parser.add_argument('--resolution', help=\"Desired resolution for the hdr images. Be aware that bigger resolutions, \"\n                                             \"take a lot of disc space.\", default=\"2k\")\n    parser.add_argument('--format', help=\"Desired download format for the images.\", default=\"jpg\")\n    parser.add_argument('--tags', nargs='+', help=\"Filter by asset tag.\", default=None)\n    parser.add_argument('--categories', nargs='+', help=\"Filter by asset category.\", default=[])\n    parser.add_argument('--threads', nargs='?', type=int, help=\"How many threads to use for downloading\", default=4)\n    parser.add_argument('--types', nargs='+', help=\"Only download the given types\",\n                        default=None, choices=[\"textures\", \"hdris\", \"models\"])\n    args = parser.parse_args()\n    args_output_dir = Path(args.output_folder)\n\n    args_max_workers = min(args.threads, cpu_count())\n\n\n    def download_file(url: str, output_path: str):\n        # Download\n        request = requests.get(url, timeout=30)\n        # Write to file\n        with open(output_path, \"wb\") as file:\n            file.write(request.content)\n\n    def download_items(item_type: str, output_dir: Path, item_download_func: Callable[[str, Path], None]):\n        # Filter for type\n        if args.types and item_type not in args.types:\n            return\n\n        print(\"Preparing download of\\t\" + output_dir.name )\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Download listing\n        response = requests.get(f\"https://api.polyhaven.com/assets?t={item_type}\"\n                                f\"&categories={','.join(args.categories)}\", timeout=30)\n        data = response.json()\n\n        # Filter for tags\n        if args.tags:\n            data = {key: value for key, value in data.items() if\n                    any(tag_list in args.tags for tag_list in value.get(\"tags\"))}\n\n        # Helper function for filter\n        def item_file_exists(item_id):\n            item_output: Path = output_dir / item_id\n            return not item_output.exists() or not any(item_output.iterdir())\n\n        # Filter to only fetch files not in directory\n        missing_item_ids = list(filter(item_file_exists, map(lambda item_id: item_id, data.keys())))\n\n        # Skip download if no items are missing\n        if not missing_item_ids:\n            print(\"Skipping download of\\t\" + output_dir.name + \" All files exist\")\n            return\n        print(\"Starting download of\\t\" + output_dir.name )\n\n        # Start threadpool to download\n        with concurrent.futures.ThreadPoolExecutor(max_workers= args_max_workers) as executor:\n            # Create a list of futures\n            futures = []\n            for item_id in missing_item_ids:\n                item_output: Path = output_dir / item_id\n                item_output.mkdir(exist_ok=True)\n                futures.append(executor.submit(item_download_func, item_id, item_output))\n\n            # Initialize progress bar\n            widgets = [Percentage(),' ', Bar(), ' ', ETA(),' ', AdaptiveETA()]\n            progress = ProgressBar(widgets= widgets, maxval= len(futures))\n\n            # Execute list of futures\n            for future in progress(concurrent.futures.as_completed(futures)):\n                # Check for any exceptions in the threads\n                # pylint: disable=broad-exception-caught\n                try:\n                    future.result()\n                except Exception as exc:\n                    print(f\"Thread generated an exception: {exc}\")\n                # pylint: enable=broad-exception-caught\n\n\n\n    def download_texture(item_id: str, output_dir: Path):\n        request = requests.get(f\"https://api.polyhaven.com/files/{item_id}\", timeout=30)\n        data = request.json()\n\n        # Go over all available texture types\n        for key in data.keys():\n            # Filter out the ones we need\n            if key in [\"AO\", \"Displacement\", \"Diffuse\", \"rough_ao\", \"nor_gl\", \"Rough\"]:\n                # Check resolution is available\n                if args.resolution not in data[key]:\n                    print(f\"Skipping {key} texture {item_id} as the desired resolution is not available.\")\n                    continue\n\n                # Check format is available\n                if args.format not in data[key][args.resolution]:\n                    print(f\"Skipping {key} texture {item_id} as the desired format is not available.\")\n                    continue\n\n                # Download image\n                download_url = data[key][args.resolution][args.format][\"url\"]\n                download_file(download_url, output_dir / download_url.split(\"/\")[-1])\n\n    def download_hdri(item_id: str, output_dir: Path):\n        # Collect metadata to hdri\n        request = requests.get(f\"https://api.polyhaven.com/files/{item_id}\", timeout=30)\n        data = request.json()\n\n        # Check resolution is available\n        if args.resolution not in data[\"hdri\"]:\n            print(f\"Skipping hdri {item_id} as the desired resolution is not available.\")\n            return\n\n        # Download hdri\n        download_url = data[\"hdri\"][args.resolution][\"hdr\"][\"url\"]\n        download_file(download_url, output_dir / download_url.split(\"/\")[-1])\n\n    def download_model(item_id: str, output_dir: Path):\n        # Collect metadata to model\n        request = requests.get(f\"https://api.polyhaven.com/files/{item_id}\", timeout=30)\n        data = request.json()\n\n        # Check resolution is available\n        if args.resolution not in data[\"blend\"]:\n            print(f\"Skipping model {item_id} as the desired resolution is not available.\")\n            return\n\n        # Download blend file\n        blend_data = data[\"blend\"][args.resolution][\"blend\"]\n        download_file(blend_data[\"url\"], output_dir / blend_data[\"url\"].split(\"/\")[-1])\n\n        # Download textures\n        for texture_path, texture_data in blend_data[\"include\"].items():\n            destination = output_dir / texture_path\n            destination.parent.mkdir(parents=True, exist_ok=True)\n            download_file(texture_data[\"url\"], destination)\n\n    download_items(\"textures\", args_output_dir / \"textures\", download_texture)\n    download_items(\"hdris\", args_output_dir / \"hdris\", download_hdri)\n    download_items(\"models\", args_output_dir / \"models\", download_model)",
  "def download_file(url: str, output_path: str):\n        # Download\n        request = requests.get(url, timeout=30)\n        # Write to file\n        with open(output_path, \"wb\") as file:\n            file.write(request.content)",
  "def download_items(item_type: str, output_dir: Path, item_download_func: Callable[[str, Path], None]):\n        # Filter for type\n        if args.types and item_type not in args.types:\n            return\n\n        print(\"Preparing download of\\t\" + output_dir.name )\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Download listing\n        response = requests.get(f\"https://api.polyhaven.com/assets?t={item_type}\"\n                                f\"&categories={','.join(args.categories)}\", timeout=30)\n        data = response.json()\n\n        # Filter for tags\n        if args.tags:\n            data = {key: value for key, value in data.items() if\n                    any(tag_list in args.tags for tag_list in value.get(\"tags\"))}\n\n        # Helper function for filter\n        def item_file_exists(item_id):\n            item_output: Path = output_dir / item_id\n            return not item_output.exists() or not any(item_output.iterdir())\n\n        # Filter to only fetch files not in directory\n        missing_item_ids = list(filter(item_file_exists, map(lambda item_id: item_id, data.keys())))\n\n        # Skip download if no items are missing\n        if not missing_item_ids:\n            print(\"Skipping download of\\t\" + output_dir.name + \" All files exist\")\n            return\n        print(\"Starting download of\\t\" + output_dir.name )\n\n        # Start threadpool to download\n        with concurrent.futures.ThreadPoolExecutor(max_workers= args_max_workers) as executor:\n            # Create a list of futures\n            futures = []\n            for item_id in missing_item_ids:\n                item_output: Path = output_dir / item_id\n                item_output.mkdir(exist_ok=True)\n                futures.append(executor.submit(item_download_func, item_id, item_output))\n\n            # Initialize progress bar\n            widgets = [Percentage(),' ', Bar(), ' ', ETA(),' ', AdaptiveETA()]\n            progress = ProgressBar(widgets= widgets, maxval= len(futures))\n\n            # Execute list of futures\n            for future in progress(concurrent.futures.as_completed(futures)):\n                # Check for any exceptions in the threads\n                # pylint: disable=broad-exception-caught\n                try:\n                    future.result()\n                except Exception as exc:\n                    print(f\"Thread generated an exception: {exc}\")",
  "def download_texture(item_id: str, output_dir: Path):\n        request = requests.get(f\"https://api.polyhaven.com/files/{item_id}\", timeout=30)\n        data = request.json()\n\n        # Go over all available texture types\n        for key in data.keys():\n            # Filter out the ones we need\n            if key in [\"AO\", \"Displacement\", \"Diffuse\", \"rough_ao\", \"nor_gl\", \"Rough\"]:\n                # Check resolution is available\n                if args.resolution not in data[key]:\n                    print(f\"Skipping {key} texture {item_id} as the desired resolution is not available.\")\n                    continue\n\n                # Check format is available\n                if args.format not in data[key][args.resolution]:\n                    print(f\"Skipping {key} texture {item_id} as the desired format is not available.\")\n                    continue\n\n                # Download image\n                download_url = data[key][args.resolution][args.format][\"url\"]\n                download_file(download_url, output_dir / download_url.split(\"/\")[-1])",
  "def download_hdri(item_id: str, output_dir: Path):\n        # Collect metadata to hdri\n        request = requests.get(f\"https://api.polyhaven.com/files/{item_id}\", timeout=30)\n        data = request.json()\n\n        # Check resolution is available\n        if args.resolution not in data[\"hdri\"]:\n            print(f\"Skipping hdri {item_id} as the desired resolution is not available.\")\n            return\n\n        # Download hdri\n        download_url = data[\"hdri\"][args.resolution][\"hdr\"][\"url\"]\n        download_file(download_url, output_dir / download_url.split(\"/\")[-1])",
  "def download_model(item_id: str, output_dir: Path):\n        # Collect metadata to model\n        request = requests.get(f\"https://api.polyhaven.com/files/{item_id}\", timeout=30)\n        data = request.json()\n\n        # Check resolution is available\n        if args.resolution not in data[\"blend\"]:\n            print(f\"Skipping model {item_id} as the desired resolution is not available.\")\n            return\n\n        # Download blend file\n        blend_data = data[\"blend\"][args.resolution][\"blend\"]\n        download_file(blend_data[\"url\"], output_dir / blend_data[\"url\"].split(\"/\")[-1])\n\n        # Download textures\n        for texture_path, texture_data in blend_data[\"include\"].items():\n            destination = output_dir / texture_path\n            destination.parent.mkdir(parents=True, exist_ok=True)\n            download_file(texture_data[\"url\"], destination)",
  "def item_file_exists(item_id):\n            item_output: Path = output_dir / item_id\n            return not item_output.exists() or not any(item_output.iterdir())"
]