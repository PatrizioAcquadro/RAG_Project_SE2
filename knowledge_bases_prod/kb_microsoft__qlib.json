[
  "class DumpDataBase:\n    INSTRUMENTS_START_FIELD = \"start_datetime\"\n    INSTRUMENTS_END_FIELD = \"end_datetime\"\n    CALENDARS_DIR_NAME = \"calendars\"\n    FEATURES_DIR_NAME = \"features\"\n    INSTRUMENTS_DIR_NAME = \"instruments\"\n    DUMP_FILE_SUFFIX = \".bin\"\n    DAILY_FORMAT = \"%Y-%m-%d\"\n    HIGH_FREQ_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n    INSTRUMENTS_SEP = \"\\t\"\n    INSTRUMENTS_FILE_NAME = \"all.txt\"\n\n    UPDATE_MODE = \"update\"\n    ALL_MODE = \"all\"\n\n    def __init__(\n        self,\n        csv_path: str,\n        qlib_dir: str,\n        backup_dir: str = None,\n        freq: str = \"day\",\n        max_workers: int = 16,\n        date_field_name: str = \"date\",\n        file_suffix: str = \".csv\",\n        symbol_field_name: str = \"symbol\",\n        exclude_fields: str = \"\",\n        include_fields: str = \"\",\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        csv_path: str\n            stock data path or directory\n        qlib_dir: str\n            qlib(dump) data director\n        backup_dir: str, default None\n            if backup_dir is not None, backup qlib_dir to backup_dir\n        freq: str, default \"day\"\n            transaction frequency\n        max_workers: int, default None\n            number of threads\n        date_field_name: str, default \"date\"\n            the name of the date field in the csv\n        file_suffix: str, default \".csv\"\n            file suffix\n        symbol_field_name: str, default \"symbol\"\n            symbol field name\n        include_fields: tuple\n            dump fields\n        exclude_fields: tuple\n            fields not dumped\n        limit_nums: int\n            Use when debugging, default None\n        \"\"\"\n        csv_path = Path(csv_path).expanduser()\n        if isinstance(exclude_fields, str):\n            exclude_fields = exclude_fields.split(\",\")\n        if isinstance(include_fields, str):\n            include_fields = include_fields.split(\",\")\n        self._exclude_fields = tuple(filter(lambda x: len(x) > 0, map(str.strip, exclude_fields)))\n        self._include_fields = tuple(filter(lambda x: len(x) > 0, map(str.strip, include_fields)))\n        self.file_suffix = file_suffix\n        self.symbol_field_name = symbol_field_name\n        self.csv_files = sorted(csv_path.glob(f\"*{self.file_suffix}\") if csv_path.is_dir() else [csv_path])\n        if limit_nums is not None:\n            self.csv_files = self.csv_files[: int(limit_nums)]\n        self.qlib_dir = Path(qlib_dir).expanduser()\n        self.backup_dir = backup_dir if backup_dir is None else Path(backup_dir).expanduser()\n        if backup_dir is not None:\n            self._backup_qlib_dir(Path(backup_dir).expanduser())\n\n        self.freq = freq\n        self.calendar_format = self.DAILY_FORMAT if self.freq == \"day\" else self.HIGH_FREQ_FORMAT\n\n        self.works = max_workers\n        self.date_field_name = date_field_name\n\n        self._calendars_dir = self.qlib_dir.joinpath(self.CALENDARS_DIR_NAME)\n        self._features_dir = self.qlib_dir.joinpath(self.FEATURES_DIR_NAME)\n        self._instruments_dir = self.qlib_dir.joinpath(self.INSTRUMENTS_DIR_NAME)\n\n        self._calendars_list = []\n\n        self._mode = self.ALL_MODE\n        self._kwargs = {}\n\n    def _backup_qlib_dir(self, target_dir: Path):\n        shutil.copytree(str(self.qlib_dir.resolve()), str(target_dir.resolve()))\n\n    def _format_datetime(self, datetime_d: [str, pd.Timestamp]):\n        datetime_d = pd.Timestamp(datetime_d)\n        return datetime_d.strftime(self.calendar_format)\n\n    def _get_date(\n        self, file_or_df: [Path, pd.DataFrame], *, is_begin_end: bool = False, as_set: bool = False\n    ) -> Iterable[pd.Timestamp]:\n        if not isinstance(file_or_df, pd.DataFrame):\n            df = self._get_source_data(file_or_df)\n        else:\n            df = file_or_df\n        if df.empty or self.date_field_name not in df.columns.tolist():\n            _calendars = pd.Series()\n        else:\n            _calendars = df[self.date_field_name]\n\n        if is_begin_end and as_set:\n            return (_calendars.min(), _calendars.max()), set(_calendars)\n        elif is_begin_end:\n            return _calendars.min(), _calendars.max()\n        elif as_set:\n            return set(_calendars)\n        else:\n            return _calendars.tolist()\n\n    def _get_source_data(self, file_path: Path) -> pd.DataFrame:\n        df = pd.read_csv(str(file_path.resolve()), low_memory=False)\n        df[self.date_field_name] = df[self.date_field_name].astype(str).astype(np.datetime64)\n        # df.drop_duplicates([self.date_field_name], inplace=True)\n        return df\n\n    def get_symbol_from_file(self, file_path: Path) -> str:\n        return fname_to_code(file_path.name[: -len(self.file_suffix)].strip().lower())\n\n    def get_dump_fields(self, df_columns: Iterable[str]) -> Iterable[str]:\n        return (\n            self._include_fields\n            if self._include_fields\n            else set(df_columns) - set(self._exclude_fields)\n            if self._exclude_fields\n            else df_columns\n        )\n\n    @staticmethod\n    def _read_calendars(calendar_path: Path) -> List[pd.Timestamp]:\n        return sorted(\n            map(\n                pd.Timestamp,\n                pd.read_csv(calendar_path, header=None).loc[:, 0].tolist(),\n            )\n        )\n\n    def _read_instruments(self, instrument_path: Path) -> pd.DataFrame:\n        df = pd.read_csv(\n            instrument_path,\n            sep=self.INSTRUMENTS_SEP,\n            names=[\n                self.symbol_field_name,\n                self.INSTRUMENTS_START_FIELD,\n                self.INSTRUMENTS_END_FIELD,\n            ],\n        )\n\n        return df\n\n    def save_calendars(self, calendars_data: list):\n        self._calendars_dir.mkdir(parents=True, exist_ok=True)\n        calendars_path = str(self._calendars_dir.joinpath(f\"{self.freq}.txt\").expanduser().resolve())\n        result_calendars_list = list(map(lambda x: self._format_datetime(x), calendars_data))\n        np.savetxt(calendars_path, result_calendars_list, fmt=\"%s\", encoding=\"utf-8\")\n\n    def save_instruments(self, instruments_data: Union[list, pd.DataFrame]):\n        self._instruments_dir.mkdir(parents=True, exist_ok=True)\n        instruments_path = str(self._instruments_dir.joinpath(self.INSTRUMENTS_FILE_NAME).resolve())\n        if isinstance(instruments_data, pd.DataFrame):\n            _df_fields = [self.symbol_field_name, self.INSTRUMENTS_START_FIELD, self.INSTRUMENTS_END_FIELD]\n            instruments_data = instruments_data.loc[:, _df_fields]\n            instruments_data[self.symbol_field_name] = instruments_data[self.symbol_field_name].apply(\n                lambda x: fname_to_code(x.lower()).upper()\n            )\n            instruments_data.to_csv(instruments_path, header=False, sep=self.INSTRUMENTS_SEP, index=False)\n        else:\n            np.savetxt(instruments_path, instruments_data, fmt=\"%s\", encoding=\"utf-8\")\n\n    def data_merge_calendar(self, df: pd.DataFrame, calendars_list: List[pd.Timestamp]) -> pd.DataFrame:\n        # calendars\n        calendars_df = pd.DataFrame(data=calendars_list, columns=[self.date_field_name])\n        calendars_df[self.date_field_name] = calendars_df[self.date_field_name].astype(np.datetime64)\n        cal_df = calendars_df[\n            (calendars_df[self.date_field_name] >= df[self.date_field_name].min())\n            & (calendars_df[self.date_field_name] <= df[self.date_field_name].max())\n        ]\n        # align index\n        cal_df.set_index(self.date_field_name, inplace=True)\n        df.set_index(self.date_field_name, inplace=True)\n        r_df = df.reindex(cal_df.index)\n        return r_df\n\n    @staticmethod\n    def get_datetime_index(df: pd.DataFrame, calendar_list: List[pd.Timestamp]) -> int:\n        return calendar_list.index(df.index.min())\n\n    def _data_to_bin(self, df: pd.DataFrame, calendar_list: List[pd.Timestamp], features_dir: Path):\n        if df.empty:\n            logger.warning(f\"{features_dir.name} data is None or empty\")\n            return\n        # align index\n        _df = self.data_merge_calendar(df, calendar_list)\n        # used when creating a bin file\n        date_index = self.get_datetime_index(_df, calendar_list)\n        for field in self.get_dump_fields(_df.columns):\n            bin_path = features_dir.joinpath(f\"{field.lower()}.{self.freq}{self.DUMP_FILE_SUFFIX}\")\n            if field not in _df.columns:\n                continue\n            if bin_path.exists() and self._mode == self.UPDATE_MODE:\n                # update\n                with bin_path.open(\"ab\") as fp:\n                    np.array(_df[field]).astype(\"<f\").tofile(fp)\n            else:\n                # append; self._mode == self.ALL_MODE or not bin_path.exists()\n                np.hstack([date_index, _df[field]]).astype(\"<f\").tofile(str(bin_path.resolve()))\n\n    def _dump_bin(self, file_or_data: [Path, pd.DataFrame], calendar_list: List[pd.Timestamp]):\n        if isinstance(file_or_data, pd.DataFrame):\n            if file_or_data.empty:\n                return\n            code = fname_to_code(str(file_or_data.iloc[0][self.symbol_field_name]).lower())\n            df = file_or_data\n        elif isinstance(file_or_data, Path):\n            code = self.get_symbol_from_file(file_or_data)\n            df = self._get_source_data(file_or_data)\n        else:\n            raise ValueError(f\"not support {type(file_or_data)}\")\n        if df is None or df.empty:\n            logger.warning(f\"{code} data is None or empty\")\n            return\n        # features save dir\n        features_dir = self._features_dir.joinpath(code_to_fname(code).lower())\n        features_dir.mkdir(parents=True, exist_ok=True)\n        self._data_to_bin(df, calendar_list, features_dir)\n\n    @abc.abstractmethod\n    def dump(self):\n        raise NotImplementedError(\"dump not implemented!\")\n\n    def __call__(self, *args, **kwargs):\n        self.dump()",
  "class DumpDataAll(DumpDataBase):\n    def _get_all_date(self):\n        logger.info(\"start get all date......\")\n        all_datetime = set()\n        date_range_list = []\n        _fun = partial(self._get_date, as_set=True, is_begin_end=True)\n        with tqdm(total=len(self.csv_files)) as p_bar:\n            with ProcessPoolExecutor(max_workers=self.works) as executor:\n                for file_path, ((_begin_time, _end_time), _set_calendars) in zip(\n                    self.csv_files, executor.map(_fun, self.csv_files)\n                ):\n                    all_datetime = all_datetime | _set_calendars\n                    if isinstance(_begin_time, pd.Timestamp) and isinstance(_end_time, pd.Timestamp):\n                        _begin_time = self._format_datetime(_begin_time)\n                        _end_time = self._format_datetime(_end_time)\n                        symbol = self.get_symbol_from_file(file_path)\n                        _inst_fields = [symbol.upper(), _begin_time, _end_time]\n                        date_range_list.append(f\"{self.INSTRUMENTS_SEP.join(_inst_fields)}\")\n                    p_bar.update()\n        self._kwargs[\"all_datetime_set\"] = all_datetime\n        self._kwargs[\"date_range_list\"] = date_range_list\n        logger.info(\"end of get all date.\\n\")\n\n    def _dump_calendars(self):\n        logger.info(\"start dump calendars......\")\n        self._calendars_list = sorted(map(pd.Timestamp, self._kwargs[\"all_datetime_set\"]))\n        self.save_calendars(self._calendars_list)\n        logger.info(\"end of calendars dump.\\n\")\n\n    def _dump_instruments(self):\n        logger.info(\"start dump instruments......\")\n        self.save_instruments(self._kwargs[\"date_range_list\"])\n        logger.info(\"end of instruments dump.\\n\")\n\n    def _dump_features(self):\n        logger.info(\"start dump features......\")\n        _dump_func = partial(self._dump_bin, calendar_list=self._calendars_list)\n        with tqdm(total=len(self.csv_files)) as p_bar:\n            with ProcessPoolExecutor(max_workers=self.works) as executor:\n                for _ in executor.map(_dump_func, self.csv_files):\n                    p_bar.update()\n\n        logger.info(\"end of features dump.\\n\")\n\n    def dump(self):\n        self._get_all_date()\n        self._dump_calendars()\n        self._dump_instruments()\n        self._dump_features()",
  "class DumpDataFix(DumpDataAll):\n    def _dump_instruments(self):\n        logger.info(\"start dump instruments......\")\n        _fun = partial(self._get_date, is_begin_end=True)\n        new_stock_files = sorted(\n            filter(\n                lambda x: fname_to_code(x.name[: -len(self.file_suffix)].strip().lower()).upper()\n                not in self._old_instruments,\n                self.csv_files,\n            )\n        )\n        with tqdm(total=len(new_stock_files)) as p_bar:\n            with ProcessPoolExecutor(max_workers=self.works) as execute:\n                for file_path, (_begin_time, _end_time) in zip(new_stock_files, execute.map(_fun, new_stock_files)):\n                    if isinstance(_begin_time, pd.Timestamp) and isinstance(_end_time, pd.Timestamp):\n                        symbol = fname_to_code(self.get_symbol_from_file(file_path).lower()).upper()\n                        _dt_map = self._old_instruments.setdefault(symbol, dict())\n                        _dt_map[self.INSTRUMENTS_START_FIELD] = self._format_datetime(_begin_time)\n                        _dt_map[self.INSTRUMENTS_END_FIELD] = self._format_datetime(_end_time)\n                    p_bar.update()\n        _inst_df = pd.DataFrame.from_dict(self._old_instruments, orient=\"index\")\n        _inst_df.index.names = [self.symbol_field_name]\n        self.save_instruments(_inst_df.reset_index())\n        logger.info(\"end of instruments dump.\\n\")\n\n    def dump(self):\n        self._calendars_list = self._read_calendars(self._calendars_dir.joinpath(f\"{self.freq}.txt\"))\n        # noinspection PyAttributeOutsideInit\n        self._old_instruments = (\n            self._read_instruments(self._instruments_dir.joinpath(self.INSTRUMENTS_FILE_NAME))\n            .set_index([self.symbol_field_name])\n            .to_dict(orient=\"index\")\n        )  # type: dict\n        self._dump_instruments()\n        self._dump_features()",
  "class DumpDataUpdate(DumpDataBase):\n    def __init__(\n        self,\n        csv_path: str,\n        qlib_dir: str,\n        backup_dir: str = None,\n        freq: str = \"day\",\n        max_workers: int = 16,\n        date_field_name: str = \"date\",\n        file_suffix: str = \".csv\",\n        symbol_field_name: str = \"symbol\",\n        exclude_fields: str = \"\",\n        include_fields: str = \"\",\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        csv_path: str\n            stock data path or directory\n        qlib_dir: str\n            qlib(dump) data director\n        backup_dir: str, default None\n            if backup_dir is not None, backup qlib_dir to backup_dir\n        freq: str, default \"day\"\n            transaction frequency\n        max_workers: int, default None\n            number of threads\n        date_field_name: str, default \"date\"\n            the name of the date field in the csv\n        file_suffix: str, default \".csv\"\n            file suffix\n        symbol_field_name: str, default \"symbol\"\n            symbol field name\n        include_fields: tuple\n            dump fields\n        exclude_fields: tuple\n            fields not dumped\n        limit_nums: int\n            Use when debugging, default None\n        \"\"\"\n        super().__init__(\n            csv_path,\n            qlib_dir,\n            backup_dir,\n            freq,\n            max_workers,\n            date_field_name,\n            file_suffix,\n            symbol_field_name,\n            exclude_fields,\n            include_fields,\n        )\n        self._mode = self.UPDATE_MODE\n        self._old_calendar_list = self._read_calendars(self._calendars_dir.joinpath(f\"{self.freq}.txt\"))\n        self._update_instruments = (\n            self._read_instruments(self._instruments_dir.joinpath(self.INSTRUMENTS_FILE_NAME))\n            .set_index([self.symbol_field_name])\n            .to_dict(orient=\"index\")\n        )  # type: dict\n\n        # load all csv files\n        self._all_data = self._load_all_source_data()  # type: pd.DataFrame\n        self._update_calendars = sorted(\n            filter(lambda x: x > self._old_calendar_list[-1], self._all_data[self.date_field_name].unique())\n        )\n        self._new_calendar_list = self._old_calendar_list + self._update_calendars\n\n    def _load_all_source_data(self):\n        # NOTE: Need more memory\n        logger.info(\"start load all source data....\")\n        all_df = []\n\n        def _read_csv(file_path: Path):\n            _df = pd.read_csv(file_path, parse_dates=[self.date_field_name])\n            if self.symbol_field_name not in _df.columns:\n                _df[self.symbol_field_name] = self.get_symbol_from_file(file_path)\n            return _df\n\n        with tqdm(total=len(self.csv_files)) as p_bar:\n            with ThreadPoolExecutor(max_workers=self.works) as executor:\n                for df in executor.map(_read_csv, self.csv_files):\n                    if not df.empty:\n                        all_df.append(df)\n                    p_bar.update()\n\n        logger.info(\"end of load all data.\\n\")\n        return pd.concat(all_df, sort=False)\n\n    def _dump_calendars(self):\n        pass\n\n    def _dump_instruments(self):\n        pass\n\n    def _dump_features(self):\n        logger.info(\"start dump features......\")\n        error_code = {}\n        with ProcessPoolExecutor(max_workers=self.works) as executor:\n            futures = {}\n            for _code, _df in self._all_data.groupby(self.symbol_field_name):\n                _code = fname_to_code(str(_code).lower()).upper()\n                _start, _end = self._get_date(_df, is_begin_end=True)\n                if not (isinstance(_start, pd.Timestamp) and isinstance(_end, pd.Timestamp)):\n                    continue\n                if _code in self._update_instruments:\n                    self._update_instruments[_code][self.INSTRUMENTS_END_FIELD] = self._format_datetime(_end)\n                    futures[executor.submit(self._dump_bin, _df, self._update_calendars)] = _code\n                else:\n                    # new stock\n                    _dt_range = self._update_instruments.setdefault(_code, dict())\n                    _dt_range[self.INSTRUMENTS_START_FIELD] = self._format_datetime(_start)\n                    _dt_range[self.INSTRUMENTS_END_FIELD] = self._format_datetime(_end)\n                    futures[executor.submit(self._dump_bin, _df, self._new_calendar_list)] = _code\n\n            with tqdm(total=len(futures)) as p_bar:\n                for _future in as_completed(futures):\n                    try:\n                        _future.result()\n                    except Exception:\n                        error_code[futures[_future]] = traceback.format_exc()\n                    p_bar.update()\n            logger.info(f\"dump bin errors\uff1a {error_code}\")\n\n        logger.info(\"end of features dump.\\n\")\n\n    def dump(self):\n        self.save_calendars(self._new_calendar_list)\n        self._dump_features()\n        df = pd.DataFrame.from_dict(self._update_instruments, orient=\"index\")\n        df.index.names = [self.symbol_field_name]\n        self.save_instruments(df.reset_index())",
  "def __init__(\n        self,\n        csv_path: str,\n        qlib_dir: str,\n        backup_dir: str = None,\n        freq: str = \"day\",\n        max_workers: int = 16,\n        date_field_name: str = \"date\",\n        file_suffix: str = \".csv\",\n        symbol_field_name: str = \"symbol\",\n        exclude_fields: str = \"\",\n        include_fields: str = \"\",\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        csv_path: str\n            stock data path or directory\n        qlib_dir: str\n            qlib(dump) data director\n        backup_dir: str, default None\n            if backup_dir is not None, backup qlib_dir to backup_dir\n        freq: str, default \"day\"\n            transaction frequency\n        max_workers: int, default None\n            number of threads\n        date_field_name: str, default \"date\"\n            the name of the date field in the csv\n        file_suffix: str, default \".csv\"\n            file suffix\n        symbol_field_name: str, default \"symbol\"\n            symbol field name\n        include_fields: tuple\n            dump fields\n        exclude_fields: tuple\n            fields not dumped\n        limit_nums: int\n            Use when debugging, default None\n        \"\"\"\n        csv_path = Path(csv_path).expanduser()\n        if isinstance(exclude_fields, str):\n            exclude_fields = exclude_fields.split(\",\")\n        if isinstance(include_fields, str):\n            include_fields = include_fields.split(\",\")\n        self._exclude_fields = tuple(filter(lambda x: len(x) > 0, map(str.strip, exclude_fields)))\n        self._include_fields = tuple(filter(lambda x: len(x) > 0, map(str.strip, include_fields)))\n        self.file_suffix = file_suffix\n        self.symbol_field_name = symbol_field_name\n        self.csv_files = sorted(csv_path.glob(f\"*{self.file_suffix}\") if csv_path.is_dir() else [csv_path])\n        if limit_nums is not None:\n            self.csv_files = self.csv_files[: int(limit_nums)]\n        self.qlib_dir = Path(qlib_dir).expanduser()\n        self.backup_dir = backup_dir if backup_dir is None else Path(backup_dir).expanduser()\n        if backup_dir is not None:\n            self._backup_qlib_dir(Path(backup_dir).expanduser())\n\n        self.freq = freq\n        self.calendar_format = self.DAILY_FORMAT if self.freq == \"day\" else self.HIGH_FREQ_FORMAT\n\n        self.works = max_workers\n        self.date_field_name = date_field_name\n\n        self._calendars_dir = self.qlib_dir.joinpath(self.CALENDARS_DIR_NAME)\n        self._features_dir = self.qlib_dir.joinpath(self.FEATURES_DIR_NAME)\n        self._instruments_dir = self.qlib_dir.joinpath(self.INSTRUMENTS_DIR_NAME)\n\n        self._calendars_list = []\n\n        self._mode = self.ALL_MODE\n        self._kwargs = {}",
  "def _backup_qlib_dir(self, target_dir: Path):\n        shutil.copytree(str(self.qlib_dir.resolve()), str(target_dir.resolve()))",
  "def _format_datetime(self, datetime_d: [str, pd.Timestamp]):\n        datetime_d = pd.Timestamp(datetime_d)\n        return datetime_d.strftime(self.calendar_format)",
  "def _get_date(\n        self, file_or_df: [Path, pd.DataFrame], *, is_begin_end: bool = False, as_set: bool = False\n    ) -> Iterable[pd.Timestamp]:\n        if not isinstance(file_or_df, pd.DataFrame):\n            df = self._get_source_data(file_or_df)\n        else:\n            df = file_or_df\n        if df.empty or self.date_field_name not in df.columns.tolist():\n            _calendars = pd.Series()\n        else:\n            _calendars = df[self.date_field_name]\n\n        if is_begin_end and as_set:\n            return (_calendars.min(), _calendars.max()), set(_calendars)\n        elif is_begin_end:\n            return _calendars.min(), _calendars.max()\n        elif as_set:\n            return set(_calendars)\n        else:\n            return _calendars.tolist()",
  "def _get_source_data(self, file_path: Path) -> pd.DataFrame:\n        df = pd.read_csv(str(file_path.resolve()), low_memory=False)\n        df[self.date_field_name] = df[self.date_field_name].astype(str).astype(np.datetime64)\n        # df.drop_duplicates([self.date_field_name], inplace=True)\n        return df",
  "def get_symbol_from_file(self, file_path: Path) -> str:\n        return fname_to_code(file_path.name[: -len(self.file_suffix)].strip().lower())",
  "def get_dump_fields(self, df_columns: Iterable[str]) -> Iterable[str]:\n        return (\n            self._include_fields\n            if self._include_fields\n            else set(df_columns) - set(self._exclude_fields)\n            if self._exclude_fields\n            else df_columns\n        )",
  "def _read_calendars(calendar_path: Path) -> List[pd.Timestamp]:\n        return sorted(\n            map(\n                pd.Timestamp,\n                pd.read_csv(calendar_path, header=None).loc[:, 0].tolist(),\n            )\n        )",
  "def _read_instruments(self, instrument_path: Path) -> pd.DataFrame:\n        df = pd.read_csv(\n            instrument_path,\n            sep=self.INSTRUMENTS_SEP,\n            names=[\n                self.symbol_field_name,\n                self.INSTRUMENTS_START_FIELD,\n                self.INSTRUMENTS_END_FIELD,\n            ],\n        )\n\n        return df",
  "def save_calendars(self, calendars_data: list):\n        self._calendars_dir.mkdir(parents=True, exist_ok=True)\n        calendars_path = str(self._calendars_dir.joinpath(f\"{self.freq}.txt\").expanduser().resolve())\n        result_calendars_list = list(map(lambda x: self._format_datetime(x), calendars_data))\n        np.savetxt(calendars_path, result_calendars_list, fmt=\"%s\", encoding=\"utf-8\")",
  "def save_instruments(self, instruments_data: Union[list, pd.DataFrame]):\n        self._instruments_dir.mkdir(parents=True, exist_ok=True)\n        instruments_path = str(self._instruments_dir.joinpath(self.INSTRUMENTS_FILE_NAME).resolve())\n        if isinstance(instruments_data, pd.DataFrame):\n            _df_fields = [self.symbol_field_name, self.INSTRUMENTS_START_FIELD, self.INSTRUMENTS_END_FIELD]\n            instruments_data = instruments_data.loc[:, _df_fields]\n            instruments_data[self.symbol_field_name] = instruments_data[self.symbol_field_name].apply(\n                lambda x: fname_to_code(x.lower()).upper()\n            )\n            instruments_data.to_csv(instruments_path, header=False, sep=self.INSTRUMENTS_SEP, index=False)\n        else:\n            np.savetxt(instruments_path, instruments_data, fmt=\"%s\", encoding=\"utf-8\")",
  "def data_merge_calendar(self, df: pd.DataFrame, calendars_list: List[pd.Timestamp]) -> pd.DataFrame:\n        # calendars\n        calendars_df = pd.DataFrame(data=calendars_list, columns=[self.date_field_name])\n        calendars_df[self.date_field_name] = calendars_df[self.date_field_name].astype(np.datetime64)\n        cal_df = calendars_df[\n            (calendars_df[self.date_field_name] >= df[self.date_field_name].min())\n            & (calendars_df[self.date_field_name] <= df[self.date_field_name].max())\n        ]\n        # align index\n        cal_df.set_index(self.date_field_name, inplace=True)\n        df.set_index(self.date_field_name, inplace=True)\n        r_df = df.reindex(cal_df.index)\n        return r_df",
  "def get_datetime_index(df: pd.DataFrame, calendar_list: List[pd.Timestamp]) -> int:\n        return calendar_list.index(df.index.min())",
  "def _data_to_bin(self, df: pd.DataFrame, calendar_list: List[pd.Timestamp], features_dir: Path):\n        if df.empty:\n            logger.warning(f\"{features_dir.name} data is None or empty\")\n            return\n        # align index\n        _df = self.data_merge_calendar(df, calendar_list)\n        # used when creating a bin file\n        date_index = self.get_datetime_index(_df, calendar_list)\n        for field in self.get_dump_fields(_df.columns):\n            bin_path = features_dir.joinpath(f\"{field.lower()}.{self.freq}{self.DUMP_FILE_SUFFIX}\")\n            if field not in _df.columns:\n                continue\n            if bin_path.exists() and self._mode == self.UPDATE_MODE:\n                # update\n                with bin_path.open(\"ab\") as fp:\n                    np.array(_df[field]).astype(\"<f\").tofile(fp)\n            else:\n                # append; self._mode == self.ALL_MODE or not bin_path.exists()\n                np.hstack([date_index, _df[field]]).astype(\"<f\").tofile(str(bin_path.resolve()))",
  "def _dump_bin(self, file_or_data: [Path, pd.DataFrame], calendar_list: List[pd.Timestamp]):\n        if isinstance(file_or_data, pd.DataFrame):\n            if file_or_data.empty:\n                return\n            code = fname_to_code(str(file_or_data.iloc[0][self.symbol_field_name]).lower())\n            df = file_or_data\n        elif isinstance(file_or_data, Path):\n            code = self.get_symbol_from_file(file_or_data)\n            df = self._get_source_data(file_or_data)\n        else:\n            raise ValueError(f\"not support {type(file_or_data)}\")\n        if df is None or df.empty:\n            logger.warning(f\"{code} data is None or empty\")\n            return\n        # features save dir\n        features_dir = self._features_dir.joinpath(code_to_fname(code).lower())\n        features_dir.mkdir(parents=True, exist_ok=True)\n        self._data_to_bin(df, calendar_list, features_dir)",
  "def dump(self):\n        raise NotImplementedError(\"dump not implemented!\")",
  "def __call__(self, *args, **kwargs):\n        self.dump()",
  "def _get_all_date(self):\n        logger.info(\"start get all date......\")\n        all_datetime = set()\n        date_range_list = []\n        _fun = partial(self._get_date, as_set=True, is_begin_end=True)\n        with tqdm(total=len(self.csv_files)) as p_bar:\n            with ProcessPoolExecutor(max_workers=self.works) as executor:\n                for file_path, ((_begin_time, _end_time), _set_calendars) in zip(\n                    self.csv_files, executor.map(_fun, self.csv_files)\n                ):\n                    all_datetime = all_datetime | _set_calendars\n                    if isinstance(_begin_time, pd.Timestamp) and isinstance(_end_time, pd.Timestamp):\n                        _begin_time = self._format_datetime(_begin_time)\n                        _end_time = self._format_datetime(_end_time)\n                        symbol = self.get_symbol_from_file(file_path)\n                        _inst_fields = [symbol.upper(), _begin_time, _end_time]\n                        date_range_list.append(f\"{self.INSTRUMENTS_SEP.join(_inst_fields)}\")\n                    p_bar.update()\n        self._kwargs[\"all_datetime_set\"] = all_datetime\n        self._kwargs[\"date_range_list\"] = date_range_list\n        logger.info(\"end of get all date.\\n\")",
  "def _dump_calendars(self):\n        logger.info(\"start dump calendars......\")\n        self._calendars_list = sorted(map(pd.Timestamp, self._kwargs[\"all_datetime_set\"]))\n        self.save_calendars(self._calendars_list)\n        logger.info(\"end of calendars dump.\\n\")",
  "def _dump_instruments(self):\n        logger.info(\"start dump instruments......\")\n        self.save_instruments(self._kwargs[\"date_range_list\"])\n        logger.info(\"end of instruments dump.\\n\")",
  "def _dump_features(self):\n        logger.info(\"start dump features......\")\n        _dump_func = partial(self._dump_bin, calendar_list=self._calendars_list)\n        with tqdm(total=len(self.csv_files)) as p_bar:\n            with ProcessPoolExecutor(max_workers=self.works) as executor:\n                for _ in executor.map(_dump_func, self.csv_files):\n                    p_bar.update()\n\n        logger.info(\"end of features dump.\\n\")",
  "def dump(self):\n        self._get_all_date()\n        self._dump_calendars()\n        self._dump_instruments()\n        self._dump_features()",
  "def _dump_instruments(self):\n        logger.info(\"start dump instruments......\")\n        _fun = partial(self._get_date, is_begin_end=True)\n        new_stock_files = sorted(\n            filter(\n                lambda x: fname_to_code(x.name[: -len(self.file_suffix)].strip().lower()).upper()\n                not in self._old_instruments,\n                self.csv_files,\n            )\n        )\n        with tqdm(total=len(new_stock_files)) as p_bar:\n            with ProcessPoolExecutor(max_workers=self.works) as execute:\n                for file_path, (_begin_time, _end_time) in zip(new_stock_files, execute.map(_fun, new_stock_files)):\n                    if isinstance(_begin_time, pd.Timestamp) and isinstance(_end_time, pd.Timestamp):\n                        symbol = fname_to_code(self.get_symbol_from_file(file_path).lower()).upper()\n                        _dt_map = self._old_instruments.setdefault(symbol, dict())\n                        _dt_map[self.INSTRUMENTS_START_FIELD] = self._format_datetime(_begin_time)\n                        _dt_map[self.INSTRUMENTS_END_FIELD] = self._format_datetime(_end_time)\n                    p_bar.update()\n        _inst_df = pd.DataFrame.from_dict(self._old_instruments, orient=\"index\")\n        _inst_df.index.names = [self.symbol_field_name]\n        self.save_instruments(_inst_df.reset_index())\n        logger.info(\"end of instruments dump.\\n\")",
  "def dump(self):\n        self._calendars_list = self._read_calendars(self._calendars_dir.joinpath(f\"{self.freq}.txt\"))\n        # noinspection PyAttributeOutsideInit\n        self._old_instruments = (\n            self._read_instruments(self._instruments_dir.joinpath(self.INSTRUMENTS_FILE_NAME))\n            .set_index([self.symbol_field_name])\n            .to_dict(orient=\"index\")\n        )  # type: dict\n        self._dump_instruments()\n        self._dump_features()",
  "def __init__(\n        self,\n        csv_path: str,\n        qlib_dir: str,\n        backup_dir: str = None,\n        freq: str = \"day\",\n        max_workers: int = 16,\n        date_field_name: str = \"date\",\n        file_suffix: str = \".csv\",\n        symbol_field_name: str = \"symbol\",\n        exclude_fields: str = \"\",\n        include_fields: str = \"\",\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        csv_path: str\n            stock data path or directory\n        qlib_dir: str\n            qlib(dump) data director\n        backup_dir: str, default None\n            if backup_dir is not None, backup qlib_dir to backup_dir\n        freq: str, default \"day\"\n            transaction frequency\n        max_workers: int, default None\n            number of threads\n        date_field_name: str, default \"date\"\n            the name of the date field in the csv\n        file_suffix: str, default \".csv\"\n            file suffix\n        symbol_field_name: str, default \"symbol\"\n            symbol field name\n        include_fields: tuple\n            dump fields\n        exclude_fields: tuple\n            fields not dumped\n        limit_nums: int\n            Use when debugging, default None\n        \"\"\"\n        super().__init__(\n            csv_path,\n            qlib_dir,\n            backup_dir,\n            freq,\n            max_workers,\n            date_field_name,\n            file_suffix,\n            symbol_field_name,\n            exclude_fields,\n            include_fields,\n        )\n        self._mode = self.UPDATE_MODE\n        self._old_calendar_list = self._read_calendars(self._calendars_dir.joinpath(f\"{self.freq}.txt\"))\n        self._update_instruments = (\n            self._read_instruments(self._instruments_dir.joinpath(self.INSTRUMENTS_FILE_NAME))\n            .set_index([self.symbol_field_name])\n            .to_dict(orient=\"index\")\n        )  # type: dict\n\n        # load all csv files\n        self._all_data = self._load_all_source_data()  # type: pd.DataFrame\n        self._update_calendars = sorted(\n            filter(lambda x: x > self._old_calendar_list[-1], self._all_data[self.date_field_name].unique())\n        )\n        self._new_calendar_list = self._old_calendar_list + self._update_calendars",
  "def _load_all_source_data(self):\n        # NOTE: Need more memory\n        logger.info(\"start load all source data....\")\n        all_df = []\n\n        def _read_csv(file_path: Path):\n            _df = pd.read_csv(file_path, parse_dates=[self.date_field_name])\n            if self.symbol_field_name not in _df.columns:\n                _df[self.symbol_field_name] = self.get_symbol_from_file(file_path)\n            return _df\n\n        with tqdm(total=len(self.csv_files)) as p_bar:\n            with ThreadPoolExecutor(max_workers=self.works) as executor:\n                for df in executor.map(_read_csv, self.csv_files):\n                    if not df.empty:\n                        all_df.append(df)\n                    p_bar.update()\n\n        logger.info(\"end of load all data.\\n\")\n        return pd.concat(all_df, sort=False)",
  "def _dump_calendars(self):\n        pass",
  "def _dump_instruments(self):\n        pass",
  "def _dump_features(self):\n        logger.info(\"start dump features......\")\n        error_code = {}\n        with ProcessPoolExecutor(max_workers=self.works) as executor:\n            futures = {}\n            for _code, _df in self._all_data.groupby(self.symbol_field_name):\n                _code = fname_to_code(str(_code).lower()).upper()\n                _start, _end = self._get_date(_df, is_begin_end=True)\n                if not (isinstance(_start, pd.Timestamp) and isinstance(_end, pd.Timestamp)):\n                    continue\n                if _code in self._update_instruments:\n                    self._update_instruments[_code][self.INSTRUMENTS_END_FIELD] = self._format_datetime(_end)\n                    futures[executor.submit(self._dump_bin, _df, self._update_calendars)] = _code\n                else:\n                    # new stock\n                    _dt_range = self._update_instruments.setdefault(_code, dict())\n                    _dt_range[self.INSTRUMENTS_START_FIELD] = self._format_datetime(_start)\n                    _dt_range[self.INSTRUMENTS_END_FIELD] = self._format_datetime(_end)\n                    futures[executor.submit(self._dump_bin, _df, self._new_calendar_list)] = _code\n\n            with tqdm(total=len(futures)) as p_bar:\n                for _future in as_completed(futures):\n                    try:\n                        _future.result()\n                    except Exception:\n                        error_code[futures[_future]] = traceback.format_exc()\n                    p_bar.update()\n            logger.info(f\"dump bin errors\uff1a {error_code}\")\n\n        logger.info(\"end of features dump.\\n\")",
  "def dump(self):\n        self.save_calendars(self._new_calendar_list)\n        self._dump_features()\n        df = pd.DataFrame.from_dict(self._update_instruments, orient=\"index\")\n        df.index.names = [self.symbol_field_name]\n        self.save_instruments(df.reset_index())",
  "def _read_csv(file_path: Path):\n            _df = pd.read_csv(file_path, parse_dates=[self.date_field_name])\n            if self.symbol_field_name not in _df.columns:\n                _df[self.symbol_field_name] = self.get_symbol_from_file(file_path)\n            return _df",
  "class InfoCollector:\n    \"\"\"\n    User could collect system info by following commands\n    `cd scripts && python collect_info.py all`\n    - NOTE: please avoid running this script in the project folder which contains `qlib`\n    \"\"\"\n\n    def sys(self):\n        \"\"\"collect system related info\"\"\"\n        for method in [\"system\", \"machine\", \"platform\", \"version\"]:\n            print(getattr(platform, method)())\n\n    def py(self):\n        \"\"\"collect Python related info\"\"\"\n        print(\"Python version: {}\".format(sys.version.replace(\"\\n\", \" \")))\n\n    def qlib(self):\n        \"\"\"collect qlib related info\"\"\"\n        print(\"Qlib version: {}\".format(qlib.__version__))\n        REQUIRED = [\n            \"numpy\",\n            \"pandas\",\n            \"scipy\",\n            \"requests\",\n            \"sacred\",\n            \"python-socketio\",\n            \"redis\",\n            \"python-redis-lock\",\n            \"schedule\",\n            \"cvxpy\",\n            \"hyperopt\",\n            \"fire\",\n            \"statsmodels\",\n            \"xlrd\",\n            \"plotly\",\n            \"matplotlib\",\n            \"tables\",\n            \"pyyaml\",\n            \"mlflow\",\n            \"tqdm\",\n            \"loguru\",\n            \"lightgbm\",\n            \"tornado\",\n            \"joblib\",\n            \"fire\",\n            \"ruamel.yaml\",\n        ]\n\n        for package in REQUIRED:\n            version = pkg_resources.get_distribution(package).version\n            print(f\"{package}=={version}\")\n\n    def all(self):\n        \"\"\"collect all info\"\"\"\n        for method in [\"sys\", \"py\", \"qlib\"]:\n            getattr(self, method)()\n            print()",
  "def sys(self):\n        \"\"\"collect system related info\"\"\"\n        for method in [\"system\", \"machine\", \"platform\", \"version\"]:\n            print(getattr(platform, method)())",
  "def py(self):\n        \"\"\"collect Python related info\"\"\"\n        print(\"Python version: {}\".format(sys.version.replace(\"\\n\", \" \")))",
  "def qlib(self):\n        \"\"\"collect qlib related info\"\"\"\n        print(\"Qlib version: {}\".format(qlib.__version__))\n        REQUIRED = [\n            \"numpy\",\n            \"pandas\",\n            \"scipy\",\n            \"requests\",\n            \"sacred\",\n            \"python-socketio\",\n            \"redis\",\n            \"python-redis-lock\",\n            \"schedule\",\n            \"cvxpy\",\n            \"hyperopt\",\n            \"fire\",\n            \"statsmodels\",\n            \"xlrd\",\n            \"plotly\",\n            \"matplotlib\",\n            \"tables\",\n            \"pyyaml\",\n            \"mlflow\",\n            \"tqdm\",\n            \"loguru\",\n            \"lightgbm\",\n            \"tornado\",\n            \"joblib\",\n            \"fire\",\n            \"ruamel.yaml\",\n        ]\n\n        for package in REQUIRED:\n            version = pkg_resources.get_distribution(package).version\n            print(f\"{package}=={version}\")",
  "def all(self):\n        \"\"\"collect all info\"\"\"\n        for method in [\"sys\", \"py\", \"qlib\"]:\n            getattr(self, method)()\n            print()",
  "class CheckBin:\n\n    NOT_IN_FEATURES = \"not in features\"\n    COMPARE_FALSE = \"compare False\"\n    COMPARE_TRUE = \"compare True\"\n    COMPARE_ERROR = \"compare error\"\n\n    def __init__(\n        self,\n        qlib_dir: str,\n        csv_path: str,\n        check_fields: str = None,\n        freq: str = \"day\",\n        symbol_field_name: str = \"symbol\",\n        date_field_name: str = \"date\",\n        file_suffix: str = \".csv\",\n        max_workers: int = 16,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        qlib_dir : str\n            qlib dir\n        csv_path : str\n            origin csv path\n        check_fields : str, optional\n            check fields, by default None, check qlib_dir/features/<first_dir>/*.<freq>.bin\n        freq : str, optional\n            freq, value from [\"day\", \"1m\"]\n        symbol_field_name: str, optional\n            symbol field name, by default \"symbol\"\n        date_field_name: str, optional\n            date field name, by default \"date\"\n        file_suffix: str, optional\n            csv file suffix, by default \".csv\"\n        max_workers: int, optional\n            max workers, by default 16\n        \"\"\"\n        self.qlib_dir = Path(qlib_dir).expanduser()\n        bin_path_list = list(self.qlib_dir.joinpath(\"features\").iterdir())\n        self.qlib_symbols = sorted(map(lambda x: x.name.lower(), bin_path_list))\n        qlib.init(\n            provider_uri=str(self.qlib_dir.resolve()),\n            mount_path=str(self.qlib_dir.resolve()),\n            auto_mount=False,\n            redis_port=-1,\n        )\n        csv_path = Path(csv_path).expanduser()\n        self.csv_files = sorted(csv_path.glob(f\"*{file_suffix}\") if csv_path.is_dir() else [csv_path])\n\n        if check_fields is None:\n            check_fields = list(map(lambda x: x.name.split(\".\")[0], bin_path_list[0].glob(f\"*.bin\")))\n        else:\n            check_fields = check_fields.split(\",\") if isinstance(check_fields, str) else check_fields\n        self.check_fields = list(map(lambda x: x.strip(), check_fields))\n        self.qlib_fields = list(map(lambda x: f\"${x}\", self.check_fields))\n        self.max_workers = max_workers\n        self.symbol_field_name = symbol_field_name\n        self.date_field_name = date_field_name\n        self.freq = freq\n        self.file_suffix = file_suffix\n\n    def _compare(self, file_path: Path):\n        symbol = file_path.name.strip(self.file_suffix)\n        if symbol.lower() not in self.qlib_symbols:\n            return self.NOT_IN_FEATURES\n        # qlib data\n        qlib_df = D.features([symbol], self.qlib_fields, freq=self.freq)\n        qlib_df.rename(columns={_c: _c.strip(\"$\") for _c in qlib_df.columns}, inplace=True)\n        # csv data\n        origin_df = pd.read_csv(file_path)\n        origin_df[self.date_field_name] = pd.to_datetime(origin_df[self.date_field_name])\n        if self.symbol_field_name not in origin_df.columns:\n            origin_df[self.symbol_field_name] = symbol\n        origin_df.set_index([self.symbol_field_name, self.date_field_name], inplace=True)\n        origin_df.index.names = qlib_df.index.names\n        origin_df = origin_df.reindex(qlib_df.index)\n        try:\n            compare = datacompy.Compare(\n                origin_df,\n                qlib_df,\n                on_index=True,\n                abs_tol=1e-08,  # Optional, defaults to 0\n                rel_tol=1e-05,  # Optional, defaults to 0\n                df1_name=\"Original\",  # Optional, defaults to 'df1'\n                df2_name=\"New\",  # Optional, defaults to 'df2'\n            )\n            _r = compare.matches(ignore_extra_columns=True)\n            return self.COMPARE_TRUE if _r else self.COMPARE_FALSE\n        except Exception as e:\n            logger.warning(f\"{symbol} compare error: {e}\")\n            return self.COMPARE_ERROR\n\n    def check(self):\n        \"\"\"Check whether the bin file after ``dump_bin.py`` is executed is consistent with the original csv file data\"\"\"\n        logger.info(\"start check......\")\n\n        error_list = []\n        not_in_features = []\n        compare_false = []\n        with tqdm(total=len(self.csv_files)) as p_bar:\n            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n                for file_path, _check_res in zip(self.csv_files, executor.map(self._compare, self.csv_files)):\n                    symbol = file_path.name.strip(self.file_suffix)\n                    if _check_res == self.NOT_IN_FEATURES:\n                        not_in_features.append(symbol)\n                    elif _check_res == self.COMPARE_ERROR:\n                        error_list.append(symbol)\n                    elif _check_res == self.COMPARE_FALSE:\n                        compare_false.append(symbol)\n                    p_bar.update()\n\n        logger.info(\"end of check......\")\n        if error_list:\n            logger.warning(f\"compare error: {error_list}\")\n        if not_in_features:\n            logger.warning(f\"not in features: {not_in_features}\")\n        if compare_false:\n            logger.warning(f\"compare False: {compare_false}\")\n        logger.info(\n            f\"total {len(self.csv_files)}, {len(error_list)} errors, {len(not_in_features)} not in features, {len(compare_false)} compare false\"\n        )",
  "def __init__(\n        self,\n        qlib_dir: str,\n        csv_path: str,\n        check_fields: str = None,\n        freq: str = \"day\",\n        symbol_field_name: str = \"symbol\",\n        date_field_name: str = \"date\",\n        file_suffix: str = \".csv\",\n        max_workers: int = 16,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        qlib_dir : str\n            qlib dir\n        csv_path : str\n            origin csv path\n        check_fields : str, optional\n            check fields, by default None, check qlib_dir/features/<first_dir>/*.<freq>.bin\n        freq : str, optional\n            freq, value from [\"day\", \"1m\"]\n        symbol_field_name: str, optional\n            symbol field name, by default \"symbol\"\n        date_field_name: str, optional\n            date field name, by default \"date\"\n        file_suffix: str, optional\n            csv file suffix, by default \".csv\"\n        max_workers: int, optional\n            max workers, by default 16\n        \"\"\"\n        self.qlib_dir = Path(qlib_dir).expanduser()\n        bin_path_list = list(self.qlib_dir.joinpath(\"features\").iterdir())\n        self.qlib_symbols = sorted(map(lambda x: x.name.lower(), bin_path_list))\n        qlib.init(\n            provider_uri=str(self.qlib_dir.resolve()),\n            mount_path=str(self.qlib_dir.resolve()),\n            auto_mount=False,\n            redis_port=-1,\n        )\n        csv_path = Path(csv_path).expanduser()\n        self.csv_files = sorted(csv_path.glob(f\"*{file_suffix}\") if csv_path.is_dir() else [csv_path])\n\n        if check_fields is None:\n            check_fields = list(map(lambda x: x.name.split(\".\")[0], bin_path_list[0].glob(f\"*.bin\")))\n        else:\n            check_fields = check_fields.split(\",\") if isinstance(check_fields, str) else check_fields\n        self.check_fields = list(map(lambda x: x.strip(), check_fields))\n        self.qlib_fields = list(map(lambda x: f\"${x}\", self.check_fields))\n        self.max_workers = max_workers\n        self.symbol_field_name = symbol_field_name\n        self.date_field_name = date_field_name\n        self.freq = freq\n        self.file_suffix = file_suffix",
  "def _compare(self, file_path: Path):\n        symbol = file_path.name.strip(self.file_suffix)\n        if symbol.lower() not in self.qlib_symbols:\n            return self.NOT_IN_FEATURES\n        # qlib data\n        qlib_df = D.features([symbol], self.qlib_fields, freq=self.freq)\n        qlib_df.rename(columns={_c: _c.strip(\"$\") for _c in qlib_df.columns}, inplace=True)\n        # csv data\n        origin_df = pd.read_csv(file_path)\n        origin_df[self.date_field_name] = pd.to_datetime(origin_df[self.date_field_name])\n        if self.symbol_field_name not in origin_df.columns:\n            origin_df[self.symbol_field_name] = symbol\n        origin_df.set_index([self.symbol_field_name, self.date_field_name], inplace=True)\n        origin_df.index.names = qlib_df.index.names\n        origin_df = origin_df.reindex(qlib_df.index)\n        try:\n            compare = datacompy.Compare(\n                origin_df,\n                qlib_df,\n                on_index=True,\n                abs_tol=1e-08,  # Optional, defaults to 0\n                rel_tol=1e-05,  # Optional, defaults to 0\n                df1_name=\"Original\",  # Optional, defaults to 'df1'\n                df2_name=\"New\",  # Optional, defaults to 'df2'\n            )\n            _r = compare.matches(ignore_extra_columns=True)\n            return self.COMPARE_TRUE if _r else self.COMPARE_FALSE\n        except Exception as e:\n            logger.warning(f\"{symbol} compare error: {e}\")\n            return self.COMPARE_ERROR",
  "def check(self):\n        \"\"\"Check whether the bin file after ``dump_bin.py`` is executed is consistent with the original csv file data\"\"\"\n        logger.info(\"start check......\")\n\n        error_list = []\n        not_in_features = []\n        compare_false = []\n        with tqdm(total=len(self.csv_files)) as p_bar:\n            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n                for file_path, _check_res in zip(self.csv_files, executor.map(self._compare, self.csv_files)):\n                    symbol = file_path.name.strip(self.file_suffix)\n                    if _check_res == self.NOT_IN_FEATURES:\n                        not_in_features.append(symbol)\n                    elif _check_res == self.COMPARE_ERROR:\n                        error_list.append(symbol)\n                    elif _check_res == self.COMPARE_FALSE:\n                        compare_false.append(symbol)\n                    p_bar.update()\n\n        logger.info(\"end of check......\")\n        if error_list:\n            logger.warning(f\"compare error: {error_list}\")\n        if not_in_features:\n            logger.warning(f\"not in features: {not_in_features}\")\n        if compare_false:\n            logger.warning(f\"compare False: {compare_false}\")\n        logger.info(\n            f\"total {len(self.csv_files)}, {len(error_list)} errors, {len(not_in_features)} not in features, {len(compare_false)} compare false\"\n        )",
  "def get_calendar_list(bench_code=\"CSI300\") -> list:\n    \"\"\"get SH/SZ history calendar list\n\n    Parameters\n    ----------\n    bench_code: str\n        value from [\"CSI300\", \"CSI500\", \"ALL\", \"US_ALL\"]\n\n    Returns\n    -------\n        history calendar list\n    \"\"\"\n\n    logger.info(f\"get calendar list: {bench_code}......\")\n\n    def _get_calendar(url):\n        _value_list = requests.get(url).json()[\"data\"][\"klines\"]\n        return sorted(map(lambda x: pd.Timestamp(x.split(\",\")[0]), _value_list))\n\n    calendar = _CALENDAR_MAP.get(bench_code, None)\n    if calendar is None:\n        if bench_code.startswith(\"US_\"):\n            df = Ticker(CALENDAR_BENCH_URL_MAP[bench_code]).history(interval=\"1d\", period=\"max\")\n            calendar = df.index.get_level_values(level=\"date\").map(pd.Timestamp).unique().tolist()\n        else:\n            if bench_code.upper() == \"ALL\":\n\n                @deco_retry\n                def _get_calendar(month):\n                    _cal = []\n                    try:\n                        resp = requests.get(SZSE_CALENDAR_URL.format(month=month, random=random.random)).json()\n                        for _r in resp[\"data\"]:\n                            if int(_r[\"jybz\"]):\n                                _cal.append(pd.Timestamp(_r[\"jyrq\"]))\n                    except Exception as e:\n                        raise ValueError(f\"{month}-->{e}\")\n                    return _cal\n\n                month_range = pd.date_range(start=\"2000-01\", end=pd.Timestamp.now() + pd.Timedelta(days=31), freq=\"M\")\n                calendar = []\n                for _m in month_range:\n                    cal = _get_calendar(_m.strftime(\"%Y-%m\"))\n                    if cal:\n                        calendar += cal\n                calendar = list(filter(lambda x: x <= pd.Timestamp.now(), calendar))\n            else:\n                calendar = _get_calendar(CALENDAR_BENCH_URL_MAP[bench_code])\n        _CALENDAR_MAP[bench_code] = calendar\n    logger.info(f\"end of get calendar list: {bench_code}.\")\n    return calendar",
  "def return_date_list(date_field_name: str, file_path: Path):\n    date_list = pd.read_csv(file_path, sep=\",\", index_col=0)[date_field_name].to_list()\n    return sorted(map(lambda x: pd.Timestamp(x), date_list))",
  "def get_calendar_list_by_ratio(\n    source_dir: [str, Path],\n    date_field_name: str = \"date\",\n    threshold: float = 0.5,\n    minimum_count: int = 10,\n    max_workers: int = 16,\n) -> list:\n    \"\"\"get calendar list by selecting the date when few funds trade in this day\n\n    Parameters\n    ----------\n    source_dir: str or Path\n        The directory where the raw data collected from the Internet is saved\n    date_field_name: str\n            date field name, default is date\n    threshold: float\n        threshold to exclude some days when few funds trade in this day, default 0.5\n    minimum_count: int\n        minimum count of funds should trade in one day\n    max_workers: int\n        Concurrent number, default is 16\n\n    Returns\n    -------\n        history calendar list\n    \"\"\"\n    logger.info(f\"get calendar list from {source_dir} by threshold = {threshold}......\")\n\n    source_dir = Path(source_dir).expanduser()\n    file_list = list(source_dir.glob(\"*.csv\"))\n\n    _number_all_funds = len(file_list)\n\n    logger.info(f\"count how many funds trade in this day......\")\n    _dict_count_trade = dict()  # dict{date:count}\n    _fun = partial(return_date_list, date_field_name)\n    all_oldest_list = []\n    with tqdm(total=_number_all_funds) as p_bar:\n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            for date_list in executor.map(_fun, file_list):\n                if date_list:\n                    all_oldest_list.append(date_list[0])\n                for date in date_list:\n                    if date not in _dict_count_trade.keys():\n                        _dict_count_trade[date] = 0\n\n                    _dict_count_trade[date] += 1\n\n                p_bar.update()\n\n    logger.info(f\"count how many funds have founded in this day......\")\n    _dict_count_founding = {date: _number_all_funds for date in _dict_count_trade.keys()}  # dict{date:count}\n    with tqdm(total=_number_all_funds) as p_bar:\n        for oldest_date in all_oldest_list:\n            for date in _dict_count_founding.keys():\n                if date < oldest_date:\n                    _dict_count_founding[date] -= 1\n\n    calendar = [\n        date\n        for date in _dict_count_trade\n        if _dict_count_trade[date] >= max(int(_dict_count_founding[date] * threshold), minimum_count)\n    ]\n\n    return calendar",
  "def get_hs_stock_symbols() -> list:\n    \"\"\"get SH/SZ stock symbols\n\n    Returns\n    -------\n        stock symbols\n    \"\"\"\n    global _HS_SYMBOLS\n\n    def _get_symbol():\n        _res = set()\n        for _k, _v in ((\"ha\", \"ss\"), (\"sa\", \"sz\"), (\"gem\", \"sz\")):\n            resp = requests.get(HS_SYMBOLS_URL.format(s_type=_k))\n            _res |= set(\n                map(\n                    lambda x: \"{}.{}\".format(re.findall(r\"\\d+\", x)[0], _v),\n                    etree.HTML(resp.text).xpath(\"//div[@class='result']/ul//li/a/text()\"),\n                )\n            )\n            time.sleep(3)\n        return _res\n\n    if _HS_SYMBOLS is None:\n        symbols = set()\n        _retry = 60\n        # It may take multiple times to get the complete\n        while len(symbols) < MINIMUM_SYMBOLS_NUM:\n            symbols |= _get_symbol()\n            time.sleep(3)\n\n        symbol_cache_path = Path(\"~/.cache/hs_symbols_cache.pkl\").expanduser().resolve()\n        symbol_cache_path.parent.mkdir(parents=True, exist_ok=True)\n        if symbol_cache_path.exists():\n            with symbol_cache_path.open(\"rb\") as fp:\n                cache_symbols = pickle.load(fp)\n                symbols |= cache_symbols\n        with symbol_cache_path.open(\"wb\") as fp:\n            pickle.dump(symbols, fp)\n\n        _HS_SYMBOLS = sorted(list(symbols))\n\n    return _HS_SYMBOLS",
  "def get_us_stock_symbols(qlib_data_path: [str, Path] = None) -> list:\n    \"\"\"get US stock symbols\n\n    Returns\n    -------\n        stock symbols\n    \"\"\"\n    global _US_SYMBOLS\n\n    @deco_retry\n    def _get_eastmoney():\n        url = \"http://4.push2.eastmoney.com/api/qt/clist/get?pn=1&pz=10000&fs=m:105,m:106,m:107&fields=f12\"\n        resp = requests.get(url)\n        if resp.status_code != 200:\n            raise ValueError(\"request error\")\n        try:\n            _symbols = [_v[\"f12\"].replace(\"_\", \"-P\") for _v in resp.json()[\"data\"][\"diff\"].values()]\n        except Exception as e:\n            logger.warning(f\"request error: {e}\")\n            raise\n        if len(_symbols) < 8000:\n            raise ValueError(\"request error\")\n        return _symbols\n\n    @deco_retry\n    def _get_nasdaq():\n        _res_symbols = []\n        for _name in [\"otherlisted\", \"nasdaqtraded\"]:\n            url = f\"ftp://ftp.nasdaqtrader.com/SymbolDirectory/{_name}.txt\"\n            df = pd.read_csv(url, sep=\"|\")\n            df = df.rename(columns={\"ACT Symbol\": \"Symbol\"})\n            _symbols = df[\"Symbol\"].dropna()\n            _symbols = _symbols.str.replace(\"$\", \"-P\", regex=False)\n            _symbols = _symbols.str.replace(\".W\", \"-WT\", regex=False)\n            _symbols = _symbols.str.replace(\".U\", \"-UN\", regex=False)\n            _symbols = _symbols.str.replace(\".R\", \"-RI\", regex=False)\n            _symbols = _symbols.str.replace(\".\", \"-\", regex=False)\n            _res_symbols += _symbols.unique().tolist()\n        return _res_symbols\n\n    @deco_retry\n    def _get_nyse():\n        url = \"https://www.nyse.com/api/quotes/filter\"\n        _parms = {\n            \"instrumentType\": \"EQUITY\",\n            \"pageNumber\": 1,\n            \"sortColumn\": \"NORMALIZED_TICKER\",\n            \"sortOrder\": \"ASC\",\n            \"maxResultsPerPage\": 10000,\n            \"filterToken\": \"\",\n        }\n        resp = requests.post(url, json=_parms)\n        if resp.status_code != 200:\n            raise ValueError(\"request error\")\n        try:\n            _symbols = [_v[\"symbolTicker\"].replace(\"-\", \"-P\") for _v in resp.json()]\n        except Exception as e:\n            logger.warning(f\"request error: {e}\")\n            _symbols = []\n        return _symbols\n\n    if _US_SYMBOLS is None:\n        _all_symbols = _get_eastmoney() + _get_nasdaq() + _get_nyse()\n        if qlib_data_path is not None:\n            for _index in [\"nasdaq100\", \"sp500\"]:\n                ins_df = pd.read_csv(\n                    Path(qlib_data_path).joinpath(f\"instruments/{_index}.txt\"),\n                    sep=\"\\t\",\n                    names=[\"symbol\", \"start_date\", \"end_date\"],\n                )\n                _all_symbols += ins_df[\"symbol\"].unique().tolist()\n\n        def _format(s_):\n            s_ = s_.replace(\".\", \"-\")\n            s_ = s_.strip(\"$\")\n            s_ = s_.strip(\"*\")\n            return s_\n\n        _US_SYMBOLS = sorted(set(map(_format, filter(lambda x: len(x) < 8 and not x.endswith(\"WS\"), _all_symbols))))\n\n    return _US_SYMBOLS",
  "def get_en_fund_symbols(qlib_data_path: [str, Path] = None) -> list:\n    \"\"\"get en fund symbols\n\n    Returns\n    -------\n        fund symbols in China\n    \"\"\"\n    global _EN_FUND_SYMBOLS\n\n    @deco_retry\n    def _get_eastmoney():\n        url = \"http://fund.eastmoney.com/js/fundcode_search.js\"\n        resp = requests.get(url)\n        if resp.status_code != 200:\n            raise ValueError(\"request error\")\n        try:\n            _symbols = []\n            for sub_data in re.findall(r\"[\\[](.*?)[\\]]\", resp.content.decode().split(\"= [\")[-1].replace(\"];\", \"\")):\n                data = sub_data.replace('\"', \"\").replace(\"'\", \"\")\n                # TODO: do we need other informations, like fund_name from ['000001', 'HXCZHH', '\u534e\u590f\u6210\u957f\u6df7\u5408', '\u6df7\u5408\u578b', 'HUAXIACHENGZHANGHUNHE']\n                _symbols.append(data.split(\",\")[0])\n        except Exception as e:\n            logger.warning(f\"request error: {e}\")\n            raise\n        if len(_symbols) < 8000:\n            raise ValueError(\"request error\")\n        return _symbols\n\n    if _EN_FUND_SYMBOLS is None:\n        _all_symbols = _get_eastmoney()\n\n        _EN_FUND_SYMBOLS = sorted(set(_all_symbols))\n\n    return _EN_FUND_SYMBOLS",
  "def symbol_suffix_to_prefix(symbol: str, capital: bool = True) -> str:\n    \"\"\"symbol suffix to prefix\n\n    Parameters\n    ----------\n    symbol: str\n        symbol\n    capital : bool\n        by default True\n    Returns\n    -------\n\n    \"\"\"\n    code, exchange = symbol.split(\".\")\n    if exchange.lower() in [\"sh\", \"ss\"]:\n        res = f\"sh{code}\"\n    else:\n        res = f\"{exchange}{code}\"\n    return res.upper() if capital else res.lower()",
  "def symbol_prefix_to_sufix(symbol: str, capital: bool = True) -> str:\n    \"\"\"symbol prefix to sufix\n\n    Parameters\n    ----------\n    symbol: str\n        symbol\n    capital : bool\n        by default True\n    Returns\n    -------\n\n    \"\"\"\n    res = f\"{symbol[:-2]}.{symbol[-2:]}\"\n    return res.upper() if capital else res.lower()",
  "def deco_retry(retry: int = 5, retry_sleep: int = 3):\n    def deco_func(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            _retry = 5 if callable(retry) else retry\n            _result = None\n            for _i in range(1, _retry + 1):\n                try:\n                    _result = func(*args, **kwargs)\n                    break\n                except Exception as e:\n                    logger.warning(f\"{func.__name__}: {_i} :{e}\")\n                    if _i == _retry:\n                        raise\n                time.sleep(retry_sleep)\n            return _result\n\n        return wrapper\n\n    return deco_func(retry) if callable(retry) else deco_func",
  "def get_trading_date_by_shift(trading_list: list, trading_date: pd.Timestamp, shift: int = 1):\n    \"\"\"get trading date by shift\n\n    Parameters\n    ----------\n    trading_list: list\n        trading calendar list\n    shift : int\n        shift, default is 1\n\n    trading_date : pd.Timestamp\n        trading date\n    Returns\n    -------\n\n    \"\"\"\n    trading_date = pd.Timestamp(trading_date)\n    left_index = bisect.bisect_left(trading_list, trading_date)\n    try:\n        res = trading_list[left_index + shift]\n    except IndexError:\n        res = trading_date\n    return res",
  "def _get_calendar(url):\n        _value_list = requests.get(url).json()[\"data\"][\"klines\"]\n        return sorted(map(lambda x: pd.Timestamp(x.split(\",\")[0]), _value_list))",
  "def _get_symbol():\n        _res = set()\n        for _k, _v in ((\"ha\", \"ss\"), (\"sa\", \"sz\"), (\"gem\", \"sz\")):\n            resp = requests.get(HS_SYMBOLS_URL.format(s_type=_k))\n            _res |= set(\n                map(\n                    lambda x: \"{}.{}\".format(re.findall(r\"\\d+\", x)[0], _v),\n                    etree.HTML(resp.text).xpath(\"//div[@class='result']/ul//li/a/text()\"),\n                )\n            )\n            time.sleep(3)\n        return _res",
  "def _get_eastmoney():\n        url = \"http://4.push2.eastmoney.com/api/qt/clist/get?pn=1&pz=10000&fs=m:105,m:106,m:107&fields=f12\"\n        resp = requests.get(url)\n        if resp.status_code != 200:\n            raise ValueError(\"request error\")\n        try:\n            _symbols = [_v[\"f12\"].replace(\"_\", \"-P\") for _v in resp.json()[\"data\"][\"diff\"].values()]\n        except Exception as e:\n            logger.warning(f\"request error: {e}\")\n            raise\n        if len(_symbols) < 8000:\n            raise ValueError(\"request error\")\n        return _symbols",
  "def _get_nasdaq():\n        _res_symbols = []\n        for _name in [\"otherlisted\", \"nasdaqtraded\"]:\n            url = f\"ftp://ftp.nasdaqtrader.com/SymbolDirectory/{_name}.txt\"\n            df = pd.read_csv(url, sep=\"|\")\n            df = df.rename(columns={\"ACT Symbol\": \"Symbol\"})\n            _symbols = df[\"Symbol\"].dropna()\n            _symbols = _symbols.str.replace(\"$\", \"-P\", regex=False)\n            _symbols = _symbols.str.replace(\".W\", \"-WT\", regex=False)\n            _symbols = _symbols.str.replace(\".U\", \"-UN\", regex=False)\n            _symbols = _symbols.str.replace(\".R\", \"-RI\", regex=False)\n            _symbols = _symbols.str.replace(\".\", \"-\", regex=False)\n            _res_symbols += _symbols.unique().tolist()\n        return _res_symbols",
  "def _get_nyse():\n        url = \"https://www.nyse.com/api/quotes/filter\"\n        _parms = {\n            \"instrumentType\": \"EQUITY\",\n            \"pageNumber\": 1,\n            \"sortColumn\": \"NORMALIZED_TICKER\",\n            \"sortOrder\": \"ASC\",\n            \"maxResultsPerPage\": 10000,\n            \"filterToken\": \"\",\n        }\n        resp = requests.post(url, json=_parms)\n        if resp.status_code != 200:\n            raise ValueError(\"request error\")\n        try:\n            _symbols = [_v[\"symbolTicker\"].replace(\"-\", \"-P\") for _v in resp.json()]\n        except Exception as e:\n            logger.warning(f\"request error: {e}\")\n            _symbols = []\n        return _symbols",
  "def _get_eastmoney():\n        url = \"http://fund.eastmoney.com/js/fundcode_search.js\"\n        resp = requests.get(url)\n        if resp.status_code != 200:\n            raise ValueError(\"request error\")\n        try:\n            _symbols = []\n            for sub_data in re.findall(r\"[\\[](.*?)[\\]]\", resp.content.decode().split(\"= [\")[-1].replace(\"];\", \"\")):\n                data = sub_data.replace('\"', \"\").replace(\"'\", \"\")\n                # TODO: do we need other informations, like fund_name from ['000001', 'HXCZHH', '\u534e\u590f\u6210\u957f\u6df7\u5408', '\u6df7\u5408\u578b', 'HUAXIACHENGZHANGHUNHE']\n                _symbols.append(data.split(\",\")[0])\n        except Exception as e:\n            logger.warning(f\"request error: {e}\")\n            raise\n        if len(_symbols) < 8000:\n            raise ValueError(\"request error\")\n        return _symbols",
  "def deco_func(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            _retry = 5 if callable(retry) else retry\n            _result = None\n            for _i in range(1, _retry + 1):\n                try:\n                    _result = func(*args, **kwargs)\n                    break\n                except Exception as e:\n                    logger.warning(f\"{func.__name__}: {_i} :{e}\")\n                    if _i == _retry:\n                        raise\n                time.sleep(retry_sleep)\n            return _result\n\n        return wrapper",
  "def _format(s_):\n            s_ = s_.replace(\".\", \"-\")\n            s_ = s_.strip(\"$\")\n            s_ = s_.strip(\"*\")\n            return s_",
  "def wrapper(*args, **kwargs):\n            _retry = 5 if callable(retry) else retry\n            _result = None\n            for _i in range(1, _retry + 1):\n                try:\n                    _result = func(*args, **kwargs)\n                    break\n                except Exception as e:\n                    logger.warning(f\"{func.__name__}: {_i} :{e}\")\n                    if _i == _retry:\n                        raise\n                time.sleep(retry_sleep)\n            return _result",
  "def _get_calendar(month):\n                    _cal = []\n                    try:\n                        resp = requests.get(SZSE_CALENDAR_URL.format(month=month, random=random.random)).json()\n                        for _r in resp[\"data\"]:\n                            if int(_r[\"jybz\"]):\n                                _cal.append(pd.Timestamp(_r[\"jyrq\"]))\n                    except Exception as e:\n                        raise ValueError(f\"{month}-->{e}\")\n                    return _cal",
  "class BaseCollector(abc.ABC):\n\n    CACHE_FLAG = \"CACHED\"\n    NORMAL_FLAG = \"NORMAL\"\n\n    DEFAULT_START_DATETIME_1D = pd.Timestamp(\"2000-01-01\")\n    DEFAULT_START_DATETIME_1MIN = pd.Timestamp(datetime.datetime.now() - pd.Timedelta(days=5 * 6))\n    DEFAULT_END_DATETIME_1D = pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))\n    DEFAULT_END_DATETIME_1MIN = pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))\n\n    INTERVAL_1min = \"1min\"\n    INTERVAL_1d = \"1d\"\n\n    def __init__(\n        self,\n        save_dir: [str, Path],\n        start=None,\n        end=None,\n        interval=\"1d\",\n        max_workers=4,\n        max_collector_count=2,\n        delay=0,\n        check_data_length: bool = False,\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        save_dir: str\n            instrument save dir\n        max_workers: int\n            workers, default 4\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        start: str\n            start datetime, default None\n        end: str\n            end datetime, default None\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n        \"\"\"\n        self.save_dir = Path(save_dir).expanduser().resolve()\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n\n        self.delay = delay\n        self.max_workers = max_workers\n        self.max_collector_count = max_collector_count\n        self.mini_symbol_map = {}\n        self.interval = interval\n        self.check_small_data = check_data_length\n\n        self.start_datetime = self.normalize_start_datetime(start)\n        self.end_datetime = self.normalize_end_datetime(end)\n\n        self.instrument_list = sorted(set(self.get_instrument_list()))\n\n        if limit_nums is not None:\n            try:\n                self.instrument_list = self.instrument_list[: int(limit_nums)]\n            except Exception as e:\n                logger.warning(f\"Cannot use limit_nums={limit_nums}, the parameter will be ignored\")\n\n    def normalize_start_datetime(self, start_datetime: [str, pd.Timestamp] = None):\n        return (\n            pd.Timestamp(str(start_datetime))\n            if start_datetime\n            else getattr(self, f\"DEFAULT_START_DATETIME_{self.interval.upper()}\")\n        )\n\n    def normalize_end_datetime(self, end_datetime: [str, pd.Timestamp] = None):\n        return (\n            pd.Timestamp(str(end_datetime))\n            if end_datetime\n            else getattr(self, f\"DEFAULT_END_DATETIME_{self.interval.upper()}\")\n        )\n\n    @property\n    @abc.abstractmethod\n    def min_numbers_trading(self):\n        # daily, one year: 252 / 4\n        # us 1min, a week: 6.5 * 60 * 5\n        # cn 1min, a week: 4 * 60 * 5\n        raise NotImplementedError(\"rewrite min_numbers_trading\")\n\n    @abc.abstractmethod\n    def get_instrument_list(self):\n        raise NotImplementedError(\"rewrite get_instrument_list\")\n\n    @abc.abstractmethod\n    def normalize_symbol(self, symbol: str):\n        \"\"\"normalize symbol\"\"\"\n        raise NotImplementedError(\"rewrite normalize_symbol\")\n\n    @abc.abstractmethod\n    def get_data(\n        self, symbol: str, interval: str, start_datetime: pd.Timestamp, end_datetime: pd.Timestamp\n    ) -> pd.DataFrame:\n        \"\"\"get data with symbol\n\n        Parameters\n        ----------\n        symbol: str\n        interval: str\n            value from [1min, 1d]\n        start_datetime: pd.Timestamp\n        end_datetime: pd.Timestamp\n\n        Returns\n        ---------\n            pd.DataFrame, \"symbol\" in pd.columns\n\n        \"\"\"\n        raise NotImplementedError(\"rewrite get_timezone\")\n\n    def sleep(self):\n        time.sleep(self.delay)\n\n    def _simple_collector(self, symbol: str):\n        \"\"\"\n\n        Parameters\n        ----------\n        symbol: str\n\n        \"\"\"\n        self.sleep()\n        df = self.get_data(symbol, self.interval, self.start_datetime, self.end_datetime)\n        _result = self.NORMAL_FLAG\n        if self.check_small_data:\n            _result = self.cache_small_data(symbol, df)\n        if _result == self.NORMAL_FLAG:\n            self.save_instrument(symbol, df)\n        return _result\n\n    def save_instrument(self, symbol, df: pd.DataFrame):\n        \"\"\"save instrument data to file\n\n        Parameters\n        ----------\n        symbol: str\n            instrument code\n        df : pd.DataFrame\n            df.columns must contain \"symbol\" and \"datetime\"\n        \"\"\"\n        if df is None or df.empty:\n            logger.warning(f\"{symbol} is empty\")\n            return\n\n        symbol = self.normalize_symbol(symbol)\n        symbol = code_to_fname(symbol)\n        instrument_path = self.save_dir.joinpath(f\"{symbol}.csv\")\n        df[\"symbol\"] = symbol\n        if instrument_path.exists():\n            _old_df = pd.read_csv(instrument_path)\n            df = _old_df.append(df, sort=False)\n        df.to_csv(instrument_path, index=False)\n\n    def cache_small_data(self, symbol, df):\n        if len(df) <= self.min_numbers_trading:\n            logger.warning(f\"the number of trading days of {symbol} is less than {self.min_numbers_trading}!\")\n            _temp = self.mini_symbol_map.setdefault(symbol, [])\n            _temp.append(df.copy())\n            return self.CACHE_FLAG\n        else:\n            if symbol in self.mini_symbol_map:\n                self.mini_symbol_map.pop(symbol)\n            return self.NORMAL_FLAG\n\n    def _collector(self, instrument_list):\n\n        error_symbol = []\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            with tqdm(total=len(instrument_list)) as p_bar:\n                for _symbol, _result in zip(instrument_list, executor.map(self._simple_collector, instrument_list)):\n                    if _result != self.NORMAL_FLAG:\n                        error_symbol.append(_symbol)\n                    p_bar.update()\n        print(error_symbol)\n        logger.info(f\"error symbol nums: {len(error_symbol)}\")\n        logger.info(f\"current get symbol nums: {len(instrument_list)}\")\n        error_symbol.extend(self.mini_symbol_map.keys())\n        return sorted(set(error_symbol))\n\n    def collector_data(self):\n        \"\"\"collector data\"\"\"\n        logger.info(\"start collector data......\")\n        instrument_list = self.instrument_list\n        for i in range(self.max_collector_count):\n            if not instrument_list:\n                break\n            logger.info(f\"getting data: {i+1}\")\n            instrument_list = self._collector(instrument_list)\n            logger.info(f\"{i+1} finish.\")\n        for _symbol, _df_list in self.mini_symbol_map.items():\n            self.save_instrument(\n                _symbol, pd.concat(_df_list, sort=False).drop_duplicates([\"date\"]).sort_values([\"date\"])\n            )\n        if self.mini_symbol_map:\n            logger.warning(f\"less than {self.min_numbers_trading} instrument list: {list(self.mini_symbol_map.keys())}\")\n        logger.info(f\"total {len(self.instrument_list)}, error: {len(set(instrument_list))}\")",
  "class BaseNormalize(abc.ABC):\n    def __init__(\n        self,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default is date\n        symbol_field_name: str\n            symbol field name, default is symbol\n        \"\"\"\n        self._date_field_name = date_field_name\n        self._symbol_field_name = symbol_field_name\n\n        self._calendar_list = self._get_calendar_list()\n\n    @abc.abstractmethod\n    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n        # normalize\n        raise NotImplementedError(\"\")\n\n    @abc.abstractmethod\n    def _get_calendar_list(self):\n        \"\"\"Get benchmark calendar\"\"\"\n        raise NotImplementedError(\"\")",
  "class Normalize:\n    def __init__(\n        self,\n        source_dir: [str, Path],\n        target_dir: [str, Path],\n        normalize_class: Type[BaseNormalize],\n        max_workers: int = 16,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        source_dir: str or Path\n            The directory where the raw data collected from the Internet is saved\n        target_dir: str or Path\n            Directory for normalize data\n        normalize_class: Type[YahooNormalize]\n            normalize class\n        max_workers: int\n            Concurrent number, default is 16\n        date_field_name: str\n            date field name, default is date\n        symbol_field_name: str\n            symbol field name, default is symbol\n        \"\"\"\n        if not (source_dir and target_dir):\n            raise ValueError(\"source_dir and target_dir cannot be None\")\n        self._source_dir = Path(source_dir).expanduser()\n        self._target_dir = Path(target_dir).expanduser()\n        self._target_dir.mkdir(parents=True, exist_ok=True)\n\n        self._max_workers = max_workers\n\n        self._normalize_obj = normalize_class(date_field_name=date_field_name, symbol_field_name=symbol_field_name)\n\n    def _executor(self, file_path: Path):\n        file_path = Path(file_path)\n        df = pd.read_csv(file_path)\n        df = self._normalize_obj.normalize(df)\n        if not df.empty:\n            df.to_csv(self._target_dir.joinpath(file_path.name), index=False)\n\n    def normalize(self):\n        logger.info(\"normalize data......\")\n\n        with ProcessPoolExecutor(max_workers=self._max_workers) as worker:\n            file_list = list(self._source_dir.glob(\"*.csv\"))\n            with tqdm(total=len(file_list)) as p_bar:\n                for _ in worker.map(self._executor, file_list):\n                    p_bar.update()",
  "class BaseRun(abc.ABC):\n    def __init__(self, source_dir=None, normalize_dir=None, max_workers=4, interval=\"1d\"):\n        \"\"\"\n\n        Parameters\n        ----------\n        source_dir: str\n            The directory where the raw data collected from the Internet is saved, default \"Path(__file__).parent/source\"\n        normalize_dir: str\n            Directory for normalize data, default \"Path(__file__).parent/normalize\"\n        max_workers: int\n            Concurrent number, default is 4\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        \"\"\"\n        if source_dir is None:\n            source_dir = Path(self.default_base_dir).joinpath(\"_source\")\n        self.source_dir = Path(source_dir).expanduser().resolve()\n        self.source_dir.mkdir(parents=True, exist_ok=True)\n\n        if normalize_dir is None:\n            normalize_dir = Path(self.default_base_dir).joinpath(\"normalize\")\n        self.normalize_dir = Path(normalize_dir).expanduser().resolve()\n        self.normalize_dir.mkdir(parents=True, exist_ok=True)\n\n        self._cur_module = importlib.import_module(\"collector\")\n        self.max_workers = max_workers\n        self.interval = interval\n\n    @property\n    @abc.abstractmethod\n    def collector_class_name(self):\n        raise NotImplementedError(\"rewrite normalize_symbol\")\n\n    @property\n    @abc.abstractmethod\n    def normalize_class_name(self):\n        raise NotImplementedError(\"rewrite normalize_symbol\")\n\n    @property\n    @abc.abstractmethod\n    def default_base_dir(self) -> [Path, str]:\n        raise NotImplementedError(\"rewrite normalize_symbol\")\n\n    def download_data(\n        self,\n        max_collector_count=2,\n        delay=0,\n        start=None,\n        end=None,\n        interval=\"1d\",\n        check_data_length=False,\n        limit_nums=None,\n    ):\n        \"\"\"download data from Internet\n\n        Parameters\n        ----------\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        start: str\n            start datetime, default \"2000-01-01\"\n        end: str\n            end datetime, default ``pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))``\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n\n        Examples\n        ---------\n            # get daily data\n            $ python collector.py download_data --source_dir ~/.qlib/instrument_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n            # get 1m data\n            $ python collector.py download_data --source_dir ~/.qlib/instrument_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1m\n        \"\"\"\n\n        _class = getattr(self._cur_module, self.collector_class_name)  # type: Type[BaseCollector]\n        _class(\n            self.source_dir,\n            max_workers=self.max_workers,\n            max_collector_count=max_collector_count,\n            delay=delay,\n            start=start,\n            end=end,\n            interval=interval,\n            check_data_length=check_data_length,\n            limit_nums=limit_nums,\n        ).collector_data()\n\n    def normalize_data(self, date_field_name: str = \"date\", symbol_field_name: str = \"symbol\"):\n        \"\"\"normalize data\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default date\n        symbol_field_name: str\n            symbol field name, default symbol\n\n        Examples\n        ---------\n            $ python collector.py normalize_data --source_dir ~/.qlib/instrument_data/source --normalize_dir ~/.qlib/instrument_data/normalize --region CN --interval 1d\n        \"\"\"\n        _class = getattr(self._cur_module, self.normalize_class_name)\n        yc = Normalize(\n            source_dir=self.source_dir,\n            target_dir=self.normalize_dir,\n            normalize_class=_class,\n            max_workers=self.max_workers,\n            date_field_name=date_field_name,\n            symbol_field_name=symbol_field_name,\n        )\n        yc.normalize()",
  "def __init__(\n        self,\n        save_dir: [str, Path],\n        start=None,\n        end=None,\n        interval=\"1d\",\n        max_workers=4,\n        max_collector_count=2,\n        delay=0,\n        check_data_length: bool = False,\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        save_dir: str\n            instrument save dir\n        max_workers: int\n            workers, default 4\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        start: str\n            start datetime, default None\n        end: str\n            end datetime, default None\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n        \"\"\"\n        self.save_dir = Path(save_dir).expanduser().resolve()\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n\n        self.delay = delay\n        self.max_workers = max_workers\n        self.max_collector_count = max_collector_count\n        self.mini_symbol_map = {}\n        self.interval = interval\n        self.check_small_data = check_data_length\n\n        self.start_datetime = self.normalize_start_datetime(start)\n        self.end_datetime = self.normalize_end_datetime(end)\n\n        self.instrument_list = sorted(set(self.get_instrument_list()))\n\n        if limit_nums is not None:\n            try:\n                self.instrument_list = self.instrument_list[: int(limit_nums)]\n            except Exception as e:\n                logger.warning(f\"Cannot use limit_nums={limit_nums}, the parameter will be ignored\")",
  "def normalize_start_datetime(self, start_datetime: [str, pd.Timestamp] = None):\n        return (\n            pd.Timestamp(str(start_datetime))\n            if start_datetime\n            else getattr(self, f\"DEFAULT_START_DATETIME_{self.interval.upper()}\")\n        )",
  "def normalize_end_datetime(self, end_datetime: [str, pd.Timestamp] = None):\n        return (\n            pd.Timestamp(str(end_datetime))\n            if end_datetime\n            else getattr(self, f\"DEFAULT_END_DATETIME_{self.interval.upper()}\")\n        )",
  "def min_numbers_trading(self):\n        # daily, one year: 252 / 4\n        # us 1min, a week: 6.5 * 60 * 5\n        # cn 1min, a week: 4 * 60 * 5\n        raise NotImplementedError(\"rewrite min_numbers_trading\")",
  "def get_instrument_list(self):\n        raise NotImplementedError(\"rewrite get_instrument_list\")",
  "def normalize_symbol(self, symbol: str):\n        \"\"\"normalize symbol\"\"\"\n        raise NotImplementedError(\"rewrite normalize_symbol\")",
  "def get_data(\n        self, symbol: str, interval: str, start_datetime: pd.Timestamp, end_datetime: pd.Timestamp\n    ) -> pd.DataFrame:\n        \"\"\"get data with symbol\n\n        Parameters\n        ----------\n        symbol: str\n        interval: str\n            value from [1min, 1d]\n        start_datetime: pd.Timestamp\n        end_datetime: pd.Timestamp\n\n        Returns\n        ---------\n            pd.DataFrame, \"symbol\" in pd.columns\n\n        \"\"\"\n        raise NotImplementedError(\"rewrite get_timezone\")",
  "def sleep(self):\n        time.sleep(self.delay)",
  "def _simple_collector(self, symbol: str):\n        \"\"\"\n\n        Parameters\n        ----------\n        symbol: str\n\n        \"\"\"\n        self.sleep()\n        df = self.get_data(symbol, self.interval, self.start_datetime, self.end_datetime)\n        _result = self.NORMAL_FLAG\n        if self.check_small_data:\n            _result = self.cache_small_data(symbol, df)\n        if _result == self.NORMAL_FLAG:\n            self.save_instrument(symbol, df)\n        return _result",
  "def save_instrument(self, symbol, df: pd.DataFrame):\n        \"\"\"save instrument data to file\n\n        Parameters\n        ----------\n        symbol: str\n            instrument code\n        df : pd.DataFrame\n            df.columns must contain \"symbol\" and \"datetime\"\n        \"\"\"\n        if df is None or df.empty:\n            logger.warning(f\"{symbol} is empty\")\n            return\n\n        symbol = self.normalize_symbol(symbol)\n        symbol = code_to_fname(symbol)\n        instrument_path = self.save_dir.joinpath(f\"{symbol}.csv\")\n        df[\"symbol\"] = symbol\n        if instrument_path.exists():\n            _old_df = pd.read_csv(instrument_path)\n            df = _old_df.append(df, sort=False)\n        df.to_csv(instrument_path, index=False)",
  "def cache_small_data(self, symbol, df):\n        if len(df) <= self.min_numbers_trading:\n            logger.warning(f\"the number of trading days of {symbol} is less than {self.min_numbers_trading}!\")\n            _temp = self.mini_symbol_map.setdefault(symbol, [])\n            _temp.append(df.copy())\n            return self.CACHE_FLAG\n        else:\n            if symbol in self.mini_symbol_map:\n                self.mini_symbol_map.pop(symbol)\n            return self.NORMAL_FLAG",
  "def _collector(self, instrument_list):\n\n        error_symbol = []\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            with tqdm(total=len(instrument_list)) as p_bar:\n                for _symbol, _result in zip(instrument_list, executor.map(self._simple_collector, instrument_list)):\n                    if _result != self.NORMAL_FLAG:\n                        error_symbol.append(_symbol)\n                    p_bar.update()\n        print(error_symbol)\n        logger.info(f\"error symbol nums: {len(error_symbol)}\")\n        logger.info(f\"current get symbol nums: {len(instrument_list)}\")\n        error_symbol.extend(self.mini_symbol_map.keys())\n        return sorted(set(error_symbol))",
  "def collector_data(self):\n        \"\"\"collector data\"\"\"\n        logger.info(\"start collector data......\")\n        instrument_list = self.instrument_list\n        for i in range(self.max_collector_count):\n            if not instrument_list:\n                break\n            logger.info(f\"getting data: {i+1}\")\n            instrument_list = self._collector(instrument_list)\n            logger.info(f\"{i+1} finish.\")\n        for _symbol, _df_list in self.mini_symbol_map.items():\n            self.save_instrument(\n                _symbol, pd.concat(_df_list, sort=False).drop_duplicates([\"date\"]).sort_values([\"date\"])\n            )\n        if self.mini_symbol_map:\n            logger.warning(f\"less than {self.min_numbers_trading} instrument list: {list(self.mini_symbol_map.keys())}\")\n        logger.info(f\"total {len(self.instrument_list)}, error: {len(set(instrument_list))}\")",
  "def __init__(\n        self,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default is date\n        symbol_field_name: str\n            symbol field name, default is symbol\n        \"\"\"\n        self._date_field_name = date_field_name\n        self._symbol_field_name = symbol_field_name\n\n        self._calendar_list = self._get_calendar_list()",
  "def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n        # normalize\n        raise NotImplementedError(\"\")",
  "def _get_calendar_list(self):\n        \"\"\"Get benchmark calendar\"\"\"\n        raise NotImplementedError(\"\")",
  "def __init__(\n        self,\n        source_dir: [str, Path],\n        target_dir: [str, Path],\n        normalize_class: Type[BaseNormalize],\n        max_workers: int = 16,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        source_dir: str or Path\n            The directory where the raw data collected from the Internet is saved\n        target_dir: str or Path\n            Directory for normalize data\n        normalize_class: Type[YahooNormalize]\n            normalize class\n        max_workers: int\n            Concurrent number, default is 16\n        date_field_name: str\n            date field name, default is date\n        symbol_field_name: str\n            symbol field name, default is symbol\n        \"\"\"\n        if not (source_dir and target_dir):\n            raise ValueError(\"source_dir and target_dir cannot be None\")\n        self._source_dir = Path(source_dir).expanduser()\n        self._target_dir = Path(target_dir).expanduser()\n        self._target_dir.mkdir(parents=True, exist_ok=True)\n\n        self._max_workers = max_workers\n\n        self._normalize_obj = normalize_class(date_field_name=date_field_name, symbol_field_name=symbol_field_name)",
  "def _executor(self, file_path: Path):\n        file_path = Path(file_path)\n        df = pd.read_csv(file_path)\n        df = self._normalize_obj.normalize(df)\n        if not df.empty:\n            df.to_csv(self._target_dir.joinpath(file_path.name), index=False)",
  "def normalize(self):\n        logger.info(\"normalize data......\")\n\n        with ProcessPoolExecutor(max_workers=self._max_workers) as worker:\n            file_list = list(self._source_dir.glob(\"*.csv\"))\n            with tqdm(total=len(file_list)) as p_bar:\n                for _ in worker.map(self._executor, file_list):\n                    p_bar.update()",
  "def __init__(self, source_dir=None, normalize_dir=None, max_workers=4, interval=\"1d\"):\n        \"\"\"\n\n        Parameters\n        ----------\n        source_dir: str\n            The directory where the raw data collected from the Internet is saved, default \"Path(__file__).parent/source\"\n        normalize_dir: str\n            Directory for normalize data, default \"Path(__file__).parent/normalize\"\n        max_workers: int\n            Concurrent number, default is 4\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        \"\"\"\n        if source_dir is None:\n            source_dir = Path(self.default_base_dir).joinpath(\"_source\")\n        self.source_dir = Path(source_dir).expanduser().resolve()\n        self.source_dir.mkdir(parents=True, exist_ok=True)\n\n        if normalize_dir is None:\n            normalize_dir = Path(self.default_base_dir).joinpath(\"normalize\")\n        self.normalize_dir = Path(normalize_dir).expanduser().resolve()\n        self.normalize_dir.mkdir(parents=True, exist_ok=True)\n\n        self._cur_module = importlib.import_module(\"collector\")\n        self.max_workers = max_workers\n        self.interval = interval",
  "def collector_class_name(self):\n        raise NotImplementedError(\"rewrite normalize_symbol\")",
  "def normalize_class_name(self):\n        raise NotImplementedError(\"rewrite normalize_symbol\")",
  "def default_base_dir(self) -> [Path, str]:\n        raise NotImplementedError(\"rewrite normalize_symbol\")",
  "def download_data(\n        self,\n        max_collector_count=2,\n        delay=0,\n        start=None,\n        end=None,\n        interval=\"1d\",\n        check_data_length=False,\n        limit_nums=None,\n    ):\n        \"\"\"download data from Internet\n\n        Parameters\n        ----------\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        start: str\n            start datetime, default \"2000-01-01\"\n        end: str\n            end datetime, default ``pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))``\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n\n        Examples\n        ---------\n            # get daily data\n            $ python collector.py download_data --source_dir ~/.qlib/instrument_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n            # get 1m data\n            $ python collector.py download_data --source_dir ~/.qlib/instrument_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1m\n        \"\"\"\n\n        _class = getattr(self._cur_module, self.collector_class_name)  # type: Type[BaseCollector]\n        _class(\n            self.source_dir,\n            max_workers=self.max_workers,\n            max_collector_count=max_collector_count,\n            delay=delay,\n            start=start,\n            end=end,\n            interval=interval,\n            check_data_length=check_data_length,\n            limit_nums=limit_nums,\n        ).collector_data()",
  "def normalize_data(self, date_field_name: str = \"date\", symbol_field_name: str = \"symbol\"):\n        \"\"\"normalize data\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default date\n        symbol_field_name: str\n            symbol field name, default symbol\n\n        Examples\n        ---------\n            $ python collector.py normalize_data --source_dir ~/.qlib/instrument_data/source --normalize_dir ~/.qlib/instrument_data/normalize --region CN --interval 1d\n        \"\"\"\n        _class = getattr(self._cur_module, self.normalize_class_name)\n        yc = Normalize(\n            source_dir=self.source_dir,\n            target_dir=self.normalize_dir,\n            normalize_class=_class,\n            max_workers=self.max_workers,\n            date_field_name=date_field_name,\n            symbol_field_name=symbol_field_name,\n        )\n        yc.normalize()",
  "class IndexBase:\n    DEFAULT_END_DATE = pd.Timestamp(\"2099-12-31\")\n    SYMBOL_FIELD_NAME = \"symbol\"\n    DATE_FIELD_NAME = \"date\"\n    START_DATE_FIELD = \"start_date\"\n    END_DATE_FIELD = \"end_ate\"\n    CHANGE_TYPE_FIELD = \"type\"\n    INSTRUMENTS_COLUMNS = [SYMBOL_FIELD_NAME, START_DATE_FIELD, END_DATE_FIELD]\n    REMOVE = \"remove\"\n    ADD = \"add\"\n    INST_PREFIX = \"\"\n\n    def __init__(self, index_name: str, qlib_dir: [str, Path] = None, request_retry: int = 5, retry_sleep: int = 3):\n        \"\"\"\n\n        Parameters\n        ----------\n        index_name: str\n            index name\n        qlib_dir: str\n            qlib directory, by default Path(__file__).resolve().parent.joinpath(\"qlib_data\")\n        request_retry: int\n            request retry, by default 5\n        retry_sleep: int\n            request sleep, by default 3\n        \"\"\"\n        self.index_name = index_name\n        if qlib_dir is None:\n            qlib_dir = Path(__file__).resolve().parent.joinpath(\"qlib_data\")\n        self.instruments_dir = Path(qlib_dir).expanduser().resolve().joinpath(\"instruments\")\n        self.instruments_dir.mkdir(exist_ok=True, parents=True)\n        self.cache_dir = Path(f\"~/.cache/qlib/index/{self.index_name}\").expanduser().resolve()\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\n        self._request_retry = request_retry\n        self._retry_sleep = retry_sleep\n\n    @property\n    @abc.abstractmethod\n    def bench_start_date(self) -> pd.Timestamp:\n        \"\"\"\n        Returns\n        -------\n            index start date\n        \"\"\"\n        raise NotImplementedError(\"rewrite bench_start_date\")\n\n    @property\n    @abc.abstractmethod\n    def calendar_list(self) -> List[pd.Timestamp]:\n        \"\"\"get history trading date\n\n        Returns\n        -------\n            calendar list\n        \"\"\"\n        raise NotImplementedError(\"rewrite calendar_list\")\n\n    @abc.abstractmethod\n    def get_new_companies(self) -> pd.DataFrame:\n        \"\"\"\n\n        Returns\n        -------\n            pd.DataFrame:\n\n                symbol     start_date    end_date\n                SH600000   2000-01-01    2099-12-31\n\n            dtypes:\n                symbol: str\n                start_date: pd.Timestamp\n                end_date: pd.Timestamp\n        \"\"\"\n        raise NotImplementedError(\"rewrite get_new_companies\")\n\n    @abc.abstractmethod\n    def get_changes(self) -> pd.DataFrame:\n        \"\"\"get companies changes\n\n        Returns\n        -------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n        \"\"\"\n        raise NotImplementedError(\"rewrite get_changes\")\n\n    def save_new_companies(self):\n        \"\"\"save new companies\n\n        Examples\n        -------\n            $ python collector.py save_new_companies --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data\n        \"\"\"\n        df = self.get_new_companies()\n        df = df.drop_duplicates([self.SYMBOL_FIELD_NAME])\n        df.loc[:, self.INSTRUMENTS_COLUMNS].to_csv(\n            self.instruments_dir.joinpath(f\"{self.index_name.lower()}_only_new.txt\"), sep=\"\\t\", index=False, header=None\n        )\n\n    def get_changes_with_history_companies(self, history_companies: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"get changes with history companies\n\n        Parameters\n        ----------\n        history_companies : pd.DataFrame\n            symbol        date\n            SH600000   2020-11-11\n\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n\n        Return\n        --------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n\n        \"\"\"\n        logger.info(\"parse changes from history companies......\")\n        last_code = []\n        result_df_list = []\n        _columns = [self.DATE_FIELD_NAME, self.SYMBOL_FIELD_NAME, self.CHANGE_TYPE_FIELD]\n        for _trading_date in tqdm(sorted(history_companies[self.DATE_FIELD_NAME].unique(), reverse=True)):\n            _currenet_code = history_companies[history_companies[self.DATE_FIELD_NAME] == _trading_date][\n                self.SYMBOL_FIELD_NAME\n            ].tolist()\n            if last_code:\n                add_code = list(set(last_code) - set(_currenet_code))\n                remote_code = list(set(_currenet_code) - set(last_code))\n                for _code in add_code:\n                    result_df_list.append(\n                        pd.DataFrame(\n                            [[get_trading_date_by_shift(self.calendar_list, _trading_date, 1), _code, self.ADD]],\n                            columns=_columns,\n                        )\n                    )\n                for _code in remote_code:\n                    result_df_list.append(\n                        pd.DataFrame(\n                            [[get_trading_date_by_shift(self.calendar_list, _trading_date, 0), _code, self.REMOVE]],\n                            columns=_columns,\n                        )\n                    )\n            last_code = _currenet_code\n        df = pd.concat(result_df_list)\n        logger.info(\"end of parse changes from history companies.\")\n        return df\n\n    def parse_instruments(self):\n        \"\"\"parse instruments, eg: csi300.txt\n\n        Examples\n        -------\n            $ python collector.py parse_instruments --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data\n        \"\"\"\n        logger.info(f\"start parse {self.index_name.lower()} companies.....\")\n        instruments_columns = [self.SYMBOL_FIELD_NAME, self.START_DATE_FIELD, self.END_DATE_FIELD]\n        changers_df = self.get_changes()\n        new_df = self.get_new_companies().copy()\n        logger.info(\"parse history companies by changes......\")\n        for _row in tqdm(changers_df.sort_values(self.DATE_FIELD_NAME, ascending=False).itertuples(index=False)):\n            if _row.type == self.ADD:\n                min_end_date = new_df.loc[new_df[self.SYMBOL_FIELD_NAME] == _row.symbol, self.END_DATE_FIELD].min()\n                new_df.loc[\n                    (new_df[self.END_DATE_FIELD] == min_end_date) & (new_df[self.SYMBOL_FIELD_NAME] == _row.symbol),\n                    self.START_DATE_FIELD,\n                ] = _row.date\n            else:\n                _tmp_df = pd.DataFrame([[_row.symbol, self.bench_start_date, _row.date]], columns=instruments_columns)\n                new_df = new_df.append(_tmp_df, sort=False)\n\n        inst_df = new_df.loc[:, instruments_columns]\n        _inst_prefix = self.INST_PREFIX.strip()\n        if _inst_prefix:\n            inst_df[\"save_inst\"] = inst_df[self.SYMBOL_FIELD_NAME].apply(lambda x: f\"{_inst_prefix}{x}\")\n        inst_df.to_csv(\n            self.instruments_dir.joinpath(f\"{self.index_name.lower()}.txt\"), sep=\"\\t\", index=False, header=None\n        )\n        logger.info(f\"parse {self.index_name.lower()} companies finished.\")",
  "def __init__(self, index_name: str, qlib_dir: [str, Path] = None, request_retry: int = 5, retry_sleep: int = 3):\n        \"\"\"\n\n        Parameters\n        ----------\n        index_name: str\n            index name\n        qlib_dir: str\n            qlib directory, by default Path(__file__).resolve().parent.joinpath(\"qlib_data\")\n        request_retry: int\n            request retry, by default 5\n        retry_sleep: int\n            request sleep, by default 3\n        \"\"\"\n        self.index_name = index_name\n        if qlib_dir is None:\n            qlib_dir = Path(__file__).resolve().parent.joinpath(\"qlib_data\")\n        self.instruments_dir = Path(qlib_dir).expanduser().resolve().joinpath(\"instruments\")\n        self.instruments_dir.mkdir(exist_ok=True, parents=True)\n        self.cache_dir = Path(f\"~/.cache/qlib/index/{self.index_name}\").expanduser().resolve()\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\n        self._request_retry = request_retry\n        self._retry_sleep = retry_sleep",
  "def bench_start_date(self) -> pd.Timestamp:\n        \"\"\"\n        Returns\n        -------\n            index start date\n        \"\"\"\n        raise NotImplementedError(\"rewrite bench_start_date\")",
  "def calendar_list(self) -> List[pd.Timestamp]:\n        \"\"\"get history trading date\n\n        Returns\n        -------\n            calendar list\n        \"\"\"\n        raise NotImplementedError(\"rewrite calendar_list\")",
  "def get_new_companies(self) -> pd.DataFrame:\n        \"\"\"\n\n        Returns\n        -------\n            pd.DataFrame:\n\n                symbol     start_date    end_date\n                SH600000   2000-01-01    2099-12-31\n\n            dtypes:\n                symbol: str\n                start_date: pd.Timestamp\n                end_date: pd.Timestamp\n        \"\"\"\n        raise NotImplementedError(\"rewrite get_new_companies\")",
  "def get_changes(self) -> pd.DataFrame:\n        \"\"\"get companies changes\n\n        Returns\n        -------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n        \"\"\"\n        raise NotImplementedError(\"rewrite get_changes\")",
  "def save_new_companies(self):\n        \"\"\"save new companies\n\n        Examples\n        -------\n            $ python collector.py save_new_companies --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data\n        \"\"\"\n        df = self.get_new_companies()\n        df = df.drop_duplicates([self.SYMBOL_FIELD_NAME])\n        df.loc[:, self.INSTRUMENTS_COLUMNS].to_csv(\n            self.instruments_dir.joinpath(f\"{self.index_name.lower()}_only_new.txt\"), sep=\"\\t\", index=False, header=None\n        )",
  "def get_changes_with_history_companies(self, history_companies: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"get changes with history companies\n\n        Parameters\n        ----------\n        history_companies : pd.DataFrame\n            symbol        date\n            SH600000   2020-11-11\n\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n\n        Return\n        --------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n\n        \"\"\"\n        logger.info(\"parse changes from history companies......\")\n        last_code = []\n        result_df_list = []\n        _columns = [self.DATE_FIELD_NAME, self.SYMBOL_FIELD_NAME, self.CHANGE_TYPE_FIELD]\n        for _trading_date in tqdm(sorted(history_companies[self.DATE_FIELD_NAME].unique(), reverse=True)):\n            _currenet_code = history_companies[history_companies[self.DATE_FIELD_NAME] == _trading_date][\n                self.SYMBOL_FIELD_NAME\n            ].tolist()\n            if last_code:\n                add_code = list(set(last_code) - set(_currenet_code))\n                remote_code = list(set(_currenet_code) - set(last_code))\n                for _code in add_code:\n                    result_df_list.append(\n                        pd.DataFrame(\n                            [[get_trading_date_by_shift(self.calendar_list, _trading_date, 1), _code, self.ADD]],\n                            columns=_columns,\n                        )\n                    )\n                for _code in remote_code:\n                    result_df_list.append(\n                        pd.DataFrame(\n                            [[get_trading_date_by_shift(self.calendar_list, _trading_date, 0), _code, self.REMOVE]],\n                            columns=_columns,\n                        )\n                    )\n            last_code = _currenet_code\n        df = pd.concat(result_df_list)\n        logger.info(\"end of parse changes from history companies.\")\n        return df",
  "def parse_instruments(self):\n        \"\"\"parse instruments, eg: csi300.txt\n\n        Examples\n        -------\n            $ python collector.py parse_instruments --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data\n        \"\"\"\n        logger.info(f\"start parse {self.index_name.lower()} companies.....\")\n        instruments_columns = [self.SYMBOL_FIELD_NAME, self.START_DATE_FIELD, self.END_DATE_FIELD]\n        changers_df = self.get_changes()\n        new_df = self.get_new_companies().copy()\n        logger.info(\"parse history companies by changes......\")\n        for _row in tqdm(changers_df.sort_values(self.DATE_FIELD_NAME, ascending=False).itertuples(index=False)):\n            if _row.type == self.ADD:\n                min_end_date = new_df.loc[new_df[self.SYMBOL_FIELD_NAME] == _row.symbol, self.END_DATE_FIELD].min()\n                new_df.loc[\n                    (new_df[self.END_DATE_FIELD] == min_end_date) & (new_df[self.SYMBOL_FIELD_NAME] == _row.symbol),\n                    self.START_DATE_FIELD,\n                ] = _row.date\n            else:\n                _tmp_df = pd.DataFrame([[_row.symbol, self.bench_start_date, _row.date]], columns=instruments_columns)\n                new_df = new_df.append(_tmp_df, sort=False)\n\n        inst_df = new_df.loc[:, instruments_columns]\n        _inst_prefix = self.INST_PREFIX.strip()\n        if _inst_prefix:\n            inst_df[\"save_inst\"] = inst_df[self.SYMBOL_FIELD_NAME].apply(lambda x: f\"{_inst_prefix}{x}\")\n        inst_df.to_csv(\n            self.instruments_dir.joinpath(f\"{self.index_name.lower()}.txt\"), sep=\"\\t\", index=False, header=None\n        )\n        logger.info(f\"parse {self.index_name.lower()} companies finished.\")",
  "class CSIIndex(IndexBase):\n    @property\n    def calendar_list(self) -> List[pd.Timestamp]:\n        \"\"\"get history trading date\n\n        Returns\n        -------\n            calendar list\n        \"\"\"\n        return get_calendar_list(bench_code=self.index_name.upper())\n\n    @property\n    def new_companies_url(self) -> str:\n        return NEW_COMPANIES_URL.format(index_code=self.index_code)\n\n    @property\n    def changes_url(self) -> str:\n        return INDEX_CHANGES_URL\n\n    @property\n    @abc.abstractmethod\n    def bench_start_date(self) -> pd.Timestamp:\n        \"\"\"\n        Returns\n        -------\n            index start date\n        \"\"\"\n        raise NotImplementedError(\"rewrite bench_start_date\")\n\n    @property\n    @abc.abstractmethod\n    def index_code(self) -> str:\n        \"\"\"\n        Returns\n        -------\n            index code\n        \"\"\"\n        raise NotImplementedError(\"rewrite index_code\")\n\n    @property\n    @abc.abstractmethod\n    def html_table_index(self) -> int:\n        \"\"\"Which table of changes in html\n\n        CSI300: 0\n        CSI100: 1\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_changes(self) -> pd.DataFrame:\n        \"\"\"get companies changes\n\n        Returns\n        -------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n        \"\"\"\n        logger.info(\"get companies changes......\")\n        res = []\n        for _url in self._get_change_notices_url():\n            _df = self._read_change_from_url(_url)\n            res.append(_df)\n        logger.info(\"get companies changes finish\")\n        return pd.concat(res, sort=False)\n\n    @staticmethod\n    def normalize_symbol(symbol: str) -> str:\n        \"\"\"\n\n        Parameters\n        ----------\n        symbol: str\n            symbol\n\n        Returns\n        -------\n            symbol\n        \"\"\"\n        symbol = f\"{int(symbol):06}\"\n        return f\"SH{symbol}\" if symbol.startswith(\"60\") else f\"SZ{symbol}\"\n\n    def _read_change_from_url(self, url: str) -> pd.DataFrame:\n        \"\"\"read change from url\n\n        Parameters\n        ----------\n        url : str\n            change url\n\n        Returns\n        -------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n        \"\"\"\n        resp = requests.get(url)\n        _text = resp.text\n\n        date_list = re.findall(r\"(\\d{4}).*?\u5e74.*?(\\d+).*?\u6708.*?(\\d+).*?\u65e5\", _text)\n        if len(date_list) >= 2:\n            add_date = pd.Timestamp(\"-\".join(date_list[0]))\n        else:\n            _date = pd.Timestamp(\"-\".join(re.findall(r\"(\\d{4}).*?\u5e74.*?(\\d+).*?\u6708\", _text)[0]))\n            add_date = get_trading_date_by_shift(self.calendar_list, _date, shift=0)\n        remove_date = get_trading_date_by_shift(self.calendar_list, add_date, shift=-1)\n        logger.info(f\"get {add_date} changes\")\n        try:\n            excel_url = re.findall('.*href=\"(.*?xls.*?)\".*', _text)[0]\n            content = requests.get(f\"http://www.csindex.com.cn{excel_url}\").content\n            _io = BytesIO(content)\n            df_map = pd.read_excel(_io, sheet_name=None)\n            with self.cache_dir.joinpath(\n                f\"{self.index_name.lower()}_changes_{add_date.strftime('%Y%m%d')}.{excel_url.split('.')[-1]}\"\n            ).open(\"wb\") as fp:\n                fp.write(content)\n            tmp = []\n            for _s_name, _type, _date in [(\"\u8c03\u5165\", self.ADD, add_date), (\"\u8c03\u51fa\", self.REMOVE, remove_date)]:\n                _df = df_map[_s_name]\n                _df = _df.loc[_df[\"\u6307\u6570\u4ee3\u7801\"] == self.index_code, [\"\u8bc1\u5238\u4ee3\u7801\"]]\n                _df = _df.applymap(self.normalize_symbol)\n                _df.columns = [self.SYMBOL_FIELD_NAME]\n                _df[\"type\"] = _type\n                _df[self.DATE_FIELD_NAME] = _date\n                tmp.append(_df)\n            df = pd.concat(tmp)\n        except Exception as e:\n            df = None\n            _tmp_count = 0\n            for _df in pd.read_html(resp.content):\n                if _df.shape[-1] != 4:\n                    continue\n                _tmp_count += 1\n                if self.html_table_index + 1 > _tmp_count:\n                    continue\n                tmp = []\n                for _s, _type, _date in [\n                    (_df.iloc[2:, 0], self.REMOVE, remove_date),\n                    (_df.iloc[2:, 2], self.ADD, add_date),\n                ]:\n                    _tmp_df = pd.DataFrame()\n                    _tmp_df[self.SYMBOL_FIELD_NAME] = _s.map(self.normalize_symbol)\n                    _tmp_df[\"type\"] = _type\n                    _tmp_df[self.DATE_FIELD_NAME] = _date\n                    tmp.append(_tmp_df)\n                df = pd.concat(tmp)\n                df.to_csv(\n                    str(\n                        self.cache_dir.joinpath(\n                            f\"{self.index_name.lower()}_changes_{add_date.strftime('%Y%m%d')}.csv\"\n                        ).resolve()\n                    )\n                )\n                break\n        return df\n\n    def _get_change_notices_url(self) -> List[str]:\n        \"\"\"get change notices url\n\n        Returns\n        -------\n            [url1, url2]\n        \"\"\"\n        resp = requests.get(self.changes_url)\n        html = etree.HTML(resp.text)\n        return html.xpath(\"//*[@id='itemContainer']//li/a/@href\")\n\n    def get_new_companies(self) -> pd.DataFrame:\n        \"\"\"\n\n        Returns\n        -------\n            pd.DataFrame:\n\n                symbol     start_date    end_date\n                SH600000   2000-01-01    2099-12-31\n\n            dtypes:\n                symbol: str\n                start_date: pd.Timestamp\n                end_date: pd.Timestamp\n        \"\"\"\n        logger.info(\"get new companies......\")\n        context = requests.get(self.new_companies_url).content\n        with self.cache_dir.joinpath(\n            f\"{self.index_name.lower()}_new_companies.{self.new_companies_url.split('.')[-1]}\"\n        ).open(\"wb\") as fp:\n            fp.write(context)\n        _io = BytesIO(context)\n        df = pd.read_excel(_io)\n        df = df.iloc[:, [0, 4]]\n        df.columns = [self.END_DATE_FIELD, self.SYMBOL_FIELD_NAME]\n        df[self.SYMBOL_FIELD_NAME] = df[self.SYMBOL_FIELD_NAME].map(self.normalize_symbol)\n        df[self.END_DATE_FIELD] = pd.to_datetime(df[self.END_DATE_FIELD])\n        df[self.START_DATE_FIELD] = self.bench_start_date\n        logger.info(\"end of get new companies.\")\n        return df",
  "class CSI300(CSIIndex):\n    @property\n    def index_code(self):\n        return \"000300\"\n\n    @property\n    def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2005-01-01\")\n\n    @property\n    def html_table_index(self):\n        return 0",
  "class CSI100(CSIIndex):\n    @property\n    def index_code(self):\n        return \"000903\"\n\n    @property\n    def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2006-05-29\")\n\n    @property\n    def html_table_index(self):\n        return 1",
  "def get_instruments(\n    qlib_dir: str, index_name: str, method: str = \"parse_instruments\", request_retry: int = 5, retry_sleep: int = 3\n):\n    \"\"\"\n\n    Parameters\n    ----------\n    qlib_dir: str\n        qlib data dir, default \"Path(__file__).parent/qlib_data\"\n    index_name: str\n        index name, value from [\"csi100\", \"csi300\"]\n    method: str\n        method, value from [\"parse_instruments\", \"save_new_companies\"]\n    request_retry: int\n        request retry, by default 5\n    retry_sleep: int\n        request sleep, by default 3\n\n    Examples\n    -------\n        # parse instruments\n        $ python collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n        # parse new companies\n        $ python collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n    \"\"\"\n    _cur_module = importlib.import_module(\"collector\")\n    obj = getattr(_cur_module, f\"{index_name.upper()}\")(\n        qlib_dir=qlib_dir, index_name=index_name, request_retry=request_retry, retry_sleep=retry_sleep\n    )\n    getattr(obj, method)()",
  "def calendar_list(self) -> List[pd.Timestamp]:\n        \"\"\"get history trading date\n\n        Returns\n        -------\n            calendar list\n        \"\"\"\n        return get_calendar_list(bench_code=self.index_name.upper())",
  "def new_companies_url(self) -> str:\n        return NEW_COMPANIES_URL.format(index_code=self.index_code)",
  "def changes_url(self) -> str:\n        return INDEX_CHANGES_URL",
  "def bench_start_date(self) -> pd.Timestamp:\n        \"\"\"\n        Returns\n        -------\n            index start date\n        \"\"\"\n        raise NotImplementedError(\"rewrite bench_start_date\")",
  "def index_code(self) -> str:\n        \"\"\"\n        Returns\n        -------\n            index code\n        \"\"\"\n        raise NotImplementedError(\"rewrite index_code\")",
  "def html_table_index(self) -> int:\n        \"\"\"Which table of changes in html\n\n        CSI300: 0\n        CSI100: 1\n        :return:\n        \"\"\"\n        raise NotImplementedError()",
  "def get_changes(self) -> pd.DataFrame:\n        \"\"\"get companies changes\n\n        Returns\n        -------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n        \"\"\"\n        logger.info(\"get companies changes......\")\n        res = []\n        for _url in self._get_change_notices_url():\n            _df = self._read_change_from_url(_url)\n            res.append(_df)\n        logger.info(\"get companies changes finish\")\n        return pd.concat(res, sort=False)",
  "def normalize_symbol(symbol: str) -> str:\n        \"\"\"\n\n        Parameters\n        ----------\n        symbol: str\n            symbol\n\n        Returns\n        -------\n            symbol\n        \"\"\"\n        symbol = f\"{int(symbol):06}\"\n        return f\"SH{symbol}\" if symbol.startswith(\"60\") else f\"SZ{symbol}\"",
  "def _read_change_from_url(self, url: str) -> pd.DataFrame:\n        \"\"\"read change from url\n\n        Parameters\n        ----------\n        url : str\n            change url\n\n        Returns\n        -------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n        \"\"\"\n        resp = requests.get(url)\n        _text = resp.text\n\n        date_list = re.findall(r\"(\\d{4}).*?\u5e74.*?(\\d+).*?\u6708.*?(\\d+).*?\u65e5\", _text)\n        if len(date_list) >= 2:\n            add_date = pd.Timestamp(\"-\".join(date_list[0]))\n        else:\n            _date = pd.Timestamp(\"-\".join(re.findall(r\"(\\d{4}).*?\u5e74.*?(\\d+).*?\u6708\", _text)[0]))\n            add_date = get_trading_date_by_shift(self.calendar_list, _date, shift=0)\n        remove_date = get_trading_date_by_shift(self.calendar_list, add_date, shift=-1)\n        logger.info(f\"get {add_date} changes\")\n        try:\n            excel_url = re.findall('.*href=\"(.*?xls.*?)\".*', _text)[0]\n            content = requests.get(f\"http://www.csindex.com.cn{excel_url}\").content\n            _io = BytesIO(content)\n            df_map = pd.read_excel(_io, sheet_name=None)\n            with self.cache_dir.joinpath(\n                f\"{self.index_name.lower()}_changes_{add_date.strftime('%Y%m%d')}.{excel_url.split('.')[-1]}\"\n            ).open(\"wb\") as fp:\n                fp.write(content)\n            tmp = []\n            for _s_name, _type, _date in [(\"\u8c03\u5165\", self.ADD, add_date), (\"\u8c03\u51fa\", self.REMOVE, remove_date)]:\n                _df = df_map[_s_name]\n                _df = _df.loc[_df[\"\u6307\u6570\u4ee3\u7801\"] == self.index_code, [\"\u8bc1\u5238\u4ee3\u7801\"]]\n                _df = _df.applymap(self.normalize_symbol)\n                _df.columns = [self.SYMBOL_FIELD_NAME]\n                _df[\"type\"] = _type\n                _df[self.DATE_FIELD_NAME] = _date\n                tmp.append(_df)\n            df = pd.concat(tmp)\n        except Exception as e:\n            df = None\n            _tmp_count = 0\n            for _df in pd.read_html(resp.content):\n                if _df.shape[-1] != 4:\n                    continue\n                _tmp_count += 1\n                if self.html_table_index + 1 > _tmp_count:\n                    continue\n                tmp = []\n                for _s, _type, _date in [\n                    (_df.iloc[2:, 0], self.REMOVE, remove_date),\n                    (_df.iloc[2:, 2], self.ADD, add_date),\n                ]:\n                    _tmp_df = pd.DataFrame()\n                    _tmp_df[self.SYMBOL_FIELD_NAME] = _s.map(self.normalize_symbol)\n                    _tmp_df[\"type\"] = _type\n                    _tmp_df[self.DATE_FIELD_NAME] = _date\n                    tmp.append(_tmp_df)\n                df = pd.concat(tmp)\n                df.to_csv(\n                    str(\n                        self.cache_dir.joinpath(\n                            f\"{self.index_name.lower()}_changes_{add_date.strftime('%Y%m%d')}.csv\"\n                        ).resolve()\n                    )\n                )\n                break\n        return df",
  "def _get_change_notices_url(self) -> List[str]:\n        \"\"\"get change notices url\n\n        Returns\n        -------\n            [url1, url2]\n        \"\"\"\n        resp = requests.get(self.changes_url)\n        html = etree.HTML(resp.text)\n        return html.xpath(\"//*[@id='itemContainer']//li/a/@href\")",
  "def get_new_companies(self) -> pd.DataFrame:\n        \"\"\"\n\n        Returns\n        -------\n            pd.DataFrame:\n\n                symbol     start_date    end_date\n                SH600000   2000-01-01    2099-12-31\n\n            dtypes:\n                symbol: str\n                start_date: pd.Timestamp\n                end_date: pd.Timestamp\n        \"\"\"\n        logger.info(\"get new companies......\")\n        context = requests.get(self.new_companies_url).content\n        with self.cache_dir.joinpath(\n            f\"{self.index_name.lower()}_new_companies.{self.new_companies_url.split('.')[-1]}\"\n        ).open(\"wb\") as fp:\n            fp.write(context)\n        _io = BytesIO(context)\n        df = pd.read_excel(_io)\n        df = df.iloc[:, [0, 4]]\n        df.columns = [self.END_DATE_FIELD, self.SYMBOL_FIELD_NAME]\n        df[self.SYMBOL_FIELD_NAME] = df[self.SYMBOL_FIELD_NAME].map(self.normalize_symbol)\n        df[self.END_DATE_FIELD] = pd.to_datetime(df[self.END_DATE_FIELD])\n        df[self.START_DATE_FIELD] = self.bench_start_date\n        logger.info(\"end of get new companies.\")\n        return df",
  "def index_code(self):\n        return \"000300\"",
  "def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2005-01-01\")",
  "def html_table_index(self):\n        return 0",
  "def index_code(self):\n        return \"000903\"",
  "def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2006-05-29\")",
  "def html_table_index(self):\n        return 1",
  "class YahooCollector(BaseCollector):\n    def __init__(\n        self,\n        save_dir: [str, Path],\n        start=None,\n        end=None,\n        interval=\"1d\",\n        max_workers=4,\n        max_collector_count=2,\n        delay=0,\n        check_data_length: bool = False,\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        save_dir: str\n            stock save dir\n        max_workers: int\n            workers, default 4\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1min\n        start: str\n            start datetime, default None\n        end: str\n            end datetime, default None\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n        \"\"\"\n        super(YahooCollector, self).__init__(\n            save_dir=save_dir,\n            start=start,\n            end=end,\n            interval=interval,\n            max_workers=max_workers,\n            max_collector_count=max_collector_count,\n            delay=delay,\n            check_data_length=check_data_length,\n            limit_nums=limit_nums,\n        )\n\n        self.init_datetime()\n\n    def init_datetime(self):\n        if self.interval == self.INTERVAL_1min:\n            self.start_datetime = max(self.start_datetime, self.DEFAULT_START_DATETIME_1MIN)\n        elif self.interval == self.INTERVAL_1d:\n            pass\n        else:\n            raise ValueError(f\"interval error: {self.interval}\")\n\n        # using for 1min\n        self._next_datetime = self.convert_datetime(self.start_datetime.date() + pd.Timedelta(days=1), self._timezone)\n        self._latest_datetime = self.convert_datetime(self.end_datetime.date(), self._timezone)\n\n        self.start_datetime = self.convert_datetime(self.start_datetime, self._timezone)\n        self.end_datetime = self.convert_datetime(self.end_datetime, self._timezone)\n\n    @staticmethod\n    def convert_datetime(dt: [pd.Timestamp, datetime.date, str], timezone):\n        try:\n            dt = pd.Timestamp(dt, tz=timezone).timestamp()\n            dt = pd.Timestamp(dt, tz=tzlocal(), unit=\"s\")\n        except ValueError as e:\n            pass\n        return dt\n\n    @property\n    @abc.abstractmethod\n    def _timezone(self):\n        raise NotImplementedError(\"rewrite get_timezone\")\n\n    @staticmethod\n    def get_data_from_remote(symbol, interval, start, end, show_1min_logging: bool = False):\n        error_msg = f\"{symbol}-{interval}-{start}-{end}\"\n\n        def _show_logging_func():\n            if interval == YahooCollector.INTERVAL_1min and show_1min_logging:\n                logger.warning(f\"{error_msg}:{_resp}\")\n\n        interval = \"1m\" if interval in [\"1m\", \"1min\"] else interval\n        try:\n            _resp = Ticker(symbol, asynchronous=False).history(interval=interval, start=start, end=end)\n            if isinstance(_resp, pd.DataFrame):\n                return _resp.reset_index()\n            elif isinstance(_resp, dict):\n                _temp_data = _resp.get(symbol, {})\n                if isinstance(_temp_data, str) or (\n                    isinstance(_resp, dict) and _temp_data.get(\"indicators\", {}).get(\"quote\", None) is None\n                ):\n                    _show_logging_func()\n            else:\n                _show_logging_func()\n        except Exception as e:\n            logger.warning(f\"{error_msg}:{e}\")\n\n    def get_data(\n        self, symbol: str, interval: str, start_datetime: pd.Timestamp, end_datetime: pd.Timestamp\n    ) -> pd.DataFrame:\n        def _get_simple(start_, end_):\n            self.sleep()\n            _remote_interval = \"1m\" if interval == self.INTERVAL_1min else interval\n            return self.get_data_from_remote(\n                symbol,\n                interval=_remote_interval,\n                start=start_,\n                end=end_,\n            )\n\n        _result = None\n        if interval == self.INTERVAL_1d:\n            _result = _get_simple(start_datetime, end_datetime)\n        elif interval == self.INTERVAL_1min:\n            if self._next_datetime >= self._latest_datetime:\n                _result = _get_simple(start_datetime, end_datetime)\n            else:\n                _res = []\n\n                def _get_multi(start_, end_):\n                    _resp = _get_simple(start_, end_)\n                    if _resp is not None and not _resp.empty:\n                        _res.append(_resp)\n\n                for _s, _e in (\n                    (self.start_datetime, self._next_datetime),\n                    (self._latest_datetime, self.end_datetime),\n                ):\n                    _get_multi(_s, _e)\n                for _start in pd.date_range(self._next_datetime, self._latest_datetime, closed=\"left\"):\n                    _end = _start + pd.Timedelta(days=1)\n                    _get_multi(_start, _end)\n                if _res:\n                    _result = pd.concat(_res, sort=False).sort_values([\"symbol\", \"date\"])\n        else:\n            raise ValueError(f\"cannot support {self.interval}\")\n        return pd.DataFrame() if _result is None else _result\n\n    def collector_data(self):\n        \"\"\"collector data\"\"\"\n        super(YahooCollector, self).collector_data()\n        self.download_index_data()\n\n    @abc.abstractmethod\n    def download_index_data(self):\n        \"\"\"download index data\"\"\"\n        raise NotImplementedError(\"rewrite download_index_data\")",
  "class YahooCollectorCN(YahooCollector, ABC):\n    def get_stock_list(self):\n        logger.info(\"get HS stock symbos......\")\n        symbols = get_hs_stock_symbols()\n        logger.info(f\"get {len(symbols)} symbols.\")\n        return symbols\n\n    def normalize_symbol(self, symbol):\n        symbol_s = symbol.split(\".\")\n        symbol = f\"sh{symbol_s[0]}\" if symbol_s[-1] == \"ss\" else f\"sz{symbol_s[0]}\"\n        return symbol\n\n    @property\n    def _timezone(self):\n        return \"Asia/Shanghai\"",
  "class YahooCollectorCN1d(YahooCollectorCN):\n    @property\n    def min_numbers_trading(self):\n        return 252 / 4\n\n    def download_index_data(self):\n        # TODO: from MSN\n        _format = \"%Y%m%d\"\n        _begin = self.start_datetime.strftime(_format)\n        _end = (self.end_datetime + pd.Timedelta(days=-1)).strftime(_format)\n        for _index_name, _index_code in {\"csi300\": \"000300\", \"csi100\": \"000903\"}.items():\n            logger.info(f\"get bench data: {_index_name}({_index_code})......\")\n            try:\n                df = pd.DataFrame(\n                    map(\n                        lambda x: x.split(\",\"),\n                        requests.get(INDEX_BENCH_URL.format(index_code=_index_code, begin=_begin, end=_end)).json()[\n                            \"data\"\n                        ][\"klines\"],\n                    )\n                )\n            except Exception as e:\n                logger.warning(f\"get {_index_name} error: {e}\")\n                continue\n            df.columns = [\"date\", \"open\", \"close\", \"high\", \"low\", \"volume\", \"money\", \"change\"]\n            df[\"date\"] = pd.to_datetime(df[\"date\"])\n            df = df.astype(float, errors=\"ignore\")\n            df[\"adjclose\"] = df[\"close\"]\n            df[\"symbol\"] = f\"sh{_index_code}\"\n            _path = self.save_dir.joinpath(f\"sh{_index_code}.csv\")\n            if _path.exists():\n                _old_df = pd.read_csv(_path)\n                df = _old_df.append(df, sort=False)\n            df.to_csv(_path, index=False)\n            time.sleep(5)",
  "class YahooCollectorCN1min(YahooCollectorCN):\n    @property\n    def min_numbers_trading(self):\n        return 60 * 4 * 5\n\n    def download_index_data(self):\n        # TODO: 1m\n        logger.warning(f\"{self.__class__.__name__} {self.interval} does not support: download_index_data\")",
  "class YahooCollectorUS(YahooCollector, ABC):\n    def get_stock_list(self):\n        logger.info(\"get US stock symbols......\")\n        symbols = get_us_stock_symbols() + [\n            \"^GSPC\",\n            \"^NDX\",\n            \"^DJI\",\n        ]\n        logger.info(f\"get {len(symbols)} symbols.\")\n        return symbols\n\n    def download_index_data(self):\n        pass\n\n    def normalize_symbol(self, symbol):\n        return code_to_fname(symbol).upper()\n\n    @property\n    def _timezone(self):\n        return \"America/New_York\"",
  "class YahooCollectorUS1d(YahooCollectorUS):\n    @property\n    def min_numbers_trading(self):\n        return 252 / 4",
  "class YahooCollectorUS1min(YahooCollectorUS):\n    @property\n    def min_numbers_trading(self):\n        return 60 * 6.5 * 5",
  "class YahooNormalize(BaseNormalize):\n    COLUMNS = [\"open\", \"close\", \"high\", \"low\", \"volume\"]\n    DAILY_FORMAT = \"%Y-%m-%d\"\n\n    @staticmethod\n    def normalize_yahoo(\n        df: pd.DataFrame,\n        calendar_list: list = None,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        if df.empty:\n            return df\n        symbol = df.loc[df[symbol_field_name].first_valid_index(), symbol_field_name]\n        columns = copy.deepcopy(YahooNormalize.COLUMNS)\n        df = df.copy()\n        df.set_index(date_field_name, inplace=True)\n        df.index = pd.to_datetime(df.index)\n        df = df[~df.index.duplicated(keep=\"first\")]\n        if calendar_list is not None:\n            df = df.reindex(\n                pd.DataFrame(index=calendar_list)\n                .loc[\n                    pd.Timestamp(df.index.min()).date() : pd.Timestamp(df.index.max()).date()\n                    + pd.Timedelta(hours=23, minutes=59)\n                ]\n                .index\n            )\n        df.sort_index(inplace=True)\n        df.loc[(df[\"volume\"] <= 0) | np.isnan(df[\"volume\"]), set(df.columns) - {symbol_field_name}] = np.nan\n        _tmp_series = df[\"close\"].fillna(method=\"ffill\")\n        df[\"change\"] = _tmp_series / _tmp_series.shift(1) - 1\n        columns += [\"change\"]\n        df.loc[(df[\"volume\"] <= 0) | np.isnan(df[\"volume\"]), columns] = np.nan\n\n        df[symbol_field_name] = symbol\n        df.index.names = [date_field_name]\n        return df.reset_index()\n\n    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n        # normalize\n        df = self.normalize_yahoo(df, self._calendar_list, self._date_field_name, self._symbol_field_name)\n        # adjusted price\n        df = self.adjusted_price(df)\n        return df\n\n    @abc.abstractmethod\n    def adjusted_price(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"adjusted price\"\"\"\n        raise NotImplementedError(\"rewrite adjusted_price\")",
  "class YahooNormalize1d(YahooNormalize, ABC):\n    DAILY_FORMAT = \"%Y-%m-%d\"\n\n    def adjusted_price(self, df: pd.DataFrame) -> pd.DataFrame:\n        if df.empty:\n            return df\n        df = df.copy()\n        df.set_index(self._date_field_name, inplace=True)\n        if \"adjclose\" in df:\n            df[\"factor\"] = df[\"adjclose\"] / df[\"close\"]\n            df[\"factor\"] = df[\"factor\"].fillna(method=\"ffill\")\n        else:\n            df[\"factor\"] = 1\n        for _col in self.COLUMNS:\n            if _col not in df.columns:\n                continue\n            if _col == \"volume\":\n                df[_col] = df[_col] / df[\"factor\"]\n            else:\n                df[_col] = df[_col] * df[\"factor\"]\n        df.index.names = [self._date_field_name]\n        return df.reset_index()\n\n    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n        df = super(YahooNormalize1d, self).normalize(df)\n        df = self._manual_adj_data(df)\n        return df\n\n    def _manual_adj_data(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"manual adjust data: All fields (except change) are standardized according to the close of the first day\"\"\"\n        if df.empty:\n            return df\n        df = df.copy()\n        df.sort_values(self._date_field_name, inplace=True)\n        df = df.set_index(self._date_field_name)\n        df = df.loc[df[\"close\"].first_valid_index() :]\n        _close = df[\"close\"].iloc[0]\n        for _col in df.columns:\n            if _col == self._symbol_field_name:\n                continue\n            if _col == \"volume\":\n                df[_col] = df[_col] * _close\n            elif _col != \"change\":\n                df[_col] = df[_col] / _close\n            else:\n                pass\n        return df.reset_index()",
  "class YahooNormalize1min(YahooNormalize, ABC):\n    AM_RANGE = None  # type: tuple  # eg: (\"09:30:00\", \"11:29:00\")\n    PM_RANGE = None  # type: tuple  # eg: (\"13:00:00\", \"14:59:00\")\n\n    # Whether the trading day of 1min data is consistent with 1d\n    CONSISTENT_1d = False\n\n    def __init__(\n        self,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default is date\n        symbol_field_name: str\n            symbol field name, default is symbol\n        \"\"\"\n        super(YahooNormalize1min, self).__init__(date_field_name, symbol_field_name)\n        _class_name = self.__class__.__name__.replace(\"min\", \"d\")\n        _class = getattr(importlib.import_module(\"collector\"), _class_name)  # type: Type[YahooNormalize]\n        self.data_1d_obj = _class(self._date_field_name, self._symbol_field_name)\n\n    @property\n    def calendar_list_1d(self):\n        calendar_list_1d = getattr(self, \"_calendar_list_1d\", None)\n        if calendar_list_1d is None:\n            calendar_list_1d = self._get_1d_calendar_list()\n            setattr(self, \"_calendar_list_1d\", calendar_list_1d)\n        return calendar_list_1d\n\n    def generate_1min_from_daily(self, calendars: Iterable) -> pd.Index:\n        res = []\n        daily_format = self.DAILY_FORMAT\n        am_range = self.AM_RANGE\n        pm_range = self.PM_RANGE\n        for _day in calendars:\n            for _range in [am_range, pm_range]:\n                res.append(\n                    pd.date_range(\n                        f\"{_day.strftime(daily_format)} {_range[0]}\",\n                        f\"{_day.strftime(daily_format)} {_range[1]}\",\n                        freq=\"1min\",\n                    )\n                )\n\n        return pd.Index(sorted(set(np.hstack(res))))\n\n    def adjusted_price(self, df: pd.DataFrame) -> pd.DataFrame:\n        # TODO: using daily data factor\n        if df.empty:\n            return df\n        df = df.copy()\n        symbol = df.iloc[0][self._symbol_field_name]\n        # get 1d data from yahoo\n        _start = pd.Timestamp(df[self._date_field_name].min()).strftime(self.DAILY_FORMAT)\n        _end = (pd.Timestamp(df[self._date_field_name].max()) + pd.Timedelta(days=1)).strftime(self.DAILY_FORMAT)\n        data_1d = YahooCollector.get_data_from_remote(\n            self.symbol_to_yahoo(symbol), interval=\"1d\", start=_start, end=_end\n        )\n        if data_1d is None or data_1d.empty:\n            df[\"factor\"] = 1\n            # TODO: np.nan or 1 or 0\n            df[\"paused\"] = np.nan\n        else:\n            data_1d = self.data_1d_obj.normalize(data_1d)  # type: pd.DataFrame\n            # NOTE: volume is np.nan or volume <= 0, paused = 1\n            # FIXME: find a more accurate data source\n            data_1d[\"paused\"] = 0\n            data_1d.loc[(data_1d[\"volume\"].isna()) | (data_1d[\"volume\"] <= 0), \"paused\"] = 1\n            data_1d = data_1d.set_index(self._date_field_name)\n\n            # add factor from 1d data\n            df[\"date_tmp\"] = df[self._date_field_name].apply(lambda x: pd.Timestamp(x).date())\n            df.set_index(\"date_tmp\", inplace=True)\n            df.loc[:, \"factor\"] = data_1d[\"factor\"]\n            df.loc[:, \"paused\"] = data_1d[\"paused\"]\n            df.reset_index(\"date_tmp\", drop=True, inplace=True)\n\n            if self.CONSISTENT_1d:\n                # the date sequence is consistent with 1d\n                df.set_index(self._date_field_name, inplace=True)\n                df = df.reindex(\n                    self.generate_1min_from_daily(\n                        pd.to_datetime(data_1d.reset_index()[self._date_field_name].drop_duplicates())\n                    )\n                )\n                df[self._symbol_field_name] = df.loc[df[self._symbol_field_name].first_valid_index()][\n                    self._symbol_field_name\n                ]\n                df.index.names = [self._date_field_name]\n                df.reset_index(inplace=True)\n        for _col in self.COLUMNS:\n            if _col not in df.columns:\n                continue\n            if _col == \"volume\":\n                df[_col] = df[_col] / df[\"factor\"]\n            else:\n                df[_col] = df[_col] * df[\"factor\"]\n        return df\n\n    @abc.abstractmethod\n    def symbol_to_yahoo(self, symbol):\n        raise NotImplementedError(\"rewrite symbol_to_yahoo\")\n\n    @abc.abstractmethod\n    def _get_1d_calendar_list(self):\n        raise NotImplementedError(\"rewrite _get_1d_calendar_list\")",
  "class YahooNormalizeUS:\n    def _get_calendar_list(self):\n        # TODO: from MSN\n        return get_calendar_list(\"US_ALL\")",
  "class YahooNormalizeUS1d(YahooNormalizeUS, YahooNormalize1d):\n    pass",
  "class YahooNormalizeUS1min(YahooNormalizeUS, YahooNormalize1min):\n    CONSISTENT_1d = False\n\n    def _get_calendar_list(self):\n        # TODO: support 1min\n        raise ValueError(\"Does not support 1min\")\n\n    def _get_1d_calendar_list(self):\n        return get_calendar_list(\"US_ALL\")\n\n    def symbol_to_yahoo(self, symbol):\n        return fname_to_code(symbol)",
  "class YahooNormalizeCN:\n    def _get_calendar_list(self):\n        # TODO: from MSN\n        return get_calendar_list(\"ALL\")",
  "class YahooNormalizeCN1d(YahooNormalizeCN, YahooNormalize1d):\n    pass",
  "class YahooNormalizeCN1min(YahooNormalizeCN, YahooNormalize1min):\n    AM_RANGE = (\"09:30:00\", \"11:29:00\")\n    PM_RANGE = (\"13:00:00\", \"14:59:00\")\n\n    CONSISTENT_1d = True\n\n    def _get_calendar_list(self):\n        return self.generate_1min_from_daily(self.calendar_list_1d)\n\n    def symbol_to_yahoo(self, symbol):\n        if \".\" not in symbol:\n            _exchange = symbol[:2]\n            _exchange = \"ss\" if _exchange == \"sh\" else _exchange\n            symbol = symbol[2:] + \".\" + _exchange\n        return symbol\n\n    def _get_1d_calendar_list(self):\n        return get_calendar_list(\"ALL\")",
  "class Run(BaseRun):\n    def __init__(self, source_dir=None, normalize_dir=None, max_workers=4, interval=\"1d\", region=REGION_CN):\n        \"\"\"\n\n        Parameters\n        ----------\n        source_dir: str\n            The directory where the raw data collected from the Internet is saved, default \"Path(__file__).parent/source\"\n        normalize_dir: str\n            Directory for normalize data, default \"Path(__file__).parent/normalize\"\n        max_workers: int\n            Concurrent number, default is 4\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        region: str\n            region, value from [\"CN\", \"US\"], default \"CN\"\n        \"\"\"\n        super().__init__(source_dir, normalize_dir, max_workers, interval)\n        self.region = region\n\n    @property\n    def collector_class_name(self):\n        return f\"YahooCollector{self.region.upper()}{self.interval}\"\n\n    @property\n    def normalize_class_name(self):\n        return f\"YahooNormalize{self.region.upper()}{self.interval}\"\n\n    @property\n    def default_base_dir(self) -> [Path, str]:\n        return CUR_DIR\n\n    def download_data(\n        self,\n        max_collector_count=2,\n        delay=0,\n        start=None,\n        end=None,\n        interval=\"1d\",\n        check_data_length=False,\n        limit_nums=None,\n    ):\n        \"\"\"download data from Internet\n\n        Parameters\n        ----------\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        start: str\n            start datetime, default \"2000-01-01\"\n        end: str\n            end datetime, default ``pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))``\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n\n        Examples\n        ---------\n            # get daily data\n            $ python collector.py download_data --source_dir ~/.qlib/stock_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n            # get 1m data\n            $ python collector.py download_data --source_dir ~/.qlib/stock_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1m\n        \"\"\"\n\n        super(Run, self).download_data(max_collector_count, delay, start, end, interval, check_data_length, limit_nums)\n\n    def normalize_data(self, date_field_name: str = \"date\", symbol_field_name: str = \"symbol\"):\n        \"\"\"normalize data\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default date\n        symbol_field_name: str\n            symbol field name, default symbol\n\n        Examples\n        ---------\n            $ python collector.py normalize_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d\n        \"\"\"\n        super(Run, self).normalize_data(date_field_name, symbol_field_name)",
  "def __init__(\n        self,\n        save_dir: [str, Path],\n        start=None,\n        end=None,\n        interval=\"1d\",\n        max_workers=4,\n        max_collector_count=2,\n        delay=0,\n        check_data_length: bool = False,\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        save_dir: str\n            stock save dir\n        max_workers: int\n            workers, default 4\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1min\n        start: str\n            start datetime, default None\n        end: str\n            end datetime, default None\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n        \"\"\"\n        super(YahooCollector, self).__init__(\n            save_dir=save_dir,\n            start=start,\n            end=end,\n            interval=interval,\n            max_workers=max_workers,\n            max_collector_count=max_collector_count,\n            delay=delay,\n            check_data_length=check_data_length,\n            limit_nums=limit_nums,\n        )\n\n        self.init_datetime()",
  "def init_datetime(self):\n        if self.interval == self.INTERVAL_1min:\n            self.start_datetime = max(self.start_datetime, self.DEFAULT_START_DATETIME_1MIN)\n        elif self.interval == self.INTERVAL_1d:\n            pass\n        else:\n            raise ValueError(f\"interval error: {self.interval}\")\n\n        # using for 1min\n        self._next_datetime = self.convert_datetime(self.start_datetime.date() + pd.Timedelta(days=1), self._timezone)\n        self._latest_datetime = self.convert_datetime(self.end_datetime.date(), self._timezone)\n\n        self.start_datetime = self.convert_datetime(self.start_datetime, self._timezone)\n        self.end_datetime = self.convert_datetime(self.end_datetime, self._timezone)",
  "def convert_datetime(dt: [pd.Timestamp, datetime.date, str], timezone):\n        try:\n            dt = pd.Timestamp(dt, tz=timezone).timestamp()\n            dt = pd.Timestamp(dt, tz=tzlocal(), unit=\"s\")\n        except ValueError as e:\n            pass\n        return dt",
  "def _timezone(self):\n        raise NotImplementedError(\"rewrite get_timezone\")",
  "def get_data_from_remote(symbol, interval, start, end, show_1min_logging: bool = False):\n        error_msg = f\"{symbol}-{interval}-{start}-{end}\"\n\n        def _show_logging_func():\n            if interval == YahooCollector.INTERVAL_1min and show_1min_logging:\n                logger.warning(f\"{error_msg}:{_resp}\")\n\n        interval = \"1m\" if interval in [\"1m\", \"1min\"] else interval\n        try:\n            _resp = Ticker(symbol, asynchronous=False).history(interval=interval, start=start, end=end)\n            if isinstance(_resp, pd.DataFrame):\n                return _resp.reset_index()\n            elif isinstance(_resp, dict):\n                _temp_data = _resp.get(symbol, {})\n                if isinstance(_temp_data, str) or (\n                    isinstance(_resp, dict) and _temp_data.get(\"indicators\", {}).get(\"quote\", None) is None\n                ):\n                    _show_logging_func()\n            else:\n                _show_logging_func()\n        except Exception as e:\n            logger.warning(f\"{error_msg}:{e}\")",
  "def get_data(\n        self, symbol: str, interval: str, start_datetime: pd.Timestamp, end_datetime: pd.Timestamp\n    ) -> pd.DataFrame:\n        def _get_simple(start_, end_):\n            self.sleep()\n            _remote_interval = \"1m\" if interval == self.INTERVAL_1min else interval\n            return self.get_data_from_remote(\n                symbol,\n                interval=_remote_interval,\n                start=start_,\n                end=end_,\n            )\n\n        _result = None\n        if interval == self.INTERVAL_1d:\n            _result = _get_simple(start_datetime, end_datetime)\n        elif interval == self.INTERVAL_1min:\n            if self._next_datetime >= self._latest_datetime:\n                _result = _get_simple(start_datetime, end_datetime)\n            else:\n                _res = []\n\n                def _get_multi(start_, end_):\n                    _resp = _get_simple(start_, end_)\n                    if _resp is not None and not _resp.empty:\n                        _res.append(_resp)\n\n                for _s, _e in (\n                    (self.start_datetime, self._next_datetime),\n                    (self._latest_datetime, self.end_datetime),\n                ):\n                    _get_multi(_s, _e)\n                for _start in pd.date_range(self._next_datetime, self._latest_datetime, closed=\"left\"):\n                    _end = _start + pd.Timedelta(days=1)\n                    _get_multi(_start, _end)\n                if _res:\n                    _result = pd.concat(_res, sort=False).sort_values([\"symbol\", \"date\"])\n        else:\n            raise ValueError(f\"cannot support {self.interval}\")\n        return pd.DataFrame() if _result is None else _result",
  "def collector_data(self):\n        \"\"\"collector data\"\"\"\n        super(YahooCollector, self).collector_data()\n        self.download_index_data()",
  "def download_index_data(self):\n        \"\"\"download index data\"\"\"\n        raise NotImplementedError(\"rewrite download_index_data\")",
  "def get_stock_list(self):\n        logger.info(\"get HS stock symbos......\")\n        symbols = get_hs_stock_symbols()\n        logger.info(f\"get {len(symbols)} symbols.\")\n        return symbols",
  "def normalize_symbol(self, symbol):\n        symbol_s = symbol.split(\".\")\n        symbol = f\"sh{symbol_s[0]}\" if symbol_s[-1] == \"ss\" else f\"sz{symbol_s[0]}\"\n        return symbol",
  "def _timezone(self):\n        return \"Asia/Shanghai\"",
  "def min_numbers_trading(self):\n        return 252 / 4",
  "def download_index_data(self):\n        # TODO: from MSN\n        _format = \"%Y%m%d\"\n        _begin = self.start_datetime.strftime(_format)\n        _end = (self.end_datetime + pd.Timedelta(days=-1)).strftime(_format)\n        for _index_name, _index_code in {\"csi300\": \"000300\", \"csi100\": \"000903\"}.items():\n            logger.info(f\"get bench data: {_index_name}({_index_code})......\")\n            try:\n                df = pd.DataFrame(\n                    map(\n                        lambda x: x.split(\",\"),\n                        requests.get(INDEX_BENCH_URL.format(index_code=_index_code, begin=_begin, end=_end)).json()[\n                            \"data\"\n                        ][\"klines\"],\n                    )\n                )\n            except Exception as e:\n                logger.warning(f\"get {_index_name} error: {e}\")\n                continue\n            df.columns = [\"date\", \"open\", \"close\", \"high\", \"low\", \"volume\", \"money\", \"change\"]\n            df[\"date\"] = pd.to_datetime(df[\"date\"])\n            df = df.astype(float, errors=\"ignore\")\n            df[\"adjclose\"] = df[\"close\"]\n            df[\"symbol\"] = f\"sh{_index_code}\"\n            _path = self.save_dir.joinpath(f\"sh{_index_code}.csv\")\n            if _path.exists():\n                _old_df = pd.read_csv(_path)\n                df = _old_df.append(df, sort=False)\n            df.to_csv(_path, index=False)\n            time.sleep(5)",
  "def min_numbers_trading(self):\n        return 60 * 4 * 5",
  "def download_index_data(self):\n        # TODO: 1m\n        logger.warning(f\"{self.__class__.__name__} {self.interval} does not support: download_index_data\")",
  "def get_stock_list(self):\n        logger.info(\"get US stock symbols......\")\n        symbols = get_us_stock_symbols() + [\n            \"^GSPC\",\n            \"^NDX\",\n            \"^DJI\",\n        ]\n        logger.info(f\"get {len(symbols)} symbols.\")\n        return symbols",
  "def download_index_data(self):\n        pass",
  "def normalize_symbol(self, symbol):\n        return code_to_fname(symbol).upper()",
  "def _timezone(self):\n        return \"America/New_York\"",
  "def min_numbers_trading(self):\n        return 252 / 4",
  "def min_numbers_trading(self):\n        return 60 * 6.5 * 5",
  "def normalize_yahoo(\n        df: pd.DataFrame,\n        calendar_list: list = None,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        if df.empty:\n            return df\n        symbol = df.loc[df[symbol_field_name].first_valid_index(), symbol_field_name]\n        columns = copy.deepcopy(YahooNormalize.COLUMNS)\n        df = df.copy()\n        df.set_index(date_field_name, inplace=True)\n        df.index = pd.to_datetime(df.index)\n        df = df[~df.index.duplicated(keep=\"first\")]\n        if calendar_list is not None:\n            df = df.reindex(\n                pd.DataFrame(index=calendar_list)\n                .loc[\n                    pd.Timestamp(df.index.min()).date() : pd.Timestamp(df.index.max()).date()\n                    + pd.Timedelta(hours=23, minutes=59)\n                ]\n                .index\n            )\n        df.sort_index(inplace=True)\n        df.loc[(df[\"volume\"] <= 0) | np.isnan(df[\"volume\"]), set(df.columns) - {symbol_field_name}] = np.nan\n        _tmp_series = df[\"close\"].fillna(method=\"ffill\")\n        df[\"change\"] = _tmp_series / _tmp_series.shift(1) - 1\n        columns += [\"change\"]\n        df.loc[(df[\"volume\"] <= 0) | np.isnan(df[\"volume\"]), columns] = np.nan\n\n        df[symbol_field_name] = symbol\n        df.index.names = [date_field_name]\n        return df.reset_index()",
  "def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n        # normalize\n        df = self.normalize_yahoo(df, self._calendar_list, self._date_field_name, self._symbol_field_name)\n        # adjusted price\n        df = self.adjusted_price(df)\n        return df",
  "def adjusted_price(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"adjusted price\"\"\"\n        raise NotImplementedError(\"rewrite adjusted_price\")",
  "def adjusted_price(self, df: pd.DataFrame) -> pd.DataFrame:\n        if df.empty:\n            return df\n        df = df.copy()\n        df.set_index(self._date_field_name, inplace=True)\n        if \"adjclose\" in df:\n            df[\"factor\"] = df[\"adjclose\"] / df[\"close\"]\n            df[\"factor\"] = df[\"factor\"].fillna(method=\"ffill\")\n        else:\n            df[\"factor\"] = 1\n        for _col in self.COLUMNS:\n            if _col not in df.columns:\n                continue\n            if _col == \"volume\":\n                df[_col] = df[_col] / df[\"factor\"]\n            else:\n                df[_col] = df[_col] * df[\"factor\"]\n        df.index.names = [self._date_field_name]\n        return df.reset_index()",
  "def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n        df = super(YahooNormalize1d, self).normalize(df)\n        df = self._manual_adj_data(df)\n        return df",
  "def _manual_adj_data(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"manual adjust data: All fields (except change) are standardized according to the close of the first day\"\"\"\n        if df.empty:\n            return df\n        df = df.copy()\n        df.sort_values(self._date_field_name, inplace=True)\n        df = df.set_index(self._date_field_name)\n        df = df.loc[df[\"close\"].first_valid_index() :]\n        _close = df[\"close\"].iloc[0]\n        for _col in df.columns:\n            if _col == self._symbol_field_name:\n                continue\n            if _col == \"volume\":\n                df[_col] = df[_col] * _close\n            elif _col != \"change\":\n                df[_col] = df[_col] / _close\n            else:\n                pass\n        return df.reset_index()",
  "def __init__(\n        self,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default is date\n        symbol_field_name: str\n            symbol field name, default is symbol\n        \"\"\"\n        super(YahooNormalize1min, self).__init__(date_field_name, symbol_field_name)\n        _class_name = self.__class__.__name__.replace(\"min\", \"d\")\n        _class = getattr(importlib.import_module(\"collector\"), _class_name)  # type: Type[YahooNormalize]\n        self.data_1d_obj = _class(self._date_field_name, self._symbol_field_name)",
  "def calendar_list_1d(self):\n        calendar_list_1d = getattr(self, \"_calendar_list_1d\", None)\n        if calendar_list_1d is None:\n            calendar_list_1d = self._get_1d_calendar_list()\n            setattr(self, \"_calendar_list_1d\", calendar_list_1d)\n        return calendar_list_1d",
  "def generate_1min_from_daily(self, calendars: Iterable) -> pd.Index:\n        res = []\n        daily_format = self.DAILY_FORMAT\n        am_range = self.AM_RANGE\n        pm_range = self.PM_RANGE\n        for _day in calendars:\n            for _range in [am_range, pm_range]:\n                res.append(\n                    pd.date_range(\n                        f\"{_day.strftime(daily_format)} {_range[0]}\",\n                        f\"{_day.strftime(daily_format)} {_range[1]}\",\n                        freq=\"1min\",\n                    )\n                )\n\n        return pd.Index(sorted(set(np.hstack(res))))",
  "def adjusted_price(self, df: pd.DataFrame) -> pd.DataFrame:\n        # TODO: using daily data factor\n        if df.empty:\n            return df\n        df = df.copy()\n        symbol = df.iloc[0][self._symbol_field_name]\n        # get 1d data from yahoo\n        _start = pd.Timestamp(df[self._date_field_name].min()).strftime(self.DAILY_FORMAT)\n        _end = (pd.Timestamp(df[self._date_field_name].max()) + pd.Timedelta(days=1)).strftime(self.DAILY_FORMAT)\n        data_1d = YahooCollector.get_data_from_remote(\n            self.symbol_to_yahoo(symbol), interval=\"1d\", start=_start, end=_end\n        )\n        if data_1d is None or data_1d.empty:\n            df[\"factor\"] = 1\n            # TODO: np.nan or 1 or 0\n            df[\"paused\"] = np.nan\n        else:\n            data_1d = self.data_1d_obj.normalize(data_1d)  # type: pd.DataFrame\n            # NOTE: volume is np.nan or volume <= 0, paused = 1\n            # FIXME: find a more accurate data source\n            data_1d[\"paused\"] = 0\n            data_1d.loc[(data_1d[\"volume\"].isna()) | (data_1d[\"volume\"] <= 0), \"paused\"] = 1\n            data_1d = data_1d.set_index(self._date_field_name)\n\n            # add factor from 1d data\n            df[\"date_tmp\"] = df[self._date_field_name].apply(lambda x: pd.Timestamp(x).date())\n            df.set_index(\"date_tmp\", inplace=True)\n            df.loc[:, \"factor\"] = data_1d[\"factor\"]\n            df.loc[:, \"paused\"] = data_1d[\"paused\"]\n            df.reset_index(\"date_tmp\", drop=True, inplace=True)\n\n            if self.CONSISTENT_1d:\n                # the date sequence is consistent with 1d\n                df.set_index(self._date_field_name, inplace=True)\n                df = df.reindex(\n                    self.generate_1min_from_daily(\n                        pd.to_datetime(data_1d.reset_index()[self._date_field_name].drop_duplicates())\n                    )\n                )\n                df[self._symbol_field_name] = df.loc[df[self._symbol_field_name].first_valid_index()][\n                    self._symbol_field_name\n                ]\n                df.index.names = [self._date_field_name]\n                df.reset_index(inplace=True)\n        for _col in self.COLUMNS:\n            if _col not in df.columns:\n                continue\n            if _col == \"volume\":\n                df[_col] = df[_col] / df[\"factor\"]\n            else:\n                df[_col] = df[_col] * df[\"factor\"]\n        return df",
  "def symbol_to_yahoo(self, symbol):\n        raise NotImplementedError(\"rewrite symbol_to_yahoo\")",
  "def _get_1d_calendar_list(self):\n        raise NotImplementedError(\"rewrite _get_1d_calendar_list\")",
  "def _get_calendar_list(self):\n        # TODO: from MSN\n        return get_calendar_list(\"US_ALL\")",
  "def _get_calendar_list(self):\n        # TODO: support 1min\n        raise ValueError(\"Does not support 1min\")",
  "def _get_1d_calendar_list(self):\n        return get_calendar_list(\"US_ALL\")",
  "def symbol_to_yahoo(self, symbol):\n        return fname_to_code(symbol)",
  "def _get_calendar_list(self):\n        # TODO: from MSN\n        return get_calendar_list(\"ALL\")",
  "def _get_calendar_list(self):\n        return self.generate_1min_from_daily(self.calendar_list_1d)",
  "def symbol_to_yahoo(self, symbol):\n        if \".\" not in symbol:\n            _exchange = symbol[:2]\n            _exchange = \"ss\" if _exchange == \"sh\" else _exchange\n            symbol = symbol[2:] + \".\" + _exchange\n        return symbol",
  "def _get_1d_calendar_list(self):\n        return get_calendar_list(\"ALL\")",
  "def __init__(self, source_dir=None, normalize_dir=None, max_workers=4, interval=\"1d\", region=REGION_CN):\n        \"\"\"\n\n        Parameters\n        ----------\n        source_dir: str\n            The directory where the raw data collected from the Internet is saved, default \"Path(__file__).parent/source\"\n        normalize_dir: str\n            Directory for normalize data, default \"Path(__file__).parent/normalize\"\n        max_workers: int\n            Concurrent number, default is 4\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        region: str\n            region, value from [\"CN\", \"US\"], default \"CN\"\n        \"\"\"\n        super().__init__(source_dir, normalize_dir, max_workers, interval)\n        self.region = region",
  "def collector_class_name(self):\n        return f\"YahooCollector{self.region.upper()}{self.interval}\"",
  "def normalize_class_name(self):\n        return f\"YahooNormalize{self.region.upper()}{self.interval}\"",
  "def default_base_dir(self) -> [Path, str]:\n        return CUR_DIR",
  "def download_data(\n        self,\n        max_collector_count=2,\n        delay=0,\n        start=None,\n        end=None,\n        interval=\"1d\",\n        check_data_length=False,\n        limit_nums=None,\n    ):\n        \"\"\"download data from Internet\n\n        Parameters\n        ----------\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        start: str\n            start datetime, default \"2000-01-01\"\n        end: str\n            end datetime, default ``pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))``\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n\n        Examples\n        ---------\n            # get daily data\n            $ python collector.py download_data --source_dir ~/.qlib/stock_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n            # get 1m data\n            $ python collector.py download_data --source_dir ~/.qlib/stock_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1m\n        \"\"\"\n\n        super(Run, self).download_data(max_collector_count, delay, start, end, interval, check_data_length, limit_nums)",
  "def normalize_data(self, date_field_name: str = \"date\", symbol_field_name: str = \"symbol\"):\n        \"\"\"normalize data\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default date\n        symbol_field_name: str\n            symbol field name, default symbol\n\n        Examples\n        ---------\n            $ python collector.py normalize_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d\n        \"\"\"\n        super(Run, self).normalize_data(date_field_name, symbol_field_name)",
  "def _show_logging_func():\n            if interval == YahooCollector.INTERVAL_1min and show_1min_logging:\n                logger.warning(f\"{error_msg}:{_resp}\")",
  "def _get_simple(start_, end_):\n            self.sleep()\n            _remote_interval = \"1m\" if interval == self.INTERVAL_1min else interval\n            return self.get_data_from_remote(\n                symbol,\n                interval=_remote_interval,\n                start=start_,\n                end=end_,\n            )",
  "def _get_multi(start_, end_):\n                    _resp = _get_simple(start_, end_)\n                    if _resp is not None and not _resp.empty:\n                        _res.append(_resp)",
  "class WIKIIndex(IndexBase):\n    # NOTE: The US stock code contains \"PRN\", and the directory cannot be created on Windows system, use the \"_\" prefix\n    # https://superuser.com/questions/613313/why-cant-we-make-con-prn-null-folder-in-windows\n    INST_PREFIX = \"_\"\n\n    def __init__(self, index_name: str, qlib_dir: [str, Path] = None, request_retry: int = 5, retry_sleep: int = 3):\n        super(WIKIIndex, self).__init__(\n            index_name=index_name, qlib_dir=qlib_dir, request_retry=request_retry, retry_sleep=retry_sleep\n        )\n\n        self._target_url = f\"{WIKI_URL}/{WIKI_INDEX_NAME_MAP[self.index_name.upper()]}\"\n\n    @property\n    @abc.abstractmethod\n    def bench_start_date(self) -> pd.Timestamp:\n        \"\"\"\n        Returns\n        -------\n            index start date\n        \"\"\"\n        raise NotImplementedError(\"rewrite bench_start_date\")\n\n    @abc.abstractmethod\n    def get_changes(self) -> pd.DataFrame:\n        \"\"\"get companies changes\n\n        Returns\n        -------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n        \"\"\"\n        raise NotImplementedError(\"rewrite get_changes\")\n\n    @property\n    def calendar_list(self) -> List[pd.Timestamp]:\n        \"\"\"get history trading date\n\n        Returns\n        -------\n            calendar list\n        \"\"\"\n        _calendar_list = getattr(self, \"_calendar_list\", None)\n        if _calendar_list is None:\n            _calendar_list = list(filter(lambda x: x >= self.bench_start_date, get_calendar_list(\"US_ALL\")))\n            setattr(self, \"_calendar_list\", _calendar_list)\n        return _calendar_list\n\n    def _request_new_companies(self) -> requests.Response:\n        resp = requests.get(self._target_url)\n        if resp.status_code != 200:\n            raise ValueError(f\"request error: {self._target_url}\")\n\n        return resp\n\n    def set_default_date_range(self, df: pd.DataFrame) -> pd.DataFrame:\n        _df = df.copy()\n        _df[self.SYMBOL_FIELD_NAME] = _df[self.SYMBOL_FIELD_NAME].str.strip()\n        _df[self.START_DATE_FIELD] = self.bench_start_date\n        _df[self.END_DATE_FIELD] = self.DEFAULT_END_DATE\n        return _df.loc[:, self.INSTRUMENTS_COLUMNS]\n\n    def get_new_companies(self):\n        logger.info(f\"get new companies {self.index_name} ......\")\n        _data = deco_retry(retry=self._request_retry, retry_sleep=self._retry_sleep)(self._request_new_companies)()\n        df_list = pd.read_html(_data.text)\n        for _df in df_list:\n            _df = self.filter_df(_df)\n            if (_df is not None) and (not _df.empty):\n                _df.columns = [self.SYMBOL_FIELD_NAME]\n                _df = self.set_default_date_range(_df)\n                logger.info(f\"end of get new companies {self.index_name} ......\")\n                return _df\n\n    def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        raise NotImplementedError(\"rewrite filter_df\")",
  "class NASDAQ100Index(WIKIIndex):\n\n    HISTORY_COMPANIES_URL = (\n        \"https://indexes.nasdaqomx.com/Index/WeightingData?id=NDX&tradeDate={trade_date}T00%3A00%3A00.000&timeOfDay=SOD\"\n    )\n    MAX_WORKERS = 16\n\n    def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if not (set(df.columns) - {\"Company\", \"Ticker\"}):\n            return df.loc[:, [\"Ticker\"]].copy()\n\n    @property\n    def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2003-01-02\")\n\n    @deco_retry\n    def _request_history_companies(self, trade_date: pd.Timestamp, use_cache: bool = True) -> pd.DataFrame:\n        trade_date = trade_date.strftime(\"%Y-%m-%d\")\n        cache_path = self.cache_dir.joinpath(f\"{trade_date}_history_companies.pkl\")\n        if cache_path.exists() and use_cache:\n            df = pd.read_pickle(cache_path)\n        else:\n            url = self.HISTORY_COMPANIES_URL.format(trade_date=trade_date)\n            resp = requests.post(url)\n            if resp.status_code != 200:\n                raise ValueError(f\"request error: {url}\")\n            df = pd.DataFrame(resp.json()[\"aaData\"])\n            df[self.DATE_FIELD_NAME] = trade_date\n            df.rename(columns={\"Name\": \"name\", \"Symbol\": self.SYMBOL_FIELD_NAME}, inplace=True)\n            if not df.empty:\n                df.to_pickle(cache_path)\n        return df\n\n    def get_history_companies(self):\n        logger.info(f\"start get history companies......\")\n        all_history = []\n        error_list = []\n        with tqdm(total=len(self.calendar_list)) as p_bar:\n            with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:\n                for _trading_date, _df in zip(\n                    self.calendar_list, executor.map(self._request_history_companies, self.calendar_list)\n                ):\n                    if _df.empty:\n                        error_list.append(_trading_date)\n                    else:\n                        all_history.append(_df)\n                    p_bar.update()\n\n        if error_list:\n            logger.warning(f\"get error: {error_list}\")\n        logger.info(f\"total {len(self.calendar_list)}, error {len(error_list)}\")\n        logger.info(f\"end of get history companies.\")\n        return pd.concat(all_history, sort=False)\n\n    def get_changes(self):\n        return self.get_changes_with_history_companies(self.get_history_companies())",
  "class DJIAIndex(WIKIIndex):\n    @property\n    def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2000-01-01\")\n\n    def get_changes(self) -> pd.DataFrame:\n        pass\n\n    def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"Symbol\" in df.columns:\n            _df = df.loc[:, [\"Symbol\"]].copy()\n            _df[\"Symbol\"] = _df[\"Symbol\"].apply(lambda x: x.split(\":\")[-1])\n            return _df\n\n    def parse_instruments(self):\n        logger.warning(f\"No suitable data source has been found!\")",
  "class SP500Index(WIKIIndex):\n    WIKISP500_CHANGES_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\n    @property\n    def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"1999-01-01\")\n\n    def get_changes(self) -> pd.DataFrame:\n        logger.info(f\"get sp500 history changes......\")\n        # NOTE: may update the index of the table\n        changes_df = pd.read_html(self.WIKISP500_CHANGES_URL)[-1]\n        changes_df = changes_df.iloc[:, [0, 1, 3]]\n        changes_df.columns = [self.DATE_FIELD_NAME, self.ADD, self.REMOVE]\n        changes_df[self.DATE_FIELD_NAME] = pd.to_datetime(changes_df[self.DATE_FIELD_NAME])\n        _result = []\n        for _type in [self.ADD, self.REMOVE]:\n            _df = changes_df.copy()\n            _df[self.CHANGE_TYPE_FIELD] = _type\n            _df[self.SYMBOL_FIELD_NAME] = _df[_type]\n            _df.dropna(subset=[self.SYMBOL_FIELD_NAME], inplace=True)\n            if _type == self.ADD:\n                _df[self.DATE_FIELD_NAME] = _df[self.DATE_FIELD_NAME].apply(\n                    lambda x: get_trading_date_by_shift(self.calendar_list, x, 0)\n                )\n            else:\n                _df[self.DATE_FIELD_NAME] = _df[self.DATE_FIELD_NAME].apply(\n                    lambda x: get_trading_date_by_shift(self.calendar_list, x, -1)\n                )\n            _result.append(_df[[self.DATE_FIELD_NAME, self.CHANGE_TYPE_FIELD, self.SYMBOL_FIELD_NAME]])\n        logger.info(f\"end of get sp500 history changes.\")\n        return pd.concat(_result, sort=False)\n\n    def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"Symbol\" in df.columns:\n            return df.loc[:, [\"Symbol\"]].copy()",
  "class SP400Index(WIKIIndex):\n    @property\n    def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2000-01-01\")\n\n    def get_changes(self) -> pd.DataFrame:\n        pass\n\n    def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"Ticker symbol\" in df.columns:\n            return df.loc[:, [\"Ticker symbol\"]].copy()\n\n    def parse_instruments(self):\n        logger.warning(f\"No suitable data source has been found!\")",
  "def get_instruments(\n    qlib_dir: str, index_name: str, method: str = \"parse_instruments\", request_retry: int = 5, retry_sleep: int = 3\n):\n    \"\"\"\n\n    Parameters\n    ----------\n    qlib_dir: str\n        qlib data dir, default \"Path(__file__).parent/qlib_data\"\n    index_name: str\n        index name, value from [\"SP500\", \"NASDAQ100\", \"DJIA\", \"SP400\"]\n    method: str\n        method, value from [\"parse_instruments\", \"save_new_companies\"]\n    request_retry: int\n        request retry, by default 5\n    retry_sleep: int\n        request sleep, by default 3\n\n    Examples\n    -------\n        # parse instruments\n        $ python collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n        # parse new companies\n        $ python collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n    \"\"\"\n    _cur_module = importlib.import_module(\"collector\")\n    obj = getattr(_cur_module, f\"{index_name.upper()}Index\")(\n        qlib_dir=qlib_dir, index_name=index_name, request_retry=request_retry, retry_sleep=retry_sleep\n    )\n    getattr(obj, method)()",
  "def __init__(self, index_name: str, qlib_dir: [str, Path] = None, request_retry: int = 5, retry_sleep: int = 3):\n        super(WIKIIndex, self).__init__(\n            index_name=index_name, qlib_dir=qlib_dir, request_retry=request_retry, retry_sleep=retry_sleep\n        )\n\n        self._target_url = f\"{WIKI_URL}/{WIKI_INDEX_NAME_MAP[self.index_name.upper()]}\"",
  "def bench_start_date(self) -> pd.Timestamp:\n        \"\"\"\n        Returns\n        -------\n            index start date\n        \"\"\"\n        raise NotImplementedError(\"rewrite bench_start_date\")",
  "def get_changes(self) -> pd.DataFrame:\n        \"\"\"get companies changes\n\n        Returns\n        -------\n            pd.DataFrame:\n                symbol      date        type\n                SH600000  2019-11-11    add\n                SH600000  2020-11-10    remove\n            dtypes:\n                symbol: str\n                date: pd.Timestamp\n                type: str, value from [\"add\", \"remove\"]\n        \"\"\"\n        raise NotImplementedError(\"rewrite get_changes\")",
  "def calendar_list(self) -> List[pd.Timestamp]:\n        \"\"\"get history trading date\n\n        Returns\n        -------\n            calendar list\n        \"\"\"\n        _calendar_list = getattr(self, \"_calendar_list\", None)\n        if _calendar_list is None:\n            _calendar_list = list(filter(lambda x: x >= self.bench_start_date, get_calendar_list(\"US_ALL\")))\n            setattr(self, \"_calendar_list\", _calendar_list)\n        return _calendar_list",
  "def _request_new_companies(self) -> requests.Response:\n        resp = requests.get(self._target_url)\n        if resp.status_code != 200:\n            raise ValueError(f\"request error: {self._target_url}\")\n\n        return resp",
  "def set_default_date_range(self, df: pd.DataFrame) -> pd.DataFrame:\n        _df = df.copy()\n        _df[self.SYMBOL_FIELD_NAME] = _df[self.SYMBOL_FIELD_NAME].str.strip()\n        _df[self.START_DATE_FIELD] = self.bench_start_date\n        _df[self.END_DATE_FIELD] = self.DEFAULT_END_DATE\n        return _df.loc[:, self.INSTRUMENTS_COLUMNS]",
  "def get_new_companies(self):\n        logger.info(f\"get new companies {self.index_name} ......\")\n        _data = deco_retry(retry=self._request_retry, retry_sleep=self._retry_sleep)(self._request_new_companies)()\n        df_list = pd.read_html(_data.text)\n        for _df in df_list:\n            _df = self.filter_df(_df)\n            if (_df is not None) and (not _df.empty):\n                _df.columns = [self.SYMBOL_FIELD_NAME]\n                _df = self.set_default_date_range(_df)\n                logger.info(f\"end of get new companies {self.index_name} ......\")\n                return _df",
  "def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        raise NotImplementedError(\"rewrite filter_df\")",
  "def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if not (set(df.columns) - {\"Company\", \"Ticker\"}):\n            return df.loc[:, [\"Ticker\"]].copy()",
  "def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2003-01-02\")",
  "def _request_history_companies(self, trade_date: pd.Timestamp, use_cache: bool = True) -> pd.DataFrame:\n        trade_date = trade_date.strftime(\"%Y-%m-%d\")\n        cache_path = self.cache_dir.joinpath(f\"{trade_date}_history_companies.pkl\")\n        if cache_path.exists() and use_cache:\n            df = pd.read_pickle(cache_path)\n        else:\n            url = self.HISTORY_COMPANIES_URL.format(trade_date=trade_date)\n            resp = requests.post(url)\n            if resp.status_code != 200:\n                raise ValueError(f\"request error: {url}\")\n            df = pd.DataFrame(resp.json()[\"aaData\"])\n            df[self.DATE_FIELD_NAME] = trade_date\n            df.rename(columns={\"Name\": \"name\", \"Symbol\": self.SYMBOL_FIELD_NAME}, inplace=True)\n            if not df.empty:\n                df.to_pickle(cache_path)\n        return df",
  "def get_history_companies(self):\n        logger.info(f\"start get history companies......\")\n        all_history = []\n        error_list = []\n        with tqdm(total=len(self.calendar_list)) as p_bar:\n            with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:\n                for _trading_date, _df in zip(\n                    self.calendar_list, executor.map(self._request_history_companies, self.calendar_list)\n                ):\n                    if _df.empty:\n                        error_list.append(_trading_date)\n                    else:\n                        all_history.append(_df)\n                    p_bar.update()\n\n        if error_list:\n            logger.warning(f\"get error: {error_list}\")\n        logger.info(f\"total {len(self.calendar_list)}, error {len(error_list)}\")\n        logger.info(f\"end of get history companies.\")\n        return pd.concat(all_history, sort=False)",
  "def get_changes(self):\n        return self.get_changes_with_history_companies(self.get_history_companies())",
  "def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2000-01-01\")",
  "def get_changes(self) -> pd.DataFrame:\n        pass",
  "def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"Symbol\" in df.columns:\n            _df = df.loc[:, [\"Symbol\"]].copy()\n            _df[\"Symbol\"] = _df[\"Symbol\"].apply(lambda x: x.split(\":\")[-1])\n            return _df",
  "def parse_instruments(self):\n        logger.warning(f\"No suitable data source has been found!\")",
  "def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"1999-01-01\")",
  "def get_changes(self) -> pd.DataFrame:\n        logger.info(f\"get sp500 history changes......\")\n        # NOTE: may update the index of the table\n        changes_df = pd.read_html(self.WIKISP500_CHANGES_URL)[-1]\n        changes_df = changes_df.iloc[:, [0, 1, 3]]\n        changes_df.columns = [self.DATE_FIELD_NAME, self.ADD, self.REMOVE]\n        changes_df[self.DATE_FIELD_NAME] = pd.to_datetime(changes_df[self.DATE_FIELD_NAME])\n        _result = []\n        for _type in [self.ADD, self.REMOVE]:\n            _df = changes_df.copy()\n            _df[self.CHANGE_TYPE_FIELD] = _type\n            _df[self.SYMBOL_FIELD_NAME] = _df[_type]\n            _df.dropna(subset=[self.SYMBOL_FIELD_NAME], inplace=True)\n            if _type == self.ADD:\n                _df[self.DATE_FIELD_NAME] = _df[self.DATE_FIELD_NAME].apply(\n                    lambda x: get_trading_date_by_shift(self.calendar_list, x, 0)\n                )\n            else:\n                _df[self.DATE_FIELD_NAME] = _df[self.DATE_FIELD_NAME].apply(\n                    lambda x: get_trading_date_by_shift(self.calendar_list, x, -1)\n                )\n            _result.append(_df[[self.DATE_FIELD_NAME, self.CHANGE_TYPE_FIELD, self.SYMBOL_FIELD_NAME]])\n        logger.info(f\"end of get sp500 history changes.\")\n        return pd.concat(_result, sort=False)",
  "def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"Symbol\" in df.columns:\n            return df.loc[:, [\"Symbol\"]].copy()",
  "def bench_start_date(self) -> pd.Timestamp:\n        return pd.Timestamp(\"2000-01-01\")",
  "def get_changes(self) -> pd.DataFrame:\n        pass",
  "def filter_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"Ticker symbol\" in df.columns:\n            return df.loc[:, [\"Ticker symbol\"]].copy()",
  "def parse_instruments(self):\n        logger.warning(f\"No suitable data source has been found!\")",
  "class FundCollector(BaseCollector):\n    def __init__(\n        self,\n        save_dir: [str, Path],\n        start=None,\n        end=None,\n        interval=\"1d\",\n        max_workers=4,\n        max_collector_count=2,\n        delay=0,\n        check_data_length: bool = False,\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        save_dir: str\n            fund save dir\n        max_workers: int\n            workers, default 4\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1min\n        start: str\n            start datetime, default None\n        end: str\n            end datetime, default None\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n        \"\"\"\n        super(FundCollector, self).__init__(\n            save_dir=save_dir,\n            start=start,\n            end=end,\n            interval=interval,\n            max_workers=max_workers,\n            max_collector_count=max_collector_count,\n            delay=delay,\n            check_data_length=check_data_length,\n            limit_nums=limit_nums,\n        )\n\n        self.init_datetime()\n\n    def init_datetime(self):\n        if self.interval == self.INTERVAL_1min:\n            self.start_datetime = max(self.start_datetime, self.DEFAULT_START_DATETIME_1MIN)\n        elif self.interval == self.INTERVAL_1d:\n            pass\n        else:\n            raise ValueError(f\"interval error: {self.interval}\")\n\n        self.start_datetime = self.convert_datetime(self.start_datetime, self._timezone)\n        self.end_datetime = self.convert_datetime(self.end_datetime, self._timezone)\n\n    @staticmethod\n    def convert_datetime(dt: [pd.Timestamp, datetime.date, str], timezone):\n        try:\n            dt = pd.Timestamp(dt, tz=timezone).timestamp()\n            dt = pd.Timestamp(dt, tz=tzlocal(), unit=\"s\")\n        except ValueError as e:\n            pass\n        return dt\n\n    @property\n    @abc.abstractmethod\n    def _timezone(self):\n        raise NotImplementedError(\"rewrite get_timezone\")\n\n    @staticmethod\n    def get_data_from_remote(symbol, interval, start, end):\n        error_msg = f\"{symbol}-{interval}-{start}-{end}\"\n\n        try:\n            # TODO: numberOfHistoricalDaysToCrawl should be bigger enouhg\n            url = INDEX_BENCH_URL.format(\n                index_code=symbol, numberOfHistoricalDaysToCrawl=10000, startDate=start, endDate=end\n            )\n            resp = requests.get(url, headers={\"referer\": \"http://fund.eastmoney.com/110022.html\"})\n\n            if resp.status_code != 200:\n                raise ValueError(\"request error\")\n\n            data = json.loads(resp.text.split(\"(\")[-1].split(\")\")[0])\n\n            # Some funds don't show the net value, example: http://fundf10.eastmoney.com/jjjz_010288.html\n            SYType = data[\"Data\"][\"SYType\"]\n            if (SYType == \"\u6bcf\u4e07\u4efd\u6536\u76ca\") or (SYType == \"\u6bcf\u767e\u4efd\u6536\u76ca\") or (SYType == \"\u6bcf\u767e\u4e07\u4efd\u6536\u76ca\"):\n                raise Exception(\"The fund contains \u6bcf*\u4efd\u6536\u76ca\")\n\n            # TODO: should we sort the value by datetime?\n            _resp = pd.DataFrame(data[\"Data\"][\"LSJZList\"])\n\n            if isinstance(_resp, pd.DataFrame):\n                return _resp.reset_index()\n        except Exception as e:\n            logger.warning(f\"{error_msg}:{e}\")\n\n    def get_data(\n        self, symbol: str, interval: str, start_datetime: pd.Timestamp, end_datetime: pd.Timestamp\n    ) -> [pd.DataFrame]:\n        def _get_simple(start_, end_):\n            self.sleep()\n            _remote_interval = interval\n            return self.get_data_from_remote(\n                symbol,\n                interval=_remote_interval,\n                start=start_,\n                end=end_,\n            )\n\n        if interval == self.INTERVAL_1d:\n            _result = _get_simple(start_datetime, end_datetime)\n        else:\n            raise ValueError(f\"cannot support {interval}\")\n        return _result",
  "class FundollectorCN(FundCollector, ABC):\n    def get_instrument_list(self):\n        logger.info(\"get cn fund symbols......\")\n        symbols = get_en_fund_symbols()\n        logger.info(f\"get {len(symbols)} symbols.\")\n        return symbols\n\n    def normalize_symbol(self, symbol):\n        return symbol\n\n    @property\n    def _timezone(self):\n        return \"Asia/Shanghai\"",
  "class FundCollectorCN1d(FundollectorCN):\n    @property\n    def min_numbers_trading(self):\n        return 252 / 4",
  "class FundNormalize(BaseNormalize):\n    DAILY_FORMAT = \"%Y-%m-%d\"\n\n    @staticmethod\n    def normalize_fund(\n        df: pd.DataFrame,\n        calendar_list: list = None,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        if df.empty:\n            return df\n        df = df.copy()\n        df.set_index(date_field_name, inplace=True)\n        df.index = pd.to_datetime(df.index)\n        df = df[~df.index.duplicated(keep=\"first\")]\n        if calendar_list is not None:\n            df = df.reindex(\n                pd.DataFrame(index=calendar_list)\n                .loc[\n                    pd.Timestamp(df.index.min()).date() : pd.Timestamp(df.index.max()).date()\n                    + pd.Timedelta(hours=23, minutes=59)\n                ]\n                .index\n            )\n        df.sort_index(inplace=True)\n\n        df.index.names = [date_field_name]\n        return df.reset_index()\n\n    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n        # normalize\n        df = self.normalize_fund(df, self._calendar_list, self._date_field_name, self._symbol_field_name)\n        return df",
  "class FundNormalize1d(FundNormalize):\n    pass",
  "class FundNormalizeCN:\n    def _get_calendar_list(self):\n        return get_calendar_list(\"ALL\")",
  "class FundNormalizeCN1d(FundNormalizeCN, FundNormalize1d):\n    pass",
  "class Run(BaseRun):\n    def __init__(self, source_dir=None, normalize_dir=None, max_workers=4, interval=\"1d\", region=REGION_CN):\n        \"\"\"\n\n        Parameters\n        ----------\n        source_dir: str\n            The directory where the raw data collected from the Internet is saved, default \"Path(__file__).parent/source\"\n        normalize_dir: str\n            Directory for normalize data, default \"Path(__file__).parent/normalize\"\n        max_workers: int\n            Concurrent number, default is 4\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        region: str\n            region, value from [\"CN\"], default \"CN\"\n        \"\"\"\n        super().__init__(source_dir, normalize_dir, max_workers, interval)\n        self.region = region\n\n    @property\n    def collector_class_name(self):\n        return f\"FundCollector{self.region.upper()}{self.interval}\"\n\n    @property\n    def normalize_class_name(self):\n        return f\"FundNormalize{self.region.upper()}{self.interval}\"\n\n    @property\n    def default_base_dir(self) -> [Path, str]:\n        return CUR_DIR\n\n    def download_data(\n        self,\n        max_collector_count=2,\n        delay=0,\n        start=None,\n        end=None,\n        interval=\"1d\",\n        check_data_length=False,\n        limit_nums=None,\n    ):\n        \"\"\"download data from Internet\n\n        Parameters\n        ----------\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        start: str\n            start datetime, default \"2000-01-01\"\n        end: str\n            end datetime, default ``pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))``\n        check_data_length: bool # if this param useful?\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n\n        Examples\n        ---------\n            # get daily data\n            $ python collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_1d --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n        \"\"\"\n\n        super(Run, self).download_data(max_collector_count, delay, start, end, interval, check_data_length, limit_nums)\n\n    def normalize_data(self, date_field_name: str = \"date\", symbol_field_name: str = \"symbol\"):\n        \"\"\"normalize data\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default date\n        symbol_field_name: str\n            symbol field name, default symbol\n\n        Examples\n        ---------\n            $ python collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_1d --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n        \"\"\"\n        super(Run, self).normalize_data(date_field_name, symbol_field_name)",
  "def __init__(\n        self,\n        save_dir: [str, Path],\n        start=None,\n        end=None,\n        interval=\"1d\",\n        max_workers=4,\n        max_collector_count=2,\n        delay=0,\n        check_data_length: bool = False,\n        limit_nums: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        save_dir: str\n            fund save dir\n        max_workers: int\n            workers, default 4\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1min\n        start: str\n            start datetime, default None\n        end: str\n            end datetime, default None\n        check_data_length: bool\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n        \"\"\"\n        super(FundCollector, self).__init__(\n            save_dir=save_dir,\n            start=start,\n            end=end,\n            interval=interval,\n            max_workers=max_workers,\n            max_collector_count=max_collector_count,\n            delay=delay,\n            check_data_length=check_data_length,\n            limit_nums=limit_nums,\n        )\n\n        self.init_datetime()",
  "def init_datetime(self):\n        if self.interval == self.INTERVAL_1min:\n            self.start_datetime = max(self.start_datetime, self.DEFAULT_START_DATETIME_1MIN)\n        elif self.interval == self.INTERVAL_1d:\n            pass\n        else:\n            raise ValueError(f\"interval error: {self.interval}\")\n\n        self.start_datetime = self.convert_datetime(self.start_datetime, self._timezone)\n        self.end_datetime = self.convert_datetime(self.end_datetime, self._timezone)",
  "def convert_datetime(dt: [pd.Timestamp, datetime.date, str], timezone):\n        try:\n            dt = pd.Timestamp(dt, tz=timezone).timestamp()\n            dt = pd.Timestamp(dt, tz=tzlocal(), unit=\"s\")\n        except ValueError as e:\n            pass\n        return dt",
  "def _timezone(self):\n        raise NotImplementedError(\"rewrite get_timezone\")",
  "def get_data_from_remote(symbol, interval, start, end):\n        error_msg = f\"{symbol}-{interval}-{start}-{end}\"\n\n        try:\n            # TODO: numberOfHistoricalDaysToCrawl should be bigger enouhg\n            url = INDEX_BENCH_URL.format(\n                index_code=symbol, numberOfHistoricalDaysToCrawl=10000, startDate=start, endDate=end\n            )\n            resp = requests.get(url, headers={\"referer\": \"http://fund.eastmoney.com/110022.html\"})\n\n            if resp.status_code != 200:\n                raise ValueError(\"request error\")\n\n            data = json.loads(resp.text.split(\"(\")[-1].split(\")\")[0])\n\n            # Some funds don't show the net value, example: http://fundf10.eastmoney.com/jjjz_010288.html\n            SYType = data[\"Data\"][\"SYType\"]\n            if (SYType == \"\u6bcf\u4e07\u4efd\u6536\u76ca\") or (SYType == \"\u6bcf\u767e\u4efd\u6536\u76ca\") or (SYType == \"\u6bcf\u767e\u4e07\u4efd\u6536\u76ca\"):\n                raise Exception(\"The fund contains \u6bcf*\u4efd\u6536\u76ca\")\n\n            # TODO: should we sort the value by datetime?\n            _resp = pd.DataFrame(data[\"Data\"][\"LSJZList\"])\n\n            if isinstance(_resp, pd.DataFrame):\n                return _resp.reset_index()\n        except Exception as e:\n            logger.warning(f\"{error_msg}:{e}\")",
  "def get_data(\n        self, symbol: str, interval: str, start_datetime: pd.Timestamp, end_datetime: pd.Timestamp\n    ) -> [pd.DataFrame]:\n        def _get_simple(start_, end_):\n            self.sleep()\n            _remote_interval = interval\n            return self.get_data_from_remote(\n                symbol,\n                interval=_remote_interval,\n                start=start_,\n                end=end_,\n            )\n\n        if interval == self.INTERVAL_1d:\n            _result = _get_simple(start_datetime, end_datetime)\n        else:\n            raise ValueError(f\"cannot support {interval}\")\n        return _result",
  "def get_instrument_list(self):\n        logger.info(\"get cn fund symbols......\")\n        symbols = get_en_fund_symbols()\n        logger.info(f\"get {len(symbols)} symbols.\")\n        return symbols",
  "def normalize_symbol(self, symbol):\n        return symbol",
  "def _timezone(self):\n        return \"Asia/Shanghai\"",
  "def min_numbers_trading(self):\n        return 252 / 4",
  "def normalize_fund(\n        df: pd.DataFrame,\n        calendar_list: list = None,\n        date_field_name: str = \"date\",\n        symbol_field_name: str = \"symbol\",\n    ):\n        if df.empty:\n            return df\n        df = df.copy()\n        df.set_index(date_field_name, inplace=True)\n        df.index = pd.to_datetime(df.index)\n        df = df[~df.index.duplicated(keep=\"first\")]\n        if calendar_list is not None:\n            df = df.reindex(\n                pd.DataFrame(index=calendar_list)\n                .loc[\n                    pd.Timestamp(df.index.min()).date() : pd.Timestamp(df.index.max()).date()\n                    + pd.Timedelta(hours=23, minutes=59)\n                ]\n                .index\n            )\n        df.sort_index(inplace=True)\n\n        df.index.names = [date_field_name]\n        return df.reset_index()",
  "def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n        # normalize\n        df = self.normalize_fund(df, self._calendar_list, self._date_field_name, self._symbol_field_name)\n        return df",
  "def _get_calendar_list(self):\n        return get_calendar_list(\"ALL\")",
  "def __init__(self, source_dir=None, normalize_dir=None, max_workers=4, interval=\"1d\", region=REGION_CN):\n        \"\"\"\n\n        Parameters\n        ----------\n        source_dir: str\n            The directory where the raw data collected from the Internet is saved, default \"Path(__file__).parent/source\"\n        normalize_dir: str\n            Directory for normalize data, default \"Path(__file__).parent/normalize\"\n        max_workers: int\n            Concurrent number, default is 4\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        region: str\n            region, value from [\"CN\"], default \"CN\"\n        \"\"\"\n        super().__init__(source_dir, normalize_dir, max_workers, interval)\n        self.region = region",
  "def collector_class_name(self):\n        return f\"FundCollector{self.region.upper()}{self.interval}\"",
  "def normalize_class_name(self):\n        return f\"FundNormalize{self.region.upper()}{self.interval}\"",
  "def default_base_dir(self) -> [Path, str]:\n        return CUR_DIR",
  "def download_data(\n        self,\n        max_collector_count=2,\n        delay=0,\n        start=None,\n        end=None,\n        interval=\"1d\",\n        check_data_length=False,\n        limit_nums=None,\n    ):\n        \"\"\"download data from Internet\n\n        Parameters\n        ----------\n        max_collector_count: int\n            default 2\n        delay: float\n            time.sleep(delay), default 0\n        interval: str\n            freq, value from [1min, 1d], default 1d\n        start: str\n            start datetime, default \"2000-01-01\"\n        end: str\n            end datetime, default ``pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))``\n        check_data_length: bool # if this param useful?\n            check data length, by default False\n        limit_nums: int\n            using for debug, by default None\n\n        Examples\n        ---------\n            # get daily data\n            $ python collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_1d --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n        \"\"\"\n\n        super(Run, self).download_data(max_collector_count, delay, start, end, interval, check_data_length, limit_nums)",
  "def normalize_data(self, date_field_name: str = \"date\", symbol_field_name: str = \"symbol\"):\n        \"\"\"normalize data\n\n        Parameters\n        ----------\n        date_field_name: str\n            date field name, default date\n        symbol_field_name: str\n            symbol field name, default symbol\n\n        Examples\n        ---------\n            $ python collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_1d --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n        \"\"\"\n        super(Run, self).normalize_data(date_field_name, symbol_field_name)",
  "def _get_simple(start_, end_):\n            self.sleep()\n            _remote_interval = interval\n            return self.get_data_from_remote(\n                symbol,\n                interval=_remote_interval,\n                start=start_,\n                end=end_,\n            )",
  "class Config:\n    def __init__(self, default_conf):\n        self.__dict__[\"_default_config\"] = copy.deepcopy(default_conf)  # avoiding conflictions with __getattr__\n        self.reset()\n\n    def __getitem__(self, key):\n        return self.__dict__[\"_config\"][key]\n\n    def __getattr__(self, attr):\n        if attr in self.__dict__[\"_config\"]:\n            return self.__dict__[\"_config\"][attr]\n\n        raise AttributeError(f\"No such {attr} in self._config\")\n\n    def __setitem__(self, key, value):\n        self.__dict__[\"_config\"][key] = value\n\n    def __setattr__(self, attr, value):\n        self.__dict__[\"_config\"][attr] = value\n\n    def __contains__(self, item):\n        return item in self.__dict__[\"_config\"]\n\n    def __getstate__(self):\n        return self.__dict__\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n\n    def __str__(self):\n        return str(self.__dict__[\"_config\"])\n\n    def __repr__(self):\n        return str(self.__dict__[\"_config\"])\n\n    def reset(self):\n        self.__dict__[\"_config\"] = copy.deepcopy(self._default_config)\n\n    def update(self, *args, **kwargs):\n        self.__dict__[\"_config\"].update(*args, **kwargs)\n\n    def set_conf_from_C(self, config_c):\n        self.update(**config_c.__dict__[\"_config\"])",
  "class QlibConfig(Config):\n    # URI_TYPE\n    LOCAL_URI = \"local\"\n    NFS_URI = \"nfs\"\n\n    def __init__(self, default_conf):\n        super().__init__(default_conf)\n        self._registered = False\n\n    def set_mode(self, mode):\n        # raise KeyError\n        self.update(MODE_CONF[mode])\n        # TODO: update region based on kwargs\n\n    def set_region(self, region):\n        # raise KeyError\n        self.update(_default_region_config[region])\n\n    def resolve_path(self):\n        # resolve path\n        if self[\"mount_path\"] is not None:\n            self[\"mount_path\"] = str(Path(self[\"mount_path\"]).expanduser().resolve())\n\n        if self.get_uri_type() == QlibConfig.LOCAL_URI:\n            self[\"provider_uri\"] = str(Path(self[\"provider_uri\"]).expanduser().resolve())\n\n    def get_uri_type(self):\n        is_win = re.match(\"^[a-zA-Z]:.*\", self[\"provider_uri\"]) is not None  # such as 'C:\\\\data', 'D:'\n        is_nfs_or_win = (\n            re.match(\"^[^/]+:.+\", self[\"provider_uri\"]) is not None\n        )  # such as 'host:/data/'   (User may define short hostname by themselves or use localhost)\n\n        if is_nfs_or_win and not is_win:\n            return QlibConfig.NFS_URI\n        else:\n            return QlibConfig.LOCAL_URI\n\n    def get_data_path(self):\n        if self.get_uri_type() == QlibConfig.LOCAL_URI:\n            return self[\"provider_uri\"]\n        elif self.get_uri_type() == QlibConfig.NFS_URI:\n            return self[\"mount_path\"]\n        else:\n            raise NotImplementedError(f\"This type of uri is not supported\")\n\n    def set(self, default_conf=\"client\", **kwargs):\n        from .utils import set_log_with_config, get_module_logger, can_use_cache\n\n        self.reset()\n\n        _logging_config = self.logging_config\n        if \"logging_config\" in kwargs:\n            _logging_config = kwargs[\"logging_config\"]\n\n        # set global config\n        if _logging_config:\n            set_log_with_config(_logging_config)\n\n        # FIXME: this logger ignored the level in config\n        logger = get_module_logger(\"Initialization\", level=logging.INFO)\n        logger.info(f\"default_conf: {default_conf}.\")\n\n        self.set_mode(default_conf)\n        self.set_region(kwargs.get(\"region\", self[\"region\"] if \"region\" in self else REG_CN))\n\n        for k, v in kwargs.items():\n            if k not in self:\n                logger.warning(\"Unrecognized config %s\" % k)\n            self[k] = v\n\n        self.resolve_path()\n\n        if not (self[\"expression_cache\"] is None and self[\"dataset_cache\"] is None):\n            # check redis\n            if not can_use_cache():\n                logger.warning(\n                    f\"redis connection failed(host={self['redis_host']} port={self['redis_port']}), cache will not be used!\"\n                )\n                self[\"expression_cache\"] = None\n                self[\"dataset_cache\"] = None\n\n    def register(self):\n        from .utils import init_instance_by_config\n        from .data.ops import register_all_ops\n        from .data.data import register_all_wrappers\n        from .workflow import R, QlibRecorder\n        from .workflow.utils import experiment_exit_handler\n\n        register_all_ops(self)\n        register_all_wrappers(self)\n        # set up QlibRecorder\n        exp_manager = init_instance_by_config(self[\"exp_manager\"])\n        qr = QlibRecorder(exp_manager)\n        R.register(qr)\n        # clean up experiment when python program ends\n        experiment_exit_handler()\n\n        self._registered = True\n\n    @property\n    def registered(self):\n        return self._registered",
  "def __init__(self, default_conf):\n        self.__dict__[\"_default_config\"] = copy.deepcopy(default_conf)  # avoiding conflictions with __getattr__\n        self.reset()",
  "def __getitem__(self, key):\n        return self.__dict__[\"_config\"][key]",
  "def __getattr__(self, attr):\n        if attr in self.__dict__[\"_config\"]:\n            return self.__dict__[\"_config\"][attr]\n\n        raise AttributeError(f\"No such {attr} in self._config\")",
  "def __setitem__(self, key, value):\n        self.__dict__[\"_config\"][key] = value",
  "def __setattr__(self, attr, value):\n        self.__dict__[\"_config\"][attr] = value",
  "def __contains__(self, item):\n        return item in self.__dict__[\"_config\"]",
  "def __getstate__(self):\n        return self.__dict__",
  "def __setstate__(self, state):\n        self.__dict__.update(state)",
  "def __str__(self):\n        return str(self.__dict__[\"_config\"])",
  "def __repr__(self):\n        return str(self.__dict__[\"_config\"])",
  "def reset(self):\n        self.__dict__[\"_config\"] = copy.deepcopy(self._default_config)",
  "def update(self, *args, **kwargs):\n        self.__dict__[\"_config\"].update(*args, **kwargs)",
  "def set_conf_from_C(self, config_c):\n        self.update(**config_c.__dict__[\"_config\"])",
  "def __init__(self, default_conf):\n        super().__init__(default_conf)\n        self._registered = False",
  "def set_mode(self, mode):\n        # raise KeyError\n        self.update(MODE_CONF[mode])",
  "def set_region(self, region):\n        # raise KeyError\n        self.update(_default_region_config[region])",
  "def resolve_path(self):\n        # resolve path\n        if self[\"mount_path\"] is not None:\n            self[\"mount_path\"] = str(Path(self[\"mount_path\"]).expanduser().resolve())\n\n        if self.get_uri_type() == QlibConfig.LOCAL_URI:\n            self[\"provider_uri\"] = str(Path(self[\"provider_uri\"]).expanduser().resolve())",
  "def get_uri_type(self):\n        is_win = re.match(\"^[a-zA-Z]:.*\", self[\"provider_uri\"]) is not None  # such as 'C:\\\\data', 'D:'\n        is_nfs_or_win = (\n            re.match(\"^[^/]+:.+\", self[\"provider_uri\"]) is not None\n        )  # such as 'host:/data/'   (User may define short hostname by themselves or use localhost)\n\n        if is_nfs_or_win and not is_win:\n            return QlibConfig.NFS_URI\n        else:\n            return QlibConfig.LOCAL_URI",
  "def get_data_path(self):\n        if self.get_uri_type() == QlibConfig.LOCAL_URI:\n            return self[\"provider_uri\"]\n        elif self.get_uri_type() == QlibConfig.NFS_URI:\n            return self[\"mount_path\"]\n        else:\n            raise NotImplementedError(f\"This type of uri is not supported\")",
  "def set(self, default_conf=\"client\", **kwargs):\n        from .utils import set_log_with_config, get_module_logger, can_use_cache\n\n        self.reset()\n\n        _logging_config = self.logging_config\n        if \"logging_config\" in kwargs:\n            _logging_config = kwargs[\"logging_config\"]\n\n        # set global config\n        if _logging_config:\n            set_log_with_config(_logging_config)\n\n        # FIXME: this logger ignored the level in config\n        logger = get_module_logger(\"Initialization\", level=logging.INFO)\n        logger.info(f\"default_conf: {default_conf}.\")\n\n        self.set_mode(default_conf)\n        self.set_region(kwargs.get(\"region\", self[\"region\"] if \"region\" in self else REG_CN))\n\n        for k, v in kwargs.items():\n            if k not in self:\n                logger.warning(\"Unrecognized config %s\" % k)\n            self[k] = v\n\n        self.resolve_path()\n\n        if not (self[\"expression_cache\"] is None and self[\"dataset_cache\"] is None):\n            # check redis\n            if not can_use_cache():\n                logger.warning(\n                    f\"redis connection failed(host={self['redis_host']} port={self['redis_port']}), cache will not be used!\"\n                )\n                self[\"expression_cache\"] = None\n                self[\"dataset_cache\"] = None",
  "def register(self):\n        from .utils import init_instance_by_config\n        from .data.ops import register_all_ops\n        from .data.data import register_all_wrappers\n        from .workflow import R, QlibRecorder\n        from .workflow.utils import experiment_exit_handler\n\n        register_all_ops(self)\n        register_all_wrappers(self)\n        # set up QlibRecorder\n        exp_manager = init_instance_by_config(self[\"exp_manager\"])\n        qr = QlibRecorder(exp_manager)\n        R.register(qr)\n        # clean up experiment when python program ends\n        experiment_exit_handler()\n\n        self._registered = True",
  "def registered(self):\n        return self._registered",
  "def get_module_logger(module_name, level: Optional[int] = None):\n    \"\"\"\n    Get a logger for a specific module.\n\n    :param module_name: str\n        Logic module name.\n    :param level: int\n    :return: Logger\n        Logger object.\n    \"\"\"\n    if level is None:\n        level = C.logging_level\n\n    module_name = \"qlib.{}\".format(module_name)\n    # Get logger.\n    module_logger = logging.getLogger(module_name)\n    module_logger.setLevel(level)\n    return module_logger",
  "class TimeInspector:\n\n    timer_logger = get_module_logger(\"timer\", level=logging.WARNING)\n\n    time_marks = []\n\n    @classmethod\n    def set_time_mark(cls):\n        \"\"\"\n        Set a time mark with current time, and this time mark will push into a stack.\n        :return: float\n            A timestamp for current time.\n        \"\"\"\n        _time = time()\n        cls.time_marks.append(_time)\n        return _time\n\n    @classmethod\n    def pop_time_mark(cls):\n        \"\"\"\n        Pop last time mark from stack.\n        \"\"\"\n        return cls.time_marks.pop()\n\n    @classmethod\n    def get_cost_time(cls):\n        \"\"\"\n        Get last time mark from stack, calculate time diff with current time.\n        :return: float\n            Time diff calculated by last time mark with current time.\n        \"\"\"\n        cost_time = time() - cls.time_marks.pop()\n        return cost_time\n\n    @classmethod\n    def log_cost_time(cls, info=\"Done\"):\n        \"\"\"\n        Get last time mark from stack, calculate time diff with current time, and log time diff and info.\n        :param info: str\n            Info that will be log into stdout.\n        \"\"\"\n        cost_time = time() - cls.time_marks.pop()\n        cls.timer_logger.info(\"Time cost: {0:.3f}s | {1}\".format(cost_time, info))\n\n    @classmethod\n    @contextmanager\n    def logt(cls, name=\"\", show_start=False):\n        \"\"\"logt.\n        Log the time of the inside code\n\n        Parameters\n        ----------\n        name :\n            name\n        show_start :\n            show_start\n        \"\"\"\n        if show_start:\n            cls.timer_logger.info(f\"{name} Begin\")\n        cls.set_time_mark()\n        try:\n            yield None\n        finally:\n            pass\n        cls.log_cost_time(info=f\"{name} Done\")",
  "def set_log_with_config(log_config: Dict[Text, Any]):\n    \"\"\"set log with config\n\n    :param log_config:\n    :return:\n    \"\"\"\n    logging_config.dictConfig(log_config)",
  "class LogFilter(logging.Filter):\n    def __init__(self, param=None):\n        self.param = param\n\n    @staticmethod\n    def match_msg(filter_str, msg):\n        match = False\n        try:\n            if re.match(filter_str, msg):\n                match = True\n        except Exception:\n            pass\n        return match\n\n    def filter(self, record):\n        allow = True\n        if isinstance(self.param, str):\n            allow = not self.match_msg(self.param, record.msg)\n        elif isinstance(self.param, list):\n            allow = not any([self.match_msg(p, record.msg) for p in self.param])\n        return allow",
  "def set_time_mark(cls):\n        \"\"\"\n        Set a time mark with current time, and this time mark will push into a stack.\n        :return: float\n            A timestamp for current time.\n        \"\"\"\n        _time = time()\n        cls.time_marks.append(_time)\n        return _time",
  "def pop_time_mark(cls):\n        \"\"\"\n        Pop last time mark from stack.\n        \"\"\"\n        return cls.time_marks.pop()",
  "def get_cost_time(cls):\n        \"\"\"\n        Get last time mark from stack, calculate time diff with current time.\n        :return: float\n            Time diff calculated by last time mark with current time.\n        \"\"\"\n        cost_time = time() - cls.time_marks.pop()\n        return cost_time",
  "def log_cost_time(cls, info=\"Done\"):\n        \"\"\"\n        Get last time mark from stack, calculate time diff with current time, and log time diff and info.\n        :param info: str\n            Info that will be log into stdout.\n        \"\"\"\n        cost_time = time() - cls.time_marks.pop()\n        cls.timer_logger.info(\"Time cost: {0:.3f}s | {1}\".format(cost_time, info))",
  "def logt(cls, name=\"\", show_start=False):\n        \"\"\"logt.\n        Log the time of the inside code\n\n        Parameters\n        ----------\n        name :\n            name\n        show_start :\n            show_start\n        \"\"\"\n        if show_start:\n            cls.timer_logger.info(f\"{name} Begin\")\n        cls.set_time_mark()\n        try:\n            yield None\n        finally:\n            pass\n        cls.log_cost_time(info=f\"{name} Done\")",
  "def __init__(self, param=None):\n        self.param = param",
  "def match_msg(filter_str, msg):\n        match = False\n        try:\n            if re.match(filter_str, msg):\n                match = True\n        except Exception:\n            pass\n        return match",
  "def filter(self, record):\n        allow = True\n        if isinstance(self.param, str):\n            allow = not self.match_msg(self.param, record.msg)\n        elif isinstance(self.param, list):\n            allow = not any([self.match_msg(p, record.msg) for p in self.param])\n        return allow",
  "def init(default_conf=\"client\", **kwargs):\n    from .config import C\n    from .log import get_module_logger\n    from .data.cache import H\n\n    H.clear()\n\n    # FIXME: this logger ignored the level in config\n    logger = get_module_logger(\"Initialization\", level=logging.INFO)\n\n    C.set(default_conf, **kwargs)\n\n    # check path if server/local\n    if C.get_uri_type() == C.LOCAL_URI:\n        if not os.path.exists(C[\"provider_uri\"]):\n            if C[\"auto_mount\"]:\n                logger.error(\n                    f\"Invalid provider uri: {C['provider_uri']}, please check if a valid provider uri has been set. This path does not exist.\"\n                )\n            else:\n                logger.warning(f\"auto_path is False, please make sure {C['mount_path']} is mounted\")\n    elif C.get_uri_type() == C.NFS_URI:\n        _mount_nfs_uri(C)\n    else:\n        raise NotImplementedError(f\"This type of URI is not supported\")\n\n    C.register()\n\n    if \"flask_server\" in C:\n        logger.info(f\"flask_server={C['flask_server']}, flask_port={C['flask_port']}\")\n    logger.info(\"qlib successfully initialized based on %s settings.\" % default_conf)\n    logger.info(f\"data_path={C.get_data_path()}\")",
  "def _mount_nfs_uri(C):\n    from .log import get_module_logger\n\n    LOG = get_module_logger(\"mount nfs\", level=logging.INFO)\n\n    # FIXME: the C[\"provider_uri\"] is modified in this function\n    # If it is not modified, we can pass only  provider_uri or mount_path instead of C\n    mount_command = \"sudo mount.nfs %s %s\" % (C[\"provider_uri\"], C[\"mount_path\"])\n    # If the provider uri looks like this 172.23.233.89//data/csdesign'\n    # It will be a nfs path. The client provider will be used\n    if not C[\"auto_mount\"]:\n        if not os.path.exists(C[\"mount_path\"]):\n            raise FileNotFoundError(\n                f\"Invalid mount path: {C['mount_path']}! Please mount manually: {mount_command} or Set init parameter `auto_mount=True`\"\n            )\n    else:\n        # Judging system type\n        sys_type = platform.system()\n        if \"win\" in sys_type.lower():\n            # system: window\n            exec_result = os.popen(\"mount -o anon %s %s\" % (C[\"provider_uri\"], C[\"mount_path\"] + \":\"))\n            result = exec_result.read()\n            if \"85\" in result:\n                LOG.warning(\"already mounted or window mount path already exists\")\n            elif \"53\" in result:\n                raise OSError(\"not find network path\")\n            elif \"error\" in result or \"\u9519\u8bef\" in result:\n                raise OSError(\"Invalid mount path\")\n            elif C[\"provider_uri\"] in result:\n                LOG.info(\"window success mount..\")\n            else:\n                raise OSError(f\"unknown error: {result}\")\n\n            # config mount path\n            C[\"mount_path\"] = C[\"mount_path\"] + \":\\\\\"\n        else:\n            # system: linux/Unix/Mac\n            # check mount\n            _remote_uri = C[\"provider_uri\"]\n            _remote_uri = _remote_uri[:-1] if _remote_uri.endswith(\"/\") else _remote_uri\n            _mount_path = C[\"mount_path\"]\n            _mount_path = _mount_path[:-1] if _mount_path.endswith(\"/\") else _mount_path\n            _check_level_num = 2\n            _is_mount = False\n            while _check_level_num:\n                with subprocess.Popen(\n                    'mount | grep \"{}\"'.format(_remote_uri),\n                    shell=True,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                ) as shell_r:\n                    _command_log = shell_r.stdout.readlines()\n                if len(_command_log) > 0:\n                    for _c in _command_log:\n                        _temp_mount = _c.decode(\"utf-8\").split(\" \")[2]\n                        _temp_mount = _temp_mount[:-1] if _temp_mount.endswith(\"/\") else _temp_mount\n                        if _temp_mount == _mount_path:\n                            _is_mount = True\n                            break\n                if _is_mount:\n                    break\n                _remote_uri = \"/\".join(_remote_uri.split(\"/\")[:-1])\n                _mount_path = \"/\".join(_mount_path.split(\"/\")[:-1])\n                _check_level_num -= 1\n\n            if not _is_mount:\n                try:\n                    os.makedirs(C[\"mount_path\"], exist_ok=True)\n                except Exception:\n                    raise OSError(\n                        f\"Failed to create directory {C['mount_path']}, please create {C['mount_path']} manually!\"\n                    )\n\n                # check nfs-common\n                command_res = os.popen(\"dpkg -l | grep nfs-common\")\n                command_res = command_res.readlines()\n                if not command_res:\n                    raise OSError(\"nfs-common is not found, please install it by execute: sudo apt install nfs-common\")\n                # manually mount\n                command_status = os.system(mount_command)\n                if command_status == 256:\n                    raise OSError(\n                        f\"mount {C['provider_uri']} on {C['mount_path']} error! Needs SUDO! Please mount manually: {mount_command}\"\n                    )\n                elif command_status == 32512:\n                    # LOG.error(\"Command error\")\n                    raise OSError(f\"mount {C['provider_uri']} on {C['mount_path']} error! Command error\")\n                elif command_status == 0:\n                    LOG.info(\"Mount finished\")\n            else:\n                LOG.warning(f\"{_remote_uri} on {_mount_path} is already mounted\")",
  "def init_from_yaml_conf(conf_path, **kwargs):\n    \"\"\"init_from_yaml_conf\n\n    :param conf_path: A path to the qlib config in yml format\n    \"\"\"\n\n    with open(conf_path) as f:\n        config = yaml.safe_load(f)\n    config.update(kwargs)\n    default_conf = config.pop(\"default_conf\", \"client\")\n    init(default_conf, **config)",
  "def get_path_list(path):\n    if isinstance(path, str):\n        return [path]\n    else:\n        return list(path)",
  "def sys_config(config, config_path):\n    \"\"\"\n    Configure the `sys` section\n\n    Parameters\n    ----------\n    config : dict\n        configuration of the workflow.\n    config_path : str\n        path of the configuration\n    \"\"\"\n    sys_config = config.get(\"sys\", {})\n\n    # abspath\n    for p in get_path_list(sys_config.get(\"path\", [])):\n        sys.path.append(p)\n\n    # relative path to config path\n    for p in get_path_list(sys_config.get(\"rel_path\", [])):\n        sys.path.append(str(Path(config_path).parent.resolve().absolute() / p))",
  "def workflow(config_path, experiment_name=\"workflow\", uri_folder=\"mlruns\"):\n    with open(config_path) as fp:\n        config = yaml.safe_load(fp)\n\n    # config the `sys` section\n    sys_config(config, config_path)\n\n    exp_manager = C[\"exp_manager\"]\n    exp_manager[\"kwargs\"][\"uri\"] = \"file:\" + str(Path(os.getcwd()).resolve() / uri_folder)\n    qlib.init(**config.get(\"qlib_init\"), exp_manager=exp_manager)\n\n    task_train(config.get(\"task\"), experiment_name=experiment_name)",
  "def run():\n    fire.Fire(workflow)",
  "class Recorder:\n    \"\"\"\n    This is the `Recorder` class for logging the experiments. The API is designed similar to mlflow.\n    (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)\n\n    The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.\n    \"\"\"\n\n    # status type\n    STATUS_S = \"SCHEDULED\"\n    STATUS_R = \"RUNNING\"\n    STATUS_FI = \"FINISHED\"\n    STATUS_FA = \"FAILED\"\n\n    def __init__(self, experiment_id, name):\n        self.id = None\n        self.name = name\n        self.experiment_id = experiment_id\n        self.start_time = None\n        self.end_time = None\n        self.status = Recorder.STATUS_S\n\n    def __repr__(self):\n        return \"{name}(info={info})\".format(name=self.__class__.__name__, info=self.info)\n\n    def __str__(self):\n        return str(self.info)\n\n    @property\n    def info(self):\n        output = dict()\n        output[\"class\"] = \"Recorder\"\n        output[\"id\"] = self.id\n        output[\"name\"] = self.name\n        output[\"experiment_id\"] = self.experiment_id\n        output[\"start_time\"] = self.start_time\n        output[\"end_time\"] = self.end_time\n        output[\"status\"] = self.status\n        return output\n\n    def set_recorder_name(self, rname):\n        self.recorder_name = rname\n\n    def save_objects(self, local_path=None, artifact_path=None, **kwargs):\n        \"\"\"\n        Save objects such as prediction file or model checkpoints to the artifact URI. User\n        can save object through keywords arguments (name:value).\n\n        Parameters\n        ----------\n        local_path : str\n            if provided, them save the file or directory to the artifact URI.\n        artifact_path=None : str\n            the relative path for the artifact to be stored in the URI.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `save_objects` method.\")\n\n    def load_object(self, name):\n        \"\"\"\n        Load objects such as prediction file or model checkpoints.\n\n        Parameters\n        ----------\n        name : str\n            name of the file to be loaded.\n\n        Returns\n        -------\n        The saved object.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `load_object` method.\")\n\n    def start_run(self):\n        \"\"\"\n        Start running or resuming the Recorder. The return value can be used as a context manager within a `with` block;\n        otherwise, you must call end_run() to terminate the current run. (See `ActiveRun` class in mlflow)\n\n        Returns\n        -------\n        An active running object (e.g. mlflow.ActiveRun object).\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `start_run` method.\")\n\n    def end_run(self):\n        \"\"\"\n        End an active Recorder.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `end_run` method.\")\n\n    def log_params(self, **kwargs):\n        \"\"\"\n        Log a batch of params for the current run.\n\n        Parameters\n        ----------\n        keyword arguments\n            key, value pair to be logged as parameters.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `log_params` method.\")\n\n    def log_metrics(self, step=None, **kwargs):\n        \"\"\"\n        Log multiple metrics for the current run.\n\n        Parameters\n        ----------\n        keyword arguments\n            key, value pair to be logged as metrics.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `log_metrics` method.\")\n\n    def set_tags(self, **kwargs):\n        \"\"\"\n        Log a batch of tags for the current run.\n\n        Parameters\n        ----------\n        keyword arguments\n            key, value pair to be logged as tags.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `set_tags` method.\")\n\n    def delete_tags(self, *keys):\n        \"\"\"\n        Delete some tags from a run.\n\n        Parameters\n        ----------\n        keys : series of strs of the keys\n            all the name of the tag to be deleted.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `delete_tags` method.\")\n\n    def list_artifacts(self, artifact_path: str = None):\n        \"\"\"\n        List all the artifacts of a recorder.\n\n        Parameters\n        ----------\n        artifact_path : str\n            the relative path for the artifact to be stored in the URI.\n\n        Returns\n        -------\n        A list of artifacts information (name, path, etc.) that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_artifacts` method.\")\n\n    def list_metrics(self):\n        \"\"\"\n        List all the metrics of a recorder.\n\n        Returns\n        -------\n        A dictionary of metrics that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_metrics` method.\")\n\n    def list_params(self):\n        \"\"\"\n        List all the params of a recorder.\n\n        Returns\n        -------\n        A dictionary of params that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_params` method.\")\n\n    def list_tags(self):\n        \"\"\"\n        List all the tags of a recorder.\n\n        Returns\n        -------\n        A dictionary of tags that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_tags` method.\")",
  "class MLflowRecorder(Recorder):\n    \"\"\"\n    Use mlflow to implement a Recorder.\n\n    Due to the fact that mlflow will only log artifact from a file or directory, we decide to\n    use file manager to help maintain the objects in the project.\n    \"\"\"\n\n    def __init__(self, experiment_id, uri, name=None, mlflow_run=None):\n        super(MLflowRecorder, self).__init__(experiment_id, name)\n        self._uri = uri\n        self._artifact_uri = None\n        self.client = mlflow.tracking.MlflowClient(tracking_uri=self._uri)\n        # construct from mlflow run\n        if mlflow_run is not None:\n            assert isinstance(mlflow_run, mlflow.entities.run.Run), \"Please input with a MLflow Run object.\"\n            self.name = mlflow_run.data.tags[\"mlflow.runName\"]\n            self.id = mlflow_run.info.run_id\n            self.status = mlflow_run.info.status\n            self.start_time = (\n                datetime.fromtimestamp(float(mlflow_run.info.start_time) / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\")\n                if mlflow_run.info.start_time is not None\n                else None\n            )\n            self.end_time = (\n                datetime.fromtimestamp(float(mlflow_run.info.end_time) / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\")\n                if mlflow_run.info.end_time is not None\n                else None\n            )\n\n    def __repr__(self):\n        name = self.__class__.__name__\n        space_length = len(name) + 1\n        return \"{name}(info={info},\\n{space}uri={uri},\\n{space}artifact_uri={artifact_uri},\\n{space}client={client})\".format(\n            name=name,\n            space=\" \" * space_length,\n            info=self.info,\n            uri=self.uri,\n            artifact_uri=self.artifact_uri,\n            client=self.client,\n        )\n\n    @property\n    def uri(self):\n        return self._uri\n\n    @property\n    def artifact_uri(self):\n        return self._artifact_uri\n\n    def get_local_dir(self):\n        \"\"\"\n        This function will return the directory path of this recorder.\n        \"\"\"\n        if self.artifact_uri is not None:\n            local_dir_path = Path(self.artifact_uri.lstrip(\"file:\")) / \"..\"\n            local_dir_path = str(local_dir_path.resolve())\n            if os.path.isdir(local_dir_path):\n                return local_dir_path\n            else:\n                raise RuntimeError(\"This recorder is not saved in the local file system.\")\n\n        else:\n            raise Exception(\n                \"Please make sure the recorder has been created and started properly before getting artifact uri.\"\n            )\n\n    def start_run(self):\n        # set the tracking uri\n        mlflow.set_tracking_uri(self.uri)\n        # start the run\n        run = mlflow.start_run(self.id, self.experiment_id, self.name)\n        # save the run id and artifact_uri\n        self.id = run.info.run_id\n        self._artifact_uri = run.info.artifact_uri\n        self.start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.status = Recorder.STATUS_R\n        logger.info(f\"Recorder {self.id} starts running under Experiment {self.experiment_id} ...\")\n\n        return run\n\n    def end_run(self, status: str = Recorder.STATUS_S):\n        assert status in [\n            Recorder.STATUS_S,\n            Recorder.STATUS_R,\n            Recorder.STATUS_FI,\n            Recorder.STATUS_FA,\n        ], f\"The status type {status} is not supported.\"\n        mlflow.end_run(status)\n        self.end_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        if self.status != Recorder.STATUS_S:\n            self.status = status\n\n    def save_objects(self, local_path=None, artifact_path=None, **kwargs):\n        assert self.uri is not None, \"Please start the experiment and recorder first before using recorder directly.\"\n        if local_path is not None:\n            self.client.log_artifacts(self.id, local_path, artifact_path)\n        else:\n            temp_dir = Path(tempfile.mkdtemp()).resolve()\n            for name, data in kwargs.items():\n                with (temp_dir / name).open(\"wb\") as f:\n                    pickle.dump(data, f)\n                self.client.log_artifact(self.id, temp_dir / name, artifact_path)\n            shutil.rmtree(temp_dir)\n\n    def load_object(self, name):\n        assert self.uri is not None, \"Please start the experiment and recorder first before using recorder directly.\"\n        path = self.client.download_artifacts(self.id, name)\n        with Path(path).open(\"rb\") as f:\n            return pickle.load(f)\n\n    def log_params(self, **kwargs):\n        for name, data in kwargs.items():\n            self.client.log_param(self.id, name, data)\n\n    def log_metrics(self, step=None, **kwargs):\n        for name, data in kwargs.items():\n            self.client.log_metric(self.id, name, data, step=step)\n\n    def set_tags(self, **kwargs):\n        for name, data in kwargs.items():\n            self.client.set_tag(self.id, name, data)\n\n    def delete_tags(self, *keys):\n        for key in keys:\n            self.client.delete_tag(self.id, key)\n\n    def get_artifact_uri(self):\n        if self.artifact_uri is not None:\n            return self.artifact_uri\n        else:\n            raise Exception(\n                \"Please make sure the recorder has been created and started properly before getting artifact uri.\"\n            )\n\n    def list_artifacts(self, artifact_path=None):\n        assert self.uri is not None, \"Please start the experiment and recorder first before using recorder directly.\"\n        artifacts = self.client.list_artifacts(self.id, artifact_path)\n        return [art.path for art in artifacts]\n\n    def list_metrics(self):\n        run = self.client.get_run(self.id)\n        return run.data.metrics\n\n    def list_params(self):\n        run = self.client.get_run(self.id)\n        return run.data.params\n\n    def list_tags(self):\n        run = self.client.get_run(self.id)\n        return run.data.tags",
  "def __init__(self, experiment_id, name):\n        self.id = None\n        self.name = name\n        self.experiment_id = experiment_id\n        self.start_time = None\n        self.end_time = None\n        self.status = Recorder.STATUS_S",
  "def __repr__(self):\n        return \"{name}(info={info})\".format(name=self.__class__.__name__, info=self.info)",
  "def __str__(self):\n        return str(self.info)",
  "def info(self):\n        output = dict()\n        output[\"class\"] = \"Recorder\"\n        output[\"id\"] = self.id\n        output[\"name\"] = self.name\n        output[\"experiment_id\"] = self.experiment_id\n        output[\"start_time\"] = self.start_time\n        output[\"end_time\"] = self.end_time\n        output[\"status\"] = self.status\n        return output",
  "def set_recorder_name(self, rname):\n        self.recorder_name = rname",
  "def save_objects(self, local_path=None, artifact_path=None, **kwargs):\n        \"\"\"\n        Save objects such as prediction file or model checkpoints to the artifact URI. User\n        can save object through keywords arguments (name:value).\n\n        Parameters\n        ----------\n        local_path : str\n            if provided, them save the file or directory to the artifact URI.\n        artifact_path=None : str\n            the relative path for the artifact to be stored in the URI.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `save_objects` method.\")",
  "def load_object(self, name):\n        \"\"\"\n        Load objects such as prediction file or model checkpoints.\n\n        Parameters\n        ----------\n        name : str\n            name of the file to be loaded.\n\n        Returns\n        -------\n        The saved object.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `load_object` method.\")",
  "def start_run(self):\n        \"\"\"\n        Start running or resuming the Recorder. The return value can be used as a context manager within a `with` block;\n        otherwise, you must call end_run() to terminate the current run. (See `ActiveRun` class in mlflow)\n\n        Returns\n        -------\n        An active running object (e.g. mlflow.ActiveRun object).\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `start_run` method.\")",
  "def end_run(self):\n        \"\"\"\n        End an active Recorder.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `end_run` method.\")",
  "def log_params(self, **kwargs):\n        \"\"\"\n        Log a batch of params for the current run.\n\n        Parameters\n        ----------\n        keyword arguments\n            key, value pair to be logged as parameters.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `log_params` method.\")",
  "def log_metrics(self, step=None, **kwargs):\n        \"\"\"\n        Log multiple metrics for the current run.\n\n        Parameters\n        ----------\n        keyword arguments\n            key, value pair to be logged as metrics.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `log_metrics` method.\")",
  "def set_tags(self, **kwargs):\n        \"\"\"\n        Log a batch of tags for the current run.\n\n        Parameters\n        ----------\n        keyword arguments\n            key, value pair to be logged as tags.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `set_tags` method.\")",
  "def delete_tags(self, *keys):\n        \"\"\"\n        Delete some tags from a run.\n\n        Parameters\n        ----------\n        keys : series of strs of the keys\n            all the name of the tag to be deleted.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `delete_tags` method.\")",
  "def list_artifacts(self, artifact_path: str = None):\n        \"\"\"\n        List all the artifacts of a recorder.\n\n        Parameters\n        ----------\n        artifact_path : str\n            the relative path for the artifact to be stored in the URI.\n\n        Returns\n        -------\n        A list of artifacts information (name, path, etc.) that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_artifacts` method.\")",
  "def list_metrics(self):\n        \"\"\"\n        List all the metrics of a recorder.\n\n        Returns\n        -------\n        A dictionary of metrics that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_metrics` method.\")",
  "def list_params(self):\n        \"\"\"\n        List all the params of a recorder.\n\n        Returns\n        -------\n        A dictionary of params that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_params` method.\")",
  "def list_tags(self):\n        \"\"\"\n        List all the tags of a recorder.\n\n        Returns\n        -------\n        A dictionary of tags that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_tags` method.\")",
  "def __init__(self, experiment_id, uri, name=None, mlflow_run=None):\n        super(MLflowRecorder, self).__init__(experiment_id, name)\n        self._uri = uri\n        self._artifact_uri = None\n        self.client = mlflow.tracking.MlflowClient(tracking_uri=self._uri)\n        # construct from mlflow run\n        if mlflow_run is not None:\n            assert isinstance(mlflow_run, mlflow.entities.run.Run), \"Please input with a MLflow Run object.\"\n            self.name = mlflow_run.data.tags[\"mlflow.runName\"]\n            self.id = mlflow_run.info.run_id\n            self.status = mlflow_run.info.status\n            self.start_time = (\n                datetime.fromtimestamp(float(mlflow_run.info.start_time) / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\")\n                if mlflow_run.info.start_time is not None\n                else None\n            )\n            self.end_time = (\n                datetime.fromtimestamp(float(mlflow_run.info.end_time) / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\")\n                if mlflow_run.info.end_time is not None\n                else None\n            )",
  "def __repr__(self):\n        name = self.__class__.__name__\n        space_length = len(name) + 1\n        return \"{name}(info={info},\\n{space}uri={uri},\\n{space}artifact_uri={artifact_uri},\\n{space}client={client})\".format(\n            name=name,\n            space=\" \" * space_length,\n            info=self.info,\n            uri=self.uri,\n            artifact_uri=self.artifact_uri,\n            client=self.client,\n        )",
  "def uri(self):\n        return self._uri",
  "def artifact_uri(self):\n        return self._artifact_uri",
  "def get_local_dir(self):\n        \"\"\"\n        This function will return the directory path of this recorder.\n        \"\"\"\n        if self.artifact_uri is not None:\n            local_dir_path = Path(self.artifact_uri.lstrip(\"file:\")) / \"..\"\n            local_dir_path = str(local_dir_path.resolve())\n            if os.path.isdir(local_dir_path):\n                return local_dir_path\n            else:\n                raise RuntimeError(\"This recorder is not saved in the local file system.\")\n\n        else:\n            raise Exception(\n                \"Please make sure the recorder has been created and started properly before getting artifact uri.\"\n            )",
  "def start_run(self):\n        # set the tracking uri\n        mlflow.set_tracking_uri(self.uri)\n        # start the run\n        run = mlflow.start_run(self.id, self.experiment_id, self.name)\n        # save the run id and artifact_uri\n        self.id = run.info.run_id\n        self._artifact_uri = run.info.artifact_uri\n        self.start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.status = Recorder.STATUS_R\n        logger.info(f\"Recorder {self.id} starts running under Experiment {self.experiment_id} ...\")\n\n        return run",
  "def end_run(self, status: str = Recorder.STATUS_S):\n        assert status in [\n            Recorder.STATUS_S,\n            Recorder.STATUS_R,\n            Recorder.STATUS_FI,\n            Recorder.STATUS_FA,\n        ], f\"The status type {status} is not supported.\"\n        mlflow.end_run(status)\n        self.end_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        if self.status != Recorder.STATUS_S:\n            self.status = status",
  "def save_objects(self, local_path=None, artifact_path=None, **kwargs):\n        assert self.uri is not None, \"Please start the experiment and recorder first before using recorder directly.\"\n        if local_path is not None:\n            self.client.log_artifacts(self.id, local_path, artifact_path)\n        else:\n            temp_dir = Path(tempfile.mkdtemp()).resolve()\n            for name, data in kwargs.items():\n                with (temp_dir / name).open(\"wb\") as f:\n                    pickle.dump(data, f)\n                self.client.log_artifact(self.id, temp_dir / name, artifact_path)\n            shutil.rmtree(temp_dir)",
  "def load_object(self, name):\n        assert self.uri is not None, \"Please start the experiment and recorder first before using recorder directly.\"\n        path = self.client.download_artifacts(self.id, name)\n        with Path(path).open(\"rb\") as f:\n            return pickle.load(f)",
  "def log_params(self, **kwargs):\n        for name, data in kwargs.items():\n            self.client.log_param(self.id, name, data)",
  "def log_metrics(self, step=None, **kwargs):\n        for name, data in kwargs.items():\n            self.client.log_metric(self.id, name, data, step=step)",
  "def set_tags(self, **kwargs):\n        for name, data in kwargs.items():\n            self.client.set_tag(self.id, name, data)",
  "def delete_tags(self, *keys):\n        for key in keys:\n            self.client.delete_tag(self.id, key)",
  "def get_artifact_uri(self):\n        if self.artifact_uri is not None:\n            return self.artifact_uri\n        else:\n            raise Exception(\n                \"Please make sure the recorder has been created and started properly before getting artifact uri.\"\n            )",
  "def list_artifacts(self, artifact_path=None):\n        assert self.uri is not None, \"Please start the experiment and recorder first before using recorder directly.\"\n        artifacts = self.client.list_artifacts(self.id, artifact_path)\n        return [art.path for art in artifacts]",
  "def list_metrics(self):\n        run = self.client.get_run(self.id)\n        return run.data.metrics",
  "def list_params(self):\n        run = self.client.get_run(self.id)\n        return run.data.params",
  "def list_tags(self):\n        run = self.client.get_run(self.id)\n        return run.data.tags",
  "def experiment_exit_handler():\n    \"\"\"\n    Method for handling the experiment when any unusual program ending occurs.\n    The `atexit` handler should be put in the last, since, as long as the program ends, it will be called.\n    Thus, if any exception or user interuption occurs beforehead, we should handle them first. Once `R` is\n    ended, another call of `R.end_exp` will not take effect.\n    \"\"\"\n    signal.signal(signal.SIGINT, experiment_kill_signal_handler)  # handle user keyboard interupt\n    sys.excepthook = experiment_exception_hook  # handle uncaught exception\n    atexit.register(R.end_exp, recorder_status=Recorder.STATUS_FI)",
  "def experiment_exception_hook(type, value, tb):\n    \"\"\"\n    End an experiment with status to be \"FAILED\". This exception tries to catch those uncaught exception\n    and end the experiment automatically.\n\n    Parameters\n    type: Exception type\n    value: Exception's value\n    tb: Exception's traceback\n    \"\"\"\n    logger.error(f\"An exception has been raised[{type.__name__}: {value}].\")\n\n    # Same as original format\n    traceback.print_tb(tb)\n    print(f\"{type.__name__}: {value}\")\n\n    R.end_exp(recorder_status=Recorder.STATUS_FA)",
  "def experiment_kill_signal_handler(signum, frame):\n    \"\"\"\n    End an experiment when user kill the program through keyboard (CTRL+C, etc.).\n    \"\"\"\n    R.end_exp(recorder_status=Recorder.STATUS_FA)",
  "class ExpManager:\n    \"\"\"\n    This is the `ExpManager` class for managing experiments. The API is designed similar to mlflow.\n    (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)\n    \"\"\"\n\n    def __init__(self, uri: Text, default_exp_name: Optional[Text]):\n        self._current_uri = uri\n        self._default_exp_name = default_exp_name\n        self.active_experiment = None  # only one experiment can active each time\n\n    def __repr__(self):\n        return \"{name}(current_uri={curi})\".format(name=self.__class__.__name__, curi=self._current_uri)\n\n    def start_exp(\n        self,\n        experiment_name: Optional[Text] = None,\n        recorder_name: Optional[Text] = None,\n        uri: Optional[Text] = None,\n        resume: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Start an experiment. This method includes first get_or_create an experiment, and then\n        set it to be active.\n\n        Parameters\n        ----------\n        experiment_name : str\n            name of the active experiment.\n        recorder_name : str\n            name of the recorder to be started.\n        uri : str\n            the current tracking URI.\n        resume : boolean\n            whether to resume the experiment and recorder.\n\n        Returns\n        -------\n        An active experiment.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `start_exp` method.\")\n\n    def end_exp(self, recorder_status: Text = Recorder.STATUS_S, **kwargs):\n        \"\"\"\n        End an active experiment.\n\n        Parameters\n        ----------\n        experiment_name : str\n            name of the active experiment.\n        recorder_status : str\n            the status of the active recorder of the experiment.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `end_exp` method.\")\n\n    def create_exp(self, experiment_name: Optional[Text] = None):\n        \"\"\"\n        Create an experiment.\n\n        Parameters\n        ----------\n        experiment_name : str\n            the experiment name, which must be unique.\n\n        Returns\n        -------\n        An experiment object.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `create_exp` method.\")\n\n    def search_records(self, experiment_ids=None, **kwargs):\n        \"\"\"\n        Get a pandas DataFrame of records that fit the search criteria of the experiment.\n        Inputs are the search critera user want to apply.\n\n        Returns\n        -------\n        A pandas.DataFrame of records, where each metric, parameter, and tag\n        are expanded into their own columns named metrics.*, params.*, and tags.*\n        respectively. For records that don't have a particular metric, parameter, or tag, their\n        value will be (NumPy) Nan, None, or None respectively.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `search_records` method.\")\n\n    def get_exp(self, experiment_id=None, experiment_name=None, create: bool = True):\n        \"\"\"\n        Retrieve an experiment. This method includes getting an active experiment, and get_or_create a specific experiment.\n        The returned experiment will be active.\n\n        When user specify experiment id and name, the method will try to return the specific experiment.\n        When user does not provide recorder id or name, the method will try to return the current active experiment.\n        The `create` argument determines whether the method will automatically create a new experiment according\n        to user's specification if the experiment hasn't been created before.\n\n        * If `create` is True:\n\n            * If `active experiment` exists:\n\n                * no id or name specified, return the active experiment.\n                * if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name, and the experiment is set to be active.\n\n            * If `active experiment` not exists:\n\n                * no id or name specified, create a default experiment.\n                * if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name, and the experiment is set to be active.\n\n        * Else If `create` is False:\n\n            * If `active experiment` exists:\n\n                * no id or name specified, return the active experiment.\n                * if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n            * If `active experiment` not exists:\n\n                *  no id or name specified. If the default experiment exists, return it, otherwise, raise Error.\n                * if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment to return.\n        experiment_name : str\n            name of the experiment to return.\n        create : boolean\n            create the experiment it if hasn't been created before.\n\n        Returns\n        -------\n        An experiment object.\n        \"\"\"\n        # special case of getting experiment\n        if experiment_id is None and experiment_name is None:\n            if self.active_experiment is not None:\n                return self.active_experiment\n            # User don't want get active code now.\n            experiment_name = self._default_exp_name\n\n        if create:\n            exp, is_new = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)\n        else:\n            exp, is_new = self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name), False\n        if is_new:\n            self.active_experiment = exp\n            # start the recorder\n            self.active_experiment.start()\n        return exp\n\n    def _get_or_create_exp(self, experiment_id=None, experiment_name=None) -> (object, bool):\n        \"\"\"\n        Method for getting or creating an experiment. It will try to first get a valid experiment, if exception occurs, it will\n        automatically create a new experiment based on the given id and name.\n        \"\"\"\n        try:\n            return self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name), False\n        except ValueError:\n            if experiment_name is None:\n                experiment_name = self._default_exp_name\n            logger.info(f\"No valid experiment found. Create a new experiment with name {experiment_name}.\")\n            return self.create_exp(experiment_name), True\n\n    def _get_exp(self, experiment_id=None, experiment_name=None) -> Experiment:\n        \"\"\"\n        Get specific experiment by name or id. If it does not exist, raise ValueError.\n\n        Parameters\n        ----------\n        experiment_id :\n            The id of experiment\n        experiment_name :\n            The name of experiment\n\n        Returns\n        -------\n        Experiment:\n            The searched experiment\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `_get_exp` method\")\n\n    def delete_exp(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Delete an experiment.\n\n        Parameters\n        ----------\n        experiment_id  : str\n            the experiment id.\n        experiment_name  : str\n            the experiment name.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `delete_exp` method.\")\n\n    @property\n    def default_uri(self):\n        \"\"\"\n        Get the default tracking URI from qlib.config.C\n        \"\"\"\n        if \"kwargs\" not in C.exp_manager or \"uri\" not in C.exp_manager[\"kwargs\"]:\n            raise ValueError(\"The default URI is not set in qlib.config.C\")\n        return C.exp_manager[\"kwargs\"][\"uri\"]\n\n    @property\n    def uri(self):\n        \"\"\"\n        Get the default tracking URI or current URI.\n\n        Returns\n        -------\n        The tracking URI string.\n        \"\"\"\n        return self._current_uri or self.default_uri\n\n    def set_uri(self, uri: Optional[Text] = None):\n        \"\"\"\n        Set the current tracking URI and the corresponding variables.\n\n        Parameters\n        ----------\n        uri  : str\n\n        \"\"\"\n        if uri is None:\n            logger.info(\"No tracking URI is provided. Use the default tracking URI.\")\n            self._current_uri = self.default_uri\n        else:\n            # Temporarily re-set the current uri as the uri argument.\n            self._current_uri = uri\n        # Customized features for subclasses.\n        self._set_uri()\n\n    def _set_uri(self):\n        \"\"\"\n        Customized features for subclasses' set_uri function.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `_set_uri` method.\")\n\n    def list_experiments(self):\n        \"\"\"\n        List all the existing experiments.\n\n        Returns\n        -------\n        A dictionary (name -> experiment) of experiments information that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_experiments` method.\")",
  "class MLflowExpManager(ExpManager):\n    \"\"\"\n    Use mlflow to implement ExpManager.\n    \"\"\"\n\n    def __init__(self, uri: Text, default_exp_name: Optional[Text]):\n        super(MLflowExpManager, self).__init__(uri, default_exp_name)\n        self._client = None\n\n    def _set_uri(self):\n        self._client = mlflow.tracking.MlflowClient(tracking_uri=self.uri)\n        logger.info(\"{:}\".format(self._client))\n\n    @property\n    def client(self):\n        # Delay the creation of mlflow client in case of creating `mlruns` folder when importing qlib\n        if self._client is None:\n            self._client = mlflow.tracking.MlflowClient(tracking_uri=self.uri)\n        return self._client\n\n    def start_exp(\n        self,\n        experiment_name: Optional[Text] = None,\n        recorder_name: Optional[Text] = None,\n        uri: Optional[Text] = None,\n        resume: bool = False,\n    ):\n        # Set the tracking uri\n        self.set_uri(uri)\n        # Create experiment\n        if experiment_name is None:\n            experiment_name = self._default_exp_name\n        experiment, _ = self._get_or_create_exp(experiment_name=experiment_name)\n        # Set up active experiment\n        self.active_experiment = experiment\n        # Start the experiment\n        self.active_experiment.start(recorder_name, resume)\n\n        return self.active_experiment\n\n    def end_exp(self, recorder_status: Text = Recorder.STATUS_S):\n        if self.active_experiment is not None:\n            self.active_experiment.end(recorder_status)\n            self.active_experiment = None\n        # When an experiment end, we will release the current uri.\n        self._current_uri = None\n\n    def create_exp(self, experiment_name: Optional[Text] = None):\n        assert experiment_name is not None\n        # init experiment\n        experiment_id = self.client.create_experiment(experiment_name)\n        experiment = MLflowExperiment(experiment_id, experiment_name, self.uri)\n        experiment._default_name = self._default_exp_name\n\n        return experiment\n\n    def _get_exp(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for getting or creating an experiment. It will try to first get a valid experiment, if exception occurs, it will\n        raise errors.\n        \"\"\"\n        assert (\n            experiment_id is not None or experiment_name is not None\n        ), \"Please input at least one of experiment/recorder id or name before retrieving experiment/recorder.\"\n        if experiment_id is not None:\n            try:\n                exp = self.client.get_experiment(experiment_id)\n                if exp.lifecycle_stage.upper() == \"DELETED\":\n                    raise MlflowException(\"No valid experiment has been found.\")\n                experiment = MLflowExperiment(exp.experiment_id, exp.name, self.uri)\n                return experiment\n            except MlflowException:\n                raise ValueError(\n                    \"No valid experiment has been found, please make sure the input experiment id is correct.\"\n                )\n        elif experiment_name is not None:\n            try:\n                exp = self.client.get_experiment_by_name(experiment_name)\n                if exp is None or exp.lifecycle_stage.upper() == \"DELETED\":\n                    raise MlflowException(\"No valid experiment has been found.\")\n                experiment = MLflowExperiment(exp.experiment_id, experiment_name, self.uri)\n                return experiment\n            except MlflowException as e:\n                raise ValueError(\n                    \"No valid experiment has been found, please make sure the input experiment name is correct.\"\n                )\n\n    def search_records(self, experiment_ids, **kwargs):\n        filter_string = \"\" if kwargs.get(\"filter_string\") is None else kwargs.get(\"filter_string\")\n        run_view_type = 1 if kwargs.get(\"run_view_type\") is None else kwargs.get(\"run_view_type\")\n        max_results = 100000 if kwargs.get(\"max_results\") is None else kwargs.get(\"max_results\")\n        order_by = kwargs.get(\"order_by\")\n        return self.client.search_runs(experiment_ids, filter_string, run_view_type, max_results, order_by)\n\n    def delete_exp(self, experiment_id=None, experiment_name=None):\n        assert (\n            experiment_id is not None or experiment_name is not None\n        ), \"Please input a valid experiment id or name before deleting.\"\n        try:\n            if experiment_id is not None:\n                self.client.delete_experiment(experiment_id)\n            else:\n                experiment = self.client.get_experiment_by_name(experiment_name)\n                if experiment is None:\n                    raise MlflowException(\"No valid experiment has been found.\")\n                self.client.delete_experiment(experiment.experiment_id)\n        except MlflowException as e:\n            raise Exception(\n                f\"Error: {e}. Something went wrong when deleting experiment. Please check if the name/id of the experiment is correct.\"\n            )\n\n    def list_experiments(self):\n        # retrieve all the existing experiments\n        exps = self.client.list_experiments(view_type=ViewType.ACTIVE_ONLY)\n        experiments = dict()\n        for exp in exps:\n            experiment = MLflowExperiment(exp.experiment_id, exp.name, self.uri)\n            experiments[exp.name] = experiment\n        return experiments",
  "def __init__(self, uri: Text, default_exp_name: Optional[Text]):\n        self._current_uri = uri\n        self._default_exp_name = default_exp_name\n        self.active_experiment = None",
  "def __repr__(self):\n        return \"{name}(current_uri={curi})\".format(name=self.__class__.__name__, curi=self._current_uri)",
  "def start_exp(\n        self,\n        experiment_name: Optional[Text] = None,\n        recorder_name: Optional[Text] = None,\n        uri: Optional[Text] = None,\n        resume: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Start an experiment. This method includes first get_or_create an experiment, and then\n        set it to be active.\n\n        Parameters\n        ----------\n        experiment_name : str\n            name of the active experiment.\n        recorder_name : str\n            name of the recorder to be started.\n        uri : str\n            the current tracking URI.\n        resume : boolean\n            whether to resume the experiment and recorder.\n\n        Returns\n        -------\n        An active experiment.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `start_exp` method.\")",
  "def end_exp(self, recorder_status: Text = Recorder.STATUS_S, **kwargs):\n        \"\"\"\n        End an active experiment.\n\n        Parameters\n        ----------\n        experiment_name : str\n            name of the active experiment.\n        recorder_status : str\n            the status of the active recorder of the experiment.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `end_exp` method.\")",
  "def create_exp(self, experiment_name: Optional[Text] = None):\n        \"\"\"\n        Create an experiment.\n\n        Parameters\n        ----------\n        experiment_name : str\n            the experiment name, which must be unique.\n\n        Returns\n        -------\n        An experiment object.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `create_exp` method.\")",
  "def search_records(self, experiment_ids=None, **kwargs):\n        \"\"\"\n        Get a pandas DataFrame of records that fit the search criteria of the experiment.\n        Inputs are the search critera user want to apply.\n\n        Returns\n        -------\n        A pandas.DataFrame of records, where each metric, parameter, and tag\n        are expanded into their own columns named metrics.*, params.*, and tags.*\n        respectively. For records that don't have a particular metric, parameter, or tag, their\n        value will be (NumPy) Nan, None, or None respectively.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `search_records` method.\")",
  "def get_exp(self, experiment_id=None, experiment_name=None, create: bool = True):\n        \"\"\"\n        Retrieve an experiment. This method includes getting an active experiment, and get_or_create a specific experiment.\n        The returned experiment will be active.\n\n        When user specify experiment id and name, the method will try to return the specific experiment.\n        When user does not provide recorder id or name, the method will try to return the current active experiment.\n        The `create` argument determines whether the method will automatically create a new experiment according\n        to user's specification if the experiment hasn't been created before.\n\n        * If `create` is True:\n\n            * If `active experiment` exists:\n\n                * no id or name specified, return the active experiment.\n                * if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name, and the experiment is set to be active.\n\n            * If `active experiment` not exists:\n\n                * no id or name specified, create a default experiment.\n                * if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name, and the experiment is set to be active.\n\n        * Else If `create` is False:\n\n            * If `active experiment` exists:\n\n                * no id or name specified, return the active experiment.\n                * if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n            * If `active experiment` not exists:\n\n                *  no id or name specified. If the default experiment exists, return it, otherwise, raise Error.\n                * if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment to return.\n        experiment_name : str\n            name of the experiment to return.\n        create : boolean\n            create the experiment it if hasn't been created before.\n\n        Returns\n        -------\n        An experiment object.\n        \"\"\"\n        # special case of getting experiment\n        if experiment_id is None and experiment_name is None:\n            if self.active_experiment is not None:\n                return self.active_experiment\n            # User don't want get active code now.\n            experiment_name = self._default_exp_name\n\n        if create:\n            exp, is_new = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)\n        else:\n            exp, is_new = self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name), False\n        if is_new:\n            self.active_experiment = exp\n            # start the recorder\n            self.active_experiment.start()\n        return exp",
  "def _get_or_create_exp(self, experiment_id=None, experiment_name=None) -> (object, bool):\n        \"\"\"\n        Method for getting or creating an experiment. It will try to first get a valid experiment, if exception occurs, it will\n        automatically create a new experiment based on the given id and name.\n        \"\"\"\n        try:\n            return self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name), False\n        except ValueError:\n            if experiment_name is None:\n                experiment_name = self._default_exp_name\n            logger.info(f\"No valid experiment found. Create a new experiment with name {experiment_name}.\")\n            return self.create_exp(experiment_name), True",
  "def _get_exp(self, experiment_id=None, experiment_name=None) -> Experiment:\n        \"\"\"\n        Get specific experiment by name or id. If it does not exist, raise ValueError.\n\n        Parameters\n        ----------\n        experiment_id :\n            The id of experiment\n        experiment_name :\n            The name of experiment\n\n        Returns\n        -------\n        Experiment:\n            The searched experiment\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `_get_exp` method\")",
  "def delete_exp(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Delete an experiment.\n\n        Parameters\n        ----------\n        experiment_id  : str\n            the experiment id.\n        experiment_name  : str\n            the experiment name.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `delete_exp` method.\")",
  "def default_uri(self):\n        \"\"\"\n        Get the default tracking URI from qlib.config.C\n        \"\"\"\n        if \"kwargs\" not in C.exp_manager or \"uri\" not in C.exp_manager[\"kwargs\"]:\n            raise ValueError(\"The default URI is not set in qlib.config.C\")\n        return C.exp_manager[\"kwargs\"][\"uri\"]",
  "def uri(self):\n        \"\"\"\n        Get the default tracking URI or current URI.\n\n        Returns\n        -------\n        The tracking URI string.\n        \"\"\"\n        return self._current_uri or self.default_uri",
  "def set_uri(self, uri: Optional[Text] = None):\n        \"\"\"\n        Set the current tracking URI and the corresponding variables.\n\n        Parameters\n        ----------\n        uri  : str\n\n        \"\"\"\n        if uri is None:\n            logger.info(\"No tracking URI is provided. Use the default tracking URI.\")\n            self._current_uri = self.default_uri\n        else:\n            # Temporarily re-set the current uri as the uri argument.\n            self._current_uri = uri\n        # Customized features for subclasses.\n        self._set_uri()",
  "def _set_uri(self):\n        \"\"\"\n        Customized features for subclasses' set_uri function.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `_set_uri` method.\")",
  "def list_experiments(self):\n        \"\"\"\n        List all the existing experiments.\n\n        Returns\n        -------\n        A dictionary (name -> experiment) of experiments information that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_experiments` method.\")",
  "def __init__(self, uri: Text, default_exp_name: Optional[Text]):\n        super(MLflowExpManager, self).__init__(uri, default_exp_name)\n        self._client = None",
  "def _set_uri(self):\n        self._client = mlflow.tracking.MlflowClient(tracking_uri=self.uri)\n        logger.info(\"{:}\".format(self._client))",
  "def client(self):\n        # Delay the creation of mlflow client in case of creating `mlruns` folder when importing qlib\n        if self._client is None:\n            self._client = mlflow.tracking.MlflowClient(tracking_uri=self.uri)\n        return self._client",
  "def start_exp(\n        self,\n        experiment_name: Optional[Text] = None,\n        recorder_name: Optional[Text] = None,\n        uri: Optional[Text] = None,\n        resume: bool = False,\n    ):\n        # Set the tracking uri\n        self.set_uri(uri)\n        # Create experiment\n        if experiment_name is None:\n            experiment_name = self._default_exp_name\n        experiment, _ = self._get_or_create_exp(experiment_name=experiment_name)\n        # Set up active experiment\n        self.active_experiment = experiment\n        # Start the experiment\n        self.active_experiment.start(recorder_name, resume)\n\n        return self.active_experiment",
  "def end_exp(self, recorder_status: Text = Recorder.STATUS_S):\n        if self.active_experiment is not None:\n            self.active_experiment.end(recorder_status)\n            self.active_experiment = None\n        # When an experiment end, we will release the current uri.\n        self._current_uri = None",
  "def create_exp(self, experiment_name: Optional[Text] = None):\n        assert experiment_name is not None\n        # init experiment\n        experiment_id = self.client.create_experiment(experiment_name)\n        experiment = MLflowExperiment(experiment_id, experiment_name, self.uri)\n        experiment._default_name = self._default_exp_name\n\n        return experiment",
  "def _get_exp(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for getting or creating an experiment. It will try to first get a valid experiment, if exception occurs, it will\n        raise errors.\n        \"\"\"\n        assert (\n            experiment_id is not None or experiment_name is not None\n        ), \"Please input at least one of experiment/recorder id or name before retrieving experiment/recorder.\"\n        if experiment_id is not None:\n            try:\n                exp = self.client.get_experiment(experiment_id)\n                if exp.lifecycle_stage.upper() == \"DELETED\":\n                    raise MlflowException(\"No valid experiment has been found.\")\n                experiment = MLflowExperiment(exp.experiment_id, exp.name, self.uri)\n                return experiment\n            except MlflowException:\n                raise ValueError(\n                    \"No valid experiment has been found, please make sure the input experiment id is correct.\"\n                )\n        elif experiment_name is not None:\n            try:\n                exp = self.client.get_experiment_by_name(experiment_name)\n                if exp is None or exp.lifecycle_stage.upper() == \"DELETED\":\n                    raise MlflowException(\"No valid experiment has been found.\")\n                experiment = MLflowExperiment(exp.experiment_id, experiment_name, self.uri)\n                return experiment\n            except MlflowException as e:\n                raise ValueError(\n                    \"No valid experiment has been found, please make sure the input experiment name is correct.\"\n                )",
  "def search_records(self, experiment_ids, **kwargs):\n        filter_string = \"\" if kwargs.get(\"filter_string\") is None else kwargs.get(\"filter_string\")\n        run_view_type = 1 if kwargs.get(\"run_view_type\") is None else kwargs.get(\"run_view_type\")\n        max_results = 100000 if kwargs.get(\"max_results\") is None else kwargs.get(\"max_results\")\n        order_by = kwargs.get(\"order_by\")\n        return self.client.search_runs(experiment_ids, filter_string, run_view_type, max_results, order_by)",
  "def delete_exp(self, experiment_id=None, experiment_name=None):\n        assert (\n            experiment_id is not None or experiment_name is not None\n        ), \"Please input a valid experiment id or name before deleting.\"\n        try:\n            if experiment_id is not None:\n                self.client.delete_experiment(experiment_id)\n            else:\n                experiment = self.client.get_experiment_by_name(experiment_name)\n                if experiment is None:\n                    raise MlflowException(\"No valid experiment has been found.\")\n                self.client.delete_experiment(experiment.experiment_id)\n        except MlflowException as e:\n            raise Exception(\n                f\"Error: {e}. Something went wrong when deleting experiment. Please check if the name/id of the experiment is correct.\"\n            )",
  "def list_experiments(self):\n        # retrieve all the existing experiments\n        exps = self.client.list_experiments(view_type=ViewType.ACTIVE_ONLY)\n        experiments = dict()\n        for exp in exps:\n            experiment = MLflowExperiment(exp.experiment_id, exp.name, self.uri)\n            experiments[exp.name] = experiment\n        return experiments",
  "class Experiment:\n    \"\"\"\n    This is the `Experiment` class for each experiment being run. The API is designed similar to mlflow.\n    (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)\n    \"\"\"\n\n    def __init__(self, id, name):\n        self.id = id\n        self.name = name\n        self.active_recorder = None  # only one recorder can running each time\n\n    def __repr__(self):\n        return \"{name}(id={id}, info={info})\".format(name=self.__class__.__name__, id=self.id, info=self.info)\n\n    def __str__(self):\n        return str(self.info)\n\n    @property\n    def info(self):\n        recorders = self.list_recorders()\n        output = dict()\n        output[\"class\"] = \"Experiment\"\n        output[\"id\"] = self.id\n        output[\"name\"] = self.name\n        output[\"active_recorder\"] = self.active_recorder.id if self.active_recorder is not None else None\n        output[\"recorders\"] = list(recorders.keys())\n        return output\n\n    def start(self, recorder_name=None, resume=False):\n        \"\"\"\n        Start the experiment and set it to be active. This method will also start a new recorder.\n\n        Parameters\n        ----------\n        recorder_name : str\n            the name of the recorder to be created.\n        resume : bool\n            whether to resume the first recorder\n\n        Returns\n        -------\n        An active recorder.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `start` method.\")\n\n    def end(self, recorder_status=Recorder.STATUS_S):\n        \"\"\"\n        End the experiment.\n\n        Parameters\n        ----------\n        recorder_status : str\n            the status the recorder to be set with when ending (SCHEDULED, RUNNING, FINISHED, FAILED).\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `end` method.\")\n\n    def create_recorder(self, recorder_name=None):\n        \"\"\"\n        Create a recorder for each experiment.\n\n        Parameters\n        ----------\n        recorder_name : str\n            the name of the recorder to be created.\n\n        Returns\n        -------\n        A recorder object.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `create_recorder` method.\")\n\n    def search_records(self, **kwargs):\n        \"\"\"\n        Get a pandas DataFrame of records that fit the search criteria of the experiment.\n        Inputs are the search critera user want to apply.\n\n        Returns\n        -------\n        A pandas.DataFrame of records, where each metric, parameter, and tag\n        are expanded into their own columns named metrics.*, params.*, and tags.*\n        respectively. For records that don't have a particular metric, parameter, or tag, their\n        value will be (NumPy) Nan, None, or None respectively.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `search_records` method.\")\n\n    def delete_recorder(self, recorder_id):\n        \"\"\"\n        Create a recorder for each experiment.\n\n        Parameters\n        ----------\n        recorder_id : str\n            the id of the recorder to be deleted.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `delete_recorder` method.\")\n\n    def get_recorder(self, recorder_id=None, recorder_name=None, create: bool = True):\n        \"\"\"\n        Retrieve a Recorder for user. When user specify recorder id and name, the method will try to return the\n        specific recorder. When user does not provide recorder id or name, the method will try to return the current\n        active recorder. The `create` argument determines whether the method will automatically create a new recorder\n        according to user's specification if the recorder hasn't been created before\n\n        * If `create` is True:\n\n            * If `active recorder` exists:\n\n                * no id or name specified, return the active recorder.\n                * if id or name is specified, return the specified recorder. If no such exp found, create a new recorder with given id or name, and the recorder shoud be active.\n\n            * If `active recorder` not exists:\n\n                * no id or name specified, create a new recorder.\n                * if id or name is specified, return the specified experiment. If no such exp found, create a new recorder with given id or name, and the recorder shoud be active.\n\n        * Else If `create` is False:\n\n            * If `active recorder` exists:\n\n                * no id or name specified, return the active recorder.\n                * if id or name is specified, return the specified recorder. If no such exp found, raise Error.\n\n            * If `active recorder` not exists:\n\n                * no id or name specified, raise Error.\n                * if id or name is specified, return the specified recorder. If no such exp found, raise Error.\n\n        Parameters\n        ----------\n        recorder_id : str\n            the id of the recorder to be deleted.\n        recorder_name : str\n            the name of the recorder to be deleted.\n        create : boolean\n            create the recorder if it hasn't been created before.\n\n        Returns\n        -------\n        A recorder object.\n        \"\"\"\n        # special case of getting the recorder\n        if recorder_id is None and recorder_name is None:\n            if self.active_recorder is not None:\n                return self.active_recorder\n            recorder_name = self._default_rec_name\n        if create:\n            recorder, is_new = self._get_or_create_rec(recorder_id=recorder_id, recorder_name=recorder_name)\n        else:\n            recorder, is_new = self._get_recorder(recorder_id=recorder_id, recorder_name=recorder_name), False\n        if is_new:\n            self.active_recorder = recorder\n            # start the recorder\n            self.active_recorder.start_run()\n        return recorder\n\n    def _get_or_create_rec(self, recorder_id=None, recorder_name=None) -> (object, bool):\n        \"\"\"\n        Method for getting or creating a recorder. It will try to first get a valid recorder, if exception occurs, it will\n        automatically create a new recorder based on the given id and name.\n        \"\"\"\n        try:\n            if recorder_id is None and recorder_name is None:\n                recorder_name = self._default_rec_name\n            return self._get_recorder(recorder_id=recorder_id, recorder_name=recorder_name), False\n        except ValueError:\n            if recorder_name is None:\n                recorder_name = self._default_rec_name\n            logger.info(f\"No valid recorder found. Create a new recorder with name {recorder_name}.\")\n            return self.create_recorder(recorder_name), True\n\n    def _get_recorder(self, recorder_id=None, recorder_name=None):\n        \"\"\"\n        Get specific recorder by name or id. If it does not exist, raise ValueError\n\n        Parameters\n        ----------\n        recorder_id :\n            The id of recorder\n        recorder_name :\n            The name of recorder\n\n        Returns\n        -------\n        Recorder:\n            The searched recorder\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `_get_recorder` method\")\n\n    def list_recorders(self):\n        \"\"\"\n        List all the existing recorders of this experiment. Please first get the experiment instance before calling this method.\n        If user want to use the method `R.list_recorders()`, please refer to the related API document in `QlibRecorder`.\n\n        Returns\n        -------\n        A dictionary (id -> recorder) of recorder information that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_recorders` method.\")",
  "class MLflowExperiment(Experiment):\n    \"\"\"\n    Use mlflow to implement Experiment.\n    \"\"\"\n\n    def __init__(self, id, name, uri):\n        super(MLflowExperiment, self).__init__(id, name)\n        self._uri = uri\n        self._default_name = None\n        self._default_rec_name = \"mlflow_recorder\"\n        self._client = mlflow.tracking.MlflowClient(tracking_uri=self._uri)\n\n    def __repr__(self):\n        return \"{name}(id={id}, info={info})\".format(name=self.__class__.__name__, id=self.id, info=self.info)\n\n    def start(self, recorder_name=None, resume=False):\n        logger.info(f\"Experiment {self.id} starts running ...\")\n        # Get or create recorder\n        if recorder_name is None:\n            recorder_name = self._default_rec_name\n        # resume the recorder\n        if resume:\n            recorder, _ = self._get_or_create_rec(recorder_name=recorder_name)\n        # create a new recorder\n        else:\n            recorder = self.create_recorder(recorder_name)\n        # Set up active recorder\n        self.active_recorder = recorder\n        # Start the recorder\n        self.active_recorder.start_run()\n\n        return self.active_recorder\n\n    def end(self, recorder_status):\n        if self.active_recorder is not None:\n            self.active_recorder.end_run(recorder_status)\n            self.active_recorder = None\n\n    def create_recorder(self, recorder_name=None):\n        if recorder_name is None:\n            recorder_name = self._default_rec_name\n        recorder = MLflowRecorder(self.id, self._uri, recorder_name)\n\n        return recorder\n\n    def _get_recorder(self, recorder_id=None, recorder_name=None):\n        \"\"\"\n        Method for getting or creating a recorder. It will try to first get a valid recorder, if exception occurs, it will\n        raise errors.\n        \"\"\"\n        assert (\n            recorder_id is not None or recorder_name is not None\n        ), \"Please input at least one of recorder id or name before retrieving recorder.\"\n        if recorder_id is not None:\n            try:\n                run = self._client.get_run(recorder_id)\n                recorder = MLflowRecorder(self.id, self._uri, mlflow_run=run)\n                return recorder\n            except MlflowException:\n                raise ValueError(\"No valid recorder has been found, please make sure the input recorder id is correct.\")\n        elif recorder_name is not None:\n            logger.warning(\n                f\"Please make sure the recorder name {recorder_name} is unique, we will only return the latest recorder if there exist several matched the given name.\"\n            )\n            recorders = self.list_recorders()\n            for rid in recorders:\n                if recorders[rid].name == recorder_name:\n                    return recorders[rid]\n            raise ValueError(\"No valid recorder has been found, please make sure the input recorder name is correct.\")\n\n    def search_records(self, **kwargs):\n        filter_string = \"\" if kwargs.get(\"filter_string\") is None else kwargs.get(\"filter_string\")\n        run_view_type = 1 if kwargs.get(\"run_view_type\") is None else kwargs.get(\"run_view_type\")\n        max_results = 100000 if kwargs.get(\"max_results\") is None else kwargs.get(\"max_results\")\n        order_by = kwargs.get(\"order_by\")\n\n        return self._client.search_runs([self.id], filter_string, run_view_type, max_results, order_by)\n\n    def delete_recorder(self, recorder_id=None, recorder_name=None):\n        assert (\n            recorder_id is not None or recorder_name is not None\n        ), \"Please input a valid recorder id or name before deleting.\"\n        try:\n            if recorder_id is not None:\n                self._client.delete_run(recorder_id)\n            else:\n                recorder = self._get_recorder(recorder_name=recorder_name)\n                self._client.delete_run(recorder.id)\n        except MlflowException as e:\n            raise Exception(\n                f\"Error: {e}. Something went wrong when deleting recorder. Please check if the name/id of the recorder is correct.\"\n            )\n\n    UNLIMITED = 50000  # FIXME: Mlflow can only list 50000 records at most!!!!!!!\n\n    def list_recorders(self, max_results=UNLIMITED):\n        runs = self._client.search_runs(self.id, run_view_type=ViewType.ACTIVE_ONLY, max_results=max_results)\n        recorders = dict()\n        for i in range(len(runs)):\n            recorder = MLflowRecorder(self.id, self._uri, mlflow_run=runs[i])\n            recorders[runs[i].info.run_id] = recorder\n\n        return recorders",
  "def __init__(self, id, name):\n        self.id = id\n        self.name = name\n        self.active_recorder = None",
  "def __repr__(self):\n        return \"{name}(id={id}, info={info})\".format(name=self.__class__.__name__, id=self.id, info=self.info)",
  "def __str__(self):\n        return str(self.info)",
  "def info(self):\n        recorders = self.list_recorders()\n        output = dict()\n        output[\"class\"] = \"Experiment\"\n        output[\"id\"] = self.id\n        output[\"name\"] = self.name\n        output[\"active_recorder\"] = self.active_recorder.id if self.active_recorder is not None else None\n        output[\"recorders\"] = list(recorders.keys())\n        return output",
  "def start(self, recorder_name=None, resume=False):\n        \"\"\"\n        Start the experiment and set it to be active. This method will also start a new recorder.\n\n        Parameters\n        ----------\n        recorder_name : str\n            the name of the recorder to be created.\n        resume : bool\n            whether to resume the first recorder\n\n        Returns\n        -------\n        An active recorder.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `start` method.\")",
  "def end(self, recorder_status=Recorder.STATUS_S):\n        \"\"\"\n        End the experiment.\n\n        Parameters\n        ----------\n        recorder_status : str\n            the status the recorder to be set with when ending (SCHEDULED, RUNNING, FINISHED, FAILED).\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `end` method.\")",
  "def create_recorder(self, recorder_name=None):\n        \"\"\"\n        Create a recorder for each experiment.\n\n        Parameters\n        ----------\n        recorder_name : str\n            the name of the recorder to be created.\n\n        Returns\n        -------\n        A recorder object.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `create_recorder` method.\")",
  "def search_records(self, **kwargs):\n        \"\"\"\n        Get a pandas DataFrame of records that fit the search criteria of the experiment.\n        Inputs are the search critera user want to apply.\n\n        Returns\n        -------\n        A pandas.DataFrame of records, where each metric, parameter, and tag\n        are expanded into their own columns named metrics.*, params.*, and tags.*\n        respectively. For records that don't have a particular metric, parameter, or tag, their\n        value will be (NumPy) Nan, None, or None respectively.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `search_records` method.\")",
  "def delete_recorder(self, recorder_id):\n        \"\"\"\n        Create a recorder for each experiment.\n\n        Parameters\n        ----------\n        recorder_id : str\n            the id of the recorder to be deleted.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `delete_recorder` method.\")",
  "def get_recorder(self, recorder_id=None, recorder_name=None, create: bool = True):\n        \"\"\"\n        Retrieve a Recorder for user. When user specify recorder id and name, the method will try to return the\n        specific recorder. When user does not provide recorder id or name, the method will try to return the current\n        active recorder. The `create` argument determines whether the method will automatically create a new recorder\n        according to user's specification if the recorder hasn't been created before\n\n        * If `create` is True:\n\n            * If `active recorder` exists:\n\n                * no id or name specified, return the active recorder.\n                * if id or name is specified, return the specified recorder. If no such exp found, create a new recorder with given id or name, and the recorder shoud be active.\n\n            * If `active recorder` not exists:\n\n                * no id or name specified, create a new recorder.\n                * if id or name is specified, return the specified experiment. If no such exp found, create a new recorder with given id or name, and the recorder shoud be active.\n\n        * Else If `create` is False:\n\n            * If `active recorder` exists:\n\n                * no id or name specified, return the active recorder.\n                * if id or name is specified, return the specified recorder. If no such exp found, raise Error.\n\n            * If `active recorder` not exists:\n\n                * no id or name specified, raise Error.\n                * if id or name is specified, return the specified recorder. If no such exp found, raise Error.\n\n        Parameters\n        ----------\n        recorder_id : str\n            the id of the recorder to be deleted.\n        recorder_name : str\n            the name of the recorder to be deleted.\n        create : boolean\n            create the recorder if it hasn't been created before.\n\n        Returns\n        -------\n        A recorder object.\n        \"\"\"\n        # special case of getting the recorder\n        if recorder_id is None and recorder_name is None:\n            if self.active_recorder is not None:\n                return self.active_recorder\n            recorder_name = self._default_rec_name\n        if create:\n            recorder, is_new = self._get_or_create_rec(recorder_id=recorder_id, recorder_name=recorder_name)\n        else:\n            recorder, is_new = self._get_recorder(recorder_id=recorder_id, recorder_name=recorder_name), False\n        if is_new:\n            self.active_recorder = recorder\n            # start the recorder\n            self.active_recorder.start_run()\n        return recorder",
  "def _get_or_create_rec(self, recorder_id=None, recorder_name=None) -> (object, bool):\n        \"\"\"\n        Method for getting or creating a recorder. It will try to first get a valid recorder, if exception occurs, it will\n        automatically create a new recorder based on the given id and name.\n        \"\"\"\n        try:\n            if recorder_id is None and recorder_name is None:\n                recorder_name = self._default_rec_name\n            return self._get_recorder(recorder_id=recorder_id, recorder_name=recorder_name), False\n        except ValueError:\n            if recorder_name is None:\n                recorder_name = self._default_rec_name\n            logger.info(f\"No valid recorder found. Create a new recorder with name {recorder_name}.\")\n            return self.create_recorder(recorder_name), True",
  "def _get_recorder(self, recorder_id=None, recorder_name=None):\n        \"\"\"\n        Get specific recorder by name or id. If it does not exist, raise ValueError\n\n        Parameters\n        ----------\n        recorder_id :\n            The id of recorder\n        recorder_name :\n            The name of recorder\n\n        Returns\n        -------\n        Recorder:\n            The searched recorder\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `_get_recorder` method\")",
  "def list_recorders(self):\n        \"\"\"\n        List all the existing recorders of this experiment. Please first get the experiment instance before calling this method.\n        If user want to use the method `R.list_recorders()`, please refer to the related API document in `QlibRecorder`.\n\n        Returns\n        -------\n        A dictionary (id -> recorder) of recorder information that being stored.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list_recorders` method.\")",
  "def __init__(self, id, name, uri):\n        super(MLflowExperiment, self).__init__(id, name)\n        self._uri = uri\n        self._default_name = None\n        self._default_rec_name = \"mlflow_recorder\"\n        self._client = mlflow.tracking.MlflowClient(tracking_uri=self._uri)",
  "def __repr__(self):\n        return \"{name}(id={id}, info={info})\".format(name=self.__class__.__name__, id=self.id, info=self.info)",
  "def start(self, recorder_name=None, resume=False):\n        logger.info(f\"Experiment {self.id} starts running ...\")\n        # Get or create recorder\n        if recorder_name is None:\n            recorder_name = self._default_rec_name\n        # resume the recorder\n        if resume:\n            recorder, _ = self._get_or_create_rec(recorder_name=recorder_name)\n        # create a new recorder\n        else:\n            recorder = self.create_recorder(recorder_name)\n        # Set up active recorder\n        self.active_recorder = recorder\n        # Start the recorder\n        self.active_recorder.start_run()\n\n        return self.active_recorder",
  "def end(self, recorder_status):\n        if self.active_recorder is not None:\n            self.active_recorder.end_run(recorder_status)\n            self.active_recorder = None",
  "def create_recorder(self, recorder_name=None):\n        if recorder_name is None:\n            recorder_name = self._default_rec_name\n        recorder = MLflowRecorder(self.id, self._uri, recorder_name)\n\n        return recorder",
  "def _get_recorder(self, recorder_id=None, recorder_name=None):\n        \"\"\"\n        Method for getting or creating a recorder. It will try to first get a valid recorder, if exception occurs, it will\n        raise errors.\n        \"\"\"\n        assert (\n            recorder_id is not None or recorder_name is not None\n        ), \"Please input at least one of recorder id or name before retrieving recorder.\"\n        if recorder_id is not None:\n            try:\n                run = self._client.get_run(recorder_id)\n                recorder = MLflowRecorder(self.id, self._uri, mlflow_run=run)\n                return recorder\n            except MlflowException:\n                raise ValueError(\"No valid recorder has been found, please make sure the input recorder id is correct.\")\n        elif recorder_name is not None:\n            logger.warning(\n                f\"Please make sure the recorder name {recorder_name} is unique, we will only return the latest recorder if there exist several matched the given name.\"\n            )\n            recorders = self.list_recorders()\n            for rid in recorders:\n                if recorders[rid].name == recorder_name:\n                    return recorders[rid]\n            raise ValueError(\"No valid recorder has been found, please make sure the input recorder name is correct.\")",
  "def search_records(self, **kwargs):\n        filter_string = \"\" if kwargs.get(\"filter_string\") is None else kwargs.get(\"filter_string\")\n        run_view_type = 1 if kwargs.get(\"run_view_type\") is None else kwargs.get(\"run_view_type\")\n        max_results = 100000 if kwargs.get(\"max_results\") is None else kwargs.get(\"max_results\")\n        order_by = kwargs.get(\"order_by\")\n\n        return self._client.search_runs([self.id], filter_string, run_view_type, max_results, order_by)",
  "def delete_recorder(self, recorder_id=None, recorder_name=None):\n        assert (\n            recorder_id is not None or recorder_name is not None\n        ), \"Please input a valid recorder id or name before deleting.\"\n        try:\n            if recorder_id is not None:\n                self._client.delete_run(recorder_id)\n            else:\n                recorder = self._get_recorder(recorder_name=recorder_name)\n                self._client.delete_run(recorder.id)\n        except MlflowException as e:\n            raise Exception(\n                f\"Error: {e}. Something went wrong when deleting recorder. Please check if the name/id of the recorder is correct.\"\n            )",
  "def list_recorders(self, max_results=UNLIMITED):\n        runs = self._client.search_runs(self.id, run_view_type=ViewType.ACTIVE_ONLY, max_results=max_results)\n        recorders = dict()\n        for i in range(len(runs)):\n            recorder = MLflowRecorder(self.id, self._uri, mlflow_run=runs[i])\n            recorders[runs[i].info.run_id] = recorder\n\n        return recorders",
  "class QlibRecorder:\n    \"\"\"\n    A global system that helps to manage the experiments.\n    \"\"\"\n\n    def __init__(self, exp_manager):\n        self.exp_manager = exp_manager\n\n    def __repr__(self):\n        return \"{name}(manager={manager})\".format(name=self.__class__.__name__, manager=self.exp_manager)\n\n    @contextmanager\n    def start(\n        self,\n        experiment_name: Optional[Text] = None,\n        recorder_name: Optional[Text] = None,\n        uri: Optional[Text] = None,\n        resume: bool = False,\n    ):\n        \"\"\"\n        Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:\n\n        .. code-block:: Python\n\n            # start new experiment and recorder\n            with R.start('test', 'recorder_1'):\n                model.fit(dataset)\n                R.log...\n                ... # further operations\n\n            # resume previous experiment and recorder\n            with R.start('test', 'recorder_1', resume=True): # if users want to resume recorder, they have to specify the exact same name for experiment and recorder.\n                ... # further operations\n\n        Parameters\n        ----------\n        experiment_name : str\n            name of the experiment one wants to start.\n        recorder_name : str\n            name of the recorder under the experiment one wants to start.\n        uri : str\n            The tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.\n            The default uri is set in the qlib.config. Note that this uri argument will not change the one defined in the config file.\n            Therefore, the next time when users call this function in the same experiment,\n            they have to also specify this argument with the same value. Otherwise, inconsistent uri may occur.\n        resume : bool\n            whether to resume the specific recorder with given name under the given experiment.\n        \"\"\"\n        run = self.start_exp(experiment_name, recorder_name, uri, resume)\n        try:\n            yield run\n        except Exception as e:\n            self.end_exp(Recorder.STATUS_FA)  # end the experiment if something went wrong\n            raise e\n        self.end_exp(Recorder.STATUS_FI)\n\n    def start_exp(self, experiment_name=None, recorder_name=None, uri=None, resume=False):\n        \"\"\"\n        Lower level method for starting an experiment. When use this method, one should end the experiment manually\n        and the status of the recorder may not be handled properly. Here is the example code:\n\n        .. code-block:: Python\n\n            R.start_exp(experiment_name='test', recorder_name='recorder_1')\n            ... # further operations\n            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)\n\n\n        Parameters\n        ----------\n        experiment_name : str\n            the name of the experiment to be started\n        recorder_name : str\n            name of the recorder under the experiment one wants to start.\n        uri : str\n            the tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.\n            The default uri are set in the qlib.config.\n        resume : bool\n            whether to resume the specific recorder with given name under the given experiment.\n\n        Returns\n        -------\n        An experiment instance being started.\n        \"\"\"\n        return self.exp_manager.start_exp(experiment_name, recorder_name, uri, resume)\n\n    def end_exp(self, recorder_status=Recorder.STATUS_FI):\n        \"\"\"\n        Method for ending an experiment manually. It will end the current active experiment, as well as its\n        active recorder with the specified `status` type. Here is the example code of the method:\n\n        .. code-block:: Python\n\n            R.start_exp(experiment_name='test')\n            ... # further operations\n            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)\n\n        Parameters\n        ----------\n        status : str\n            The status of a recorder, which can be SCHEDULED, RUNNING, FINISHED, FAILED.\n        \"\"\"\n        self.exp_manager.end_exp(recorder_status)\n\n    def search_records(self, experiment_ids, **kwargs):\n        \"\"\"\n        Get a pandas DataFrame of records that fit the search criteria.\n\n        The arguments of this function are not set to be rigid, and they will be different with different implementation of\n        ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the\n        example code of the this method with the ``MLflowExpManager``:\n\n        .. code-block:: Python\n\n            R.log_metrics(m=2.50, step=0)\n            records = R.search_runs([experiment_id], order_by=[\"metrics.m DESC\"])\n\n        Parameters\n        ----------\n        experiment_ids : list\n            list of experiment IDs.\n        filter_string : str\n            filter query string, defaults to searching all runs.\n        run_view_type : int\n            one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType).\n        max_results  : int\n            the maximum number of runs to put in the dataframe.\n        order_by : list\n            list of columns to order by (e.g., \u201cmetrics.rmse\u201d).\n\n        Returns\n        -------\n        A pandas.DataFrame of records, where each metric, parameter, and tag\n        are expanded into their own columns named metrics.*, params.*, and tags.*\n        respectively. For records that don't have a particular metric, parameter, or tag, their\n        value will be (NumPy) Nan, None, or None respectively.\n        \"\"\"\n        return self.exp_manager.search_records(experiment_ids, **kwargs)\n\n    def list_experiments(self):\n        \"\"\"\n        Method for listing all the existing experiments (except for those being deleted.)\n\n        .. code-block:: Python\n\n            exps = R.list_experiments()\n\n        Returns\n        -------\n        A dictionary (name -> experiment) of experiments information that being stored.\n        \"\"\"\n        return self.exp_manager.list_experiments()\n\n    def list_recorders(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for listing all the recorders of experiment with given id or name.\n\n        If user doesn't provide the id or name of the experiment, this method will try to retrieve the default experiment and\n        list all the recorders of the default experiment. If the default experiment doesn't exist, the method will first\n        create the default experiment, and then create a new recorder under it. (More information about the default experiment\n        can be found `here <../component/recorder.html#qlib.workflow.exp.Experiment>`_).\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            recorders = R.list_recorders(experiment_name='test')\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n\n        Returns\n        -------\n        A dictionary (id -> recorder) of recorder information that being stored.\n        \"\"\"\n        return self.get_exp(experiment_id, experiment_name).list_recorders()\n\n    def get_exp(self, experiment_id=None, experiment_name=None, create: bool = True) -> Experiment:\n        \"\"\"\n        Method for retrieving an experiment with given id or name. Once the `create` argument is set to\n        True, if no valid experiment is found, this method will create one for you. Otherwise, it will\n        only retrieve a specific experiment or raise an Error.\n\n        - If '`create`' is True:\n\n            - If `active experiment` exists:\n\n                - no id or name specified, return the active experiment.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name, and the experiment is set to be active.\n\n            - If `active experiment` not exists:\n\n                - no id or name specified, create a default experiment, and the experiment is set to be active.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given name or the default experiment, and the experiment is set to be active.\n\n        - Else If '`create`' is False:\n\n            - If ``active experiment` exists:\n\n                - no id or name specified, return the active experiment.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n            - If `active experiment` not exists:\n\n                - no id or name specified. If the default experiment exists, return it, otherwise, raise Error.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                exp = R.get_exp()\n                recorders = exp.list_recorders()\n\n            # Case 2\n            with R.start('test'):\n                exp = R.get_exp('test1')\n\n            # Case 3\n            exp = R.get_exp() -> a default experiment.\n\n            # Case 4\n            exp = R.get_exp(experiment_name='test')\n\n            # Case 5\n            exp = R.get_exp(create=False) -> the default experiment if exists.\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n        create : boolean\n            an argument determines whether the method will automatically create a new experiment\n            according to user's specification if the experiment hasn't been created before.\n\n        Returns\n        -------\n        An experiment instance with given id or name.\n        \"\"\"\n        return self.exp_manager.get_exp(experiment_id, experiment_name, create)\n\n    def delete_exp(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for deleting the experiment with given id or name. At least one of id or name must be given,\n        otherwise, error will occur.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            R.delete_exp(experiment_name='test')\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n        \"\"\"\n        self.exp_manager.delete_exp(experiment_id, experiment_name)\n\n    def get_uri(self):\n        \"\"\"\n        Method for retrieving the uri of current experiment manager.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            uri = R.get_uri()\n\n        Returns\n        -------\n        The uri of current experiment manager.\n        \"\"\"\n        return self.exp_manager.uri\n\n    def set_uri(self, uri: Optional[Text]):\n        \"\"\"\n        Method to reset the current uri of current experiment manager.\n        \"\"\"\n        self.exp_manager.set_uri(uri)\n\n    def get_recorder(self, recorder_id=None, recorder_name=None, experiment_name=None):\n        \"\"\"\n        Method for retrieving a recorder.\n\n        - If `active recorder` exists:\n\n            - no id or name specified, return the active recorder.\n\n            - if id or name is specified, return the specified recorder.\n\n        - If `active recorder` not exists:\n\n            - no id or name specified, raise Error.\n\n            - if id or name is specified, and the corresponding experiment_name must be given, return the specified recorder. Otherwise, raise Error.\n\n        The recorder can be used for further process such as `save_object`, `load_object`, `log_params`,\n        `log_metrics`, etc.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                recorder = R.get_recorder()\n\n            # Case 2\n            with R.start('test'):\n                recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')\n\n            # Case 3\n            recorder = R.get_recorder() -> Error\n\n            # Case 4\n            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d') -> Error\n\n            # Case 5\n            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d', experiment_name='test')\n\n        Parameters\n        ----------\n        recorder_id : str\n            id of the recorder.\n        recorder_name : str\n            name of the recorder.\n        experiment_name : str\n            name of the experiment.\n\n        Returns\n        -------\n        A recorder instance.\n        \"\"\"\n        return self.get_exp(experiment_name=experiment_name, create=False).get_recorder(\n            recorder_id, recorder_name, create=False\n        )\n\n    def delete_recorder(self, recorder_id=None, recorder_name=None):\n        \"\"\"\n        Method for deleting the recorders with given id or name. At least one of id or name must be given,\n        otherwise, error will occur.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            R.delete_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')\n\n        Parameters\n        ----------\n        recorder_id : str\n            id of the experiment.\n        recorder_name : str\n            name of the experiment.\n        \"\"\"\n        self.get_exp().delete_recorder(recorder_id, recorder_name)\n\n    def save_objects(self, local_path=None, artifact_path=None, **kwargs):\n        \"\"\"\n        Method for saving objects as artifacts in the experiment to the uri. It supports either saving\n        from a local file/directory, or directly saving objects. User can use valid python's keywords arguments\n        to specify the object to be saved as well as its name (name: value).\n\n        - If `active recorder` exists: it will save the objects through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment, and a new recorder and save objects under it.\n\n        .. note::\n\n            If one wants to save objects with a specific recorder. It is recommended to first get the specific recorder through `get_recorder` API and use the recorder the save objects. The supported arguments are the same as this method.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                pred = model.predict(dataset)\n                R.save_objects(**{\"pred.pkl\": pred}, artifact_path='prediction')\n\n            # Case 2\n            with R.start('test'):\n                R.save_objects(local_path='results/pred.pkl')\n\n        Parameters\n        ----------\n        local_path : str\n            if provided, them save the file or directory to the artifact URI.\n        artifact_path : str\n            the relative path for the artifact to be stored in the URI.\n        \"\"\"\n        self.get_exp().get_recorder().save_objects(local_path, artifact_path, **kwargs)\n\n    def log_params(self, **kwargs):\n        \"\"\"\n        Method for logging parameters during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will log parameters through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log parameters under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.log_params(learning_rate=0.01)\n\n            # Case 2\n            R.log_params(learning_rate=0.01)\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().log_params(**kwargs)\n\n    def log_metrics(self, step=None, **kwargs):\n        \"\"\"\n        Method for logging metrics during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will log metrics through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log metrics under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.log_metrics(train_loss=0.33, step=1)\n\n            # Case 2\n            R.log_metrics(train_loss=0.33, step=1)\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().log_metrics(step, **kwargs)\n\n    def set_tags(self, **kwargs):\n        \"\"\"\n        Method for setting tags for a recorder. In addition to using ``R``, one can also set the tag to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will set tags through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.set_tags(release_version=\"2.2.0\")\n\n            # Case 2\n            R.set_tags(release_version=\"2.2.0\")\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().set_tags(**kwargs)",
  "def __init__(self, exp_manager):\n        self.exp_manager = exp_manager",
  "def __repr__(self):\n        return \"{name}(manager={manager})\".format(name=self.__class__.__name__, manager=self.exp_manager)",
  "def start(\n        self,\n        experiment_name: Optional[Text] = None,\n        recorder_name: Optional[Text] = None,\n        uri: Optional[Text] = None,\n        resume: bool = False,\n    ):\n        \"\"\"\n        Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:\n\n        .. code-block:: Python\n\n            # start new experiment and recorder\n            with R.start('test', 'recorder_1'):\n                model.fit(dataset)\n                R.log...\n                ... # further operations\n\n            # resume previous experiment and recorder\n            with R.start('test', 'recorder_1', resume=True): # if users want to resume recorder, they have to specify the exact same name for experiment and recorder.\n                ... # further operations\n\n        Parameters\n        ----------\n        experiment_name : str\n            name of the experiment one wants to start.\n        recorder_name : str\n            name of the recorder under the experiment one wants to start.\n        uri : str\n            The tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.\n            The default uri is set in the qlib.config. Note that this uri argument will not change the one defined in the config file.\n            Therefore, the next time when users call this function in the same experiment,\n            they have to also specify this argument with the same value. Otherwise, inconsistent uri may occur.\n        resume : bool\n            whether to resume the specific recorder with given name under the given experiment.\n        \"\"\"\n        run = self.start_exp(experiment_name, recorder_name, uri, resume)\n        try:\n            yield run\n        except Exception as e:\n            self.end_exp(Recorder.STATUS_FA)  # end the experiment if something went wrong\n            raise e\n        self.end_exp(Recorder.STATUS_FI)",
  "def start_exp(self, experiment_name=None, recorder_name=None, uri=None, resume=False):\n        \"\"\"\n        Lower level method for starting an experiment. When use this method, one should end the experiment manually\n        and the status of the recorder may not be handled properly. Here is the example code:\n\n        .. code-block:: Python\n\n            R.start_exp(experiment_name='test', recorder_name='recorder_1')\n            ... # further operations\n            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)\n\n\n        Parameters\n        ----------\n        experiment_name : str\n            the name of the experiment to be started\n        recorder_name : str\n            name of the recorder under the experiment one wants to start.\n        uri : str\n            the tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.\n            The default uri are set in the qlib.config.\n        resume : bool\n            whether to resume the specific recorder with given name under the given experiment.\n\n        Returns\n        -------\n        An experiment instance being started.\n        \"\"\"\n        return self.exp_manager.start_exp(experiment_name, recorder_name, uri, resume)",
  "def end_exp(self, recorder_status=Recorder.STATUS_FI):\n        \"\"\"\n        Method for ending an experiment manually. It will end the current active experiment, as well as its\n        active recorder with the specified `status` type. Here is the example code of the method:\n\n        .. code-block:: Python\n\n            R.start_exp(experiment_name='test')\n            ... # further operations\n            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)\n\n        Parameters\n        ----------\n        status : str\n            The status of a recorder, which can be SCHEDULED, RUNNING, FINISHED, FAILED.\n        \"\"\"\n        self.exp_manager.end_exp(recorder_status)",
  "def search_records(self, experiment_ids, **kwargs):\n        \"\"\"\n        Get a pandas DataFrame of records that fit the search criteria.\n\n        The arguments of this function are not set to be rigid, and they will be different with different implementation of\n        ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the\n        example code of the this method with the ``MLflowExpManager``:\n\n        .. code-block:: Python\n\n            R.log_metrics(m=2.50, step=0)\n            records = R.search_runs([experiment_id], order_by=[\"metrics.m DESC\"])\n\n        Parameters\n        ----------\n        experiment_ids : list\n            list of experiment IDs.\n        filter_string : str\n            filter query string, defaults to searching all runs.\n        run_view_type : int\n            one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType).\n        max_results  : int\n            the maximum number of runs to put in the dataframe.\n        order_by : list\n            list of columns to order by (e.g., \u201cmetrics.rmse\u201d).\n\n        Returns\n        -------\n        A pandas.DataFrame of records, where each metric, parameter, and tag\n        are expanded into their own columns named metrics.*, params.*, and tags.*\n        respectively. For records that don't have a particular metric, parameter, or tag, their\n        value will be (NumPy) Nan, None, or None respectively.\n        \"\"\"\n        return self.exp_manager.search_records(experiment_ids, **kwargs)",
  "def list_experiments(self):\n        \"\"\"\n        Method for listing all the existing experiments (except for those being deleted.)\n\n        .. code-block:: Python\n\n            exps = R.list_experiments()\n\n        Returns\n        -------\n        A dictionary (name -> experiment) of experiments information that being stored.\n        \"\"\"\n        return self.exp_manager.list_experiments()",
  "def list_recorders(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for listing all the recorders of experiment with given id or name.\n\n        If user doesn't provide the id or name of the experiment, this method will try to retrieve the default experiment and\n        list all the recorders of the default experiment. If the default experiment doesn't exist, the method will first\n        create the default experiment, and then create a new recorder under it. (More information about the default experiment\n        can be found `here <../component/recorder.html#qlib.workflow.exp.Experiment>`_).\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            recorders = R.list_recorders(experiment_name='test')\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n\n        Returns\n        -------\n        A dictionary (id -> recorder) of recorder information that being stored.\n        \"\"\"\n        return self.get_exp(experiment_id, experiment_name).list_recorders()",
  "def get_exp(self, experiment_id=None, experiment_name=None, create: bool = True) -> Experiment:\n        \"\"\"\n        Method for retrieving an experiment with given id or name. Once the `create` argument is set to\n        True, if no valid experiment is found, this method will create one for you. Otherwise, it will\n        only retrieve a specific experiment or raise an Error.\n\n        - If '`create`' is True:\n\n            - If `active experiment` exists:\n\n                - no id or name specified, return the active experiment.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name, and the experiment is set to be active.\n\n            - If `active experiment` not exists:\n\n                - no id or name specified, create a default experiment, and the experiment is set to be active.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given name or the default experiment, and the experiment is set to be active.\n\n        - Else If '`create`' is False:\n\n            - If ``active experiment` exists:\n\n                - no id or name specified, return the active experiment.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n            - If `active experiment` not exists:\n\n                - no id or name specified. If the default experiment exists, return it, otherwise, raise Error.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                exp = R.get_exp()\n                recorders = exp.list_recorders()\n\n            # Case 2\n            with R.start('test'):\n                exp = R.get_exp('test1')\n\n            # Case 3\n            exp = R.get_exp() -> a default experiment.\n\n            # Case 4\n            exp = R.get_exp(experiment_name='test')\n\n            # Case 5\n            exp = R.get_exp(create=False) -> the default experiment if exists.\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n        create : boolean\n            an argument determines whether the method will automatically create a new experiment\n            according to user's specification if the experiment hasn't been created before.\n\n        Returns\n        -------\n        An experiment instance with given id or name.\n        \"\"\"\n        return self.exp_manager.get_exp(experiment_id, experiment_name, create)",
  "def delete_exp(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for deleting the experiment with given id or name. At least one of id or name must be given,\n        otherwise, error will occur.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            R.delete_exp(experiment_name='test')\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n        \"\"\"\n        self.exp_manager.delete_exp(experiment_id, experiment_name)",
  "def get_uri(self):\n        \"\"\"\n        Method for retrieving the uri of current experiment manager.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            uri = R.get_uri()\n\n        Returns\n        -------\n        The uri of current experiment manager.\n        \"\"\"\n        return self.exp_manager.uri",
  "def set_uri(self, uri: Optional[Text]):\n        \"\"\"\n        Method to reset the current uri of current experiment manager.\n        \"\"\"\n        self.exp_manager.set_uri(uri)",
  "def get_recorder(self, recorder_id=None, recorder_name=None, experiment_name=None):\n        \"\"\"\n        Method for retrieving a recorder.\n\n        - If `active recorder` exists:\n\n            - no id or name specified, return the active recorder.\n\n            - if id or name is specified, return the specified recorder.\n\n        - If `active recorder` not exists:\n\n            - no id or name specified, raise Error.\n\n            - if id or name is specified, and the corresponding experiment_name must be given, return the specified recorder. Otherwise, raise Error.\n\n        The recorder can be used for further process such as `save_object`, `load_object`, `log_params`,\n        `log_metrics`, etc.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                recorder = R.get_recorder()\n\n            # Case 2\n            with R.start('test'):\n                recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')\n\n            # Case 3\n            recorder = R.get_recorder() -> Error\n\n            # Case 4\n            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d') -> Error\n\n            # Case 5\n            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d', experiment_name='test')\n\n        Parameters\n        ----------\n        recorder_id : str\n            id of the recorder.\n        recorder_name : str\n            name of the recorder.\n        experiment_name : str\n            name of the experiment.\n\n        Returns\n        -------\n        A recorder instance.\n        \"\"\"\n        return self.get_exp(experiment_name=experiment_name, create=False).get_recorder(\n            recorder_id, recorder_name, create=False\n        )",
  "def delete_recorder(self, recorder_id=None, recorder_name=None):\n        \"\"\"\n        Method for deleting the recorders with given id or name. At least one of id or name must be given,\n        otherwise, error will occur.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            R.delete_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')\n\n        Parameters\n        ----------\n        recorder_id : str\n            id of the experiment.\n        recorder_name : str\n            name of the experiment.\n        \"\"\"\n        self.get_exp().delete_recorder(recorder_id, recorder_name)",
  "def save_objects(self, local_path=None, artifact_path=None, **kwargs):\n        \"\"\"\n        Method for saving objects as artifacts in the experiment to the uri. It supports either saving\n        from a local file/directory, or directly saving objects. User can use valid python's keywords arguments\n        to specify the object to be saved as well as its name (name: value).\n\n        - If `active recorder` exists: it will save the objects through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment, and a new recorder and save objects under it.\n\n        .. note::\n\n            If one wants to save objects with a specific recorder. It is recommended to first get the specific recorder through `get_recorder` API and use the recorder the save objects. The supported arguments are the same as this method.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                pred = model.predict(dataset)\n                R.save_objects(**{\"pred.pkl\": pred}, artifact_path='prediction')\n\n            # Case 2\n            with R.start('test'):\n                R.save_objects(local_path='results/pred.pkl')\n\n        Parameters\n        ----------\n        local_path : str\n            if provided, them save the file or directory to the artifact URI.\n        artifact_path : str\n            the relative path for the artifact to be stored in the URI.\n        \"\"\"\n        self.get_exp().get_recorder().save_objects(local_path, artifact_path, **kwargs)",
  "def log_params(self, **kwargs):\n        \"\"\"\n        Method for logging parameters during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will log parameters through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log parameters under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.log_params(learning_rate=0.01)\n\n            # Case 2\n            R.log_params(learning_rate=0.01)\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().log_params(**kwargs)",
  "def log_metrics(self, step=None, **kwargs):\n        \"\"\"\n        Method for logging metrics during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will log metrics through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log metrics under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.log_metrics(train_loss=0.33, step=1)\n\n            # Case 2\n            R.log_metrics(train_loss=0.33, step=1)\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().log_metrics(step, **kwargs)",
  "def set_tags(self, **kwargs):\n        \"\"\"\n        Method for setting tags for a recorder. In addition to using ``R``, one can also set the tag to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will set tags through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.set_tags(release_version=\"2.2.0\")\n\n            # Case 2\n            R.set_tags(release_version=\"2.2.0\")\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().set_tags(**kwargs)",
  "class RecordTemp:\n    \"\"\"\n    This is the Records Template class that enables user to generate experiment results such as IC and\n    backtest in a certain format.\n    \"\"\"\n\n    artifact_path = None\n\n    @classmethod\n    def get_path(cls, path=None):\n        names = []\n        if cls.artifact_path is not None:\n            names.append(cls.artifact_path)\n\n        if path is not None:\n            names.append(path)\n\n        return \"/\".join(names)\n\n    def __init__(self, recorder):\n        self.recorder = recorder\n\n    def generate(self, **kwargs):\n        \"\"\"\n        Generate certain records such as IC, backtest etc., and save them.\n\n        Parameters\n        ----------\n        kwargs\n\n        Return\n        ------\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `generate` method.\")\n\n    def load(self, name):\n        \"\"\"\n        Load the stored records. Due to the fact that some problems occured when we tried to balancing a clean API\n        with the Python's inheritance. This method has to be used in a rather ugly way, and we will try to fix them\n        in the future::\n\n            sar = SigAnaRecord(recorder)\n            ic = sar.load(sar.get_path(\"ic.pkl\"))\n\n        Parameters\n        ----------\n        name : str\n            the name for the file to be load.\n\n        Return\n        ------\n        The stored records.\n        \"\"\"\n        # try to load the saved object\n        obj = self.recorder.load_object(name)\n        return obj\n\n    def list(self):\n        \"\"\"\n        List the stored records.\n\n        Return\n        ------\n        A list of all the stored records.\n        \"\"\"\n        return []\n\n    def check(self, parent=False):\n        \"\"\"\n        Check if the records is properly generated and saved.\n\n        Raise\n        ------\n        FileExistsError: whether the records are stored properly.\n        \"\"\"\n        artifacts = set(self.recorder.list_artifacts())\n        if parent:\n            # Downcasting have to be done here instead of using `super`\n            flist = self.__class__.__base__.list(self)  # pylint: disable=E1101\n        else:\n            flist = self.list()\n        for item in flist:\n            if item not in artifacts:\n                raise FileExistsError(item)",
  "class SignalRecord(RecordTemp):\n    \"\"\"\n    This is the Signal Record class that generates the signal prediction. This class inherits the ``RecordTemp`` class.\n    \"\"\"\n\n    def __init__(self, model=None, dataset=None, recorder=None):\n        super().__init__(recorder=recorder)\n        self.model = model\n        self.dataset = dataset\n\n    def generate(self, **kwargs):\n        # generate prediciton\n        pred = self.model.predict(self.dataset)\n        if isinstance(pred, pd.Series):\n            pred = pred.to_frame(\"score\")\n        self.recorder.save_objects(**{\"pred.pkl\": pred})\n\n        logger.info(\n            f\"Signal record 'pred.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}\"\n        )\n        # print out results\n        pprint(f\"The following are prediction results of the {type(self.model).__name__} model.\")\n        pprint(pred.head(5))\n\n        if isinstance(self.dataset, DatasetH):\n            # NOTE:\n            # Python doesn't provide the downcasting mechanism.\n            # We use the trick here to downcast the class\n            orig_cls = self.dataset.__class__\n            self.dataset.__class__ = DatasetH\n\n            params = dict(segments=\"test\", col_set=\"label\", data_key=DataHandlerLP.DK_R)\n            try:\n                # Assume the backend handler is DataHandlerLP\n                raw_label = self.dataset.prepare(**params)\n            except TypeError:\n                # The argument number is not right\n                del params[\"data_key\"]\n                # The backend handler should be DataHandler\n                raw_label = self.dataset.prepare(**params)\n\n            self.recorder.save_objects(**{\"label.pkl\": raw_label})\n            self.dataset.__class__ = orig_cls\n\n    def list(self):\n        return [\"pred.pkl\", \"label.pkl\"]\n\n    def load(self, name=\"pred.pkl\"):\n        return super().load(name)",
  "class SigAnaRecord(SignalRecord):\n    \"\"\"\n    This is the Signal Analysis Record class that generates the analysis results such as IC and IR. This class inherits the ``RecordTemp`` class.\n    \"\"\"\n\n    artifact_path = \"sig_analysis\"\n\n    def __init__(self, recorder, ana_long_short=False, ann_scaler=252, **kwargs):\n        super().__init__(recorder=recorder, **kwargs)\n        self.ana_long_short = ana_long_short\n        self.ann_scaler = ann_scaler\n\n    def generate(self, **kwargs):\n        try:\n            self.check(parent=True)\n        except FileExistsError:\n            super().generate()\n\n        pred = self.load(\"pred.pkl\")\n        label = self.load(\"label.pkl\")\n        ic, ric = calc_ic(pred.iloc[:, 0], label.iloc[:, 0])\n        metrics = {\n            \"IC\": ic.mean(),\n            \"ICIR\": ic.mean() / ic.std(),\n            \"Rank IC\": ric.mean(),\n            \"Rank ICIR\": ric.mean() / ric.std(),\n        }\n        objects = {\"ic.pkl\": ic, \"ric.pkl\": ric}\n        if self.ana_long_short:\n            long_short_r, long_avg_r = calc_long_short_return(pred.iloc[:, 0], label.iloc[:, 0])\n            metrics.update(\n                {\n                    \"Long-Short Ann Return\": long_short_r.mean() * self.ann_scaler,\n                    \"Long-Short Ann Sharpe\": long_short_r.mean() / long_short_r.std() * self.ann_scaler ** 0.5,\n                    \"Long-Avg Ann Return\": long_avg_r.mean() * self.ann_scaler,\n                    \"Long-Avg Ann Sharpe\": long_avg_r.mean() / long_avg_r.std() * self.ann_scaler ** 0.5,\n                }\n            )\n            objects.update(\n                {\n                    \"long_short_r.pkl\": long_short_r,\n                    \"long_avg_r.pkl\": long_avg_r,\n                }\n            )\n        self.recorder.log_metrics(**metrics)\n        self.recorder.save_objects(**objects, artifact_path=self.get_path())\n        pprint(metrics)\n\n    def list(self):\n        paths = [self.get_path(\"ic.pkl\"), self.get_path(\"ric.pkl\")]\n        if self.ana_long_short:\n            paths.extend([self.get_path(\"long_short_r.pkl\"), self.get_path(\"long_avg_r.pkl\")])\n        return paths",
  "class PortAnaRecord(SignalRecord):\n    \"\"\"\n    This is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.\n\n    The following files will be stored in recorder\n    - report_normal.pkl & positions_normal.pkl:\n        - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest`\n    - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`\n    \"\"\"\n\n    artifact_path = \"portfolio_analysis\"\n\n    def __init__(self, recorder, config, **kwargs):\n        \"\"\"\n        config[\"strategy\"] : dict\n            define the strategy class as well as the kwargs.\n        config[\"backtest\"] : dict\n            define the backtest kwargs.\n        \"\"\"\n        super().__init__(recorder=recorder, **kwargs)\n\n        self.strategy_config = config[\"strategy\"]\n        self.backtest_config = config[\"backtest\"]\n        self.strategy = init_instance_by_config(self.strategy_config, accept_types=BaseStrategy)\n\n    def generate(self, **kwargs):\n        # check previously stored prediction results\n        try:\n            self.check(parent=True)  # \"Make sure the parent process is completed and store the data properly.\"\n        except FileExistsError:\n            super().generate()\n\n        # custom strategy and get backtest\n        pred_score = super().load(\"pred.pkl\")\n        report_dict = normal_backtest(pred_score, strategy=self.strategy, **self.backtest_config)\n        report_normal = report_dict.get(\"report_df\")\n        positions_normal = report_dict.get(\"positions\")\n        self.recorder.save_objects(**{\"report_normal.pkl\": report_normal}, artifact_path=PortAnaRecord.get_path())\n        self.recorder.save_objects(**{\"positions_normal.pkl\": positions_normal}, artifact_path=PortAnaRecord.get_path())\n        order_normal = report_dict.get(\"order_list\")\n        if order_normal:\n            self.recorder.save_objects(**{\"order_normal.pkl\": order_normal}, artifact_path=PortAnaRecord.get_path())\n\n        # analysis\n        analysis = dict()\n        analysis[\"excess_return_without_cost\"] = risk_analysis(report_normal[\"return\"] - report_normal[\"bench\"])\n        analysis[\"excess_return_with_cost\"] = risk_analysis(\n            report_normal[\"return\"] - report_normal[\"bench\"] - report_normal[\"cost\"]\n        )\n        # save portfolio analysis results\n        analysis_df = pd.concat(analysis)  # type: pd.DataFrame\n        # log metrics\n        self.recorder.log_metrics(**flatten_dict(analysis_df[\"risk\"].unstack().T.to_dict()))\n        # save results\n        self.recorder.save_objects(**{\"port_analysis.pkl\": analysis_df}, artifact_path=PortAnaRecord.get_path())\n        logger.info(\n            f\"Portfolio analysis record 'port_analysis.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}\"\n        )\n        # print out results\n        pprint(\"The following are analysis results of the excess return without cost.\")\n        pprint(analysis[\"excess_return_without_cost\"])\n        pprint(\"The following are analysis results of the excess return with cost.\")\n        pprint(analysis[\"excess_return_with_cost\"])\n\n    def list(self):\n        return [\n            PortAnaRecord.get_path(\"report_normal.pkl\"),\n            PortAnaRecord.get_path(\"positions_normal.pkl\"),\n            PortAnaRecord.get_path(\"port_analysis.pkl\"),\n        ]",
  "def get_path(cls, path=None):\n        names = []\n        if cls.artifact_path is not None:\n            names.append(cls.artifact_path)\n\n        if path is not None:\n            names.append(path)\n\n        return \"/\".join(names)",
  "def __init__(self, recorder):\n        self.recorder = recorder",
  "def generate(self, **kwargs):\n        \"\"\"\n        Generate certain records such as IC, backtest etc., and save them.\n\n        Parameters\n        ----------\n        kwargs\n\n        Return\n        ------\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `generate` method.\")",
  "def load(self, name):\n        \"\"\"\n        Load the stored records. Due to the fact that some problems occured when we tried to balancing a clean API\n        with the Python's inheritance. This method has to be used in a rather ugly way, and we will try to fix them\n        in the future::\n\n            sar = SigAnaRecord(recorder)\n            ic = sar.load(sar.get_path(\"ic.pkl\"))\n\n        Parameters\n        ----------\n        name : str\n            the name for the file to be load.\n\n        Return\n        ------\n        The stored records.\n        \"\"\"\n        # try to load the saved object\n        obj = self.recorder.load_object(name)\n        return obj",
  "def list(self):\n        \"\"\"\n        List the stored records.\n\n        Return\n        ------\n        A list of all the stored records.\n        \"\"\"\n        return []",
  "def check(self, parent=False):\n        \"\"\"\n        Check if the records is properly generated and saved.\n\n        Raise\n        ------\n        FileExistsError: whether the records are stored properly.\n        \"\"\"\n        artifacts = set(self.recorder.list_artifacts())\n        if parent:\n            # Downcasting have to be done here instead of using `super`\n            flist = self.__class__.__base__.list(self)  # pylint: disable=E1101\n        else:\n            flist = self.list()\n        for item in flist:\n            if item not in artifacts:\n                raise FileExistsError(item)",
  "def __init__(self, model=None, dataset=None, recorder=None):\n        super().__init__(recorder=recorder)\n        self.model = model\n        self.dataset = dataset",
  "def generate(self, **kwargs):\n        # generate prediciton\n        pred = self.model.predict(self.dataset)\n        if isinstance(pred, pd.Series):\n            pred = pred.to_frame(\"score\")\n        self.recorder.save_objects(**{\"pred.pkl\": pred})\n\n        logger.info(\n            f\"Signal record 'pred.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}\"\n        )\n        # print out results\n        pprint(f\"The following are prediction results of the {type(self.model).__name__} model.\")\n        pprint(pred.head(5))\n\n        if isinstance(self.dataset, DatasetH):\n            # NOTE:\n            # Python doesn't provide the downcasting mechanism.\n            # We use the trick here to downcast the class\n            orig_cls = self.dataset.__class__\n            self.dataset.__class__ = DatasetH\n\n            params = dict(segments=\"test\", col_set=\"label\", data_key=DataHandlerLP.DK_R)\n            try:\n                # Assume the backend handler is DataHandlerLP\n                raw_label = self.dataset.prepare(**params)\n            except TypeError:\n                # The argument number is not right\n                del params[\"data_key\"]\n                # The backend handler should be DataHandler\n                raw_label = self.dataset.prepare(**params)\n\n            self.recorder.save_objects(**{\"label.pkl\": raw_label})\n            self.dataset.__class__ = orig_cls",
  "def list(self):\n        return [\"pred.pkl\", \"label.pkl\"]",
  "def load(self, name=\"pred.pkl\"):\n        return super().load(name)",
  "def __init__(self, recorder, ana_long_short=False, ann_scaler=252, **kwargs):\n        super().__init__(recorder=recorder, **kwargs)\n        self.ana_long_short = ana_long_short\n        self.ann_scaler = ann_scaler",
  "def generate(self, **kwargs):\n        try:\n            self.check(parent=True)\n        except FileExistsError:\n            super().generate()\n\n        pred = self.load(\"pred.pkl\")\n        label = self.load(\"label.pkl\")\n        ic, ric = calc_ic(pred.iloc[:, 0], label.iloc[:, 0])\n        metrics = {\n            \"IC\": ic.mean(),\n            \"ICIR\": ic.mean() / ic.std(),\n            \"Rank IC\": ric.mean(),\n            \"Rank ICIR\": ric.mean() / ric.std(),\n        }\n        objects = {\"ic.pkl\": ic, \"ric.pkl\": ric}\n        if self.ana_long_short:\n            long_short_r, long_avg_r = calc_long_short_return(pred.iloc[:, 0], label.iloc[:, 0])\n            metrics.update(\n                {\n                    \"Long-Short Ann Return\": long_short_r.mean() * self.ann_scaler,\n                    \"Long-Short Ann Sharpe\": long_short_r.mean() / long_short_r.std() * self.ann_scaler ** 0.5,\n                    \"Long-Avg Ann Return\": long_avg_r.mean() * self.ann_scaler,\n                    \"Long-Avg Ann Sharpe\": long_avg_r.mean() / long_avg_r.std() * self.ann_scaler ** 0.5,\n                }\n            )\n            objects.update(\n                {\n                    \"long_short_r.pkl\": long_short_r,\n                    \"long_avg_r.pkl\": long_avg_r,\n                }\n            )\n        self.recorder.log_metrics(**metrics)\n        self.recorder.save_objects(**objects, artifact_path=self.get_path())\n        pprint(metrics)",
  "def list(self):\n        paths = [self.get_path(\"ic.pkl\"), self.get_path(\"ric.pkl\")]\n        if self.ana_long_short:\n            paths.extend([self.get_path(\"long_short_r.pkl\"), self.get_path(\"long_avg_r.pkl\")])\n        return paths",
  "def __init__(self, recorder, config, **kwargs):\n        \"\"\"\n        config[\"strategy\"] : dict\n            define the strategy class as well as the kwargs.\n        config[\"backtest\"] : dict\n            define the backtest kwargs.\n        \"\"\"\n        super().__init__(recorder=recorder, **kwargs)\n\n        self.strategy_config = config[\"strategy\"]\n        self.backtest_config = config[\"backtest\"]\n        self.strategy = init_instance_by_config(self.strategy_config, accept_types=BaseStrategy)",
  "def generate(self, **kwargs):\n        # check previously stored prediction results\n        try:\n            self.check(parent=True)  # \"Make sure the parent process is completed and store the data properly.\"\n        except FileExistsError:\n            super().generate()\n\n        # custom strategy and get backtest\n        pred_score = super().load(\"pred.pkl\")\n        report_dict = normal_backtest(pred_score, strategy=self.strategy, **self.backtest_config)\n        report_normal = report_dict.get(\"report_df\")\n        positions_normal = report_dict.get(\"positions\")\n        self.recorder.save_objects(**{\"report_normal.pkl\": report_normal}, artifact_path=PortAnaRecord.get_path())\n        self.recorder.save_objects(**{\"positions_normal.pkl\": positions_normal}, artifact_path=PortAnaRecord.get_path())\n        order_normal = report_dict.get(\"order_list\")\n        if order_normal:\n            self.recorder.save_objects(**{\"order_normal.pkl\": order_normal}, artifact_path=PortAnaRecord.get_path())\n\n        # analysis\n        analysis = dict()\n        analysis[\"excess_return_without_cost\"] = risk_analysis(report_normal[\"return\"] - report_normal[\"bench\"])\n        analysis[\"excess_return_with_cost\"] = risk_analysis(\n            report_normal[\"return\"] - report_normal[\"bench\"] - report_normal[\"cost\"]\n        )\n        # save portfolio analysis results\n        analysis_df = pd.concat(analysis)  # type: pd.DataFrame\n        # log metrics\n        self.recorder.log_metrics(**flatten_dict(analysis_df[\"risk\"].unstack().T.to_dict()))\n        # save results\n        self.recorder.save_objects(**{\"port_analysis.pkl\": analysis_df}, artifact_path=PortAnaRecord.get_path())\n        logger.info(\n            f\"Portfolio analysis record 'port_analysis.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}\"\n        )\n        # print out results\n        pprint(\"The following are analysis results of the excess return without cost.\")\n        pprint(analysis[\"excess_return_without_cost\"])\n        pprint(\"The following are analysis results of the excess return with cost.\")\n        pprint(analysis[\"excess_return_with_cost\"])",
  "def list(self):\n        return [\n            PortAnaRecord.get_path(\"report_normal.pkl\"),\n            PortAnaRecord.get_path(\"positions_normal.pkl\"),\n            PortAnaRecord.get_path(\"port_analysis.pkl\"),\n        ]",
  "class Expression(abc.ABC):\n    \"\"\"Expression base class\"\"\"\n\n    def __str__(self):\n        return type(self).__name__\n\n    def __repr__(self):\n        return str(self)\n\n    def __gt__(self, other):\n        from .ops import Gt\n\n        return Gt(self, other)\n\n    def __ge__(self, other):\n        from .ops import Ge\n\n        return Ge(self, other)\n\n    def __lt__(self, other):\n        from .ops import Lt\n\n        return Lt(self, other)\n\n    def __le__(self, other):\n        from .ops import Le\n\n        return Le(self, other)\n\n    def __eq__(self, other):\n        from .ops import Eq\n\n        return Eq(self, other)\n\n    def __ne__(self, other):\n        from .ops import Ne\n\n        return Ne(self, other)\n\n    def __add__(self, other):\n        from .ops import Add\n\n        return Add(self, other)\n\n    def __radd__(self, other):\n        from .ops import Add\n\n        return Add(other, self)\n\n    def __sub__(self, other):\n        from .ops import Sub\n\n        return Sub(self, other)\n\n    def __rsub__(self, other):\n        from .ops import Sub\n\n        return Sub(other, self)\n\n    def __mul__(self, other):\n        from .ops import Mul\n\n        return Mul(self, other)\n\n    def __rmul__(self, other):\n        from .ops import Mul\n\n        return Mul(self, other)\n\n    def __div__(self, other):\n        from .ops import Div\n\n        return Div(self, other)\n\n    def __rdiv__(self, other):\n        from .ops import Div\n\n        return Div(other, self)\n\n    def __truediv__(self, other):\n        from .ops import Div\n\n        return Div(self, other)\n\n    def __rtruediv__(self, other):\n        from .ops import Div\n\n        return Div(other, self)\n\n    def __pow__(self, other):\n        from .ops import Power\n\n        return Power(self, other)\n\n    def __and__(self, other):\n        from .ops import And\n\n        return And(self, other)\n\n    def __rand__(self, other):\n        from .ops import And\n\n        return And(other, self)\n\n    def __or__(self, other):\n        from .ops import Or\n\n        return Or(self, other)\n\n    def __ror__(self, other):\n        from .ops import Or\n\n        return Or(other, self)\n\n    def load(self, instrument, start_index, end_index, freq):\n        \"\"\"load  feature\n\n        Parameters\n        ----------\n        instrument : str\n            instrument code.\n        start_index : str\n            feature start index [in calendar].\n        end_index : str\n            feature end  index  [in calendar].\n        freq : str\n            feature frequency.\n\n        Returns\n        ----------\n        pd.Series\n            feature series: The index of the series is the calendar index\n        \"\"\"\n        from .cache import H\n\n        # cache\n        args = str(self), instrument, start_index, end_index, freq\n        if args in H[\"f\"]:\n            return H[\"f\"][args]\n        if start_index is None or end_index is None or start_index > end_index:\n            raise ValueError(\"Invalid index range: {} {}\".format(start_index, end_index))\n        series = self._load_internal(instrument, start_index, end_index, freq)\n        series.name = str(self)\n        H[\"f\"][args] = series\n        return series\n\n    @abc.abstractmethod\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        raise NotImplementedError(\"This function must be implemented in your newly defined feature\")\n\n    @abc.abstractmethod\n    def get_longest_back_rolling(self):\n        \"\"\"Get the longest length of historical data the feature has accessed\n\n        This is designed for getting the needed range of the data to calculate\n        the features in specific range at first.  However, situations like\n        Ref(Ref($close, -1), 1) can not be handled rightly.\n\n        So this will only used for detecting the length of historical data needed.\n        \"\"\"\n        # TODO: forward operator like Ref($close, -1) is not supported yet.\n        raise NotImplementedError(\"This function must be implemented in your newly defined feature\")\n\n    @abc.abstractmethod\n    def get_extended_window_size(self):\n        \"\"\"get_extend_window_size\n\n        For to calculate this Operator in range[start_index, end_index]\n        We have to get the *leaf feature* in\n        range[start_index - lft_etd, end_index + rght_etd].\n\n        Returns\n        ----------\n        (int, int)\n            lft_etd, rght_etd\n        \"\"\"\n        raise NotImplementedError(\"This function must be implemented in your newly defined feature\")",
  "class Feature(Expression):\n    \"\"\"Static Expression\n\n    This kind of feature will load data from provider\n    \"\"\"\n\n    def __init__(self, name=None):\n        if name:\n            self._name = name.lower()\n        else:\n            self._name = type(self).__name__.lower()\n\n    def __str__(self):\n        return \"$\" + self._name\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        # load\n        from .data import FeatureD\n\n        return FeatureD.feature(instrument, str(self), start_index, end_index, freq)\n\n    def get_longest_back_rolling(self):\n        return 0\n\n    def get_extended_window_size(self):\n        return 0, 0",
  "class ExpressionOps(Expression):\n    \"\"\"Operator Expression\n\n    This kind of feature will use operator for feature\n    construction on the fly.\n    \"\"\"\n\n    pass",
  "def __str__(self):\n        return type(self).__name__",
  "def __repr__(self):\n        return str(self)",
  "def __gt__(self, other):\n        from .ops import Gt\n\n        return Gt(self, other)",
  "def __ge__(self, other):\n        from .ops import Ge\n\n        return Ge(self, other)",
  "def __lt__(self, other):\n        from .ops import Lt\n\n        return Lt(self, other)",
  "def __le__(self, other):\n        from .ops import Le\n\n        return Le(self, other)",
  "def __eq__(self, other):\n        from .ops import Eq\n\n        return Eq(self, other)",
  "def __ne__(self, other):\n        from .ops import Ne\n\n        return Ne(self, other)",
  "def __add__(self, other):\n        from .ops import Add\n\n        return Add(self, other)",
  "def __radd__(self, other):\n        from .ops import Add\n\n        return Add(other, self)",
  "def __sub__(self, other):\n        from .ops import Sub\n\n        return Sub(self, other)",
  "def __rsub__(self, other):\n        from .ops import Sub\n\n        return Sub(other, self)",
  "def __mul__(self, other):\n        from .ops import Mul\n\n        return Mul(self, other)",
  "def __rmul__(self, other):\n        from .ops import Mul\n\n        return Mul(self, other)",
  "def __div__(self, other):\n        from .ops import Div\n\n        return Div(self, other)",
  "def __rdiv__(self, other):\n        from .ops import Div\n\n        return Div(other, self)",
  "def __truediv__(self, other):\n        from .ops import Div\n\n        return Div(self, other)",
  "def __rtruediv__(self, other):\n        from .ops import Div\n\n        return Div(other, self)",
  "def __pow__(self, other):\n        from .ops import Power\n\n        return Power(self, other)",
  "def __and__(self, other):\n        from .ops import And\n\n        return And(self, other)",
  "def __rand__(self, other):\n        from .ops import And\n\n        return And(other, self)",
  "def __or__(self, other):\n        from .ops import Or\n\n        return Or(self, other)",
  "def __ror__(self, other):\n        from .ops import Or\n\n        return Or(other, self)",
  "def load(self, instrument, start_index, end_index, freq):\n        \"\"\"load  feature\n\n        Parameters\n        ----------\n        instrument : str\n            instrument code.\n        start_index : str\n            feature start index [in calendar].\n        end_index : str\n            feature end  index  [in calendar].\n        freq : str\n            feature frequency.\n\n        Returns\n        ----------\n        pd.Series\n            feature series: The index of the series is the calendar index\n        \"\"\"\n        from .cache import H\n\n        # cache\n        args = str(self), instrument, start_index, end_index, freq\n        if args in H[\"f\"]:\n            return H[\"f\"][args]\n        if start_index is None or end_index is None or start_index > end_index:\n            raise ValueError(\"Invalid index range: {} {}\".format(start_index, end_index))\n        series = self._load_internal(instrument, start_index, end_index, freq)\n        series.name = str(self)\n        H[\"f\"][args] = series\n        return series",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        raise NotImplementedError(\"This function must be implemented in your newly defined feature\")",
  "def get_longest_back_rolling(self):\n        \"\"\"Get the longest length of historical data the feature has accessed\n\n        This is designed for getting the needed range of the data to calculate\n        the features in specific range at first.  However, situations like\n        Ref(Ref($close, -1), 1) can not be handled rightly.\n\n        So this will only used for detecting the length of historical data needed.\n        \"\"\"\n        # TODO: forward operator like Ref($close, -1) is not supported yet.\n        raise NotImplementedError(\"This function must be implemented in your newly defined feature\")",
  "def get_extended_window_size(self):\n        \"\"\"get_extend_window_size\n\n        For to calculate this Operator in range[start_index, end_index]\n        We have to get the *leaf feature* in\n        range[start_index - lft_etd, end_index + rght_etd].\n\n        Returns\n        ----------\n        (int, int)\n            lft_etd, rght_etd\n        \"\"\"\n        raise NotImplementedError(\"This function must be implemented in your newly defined feature\")",
  "def __init__(self, name=None):\n        if name:\n            self._name = name.lower()\n        else:\n            self._name = type(self).__name__.lower()",
  "def __str__(self):\n        return \"$\" + self._name",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        # load\n        from .data import FeatureD\n\n        return FeatureD.feature(instrument, str(self), start_index, end_index, freq)",
  "def get_longest_back_rolling(self):\n        return 0",
  "def get_extended_window_size(self):\n        return 0, 0",
  "class ElemOperator(ExpressionOps):\n    \"\"\"Element-wise Operator\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n\n    Returns\n    ----------\n    Expression\n        feature operation output\n    \"\"\"\n\n    def __init__(self, feature):\n        self.feature = feature\n\n    def __str__(self):\n        return \"{}({})\".format(type(self).__name__, self.feature)\n\n    def get_longest_back_rolling(self):\n        return self.feature.get_longest_back_rolling()\n\n    def get_extended_window_size(self):\n        return self.feature.get_extended_window_size()",
  "class NpElemOperator(ElemOperator):\n    \"\"\"Numpy Element-wise Operator\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    func : str\n        numpy feature operation method\n\n    Returns\n    ----------\n    Expression\n        feature operation output\n    \"\"\"\n\n    def __init__(self, feature, func):\n        self.func = func\n        super(NpElemOperator, self).__init__(feature)\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        return getattr(np, self.func)(series)",
  "class Abs(NpElemOperator):\n    \"\"\"Feature Absolute Value\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n\n    Returns\n    ----------\n    Expression\n        a feature instance with absolute output\n    \"\"\"\n\n    def __init__(self, feature):\n        super(Abs, self).__init__(feature, \"abs\")",
  "class Sign(NpElemOperator):\n    \"\"\"Feature Sign\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n\n    Returns\n    ----------\n    Expression\n        a feature instance with sign\n    \"\"\"\n\n    def __init__(self, feature):\n        super(Sign, self).__init__(feature, \"sign\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        \"\"\"\n        To avoid error raised by bool type input, we transform the data into float32.\n        \"\"\"\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # TODO:  More precision types should be configurable\n        series = series.astype(np.float32)\n        return getattr(np, self.func)(series)",
  "class Log(NpElemOperator):\n    \"\"\"Feature Log\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n\n    Returns\n    ----------\n    Expression\n        a feature instance with log\n    \"\"\"\n\n    def __init__(self, feature):\n        super(Log, self).__init__(feature, \"log\")",
  "class Power(NpElemOperator):\n    \"\"\"Feature Power\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n\n    Returns\n    ----------\n    Expression\n        a feature instance with power\n    \"\"\"\n\n    def __init__(self, feature, exponent):\n        super(Power, self).__init__(feature, \"power\")\n        self.exponent = exponent\n\n    def __str__(self):\n        return \"{}({},{})\".format(type(self).__name__, self.feature, self.exponent)\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        return getattr(np, self.func)(series, self.exponent)",
  "class Mask(NpElemOperator):\n    \"\"\"Feature Mask\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    instrument : str\n        instrument mask\n\n    Returns\n    ----------\n    Expression\n        a feature instance with masked instrument\n    \"\"\"\n\n    def __init__(self, feature, instrument):\n        super(Mask, self).__init__(feature, \"mask\")\n        self.instrument = instrument\n\n    def __str__(self):\n        return \"{}({},{})\".format(type(self).__name__, self.feature, self.instrument.lower())\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        return self.feature.load(self.instrument, start_index, end_index, freq)",
  "class Not(NpElemOperator):\n    \"\"\"Not Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        feature elementwise not output\n    \"\"\"\n\n    def __init__(self, feature):\n        super(Not, self).__init__(feature, \"bitwise_not\")",
  "class PairOperator(ExpressionOps):\n    \"\"\"Pair-wise operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance or numeric value\n    feature_right : Expression\n        feature instance or numeric value\n    func : str\n        operator function\n\n    Returns\n    ----------\n    Feature:\n        two features' operation output\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        self.feature_left = feature_left\n        self.feature_right = feature_right\n\n    def __str__(self):\n        return \"{}({},{})\".format(type(self).__name__, self.feature_left, self.feature_right)\n\n    def get_longest_back_rolling(self):\n        if isinstance(self.feature_left, Expression):\n            left_br = self.feature_left.get_longest_back_rolling()\n        else:\n            left_br = 0\n\n        if isinstance(self.feature_right, Expression):\n            right_br = self.feature_right.get_longest_back_rolling()\n        else:\n            right_br = 0\n        return max(left_br, right_br)\n\n    def get_extended_window_size(self):\n        if isinstance(self.feature_left, Expression):\n            ll, lr = self.feature_left.get_extended_window_size()\n        else:\n            ll, lr = 0, 0\n\n        if isinstance(self.feature_right, Expression):\n            rl, rr = self.feature_right.get_extended_window_size()\n        else:\n            rl, rr = 0, 0\n        return max(ll, rl), max(lr, rr)",
  "class NpPairOperator(PairOperator):\n    \"\"\"Numpy Pair-wise operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance or numeric value\n    feature_right : Expression\n        feature instance or numeric value\n    func : str\n        operator function\n\n    Returns\n    ----------\n    Feature:\n        two features' operation output\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right, func):\n        self.func = func\n        super(NpPairOperator, self).__init__(feature_left, feature_right)\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        assert any(\n            [isinstance(self.feature_left, Expression), self.feature_right, Expression]\n        ), \"at least one of two inputs is Expression instance\"\n        if isinstance(self.feature_left, Expression):\n            series_left = self.feature_left.load(instrument, start_index, end_index, freq)\n        else:\n            series_left = self.feature_left  # numeric value\n        if isinstance(self.feature_right, Expression):\n            series_right = self.feature_right.load(instrument, start_index, end_index, freq)\n        else:\n            series_right = self.feature_right\n        return getattr(np, self.func)(series_left, series_right)",
  "class Add(NpPairOperator):\n    \"\"\"Add Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        two features' sum\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Add, self).__init__(feature_left, feature_right, \"add\")",
  "class Sub(NpPairOperator):\n    \"\"\"Subtract Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        two features' subtraction\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Sub, self).__init__(feature_left, feature_right, \"subtract\")",
  "class Mul(NpPairOperator):\n    \"\"\"Multiply Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        two features' product\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Mul, self).__init__(feature_left, feature_right, \"multiply\")",
  "class Div(NpPairOperator):\n    \"\"\"Division Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        two features' division\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Div, self).__init__(feature_left, feature_right, \"divide\")",
  "class Greater(NpPairOperator):\n    \"\"\"Greater Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        greater elements taken from the input two features\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Greater, self).__init__(feature_left, feature_right, \"maximum\")",
  "class Less(NpPairOperator):\n    \"\"\"Less Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        smaller elements taken from the input two features\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Less, self).__init__(feature_left, feature_right, \"minimum\")",
  "class Gt(NpPairOperator):\n    \"\"\"Greater Than Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        bool series indicate `left > right`\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Gt, self).__init__(feature_left, feature_right, \"greater\")",
  "class Ge(NpPairOperator):\n    \"\"\"Greater Equal Than Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        bool series indicate `left >= right`\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Ge, self).__init__(feature_left, feature_right, \"greater_equal\")",
  "class Lt(NpPairOperator):\n    \"\"\"Less Than Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        bool series indicate `left < right`\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Lt, self).__init__(feature_left, feature_right, \"less\")",
  "class Le(NpPairOperator):\n    \"\"\"Less Equal Than Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        bool series indicate `left <= right`\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Le, self).__init__(feature_left, feature_right, \"less_equal\")",
  "class Eq(NpPairOperator):\n    \"\"\"Equal Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        bool series indicate `left == right`\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Eq, self).__init__(feature_left, feature_right, \"equal\")",
  "class Ne(NpPairOperator):\n    \"\"\"Not Equal Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        bool series indicate `left != right`\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Ne, self).__init__(feature_left, feature_right, \"not_equal\")",
  "class And(NpPairOperator):\n    \"\"\"And Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        two features' row by row & output\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(And, self).__init__(feature_left, feature_right, \"bitwise_and\")",
  "class Or(NpPairOperator):\n    \"\"\"Or Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n\n    Returns\n    ----------\n    Feature:\n        two features' row by row | outputs\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right):\n        super(Or, self).__init__(feature_left, feature_right, \"bitwise_or\")",
  "class If(ExpressionOps):\n    \"\"\"If Operator\n\n    Parameters\n    ----------\n    condition : Expression\n        feature instance with bool values as condition\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n    \"\"\"\n\n    def __init__(self, condition, feature_left, feature_right):\n        self.condition = condition\n        self.feature_left = feature_left\n        self.feature_right = feature_right\n\n    def __str__(self):\n        return \"If({},{},{})\".format(self.condition, self.feature_left, self.feature_right)\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series_cond = self.condition.load(instrument, start_index, end_index, freq)\n        if isinstance(self.feature_left, Expression):\n            series_left = self.feature_left.load(instrument, start_index, end_index, freq)\n        else:\n            series_left = self.feature_left\n        if isinstance(self.feature_right, Expression):\n            series_right = self.feature_right.load(instrument, start_index, end_index, freq)\n        else:\n            series_right = self.feature_right\n        series = pd.Series(np.where(series_cond, series_left, series_right), index=series_cond.index)\n        return series\n\n    def get_longest_back_rolling(self):\n        if isinstance(self.feature_left, Expression):\n            left_br = self.feature_left.get_longest_back_rolling()\n        else:\n            left_br = 0\n\n        if isinstance(self.feature_right, Expression):\n            right_br = self.feature_right.get_longest_back_rolling()\n        else:\n            right_br = 0\n\n        if isinstance(self.condition, Expression):\n            c_br = self.condition.get_longest_back_rolling()\n        else:\n            c_br = 0\n        return max(left_br, right_br, c_br)\n\n    def get_extended_window_size(self):\n        if isinstance(self.feature_left, Expression):\n            ll, lr = self.feature_left.get_extended_window_size()\n        else:\n            ll, lr = 0, 0\n\n        if isinstance(self.feature_right, Expression):\n            rl, rr = self.feature_right.get_extended_window_size()\n        else:\n            rl, rr = 0, 0\n\n        if isinstance(self.condition, Expression):\n            cl, cr = self.condition.get_extended_window_size()\n        else:\n            cl, cr = 0, 0\n        return max(ll, rl, cl), max(lr, rr, cr)",
  "class Rolling(ExpressionOps):\n    \"\"\"Rolling Operator\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n    func : str\n        rolling method\n\n    Returns\n    ----------\n    Expression\n        rolling outputs\n    \"\"\"\n\n    def __init__(self, feature, N, func):\n        self.feature = feature\n        self.N = N\n        self.func = func\n\n    def __str__(self):\n        return \"{}({},{})\".format(type(self).__name__, self.feature, self.N)\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # NOTE: remove all null check,\n        # now it's user's responsibility to decide whether use features in null days\n        # isnull = series.isnull() # NOTE: isnull = NaN, inf is not null\n        if self.N == 0:\n            series = getattr(series.expanding(min_periods=1), self.func)()\n        elif 0 < self.N < 1:\n            series = series.ewm(alpha=self.N, min_periods=1).mean()\n        else:\n            series = getattr(series.rolling(self.N, min_periods=1), self.func)()\n            # series.iloc[:self.N-1] = np.nan\n        # series[isnull] = np.nan\n        return series\n\n    def get_longest_back_rolling(self):\n        if self.N == 0:\n            return np.inf\n        if 0 < self.N < 1:\n            return int(np.log(1e-6) / np.log(1 - self.N))  # (1 - N)**window == 1e-6\n        return self.feature.get_longest_back_rolling() + self.N - 1\n\n    def get_extended_window_size(self):\n        if self.N == 0:\n            # FIXME: How to make this accurate and efficiently? Or  should we\n            # remove such support for N == 0?\n            get_module_logger(self.__class__.__name__).warning(\"The Rolling(ATTR, 0) will not be accurately calculated\")\n            return self.feature.get_extended_window_size()\n        elif 0 < self.N < 1:\n            lft_etd, rght_etd = self.feature.get_extended_window_size()\n            size = int(np.log(1e-6) / np.log(1 - self.N))\n            lft_etd = max(lft_etd + size - 1, lft_etd)\n            return lft_etd, rght_etd\n        else:\n            lft_etd, rght_etd = self.feature.get_extended_window_size()\n            lft_etd = max(lft_etd + self.N - 1, lft_etd)\n            return lft_etd, rght_etd",
  "class Ref(Rolling):\n    \"\"\"Feature Reference\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        N = 0, retrieve the first data; N > 0, retrieve data of N periods ago; N < 0, future data\n\n    Returns\n    ----------\n    Expression\n        a feature instance with target reference\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Ref, self).__init__(feature, N, \"ref\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # N = 0, return first day\n        if series.empty:\n            return series  # Pandas bug, see: https://github.com/pandas-dev/pandas/issues/21049\n        elif self.N == 0:\n            series = pd.Series(series.iloc[0], index=series.index)\n        else:\n            series = series.shift(self.N)  # copy\n        return series\n\n    def get_longest_back_rolling(self):\n        if self.N == 0:\n            return np.inf\n        return self.feature.get_longest_back_rolling() + self.N\n\n    def get_extended_window_size(self):\n        if self.N == 0:\n            get_module_logger(self.__class__.__name__).warning(\"The Ref(ATTR, 0) will not be accurately calculated\")\n            return self.feature.get_extended_window_size()\n        else:\n            lft_etd, rght_etd = self.feature.get_extended_window_size()\n            lft_etd = max(lft_etd + self.N, lft_etd)\n            rght_etd = max(rght_etd - self.N, rght_etd)\n            return lft_etd, rght_etd",
  "class Mean(Rolling):\n    \"\"\"Rolling Mean (MA)\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling average\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Mean, self).__init__(feature, N, \"mean\")",
  "class Sum(Rolling):\n    \"\"\"Rolling Sum\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling sum\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Sum, self).__init__(feature, N, \"sum\")",
  "class Std(Rolling):\n    \"\"\"Rolling Std\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling std\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Std, self).__init__(feature, N, \"std\")",
  "class Var(Rolling):\n    \"\"\"Rolling Variance\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling variance\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Var, self).__init__(feature, N, \"var\")",
  "class Skew(Rolling):\n    \"\"\"Rolling Skewness\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling skewness\n    \"\"\"\n\n    def __init__(self, feature, N):\n        if N != 0 and N < 3:\n            raise ValueError(\"The rolling window size of Skewness operation should >= 3\")\n        super(Skew, self).__init__(feature, N, \"skew\")",
  "class Kurt(Rolling):\n    \"\"\"Rolling Kurtosis\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling kurtosis\n    \"\"\"\n\n    def __init__(self, feature, N):\n        if N != 0 and N < 4:\n            raise ValueError(\"The rolling window size of Kurtosis operation should >= 5\")\n        super(Kurt, self).__init__(feature, N, \"kurt\")",
  "class Max(Rolling):\n    \"\"\"Rolling Max\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling max\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Max, self).__init__(feature, N, \"max\")",
  "class IdxMax(Rolling):\n    \"\"\"Rolling Max Index\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling max index\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(IdxMax, self).__init__(feature, N, \"idxmax\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(lambda x: x.argmax() + 1, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(lambda x: x.argmax() + 1, raw=True)\n        return series",
  "class Min(Rolling):\n    \"\"\"Rolling Min\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling min\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Min, self).__init__(feature, N, \"min\")",
  "class IdxMin(Rolling):\n    \"\"\"Rolling Min Index\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling min index\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(IdxMin, self).__init__(feature, N, \"idxmin\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(lambda x: x.argmin() + 1, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(lambda x: x.argmin() + 1, raw=True)\n        return series",
  "class Quantile(Rolling):\n    \"\"\"Rolling Quantile\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling quantile\n    \"\"\"\n\n    def __init__(self, feature, N, qscore):\n        super(Quantile, self).__init__(feature, N, \"quantile\")\n        self.qscore = qscore\n\n    def __str__(self):\n        return \"{}({},{},{})\".format(type(self).__name__, self.feature, self.N, self.qscore)\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = series.expanding(min_periods=1).quantile(self.qscore)\n        else:\n            series = series.rolling(self.N, min_periods=1).quantile(self.qscore)\n        return series",
  "class Med(Rolling):\n    \"\"\"Rolling Median\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling median\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Med, self).__init__(feature, N, \"median\")",
  "class Mad(Rolling):\n    \"\"\"Rolling Mean Absolute Deviation\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling mean absolute deviation\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Mad, self).__init__(feature, N, \"mad\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # TODO: implement in Cython\n\n        def mad(x):\n            x1 = x[~np.isnan(x)]\n            return np.mean(np.abs(x1 - x1.mean()))\n\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(mad, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(mad, raw=True)\n        return series",
  "class Rank(Rolling):\n    \"\"\"Rolling Rank (Percentile)\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling rank\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Rank, self).__init__(feature, N, \"rank\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # TODO: implement in Cython\n\n        def rank(x):\n            if np.isnan(x[-1]):\n                return np.nan\n            x1 = x[~np.isnan(x)]\n            if x1.shape[0] == 0:\n                return np.nan\n            return percentileofscore(x1, x1[-1]) / len(x1)\n\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(rank, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(rank, raw=True)\n        return series",
  "class Count(Rolling):\n    \"\"\"Rolling Count\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling count of number of non-NaN elements\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Count, self).__init__(feature, N, \"count\")",
  "class Delta(Rolling):\n    \"\"\"Rolling Delta\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with end minus start in rolling window\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Delta, self).__init__(feature, N, \"delta\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = series - series.iloc[0]\n        else:\n            series = series - series.shift(self.N)\n        return series",
  "class Slope(Rolling):\n    \"\"\"Rolling Slope\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with linear regression slope of given window\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Slope, self).__init__(feature, N, \"slope\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = pd.Series(expanding_slope(series.values), index=series.index)\n        else:\n            series = pd.Series(rolling_slope(series.values, self.N), index=series.index)\n        return series",
  "class Rsquare(Rolling):\n    \"\"\"Rolling R-value Square\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with linear regression r-value square of given window\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Rsquare, self).__init__(feature, N, \"rsquare\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        _series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = pd.Series(expanding_rsquare(_series.values), index=_series.index)\n        else:\n            series = pd.Series(rolling_rsquare(_series.values, self.N), index=_series.index)\n            series.loc[np.isclose(_series.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)] = np.nan\n        return series",
  "class Resi(Rolling):\n    \"\"\"Rolling Regression Residuals\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with regression residuals of given window\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(Resi, self).__init__(feature, N, \"resi\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = pd.Series(expanding_resi(series.values), index=series.index)\n        else:\n            series = pd.Series(rolling_resi(series.values, self.N), index=series.index)\n        return series",
  "class WMA(Rolling):\n    \"\"\"Rolling WMA\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with weighted moving average output\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(WMA, self).__init__(feature, N, \"wma\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # TODO: implement in Cython\n\n        def weighted_mean(x):\n            w = np.arange(len(x))\n            w = w / w.sum()\n            return np.nanmean(w * x)\n\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(weighted_mean, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(weighted_mean, raw=True)\n        return series",
  "class EMA(Rolling):\n    \"\"\"Rolling Exponential Mean (EMA)\n\n    Parameters\n    ----------\n    feature : Expression\n        feature instance\n    N : int, float\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with regression r-value square of given window\n    \"\"\"\n\n    def __init__(self, feature, N):\n        super(EMA, self).__init__(feature, N, \"ema\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n\n        def exp_weighted_mean(x):\n            a = 1 - 2 / (1 + len(x))\n            w = a ** np.arange(len(x))[::-1]\n            w /= w.sum()\n            return np.nansum(w * x)\n\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(exp_weighted_mean, raw=True)\n        elif 0 < self.N < 1:\n            series = series.ewm(alpha=self.N, min_periods=1).mean()\n        else:\n            series = series.ewm(span=self.N, min_periods=1).mean()\n        return series",
  "class PairRolling(ExpressionOps):\n    \"\"\"Pair Rolling Operator\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling output of two input features\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right, N, func):\n        self.feature_left = feature_left\n        self.feature_right = feature_right\n        self.N = N\n        self.func = func\n\n    def __str__(self):\n        return \"{}({},{},{})\".format(type(self).__name__, self.feature_left, self.feature_right, self.N)\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        series_left = self.feature_left.load(instrument, start_index, end_index, freq)\n        series_right = self.feature_right.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = getattr(series_left.expanding(min_periods=1), self.func)(series_right)\n        else:\n            series = getattr(series_left.rolling(self.N, min_periods=1), self.func)(series_right)\n        return series\n\n    def get_longest_back_rolling(self):\n        if self.N == 0:\n            return np.inf\n        return (\n            max(self.feature_left.get_longest_back_rolling(), self.feature_right.get_longest_back_rolling())\n            + self.N\n            - 1\n        )\n\n    def get_extended_window_size(self):\n        if self.N == 0:\n            get_module_logger(self.__class__.__name__).warning(\n                \"The PairRolling(ATTR, 0) will not be accurately calculated\"\n            )\n            return self.feature.get_extended_window_size()\n        else:\n            ll, lr = self.feature_left.get_extended_window_size()\n            rl, rr = self.feature_right.get_extended_window_size()\n            return max(ll, rl) + self.N - 1, max(lr, rr)",
  "class Corr(PairRolling):\n    \"\"\"Rolling Correlation\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling correlation of two input features\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right, N):\n        super(Corr, self).__init__(feature_left, feature_right, N, \"corr\")\n\n    def _load_internal(self, instrument, start_index, end_index, freq):\n        res = super(Corr, self)._load_internal(instrument, start_index, end_index, freq)\n\n        # NOTE: Load uses MemCache, so calling load again will not cause performance degradation\n        series_left = self.feature_left.load(instrument, start_index, end_index, freq)\n        series_right = self.feature_right.load(instrument, start_index, end_index, freq)\n        res.loc[\n            np.isclose(series_left.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)\n            | np.isclose(series_right.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)\n        ] = np.nan\n        return res",
  "class Cov(PairRolling):\n    \"\"\"Rolling Covariance\n\n    Parameters\n    ----------\n    feature_left : Expression\n        feature instance\n    feature_right : Expression\n        feature instance\n    N : int\n        rolling window size\n\n    Returns\n    ----------\n    Expression\n        a feature instance with rolling max of two input features\n    \"\"\"\n\n    def __init__(self, feature_left, feature_right, N):\n        super(Cov, self).__init__(feature_left, feature_right, N, \"cov\")",
  "class OpsWrapper:\n    \"\"\"Ops Wrapper\"\"\"\n\n    def __init__(self):\n        self._ops = {}\n\n    def reset(self):\n        self._ops = {}\n\n    def register(self, ops_list):\n        for operator in ops_list:\n            if not issubclass(operator, ExpressionOps):\n                raise TypeError(\"operator must be subclass of ExpressionOps, not {}\".format(operator))\n\n            if operator.__name__ in self._ops:\n                get_module_logger(self.__class__.__name__).warning(\n                    \"The custom operator [{}] will override the qlib default definition\".format(operator.__name__)\n                )\n            self._ops[operator.__name__] = operator\n\n    def __getattr__(self, key):\n        if key not in self._ops:\n            raise AttributeError(\"The operator [{0}] is not registered\".format(key))\n        return self._ops[key]",
  "def register_all_ops(C):\n    \"\"\"register all operator\"\"\"\n    logger = get_module_logger(\"ops\")\n\n    Operators.reset()\n    Operators.register(OpsList)\n\n    if getattr(C, \"custom_ops\", None) is not None:\n        Operators.register(C.custom_ops)\n        logger.debug(\"register custom operator {}\".format(C.custom_ops))",
  "def __init__(self, feature):\n        self.feature = feature",
  "def __str__(self):\n        return \"{}({})\".format(type(self).__name__, self.feature)",
  "def get_longest_back_rolling(self):\n        return self.feature.get_longest_back_rolling()",
  "def get_extended_window_size(self):\n        return self.feature.get_extended_window_size()",
  "def __init__(self, feature, func):\n        self.func = func\n        super(NpElemOperator, self).__init__(feature)",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        return getattr(np, self.func)(series)",
  "def __init__(self, feature):\n        super(Abs, self).__init__(feature, \"abs\")",
  "def __init__(self, feature):\n        super(Sign, self).__init__(feature, \"sign\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        \"\"\"\n        To avoid error raised by bool type input, we transform the data into float32.\n        \"\"\"\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # TODO:  More precision types should be configurable\n        series = series.astype(np.float32)\n        return getattr(np, self.func)(series)",
  "def __init__(self, feature):\n        super(Log, self).__init__(feature, \"log\")",
  "def __init__(self, feature, exponent):\n        super(Power, self).__init__(feature, \"power\")\n        self.exponent = exponent",
  "def __str__(self):\n        return \"{}({},{})\".format(type(self).__name__, self.feature, self.exponent)",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        return getattr(np, self.func)(series, self.exponent)",
  "def __init__(self, feature, instrument):\n        super(Mask, self).__init__(feature, \"mask\")\n        self.instrument = instrument",
  "def __str__(self):\n        return \"{}({},{})\".format(type(self).__name__, self.feature, self.instrument.lower())",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        return self.feature.load(self.instrument, start_index, end_index, freq)",
  "def __init__(self, feature):\n        super(Not, self).__init__(feature, \"bitwise_not\")",
  "def __init__(self, feature_left, feature_right):\n        self.feature_left = feature_left\n        self.feature_right = feature_right",
  "def __str__(self):\n        return \"{}({},{})\".format(type(self).__name__, self.feature_left, self.feature_right)",
  "def get_longest_back_rolling(self):\n        if isinstance(self.feature_left, Expression):\n            left_br = self.feature_left.get_longest_back_rolling()\n        else:\n            left_br = 0\n\n        if isinstance(self.feature_right, Expression):\n            right_br = self.feature_right.get_longest_back_rolling()\n        else:\n            right_br = 0\n        return max(left_br, right_br)",
  "def get_extended_window_size(self):\n        if isinstance(self.feature_left, Expression):\n            ll, lr = self.feature_left.get_extended_window_size()\n        else:\n            ll, lr = 0, 0\n\n        if isinstance(self.feature_right, Expression):\n            rl, rr = self.feature_right.get_extended_window_size()\n        else:\n            rl, rr = 0, 0\n        return max(ll, rl), max(lr, rr)",
  "def __init__(self, feature_left, feature_right, func):\n        self.func = func\n        super(NpPairOperator, self).__init__(feature_left, feature_right)",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        assert any(\n            [isinstance(self.feature_left, Expression), self.feature_right, Expression]\n        ), \"at least one of two inputs is Expression instance\"\n        if isinstance(self.feature_left, Expression):\n            series_left = self.feature_left.load(instrument, start_index, end_index, freq)\n        else:\n            series_left = self.feature_left  # numeric value\n        if isinstance(self.feature_right, Expression):\n            series_right = self.feature_right.load(instrument, start_index, end_index, freq)\n        else:\n            series_right = self.feature_right\n        return getattr(np, self.func)(series_left, series_right)",
  "def __init__(self, feature_left, feature_right):\n        super(Add, self).__init__(feature_left, feature_right, \"add\")",
  "def __init__(self, feature_left, feature_right):\n        super(Sub, self).__init__(feature_left, feature_right, \"subtract\")",
  "def __init__(self, feature_left, feature_right):\n        super(Mul, self).__init__(feature_left, feature_right, \"multiply\")",
  "def __init__(self, feature_left, feature_right):\n        super(Div, self).__init__(feature_left, feature_right, \"divide\")",
  "def __init__(self, feature_left, feature_right):\n        super(Greater, self).__init__(feature_left, feature_right, \"maximum\")",
  "def __init__(self, feature_left, feature_right):\n        super(Less, self).__init__(feature_left, feature_right, \"minimum\")",
  "def __init__(self, feature_left, feature_right):\n        super(Gt, self).__init__(feature_left, feature_right, \"greater\")",
  "def __init__(self, feature_left, feature_right):\n        super(Ge, self).__init__(feature_left, feature_right, \"greater_equal\")",
  "def __init__(self, feature_left, feature_right):\n        super(Lt, self).__init__(feature_left, feature_right, \"less\")",
  "def __init__(self, feature_left, feature_right):\n        super(Le, self).__init__(feature_left, feature_right, \"less_equal\")",
  "def __init__(self, feature_left, feature_right):\n        super(Eq, self).__init__(feature_left, feature_right, \"equal\")",
  "def __init__(self, feature_left, feature_right):\n        super(Ne, self).__init__(feature_left, feature_right, \"not_equal\")",
  "def __init__(self, feature_left, feature_right):\n        super(And, self).__init__(feature_left, feature_right, \"bitwise_and\")",
  "def __init__(self, feature_left, feature_right):\n        super(Or, self).__init__(feature_left, feature_right, \"bitwise_or\")",
  "def __init__(self, condition, feature_left, feature_right):\n        self.condition = condition\n        self.feature_left = feature_left\n        self.feature_right = feature_right",
  "def __str__(self):\n        return \"If({},{},{})\".format(self.condition, self.feature_left, self.feature_right)",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series_cond = self.condition.load(instrument, start_index, end_index, freq)\n        if isinstance(self.feature_left, Expression):\n            series_left = self.feature_left.load(instrument, start_index, end_index, freq)\n        else:\n            series_left = self.feature_left\n        if isinstance(self.feature_right, Expression):\n            series_right = self.feature_right.load(instrument, start_index, end_index, freq)\n        else:\n            series_right = self.feature_right\n        series = pd.Series(np.where(series_cond, series_left, series_right), index=series_cond.index)\n        return series",
  "def get_longest_back_rolling(self):\n        if isinstance(self.feature_left, Expression):\n            left_br = self.feature_left.get_longest_back_rolling()\n        else:\n            left_br = 0\n\n        if isinstance(self.feature_right, Expression):\n            right_br = self.feature_right.get_longest_back_rolling()\n        else:\n            right_br = 0\n\n        if isinstance(self.condition, Expression):\n            c_br = self.condition.get_longest_back_rolling()\n        else:\n            c_br = 0\n        return max(left_br, right_br, c_br)",
  "def get_extended_window_size(self):\n        if isinstance(self.feature_left, Expression):\n            ll, lr = self.feature_left.get_extended_window_size()\n        else:\n            ll, lr = 0, 0\n\n        if isinstance(self.feature_right, Expression):\n            rl, rr = self.feature_right.get_extended_window_size()\n        else:\n            rl, rr = 0, 0\n\n        if isinstance(self.condition, Expression):\n            cl, cr = self.condition.get_extended_window_size()\n        else:\n            cl, cr = 0, 0\n        return max(ll, rl, cl), max(lr, rr, cr)",
  "def __init__(self, feature, N, func):\n        self.feature = feature\n        self.N = N\n        self.func = func",
  "def __str__(self):\n        return \"{}({},{})\".format(type(self).__name__, self.feature, self.N)",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # NOTE: remove all null check,\n        # now it's user's responsibility to decide whether use features in null days\n        # isnull = series.isnull() # NOTE: isnull = NaN, inf is not null\n        if self.N == 0:\n            series = getattr(series.expanding(min_periods=1), self.func)()\n        elif 0 < self.N < 1:\n            series = series.ewm(alpha=self.N, min_periods=1).mean()\n        else:\n            series = getattr(series.rolling(self.N, min_periods=1), self.func)()\n            # series.iloc[:self.N-1] = np.nan\n        # series[isnull] = np.nan\n        return series",
  "def get_longest_back_rolling(self):\n        if self.N == 0:\n            return np.inf\n        if 0 < self.N < 1:\n            return int(np.log(1e-6) / np.log(1 - self.N))  # (1 - N)**window == 1e-6\n        return self.feature.get_longest_back_rolling() + self.N - 1",
  "def get_extended_window_size(self):\n        if self.N == 0:\n            # FIXME: How to make this accurate and efficiently? Or  should we\n            # remove such support for N == 0?\n            get_module_logger(self.__class__.__name__).warning(\"The Rolling(ATTR, 0) will not be accurately calculated\")\n            return self.feature.get_extended_window_size()\n        elif 0 < self.N < 1:\n            lft_etd, rght_etd = self.feature.get_extended_window_size()\n            size = int(np.log(1e-6) / np.log(1 - self.N))\n            lft_etd = max(lft_etd + size - 1, lft_etd)\n            return lft_etd, rght_etd\n        else:\n            lft_etd, rght_etd = self.feature.get_extended_window_size()\n            lft_etd = max(lft_etd + self.N - 1, lft_etd)\n            return lft_etd, rght_etd",
  "def __init__(self, feature, N):\n        super(Ref, self).__init__(feature, N, \"ref\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # N = 0, return first day\n        if series.empty:\n            return series  # Pandas bug, see: https://github.com/pandas-dev/pandas/issues/21049\n        elif self.N == 0:\n            series = pd.Series(series.iloc[0], index=series.index)\n        else:\n            series = series.shift(self.N)  # copy\n        return series",
  "def get_longest_back_rolling(self):\n        if self.N == 0:\n            return np.inf\n        return self.feature.get_longest_back_rolling() + self.N",
  "def get_extended_window_size(self):\n        if self.N == 0:\n            get_module_logger(self.__class__.__name__).warning(\"The Ref(ATTR, 0) will not be accurately calculated\")\n            return self.feature.get_extended_window_size()\n        else:\n            lft_etd, rght_etd = self.feature.get_extended_window_size()\n            lft_etd = max(lft_etd + self.N, lft_etd)\n            rght_etd = max(rght_etd - self.N, rght_etd)\n            return lft_etd, rght_etd",
  "def __init__(self, feature, N):\n        super(Mean, self).__init__(feature, N, \"mean\")",
  "def __init__(self, feature, N):\n        super(Sum, self).__init__(feature, N, \"sum\")",
  "def __init__(self, feature, N):\n        super(Std, self).__init__(feature, N, \"std\")",
  "def __init__(self, feature, N):\n        super(Var, self).__init__(feature, N, \"var\")",
  "def __init__(self, feature, N):\n        if N != 0 and N < 3:\n            raise ValueError(\"The rolling window size of Skewness operation should >= 3\")\n        super(Skew, self).__init__(feature, N, \"skew\")",
  "def __init__(self, feature, N):\n        if N != 0 and N < 4:\n            raise ValueError(\"The rolling window size of Kurtosis operation should >= 5\")\n        super(Kurt, self).__init__(feature, N, \"kurt\")",
  "def __init__(self, feature, N):\n        super(Max, self).__init__(feature, N, \"max\")",
  "def __init__(self, feature, N):\n        super(IdxMax, self).__init__(feature, N, \"idxmax\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(lambda x: x.argmax() + 1, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(lambda x: x.argmax() + 1, raw=True)\n        return series",
  "def __init__(self, feature, N):\n        super(Min, self).__init__(feature, N, \"min\")",
  "def __init__(self, feature, N):\n        super(IdxMin, self).__init__(feature, N, \"idxmin\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(lambda x: x.argmin() + 1, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(lambda x: x.argmin() + 1, raw=True)\n        return series",
  "def __init__(self, feature, N, qscore):\n        super(Quantile, self).__init__(feature, N, \"quantile\")\n        self.qscore = qscore",
  "def __str__(self):\n        return \"{}({},{},{})\".format(type(self).__name__, self.feature, self.N, self.qscore)",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = series.expanding(min_periods=1).quantile(self.qscore)\n        else:\n            series = series.rolling(self.N, min_periods=1).quantile(self.qscore)\n        return series",
  "def __init__(self, feature, N):\n        super(Med, self).__init__(feature, N, \"median\")",
  "def __init__(self, feature, N):\n        super(Mad, self).__init__(feature, N, \"mad\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # TODO: implement in Cython\n\n        def mad(x):\n            x1 = x[~np.isnan(x)]\n            return np.mean(np.abs(x1 - x1.mean()))\n\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(mad, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(mad, raw=True)\n        return series",
  "def __init__(self, feature, N):\n        super(Rank, self).__init__(feature, N, \"rank\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # TODO: implement in Cython\n\n        def rank(x):\n            if np.isnan(x[-1]):\n                return np.nan\n            x1 = x[~np.isnan(x)]\n            if x1.shape[0] == 0:\n                return np.nan\n            return percentileofscore(x1, x1[-1]) / len(x1)\n\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(rank, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(rank, raw=True)\n        return series",
  "def __init__(self, feature, N):\n        super(Count, self).__init__(feature, N, \"count\")",
  "def __init__(self, feature, N):\n        super(Delta, self).__init__(feature, N, \"delta\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = series - series.iloc[0]\n        else:\n            series = series - series.shift(self.N)\n        return series",
  "def __init__(self, feature, N):\n        super(Slope, self).__init__(feature, N, \"slope\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = pd.Series(expanding_slope(series.values), index=series.index)\n        else:\n            series = pd.Series(rolling_slope(series.values, self.N), index=series.index)\n        return series",
  "def __init__(self, feature, N):\n        super(Rsquare, self).__init__(feature, N, \"rsquare\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        _series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = pd.Series(expanding_rsquare(_series.values), index=_series.index)\n        else:\n            series = pd.Series(rolling_rsquare(_series.values, self.N), index=_series.index)\n            series.loc[np.isclose(_series.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)] = np.nan\n        return series",
  "def __init__(self, feature, N):\n        super(Resi, self).__init__(feature, N, \"resi\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = pd.Series(expanding_resi(series.values), index=series.index)\n        else:\n            series = pd.Series(rolling_resi(series.values, self.N), index=series.index)\n        return series",
  "def __init__(self, feature, N):\n        super(WMA, self).__init__(feature, N, \"wma\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n        # TODO: implement in Cython\n\n        def weighted_mean(x):\n            w = np.arange(len(x))\n            w = w / w.sum()\n            return np.nanmean(w * x)\n\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(weighted_mean, raw=True)\n        else:\n            series = series.rolling(self.N, min_periods=1).apply(weighted_mean, raw=True)\n        return series",
  "def __init__(self, feature, N):\n        super(EMA, self).__init__(feature, N, \"ema\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series = self.feature.load(instrument, start_index, end_index, freq)\n\n        def exp_weighted_mean(x):\n            a = 1 - 2 / (1 + len(x))\n            w = a ** np.arange(len(x))[::-1]\n            w /= w.sum()\n            return np.nansum(w * x)\n\n        if self.N == 0:\n            series = series.expanding(min_periods=1).apply(exp_weighted_mean, raw=True)\n        elif 0 < self.N < 1:\n            series = series.ewm(alpha=self.N, min_periods=1).mean()\n        else:\n            series = series.ewm(span=self.N, min_periods=1).mean()\n        return series",
  "def __init__(self, feature_left, feature_right, N, func):\n        self.feature_left = feature_left\n        self.feature_right = feature_right\n        self.N = N\n        self.func = func",
  "def __str__(self):\n        return \"{}({},{},{})\".format(type(self).__name__, self.feature_left, self.feature_right, self.N)",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        series_left = self.feature_left.load(instrument, start_index, end_index, freq)\n        series_right = self.feature_right.load(instrument, start_index, end_index, freq)\n        if self.N == 0:\n            series = getattr(series_left.expanding(min_periods=1), self.func)(series_right)\n        else:\n            series = getattr(series_left.rolling(self.N, min_periods=1), self.func)(series_right)\n        return series",
  "def get_longest_back_rolling(self):\n        if self.N == 0:\n            return np.inf\n        return (\n            max(self.feature_left.get_longest_back_rolling(), self.feature_right.get_longest_back_rolling())\n            + self.N\n            - 1\n        )",
  "def get_extended_window_size(self):\n        if self.N == 0:\n            get_module_logger(self.__class__.__name__).warning(\n                \"The PairRolling(ATTR, 0) will not be accurately calculated\"\n            )\n            return self.feature.get_extended_window_size()\n        else:\n            ll, lr = self.feature_left.get_extended_window_size()\n            rl, rr = self.feature_right.get_extended_window_size()\n            return max(ll, rl) + self.N - 1, max(lr, rr)",
  "def __init__(self, feature_left, feature_right, N):\n        super(Corr, self).__init__(feature_left, feature_right, N, \"corr\")",
  "def _load_internal(self, instrument, start_index, end_index, freq):\n        res = super(Corr, self)._load_internal(instrument, start_index, end_index, freq)\n\n        # NOTE: Load uses MemCache, so calling load again will not cause performance degradation\n        series_left = self.feature_left.load(instrument, start_index, end_index, freq)\n        series_right = self.feature_right.load(instrument, start_index, end_index, freq)\n        res.loc[\n            np.isclose(series_left.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)\n            | np.isclose(series_right.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)\n        ] = np.nan\n        return res",
  "def __init__(self, feature_left, feature_right, N):\n        super(Cov, self).__init__(feature_left, feature_right, N, \"cov\")",
  "def __init__(self):\n        self._ops = {}",
  "def reset(self):\n        self._ops = {}",
  "def register(self, ops_list):\n        for operator in ops_list:\n            if not issubclass(operator, ExpressionOps):\n                raise TypeError(\"operator must be subclass of ExpressionOps, not {}\".format(operator))\n\n            if operator.__name__ in self._ops:\n                get_module_logger(self.__class__.__name__).warning(\n                    \"The custom operator [{}] will override the qlib default definition\".format(operator.__name__)\n                )\n            self._ops[operator.__name__] = operator",
  "def __getattr__(self, key):\n        if key not in self._ops:\n            raise AttributeError(\"The operator [{0}] is not registered\".format(key))\n        return self._ops[key]",
  "def mad(x):\n            x1 = x[~np.isnan(x)]\n            return np.mean(np.abs(x1 - x1.mean()))",
  "def rank(x):\n            if np.isnan(x[-1]):\n                return np.nan\n            x1 = x[~np.isnan(x)]\n            if x1.shape[0] == 0:\n                return np.nan\n            return percentileofscore(x1, x1[-1]) / len(x1)",
  "def weighted_mean(x):\n            w = np.arange(len(x))\n            w = w / w.sum()\n            return np.nanmean(w * x)",
  "def exp_weighted_mean(x):\n            a = 1 - 2 / (1 + len(x))\n            w = a ** np.arange(len(x))[::-1]\n            w /= w.sum()\n            return np.nansum(w * x)",
  "class BaseDFilter(abc.ABC):\n    \"\"\"Dynamic Instruments Filter Abstract class\n\n    Users can override this class to construct their own filter\n\n    Override __init__ to input filter regulations\n\n    Override filter_main to use the regulations to filter instruments\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def from_config(config):\n        \"\"\"Construct an instance from config dict.\n\n        Parameters\n        ----------\n        config : dict\n            dict of config parameters.\n        \"\"\"\n        raise NotImplementedError(\"Subclass of BaseDFilter must reimplement `from_config` method\")\n\n    @abstractmethod\n    def to_config(self):\n        \"\"\"Construct an instance from config dict.\n\n        Returns\n        ----------\n        dict\n            return the dict of config parameters.\n        \"\"\"\n        raise NotImplementedError(\"Subclass of BaseDFilter must reimplement `to_config` method\")",
  "class SeriesDFilter(BaseDFilter):\n    \"\"\"Dynamic Instruments Filter Abstract class to filter a series of certain features\n\n    Filters should provide parameters:\n\n    - filter start time\n    - filter end time\n    - filter rule\n\n    Override __init__ to assign a certain rule to filter the series.\n\n    Override _getFilterSeries to use the rule to filter the series and get a dict of {inst => series}, or override filter_main for more advanced series filter rule\n    \"\"\"\n\n    def __init__(self, fstart_time=None, fend_time=None):\n        \"\"\"Init function for filter base class.\n            Filter a set of instruments based on a certain rule within a certain period assigned by fstart_time and fend_time.\n\n        Parameters\n        ----------\n        fstart_time: str\n            the time for the filter rule to start filter the instruments.\n        fend_time: str\n            the time for the filter rule to stop filter the instruments.\n        \"\"\"\n        super(SeriesDFilter, self).__init__()\n        self.filter_start_time = pd.Timestamp(fstart_time) if fstart_time else None\n        self.filter_end_time = pd.Timestamp(fend_time) if fend_time else None\n\n    def _getTimeBound(self, instruments):\n        \"\"\"Get time bound for all instruments.\n\n        Parameters\n        ----------\n        instruments: dict\n            the dict of instruments in the form {instrument_name => list of timestamp tuple}.\n\n        Returns\n        ----------\n        pd.Timestamp, pd.Timestamp\n            the lower time bound and upper time bound of all the instruments.\n        \"\"\"\n        trange = Cal.calendar(freq=self.filter_freq)\n        ubound, lbound = trange[0], trange[-1]\n        for _, timestamp in instruments.items():\n            if timestamp:\n                lbound = timestamp[0][0] if timestamp[0][0] < lbound else lbound\n                ubound = timestamp[-1][-1] if timestamp[-1][-1] > ubound else ubound\n        return lbound, ubound\n\n    def _toSeries(self, time_range, target_timestamp):\n        \"\"\"Convert the target timestamp to a pandas series of bool value within a time range.\n            Make the time inside the target_timestamp range TRUE, others FALSE.\n\n        Parameters\n        ----------\n        time_range : D.calendar\n            the time range of the instruments.\n        target_timestamp : list\n            the list of tuple (timestamp, timestamp).\n\n        Returns\n        ----------\n        pd.Series\n            the series of bool value for an instrument.\n        \"\"\"\n        # Construct a whole dict of {date => bool}\n        timestamp_series = {timestamp: False for timestamp in time_range}\n        # Convert to pd.Series\n        timestamp_series = pd.Series(timestamp_series)\n        # Fill the date within target_timestamp with TRUE\n        for start, end in target_timestamp:\n            timestamp_series[Cal.calendar(start_time=start, end_time=end, freq=self.filter_freq)] = True\n        return timestamp_series\n\n    def _filterSeries(self, timestamp_series, filter_series):\n        \"\"\"Filter the timestamp series with filter series by using element-wise AND operation of the two series.\n\n        Parameters\n        ----------\n        timestamp_series : pd.Series\n            the series of bool value indicating existing time.\n        filter_series : pd.Series\n            the series of bool value indicating filter feature.\n\n        Returns\n        ----------\n        pd.Series\n            the series of bool value indicating whether the date satisfies the filter condition and exists in target timestamp.\n        \"\"\"\n        fstart, fend = list(filter_series.keys())[0], list(filter_series.keys())[-1]\n        filter_series = filter_series.astype(\"bool\")  # Make sure the filter_series is boolean\n        timestamp_series[fstart:fend] = timestamp_series[fstart:fend] & filter_series\n        return timestamp_series\n\n    def _toTimestamp(self, timestamp_series):\n        \"\"\"Convert the timestamp series to a list of tuple (timestamp, timestamp) indicating a continuous range of TRUE.\n\n        Parameters\n        ----------\n        timestamp_series: pd.Series\n            the series of bool value after being filtered.\n\n        Returns\n        ----------\n        list\n            the list of tuple (timestamp, timestamp).\n        \"\"\"\n        # sort the timestamp_series according to the timestamps\n        timestamp_series.sort_index()\n        timestamp = []\n        _lbool = None\n        _ltime = None\n        for _ts, _bool in timestamp_series.items():\n            # there is likely to be NAN when the filter series don't have the\n            # bool value, so we just change the NAN into False\n            if _bool == np.nan:\n                _bool = False\n            if _lbool is None:\n                _cur_start = _ts\n                _lbool = _bool\n                _ltime = _ts\n                continue\n            if (_lbool, _bool) == (True, False):\n                if _cur_start:\n                    timestamp.append((_cur_start, _ltime))\n            elif (_lbool, _bool) == (False, True):\n                _cur_start = _ts\n            _lbool = _bool\n            _ltime = _ts\n        if _lbool:\n            timestamp.append((_cur_start, _ltime))\n        return timestamp\n\n    def __call__(self, instruments, start_time=None, end_time=None, freq=\"day\"):\n        \"\"\"Call this filter to get filtered instruments list\"\"\"\n        self.filter_freq = freq\n        return self.filter_main(instruments, start_time, end_time)\n\n    @abstractmethod\n    def _getFilterSeries(self, instruments, fstart, fend):\n        \"\"\"Get filter series based on the rules assigned during the initialization and the input time range.\n\n        Parameters\n        ----------\n        instruments : dict\n            the dict of instruments to be filtered.\n        fstart : pd.Timestamp\n            start time of filter.\n        fend : pd.Timestamp\n            end time of filter.\n\n        .. note:: fstart/fend indicates the intersection of instruments start/end time and filter start/end time.\n\n        Returns\n        ----------\n        pd.Dataframe\n            a series of {pd.Timestamp => bool}.\n        \"\"\"\n        raise NotImplementedError(\"Subclass of SeriesDFilter must reimplement `getFilterSeries` method\")\n\n    def filter_main(self, instruments, start_time=None, end_time=None):\n        \"\"\"Implement this method to filter the instruments.\n\n        Parameters\n        ----------\n        instruments: dict\n            input instruments to be filtered.\n        start_time: str\n            start of the time range.\n        end_time: str\n            end of the time range.\n\n        Returns\n        ----------\n        dict\n            filtered instruments, same structure as input instruments.\n        \"\"\"\n        lbound, ubound = self._getTimeBound(instruments)\n        start_time = pd.Timestamp(start_time or lbound)\n        end_time = pd.Timestamp(end_time or ubound)\n        _instruments_filtered = {}\n        _all_calendar = Cal.calendar(start_time=start_time, end_time=end_time, freq=self.filter_freq)\n        _filter_calendar = Cal.calendar(\n            start_time=self.filter_start_time and max(self.filter_start_time, _all_calendar[0]) or _all_calendar[0],\n            end_time=self.filter_end_time and min(self.filter_end_time, _all_calendar[-1]) or _all_calendar[-1],\n            freq=self.filter_freq,\n        )\n        _all_filter_series = self._getFilterSeries(instruments, _filter_calendar[0], _filter_calendar[-1])\n        for inst, timestamp in instruments.items():\n            # Construct a whole map of date\n            _timestamp_series = self._toSeries(_all_calendar, timestamp)\n            # Get filter series\n            if inst in _all_filter_series:\n                _filter_series = _all_filter_series[inst]\n            else:\n                if self.keep:\n                    _filter_series = pd.Series({timestamp: True for timestamp in _filter_calendar})\n                else:\n                    _filter_series = pd.Series({timestamp: False for timestamp in _filter_calendar})\n            # Calculate bool value within the range of filter\n            _timestamp_series = self._filterSeries(_timestamp_series, _filter_series)\n            # Reform the map to (start_timestamp, end_timestamp) format\n            _timestamp = self._toTimestamp(_timestamp_series)\n            # Remove empty timestamp\n            if _timestamp:\n                _instruments_filtered[inst] = _timestamp\n        return _instruments_filtered",
  "class NameDFilter(SeriesDFilter):\n    \"\"\"Name dynamic instrument filter\n\n    Filter the instruments based on a regulated name format.\n\n    A name rule regular expression is required.\n    \"\"\"\n\n    def __init__(self, name_rule_re, fstart_time=None, fend_time=None):\n        \"\"\"Init function for name filter class\n\n        params:\n        ------\n        name_rule_re: str\n            regular expression for the name rule.\n        \"\"\"\n        super(NameDFilter, self).__init__(fstart_time, fend_time)\n        self.name_rule_re = name_rule_re\n\n    def _getFilterSeries(self, instruments, fstart, fend):\n        all_filter_series = {}\n        filter_calendar = Cal.calendar(start_time=fstart, end_time=fend, freq=self.filter_freq)\n        for inst, timestamp in instruments.items():\n            if re.match(self.name_rule_re, inst):\n                _filter_series = pd.Series({timestamp: True for timestamp in filter_calendar})\n            else:\n                _filter_series = pd.Series({timestamp: False for timestamp in filter_calendar})\n            all_filter_series[inst] = _filter_series\n        return all_filter_series\n\n    @staticmethod\n    def from_config(config):\n        return NameDFilter(\n            name_rule_re=config[\"name_rule_re\"],\n            fstart_time=config[\"filter_start_time\"],\n            fend_time=config[\"filter_end_time\"],\n        )\n\n    def to_config(self):\n        return {\n            \"filter_type\": \"NameDFilter\",\n            \"name_rule_re\": self.name_rule_re,\n            \"filter_start_time\": str(self.filter_start_time) if self.filter_start_time else self.filter_start_time,\n            \"filter_end_time\": str(self.filter_end_time) if self.filter_end_time else self.filter_end_time,\n        }",
  "class ExpressionDFilter(SeriesDFilter):\n    \"\"\"Expression dynamic instrument filter\n\n    Filter the instruments based on a certain expression.\n\n    An expression rule indicating a certain feature field is required.\n\n    Examples\n    ----------\n    - *basic features filter* : rule_expression = '$close/$open>5'\n    - *cross-sectional features filter* : rule_expression = '$rank($close)<10'\n    - *time-sequence features filter* : rule_expression = '$Ref($close, 3)>100'\n    \"\"\"\n\n    def __init__(self, rule_expression, fstart_time=None, fend_time=None, keep=False):\n        \"\"\"Init function for expression filter class\n\n        params:\n        ------\n        fstart_time: str\n            filter the feature starting from this time.\n        fend_time: str\n            filter the feature ending by this time.\n        rule_expression: str\n            an input expression for the rule.\n        keep: bool\n            whether to keep the instruments of which features don't exist in the filter time span.\n        \"\"\"\n        super(ExpressionDFilter, self).__init__(fstart_time, fend_time)\n        self.rule_expression = rule_expression\n        self.keep = keep\n\n    def _getFilterSeries(self, instruments, fstart, fend):\n        # do not use dataset cache\n        try:\n            _features = DatasetD.dataset(\n                instruments,\n                [self.rule_expression],\n                fstart,\n                fend,\n                freq=self.filter_freq,\n                disk_cache=0,\n            )\n        except TypeError:\n            # use LocalDatasetProvider\n            _features = DatasetD.dataset(instruments, [self.rule_expression], fstart, fend, freq=self.filter_freq)\n        rule_expression_field_name = list(_features.keys())[0]\n        all_filter_series = _features[rule_expression_field_name]\n        return all_filter_series\n\n    @staticmethod\n    def from_config(config):\n        return ExpressionDFilter(\n            rule_expression=config[\"rule_expression\"],\n            fstart_time=config[\"filter_start_time\"],\n            fend_time=config[\"filter_end_time\"],\n            keep=config[\"keep\"],\n        )\n\n    def to_config(self):\n        return {\n            \"filter_type\": \"ExpressionDFilter\",\n            \"rule_expression\": self.rule_expression,\n            \"filter_start_time\": str(self.filter_start_time) if self.filter_start_time else self.filter_start_time,\n            \"filter_end_time\": str(self.filter_end_time) if self.filter_end_time else self.filter_end_time,\n            \"keep\": self.keep,\n        }",
  "def __init__(self):\n        pass",
  "def from_config(config):\n        \"\"\"Construct an instance from config dict.\n\n        Parameters\n        ----------\n        config : dict\n            dict of config parameters.\n        \"\"\"\n        raise NotImplementedError(\"Subclass of BaseDFilter must reimplement `from_config` method\")",
  "def to_config(self):\n        \"\"\"Construct an instance from config dict.\n\n        Returns\n        ----------\n        dict\n            return the dict of config parameters.\n        \"\"\"\n        raise NotImplementedError(\"Subclass of BaseDFilter must reimplement `to_config` method\")",
  "def __init__(self, fstart_time=None, fend_time=None):\n        \"\"\"Init function for filter base class.\n            Filter a set of instruments based on a certain rule within a certain period assigned by fstart_time and fend_time.\n\n        Parameters\n        ----------\n        fstart_time: str\n            the time for the filter rule to start filter the instruments.\n        fend_time: str\n            the time for the filter rule to stop filter the instruments.\n        \"\"\"\n        super(SeriesDFilter, self).__init__()\n        self.filter_start_time = pd.Timestamp(fstart_time) if fstart_time else None\n        self.filter_end_time = pd.Timestamp(fend_time) if fend_time else None",
  "def _getTimeBound(self, instruments):\n        \"\"\"Get time bound for all instruments.\n\n        Parameters\n        ----------\n        instruments: dict\n            the dict of instruments in the form {instrument_name => list of timestamp tuple}.\n\n        Returns\n        ----------\n        pd.Timestamp, pd.Timestamp\n            the lower time bound and upper time bound of all the instruments.\n        \"\"\"\n        trange = Cal.calendar(freq=self.filter_freq)\n        ubound, lbound = trange[0], trange[-1]\n        for _, timestamp in instruments.items():\n            if timestamp:\n                lbound = timestamp[0][0] if timestamp[0][0] < lbound else lbound\n                ubound = timestamp[-1][-1] if timestamp[-1][-1] > ubound else ubound\n        return lbound, ubound",
  "def _toSeries(self, time_range, target_timestamp):\n        \"\"\"Convert the target timestamp to a pandas series of bool value within a time range.\n            Make the time inside the target_timestamp range TRUE, others FALSE.\n\n        Parameters\n        ----------\n        time_range : D.calendar\n            the time range of the instruments.\n        target_timestamp : list\n            the list of tuple (timestamp, timestamp).\n\n        Returns\n        ----------\n        pd.Series\n            the series of bool value for an instrument.\n        \"\"\"\n        # Construct a whole dict of {date => bool}\n        timestamp_series = {timestamp: False for timestamp in time_range}\n        # Convert to pd.Series\n        timestamp_series = pd.Series(timestamp_series)\n        # Fill the date within target_timestamp with TRUE\n        for start, end in target_timestamp:\n            timestamp_series[Cal.calendar(start_time=start, end_time=end, freq=self.filter_freq)] = True\n        return timestamp_series",
  "def _filterSeries(self, timestamp_series, filter_series):\n        \"\"\"Filter the timestamp series with filter series by using element-wise AND operation of the two series.\n\n        Parameters\n        ----------\n        timestamp_series : pd.Series\n            the series of bool value indicating existing time.\n        filter_series : pd.Series\n            the series of bool value indicating filter feature.\n\n        Returns\n        ----------\n        pd.Series\n            the series of bool value indicating whether the date satisfies the filter condition and exists in target timestamp.\n        \"\"\"\n        fstart, fend = list(filter_series.keys())[0], list(filter_series.keys())[-1]\n        filter_series = filter_series.astype(\"bool\")  # Make sure the filter_series is boolean\n        timestamp_series[fstart:fend] = timestamp_series[fstart:fend] & filter_series\n        return timestamp_series",
  "def _toTimestamp(self, timestamp_series):\n        \"\"\"Convert the timestamp series to a list of tuple (timestamp, timestamp) indicating a continuous range of TRUE.\n\n        Parameters\n        ----------\n        timestamp_series: pd.Series\n            the series of bool value after being filtered.\n\n        Returns\n        ----------\n        list\n            the list of tuple (timestamp, timestamp).\n        \"\"\"\n        # sort the timestamp_series according to the timestamps\n        timestamp_series.sort_index()\n        timestamp = []\n        _lbool = None\n        _ltime = None\n        for _ts, _bool in timestamp_series.items():\n            # there is likely to be NAN when the filter series don't have the\n            # bool value, so we just change the NAN into False\n            if _bool == np.nan:\n                _bool = False\n            if _lbool is None:\n                _cur_start = _ts\n                _lbool = _bool\n                _ltime = _ts\n                continue\n            if (_lbool, _bool) == (True, False):\n                if _cur_start:\n                    timestamp.append((_cur_start, _ltime))\n            elif (_lbool, _bool) == (False, True):\n                _cur_start = _ts\n            _lbool = _bool\n            _ltime = _ts\n        if _lbool:\n            timestamp.append((_cur_start, _ltime))\n        return timestamp",
  "def __call__(self, instruments, start_time=None, end_time=None, freq=\"day\"):\n        \"\"\"Call this filter to get filtered instruments list\"\"\"\n        self.filter_freq = freq\n        return self.filter_main(instruments, start_time, end_time)",
  "def _getFilterSeries(self, instruments, fstart, fend):\n        \"\"\"Get filter series based on the rules assigned during the initialization and the input time range.\n\n        Parameters\n        ----------\n        instruments : dict\n            the dict of instruments to be filtered.\n        fstart : pd.Timestamp\n            start time of filter.\n        fend : pd.Timestamp\n            end time of filter.\n\n        .. note:: fstart/fend indicates the intersection of instruments start/end time and filter start/end time.\n\n        Returns\n        ----------\n        pd.Dataframe\n            a series of {pd.Timestamp => bool}.\n        \"\"\"\n        raise NotImplementedError(\"Subclass of SeriesDFilter must reimplement `getFilterSeries` method\")",
  "def filter_main(self, instruments, start_time=None, end_time=None):\n        \"\"\"Implement this method to filter the instruments.\n\n        Parameters\n        ----------\n        instruments: dict\n            input instruments to be filtered.\n        start_time: str\n            start of the time range.\n        end_time: str\n            end of the time range.\n\n        Returns\n        ----------\n        dict\n            filtered instruments, same structure as input instruments.\n        \"\"\"\n        lbound, ubound = self._getTimeBound(instruments)\n        start_time = pd.Timestamp(start_time or lbound)\n        end_time = pd.Timestamp(end_time or ubound)\n        _instruments_filtered = {}\n        _all_calendar = Cal.calendar(start_time=start_time, end_time=end_time, freq=self.filter_freq)\n        _filter_calendar = Cal.calendar(\n            start_time=self.filter_start_time and max(self.filter_start_time, _all_calendar[0]) or _all_calendar[0],\n            end_time=self.filter_end_time and min(self.filter_end_time, _all_calendar[-1]) or _all_calendar[-1],\n            freq=self.filter_freq,\n        )\n        _all_filter_series = self._getFilterSeries(instruments, _filter_calendar[0], _filter_calendar[-1])\n        for inst, timestamp in instruments.items():\n            # Construct a whole map of date\n            _timestamp_series = self._toSeries(_all_calendar, timestamp)\n            # Get filter series\n            if inst in _all_filter_series:\n                _filter_series = _all_filter_series[inst]\n            else:\n                if self.keep:\n                    _filter_series = pd.Series({timestamp: True for timestamp in _filter_calendar})\n                else:\n                    _filter_series = pd.Series({timestamp: False for timestamp in _filter_calendar})\n            # Calculate bool value within the range of filter\n            _timestamp_series = self._filterSeries(_timestamp_series, _filter_series)\n            # Reform the map to (start_timestamp, end_timestamp) format\n            _timestamp = self._toTimestamp(_timestamp_series)\n            # Remove empty timestamp\n            if _timestamp:\n                _instruments_filtered[inst] = _timestamp\n        return _instruments_filtered",
  "def __init__(self, name_rule_re, fstart_time=None, fend_time=None):\n        \"\"\"Init function for name filter class\n\n        params:\n        ------\n        name_rule_re: str\n            regular expression for the name rule.\n        \"\"\"\n        super(NameDFilter, self).__init__(fstart_time, fend_time)\n        self.name_rule_re = name_rule_re",
  "def _getFilterSeries(self, instruments, fstart, fend):\n        all_filter_series = {}\n        filter_calendar = Cal.calendar(start_time=fstart, end_time=fend, freq=self.filter_freq)\n        for inst, timestamp in instruments.items():\n            if re.match(self.name_rule_re, inst):\n                _filter_series = pd.Series({timestamp: True for timestamp in filter_calendar})\n            else:\n                _filter_series = pd.Series({timestamp: False for timestamp in filter_calendar})\n            all_filter_series[inst] = _filter_series\n        return all_filter_series",
  "def from_config(config):\n        return NameDFilter(\n            name_rule_re=config[\"name_rule_re\"],\n            fstart_time=config[\"filter_start_time\"],\n            fend_time=config[\"filter_end_time\"],\n        )",
  "def to_config(self):\n        return {\n            \"filter_type\": \"NameDFilter\",\n            \"name_rule_re\": self.name_rule_re,\n            \"filter_start_time\": str(self.filter_start_time) if self.filter_start_time else self.filter_start_time,\n            \"filter_end_time\": str(self.filter_end_time) if self.filter_end_time else self.filter_end_time,\n        }",
  "def __init__(self, rule_expression, fstart_time=None, fend_time=None, keep=False):\n        \"\"\"Init function for expression filter class\n\n        params:\n        ------\n        fstart_time: str\n            filter the feature starting from this time.\n        fend_time: str\n            filter the feature ending by this time.\n        rule_expression: str\n            an input expression for the rule.\n        keep: bool\n            whether to keep the instruments of which features don't exist in the filter time span.\n        \"\"\"\n        super(ExpressionDFilter, self).__init__(fstart_time, fend_time)\n        self.rule_expression = rule_expression\n        self.keep = keep",
  "def _getFilterSeries(self, instruments, fstart, fend):\n        # do not use dataset cache\n        try:\n            _features = DatasetD.dataset(\n                instruments,\n                [self.rule_expression],\n                fstart,\n                fend,\n                freq=self.filter_freq,\n                disk_cache=0,\n            )\n        except TypeError:\n            # use LocalDatasetProvider\n            _features = DatasetD.dataset(instruments, [self.rule_expression], fstart, fend, freq=self.filter_freq)\n        rule_expression_field_name = list(_features.keys())[0]\n        all_filter_series = _features[rule_expression_field_name]\n        return all_filter_series",
  "def from_config(config):\n        return ExpressionDFilter(\n            rule_expression=config[\"rule_expression\"],\n            fstart_time=config[\"filter_start_time\"],\n            fend_time=config[\"filter_end_time\"],\n            keep=config[\"keep\"],\n        )",
  "def to_config(self):\n        return {\n            \"filter_type\": \"ExpressionDFilter\",\n            \"rule_expression\": self.rule_expression,\n            \"filter_start_time\": str(self.filter_start_time) if self.filter_start_time else self.filter_start_time,\n            \"filter_end_time\": str(self.filter_end_time) if self.filter_end_time else self.filter_end_time,\n            \"keep\": self.keep,\n        }",
  "class Client:\n    \"\"\"A client class\n\n    Provide the connection tool functions for ClientProvider.\n    \"\"\"\n\n    def __init__(self, host, port):\n        super(Client, self).__init__()\n        self.sio = socketio.Client()\n        self.server_host = host\n        self.server_port = port\n        self.logger = get_module_logger(self.__class__.__name__)\n        # bind connect/disconnect callbacks\n        self.sio.on(\n            \"connect\",\n            lambda: self.logger.debug(\"Connect to server {}\".format(self.sio.connection_url)),\n        )\n        self.sio.on(\"disconnect\", lambda: self.logger.debug(\"Disconnect from server!\"))\n\n    def connect_server(self):\n        \"\"\"Connect to server.\"\"\"\n        try:\n            self.sio.connect(\"ws://\" + self.server_host + \":\" + str(self.server_port))\n        except socketio.exceptions.ConnectionError:\n            self.logger.error(\"Cannot connect to server - check your network or server status\")\n\n    def disconnect(self):\n        \"\"\"Disconnect from server.\"\"\"\n        try:\n            self.sio.eio.disconnect(True)\n        except Exception as e:\n            self.logger.error(\"Cannot disconnect from server : %s\" % e)\n\n    def send_request(self, request_type, request_content, msg_queue, msg_proc_func=None):\n        \"\"\"Send a certain request to server.\n\n        Parameters\n        ----------\n        request_type : str\n            type of proposed request, 'calendar'/'instrument'/'feature'.\n        request_content : dict\n            records the information of the request.\n        msg_proc_func : func\n            the function to process the message when receiving response, should have arg `*args`.\n        msg_queue: Queue\n            The queue to pass the messsage after callback.\n        \"\"\"\n        head_info = {\"version\": qlib.__version__}\n\n        def request_callback(*args):\n            \"\"\"callback_wrapper\n\n            :param *args: args[0] is the response content\n            \"\"\"\n            # args[0] is the response content\n            self.logger.debug(\"receive data and enter queue\")\n            msg = dict(args[0])\n            if msg[\"detailed_info\"] is not None:\n                if msg[\"status\"] != 0:\n                    self.logger.error(msg[\"detailed_info\"])\n                else:\n                    self.logger.info(msg[\"detailed_info\"])\n            if msg[\"status\"] != 0:\n                ex = ValueError(f\"Bad response(status=={msg['status']}), detailed info: {msg['detailed_info']}\")\n                msg_queue.put(ex)\n            else:\n                if msg_proc_func is not None:\n                    try:\n                        ret = msg_proc_func(msg[\"result\"])\n                    except Exception as e:\n                        self.logger.exception(\"Error when processing message.\")\n                        ret = e\n                else:\n                    ret = msg[\"result\"]\n                msg_queue.put(ret)\n            self.disconnect()\n            self.logger.debug(\"disconnected\")\n\n        self.logger.debug(\"try connecting\")\n        self.connect_server()\n        self.logger.debug(\"connected\")\n        # The pickle is for passing some parameters with special type(such as\n        # pd.Timestamp)\n        request_content = {\"head\": head_info, \"body\": pickle.dumps(request_content)}\n        self.sio.on(request_type + \"_response\", request_callback)\n        self.logger.debug(\"try sending\")\n        self.sio.emit(request_type + \"_request\", request_content)\n        self.sio.wait()",
  "def __init__(self, host, port):\n        super(Client, self).__init__()\n        self.sio = socketio.Client()\n        self.server_host = host\n        self.server_port = port\n        self.logger = get_module_logger(self.__class__.__name__)\n        # bind connect/disconnect callbacks\n        self.sio.on(\n            \"connect\",\n            lambda: self.logger.debug(\"Connect to server {}\".format(self.sio.connection_url)),\n        )\n        self.sio.on(\"disconnect\", lambda: self.logger.debug(\"Disconnect from server!\"))",
  "def connect_server(self):\n        \"\"\"Connect to server.\"\"\"\n        try:\n            self.sio.connect(\"ws://\" + self.server_host + \":\" + str(self.server_port))\n        except socketio.exceptions.ConnectionError:\n            self.logger.error(\"Cannot connect to server - check your network or server status\")",
  "def disconnect(self):\n        \"\"\"Disconnect from server.\"\"\"\n        try:\n            self.sio.eio.disconnect(True)\n        except Exception as e:\n            self.logger.error(\"Cannot disconnect from server : %s\" % e)",
  "def send_request(self, request_type, request_content, msg_queue, msg_proc_func=None):\n        \"\"\"Send a certain request to server.\n\n        Parameters\n        ----------\n        request_type : str\n            type of proposed request, 'calendar'/'instrument'/'feature'.\n        request_content : dict\n            records the information of the request.\n        msg_proc_func : func\n            the function to process the message when receiving response, should have arg `*args`.\n        msg_queue: Queue\n            The queue to pass the messsage after callback.\n        \"\"\"\n        head_info = {\"version\": qlib.__version__}\n\n        def request_callback(*args):\n            \"\"\"callback_wrapper\n\n            :param *args: args[0] is the response content\n            \"\"\"\n            # args[0] is the response content\n            self.logger.debug(\"receive data and enter queue\")\n            msg = dict(args[0])\n            if msg[\"detailed_info\"] is not None:\n                if msg[\"status\"] != 0:\n                    self.logger.error(msg[\"detailed_info\"])\n                else:\n                    self.logger.info(msg[\"detailed_info\"])\n            if msg[\"status\"] != 0:\n                ex = ValueError(f\"Bad response(status=={msg['status']}), detailed info: {msg['detailed_info']}\")\n                msg_queue.put(ex)\n            else:\n                if msg_proc_func is not None:\n                    try:\n                        ret = msg_proc_func(msg[\"result\"])\n                    except Exception as e:\n                        self.logger.exception(\"Error when processing message.\")\n                        ret = e\n                else:\n                    ret = msg[\"result\"]\n                msg_queue.put(ret)\n            self.disconnect()\n            self.logger.debug(\"disconnected\")\n\n        self.logger.debug(\"try connecting\")\n        self.connect_server()\n        self.logger.debug(\"connected\")\n        # The pickle is for passing some parameters with special type(such as\n        # pd.Timestamp)\n        request_content = {\"head\": head_info, \"body\": pickle.dumps(request_content)}\n        self.sio.on(request_type + \"_response\", request_callback)\n        self.logger.debug(\"try sending\")\n        self.sio.emit(request_type + \"_request\", request_content)\n        self.sio.wait()",
  "def request_callback(*args):\n            \"\"\"callback_wrapper\n\n            :param *args: args[0] is the response content\n            \"\"\"\n            # args[0] is the response content\n            self.logger.debug(\"receive data and enter queue\")\n            msg = dict(args[0])\n            if msg[\"detailed_info\"] is not None:\n                if msg[\"status\"] != 0:\n                    self.logger.error(msg[\"detailed_info\"])\n                else:\n                    self.logger.info(msg[\"detailed_info\"])\n            if msg[\"status\"] != 0:\n                ex = ValueError(f\"Bad response(status=={msg['status']}), detailed info: {msg['detailed_info']}\")\n                msg_queue.put(ex)\n            else:\n                if msg_proc_func is not None:\n                    try:\n                        ret = msg_proc_func(msg[\"result\"])\n                    except Exception as e:\n                        self.logger.exception(\"Error when processing message.\")\n                        ret = e\n                else:\n                    ret = msg[\"result\"]\n                msg_queue.put(ret)\n            self.disconnect()\n            self.logger.debug(\"disconnected\")",
  "class CalendarProvider(abc.ABC):\n    \"\"\"Calendar provider base class\n\n    Provide calendar data.\n    \"\"\"\n\n    @abc.abstractmethod\n    def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        \"\"\"Get calendar of certain market in given time range.\n\n        Parameters\n        ----------\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency, available: year/quarter/month/week/day.\n        future : bool\n            whether including future trading day.\n\n        Returns\n        ----------\n        list\n            calendar list\n        \"\"\"\n        raise NotImplementedError(\"Subclass of CalendarProvider must implement `calendar` method\")\n\n    def locate_index(self, start_time, end_time, freq, future):\n        \"\"\"Locate the start time index and end time index in a calendar under certain frequency.\n\n        Parameters\n        ----------\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency, available: year/quarter/month/week/day.\n        future : bool\n            whether including future trading day.\n\n        Returns\n        -------\n        pd.Timestamp\n            the real start time.\n        pd.Timestamp\n            the real end time.\n        int\n            the index of start time.\n        int\n            the index of end time.\n        \"\"\"\n        start_time = pd.Timestamp(start_time)\n        end_time = pd.Timestamp(end_time)\n        calendar, calendar_index = self._get_calendar(freq=freq, future=future)\n        if start_time not in calendar_index:\n            try:\n                start_time = calendar[bisect.bisect_left(calendar, start_time)]\n            except IndexError:\n                raise IndexError(\n                    \"`start_time` uses a future date, if you want to get future trading days, you can use: `future=True`\"\n                )\n        start_index = calendar_index[start_time]\n        if end_time not in calendar_index:\n            end_time = calendar[bisect.bisect_right(calendar, end_time) - 1]\n        end_index = calendar_index[end_time]\n        return start_time, end_time, start_index, end_index\n\n    def _get_calendar(self, freq, future):\n        \"\"\"Load calendar using memcache.\n\n        Parameters\n        ----------\n        freq : str\n            frequency of read calendar file.\n        future : bool\n            whether including future trading day.\n\n        Returns\n        -------\n        list\n            list of timestamps.\n        dict\n            dict composed by timestamp as key and index as value for fast search.\n        \"\"\"\n        flag = f\"{freq}_future_{future}\"\n        if flag in H[\"c\"]:\n            _calendar, _calendar_index = H[\"c\"][flag]\n        else:\n            _calendar = np.array(self.load_calendar(freq, future))\n            _calendar_index = {x: i for i, x in enumerate(_calendar)}  # for fast search\n            H[\"c\"][flag] = _calendar, _calendar_index\n        return _calendar, _calendar_index\n\n    def _uri(self, start_time, end_time, freq, future=False):\n        \"\"\"Get the uri of calendar generation task.\"\"\"\n        return hash_args(start_time, end_time, freq, future)",
  "class InstrumentProvider(abc.ABC):\n    \"\"\"Instrument provider base class\n\n    Provide instrument data.\n    \"\"\"\n\n    @staticmethod\n    def instruments(market=\"all\", filter_pipe=None):\n        \"\"\"Get the general config dictionary for a base market adding several dynamic filters.\n\n        Parameters\n        ----------\n        market : str\n            market/industry/index shortname, e.g. all/sse/szse/sse50/csi300/csi500.\n        filter_pipe : list\n            the list of dynamic filters.\n\n        Returns\n        ----------\n        dict\n            dict of stockpool config.\n            {`market`=>base market name, `filter_pipe`=>list of filters}\n\n            example :\n\n            .. code-block::\n\n                {'market': 'csi500',\n                'filter_pipe': [{'filter_type': 'ExpressionDFilter',\n                'rule_expression': '$open<40',\n                'filter_start_time': None,\n                'filter_end_time': None,\n                'keep': False},\n                {'filter_type': 'NameDFilter',\n                'name_rule_re': 'SH[0-9]{4}55',\n                'filter_start_time': None,\n                'filter_end_time': None}]}\n        \"\"\"\n        if filter_pipe is None:\n            filter_pipe = []\n        config = {\"market\": market, \"filter_pipe\": []}\n        # the order of the filters will affect the result, so we need to keep\n        # the order\n        for filter_t in filter_pipe:\n            config[\"filter_pipe\"].append(filter_t.to_config())\n        return config\n\n    @abc.abstractmethod\n    def list_instruments(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        \"\"\"List the instruments based on a certain stockpool config.\n\n        Parameters\n        ----------\n        instruments : dict\n            stockpool config.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        as_list : bool\n            return instruments as list or dict.\n\n        Returns\n        -------\n        dict or list\n            instruments list or dictionary with time spans\n        \"\"\"\n        raise NotImplementedError(\"Subclass of InstrumentProvider must implement `list_instruments` method\")\n\n    def _uri(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        return hash_args(instruments, start_time, end_time, freq, as_list)\n\n    # instruments type\n    LIST = \"LIST\"\n    DICT = \"DICT\"\n    CONF = \"CONF\"\n\n    @classmethod\n    def get_inst_type(cls, inst):\n        if \"market\" in inst:\n            return cls.CONF\n        if isinstance(inst, dict):\n            return cls.DICT\n        if isinstance(inst, (list, tuple, pd.Index, np.ndarray)):\n            return cls.LIST\n        raise ValueError(f\"Unknown instrument type {inst}\")",
  "class FeatureProvider(abc.ABC):\n    \"\"\"Feature provider class\n\n    Provide feature data.\n    \"\"\"\n\n    @abc.abstractmethod\n    def feature(self, instrument, field, start_time, end_time, freq):\n        \"\"\"Get feature data.\n\n        Parameters\n        ----------\n        instrument : str\n            a certain instrument.\n        field : str\n            a certain field of feature.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency, available: year/quarter/month/week/day.\n\n        Returns\n        -------\n        pd.Series\n            data of a certain feature\n        \"\"\"\n        raise NotImplementedError(\"Subclass of FeatureProvider must implement `feature` method\")",
  "class ExpressionProvider(abc.ABC):\n    \"\"\"Expression provider class\n\n    Provide Expression data.\n    \"\"\"\n\n    def __init__(self):\n        self.expression_instance_cache = {}\n\n    def get_expression_instance(self, field):\n        try:\n            if field in self.expression_instance_cache:\n                expression = self.expression_instance_cache[field]\n            else:\n                expression = eval(parse_field(field))\n                self.expression_instance_cache[field] = expression\n        except NameError as e:\n            get_module_logger(\"data\").exception(\n                \"ERROR: field [%s] contains invalid operator/variable [%s]\" % (str(field), str(e).split()[1])\n            )\n            raise\n        except SyntaxError:\n            get_module_logger(\"data\").exception(\"ERROR: field [%s] contains invalid syntax\" % str(field))\n            raise\n        return expression\n\n    @abc.abstractmethod\n    def expression(self, instrument, field, start_time=None, end_time=None, freq=\"day\"):\n        \"\"\"Get Expression data.\n\n        Parameters\n        ----------\n        instrument : str\n            a certain instrument.\n        field : str\n            a certain field of feature.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency, available: year/quarter/month/week/day.\n\n        Returns\n        -------\n        pd.Series\n            data of a certain expression\n        \"\"\"\n        raise NotImplementedError(\"Subclass of ExpressionProvider must implement `Expression` method\")",
  "class DatasetProvider(abc.ABC):\n    \"\"\"Dataset provider class\n\n    Provide Dataset data.\n    \"\"\"\n\n    @abc.abstractmethod\n    def dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\"):\n        \"\"\"Get dataset data.\n\n        Parameters\n        ----------\n        instruments : list or dict\n            list/dict of instruments or dict of stockpool config.\n        fields : list\n            list of feature instances.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency.\n\n        Returns\n        ----------\n        pd.DataFrame\n            a pandas dataframe with <instrument, datetime> index.\n        \"\"\"\n        raise NotImplementedError(\"Subclass of DatasetProvider must implement `Dataset` method\")\n\n    def _uri(\n        self,\n        instruments,\n        fields,\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        disk_cache=1,\n        **kwargs,\n    ):\n        \"\"\"Get task uri, used when generating rabbitmq task in qlib_server\n\n        Parameters\n        ----------\n        instruments : list or dict\n            list/dict of instruments or dict of stockpool config.\n        fields : list\n            list of feature instances.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency.\n        disk_cache : int\n            whether to skip(0)/use(1)/replace(2) disk_cache.\n\n        \"\"\"\n        return DiskDatasetCache._uri(instruments, fields, start_time, end_time, freq, disk_cache)\n\n    @staticmethod\n    def get_instruments_d(instruments, freq):\n        \"\"\"\n        Parse different types of input instruments to output instruments_d\n        Wrong format of input instruments will lead to exception.\n\n        \"\"\"\n        if isinstance(instruments, dict):\n            if \"market\" in instruments:\n                # dict of stockpool config\n                instruments_d = Inst.list_instruments(instruments=instruments, freq=freq, as_list=False)\n            else:\n                # dict of instruments and timestamp\n                instruments_d = instruments\n        elif isinstance(instruments, (list, tuple, pd.Index, np.ndarray)):\n            # list or tuple of a group of instruments\n            instruments_d = list(instruments)\n        else:\n            raise ValueError(\"Unsupported input type for param `instrument`\")\n        return instruments_d\n\n    @staticmethod\n    def get_column_names(fields):\n        \"\"\"\n        Get column names from input fields\n\n        \"\"\"\n        if len(fields) == 0:\n            raise ValueError(\"fields cannot be empty\")\n        fields = fields.copy()\n        column_names = [str(f) for f in fields]\n        return column_names\n\n    @staticmethod\n    def parse_fields(fields):\n        # parse and check the input fields\n        return [ExpressionD.get_expression_instance(f) for f in fields]\n\n    @staticmethod\n    def dataset_processor(instruments_d, column_names, start_time, end_time, freq):\n        \"\"\"\n        Load and process the data, return the data set.\n        - default using multi-kernel method.\n\n        \"\"\"\n        normalize_column_names = normalize_cache_fields(column_names)\n        data = dict()\n        # One process for one task, so that the memory will be freed quicker.\n        workers = min(C.kernels, len(instruments_d))\n        if C.maxtasksperchild is None:\n            p = Pool(processes=workers)\n        else:\n            p = Pool(processes=workers, maxtasksperchild=C.maxtasksperchild)\n        if isinstance(instruments_d, dict):\n            for inst, spans in instruments_d.items():\n                data[inst] = p.apply_async(\n                    DatasetProvider.expression_calculator,\n                    args=(\n                        inst,\n                        start_time,\n                        end_time,\n                        freq,\n                        normalize_column_names,\n                        spans,\n                        C,\n                    ),\n                )\n        else:\n            for inst in instruments_d:\n                data[inst] = p.apply_async(\n                    DatasetProvider.expression_calculator,\n                    args=(\n                        inst,\n                        start_time,\n                        end_time,\n                        freq,\n                        normalize_column_names,\n                        None,\n                        C,\n                    ),\n                )\n\n        p.close()\n        p.join()\n\n        new_data = dict()\n        for inst in sorted(data.keys()):\n            if len(data[inst].get()) > 0:\n                # NOTE: Python version >= 3.6; in versions after python3.6, dict will always guarantee the insertion order\n                new_data[inst] = data[inst].get()\n\n        if len(new_data) > 0:\n            data = pd.concat(new_data, names=[\"instrument\"], sort=False)\n            data = DiskDatasetCache.cache_to_origin_data(data, column_names)\n        else:\n            data = pd.DataFrame(columns=column_names)\n\n        return data\n\n    @staticmethod\n    def expression_calculator(inst, start_time, end_time, freq, column_names, spans=None, g_config=None):\n        \"\"\"\n        Calculate the expressions for one instrument, return a df result.\n        If the expression has been calculated before, load from cache.\n\n        return value: A data frame with index 'datetime' and other data columns.\n\n        \"\"\"\n        # FIXME: Windows OS or MacOS using spawn: https://docs.python.org/3.8/library/multiprocessing.html?highlight=spawn#contexts-and-start-methods\n        # NOTE: This place is compatible with windows, windows multi-process is spawn\n        if not C.registered:\n            C.set_conf_from_C(g_config)\n            C.register()\n\n        obj = dict()\n        for field in column_names:\n            #  The client does not have expression provider, the data will be loaded from cache using static method.\n            obj[field] = ExpressionD.expression(inst, field, start_time, end_time, freq)\n\n        data = pd.DataFrame(obj)\n        _calendar = Cal.calendar(freq=freq)\n        data.index = _calendar[data.index.values.astype(int)]\n        data.index.names = [\"datetime\"]\n\n        if spans is None:\n            return data\n        else:\n            mask = np.zeros(len(data), dtype=bool)\n            for begin, end in spans:\n                mask |= (data.index >= begin) & (data.index <= end)\n            return data[mask]",
  "class LocalCalendarProvider(CalendarProvider):\n    \"\"\"Local calendar data provider class\n\n    Provide calendar data from local data source.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.remote = kwargs.get(\"remote\", False)\n\n    @property\n    def _uri_cal(self):\n        \"\"\"Calendar file uri.\"\"\"\n        return os.path.join(C.get_data_path(), \"calendars\", \"{}.txt\")\n\n    def load_calendar(self, freq, future):\n        \"\"\"Load original calendar timestamp from file.\n\n        Parameters\n        ----------\n        freq : str\n            frequency of read calendar file.\n\n        Returns\n        ----------\n        list\n            list of timestamps\n        \"\"\"\n        if future:\n            fname = self._uri_cal.format(freq + \"_future\")\n            # if future calendar not exists, return current calendar\n            if not os.path.exists(fname):\n                get_module_logger(\"data\").warning(f\"{freq}_future.txt not exists, return current calendar!\")\n                fname = self._uri_cal.format(freq)\n        else:\n            fname = self._uri_cal.format(freq)\n        if not os.path.exists(fname):\n            raise ValueError(\"calendar not exists for freq \" + freq)\n        with open(fname) as f:\n            return [pd.Timestamp(x.strip()) for x in f]\n\n    def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        _calendar, _calendar_index = self._get_calendar(freq, future)\n        if start_time == \"None\":\n            start_time = None\n        if end_time == \"None\":\n            end_time = None\n        # strip\n        if start_time:\n            start_time = pd.Timestamp(start_time)\n            if start_time > _calendar[-1]:\n                return np.array([])\n        else:\n            start_time = _calendar[0]\n        if end_time:\n            end_time = pd.Timestamp(end_time)\n            if end_time < _calendar[0]:\n                return np.array([])\n        else:\n            end_time = _calendar[-1]\n        _, _, si, ei = self.locate_index(start_time, end_time, freq, future)\n        return _calendar[si : ei + 1]",
  "class LocalInstrumentProvider(InstrumentProvider):\n    \"\"\"Local instrument data provider class\n\n    Provide instrument data from local data source.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @property\n    def _uri_inst(self):\n        \"\"\"Instrument file uri.\"\"\"\n        return os.path.join(C.get_data_path(), \"instruments\", \"{}.txt\")\n\n    def _load_instruments(self, market):\n        fname = self._uri_inst.format(market)\n        if not os.path.exists(fname):\n            raise ValueError(\"instruments not exists for market \" + market)\n\n        _instruments = dict()\n        df = pd.read_csv(\n            fname,\n            sep=\"\\t\",\n            usecols=[0, 1, 2],\n            names=[\"inst\", \"start_datetime\", \"end_datetime\"],\n            dtype={\"inst\": str},\n            parse_dates=[\"start_datetime\", \"end_datetime\"],\n        )\n        for row in df.itertuples(index=False):\n            _instruments.setdefault(row[0], []).append((row[1], row[2]))\n        return _instruments\n\n    def list_instruments(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        market = instruments[\"market\"]\n        if market in H[\"i\"]:\n            _instruments = H[\"i\"][market]\n        else:\n            _instruments = self._load_instruments(market)\n            H[\"i\"][market] = _instruments\n        # strip\n        # use calendar boundary\n        cal = Cal.calendar(freq=freq)\n        start_time = pd.Timestamp(start_time or cal[0])\n        end_time = pd.Timestamp(end_time or cal[-1])\n        _instruments_filtered = {\n            inst: list(\n                filter(\n                    lambda x: x[0] <= x[1],\n                    [(max(start_time, x[0]), min(end_time, x[1])) for x in spans],\n                )\n            )\n            for inst, spans in _instruments.items()\n        }\n        _instruments_filtered = {key: value for key, value in _instruments_filtered.items() if value}\n        # filter\n        filter_pipe = instruments[\"filter_pipe\"]\n        for filter_config in filter_pipe:\n            from . import filter as F\n\n            filter_t = getattr(F, filter_config[\"filter_type\"]).from_config(filter_config)\n            _instruments_filtered = filter_t(_instruments_filtered, start_time, end_time, freq)\n        # as list\n        if as_list:\n            return list(_instruments_filtered)\n        return _instruments_filtered",
  "class LocalFeatureProvider(FeatureProvider):\n    \"\"\"Local feature data provider class\n\n    Provide feature data from local data source.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.remote = kwargs.get(\"remote\", False)\n\n    @property\n    def _uri_data(self):\n        \"\"\"Static feature file uri.\"\"\"\n        return os.path.join(C.get_data_path(), \"features\", \"{}\", \"{}.{}.bin\")\n\n    def feature(self, instrument, field, start_index, end_index, freq):\n        # validate\n        field = str(field).lower()[1:]\n        instrument = code_to_fname(instrument)\n        uri_data = self._uri_data.format(instrument.lower(), field, freq)\n        if not os.path.exists(uri_data):\n            get_module_logger(\"data\").warning(\"WARN: data not found for %s.%s\" % (instrument, field))\n            return pd.Series(dtype=np.float32)\n            # raise ValueError('uri_data not found: ' + uri_data)\n        # load\n        series = read_bin(uri_data, start_index, end_index)\n        return series",
  "class LocalExpressionProvider(ExpressionProvider):\n    \"\"\"Local expression data provider class\n\n    Provide expression data from local data source.\n    \"\"\"\n\n    def expression(self, instrument, field, start_time=None, end_time=None, freq=\"day\"):\n        expression = self.get_expression_instance(field)\n        start_time = pd.Timestamp(start_time)\n        end_time = pd.Timestamp(end_time)\n        _, _, start_index, end_index = Cal.locate_index(start_time, end_time, freq, future=False)\n        lft_etd, rght_etd = expression.get_extended_window_size()\n        series = expression.load(instrument, max(0, start_index - lft_etd), end_index + rght_etd, freq)\n        # Ensure that each column type is consistent\n        # FIXME:\n        # 1) The stock data is currently float. If there is other types of data, this part needs to be re-implemented.\n        # 2) The the precision should be configurable\n        try:\n            series = series.astype(np.float32)\n        except ValueError:\n            pass\n        except TypeError:\n            pass\n        if not series.empty:\n            series = series.loc[start_index:end_index]\n        return series",
  "class LocalDatasetProvider(DatasetProvider):\n    \"\"\"Local dataset data provider class\n\n    Provide dataset data from local data source.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\"):\n        instruments_d = self.get_instruments_d(instruments, freq)\n        column_names = self.get_column_names(fields)\n        cal = Cal.calendar(start_time, end_time, freq)\n        if len(cal) == 0:\n            return pd.DataFrame(columns=column_names)\n        start_time = cal[0]\n        end_time = cal[-1]\n\n        data = self.dataset_processor(instruments_d, column_names, start_time, end_time, freq)\n\n        return data\n\n    @staticmethod\n    def multi_cache_walker(instruments, fields, start_time=None, end_time=None, freq=\"day\"):\n        \"\"\"\n        This method is used to prepare the expression cache for the client.\n        Then the client will load the data from expression cache by itself.\n\n        \"\"\"\n        instruments_d = DatasetProvider.get_instruments_d(instruments, freq)\n        column_names = DatasetProvider.get_column_names(fields)\n        cal = Cal.calendar(start_time, end_time, freq)\n        if len(cal) == 0:\n            return\n        start_time = cal[0]\n        end_time = cal[-1]\n        workers = min(C.kernels, len(instruments_d))\n        if C.maxtasksperchild is None:\n            p = Pool(processes=workers)\n        else:\n            p = Pool(processes=workers, maxtasksperchild=C.maxtasksperchild)\n\n        for inst in instruments_d:\n            p.apply_async(\n                LocalDatasetProvider.cache_walker,\n                args=(\n                    inst,\n                    start_time,\n                    end_time,\n                    freq,\n                    column_names,\n                ),\n            )\n\n        p.close()\n        p.join()\n\n    @staticmethod\n    def cache_walker(inst, start_time, end_time, freq, column_names):\n        \"\"\"\n        If the expressions of one instrument haven't been calculated before,\n        calculate it and write it into expression cache.\n\n        \"\"\"\n        for field in column_names:\n            ExpressionD.expression(inst, field, start_time, end_time, freq)",
  "class ClientCalendarProvider(CalendarProvider):\n    \"\"\"Client calendar data provider class\n\n    Provide calendar data by requesting data from server as a client.\n    \"\"\"\n\n    def __init__(self):\n        self.conn = None\n        self.queue = queue.Queue()\n\n    def set_conn(self, conn):\n        self.conn = conn\n\n    def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        self.conn.send_request(\n            request_type=\"calendar\",\n            request_content={\n                \"start_time\": str(start_time),\n                \"end_time\": str(end_time),\n                \"freq\": freq,\n                \"future\": future,\n            },\n            msg_queue=self.queue,\n            msg_proc_func=lambda response_content: [pd.Timestamp(c) for c in response_content],\n        )\n        result = self.queue.get(timeout=C[\"timeout\"])\n        return result",
  "class ClientInstrumentProvider(InstrumentProvider):\n    \"\"\"Client instrument data provider class\n\n    Provide instrument data by requesting data from server as a client.\n    \"\"\"\n\n    def __init__(self):\n        self.conn = None\n        self.queue = queue.Queue()\n\n    def set_conn(self, conn):\n        self.conn = conn\n\n    def list_instruments(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        def inst_msg_proc_func(response_content):\n            if isinstance(response_content, dict):\n                instrument = {\n                    i: [(pd.Timestamp(s), pd.Timestamp(e)) for s, e in t] for i, t in response_content.items()\n                }\n            else:\n                instrument = response_content\n            return instrument\n\n        self.conn.send_request(\n            request_type=\"instrument\",\n            request_content={\n                \"instruments\": instruments,\n                \"start_time\": str(start_time),\n                \"end_time\": str(end_time),\n                \"freq\": freq,\n                \"as_list\": as_list,\n            },\n            msg_queue=self.queue,\n            msg_proc_func=inst_msg_proc_func,\n        )\n        result = self.queue.get(timeout=C[\"timeout\"])\n        if isinstance(result, Exception):\n            raise result\n        get_module_logger(\"data\").debug(\"get result\")\n        return result",
  "class ClientDatasetProvider(DatasetProvider):\n    \"\"\"Client dataset data provider class\n\n    Provide dataset data by requesting data from server as a client.\n    \"\"\"\n\n    def __init__(self):\n        self.conn = None\n\n    def set_conn(self, conn):\n        self.conn = conn\n        self.queue = queue.Queue()\n\n    def dataset(\n        self,\n        instruments,\n        fields,\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        disk_cache=0,\n        return_uri=False,\n    ):\n        if Inst.get_inst_type(instruments) == Inst.DICT:\n            get_module_logger(\"data\").warning(\n                \"Getting features from a dict of instruments is not recommended because the features will not be \"\n                \"cached! \"\n                \"The dict of instruments will be cleaned every day.\"\n            )\n\n        if disk_cache == 0:\n            \"\"\"\n            Call the server to generate the expression cache.\n            Then load the data from the expression cache directly.\n            - default using multi-kernel method.\n\n            \"\"\"\n            self.conn.send_request(\n                request_type=\"feature\",\n                request_content={\n                    \"instruments\": instruments,\n                    \"fields\": fields,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                    \"freq\": freq,\n                    \"disk_cache\": 0,\n                },\n                msg_queue=self.queue,\n            )\n            feature_uri = self.queue.get(timeout=C[\"timeout\"])\n            if isinstance(feature_uri, Exception):\n                raise feature_uri\n            else:\n                instruments_d = self.get_instruments_d(instruments, freq)\n                column_names = self.get_column_names(fields)\n                cal = Cal.calendar(start_time, end_time, freq)\n                if len(cal) == 0:\n                    return pd.DataFrame(columns=column_names)\n                start_time = cal[0]\n                end_time = cal[-1]\n\n                data = self.dataset_processor(instruments_d, column_names, start_time, end_time, freq)\n                if return_uri:\n                    return data, feature_uri\n                else:\n                    return data\n        else:\n\n            \"\"\"\n            Call the server to generate the data-set cache, get the uri of the cache file.\n            Then load the data from the file on NFS directly.\n            - using single-process implementation.\n\n            \"\"\"\n            self.conn.send_request(\n                request_type=\"feature\",\n                request_content={\n                    \"instruments\": instruments,\n                    \"fields\": fields,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                    \"freq\": freq,\n                    \"disk_cache\": 1,\n                },\n                msg_queue=self.queue,\n            )\n            # - Done in callback\n            feature_uri = self.queue.get(timeout=C[\"timeout\"])\n            if isinstance(feature_uri, Exception):\n                raise feature_uri\n            get_module_logger(\"data\").debug(\"get result\")\n            try:\n                # pre-mound nfs, used for demo\n                mnt_feature_uri = os.path.join(C.get_data_path(), C.dataset_cache_dir_name, feature_uri)\n                df = DiskDatasetCache.read_data_from_cache(mnt_feature_uri, start_time, end_time, fields)\n                get_module_logger(\"data\").debug(\"finish slicing data\")\n                if return_uri:\n                    return df, feature_uri\n                return df\n            except AttributeError:\n                raise IOError(\"Unable to fetch instruments from remote server!\")",
  "class BaseProvider:\n    \"\"\"Local provider class\n\n    To keep compatible with old qlib provider.\n    \"\"\"\n\n    def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        return Cal.calendar(start_time, end_time, freq, future=future)\n\n    def instruments(self, market=\"all\", filter_pipe=None, start_time=None, end_time=None):\n        if start_time is not None or end_time is not None:\n            get_module_logger(\"Provider\").warning(\n                \"The instruments corresponds to a stock pool. \"\n                \"Parameters `start_time` and `end_time` does not take effect now.\"\n            )\n        return InstrumentProvider.instruments(market, filter_pipe)\n\n    def list_instruments(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        return Inst.list_instruments(instruments, start_time, end_time, freq, as_list)\n\n    def features(\n        self,\n        instruments,\n        fields,\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        disk_cache=None,\n    ):\n        \"\"\"\n        Parameters:\n        -----------\n        disk_cache : int\n            whether to skip(0)/use(1)/replace(2) disk_cache\n\n        This function will try to use cache method which has a keyword `disk_cache`,\n        and will use provider method if a type error is raised because the DatasetD instance\n        is a provider class.\n        \"\"\"\n        disk_cache = C.default_disk_cache if disk_cache is None else disk_cache\n        fields = list(fields)  # In case of tuple.\n        try:\n            return DatasetD.dataset(instruments, fields, start_time, end_time, freq, disk_cache)\n        except TypeError:\n            return DatasetD.dataset(instruments, fields, start_time, end_time, freq)",
  "class LocalProvider(BaseProvider):\n    def _uri(self, type, **kwargs):\n        \"\"\"_uri\n        The server hope to get the uri of the request. The uri will be decided\n        by the dataprovider. For ex, different cache layer has different uri.\n\n        :param type: The type of resource for the uri\n        :param **kwargs:\n        \"\"\"\n        if type == \"calendar\":\n            return Cal._uri(**kwargs)\n        elif type == \"instrument\":\n            return Inst._uri(**kwargs)\n        elif type == \"feature\":\n            return DatasetD._uri(**kwargs)\n\n    def features_uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1):\n        \"\"\"features_uri\n\n        Return the uri of the generated cache of features/dataset\n\n        :param disk_cache:\n        :param instruments:\n        :param fields:\n        :param start_time:\n        :param end_time:\n        :param freq:\n        \"\"\"\n        return DatasetD._dataset_uri(instruments, fields, start_time, end_time, freq, disk_cache)",
  "class ClientProvider(BaseProvider):\n    \"\"\"Client Provider\n\n    Requesting data from server as a client. Can propose requests:\n        - Calendar : Directly respond a list of calendars\n        - Instruments (without filter): Directly respond a list/dict of instruments\n        - Instruments (with filters):  Respond a list/dict of instruments\n        - Features : Respond a cache uri\n    The general workflow is described as follows:\n    When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the reponse saying `feature_available` is true.\n    `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client.\n    \"\"\"\n\n    def __init__(self):\n        from .client import Client\n\n        self.client = Client(C.flask_server, C.flask_port)\n        self.logger = get_module_logger(self.__class__.__name__)\n        if isinstance(Cal, ClientCalendarProvider):\n            Cal.set_conn(self.client)\n        Inst.set_conn(self.client)\n        if hasattr(DatasetD, \"provider\"):\n            DatasetD.provider.set_conn(self.client)\n        else:\n            DatasetD.set_conn(self.client)",
  "def register_all_wrappers(C):\n    \"\"\"register_all_wrappers\"\"\"\n    logger = get_module_logger(\"data\")\n    module = get_module_by_module_path(\"qlib.data\")\n\n    _calendar_provider = init_instance_by_config(C.calendar_provider, module)\n    if getattr(C, \"calendar_cache\", None) is not None:\n        _calendar_provider = init_instance_by_config(C.calendar_cache, module, provide=_calendar_provider)\n    register_wrapper(Cal, _calendar_provider, \"qlib.data\")\n    logger.debug(f\"registering Cal {C.calendar_provider}-{C.calendar_cache}\")\n\n    register_wrapper(Inst, C.instrument_provider, \"qlib.data\")\n    logger.debug(f\"registering Inst {C.instrument_provider}\")\n\n    if getattr(C, \"feature_provider\", None) is not None:\n        feature_provider = init_instance_by_config(C.feature_provider, module)\n        register_wrapper(FeatureD, feature_provider, \"qlib.data\")\n        logger.debug(f\"registering FeatureD {C.feature_provider}\")\n\n    if getattr(C, \"expression_provider\", None) is not None:\n        # This provider is unnecessary in client provider\n        _eprovider = init_instance_by_config(C.expression_provider, module)\n        if getattr(C, \"expression_cache\", None) is not None:\n            _eprovider = init_instance_by_config(C.expression_cache, module, provider=_eprovider)\n        register_wrapper(ExpressionD, _eprovider, \"qlib.data\")\n        logger.debug(f\"registering ExpressioneD {C.expression_provider}-{C.expression_cache}\")\n\n    _dprovider = init_instance_by_config(C.dataset_provider, module)\n    if getattr(C, \"dataset_cache\", None) is not None:\n        _dprovider = init_instance_by_config(C.dataset_cache, module, provider=_dprovider)\n    register_wrapper(DatasetD, _dprovider, \"qlib.data\")\n    logger.debug(f\"registering DataseteD {C.dataset_provider}-{C.dataset_cache}\")\n\n    register_wrapper(D, C.provider, \"qlib.data\")\n    logger.debug(f\"registering D {C.provider}\")",
  "def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        \"\"\"Get calendar of certain market in given time range.\n\n        Parameters\n        ----------\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency, available: year/quarter/month/week/day.\n        future : bool\n            whether including future trading day.\n\n        Returns\n        ----------\n        list\n            calendar list\n        \"\"\"\n        raise NotImplementedError(\"Subclass of CalendarProvider must implement `calendar` method\")",
  "def locate_index(self, start_time, end_time, freq, future):\n        \"\"\"Locate the start time index and end time index in a calendar under certain frequency.\n\n        Parameters\n        ----------\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency, available: year/quarter/month/week/day.\n        future : bool\n            whether including future trading day.\n\n        Returns\n        -------\n        pd.Timestamp\n            the real start time.\n        pd.Timestamp\n            the real end time.\n        int\n            the index of start time.\n        int\n            the index of end time.\n        \"\"\"\n        start_time = pd.Timestamp(start_time)\n        end_time = pd.Timestamp(end_time)\n        calendar, calendar_index = self._get_calendar(freq=freq, future=future)\n        if start_time not in calendar_index:\n            try:\n                start_time = calendar[bisect.bisect_left(calendar, start_time)]\n            except IndexError:\n                raise IndexError(\n                    \"`start_time` uses a future date, if you want to get future trading days, you can use: `future=True`\"\n                )\n        start_index = calendar_index[start_time]\n        if end_time not in calendar_index:\n            end_time = calendar[bisect.bisect_right(calendar, end_time) - 1]\n        end_index = calendar_index[end_time]\n        return start_time, end_time, start_index, end_index",
  "def _get_calendar(self, freq, future):\n        \"\"\"Load calendar using memcache.\n\n        Parameters\n        ----------\n        freq : str\n            frequency of read calendar file.\n        future : bool\n            whether including future trading day.\n\n        Returns\n        -------\n        list\n            list of timestamps.\n        dict\n            dict composed by timestamp as key and index as value for fast search.\n        \"\"\"\n        flag = f\"{freq}_future_{future}\"\n        if flag in H[\"c\"]:\n            _calendar, _calendar_index = H[\"c\"][flag]\n        else:\n            _calendar = np.array(self.load_calendar(freq, future))\n            _calendar_index = {x: i for i, x in enumerate(_calendar)}  # for fast search\n            H[\"c\"][flag] = _calendar, _calendar_index\n        return _calendar, _calendar_index",
  "def _uri(self, start_time, end_time, freq, future=False):\n        \"\"\"Get the uri of calendar generation task.\"\"\"\n        return hash_args(start_time, end_time, freq, future)",
  "def instruments(market=\"all\", filter_pipe=None):\n        \"\"\"Get the general config dictionary for a base market adding several dynamic filters.\n\n        Parameters\n        ----------\n        market : str\n            market/industry/index shortname, e.g. all/sse/szse/sse50/csi300/csi500.\n        filter_pipe : list\n            the list of dynamic filters.\n\n        Returns\n        ----------\n        dict\n            dict of stockpool config.\n            {`market`=>base market name, `filter_pipe`=>list of filters}\n\n            example :\n\n            .. code-block::\n\n                {'market': 'csi500',\n                'filter_pipe': [{'filter_type': 'ExpressionDFilter',\n                'rule_expression': '$open<40',\n                'filter_start_time': None,\n                'filter_end_time': None,\n                'keep': False},\n                {'filter_type': 'NameDFilter',\n                'name_rule_re': 'SH[0-9]{4}55',\n                'filter_start_time': None,\n                'filter_end_time': None}]}\n        \"\"\"\n        if filter_pipe is None:\n            filter_pipe = []\n        config = {\"market\": market, \"filter_pipe\": []}\n        # the order of the filters will affect the result, so we need to keep\n        # the order\n        for filter_t in filter_pipe:\n            config[\"filter_pipe\"].append(filter_t.to_config())\n        return config",
  "def list_instruments(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        \"\"\"List the instruments based on a certain stockpool config.\n\n        Parameters\n        ----------\n        instruments : dict\n            stockpool config.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        as_list : bool\n            return instruments as list or dict.\n\n        Returns\n        -------\n        dict or list\n            instruments list or dictionary with time spans\n        \"\"\"\n        raise NotImplementedError(\"Subclass of InstrumentProvider must implement `list_instruments` method\")",
  "def _uri(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        return hash_args(instruments, start_time, end_time, freq, as_list)",
  "def get_inst_type(cls, inst):\n        if \"market\" in inst:\n            return cls.CONF\n        if isinstance(inst, dict):\n            return cls.DICT\n        if isinstance(inst, (list, tuple, pd.Index, np.ndarray)):\n            return cls.LIST\n        raise ValueError(f\"Unknown instrument type {inst}\")",
  "def feature(self, instrument, field, start_time, end_time, freq):\n        \"\"\"Get feature data.\n\n        Parameters\n        ----------\n        instrument : str\n            a certain instrument.\n        field : str\n            a certain field of feature.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency, available: year/quarter/month/week/day.\n\n        Returns\n        -------\n        pd.Series\n            data of a certain feature\n        \"\"\"\n        raise NotImplementedError(\"Subclass of FeatureProvider must implement `feature` method\")",
  "def __init__(self):\n        self.expression_instance_cache = {}",
  "def get_expression_instance(self, field):\n        try:\n            if field in self.expression_instance_cache:\n                expression = self.expression_instance_cache[field]\n            else:\n                expression = eval(parse_field(field))\n                self.expression_instance_cache[field] = expression\n        except NameError as e:\n            get_module_logger(\"data\").exception(\n                \"ERROR: field [%s] contains invalid operator/variable [%s]\" % (str(field), str(e).split()[1])\n            )\n            raise\n        except SyntaxError:\n            get_module_logger(\"data\").exception(\"ERROR: field [%s] contains invalid syntax\" % str(field))\n            raise\n        return expression",
  "def expression(self, instrument, field, start_time=None, end_time=None, freq=\"day\"):\n        \"\"\"Get Expression data.\n\n        Parameters\n        ----------\n        instrument : str\n            a certain instrument.\n        field : str\n            a certain field of feature.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency, available: year/quarter/month/week/day.\n\n        Returns\n        -------\n        pd.Series\n            data of a certain expression\n        \"\"\"\n        raise NotImplementedError(\"Subclass of ExpressionProvider must implement `Expression` method\")",
  "def dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\"):\n        \"\"\"Get dataset data.\n\n        Parameters\n        ----------\n        instruments : list or dict\n            list/dict of instruments or dict of stockpool config.\n        fields : list\n            list of feature instances.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency.\n\n        Returns\n        ----------\n        pd.DataFrame\n            a pandas dataframe with <instrument, datetime> index.\n        \"\"\"\n        raise NotImplementedError(\"Subclass of DatasetProvider must implement `Dataset` method\")",
  "def _uri(\n        self,\n        instruments,\n        fields,\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        disk_cache=1,\n        **kwargs,\n    ):\n        \"\"\"Get task uri, used when generating rabbitmq task in qlib_server\n\n        Parameters\n        ----------\n        instruments : list or dict\n            list/dict of instruments or dict of stockpool config.\n        fields : list\n            list of feature instances.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n        freq : str\n            time frequency.\n        disk_cache : int\n            whether to skip(0)/use(1)/replace(2) disk_cache.\n\n        \"\"\"\n        return DiskDatasetCache._uri(instruments, fields, start_time, end_time, freq, disk_cache)",
  "def get_instruments_d(instruments, freq):\n        \"\"\"\n        Parse different types of input instruments to output instruments_d\n        Wrong format of input instruments will lead to exception.\n\n        \"\"\"\n        if isinstance(instruments, dict):\n            if \"market\" in instruments:\n                # dict of stockpool config\n                instruments_d = Inst.list_instruments(instruments=instruments, freq=freq, as_list=False)\n            else:\n                # dict of instruments and timestamp\n                instruments_d = instruments\n        elif isinstance(instruments, (list, tuple, pd.Index, np.ndarray)):\n            # list or tuple of a group of instruments\n            instruments_d = list(instruments)\n        else:\n            raise ValueError(\"Unsupported input type for param `instrument`\")\n        return instruments_d",
  "def get_column_names(fields):\n        \"\"\"\n        Get column names from input fields\n\n        \"\"\"\n        if len(fields) == 0:\n            raise ValueError(\"fields cannot be empty\")\n        fields = fields.copy()\n        column_names = [str(f) for f in fields]\n        return column_names",
  "def parse_fields(fields):\n        # parse and check the input fields\n        return [ExpressionD.get_expression_instance(f) for f in fields]",
  "def dataset_processor(instruments_d, column_names, start_time, end_time, freq):\n        \"\"\"\n        Load and process the data, return the data set.\n        - default using multi-kernel method.\n\n        \"\"\"\n        normalize_column_names = normalize_cache_fields(column_names)\n        data = dict()\n        # One process for one task, so that the memory will be freed quicker.\n        workers = min(C.kernels, len(instruments_d))\n        if C.maxtasksperchild is None:\n            p = Pool(processes=workers)\n        else:\n            p = Pool(processes=workers, maxtasksperchild=C.maxtasksperchild)\n        if isinstance(instruments_d, dict):\n            for inst, spans in instruments_d.items():\n                data[inst] = p.apply_async(\n                    DatasetProvider.expression_calculator,\n                    args=(\n                        inst,\n                        start_time,\n                        end_time,\n                        freq,\n                        normalize_column_names,\n                        spans,\n                        C,\n                    ),\n                )\n        else:\n            for inst in instruments_d:\n                data[inst] = p.apply_async(\n                    DatasetProvider.expression_calculator,\n                    args=(\n                        inst,\n                        start_time,\n                        end_time,\n                        freq,\n                        normalize_column_names,\n                        None,\n                        C,\n                    ),\n                )\n\n        p.close()\n        p.join()\n\n        new_data = dict()\n        for inst in sorted(data.keys()):\n            if len(data[inst].get()) > 0:\n                # NOTE: Python version >= 3.6; in versions after python3.6, dict will always guarantee the insertion order\n                new_data[inst] = data[inst].get()\n\n        if len(new_data) > 0:\n            data = pd.concat(new_data, names=[\"instrument\"], sort=False)\n            data = DiskDatasetCache.cache_to_origin_data(data, column_names)\n        else:\n            data = pd.DataFrame(columns=column_names)\n\n        return data",
  "def expression_calculator(inst, start_time, end_time, freq, column_names, spans=None, g_config=None):\n        \"\"\"\n        Calculate the expressions for one instrument, return a df result.\n        If the expression has been calculated before, load from cache.\n\n        return value: A data frame with index 'datetime' and other data columns.\n\n        \"\"\"\n        # FIXME: Windows OS or MacOS using spawn: https://docs.python.org/3.8/library/multiprocessing.html?highlight=spawn#contexts-and-start-methods\n        # NOTE: This place is compatible with windows, windows multi-process is spawn\n        if not C.registered:\n            C.set_conf_from_C(g_config)\n            C.register()\n\n        obj = dict()\n        for field in column_names:\n            #  The client does not have expression provider, the data will be loaded from cache using static method.\n            obj[field] = ExpressionD.expression(inst, field, start_time, end_time, freq)\n\n        data = pd.DataFrame(obj)\n        _calendar = Cal.calendar(freq=freq)\n        data.index = _calendar[data.index.values.astype(int)]\n        data.index.names = [\"datetime\"]\n\n        if spans is None:\n            return data\n        else:\n            mask = np.zeros(len(data), dtype=bool)\n            for begin, end in spans:\n                mask |= (data.index >= begin) & (data.index <= end)\n            return data[mask]",
  "def __init__(self, **kwargs):\n        self.remote = kwargs.get(\"remote\", False)",
  "def _uri_cal(self):\n        \"\"\"Calendar file uri.\"\"\"\n        return os.path.join(C.get_data_path(), \"calendars\", \"{}.txt\")",
  "def load_calendar(self, freq, future):\n        \"\"\"Load original calendar timestamp from file.\n\n        Parameters\n        ----------\n        freq : str\n            frequency of read calendar file.\n\n        Returns\n        ----------\n        list\n            list of timestamps\n        \"\"\"\n        if future:\n            fname = self._uri_cal.format(freq + \"_future\")\n            # if future calendar not exists, return current calendar\n            if not os.path.exists(fname):\n                get_module_logger(\"data\").warning(f\"{freq}_future.txt not exists, return current calendar!\")\n                fname = self._uri_cal.format(freq)\n        else:\n            fname = self._uri_cal.format(freq)\n        if not os.path.exists(fname):\n            raise ValueError(\"calendar not exists for freq \" + freq)\n        with open(fname) as f:\n            return [pd.Timestamp(x.strip()) for x in f]",
  "def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        _calendar, _calendar_index = self._get_calendar(freq, future)\n        if start_time == \"None\":\n            start_time = None\n        if end_time == \"None\":\n            end_time = None\n        # strip\n        if start_time:\n            start_time = pd.Timestamp(start_time)\n            if start_time > _calendar[-1]:\n                return np.array([])\n        else:\n            start_time = _calendar[0]\n        if end_time:\n            end_time = pd.Timestamp(end_time)\n            if end_time < _calendar[0]:\n                return np.array([])\n        else:\n            end_time = _calendar[-1]\n        _, _, si, ei = self.locate_index(start_time, end_time, freq, future)\n        return _calendar[si : ei + 1]",
  "def __init__(self):\n        pass",
  "def _uri_inst(self):\n        \"\"\"Instrument file uri.\"\"\"\n        return os.path.join(C.get_data_path(), \"instruments\", \"{}.txt\")",
  "def _load_instruments(self, market):\n        fname = self._uri_inst.format(market)\n        if not os.path.exists(fname):\n            raise ValueError(\"instruments not exists for market \" + market)\n\n        _instruments = dict()\n        df = pd.read_csv(\n            fname,\n            sep=\"\\t\",\n            usecols=[0, 1, 2],\n            names=[\"inst\", \"start_datetime\", \"end_datetime\"],\n            dtype={\"inst\": str},\n            parse_dates=[\"start_datetime\", \"end_datetime\"],\n        )\n        for row in df.itertuples(index=False):\n            _instruments.setdefault(row[0], []).append((row[1], row[2]))\n        return _instruments",
  "def list_instruments(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        market = instruments[\"market\"]\n        if market in H[\"i\"]:\n            _instruments = H[\"i\"][market]\n        else:\n            _instruments = self._load_instruments(market)\n            H[\"i\"][market] = _instruments\n        # strip\n        # use calendar boundary\n        cal = Cal.calendar(freq=freq)\n        start_time = pd.Timestamp(start_time or cal[0])\n        end_time = pd.Timestamp(end_time or cal[-1])\n        _instruments_filtered = {\n            inst: list(\n                filter(\n                    lambda x: x[0] <= x[1],\n                    [(max(start_time, x[0]), min(end_time, x[1])) for x in spans],\n                )\n            )\n            for inst, spans in _instruments.items()\n        }\n        _instruments_filtered = {key: value for key, value in _instruments_filtered.items() if value}\n        # filter\n        filter_pipe = instruments[\"filter_pipe\"]\n        for filter_config in filter_pipe:\n            from . import filter as F\n\n            filter_t = getattr(F, filter_config[\"filter_type\"]).from_config(filter_config)\n            _instruments_filtered = filter_t(_instruments_filtered, start_time, end_time, freq)\n        # as list\n        if as_list:\n            return list(_instruments_filtered)\n        return _instruments_filtered",
  "def __init__(self, **kwargs):\n        self.remote = kwargs.get(\"remote\", False)",
  "def _uri_data(self):\n        \"\"\"Static feature file uri.\"\"\"\n        return os.path.join(C.get_data_path(), \"features\", \"{}\", \"{}.{}.bin\")",
  "def feature(self, instrument, field, start_index, end_index, freq):\n        # validate\n        field = str(field).lower()[1:]\n        instrument = code_to_fname(instrument)\n        uri_data = self._uri_data.format(instrument.lower(), field, freq)\n        if not os.path.exists(uri_data):\n            get_module_logger(\"data\").warning(\"WARN: data not found for %s.%s\" % (instrument, field))\n            return pd.Series(dtype=np.float32)\n            # raise ValueError('uri_data not found: ' + uri_data)\n        # load\n        series = read_bin(uri_data, start_index, end_index)\n        return series",
  "def expression(self, instrument, field, start_time=None, end_time=None, freq=\"day\"):\n        expression = self.get_expression_instance(field)\n        start_time = pd.Timestamp(start_time)\n        end_time = pd.Timestamp(end_time)\n        _, _, start_index, end_index = Cal.locate_index(start_time, end_time, freq, future=False)\n        lft_etd, rght_etd = expression.get_extended_window_size()\n        series = expression.load(instrument, max(0, start_index - lft_etd), end_index + rght_etd, freq)\n        # Ensure that each column type is consistent\n        # FIXME:\n        # 1) The stock data is currently float. If there is other types of data, this part needs to be re-implemented.\n        # 2) The the precision should be configurable\n        try:\n            series = series.astype(np.float32)\n        except ValueError:\n            pass\n        except TypeError:\n            pass\n        if not series.empty:\n            series = series.loc[start_index:end_index]\n        return series",
  "def __init__(self):\n        pass",
  "def dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\"):\n        instruments_d = self.get_instruments_d(instruments, freq)\n        column_names = self.get_column_names(fields)\n        cal = Cal.calendar(start_time, end_time, freq)\n        if len(cal) == 0:\n            return pd.DataFrame(columns=column_names)\n        start_time = cal[0]\n        end_time = cal[-1]\n\n        data = self.dataset_processor(instruments_d, column_names, start_time, end_time, freq)\n\n        return data",
  "def multi_cache_walker(instruments, fields, start_time=None, end_time=None, freq=\"day\"):\n        \"\"\"\n        This method is used to prepare the expression cache for the client.\n        Then the client will load the data from expression cache by itself.\n\n        \"\"\"\n        instruments_d = DatasetProvider.get_instruments_d(instruments, freq)\n        column_names = DatasetProvider.get_column_names(fields)\n        cal = Cal.calendar(start_time, end_time, freq)\n        if len(cal) == 0:\n            return\n        start_time = cal[0]\n        end_time = cal[-1]\n        workers = min(C.kernels, len(instruments_d))\n        if C.maxtasksperchild is None:\n            p = Pool(processes=workers)\n        else:\n            p = Pool(processes=workers, maxtasksperchild=C.maxtasksperchild)\n\n        for inst in instruments_d:\n            p.apply_async(\n                LocalDatasetProvider.cache_walker,\n                args=(\n                    inst,\n                    start_time,\n                    end_time,\n                    freq,\n                    column_names,\n                ),\n            )\n\n        p.close()\n        p.join()",
  "def cache_walker(inst, start_time, end_time, freq, column_names):\n        \"\"\"\n        If the expressions of one instrument haven't been calculated before,\n        calculate it and write it into expression cache.\n\n        \"\"\"\n        for field in column_names:\n            ExpressionD.expression(inst, field, start_time, end_time, freq)",
  "def __init__(self):\n        self.conn = None\n        self.queue = queue.Queue()",
  "def set_conn(self, conn):\n        self.conn = conn",
  "def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        self.conn.send_request(\n            request_type=\"calendar\",\n            request_content={\n                \"start_time\": str(start_time),\n                \"end_time\": str(end_time),\n                \"freq\": freq,\n                \"future\": future,\n            },\n            msg_queue=self.queue,\n            msg_proc_func=lambda response_content: [pd.Timestamp(c) for c in response_content],\n        )\n        result = self.queue.get(timeout=C[\"timeout\"])\n        return result",
  "def __init__(self):\n        self.conn = None\n        self.queue = queue.Queue()",
  "def set_conn(self, conn):\n        self.conn = conn",
  "def list_instruments(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        def inst_msg_proc_func(response_content):\n            if isinstance(response_content, dict):\n                instrument = {\n                    i: [(pd.Timestamp(s), pd.Timestamp(e)) for s, e in t] for i, t in response_content.items()\n                }\n            else:\n                instrument = response_content\n            return instrument\n\n        self.conn.send_request(\n            request_type=\"instrument\",\n            request_content={\n                \"instruments\": instruments,\n                \"start_time\": str(start_time),\n                \"end_time\": str(end_time),\n                \"freq\": freq,\n                \"as_list\": as_list,\n            },\n            msg_queue=self.queue,\n            msg_proc_func=inst_msg_proc_func,\n        )\n        result = self.queue.get(timeout=C[\"timeout\"])\n        if isinstance(result, Exception):\n            raise result\n        get_module_logger(\"data\").debug(\"get result\")\n        return result",
  "def __init__(self):\n        self.conn = None",
  "def set_conn(self, conn):\n        self.conn = conn\n        self.queue = queue.Queue()",
  "def dataset(\n        self,\n        instruments,\n        fields,\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        disk_cache=0,\n        return_uri=False,\n    ):\n        if Inst.get_inst_type(instruments) == Inst.DICT:\n            get_module_logger(\"data\").warning(\n                \"Getting features from a dict of instruments is not recommended because the features will not be \"\n                \"cached! \"\n                \"The dict of instruments will be cleaned every day.\"\n            )\n\n        if disk_cache == 0:\n            \"\"\"\n            Call the server to generate the expression cache.\n            Then load the data from the expression cache directly.\n            - default using multi-kernel method.\n\n            \"\"\"\n            self.conn.send_request(\n                request_type=\"feature\",\n                request_content={\n                    \"instruments\": instruments,\n                    \"fields\": fields,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                    \"freq\": freq,\n                    \"disk_cache\": 0,\n                },\n                msg_queue=self.queue,\n            )\n            feature_uri = self.queue.get(timeout=C[\"timeout\"])\n            if isinstance(feature_uri, Exception):\n                raise feature_uri\n            else:\n                instruments_d = self.get_instruments_d(instruments, freq)\n                column_names = self.get_column_names(fields)\n                cal = Cal.calendar(start_time, end_time, freq)\n                if len(cal) == 0:\n                    return pd.DataFrame(columns=column_names)\n                start_time = cal[0]\n                end_time = cal[-1]\n\n                data = self.dataset_processor(instruments_d, column_names, start_time, end_time, freq)\n                if return_uri:\n                    return data, feature_uri\n                else:\n                    return data\n        else:\n\n            \"\"\"\n            Call the server to generate the data-set cache, get the uri of the cache file.\n            Then load the data from the file on NFS directly.\n            - using single-process implementation.\n\n            \"\"\"\n            self.conn.send_request(\n                request_type=\"feature\",\n                request_content={\n                    \"instruments\": instruments,\n                    \"fields\": fields,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                    \"freq\": freq,\n                    \"disk_cache\": 1,\n                },\n                msg_queue=self.queue,\n            )\n            # - Done in callback\n            feature_uri = self.queue.get(timeout=C[\"timeout\"])\n            if isinstance(feature_uri, Exception):\n                raise feature_uri\n            get_module_logger(\"data\").debug(\"get result\")\n            try:\n                # pre-mound nfs, used for demo\n                mnt_feature_uri = os.path.join(C.get_data_path(), C.dataset_cache_dir_name, feature_uri)\n                df = DiskDatasetCache.read_data_from_cache(mnt_feature_uri, start_time, end_time, fields)\n                get_module_logger(\"data\").debug(\"finish slicing data\")\n                if return_uri:\n                    return df, feature_uri\n                return df\n            except AttributeError:\n                raise IOError(\"Unable to fetch instruments from remote server!\")",
  "def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        return Cal.calendar(start_time, end_time, freq, future=future)",
  "def instruments(self, market=\"all\", filter_pipe=None, start_time=None, end_time=None):\n        if start_time is not None or end_time is not None:\n            get_module_logger(\"Provider\").warning(\n                \"The instruments corresponds to a stock pool. \"\n                \"Parameters `start_time` and `end_time` does not take effect now.\"\n            )\n        return InstrumentProvider.instruments(market, filter_pipe)",
  "def list_instruments(self, instruments, start_time=None, end_time=None, freq=\"day\", as_list=False):\n        return Inst.list_instruments(instruments, start_time, end_time, freq, as_list)",
  "def features(\n        self,\n        instruments,\n        fields,\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        disk_cache=None,\n    ):\n        \"\"\"\n        Parameters:\n        -----------\n        disk_cache : int\n            whether to skip(0)/use(1)/replace(2) disk_cache\n\n        This function will try to use cache method which has a keyword `disk_cache`,\n        and will use provider method if a type error is raised because the DatasetD instance\n        is a provider class.\n        \"\"\"\n        disk_cache = C.default_disk_cache if disk_cache is None else disk_cache\n        fields = list(fields)  # In case of tuple.\n        try:\n            return DatasetD.dataset(instruments, fields, start_time, end_time, freq, disk_cache)\n        except TypeError:\n            return DatasetD.dataset(instruments, fields, start_time, end_time, freq)",
  "def _uri(self, type, **kwargs):\n        \"\"\"_uri\n        The server hope to get the uri of the request. The uri will be decided\n        by the dataprovider. For ex, different cache layer has different uri.\n\n        :param type: The type of resource for the uri\n        :param **kwargs:\n        \"\"\"\n        if type == \"calendar\":\n            return Cal._uri(**kwargs)\n        elif type == \"instrument\":\n            return Inst._uri(**kwargs)\n        elif type == \"feature\":\n            return DatasetD._uri(**kwargs)",
  "def features_uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1):\n        \"\"\"features_uri\n\n        Return the uri of the generated cache of features/dataset\n\n        :param disk_cache:\n        :param instruments:\n        :param fields:\n        :param start_time:\n        :param end_time:\n        :param freq:\n        \"\"\"\n        return DatasetD._dataset_uri(instruments, fields, start_time, end_time, freq, disk_cache)",
  "def __init__(self):\n        from .client import Client\n\n        self.client = Client(C.flask_server, C.flask_port)\n        self.logger = get_module_logger(self.__class__.__name__)\n        if isinstance(Cal, ClientCalendarProvider):\n            Cal.set_conn(self.client)\n        Inst.set_conn(self.client)\n        if hasattr(DatasetD, \"provider\"):\n            DatasetD.provider.set_conn(self.client)\n        else:\n            DatasetD.set_conn(self.client)",
  "def inst_msg_proc_func(response_content):\n            if isinstance(response_content, dict):\n                instrument = {\n                    i: [(pd.Timestamp(s), pd.Timestamp(e)) for s, e in t] for i, t in response_content.items()\n                }\n            else:\n                instrument = response_content\n            return instrument",
  "class QlibCacheException(RuntimeError):\n    pass",
  "class MemCacheUnit(abc.ABC):\n    \"\"\"Memory Cache Unit.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.size_limit = kwargs.pop(\"size_limit\", 0)\n        self._size = 0\n        self.od = OrderedDict()\n\n    def __setitem__(self, key, value):\n        # TODO: thread safe?__setitem__ failure might cause inconsistent size?\n\n        # precalculate the size after od.__setitem__\n        self._adjust_size(key, value)\n\n        self.od.__setitem__(key, value)\n\n        # move the key to end,make it latest\n        self.od.move_to_end(key)\n\n        if self.limited:\n            # pop the oldest items beyond size limit\n            while self._size > self.size_limit:\n                self.popitem(last=False)\n\n    def __getitem__(self, key):\n        v = self.od.__getitem__(key)\n        self.od.move_to_end(key)\n        return v\n\n    def __contains__(self, key):\n        return key in self.od\n\n    def __len__(self):\n        return self.od.__len__()\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}<size_limit:{self.size_limit if self.limited else 'no limit'} total_size:{self._size}>\\n{self.od.__repr__()}\"\n\n    def set_limit_size(self, limit):\n        self.size_limit = limit\n\n    @property\n    def limited(self):\n        \"\"\"whether memory cache is limited\"\"\"\n        return self.size_limit > 0\n\n    @property\n    def total_size(self):\n        return self._size\n\n    def clear(self):\n        self._size = 0\n        self.od.clear()\n\n    def popitem(self, last=True):\n        k, v = self.od.popitem(last=last)\n        self._size -= self._get_value_size(v)\n\n        return k, v\n\n    def pop(self, key):\n        v = self.od.pop(key)\n        self._size -= self._get_value_size(v)\n\n        return v\n\n    def _adjust_size(self, key, value):\n        if key in self.od:\n            self._size -= self._get_value_size(self.od[key])\n\n        self._size += self._get_value_size(value)\n\n    @abc.abstractmethod\n    def _get_value_size(self, value):\n        raise NotImplementedError",
  "class MemCacheLengthUnit(MemCacheUnit):\n    def __init__(self, size_limit=0):\n        super().__init__(size_limit=size_limit)\n\n    def _get_value_size(self, value):\n        return 1",
  "class MemCacheSizeofUnit(MemCacheUnit):\n    def __init__(self, size_limit=0):\n        super().__init__(size_limit=size_limit)\n\n    def _get_value_size(self, value):\n        return sys.getsizeof(value)",
  "class MemCache:\n    \"\"\"Memory cache.\"\"\"\n\n    def __init__(self, mem_cache_size_limit=None, limit_type=\"length\"):\n        \"\"\"\n\n        Parameters\n        ----------\n        mem_cache_size_limit: cache max size.\n        limit_type: length or sizeof; length(call fun: len), size(call fun: sys.getsizeof).\n        \"\"\"\n\n        size_limit = C.mem_cache_size_limit if mem_cache_size_limit is None else mem_cache_size_limit\n\n        if limit_type == \"length\":\n            klass = MemCacheLengthUnit\n        elif limit_type == \"sizeof\":\n            klass = MemCacheSizeofUnit\n        else:\n            raise ValueError(f\"limit_type must be length or sizeof, your limit_type is {limit_type}\")\n\n        self.__calendar_mem_cache = klass(size_limit)\n        self.__instrument_mem_cache = klass(size_limit)\n        self.__feature_mem_cache = klass(size_limit)\n\n    def __getitem__(self, key):\n        if key == \"c\":\n            return self.__calendar_mem_cache\n        elif key == \"i\":\n            return self.__instrument_mem_cache\n        elif key == \"f\":\n            return self.__feature_mem_cache\n        else:\n            raise KeyError(\"Unknown memcache unit\")\n\n    def clear(self):\n        self.__calendar_mem_cache.clear()\n        self.__instrument_mem_cache.clear()\n        self.__feature_mem_cache.clear()",
  "class MemCacheExpire:\n    CACHE_EXPIRE = C.mem_cache_expire\n\n    @staticmethod\n    def set_cache(mem_cache, key, value):\n        \"\"\"set cache\n\n        :param mem_cache: MemCache attribute('c'/'i'/'f').\n        :param key: cache key.\n        :param value: cache value.\n        \"\"\"\n        mem_cache[key] = value, time.time()\n\n    @staticmethod\n    def get_cache(mem_cache, key):\n        \"\"\"get mem cache\n\n        :param mem_cache: MemCache attribute('c'/'i'/'f').\n        :param key: cache key.\n        :return: cache value; if cache not exist, return None.\n        \"\"\"\n        value = None\n        expire = False\n        if key in mem_cache:\n            value, latest_time = mem_cache[key]\n            expire = (time.time() - latest_time) > MemCacheExpire.CACHE_EXPIRE\n        return value, expire",
  "class CacheUtils:\n    LOCK_ID = \"QLIB\"\n\n    @staticmethod\n    def organize_meta_file():\n        pass\n\n    @staticmethod\n    def reset_lock():\n        r = get_redis_connection()\n        redis_lock.reset_all(r)\n\n    @staticmethod\n    def visit(cache_path):\n        # FIXME: Because read_lock was canceled when reading the cache, multiple processes may have read and write exceptions here\n        try:\n            with open(cache_path + \".meta\", \"rb\") as f:\n                d = pickle.load(f)\n            with open(cache_path + \".meta\", \"wb\") as f:\n                try:\n                    d[\"meta\"][\"last_visit\"] = str(time.time())\n                    d[\"meta\"][\"visits\"] = d[\"meta\"][\"visits\"] + 1\n                except KeyError:\n                    raise KeyError(\"Unknown meta keyword\")\n                pickle.dump(d, f)\n        except Exception as e:\n            get_module_logger(\"CacheUtils\").warning(f\"visit {cache_path} cache error: {e}\")\n\n    @staticmethod\n    def acquire(lock, lock_name):\n        try:\n            lock.acquire()\n        except redis_lock.AlreadyAcquired:\n            raise QlibCacheException(\n                f\"\"\"It sees the key(lock:{repr(lock_name)[1:-1]}-wlock) of the redis lock has existed in your redis db now. \n                    You can use the following command to clear your redis keys and rerun your commands:\n                    $ redis-cli\n                    > select {C.redis_task_db}\n                    > del \"lock:{repr(lock_name)[1:-1]}-wlock\"\n                    > quit\n                    If the issue is not resolved, use \"keys *\" to find if multiple keys exist. If so, try using \"flushall\" to clear all the keys.\n                \"\"\"\n            )\n\n    @staticmethod\n    @contextlib.contextmanager\n    def reader_lock(redis_t, lock_name):\n        lock_name = f\"{C.provider_uri}:{lock_name}\"\n        current_cache_rlock = redis_lock.Lock(redis_t, \"%s-rlock\" % lock_name)\n        current_cache_wlock = redis_lock.Lock(redis_t, \"%s-wlock\" % lock_name)\n        # make sure only one reader is entering\n        current_cache_rlock.acquire(timeout=60)\n        try:\n            current_cache_readers = redis_t.get(\"%s-reader\" % lock_name)\n            if current_cache_readers is None or int(current_cache_readers) == 0:\n                CacheUtils.acquire(current_cache_wlock, lock_name)\n            redis_t.incr(\"%s-reader\" % lock_name)\n        finally:\n            current_cache_rlock.release()\n        try:\n            yield\n        finally:\n            # make sure only one reader is leaving\n            current_cache_rlock.acquire(timeout=60)\n            try:\n                redis_t.decr(\"%s-reader\" % lock_name)\n                if int(redis_t.get(\"%s-reader\" % lock_name)) == 0:\n                    redis_t.delete(\"%s-reader\" % lock_name)\n                    current_cache_wlock.reset()\n            finally:\n                current_cache_rlock.release()\n\n    @staticmethod\n    @contextlib.contextmanager\n    def writer_lock(redis_t, lock_name):\n        lock_name = f\"{C.provider_uri}:{lock_name}\"\n        current_cache_wlock = redis_lock.Lock(redis_t, \"%s-wlock\" % lock_name, id=CacheUtils.LOCK_ID)\n        CacheUtils.acquire(current_cache_wlock, lock_name)\n        try:\n            yield\n        finally:\n            current_cache_wlock.release()",
  "class BaseProviderCache:\n    \"\"\"Provider cache base class\"\"\"\n\n    def __init__(self, provider):\n        self.provider = provider\n        self.logger = get_module_logger(self.__class__.__name__)\n\n    def __getattr__(self, attr):\n        return getattr(self.provider, attr)",
  "class ExpressionCache(BaseProviderCache):\n    \"\"\"Expression cache mechanism base class.\n\n    This class is used to wrap expression provider with self-defined expression cache mechanism.\n\n    .. note:: Override the `_uri` and `_expression` method to create your own expression cache mechanism.\n    \"\"\"\n\n    def expression(self, instrument, field, start_time, end_time, freq):\n        \"\"\"Get expression data.\n\n        .. note:: Same interface as `expression` method in expression provider\n        \"\"\"\n        try:\n            return self._expression(instrument, field, start_time, end_time, freq)\n        except NotImplementedError:\n            return self.provider.expression(instrument, field, start_time, end_time, freq)\n\n    def _uri(self, instrument, field, start_time, end_time, freq):\n        \"\"\"Get expression cache file uri.\n\n        Override this method to define how to get expression cache file uri corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\"Implement this function to match your own cache mechanism\")\n\n    def _expression(self, instrument, field, start_time, end_time, freq):\n        \"\"\"Get expression data using cache.\n\n        Override this method to define how to get expression data corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\"Implement this method if you want to use expression cache\")\n\n    def update(self, cache_uri):\n        \"\"\"Update expression cache to latest calendar.\n\n        Overide this method to define how to update expression cache corresponding to users' own cache mechanism.\n\n        Parameters\n        ----------\n        cache_uri : str\n            the complete uri of expression cache file (include dir path).\n\n        Returns\n        -------\n        int\n            0(successful update)/ 1(no need to update)/ 2(update failure).\n        \"\"\"\n        raise NotImplementedError(\"Implement this method if you want to make expression cache up to date\")",
  "class DatasetCache(BaseProviderCache):\n    \"\"\"Dataset cache mechanism base class.\n\n    This class is used to wrap dataset provider with self-defined dataset cache mechanism.\n\n    .. note:: Override the `_uri` and `_dataset` method to create your own dataset cache mechanism.\n    \"\"\"\n\n    HDF_KEY = \"df\"\n\n    def dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=1):\n        \"\"\"Get feature dataset.\n\n        .. note:: Same interface as `dataset` method in dataset provider\n\n        .. note:: The server use redis_lock to make sure\n            read-write conflicts will not be triggered\n                but client readers are not considered.\n        \"\"\"\n        if disk_cache == 0:\n            # skip cache\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n        else:\n            # use and replace cache\n            try:\n                return self._dataset(instruments, fields, start_time, end_time, freq, disk_cache)\n            except NotImplementedError:\n                return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n\n    def _uri(self, instruments, fields, start_time, end_time, freq, **kwargs):\n        \"\"\"Get dataset cache file uri.\n\n        Override this method to define how to get dataset cache file uri corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\"Implement this function to match your own cache mechanism\")\n\n    def _dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=1):\n        \"\"\"Get feature dataset using cache.\n\n        Override this method to define how to get feature dataset corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\"Implement this method if you want to use dataset feature cache\")\n\n    def _dataset_uri(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=1):\n        \"\"\"Get a uri of feature dataset using cache.\n        specially:\n            disk_cache=1 means using data set cache and return the uri of cache file.\n            disk_cache=0 means client knows the path of expression cache,\n                         server checks if the cache exists(if not, generate it), and client loads data by itself.\n        Override this method to define how to get feature dataset uri corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\n            \"Implement this method if you want to use dataset feature cache as a cache file for client\"\n        )\n\n    def update(self, cache_uri):\n        \"\"\"Update dataset cache to latest calendar.\n\n        Overide this method to define how to update dataset cache corresponding to users' own cache mechanism.\n\n        Parameters\n        ----------\n        cache_uri : str\n            the complete uri of dataset cache file (include dir path).\n\n        Returns\n        -------\n        int\n            0(successful update)/ 1(no need to update)/ 2(update failure)\n        \"\"\"\n        raise NotImplementedError(\"Implement this method if you want to make expression cache up to date\")\n\n    @staticmethod\n    def cache_to_origin_data(data, fields):\n        \"\"\"cache data to origin data\n\n        :param data: pd.DataFrame, cache data.\n        :param fields: feature fields.\n        :return: pd.DataFrame.\n        \"\"\"\n        not_space_fields = remove_fields_space(fields)\n        data = data.loc[:, not_space_fields]\n        # set features fields\n        data.columns = list(fields)\n        return data\n\n    @staticmethod\n    def normalize_uri_args(instruments, fields, freq):\n        \"\"\"normalize uri args\"\"\"\n        instruments = normalize_cache_instruments(instruments)\n        fields = normalize_cache_fields(fields)\n        freq = freq.lower()\n\n        return instruments, fields, freq",
  "class DiskExpressionCache(ExpressionCache):\n    \"\"\"Prepared cache mechanism for server.\"\"\"\n\n    def __init__(self, provider, **kwargs):\n        super(DiskExpressionCache, self).__init__(provider)\n        self.r = get_redis_connection()\n        # remote==True means client is using this module, writing behaviour will not be allowed.\n        self.remote = kwargs.get(\"remote\", False)\n        self.expr_cache_path = os.path.join(C.get_data_path(), C.features_cache_dir_name)\n        os.makedirs(self.expr_cache_path, exist_ok=True)\n\n    def _uri(self, instrument, field, start_time, end_time, freq):\n        field = remove_fields_space(field)\n        instrument = str(instrument).lower()\n        return hash_args(instrument, field, freq)\n\n    @staticmethod\n    def check_cache_exists(cache_path):\n        for p in [cache_path, cache_path + \".meta\"]:\n            if not Path(p).exists():\n                return False\n        return True\n\n    def _expression(self, instrument, field, start_time=None, end_time=None, freq=\"day\"):\n        _cache_uri = self._uri(instrument=instrument, field=field, start_time=None, end_time=None, freq=freq)\n        _instrument_dir = os.path.join(self.expr_cache_path, instrument.lower())\n        cache_path = os.path.join(_instrument_dir, _cache_uri)\n        # get calendar\n        from .data import Cal\n\n        _calendar = Cal.calendar(freq=freq)\n\n        _, _, start_index, end_index = Cal.locate_index(start_time, end_time, freq, future=False)\n\n        if self.check_cache_exists(cache_path):\n            \"\"\"\n            In most cases, we do not need reader_lock.\n            Because updating data is a small probability event compare to reading data.\n\n            \"\"\"\n            # FIXME: Removing the reader lock may result in conflicts.\n            # with CacheUtils.reader_lock(self.r, 'expression-%s' % _cache_uri):\n\n            # modify expression cache meta file\n            try:\n                # FIXME: Multiple readers may result in error visit number\n                if not self.remote:\n                    CacheUtils.visit(cache_path)\n                series = read_bin(cache_path, start_index, end_index)\n                return series\n            except Exception as e:\n                series = None\n                self.logger.error(\"reading %s file error : %s\" % (cache_path, traceback.format_exc()))\n            return series\n        else:\n            # normalize field\n            field = remove_fields_space(field)\n            # cache unavailable, generate the cache\n            if not os.path.exists(_instrument_dir):\n                os.makedirs(_instrument_dir, exist_ok=True)\n            if not isinstance(eval(parse_field(field)), Feature):\n                # When the expression is not a raw feature\n                # generate expression cache if the feature is not a Feature\n                # instance\n                series = self.provider.expression(instrument, field, _calendar[0], _calendar[-1], freq)\n                if not series.empty:\n                    # This expresion is empty, we don't generate any cache for it.\n                    with CacheUtils.writer_lock(self.r, \"expression-%s\" % _cache_uri):\n                        self.gen_expression_cache(\n                            expression_data=series,\n                            cache_path=cache_path,\n                            instrument=instrument,\n                            field=field,\n                            freq=freq,\n                            last_update=str(_calendar[-1]),\n                        )\n                    return series.loc[start_index:end_index]\n                else:\n                    return series\n            else:\n                # If the expression is a raw feature(such as $close, $open)\n                return self.provider.expression(instrument, field, start_time, end_time, freq)\n\n    @staticmethod\n    def clear_cache(cache_path):\n        meta_path = cache_path + \".meta\"\n        for p in [cache_path, meta_path]:\n            p = Path(p)\n            if p.exists():\n                p.unlink()\n\n    def gen_expression_cache(self, expression_data, cache_path, instrument, field, freq, last_update):\n        \"\"\"use bin file to save like feature-data.\"\"\"\n        # Make sure the cache runs right when the directory is deleted\n        # while running\n        meta = {\n            \"info\": {\"instrument\": instrument, \"field\": field, \"freq\": freq, \"last_update\": last_update},\n            \"meta\": {\"last_visit\": time.time(), \"visits\": 1},\n        }\n        self.logger.debug(f\"generating expression cache: {meta}\")\n        os.makedirs(self.expr_cache_path, exist_ok=True)\n        self.clear_cache(cache_path)\n        meta_path = cache_path + \".meta\"\n\n        with open(meta_path, \"wb\") as f:\n            pickle.dump(meta, f)\n        os.chmod(meta_path, stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)\n        df = expression_data.to_frame()\n\n        r = np.hstack([df.index[0], expression_data]).astype(\"<f\")\n        r.tofile(str(cache_path))\n\n    def update(self, sid, cache_uri):\n        cp_cache_uri = os.path.join(self.expr_cache_path, sid, cache_uri)\n        if not self.check_cache_exists(cp_cache_uri):\n            self.logger.info(f\"The cache {cp_cache_uri} has corrupted. It will be removed\")\n            self.clear_cache(cp_cache_uri)\n            return 2\n\n        with CacheUtils.writer_lock(self.r, \"expression-%s\" % cache_uri):\n            with open(cp_cache_uri + \".meta\", \"rb\") as f:\n                d = pickle.load(f)\n            instrument = d[\"info\"][\"instrument\"]\n            field = d[\"info\"][\"field\"]\n            freq = d[\"info\"][\"freq\"]\n            last_update_time = d[\"info\"][\"last_update\"]\n\n            # get newest calendar\n            from .data import Cal, ExpressionD\n\n            whole_calendar = Cal.calendar(start_time=None, end_time=None, freq=freq)\n            # calendar since last updated.\n            new_calendar = Cal.calendar(start_time=last_update_time, end_time=None, freq=freq)\n\n            # get append data\n            if len(new_calendar) <= 1:\n                # Including last updated calendar, we only get 1 item.\n                # No future updating is needed.\n                return 1\n            else:\n                # get the data needed after the historical data are removed.\n                # The start index of new data\n                current_index = len(whole_calendar) - len(new_calendar) + 1\n\n                # The existing data length\n                size_bytes = os.path.getsize(cp_cache_uri)\n                ele_size = np.dtype(\"<f\").itemsize\n                assert size_bytes % ele_size == 0\n                ele_n = size_bytes // ele_size - 1\n\n                expr = ExpressionD.get_expression_instance(field)\n                lft_etd, rght_etd = expr.get_extended_window_size()\n                # The expression used the future data after rght_etd days.\n                # So the last rght_etd data should be removed.\n                # There are most `ele_n` period of data can be remove\n                remove_n = min(rght_etd, ele_n)\n                assert new_calendar[1] == whole_calendar[current_index]\n                data = self.provider.expression(\n                    instrument, field, whole_calendar[current_index - remove_n], new_calendar[-1], freq\n                )\n                with open(cp_cache_uri, \"ab\") as f:\n                    data = np.array(data).astype(\"<f\")\n                    # Remove the last bits\n                    f.truncate(size_bytes - ele_size * remove_n)\n                    f.write(data)\n                # update meta file\n                d[\"info\"][\"last_update\"] = str(new_calendar[-1])\n                with open(cp_cache_uri + \".meta\", \"wb\") as f:\n                    pickle.dump(d, f)\n        return 0",
  "class DiskDatasetCache(DatasetCache):\n    \"\"\"Prepared cache mechanism for server.\"\"\"\n\n    def __init__(self, provider, **kwargs):\n        super(DiskDatasetCache, self).__init__(provider)\n        self.r = get_redis_connection()\n        self.remote = kwargs.get(\"remote\", False)\n        self.dtst_cache_path = os.path.join(C.get_data_path(), C.dataset_cache_dir_name)\n        os.makedirs(self.dtst_cache_path, exist_ok=True)\n\n    @staticmethod\n    def _uri(instruments, fields, start_time, end_time, freq, disk_cache=1, **kwargs):\n        return hash_args(*DatasetCache.normalize_uri_args(instruments, fields, freq), disk_cache)\n\n    @staticmethod\n    def check_cache_exists(cache_path):\n        for p in [cache_path, cache_path + \".index\", cache_path + \".meta\"]:\n            if not Path(p).exists():\n                return False\n        return True\n\n    @classmethod\n    def read_data_from_cache(cls, cache_path, start_time, end_time, fields):\n        \"\"\"read_cache_from\n\n        This function can read data from the disk cache dataset\n\n        :param cache_path:\n        :param start_time:\n        :param end_time:\n        :param fields: The fields order of the dataset cache is sorted. So rearrange the columns to make it consistent.\n        :return:\n        \"\"\"\n\n        im = DiskDatasetCache.IndexManager(cache_path)\n        index_data = im.get_index(start_time, end_time)\n        if index_data.shape[0] > 0:\n            start, stop = (\n                index_data[\"start\"].iloc[0].item(),\n                index_data[\"end\"].iloc[-1].item(),\n            )\n        else:\n            start = stop = 0\n\n        with pd.HDFStore(cache_path, mode=\"r\") as store:\n            if \"/{}\".format(im.KEY) in store.keys():\n                df = store.select(key=im.KEY, start=start, stop=stop)\n                df = df.swaplevel(\"datetime\", \"instrument\").sort_index()\n                # read cache and need to replace not-space fields to field\n                df = cls.cache_to_origin_data(df, fields)\n\n            else:\n                df = pd.DataFrame(columns=fields)\n        return df\n\n    def _dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=0):\n\n        if disk_cache == 0:\n            # In this case, data_set cache is configured but will not be used.\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n\n        _cache_uri = self._uri(\n            instruments=instruments, fields=fields, start_time=None, end_time=None, freq=freq, disk_cache=disk_cache\n        )\n\n        cache_path = os.path.join(self.dtst_cache_path, _cache_uri)\n\n        features = pd.DataFrame()\n        gen_flag = False\n\n        if self.check_cache_exists(cache_path):\n            if disk_cache == 1:\n                # use cache\n                with CacheUtils.reader_lock(self.r, \"dataset-%s\" % _cache_uri):\n                    CacheUtils.visit(cache_path)\n                    features = self.read_data_from_cache(cache_path, start_time, end_time, fields)\n            elif disk_cache == 2:\n                gen_flag = True\n        else:\n            gen_flag = True\n\n        if gen_flag:\n            # cache unavailable, generate the cache\n            with CacheUtils.writer_lock(self.r, \"dataset-%s\" % _cache_uri):\n                features = self.gen_dataset_cache(\n                    cache_path=cache_path, instruments=instruments, fields=fields, freq=freq\n                )\n            if not features.empty:\n                features = features.sort_index().loc(axis=0)[:, start_time:end_time]\n        return features\n\n    def _dataset_uri(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=0):\n        if disk_cache == 0:\n            # In this case, server only checks the expression cache.\n            # The client will load the cache data by itself.\n            from .data import LocalDatasetProvider\n\n            LocalDatasetProvider.multi_cache_walker(instruments, fields, start_time, end_time, freq)\n            return \"\"\n\n        _cache_uri = self._uri(\n            instruments=instruments, fields=fields, start_time=None, end_time=None, freq=freq, disk_cache=disk_cache\n        )\n        cache_path = os.path.join(self.dtst_cache_path, _cache_uri)\n\n        if self.check_cache_exists(cache_path):\n            self.logger.debug(f\"The cache dataset has already existed {cache_path}. Return the uri directly\")\n            with CacheUtils.reader_lock(self.r, \"dataset-%s\" % _cache_uri):\n                CacheUtils.visit(cache_path)\n            return _cache_uri\n        else:\n            # cache unavailable, generate the cache\n            with CacheUtils.writer_lock(self.r, \"dataset-%s\" % _cache_uri):\n                self.gen_dataset_cache(cache_path=cache_path, instruments=instruments, fields=fields, freq=freq)\n            return _cache_uri\n\n    class IndexManager:\n        \"\"\"\n        The lock is not considered in the class. Please consider the lock outside the code.\n        This class is the proxy of the disk data.\n        \"\"\"\n\n        KEY = \"df\"\n\n        def __init__(self, cache_path):\n            self.index_path = cache_path + \".index\"\n            self._data = None\n            self.logger = get_module_logger(self.__class__.__name__)\n\n        def get_index(self, start_time=None, end_time=None):\n            # TODO: fast read index from the disk.\n            if self._data is None:\n                self.sync_from_disk()\n            return self._data.loc[start_time:end_time].copy()\n\n        def sync_to_disk(self):\n            if self._data is None:\n                raise ValueError(\"No data to sync to disk.\")\n            self._data.sort_index(inplace=True)\n            self._data.to_hdf(self.index_path, key=self.KEY, mode=\"w\", format=\"table\")\n            # The index should be readable for all users\n            os.chmod(self.index_path, stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)\n\n        def sync_from_disk(self):\n            # The file will not be closed directly if we read_hdf from the disk directly\n            with pd.HDFStore(self.index_path, mode=\"r\") as store:\n                if \"/{}\".format(self.KEY) in store.keys():\n                    self._data = pd.read_hdf(store, key=self.KEY)\n                else:\n                    self._data = pd.DataFrame()\n\n        def update(self, data, sync=True):\n            self._data = data.astype(np.int32).copy()\n            if sync:\n                self.sync_to_disk()\n\n        def append_index(self, data, to_disk=True):\n            data = data.astype(np.int32).copy()\n            data.sort_index(inplace=True)\n            self._data = pd.concat([self._data, data])\n            if to_disk:\n                with pd.HDFStore(self.index_path) as store:\n                    store.append(self.KEY, data, append=True)\n\n        @staticmethod\n        def build_index_from_data(data, start_index=0):\n            if data.empty:\n                return pd.DataFrame()\n            line_data = data.iloc[:, 0].fillna(0).groupby(\"datetime\").count()\n            line_data.sort_index(inplace=True)\n            index_end = line_data.cumsum()\n            index_start = index_end.shift(1).fillna(0)\n\n            index_data = pd.DataFrame()\n            index_data[\"start\"] = index_start\n            index_data[\"end\"] = index_end\n            index_data += start_index\n            return index_data\n\n    @staticmethod\n    def clear_cache(cache_path):\n        meta_path = cache_path + \".meta\"\n        for p in [cache_path, meta_path, cache_path + \".index\", cache_path + \".data\"]:\n            p = Path(p)\n            if p.exists():\n                p.unlink()\n\n    def gen_dataset_cache(self, cache_path, instruments, fields, freq):\n        \"\"\"gen_dataset_cache\n\n        .. note:: This function does not consider the cache read write lock. Please\n        Aquire the lock outside this function\n\n        The format the cache contains 3 parts(followed by typical filename).\n\n        - index : cache/d41366901e25de3ec47297f12e2ba11d.index\n\n            - The content of the file may be in following format(pandas.Series)\n\n                .. code-block:: python\n\n                                        start end\n                    1999-11-10 00:00:00     0   1\n                    1999-11-11 00:00:00     1   2\n                    1999-11-12 00:00:00     2   3\n                    ...\n\n            .. note:: The start is closed. The end is open!!!!!\n\n            - Each line contains two element <start_index, end_index> with a timestamp as its index.\n            - It indicates the `start_index`(included) and `end_index`(excluded) of the data for `timestamp`\n\n        - meta data: cache/d41366901e25de3ec47297f12e2ba11d.meta\n\n        - data     : cache/d41366901e25de3ec47297f12e2ba11d\n\n            - This is a hdf file sorted by datetime\n\n        :param cache_path:  The path to store the cache.\n        :param instruments:  The instruments to store the cache.\n        :param fields:  The fields to store the cache.\n        :param freq:  The freq to store the cache.\n\n        :return type pd.DataFrame; The fields of the returned DataFrame are consistent with the parameters of the function.\n        \"\"\"\n        # get calendar\n        from .data import Cal\n\n        _calendar = Cal.calendar(freq=freq)\n        self.logger.debug(f\"Generating dataset cache {cache_path}\")\n        # Make sure the cache runs right when the directory is deleted\n        # while running\n        os.makedirs(self.dtst_cache_path, exist_ok=True)\n        self.clear_cache(cache_path)\n\n        features = self.provider.dataset(instruments, fields, _calendar[0], _calendar[-1], freq)\n\n        if features.empty:\n            return features\n\n        # swap index and sorted\n        features = features.swaplevel(\"instrument\", \"datetime\").sort_index()\n\n        # write cache data\n        with pd.HDFStore(cache_path + \".data\") as store:\n            cache_to_orig_map = dict(zip(remove_fields_space(features.columns), features.columns))\n            orig_to_cache_map = dict(zip(features.columns, remove_fields_space(features.columns)))\n            cache_features = features[list(cache_to_orig_map.values())].rename(columns=orig_to_cache_map)\n            # cache columns\n            cache_columns = sorted(cache_features.columns)\n            cache_features = cache_features.loc[:, cache_columns]\n            cache_features = cache_features.loc[:, ~cache_features.columns.duplicated()]\n            store.append(DatasetCache.HDF_KEY, cache_features, append=False)\n        # write meta file\n        meta = {\n            \"info\": {\n                \"instruments\": instruments,\n                \"fields\": cache_columns,\n                \"freq\": freq,\n                \"last_update\": str(_calendar[-1]),  # The last_update to store the cache\n            },\n            \"meta\": {\"last_visit\": time.time(), \"visits\": 1},\n        }\n        with open(cache_path + \".meta\", \"wb\") as f:\n            pickle.dump(meta, f)\n        os.chmod(cache_path + \".meta\", stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)\n        # write index file\n        im = DiskDatasetCache.IndexManager(cache_path)\n        index_data = im.build_index_from_data(features)\n        im.update(index_data)\n\n        # rename the file after the cache has been generated\n        # this doesn't work well on windows, but our server won't use windows\n        # temporarily\n        os.replace(cache_path + \".data\", cache_path)\n        # the fields of the cached features are converted to the original fields\n        return features.swaplevel(\"datetime\", \"instrument\")\n\n    def update(self, cache_uri):\n        cp_cache_uri = os.path.join(self.dtst_cache_path, cache_uri)\n\n        if not self.check_cache_exists(cp_cache_uri):\n            self.logger.info(f\"The cache {cp_cache_uri} has corrupted. It will be removed\")\n            self.clear_cache(cp_cache_uri)\n            return 2\n\n        im = DiskDatasetCache.IndexManager(cp_cache_uri)\n        with CacheUtils.writer_lock(self.r, \"dataset-%s\" % cache_uri):\n            with open(cp_cache_uri + \".meta\", \"rb\") as f:\n                d = pickle.load(f)\n            instruments = d[\"info\"][\"instruments\"]\n            fields = d[\"info\"][\"fields\"]\n            freq = d[\"info\"][\"freq\"]\n            last_update_time = d[\"info\"][\"last_update\"]\n            index_data = im.get_index()\n\n            self.logger.debug(\"Updating dataset: {}\".format(d))\n            from .data import Inst\n\n            if Inst.get_inst_type(instruments) == Inst.DICT:\n                self.logger.info(f\"The file {cache_uri} has dict cache. Skip updating\")\n                return 1\n\n            # get newest calendar\n            from .data import Cal\n\n            whole_calendar = Cal.calendar(start_time=None, end_time=None, freq=freq)\n            # The calendar since last updated\n            new_calendar = Cal.calendar(start_time=last_update_time, end_time=None, freq=freq)\n\n            # get append data\n            if len(new_calendar) <= 1:\n                # Including last updated calendar, we only get 1 item.\n                # No future updating is needed.\n                return 1\n            else:\n                # get the data needed after the historical data are removed.\n                # The start index of new data\n                current_index = len(whole_calendar) - len(new_calendar) + 1\n\n                # To avoid recursive import\n                from .data import ExpressionD\n\n                # The existing data length\n                lft_etd = rght_etd = 0\n                for field in fields:\n                    expr = ExpressionD.get_expression_instance(field)\n                    l, r = expr.get_extended_window_size()\n                    lft_etd = max(lft_etd, l)\n                    rght_etd = max(rght_etd, r)\n                # remove the period that should be updated.\n                if index_data.empty:\n                    # We don't have any data for such dataset. Nothing to remove\n                    rm_n_period = rm_lines = 0\n                else:\n                    rm_n_period = min(rght_etd, index_data.shape[0])\n                    rm_lines = (\n                        (index_data[\"end\"] - index_data[\"start\"])\n                        .loc[whole_calendar[current_index - rm_n_period] :]\n                        .sum()\n                        .item()\n                    )\n\n                data = self.provider.dataset(\n                    instruments, fields, whole_calendar[current_index - rm_n_period], new_calendar[-1], freq\n                )\n\n                if not data.empty:\n                    data.reset_index(inplace=True)\n                    data.set_index([\"datetime\", \"instrument\"], inplace=True)\n                    data.sort_index(inplace=True)\n                else:\n                    return 0  # No data to update cache\n\n                store = pd.HDFStore(cp_cache_uri)\n                # FIXME:\n                # Because the feature cache are stored as .bin file.\n                # So the series read from features are all float32.\n                # However, the first dataset cache is calulated based on the\n                # raw data. So the data type may be float64.\n                # Different data type will result in failure of appending data\n                if \"/{}\".format(DatasetCache.HDF_KEY) in store.keys():\n                    schema = store.select(DatasetCache.HDF_KEY, start=0, stop=0)\n                    for col, dtype in schema.dtypes.items():\n                        data[col] = data[col].astype(dtype)\n                if rm_lines > 0:\n                    store.remove(key=im.KEY, start=-rm_lines)\n                store.append(DatasetCache.HDF_KEY, data)\n                store.close()\n\n                # update index file\n                new_index_data = im.build_index_from_data(\n                    data.loc(axis=0)[whole_calendar[current_index] :, :],\n                    start_index=0 if index_data.empty else index_data[\"end\"].iloc[-1],\n                )\n                im.append_index(new_index_data)\n\n                # update meta file\n                d[\"info\"][\"last_update\"] = str(new_calendar[-1])\n                with open(cp_cache_uri + \".meta\", \"wb\") as f:\n                    pickle.dump(d, f)\n                return 0",
  "class SimpleDatasetCache(DatasetCache):\n    \"\"\"Simple dataset cache that can be used locally or on client.\"\"\"\n\n    def __init__(self, provider):\n        super(SimpleDatasetCache, self).__init__(provider)\n        try:\n            self.local_cache_path = C[\"local_cache_path\"]\n        except KeyError as e:\n            self.logger.error(\"Assign a local_cache_path in config if you want to use this cache mechanism\")\n\n    def _uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1, **kwargs):\n        instruments, fields, freq = self.normalize_uri_args(instruments, fields, freq)\n        local_cache_path = str(Path(self.local_cache_path).expanduser().resolve())\n        return hash_args(instruments, fields, start_time, end_time, freq, disk_cache, local_cache_path)\n\n    def _dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=1):\n        if disk_cache == 0:\n            # In this case, data_set cache is configured but will not be used.\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n        os.makedirs(os.path.expanduser(self.local_cache_path), exist_ok=True)\n        cache_file = os.path.join(\n            self.local_cache_path, self._uri(instruments, fields, start_time, end_time, freq, disk_cache=disk_cache)\n        )\n        gen_flag = False\n\n        if os.path.exists(cache_file):\n            if disk_cache == 1:\n                # use cache\n                df = pd.read_pickle(cache_file)\n                return self.cache_to_origin_data(df, fields)\n            elif disk_cache == 2:\n                # replace cache\n                gen_flag = True\n        else:\n            gen_flag = True\n\n        if gen_flag:\n            data = self.provider.dataset(instruments, normalize_cache_fields(fields), start_time, end_time, freq)\n            data.to_pickle(cache_file)\n            return self.cache_to_origin_data(data, fields)",
  "class DatasetURICache(DatasetCache):\n    \"\"\"Prepared cache mechanism for server.\"\"\"\n\n    def _uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1, **kwargs):\n        return hash_args(*self.normalize_uri_args(instruments, fields, freq), disk_cache)\n\n    def dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=0):\n\n        if \"local\" in C.dataset_provider.lower():\n            # use LocalDatasetProvider\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n\n        if disk_cache == 0:\n            # do not use data_set cache, load data from remote expression cache directly\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq, disk_cache, return_uri=False)\n\n        # use ClientDatasetProvider\n        feature_uri = self._uri(instruments, fields, None, None, freq, disk_cache=disk_cache)\n        value, expire = MemCacheExpire.get_cache(H[\"f\"], feature_uri)\n        mnt_feature_uri = os.path.join(C.get_data_path(), C.dataset_cache_dir_name, feature_uri)\n        if value is None or expire or not os.path.exists(mnt_feature_uri):\n            df, uri = self.provider.dataset(\n                instruments, fields, start_time, end_time, freq, disk_cache, return_uri=True\n            )\n            # cache uri\n            MemCacheExpire.set_cache(H[\"f\"], uri, uri)\n            # cache DataFrame\n            # HZ['f'][uri] = df.copy()\n            get_module_logger(\"cache\").debug(f\"get feature from {C.dataset_provider}\")\n        else:\n            mnt_feature_uri = os.path.join(C.get_data_path(), C.dataset_cache_dir_name, feature_uri)\n            df = DiskDatasetCache.read_data_from_cache(mnt_feature_uri, start_time, end_time, fields)\n            get_module_logger(\"cache\").debug(\"get feature from uri cache\")\n\n        return df",
  "class CalendarCache(BaseProviderCache):\n    pass",
  "class MemoryCalendarCache(CalendarCache):\n    def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        uri = self._uri(start_time, end_time, freq, future)\n        result, expire = MemCacheExpire.get_cache(H[\"c\"], uri)\n        if result is None or expire:\n\n            result = self.provider.calendar(start_time, end_time, freq, future)\n            MemCacheExpire.set_cache(H[\"c\"], uri, result)\n\n            get_module_logger(\"data\").debug(f\"get calendar from {C.calendar_provider}\")\n        else:\n            get_module_logger(\"data\").debug(\"get calendar from local cache\")\n\n        return result",
  "def __init__(self, *args, **kwargs):\n        self.size_limit = kwargs.pop(\"size_limit\", 0)\n        self._size = 0\n        self.od = OrderedDict()",
  "def __setitem__(self, key, value):\n        # TODO: thread safe?__setitem__ failure might cause inconsistent size?\n\n        # precalculate the size after od.__setitem__\n        self._adjust_size(key, value)\n\n        self.od.__setitem__(key, value)\n\n        # move the key to end,make it latest\n        self.od.move_to_end(key)\n\n        if self.limited:\n            # pop the oldest items beyond size limit\n            while self._size > self.size_limit:\n                self.popitem(last=False)",
  "def __getitem__(self, key):\n        v = self.od.__getitem__(key)\n        self.od.move_to_end(key)\n        return v",
  "def __contains__(self, key):\n        return key in self.od",
  "def __len__(self):\n        return self.od.__len__()",
  "def __repr__(self):\n        return f\"{self.__class__.__name__}<size_limit:{self.size_limit if self.limited else 'no limit'} total_size:{self._size}>\\n{self.od.__repr__()}\"",
  "def set_limit_size(self, limit):\n        self.size_limit = limit",
  "def limited(self):\n        \"\"\"whether memory cache is limited\"\"\"\n        return self.size_limit > 0",
  "def total_size(self):\n        return self._size",
  "def clear(self):\n        self._size = 0\n        self.od.clear()",
  "def popitem(self, last=True):\n        k, v = self.od.popitem(last=last)\n        self._size -= self._get_value_size(v)\n\n        return k, v",
  "def pop(self, key):\n        v = self.od.pop(key)\n        self._size -= self._get_value_size(v)\n\n        return v",
  "def _adjust_size(self, key, value):\n        if key in self.od:\n            self._size -= self._get_value_size(self.od[key])\n\n        self._size += self._get_value_size(value)",
  "def _get_value_size(self, value):\n        raise NotImplementedError",
  "def __init__(self, size_limit=0):\n        super().__init__(size_limit=size_limit)",
  "def _get_value_size(self, value):\n        return 1",
  "def __init__(self, size_limit=0):\n        super().__init__(size_limit=size_limit)",
  "def _get_value_size(self, value):\n        return sys.getsizeof(value)",
  "def __init__(self, mem_cache_size_limit=None, limit_type=\"length\"):\n        \"\"\"\n\n        Parameters\n        ----------\n        mem_cache_size_limit: cache max size.\n        limit_type: length or sizeof; length(call fun: len), size(call fun: sys.getsizeof).\n        \"\"\"\n\n        size_limit = C.mem_cache_size_limit if mem_cache_size_limit is None else mem_cache_size_limit\n\n        if limit_type == \"length\":\n            klass = MemCacheLengthUnit\n        elif limit_type == \"sizeof\":\n            klass = MemCacheSizeofUnit\n        else:\n            raise ValueError(f\"limit_type must be length or sizeof, your limit_type is {limit_type}\")\n\n        self.__calendar_mem_cache = klass(size_limit)\n        self.__instrument_mem_cache = klass(size_limit)\n        self.__feature_mem_cache = klass(size_limit)",
  "def __getitem__(self, key):\n        if key == \"c\":\n            return self.__calendar_mem_cache\n        elif key == \"i\":\n            return self.__instrument_mem_cache\n        elif key == \"f\":\n            return self.__feature_mem_cache\n        else:\n            raise KeyError(\"Unknown memcache unit\")",
  "def clear(self):\n        self.__calendar_mem_cache.clear()\n        self.__instrument_mem_cache.clear()\n        self.__feature_mem_cache.clear()",
  "def set_cache(mem_cache, key, value):\n        \"\"\"set cache\n\n        :param mem_cache: MemCache attribute('c'/'i'/'f').\n        :param key: cache key.\n        :param value: cache value.\n        \"\"\"\n        mem_cache[key] = value, time.time()",
  "def get_cache(mem_cache, key):\n        \"\"\"get mem cache\n\n        :param mem_cache: MemCache attribute('c'/'i'/'f').\n        :param key: cache key.\n        :return: cache value; if cache not exist, return None.\n        \"\"\"\n        value = None\n        expire = False\n        if key in mem_cache:\n            value, latest_time = mem_cache[key]\n            expire = (time.time() - latest_time) > MemCacheExpire.CACHE_EXPIRE\n        return value, expire",
  "def organize_meta_file():\n        pass",
  "def reset_lock():\n        r = get_redis_connection()\n        redis_lock.reset_all(r)",
  "def visit(cache_path):\n        # FIXME: Because read_lock was canceled when reading the cache, multiple processes may have read and write exceptions here\n        try:\n            with open(cache_path + \".meta\", \"rb\") as f:\n                d = pickle.load(f)\n            with open(cache_path + \".meta\", \"wb\") as f:\n                try:\n                    d[\"meta\"][\"last_visit\"] = str(time.time())\n                    d[\"meta\"][\"visits\"] = d[\"meta\"][\"visits\"] + 1\n                except KeyError:\n                    raise KeyError(\"Unknown meta keyword\")\n                pickle.dump(d, f)\n        except Exception as e:\n            get_module_logger(\"CacheUtils\").warning(f\"visit {cache_path} cache error: {e}\")",
  "def acquire(lock, lock_name):\n        try:\n            lock.acquire()\n        except redis_lock.AlreadyAcquired:\n            raise QlibCacheException(\n                f\"\"\"It sees the key(lock:{repr(lock_name)[1:-1]}-wlock) of the redis lock has existed in your redis db now. \n                    You can use the following command to clear your redis keys and rerun your commands:\n                    $ redis-cli\n                    > select {C.redis_task_db}\n                    > del \"lock:{repr(lock_name)[1:-1]}-wlock\"\n                    > quit\n                    If the issue is not resolved, use \"keys *\" to find if multiple keys exist. If so, try using \"flushall\" to clear all the keys.\n                \"\"\"\n            )",
  "def reader_lock(redis_t, lock_name):\n        lock_name = f\"{C.provider_uri}:{lock_name}\"\n        current_cache_rlock = redis_lock.Lock(redis_t, \"%s-rlock\" % lock_name)\n        current_cache_wlock = redis_lock.Lock(redis_t, \"%s-wlock\" % lock_name)\n        # make sure only one reader is entering\n        current_cache_rlock.acquire(timeout=60)\n        try:\n            current_cache_readers = redis_t.get(\"%s-reader\" % lock_name)\n            if current_cache_readers is None or int(current_cache_readers) == 0:\n                CacheUtils.acquire(current_cache_wlock, lock_name)\n            redis_t.incr(\"%s-reader\" % lock_name)\n        finally:\n            current_cache_rlock.release()\n        try:\n            yield\n        finally:\n            # make sure only one reader is leaving\n            current_cache_rlock.acquire(timeout=60)\n            try:\n                redis_t.decr(\"%s-reader\" % lock_name)\n                if int(redis_t.get(\"%s-reader\" % lock_name)) == 0:\n                    redis_t.delete(\"%s-reader\" % lock_name)\n                    current_cache_wlock.reset()\n            finally:\n                current_cache_rlock.release()",
  "def writer_lock(redis_t, lock_name):\n        lock_name = f\"{C.provider_uri}:{lock_name}\"\n        current_cache_wlock = redis_lock.Lock(redis_t, \"%s-wlock\" % lock_name, id=CacheUtils.LOCK_ID)\n        CacheUtils.acquire(current_cache_wlock, lock_name)\n        try:\n            yield\n        finally:\n            current_cache_wlock.release()",
  "def __init__(self, provider):\n        self.provider = provider\n        self.logger = get_module_logger(self.__class__.__name__)",
  "def __getattr__(self, attr):\n        return getattr(self.provider, attr)",
  "def expression(self, instrument, field, start_time, end_time, freq):\n        \"\"\"Get expression data.\n\n        .. note:: Same interface as `expression` method in expression provider\n        \"\"\"\n        try:\n            return self._expression(instrument, field, start_time, end_time, freq)\n        except NotImplementedError:\n            return self.provider.expression(instrument, field, start_time, end_time, freq)",
  "def _uri(self, instrument, field, start_time, end_time, freq):\n        \"\"\"Get expression cache file uri.\n\n        Override this method to define how to get expression cache file uri corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\"Implement this function to match your own cache mechanism\")",
  "def _expression(self, instrument, field, start_time, end_time, freq):\n        \"\"\"Get expression data using cache.\n\n        Override this method to define how to get expression data corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\"Implement this method if you want to use expression cache\")",
  "def update(self, cache_uri):\n        \"\"\"Update expression cache to latest calendar.\n\n        Overide this method to define how to update expression cache corresponding to users' own cache mechanism.\n\n        Parameters\n        ----------\n        cache_uri : str\n            the complete uri of expression cache file (include dir path).\n\n        Returns\n        -------\n        int\n            0(successful update)/ 1(no need to update)/ 2(update failure).\n        \"\"\"\n        raise NotImplementedError(\"Implement this method if you want to make expression cache up to date\")",
  "def dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=1):\n        \"\"\"Get feature dataset.\n\n        .. note:: Same interface as `dataset` method in dataset provider\n\n        .. note:: The server use redis_lock to make sure\n            read-write conflicts will not be triggered\n                but client readers are not considered.\n        \"\"\"\n        if disk_cache == 0:\n            # skip cache\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n        else:\n            # use and replace cache\n            try:\n                return self._dataset(instruments, fields, start_time, end_time, freq, disk_cache)\n            except NotImplementedError:\n                return self.provider.dataset(instruments, fields, start_time, end_time, freq)",
  "def _uri(self, instruments, fields, start_time, end_time, freq, **kwargs):\n        \"\"\"Get dataset cache file uri.\n\n        Override this method to define how to get dataset cache file uri corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\"Implement this function to match your own cache mechanism\")",
  "def _dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=1):\n        \"\"\"Get feature dataset using cache.\n\n        Override this method to define how to get feature dataset corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\"Implement this method if you want to use dataset feature cache\")",
  "def _dataset_uri(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=1):\n        \"\"\"Get a uri of feature dataset using cache.\n        specially:\n            disk_cache=1 means using data set cache and return the uri of cache file.\n            disk_cache=0 means client knows the path of expression cache,\n                         server checks if the cache exists(if not, generate it), and client loads data by itself.\n        Override this method to define how to get feature dataset uri corresponding to users' own cache mechanism.\n        \"\"\"\n        raise NotImplementedError(\n            \"Implement this method if you want to use dataset feature cache as a cache file for client\"\n        )",
  "def update(self, cache_uri):\n        \"\"\"Update dataset cache to latest calendar.\n\n        Overide this method to define how to update dataset cache corresponding to users' own cache mechanism.\n\n        Parameters\n        ----------\n        cache_uri : str\n            the complete uri of dataset cache file (include dir path).\n\n        Returns\n        -------\n        int\n            0(successful update)/ 1(no need to update)/ 2(update failure)\n        \"\"\"\n        raise NotImplementedError(\"Implement this method if you want to make expression cache up to date\")",
  "def cache_to_origin_data(data, fields):\n        \"\"\"cache data to origin data\n\n        :param data: pd.DataFrame, cache data.\n        :param fields: feature fields.\n        :return: pd.DataFrame.\n        \"\"\"\n        not_space_fields = remove_fields_space(fields)\n        data = data.loc[:, not_space_fields]\n        # set features fields\n        data.columns = list(fields)\n        return data",
  "def normalize_uri_args(instruments, fields, freq):\n        \"\"\"normalize uri args\"\"\"\n        instruments = normalize_cache_instruments(instruments)\n        fields = normalize_cache_fields(fields)\n        freq = freq.lower()\n\n        return instruments, fields, freq",
  "def __init__(self, provider, **kwargs):\n        super(DiskExpressionCache, self).__init__(provider)\n        self.r = get_redis_connection()\n        # remote==True means client is using this module, writing behaviour will not be allowed.\n        self.remote = kwargs.get(\"remote\", False)\n        self.expr_cache_path = os.path.join(C.get_data_path(), C.features_cache_dir_name)\n        os.makedirs(self.expr_cache_path, exist_ok=True)",
  "def _uri(self, instrument, field, start_time, end_time, freq):\n        field = remove_fields_space(field)\n        instrument = str(instrument).lower()\n        return hash_args(instrument, field, freq)",
  "def check_cache_exists(cache_path):\n        for p in [cache_path, cache_path + \".meta\"]:\n            if not Path(p).exists():\n                return False\n        return True",
  "def _expression(self, instrument, field, start_time=None, end_time=None, freq=\"day\"):\n        _cache_uri = self._uri(instrument=instrument, field=field, start_time=None, end_time=None, freq=freq)\n        _instrument_dir = os.path.join(self.expr_cache_path, instrument.lower())\n        cache_path = os.path.join(_instrument_dir, _cache_uri)\n        # get calendar\n        from .data import Cal\n\n        _calendar = Cal.calendar(freq=freq)\n\n        _, _, start_index, end_index = Cal.locate_index(start_time, end_time, freq, future=False)\n\n        if self.check_cache_exists(cache_path):\n            \"\"\"\n            In most cases, we do not need reader_lock.\n            Because updating data is a small probability event compare to reading data.\n\n            \"\"\"\n            # FIXME: Removing the reader lock may result in conflicts.\n            # with CacheUtils.reader_lock(self.r, 'expression-%s' % _cache_uri):\n\n            # modify expression cache meta file\n            try:\n                # FIXME: Multiple readers may result in error visit number\n                if not self.remote:\n                    CacheUtils.visit(cache_path)\n                series = read_bin(cache_path, start_index, end_index)\n                return series\n            except Exception as e:\n                series = None\n                self.logger.error(\"reading %s file error : %s\" % (cache_path, traceback.format_exc()))\n            return series\n        else:\n            # normalize field\n            field = remove_fields_space(field)\n            # cache unavailable, generate the cache\n            if not os.path.exists(_instrument_dir):\n                os.makedirs(_instrument_dir, exist_ok=True)\n            if not isinstance(eval(parse_field(field)), Feature):\n                # When the expression is not a raw feature\n                # generate expression cache if the feature is not a Feature\n                # instance\n                series = self.provider.expression(instrument, field, _calendar[0], _calendar[-1], freq)\n                if not series.empty:\n                    # This expresion is empty, we don't generate any cache for it.\n                    with CacheUtils.writer_lock(self.r, \"expression-%s\" % _cache_uri):\n                        self.gen_expression_cache(\n                            expression_data=series,\n                            cache_path=cache_path,\n                            instrument=instrument,\n                            field=field,\n                            freq=freq,\n                            last_update=str(_calendar[-1]),\n                        )\n                    return series.loc[start_index:end_index]\n                else:\n                    return series\n            else:\n                # If the expression is a raw feature(such as $close, $open)\n                return self.provider.expression(instrument, field, start_time, end_time, freq)",
  "def clear_cache(cache_path):\n        meta_path = cache_path + \".meta\"\n        for p in [cache_path, meta_path]:\n            p = Path(p)\n            if p.exists():\n                p.unlink()",
  "def gen_expression_cache(self, expression_data, cache_path, instrument, field, freq, last_update):\n        \"\"\"use bin file to save like feature-data.\"\"\"\n        # Make sure the cache runs right when the directory is deleted\n        # while running\n        meta = {\n            \"info\": {\"instrument\": instrument, \"field\": field, \"freq\": freq, \"last_update\": last_update},\n            \"meta\": {\"last_visit\": time.time(), \"visits\": 1},\n        }\n        self.logger.debug(f\"generating expression cache: {meta}\")\n        os.makedirs(self.expr_cache_path, exist_ok=True)\n        self.clear_cache(cache_path)\n        meta_path = cache_path + \".meta\"\n\n        with open(meta_path, \"wb\") as f:\n            pickle.dump(meta, f)\n        os.chmod(meta_path, stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)\n        df = expression_data.to_frame()\n\n        r = np.hstack([df.index[0], expression_data]).astype(\"<f\")\n        r.tofile(str(cache_path))",
  "def update(self, sid, cache_uri):\n        cp_cache_uri = os.path.join(self.expr_cache_path, sid, cache_uri)\n        if not self.check_cache_exists(cp_cache_uri):\n            self.logger.info(f\"The cache {cp_cache_uri} has corrupted. It will be removed\")\n            self.clear_cache(cp_cache_uri)\n            return 2\n\n        with CacheUtils.writer_lock(self.r, \"expression-%s\" % cache_uri):\n            with open(cp_cache_uri + \".meta\", \"rb\") as f:\n                d = pickle.load(f)\n            instrument = d[\"info\"][\"instrument\"]\n            field = d[\"info\"][\"field\"]\n            freq = d[\"info\"][\"freq\"]\n            last_update_time = d[\"info\"][\"last_update\"]\n\n            # get newest calendar\n            from .data import Cal, ExpressionD\n\n            whole_calendar = Cal.calendar(start_time=None, end_time=None, freq=freq)\n            # calendar since last updated.\n            new_calendar = Cal.calendar(start_time=last_update_time, end_time=None, freq=freq)\n\n            # get append data\n            if len(new_calendar) <= 1:\n                # Including last updated calendar, we only get 1 item.\n                # No future updating is needed.\n                return 1\n            else:\n                # get the data needed after the historical data are removed.\n                # The start index of new data\n                current_index = len(whole_calendar) - len(new_calendar) + 1\n\n                # The existing data length\n                size_bytes = os.path.getsize(cp_cache_uri)\n                ele_size = np.dtype(\"<f\").itemsize\n                assert size_bytes % ele_size == 0\n                ele_n = size_bytes // ele_size - 1\n\n                expr = ExpressionD.get_expression_instance(field)\n                lft_etd, rght_etd = expr.get_extended_window_size()\n                # The expression used the future data after rght_etd days.\n                # So the last rght_etd data should be removed.\n                # There are most `ele_n` period of data can be remove\n                remove_n = min(rght_etd, ele_n)\n                assert new_calendar[1] == whole_calendar[current_index]\n                data = self.provider.expression(\n                    instrument, field, whole_calendar[current_index - remove_n], new_calendar[-1], freq\n                )\n                with open(cp_cache_uri, \"ab\") as f:\n                    data = np.array(data).astype(\"<f\")\n                    # Remove the last bits\n                    f.truncate(size_bytes - ele_size * remove_n)\n                    f.write(data)\n                # update meta file\n                d[\"info\"][\"last_update\"] = str(new_calendar[-1])\n                with open(cp_cache_uri + \".meta\", \"wb\") as f:\n                    pickle.dump(d, f)\n        return 0",
  "def __init__(self, provider, **kwargs):\n        super(DiskDatasetCache, self).__init__(provider)\n        self.r = get_redis_connection()\n        self.remote = kwargs.get(\"remote\", False)\n        self.dtst_cache_path = os.path.join(C.get_data_path(), C.dataset_cache_dir_name)\n        os.makedirs(self.dtst_cache_path, exist_ok=True)",
  "def _uri(instruments, fields, start_time, end_time, freq, disk_cache=1, **kwargs):\n        return hash_args(*DatasetCache.normalize_uri_args(instruments, fields, freq), disk_cache)",
  "def check_cache_exists(cache_path):\n        for p in [cache_path, cache_path + \".index\", cache_path + \".meta\"]:\n            if not Path(p).exists():\n                return False\n        return True",
  "def read_data_from_cache(cls, cache_path, start_time, end_time, fields):\n        \"\"\"read_cache_from\n\n        This function can read data from the disk cache dataset\n\n        :param cache_path:\n        :param start_time:\n        :param end_time:\n        :param fields: The fields order of the dataset cache is sorted. So rearrange the columns to make it consistent.\n        :return:\n        \"\"\"\n\n        im = DiskDatasetCache.IndexManager(cache_path)\n        index_data = im.get_index(start_time, end_time)\n        if index_data.shape[0] > 0:\n            start, stop = (\n                index_data[\"start\"].iloc[0].item(),\n                index_data[\"end\"].iloc[-1].item(),\n            )\n        else:\n            start = stop = 0\n\n        with pd.HDFStore(cache_path, mode=\"r\") as store:\n            if \"/{}\".format(im.KEY) in store.keys():\n                df = store.select(key=im.KEY, start=start, stop=stop)\n                df = df.swaplevel(\"datetime\", \"instrument\").sort_index()\n                # read cache and need to replace not-space fields to field\n                df = cls.cache_to_origin_data(df, fields)\n\n            else:\n                df = pd.DataFrame(columns=fields)\n        return df",
  "def _dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=0):\n\n        if disk_cache == 0:\n            # In this case, data_set cache is configured but will not be used.\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n\n        _cache_uri = self._uri(\n            instruments=instruments, fields=fields, start_time=None, end_time=None, freq=freq, disk_cache=disk_cache\n        )\n\n        cache_path = os.path.join(self.dtst_cache_path, _cache_uri)\n\n        features = pd.DataFrame()\n        gen_flag = False\n\n        if self.check_cache_exists(cache_path):\n            if disk_cache == 1:\n                # use cache\n                with CacheUtils.reader_lock(self.r, \"dataset-%s\" % _cache_uri):\n                    CacheUtils.visit(cache_path)\n                    features = self.read_data_from_cache(cache_path, start_time, end_time, fields)\n            elif disk_cache == 2:\n                gen_flag = True\n        else:\n            gen_flag = True\n\n        if gen_flag:\n            # cache unavailable, generate the cache\n            with CacheUtils.writer_lock(self.r, \"dataset-%s\" % _cache_uri):\n                features = self.gen_dataset_cache(\n                    cache_path=cache_path, instruments=instruments, fields=fields, freq=freq\n                )\n            if not features.empty:\n                features = features.sort_index().loc(axis=0)[:, start_time:end_time]\n        return features",
  "def _dataset_uri(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=0):\n        if disk_cache == 0:\n            # In this case, server only checks the expression cache.\n            # The client will load the cache data by itself.\n            from .data import LocalDatasetProvider\n\n            LocalDatasetProvider.multi_cache_walker(instruments, fields, start_time, end_time, freq)\n            return \"\"\n\n        _cache_uri = self._uri(\n            instruments=instruments, fields=fields, start_time=None, end_time=None, freq=freq, disk_cache=disk_cache\n        )\n        cache_path = os.path.join(self.dtst_cache_path, _cache_uri)\n\n        if self.check_cache_exists(cache_path):\n            self.logger.debug(f\"The cache dataset has already existed {cache_path}. Return the uri directly\")\n            with CacheUtils.reader_lock(self.r, \"dataset-%s\" % _cache_uri):\n                CacheUtils.visit(cache_path)\n            return _cache_uri\n        else:\n            # cache unavailable, generate the cache\n            with CacheUtils.writer_lock(self.r, \"dataset-%s\" % _cache_uri):\n                self.gen_dataset_cache(cache_path=cache_path, instruments=instruments, fields=fields, freq=freq)\n            return _cache_uri",
  "class IndexManager:\n        \"\"\"\n        The lock is not considered in the class. Please consider the lock outside the code.\n        This class is the proxy of the disk data.\n        \"\"\"\n\n        KEY = \"df\"\n\n        def __init__(self, cache_path):\n            self.index_path = cache_path + \".index\"\n            self._data = None\n            self.logger = get_module_logger(self.__class__.__name__)\n\n        def get_index(self, start_time=None, end_time=None):\n            # TODO: fast read index from the disk.\n            if self._data is None:\n                self.sync_from_disk()\n            return self._data.loc[start_time:end_time].copy()\n\n        def sync_to_disk(self):\n            if self._data is None:\n                raise ValueError(\"No data to sync to disk.\")\n            self._data.sort_index(inplace=True)\n            self._data.to_hdf(self.index_path, key=self.KEY, mode=\"w\", format=\"table\")\n            # The index should be readable for all users\n            os.chmod(self.index_path, stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)\n\n        def sync_from_disk(self):\n            # The file will not be closed directly if we read_hdf from the disk directly\n            with pd.HDFStore(self.index_path, mode=\"r\") as store:\n                if \"/{}\".format(self.KEY) in store.keys():\n                    self._data = pd.read_hdf(store, key=self.KEY)\n                else:\n                    self._data = pd.DataFrame()\n\n        def update(self, data, sync=True):\n            self._data = data.astype(np.int32).copy()\n            if sync:\n                self.sync_to_disk()\n\n        def append_index(self, data, to_disk=True):\n            data = data.astype(np.int32).copy()\n            data.sort_index(inplace=True)\n            self._data = pd.concat([self._data, data])\n            if to_disk:\n                with pd.HDFStore(self.index_path) as store:\n                    store.append(self.KEY, data, append=True)\n\n        @staticmethod\n        def build_index_from_data(data, start_index=0):\n            if data.empty:\n                return pd.DataFrame()\n            line_data = data.iloc[:, 0].fillna(0).groupby(\"datetime\").count()\n            line_data.sort_index(inplace=True)\n            index_end = line_data.cumsum()\n            index_start = index_end.shift(1).fillna(0)\n\n            index_data = pd.DataFrame()\n            index_data[\"start\"] = index_start\n            index_data[\"end\"] = index_end\n            index_data += start_index\n            return index_data",
  "def clear_cache(cache_path):\n        meta_path = cache_path + \".meta\"\n        for p in [cache_path, meta_path, cache_path + \".index\", cache_path + \".data\"]:\n            p = Path(p)\n            if p.exists():\n                p.unlink()",
  "def gen_dataset_cache(self, cache_path, instruments, fields, freq):\n        \"\"\"gen_dataset_cache\n\n        .. note:: This function does not consider the cache read write lock. Please\n        Aquire the lock outside this function\n\n        The format the cache contains 3 parts(followed by typical filename).\n\n        - index : cache/d41366901e25de3ec47297f12e2ba11d.index\n\n            - The content of the file may be in following format(pandas.Series)\n\n                .. code-block:: python\n\n                                        start end\n                    1999-11-10 00:00:00     0   1\n                    1999-11-11 00:00:00     1   2\n                    1999-11-12 00:00:00     2   3\n                    ...\n\n            .. note:: The start is closed. The end is open!!!!!\n\n            - Each line contains two element <start_index, end_index> with a timestamp as its index.\n            - It indicates the `start_index`(included) and `end_index`(excluded) of the data for `timestamp`\n\n        - meta data: cache/d41366901e25de3ec47297f12e2ba11d.meta\n\n        - data     : cache/d41366901e25de3ec47297f12e2ba11d\n\n            - This is a hdf file sorted by datetime\n\n        :param cache_path:  The path to store the cache.\n        :param instruments:  The instruments to store the cache.\n        :param fields:  The fields to store the cache.\n        :param freq:  The freq to store the cache.\n\n        :return type pd.DataFrame; The fields of the returned DataFrame are consistent with the parameters of the function.\n        \"\"\"\n        # get calendar\n        from .data import Cal\n\n        _calendar = Cal.calendar(freq=freq)\n        self.logger.debug(f\"Generating dataset cache {cache_path}\")\n        # Make sure the cache runs right when the directory is deleted\n        # while running\n        os.makedirs(self.dtst_cache_path, exist_ok=True)\n        self.clear_cache(cache_path)\n\n        features = self.provider.dataset(instruments, fields, _calendar[0], _calendar[-1], freq)\n\n        if features.empty:\n            return features\n\n        # swap index and sorted\n        features = features.swaplevel(\"instrument\", \"datetime\").sort_index()\n\n        # write cache data\n        with pd.HDFStore(cache_path + \".data\") as store:\n            cache_to_orig_map = dict(zip(remove_fields_space(features.columns), features.columns))\n            orig_to_cache_map = dict(zip(features.columns, remove_fields_space(features.columns)))\n            cache_features = features[list(cache_to_orig_map.values())].rename(columns=orig_to_cache_map)\n            # cache columns\n            cache_columns = sorted(cache_features.columns)\n            cache_features = cache_features.loc[:, cache_columns]\n            cache_features = cache_features.loc[:, ~cache_features.columns.duplicated()]\n            store.append(DatasetCache.HDF_KEY, cache_features, append=False)\n        # write meta file\n        meta = {\n            \"info\": {\n                \"instruments\": instruments,\n                \"fields\": cache_columns,\n                \"freq\": freq,\n                \"last_update\": str(_calendar[-1]),  # The last_update to store the cache\n            },\n            \"meta\": {\"last_visit\": time.time(), \"visits\": 1},\n        }\n        with open(cache_path + \".meta\", \"wb\") as f:\n            pickle.dump(meta, f)\n        os.chmod(cache_path + \".meta\", stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)\n        # write index file\n        im = DiskDatasetCache.IndexManager(cache_path)\n        index_data = im.build_index_from_data(features)\n        im.update(index_data)\n\n        # rename the file after the cache has been generated\n        # this doesn't work well on windows, but our server won't use windows\n        # temporarily\n        os.replace(cache_path + \".data\", cache_path)\n        # the fields of the cached features are converted to the original fields\n        return features.swaplevel(\"datetime\", \"instrument\")",
  "def update(self, cache_uri):\n        cp_cache_uri = os.path.join(self.dtst_cache_path, cache_uri)\n\n        if not self.check_cache_exists(cp_cache_uri):\n            self.logger.info(f\"The cache {cp_cache_uri} has corrupted. It will be removed\")\n            self.clear_cache(cp_cache_uri)\n            return 2\n\n        im = DiskDatasetCache.IndexManager(cp_cache_uri)\n        with CacheUtils.writer_lock(self.r, \"dataset-%s\" % cache_uri):\n            with open(cp_cache_uri + \".meta\", \"rb\") as f:\n                d = pickle.load(f)\n            instruments = d[\"info\"][\"instruments\"]\n            fields = d[\"info\"][\"fields\"]\n            freq = d[\"info\"][\"freq\"]\n            last_update_time = d[\"info\"][\"last_update\"]\n            index_data = im.get_index()\n\n            self.logger.debug(\"Updating dataset: {}\".format(d))\n            from .data import Inst\n\n            if Inst.get_inst_type(instruments) == Inst.DICT:\n                self.logger.info(f\"The file {cache_uri} has dict cache. Skip updating\")\n                return 1\n\n            # get newest calendar\n            from .data import Cal\n\n            whole_calendar = Cal.calendar(start_time=None, end_time=None, freq=freq)\n            # The calendar since last updated\n            new_calendar = Cal.calendar(start_time=last_update_time, end_time=None, freq=freq)\n\n            # get append data\n            if len(new_calendar) <= 1:\n                # Including last updated calendar, we only get 1 item.\n                # No future updating is needed.\n                return 1\n            else:\n                # get the data needed after the historical data are removed.\n                # The start index of new data\n                current_index = len(whole_calendar) - len(new_calendar) + 1\n\n                # To avoid recursive import\n                from .data import ExpressionD\n\n                # The existing data length\n                lft_etd = rght_etd = 0\n                for field in fields:\n                    expr = ExpressionD.get_expression_instance(field)\n                    l, r = expr.get_extended_window_size()\n                    lft_etd = max(lft_etd, l)\n                    rght_etd = max(rght_etd, r)\n                # remove the period that should be updated.\n                if index_data.empty:\n                    # We don't have any data for such dataset. Nothing to remove\n                    rm_n_period = rm_lines = 0\n                else:\n                    rm_n_period = min(rght_etd, index_data.shape[0])\n                    rm_lines = (\n                        (index_data[\"end\"] - index_data[\"start\"])\n                        .loc[whole_calendar[current_index - rm_n_period] :]\n                        .sum()\n                        .item()\n                    )\n\n                data = self.provider.dataset(\n                    instruments, fields, whole_calendar[current_index - rm_n_period], new_calendar[-1], freq\n                )\n\n                if not data.empty:\n                    data.reset_index(inplace=True)\n                    data.set_index([\"datetime\", \"instrument\"], inplace=True)\n                    data.sort_index(inplace=True)\n                else:\n                    return 0  # No data to update cache\n\n                store = pd.HDFStore(cp_cache_uri)\n                # FIXME:\n                # Because the feature cache are stored as .bin file.\n                # So the series read from features are all float32.\n                # However, the first dataset cache is calulated based on the\n                # raw data. So the data type may be float64.\n                # Different data type will result in failure of appending data\n                if \"/{}\".format(DatasetCache.HDF_KEY) in store.keys():\n                    schema = store.select(DatasetCache.HDF_KEY, start=0, stop=0)\n                    for col, dtype in schema.dtypes.items():\n                        data[col] = data[col].astype(dtype)\n                if rm_lines > 0:\n                    store.remove(key=im.KEY, start=-rm_lines)\n                store.append(DatasetCache.HDF_KEY, data)\n                store.close()\n\n                # update index file\n                new_index_data = im.build_index_from_data(\n                    data.loc(axis=0)[whole_calendar[current_index] :, :],\n                    start_index=0 if index_data.empty else index_data[\"end\"].iloc[-1],\n                )\n                im.append_index(new_index_data)\n\n                # update meta file\n                d[\"info\"][\"last_update\"] = str(new_calendar[-1])\n                with open(cp_cache_uri + \".meta\", \"wb\") as f:\n                    pickle.dump(d, f)\n                return 0",
  "def __init__(self, provider):\n        super(SimpleDatasetCache, self).__init__(provider)\n        try:\n            self.local_cache_path = C[\"local_cache_path\"]\n        except KeyError as e:\n            self.logger.error(\"Assign a local_cache_path in config if you want to use this cache mechanism\")",
  "def _uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1, **kwargs):\n        instruments, fields, freq = self.normalize_uri_args(instruments, fields, freq)\n        local_cache_path = str(Path(self.local_cache_path).expanduser().resolve())\n        return hash_args(instruments, fields, start_time, end_time, freq, disk_cache, local_cache_path)",
  "def _dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=1):\n        if disk_cache == 0:\n            # In this case, data_set cache is configured but will not be used.\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n        os.makedirs(os.path.expanduser(self.local_cache_path), exist_ok=True)\n        cache_file = os.path.join(\n            self.local_cache_path, self._uri(instruments, fields, start_time, end_time, freq, disk_cache=disk_cache)\n        )\n        gen_flag = False\n\n        if os.path.exists(cache_file):\n            if disk_cache == 1:\n                # use cache\n                df = pd.read_pickle(cache_file)\n                return self.cache_to_origin_data(df, fields)\n            elif disk_cache == 2:\n                # replace cache\n                gen_flag = True\n        else:\n            gen_flag = True\n\n        if gen_flag:\n            data = self.provider.dataset(instruments, normalize_cache_fields(fields), start_time, end_time, freq)\n            data.to_pickle(cache_file)\n            return self.cache_to_origin_data(data, fields)",
  "def _uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1, **kwargs):\n        return hash_args(*self.normalize_uri_args(instruments, fields, freq), disk_cache)",
  "def dataset(self, instruments, fields, start_time=None, end_time=None, freq=\"day\", disk_cache=0):\n\n        if \"local\" in C.dataset_provider.lower():\n            # use LocalDatasetProvider\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq)\n\n        if disk_cache == 0:\n            # do not use data_set cache, load data from remote expression cache directly\n            return self.provider.dataset(instruments, fields, start_time, end_time, freq, disk_cache, return_uri=False)\n\n        # use ClientDatasetProvider\n        feature_uri = self._uri(instruments, fields, None, None, freq, disk_cache=disk_cache)\n        value, expire = MemCacheExpire.get_cache(H[\"f\"], feature_uri)\n        mnt_feature_uri = os.path.join(C.get_data_path(), C.dataset_cache_dir_name, feature_uri)\n        if value is None or expire or not os.path.exists(mnt_feature_uri):\n            df, uri = self.provider.dataset(\n                instruments, fields, start_time, end_time, freq, disk_cache, return_uri=True\n            )\n            # cache uri\n            MemCacheExpire.set_cache(H[\"f\"], uri, uri)\n            # cache DataFrame\n            # HZ['f'][uri] = df.copy()\n            get_module_logger(\"cache\").debug(f\"get feature from {C.dataset_provider}\")\n        else:\n            mnt_feature_uri = os.path.join(C.get_data_path(), C.dataset_cache_dir_name, feature_uri)\n            df = DiskDatasetCache.read_data_from_cache(mnt_feature_uri, start_time, end_time, fields)\n            get_module_logger(\"cache\").debug(\"get feature from uri cache\")\n\n        return df",
  "def calendar(self, start_time=None, end_time=None, freq=\"day\", future=False):\n        uri = self._uri(start_time, end_time, freq, future)\n        result, expire = MemCacheExpire.get_cache(H[\"c\"], uri)\n        if result is None or expire:\n\n            result = self.provider.calendar(start_time, end_time, freq, future)\n            MemCacheExpire.set_cache(H[\"c\"], uri, result)\n\n            get_module_logger(\"data\").debug(f\"get calendar from {C.calendar_provider}\")\n        else:\n            get_module_logger(\"data\").debug(\"get calendar from local cache\")\n\n        return result",
  "def __init__(self, cache_path):\n            self.index_path = cache_path + \".index\"\n            self._data = None\n            self.logger = get_module_logger(self.__class__.__name__)",
  "def get_index(self, start_time=None, end_time=None):\n            # TODO: fast read index from the disk.\n            if self._data is None:\n                self.sync_from_disk()\n            return self._data.loc[start_time:end_time].copy()",
  "def sync_to_disk(self):\n            if self._data is None:\n                raise ValueError(\"No data to sync to disk.\")\n            self._data.sort_index(inplace=True)\n            self._data.to_hdf(self.index_path, key=self.KEY, mode=\"w\", format=\"table\")\n            # The index should be readable for all users\n            os.chmod(self.index_path, stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)",
  "def sync_from_disk(self):\n            # The file will not be closed directly if we read_hdf from the disk directly\n            with pd.HDFStore(self.index_path, mode=\"r\") as store:\n                if \"/{}\".format(self.KEY) in store.keys():\n                    self._data = pd.read_hdf(store, key=self.KEY)\n                else:\n                    self._data = pd.DataFrame()",
  "def update(self, data, sync=True):\n            self._data = data.astype(np.int32).copy()\n            if sync:\n                self.sync_to_disk()",
  "def append_index(self, data, to_disk=True):\n            data = data.astype(np.int32).copy()\n            data.sort_index(inplace=True)\n            self._data = pd.concat([self._data, data])\n            if to_disk:\n                with pd.HDFStore(self.index_path) as store:\n                    store.append(self.KEY, data, append=True)",
  "def build_index_from_data(data, start_index=0):\n            if data.empty:\n                return pd.DataFrame()\n            line_data = data.iloc[:, 0].fillna(0).groupby(\"datetime\").count()\n            line_data.sort_index(inplace=True)\n            index_end = line_data.cumsum()\n            index_start = index_end.shift(1).fillna(0)\n\n            index_data = pd.DataFrame()\n            index_data[\"start\"] = index_start\n            index_data[\"end\"] = index_end\n            index_data += start_index\n            return index_data",
  "def get_level_index(df: pd.DataFrame, level=Union[str, int]) -> int:\n    \"\"\"\n\n    get the level index of `df` given `level`\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        data\n    level : Union[str, int]\n        index level\n\n    Returns\n    -------\n    int:\n        The level index in the multiple index\n    \"\"\"\n    if isinstance(level, str):\n        try:\n            return df.index.names.index(level)\n        except (AttributeError, ValueError):\n            # NOTE: If level index is not given in the data, the default level index will be ('datetime', 'instrument')\n            return (\"datetime\", \"instrument\").index(level)\n    elif isinstance(level, int):\n        return level\n    else:\n        raise NotImplementedError(f\"This type of input is not supported\")",
  "def fetch_df_by_index(\n    df: pd.DataFrame,\n    selector: Union[pd.Timestamp, slice, str, list],\n    level: Union[str, int],\n    fetch_orig=True,\n) -> pd.DataFrame:\n    \"\"\"\n    fetch data from `data` with `selector` and `level`\n\n    Parameters\n    ----------\n    selector : Union[pd.Timestamp, slice, str, list]\n        selector\n    level : Union[int, str]\n        the level to use the selector\n\n    Returns\n    -------\n    Data of the given index.\n    \"\"\"\n    # level = None -> use selector directly\n    if level == None:\n        return df.loc(axis=0)[selector]\n    # Try to get the right index\n    idx_slc = (selector, slice(None, None))\n    if get_level_index(df, level) == 1:\n        idx_slc = idx_slc[1], idx_slc[0]\n    if fetch_orig:\n        for slc in idx_slc:\n            if slc != slice(None, None):\n                return df.loc[\n                    pd.IndexSlice[idx_slc],\n                ]\n        else:\n            return df\n    else:\n        return df.loc[\n            pd.IndexSlice[idx_slc],\n        ]",
  "class DataHandler(Serializable):\n    \"\"\"\n    The steps to using a handler\n    1. initialized data handler  (call by `init`).\n    2. use the data.\n\n\n    The data handler try to maintain a handler with 2 level.\n    `datetime` & `instruments`.\n\n    Any order of the index level can be suported (The order will be implied in the data).\n    The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.\n\n    Example of the data:\n    The multi-index of the columns is optional.\n\n    .. code-block:: python\n\n                                feature                                                            label\n                                $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0\n        datetime   instrument\n        2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032\n                   SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042\n                   SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289\n\n    \"\"\"\n\n    def __init__(\n        self,\n        instruments=None,\n        start_time=None,\n        end_time=None,\n        data_loader: Tuple[dict, str, DataLoader] = None,\n        init_data=True,\n        fetch_orig=True,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        instruments :\n            The stock list to retrive.\n        start_time :\n            start_time of the original data.\n        end_time :\n            end_time of the original data.\n        data_loader : Tuple[dict, str, DataLoader]\n            data loader to load the data.\n        init_data :\n            intialize the original data in the constructor.\n        fetch_orig : bool\n            Return the original data instead of copy if possible.\n        \"\"\"\n        # Set logger\n        self.logger = get_module_logger(\"DataHandler\")\n\n        # Setup data loader\n        assert data_loader is not None  # to make start_time end_time could have None default value\n\n        # what data source to load data\n        self.data_loader = init_instance_by_config(\n            data_loader,\n            None if (isinstance(data_loader, dict) and \"module_path\" in data_loader) else data_loader_module,\n            accept_types=DataLoader,\n        )\n\n        # what data to be loaded from data source\n        # For IDE auto-completion.\n        self.instruments = instruments\n        self.start_time = start_time\n        self.end_time = end_time\n\n        self.fetch_orig = fetch_orig\n        if init_data:\n            with TimeInspector.logt(\"Init data\"):\n                self.init()\n        super().__init__()\n\n    def conf_data(self, **kwargs):\n        \"\"\"\n        configuration of data.\n        # what data to be loaded from data source\n\n        This method will be used when loading pickled handler from dataset.\n        The data will be initialized with different time range.\n\n        \"\"\"\n        attr_list = {\"instruments\", \"start_time\", \"end_time\"}\n        for k, v in kwargs.items():\n            if k in attr_list:\n                setattr(self, k, v)\n            else:\n                raise KeyError(\"Such config is not supported.\")\n\n    def init(self, enable_cache: bool = False):\n        \"\"\"\n        initialize the data.\n        In case of running intialization for multiple time, it will do nothing for the second time.\n\n        It is responsible for maintaining following variable\n        1) self._data\n\n        Parameters\n        ----------\n        enable_cache : bool\n            default value is false:\n\n            - if `enable_cache` == True:\n\n                the processed data will be saved on disk, and handler will load the cached data from the disk directly\n                when we call `init` next time\n        \"\"\"\n        # Setup data.\n        # _data may be with multiple column index level. The outer level indicates the feature set name\n        with TimeInspector.logt(\"Loading data\"):\n            self._data = self.data_loader.load(self.instruments, self.start_time, self.end_time)\n        # TODO: cache\n\n    CS_ALL = \"__all\"  # return all columns with single-level index column\n    CS_RAW = \"__raw\"  # return raw data with multi-level index column\n\n    def _fetch_df_by_col(self, df: pd.DataFrame, col_set: str) -> pd.DataFrame:\n        if not isinstance(df.columns, pd.MultiIndex) or col_set == self.CS_RAW:\n            return df\n        elif col_set == self.CS_ALL:\n            return df.droplevel(axis=1, level=0)\n        else:\n            return df.loc(axis=1)[col_set]\n\n    def fetch(\n        self,\n        selector: Union[pd.Timestamp, slice, str] = slice(None, None),\n        level: Union[str, int] = \"datetime\",\n        col_set: Union[str, List[str]] = CS_ALL,\n        squeeze: bool = False,\n    ) -> pd.DataFrame:\n        \"\"\"\n        fetch data from underlying data source\n\n        Parameters\n        ----------\n        selector : Union[pd.Timestamp, slice, str]\n            describe how to select data by index\n        level : Union[str, int]\n            which index level to select the data\n        col_set : Union[str, List[str]]\n\n            - if isinstance(col_set, str):\n\n                select a set of meaningful columns.(e.g. features, columns)\n\n                if cal_set == CS_RAW:\n                    the raw dataset will be returned.\n\n            - if isinstance(col_set, List[str]):\n\n                select several sets of meaningful columns, the returned data has multiple levels\n\n        squeeze : bool\n            whether squeeze columns and index\n\n        Returns\n        -------\n        pd.DataFrame.\n        \"\"\"\n        # Fetch column  first will be more friendly to SepDataFrame\n        df = self._fetch_df_by_col(self._data, col_set)\n        df = fetch_df_by_index(df, selector, level, fetch_orig=self.fetch_orig)\n        if squeeze:\n            # squeeze columns\n            df = df.squeeze()\n            # squeeze index\n            if isinstance(selector, (str, pd.Timestamp)):\n                df = df.reset_index(level=level, drop=True)\n        return df\n\n    def get_cols(self, col_set=CS_ALL) -> list:\n        \"\"\"\n        get the column names\n\n        Parameters\n        ----------\n        col_set : str\n            select a set of meaningful columns.(e.g. features, columns)\n\n        Returns\n        -------\n        list:\n            list of column names\n        \"\"\"\n        df = self._data.head()\n        df = self._fetch_df_by_col(df, col_set)\n        return df.columns.to_list()\n\n    def get_range_selector(self, cur_date: Union[pd.Timestamp, str], periods: int) -> slice:\n        \"\"\"\n        get range selector by number of periods\n\n        Args:\n            cur_date (pd.Timestamp or str): current date\n            periods (int): number of periods\n        \"\"\"\n        trading_dates = self._data.index.unique(level=\"datetime\")\n        cur_loc = trading_dates.get_loc(cur_date)\n        pre_loc = cur_loc - periods + 1\n        if pre_loc < 0:\n            warnings.warn(\"`periods` is too large. the first date will be returned.\")\n            pre_loc = 0\n        ref_date = trading_dates[pre_loc]\n        return slice(ref_date, cur_date)\n\n    def get_range_iterator(\n        self, periods: int, min_periods: Optional[int] = None, **kwargs\n    ) -> Iterator[Tuple[pd.Timestamp, pd.DataFrame]]:\n        \"\"\"\n        get a iterator of sliced data with given periods\n\n        Args:\n            periods (int): number of periods.\n            min_periods (int): minimum periods for sliced dataframe.\n            kwargs (dict): will be passed to `self.fetch`.\n        \"\"\"\n        trading_dates = self._data.index.unique(level=\"datetime\")\n        if min_periods is None:\n            min_periods = periods\n        for cur_date in trading_dates[min_periods:]:\n            selector = self.get_range_selector(cur_date, periods)\n            yield cur_date, self.fetch(selector, **kwargs)",
  "class DataHandlerLP(DataHandler):\n    \"\"\"\n    DataHandler with **(L)earnable (P)rocessor**\n    \"\"\"\n\n    # data key\n    DK_R = \"raw\"\n    DK_I = \"infer\"\n    DK_L = \"learn\"\n\n    # process type\n    PTYPE_I = \"independent\"\n    # - self._infer will be processed by infer_processors\n    # - self._learn will be processed by learn_processors\n    PTYPE_A = \"append\"\n    # - self._infer will be processed by infer_processors\n    # - self._learn will be processed by infer_processors + learn_processors\n    #   - (e.g. self._infer processed by learn_processors )\n\n    def __init__(\n        self,\n        instruments=None,\n        start_time=None,\n        end_time=None,\n        data_loader: Tuple[dict, str, DataLoader] = None,\n        infer_processors=[],\n        learn_processors=[],\n        process_type=PTYPE_A,\n        drop_raw=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        infer_processors : list\n            - list of <description info> of processors to generate data for inference\n\n            - example of <description info>:\n\n            .. code-block::\n\n                1) classname & kwargs:\n                    {\n                        \"class\": \"MinMaxNorm\",\n                        \"kwargs\": {\n                            \"fit_start_time\": \"20080101\",\n                            \"fit_end_time\": \"20121231\"\n                        }\n                    }\n                2) Only classname:\n                    \"DropnaFeature\"\n                3) object instance of Processor\n\n        learn_processors : list\n            similar to infer_processors, but for generating data for learning models\n\n        process_type: str\n            PTYPE_I = 'independent'\n\n            - self._infer will processed by infer_processors\n\n            - self._learn will be processed by learn_processors\n\n            PTYPE_A = 'append'\n\n            - self._infer will processed by infer_processors\n\n            - self._learn will be processed by infer_processors + learn_processors\n\n              - (e.g. self._infer processed by learn_processors )\n        drop_raw: bool\n            Whether to drop the raw data\n        \"\"\"\n\n        # Setup preprocessor\n        self.infer_processors = []  # for lint\n        self.learn_processors = []  # for lint\n        for pname in \"infer_processors\", \"learn_processors\":\n            for proc in locals()[pname]:\n                getattr(self, pname).append(\n                    init_instance_by_config(\n                        proc,\n                        None if (isinstance(proc, dict) and \"module_path\" in proc) else processor_module,\n                        accept_types=processor_module.Processor,\n                    )\n                )\n\n        self.process_type = process_type\n        self.drop_raw = drop_raw\n        super().__init__(instruments, start_time, end_time, data_loader, **kwargs)\n\n    def get_all_processors(self):\n        return self.infer_processors + self.learn_processors\n\n    def fit(self):\n        for proc in self.get_all_processors():\n            with TimeInspector.logt(f\"{proc.__class__.__name__}\"):\n                proc.fit(self._data)\n\n    def fit_process_data(self):\n        \"\"\"\n        fit and process data\n\n        The input of the `fit` will be the output of the previous processor\n        \"\"\"\n        self.process_data(with_fit=True)\n\n    def process_data(self, with_fit: bool = False):\n        \"\"\"\n        process_data data. Fun `processor.fit` if necessary\n\n        Parameters\n        ----------\n        with_fit : bool\n            The input of the `fit` will be the output of the previous processor\n        \"\"\"\n        # data for inference\n        _infer_df = self._data\n        if len(self.infer_processors) > 0 and not self.drop_raw:  # avoid modifying the original  data\n            _infer_df = _infer_df.copy()\n\n        for proc in self.infer_processors:\n            if not proc.is_for_infer():\n                raise TypeError(\"Only processors usable for inference can be used in `infer_processors` \")\n            with TimeInspector.logt(f\"{proc.__class__.__name__}\"):\n                if with_fit:\n                    proc.fit(_infer_df)\n                _infer_df = proc(_infer_df)\n        self._infer = _infer_df\n\n        # data for learning\n        if self.process_type == DataHandlerLP.PTYPE_I:\n            _learn_df = self._data\n        elif self.process_type == DataHandlerLP.PTYPE_A:\n            # based on `infer_df` and append the processor\n            _learn_df = _infer_df\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n\n        if len(self.learn_processors) > 0:  # avoid modifying the original  data\n            _learn_df = _learn_df.copy()\n        for proc in self.learn_processors:\n            with TimeInspector.logt(f\"{proc.__class__.__name__}\"):\n                if with_fit:\n                    proc.fit(_learn_df)\n                _learn_df = proc(_learn_df)\n        self._learn = _learn_df\n\n        if self.drop_raw:\n            del self._data\n\n    # init type\n    IT_FIT_SEQ = \"fit_seq\"  # the input of `fit` will be the output of the previous processor\n    IT_FIT_IND = \"fit_ind\"  # the input of `fit` will be the original df\n    IT_LS = \"load_state\"  # The state of the object has been load by pickle\n\n    def init(self, init_type: str = IT_FIT_SEQ, enable_cache: bool = False):\n        \"\"\"\n        Initialize the data of Qlib\n\n        Parameters\n        ----------\n        init_type : str\n            The type `IT_*` listed above.\n        enable_cache : bool\n            default value is false:\n\n            - if `enable_cache` == True:\n\n                the processed data will be saved on disk, and handler will load the cached data from the disk directly\n                when we call `init` next time\n        \"\"\"\n        # init raw data\n        super().init(enable_cache=enable_cache)\n\n        with TimeInspector.logt(\"fit & process data\"):\n            if init_type == DataHandlerLP.IT_FIT_IND:\n                self.fit()\n                self.process_data()\n            elif init_type == DataHandlerLP.IT_LS:\n                self.process_data()\n            elif init_type == DataHandlerLP.IT_FIT_SEQ:\n                self.fit_process_data()\n            else:\n                raise NotImplementedError(f\"This type of input is not supported\")\n\n        # TODO: Be able to cache handler data. Save the memory for data processing\n\n    def _get_df_by_key(self, data_key: str = DK_I) -> pd.DataFrame:\n        if data_key == self.DK_R and self.drop_raw:\n            raise AttributeError(\n                \"DataHandlerLP has not attribute _data, please set drop_raw = False if you want to use raw data\"\n            )\n        df = getattr(self, {self.DK_R: \"_data\", self.DK_I: \"_infer\", self.DK_L: \"_learn\"}[data_key])\n        return df\n\n    def fetch(\n        self,\n        selector: Union[pd.Timestamp, slice, str] = slice(None, None),\n        level: Union[str, int] = \"datetime\",\n        col_set=DataHandler.CS_ALL,\n        data_key: str = DK_I,\n    ) -> pd.DataFrame:\n        \"\"\"\n        fetch data from underlying data source\n\n        Parameters\n        ----------\n        selector : Union[pd.Timestamp, slice, str]\n            describe how to select data by index.\n        level : Union[str, int]\n            which index level to select the data.\n        col_set : str\n            select a set of meaningful columns.(e.g. features, columns).\n        data_key : str\n            the data to fetch:  DK_*.\n\n        Returns\n        -------\n        pd.DataFrame:\n        \"\"\"\n        df = self._get_df_by_key(data_key)\n        # Fetch column  first will be more friendly to SepDataFrame\n        df = self._fetch_df_by_col(df, col_set)\n        return fetch_df_by_index(df, selector, level, fetch_orig=self.fetch_orig)\n\n    def get_cols(self, col_set=DataHandler.CS_ALL, data_key: str = DK_I) -> list:\n        \"\"\"\n        get the column names\n\n        Parameters\n        ----------\n        col_set : str\n            select a set of meaningful columns.(e.g. features, columns).\n        data_key : str\n            the data to fetch:  DK_*.\n\n        Returns\n        -------\n        list:\n            list of column names\n        \"\"\"\n        df = self._get_df_by_key(data_key).head()\n        df = self._fetch_df_by_col(df, col_set)\n        return df.columns.to_list()",
  "def __init__(\n        self,\n        instruments=None,\n        start_time=None,\n        end_time=None,\n        data_loader: Tuple[dict, str, DataLoader] = None,\n        init_data=True,\n        fetch_orig=True,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        instruments :\n            The stock list to retrive.\n        start_time :\n            start_time of the original data.\n        end_time :\n            end_time of the original data.\n        data_loader : Tuple[dict, str, DataLoader]\n            data loader to load the data.\n        init_data :\n            intialize the original data in the constructor.\n        fetch_orig : bool\n            Return the original data instead of copy if possible.\n        \"\"\"\n        # Set logger\n        self.logger = get_module_logger(\"DataHandler\")\n\n        # Setup data loader\n        assert data_loader is not None  # to make start_time end_time could have None default value\n\n        # what data source to load data\n        self.data_loader = init_instance_by_config(\n            data_loader,\n            None if (isinstance(data_loader, dict) and \"module_path\" in data_loader) else data_loader_module,\n            accept_types=DataLoader,\n        )\n\n        # what data to be loaded from data source\n        # For IDE auto-completion.\n        self.instruments = instruments\n        self.start_time = start_time\n        self.end_time = end_time\n\n        self.fetch_orig = fetch_orig\n        if init_data:\n            with TimeInspector.logt(\"Init data\"):\n                self.init()\n        super().__init__()",
  "def conf_data(self, **kwargs):\n        \"\"\"\n        configuration of data.\n        # what data to be loaded from data source\n\n        This method will be used when loading pickled handler from dataset.\n        The data will be initialized with different time range.\n\n        \"\"\"\n        attr_list = {\"instruments\", \"start_time\", \"end_time\"}\n        for k, v in kwargs.items():\n            if k in attr_list:\n                setattr(self, k, v)\n            else:\n                raise KeyError(\"Such config is not supported.\")",
  "def init(self, enable_cache: bool = False):\n        \"\"\"\n        initialize the data.\n        In case of running intialization for multiple time, it will do nothing for the second time.\n\n        It is responsible for maintaining following variable\n        1) self._data\n\n        Parameters\n        ----------\n        enable_cache : bool\n            default value is false:\n\n            - if `enable_cache` == True:\n\n                the processed data will be saved on disk, and handler will load the cached data from the disk directly\n                when we call `init` next time\n        \"\"\"\n        # Setup data.\n        # _data may be with multiple column index level. The outer level indicates the feature set name\n        with TimeInspector.logt(\"Loading data\"):\n            self._data = self.data_loader.load(self.instruments, self.start_time, self.end_time)",
  "def _fetch_df_by_col(self, df: pd.DataFrame, col_set: str) -> pd.DataFrame:\n        if not isinstance(df.columns, pd.MultiIndex) or col_set == self.CS_RAW:\n            return df\n        elif col_set == self.CS_ALL:\n            return df.droplevel(axis=1, level=0)\n        else:\n            return df.loc(axis=1)[col_set]",
  "def fetch(\n        self,\n        selector: Union[pd.Timestamp, slice, str] = slice(None, None),\n        level: Union[str, int] = \"datetime\",\n        col_set: Union[str, List[str]] = CS_ALL,\n        squeeze: bool = False,\n    ) -> pd.DataFrame:\n        \"\"\"\n        fetch data from underlying data source\n\n        Parameters\n        ----------\n        selector : Union[pd.Timestamp, slice, str]\n            describe how to select data by index\n        level : Union[str, int]\n            which index level to select the data\n        col_set : Union[str, List[str]]\n\n            - if isinstance(col_set, str):\n\n                select a set of meaningful columns.(e.g. features, columns)\n\n                if cal_set == CS_RAW:\n                    the raw dataset will be returned.\n\n            - if isinstance(col_set, List[str]):\n\n                select several sets of meaningful columns, the returned data has multiple levels\n\n        squeeze : bool\n            whether squeeze columns and index\n\n        Returns\n        -------\n        pd.DataFrame.\n        \"\"\"\n        # Fetch column  first will be more friendly to SepDataFrame\n        df = self._fetch_df_by_col(self._data, col_set)\n        df = fetch_df_by_index(df, selector, level, fetch_orig=self.fetch_orig)\n        if squeeze:\n            # squeeze columns\n            df = df.squeeze()\n            # squeeze index\n            if isinstance(selector, (str, pd.Timestamp)):\n                df = df.reset_index(level=level, drop=True)\n        return df",
  "def get_cols(self, col_set=CS_ALL) -> list:\n        \"\"\"\n        get the column names\n\n        Parameters\n        ----------\n        col_set : str\n            select a set of meaningful columns.(e.g. features, columns)\n\n        Returns\n        -------\n        list:\n            list of column names\n        \"\"\"\n        df = self._data.head()\n        df = self._fetch_df_by_col(df, col_set)\n        return df.columns.to_list()",
  "def get_range_selector(self, cur_date: Union[pd.Timestamp, str], periods: int) -> slice:\n        \"\"\"\n        get range selector by number of periods\n\n        Args:\n            cur_date (pd.Timestamp or str): current date\n            periods (int): number of periods\n        \"\"\"\n        trading_dates = self._data.index.unique(level=\"datetime\")\n        cur_loc = trading_dates.get_loc(cur_date)\n        pre_loc = cur_loc - periods + 1\n        if pre_loc < 0:\n            warnings.warn(\"`periods` is too large. the first date will be returned.\")\n            pre_loc = 0\n        ref_date = trading_dates[pre_loc]\n        return slice(ref_date, cur_date)",
  "def get_range_iterator(\n        self, periods: int, min_periods: Optional[int] = None, **kwargs\n    ) -> Iterator[Tuple[pd.Timestamp, pd.DataFrame]]:\n        \"\"\"\n        get a iterator of sliced data with given periods\n\n        Args:\n            periods (int): number of periods.\n            min_periods (int): minimum periods for sliced dataframe.\n            kwargs (dict): will be passed to `self.fetch`.\n        \"\"\"\n        trading_dates = self._data.index.unique(level=\"datetime\")\n        if min_periods is None:\n            min_periods = periods\n        for cur_date in trading_dates[min_periods:]:\n            selector = self.get_range_selector(cur_date, periods)\n            yield cur_date, self.fetch(selector, **kwargs)",
  "def __init__(\n        self,\n        instruments=None,\n        start_time=None,\n        end_time=None,\n        data_loader: Tuple[dict, str, DataLoader] = None,\n        infer_processors=[],\n        learn_processors=[],\n        process_type=PTYPE_A,\n        drop_raw=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        infer_processors : list\n            - list of <description info> of processors to generate data for inference\n\n            - example of <description info>:\n\n            .. code-block::\n\n                1) classname & kwargs:\n                    {\n                        \"class\": \"MinMaxNorm\",\n                        \"kwargs\": {\n                            \"fit_start_time\": \"20080101\",\n                            \"fit_end_time\": \"20121231\"\n                        }\n                    }\n                2) Only classname:\n                    \"DropnaFeature\"\n                3) object instance of Processor\n\n        learn_processors : list\n            similar to infer_processors, but for generating data for learning models\n\n        process_type: str\n            PTYPE_I = 'independent'\n\n            - self._infer will processed by infer_processors\n\n            - self._learn will be processed by learn_processors\n\n            PTYPE_A = 'append'\n\n            - self._infer will processed by infer_processors\n\n            - self._learn will be processed by infer_processors + learn_processors\n\n              - (e.g. self._infer processed by learn_processors )\n        drop_raw: bool\n            Whether to drop the raw data\n        \"\"\"\n\n        # Setup preprocessor\n        self.infer_processors = []  # for lint\n        self.learn_processors = []  # for lint\n        for pname in \"infer_processors\", \"learn_processors\":\n            for proc in locals()[pname]:\n                getattr(self, pname).append(\n                    init_instance_by_config(\n                        proc,\n                        None if (isinstance(proc, dict) and \"module_path\" in proc) else processor_module,\n                        accept_types=processor_module.Processor,\n                    )\n                )\n\n        self.process_type = process_type\n        self.drop_raw = drop_raw\n        super().__init__(instruments, start_time, end_time, data_loader, **kwargs)",
  "def get_all_processors(self):\n        return self.infer_processors + self.learn_processors",
  "def fit(self):\n        for proc in self.get_all_processors():\n            with TimeInspector.logt(f\"{proc.__class__.__name__}\"):\n                proc.fit(self._data)",
  "def fit_process_data(self):\n        \"\"\"\n        fit and process data\n\n        The input of the `fit` will be the output of the previous processor\n        \"\"\"\n        self.process_data(with_fit=True)",
  "def process_data(self, with_fit: bool = False):\n        \"\"\"\n        process_data data. Fun `processor.fit` if necessary\n\n        Parameters\n        ----------\n        with_fit : bool\n            The input of the `fit` will be the output of the previous processor\n        \"\"\"\n        # data for inference\n        _infer_df = self._data\n        if len(self.infer_processors) > 0 and not self.drop_raw:  # avoid modifying the original  data\n            _infer_df = _infer_df.copy()\n\n        for proc in self.infer_processors:\n            if not proc.is_for_infer():\n                raise TypeError(\"Only processors usable for inference can be used in `infer_processors` \")\n            with TimeInspector.logt(f\"{proc.__class__.__name__}\"):\n                if with_fit:\n                    proc.fit(_infer_df)\n                _infer_df = proc(_infer_df)\n        self._infer = _infer_df\n\n        # data for learning\n        if self.process_type == DataHandlerLP.PTYPE_I:\n            _learn_df = self._data\n        elif self.process_type == DataHandlerLP.PTYPE_A:\n            # based on `infer_df` and append the processor\n            _learn_df = _infer_df\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n\n        if len(self.learn_processors) > 0:  # avoid modifying the original  data\n            _learn_df = _learn_df.copy()\n        for proc in self.learn_processors:\n            with TimeInspector.logt(f\"{proc.__class__.__name__}\"):\n                if with_fit:\n                    proc.fit(_learn_df)\n                _learn_df = proc(_learn_df)\n        self._learn = _learn_df\n\n        if self.drop_raw:\n            del self._data",
  "def init(self, init_type: str = IT_FIT_SEQ, enable_cache: bool = False):\n        \"\"\"\n        Initialize the data of Qlib\n\n        Parameters\n        ----------\n        init_type : str\n            The type `IT_*` listed above.\n        enable_cache : bool\n            default value is false:\n\n            - if `enable_cache` == True:\n\n                the processed data will be saved on disk, and handler will load the cached data from the disk directly\n                when we call `init` next time\n        \"\"\"\n        # init raw data\n        super().init(enable_cache=enable_cache)\n\n        with TimeInspector.logt(\"fit & process data\"):\n            if init_type == DataHandlerLP.IT_FIT_IND:\n                self.fit()\n                self.process_data()\n            elif init_type == DataHandlerLP.IT_LS:\n                self.process_data()\n            elif init_type == DataHandlerLP.IT_FIT_SEQ:\n                self.fit_process_data()\n            else:\n                raise NotImplementedError(f\"This type of input is not supported\")",
  "def _get_df_by_key(self, data_key: str = DK_I) -> pd.DataFrame:\n        if data_key == self.DK_R and self.drop_raw:\n            raise AttributeError(\n                \"DataHandlerLP has not attribute _data, please set drop_raw = False if you want to use raw data\"\n            )\n        df = getattr(self, {self.DK_R: \"_data\", self.DK_I: \"_infer\", self.DK_L: \"_learn\"}[data_key])\n        return df",
  "def fetch(\n        self,\n        selector: Union[pd.Timestamp, slice, str] = slice(None, None),\n        level: Union[str, int] = \"datetime\",\n        col_set=DataHandler.CS_ALL,\n        data_key: str = DK_I,\n    ) -> pd.DataFrame:\n        \"\"\"\n        fetch data from underlying data source\n\n        Parameters\n        ----------\n        selector : Union[pd.Timestamp, slice, str]\n            describe how to select data by index.\n        level : Union[str, int]\n            which index level to select the data.\n        col_set : str\n            select a set of meaningful columns.(e.g. features, columns).\n        data_key : str\n            the data to fetch:  DK_*.\n\n        Returns\n        -------\n        pd.DataFrame:\n        \"\"\"\n        df = self._get_df_by_key(data_key)\n        # Fetch column  first will be more friendly to SepDataFrame\n        df = self._fetch_df_by_col(df, col_set)\n        return fetch_df_by_index(df, selector, level, fetch_orig=self.fetch_orig)",
  "def get_cols(self, col_set=DataHandler.CS_ALL, data_key: str = DK_I) -> list:\n        \"\"\"\n        get the column names\n\n        Parameters\n        ----------\n        col_set : str\n            select a set of meaningful columns.(e.g. features, columns).\n        data_key : str\n            the data to fetch:  DK_*.\n\n        Returns\n        -------\n        list:\n            list of column names\n        \"\"\"\n        df = self._get_df_by_key(data_key).head()\n        df = self._fetch_df_by_col(df, col_set)\n        return df.columns.to_list()",
  "class DataLoader(abc.ABC):\n    \"\"\"\n    DataLoader is designed for loading raw data from original data source.\n    \"\"\"\n\n    @abc.abstractmethod\n    def load(self, instruments, start_time=None, end_time=None) -> pd.DataFrame:\n        \"\"\"\n        load the data as pd.DataFrame.\n\n        Example of the data (The multi-index of the columns is optional.):\n\n            .. code-block:: python\n\n                                        feature                                                             label\n                                        $close     $volume     Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0\n                datetime    instrument\n                2010-01-04  SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032\n                            SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042\n                            SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289\n\n\n        Parameters\n        ----------\n        instruments : str or dict\n            it can either be the market name or the config file of instruments generated by InstrumentProvider.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n\n        Returns\n        -------\n        pd.DataFrame:\n            data load from the under layer source\n        \"\"\"\n        pass",
  "class DLWParser(DataLoader):\n    \"\"\"\n    (D)ata(L)oader (W)ith (P)arser for features and names\n\n    Extracting this class so that QlibDataLoader and other dataloaders(such as QdbDataLoader) can share the fields.\n    \"\"\"\n\n    def __init__(self, config: Tuple[list, tuple, dict]):\n        \"\"\"\n        Parameters\n        ----------\n        config : Tuple[list, tuple, dict]\n            Config will be used to describe the fields and column names\n\n            .. code-block::\n\n                <config> := {\n                    \"group_name1\": <fields_info1>\n                    \"group_name2\": <fields_info2>\n                }\n                or\n                <config> := <fields_info>\n\n                <fields_info> := [\"expr\", ...] | ([\"expr\", ...], [\"col_name\", ...])\n                # NOTE: list or tuple will be treated as the things when parsing\n        \"\"\"\n        self.is_group = isinstance(config, dict)\n\n        if self.is_group:\n            self.fields = {grp: self._parse_fields_info(fields_info) for grp, fields_info in config.items()}\n        else:\n            self.fields = self._parse_fields_info(config)\n\n    def _parse_fields_info(self, fields_info: Tuple[list, tuple]) -> Tuple[list, list]:\n        if len(fields_info) == 0:\n            raise ValueError(\"The size of fields must be greater than 0\")\n\n        if not isinstance(fields_info, (list, tuple)):\n            raise TypeError(\"Unsupported type\")\n\n        if isinstance(fields_info[0], str):\n            exprs = names = fields_info\n        elif isinstance(fields_info[0], (list, tuple)):\n            exprs, names = fields_info\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n        return exprs, names\n\n    @abc.abstractmethod\n    def load_group_df(self, instruments, exprs: list, names: list, start_time=None, end_time=None) -> pd.DataFrame:\n        \"\"\"\n        load the dataframe for specific group\n\n        Parameters\n        ----------\n        instruments :\n            the instruments.\n        exprs : list\n            the expressions to describe the content of the data.\n        names : list\n            the name of the data.\n\n        Returns\n        -------\n        pd.DataFrame:\n            the queried dataframe.\n        \"\"\"\n        pass\n\n    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:\n        if self.is_group:\n            df = pd.concat(\n                {\n                    grp: self.load_group_df(instruments, exprs, names, start_time, end_time)\n                    for grp, (exprs, names) in self.fields.items()\n                },\n                axis=1,\n            )\n        else:\n            exprs, names = self.fields\n            df = self.load_group_df(instruments, exprs, names, start_time, end_time)\n        return df",
  "class QlibDataLoader(DLWParser):\n    \"\"\"Same as QlibDataLoader. The fields can be define by config\"\"\"\n\n    def __init__(self, config: Tuple[list, tuple, dict], filter_pipe=None, swap_level=True, freq=\"day\"):\n        \"\"\"\n        Parameters\n        ----------\n        config : Tuple[list, tuple, dict]\n            Please refer to the doc of DLWParser\n        filter_pipe :\n            Filter pipe for the instruments\n        swap_level :\n            Whether to swap level of MultiIndex\n        \"\"\"\n        if filter_pipe is not None:\n            assert isinstance(filter_pipe, list), \"The type of `filter_pipe` must be list.\"\n            filter_pipe = [\n                init_instance_by_config(fp, None if \"module_path\" in fp else filter_module, accept_types=BaseDFilter)\n                for fp in filter_pipe\n            ]\n\n        self.filter_pipe = filter_pipe\n        self.swap_level = swap_level\n        self.freq = freq\n        super().__init__(config)\n\n    def load_group_df(self, instruments, exprs: list, names: list, start_time=None, end_time=None) -> pd.DataFrame:\n        if instruments is None:\n            warnings.warn(\"`instruments` is not set, will load all stocks\")\n            instruments = \"all\"\n        if isinstance(instruments, str):\n            instruments = D.instruments(instruments, filter_pipe=self.filter_pipe)\n        elif self.filter_pipe is not None:\n            warnings.warn(\"`filter_pipe` is not None, but it will not be used with `instruments` as list\")\n\n        df = D.features(instruments, exprs, start_time, end_time, self.freq)\n        df.columns = names\n        if self.swap_level:\n            df = df.swaplevel().sort_index()  # NOTE: if swaplevel, return <datetime, instrument>\n        return df",
  "class StaticDataLoader(DataLoader):\n    \"\"\"\n    DataLoader that supports loading data from file or as provided.\n    \"\"\"\n\n    def __init__(self, config: dict, join=\"outer\"):\n        \"\"\"\n        Parameters\n        ----------\n        config : dict\n            {fields_group: <path or object>}\n        join : str\n            How to align different dataframes\n        \"\"\"\n        self.config = config\n        self.join = join\n        self._data = None\n\n    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:\n        self._maybe_load_raw_data()\n        if instruments is None:\n            df = self._data\n        else:\n            df = self._data.loc(axis=0)[:, instruments]\n        if start_time is None and end_time is None:\n            return df  # NOTE: avoid copy by loc\n        return df.loc[pd.Timestamp(start_time) : pd.Timestamp(end_time)]\n\n    def _maybe_load_raw_data(self):\n        if self._data is not None:\n            return\n        self._data = pd.concat(\n            {fields_group: load_dataset(path_or_obj) for fields_group, path_or_obj in self.config.items()},\n            axis=1,\n            join=self.join,\n        )\n        self._data.sort_index(inplace=True)",
  "def load(self, instruments, start_time=None, end_time=None) -> pd.DataFrame:\n        \"\"\"\n        load the data as pd.DataFrame.\n\n        Example of the data (The multi-index of the columns is optional.):\n\n            .. code-block:: python\n\n                                        feature                                                             label\n                                        $close     $volume     Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0\n                datetime    instrument\n                2010-01-04  SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032\n                            SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042\n                            SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289\n\n\n        Parameters\n        ----------\n        instruments : str or dict\n            it can either be the market name or the config file of instruments generated by InstrumentProvider.\n        start_time : str\n            start of the time range.\n        end_time : str\n            end of the time range.\n\n        Returns\n        -------\n        pd.DataFrame:\n            data load from the under layer source\n        \"\"\"\n        pass",
  "def __init__(self, config: Tuple[list, tuple, dict]):\n        \"\"\"\n        Parameters\n        ----------\n        config : Tuple[list, tuple, dict]\n            Config will be used to describe the fields and column names\n\n            .. code-block::\n\n                <config> := {\n                    \"group_name1\": <fields_info1>\n                    \"group_name2\": <fields_info2>\n                }\n                or\n                <config> := <fields_info>\n\n                <fields_info> := [\"expr\", ...] | ([\"expr\", ...], [\"col_name\", ...])\n                # NOTE: list or tuple will be treated as the things when parsing\n        \"\"\"\n        self.is_group = isinstance(config, dict)\n\n        if self.is_group:\n            self.fields = {grp: self._parse_fields_info(fields_info) for grp, fields_info in config.items()}\n        else:\n            self.fields = self._parse_fields_info(config)",
  "def _parse_fields_info(self, fields_info: Tuple[list, tuple]) -> Tuple[list, list]:\n        if len(fields_info) == 0:\n            raise ValueError(\"The size of fields must be greater than 0\")\n\n        if not isinstance(fields_info, (list, tuple)):\n            raise TypeError(\"Unsupported type\")\n\n        if isinstance(fields_info[0], str):\n            exprs = names = fields_info\n        elif isinstance(fields_info[0], (list, tuple)):\n            exprs, names = fields_info\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n        return exprs, names",
  "def load_group_df(self, instruments, exprs: list, names: list, start_time=None, end_time=None) -> pd.DataFrame:\n        \"\"\"\n        load the dataframe for specific group\n\n        Parameters\n        ----------\n        instruments :\n            the instruments.\n        exprs : list\n            the expressions to describe the content of the data.\n        names : list\n            the name of the data.\n\n        Returns\n        -------\n        pd.DataFrame:\n            the queried dataframe.\n        \"\"\"\n        pass",
  "def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:\n        if self.is_group:\n            df = pd.concat(\n                {\n                    grp: self.load_group_df(instruments, exprs, names, start_time, end_time)\n                    for grp, (exprs, names) in self.fields.items()\n                },\n                axis=1,\n            )\n        else:\n            exprs, names = self.fields\n            df = self.load_group_df(instruments, exprs, names, start_time, end_time)\n        return df",
  "def __init__(self, config: Tuple[list, tuple, dict], filter_pipe=None, swap_level=True, freq=\"day\"):\n        \"\"\"\n        Parameters\n        ----------\n        config : Tuple[list, tuple, dict]\n            Please refer to the doc of DLWParser\n        filter_pipe :\n            Filter pipe for the instruments\n        swap_level :\n            Whether to swap level of MultiIndex\n        \"\"\"\n        if filter_pipe is not None:\n            assert isinstance(filter_pipe, list), \"The type of `filter_pipe` must be list.\"\n            filter_pipe = [\n                init_instance_by_config(fp, None if \"module_path\" in fp else filter_module, accept_types=BaseDFilter)\n                for fp in filter_pipe\n            ]\n\n        self.filter_pipe = filter_pipe\n        self.swap_level = swap_level\n        self.freq = freq\n        super().__init__(config)",
  "def load_group_df(self, instruments, exprs: list, names: list, start_time=None, end_time=None) -> pd.DataFrame:\n        if instruments is None:\n            warnings.warn(\"`instruments` is not set, will load all stocks\")\n            instruments = \"all\"\n        if isinstance(instruments, str):\n            instruments = D.instruments(instruments, filter_pipe=self.filter_pipe)\n        elif self.filter_pipe is not None:\n            warnings.warn(\"`filter_pipe` is not None, but it will not be used with `instruments` as list\")\n\n        df = D.features(instruments, exprs, start_time, end_time, self.freq)\n        df.columns = names\n        if self.swap_level:\n            df = df.swaplevel().sort_index()  # NOTE: if swaplevel, return <datetime, instrument>\n        return df",
  "def __init__(self, config: dict, join=\"outer\"):\n        \"\"\"\n        Parameters\n        ----------\n        config : dict\n            {fields_group: <path or object>}\n        join : str\n            How to align different dataframes\n        \"\"\"\n        self.config = config\n        self.join = join\n        self._data = None",
  "def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:\n        self._maybe_load_raw_data()\n        if instruments is None:\n            df = self._data\n        else:\n            df = self._data.loc(axis=0)[:, instruments]\n        if start_time is None and end_time is None:\n            return df  # NOTE: avoid copy by loc\n        return df.loc[pd.Timestamp(start_time) : pd.Timestamp(end_time)]",
  "def _maybe_load_raw_data(self):\n        if self._data is not None:\n            return\n        self._data = pd.concat(\n            {fields_group: load_dataset(path_or_obj) for fields_group, path_or_obj in self.config.items()},\n            axis=1,\n            join=self.join,\n        )\n        self._data.sort_index(inplace=True)",
  "def get_group_columns(df: pd.DataFrame, group: str):\n    \"\"\"\n    get a group of columns from multi-index columns DataFrame\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        with multi of columns.\n    group : str\n        the name of the feature group, i.e. the first level value of the group index.\n    \"\"\"\n    if group is None:\n        return df.columns\n    else:\n        return df.columns[df.columns.get_loc(group)]",
  "class Processor(Serializable):\n    def fit(self, df: pd.DataFrame = None):\n        \"\"\"\n        learn data processing parameters\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            When we fit and process data with processor one by one. The fit function reiles on the output of previous\n            processor, i.e. `df`.\n\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __call__(self, df: pd.DataFrame):\n        \"\"\"\n        process the data\n\n        NOTE: **The processor could change the content of `df` inplace !!!!! **\n        User should keep a copy of data outside\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            The raw_df of handler or result from previous processor.\n        \"\"\"\n        pass\n\n    def is_for_infer(self) -> bool:\n        \"\"\"\n        Is this processor usable for inference\n        Some processors are not usable for inference.\n\n        Returns\n        -------\n        bool:\n            if it is usable for infenrece.\n        \"\"\"\n        return True",
  "class DropnaProcessor(Processor):\n    def __init__(self, fields_group=None):\n        self.fields_group = fields_group\n\n    def __call__(self, df):\n        return df.dropna(subset=get_group_columns(df, self.fields_group))",
  "class DropnaLabel(DropnaProcessor):\n    def __init__(self, fields_group=\"label\"):\n        super().__init__(fields_group=fields_group)\n\n    def is_for_infer(self) -> bool:\n        \"\"\"The samples are dropped according to label. So it is not usable for inference\"\"\"\n        return False",
  "class DropCol(Processor):\n    def __init__(self, col_list=[]):\n        self.col_list = col_list\n\n    def __call__(self, df):\n        if isinstance(df.columns, pd.MultiIndex):\n            mask = df.columns.get_level_values(-1).isin(self.col_list)\n        else:\n            mask = df.columns.isin(self.col_list)\n        return df.loc[:, ~mask]",
  "class FilterCol(Processor):\n    def __init__(self, fields_group=\"feature\", col_list=[]):\n        self.fields_group = fields_group\n        self.col_list = col_list\n\n    def __call__(self, df):\n\n        cols = get_group_columns(df, self.fields_group)\n        all_cols = df.columns\n        diff_cols = np.setdiff1d(all_cols.get_level_values(-1), cols.get_level_values(-1))\n        self.col_list = np.union1d(diff_cols, self.col_list)\n        mask = df.columns.get_level_values(-1).isin(self.col_list)\n        return df.loc[:, mask]",
  "class TanhProcess(Processor):\n    \"\"\" Use tanh to process noise data\"\"\"\n\n    def __call__(self, df):\n        def tanh_denoise(data):\n            mask = data.columns.get_level_values(1).str.contains(\"LABEL\")\n            col = df.columns[~mask]\n            data[col] = data[col] - 1\n            data[col] = np.tanh(data[col])\n\n            return data\n\n        return tanh_denoise(df)",
  "class ProcessInf(Processor):\n    \"\"\"Process infinity  \"\"\"\n\n    def __call__(self, df):\n        def replace_inf(data):\n            def process_inf(df):\n                for col in df.columns:\n                    # FIXME: Such behavior is very weird\n                    df[col] = df[col].replace([np.inf, -np.inf], df[col][~np.isinf(df[col])].mean())\n                return df\n\n            data = datetime_groupby_apply(data, process_inf)\n            data.sort_index(inplace=True)\n            return data\n\n        return replace_inf(df)",
  "class Fillna(Processor):\n    \"\"\"Process NaN\"\"\"\n\n    def __init__(self, fields_group=None, fill_value=0):\n        self.fields_group = fields_group\n        self.fill_value = fill_value\n\n    def __call__(self, df):\n        if self.fields_group is None:\n            df.fillna(self.fill_value, inplace=True)\n        else:\n            cols = get_group_columns(df, self.fields_group)\n            df.fillna({col: self.fill_value for col in cols}, inplace=True)\n        return df",
  "class MinMaxNorm(Processor):\n    def __init__(self, fit_start_time, fit_end_time, fields_group=None):\n        self.fit_start_time = fit_start_time\n        self.fit_end_time = fit_end_time\n        self.fields_group = fields_group\n\n    def fit(self, df):\n        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level=\"datetime\")\n        cols = get_group_columns(df, self.fields_group)\n        self.min_val = np.nanmin(df[cols].values, axis=0)\n        self.max_val = np.nanmax(df[cols].values, axis=0)\n        self.ignore = self.min_val == self.max_val\n        self.cols = cols\n\n    def __call__(self, df):\n        def normalize(x, min_val=self.min_val, max_val=self.max_val, ignore=self.ignore):\n            if (~ignore).all():\n                return (x - min_val) / (max_val - min_val)\n            for i in range(ignore.size):\n                if not ignore[i]:\n                    x[i] = (x[i] - min_val) / (max_val - min_val)\n            return x\n\n        df.loc(axis=1)[self.cols] = normalize(df[self.cols].values)\n        return df",
  "class ZScoreNorm(Processor):\n    \"\"\"ZScore Normalization\"\"\"\n\n    def __init__(self, fit_start_time, fit_end_time, fields_group=None):\n        self.fit_start_time = fit_start_time\n        self.fit_end_time = fit_end_time\n        self.fields_group = fields_group\n\n    def fit(self, df):\n        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level=\"datetime\")\n        cols = get_group_columns(df, self.fields_group)\n        self.mean_train = np.nanmean(df[cols].values, axis=0)\n        self.std_train = np.nanstd(df[cols].values, axis=0)\n        self.ignore = self.std_train == 0\n        self.cols = cols\n\n    def __call__(self, df):\n        def normalize(x, mean_train=self.mean_train, std_train=self.std_train, ignore=self.ignore):\n            if (~ignore).all():\n                return (x - mean_train) / std_train\n            for i in range(ignore.size):\n                if not ignore[i]:\n                    x[i] = (x[i] - mean_train) / std_train\n            return x\n\n        df.loc(axis=1)[self.cols] = normalize(df[self.cols].values)\n        return df",
  "class RobustZScoreNorm(Processor):\n    \"\"\"Robust ZScore Normalization\n\n    Use robust statistics for Z-Score normalization:\n        mean(x) = median(x)\n        std(x) = MAD(x) * 1.4826\n\n    Reference:\n        https://en.wikipedia.org/wiki/Median_absolute_deviation.\n    \"\"\"\n\n    def __init__(self, fit_start_time, fit_end_time, fields_group=None, clip_outlier=True):\n        self.fit_start_time = fit_start_time\n        self.fit_end_time = fit_end_time\n        self.fields_group = fields_group\n        self.clip_outlier = clip_outlier\n\n    def fit(self, df):\n        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level=\"datetime\")\n        self.cols = get_group_columns(df, self.fields_group)\n        X = df[self.cols].values\n        self.mean_train = np.nanmedian(X, axis=0)\n        self.std_train = np.nanmedian(np.abs(X - self.mean_train), axis=0)\n        self.std_train += EPS\n        self.std_train *= 1.4826\n\n    def __call__(self, df):\n        X = df[self.cols]\n        X -= self.mean_train\n        X /= self.std_train\n        df[self.cols] = X\n        if self.clip_outlier:\n            df.clip(-3, 3, inplace=True)\n        return df",
  "class CSZScoreNorm(Processor):\n    \"\"\"Cross Sectional ZScore Normalization\"\"\"\n\n    def __init__(self, fields_group=None):\n        self.fields_group = fields_group\n\n    def __call__(self, df):\n        # try not modify original dataframe\n        cols = get_group_columns(df, self.fields_group)\n        df[cols] = df[cols].groupby(\"datetime\").apply(lambda x: (x - x.mean()).div(x.std()))\n\n        return df",
  "class CSRankNorm(Processor):\n    \"\"\"Cross Sectional Rank Normalization\"\"\"\n\n    def __init__(self, fields_group=None):\n        self.fields_group = fields_group\n\n    def __call__(self, df):\n        # try not modify original dataframe\n        cols = get_group_columns(df, self.fields_group)\n        t = df[cols].groupby(\"datetime\").rank(pct=True)\n        t -= 0.5\n        t *= 3.46  # NOTE: towards unit std\n        df[cols] = t\n        return df",
  "class CSZFillna(Processor):\n    \"\"\"Cross Sectional Fill Nan\"\"\"\n\n    def __init__(self, fields_group=None):\n        self.fields_group = fields_group\n\n    def __call__(self, df):\n        cols = get_group_columns(df, self.fields_group)\n        df[cols] = df[cols].groupby(\"datetime\").apply(lambda x: x.fillna(x.mean()))\n        return df",
  "def fit(self, df: pd.DataFrame = None):\n        \"\"\"\n        learn data processing parameters\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            When we fit and process data with processor one by one. The fit function reiles on the output of previous\n            processor, i.e. `df`.\n\n        \"\"\"\n        pass",
  "def __call__(self, df: pd.DataFrame):\n        \"\"\"\n        process the data\n\n        NOTE: **The processor could change the content of `df` inplace !!!!! **\n        User should keep a copy of data outside\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            The raw_df of handler or result from previous processor.\n        \"\"\"\n        pass",
  "def is_for_infer(self) -> bool:\n        \"\"\"\n        Is this processor usable for inference\n        Some processors are not usable for inference.\n\n        Returns\n        -------\n        bool:\n            if it is usable for infenrece.\n        \"\"\"\n        return True",
  "def __init__(self, fields_group=None):\n        self.fields_group = fields_group",
  "def __call__(self, df):\n        return df.dropna(subset=get_group_columns(df, self.fields_group))",
  "def __init__(self, fields_group=\"label\"):\n        super().__init__(fields_group=fields_group)",
  "def is_for_infer(self) -> bool:\n        \"\"\"The samples are dropped according to label. So it is not usable for inference\"\"\"\n        return False",
  "def __init__(self, col_list=[]):\n        self.col_list = col_list",
  "def __call__(self, df):\n        if isinstance(df.columns, pd.MultiIndex):\n            mask = df.columns.get_level_values(-1).isin(self.col_list)\n        else:\n            mask = df.columns.isin(self.col_list)\n        return df.loc[:, ~mask]",
  "def __init__(self, fields_group=\"feature\", col_list=[]):\n        self.fields_group = fields_group\n        self.col_list = col_list",
  "def __call__(self, df):\n\n        cols = get_group_columns(df, self.fields_group)\n        all_cols = df.columns\n        diff_cols = np.setdiff1d(all_cols.get_level_values(-1), cols.get_level_values(-1))\n        self.col_list = np.union1d(diff_cols, self.col_list)\n        mask = df.columns.get_level_values(-1).isin(self.col_list)\n        return df.loc[:, mask]",
  "def __call__(self, df):\n        def tanh_denoise(data):\n            mask = data.columns.get_level_values(1).str.contains(\"LABEL\")\n            col = df.columns[~mask]\n            data[col] = data[col] - 1\n            data[col] = np.tanh(data[col])\n\n            return data\n\n        return tanh_denoise(df)",
  "def __call__(self, df):\n        def replace_inf(data):\n            def process_inf(df):\n                for col in df.columns:\n                    # FIXME: Such behavior is very weird\n                    df[col] = df[col].replace([np.inf, -np.inf], df[col][~np.isinf(df[col])].mean())\n                return df\n\n            data = datetime_groupby_apply(data, process_inf)\n            data.sort_index(inplace=True)\n            return data\n\n        return replace_inf(df)",
  "def __init__(self, fields_group=None, fill_value=0):\n        self.fields_group = fields_group\n        self.fill_value = fill_value",
  "def __call__(self, df):\n        if self.fields_group is None:\n            df.fillna(self.fill_value, inplace=True)\n        else:\n            cols = get_group_columns(df, self.fields_group)\n            df.fillna({col: self.fill_value for col in cols}, inplace=True)\n        return df",
  "def __init__(self, fit_start_time, fit_end_time, fields_group=None):\n        self.fit_start_time = fit_start_time\n        self.fit_end_time = fit_end_time\n        self.fields_group = fields_group",
  "def fit(self, df):\n        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level=\"datetime\")\n        cols = get_group_columns(df, self.fields_group)\n        self.min_val = np.nanmin(df[cols].values, axis=0)\n        self.max_val = np.nanmax(df[cols].values, axis=0)\n        self.ignore = self.min_val == self.max_val\n        self.cols = cols",
  "def __call__(self, df):\n        def normalize(x, min_val=self.min_val, max_val=self.max_val, ignore=self.ignore):\n            if (~ignore).all():\n                return (x - min_val) / (max_val - min_val)\n            for i in range(ignore.size):\n                if not ignore[i]:\n                    x[i] = (x[i] - min_val) / (max_val - min_val)\n            return x\n\n        df.loc(axis=1)[self.cols] = normalize(df[self.cols].values)\n        return df",
  "def __init__(self, fit_start_time, fit_end_time, fields_group=None):\n        self.fit_start_time = fit_start_time\n        self.fit_end_time = fit_end_time\n        self.fields_group = fields_group",
  "def fit(self, df):\n        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level=\"datetime\")\n        cols = get_group_columns(df, self.fields_group)\n        self.mean_train = np.nanmean(df[cols].values, axis=0)\n        self.std_train = np.nanstd(df[cols].values, axis=0)\n        self.ignore = self.std_train == 0\n        self.cols = cols",
  "def __call__(self, df):\n        def normalize(x, mean_train=self.mean_train, std_train=self.std_train, ignore=self.ignore):\n            if (~ignore).all():\n                return (x - mean_train) / std_train\n            for i in range(ignore.size):\n                if not ignore[i]:\n                    x[i] = (x[i] - mean_train) / std_train\n            return x\n\n        df.loc(axis=1)[self.cols] = normalize(df[self.cols].values)\n        return df",
  "def __init__(self, fit_start_time, fit_end_time, fields_group=None, clip_outlier=True):\n        self.fit_start_time = fit_start_time\n        self.fit_end_time = fit_end_time\n        self.fields_group = fields_group\n        self.clip_outlier = clip_outlier",
  "def fit(self, df):\n        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level=\"datetime\")\n        self.cols = get_group_columns(df, self.fields_group)\n        X = df[self.cols].values\n        self.mean_train = np.nanmedian(X, axis=0)\n        self.std_train = np.nanmedian(np.abs(X - self.mean_train), axis=0)\n        self.std_train += EPS\n        self.std_train *= 1.4826",
  "def __call__(self, df):\n        X = df[self.cols]\n        X -= self.mean_train\n        X /= self.std_train\n        df[self.cols] = X\n        if self.clip_outlier:\n            df.clip(-3, 3, inplace=True)\n        return df",
  "def __init__(self, fields_group=None):\n        self.fields_group = fields_group",
  "def __call__(self, df):\n        # try not modify original dataframe\n        cols = get_group_columns(df, self.fields_group)\n        df[cols] = df[cols].groupby(\"datetime\").apply(lambda x: (x - x.mean()).div(x.std()))\n\n        return df",
  "def __init__(self, fields_group=None):\n        self.fields_group = fields_group",
  "def __call__(self, df):\n        # try not modify original dataframe\n        cols = get_group_columns(df, self.fields_group)\n        t = df[cols].groupby(\"datetime\").rank(pct=True)\n        t -= 0.5\n        t *= 3.46  # NOTE: towards unit std\n        df[cols] = t\n        return df",
  "def __init__(self, fields_group=None):\n        self.fields_group = fields_group",
  "def __call__(self, df):\n        cols = get_group_columns(df, self.fields_group)\n        df[cols] = df[cols].groupby(\"datetime\").apply(lambda x: x.fillna(x.mean()))\n        return df",
  "def tanh_denoise(data):\n            mask = data.columns.get_level_values(1).str.contains(\"LABEL\")\n            col = df.columns[~mask]\n            data[col] = data[col] - 1\n            data[col] = np.tanh(data[col])\n\n            return data",
  "def replace_inf(data):\n            def process_inf(df):\n                for col in df.columns:\n                    # FIXME: Such behavior is very weird\n                    df[col] = df[col].replace([np.inf, -np.inf], df[col][~np.isinf(df[col])].mean())\n                return df\n\n            data = datetime_groupby_apply(data, process_inf)\n            data.sort_index(inplace=True)\n            return data",
  "def normalize(x, min_val=self.min_val, max_val=self.max_val, ignore=self.ignore):\n            if (~ignore).all():\n                return (x - min_val) / (max_val - min_val)\n            for i in range(ignore.size):\n                if not ignore[i]:\n                    x[i] = (x[i] - min_val) / (max_val - min_val)\n            return x",
  "def normalize(x, mean_train=self.mean_train, std_train=self.std_train, ignore=self.ignore):\n            if (~ignore).all():\n                return (x - mean_train) / std_train\n            for i in range(ignore.size):\n                if not ignore[i]:\n                    x[i] = (x[i] - mean_train) / std_train\n            return x",
  "def process_inf(df):\n                for col in df.columns:\n                    # FIXME: Such behavior is very weird\n                    df[col] = df[col].replace([np.inf, -np.inf], df[col][~np.isinf(df[col])].mean())\n                return df",
  "class Dataset(Serializable):\n    \"\"\"\n    Preparing data for model training and inferencing.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        init is designed to finish following steps:\n\n        - setup data\n            - The data related attributes' names should start with '_' so that it will not be saved on disk when serializing.\n\n        - initialize the state of the dataset(info to prepare the data)\n            - The name of essential state for preparing data should not start with '_' so that it could be serialized on disk when serializing.\n\n        The data could specify the info to caculate the essential data for preparation\n        \"\"\"\n        self.setup_data(*args, **kwargs)\n        super().__init__()\n\n    def setup_data(self, *args, **kwargs):\n        \"\"\"\n        Setup the data.\n\n        We split the setup_data function for following situation:\n\n        - User have a Dataset object with learned status on disk.\n\n        - User load the Dataset object from the disk(Note the init function is skiped).\n\n        - User call `setup_data` to load new data.\n\n        - User prepare data for model based on previous status.\n        \"\"\"\n        pass\n\n    def prepare(self, *args, **kwargs) -> object:\n        \"\"\"\n        The type of dataset depends on the model. (It could be pd.DataFrame, pytorch.DataLoader, etc.)\n        The parameters should specify the scope for the prepared data\n        The method should:\n        - process the data\n\n        - return the processed data\n\n        Returns\n        -------\n        object:\n            return the object\n        \"\"\"\n        pass",
  "class DatasetH(Dataset):\n    \"\"\"\n    Dataset with Data(H)andler\n\n    User should try to put the data preprocessing functions into handler.\n    Only following data processing functions should be placed in Dataset:\n\n    - The processing is related to specific model.\n\n    - The processing is related to data split.\n    \"\"\"\n\n    def init(self, handler_kwargs: dict = None, segment_kwargs: dict = None):\n        \"\"\"\n        Initialize the DatasetH\n\n        Parameters\n        ----------\n        handler_kwargs : dict\n            Config of DataHanlder, which could include the following arguments:\n\n            - arguments of DataHandler.conf_data, such as 'instruments', 'start_time' and 'end_time'.\n\n            - arguments of DataHandler.init, such as 'enable_cache', etc.\n\n        segment_kwargs : dict\n            Config of segments which is same as 'segments' in DatasetH.setup_data\n\n        \"\"\"\n        if handler_kwargs:\n            if not isinstance(handler_kwargs, dict):\n                raise TypeError(f\"param handler_kwargs must be type dict, not {type(handler_kwargs)}\")\n            kwargs_init = {}\n            kwargs_conf_data = {}\n            conf_data_arg = {\"instruments\", \"start_time\", \"end_time\"}\n            for k, v in handler_kwargs.items():\n                if k in conf_data_arg:\n                    kwargs_conf_data.update({k: v})\n                else:\n                    kwargs_init.update({k: v})\n\n            self.handler.conf_data(**kwargs_conf_data)\n            self.handler.init(**kwargs_init)\n\n        if segment_kwargs:\n            if not isinstance(segment_kwargs, dict):\n                raise TypeError(f\"param handler_kwargs must be type dict, not {type(segment_kwargs)}\")\n            self.segments = segment_kwargs.copy()\n\n    def setup_data(self, handler: Union[Dict, DataHandler], segments: Dict[Text, Tuple]):\n        \"\"\"\n        Setup the underlying data.\n\n        Parameters\n        ----------\n        handler : Union[dict, DataHandler]\n            handler could be:\n\n            - insntance of `DataHandler`\n\n            - config of `DataHandler`.  Please refer to `DataHandler`\n\n        segments : dict\n            Describe the options to segment the data.\n            Here are some examples:\n\n            .. code-block::\n\n                1) 'segments': {\n                        'train': (\"2008-01-01\", \"2014-12-31\"),\n                        'valid': (\"2017-01-01\", \"2020-08-01\",),\n                        'test': (\"2015-01-01\", \"2016-12-31\",),\n                    }\n                2) 'segments': {\n                        'insample': (\"2008-01-01\", \"2014-12-31\"),\n                        'outsample': (\"2017-01-01\", \"2020-08-01\",),\n                    }\n        \"\"\"\n        self.handler = init_instance_by_config(handler, accept_types=DataHandler)\n        self.segments = segments.copy()\n\n    def __repr__(self):\n        return \"{name}(handler={handler}, segments={segments})\".format(\n            name=self.__class__.__name__, handler=self.handler, segments=self.segments\n        )\n\n    def _prepare_seg(self, slc: slice, **kwargs):\n        \"\"\"\n        Give a slice, retrieve the according data\n\n        Parameters\n        ----------\n        slc : slice\n        \"\"\"\n        return self.handler.fetch(slc, **kwargs)\n\n    def prepare(\n        self,\n        segments: Union[List[Text], Tuple[Text], Text, slice],\n        col_set=DataHandler.CS_ALL,\n        data_key=DataHandlerLP.DK_I,\n        **kwargs,\n    ) -> Union[List[pd.DataFrame], pd.DataFrame]:\n        \"\"\"\n        Prepare the data for learning and inference.\n\n        Parameters\n        ----------\n        segments : Union[List[Text], Tuple[Text], Text, slice]\n            Describe the scope of the data to be prepared\n            Here are some examples:\n\n            - 'train'\n\n            - ['train', 'valid']\n\n        col_set : str\n            The col_set will be passed to self.handler when fetching data.\n        data_key : str\n            The data to fetch:  DK_*\n            Default is DK_I, which indicate fetching data for **inference**.\n\n        Returns\n        -------\n        Union[List[pd.DataFrame], pd.DataFrame]:\n\n        Raises\n        ------\n        NotImplementedError:\n        \"\"\"\n        logger = get_module_logger(\"DatasetH\")\n        fetch_kwargs = {\"col_set\": col_set}\n        fetch_kwargs.update(kwargs)\n        if \"data_key\" in getfullargspec(self.handler.fetch).args:\n            fetch_kwargs[\"data_key\"] = data_key\n        else:\n            logger.info(f\"data_key[{data_key}] is ignored.\")\n\n        # Handle all kinds of segments format\n        if isinstance(segments, (list, tuple)):\n            return [self._prepare_seg(slice(*self.segments[seg]), **fetch_kwargs) for seg in segments]\n        elif isinstance(segments, str):\n            return self._prepare_seg(slice(*self.segments[segments]), **fetch_kwargs)\n        elif isinstance(segments, slice):\n            return self._prepare_seg(segments, **fetch_kwargs)\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")",
  "class TSDataSampler:\n    \"\"\"\n    (T)ime-(S)eries DataSampler\n    This is the result of TSDatasetH\n\n    It works like `torch.data.utils.Dataset`, it provides a very convient interface for constructing time-series\n    dataset based on tabular data.\n\n    If user have further requirements for processing data, user could process them based on `TSDataSampler` or create\n    more powerful subclasses.\n\n    Known Issues:\n    - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result\n      in a different data type\n\n    \"\"\"\n\n    def __init__(self, data: pd.DataFrame, start, end, step_len: int, fillna_type: str = \"none\"):\n        \"\"\"\n        Build a dataset which looks like torch.data.utils.Dataset.\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The raw tabular data\n        start :\n            The indexable start time\n        end :\n            The indexable end time\n        step_len : int\n            The length of the time-series step\n        fillna_type : int\n            How will qlib handle the sample if there is on sample in a specific date.\n            none:\n                fill with np.nan\n            ffill:\n                ffill with previous sample\n            ffill+bfill:\n                ffill with previous samples first and fill with later samples second\n        \"\"\"\n        self.start = start\n        self.end = end\n        self.step_len = step_len\n        self.fillna_type = fillna_type\n        assert get_level_index(data, \"datetime\") == 0\n        self.data = lazy_sort_index(data)\n        self.data_arr = np.array(self.data)  # Get index from numpy.array will much faster than DataFrame.values!\n        # NOTE: append last line with full NaN for better performance in `__getitem__`\n        self.data_arr = np.append(self.data_arr, np.full((1, self.data_arr.shape[1]), np.nan), axis=0)\n        self.nan_idx = -1  # The last line is all NaN\n\n        # the data type will be changed\n        # The index of usable data is between start_idx and end_idx\n        self.start_idx, self.end_idx = self.data.index.slice_locs(start=pd.Timestamp(start), end=pd.Timestamp(end))\n        self.idx_df, self.idx_map = self.build_index(self.data)\n        self.idx_arr = np.array(self.idx_df.values, dtype=np.float64)  # for better performance\n\n    def get_index(self):\n        \"\"\"\n        Get the pandas index of the data, it will be useful in following scenarios\n        - Special sampler will be used (e.g. user want to sample day by day)\n        \"\"\"\n        return self.data.index[self.start_idx : self.end_idx]\n\n    def config(self, **kwargs):\n        # Config the attributes\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n    @staticmethod\n    def build_index(data: pd.DataFrame) -> dict:\n        \"\"\"\n        The relation of the data\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The dataframe with <datetime, DataFrame>\n\n        Returns\n        -------\n        dict:\n            {<index>: <prev_index or None>}\n            # get the previous index of a line given index\n        \"\"\"\n        # object incase of pandas converting int to flaot\n        idx_df = pd.Series(range(data.shape[0]), index=data.index, dtype=np.object)\n        idx_df = lazy_sort_index(idx_df.unstack())\n        # NOTE: the correctness of `__getitem__` depends on columns sorted here\n        idx_df = lazy_sort_index(idx_df, axis=1)\n\n        idx_map = {}\n        for i, (_, row) in enumerate(idx_df.iterrows()):\n            for j, real_idx in enumerate(row):\n                if not np.isnan(real_idx):\n                    idx_map[real_idx] = (i, j)\n        return idx_df, idx_map\n\n    def _get_indices(self, row: int, col: int) -> np.array:\n        \"\"\"\n        get series indices of self.data_arr from the row, col indices of self.idx_df\n\n        Parameters\n        ----------\n        row : int\n            the row in self.idx_df\n        col : int\n            the col in self.idx_df\n\n        Returns\n        -------\n        np.array:\n            The indices of data of the data\n        \"\"\"\n        indices = self.idx_arr[max(row - self.step_len + 1, 0) : row + 1, col]\n\n        if len(indices) < self.step_len:\n            indices = np.concatenate([np.full((self.step_len - len(indices),), np.nan), indices])\n\n        if self.fillna_type == \"ffill\":\n            indices = np_ffill(indices)\n        elif self.fillna_type == \"ffill+bfill\":\n            indices = np_ffill(np_ffill(indices)[::-1])[::-1]\n        else:\n            assert self.fillna_type == \"none\"\n        return indices\n\n    def _get_row_col(self, idx) -> Tuple[int]:\n        \"\"\"\n        get the col index and row index of a given sample index in self.idx_df\n\n        Parameters\n        ----------\n        idx :\n            the input of  `__getitem__`\n\n        Returns\n        -------\n        Tuple[int]:\n            the row and col index\n        \"\"\"\n        # The the right row number `i` and col number `j` in idx_df\n        if isinstance(idx, (int, np.integer)):\n            real_idx = self.start_idx + idx\n            if self.start_idx <= real_idx < self.end_idx:\n                i, j = self.idx_map[real_idx]  # TODO: The performance of this line is not good\n            else:\n                raise KeyError(f\"{real_idx} is out of [{self.start_idx}, {self.end_idx})\")\n        elif isinstance(idx, tuple):\n            # <TSDataSampler object>[\"datetime\", \"instruments\"]\n            date, inst = idx\n            date = pd.Timestamp(date)\n            i = bisect.bisect_right(self.idx_df.index, date) - 1\n            # NOTE: This relies on the idx_df columns sorted in `__init__`\n            j = bisect.bisect_left(self.idx_df.columns, inst)\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n        return i, j\n\n    def __getitem__(self, idx: Union[int, Tuple[object, str], List[int]]):\n        \"\"\"\n        # We have two method to get the time-series of a sample\n        tsds is a instance of TSDataSampler\n\n        # 1) sample by int index directly\n        tsds[len(tsds) - 1]\n\n        # 2) sample by <datetime,instrument> index\n        tsds['2016-12-31', \"SZ300315\"]\n\n        # The return value will be similar to the data retrieved by following code\n        df.loc(axis=0)['2015-01-01':'2016-12-31', \"SZ300315\"].iloc[-30:]\n\n        Parameters\n        ----------\n        idx : Union[int, Tuple[object, str]]\n        \"\"\"\n        # Multi-index type\n        mtit = (list, np.ndarray)\n        if isinstance(idx, mtit):\n            indices = [self._get_indices(*self._get_row_col(i)) for i in idx]\n            indices = np.concatenate(indices)\n        else:\n            indices = self._get_indices(*self._get_row_col(idx))\n\n        # 1) for better performance, use the last nan line for padding the lost date\n        # 2) In case of precision problems. We use np.float64. # TODO: I'm not sure if whether np.float64 will result in\n        # precision problems. It will not cause any problems in my tests at least\n        indices = np.nan_to_num(indices.astype(np.float64), nan=self.nan_idx).astype(int)\n\n        data = self.data_arr[indices]\n        if isinstance(idx, mtit):\n            # if we get multiple indexes, addition dimension should be added.\n            # <sample_idx, step_idx, feature_idx>\n            data = data.reshape(-1, self.step_len, *data.shape[1:])\n        return data\n\n    def __len__(self):\n        return self.end_idx - self.start_idx",
  "class TSDatasetH(DatasetH):\n    \"\"\"\n    (T)ime-(S)eries Dataset (H)andler\n\n\n    Covnert the tabular data to Time-Series data\n\n    Requirements analysis\n\n    The typical workflow of a user to get time-series data for an sample\n    - process features\n    - slice proper data from data handler:  dimension of sample <feature, >\n    - Build relation of samples by <time, instrument> index\n        - Be able to sample times series of data <timestep, feature>\n        - It will be better if the interface is like \"torch.utils.data.Dataset\"\n    - User could build customized batch based on the data\n        - The dimension of a batch of data <batch_idx, feature, timestep>\n    \"\"\"\n\n    def __init__(self, step_len=30, *args, **kwargs):\n        self.step_len = step_len\n        super().__init__(*args, **kwargs)\n\n    def setup_data(self, *args, **kwargs):\n        super().setup_data(*args, **kwargs)\n        cal = self.handler.fetch(col_set=self.handler.CS_RAW).index.get_level_values(\"datetime\").unique()\n        cal = sorted(cal)\n        # Get the datatime index for building timestamp\n        self.cal = cal\n\n    def _prepare_seg(self, slc: slice, **kwargs) -> TSDataSampler:\n        # Dataset decide how to slice data(Get more data for timeseries).\n        start, end = slc.start, slc.stop\n        start_idx = bisect.bisect_left(self.cal, pd.Timestamp(start))\n        pad_start_idx = max(0, start_idx - self.step_len)\n        pad_start = self.cal[pad_start_idx]\n\n        # TSDatasetH will retrieve more data for complete\n        data = super()._prepare_seg(slice(pad_start, end), **kwargs)\n\n        tsds = TSDataSampler(data=data, start=start, end=end, step_len=self.step_len)\n        return tsds",
  "def __init__(self, *args, **kwargs):\n        \"\"\"\n        init is designed to finish following steps:\n\n        - setup data\n            - The data related attributes' names should start with '_' so that it will not be saved on disk when serializing.\n\n        - initialize the state of the dataset(info to prepare the data)\n            - The name of essential state for preparing data should not start with '_' so that it could be serialized on disk when serializing.\n\n        The data could specify the info to caculate the essential data for preparation\n        \"\"\"\n        self.setup_data(*args, **kwargs)\n        super().__init__()",
  "def setup_data(self, *args, **kwargs):\n        \"\"\"\n        Setup the data.\n\n        We split the setup_data function for following situation:\n\n        - User have a Dataset object with learned status on disk.\n\n        - User load the Dataset object from the disk(Note the init function is skiped).\n\n        - User call `setup_data` to load new data.\n\n        - User prepare data for model based on previous status.\n        \"\"\"\n        pass",
  "def prepare(self, *args, **kwargs) -> object:\n        \"\"\"\n        The type of dataset depends on the model. (It could be pd.DataFrame, pytorch.DataLoader, etc.)\n        The parameters should specify the scope for the prepared data\n        The method should:\n        - process the data\n\n        - return the processed data\n\n        Returns\n        -------\n        object:\n            return the object\n        \"\"\"\n        pass",
  "def init(self, handler_kwargs: dict = None, segment_kwargs: dict = None):\n        \"\"\"\n        Initialize the DatasetH\n\n        Parameters\n        ----------\n        handler_kwargs : dict\n            Config of DataHanlder, which could include the following arguments:\n\n            - arguments of DataHandler.conf_data, such as 'instruments', 'start_time' and 'end_time'.\n\n            - arguments of DataHandler.init, such as 'enable_cache', etc.\n\n        segment_kwargs : dict\n            Config of segments which is same as 'segments' in DatasetH.setup_data\n\n        \"\"\"\n        if handler_kwargs:\n            if not isinstance(handler_kwargs, dict):\n                raise TypeError(f\"param handler_kwargs must be type dict, not {type(handler_kwargs)}\")\n            kwargs_init = {}\n            kwargs_conf_data = {}\n            conf_data_arg = {\"instruments\", \"start_time\", \"end_time\"}\n            for k, v in handler_kwargs.items():\n                if k in conf_data_arg:\n                    kwargs_conf_data.update({k: v})\n                else:\n                    kwargs_init.update({k: v})\n\n            self.handler.conf_data(**kwargs_conf_data)\n            self.handler.init(**kwargs_init)\n\n        if segment_kwargs:\n            if not isinstance(segment_kwargs, dict):\n                raise TypeError(f\"param handler_kwargs must be type dict, not {type(segment_kwargs)}\")\n            self.segments = segment_kwargs.copy()",
  "def setup_data(self, handler: Union[Dict, DataHandler], segments: Dict[Text, Tuple]):\n        \"\"\"\n        Setup the underlying data.\n\n        Parameters\n        ----------\n        handler : Union[dict, DataHandler]\n            handler could be:\n\n            - insntance of `DataHandler`\n\n            - config of `DataHandler`.  Please refer to `DataHandler`\n\n        segments : dict\n            Describe the options to segment the data.\n            Here are some examples:\n\n            .. code-block::\n\n                1) 'segments': {\n                        'train': (\"2008-01-01\", \"2014-12-31\"),\n                        'valid': (\"2017-01-01\", \"2020-08-01\",),\n                        'test': (\"2015-01-01\", \"2016-12-31\",),\n                    }\n                2) 'segments': {\n                        'insample': (\"2008-01-01\", \"2014-12-31\"),\n                        'outsample': (\"2017-01-01\", \"2020-08-01\",),\n                    }\n        \"\"\"\n        self.handler = init_instance_by_config(handler, accept_types=DataHandler)\n        self.segments = segments.copy()",
  "def __repr__(self):\n        return \"{name}(handler={handler}, segments={segments})\".format(\n            name=self.__class__.__name__, handler=self.handler, segments=self.segments\n        )",
  "def _prepare_seg(self, slc: slice, **kwargs):\n        \"\"\"\n        Give a slice, retrieve the according data\n\n        Parameters\n        ----------\n        slc : slice\n        \"\"\"\n        return self.handler.fetch(slc, **kwargs)",
  "def prepare(\n        self,\n        segments: Union[List[Text], Tuple[Text], Text, slice],\n        col_set=DataHandler.CS_ALL,\n        data_key=DataHandlerLP.DK_I,\n        **kwargs,\n    ) -> Union[List[pd.DataFrame], pd.DataFrame]:\n        \"\"\"\n        Prepare the data for learning and inference.\n\n        Parameters\n        ----------\n        segments : Union[List[Text], Tuple[Text], Text, slice]\n            Describe the scope of the data to be prepared\n            Here are some examples:\n\n            - 'train'\n\n            - ['train', 'valid']\n\n        col_set : str\n            The col_set will be passed to self.handler when fetching data.\n        data_key : str\n            The data to fetch:  DK_*\n            Default is DK_I, which indicate fetching data for **inference**.\n\n        Returns\n        -------\n        Union[List[pd.DataFrame], pd.DataFrame]:\n\n        Raises\n        ------\n        NotImplementedError:\n        \"\"\"\n        logger = get_module_logger(\"DatasetH\")\n        fetch_kwargs = {\"col_set\": col_set}\n        fetch_kwargs.update(kwargs)\n        if \"data_key\" in getfullargspec(self.handler.fetch).args:\n            fetch_kwargs[\"data_key\"] = data_key\n        else:\n            logger.info(f\"data_key[{data_key}] is ignored.\")\n\n        # Handle all kinds of segments format\n        if isinstance(segments, (list, tuple)):\n            return [self._prepare_seg(slice(*self.segments[seg]), **fetch_kwargs) for seg in segments]\n        elif isinstance(segments, str):\n            return self._prepare_seg(slice(*self.segments[segments]), **fetch_kwargs)\n        elif isinstance(segments, slice):\n            return self._prepare_seg(segments, **fetch_kwargs)\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")",
  "def __init__(self, data: pd.DataFrame, start, end, step_len: int, fillna_type: str = \"none\"):\n        \"\"\"\n        Build a dataset which looks like torch.data.utils.Dataset.\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The raw tabular data\n        start :\n            The indexable start time\n        end :\n            The indexable end time\n        step_len : int\n            The length of the time-series step\n        fillna_type : int\n            How will qlib handle the sample if there is on sample in a specific date.\n            none:\n                fill with np.nan\n            ffill:\n                ffill with previous sample\n            ffill+bfill:\n                ffill with previous samples first and fill with later samples second\n        \"\"\"\n        self.start = start\n        self.end = end\n        self.step_len = step_len\n        self.fillna_type = fillna_type\n        assert get_level_index(data, \"datetime\") == 0\n        self.data = lazy_sort_index(data)\n        self.data_arr = np.array(self.data)  # Get index from numpy.array will much faster than DataFrame.values!\n        # NOTE: append last line with full NaN for better performance in `__getitem__`\n        self.data_arr = np.append(self.data_arr, np.full((1, self.data_arr.shape[1]), np.nan), axis=0)\n        self.nan_idx = -1  # The last line is all NaN\n\n        # the data type will be changed\n        # The index of usable data is between start_idx and end_idx\n        self.start_idx, self.end_idx = self.data.index.slice_locs(start=pd.Timestamp(start), end=pd.Timestamp(end))\n        self.idx_df, self.idx_map = self.build_index(self.data)\n        self.idx_arr = np.array(self.idx_df.values, dtype=np.float64)",
  "def get_index(self):\n        \"\"\"\n        Get the pandas index of the data, it will be useful in following scenarios\n        - Special sampler will be used (e.g. user want to sample day by day)\n        \"\"\"\n        return self.data.index[self.start_idx : self.end_idx]",
  "def config(self, **kwargs):\n        # Config the attributes\n        for k, v in kwargs.items():\n            setattr(self, k, v)",
  "def build_index(data: pd.DataFrame) -> dict:\n        \"\"\"\n        The relation of the data\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The dataframe with <datetime, DataFrame>\n\n        Returns\n        -------\n        dict:\n            {<index>: <prev_index or None>}\n            # get the previous index of a line given index\n        \"\"\"\n        # object incase of pandas converting int to flaot\n        idx_df = pd.Series(range(data.shape[0]), index=data.index, dtype=np.object)\n        idx_df = lazy_sort_index(idx_df.unstack())\n        # NOTE: the correctness of `__getitem__` depends on columns sorted here\n        idx_df = lazy_sort_index(idx_df, axis=1)\n\n        idx_map = {}\n        for i, (_, row) in enumerate(idx_df.iterrows()):\n            for j, real_idx in enumerate(row):\n                if not np.isnan(real_idx):\n                    idx_map[real_idx] = (i, j)\n        return idx_df, idx_map",
  "def _get_indices(self, row: int, col: int) -> np.array:\n        \"\"\"\n        get series indices of self.data_arr from the row, col indices of self.idx_df\n\n        Parameters\n        ----------\n        row : int\n            the row in self.idx_df\n        col : int\n            the col in self.idx_df\n\n        Returns\n        -------\n        np.array:\n            The indices of data of the data\n        \"\"\"\n        indices = self.idx_arr[max(row - self.step_len + 1, 0) : row + 1, col]\n\n        if len(indices) < self.step_len:\n            indices = np.concatenate([np.full((self.step_len - len(indices),), np.nan), indices])\n\n        if self.fillna_type == \"ffill\":\n            indices = np_ffill(indices)\n        elif self.fillna_type == \"ffill+bfill\":\n            indices = np_ffill(np_ffill(indices)[::-1])[::-1]\n        else:\n            assert self.fillna_type == \"none\"\n        return indices",
  "def _get_row_col(self, idx) -> Tuple[int]:\n        \"\"\"\n        get the col index and row index of a given sample index in self.idx_df\n\n        Parameters\n        ----------\n        idx :\n            the input of  `__getitem__`\n\n        Returns\n        -------\n        Tuple[int]:\n            the row and col index\n        \"\"\"\n        # The the right row number `i` and col number `j` in idx_df\n        if isinstance(idx, (int, np.integer)):\n            real_idx = self.start_idx + idx\n            if self.start_idx <= real_idx < self.end_idx:\n                i, j = self.idx_map[real_idx]  # TODO: The performance of this line is not good\n            else:\n                raise KeyError(f\"{real_idx} is out of [{self.start_idx}, {self.end_idx})\")\n        elif isinstance(idx, tuple):\n            # <TSDataSampler object>[\"datetime\", \"instruments\"]\n            date, inst = idx\n            date = pd.Timestamp(date)\n            i = bisect.bisect_right(self.idx_df.index, date) - 1\n            # NOTE: This relies on the idx_df columns sorted in `__init__`\n            j = bisect.bisect_left(self.idx_df.columns, inst)\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n        return i, j",
  "def __getitem__(self, idx: Union[int, Tuple[object, str], List[int]]):\n        \"\"\"\n        # We have two method to get the time-series of a sample\n        tsds is a instance of TSDataSampler\n\n        # 1) sample by int index directly\n        tsds[len(tsds) - 1]\n\n        # 2) sample by <datetime,instrument> index\n        tsds['2016-12-31', \"SZ300315\"]\n\n        # The return value will be similar to the data retrieved by following code\n        df.loc(axis=0)['2015-01-01':'2016-12-31', \"SZ300315\"].iloc[-30:]\n\n        Parameters\n        ----------\n        idx : Union[int, Tuple[object, str]]\n        \"\"\"\n        # Multi-index type\n        mtit = (list, np.ndarray)\n        if isinstance(idx, mtit):\n            indices = [self._get_indices(*self._get_row_col(i)) for i in idx]\n            indices = np.concatenate(indices)\n        else:\n            indices = self._get_indices(*self._get_row_col(idx))\n\n        # 1) for better performance, use the last nan line for padding the lost date\n        # 2) In case of precision problems. We use np.float64. # TODO: I'm not sure if whether np.float64 will result in\n        # precision problems. It will not cause any problems in my tests at least\n        indices = np.nan_to_num(indices.astype(np.float64), nan=self.nan_idx).astype(int)\n\n        data = self.data_arr[indices]\n        if isinstance(idx, mtit):\n            # if we get multiple indexes, addition dimension should be added.\n            # <sample_idx, step_idx, feature_idx>\n            data = data.reshape(-1, self.step_len, *data.shape[1:])\n        return data",
  "def __len__(self):\n        return self.end_idx - self.start_idx",
  "def __init__(self, step_len=30, *args, **kwargs):\n        self.step_len = step_len\n        super().__init__(*args, **kwargs)",
  "def setup_data(self, *args, **kwargs):\n        super().setup_data(*args, **kwargs)\n        cal = self.handler.fetch(col_set=self.handler.CS_RAW).index.get_level_values(\"datetime\").unique()\n        cal = sorted(cal)\n        # Get the datatime index for building timestamp\n        self.cal = cal",
  "def _prepare_seg(self, slc: slice, **kwargs) -> TSDataSampler:\n        # Dataset decide how to slice data(Get more data for timeseries).\n        start, end = slc.start, slc.stop\n        start_idx = bisect.bisect_left(self.cal, pd.Timestamp(start))\n        pad_start_idx = max(0, start_idx - self.step_len)\n        pad_start = self.cal[pad_start_idx]\n\n        # TSDatasetH will retrieve more data for complete\n        data = super()._prepare_seg(slice(pad_start, end), **kwargs)\n\n        tsds = TSDataSampler(data=data, start=start, end=end, step_len=self.step_len)\n        return tsds",
  "def risk_analysis(r, N=252):\n    \"\"\"Risk Analysis\n\n    Parameters\n    ----------\n    r : pandas.Series\n        daily return series.\n    N: int\n        scaler for annualizing information_ratio (day: 250, week: 50, month: 12).\n    \"\"\"\n    mean = r.mean()\n    std = r.std(ddof=1)\n    annualized_return = mean * N\n    information_ratio = mean / std * np.sqrt(N)\n    max_drawdown = (r.cumsum() - r.cumsum().cummax()).min()\n    data = {\n        \"mean\": mean,\n        \"std\": std,\n        \"annualized_return\": annualized_return,\n        \"information_ratio\": information_ratio,\n        \"max_drawdown\": max_drawdown,\n    }\n    res = pd.Series(data, index=data.keys()).to_frame(\"risk\")\n    return res",
  "def backtest(pred, account=1e9, shift=1, benchmark=\"SH000905\", verbose=True, **kwargs):\n    \"\"\"This function will help you set a reasonable Exchange and provide default value for strategy\n    Parameters\n    ----------\n\n    - **backtest workflow related or commmon arguments**\n\n    pred : pandas.DataFrame\n        predict should has <datetime, instrument> index and one `score` column.\n    account : float\n        init account value.\n    shift : int\n        whether to shift prediction by one day.\n    benchmark : str\n        benchmark code, default is SH000905 CSI 500.\n    verbose : bool\n        whether to print log.\n\n    - **strategy related arguments**\n\n    strategy : Strategy()\n        strategy used in backtest.\n    topk : int (Default value: 50)\n        top-N stocks to buy.\n    margin : int or float(Default value: 0.5)\n        - if isinstance(margin, int):\n\n            sell_limit = margin\n\n        - else:\n\n            sell_limit = pred_in_a_day.count() * margin\n\n        buffer margin, in single score_mode, continue holding stock if it is in nlargest(sell_limit).\n        sell_limit should be no less than topk.\n    n_drop : int\n        number of stocks to be replaced in each trading date.\n    risk_degree: float\n        0-1, 0.95 for example, use 95% money to trade.\n    str_type: 'amount', 'weight' or 'dropout'\n        strategy type: TopkAmountStrategy ,TopkWeightStrategy or TopkDropoutStrategy.\n\n    - **exchange related arguments**\n\n    exchange: Exchange()\n        pass the exchange for speeding up.\n    subscribe_fields: list\n        subscribe fields.\n    open_cost : float\n        open transaction cost. The default value is 0.002(0.2%).\n    close_cost : float\n        close transaction cost. The default value is 0.002(0.2%).\n    min_cost : float\n        min transaction cost.\n    trade_unit : int\n        100 for China A.\n    deal_price: str\n        dealing price type: 'close', 'open', 'vwap'.\n    limit_threshold : float\n        limit move 0.1 (10%) for example, long and short with same limit.\n    extract_codes: bool\n        will we pass the codes extracted from the pred to the exchange.\n\n        .. note:: This will be faster with offline qlib.\n\n    - **executor related arguments**\n\n    executor : BaseExecutor()\n        executor used in backtest.\n    verbose : bool\n        whether to print log.\n\n    \"\"\"\n    warnings.warn(\n        \"this function is deprecated, please use backtest function in qlib.contrib.backtest\", DeprecationWarning\n    )\n    report_dict = backtest_func(\n        pred=pred, account=account, shift=shift, benchmark=benchmark, verbose=verbose, return_order=False, **kwargs\n    )\n    return report_dict.get(\"report_df\"), report_dict.get(\"positions\")",
  "def long_short_backtest(\n    pred,\n    topk=50,\n    deal_price=None,\n    shift=1,\n    open_cost=0,\n    close_cost=0,\n    trade_unit=None,\n    limit_threshold=None,\n    min_cost=5,\n    subscribe_fields=[],\n    extract_codes=False,\n):\n    \"\"\"\n    A backtest for long-short strategy\n\n    :param pred:        The trading signal produced on day `T`.\n    :param topk:       The short topk securities and long topk securities.\n    :param deal_price:  The price to deal the trading.\n    :param shift:       Whether to shift prediction by one day.  The trading day will be T+1 if shift==1.\n    :param open_cost:   open transaction cost.\n    :param close_cost:  close transaction cost.\n    :param trade_unit:  100 for China A.\n    :param limit_threshold: limit move 0.1 (10%) for example, long and short with same limit.\n    :param min_cost:    min transaction cost.\n    :param subscribe_fields: subscribe fields.\n    :param extract_codes:  bool.\n                       will we pass the codes extracted from the pred to the exchange.\n                       NOTE: This will be faster with offline qlib.\n    :return:            The result of backtest, it is represented by a dict.\n                        { \"long\": long_returns(excess),\n                          \"short\": short_returns(excess),\n                          \"long_short\": long_short_returns}\n    \"\"\"\n    if get_level_index(pred, level=\"datetime\") == 1:\n        pred = pred.swaplevel().sort_index()\n\n    if trade_unit is None:\n        trade_unit = C.trade_unit\n    if limit_threshold is None:\n        limit_threshold = C.limit_threshold\n    if deal_price is None:\n        deal_price = C.deal_price\n    if deal_price[0] != \"$\":\n        deal_price = \"$\" + deal_price\n\n    subscribe_fields = subscribe_fields.copy()\n    profit_str = f\"Ref({deal_price}, -1)/{deal_price} - 1\"\n    subscribe_fields.append(profit_str)\n\n    trade_exchange = get_exchange(\n        pred=pred,\n        deal_price=deal_price,\n        subscribe_fields=subscribe_fields,\n        limit_threshold=limit_threshold,\n        open_cost=open_cost,\n        close_cost=close_cost,\n        min_cost=min_cost,\n        trade_unit=trade_unit,\n        extract_codes=extract_codes,\n        shift=shift,\n    )\n\n    _pred_dates = pred.index.get_level_values(level=\"datetime\")\n    predict_dates = D.calendar(start_time=_pred_dates.min(), end_time=_pred_dates.max())\n    trade_dates = np.append(predict_dates[shift:], get_date_range(predict_dates[-1], left_shift=1, right_shift=shift))\n\n    long_returns = {}\n    short_returns = {}\n    ls_returns = {}\n\n    for pdate, date in zip(predict_dates, trade_dates):\n        score = pred.loc(axis=0)[pdate, :]\n        score = score.reset_index().sort_values(by=\"score\", ascending=False)\n\n        long_stocks = list(score.iloc[:topk][\"instrument\"])\n        short_stocks = list(score.iloc[-topk:][\"instrument\"])\n\n        score = score.set_index([\"datetime\", \"instrument\"]).sort_index()\n\n        long_profit = []\n        short_profit = []\n        all_profit = []\n\n        for stock in long_stocks:\n            if not trade_exchange.is_stock_tradable(stock_id=stock, trade_date=date):\n                continue\n            profit = trade_exchange.get_quote_info(stock_id=stock, trade_date=date)[profit_str]\n            if np.isnan(profit):\n                long_profit.append(0)\n            else:\n                long_profit.append(profit)\n\n        for stock in short_stocks:\n            if not trade_exchange.is_stock_tradable(stock_id=stock, trade_date=date):\n                continue\n            profit = trade_exchange.get_quote_info(stock_id=stock, trade_date=date)[profit_str]\n            if np.isnan(profit):\n                short_profit.append(0)\n            else:\n                short_profit.append(-profit)\n\n        for stock in list(score.loc(axis=0)[pdate, :].index.get_level_values(level=0)):\n            # exclude the suspend stock\n            if trade_exchange.check_stock_suspended(stock_id=stock, trade_date=date):\n                continue\n            profit = trade_exchange.get_quote_info(stock_id=stock, trade_date=date)[profit_str]\n            if np.isnan(profit):\n                all_profit.append(0)\n            else:\n                all_profit.append(profit)\n\n        long_returns[date] = np.mean(long_profit) - np.mean(all_profit)\n        short_returns[date] = np.mean(short_profit) + np.mean(all_profit)\n        ls_returns[date] = np.mean(short_profit) + np.mean(long_profit)\n\n    return dict(\n        zip(\n            [\"long\", \"short\", \"long_short\"],\n            map(pd.Series, [long_returns, short_returns, ls_returns]),\n        )\n    )",
  "def t_run():\n    pred_FN = \"./check_pred.csv\"\n    pred = pd.read_csv(pred_FN)\n    pred[\"datetime\"] = pd.to_datetime(pred[\"datetime\"])\n    pred = pred.set_index([pred.columns[0], pred.columns[1]])\n    pred = pred.iloc[:9000]\n    report_df, positions = backtest(pred=pred)\n    print(report_df.head())\n    print(positions.keys())\n    print(positions[list(positions.keys())[0]])\n    return 0",
  "def _get_position_value_from_df(evaluate_date, position, close_data_df):\n    \"\"\"Get position value by existed close data df\n    close_data_df:\n        pd.DataFrame\n        multi-index\n        close_data_df['$close'][stock_id][evaluate_date]: close price for (stock_id, evaluate_date)\n    position:\n        same in get_position_value()\n    \"\"\"\n    value = 0\n    for stock_id, report in position.items():\n        if stock_id != \"cash\":\n            value += report[\"amount\"] * close_data_df[\"$close\"][stock_id][evaluate_date]\n            # value += report['amount'] * report['price']\n    if \"cash\" in position:\n        value += position[\"cash\"]\n    return value",
  "def get_position_value(evaluate_date, position):\n    \"\"\"sum of close*amount\n\n    get value of postion\n\n    use close price\n\n        postions:\n        {\n            Timestamp('2016-01-05 00:00:00'):\n            {\n                'SH600022':\n                {\n                    'amount':100.00,\n                    'price':12.00\n                },\n\n                'cash':100000.0\n            }\n        }\n\n    It means Hold 100.0 'SH600022' and 100000.0 RMB in '2016-01-05'\n    \"\"\"\n    # load close price for position\n    # position should also consider cash\n    instruments = list(position.keys())\n    instruments = list(set(instruments) - {\"cash\"})  # filter 'cash'\n    fields = [\"$close\"]\n    close_data_df = D.features(\n        instruments,\n        fields,\n        start_time=evaluate_date,\n        end_time=evaluate_date,\n        freq=\"day\",\n        disk_cache=0,\n    )\n    value = _get_position_value_from_df(evaluate_date, position, close_data_df)\n    return value",
  "def get_position_list_value(positions):\n    # generate instrument list and date for whole poitions\n    instruments = set()\n    for day, position in positions.items():\n        instruments.update(position.keys())\n    instruments = list(set(instruments) - {\"cash\"})  # filter 'cash'\n    instruments.sort()\n    day_list = list(positions.keys())\n    day_list.sort()\n    start_date, end_date = day_list[0], day_list[-1]\n    # load data\n    fields = [\"$close\"]\n    close_data_df = D.features(\n        instruments,\n        fields,\n        start_time=start_date,\n        end_time=end_date,\n        freq=\"day\",\n        disk_cache=0,\n    )\n    # generate value\n    # return dict for time:position_value\n    value_dict = OrderedDict()\n    for day, position in positions.items():\n        value = _get_position_value_from_df(evaluate_date=day, position=position, close_data_df=close_data_df)\n        value_dict[day] = value\n    return value_dict",
  "def get_daily_return_series_from_positions(positions, init_asset_value):\n    \"\"\"Parameters\n    generate daily return series from  position view\n    positions: positions generated by strategy\n    init_asset_value : init asset value\n    return: pd.Series of daily return , return_series[date] = daily return rate\n    \"\"\"\n    value_dict = get_position_list_value(positions)\n    value_series = pd.Series(value_dict)\n    value_series = value_series.sort_index()  # check date\n    return_series = value_series.pct_change()\n    return_series[value_series.index[0]] = (\n        value_series[value_series.index[0]] / init_asset_value - 1\n    )  # update daily return for the first date\n    return return_series",
  "def get_annual_return_from_positions(positions, init_asset_value):\n    \"\"\"Annualized Returns\n\n    p_r = (p_end / p_start)^{(250/n)} - 1\n\n    p_r     annual return\n    p_end   final value\n    p_start init value\n    n       days of backtest\n\n    \"\"\"\n    date_range_list = sorted(list(positions.keys()))\n    end_time = date_range_list[-1]\n    p_end = get_position_value(end_time, positions[end_time])\n    p_start = init_asset_value\n    n_period = len(date_range_list)\n    annual = pow((p_end / p_start), (250 / n_period)) - 1\n\n    return annual",
  "def get_annaul_return_from_return_series(r, method=\"ci\"):\n    \"\"\"Risk Analysis from daily return series\n\n    Parameters\n    ----------\n    r : pandas.Series\n        daily return series\n    method : str\n        interest calculation method, ci(compound interest)/si(simple interest)\n    \"\"\"\n    mean = r.mean()\n    annual = (1 + mean) ** 250 - 1 if method == \"ci\" else mean * 250\n\n    return annual",
  "def get_sharpe_ratio_from_return_series(r, risk_free_rate=0.00, method=\"ci\"):\n    \"\"\"Risk Analysis\n\n    Parameters\n    ----------\n    r : pandas.Series\n        daily return series\n    method : str\n        interest calculation method, ci(compound interest)/si(simple interest)\n    risk_free_rate : float\n        risk_free_rate, default as 0.00, can set as 0.03 etc\n    \"\"\"\n    std = r.std(ddof=1)\n    annual = get_annaul_return_from_return_series(r, method=method)\n    sharpe = (annual - risk_free_rate) / std / np.sqrt(250)\n\n    return sharpe",
  "def get_max_drawdown_from_series(r):\n    \"\"\"Risk Analysis from asset value\n\n    cumprod way\n\n    Parameters\n    ----------\n    r : pandas.Series\n        daily return series\n    \"\"\"\n    # mdd = ((r.cumsum() - r.cumsum().cummax()) / (1 + r.cumsum().cummax())).min()\n\n    mdd = (((1 + r).cumprod() - (1 + r).cumprod().cummax()) / ((1 + r).cumprod().cummax())).min()\n\n    return mdd",
  "def get_turnover_rate():\n    # in backtest\n    pass",
  "def get_beta(r, b):\n    \"\"\"Risk Analysis  beta\n\n    Parameters\n    ----------\n    r : pandas.Series\n        daily return series of strategy\n    b : pandas.Series\n        daily return series of baseline\n    \"\"\"\n    cov_r_b = np.cov(r, b)\n    var_b = np.var(b)\n    return cov_r_b / var_b",
  "def get_alpha(r, b, risk_free_rate=0.03):\n    beta = get_beta(r, b)\n    annaul_r = get_annaul_return_from_return_series(r)\n    annaul_b = get_annaul_return_from_return_series(b)\n\n    alpha = annaul_r - risk_free_rate - beta * (annaul_b - risk_free_rate)\n\n    return alpha",
  "def get_volatility_from_series(r):\n    return r.std(ddof=1)",
  "def get_rank_ic(a, b):\n    \"\"\"Rank IC\n\n    Parameters\n    ----------\n    r : pandas.Series\n        daily score series of feature\n    b : pandas.Series\n        daily return series\n\n    \"\"\"\n    return spearmanr(a, b).correlation",
  "def get_normal_ic(a, b):\n    return pearsonr(a, b).correlation",
  "class OrderGenerator:\n    def generate_order_list_from_target_weight_position(\n        self,\n        current: Position,\n        trade_exchange: Exchange,\n        target_weight_position: dict,\n        risk_degree: float,\n        pred_date: pd.Timestamp,\n        trade_date: pd.Timestamp,\n    ) -> list:\n        \"\"\"generate_order_list_from_target_weight_position\n\n        :param current: The current position\n        :type current: Position\n        :param trade_exchange:\n        :type trade_exchange: Exchange\n        :param target_weight_position: {stock_id : weight}\n        :type target_weight_position: dict\n        :param risk_degree:\n        :type risk_degree: float\n        :param pred_date: the date the score is predicted\n        :type pred_date: pd.Timestamp\n        :param trade_date: the date the stock is traded\n        :type trade_date: pd.Timestamp\n\n        :rtype: list\n        \"\"\"\n        raise NotImplementedError()",
  "class OrderGenWInteract(OrderGenerator):\n    \"\"\"Order Generator With Interact\"\"\"\n\n    def generate_order_list_from_target_weight_position(\n        self,\n        current: Position,\n        trade_exchange: Exchange,\n        target_weight_position: dict,\n        risk_degree: float,\n        pred_date: pd.Timestamp,\n        trade_date: pd.Timestamp,\n    ) -> list:\n        \"\"\"generate_order_list_from_target_weight_position\n\n        No adjustment for for the nontradable share.\n        All the tadable value is assigned to the tadable stock according to the weight.\n        if interact == True, will use the price at trade date to generate order list\n        else, will only use the price before the trade date to generate order list\n\n        :param current:\n        :type current: Position\n        :param trade_exchange:\n        :type trade_exchange: Exchange\n        :param target_weight_position:\n        :type target_weight_position: dict\n        :param risk_degree:\n        :type risk_degree: float\n        :param pred_date:\n        :type pred_date: pd.Timestamp\n        :param trade_date:\n        :type trade_date: pd.Timestamp\n\n        :rtype: list\n        \"\"\"\n        # calculate current_tradable_value\n        current_amount_dict = current.get_stock_amount_dict()\n        current_total_value = trade_exchange.calculate_amount_position_value(\n            amount_dict=current_amount_dict, trade_date=trade_date, only_tradable=False\n        )\n        current_tradable_value = trade_exchange.calculate_amount_position_value(\n            amount_dict=current_amount_dict, trade_date=trade_date, only_tradable=True\n        )\n        # add cash\n        current_tradable_value += current.get_cash()\n\n        reserved_cash = (1.0 - risk_degree) * (current_total_value + current.get_cash())\n        current_tradable_value -= reserved_cash\n\n        if current_tradable_value < 0:\n            # if you sell all the tradable stock can not meet the reserved\n            # value. Then just sell all the stocks\n            target_amount_dict = copy.deepcopy(current_amount_dict.copy())\n            for stock_id in list(target_amount_dict.keys()):\n                if trade_exchange.is_stock_tradable(stock_id, trade_date):\n                    del target_amount_dict[stock_id]\n        else:\n            # consider cost rate\n            current_tradable_value /= 1 + max(trade_exchange.close_cost, trade_exchange.open_cost)\n\n            # strategy 1 : generate amount_position by weight_position\n            # Use API in Exchange()\n            target_amount_dict = trade_exchange.generate_amount_position_from_weight_position(\n                weight_position=target_weight_position,\n                cash=current_tradable_value,\n                trade_date=trade_date,\n            )\n        order_list = trade_exchange.generate_order_for_target_amount_position(\n            target_position=target_amount_dict,\n            current_position=current_amount_dict,\n            trade_date=trade_date,\n        )\n        return order_list",
  "class OrderGenWOInteract(OrderGenerator):\n    \"\"\"Order Generator Without Interact\"\"\"\n\n    def generate_order_list_from_target_weight_position(\n        self,\n        current: Position,\n        trade_exchange: Exchange,\n        target_weight_position: dict,\n        risk_degree: float,\n        pred_date: pd.Timestamp,\n        trade_date: pd.Timestamp,\n    ) -> list:\n        \"\"\"generate_order_list_from_target_weight_position\n\n        generate order list directly not using the information (e.g. whether can be traded, the accurate trade price) at trade date.\n        In target weight position, generating order list need to know the price of objective stock in trade date, but we cannot get that\n        value when do not interact with exchange, so we check the %close price at pred_date or price recorded in current position.\n\n        :param current:\n        :type current: Position\n        :param trade_exchange:\n        :type trade_exchange: Exchange\n        :param target_weight_position:\n        :type target_weight_position: dict\n        :param risk_degree:\n        :type risk_degree: float\n        :param pred_date:\n        :type pred_date: pd.Timestamp\n        :param trade_date:\n        :type trade_date: pd.Timestamp\n\n        :rtype: list\n        \"\"\"\n        risk_total_value = risk_degree * current.calculate_value()\n\n        current_stock = current.get_stock_list()\n        amount_dict = {}\n        for stock_id in target_weight_position:\n            # Current rule will ignore the stock that not hold and cannot be traded at predict date\n            if trade_exchange.is_stock_tradable(stock_id=stock_id, trade_date=pred_date):\n                amount_dict[stock_id] = (\n                    risk_total_value * target_weight_position[stock_id] / trade_exchange.get_close(stock_id, pred_date)\n                )\n            elif stock_id in current_stock:\n                amount_dict[stock_id] = (\n                    risk_total_value * target_weight_position[stock_id] / current.get_stock_price(stock_id)\n                )\n            else:\n                continue\n        order_list = trade_exchange.generate_order_for_target_amount_position(\n            target_position=amount_dict,\n            current_position=current.get_stock_amount_dict(),\n            trade_date=trade_date,\n        )\n        return order_list",
  "def generate_order_list_from_target_weight_position(\n        self,\n        current: Position,\n        trade_exchange: Exchange,\n        target_weight_position: dict,\n        risk_degree: float,\n        pred_date: pd.Timestamp,\n        trade_date: pd.Timestamp,\n    ) -> list:\n        \"\"\"generate_order_list_from_target_weight_position\n\n        :param current: The current position\n        :type current: Position\n        :param trade_exchange:\n        :type trade_exchange: Exchange\n        :param target_weight_position: {stock_id : weight}\n        :type target_weight_position: dict\n        :param risk_degree:\n        :type risk_degree: float\n        :param pred_date: the date the score is predicted\n        :type pred_date: pd.Timestamp\n        :param trade_date: the date the stock is traded\n        :type trade_date: pd.Timestamp\n\n        :rtype: list\n        \"\"\"\n        raise NotImplementedError()",
  "def generate_order_list_from_target_weight_position(\n        self,\n        current: Position,\n        trade_exchange: Exchange,\n        target_weight_position: dict,\n        risk_degree: float,\n        pred_date: pd.Timestamp,\n        trade_date: pd.Timestamp,\n    ) -> list:\n        \"\"\"generate_order_list_from_target_weight_position\n\n        No adjustment for for the nontradable share.\n        All the tadable value is assigned to the tadable stock according to the weight.\n        if interact == True, will use the price at trade date to generate order list\n        else, will only use the price before the trade date to generate order list\n\n        :param current:\n        :type current: Position\n        :param trade_exchange:\n        :type trade_exchange: Exchange\n        :param target_weight_position:\n        :type target_weight_position: dict\n        :param risk_degree:\n        :type risk_degree: float\n        :param pred_date:\n        :type pred_date: pd.Timestamp\n        :param trade_date:\n        :type trade_date: pd.Timestamp\n\n        :rtype: list\n        \"\"\"\n        # calculate current_tradable_value\n        current_amount_dict = current.get_stock_amount_dict()\n        current_total_value = trade_exchange.calculate_amount_position_value(\n            amount_dict=current_amount_dict, trade_date=trade_date, only_tradable=False\n        )\n        current_tradable_value = trade_exchange.calculate_amount_position_value(\n            amount_dict=current_amount_dict, trade_date=trade_date, only_tradable=True\n        )\n        # add cash\n        current_tradable_value += current.get_cash()\n\n        reserved_cash = (1.0 - risk_degree) * (current_total_value + current.get_cash())\n        current_tradable_value -= reserved_cash\n\n        if current_tradable_value < 0:\n            # if you sell all the tradable stock can not meet the reserved\n            # value. Then just sell all the stocks\n            target_amount_dict = copy.deepcopy(current_amount_dict.copy())\n            for stock_id in list(target_amount_dict.keys()):\n                if trade_exchange.is_stock_tradable(stock_id, trade_date):\n                    del target_amount_dict[stock_id]\n        else:\n            # consider cost rate\n            current_tradable_value /= 1 + max(trade_exchange.close_cost, trade_exchange.open_cost)\n\n            # strategy 1 : generate amount_position by weight_position\n            # Use API in Exchange()\n            target_amount_dict = trade_exchange.generate_amount_position_from_weight_position(\n                weight_position=target_weight_position,\n                cash=current_tradable_value,\n                trade_date=trade_date,\n            )\n        order_list = trade_exchange.generate_order_for_target_amount_position(\n            target_position=target_amount_dict,\n            current_position=current_amount_dict,\n            trade_date=trade_date,\n        )\n        return order_list",
  "def generate_order_list_from_target_weight_position(\n        self,\n        current: Position,\n        trade_exchange: Exchange,\n        target_weight_position: dict,\n        risk_degree: float,\n        pred_date: pd.Timestamp,\n        trade_date: pd.Timestamp,\n    ) -> list:\n        \"\"\"generate_order_list_from_target_weight_position\n\n        generate order list directly not using the information (e.g. whether can be traded, the accurate trade price) at trade date.\n        In target weight position, generating order list need to know the price of objective stock in trade date, but we cannot get that\n        value when do not interact with exchange, so we check the %close price at pred_date or price recorded in current position.\n\n        :param current:\n        :type current: Position\n        :param trade_exchange:\n        :type trade_exchange: Exchange\n        :param target_weight_position:\n        :type target_weight_position: dict\n        :param risk_degree:\n        :type risk_degree: float\n        :param pred_date:\n        :type pred_date: pd.Timestamp\n        :param trade_date:\n        :type trade_date: pd.Timestamp\n\n        :rtype: list\n        \"\"\"\n        risk_total_value = risk_degree * current.calculate_value()\n\n        current_stock = current.get_stock_list()\n        amount_dict = {}\n        for stock_id in target_weight_position:\n            # Current rule will ignore the stock that not hold and cannot be traded at predict date\n            if trade_exchange.is_stock_tradable(stock_id=stock_id, trade_date=pred_date):\n                amount_dict[stock_id] = (\n                    risk_total_value * target_weight_position[stock_id] / trade_exchange.get_close(stock_id, pred_date)\n                )\n            elif stock_id in current_stock:\n                amount_dict[stock_id] = (\n                    risk_total_value * target_weight_position[stock_id] / current.get_stock_price(stock_id)\n                )\n            else:\n                continue\n        order_list = trade_exchange.generate_order_for_target_amount_position(\n            target_position=amount_dict,\n            current_position=current.get_stock_amount_dict(),\n            trade_date=trade_date,\n        )\n        return order_list",
  "class BaseStrategy:\n    def __init__(self):\n        pass\n\n    def get_risk_degree(self, date):\n        \"\"\"get_risk_degree\n        Return the proportion of your total value you will used in investment.\n        Dynamically risk_degree will result in Market timing\n        \"\"\"\n        # It will use 95% amount of your total value by default\n        return 0.95\n\n    def generate_order_list(self, score_series, current, trade_exchange, pred_date, trade_date):\n        \"\"\"\n        DO NOT directly change the state of current\n\n        Parameters\n        -----------\n        score_series : pd.Series\n            stock_id , score.\n        current : Position()\n            current state of position.\n            DO NOT directly change the state of current.\n        trade_exchange : Exchange()\n            trade exchange.\n        pred_date : pd.Timestamp\n            predict date.\n        trade_date : pd.Timestamp\n            trade date.\n        \"\"\"\n        pass\n\n    def update(self, score_series, pred_date, trade_date):\n        \"\"\"User can use this method to update strategy state each trade date.\n        Parameters\n        -----------\n        score_series : pd.Series\n            stock_id , score.\n        pred_date : pd.Timestamp\n            oredict date.\n        trade_date : pd.Timestamp\n            trade date.\n        \"\"\"\n        pass\n\n    def init(self, **kwargs):\n        \"\"\"Some strategy need to be initial after been implemented,\n        User can use this method to init his strategy with parameters needed.\n        \"\"\"\n        pass\n\n    def get_init_args_from_model(self, model, init_date):\n        \"\"\"\n        This method only be used in 'online' module, it will generate the *args to initial the strategy.\n            :param\n                mode : model used in 'online' module.\n        \"\"\"\n        return {}",
  "class StrategyWrapper:\n    \"\"\"\n    StrategyWrapper is a wrapper of another strategy.\n    By overriding some methods to make some changes on the basic strategy\n    Cost control and risk control will base on this class.\n    \"\"\"\n\n    def __init__(self, inner_strategy):\n        \"\"\"__init__\n\n        :param inner_strategy: set the inner strategy.\n        \"\"\"\n        self.inner_strategy = inner_strategy\n\n    def __getattr__(self, name):\n        \"\"\"__getattr__\n\n        :param name: If no implementation in this method. Call the method in the innter_strategy by default.\n        \"\"\"\n        return getattr(self.inner_strategy, name)",
  "class AdjustTimer:\n    \"\"\"AdjustTimer\n    Responsible for timing of position adjusting\n\n    This is designed as multiple inheritance mechanism due to:\n    - the is_adjust may need access to the internel state of a strategy.\n\n    - it can be reguard as a enhancement to the existing strategy.\n    \"\"\"\n\n    # adjust position in each trade date\n    def is_adjust(self, trade_date):\n        \"\"\"is_adjust\n        Return if the strategy can adjust positions on `trade_date`\n        Will normally be used in strategy do trading with trade frequency\n        \"\"\"\n        return True",
  "class ListAdjustTimer(AdjustTimer):\n    def __init__(self, adjust_dates=None):\n        \"\"\"__init__\n\n        :param adjust_dates: an iterable object, it will return a timelist for trading dates\n        \"\"\"\n        if adjust_dates is None:\n            # None indicates that all dates is OK for adjusting\n            self.adjust_dates = None\n        else:\n            self.adjust_dates = {pd.Timestamp(dt) for dt in adjust_dates}\n\n    def is_adjust(self, trade_date):\n        if self.adjust_dates is None:\n            return True\n        return pd.Timestamp(trade_date) in self.adjust_dates",
  "class WeightStrategyBase(BaseStrategy, AdjustTimer):\n    def __init__(self, order_generator_cls_or_obj=OrderGenWInteract, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if isinstance(order_generator_cls_or_obj, type):\n            self.order_generator = order_generator_cls_or_obj()\n        else:\n            self.order_generator = order_generator_cls_or_obj\n\n    def generate_target_weight_position(self, score, current, trade_date):\n        \"\"\"\n        Generate target position from score for this date and the current position.The cash is not considered in the position\n\n        Parameters\n        -----------\n        score : pd.Series\n            pred score for this trade date, index is stock_id, contain 'score' column.\n        current : Position()\n            current position.\n        trade_exchange : Exchange()\n        trade_date : pd.Timestamp\n            trade date.\n        \"\"\"\n        raise NotImplementedError()\n\n    def generate_order_list(self, score_series, current, trade_exchange, pred_date, trade_date):\n        \"\"\"\n        Parameters\n        -----------\n        score_series : pd.Seires\n            stock_id , score.\n        current : Position()\n            current of account.\n        trade_exchange : Exchange()\n            exchange.\n        trade_date : pd.Timestamp\n            date.\n        \"\"\"\n        # judge if to adjust\n        if not self.is_adjust(trade_date):\n            return []\n        # generate_order_list\n        # generate_target_weight_position() and generate_order_list_from_target_weight_position() to generate order_list\n        current_temp = copy.deepcopy(current)\n        target_weight_position = self.generate_target_weight_position(\n            score=score_series, current=current_temp, trade_date=trade_date\n        )\n\n        order_list = self.order_generator.generate_order_list_from_target_weight_position(\n            current=current_temp,\n            trade_exchange=trade_exchange,\n            risk_degree=self.get_risk_degree(trade_date),\n            target_weight_position=target_weight_position,\n            pred_date=pred_date,\n            trade_date=trade_date,\n        )\n        return order_list",
  "class TopkDropoutStrategy(BaseStrategy, ListAdjustTimer):\n    def __init__(\n        self,\n        topk,\n        n_drop,\n        method_sell=\"bottom\",\n        method_buy=\"top\",\n        risk_degree=0.95,\n        thresh=1,\n        hold_thresh=1,\n        only_tradable=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters\n        -----------\n        topk : int\n            the number of stocks in the portfolio.\n        n_drop : int\n            number of stocks to be replaced in each trading date.\n        method_sell : str\n            dropout method_sell, random/bottom.\n        method_buy : str\n            dropout method_buy, random/top.\n        risk_degree : float\n            position percentage of total value.\n        thresh : int\n            minimun holding days since last buy singal of the stock.\n        hold_thresh : int\n            minimum holding days\n            before sell stock , will check current.get_stock_count(order.stock_id) >= self.thresh.\n        only_tradable : bool\n            will the strategy only consider the tradable stock when buying and selling.\n            if only_tradable:\n                strategy will make buy sell decision without checking the tradable state of the stock.\n            else:\n                strategy will make decision with the tradable state of the stock info and avoid buy and sell them.\n        \"\"\"\n        super(TopkDropoutStrategy, self).__init__()\n        ListAdjustTimer.__init__(self, kwargs.get(\"adjust_dates\", None))\n        self.topk = topk\n        self.n_drop = n_drop\n        self.method_sell = method_sell\n        self.method_buy = method_buy\n        self.risk_degree = risk_degree\n        self.thresh = thresh\n        # self.stock_count['code'] will be the days the stock has been hold\n        # since last buy signal. This is designed for thresh\n        self.stock_count = {}\n\n        self.hold_thresh = hold_thresh\n        self.only_tradable = only_tradable\n\n    def get_risk_degree(self, date):\n        \"\"\"get_risk_degree\n        Return the proportion of your total value you will used in investment.\n        Dynamically risk_degree will result in Market timing.\n        \"\"\"\n        # It will use 95% amoutn of your total value by default\n        return self.risk_degree\n\n    def generate_order_list(self, score_series, current, trade_exchange, pred_date, trade_date):\n        \"\"\"\n        Gnererate order list according to score_series at trade_date, will not change current.\n\n        Parameters\n        -----------\n        score_series : pd.Series\n            stock_id , score.\n        current : Position()\n            current of account.\n        trade_exchange : Exchange()\n            exchange.\n        pred_date : pd.Timestamp\n            predict date.\n        trade_date : pd.Timestamp\n            trade date.\n        \"\"\"\n        if not self.is_adjust(trade_date):\n            return []\n\n        if self.only_tradable:\n            # If The strategy only consider tradable stock when make decision\n            # It needs following actions to filter stocks\n            def get_first_n(l, n, reverse=False):\n                cur_n = 0\n                res = []\n                for si in reversed(l) if reverse else l:\n                    if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date):\n                        res.append(si)\n                        cur_n += 1\n                        if cur_n >= n:\n                            break\n                return res[::-1] if reverse else res\n\n            def get_last_n(l, n):\n                return get_first_n(l, n, reverse=True)\n\n            def filter_stock(l):\n                return [si for si in l if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date)]\n\n        else:\n            # Otherwise, the stock will make decision with out the stock tradable info\n            def get_first_n(l, n):\n                return list(l)[:n]\n\n            def get_last_n(l, n):\n                return list(l)[-n:]\n\n            def filter_stock(l):\n                return l\n\n        current_temp = copy.deepcopy(current)\n        # generate order list for this adjust date\n        sell_order_list = []\n        buy_order_list = []\n        # load score\n        cash = current_temp.get_cash()\n        current_stock_list = current_temp.get_stock_list()\n        # last position (sorted by score)\n        last = score_series.reindex(current_stock_list).sort_values(ascending=False).index\n        # The new stocks today want to buy **at most**\n        if self.method_buy == \"top\":\n            today = get_first_n(\n                score_series[~score_series.index.isin(last)].sort_values(ascending=False).index,\n                self.n_drop + self.topk - len(last),\n            )\n        elif self.method_buy == \"random\":\n            topk_candi = get_first_n(score_series.sort_values(ascending=False).index, self.topk)\n            candi = list(filter(lambda x: x not in last, topk_candi))\n            n = self.n_drop + self.topk - len(last)\n            try:\n                today = np.random.choice(candi, n, replace=False)\n            except ValueError:\n                today = candi\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n        # combine(new stocks + last stocks),  we will drop stocks from this list\n        # In case of dropping higher score stock and buying lower score stock.\n        comb = score_series.reindex(last.union(pd.Index(today))).sort_values(ascending=False).index\n\n        # Get the stock list we really want to sell (After filtering the case that we sell high and buy low)\n        if self.method_sell == \"bottom\":\n            sell = last[last.isin(get_last_n(comb, self.n_drop))]\n        elif self.method_sell == \"random\":\n            candi = filter_stock(last)\n            try:\n                sell = pd.Index(np.random.choice(candi, self.n_drop, replace=False) if len(last) else [])\n            except ValueError:  #  No enough candidates\n                sell = candi\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n\n        # Get the stock list we really want to buy\n        buy = today[: len(sell) + self.topk - len(last)]\n\n        # buy singal: if a stock falls into topk, it appear in the buy_sinal\n        buy_signal = score_series.sort_values(ascending=False).iloc[: self.topk].index\n\n        for code in current_stock_list:\n            if not trade_exchange.is_stock_tradable(stock_id=code, trade_date=trade_date):\n                continue\n            if code in sell:\n                # check hold limit\n                if self.stock_count[code] < self.thresh or current_temp.get_stock_count(code) < self.hold_thresh:\n                    # can not sell this code\n                    # no buy signal, but the stock is kept\n                    self.stock_count[code] += 1\n                    continue\n                # sell order\n                sell_amount = current_temp.get_stock_amount(code=code)\n                sell_order = Order(\n                    stock_id=code,\n                    amount=sell_amount,\n                    trade_date=trade_date,\n                    direction=Order.SELL,  # 0 for sell, 1 for buy\n                    factor=trade_exchange.get_factor(code, trade_date),\n                )\n                # is order executable\n                if trade_exchange.check_order(sell_order):\n                    sell_order_list.append(sell_order)\n                    trade_val, trade_cost, trade_price = trade_exchange.deal_order(sell_order, position=current_temp)\n                    # update cash\n                    cash += trade_val - trade_cost\n                    # sold\n                    del self.stock_count[code]\n                else:\n                    # no buy signal, but the stock is kept\n                    self.stock_count[code] += 1\n            elif code in buy_signal:\n                # NOTE: This is different from the original version\n                # get new buy signal\n                # Only the stock fall in to topk will produce buy signal\n                self.stock_count[code] = 1\n            else:\n                self.stock_count[code] += 1\n        # buy new stock\n        # note the current has been changed\n        current_stock_list = current_temp.get_stock_list()\n        value = cash * self.risk_degree / len(buy) if len(buy) > 0 else 0\n\n        # open_cost should be considered in the real trading environment, while the backtest in evaluate.py does not\n        # consider it as the aim of demo is to accomplish same strategy as evaluate.py, so comment out this line\n        # value = value / (1+trade_exchange.open_cost) # set open_cost limit\n        for code in buy:\n            # check is stock suspended\n            if not trade_exchange.is_stock_tradable(stock_id=code, trade_date=trade_date):\n                continue\n            # buy order\n            buy_price = trade_exchange.get_deal_price(stock_id=code, trade_date=trade_date)\n            buy_amount = value / buy_price\n            factor = trade_exchange.quote[(code, trade_date)][\"$factor\"]\n            buy_amount = trade_exchange.round_amount_by_trade_unit(buy_amount, factor)\n            buy_order = Order(\n                stock_id=code,\n                amount=buy_amount,\n                trade_date=trade_date,\n                direction=Order.BUY,  # 1 for buy\n                factor=factor,\n            )\n            buy_order_list.append(buy_order)\n            self.stock_count[code] = 1\n        return sell_order_list + buy_order_list",
  "def __init__(self):\n        pass",
  "def get_risk_degree(self, date):\n        \"\"\"get_risk_degree\n        Return the proportion of your total value you will used in investment.\n        Dynamically risk_degree will result in Market timing\n        \"\"\"\n        # It will use 95% amount of your total value by default\n        return 0.95",
  "def generate_order_list(self, score_series, current, trade_exchange, pred_date, trade_date):\n        \"\"\"\n        DO NOT directly change the state of current\n\n        Parameters\n        -----------\n        score_series : pd.Series\n            stock_id , score.\n        current : Position()\n            current state of position.\n            DO NOT directly change the state of current.\n        trade_exchange : Exchange()\n            trade exchange.\n        pred_date : pd.Timestamp\n            predict date.\n        trade_date : pd.Timestamp\n            trade date.\n        \"\"\"\n        pass",
  "def update(self, score_series, pred_date, trade_date):\n        \"\"\"User can use this method to update strategy state each trade date.\n        Parameters\n        -----------\n        score_series : pd.Series\n            stock_id , score.\n        pred_date : pd.Timestamp\n            oredict date.\n        trade_date : pd.Timestamp\n            trade date.\n        \"\"\"\n        pass",
  "def init(self, **kwargs):\n        \"\"\"Some strategy need to be initial after been implemented,\n        User can use this method to init his strategy with parameters needed.\n        \"\"\"\n        pass",
  "def get_init_args_from_model(self, model, init_date):\n        \"\"\"\n        This method only be used in 'online' module, it will generate the *args to initial the strategy.\n            :param\n                mode : model used in 'online' module.\n        \"\"\"\n        return {}",
  "def __init__(self, inner_strategy):\n        \"\"\"__init__\n\n        :param inner_strategy: set the inner strategy.\n        \"\"\"\n        self.inner_strategy = inner_strategy",
  "def __getattr__(self, name):\n        \"\"\"__getattr__\n\n        :param name: If no implementation in this method. Call the method in the innter_strategy by default.\n        \"\"\"\n        return getattr(self.inner_strategy, name)",
  "def is_adjust(self, trade_date):\n        \"\"\"is_adjust\n        Return if the strategy can adjust positions on `trade_date`\n        Will normally be used in strategy do trading with trade frequency\n        \"\"\"\n        return True",
  "def __init__(self, adjust_dates=None):\n        \"\"\"__init__\n\n        :param adjust_dates: an iterable object, it will return a timelist for trading dates\n        \"\"\"\n        if adjust_dates is None:\n            # None indicates that all dates is OK for adjusting\n            self.adjust_dates = None\n        else:\n            self.adjust_dates = {pd.Timestamp(dt) for dt in adjust_dates}",
  "def is_adjust(self, trade_date):\n        if self.adjust_dates is None:\n            return True\n        return pd.Timestamp(trade_date) in self.adjust_dates",
  "def __init__(self, order_generator_cls_or_obj=OrderGenWInteract, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if isinstance(order_generator_cls_or_obj, type):\n            self.order_generator = order_generator_cls_or_obj()\n        else:\n            self.order_generator = order_generator_cls_or_obj",
  "def generate_target_weight_position(self, score, current, trade_date):\n        \"\"\"\n        Generate target position from score for this date and the current position.The cash is not considered in the position\n\n        Parameters\n        -----------\n        score : pd.Series\n            pred score for this trade date, index is stock_id, contain 'score' column.\n        current : Position()\n            current position.\n        trade_exchange : Exchange()\n        trade_date : pd.Timestamp\n            trade date.\n        \"\"\"\n        raise NotImplementedError()",
  "def generate_order_list(self, score_series, current, trade_exchange, pred_date, trade_date):\n        \"\"\"\n        Parameters\n        -----------\n        score_series : pd.Seires\n            stock_id , score.\n        current : Position()\n            current of account.\n        trade_exchange : Exchange()\n            exchange.\n        trade_date : pd.Timestamp\n            date.\n        \"\"\"\n        # judge if to adjust\n        if not self.is_adjust(trade_date):\n            return []\n        # generate_order_list\n        # generate_target_weight_position() and generate_order_list_from_target_weight_position() to generate order_list\n        current_temp = copy.deepcopy(current)\n        target_weight_position = self.generate_target_weight_position(\n            score=score_series, current=current_temp, trade_date=trade_date\n        )\n\n        order_list = self.order_generator.generate_order_list_from_target_weight_position(\n            current=current_temp,\n            trade_exchange=trade_exchange,\n            risk_degree=self.get_risk_degree(trade_date),\n            target_weight_position=target_weight_position,\n            pred_date=pred_date,\n            trade_date=trade_date,\n        )\n        return order_list",
  "def __init__(\n        self,\n        topk,\n        n_drop,\n        method_sell=\"bottom\",\n        method_buy=\"top\",\n        risk_degree=0.95,\n        thresh=1,\n        hold_thresh=1,\n        only_tradable=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters\n        -----------\n        topk : int\n            the number of stocks in the portfolio.\n        n_drop : int\n            number of stocks to be replaced in each trading date.\n        method_sell : str\n            dropout method_sell, random/bottom.\n        method_buy : str\n            dropout method_buy, random/top.\n        risk_degree : float\n            position percentage of total value.\n        thresh : int\n            minimun holding days since last buy singal of the stock.\n        hold_thresh : int\n            minimum holding days\n            before sell stock , will check current.get_stock_count(order.stock_id) >= self.thresh.\n        only_tradable : bool\n            will the strategy only consider the tradable stock when buying and selling.\n            if only_tradable:\n                strategy will make buy sell decision without checking the tradable state of the stock.\n            else:\n                strategy will make decision with the tradable state of the stock info and avoid buy and sell them.\n        \"\"\"\n        super(TopkDropoutStrategy, self).__init__()\n        ListAdjustTimer.__init__(self, kwargs.get(\"adjust_dates\", None))\n        self.topk = topk\n        self.n_drop = n_drop\n        self.method_sell = method_sell\n        self.method_buy = method_buy\n        self.risk_degree = risk_degree\n        self.thresh = thresh\n        # self.stock_count['code'] will be the days the stock has been hold\n        # since last buy signal. This is designed for thresh\n        self.stock_count = {}\n\n        self.hold_thresh = hold_thresh\n        self.only_tradable = only_tradable",
  "def get_risk_degree(self, date):\n        \"\"\"get_risk_degree\n        Return the proportion of your total value you will used in investment.\n        Dynamically risk_degree will result in Market timing.\n        \"\"\"\n        # It will use 95% amoutn of your total value by default\n        return self.risk_degree",
  "def generate_order_list(self, score_series, current, trade_exchange, pred_date, trade_date):\n        \"\"\"\n        Gnererate order list according to score_series at trade_date, will not change current.\n\n        Parameters\n        -----------\n        score_series : pd.Series\n            stock_id , score.\n        current : Position()\n            current of account.\n        trade_exchange : Exchange()\n            exchange.\n        pred_date : pd.Timestamp\n            predict date.\n        trade_date : pd.Timestamp\n            trade date.\n        \"\"\"\n        if not self.is_adjust(trade_date):\n            return []\n\n        if self.only_tradable:\n            # If The strategy only consider tradable stock when make decision\n            # It needs following actions to filter stocks\n            def get_first_n(l, n, reverse=False):\n                cur_n = 0\n                res = []\n                for si in reversed(l) if reverse else l:\n                    if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date):\n                        res.append(si)\n                        cur_n += 1\n                        if cur_n >= n:\n                            break\n                return res[::-1] if reverse else res\n\n            def get_last_n(l, n):\n                return get_first_n(l, n, reverse=True)\n\n            def filter_stock(l):\n                return [si for si in l if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date)]\n\n        else:\n            # Otherwise, the stock will make decision with out the stock tradable info\n            def get_first_n(l, n):\n                return list(l)[:n]\n\n            def get_last_n(l, n):\n                return list(l)[-n:]\n\n            def filter_stock(l):\n                return l\n\n        current_temp = copy.deepcopy(current)\n        # generate order list for this adjust date\n        sell_order_list = []\n        buy_order_list = []\n        # load score\n        cash = current_temp.get_cash()\n        current_stock_list = current_temp.get_stock_list()\n        # last position (sorted by score)\n        last = score_series.reindex(current_stock_list).sort_values(ascending=False).index\n        # The new stocks today want to buy **at most**\n        if self.method_buy == \"top\":\n            today = get_first_n(\n                score_series[~score_series.index.isin(last)].sort_values(ascending=False).index,\n                self.n_drop + self.topk - len(last),\n            )\n        elif self.method_buy == \"random\":\n            topk_candi = get_first_n(score_series.sort_values(ascending=False).index, self.topk)\n            candi = list(filter(lambda x: x not in last, topk_candi))\n            n = self.n_drop + self.topk - len(last)\n            try:\n                today = np.random.choice(candi, n, replace=False)\n            except ValueError:\n                today = candi\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n        # combine(new stocks + last stocks),  we will drop stocks from this list\n        # In case of dropping higher score stock and buying lower score stock.\n        comb = score_series.reindex(last.union(pd.Index(today))).sort_values(ascending=False).index\n\n        # Get the stock list we really want to sell (After filtering the case that we sell high and buy low)\n        if self.method_sell == \"bottom\":\n            sell = last[last.isin(get_last_n(comb, self.n_drop))]\n        elif self.method_sell == \"random\":\n            candi = filter_stock(last)\n            try:\n                sell = pd.Index(np.random.choice(candi, self.n_drop, replace=False) if len(last) else [])\n            except ValueError:  #  No enough candidates\n                sell = candi\n        else:\n            raise NotImplementedError(f\"This type of input is not supported\")\n\n        # Get the stock list we really want to buy\n        buy = today[: len(sell) + self.topk - len(last)]\n\n        # buy singal: if a stock falls into topk, it appear in the buy_sinal\n        buy_signal = score_series.sort_values(ascending=False).iloc[: self.topk].index\n\n        for code in current_stock_list:\n            if not trade_exchange.is_stock_tradable(stock_id=code, trade_date=trade_date):\n                continue\n            if code in sell:\n                # check hold limit\n                if self.stock_count[code] < self.thresh or current_temp.get_stock_count(code) < self.hold_thresh:\n                    # can not sell this code\n                    # no buy signal, but the stock is kept\n                    self.stock_count[code] += 1\n                    continue\n                # sell order\n                sell_amount = current_temp.get_stock_amount(code=code)\n                sell_order = Order(\n                    stock_id=code,\n                    amount=sell_amount,\n                    trade_date=trade_date,\n                    direction=Order.SELL,  # 0 for sell, 1 for buy\n                    factor=trade_exchange.get_factor(code, trade_date),\n                )\n                # is order executable\n                if trade_exchange.check_order(sell_order):\n                    sell_order_list.append(sell_order)\n                    trade_val, trade_cost, trade_price = trade_exchange.deal_order(sell_order, position=current_temp)\n                    # update cash\n                    cash += trade_val - trade_cost\n                    # sold\n                    del self.stock_count[code]\n                else:\n                    # no buy signal, but the stock is kept\n                    self.stock_count[code] += 1\n            elif code in buy_signal:\n                # NOTE: This is different from the original version\n                # get new buy signal\n                # Only the stock fall in to topk will produce buy signal\n                self.stock_count[code] = 1\n            else:\n                self.stock_count[code] += 1\n        # buy new stock\n        # note the current has been changed\n        current_stock_list = current_temp.get_stock_list()\n        value = cash * self.risk_degree / len(buy) if len(buy) > 0 else 0\n\n        # open_cost should be considered in the real trading environment, while the backtest in evaluate.py does not\n        # consider it as the aim of demo is to accomplish same strategy as evaluate.py, so comment out this line\n        # value = value / (1+trade_exchange.open_cost) # set open_cost limit\n        for code in buy:\n            # check is stock suspended\n            if not trade_exchange.is_stock_tradable(stock_id=code, trade_date=trade_date):\n                continue\n            # buy order\n            buy_price = trade_exchange.get_deal_price(stock_id=code, trade_date=trade_date)\n            buy_amount = value / buy_price\n            factor = trade_exchange.quote[(code, trade_date)][\"$factor\"]\n            buy_amount = trade_exchange.round_amount_by_trade_unit(buy_amount, factor)\n            buy_order = Order(\n                stock_id=code,\n                amount=buy_amount,\n                trade_date=trade_date,\n                direction=Order.BUY,  # 1 for buy\n                factor=factor,\n            )\n            buy_order_list.append(buy_order)\n            self.stock_count[code] = 1\n        return sell_order_list + buy_order_list",
  "def get_first_n(l, n, reverse=False):\n                cur_n = 0\n                res = []\n                for si in reversed(l) if reverse else l:\n                    if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date):\n                        res.append(si)\n                        cur_n += 1\n                        if cur_n >= n:\n                            break\n                return res[::-1] if reverse else res",
  "def get_last_n(l, n):\n                return get_first_n(l, n, reverse=True)",
  "def filter_stock(l):\n                return [si for si in l if trade_exchange.is_stock_tradable(stock_id=si, trade_date=trade_date)]",
  "def get_first_n(l, n):\n                return list(l)[:n]",
  "def get_last_n(l, n):\n                return list(l)[-n:]",
  "def filter_stock(l):\n                return l",
  "class SoftTopkStrategy(WeightStrategyBase):\n    def __init__(self, topk, max_sold_weight=1.0, risk_degree=0.95, buy_method=\"first_fill\"):\n        \"\"\"Parameter\n        topk : int\n            top-N stocks to buy\n        risk_degree : float\n            position percentage of total value\n            buy_method :\n                rank_fill: assign the weight stocks that rank high first(1/topk max)\n                average_fill: assign the weight to the stocks rank high averagely.\n        \"\"\"\n        super().__init__()\n        self.topk = topk\n        self.max_sold_weight = max_sold_weight\n        self.risk_degree = risk_degree\n        self.buy_method = buy_method\n\n    def get_risk_degree(self, date):\n        \"\"\"get_risk_degree\n        Return the proportion of your total value you will used in investment.\n        Dynamically risk_degree will result in Market timing\n        \"\"\"\n        # It will use 95% amoutn of your total value by default\n        return self.risk_degree\n\n    def generate_target_weight_position(self, score, current, trade_date):\n        \"\"\"Parameter:\n        score : pred score for this trade date, pd.Series, index is stock_id, contain 'score' column\n        current : current position, use Position() class\n        trade_date : trade date\n        generate target position from score for this date and the current position\n        The cache is not considered in the position\n        \"\"\"\n        # TODO:\n        # If the current stock list is more than topk(eg. The weights are modified\n        # by risk control), the weight will not be handled correctly.\n        buy_signal_stocks = set(score.sort_values(ascending=False).iloc[: self.topk].index)\n        cur_stock_weight = current.get_stock_weight_dict(only_stock=True)\n\n        if len(cur_stock_weight) == 0:\n            final_stock_weight = {code: 1 / self.topk for code in buy_signal_stocks}\n        else:\n            final_stock_weight = copy.deepcopy(cur_stock_weight)\n            sold_stock_weight = 0.0\n            for stock_id in final_stock_weight:\n                if stock_id not in buy_signal_stocks:\n                    sw = min(self.max_sold_weight, final_stock_weight[stock_id])\n                    sold_stock_weight += sw\n                    final_stock_weight[stock_id] -= sw\n            if self.buy_method == \"first_fill\":\n                for stock_id in buy_signal_stocks:\n                    add_weight = min(\n                        max(1 / self.topk - final_stock_weight.get(stock_id, 0), 0.0),\n                        sold_stock_weight,\n                    )\n                    final_stock_weight[stock_id] = final_stock_weight.get(stock_id, 0.0) + add_weight\n                    sold_stock_weight -= add_weight\n            elif self.buy_method == \"average_fill\":\n                for stock_id in buy_signal_stocks:\n                    final_stock_weight[stock_id] = final_stock_weight.get(stock_id, 0.0) + sold_stock_weight / len(\n                        buy_signal_stocks\n                    )\n            else:\n                raise ValueError(\"Buy method not found\")\n        return final_stock_weight",
  "def __init__(self, topk, max_sold_weight=1.0, risk_degree=0.95, buy_method=\"first_fill\"):\n        \"\"\"Parameter\n        topk : int\n            top-N stocks to buy\n        risk_degree : float\n            position percentage of total value\n            buy_method :\n                rank_fill: assign the weight stocks that rank high first(1/topk max)\n                average_fill: assign the weight to the stocks rank high averagely.\n        \"\"\"\n        super().__init__()\n        self.topk = topk\n        self.max_sold_weight = max_sold_weight\n        self.risk_degree = risk_degree\n        self.buy_method = buy_method",
  "def get_risk_degree(self, date):\n        \"\"\"get_risk_degree\n        Return the proportion of your total value you will used in investment.\n        Dynamically risk_degree will result in Market timing\n        \"\"\"\n        # It will use 95% amoutn of your total value by default\n        return self.risk_degree",
  "def generate_target_weight_position(self, score, current, trade_date):\n        \"\"\"Parameter:\n        score : pred score for this trade date, pd.Series, index is stock_id, contain 'score' column\n        current : current position, use Position() class\n        trade_date : trade date\n        generate target position from score for this date and the current position\n        The cache is not considered in the position\n        \"\"\"\n        # TODO:\n        # If the current stock list is more than topk(eg. The weights are modified\n        # by risk control), the weight will not be handled correctly.\n        buy_signal_stocks = set(score.sort_values(ascending=False).iloc[: self.topk].index)\n        cur_stock_weight = current.get_stock_weight_dict(only_stock=True)\n\n        if len(cur_stock_weight) == 0:\n            final_stock_weight = {code: 1 / self.topk for code in buy_signal_stocks}\n        else:\n            final_stock_weight = copy.deepcopy(cur_stock_weight)\n            sold_stock_weight = 0.0\n            for stock_id in final_stock_weight:\n                if stock_id not in buy_signal_stocks:\n                    sw = min(self.max_sold_weight, final_stock_weight[stock_id])\n                    sold_stock_weight += sw\n                    final_stock_weight[stock_id] -= sw\n            if self.buy_method == \"first_fill\":\n                for stock_id in buy_signal_stocks:\n                    add_weight = min(\n                        max(1 / self.topk - final_stock_weight.get(stock_id, 0), 0.0),\n                        sold_stock_weight,\n                    )\n                    final_stock_weight[stock_id] = final_stock_weight.get(stock_id, 0.0) + add_weight\n                    sold_stock_weight -= add_weight\n            elif self.buy_method == \"average_fill\":\n                for stock_id in buy_signal_stocks:\n                    final_stock_weight[stock_id] = final_stock_weight.get(stock_id, 0.0) + sold_stock_weight / len(\n                        buy_signal_stocks\n                    )\n            else:\n                raise ValueError(\"Buy method not found\")\n        return final_stock_weight",
  "class SignalMseRecord(SignalRecord):\n    \"\"\"\n    This is the Signal MSE Record class that computes the mean squared error (MSE).\n    This class inherits the ``SignalMseRecord`` class.\n    \"\"\"\n\n    artifact_path = \"sig_analysis\"\n\n    def __init__(self, recorder, **kwargs):\n        super().__init__(recorder=recorder, **kwargs)\n\n    def generate(self, **kwargs):\n        try:\n            self.check(parent=True)\n        except FileExistsError:\n            super().generate()\n\n        pred = self.load(\"pred.pkl\")\n        label = self.load(\"label.pkl\")\n        masks = ~np.isnan(label.values)\n        mse = mean_squared_error(pred.values[masks], label[masks])\n        metrics = {\"MSE\": mse, \"RMSE\": np.sqrt(mse)}\n        objects = {\"mse.pkl\": mse, \"rmse.pkl\": np.sqrt(mse)}\n        self.recorder.log_metrics(**metrics)\n        self.recorder.save_objects(**objects, artifact_path=self.get_path())\n        pprint(metrics)\n\n    def list(self):\n        paths = [self.get_path(\"mse.pkl\"), self.get_path(\"rmse.pkl\")]\n        return paths",
  "def __init__(self, recorder, **kwargs):\n        super().__init__(recorder=recorder, **kwargs)",
  "def generate(self, **kwargs):\n        try:\n            self.check(parent=True)\n        except FileExistsError:\n            super().generate()\n\n        pred = self.load(\"pred.pkl\")\n        label = self.load(\"label.pkl\")\n        masks = ~np.isnan(label.values)\n        mse = mean_squared_error(pred.values[masks], label[masks])\n        metrics = {\"MSE\": mse, \"RMSE\": np.sqrt(mse)}\n        objects = {\"mse.pkl\": mse, \"rmse.pkl\": np.sqrt(mse)}\n        self.recorder.log_metrics(**metrics)\n        self.recorder.save_objects(**objects, artifact_path=self.get_path())\n        pprint(metrics)",
  "def list(self):\n        paths = [self.get_path(\"mse.pkl\"), self.get_path(\"rmse.pkl\")]\n        return paths",
  "def check_transform_proc(proc_l, fit_start_time, fit_end_time):\n    new_l = []\n    for p in proc_l:\n        if not isinstance(p, Processor):\n            klass, pkwargs = get_cls_kwargs(p, processor_module)\n            args = getfullargspec(klass).args\n            if \"fit_start_time\" in args and \"fit_end_time\" in args:\n                assert (\n                    fit_start_time is not None and fit_end_time is not None\n                ), \"Make sure `fit_start_time` and `fit_end_time` are not None.\"\n                pkwargs.update(\n                    {\n                        \"fit_start_time\": fit_start_time,\n                        \"fit_end_time\": fit_end_time,\n                    }\n                )\n            new_l.append({\"class\": klass.__name__, \"kwargs\": pkwargs})\n        else:\n            new_l.append(p)\n    return new_l",
  "class Alpha360(DataHandlerLP):\n    def __init__(\n        self,\n        instruments=\"csi500\",\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        infer_processors=_DEFAULT_INFER_PROCESSORS,\n        learn_processors=_DEFAULT_LEARN_PROCESSORS,\n        fit_start_time=None,\n        fit_end_time=None,\n        filter_pipe=None,\n        **kwargs,\n    ):\n        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)\n        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)\n\n        data_loader = {\n            \"class\": \"QlibDataLoader\",\n            \"kwargs\": {\n                \"config\": {\n                    \"feature\": self.get_feature_config(),\n                    \"label\": kwargs.get(\"label\", self.get_label_config()),\n                },\n                \"filter_pipe\": filter_pipe,\n                \"freq\": freq,\n            },\n        }\n\n        super().__init__(\n            instruments=instruments,\n            start_time=start_time,\n            end_time=end_time,\n            data_loader=data_loader,\n            learn_processors=learn_processors,\n            infer_processors=infer_processors,\n        )\n\n    def get_label_config(self):\n        return ([\"Ref($close, -2)/Ref($close, -1) - 1\"], [\"LABEL0\"])\n\n    def get_feature_config(self):\n\n        fields = []\n        names = []\n\n        for i in range(59, 0, -1):\n            fields += [\"Ref($close, %d)/$close\" % (i)]\n            names += [\"CLOSE%d\" % (i)]\n        fields += [\"$close/$close\"]\n        names += [\"CLOSE0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($open, %d)/$close\" % (i)]\n            names += [\"OPEN%d\" % (i)]\n        fields += [\"$open/$close\"]\n        names += [\"OPEN0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($high, %d)/$close\" % (i)]\n            names += [\"HIGH%d\" % (i)]\n        fields += [\"$high/$close\"]\n        names += [\"HIGH0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($low, %d)/$close\" % (i)]\n            names += [\"LOW%d\" % (i)]\n        fields += [\"$low/$close\"]\n        names += [\"LOW0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($vwap, %d)/$close\" % (i)]\n            names += [\"VWAP%d\" % (i)]\n        fields += [\"$vwap/$close\"]\n        names += [\"VWAP0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($volume, %d)/$volume\" % (i)]\n            names += [\"VOLUME%d\" % (i)]\n        fields += [\"$volume/$volume\"]\n        names += [\"VOLUME0\"]\n\n        return fields, names",
  "class Alpha360vwap(Alpha360):\n    def get_label_config(self):\n        return ([\"Ref($vwap, -2)/Ref($vwap, -1) - 1\"], [\"LABEL0\"])",
  "class Alpha158(DataHandlerLP):\n    def __init__(\n        self,\n        instruments=\"csi500\",\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        infer_processors=[],\n        learn_processors=_DEFAULT_LEARN_PROCESSORS,\n        fit_start_time=None,\n        fit_end_time=None,\n        process_type=DataHandlerLP.PTYPE_A,\n        filter_pipe=None,\n        **kwargs,\n    ):\n        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)\n        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)\n\n        data_loader = {\n            \"class\": \"QlibDataLoader\",\n            \"kwargs\": {\n                \"config\": {\n                    \"feature\": self.get_feature_config(),\n                    \"label\": kwargs.get(\"label\", self.get_label_config()),\n                },\n                \"filter_pipe\": filter_pipe,\n                \"freq\": freq,\n            },\n        }\n        super().__init__(\n            instruments=instruments,\n            start_time=start_time,\n            end_time=end_time,\n            data_loader=data_loader,\n            infer_processors=infer_processors,\n            learn_processors=learn_processors,\n            process_type=process_type,\n        )\n\n    def get_feature_config(self):\n        conf = {\n            \"kbar\": {},\n            \"price\": {\n                \"windows\": [0],\n                \"feature\": [\"OPEN\", \"HIGH\", \"LOW\", \"VWAP\"],\n            },\n            \"rolling\": {},\n        }\n        return self.parse_config_to_fields(conf)\n\n    def get_label_config(self):\n        return ([\"Ref($close, -2)/Ref($close, -1) - 1\"], [\"LABEL0\"])\n\n    @staticmethod\n    def parse_config_to_fields(config):\n        \"\"\"create factors from config\n\n        config = {\n            'kbar': {}, # whether to use some hard-code kbar features\n            'price': { # whether to use raw price features\n                'windows': [0, 1, 2, 3, 4], # use price at n days ago\n                'feature': ['OPEN', 'HIGH', 'LOW'] # which price field to use\n            },\n            'volume': { # whether to use raw volume features\n                'windows': [0, 1, 2, 3, 4], # use volume at n days ago\n            },\n            'rolling': { # whether to use rolling operator based features\n                'windows': [5, 10, 20, 30, 60], # rolling windows size\n                'include': ['ROC', 'MA', 'STD'], # rolling operator to use\n                #if include is None we will use default operators\n                'exclude': ['RANK'], # rolling operator not to use\n            }\n        }\n        \"\"\"\n        fields = []\n        names = []\n        if \"kbar\" in config:\n            fields += [\n                \"($close-$open)/$open\",\n                \"($high-$low)/$open\",\n                \"($close-$open)/($high-$low+1e-12)\",\n                \"($high-Greater($open, $close))/$open\",\n                \"($high-Greater($open, $close))/($high-$low+1e-12)\",\n                \"(Less($open, $close)-$low)/$open\",\n                \"(Less($open, $close)-$low)/($high-$low+1e-12)\",\n                \"(2*$close-$high-$low)/$open\",\n                \"(2*$close-$high-$low)/($high-$low+1e-12)\",\n            ]\n            names += [\n                \"KMID\",\n                \"KLEN\",\n                \"KMID2\",\n                \"KUP\",\n                \"KUP2\",\n                \"KLOW\",\n                \"KLOW2\",\n                \"KSFT\",\n                \"KSFT2\",\n            ]\n        if \"price\" in config:\n            windows = config[\"price\"].get(\"windows\", range(5))\n            feature = config[\"price\"].get(\"feature\", [\"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"VWAP\"])\n            for field in feature:\n                field = field.lower()\n                fields += [\"Ref($%s, %d)/$close\" % (field, d) if d != 0 else \"$%s/$close\" % field for d in windows]\n                names += [field.upper() + str(d) for d in windows]\n        if \"volume\" in config:\n            windows = config[\"volume\"].get(\"windows\", range(5))\n            fields += [\"Ref($volume, %d)/$volume\" % d if d != 0 else \"$volume/$volume\" for d in windows]\n            names += [\"VOLUME\" + str(d) for d in windows]\n        if \"rolling\" in config:\n            windows = config[\"rolling\"].get(\"windows\", [5, 10, 20, 30, 60])\n            include = config[\"rolling\"].get(\"include\", None)\n            exclude = config[\"rolling\"].get(\"exclude\", [])\n            # `exclude` in dataset config unnecessary filed\n            # `include` in dataset config necessary field\n            use = lambda x: x not in exclude and (include is None or x in include)\n            if use(\"ROC\"):\n                fields += [\"Ref($close, %d)/$close\" % d for d in windows]\n                names += [\"ROC%d\" % d for d in windows]\n            if use(\"MA\"):\n                fields += [\"Mean($close, %d)/$close\" % d for d in windows]\n                names += [\"MA%d\" % d for d in windows]\n            if use(\"STD\"):\n                fields += [\"Std($close, %d)/$close\" % d for d in windows]\n                names += [\"STD%d\" % d for d in windows]\n            if use(\"BETA\"):\n                fields += [\"Slope($close, %d)/$close\" % d for d in windows]\n                names += [\"BETA%d\" % d for d in windows]\n            if use(\"RSQR\"):\n                fields += [\"Rsquare($close, %d)\" % d for d in windows]\n                names += [\"RSQR%d\" % d for d in windows]\n            if use(\"RESI\"):\n                fields += [\"Resi($close, %d)/$close\" % d for d in windows]\n                names += [\"RESI%d\" % d for d in windows]\n            if use(\"MAX\"):\n                fields += [\"Max($high, %d)/$close\" % d for d in windows]\n                names += [\"MAX%d\" % d for d in windows]\n            if use(\"LOW\"):\n                fields += [\"Min($low, %d)/$close\" % d for d in windows]\n                names += [\"MIN%d\" % d for d in windows]\n            if use(\"QTLU\"):\n                fields += [\"Quantile($close, %d, 0.8)/$close\" % d for d in windows]\n                names += [\"QTLU%d\" % d for d in windows]\n            if use(\"QTLD\"):\n                fields += [\"Quantile($close, %d, 0.2)/$close\" % d for d in windows]\n                names += [\"QTLD%d\" % d for d in windows]\n            if use(\"RANK\"):\n                fields += [\"Rank($close, %d)\" % d for d in windows]\n                names += [\"RANK%d\" % d for d in windows]\n            if use(\"RSV\"):\n                fields += [\"($close-Min($low, %d))/(Max($high, %d)-Min($low, %d)+1e-12)\" % (d, d, d) for d in windows]\n                names += [\"RSV%d\" % d for d in windows]\n            if use(\"IMAX\"):\n                fields += [\"IdxMax($high, %d)/%d\" % (d, d) for d in windows]\n                names += [\"IMAX%d\" % d for d in windows]\n            if use(\"IMIN\"):\n                fields += [\"IdxMin($low, %d)/%d\" % (d, d) for d in windows]\n                names += [\"IMIN%d\" % d for d in windows]\n            if use(\"IMXD\"):\n                fields += [\"(IdxMax($high, %d)-IdxMin($low, %d))/%d\" % (d, d, d) for d in windows]\n                names += [\"IMXD%d\" % d for d in windows]\n            if use(\"CORR\"):\n                fields += [\"Corr($close, Log($volume+1), %d)\" % d for d in windows]\n                names += [\"CORR%d\" % d for d in windows]\n            if use(\"CORD\"):\n                fields += [\"Corr($close/Ref($close,1), Log($volume/Ref($volume, 1)+1), %d)\" % d for d in windows]\n                names += [\"CORD%d\" % d for d in windows]\n            if use(\"CNTP\"):\n                fields += [\"Mean($close>Ref($close, 1), %d)\" % d for d in windows]\n                names += [\"CNTP%d\" % d for d in windows]\n            if use(\"CNTN\"):\n                fields += [\"Mean($close<Ref($close, 1), %d)\" % d for d in windows]\n                names += [\"CNTN%d\" % d for d in windows]\n            if use(\"CNTD\"):\n                fields += [\"Mean($close>Ref($close, 1), %d)-Mean($close<Ref($close, 1), %d)\" % (d, d) for d in windows]\n                names += [\"CNTD%d\" % d for d in windows]\n            if use(\"SUMP\"):\n                fields += [\n                    \"Sum(Greater($close-Ref($close, 1), 0), %d)/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)\" % (d, d)\n                    for d in windows\n                ]\n                names += [\"SUMP%d\" % d for d in windows]\n            if use(\"SUMN\"):\n                fields += [\n                    \"Sum(Greater(Ref($close, 1)-$close, 0), %d)/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)\" % (d, d)\n                    for d in windows\n                ]\n                names += [\"SUMN%d\" % d for d in windows]\n            if use(\"SUMD\"):\n                fields += [\n                    \"(Sum(Greater($close-Ref($close, 1), 0), %d)-Sum(Greater(Ref($close, 1)-$close, 0), %d))\"\n                    \"/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)\" % (d, d, d)\n                    for d in windows\n                ]\n                names += [\"SUMD%d\" % d for d in windows]\n            if use(\"VMA\"):\n                fields += [\"Mean($volume, %d)/($volume+1e-12)\" % d for d in windows]\n                names += [\"VMA%d\" % d for d in windows]\n            if use(\"VSTD\"):\n                fields += [\"Std($volume, %d)/($volume+1e-12)\" % d for d in windows]\n                names += [\"VSTD%d\" % d for d in windows]\n            if use(\"WVMA\"):\n                fields += [\n                    \"Std(Abs($close/Ref($close, 1)-1)*$volume, %d)/(Mean(Abs($close/Ref($close, 1)-1)*$volume, %d)+1e-12)\"\n                    % (d, d)\n                    for d in windows\n                ]\n                names += [\"WVMA%d\" % d for d in windows]\n            if use(\"VSUMP\"):\n                fields += [\n                    \"Sum(Greater($volume-Ref($volume, 1), 0), %d)/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)\"\n                    % (d, d)\n                    for d in windows\n                ]\n                names += [\"VSUMP%d\" % d for d in windows]\n            if use(\"VSUMN\"):\n                fields += [\n                    \"Sum(Greater(Ref($volume, 1)-$volume, 0), %d)/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)\"\n                    % (d, d)\n                    for d in windows\n                ]\n                names += [\"VSUMN%d\" % d for d in windows]\n            if use(\"VSUMD\"):\n                fields += [\n                    \"(Sum(Greater($volume-Ref($volume, 1), 0), %d)-Sum(Greater(Ref($volume, 1)-$volume, 0), %d))\"\n                    \"/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)\" % (d, d, d)\n                    for d in windows\n                ]\n                names += [\"VSUMD%d\" % d for d in windows]\n\n        return fields, names",
  "class Alpha158vwap(Alpha158):\n    def get_label_config(self):\n        return ([\"Ref($vwap, -2)/Ref($vwap, -1) - 1\"], [\"LABEL0\"])",
  "def __init__(\n        self,\n        instruments=\"csi500\",\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        infer_processors=_DEFAULT_INFER_PROCESSORS,\n        learn_processors=_DEFAULT_LEARN_PROCESSORS,\n        fit_start_time=None,\n        fit_end_time=None,\n        filter_pipe=None,\n        **kwargs,\n    ):\n        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)\n        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)\n\n        data_loader = {\n            \"class\": \"QlibDataLoader\",\n            \"kwargs\": {\n                \"config\": {\n                    \"feature\": self.get_feature_config(),\n                    \"label\": kwargs.get(\"label\", self.get_label_config()),\n                },\n                \"filter_pipe\": filter_pipe,\n                \"freq\": freq,\n            },\n        }\n\n        super().__init__(\n            instruments=instruments,\n            start_time=start_time,\n            end_time=end_time,\n            data_loader=data_loader,\n            learn_processors=learn_processors,\n            infer_processors=infer_processors,\n        )",
  "def get_label_config(self):\n        return ([\"Ref($close, -2)/Ref($close, -1) - 1\"], [\"LABEL0\"])",
  "def get_feature_config(self):\n\n        fields = []\n        names = []\n\n        for i in range(59, 0, -1):\n            fields += [\"Ref($close, %d)/$close\" % (i)]\n            names += [\"CLOSE%d\" % (i)]\n        fields += [\"$close/$close\"]\n        names += [\"CLOSE0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($open, %d)/$close\" % (i)]\n            names += [\"OPEN%d\" % (i)]\n        fields += [\"$open/$close\"]\n        names += [\"OPEN0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($high, %d)/$close\" % (i)]\n            names += [\"HIGH%d\" % (i)]\n        fields += [\"$high/$close\"]\n        names += [\"HIGH0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($low, %d)/$close\" % (i)]\n            names += [\"LOW%d\" % (i)]\n        fields += [\"$low/$close\"]\n        names += [\"LOW0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($vwap, %d)/$close\" % (i)]\n            names += [\"VWAP%d\" % (i)]\n        fields += [\"$vwap/$close\"]\n        names += [\"VWAP0\"]\n        for i in range(59, 0, -1):\n            fields += [\"Ref($volume, %d)/$volume\" % (i)]\n            names += [\"VOLUME%d\" % (i)]\n        fields += [\"$volume/$volume\"]\n        names += [\"VOLUME0\"]\n\n        return fields, names",
  "def get_label_config(self):\n        return ([\"Ref($vwap, -2)/Ref($vwap, -1) - 1\"], [\"LABEL0\"])",
  "def __init__(\n        self,\n        instruments=\"csi500\",\n        start_time=None,\n        end_time=None,\n        freq=\"day\",\n        infer_processors=[],\n        learn_processors=_DEFAULT_LEARN_PROCESSORS,\n        fit_start_time=None,\n        fit_end_time=None,\n        process_type=DataHandlerLP.PTYPE_A,\n        filter_pipe=None,\n        **kwargs,\n    ):\n        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)\n        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)\n\n        data_loader = {\n            \"class\": \"QlibDataLoader\",\n            \"kwargs\": {\n                \"config\": {\n                    \"feature\": self.get_feature_config(),\n                    \"label\": kwargs.get(\"label\", self.get_label_config()),\n                },\n                \"filter_pipe\": filter_pipe,\n                \"freq\": freq,\n            },\n        }\n        super().__init__(\n            instruments=instruments,\n            start_time=start_time,\n            end_time=end_time,\n            data_loader=data_loader,\n            infer_processors=infer_processors,\n            learn_processors=learn_processors,\n            process_type=process_type,\n        )",
  "def get_feature_config(self):\n        conf = {\n            \"kbar\": {},\n            \"price\": {\n                \"windows\": [0],\n                \"feature\": [\"OPEN\", \"HIGH\", \"LOW\", \"VWAP\"],\n            },\n            \"rolling\": {},\n        }\n        return self.parse_config_to_fields(conf)",
  "def get_label_config(self):\n        return ([\"Ref($close, -2)/Ref($close, -1) - 1\"], [\"LABEL0\"])",
  "def parse_config_to_fields(config):\n        \"\"\"create factors from config\n\n        config = {\n            'kbar': {}, # whether to use some hard-code kbar features\n            'price': { # whether to use raw price features\n                'windows': [0, 1, 2, 3, 4], # use price at n days ago\n                'feature': ['OPEN', 'HIGH', 'LOW'] # which price field to use\n            },\n            'volume': { # whether to use raw volume features\n                'windows': [0, 1, 2, 3, 4], # use volume at n days ago\n            },\n            'rolling': { # whether to use rolling operator based features\n                'windows': [5, 10, 20, 30, 60], # rolling windows size\n                'include': ['ROC', 'MA', 'STD'], # rolling operator to use\n                #if include is None we will use default operators\n                'exclude': ['RANK'], # rolling operator not to use\n            }\n        }\n        \"\"\"\n        fields = []\n        names = []\n        if \"kbar\" in config:\n            fields += [\n                \"($close-$open)/$open\",\n                \"($high-$low)/$open\",\n                \"($close-$open)/($high-$low+1e-12)\",\n                \"($high-Greater($open, $close))/$open\",\n                \"($high-Greater($open, $close))/($high-$low+1e-12)\",\n                \"(Less($open, $close)-$low)/$open\",\n                \"(Less($open, $close)-$low)/($high-$low+1e-12)\",\n                \"(2*$close-$high-$low)/$open\",\n                \"(2*$close-$high-$low)/($high-$low+1e-12)\",\n            ]\n            names += [\n                \"KMID\",\n                \"KLEN\",\n                \"KMID2\",\n                \"KUP\",\n                \"KUP2\",\n                \"KLOW\",\n                \"KLOW2\",\n                \"KSFT\",\n                \"KSFT2\",\n            ]\n        if \"price\" in config:\n            windows = config[\"price\"].get(\"windows\", range(5))\n            feature = config[\"price\"].get(\"feature\", [\"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"VWAP\"])\n            for field in feature:\n                field = field.lower()\n                fields += [\"Ref($%s, %d)/$close\" % (field, d) if d != 0 else \"$%s/$close\" % field for d in windows]\n                names += [field.upper() + str(d) for d in windows]\n        if \"volume\" in config:\n            windows = config[\"volume\"].get(\"windows\", range(5))\n            fields += [\"Ref($volume, %d)/$volume\" % d if d != 0 else \"$volume/$volume\" for d in windows]\n            names += [\"VOLUME\" + str(d) for d in windows]\n        if \"rolling\" in config:\n            windows = config[\"rolling\"].get(\"windows\", [5, 10, 20, 30, 60])\n            include = config[\"rolling\"].get(\"include\", None)\n            exclude = config[\"rolling\"].get(\"exclude\", [])\n            # `exclude` in dataset config unnecessary filed\n            # `include` in dataset config necessary field\n            use = lambda x: x not in exclude and (include is None or x in include)\n            if use(\"ROC\"):\n                fields += [\"Ref($close, %d)/$close\" % d for d in windows]\n                names += [\"ROC%d\" % d for d in windows]\n            if use(\"MA\"):\n                fields += [\"Mean($close, %d)/$close\" % d for d in windows]\n                names += [\"MA%d\" % d for d in windows]\n            if use(\"STD\"):\n                fields += [\"Std($close, %d)/$close\" % d for d in windows]\n                names += [\"STD%d\" % d for d in windows]\n            if use(\"BETA\"):\n                fields += [\"Slope($close, %d)/$close\" % d for d in windows]\n                names += [\"BETA%d\" % d for d in windows]\n            if use(\"RSQR\"):\n                fields += [\"Rsquare($close, %d)\" % d for d in windows]\n                names += [\"RSQR%d\" % d for d in windows]\n            if use(\"RESI\"):\n                fields += [\"Resi($close, %d)/$close\" % d for d in windows]\n                names += [\"RESI%d\" % d for d in windows]\n            if use(\"MAX\"):\n                fields += [\"Max($high, %d)/$close\" % d for d in windows]\n                names += [\"MAX%d\" % d for d in windows]\n            if use(\"LOW\"):\n                fields += [\"Min($low, %d)/$close\" % d for d in windows]\n                names += [\"MIN%d\" % d for d in windows]\n            if use(\"QTLU\"):\n                fields += [\"Quantile($close, %d, 0.8)/$close\" % d for d in windows]\n                names += [\"QTLU%d\" % d for d in windows]\n            if use(\"QTLD\"):\n                fields += [\"Quantile($close, %d, 0.2)/$close\" % d for d in windows]\n                names += [\"QTLD%d\" % d for d in windows]\n            if use(\"RANK\"):\n                fields += [\"Rank($close, %d)\" % d for d in windows]\n                names += [\"RANK%d\" % d for d in windows]\n            if use(\"RSV\"):\n                fields += [\"($close-Min($low, %d))/(Max($high, %d)-Min($low, %d)+1e-12)\" % (d, d, d) for d in windows]\n                names += [\"RSV%d\" % d for d in windows]\n            if use(\"IMAX\"):\n                fields += [\"IdxMax($high, %d)/%d\" % (d, d) for d in windows]\n                names += [\"IMAX%d\" % d for d in windows]\n            if use(\"IMIN\"):\n                fields += [\"IdxMin($low, %d)/%d\" % (d, d) for d in windows]\n                names += [\"IMIN%d\" % d for d in windows]\n            if use(\"IMXD\"):\n                fields += [\"(IdxMax($high, %d)-IdxMin($low, %d))/%d\" % (d, d, d) for d in windows]\n                names += [\"IMXD%d\" % d for d in windows]\n            if use(\"CORR\"):\n                fields += [\"Corr($close, Log($volume+1), %d)\" % d for d in windows]\n                names += [\"CORR%d\" % d for d in windows]\n            if use(\"CORD\"):\n                fields += [\"Corr($close/Ref($close,1), Log($volume/Ref($volume, 1)+1), %d)\" % d for d in windows]\n                names += [\"CORD%d\" % d for d in windows]\n            if use(\"CNTP\"):\n                fields += [\"Mean($close>Ref($close, 1), %d)\" % d for d in windows]\n                names += [\"CNTP%d\" % d for d in windows]\n            if use(\"CNTN\"):\n                fields += [\"Mean($close<Ref($close, 1), %d)\" % d for d in windows]\n                names += [\"CNTN%d\" % d for d in windows]\n            if use(\"CNTD\"):\n                fields += [\"Mean($close>Ref($close, 1), %d)-Mean($close<Ref($close, 1), %d)\" % (d, d) for d in windows]\n                names += [\"CNTD%d\" % d for d in windows]\n            if use(\"SUMP\"):\n                fields += [\n                    \"Sum(Greater($close-Ref($close, 1), 0), %d)/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)\" % (d, d)\n                    for d in windows\n                ]\n                names += [\"SUMP%d\" % d for d in windows]\n            if use(\"SUMN\"):\n                fields += [\n                    \"Sum(Greater(Ref($close, 1)-$close, 0), %d)/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)\" % (d, d)\n                    for d in windows\n                ]\n                names += [\"SUMN%d\" % d for d in windows]\n            if use(\"SUMD\"):\n                fields += [\n                    \"(Sum(Greater($close-Ref($close, 1), 0), %d)-Sum(Greater(Ref($close, 1)-$close, 0), %d))\"\n                    \"/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)\" % (d, d, d)\n                    for d in windows\n                ]\n                names += [\"SUMD%d\" % d for d in windows]\n            if use(\"VMA\"):\n                fields += [\"Mean($volume, %d)/($volume+1e-12)\" % d for d in windows]\n                names += [\"VMA%d\" % d for d in windows]\n            if use(\"VSTD\"):\n                fields += [\"Std($volume, %d)/($volume+1e-12)\" % d for d in windows]\n                names += [\"VSTD%d\" % d for d in windows]\n            if use(\"WVMA\"):\n                fields += [\n                    \"Std(Abs($close/Ref($close, 1)-1)*$volume, %d)/(Mean(Abs($close/Ref($close, 1)-1)*$volume, %d)+1e-12)\"\n                    % (d, d)\n                    for d in windows\n                ]\n                names += [\"WVMA%d\" % d for d in windows]\n            if use(\"VSUMP\"):\n                fields += [\n                    \"Sum(Greater($volume-Ref($volume, 1), 0), %d)/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)\"\n                    % (d, d)\n                    for d in windows\n                ]\n                names += [\"VSUMP%d\" % d for d in windows]\n            if use(\"VSUMN\"):\n                fields += [\n                    \"Sum(Greater(Ref($volume, 1)-$volume, 0), %d)/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)\"\n                    % (d, d)\n                    for d in windows\n                ]\n                names += [\"VSUMN%d\" % d for d in windows]\n            if use(\"VSUMD\"):\n                fields += [\n                    \"(Sum(Greater($volume-Ref($volume, 1), 0), %d)-Sum(Greater(Ref($volume, 1)-$volume, 0), %d))\"\n                    \"/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)\" % (d, d, d)\n                    for d in windows\n                ]\n                names += [\"VSUMD%d\" % d for d in windows]\n\n        return fields, names",
  "def get_label_config(self):\n        return ([\"Ref($vwap, -2)/Ref($vwap, -1) - 1\"], [\"LABEL0\"])",
  "class ConfigSectionProcessor(Processor):\n    \"\"\"\n    This processor is designed for Alpha158. And will be replaced by simple processors in the future\n    \"\"\"\n\n    def __init__(self, fields_group=None, **kwargs):\n        super().__init__()\n        # Options\n        self.fillna_feature = kwargs.get(\"fillna_feature\", True)\n        self.fillna_label = kwargs.get(\"fillna_label\", True)\n        self.clip_feature_outlier = kwargs.get(\"clip_feature_outlier\", False)\n        self.shrink_feature_outlier = kwargs.get(\"shrink_feature_outlier\", True)\n        self.clip_label_outlier = kwargs.get(\"clip_label_outlier\", False)\n\n        self.fields_group = None\n\n    def __call__(self, df):\n        return self._transform(df)\n\n    def _transform(self, df):\n        def _label_norm(x):\n            x = x - x.mean()  # copy\n            x /= x.std()\n            if self.clip_label_outlier:\n                x.clip(-3, 3, inplace=True)\n            if self.fillna_label:\n                x.fillna(0, inplace=True)\n            return x\n\n        def _feature_norm(x):\n            x = x - x.median()  # copy\n            x /= x.abs().median() * 1.4826\n            if self.clip_feature_outlier:\n                x.clip(-3, 3, inplace=True)\n            if self.shrink_feature_outlier:\n                x.where(x <= 3, 3 + (x - 3).div(x.max() - 3) * 0.5, inplace=True)\n                x.where(x >= -3, -3 - (x + 3).div(x.min() + 3) * 0.5, inplace=True)\n            if self.fillna_feature:\n                x.fillna(0, inplace=True)\n            return x\n\n        TimeInspector.set_time_mark()\n\n        # Copy the focus part and change it to single level\n        selected_cols = get_group_columns(df, self.fields_group)\n        df_focus = df[selected_cols].copy()\n        if len(df_focus.columns.levels) > 1:\n            df_focus = df_focus.droplevel(level=0)\n\n        # Label\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^LABEL\")]\n        df_focus[cols] = df_focus[cols].groupby(level=\"datetime\").apply(_label_norm)\n\n        # Features\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^KLEN|^KLOW|^KUP\")]\n        df_focus[cols] = df_focus[cols].apply(lambda x: x ** 0.25).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^KLOW2|^KUP2\")]\n        df_focus[cols] = df_focus[cols].apply(lambda x: x ** 0.5).groupby(level=\"datetime\").apply(_feature_norm)\n\n        _cols = [\n            \"KMID\",\n            \"KSFT\",\n            \"OPEN\",\n            \"HIGH\",\n            \"LOW\",\n            \"CLOSE\",\n            \"VWAP\",\n            \"ROC\",\n            \"MA\",\n            \"BETA\",\n            \"RESI\",\n            \"QTLU\",\n            \"QTLD\",\n            \"RSV\",\n            \"SUMP\",\n            \"SUMN\",\n            \"SUMD\",\n            \"VSUMP\",\n            \"VSUMN\",\n            \"VSUMD\",\n        ]\n        pat = \"|\".join([\"^\" + x for x in _cols])\n        cols = df_focus.columns[df_focus.columns.str.contains(pat) & (~df_focus.columns.isin([\"HIGH0\", \"LOW0\"]))]\n        df_focus[cols] = df_focus[cols].groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^STD|^VOLUME|^VMA|^VSTD\")]\n        df_focus[cols] = df_focus[cols].apply(np.log).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^RSQR\")]\n        df_focus[cols] = df_focus[cols].fillna(0).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^MAX|^HIGH0\")]\n        df_focus[cols] = df_focus[cols].apply(lambda x: (x - 1) ** 0.5).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^MIN|^LOW0\")]\n        df_focus[cols] = df_focus[cols].apply(lambda x: (1 - x) ** 0.5).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^CORR|^CORD\")]\n        df_focus[cols] = df_focus[cols].apply(np.exp).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^WVMA\")]\n        df_focus[cols] = df_focus[cols].apply(np.log1p).groupby(level=\"datetime\").apply(_feature_norm)\n\n        df[selected_cols] = df_focus.values\n\n        TimeInspector.log_cost_time(\"Finished preprocessing data.\")\n\n        return df",
  "def __init__(self, fields_group=None, **kwargs):\n        super().__init__()\n        # Options\n        self.fillna_feature = kwargs.get(\"fillna_feature\", True)\n        self.fillna_label = kwargs.get(\"fillna_label\", True)\n        self.clip_feature_outlier = kwargs.get(\"clip_feature_outlier\", False)\n        self.shrink_feature_outlier = kwargs.get(\"shrink_feature_outlier\", True)\n        self.clip_label_outlier = kwargs.get(\"clip_label_outlier\", False)\n\n        self.fields_group = None",
  "def __call__(self, df):\n        return self._transform(df)",
  "def _transform(self, df):\n        def _label_norm(x):\n            x = x - x.mean()  # copy\n            x /= x.std()\n            if self.clip_label_outlier:\n                x.clip(-3, 3, inplace=True)\n            if self.fillna_label:\n                x.fillna(0, inplace=True)\n            return x\n\n        def _feature_norm(x):\n            x = x - x.median()  # copy\n            x /= x.abs().median() * 1.4826\n            if self.clip_feature_outlier:\n                x.clip(-3, 3, inplace=True)\n            if self.shrink_feature_outlier:\n                x.where(x <= 3, 3 + (x - 3).div(x.max() - 3) * 0.5, inplace=True)\n                x.where(x >= -3, -3 - (x + 3).div(x.min() + 3) * 0.5, inplace=True)\n            if self.fillna_feature:\n                x.fillna(0, inplace=True)\n            return x\n\n        TimeInspector.set_time_mark()\n\n        # Copy the focus part and change it to single level\n        selected_cols = get_group_columns(df, self.fields_group)\n        df_focus = df[selected_cols].copy()\n        if len(df_focus.columns.levels) > 1:\n            df_focus = df_focus.droplevel(level=0)\n\n        # Label\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^LABEL\")]\n        df_focus[cols] = df_focus[cols].groupby(level=\"datetime\").apply(_label_norm)\n\n        # Features\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^KLEN|^KLOW|^KUP\")]\n        df_focus[cols] = df_focus[cols].apply(lambda x: x ** 0.25).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^KLOW2|^KUP2\")]\n        df_focus[cols] = df_focus[cols].apply(lambda x: x ** 0.5).groupby(level=\"datetime\").apply(_feature_norm)\n\n        _cols = [\n            \"KMID\",\n            \"KSFT\",\n            \"OPEN\",\n            \"HIGH\",\n            \"LOW\",\n            \"CLOSE\",\n            \"VWAP\",\n            \"ROC\",\n            \"MA\",\n            \"BETA\",\n            \"RESI\",\n            \"QTLU\",\n            \"QTLD\",\n            \"RSV\",\n            \"SUMP\",\n            \"SUMN\",\n            \"SUMD\",\n            \"VSUMP\",\n            \"VSUMN\",\n            \"VSUMD\",\n        ]\n        pat = \"|\".join([\"^\" + x for x in _cols])\n        cols = df_focus.columns[df_focus.columns.str.contains(pat) & (~df_focus.columns.isin([\"HIGH0\", \"LOW0\"]))]\n        df_focus[cols] = df_focus[cols].groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^STD|^VOLUME|^VMA|^VSTD\")]\n        df_focus[cols] = df_focus[cols].apply(np.log).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^RSQR\")]\n        df_focus[cols] = df_focus[cols].fillna(0).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^MAX|^HIGH0\")]\n        df_focus[cols] = df_focus[cols].apply(lambda x: (x - 1) ** 0.5).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^MIN|^LOW0\")]\n        df_focus[cols] = df_focus[cols].apply(lambda x: (1 - x) ** 0.5).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^CORR|^CORD\")]\n        df_focus[cols] = df_focus[cols].apply(np.exp).groupby(level=\"datetime\").apply(_feature_norm)\n\n        cols = df_focus.columns[df_focus.columns.str.contains(\"^WVMA\")]\n        df_focus[cols] = df_focus[cols].apply(np.log1p).groupby(level=\"datetime\").apply(_feature_norm)\n\n        df[selected_cols] = df_focus.values\n\n        TimeInspector.log_cost_time(\"Finished preprocessing data.\")\n\n        return df",
  "def _label_norm(x):\n            x = x - x.mean()  # copy\n            x /= x.std()\n            if self.clip_label_outlier:\n                x.clip(-3, 3, inplace=True)\n            if self.fillna_label:\n                x.fillna(0, inplace=True)\n            return x",
  "def _feature_norm(x):\n            x = x - x.median()  # copy\n            x /= x.abs().median() * 1.4826\n            if self.clip_feature_outlier:\n                x.clip(-3, 3, inplace=True)\n            if self.shrink_feature_outlier:\n                x.where(x <= 3, 3 + (x - 3).div(x.max() - 3) * 0.5, inplace=True)\n                x.where(x >= -3, -3 - (x + 3).div(x.min() + 3) * 0.5, inplace=True)\n            if self.fillna_feature:\n                x.fillna(0, inplace=True)\n            return x",
  "class BaseGraph:\n    \"\"\"\"\"\"\n\n    _name = None\n\n    def __init__(\n        self, df: pd.DataFrame = None, layout: dict = None, graph_kwargs: dict = None, name_dict: dict = None, **kwargs\n    ):\n        \"\"\"\n\n        :param df:\n        :param layout:\n        :param graph_kwargs:\n        :param name_dict:\n        :param kwargs:\n            layout: dict\n                go.Layout parameters\n            graph_kwargs: dict\n                Graph parameters, eg: go.Bar(**graph_kwargs)\n        \"\"\"\n        self._df = df\n\n        self._layout = dict() if layout is None else layout\n        self._graph_kwargs = dict() if graph_kwargs is None else graph_kwargs\n        self._name_dict = name_dict\n\n        self.data = None\n\n        self._init_parameters(**kwargs)\n        self._init_data()\n\n    def _init_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        if self._df.empty:\n            raise ValueError(\"df is empty.\")\n\n        self.data = self._get_data()\n\n    def _init_parameters(self, **kwargs):\n        \"\"\"\n\n        :param kwargs\n        \"\"\"\n\n        # Instantiate graphics parameters\n        self._graph_type = self._name.lower().capitalize()\n\n        # Displayed column name\n        if self._name_dict is None:\n            self._name_dict = {_item: _item for _item in self._df.columns}\n\n    @staticmethod\n    def get_instance_with_graph_parameters(graph_type: str = None, **kwargs):\n        \"\"\"\n\n        :param graph_type:\n        :param kwargs:\n        :return:\n        \"\"\"\n        try:\n            _graph_module = importlib.import_module(\"plotly.graph_objs\")\n            _graph_class = getattr(_graph_module, graph_type)\n        except AttributeError:\n            _graph_module = importlib.import_module(\"qlib.contrib.report.graph\")\n            _graph_class = getattr(_graph_module, graph_type)\n        return _graph_class(**kwargs)\n\n    @staticmethod\n    def show_graph_in_notebook(figure_list: Iterable[go.Figure] = None):\n        \"\"\"\n\n        :param figure_list:\n        :return:\n        \"\"\"\n        py.init_notebook_mode()\n        for _fig in figure_list:\n            # NOTE: displays figures: https://plotly.com/python/renderers/\n            # default: plotly_mimetype+notebook\n            # support renderers: import plotly.io as pio; print(pio.renderers)\n            renderer = None\n            try:\n                # in notebook\n                _ipykernel = str(type(get_ipython()))\n                if \"google.colab\" in _ipykernel:\n                    renderer = \"colab\"\n            except NameError:\n                pass\n\n            _fig.show(renderer=renderer)\n\n    def _get_layout(self) -> go.Layout:\n        \"\"\"\n\n        :return:\n        \"\"\"\n        return go.Layout(**self._layout)\n\n    def _get_data(self) -> list:\n        \"\"\"\n\n        :return:\n        \"\"\"\n\n        _data = [\n            self.get_instance_with_graph_parameters(\n                graph_type=self._graph_type, x=self._df.index, y=self._df[_col], name=_name, **self._graph_kwargs\n            )\n            for _col, _name in self._name_dict.items()\n        ]\n        return _data\n\n    @property\n    def figure(self) -> go.Figure:\n        \"\"\"\n\n        :return:\n        \"\"\"\n        _figure = go.Figure(data=self.data, layout=self._get_layout())\n        # NOTE: using default 3.x theme\n        _figure[\"layout\"].update(template=None)\n        return _figure",
  "class ScatterGraph(BaseGraph):\n    _name = \"scatter\"",
  "class BarGraph(BaseGraph):\n    _name = \"bar\"",
  "class DistplotGraph(BaseGraph):\n    _name = \"distplot\"\n\n    def _get_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        _t_df = self._df.dropna()\n        _data_list = [_t_df[_col] for _col in self._name_dict]\n        _label_list = list(self._name_dict.values())\n        _fig = create_distplot(_data_list, _label_list, show_rug=False, **self._graph_kwargs)\n\n        return _fig[\"data\"]",
  "class HeatmapGraph(BaseGraph):\n    _name = \"heatmap\"\n\n    def _get_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        _data = [\n            self.get_instance_with_graph_parameters(\n                graph_type=self._graph_type,\n                x=self._df.columns,\n                y=self._df.index,\n                z=self._df.values.tolist(),\n                **self._graph_kwargs\n            )\n        ]\n        return _data",
  "class HistogramGraph(BaseGraph):\n    _name = \"histogram\"\n\n    def _get_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        _data = [\n            self.get_instance_with_graph_parameters(\n                graph_type=self._graph_type, x=self._df[_col], name=_name, **self._graph_kwargs\n            )\n            for _col, _name in self._name_dict.items()\n        ]\n        return _data",
  "class SubplotsGraph:\n    \"\"\"Create subplots same as df.plot(subplots=True)\n\n    Simple package for `plotly.tools.subplots`\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame = None,\n        kind_map: dict = None,\n        layout: dict = None,\n        sub_graph_layout: dict = None,\n        sub_graph_data: list = None,\n        subplots_kwargs: dict = None,\n        **kwargs\n    ):\n        \"\"\"\n\n        :param df: pd.DataFrame\n\n        :param kind_map: dict, subplots graph kind and kwargs\n            eg: dict(kind='ScatterGraph', kwargs=dict())\n\n        :param layout: `go.Layout` parameters\n\n        :param sub_graph_layout: Layout of each graphic, similar to 'layout'\n\n        :param sub_graph_data: Instantiation parameters for each sub-graphic\n            eg: [(column_name, instance_parameters), ]\n\n            column_name: str or go.Figure\n\n            Instance_parameters:\n\n                - row: int, the row where the graph is located\n\n                - col: int, the col where the graph is located\n\n                - name: str, show name, default column_name in 'df'\n\n                - kind: str, graph kind, default `kind` param, eg: bar, scatter, ...\n\n                - graph_kwargs: dict, graph kwargs, default {}, used in `go.Bar(**graph_kwargs)`\n\n        :param subplots_kwargs: `plotly.tools.make_subplots` original parameters\n\n                - shared_xaxes: bool, default False\n\n                - shared_yaxes: bool, default False\n\n                - vertical_spacing: float, default 0.3 / rows\n\n                - subplot_titles: list, default []\n                    If `sub_graph_data` is None, will generate 'subplot_titles' according to `df.columns`,\n                    this field will be discarded\n\n\n                - specs: list, see `make_subplots` docs\n\n                - rows: int, Number of rows in the subplot grid, default 1\n                    If `sub_graph_data` is None, will generate 'rows' according to `df`, this field will be discarded\n\n                - cols: int, Number of cols in the subplot grid, default 1\n                    If `sub_graph_data` is None, will generate 'cols' according to `df`, this field will be discarded\n\n\n        :param kwargs:\n\n        \"\"\"\n\n        self._df = df\n        self._layout = layout\n        self._sub_graph_layout = sub_graph_layout\n\n        self._kind_map = kind_map\n        if self._kind_map is None:\n            self._kind_map = dict(kind=\"ScatterGraph\", kwargs=dict())\n\n        self._subplots_kwargs = subplots_kwargs\n        if self._subplots_kwargs is None:\n            self._init_subplots_kwargs()\n\n        self.__cols = self._subplots_kwargs.get(\"cols\", 2)\n        self.__rows = self._subplots_kwargs.get(\"rows\", math.ceil(len(self._df.columns) / self.__cols))\n\n        self._sub_graph_data = sub_graph_data\n        if self._sub_graph_data is None:\n            self._init_sub_graph_data()\n\n        self._init_figure()\n\n    def _init_sub_graph_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        self._sub_graph_data = list()\n        self._subplot_titles = list()\n\n        for i, column_name in enumerate(self._df.columns):\n            row = math.ceil((i + 1) / self.__cols)\n            _temp = (i + 1) % self.__cols\n            col = _temp if _temp else self.__cols\n            res_name = column_name.replace(\"_\", \" \")\n            _temp_row_data = (\n                column_name,\n                dict(\n                    row=row,\n                    col=col,\n                    name=res_name,\n                    kind=self._kind_map[\"kind\"],\n                    graph_kwargs=self._kind_map[\"kwargs\"],\n                ),\n            )\n            self._sub_graph_data.append(_temp_row_data)\n            self._subplot_titles.append(res_name)\n\n    def _init_subplots_kwargs(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        # Default cols, rows\n        _cols = 2\n        _rows = math.ceil(len(self._df.columns) / 2)\n        self._subplots_kwargs = dict()\n        self._subplots_kwargs[\"rows\"] = _rows\n        self._subplots_kwargs[\"cols\"] = _cols\n        self._subplots_kwargs[\"shared_xaxes\"] = False\n        self._subplots_kwargs[\"shared_yaxes\"] = False\n        self._subplots_kwargs[\"vertical_spacing\"] = 0.3 / _rows\n        self._subplots_kwargs[\"print_grid\"] = False\n        self._subplots_kwargs[\"subplot_titles\"] = self._df.columns.tolist()\n\n    def _init_figure(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        self._figure = make_subplots(**self._subplots_kwargs)\n\n        for column_name, column_map in self._sub_graph_data:\n            if isinstance(column_name, go.Figure):\n                _graph_obj = column_name\n            elif isinstance(column_name, str):\n                temp_name = column_map.get(\"name\", column_name.replace(\"_\", \" \"))\n                kind = column_map.get(\"kind\", self._kind_map.get(\"kind\", \"ScatterGraph\"))\n                _graph_kwargs = column_map.get(\"graph_kwargs\", self._kind_map.get(\"kwargs\", {}))\n                _graph_obj = BaseGraph.get_instance_with_graph_parameters(\n                    kind,\n                    **dict(\n                        df=self._df.loc[:, [column_name]],\n                        name_dict={column_name: temp_name},\n                        graph_kwargs=_graph_kwargs,\n                    )\n                )\n            else:\n                raise TypeError()\n\n            row = column_map[\"row\"]\n            col = column_map[\"col\"]\n\n            _graph_data = getattr(_graph_obj, \"data\")\n            # for _item in _graph_data:\n            #     _item.pop('xaxis', None)\n            #     _item.pop('yaxis', None)\n\n            for _g_obj in _graph_data:\n                self._figure.add_trace(_g_obj, row=row, col=col)\n\n        if self._sub_graph_layout is not None:\n            for k, v in self._sub_graph_layout.items():\n                self._figure[\"layout\"][k].update(v)\n\n        # NOTE: using default 3.x theme\n        self._figure[\"layout\"].update(self._layout, template=None)\n\n    @property\n    def figure(self):\n        return self._figure",
  "def __init__(\n        self, df: pd.DataFrame = None, layout: dict = None, graph_kwargs: dict = None, name_dict: dict = None, **kwargs\n    ):\n        \"\"\"\n\n        :param df:\n        :param layout:\n        :param graph_kwargs:\n        :param name_dict:\n        :param kwargs:\n            layout: dict\n                go.Layout parameters\n            graph_kwargs: dict\n                Graph parameters, eg: go.Bar(**graph_kwargs)\n        \"\"\"\n        self._df = df\n\n        self._layout = dict() if layout is None else layout\n        self._graph_kwargs = dict() if graph_kwargs is None else graph_kwargs\n        self._name_dict = name_dict\n\n        self.data = None\n\n        self._init_parameters(**kwargs)\n        self._init_data()",
  "def _init_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        if self._df.empty:\n            raise ValueError(\"df is empty.\")\n\n        self.data = self._get_data()",
  "def _init_parameters(self, **kwargs):\n        \"\"\"\n\n        :param kwargs\n        \"\"\"\n\n        # Instantiate graphics parameters\n        self._graph_type = self._name.lower().capitalize()\n\n        # Displayed column name\n        if self._name_dict is None:\n            self._name_dict = {_item: _item for _item in self._df.columns}",
  "def get_instance_with_graph_parameters(graph_type: str = None, **kwargs):\n        \"\"\"\n\n        :param graph_type:\n        :param kwargs:\n        :return:\n        \"\"\"\n        try:\n            _graph_module = importlib.import_module(\"plotly.graph_objs\")\n            _graph_class = getattr(_graph_module, graph_type)\n        except AttributeError:\n            _graph_module = importlib.import_module(\"qlib.contrib.report.graph\")\n            _graph_class = getattr(_graph_module, graph_type)\n        return _graph_class(**kwargs)",
  "def show_graph_in_notebook(figure_list: Iterable[go.Figure] = None):\n        \"\"\"\n\n        :param figure_list:\n        :return:\n        \"\"\"\n        py.init_notebook_mode()\n        for _fig in figure_list:\n            # NOTE: displays figures: https://plotly.com/python/renderers/\n            # default: plotly_mimetype+notebook\n            # support renderers: import plotly.io as pio; print(pio.renderers)\n            renderer = None\n            try:\n                # in notebook\n                _ipykernel = str(type(get_ipython()))\n                if \"google.colab\" in _ipykernel:\n                    renderer = \"colab\"\n            except NameError:\n                pass\n\n            _fig.show(renderer=renderer)",
  "def _get_layout(self) -> go.Layout:\n        \"\"\"\n\n        :return:\n        \"\"\"\n        return go.Layout(**self._layout)",
  "def _get_data(self) -> list:\n        \"\"\"\n\n        :return:\n        \"\"\"\n\n        _data = [\n            self.get_instance_with_graph_parameters(\n                graph_type=self._graph_type, x=self._df.index, y=self._df[_col], name=_name, **self._graph_kwargs\n            )\n            for _col, _name in self._name_dict.items()\n        ]\n        return _data",
  "def figure(self) -> go.Figure:\n        \"\"\"\n\n        :return:\n        \"\"\"\n        _figure = go.Figure(data=self.data, layout=self._get_layout())\n        # NOTE: using default 3.x theme\n        _figure[\"layout\"].update(template=None)\n        return _figure",
  "def _get_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        _t_df = self._df.dropna()\n        _data_list = [_t_df[_col] for _col in self._name_dict]\n        _label_list = list(self._name_dict.values())\n        _fig = create_distplot(_data_list, _label_list, show_rug=False, **self._graph_kwargs)\n\n        return _fig[\"data\"]",
  "def _get_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        _data = [\n            self.get_instance_with_graph_parameters(\n                graph_type=self._graph_type,\n                x=self._df.columns,\n                y=self._df.index,\n                z=self._df.values.tolist(),\n                **self._graph_kwargs\n            )\n        ]\n        return _data",
  "def _get_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        _data = [\n            self.get_instance_with_graph_parameters(\n                graph_type=self._graph_type, x=self._df[_col], name=_name, **self._graph_kwargs\n            )\n            for _col, _name in self._name_dict.items()\n        ]\n        return _data",
  "def __init__(\n        self,\n        df: pd.DataFrame = None,\n        kind_map: dict = None,\n        layout: dict = None,\n        sub_graph_layout: dict = None,\n        sub_graph_data: list = None,\n        subplots_kwargs: dict = None,\n        **kwargs\n    ):\n        \"\"\"\n\n        :param df: pd.DataFrame\n\n        :param kind_map: dict, subplots graph kind and kwargs\n            eg: dict(kind='ScatterGraph', kwargs=dict())\n\n        :param layout: `go.Layout` parameters\n\n        :param sub_graph_layout: Layout of each graphic, similar to 'layout'\n\n        :param sub_graph_data: Instantiation parameters for each sub-graphic\n            eg: [(column_name, instance_parameters), ]\n\n            column_name: str or go.Figure\n\n            Instance_parameters:\n\n                - row: int, the row where the graph is located\n\n                - col: int, the col where the graph is located\n\n                - name: str, show name, default column_name in 'df'\n\n                - kind: str, graph kind, default `kind` param, eg: bar, scatter, ...\n\n                - graph_kwargs: dict, graph kwargs, default {}, used in `go.Bar(**graph_kwargs)`\n\n        :param subplots_kwargs: `plotly.tools.make_subplots` original parameters\n\n                - shared_xaxes: bool, default False\n\n                - shared_yaxes: bool, default False\n\n                - vertical_spacing: float, default 0.3 / rows\n\n                - subplot_titles: list, default []\n                    If `sub_graph_data` is None, will generate 'subplot_titles' according to `df.columns`,\n                    this field will be discarded\n\n\n                - specs: list, see `make_subplots` docs\n\n                - rows: int, Number of rows in the subplot grid, default 1\n                    If `sub_graph_data` is None, will generate 'rows' according to `df`, this field will be discarded\n\n                - cols: int, Number of cols in the subplot grid, default 1\n                    If `sub_graph_data` is None, will generate 'cols' according to `df`, this field will be discarded\n\n\n        :param kwargs:\n\n        \"\"\"\n\n        self._df = df\n        self._layout = layout\n        self._sub_graph_layout = sub_graph_layout\n\n        self._kind_map = kind_map\n        if self._kind_map is None:\n            self._kind_map = dict(kind=\"ScatterGraph\", kwargs=dict())\n\n        self._subplots_kwargs = subplots_kwargs\n        if self._subplots_kwargs is None:\n            self._init_subplots_kwargs()\n\n        self.__cols = self._subplots_kwargs.get(\"cols\", 2)\n        self.__rows = self._subplots_kwargs.get(\"rows\", math.ceil(len(self._df.columns) / self.__cols))\n\n        self._sub_graph_data = sub_graph_data\n        if self._sub_graph_data is None:\n            self._init_sub_graph_data()\n\n        self._init_figure()",
  "def _init_sub_graph_data(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        self._sub_graph_data = list()\n        self._subplot_titles = list()\n\n        for i, column_name in enumerate(self._df.columns):\n            row = math.ceil((i + 1) / self.__cols)\n            _temp = (i + 1) % self.__cols\n            col = _temp if _temp else self.__cols\n            res_name = column_name.replace(\"_\", \" \")\n            _temp_row_data = (\n                column_name,\n                dict(\n                    row=row,\n                    col=col,\n                    name=res_name,\n                    kind=self._kind_map[\"kind\"],\n                    graph_kwargs=self._kind_map[\"kwargs\"],\n                ),\n            )\n            self._sub_graph_data.append(_temp_row_data)\n            self._subplot_titles.append(res_name)",
  "def _init_subplots_kwargs(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        # Default cols, rows\n        _cols = 2\n        _rows = math.ceil(len(self._df.columns) / 2)\n        self._subplots_kwargs = dict()\n        self._subplots_kwargs[\"rows\"] = _rows\n        self._subplots_kwargs[\"cols\"] = _cols\n        self._subplots_kwargs[\"shared_xaxes\"] = False\n        self._subplots_kwargs[\"shared_yaxes\"] = False\n        self._subplots_kwargs[\"vertical_spacing\"] = 0.3 / _rows\n        self._subplots_kwargs[\"print_grid\"] = False\n        self._subplots_kwargs[\"subplot_titles\"] = self._df.columns.tolist()",
  "def _init_figure(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        self._figure = make_subplots(**self._subplots_kwargs)\n\n        for column_name, column_map in self._sub_graph_data:\n            if isinstance(column_name, go.Figure):\n                _graph_obj = column_name\n            elif isinstance(column_name, str):\n                temp_name = column_map.get(\"name\", column_name.replace(\"_\", \" \"))\n                kind = column_map.get(\"kind\", self._kind_map.get(\"kind\", \"ScatterGraph\"))\n                _graph_kwargs = column_map.get(\"graph_kwargs\", self._kind_map.get(\"kwargs\", {}))\n                _graph_obj = BaseGraph.get_instance_with_graph_parameters(\n                    kind,\n                    **dict(\n                        df=self._df.loc[:, [column_name]],\n                        name_dict={column_name: temp_name},\n                        graph_kwargs=_graph_kwargs,\n                    )\n                )\n            else:\n                raise TypeError()\n\n            row = column_map[\"row\"]\n            col = column_map[\"col\"]\n\n            _graph_data = getattr(_graph_obj, \"data\")\n            # for _item in _graph_data:\n            #     _item.pop('xaxis', None)\n            #     _item.pop('yaxis', None)\n\n            for _g_obj in _graph_data:\n                self._figure.add_trace(_g_obj, row=row, col=col)\n\n        if self._sub_graph_layout is not None:\n            for k, v in self._sub_graph_layout.items():\n                self._figure[\"layout\"][k].update(v)\n\n        # NOTE: using default 3.x theme\n        self._figure[\"layout\"].update(self._layout, template=None)",
  "def figure(self):\n        return self._figure",
  "def _get_risk_analysis_data_with_report(\n    report_normal_df: pd.DataFrame,\n    # report_long_short_df: pd.DataFrame,\n    date: pd.Timestamp,\n) -> pd.DataFrame:\n    \"\"\"Get risk analysis data with report\n\n    :param report_normal_df: report data\n    :param report_long_short_df: report data\n    :param date: date string\n    :return:\n    \"\"\"\n\n    analysis = dict()\n    # if not report_long_short_df.empty:\n    #     analysis[\"pred_long\"] = risk_analysis(report_long_short_df[\"long\"])\n    #     analysis[\"pred_short\"] = risk_analysis(report_long_short_df[\"short\"])\n    #     analysis[\"pred_long_short\"] = risk_analysis(report_long_short_df[\"long_short\"])\n\n    if not report_normal_df.empty:\n        analysis[\"excess_return_without_cost\"] = risk_analysis(report_normal_df[\"return\"] - report_normal_df[\"bench\"])\n        analysis[\"excess_return_with_cost\"] = risk_analysis(\n            report_normal_df[\"return\"] - report_normal_df[\"bench\"] - report_normal_df[\"cost\"]\n        )\n    analysis_df = pd.concat(analysis)  # type: pd.DataFrame\n    analysis_df[\"date\"] = date\n    return analysis_df",
  "def _get_all_risk_analysis(risk_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"risk_df to standard\n\n    :param risk_df: risk data\n    :return:\n    \"\"\"\n    if risk_df is None:\n        return pd.DataFrame()\n    risk_df = risk_df.unstack()\n    risk_df.columns = risk_df.columns.droplevel(0)\n    return risk_df.drop(\"mean\", axis=1)",
  "def _get_monthly_risk_analysis_with_report(report_normal_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Get monthly analysis data\n\n    :param report_normal_df:\n    # :param report_long_short_df:\n    :return:\n    \"\"\"\n\n    # Group by month\n    report_normal_gp = report_normal_df.groupby([report_normal_df.index.year, report_normal_df.index.month])\n    # report_long_short_gp = report_long_short_df.groupby(\n    #     [report_long_short_df.index.year, report_long_short_df.index.month]\n    # )\n\n    gp_month = sorted(set(report_normal_gp.size().index))\n\n    _monthly_df = pd.DataFrame()\n    for gp_m in gp_month:\n        _m_report_normal = report_normal_gp.get_group(gp_m)\n        # _m_report_long_short = report_long_short_gp.get_group(gp_m)\n\n        if len(_m_report_normal) < 3:\n            # The month's data is less than 3, not displayed\n            # FIXME: If the trading day of a month is less than 3 days, a breakpoint will appear in the graph\n            continue\n        month_days = pd.Timestamp(year=gp_m[0], month=gp_m[1], day=1).days_in_month\n        _temp_df = _get_risk_analysis_data_with_report(\n            _m_report_normal,\n            # _m_report_long_short,\n            pd.Timestamp(year=gp_m[0], month=gp_m[1], day=month_days),\n        )\n        _monthly_df = _monthly_df.append(_temp_df, sort=False)\n\n    return _monthly_df",
  "def _get_monthly_analysis_with_feature(monthly_df: pd.DataFrame, feature: str = \"annualized_return\") -> pd.DataFrame:\n    \"\"\"\n\n    :param monthly_df:\n    :param feature:\n    :return:\n    \"\"\"\n    _monthly_df_gp = monthly_df.reset_index().groupby([\"level_1\"])\n\n    _name_df = _monthly_df_gp.get_group(feature).set_index([\"level_0\", \"level_1\"])\n    _temp_df = _name_df.pivot_table(index=\"date\", values=[\"risk\"], columns=_name_df.index)\n    _temp_df.columns = map(lambda x: \"_\".join(x[-1]), _temp_df.columns)\n    _temp_df.index = _temp_df.index.strftime(\"%Y-%m\")\n\n    return _temp_df",
  "def _get_risk_analysis_figure(analysis_df: pd.DataFrame) -> Iterable[py.Figure]:\n    \"\"\"Get analysis graph figure\n\n    :param analysis_df:\n    :return:\n    \"\"\"\n    if analysis_df is None:\n        return []\n\n    _figure = SubplotsGraph(\n        _get_all_risk_analysis(analysis_df),\n        kind_map=dict(kind=\"BarGraph\", kwargs={}),\n        subplots_kwargs={\"rows\": 4, \"cols\": 1},\n    ).figure\n    return (_figure,)",
  "def _get_monthly_risk_analysis_figure(report_normal_df: pd.DataFrame) -> Iterable[py.Figure]:\n    \"\"\"Get analysis monthly graph figure\n\n    :param report_normal_df:\n    :param report_long_short_df:\n    :return:\n    \"\"\"\n\n    # if report_normal_df is None and report_long_short_df is None:\n    #     return []\n    if report_normal_df is None:\n        return []\n\n    # if report_normal_df is None:\n    #     report_normal_df = pd.DataFrame(index=report_long_short_df.index)\n\n    # if report_long_short_df is None:\n    #     report_long_short_df = pd.DataFrame(index=report_normal_df.index)\n\n    _monthly_df = _get_monthly_risk_analysis_with_report(\n        report_normal_df=report_normal_df,\n        # report_long_short_df=report_long_short_df,\n    )\n\n    for _feature in [\"annualized_return\", \"max_drawdown\", \"information_ratio\", \"std\"]:\n        _temp_df = _get_monthly_analysis_with_feature(_monthly_df, _feature)\n        yield ScatterGraph(\n            _temp_df,\n            layout=dict(title=_feature, xaxis=dict(type=\"category\", tickangle=45)),\n            graph_kwargs={\"mode\": \"lines+markers\"},\n        ).figure",
  "def risk_analysis_graph(\n    analysis_df: pd.DataFrame = None,\n    report_normal_df: pd.DataFrame = None,\n    report_long_short_df: pd.DataFrame = None,\n    show_notebook: bool = True,\n) -> Iterable[py.Figure]:\n    \"\"\"Generate analysis graph and monthly analysis\n\n        Example:\n\n\n            .. code-block:: python\n\n                from qlib.contrib.evaluate import risk_analysis, backtest, long_short_backtest\n                from qlib.contrib.strategy import TopkDropoutStrategy\n                from qlib.contrib.report import analysis_position\n\n                # backtest parameters\n                bparas = {}\n                bparas['limit_threshold'] = 0.095\n                bparas['account'] = 1000000000\n\n                sparas = {}\n                sparas['topk'] = 50\n                sparas['n_drop'] = 230\n                strategy = TopkDropoutStrategy(**sparas)\n\n                report_normal_df, positions = backtest(pred_df, strategy, **bparas)\n                # long_short_map = long_short_backtest(pred_df)\n                # report_long_short_df = pd.DataFrame(long_short_map)\n\n                analysis = dict()\n                # analysis['pred_long'] = risk_analysis(report_long_short_df['long'])\n                # analysis['pred_short'] = risk_analysis(report_long_short_df['short'])\n                # analysis['pred_long_short'] = risk_analysis(report_long_short_df['long_short'])\n                analysis['excess_return_without_cost'] = risk_analysis(report_normal_df['return'] - report_normal_df['bench'])\n                analysis['excess_return_with_cost'] = risk_analysis(report_normal_df['return'] - report_normal_df['bench'] - report_normal_df['cost'])\n                analysis_df = pd.concat(analysis)\n\n                analysis_position.risk_analysis_graph(analysis_df, report_normal_df)\n\n\n\n    :param analysis_df: analysis data, index is **pd.MultiIndex**; columns names is **[risk]**.\n\n\n            .. code-block:: python\n\n                                                                  risk\n                excess_return_without_cost mean               0.000692\n                                           std                0.005374\n                                           annualized_return  0.174495\n                                           information_ratio  2.045576\n                                           max_drawdown      -0.079103\n                excess_return_with_cost    mean               0.000499\n                                           std                0.005372\n                                           annualized_return  0.125625\n                                           information_ratio  1.473152\n                                           max_drawdown      -0.088263\n\n\n    :param report_normal_df: **df.index.name** must be **date**, df.columns must contain **return**, **turnover**, **cost**, **bench**.\n\n\n            .. code-block:: python\n\n                            return      cost        bench       turnover\n                date\n                2017-01-04  0.003421    0.000864    0.011693    0.576325\n                2017-01-05  0.000508    0.000447    0.000721    0.227882\n                2017-01-06  -0.003321   0.000212    -0.004322   0.102765\n                2017-01-09  0.006753    0.000212    0.006874    0.105864\n                2017-01-10  -0.000416   0.000440    -0.003350   0.208396\n\n\n    :param report_long_short_df: **df.index.name** must be **date**, df.columns contain **long**, **short**, **long_short**.\n\n\n            .. code-block:: python\n\n                            long        short       long_short\n                date\n                2017-01-04  -0.001360   0.001394    0.000034\n                2017-01-05  0.002456    0.000058    0.002514\n                2017-01-06  0.000120    0.002739    0.002859\n                2017-01-09  0.001436    0.001838    0.003273\n                2017-01-10  0.000824    -0.001944   -0.001120\n\n\n    :param show_notebook: Whether to display graphics in a notebook, default **True**.\n        If True, show graph in notebook\n        If False, return graph figure\n    :return:\n    \"\"\"\n    _figure_list = list(_get_risk_analysis_figure(analysis_df)) + list(\n        _get_monthly_risk_analysis_figure(\n            report_normal_df,\n            # report_long_short_df,\n        )\n    )\n    if show_notebook:\n        ScatterGraph.show_graph_in_notebook(_figure_list)\n    else:\n        return _figure_list",
  "def _get_figure_with_position(\n    position: dict, label_data: pd.DataFrame, start_date=None, end_date=None\n) -> Iterable[go.Figure]:\n    \"\"\"Get average analysis figures\n\n    :param position: position\n    :param label_data:\n    :param start_date:\n    :param end_date:\n    :return:\n    \"\"\"\n    _position_df = get_position_data(\n        position,\n        label_data,\n        calculate_label_rank=True,\n        start_date=start_date,\n        end_date=end_date,\n    )\n\n    res_dict = dict()\n    _pos_gp = _position_df.groupby(level=1)\n    for _item in _pos_gp:\n        _date = _item[0]\n        _day_df = _item[1]\n\n        _day_value = res_dict.setdefault(_date, {})\n        for _i, _name in {0: \"Hold\", 1: \"Buy\", -1: \"Sell\"}.items():\n            _temp_df = _day_df[_day_df[\"status\"] == _i]\n            if _temp_df.empty:\n                _day_value[_name] = 0\n            else:\n                _day_value[_name] = _temp_df[\"rank_label_mean\"].values[0]\n\n    _res_df = pd.DataFrame.from_dict(res_dict, orient=\"index\")\n    # FIXME: support HIGH-FREQ\n    _res_df.index = _res_df.index.strftime(\"%Y-%m-%d\")\n    for _col in _res_df.columns:\n        yield ScatterGraph(\n            _res_df.loc[:, [_col]],\n            layout=dict(\n                title=_col,\n                xaxis=dict(type=\"category\", tickangle=45),\n                yaxis=dict(title=\"lable-rank-ratio: %\"),\n            ),\n            graph_kwargs=dict(mode=\"lines+markers\"),\n        ).figure",
  "def rank_label_graph(\n    position: dict,\n    label_data: pd.DataFrame,\n    start_date=None,\n    end_date=None,\n    show_notebook=True,\n) -> Iterable[go.Figure]:\n    \"\"\"Ranking percentage of stocks buy, sell, and holding on the trading day.\n    Average rank-ratio(similar to **sell_df['label'].rank(ascending=False) / len(sell_df)**) of daily trading\n\n        Example:\n\n\n            .. code-block:: python\n\n                from qlib.data import D\n                from qlib.contrib.evaluate import backtest\n                from qlib.contrib.strategy import TopkDropoutStrategy\n\n                # backtest parameters\n                bparas = {}\n                bparas['limit_threshold'] = 0.095\n                bparas['account'] = 1000000000\n\n                sparas = {}\n                sparas['topk'] = 50\n                sparas['n_drop'] = 230\n                strategy = TopkDropoutStrategy(**sparas)\n\n                _, positions = backtest(pred_df, strategy, **bparas)\n\n                pred_df_dates = pred_df.index.get_level_values(level='datetime')\n                features_df = D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'], pred_df_dates.min(), pred_df_dates.max())\n                features_df.columns = ['label']\n\n                qcr.rank_label_graph(positions, features_df, pred_df_dates.min(), pred_df_dates.max())\n\n\n    :param position: position data; **qlib.contrib.backtest.backtest.backtest** result.\n    :param label_data: **D.features** result; index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[label]**.\n    **The label T is the change from T to T+1**, it is recommended to use ``close``, example: `D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'])`.\n\n\n            .. code-block:: python\n\n                                                label\n                instrument  datetime\n                SH600004        2017-12-11  -0.013502\n                                2017-12-12  -0.072367\n                                2017-12-13  -0.068605\n                                2017-12-14  0.012440\n                                2017-12-15  -0.102778\n\n\n    :param start_date: start date\n    :param end_date: end_date\n    :param show_notebook: **True** or **False**. If True, show graph in notebook, else return figures.\n    :return:\n    \"\"\"\n    position = copy.deepcopy(position)\n    label_data.columns = [\"label\"]\n    _figures = _get_figure_with_position(position, label_data, start_date, end_date)\n    if show_notebook:\n        ScatterGraph.show_graph_in_notebook(_figures)\n    else:\n        return _figures",
  "def _get_score_ic(pred_label: pd.DataFrame):\n    \"\"\"\n\n    :param pred_label:\n    :return:\n    \"\"\"\n    concat_data = pred_label.copy()\n    concat_data.dropna(axis=0, how=\"any\", inplace=True)\n    _ic = concat_data.groupby(level=\"datetime\").apply(lambda x: x[\"label\"].corr(x[\"score\"]))\n    _rank_ic = concat_data.groupby(level=\"datetime\").apply(lambda x: x[\"label\"].corr(x[\"score\"], method=\"spearman\"))\n    return pd.DataFrame({\"ic\": _ic, \"rank_ic\": _rank_ic})",
  "def score_ic_graph(pred_label: pd.DataFrame, show_notebook: bool = True) -> [list, tuple]:\n    \"\"\"score IC\n\n        Example:\n\n\n            .. code-block:: python\n\n                from qlib.data import D\n                from qlib.contrib.report import analysis_position\n                pred_df_dates = pred_df.index.get_level_values(level='datetime')\n                features_df = D.features(D.instruments('csi500'), ['Ref($close, -2)/Ref($close, -1)-1'], pred_df_dates.min(), pred_df_dates.max())\n                features_df.columns = ['label']\n                pred_label = pd.concat([features_df, pred], axis=1, sort=True).reindex(features_df.index)\n                analysis_position.score_ic_graph(pred_label)\n\n\n    :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score, label]**.\n\n\n            .. code-block:: python\n\n                instrument  datetime        score         label\n                SH600004  2017-12-11     -0.013502       -0.013502\n                            2017-12-12   -0.072367       -0.072367\n                            2017-12-13   -0.068605       -0.068605\n                            2017-12-14    0.012440        0.012440\n                            2017-12-15   -0.102778       -0.102778\n\n\n    :param show_notebook: whether to display graphics in notebook, the default is **True**.\n    :return: if show_notebook is True, display in notebook; else return **plotly.graph_objs.Figure** list.\n    \"\"\"\n    _ic_df = _get_score_ic(pred_label)\n    # FIXME: support HIGH-FREQ\n    _ic_df.index = _ic_df.index.strftime(\"%Y-%m-%d\")\n    _figure = ScatterGraph(\n        _ic_df,\n        layout=dict(title=\"Score IC\", xaxis=dict(type=\"category\", tickangle=45)),\n        graph_kwargs={\"mode\": \"lines+markers\"},\n    ).figure\n    if show_notebook:\n        ScatterGraph.show_graph_in_notebook([_figure])\n    else:\n        return (_figure,)",
  "def parse_position(position: dict = None) -> pd.DataFrame:\n    \"\"\"Parse position dict to position DataFrame\n\n    :param position: position data\n    :return: position DataFrame;\n\n\n        .. code-block:: python\n\n            position_df = parse_position(positions)\n            print(position_df.head())\n            # status: 0-hold, -1-sell, 1-buy\n\n                                        amount      cash      count    price status weight\n            instrument  datetime\n            SZ000547    2017-01-04  44.154290   211405.285654   1   205.189575  1   0.031255\n            SZ300202    2017-01-04  60.638845   211405.285654   1   154.356506  1   0.032290\n            SH600158    2017-01-04  46.531681   211405.285654   1   153.895142  1   0.024704\n            SH600545    2017-01-04  197.173093  211405.285654   1   48.607037   1   0.033063\n            SZ000930    2017-01-04  103.938300  211405.285654   1   80.759453   1   0.028958\n\n\n    \"\"\"\n\n    position_weight_df = get_stock_weight_df(position)\n    # If the day does not exist, use the last weight\n    position_weight_df.fillna(method=\"ffill\", inplace=True)\n\n    previous_data = {\"date\": None, \"code_list\": []}\n\n    result_df = pd.DataFrame()\n    for _trading_date, _value in position.items():\n        # pd_date type: pd.Timestamp\n        _cash = _value.pop(\"cash\")\n        for _item in [\"today_account_value\"]:\n            if _item in _value:\n                _value.pop(_item)\n\n        _trading_day_df = pd.DataFrame.from_dict(_value, orient=\"index\")\n        _trading_day_df[\"weight\"] = position_weight_df.loc[_trading_date]\n        _trading_day_df[\"cash\"] = _cash\n        _trading_day_df[\"date\"] = _trading_date\n        # status: 0-hold, -1-sell, 1-buy\n        _trading_day_df[\"status\"] = 0\n\n        # T not exist, T-1 exist, T sell\n        _cur_day_sell = set(previous_data[\"code_list\"]) - set(_trading_day_df.index)\n        # T exist, T-1 not exist, T buy\n        _cur_day_buy = set(_trading_day_df.index) - set(previous_data[\"code_list\"])\n\n        # Trading day buy\n        _trading_day_df.loc[_trading_day_df.index.isin(_cur_day_buy), \"status\"] = 1\n\n        # Trading day sell\n        if not result_df.empty:\n            _trading_day_sell_df = result_df.loc[\n                (result_df[\"date\"] == previous_data[\"date\"]) & (result_df.index.isin(_cur_day_sell))\n            ].copy()\n            if not _trading_day_sell_df.empty:\n                _trading_day_sell_df[\"status\"] = -1\n                _trading_day_sell_df[\"date\"] = _trading_date\n                _trading_day_df = _trading_day_df.append(_trading_day_sell_df, sort=False)\n\n        result_df = result_df.append(_trading_day_df, sort=True)\n\n        previous_data = dict(\n            date=_trading_date,\n            code_list=_trading_day_df[_trading_day_df[\"status\"] != -1].index,\n        )\n\n    result_df.reset_index(inplace=True)\n    result_df.rename(columns={\"date\": \"datetime\", \"index\": \"instrument\"}, inplace=True)\n    return result_df.set_index([\"instrument\", \"datetime\"])",
  "def _add_label_to_position(position_df: pd.DataFrame, label_data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Concat position with custom label\n\n    :param position_df: position DataFrame\n    :param label_data:\n    :return: concat result\n    \"\"\"\n\n    _start_time = position_df.index.get_level_values(level=\"datetime\").min()\n    _end_time = position_df.index.get_level_values(level=\"datetime\").max()\n    label_data = label_data.loc(axis=0)[:, pd.to_datetime(_start_time) :]\n    _result_df = pd.concat([position_df, label_data], axis=1, sort=True).reindex(label_data.index)\n    _result_df = _result_df.loc[_result_df.index.get_level_values(1) <= _end_time]\n    return _result_df",
  "def _add_bench_to_position(position_df: pd.DataFrame = None, bench: pd.Series = None) -> pd.DataFrame:\n    \"\"\"Concat position with bench\n\n    :param position_df: position DataFrame\n    :param bench: report normal data\n    :return: concat result\n    \"\"\"\n    _temp_df = position_df.reset_index(level=\"instrument\")\n    # FIXME: After the stock is bought and sold, the rise and fall of the next trading day are calculated.\n    _temp_df[\"bench\"] = bench.shift(-1)\n    res_df = _temp_df.set_index([\"instrument\", _temp_df.index])\n    return res_df",
  "def _calculate_label_rank(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"calculate label rank\n\n    :param df:\n    :return:\n    \"\"\"\n    _label_name = \"label\"\n\n    def _calculate_day_value(g_df: pd.DataFrame):\n        g_df = g_df.copy()\n        g_df[\"rank_ratio\"] = g_df[_label_name].rank(ascending=False) / len(g_df) * 100\n\n        # Sell: -1, Hold: 0, Buy: 1\n        for i in [-1, 0, 1]:\n            g_df.loc[g_df[\"status\"] == i, \"rank_label_mean\"] = g_df[g_df[\"status\"] == i][\"rank_ratio\"].mean()\n\n        g_df[\"excess_return\"] = g_df[_label_name] - g_df[_label_name].mean()\n        return g_df\n\n    return df.groupby(level=\"datetime\").apply(_calculate_day_value)",
  "def get_position_data(\n    position: dict,\n    label_data: pd.DataFrame,\n    report_normal: pd.DataFrame = None,\n    calculate_label_rank=False,\n    start_date=None,\n    end_date=None,\n) -> pd.DataFrame:\n    \"\"\"Concat position data with pred/report_normal\n\n    :param position: position data\n    :param report_normal: report normal, must be container 'bench' column\n    :param label_data:\n    :param calculate_label_rank:\n    :param start_date: start date\n    :param end_date: end date\n    :return: concat result,\n        columns: ['amount', 'cash', 'count', 'price', 'status', 'weight', 'label',\n                    'rank_ratio', 'rank_label_mean', 'excess_return', 'score', 'bench']\n        index: ['instrument', 'date']\n    \"\"\"\n    _position_df = parse_position(position)\n\n    # Add custom_label, rank_ratio, rank_mean, and excess_return field\n    _position_df = _add_label_to_position(_position_df, label_data)\n\n    if calculate_label_rank:\n        _position_df = _calculate_label_rank(_position_df)\n\n    if report_normal is not None:\n        # Add bench field\n        _position_df = _add_bench_to_position(_position_df, report_normal[\"bench\"])\n\n    _date_list = _position_df.index.get_level_values(level=\"datetime\")\n    start_date = _date_list.min() if start_date is None else start_date\n    end_date = _date_list.max() if end_date is None else end_date\n    _position_df = _position_df.loc[(start_date <= _date_list) & (_date_list <= end_date)]\n    return _position_df",
  "def _calculate_day_value(g_df: pd.DataFrame):\n        g_df = g_df.copy()\n        g_df[\"rank_ratio\"] = g_df[_label_name].rank(ascending=False) / len(g_df) * 100\n\n        # Sell: -1, Hold: 0, Buy: 1\n        for i in [-1, 0, 1]:\n            g_df.loc[g_df[\"status\"] == i, \"rank_label_mean\"] = g_df[g_df[\"status\"] == i][\"rank_ratio\"].mean()\n\n        g_df[\"excess_return\"] = g_df[_label_name] - g_df[_label_name].mean()\n        return g_df",
  "def _calculate_maximum(df: pd.DataFrame, is_ex: bool = False):\n    \"\"\"\n\n    :param df:\n    :param is_ex:\n    :return:\n    \"\"\"\n    if is_ex:\n        end_date = df[\"cum_ex_return_wo_cost_mdd\"].idxmin()\n        start_date = df.loc[df.index <= end_date][\"cum_ex_return_wo_cost\"].idxmax()\n    else:\n        end_date = df[\"return_wo_mdd\"].idxmin()\n        start_date = df.loc[df.index <= end_date][\"cum_return_wo_cost\"].idxmax()\n    return start_date, end_date",
  "def _calculate_mdd(series):\n    \"\"\"\n    Calculate mdd\n\n    :param series:\n    :return:\n    \"\"\"\n    return series - series.cummax()",
  "def _calculate_report_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n\n    :param df:\n    :return:\n    \"\"\"\n    index_names = df.index.names\n    df.index = df.index.strftime(\"%Y-%m-%d\")\n\n    report_df = pd.DataFrame()\n\n    report_df[\"cum_bench\"] = df[\"bench\"].cumsum()\n    report_df[\"cum_return_wo_cost\"] = df[\"return\"].cumsum()\n    report_df[\"cum_return_w_cost\"] = (df[\"return\"] - df[\"cost\"]).cumsum()\n    # report_df['cum_return'] - report_df['cum_return'].cummax()\n    report_df[\"return_wo_mdd\"] = _calculate_mdd(report_df[\"cum_return_wo_cost\"])\n    report_df[\"return_w_cost_mdd\"] = _calculate_mdd((df[\"return\"] - df[\"cost\"]).cumsum())\n\n    report_df[\"cum_ex_return_wo_cost\"] = (df[\"return\"] - df[\"bench\"]).cumsum()\n    report_df[\"cum_ex_return_w_cost\"] = (df[\"return\"] - df[\"bench\"] - df[\"cost\"]).cumsum()\n    report_df[\"cum_ex_return_wo_cost_mdd\"] = _calculate_mdd((df[\"return\"] - df[\"bench\"]).cumsum())\n    report_df[\"cum_ex_return_w_cost_mdd\"] = _calculate_mdd((df[\"return\"] - df[\"cost\"] - df[\"bench\"]).cumsum())\n    # return_wo_mdd , return_w_cost_mdd,  cum_ex_return_wo_cost_mdd, cum_ex_return_w\n\n    report_df[\"turnover\"] = df[\"turnover\"]\n    report_df.sort_index(ascending=True, inplace=True)\n\n    report_df.index.names = index_names\n    return report_df",
  "def _report_figure(df: pd.DataFrame) -> [list, tuple]:\n    \"\"\"\n\n    :param df:\n    :return:\n    \"\"\"\n\n    # Get data\n    report_df = _calculate_report_data(df)\n\n    # Maximum Drawdown\n    max_start_date, max_end_date = _calculate_maximum(report_df)\n    ex_max_start_date, ex_max_end_date = _calculate_maximum(report_df, True)\n\n    index_name = report_df.index.name\n    _temp_df = report_df.reset_index()\n    _temp_df.loc[-1] = 0\n    _temp_df = _temp_df.shift(1)\n    _temp_df.loc[0, index_name] = \"T0\"\n    _temp_df.set_index(index_name, inplace=True)\n    _temp_df.iloc[0] = 0\n    report_df = _temp_df\n\n    # Create figure\n    _default_kind_map = dict(kind=\"ScatterGraph\", kwargs={\"mode\": \"lines+markers\"})\n    _temp_fill_args = {\"fill\": \"tozeroy\", \"mode\": \"lines+markers\"}\n    _column_row_col_dict = [\n        (\"cum_bench\", dict(row=1, col=1)),\n        (\"cum_return_wo_cost\", dict(row=1, col=1)),\n        (\"cum_return_w_cost\", dict(row=1, col=1)),\n        (\"return_wo_mdd\", dict(row=2, col=1, graph_kwargs=_temp_fill_args)),\n        (\"return_w_cost_mdd\", dict(row=3, col=1, graph_kwargs=_temp_fill_args)),\n        (\"cum_ex_return_wo_cost\", dict(row=4, col=1)),\n        (\"cum_ex_return_w_cost\", dict(row=4, col=1)),\n        (\"turnover\", dict(row=5, col=1)),\n        (\"cum_ex_return_w_cost_mdd\", dict(row=6, col=1, graph_kwargs=_temp_fill_args)),\n        (\"cum_ex_return_wo_cost_mdd\", dict(row=7, col=1, graph_kwargs=_temp_fill_args)),\n    ]\n\n    _subplot_layout = dict()\n    for i in range(1, 8):\n        # yaxis\n        _subplot_layout.update({\"yaxis{}\".format(i): dict(zeroline=True, showline=True, showticklabels=True)})\n        _show_line = i == 7\n        _subplot_layout.update({\"xaxis{}\".format(i): dict(showline=_show_line, type=\"category\", tickangle=45)})\n\n    _layout_style = dict(\n        height=1200,\n        title=\" \",\n        shapes=[\n            {\n                \"type\": \"rect\",\n                \"xref\": \"x\",\n                \"yref\": \"paper\",\n                \"x0\": max_start_date,\n                \"y0\": 0.55,\n                \"x1\": max_end_date,\n                \"y1\": 1,\n                \"fillcolor\": \"#d3d3d3\",\n                \"opacity\": 0.3,\n                \"line\": {\n                    \"width\": 0,\n                },\n            },\n            {\n                \"type\": \"rect\",\n                \"xref\": \"x\",\n                \"yref\": \"paper\",\n                \"x0\": ex_max_start_date,\n                \"y0\": 0,\n                \"x1\": ex_max_end_date,\n                \"y1\": 0.55,\n                \"fillcolor\": \"#d3d3d3\",\n                \"opacity\": 0.3,\n                \"line\": {\n                    \"width\": 0,\n                },\n            },\n        ],\n    )\n\n    _subplot_kwargs = dict(\n        shared_xaxes=True,\n        vertical_spacing=0.01,\n        rows=7,\n        cols=1,\n        row_width=[1, 1, 1, 3, 1, 1, 3],\n        print_grid=False,\n    )\n    figure = SubplotsGraph(\n        df=report_df,\n        layout=_layout_style,\n        sub_graph_data=_column_row_col_dict,\n        subplots_kwargs=_subplot_kwargs,\n        kind_map=_default_kind_map,\n        sub_graph_layout=_subplot_layout,\n    ).figure\n    return (figure,)",
  "def report_graph(report_df: pd.DataFrame, show_notebook: bool = True) -> [list, tuple]:\n    \"\"\"display backtest report\n\n        Example:\n\n\n            .. code-block:: python\n\n                from qlib.contrib.evaluate import backtest\n                from qlib.contrib.strategy import TopkDropoutStrategy\n\n                # backtest parameters\n                bparas = {}\n                bparas['limit_threshold'] = 0.095\n                bparas['account'] = 1000000000\n\n                sparas = {}\n                sparas['topk'] = 50\n                sparas['n_drop'] = 230\n                strategy = TopkDropoutStrategy(**sparas)\n\n                report_normal_df, _ = backtest(pred_df, strategy, **bparas)\n\n                qcr.report_graph(report_normal_df)\n\n    :param report_df: **df.index.name** must be **date**, **df.columns** must contain **return**, **turnover**, **cost**, **bench**.\n\n\n            .. code-block:: python\n\n                            return      cost        bench       turnover\n                date\n                2017-01-04  0.003421    0.000864    0.011693    0.576325\n                2017-01-05  0.000508    0.000447    0.000721    0.227882\n                2017-01-06  -0.003321   0.000212    -0.004322   0.102765\n                2017-01-09  0.006753    0.000212    0.006874    0.105864\n                2017-01-10  -0.000416   0.000440    -0.003350   0.208396\n\n\n    :param show_notebook: whether to display graphics in notebook, the default is **True**.\n    :return: if show_notebook is True, display in notebook; else return **plotly.graph_objs.Figure** list.\n    \"\"\"\n    report_df = report_df.copy()\n    fig_list = _report_figure(report_df)\n    if show_notebook:\n        BaseGraph.show_graph_in_notebook(fig_list)\n    else:\n        return fig_list",
  "def _get_cum_return_data_with_position(\n    position: dict,\n    report_normal: pd.DataFrame,\n    label_data: pd.DataFrame,\n    start_date=None,\n    end_date=None,\n):\n    \"\"\"\n\n    :param position:\n    :param report_normal:\n    :param label_data:\n    :param start_date:\n    :param end_date:\n    :return:\n    \"\"\"\n    _cumulative_return_df = get_position_data(\n        position=position,\n        report_normal=report_normal,\n        label_data=label_data,\n        start_date=start_date,\n        end_date=end_date,\n    ).copy()\n\n    _cumulative_return_df[\"label\"] = _cumulative_return_df[\"label\"] - _cumulative_return_df[\"bench\"]\n    _cumulative_return_df = _cumulative_return_df.dropna()\n    df_gp = _cumulative_return_df.groupby(level=\"datetime\")\n    result_list = []\n    for gp in df_gp:\n        date = gp[0]\n        day_df = gp[1]\n\n        _hold_df = day_df[day_df[\"status\"] == 0]\n        _buy_df = day_df[day_df[\"status\"] == 1]\n        _sell_df = day_df[day_df[\"status\"] == -1]\n\n        hold_value = (_hold_df[\"label\"] * _hold_df[\"weight\"]).sum()\n        hold_weight = _hold_df[\"weight\"].sum()\n        hold_mean = (hold_value / hold_weight) if hold_weight else 0\n\n        sell_value = (_sell_df[\"label\"] * _sell_df[\"weight\"]).sum()\n        sell_weight = _sell_df[\"weight\"].sum()\n        sell_mean = (sell_value / sell_weight) if sell_weight else 0\n\n        buy_value = (_buy_df[\"label\"] * _buy_df[\"weight\"]).sum()\n        buy_weight = _buy_df[\"weight\"].sum()\n        buy_mean = (buy_value / buy_weight) if buy_weight else 0\n\n        result_list.append(\n            dict(\n                hold_value=hold_value,\n                hold_mean=hold_mean,\n                hold_weight=hold_weight,\n                buy_value=buy_value,\n                buy_mean=buy_mean,\n                buy_weight=buy_weight,\n                sell_value=sell_value,\n                sell_mean=sell_mean,\n                sell_weight=sell_weight,\n                buy_minus_sell_value=buy_value - sell_value,\n                buy_minus_sell_mean=buy_mean - sell_mean,\n                buy_plus_sell_weight=buy_weight + sell_weight,\n                date=date,\n            )\n        )\n\n    r_df = pd.DataFrame(data=result_list)\n    r_df[\"cum_hold\"] = r_df[\"hold_mean\"].cumsum()\n    r_df[\"cum_buy\"] = r_df[\"buy_mean\"].cumsum()\n    r_df[\"cum_sell\"] = r_df[\"sell_mean\"].cumsum()\n    r_df[\"cum_buy_minus_sell\"] = r_df[\"buy_minus_sell_mean\"].cumsum()\n    return r_df",
  "def _get_figure_with_position(\n    position: dict,\n    report_normal: pd.DataFrame,\n    label_data: pd.DataFrame,\n    start_date=None,\n    end_date=None,\n) -> Iterable[go.Figure]:\n    \"\"\"Get average analysis figures\n\n    :param position: position\n    :param report_normal:\n    :param label_data:\n    :param start_date:\n    :param end_date:\n    :return:\n    \"\"\"\n\n    cum_return_df = _get_cum_return_data_with_position(position, report_normal, label_data, start_date, end_date)\n    cum_return_df = cum_return_df.set_index(\"date\")\n    # FIXME: support HIGH-FREQ\n    cum_return_df.index = cum_return_df.index.strftime(\"%Y-%m-%d\")\n\n    # Create figures\n    for _t_name in [\"buy\", \"sell\", \"buy_minus_sell\", \"hold\"]:\n        sub_graph_data = [\n            (\n                \"cum_{}\".format(_t_name),\n                dict(row=1, col=1, graph_kwargs={\"mode\": \"lines+markers\", \"xaxis\": \"x3\"}),\n            ),\n            (\n                \"{}_weight\".format(_t_name.replace(\"minus\", \"plus\") if \"minus\" in _t_name else _t_name),\n                dict(row=2, col=1),\n            ),\n            (\n                \"{}_value\".format(_t_name),\n                dict(row=1, col=2, kind=\"HistogramGraph\", graph_kwargs={}),\n            ),\n        ]\n\n        _default_xaxis = dict(showline=False, zeroline=True, tickangle=45)\n        _default_yaxis = dict(zeroline=True, showline=True, showticklabels=True)\n        sub_graph_layout = dict(\n            xaxis1=dict(**_default_xaxis, type=\"category\", showticklabels=False),\n            xaxis3=dict(**_default_xaxis, type=\"category\"),\n            xaxis2=_default_xaxis,\n            yaxis1=dict(**_default_yaxis, title=_t_name),\n            yaxis2=_default_yaxis,\n            yaxis3=_default_yaxis,\n        )\n\n        mean_value = cum_return_df[\"{}_value\".format(_t_name)].mean()\n        layout = dict(\n            height=500,\n            title=f\"{_t_name}(the red line in the histogram on the right represents the average)\",\n            shapes=[\n                {\n                    \"type\": \"line\",\n                    \"xref\": \"x2\",\n                    \"yref\": \"paper\",\n                    \"x0\": mean_value,\n                    \"y0\": 0,\n                    \"x1\": mean_value,\n                    \"y1\": 1,\n                    # NOTE: 'fillcolor': '#d3d3d3', 'opacity': 0.3,\n                    \"line\": {\"color\": \"red\", \"width\": 1},\n                },\n            ],\n        )\n\n        kind_map = dict(kind=\"ScatterGraph\", kwargs=dict(mode=\"lines+markers\"))\n        specs = [\n            [{\"rowspan\": 1}, {\"rowspan\": 2}],\n            [{\"rowspan\": 1}, None],\n        ]\n        subplots_kwargs = dict(\n            vertical_spacing=0.01,\n            rows=2,\n            cols=2,\n            row_width=[1, 2],\n            column_width=[3, 1],\n            print_grid=False,\n            specs=specs,\n        )\n        yield SubplotsGraph(\n            cum_return_df,\n            layout=layout,\n            kind_map=kind_map,\n            sub_graph_layout=sub_graph_layout,\n            sub_graph_data=sub_graph_data,\n            subplots_kwargs=subplots_kwargs,\n        ).figure",
  "def cumulative_return_graph(\n    position: dict,\n    report_normal: pd.DataFrame,\n    label_data: pd.DataFrame,\n    show_notebook=True,\n    start_date=None,\n    end_date=None,\n) -> Iterable[go.Figure]:\n    \"\"\"Backtest buy, sell, and holding cumulative return graph\n\n        Example:\n\n\n            .. code-block:: python\n\n                from qlib.data import D\n                from qlib.contrib.evaluate import risk_analysis, backtest, long_short_backtest\n                from qlib.contrib.strategy import TopkDropoutStrategy\n\n                # backtest parameters\n                bparas = {}\n                bparas['limit_threshold'] = 0.095\n                bparas['account'] = 1000000000\n\n                sparas = {}\n                sparas['topk'] = 50\n                sparas['n_drop'] = 5\n                strategy = TopkDropoutStrategy(**sparas)\n\n                report_normal_df, positions = backtest(pred_df, strategy, **bparas)\n\n                pred_df_dates = pred_df.index.get_level_values(level='datetime')\n                features_df = D.features(D.instruments('csi500'), ['Ref($close, -1)/$close - 1'], pred_df_dates.min(), pred_df_dates.max())\n                features_df.columns = ['label']\n\n                qcr.cumulative_return_graph(positions, report_normal_df, features_df)\n\n\n        Graph desc:\n            - Axis X: Trading day.\n            - Axis Y:\n            - Above axis Y: `(((Ref($close, -1)/$close - 1) * weight).sum() / weight.sum()).cumsum()`.\n            - Below axis Y: Daily weight sum.\n            - In the **sell** graph, `y < 0` stands for profit; in other cases, `y > 0` stands for profit.\n            - In the **buy_minus_sell** graph, the **y** value of the **weight** graph at the bottom is `buy_weight + sell_weight`.\n            - In each graph, the **red line** in the histogram on the right represents the average.\n\n    :param position: position data\n    :param report_normal:\n\n\n            .. code-block:: python\n\n                                return      cost        bench       turnover\n                date\n                2017-01-04  0.003421    0.000864    0.011693    0.576325\n                2017-01-05  0.000508    0.000447    0.000721    0.227882\n                2017-01-06  -0.003321   0.000212    -0.004322   0.102765\n                2017-01-09  0.006753    0.000212    0.006874    0.105864\n                2017-01-10  -0.000416   0.000440    -0.003350   0.208396\n\n\n    :param label_data: `D.features` result; index is `pd.MultiIndex`, index name is [`instrument`, `datetime`]; columns names is [`label`].\n    **The label T is the change from T to T+1**, it is recommended to use ``close``, example: `D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'])`\n\n\n            .. code-block:: python\n\n                                                label\n                instrument  datetime\n                SH600004        2017-12-11  -0.013502\n                                2017-12-12  -0.072367\n                                2017-12-13  -0.068605\n                                2017-12-14  0.012440\n                                2017-12-15  -0.102778\n\n\n    :param show_notebook: True or False. If True, show graph in notebook, else return figures\n    :param start_date: start date\n    :param end_date: end date\n    :return:\n    \"\"\"\n    position = copy.deepcopy(position)\n    report_normal = report_normal.copy()\n    label_data.columns = [\"label\"]\n    _figures = _get_figure_with_position(position, report_normal, label_data, start_date, end_date)\n    if show_notebook:\n        BaseGraph.show_graph_in_notebook(_figures)\n    else:\n        return _figures",
  "def _group_return(pred_label: pd.DataFrame = None, reverse: bool = False, N: int = 5, **kwargs) -> tuple:\n    \"\"\"\n\n    :param pred_label:\n    :param reverse:\n    :param N:\n    :return:\n    \"\"\"\n    if reverse:\n        pred_label[\"score\"] *= -1\n\n    pred_label = pred_label.sort_values(\"score\", ascending=False)\n\n    # Group1 ~ Group5 only consider the dropna values\n    pred_label_drop = pred_label.dropna(subset=[\"score\"])\n\n    # Group\n    t_df = pd.DataFrame(\n        {\n            \"Group%d\"\n            % (i + 1): pred_label_drop.groupby(level=\"datetime\")[\"label\"].apply(\n                lambda x: x[len(x) // N * i : len(x) // N * (i + 1)].mean()\n            )\n            for i in range(N)\n        }\n    )\n    t_df.index = pd.to_datetime(t_df.index)\n\n    # Long-Short\n    t_df[\"long-short\"] = t_df[\"Group1\"] - t_df[\"Group%d\" % N]\n\n    # Long-Average\n    t_df[\"long-average\"] = t_df[\"Group1\"] - pred_label.groupby(level=\"datetime\")[\"label\"].mean()\n\n    t_df = t_df.dropna(how=\"all\")  # for days which does not contain label\n    # FIXME: support HIGH-FREQ\n    t_df.index = t_df.index.strftime(\"%Y-%m-%d\")\n    # Cumulative Return By Group\n    group_scatter_figure = ScatterGraph(\n        t_df.cumsum(),\n        layout=dict(title=\"Cumulative Return\", xaxis=dict(type=\"category\", tickangle=45)),\n    ).figure\n\n    t_df = t_df.loc[:, [\"long-short\", \"long-average\"]]\n    _bin_size = ((t_df.max() - t_df.min()) / 20).min()\n    group_hist_figure = SubplotsGraph(\n        t_df,\n        kind_map=dict(kind=\"DistplotGraph\", kwargs=dict(bin_size=_bin_size)),\n        subplots_kwargs=dict(\n            rows=1,\n            cols=2,\n            print_grid=False,\n            subplot_titles=[\"long-short\", \"long-average\"],\n        ),\n    ).figure\n\n    return group_scatter_figure, group_hist_figure",
  "def _plot_qq(data: pd.Series = None, dist=stats.norm) -> go.Figure:\n    \"\"\"\n\n    :param data:\n    :param dist:\n    :return:\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(8, 5))\n    _mpl_fig = sm.qqplot(data.dropna(), dist, fit=True, line=\"45\", ax=ax)\n    return tls.mpl_to_plotly(_mpl_fig)",
  "def _pred_ic(pred_label: pd.DataFrame = None, rank: bool = False, **kwargs) -> tuple:\n    \"\"\"\n\n    :param pred_label:\n    :param rank:\n    :return:\n    \"\"\"\n    if rank:\n        ic = pred_label.groupby(level=\"datetime\").apply(\n            lambda x: x[\"label\"].rank(pct=True).corr(x[\"score\"].rank(pct=True))\n        )\n    else:\n        ic = pred_label.groupby(level=\"datetime\").apply(lambda x: x[\"label\"].corr(x[\"score\"]))\n\n    _index = ic.index.get_level_values(0).astype(\"str\").str.replace(\"-\", \"\").str.slice(0, 6)\n    _monthly_ic = ic.groupby(_index).mean()\n    _monthly_ic.index = pd.MultiIndex.from_arrays(\n        [_monthly_ic.index.str.slice(0, 4), _monthly_ic.index.str.slice(4, 6)],\n        names=[\"year\", \"month\"],\n    )\n\n    # fill month\n    _month_list = pd.date_range(\n        start=pd.Timestamp(f\"{_index.min()[:4]}0101\"),\n        end=pd.Timestamp(f\"{_index.max()[:4]}1231\"),\n        freq=\"1M\",\n    )\n    _years = []\n    _month = []\n    for _date in _month_list:\n        _date = _date.strftime(\"%Y%m%d\")\n        _years.append(_date[:4])\n        _month.append(_date[4:6])\n\n    fill_index = pd.MultiIndex.from_arrays([_years, _month], names=[\"year\", \"month\"])\n\n    _monthly_ic = _monthly_ic.reindex(fill_index)\n\n    _ic_df = ic.to_frame(\"ic\")\n    ic_bar_figure = ic_figure(_ic_df, kwargs.get(\"show_nature_day\", True))\n\n    ic_heatmap_figure = HeatmapGraph(\n        _monthly_ic.unstack(),\n        layout=dict(title=\"Monthly IC\", yaxis=dict(tickformat=\",d\")),\n        graph_kwargs=dict(xtype=\"array\", ytype=\"array\"),\n    ).figure\n\n    dist = stats.norm\n    _qqplot_fig = _plot_qq(ic, dist)\n\n    if isinstance(dist, stats.norm.__class__):\n        dist_name = \"Normal\"\n    else:\n        dist_name = \"Unknown\"\n\n    _bin_size = ((_ic_df.max() - _ic_df.min()) / 20).min()\n    _sub_graph_data = [\n        (\n            \"ic\",\n            dict(\n                row=1,\n                col=1,\n                name=\"\",\n                kind=\"DistplotGraph\",\n                graph_kwargs=dict(bin_size=_bin_size),\n            ),\n        ),\n        (_qqplot_fig, dict(row=1, col=2)),\n    ]\n    ic_hist_figure = SubplotsGraph(\n        _ic_df.dropna(),\n        kind_map=dict(kind=\"HistogramGraph\", kwargs=dict()),\n        subplots_kwargs=dict(\n            rows=1,\n            cols=2,\n            print_grid=False,\n            subplot_titles=[\"IC\", \"IC %s Dist. Q-Q\" % dist_name],\n        ),\n        sub_graph_data=_sub_graph_data,\n        layout=dict(\n            yaxis2=dict(title=\"Observed Quantile\"),\n            xaxis2=dict(title=f\"{dist_name} Distribution Quantile\"),\n        ),\n    ).figure\n\n    return ic_bar_figure, ic_heatmap_figure, ic_hist_figure",
  "def _pred_autocorr(pred_label: pd.DataFrame, lag=1, **kwargs) -> tuple:\n    pred = pred_label.copy()\n    pred[\"score_last\"] = pred.groupby(level=\"instrument\")[\"score\"].shift(lag)\n    ac = pred.groupby(level=\"datetime\").apply(lambda x: x[\"score\"].rank(pct=True).corr(x[\"score_last\"].rank(pct=True)))\n    # FIXME: support HIGH-FREQ\n    _df = ac.to_frame(\"value\")\n    _df.index = _df.index.strftime(\"%Y-%m-%d\")\n    ac_figure = ScatterGraph(\n        _df,\n        layout=dict(title=\"Auto Correlation\", xaxis=dict(type=\"category\", tickangle=45)),\n    ).figure\n    return (ac_figure,)",
  "def _pred_turnover(pred_label: pd.DataFrame, N=5, lag=1, **kwargs) -> tuple:\n    pred = pred_label.copy()\n    pred[\"score_last\"] = pred.groupby(level=\"instrument\")[\"score\"].shift(lag)\n    top = pred.groupby(level=\"datetime\").apply(\n        lambda x: 1\n        - x.nlargest(len(x) // N, columns=\"score\").index.isin(x.nlargest(len(x) // N, columns=\"score_last\").index).sum()\n        / (len(x) // N)\n    )\n    bottom = pred.groupby(level=\"datetime\").apply(\n        lambda x: 1\n        - x.nsmallest(len(x) // N, columns=\"score\")\n        .index.isin(x.nsmallest(len(x) // N, columns=\"score_last\").index)\n        .sum()\n        / (len(x) // N)\n    )\n    r_df = pd.DataFrame(\n        {\n            \"Top\": top,\n            \"Bottom\": bottom,\n        }\n    )\n    # FIXME: support HIGH-FREQ\n    r_df.index = r_df.index.strftime(\"%Y-%m-%d\")\n    turnover_figure = ScatterGraph(\n        r_df,\n        layout=dict(title=\"Top-Bottom Turnover\", xaxis=dict(type=\"category\", tickangle=45)),\n    ).figure\n    return (turnover_figure,)",
  "def ic_figure(ic_df: pd.DataFrame, show_nature_day=True, **kwargs) -> go.Figure:\n    \"\"\"IC figure\n\n    :param ic_df: ic DataFrame\n    :param show_nature_day: whether to display the abscissa of non-trading day\n    :return: plotly.graph_objs.Figure\n    \"\"\"\n    if show_nature_day:\n        date_index = pd.date_range(ic_df.index.min(), ic_df.index.max())\n        ic_df = ic_df.reindex(date_index)\n    # FIXME: support HIGH-FREQ\n    ic_df.index = ic_df.index.strftime(\"%Y-%m-%d\")\n    ic_bar_figure = BarGraph(\n        ic_df,\n        layout=dict(\n            title=\"Information Coefficient (IC)\",\n            xaxis=dict(type=\"category\", tickangle=45),\n        ),\n    ).figure\n    return ic_bar_figure",
  "def model_performance_graph(\n    pred_label: pd.DataFrame,\n    lag: int = 1,\n    N: int = 5,\n    reverse=False,\n    rank=False,\n    graph_names: list = [\"group_return\", \"pred_ic\", \"pred_autocorr\"],\n    show_notebook: bool = True,\n    show_nature_day=True,\n) -> [list, tuple]:\n    \"\"\"Model performance\n\n    :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score,\n    label]**. It is usually same as the label of model training(e.g. \"Ref($close, -2)/Ref($close, -1) - 1\").\n\n\n            .. code-block:: python\n\n                instrument  datetime        score       label\n                SH600004    2017-12-11  -0.013502       -0.013502\n                                2017-12-12  -0.072367       -0.072367\n                                2017-12-13  -0.068605       -0.068605\n                                2017-12-14  0.012440        0.012440\n                                2017-12-15  -0.102778       -0.102778\n\n\n    :param lag: `pred.groupby(level='instrument')['score'].shift(lag)`. It will be only used in the auto-correlation computing.\n    :param N: group number, default 5.\n    :param reverse: if `True`, `pred['score'] *= -1`.\n    :param rank: if **True**, calculate rank ic.\n    :param graph_names: graph names; default ['cumulative_return', 'pred_ic', 'pred_autocorr', 'pred_turnover'].\n    :param show_notebook: whether to display graphics in notebook, the default is `True`.\n    :param show_nature_day: whether to display the abscissa of non-trading day.\n    :return: if show_notebook is True, display in notebook; else return `plotly.graph_objs.Figure` list.\n    \"\"\"\n    figure_list = []\n    for graph_name in graph_names:\n        fun_res = eval(f\"_{graph_name}\")(\n            pred_label=pred_label,\n            lag=lag,\n            N=N,\n            reverse=reverse,\n            rank=rank,\n            show_nature_day=show_nature_day,\n        )\n        figure_list += fun_res\n\n    if show_notebook:\n        BarGraph.show_graph_in_notebook(figure_list)\n    else:\n        return figure_list",
  "def load_instance(file_path):\n    \"\"\"\n    load a pickle file\n        Parameter\n           file_path : string / pathlib.Path()\n                path of file to be loaded\n        :return\n            An instance loaded from file\n    \"\"\"\n    file_path = pathlib.Path(file_path)\n    if not file_path.exists():\n        raise ValueError(\"Cannot find file {}\".format(file_path))\n    with file_path.open(\"rb\") as fr:\n        instance = pickle.load(fr)\n    return instance",
  "def save_instance(instance, file_path):\n    \"\"\"\n    save(dump) an instance to a pickle file\n        Parameter\n            instance :\n                data to te dumped\n            file_path : string / pathlib.Path()\n                path of file to be dumped\n    \"\"\"\n    file_path = pathlib.Path(file_path)\n    with file_path.open(\"wb\") as fr:\n        pickle.dump(instance, fr)",
  "def create_user_folder(path):\n    path = pathlib.Path(path)\n    if path.exists():\n        return\n    path.mkdir(parents=True)\n    head = pd.DataFrame(columns=(\"user_id\", \"add_date\"))\n    head.to_csv(path / \"users.csv\", index=None)",
  "def prepare(um, today, user_id, exchange_config=None):\n    \"\"\"\n    1. Get the dates that need to do trading till today for user {user_id}\n        dates[0] indicate the latest trading date of User{user_id},\n        if User{user_id} haven't do trading before, than dates[0] presents the init date of User{user_id}.\n    2. Set the exchange with exchange_config file\n\n        Parameter\n            um : UserManager()\n            today : pd.Timestamp()\n            user_id : str\n        :return\n            dates : list of pd.Timestamp\n            trade_exchange : Exchange()\n    \"\"\"\n    # get latest trading date for {user_id}\n    # if is None, indicate it haven't traded, then last trading date is init date of {user_id}\n    latest_trading_date = um.users[user_id].get_latest_trading_date()\n    if not latest_trading_date:\n        latest_trading_date = um.user_record.loc[user_id][0]\n\n    if str(today.date()) < latest_trading_date:\n        log.warning(\"user_id:{}, last trading date {} after today {}\".format(user_id, latest_trading_date, today))\n        return [pd.Timestamp(latest_trading_date)], None\n\n    dates = D.calendar(\n        start_time=pd.Timestamp(latest_trading_date),\n        end_time=pd.Timestamp(today),\n        future=True,\n    )\n    dates = list(dates)\n    dates.append(get_next_trading_date(dates[-1], future=True))\n    if exchange_config:\n        with pathlib.Path(exchange_config).open(\"r\") as fp:\n            exchange_paras = yaml.safe_load(fp)\n    else:\n        exchange_paras = {}\n    trade_exchange = Exchange(trade_dates=dates, **exchange_paras)\n    return dates, trade_exchange",
  "class User:\n    def __init__(self, account, strategy, model, verbose=False):\n        \"\"\"\n        A user in online system, which contains account, strategy and model three module.\n            Parameter\n                account : Account()\n                strategy :\n                    a strategy instance\n                model :\n                    a model instance\n                report_save_path : string\n                    the path to save report. Will not save report if None\n                verbose : bool\n                    Whether to print the info during the process\n        \"\"\"\n        self.logger = get_module_logger(\"User\", level=logging.INFO)\n        self.account = account\n        self.strategy = strategy\n        self.model = model\n        self.verbose = verbose\n\n    def init_state(self, date):\n        \"\"\"\n        init state when each trading date begin\n            Parameter\n                date : pd.Timestamp\n        \"\"\"\n        self.account.init_state(today=date)\n        self.strategy.init_state(trade_date=date, model=self.model, account=self.account)\n        return\n\n    def get_latest_trading_date(self):\n        \"\"\"\n        return the latest trading date for user {user_id}\n            Parameter\n                user_id : string\n            :return\n                date : string (e.g '2018-10-08')\n        \"\"\"\n        if not self.account.last_trade_date:\n            return None\n        return str(self.account.last_trade_date.date())\n\n    def showReport(self, benchmark=\"SH000905\"):\n        \"\"\"\n        show the newly report (mean, std, information_ratio, annualized_return)\n            Parameter\n                benchmark : string\n                    bench that to be compared, 'SH000905' for csi500\n        \"\"\"\n        bench = D.features([benchmark], [\"$change\"], disk_cache=True).loc[benchmark, \"$change\"]\n        report = self.account.report.generate_report_dataframe()\n        report[\"bench\"] = bench\n        analysis_result = {\"pred\": {}, \"excess_return_without_cost\": {}, \"excess_return_with_cost\": {}}\n        r = (report[\"return\"] - report[\"bench\"]).dropna()\n        analysis_result[\"excess_return_without_cost\"][0] = risk_analysis(r)\n        r = (report[\"return\"] - report[\"bench\"] - report[\"cost\"]).dropna()\n        analysis_result[\"excess_return_with_cost\"][0] = risk_analysis(r)\n        self.logger.info(\"Result of porfolio:\")\n        self.logger.info(\"excess_return_without_cost:\")\n        self.logger.info(analysis_result[\"excess_return_without_cost\"][0])\n        self.logger.info(\"excess_return_with_cost:\")\n        self.logger.info(analysis_result[\"excess_return_with_cost\"][0])\n        return report",
  "def __init__(self, account, strategy, model, verbose=False):\n        \"\"\"\n        A user in online system, which contains account, strategy and model three module.\n            Parameter\n                account : Account()\n                strategy :\n                    a strategy instance\n                model :\n                    a model instance\n                report_save_path : string\n                    the path to save report. Will not save report if None\n                verbose : bool\n                    Whether to print the info during the process\n        \"\"\"\n        self.logger = get_module_logger(\"User\", level=logging.INFO)\n        self.account = account\n        self.strategy = strategy\n        self.model = model\n        self.verbose = verbose",
  "def init_state(self, date):\n        \"\"\"\n        init state when each trading date begin\n            Parameter\n                date : pd.Timestamp\n        \"\"\"\n        self.account.init_state(today=date)\n        self.strategy.init_state(trade_date=date, model=self.model, account=self.account)\n        return",
  "def get_latest_trading_date(self):\n        \"\"\"\n        return the latest trading date for user {user_id}\n            Parameter\n                user_id : string\n            :return\n                date : string (e.g '2018-10-08')\n        \"\"\"\n        if not self.account.last_trade_date:\n            return None\n        return str(self.account.last_trade_date.date())",
  "def showReport(self, benchmark=\"SH000905\"):\n        \"\"\"\n        show the newly report (mean, std, information_ratio, annualized_return)\n            Parameter\n                benchmark : string\n                    bench that to be compared, 'SH000905' for csi500\n        \"\"\"\n        bench = D.features([benchmark], [\"$change\"], disk_cache=True).loc[benchmark, \"$change\"]\n        report = self.account.report.generate_report_dataframe()\n        report[\"bench\"] = bench\n        analysis_result = {\"pred\": {}, \"excess_return_without_cost\": {}, \"excess_return_with_cost\": {}}\n        r = (report[\"return\"] - report[\"bench\"]).dropna()\n        analysis_result[\"excess_return_without_cost\"][0] = risk_analysis(r)\n        r = (report[\"return\"] - report[\"bench\"] - report[\"cost\"]).dropna()\n        analysis_result[\"excess_return_with_cost\"][0] = risk_analysis(r)\n        self.logger.info(\"Result of porfolio:\")\n        self.logger.info(\"excess_return_without_cost:\")\n        self.logger.info(analysis_result[\"excess_return_without_cost\"][0])\n        self.logger.info(\"excess_return_with_cost:\")\n        self.logger.info(analysis_result[\"excess_return_with_cost\"][0])\n        return report",
  "class ScoreFileModel(Model):\n    \"\"\"\n    This model will load a score file, and return score at date exists in score file.\n    \"\"\"\n\n    def __init__(self, score_path):\n        pred_test = pd.read_csv(score_path, index_col=[0, 1], parse_dates=True, infer_datetime_format=True)\n        self.pred = pred_test\n\n    def get_data_with_date(self, date, **kwargs):\n        score = self.pred.loc(axis=0)[:, date]  # (stock_id, trade_date) multi_index, score in pdate\n        score_series = score.reset_index(level=\"datetime\", drop=True)[\n            \"score\"\n        ]  # pd.Series ; index:stock_id, data: score\n        return score_series\n\n    def predict(self, x_test, **kwargs):\n        return x_test\n\n    def score(self, x_test, **kwargs):\n        return\n\n    def fit(self, x_train, y_train, x_valid, y_valid, w_train=None, w_valid=None, **kwargs):\n        return\n\n    def save(self, fname, **kwargs):\n        return",
  "def __init__(self, score_path):\n        pred_test = pd.read_csv(score_path, index_col=[0, 1], parse_dates=True, infer_datetime_format=True)\n        self.pred = pred_test",
  "def get_data_with_date(self, date, **kwargs):\n        score = self.pred.loc(axis=0)[:, date]  # (stock_id, trade_date) multi_index, score in pdate\n        score_series = score.reset_index(level=\"datetime\", drop=True)[\n            \"score\"\n        ]  # pd.Series ; index:stock_id, data: score\n        return score_series",
  "def predict(self, x_test, **kwargs):\n        return x_test",
  "def score(self, x_test, **kwargs):\n        return",
  "def fit(self, x_train, y_train, x_valid, y_valid, w_train=None, w_valid=None, **kwargs):\n        return",
  "def save(self, fname, **kwargs):\n        return",
  "class UserManager:\n    def __init__(self, user_data_path, save_report=True):\n        \"\"\"\n        This module is designed to manager the users in online system\n        all users' data were assumed to be saved in user_data_path\n            Parameter\n                user_data_path : string\n                    data path that all users' data were saved in\n\n        variables:\n            data_path : string\n                data path that all users' data were saved in\n            users_file : string\n                A path of the file record the add_date of users\n            save_report : bool\n                whether to save report after each trading process\n            users : dict{}\n                [user_id]->User()\n                the python dict save instances of User() for each user_id\n            user_record : pd.Dataframe\n                user_id(string), add_date(string)\n                indicate the add_date for each users\n        \"\"\"\n        self.data_path = pathlib.Path(user_data_path)\n        self.users_file = self.data_path / \"users.csv\"\n        self.save_report = save_report\n        self.users = {}\n        self.user_record = None\n\n    def load_users(self):\n        \"\"\"\n        load all users' data into manager\n        \"\"\"\n        self.users = {}\n        self.user_record = pd.read_csv(self.users_file, index_col=0)\n        for user_id in self.user_record.index:\n            self.users[user_id] = self.load_user(user_id)\n\n    def load_user(self, user_id):\n        \"\"\"\n        return a instance of User() represents a user to be processed\n            Parameter\n                user_id : string\n            :return\n                user : User()\n        \"\"\"\n        account_path = self.data_path / user_id\n        strategy_file = self.data_path / user_id / \"strategy_{}.pickle\".format(user_id)\n        model_file = self.data_path / user_id / \"model_{}.pickle\".format(user_id)\n        cur_user_list = list(self.users)\n        if user_id in cur_user_list:\n            raise ValueError(\"User {} has been loaded\".format(user_id))\n        else:\n            trade_account = Account(0)\n            trade_account.load_account(account_path)\n            strategy = load_instance(strategy_file)\n            model = load_instance(model_file)\n            user = User(account=trade_account, strategy=strategy, model=model)\n            return user\n\n    def save_user_data(self, user_id):\n        \"\"\"\n        save a instance of User() to user data path\n            Parameter\n                user_id : string\n        \"\"\"\n        if not user_id in self.users:\n            raise ValueError(\"Cannot find user {}\".format(user_id))\n        self.users[user_id].account.save_account(self.data_path / user_id)\n        save_instance(\n            self.users[user_id].strategy,\n            self.data_path / user_id / \"strategy_{}.pickle\".format(user_id),\n        )\n        save_instance(\n            self.users[user_id].model,\n            self.data_path / user_id / \"model_{}.pickle\".format(user_id),\n        )\n\n    def add_user(self, user_id, config_file, add_date):\n        \"\"\"\n        add the new user {user_id} into user data\n        will create a new folder named \"{user_id}\" in user data path\n            Parameter\n                user_id : string\n                init_cash : int\n                config_file : str/pathlib.Path()\n                   path of config file\n        \"\"\"\n        config_file = pathlib.Path(config_file)\n        if not config_file.exists():\n            raise ValueError(\"Cannot find config file {}\".format(config_file))\n        user_path = self.data_path / user_id\n        if user_path.exists():\n            raise ValueError(\"User data for {} already exists\".format(user_id))\n\n        with config_file.open(\"r\") as fp:\n            config = yaml.safe_load(fp)\n        # load model\n        model = init_instance_by_config(config[\"model\"])\n\n        # load strategy\n        strategy = init_instance_by_config(config[\"strategy\"])\n        init_args = strategy.get_init_args_from_model(model, add_date)\n        strategy.init(**init_args)\n\n        # init Account\n        trade_account = Account(init_cash=config[\"init_cash\"])\n\n        # save user\n        user_path.mkdir()\n        save_instance(model, self.data_path / user_id / \"model_{}.pickle\".format(user_id))\n        save_instance(strategy, self.data_path / user_id / \"strategy_{}.pickle\".format(user_id))\n        trade_account.save_account(self.data_path / user_id)\n        user_record = pd.read_csv(self.users_file, index_col=0)\n        user_record.loc[user_id] = [add_date]\n        user_record.to_csv(self.users_file)\n\n    def remove_user(self, user_id):\n        \"\"\"\n        remove user {user_id} in current user dataset\n        will delete the folder \"{user_id}\" in user data path\n            :param\n                user_id : string\n        \"\"\"\n        user_path = self.data_path / user_id\n        if not user_path.exists():\n            raise ValueError(\"Cannot find user data {}\".format(user_id))\n        shutil.rmtree(user_path)\n        user_record = pd.read_csv(self.users_file, index_col=0)\n        user_record.drop([user_id], inplace=True)\n        user_record.to_csv(self.users_file)",
  "def __init__(self, user_data_path, save_report=True):\n        \"\"\"\n        This module is designed to manager the users in online system\n        all users' data were assumed to be saved in user_data_path\n            Parameter\n                user_data_path : string\n                    data path that all users' data were saved in\n\n        variables:\n            data_path : string\n                data path that all users' data were saved in\n            users_file : string\n                A path of the file record the add_date of users\n            save_report : bool\n                whether to save report after each trading process\n            users : dict{}\n                [user_id]->User()\n                the python dict save instances of User() for each user_id\n            user_record : pd.Dataframe\n                user_id(string), add_date(string)\n                indicate the add_date for each users\n        \"\"\"\n        self.data_path = pathlib.Path(user_data_path)\n        self.users_file = self.data_path / \"users.csv\"\n        self.save_report = save_report\n        self.users = {}\n        self.user_record = None",
  "def load_users(self):\n        \"\"\"\n        load all users' data into manager\n        \"\"\"\n        self.users = {}\n        self.user_record = pd.read_csv(self.users_file, index_col=0)\n        for user_id in self.user_record.index:\n            self.users[user_id] = self.load_user(user_id)",
  "def load_user(self, user_id):\n        \"\"\"\n        return a instance of User() represents a user to be processed\n            Parameter\n                user_id : string\n            :return\n                user : User()\n        \"\"\"\n        account_path = self.data_path / user_id\n        strategy_file = self.data_path / user_id / \"strategy_{}.pickle\".format(user_id)\n        model_file = self.data_path / user_id / \"model_{}.pickle\".format(user_id)\n        cur_user_list = list(self.users)\n        if user_id in cur_user_list:\n            raise ValueError(\"User {} has been loaded\".format(user_id))\n        else:\n            trade_account = Account(0)\n            trade_account.load_account(account_path)\n            strategy = load_instance(strategy_file)\n            model = load_instance(model_file)\n            user = User(account=trade_account, strategy=strategy, model=model)\n            return user",
  "def save_user_data(self, user_id):\n        \"\"\"\n        save a instance of User() to user data path\n            Parameter\n                user_id : string\n        \"\"\"\n        if not user_id in self.users:\n            raise ValueError(\"Cannot find user {}\".format(user_id))\n        self.users[user_id].account.save_account(self.data_path / user_id)\n        save_instance(\n            self.users[user_id].strategy,\n            self.data_path / user_id / \"strategy_{}.pickle\".format(user_id),\n        )\n        save_instance(\n            self.users[user_id].model,\n            self.data_path / user_id / \"model_{}.pickle\".format(user_id),\n        )",
  "def add_user(self, user_id, config_file, add_date):\n        \"\"\"\n        add the new user {user_id} into user data\n        will create a new folder named \"{user_id}\" in user data path\n            Parameter\n                user_id : string\n                init_cash : int\n                config_file : str/pathlib.Path()\n                   path of config file\n        \"\"\"\n        config_file = pathlib.Path(config_file)\n        if not config_file.exists():\n            raise ValueError(\"Cannot find config file {}\".format(config_file))\n        user_path = self.data_path / user_id\n        if user_path.exists():\n            raise ValueError(\"User data for {} already exists\".format(user_id))\n\n        with config_file.open(\"r\") as fp:\n            config = yaml.safe_load(fp)\n        # load model\n        model = init_instance_by_config(config[\"model\"])\n\n        # load strategy\n        strategy = init_instance_by_config(config[\"strategy\"])\n        init_args = strategy.get_init_args_from_model(model, add_date)\n        strategy.init(**init_args)\n\n        # init Account\n        trade_account = Account(init_cash=config[\"init_cash\"])\n\n        # save user\n        user_path.mkdir()\n        save_instance(model, self.data_path / user_id / \"model_{}.pickle\".format(user_id))\n        save_instance(strategy, self.data_path / user_id / \"strategy_{}.pickle\".format(user_id))\n        trade_account.save_account(self.data_path / user_id)\n        user_record = pd.read_csv(self.users_file, index_col=0)\n        user_record.loc[user_id] = [add_date]\n        user_record.to_csv(self.users_file)",
  "def remove_user(self, user_id):\n        \"\"\"\n        remove user {user_id} in current user dataset\n        will delete the folder \"{user_id}\" in user data path\n            :param\n                user_id : string\n        \"\"\"\n        user_path = self.data_path / user_id\n        if not user_path.exists():\n            raise ValueError(\"Cannot find user data {}\".format(user_id))\n        shutil.rmtree(user_path)\n        user_record = pd.read_csv(self.users_file, index_col=0)\n        user_record.drop([user_id], inplace=True)\n        user_record.to_csv(self.users_file)",
  "class BaseExecutor:\n    \"\"\"\n    # Strategy framework document\n\n    class Executor(BaseExecutor):\n    \"\"\"\n\n    def execute(self, trade_account, order_list, trade_date):\n        \"\"\"\n        return the executed result (trade_info) after trading at trade_date.\n        NOTICE: trade_account will not be modified after executing.\n            Parameter\n            ---------\n                trade_account : Account()\n                order_list : list\n                    [Order()]\n                trade_date : pd.Timestamp\n            Return\n            ---------\n            trade_info : list\n                    [Order(), float, float, float]\n        \"\"\"\n        raise NotImplementedError(\"get_execute_result for this model is not implemented.\")\n\n    def save_executed_file_from_trade_info(self, trade_info, user_path, trade_date):\n        \"\"\"\n        Save the trade_info to the .csv transaction file in disk\n        the columns of result file is\n        ['date', 'stock_id', 'direction', 'trade_val', 'trade_cost', 'trade_price', 'factor']\n            Parameter\n            ---------\n                trade_info : list of [Order(), float, float, float]\n                    (order, trade_val, trade_cost, trade_price), trade_info with out factor\n                user_path: str / pathlib.Path()\n                    the sub folder to save user data\n\n                transaction_path : string / pathlib.Path()\n        \"\"\"\n        YYYY, MM, DD = str(trade_date.date()).split(\"-\")\n        folder_path = pathlib.Path(user_path) / \"trade\" / YYYY / MM\n        if not folder_path.exists():\n            folder_path.mkdir(parents=True)\n        transaction_path = folder_path / \"transaction_{}.csv\".format(str(trade_date.date()))\n        columns = [\n            \"date\",\n            \"stock_id\",\n            \"direction\",\n            \"amount\",\n            \"trade_val\",\n            \"trade_cost\",\n            \"trade_price\",\n            \"factor\",\n        ]\n        data = []\n        for [order, trade_val, trade_cost, trade_price] in trade_info:\n            data.append(\n                [\n                    trade_date,\n                    order.stock_id,\n                    order.direction,\n                    order.amount,\n                    trade_val,\n                    trade_cost,\n                    trade_price,\n                    order.factor,\n                ]\n            )\n        df = pd.DataFrame(data, columns=columns)\n        df.to_csv(transaction_path, index=False)\n\n    def load_trade_info_from_executed_file(self, user_path, trade_date):\n        YYYY, MM, DD = str(trade_date.date()).split(\"-\")\n        file_path = pathlib.Path(user_path) / \"trade\" / YYYY / MM / \"transaction_{}.csv\".format(str(trade_date.date()))\n        if not file_path.exists():\n            raise ValueError(\"File {} not exists!\".format(file_path))\n\n        filedate = get_date_in_file_name(file_path)\n        transaction = pd.read_csv(file_path)\n        trade_info = []\n        for i in range(len(transaction)):\n            date = transaction.loc[i][\"date\"]\n            if not date == filedate:\n                continue\n                # raise ValueError(\"date in transaction file {} not equal to it's file date{}\".format(date, filedate))\n            order = Order(\n                stock_id=transaction.loc[i][\"stock_id\"],\n                amount=transaction.loc[i][\"amount\"],\n                trade_date=transaction.loc[i][\"date\"],\n                direction=transaction.loc[i][\"direction\"],\n                factor=transaction.loc[i][\"factor\"],\n            )\n            trade_val = transaction.loc[i][\"trade_val\"]\n            trade_cost = transaction.loc[i][\"trade_cost\"]\n            trade_price = transaction.loc[i][\"trade_price\"]\n            trade_info.append([order, trade_val, trade_cost, trade_price])\n        return trade_info",
  "class SimulatorExecutor(BaseExecutor):\n    def __init__(self, trade_exchange, verbose=False):\n        self.trade_exchange = trade_exchange\n        self.verbose = verbose\n        self.order_list = []\n\n    def execute(self, trade_account, order_list, trade_date):\n        \"\"\"\n        execute the order list, do the trading wil exchange at date.\n        Will not modify the trade_account.\n            Parameter\n                trade_account : Account()\n                order_list : list\n                    list or orders\n                trade_date : pd.Timestamp\n            :return:\n                trade_info : list of [Order(), float, float, float]\n                    (order, trade_val, trade_cost, trade_price), trade_info with out factor\n        \"\"\"\n        account = copy.deepcopy(trade_account)\n        trade_info = []\n\n        for order in order_list:\n            # check holding thresh is done in strategy\n            # if order.direction==0: # sell order\n            #     # checking holding thresh limit for sell order\n            #     if trade_account.current.get_stock_count(order.stock_id) < thresh:\n            #         # can not sell this code\n            #         continue\n            # is order executable\n            # check order\n            if self.trade_exchange.check_order(order) is True:\n                # execute the order\n                trade_val, trade_cost, trade_price = self.trade_exchange.deal_order(order, trade_account=account)\n                trade_info.append([order, trade_val, trade_cost, trade_price])\n                if self.verbose:\n                    if order.direction == Order.SELL:  # sell\n                        print(\n                            \"[I {:%Y-%m-%d}]: sell {}, price {:.2f}, amount {}, value {:.2f}.\".format(\n                                trade_date,\n                                order.stock_id,\n                                trade_price,\n                                order.deal_amount,\n                                trade_val,\n                            )\n                        )\n                    else:\n                        print(\n                            \"[I {:%Y-%m-%d}]: buy {}, price {:.2f}, amount {}, value {:.2f}.\".format(\n                                trade_date,\n                                order.stock_id,\n                                trade_price,\n                                order.deal_amount,\n                                trade_val,\n                            )\n                        )\n\n            else:\n                if self.verbose:\n                    print(\"[W {:%Y-%m-%d}]: {} wrong.\".format(trade_date, order.stock_id))\n                # do nothing\n                pass\n        return trade_info",
  "def save_score_series(score_series, user_path, trade_date):\n    \"\"\"Save the score_series into a .csv file.\n    The columns of saved file is\n        [stock_id, score]\n\n    Parameter\n    ---------\n        order_list: [Order()]\n            list of Order()\n        date: pd.Timestamp\n            the date to save the order list\n        user_path: str / pathlib.Path()\n            the sub folder to save user data\n    \"\"\"\n    user_path = pathlib.Path(user_path)\n    YYYY, MM, DD = str(trade_date.date()).split(\"-\")\n    folder_path = user_path / \"score\" / YYYY / MM\n    if not folder_path.exists():\n        folder_path.mkdir(parents=True)\n    file_path = folder_path / \"score_{}.csv\".format(str(trade_date.date()))\n    score_series.to_csv(file_path)",
  "def load_score_series(user_path, trade_date):\n    \"\"\"Save the score_series into a .csv file.\n    The columns of saved file is\n        [stock_id, score]\n\n    Parameter\n    ---------\n        order_list: [Order()]\n            list of Order()\n        date: pd.Timestamp\n            the date to save the order list\n        user_path: str / pathlib.Path()\n            the sub folder to save user data\n    \"\"\"\n    user_path = pathlib.Path(user_path)\n    YYYY, MM, DD = str(trade_date.date()).split(\"-\")\n    folder_path = user_path / \"score\" / YYYY / MM\n    if not folder_path.exists():\n        folder_path.mkdir(parents=True)\n    file_path = folder_path / \"score_{}.csv\".format(str(trade_date.date()))\n    score_series = pd.read_csv(file_path, index_col=0, header=None, names=[\"instrument\", \"score\"])\n    return score_series",
  "def save_order_list(order_list, user_path, trade_date):\n    \"\"\"\n    Save the order list into a json file.\n    Will calculate the real amount in order according to factors at date.\n\n    The format in json file like\n    {\"sell\": {\"stock_id\": amount, ...}\n    ,\"buy\": {\"stock_id\": amount, ...}}\n\n        :param\n            order_list: [Order()]\n                list of Order()\n            date: pd.Timestamp\n                the date to save the order list\n            user_path: str / pathlib.Path()\n                the sub folder to save user data\n    \"\"\"\n    user_path = pathlib.Path(user_path)\n    YYYY, MM, DD = str(trade_date.date()).split(\"-\")\n    folder_path = user_path / \"trade\" / YYYY / MM\n    if not folder_path.exists():\n        folder_path.mkdir(parents=True)\n    sell = {}\n    buy = {}\n    for order in order_list:\n        if order.direction == 0:  # sell\n            sell[order.stock_id] = [order.amount, order.factor]\n        else:\n            buy[order.stock_id] = [order.amount, order.factor]\n    order_dict = {\"sell\": sell, \"buy\": buy}\n    file_path = folder_path / \"orderlist_{}.json\".format(str(trade_date.date()))\n    with file_path.open(\"w\") as fp:\n        json.dump(order_dict, fp)",
  "def load_order_list(user_path, trade_date):\n    user_path = pathlib.Path(user_path)\n    YYYY, MM, DD = str(trade_date.date()).split(\"-\")\n    path = user_path / \"trade\" / YYYY / MM / \"orderlist_{}.json\".format(str(trade_date.date()))\n    if not path.exists():\n        raise ValueError(\"File {} not exists!\".format(path))\n    # get orders\n    with path.open(\"r\") as fp:\n        order_dict = json.load(fp)\n    order_list = []\n    for stock_id in order_dict[\"sell\"]:\n        amount, factor = order_dict[\"sell\"][stock_id]\n        order = Order(\n            stock_id=stock_id,\n            amount=amount,\n            trade_date=pd.Timestamp(trade_date),\n            direction=Order.SELL,\n            factor=factor,\n        )\n        order_list.append(order)\n    for stock_id in order_dict[\"buy\"]:\n        amount, factor = order_dict[\"buy\"][stock_id]\n        order = Order(\n            stock_id=stock_id,\n            amount=amount,\n            trade_date=pd.Timestamp(trade_date),\n            direction=Order.BUY,\n            factor=factor,\n        )\n        order_list.append(order)\n    return order_list",
  "def execute(self, trade_account, order_list, trade_date):\n        \"\"\"\n        return the executed result (trade_info) after trading at trade_date.\n        NOTICE: trade_account will not be modified after executing.\n            Parameter\n            ---------\n                trade_account : Account()\n                order_list : list\n                    [Order()]\n                trade_date : pd.Timestamp\n            Return\n            ---------\n            trade_info : list\n                    [Order(), float, float, float]\n        \"\"\"\n        raise NotImplementedError(\"get_execute_result for this model is not implemented.\")",
  "def save_executed_file_from_trade_info(self, trade_info, user_path, trade_date):\n        \"\"\"\n        Save the trade_info to the .csv transaction file in disk\n        the columns of result file is\n        ['date', 'stock_id', 'direction', 'trade_val', 'trade_cost', 'trade_price', 'factor']\n            Parameter\n            ---------\n                trade_info : list of [Order(), float, float, float]\n                    (order, trade_val, trade_cost, trade_price), trade_info with out factor\n                user_path: str / pathlib.Path()\n                    the sub folder to save user data\n\n                transaction_path : string / pathlib.Path()\n        \"\"\"\n        YYYY, MM, DD = str(trade_date.date()).split(\"-\")\n        folder_path = pathlib.Path(user_path) / \"trade\" / YYYY / MM\n        if not folder_path.exists():\n            folder_path.mkdir(parents=True)\n        transaction_path = folder_path / \"transaction_{}.csv\".format(str(trade_date.date()))\n        columns = [\n            \"date\",\n            \"stock_id\",\n            \"direction\",\n            \"amount\",\n            \"trade_val\",\n            \"trade_cost\",\n            \"trade_price\",\n            \"factor\",\n        ]\n        data = []\n        for [order, trade_val, trade_cost, trade_price] in trade_info:\n            data.append(\n                [\n                    trade_date,\n                    order.stock_id,\n                    order.direction,\n                    order.amount,\n                    trade_val,\n                    trade_cost,\n                    trade_price,\n                    order.factor,\n                ]\n            )\n        df = pd.DataFrame(data, columns=columns)\n        df.to_csv(transaction_path, index=False)",
  "def load_trade_info_from_executed_file(self, user_path, trade_date):\n        YYYY, MM, DD = str(trade_date.date()).split(\"-\")\n        file_path = pathlib.Path(user_path) / \"trade\" / YYYY / MM / \"transaction_{}.csv\".format(str(trade_date.date()))\n        if not file_path.exists():\n            raise ValueError(\"File {} not exists!\".format(file_path))\n\n        filedate = get_date_in_file_name(file_path)\n        transaction = pd.read_csv(file_path)\n        trade_info = []\n        for i in range(len(transaction)):\n            date = transaction.loc[i][\"date\"]\n            if not date == filedate:\n                continue\n                # raise ValueError(\"date in transaction file {} not equal to it's file date{}\".format(date, filedate))\n            order = Order(\n                stock_id=transaction.loc[i][\"stock_id\"],\n                amount=transaction.loc[i][\"amount\"],\n                trade_date=transaction.loc[i][\"date\"],\n                direction=transaction.loc[i][\"direction\"],\n                factor=transaction.loc[i][\"factor\"],\n            )\n            trade_val = transaction.loc[i][\"trade_val\"]\n            trade_cost = transaction.loc[i][\"trade_cost\"]\n            trade_price = transaction.loc[i][\"trade_price\"]\n            trade_info.append([order, trade_val, trade_cost, trade_price])\n        return trade_info",
  "def __init__(self, trade_exchange, verbose=False):\n        self.trade_exchange = trade_exchange\n        self.verbose = verbose\n        self.order_list = []",
  "def execute(self, trade_account, order_list, trade_date):\n        \"\"\"\n        execute the order list, do the trading wil exchange at date.\n        Will not modify the trade_account.\n            Parameter\n                trade_account : Account()\n                order_list : list\n                    list or orders\n                trade_date : pd.Timestamp\n            :return:\n                trade_info : list of [Order(), float, float, float]\n                    (order, trade_val, trade_cost, trade_price), trade_info with out factor\n        \"\"\"\n        account = copy.deepcopy(trade_account)\n        trade_info = []\n\n        for order in order_list:\n            # check holding thresh is done in strategy\n            # if order.direction==0: # sell order\n            #     # checking holding thresh limit for sell order\n            #     if trade_account.current.get_stock_count(order.stock_id) < thresh:\n            #         # can not sell this code\n            #         continue\n            # is order executable\n            # check order\n            if self.trade_exchange.check_order(order) is True:\n                # execute the order\n                trade_val, trade_cost, trade_price = self.trade_exchange.deal_order(order, trade_account=account)\n                trade_info.append([order, trade_val, trade_cost, trade_price])\n                if self.verbose:\n                    if order.direction == Order.SELL:  # sell\n                        print(\n                            \"[I {:%Y-%m-%d}]: sell {}, price {:.2f}, amount {}, value {:.2f}.\".format(\n                                trade_date,\n                                order.stock_id,\n                                trade_price,\n                                order.deal_amount,\n                                trade_val,\n                            )\n                        )\n                    else:\n                        print(\n                            \"[I {:%Y-%m-%d}]: buy {}, price {:.2f}, amount {}, value {:.2f}.\".format(\n                                trade_date,\n                                order.stock_id,\n                                trade_price,\n                                order.deal_amount,\n                                trade_val,\n                            )\n                        )\n\n            else:\n                if self.verbose:\n                    print(\"[W {:%Y-%m-%d}]: {} wrong.\".format(trade_date, order.stock_id))\n                # do nothing\n                pass\n        return trade_info",
  "class Operator:\n    def __init__(self, client: str):\n        \"\"\"\n        Parameters\n        ----------\n            client: str\n                The qlib client config file(.yaml)\n        \"\"\"\n        self.logger = get_module_logger(\"online operator\", level=logging.INFO)\n        self.client = client\n\n    @staticmethod\n    def init(client, path, date=None):\n        \"\"\"Initial UserManager(), get predict date and trade date\n        Parameters\n        ----------\n            client: str\n                The qlib client config file(.yaml)\n            path : str\n                Path to save user account.\n            date : str (YYYY-MM-DD)\n                Trade date, when the generated order list will be traded.\n        Return\n        ----------\n            um: UserManager()\n            pred_date: pd.Timestamp\n            trade_date: pd.Timestamp\n        \"\"\"\n        qlib.init_from_yaml_conf(client)\n        um = UserManager(user_data_path=pathlib.Path(path))\n        um.load_users()\n        if not date:\n            trade_date, pred_date = None, None\n        else:\n            trade_date = pd.Timestamp(date)\n            if not is_tradable_date(trade_date):\n                raise ValueError(\"trade date is not tradable date\".format(trade_date.date()))\n            pred_date = get_pre_trading_date(trade_date, future=True)\n        return um, pred_date, trade_date\n\n    def add_user(self, id, config, path, date):\n        \"\"\"Add a new user into the a folder to run 'online' module.\n\n        Parameters\n        ----------\n        id : str\n            User id, should be unique.\n        config : str\n            The file path (yaml) of user config\n        path : str\n            Path to save user account.\n        date : str (YYYY-MM-DD)\n            The date that user account was added.\n        \"\"\"\n        create_user_folder(path)\n        qlib.init_from_yaml_conf(self.client)\n        um = UserManager(user_data_path=path)\n        add_date = D.calendar(end_time=date)[-1]\n        if not is_tradable_date(add_date):\n            raise ValueError(\"add date is not tradable date\".format(add_date.date()))\n        um.add_user(user_id=id, config_file=config, add_date=add_date)\n\n    def remove_user(self, id, path):\n        \"\"\"Remove user from folder used in 'online' module.\n\n        Parameters\n        ----------\n        id : str\n            User id, should be unique.\n        path : str\n            Path to save user account.\n        \"\"\"\n        um = UserManager(user_data_path=path)\n        um.remove_user(user_id=id)\n\n    def generate(self, date, path):\n        \"\"\"Generate order list that will be traded at 'date'.\n\n        Parameters\n        ----------\n        date : str (YYYY-MM-DD)\n            Trade date, when the generated order list will be traded.\n        path : str\n            Path to save user account.\n        \"\"\"\n        um, pred_date, trade_date = self.init(self.client, path, date)\n        for user_id, user in um.users.items():\n            dates, trade_exchange = prepare(um, pred_date, user_id)\n            # get and save the score at predict date\n            input_data = user.model.get_data_with_date(pred_date)\n            score_series = user.model.predict(input_data)\n            save_score_series(score_series, (pathlib.Path(path) / user_id), trade_date)\n\n            # update strategy (and model)\n            user.strategy.update(score_series, pred_date, trade_date)\n\n            # generate and save order list\n            order_list = user.strategy.generate_order_list(\n                score_series=score_series,\n                current=user.account.current,\n                trade_exchange=trade_exchange,\n                trade_date=trade_date,\n            )\n            save_order_list(\n                order_list=order_list,\n                user_path=(pathlib.Path(path) / user_id),\n                trade_date=trade_date,\n            )\n            self.logger.info(\"Generate order list at {} for {}\".format(trade_date, user_id))\n            um.save_user_data(user_id)\n\n    def execute(self, date, exchange_config, path):\n        \"\"\"Execute the orderlist at 'date'.\n\n        Parameters\n        ----------\n           date : str (YYYY-MM-DD)\n               Trade date, that the generated order list will be traded.\n           exchange_config: str\n               The file path (yaml) of exchange config\n           path : str\n               Path to save user account.\n        \"\"\"\n        um, pred_date, trade_date = self.init(self.client, path, date)\n        for user_id, user in um.users.items():\n            dates, trade_exchange = prepare(um, trade_date, user_id, exchange_config)\n            executor = SimulatorExecutor(trade_exchange=trade_exchange)\n            if str(dates[0].date()) != str(pred_date.date()):\n                raise ValueError(\n                    \"The account data is not newest! last trading date {}, today {}\".format(\n                        dates[0].date(), trade_date.date()\n                    )\n                )\n\n            # load and execute the order list\n            # will not modify the trade_account after executing\n            order_list = load_order_list(user_path=(pathlib.Path(path) / user_id), trade_date=trade_date)\n            trade_info = executor.execute(order_list=order_list, trade_account=user.account, trade_date=trade_date)\n            executor.save_executed_file_from_trade_info(\n                trade_info=trade_info,\n                user_path=(pathlib.Path(path) / user_id),\n                trade_date=trade_date,\n            )\n            self.logger.info(\"execute order list at {} for {}\".format(trade_date.date(), user_id))\n\n    def update(self, date, path, type=\"SIM\"):\n        \"\"\"Update account at 'date'.\n\n        Parameters\n        ----------\n        date : str (YYYY-MM-DD)\n            Trade date, that the generated order list will be traded.\n        path : str\n            Path to save user account.\n        type : str\n            which executor was been used to execute the order list\n            'SIM': SimulatorExecutor()\n        \"\"\"\n        if type not in [\"SIM\", \"YC\"]:\n            raise ValueError(\"type is invalid, {}\".format(type))\n        um, pred_date, trade_date = self.init(self.client, path, date)\n        for user_id, user in um.users.items():\n            dates, trade_exchange = prepare(um, trade_date, user_id)\n            if type == \"SIM\":\n                executor = SimulatorExecutor(trade_exchange=trade_exchange)\n            else:\n                raise ValueError(\"not found executor\")\n            # dates[0] is the last_trading_date\n            if str(dates[0].date()) > str(pred_date.date()):\n                raise ValueError(\n                    \"The account data is not newest! last trading date {}, today {}\".format(\n                        dates[0].date(), trade_date.date()\n                    )\n                )\n            # load trade info and update account\n            trade_info = executor.load_trade_info_from_executed_file(\n                user_path=(pathlib.Path(path) / user_id), trade_date=trade_date\n            )\n            score_series = load_score_series((pathlib.Path(path) / user_id), trade_date)\n            update_account(user.account, trade_info, trade_exchange, trade_date)\n\n            report = user.account.report.generate_report_dataframe()\n            self.logger.info(report)\n            um.save_user_data(user_id)\n            self.logger.info(\"Update account state {} for {}\".format(trade_date, user_id))\n\n    def simulate(self, id, config, exchange_config, start, end, path, bench=\"SH000905\"):\n        \"\"\"Run the ( generate_order_list -> execute_order_list -> update_account) process everyday\n            from start date to end date.\n\n        Parameters\n        ----------\n        id : str\n            user id, need to be unique\n        config : str\n            The file path (yaml) of user config\n        exchange_config: str\n            The file path (yaml) of exchange config\n        start : str \"YYYY-MM-DD\"\n            The start date to run the online simulate\n        end : str \"YYYY-MM-DD\"\n            The end date to run the online simulate\n        path : str\n            Path to save user account.\n        bench : str\n            The benchmark that our result compared with.\n            'SH000905' for csi500, 'SH000300' for csi300\n        \"\"\"\n        # Clear the current user if exists, then add a new user.\n        create_user_folder(path)\n        um = self.init(self.client, path, None)[0]\n        start_date, end_date = pd.Timestamp(start), pd.Timestamp(end)\n        try:\n            um.remove_user(user_id=id)\n        except BaseException:\n            pass\n        um.add_user(user_id=id, config_file=config, add_date=pd.Timestamp(start_date))\n\n        # Do the online simulate\n        um.load_users()\n        user = um.users[id]\n        dates, trade_exchange = prepare(um, end_date, id, exchange_config)\n        executor = SimulatorExecutor(trade_exchange=trade_exchange)\n        for pred_date, trade_date in zip(dates[:-2], dates[1:-1]):\n            user_path = pathlib.Path(path) / id\n\n            # 1. load and save score_series\n            input_data = user.model.get_data_with_date(pred_date)\n            score_series = user.model.predict(input_data)\n            save_score_series(score_series, (pathlib.Path(path) / id), trade_date)\n\n            # 2. update strategy (and model)\n            user.strategy.update(score_series, pred_date, trade_date)\n\n            # 3. generate and save order list\n            order_list = user.strategy.generate_order_list(\n                score_series=score_series,\n                current=user.account.current,\n                trade_exchange=trade_exchange,\n                trade_date=trade_date,\n            )\n            save_order_list(order_list=order_list, user_path=user_path, trade_date=trade_date)\n\n            # 4. auto execute order list\n            order_list = load_order_list(user_path=user_path, trade_date=trade_date)\n            trade_info = executor.execute(trade_account=user.account, order_list=order_list, trade_date=trade_date)\n            executor.save_executed_file_from_trade_info(\n                trade_info=trade_info, user_path=user_path, trade_date=trade_date\n            )\n            # 5. update account state\n            trade_info = executor.load_trade_info_from_executed_file(user_path=user_path, trade_date=trade_date)\n            update_account(user.account, trade_info, trade_exchange, trade_date)\n        report = user.account.report.generate_report_dataframe()\n        self.logger.info(report)\n        um.save_user_data(id)\n        self.show(id, path, bench)\n\n    def show(self, id, path, bench=\"SH000905\"):\n        \"\"\"show the newly report (mean, std, information_ratio, annualized_return)\n\n        Parameters\n        ----------\n        id : str\n            user id, need to be unique\n        path : str\n            Path to save user account.\n        bench : str\n            The benchmark that our result compared with.\n            'SH000905' for csi500, 'SH000300' for csi300\n        \"\"\"\n        um = self.init(self.client, path, None)[0]\n        if id not in um.users:\n            raise ValueError(\"Cannot find user \".format(id))\n        bench = D.features([bench], [\"$change\"]).loc[bench, \"$change\"]\n        report = um.users[id].account.report.generate_report_dataframe()\n        report[\"bench\"] = bench\n        analysis_result = {}\n        r = (report[\"return\"] - report[\"bench\"]).dropna()\n        analysis_result[\"excess_return_without_cost\"] = risk_analysis(r)\n        r = (report[\"return\"] - report[\"bench\"] - report[\"cost\"]).dropna()\n        analysis_result[\"excess_return_with_cost\"] = risk_analysis(r)\n        print(\"Result:\")\n        print(\"excess_return_without_cost:\")\n        print(analysis_result[\"excess_return_without_cost\"])\n        print(\"excess_return_with_cost:\")\n        print(analysis_result[\"excess_return_with_cost\"])",
  "def run():\n    fire.Fire(Operator)",
  "def __init__(self, client: str):\n        \"\"\"\n        Parameters\n        ----------\n            client: str\n                The qlib client config file(.yaml)\n        \"\"\"\n        self.logger = get_module_logger(\"online operator\", level=logging.INFO)\n        self.client = client",
  "def init(client, path, date=None):\n        \"\"\"Initial UserManager(), get predict date and trade date\n        Parameters\n        ----------\n            client: str\n                The qlib client config file(.yaml)\n            path : str\n                Path to save user account.\n            date : str (YYYY-MM-DD)\n                Trade date, when the generated order list will be traded.\n        Return\n        ----------\n            um: UserManager()\n            pred_date: pd.Timestamp\n            trade_date: pd.Timestamp\n        \"\"\"\n        qlib.init_from_yaml_conf(client)\n        um = UserManager(user_data_path=pathlib.Path(path))\n        um.load_users()\n        if not date:\n            trade_date, pred_date = None, None\n        else:\n            trade_date = pd.Timestamp(date)\n            if not is_tradable_date(trade_date):\n                raise ValueError(\"trade date is not tradable date\".format(trade_date.date()))\n            pred_date = get_pre_trading_date(trade_date, future=True)\n        return um, pred_date, trade_date",
  "def add_user(self, id, config, path, date):\n        \"\"\"Add a new user into the a folder to run 'online' module.\n\n        Parameters\n        ----------\n        id : str\n            User id, should be unique.\n        config : str\n            The file path (yaml) of user config\n        path : str\n            Path to save user account.\n        date : str (YYYY-MM-DD)\n            The date that user account was added.\n        \"\"\"\n        create_user_folder(path)\n        qlib.init_from_yaml_conf(self.client)\n        um = UserManager(user_data_path=path)\n        add_date = D.calendar(end_time=date)[-1]\n        if not is_tradable_date(add_date):\n            raise ValueError(\"add date is not tradable date\".format(add_date.date()))\n        um.add_user(user_id=id, config_file=config, add_date=add_date)",
  "def remove_user(self, id, path):\n        \"\"\"Remove user from folder used in 'online' module.\n\n        Parameters\n        ----------\n        id : str\n            User id, should be unique.\n        path : str\n            Path to save user account.\n        \"\"\"\n        um = UserManager(user_data_path=path)\n        um.remove_user(user_id=id)",
  "def generate(self, date, path):\n        \"\"\"Generate order list that will be traded at 'date'.\n\n        Parameters\n        ----------\n        date : str (YYYY-MM-DD)\n            Trade date, when the generated order list will be traded.\n        path : str\n            Path to save user account.\n        \"\"\"\n        um, pred_date, trade_date = self.init(self.client, path, date)\n        for user_id, user in um.users.items():\n            dates, trade_exchange = prepare(um, pred_date, user_id)\n            # get and save the score at predict date\n            input_data = user.model.get_data_with_date(pred_date)\n            score_series = user.model.predict(input_data)\n            save_score_series(score_series, (pathlib.Path(path) / user_id), trade_date)\n\n            # update strategy (and model)\n            user.strategy.update(score_series, pred_date, trade_date)\n\n            # generate and save order list\n            order_list = user.strategy.generate_order_list(\n                score_series=score_series,\n                current=user.account.current,\n                trade_exchange=trade_exchange,\n                trade_date=trade_date,\n            )\n            save_order_list(\n                order_list=order_list,\n                user_path=(pathlib.Path(path) / user_id),\n                trade_date=trade_date,\n            )\n            self.logger.info(\"Generate order list at {} for {}\".format(trade_date, user_id))\n            um.save_user_data(user_id)",
  "def execute(self, date, exchange_config, path):\n        \"\"\"Execute the orderlist at 'date'.\n\n        Parameters\n        ----------\n           date : str (YYYY-MM-DD)\n               Trade date, that the generated order list will be traded.\n           exchange_config: str\n               The file path (yaml) of exchange config\n           path : str\n               Path to save user account.\n        \"\"\"\n        um, pred_date, trade_date = self.init(self.client, path, date)\n        for user_id, user in um.users.items():\n            dates, trade_exchange = prepare(um, trade_date, user_id, exchange_config)\n            executor = SimulatorExecutor(trade_exchange=trade_exchange)\n            if str(dates[0].date()) != str(pred_date.date()):\n                raise ValueError(\n                    \"The account data is not newest! last trading date {}, today {}\".format(\n                        dates[0].date(), trade_date.date()\n                    )\n                )\n\n            # load and execute the order list\n            # will not modify the trade_account after executing\n            order_list = load_order_list(user_path=(pathlib.Path(path) / user_id), trade_date=trade_date)\n            trade_info = executor.execute(order_list=order_list, trade_account=user.account, trade_date=trade_date)\n            executor.save_executed_file_from_trade_info(\n                trade_info=trade_info,\n                user_path=(pathlib.Path(path) / user_id),\n                trade_date=trade_date,\n            )\n            self.logger.info(\"execute order list at {} for {}\".format(trade_date.date(), user_id))",
  "def update(self, date, path, type=\"SIM\"):\n        \"\"\"Update account at 'date'.\n\n        Parameters\n        ----------\n        date : str (YYYY-MM-DD)\n            Trade date, that the generated order list will be traded.\n        path : str\n            Path to save user account.\n        type : str\n            which executor was been used to execute the order list\n            'SIM': SimulatorExecutor()\n        \"\"\"\n        if type not in [\"SIM\", \"YC\"]:\n            raise ValueError(\"type is invalid, {}\".format(type))\n        um, pred_date, trade_date = self.init(self.client, path, date)\n        for user_id, user in um.users.items():\n            dates, trade_exchange = prepare(um, trade_date, user_id)\n            if type == \"SIM\":\n                executor = SimulatorExecutor(trade_exchange=trade_exchange)\n            else:\n                raise ValueError(\"not found executor\")\n            # dates[0] is the last_trading_date\n            if str(dates[0].date()) > str(pred_date.date()):\n                raise ValueError(\n                    \"The account data is not newest! last trading date {}, today {}\".format(\n                        dates[0].date(), trade_date.date()\n                    )\n                )\n            # load trade info and update account\n            trade_info = executor.load_trade_info_from_executed_file(\n                user_path=(pathlib.Path(path) / user_id), trade_date=trade_date\n            )\n            score_series = load_score_series((pathlib.Path(path) / user_id), trade_date)\n            update_account(user.account, trade_info, trade_exchange, trade_date)\n\n            report = user.account.report.generate_report_dataframe()\n            self.logger.info(report)\n            um.save_user_data(user_id)\n            self.logger.info(\"Update account state {} for {}\".format(trade_date, user_id))",
  "def simulate(self, id, config, exchange_config, start, end, path, bench=\"SH000905\"):\n        \"\"\"Run the ( generate_order_list -> execute_order_list -> update_account) process everyday\n            from start date to end date.\n\n        Parameters\n        ----------\n        id : str\n            user id, need to be unique\n        config : str\n            The file path (yaml) of user config\n        exchange_config: str\n            The file path (yaml) of exchange config\n        start : str \"YYYY-MM-DD\"\n            The start date to run the online simulate\n        end : str \"YYYY-MM-DD\"\n            The end date to run the online simulate\n        path : str\n            Path to save user account.\n        bench : str\n            The benchmark that our result compared with.\n            'SH000905' for csi500, 'SH000300' for csi300\n        \"\"\"\n        # Clear the current user if exists, then add a new user.\n        create_user_folder(path)\n        um = self.init(self.client, path, None)[0]\n        start_date, end_date = pd.Timestamp(start), pd.Timestamp(end)\n        try:\n            um.remove_user(user_id=id)\n        except BaseException:\n            pass\n        um.add_user(user_id=id, config_file=config, add_date=pd.Timestamp(start_date))\n\n        # Do the online simulate\n        um.load_users()\n        user = um.users[id]\n        dates, trade_exchange = prepare(um, end_date, id, exchange_config)\n        executor = SimulatorExecutor(trade_exchange=trade_exchange)\n        for pred_date, trade_date in zip(dates[:-2], dates[1:-1]):\n            user_path = pathlib.Path(path) / id\n\n            # 1. load and save score_series\n            input_data = user.model.get_data_with_date(pred_date)\n            score_series = user.model.predict(input_data)\n            save_score_series(score_series, (pathlib.Path(path) / id), trade_date)\n\n            # 2. update strategy (and model)\n            user.strategy.update(score_series, pred_date, trade_date)\n\n            # 3. generate and save order list\n            order_list = user.strategy.generate_order_list(\n                score_series=score_series,\n                current=user.account.current,\n                trade_exchange=trade_exchange,\n                trade_date=trade_date,\n            )\n            save_order_list(order_list=order_list, user_path=user_path, trade_date=trade_date)\n\n            # 4. auto execute order list\n            order_list = load_order_list(user_path=user_path, trade_date=trade_date)\n            trade_info = executor.execute(trade_account=user.account, order_list=order_list, trade_date=trade_date)\n            executor.save_executed_file_from_trade_info(\n                trade_info=trade_info, user_path=user_path, trade_date=trade_date\n            )\n            # 5. update account state\n            trade_info = executor.load_trade_info_from_executed_file(user_path=user_path, trade_date=trade_date)\n            update_account(user.account, trade_info, trade_exchange, trade_date)\n        report = user.account.report.generate_report_dataframe()\n        self.logger.info(report)\n        um.save_user_data(id)\n        self.show(id, path, bench)",
  "def show(self, id, path, bench=\"SH000905\"):\n        \"\"\"show the newly report (mean, std, information_ratio, annualized_return)\n\n        Parameters\n        ----------\n        id : str\n            user id, need to be unique\n        path : str\n            Path to save user account.\n        bench : str\n            The benchmark that our result compared with.\n            'SH000905' for csi500, 'SH000300' for csi300\n        \"\"\"\n        um = self.init(self.client, path, None)[0]\n        if id not in um.users:\n            raise ValueError(\"Cannot find user \".format(id))\n        bench = D.features([bench], [\"$change\"]).loc[bench, \"$change\"]\n        report = um.users[id].account.report.generate_report_dataframe()\n        report[\"bench\"] = bench\n        analysis_result = {}\n        r = (report[\"return\"] - report[\"bench\"]).dropna()\n        analysis_result[\"excess_return_without_cost\"] = risk_analysis(r)\n        r = (report[\"return\"] - report[\"bench\"] - report[\"cost\"]).dropna()\n        analysis_result[\"excess_return_with_cost\"] = risk_analysis(r)\n        print(\"Result:\")\n        print(\"excess_return_without_cost:\")\n        print(analysis_result[\"excess_return_without_cost\"])\n        print(\"excess_return_with_cost:\")\n        print(analysis_result[\"excess_return_with_cost\"])",
  "class ALSTM(Model):\n    \"\"\"ALSTM Model\n\n    Parameters\n    ----------\n    d_feat : int\n        input dimension for each time step\n    metric: str\n        the evaluate metric used in early stop\n    optimizer : str\n        optimizer name\n    GPU : int\n        the GPU ID used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"ALSTM\")\n        self.logger.info(\"ALSTM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n\n        self.logger.info(\n            \"ALSTM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                self.device,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.ALSTM_model = ALSTMModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.ALSTM_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.ALSTM_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.ALSTM_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.ALSTM_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.ALSTM_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n\n        self.ALSTM_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.ALSTM_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.ALSTM_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.ALSTM_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.ALSTM_model(feature)\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid, df_test = dataset.prepare(\n            [\"train\", \"valid\", \"test\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.ALSTM_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.ALSTM_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.ALSTM_model.eval()\n        x_values = x_test.values\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.ALSTM_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "class ALSTMModel(nn.Module):\n    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type=\"GRU\"):\n        super().__init__()\n        self.hid_size = hidden_size\n        self.input_size = d_feat\n        self.dropout = dropout\n        self.rnn_type = rnn_type\n        self.rnn_layer = num_layers\n        self._build_model()\n\n    def _build_model(self):\n        try:\n            klass = getattr(nn, self.rnn_type.upper())\n        except:\n            raise ValueError(\"unknown rnn_type `%s`\" % self.rnn_type)\n        self.net = nn.Sequential()\n        self.net.add_module(\"fc_in\", nn.Linear(in_features=self.input_size, out_features=self.hid_size))\n        self.net.add_module(\"act\", nn.Tanh())\n        self.rnn = klass(\n            input_size=self.hid_size,\n            hidden_size=self.hid_size,\n            num_layers=self.rnn_layer,\n            batch_first=True,\n            dropout=self.dropout,\n        )\n        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)\n        self.att_net = nn.Sequential()\n        self.att_net.add_module(\n            \"att_fc_in\",\n            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),\n        )\n        self.att_net.add_module(\"att_dropout\", torch.nn.Dropout(self.dropout))\n        self.att_net.add_module(\"att_act\", nn.Tanh())\n        self.att_net.add_module(\n            \"att_fc_out\",\n            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),\n        )\n        self.att_net.add_module(\"att_softmax\", nn.Softmax(dim=1))\n\n    def forward(self, inputs):\n        # inputs: [batch_size, input_size*input_day]\n        inputs = inputs.view(len(inputs), self.input_size, -1)\n        inputs = inputs.permute(0, 2, 1)  # [batch, input_size, seq_len] -> [batch, seq_len, input_size]\n        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]\n        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]\n        out_att = torch.mul(rnn_out, attention_score)\n        out_att = torch.sum(out_att, dim=1)\n        out = self.fc_out(\n            torch.cat((rnn_out[:, -1, :], out_att), dim=1)\n        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]\n        return out[..., 0]",
  "def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"ALSTM\")\n        self.logger.info(\"ALSTM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n\n        self.logger.info(\n            \"ALSTM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                self.device,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.ALSTM_model = ALSTMModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.ALSTM_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.ALSTM_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.ALSTM_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.ALSTM_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.ALSTM_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n\n        self.ALSTM_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.ALSTM_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.ALSTM_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.ALSTM_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.ALSTM_model(feature)\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid, df_test = dataset.prepare(\n            [\"train\", \"valid\", \"test\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.ALSTM_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.ALSTM_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.ALSTM_model.eval()\n        x_values = x_test.values\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.ALSTM_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type=\"GRU\"):\n        super().__init__()\n        self.hid_size = hidden_size\n        self.input_size = d_feat\n        self.dropout = dropout\n        self.rnn_type = rnn_type\n        self.rnn_layer = num_layers\n        self._build_model()",
  "def _build_model(self):\n        try:\n            klass = getattr(nn, self.rnn_type.upper())\n        except:\n            raise ValueError(\"unknown rnn_type `%s`\" % self.rnn_type)\n        self.net = nn.Sequential()\n        self.net.add_module(\"fc_in\", nn.Linear(in_features=self.input_size, out_features=self.hid_size))\n        self.net.add_module(\"act\", nn.Tanh())\n        self.rnn = klass(\n            input_size=self.hid_size,\n            hidden_size=self.hid_size,\n            num_layers=self.rnn_layer,\n            batch_first=True,\n            dropout=self.dropout,\n        )\n        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)\n        self.att_net = nn.Sequential()\n        self.att_net.add_module(\n            \"att_fc_in\",\n            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),\n        )\n        self.att_net.add_module(\"att_dropout\", torch.nn.Dropout(self.dropout))\n        self.att_net.add_module(\"att_act\", nn.Tanh())\n        self.att_net.add_module(\n            \"att_fc_out\",\n            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),\n        )\n        self.att_net.add_module(\"att_softmax\", nn.Softmax(dim=1))",
  "def forward(self, inputs):\n        # inputs: [batch_size, input_size*input_day]\n        inputs = inputs.view(len(inputs), self.input_size, -1)\n        inputs = inputs.permute(0, 2, 1)  # [batch, input_size, seq_len] -> [batch, seq_len, input_size]\n        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]\n        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]\n        out_att = torch.mul(rnn_out, attention_score)\n        out_att = torch.sum(out_att, dim=1)\n        out = self.fc_out(\n            torch.cat((rnn_out[:, -1, :], out_att), dim=1)\n        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]\n        return out[..., 0]",
  "class ALSTM(Model):\n    \"\"\"ALSTM Model\n\n    Parameters\n    ----------\n    d_feat : int\n        input dimension for each time step\n    metric: str\n        the evaluate metric used in early stop\n    optimizer : str\n        optimizer name\n    GPU : int\n        the GPU ID used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        n_jobs=10,\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"ALSTM\")\n        self.logger.info(\"ALSTM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.n_jobs = n_jobs\n        self.seed = seed\n\n        self.logger.info(\n            \"ALSTM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nn_jobs : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                self.device,\n                n_jobs,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.ALSTM_model = ALSTMModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.ALSTM_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.ALSTM_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.ALSTM_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.ALSTM_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.ALSTM_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def train_epoch(self, data_loader):\n\n        self.ALSTM_model.train()\n\n        for data in data_loader:\n            feature = data[:, :, 0:-1].to(self.device)\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.ALSTM_model(feature.float())\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.ALSTM_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def test_epoch(self, data_loader):\n\n        self.ALSTM_model.eval()\n\n        scores = []\n        losses = []\n\n        for data in data_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n            # feature[torch.isnan(feature)] = 0\n            label = data[:, -1, -1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.ALSTM_model(feature.float())\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def fit(\n        self,\n        dataset,\n        evals_result=dict(),\n        save_path=None,\n    ):\n        dl_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        dl_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n\n        dl_train.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n        dl_valid.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n\n        train_loader = DataLoader(\n            dl_train, batch_size=self.batch_size, shuffle=True, num_workers=self.n_jobs, drop_last=True\n        )\n        valid_loader = DataLoader(\n            dl_valid, batch_size=self.batch_size, shuffle=False, num_workers=self.n_jobs, drop_last=True\n        )\n\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(train_loader)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(train_loader)\n            val_loss, val_score = self.test_epoch(valid_loader)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.ALSTM_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.ALSTM_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        dl_test = dataset.prepare(\"test\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_I)\n        dl_test.config(fillna_type=\"ffill+bfill\")\n        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)\n        self.ALSTM_model.eval()\n        preds = []\n\n        for data in test_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.ALSTM_model(feature.float()).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=dl_test.get_index())",
  "class ALSTMModel(nn.Module):\n    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type=\"GRU\"):\n        super().__init__()\n        self.hid_size = hidden_size\n        self.input_size = d_feat\n        self.dropout = dropout\n        self.rnn_type = rnn_type\n        self.rnn_layer = num_layers\n        self._build_model()\n\n    def _build_model(self):\n        try:\n            klass = getattr(nn, self.rnn_type.upper())\n        except:\n            raise ValueError(\"unknown rnn_type `%s`\" % self.rnn_type)\n        self.net = nn.Sequential()\n        self.net.add_module(\"fc_in\", nn.Linear(in_features=self.input_size, out_features=self.hid_size))\n        self.net.add_module(\"act\", nn.Tanh())\n        self.rnn = klass(\n            input_size=self.hid_size,\n            hidden_size=self.hid_size,\n            num_layers=self.rnn_layer,\n            batch_first=True,\n            dropout=self.dropout,\n        )\n        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)\n        self.att_net = nn.Sequential()\n        self.att_net.add_module(\n            \"att_fc_in\",\n            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),\n        )\n        self.att_net.add_module(\"att_dropout\", torch.nn.Dropout(self.dropout))\n        self.att_net.add_module(\"att_act\", nn.Tanh())\n        self.att_net.add_module(\n            \"att_fc_out\",\n            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),\n        )\n        self.att_net.add_module(\"att_softmax\", nn.Softmax(dim=1))\n\n    def forward(self, inputs):\n        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]\n        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]\n        out_att = torch.mul(rnn_out, attention_score)\n        out_att = torch.sum(out_att, dim=1)\n        out = self.fc_out(\n            torch.cat((rnn_out[:, -1, :], out_att), dim=1)\n        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]\n        return out[..., 0]",
  "def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        n_jobs=10,\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"ALSTM\")\n        self.logger.info(\"ALSTM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.n_jobs = n_jobs\n        self.seed = seed\n\n        self.logger.info(\n            \"ALSTM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nn_jobs : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                self.device,\n                n_jobs,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.ALSTM_model = ALSTMModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.ALSTM_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.ALSTM_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.ALSTM_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.ALSTM_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.ALSTM_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def train_epoch(self, data_loader):\n\n        self.ALSTM_model.train()\n\n        for data in data_loader:\n            feature = data[:, :, 0:-1].to(self.device)\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.ALSTM_model(feature.float())\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.ALSTM_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def test_epoch(self, data_loader):\n\n        self.ALSTM_model.eval()\n\n        scores = []\n        losses = []\n\n        for data in data_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n            # feature[torch.isnan(feature)] = 0\n            label = data[:, -1, -1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.ALSTM_model(feature.float())\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def fit(\n        self,\n        dataset,\n        evals_result=dict(),\n        save_path=None,\n    ):\n        dl_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        dl_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n\n        dl_train.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n        dl_valid.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n\n        train_loader = DataLoader(\n            dl_train, batch_size=self.batch_size, shuffle=True, num_workers=self.n_jobs, drop_last=True\n        )\n        valid_loader = DataLoader(\n            dl_valid, batch_size=self.batch_size, shuffle=False, num_workers=self.n_jobs, drop_last=True\n        )\n\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(train_loader)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(train_loader)\n            val_loss, val_score = self.test_epoch(valid_loader)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.ALSTM_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.ALSTM_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        dl_test = dataset.prepare(\"test\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_I)\n        dl_test.config(fillna_type=\"ffill+bfill\")\n        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)\n        self.ALSTM_model.eval()\n        preds = []\n\n        for data in test_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.ALSTM_model(feature.float()).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=dl_test.get_index())",
  "def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type=\"GRU\"):\n        super().__init__()\n        self.hid_size = hidden_size\n        self.input_size = d_feat\n        self.dropout = dropout\n        self.rnn_type = rnn_type\n        self.rnn_layer = num_layers\n        self._build_model()",
  "def _build_model(self):\n        try:\n            klass = getattr(nn, self.rnn_type.upper())\n        except:\n            raise ValueError(\"unknown rnn_type `%s`\" % self.rnn_type)\n        self.net = nn.Sequential()\n        self.net.add_module(\"fc_in\", nn.Linear(in_features=self.input_size, out_features=self.hid_size))\n        self.net.add_module(\"act\", nn.Tanh())\n        self.rnn = klass(\n            input_size=self.hid_size,\n            hidden_size=self.hid_size,\n            num_layers=self.rnn_layer,\n            batch_first=True,\n            dropout=self.dropout,\n        )\n        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)\n        self.att_net = nn.Sequential()\n        self.att_net.add_module(\n            \"att_fc_in\",\n            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),\n        )\n        self.att_net.add_module(\"att_dropout\", torch.nn.Dropout(self.dropout))\n        self.att_net.add_module(\"att_act\", nn.Tanh())\n        self.att_net.add_module(\n            \"att_fc_out\",\n            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),\n        )\n        self.att_net.add_module(\"att_softmax\", nn.Softmax(dim=1))",
  "def forward(self, inputs):\n        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]\n        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]\n        out_att = torch.mul(rnn_out, attention_score)\n        out_att = torch.sum(out_att, dim=1)\n        out = self.fc_out(\n            torch.cat((rnn_out[:, -1, :], out_att), dim=1)\n        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]\n        return out[..., 0]",
  "class SFM_Model(nn.Module):\n    def __init__(\n        self,\n        d_feat=6,\n        output_dim=1,\n        freq_dim=10,\n        hidden_size=64,\n        dropout_W=0.0,\n        dropout_U=0.0,\n        device=\"cpu\",\n    ):\n        super().__init__()\n\n        self.input_dim = d_feat\n        self.output_dim = output_dim\n        self.freq_dim = freq_dim\n        self.hidden_dim = hidden_size\n        self.device = device\n\n        self.W_i = nn.Parameter(init.xavier_uniform_(torch.empty((self.input_dim, self.hidden_dim))))\n        self.U_i = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))\n        self.b_i = nn.Parameter(torch.zeros(self.hidden_dim))\n\n        self.W_ste = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))\n        self.U_ste = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))\n        self.b_ste = nn.Parameter(torch.ones(self.hidden_dim))\n\n        self.W_fre = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.freq_dim)))\n        self.U_fre = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.freq_dim)))\n        self.b_fre = nn.Parameter(torch.ones(self.freq_dim))\n\n        self.W_c = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))\n        self.U_c = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))\n        self.b_c = nn.Parameter(torch.zeros(self.hidden_dim))\n\n        self.W_o = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))\n        self.U_o = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))\n        self.b_o = nn.Parameter(torch.zeros(self.hidden_dim))\n\n        self.U_a = nn.Parameter(init.orthogonal_(torch.empty(self.freq_dim, 1)))\n        self.b_a = nn.Parameter(torch.zeros(self.hidden_dim))\n\n        self.W_p = nn.Parameter(init.xavier_uniform_(torch.empty(self.hidden_dim, self.output_dim)))\n        self.b_p = nn.Parameter(torch.zeros(self.output_dim))\n\n        self.activation = nn.Tanh()\n        self.inner_activation = nn.Hardsigmoid()\n        self.dropout_W, self.dropout_U = (dropout_W, dropout_U)\n        self.fc_out = nn.Linear(self.output_dim, 1)\n\n        self.states = []\n\n    def forward(self, input):\n        input = input.reshape(len(input), self.input_dim, -1)  # [N, F, T]\n        input = input.permute(0, 2, 1)  # [N, T, F]\n        time_step = input.shape[1]\n\n        for ts in range(time_step):\n            x = input[:, ts, :]\n            if len(self.states) == 0:  # hasn't initialized yet\n                self.init_states(x)\n            self.get_constants(x)\n            p_tm1 = self.states[0]\n            h_tm1 = self.states[1]\n            S_re_tm1 = self.states[2]\n            S_im_tm1 = self.states[3]\n            time_tm1 = self.states[4]\n            B_U = self.states[5]\n            B_W = self.states[6]\n            frequency = self.states[7]\n\n            x_i = torch.matmul(x * B_W[0], self.W_i) + self.b_i\n            x_ste = torch.matmul(x * B_W[0], self.W_ste) + self.b_ste\n            x_fre = torch.matmul(x * B_W[0], self.W_fre) + self.b_fre\n            x_c = torch.matmul(x * B_W[0], self.W_c) + self.b_c\n            x_o = torch.matmul(x * B_W[0], self.W_o) + self.b_o\n\n            i = self.inner_activation(x_i + torch.matmul(h_tm1 * B_U[0], self.U_i))\n            ste = self.inner_activation(x_ste + torch.matmul(h_tm1 * B_U[0], self.U_ste))\n            fre = self.inner_activation(x_fre + torch.matmul(h_tm1 * B_U[0], self.U_fre))\n\n            ste = torch.reshape(ste, (-1, self.hidden_dim, 1))\n            fre = torch.reshape(fre, (-1, 1, self.freq_dim))\n\n            f = ste * fre\n\n            c = i * self.activation(x_c + torch.matmul(h_tm1 * B_U[0], self.U_c))\n\n            time = time_tm1 + 1\n\n            omega = torch.tensor(2 * np.pi) * time * frequency\n\n            re = torch.cos(omega)\n            im = torch.sin(omega)\n\n            c = torch.reshape(c, (-1, self.hidden_dim, 1))\n\n            S_re = f * S_re_tm1 + c * re\n            S_im = f * S_im_tm1 + c * im\n\n            A = torch.square(S_re) + torch.square(S_im)\n\n            A = torch.reshape(A, (-1, self.freq_dim)).float()\n            A_a = torch.matmul(A * B_U[0], self.U_a)\n            A_a = torch.reshape(A_a, (-1, self.hidden_dim))\n            a = self.activation(A_a + self.b_a)\n\n            o = self.inner_activation(x_o + torch.matmul(h_tm1 * B_U[0], self.U_o))\n\n            h = o * a\n            p = torch.matmul(h, self.W_p) + self.b_p\n\n            self.states = [p, h, S_re, S_im, time, None, None, None]\n        self.states = []\n        return self.fc_out(p).squeeze()\n\n    def init_states(self, x):\n        reducer_f = torch.zeros((self.hidden_dim, self.freq_dim)).to(self.device)\n        reducer_p = torch.zeros((self.hidden_dim, self.output_dim)).to(self.device)\n\n        init_state_h = torch.zeros(self.hidden_dim).to(self.device)\n        init_state_p = torch.matmul(init_state_h, reducer_p)\n\n        init_state = torch.zeros_like(init_state_h).to(self.device)\n        init_freq = torch.matmul(init_state_h, reducer_f)\n\n        init_state = torch.reshape(init_state, (-1, self.hidden_dim, 1))\n        init_freq = torch.reshape(init_freq, (-1, 1, self.freq_dim))\n\n        init_state_S_re = init_state * init_freq\n        init_state_S_im = init_state * init_freq\n\n        init_state_time = torch.tensor(0).to(self.device)\n\n        self.states = [\n            init_state_p,\n            init_state_h,\n            init_state_S_re,\n            init_state_S_im,\n            init_state_time,\n            None,\n            None,\n            None,\n        ]\n\n    def get_constants(self, x):\n        constants = []\n        constants.append([torch.tensor(1.0).to(self.device) for _ in range(6)])\n        constants.append([torch.tensor(1.0).to(self.device) for _ in range(7)])\n        array = np.array([float(ii) / self.freq_dim for ii in range(self.freq_dim)])\n        constants.append(torch.tensor(array).to(self.device))\n\n        self.states[5:] = constants",
  "class SFM(Model):\n    \"\"\"SFM Model\n\n    Parameters\n    ----------\n    input_dim : int\n        input dimension\n    output_dim : int\n        output dimension\n    lr : float\n        learning rate\n    optimizer : str\n        optimizer name\n    GPU : int\n        the GPU ID used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        output_dim=1,\n        freq_dim=10,\n        dropout_W=0.0,\n        dropout_U=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        eval_steps=5,\n        loss=\"mse\",\n        optimizer=\"gd\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"SFM\")\n        self.logger.info(\"SFM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.output_dim = output_dim\n        self.freq_dim = freq_dim\n        self.dropout_W = dropout_W\n        self.dropout_U = dropout_U\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.eval_steps = eval_steps\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n\n        self.logger.info(\n            \"SFM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\noutput_size : {}\"\n            \"\\nfrequency_dimension : {}\"\n            \"\\ndropout_W: {}\"\n            \"\\ndropout_U: {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\neval_steps : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                output_dim,\n                freq_dim,\n                dropout_W,\n                dropout_U,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                eval_steps,\n                optimizer.lower(),\n                loss,\n                self.device,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.sfm_model = SFM_Model(\n            d_feat=self.d_feat,\n            output_dim=self.output_dim,\n            hidden_size=self.hidden_size,\n            freq_dim=self.freq_dim,\n            dropout_W=self.dropout_W,\n            dropout_U=self.dropout_U,\n            device=self.device,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.sfm_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.sfm_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.sfm_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.sfm_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.sfm_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.sfm_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.sfm_model(feature)\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n\n        self.sfm_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.sfm_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.sfm_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.sfm_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.sfm_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n        if self.device != \"cpu\":\n            torch.cuda.empty_cache()\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.sfm_model.eval()\n        x_values = x_test.values\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = torch.from_numpy(x_values[begin:end]).float()\n\n            if self.device != \"cpu\":\n                x_batch = x_batch.to(self.device)\n\n            with torch.no_grad():\n                pred = self.sfm_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count",
  "def __init__(\n        self,\n        d_feat=6,\n        output_dim=1,\n        freq_dim=10,\n        hidden_size=64,\n        dropout_W=0.0,\n        dropout_U=0.0,\n        device=\"cpu\",\n    ):\n        super().__init__()\n\n        self.input_dim = d_feat\n        self.output_dim = output_dim\n        self.freq_dim = freq_dim\n        self.hidden_dim = hidden_size\n        self.device = device\n\n        self.W_i = nn.Parameter(init.xavier_uniform_(torch.empty((self.input_dim, self.hidden_dim))))\n        self.U_i = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))\n        self.b_i = nn.Parameter(torch.zeros(self.hidden_dim))\n\n        self.W_ste = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))\n        self.U_ste = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))\n        self.b_ste = nn.Parameter(torch.ones(self.hidden_dim))\n\n        self.W_fre = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.freq_dim)))\n        self.U_fre = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.freq_dim)))\n        self.b_fre = nn.Parameter(torch.ones(self.freq_dim))\n\n        self.W_c = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))\n        self.U_c = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))\n        self.b_c = nn.Parameter(torch.zeros(self.hidden_dim))\n\n        self.W_o = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))\n        self.U_o = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))\n        self.b_o = nn.Parameter(torch.zeros(self.hidden_dim))\n\n        self.U_a = nn.Parameter(init.orthogonal_(torch.empty(self.freq_dim, 1)))\n        self.b_a = nn.Parameter(torch.zeros(self.hidden_dim))\n\n        self.W_p = nn.Parameter(init.xavier_uniform_(torch.empty(self.hidden_dim, self.output_dim)))\n        self.b_p = nn.Parameter(torch.zeros(self.output_dim))\n\n        self.activation = nn.Tanh()\n        self.inner_activation = nn.Hardsigmoid()\n        self.dropout_W, self.dropout_U = (dropout_W, dropout_U)\n        self.fc_out = nn.Linear(self.output_dim, 1)\n\n        self.states = []",
  "def forward(self, input):\n        input = input.reshape(len(input), self.input_dim, -1)  # [N, F, T]\n        input = input.permute(0, 2, 1)  # [N, T, F]\n        time_step = input.shape[1]\n\n        for ts in range(time_step):\n            x = input[:, ts, :]\n            if len(self.states) == 0:  # hasn't initialized yet\n                self.init_states(x)\n            self.get_constants(x)\n            p_tm1 = self.states[0]\n            h_tm1 = self.states[1]\n            S_re_tm1 = self.states[2]\n            S_im_tm1 = self.states[3]\n            time_tm1 = self.states[4]\n            B_U = self.states[5]\n            B_W = self.states[6]\n            frequency = self.states[7]\n\n            x_i = torch.matmul(x * B_W[0], self.W_i) + self.b_i\n            x_ste = torch.matmul(x * B_W[0], self.W_ste) + self.b_ste\n            x_fre = torch.matmul(x * B_W[0], self.W_fre) + self.b_fre\n            x_c = torch.matmul(x * B_W[0], self.W_c) + self.b_c\n            x_o = torch.matmul(x * B_W[0], self.W_o) + self.b_o\n\n            i = self.inner_activation(x_i + torch.matmul(h_tm1 * B_U[0], self.U_i))\n            ste = self.inner_activation(x_ste + torch.matmul(h_tm1 * B_U[0], self.U_ste))\n            fre = self.inner_activation(x_fre + torch.matmul(h_tm1 * B_U[0], self.U_fre))\n\n            ste = torch.reshape(ste, (-1, self.hidden_dim, 1))\n            fre = torch.reshape(fre, (-1, 1, self.freq_dim))\n\n            f = ste * fre\n\n            c = i * self.activation(x_c + torch.matmul(h_tm1 * B_U[0], self.U_c))\n\n            time = time_tm1 + 1\n\n            omega = torch.tensor(2 * np.pi) * time * frequency\n\n            re = torch.cos(omega)\n            im = torch.sin(omega)\n\n            c = torch.reshape(c, (-1, self.hidden_dim, 1))\n\n            S_re = f * S_re_tm1 + c * re\n            S_im = f * S_im_tm1 + c * im\n\n            A = torch.square(S_re) + torch.square(S_im)\n\n            A = torch.reshape(A, (-1, self.freq_dim)).float()\n            A_a = torch.matmul(A * B_U[0], self.U_a)\n            A_a = torch.reshape(A_a, (-1, self.hidden_dim))\n            a = self.activation(A_a + self.b_a)\n\n            o = self.inner_activation(x_o + torch.matmul(h_tm1 * B_U[0], self.U_o))\n\n            h = o * a\n            p = torch.matmul(h, self.W_p) + self.b_p\n\n            self.states = [p, h, S_re, S_im, time, None, None, None]\n        self.states = []\n        return self.fc_out(p).squeeze()",
  "def init_states(self, x):\n        reducer_f = torch.zeros((self.hidden_dim, self.freq_dim)).to(self.device)\n        reducer_p = torch.zeros((self.hidden_dim, self.output_dim)).to(self.device)\n\n        init_state_h = torch.zeros(self.hidden_dim).to(self.device)\n        init_state_p = torch.matmul(init_state_h, reducer_p)\n\n        init_state = torch.zeros_like(init_state_h).to(self.device)\n        init_freq = torch.matmul(init_state_h, reducer_f)\n\n        init_state = torch.reshape(init_state, (-1, self.hidden_dim, 1))\n        init_freq = torch.reshape(init_freq, (-1, 1, self.freq_dim))\n\n        init_state_S_re = init_state * init_freq\n        init_state_S_im = init_state * init_freq\n\n        init_state_time = torch.tensor(0).to(self.device)\n\n        self.states = [\n            init_state_p,\n            init_state_h,\n            init_state_S_re,\n            init_state_S_im,\n            init_state_time,\n            None,\n            None,\n            None,\n        ]",
  "def get_constants(self, x):\n        constants = []\n        constants.append([torch.tensor(1.0).to(self.device) for _ in range(6)])\n        constants.append([torch.tensor(1.0).to(self.device) for _ in range(7)])\n        array = np.array([float(ii) / self.freq_dim for ii in range(self.freq_dim)])\n        constants.append(torch.tensor(array).to(self.device))\n\n        self.states[5:] = constants",
  "def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        output_dim=1,\n        freq_dim=10,\n        dropout_W=0.0,\n        dropout_U=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        eval_steps=5,\n        loss=\"mse\",\n        optimizer=\"gd\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"SFM\")\n        self.logger.info(\"SFM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.output_dim = output_dim\n        self.freq_dim = freq_dim\n        self.dropout_W = dropout_W\n        self.dropout_U = dropout_U\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.eval_steps = eval_steps\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n\n        self.logger.info(\n            \"SFM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\noutput_size : {}\"\n            \"\\nfrequency_dimension : {}\"\n            \"\\ndropout_W: {}\"\n            \"\\ndropout_U: {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\neval_steps : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                output_dim,\n                freq_dim,\n                dropout_W,\n                dropout_U,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                eval_steps,\n                optimizer.lower(),\n                loss,\n                self.device,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.sfm_model = SFM_Model(\n            d_feat=self.d_feat,\n            output_dim=self.output_dim,\n            hidden_size=self.hidden_size,\n            freq_dim=self.freq_dim,\n            dropout_W=self.dropout_W,\n            dropout_U=self.dropout_U,\n            device=self.device,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.sfm_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.sfm_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.sfm_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.sfm_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.sfm_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.sfm_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.sfm_model(feature)\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n\n        self.sfm_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.sfm_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.sfm_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.sfm_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.sfm_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n        if self.device != \"cpu\":\n            torch.cuda.empty_cache()",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.sfm_model.eval()\n        x_values = x_test.values\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = torch.from_numpy(x_values[begin:end]).float()\n\n            if self.device != \"cpu\":\n                x_batch = x_batch.to(self.device)\n\n            with torch.no_grad():\n                pred = self.sfm_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "def __init__(self):\n        self.reset()",
  "def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0",
  "def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count",
  "class DailyBatchSampler(Sampler):\n    def __init__(self, data_source):\n\n        self.data_source = data_source\n        self.data = self.data_source.data.loc[self.data_source.get_index()]\n        self.daily_count = self.data.groupby(level=0).size().values  # calculate number of samples in each batch\n        self.daily_index = np.roll(np.cumsum(self.daily_count), 1)  # calculate begin index of each batch\n        self.daily_index[0] = 0\n\n    def __iter__(self):\n\n        for idx, count in zip(self.daily_index, self.daily_count):\n            yield np.arange(idx, idx + count)\n\n    def __len__(self):\n        return len(self.data_source)",
  "class GATs(Model):\n    \"\"\"GATs Model\n\n    Parameters\n    ----------\n    lr : float\n        learning rate\n    d_feat : int\n        input dimensions for each time step\n    metric : str\n        the evaluate metric used in early stop\n    optimizer : str\n        optimizer name\n    GPU : int\n        the GPU ID used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=20,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        early_stop=20,\n        loss=\"mse\",\n        base_model=\"GRU\",\n        with_pretrain=True,\n        model_path=None,\n        optimizer=\"adam\",\n        GPU=\"0\",\n        n_jobs=10,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"GATs\")\n        self.logger.info(\"GATs pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.base_model = base_model\n        self.with_pretrain = with_pretrain\n        self.model_path = model_path\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.n_jobs = n_jobs\n        self.seed = seed\n\n        self.logger.info(\n            \"GATs parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\nbase_model : {}\"\n            \"\\nwith_pretrain : {}\"\n            \"\\nmodel_path : {}\"\n            \"\\nvisible_GPU : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                base_model,\n                with_pretrain,\n                model_path,\n                GPU,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.GAT_model = GATModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n            base_model=self.base_model,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.GAT_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.GAT_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.GAT_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.GAT_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.GAT_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def get_daily_inter(self, df, shuffle=False):\n        # organize the train data into daily batches\n        daily_count = df.groupby(level=0).size().values\n        daily_index = np.roll(np.cumsum(daily_count), 1)\n        daily_index[0] = 0\n        if shuffle:\n            # shuffle data\n            daily_shuffle = list(zip(daily_index, daily_count))\n            np.random.shuffle(daily_shuffle)\n            daily_index, daily_count = zip(*daily_shuffle)\n        return daily_index, daily_count\n\n    def train_epoch(self, data_loader):\n\n        self.GAT_model.train()\n\n        for data in data_loader:\n\n            data = data.squeeze()\n            feature = data[:, :, 0:-1].to(self.device)\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.GAT_model(feature.float())\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.GAT_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def test_epoch(self, data_loader):\n\n        self.GAT_model.eval()\n\n        scores = []\n        losses = []\n\n        for data in data_loader:\n\n            data = data.squeeze()\n            feature = data[:, :, 0:-1].to(self.device)\n            # feature[torch.isnan(feature)] = 0\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.GAT_model(feature.float())\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def fit(\n        self,\n        dataset,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        dl_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        dl_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n\n        dl_train.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n        dl_valid.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n\n        sampler_train = DailyBatchSampler(dl_train)\n        sampler_valid = DailyBatchSampler(dl_valid)\n\n        train_loader = DataLoader(dl_train, sampler=sampler_train, num_workers=self.n_jobs, drop_last=True)\n        valid_loader = DataLoader(dl_valid, sampler=sampler_valid, num_workers=self.n_jobs, drop_last=True)\n\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # load pretrained base_model\n        if self.with_pretrain:\n            if self.model_path == None:\n                raise ValueError(\"the path of the pretrained model should be given first!\")\n            self.logger.info(\"Loading pretrained model...\")\n            if self.base_model == \"LSTM\":\n                pretrained_model = LSTMModel(\n                    d_feat=self.d_feat, hidden_size=self.hidden_size, num_layers=self.num_layers\n                )\n                pretrained_model.load_state_dict(torch.load(self.model_path))\n            elif self.base_model == \"GRU\":\n                pretrained_model = GRUModel(\n                    d_feat=self.d_feat, hidden_size=self.hidden_size, num_layers=self.num_layers\n                )\n                pretrained_model.load_state_dict(torch.load(self.model_path))\n            else:\n                raise ValueError(\"unknown base model name `%s`\" % self.base_model)\n\n            model_dict = self.GAT_model.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            self.GAT_model.load_state_dict(model_dict)\n            self.logger.info(\"Loading pretrained model Done...\")\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(train_loader)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(train_loader)\n            val_loss, val_score = self.test_epoch(valid_loader)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.GAT_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.GAT_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        dl_test = dataset.prepare(\"test\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_I)\n        dl_test.config(fillna_type=\"ffill+bfill\")\n        sampler_test = DailyBatchSampler(dl_test)\n        test_loader = DataLoader(dl_test, sampler=sampler_test, num_workers=self.n_jobs)\n        self.GAT_model.eval()\n        preds = []\n\n        for data in test_loader:\n\n            data = data.squeeze()\n            feature = data[:, :, 0:-1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.GAT_model(feature.float()).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=dl_test.get_index())",
  "class GATModel(nn.Module):\n    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model=\"GRU\"):\n        super().__init__()\n\n        if base_model == \"GRU\":\n            self.rnn = nn.GRU(\n                input_size=d_feat,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=dropout,\n            )\n        elif base_model == \"LSTM\":\n            self.rnn = nn.LSTM(\n                input_size=d_feat,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=dropout,\n            )\n        else:\n            raise ValueError(\"unknown base model name `%s`\" % base_model)\n\n        self.hidden_size = hidden_size\n        self.d_feat = d_feat\n        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)\n        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))\n        self.a.requires_grad = True\n        self.fc = nn.Linear(self.hidden_size, self.hidden_size)\n        self.fc_out = nn.Linear(hidden_size, 1)\n        self.leaky_relu = nn.LeakyReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def cal_attention(self, x, y):\n        x = self.transformation(x)\n        y = self.transformation(y)\n\n        sample_num = x.shape[0]\n        dim = x.shape[1]\n        e_x = x.expand(sample_num, sample_num, dim)\n        e_y = torch.transpose(e_x, 0, 1)\n        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)\n        self.a_t = torch.t(self.a)\n        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)\n        attention_out = self.leaky_relu(attention_out)\n        att_weight = self.softmax(attention_out)\n        return att_weight\n\n    def forward(self, x):\n        out, _ = self.rnn(x)\n        hidden = out[:, -1, :]\n        att_weight = self.cal_attention(hidden, hidden)\n        hidden = att_weight.mm(hidden) + hidden\n        hidden = self.fc(hidden)\n        hidden = self.leaky_relu(hidden)\n        return self.fc_out(hidden).squeeze()",
  "def __init__(self, data_source):\n\n        self.data_source = data_source\n        self.data = self.data_source.data.loc[self.data_source.get_index()]\n        self.daily_count = self.data.groupby(level=0).size().values  # calculate number of samples in each batch\n        self.daily_index = np.roll(np.cumsum(self.daily_count), 1)  # calculate begin index of each batch\n        self.daily_index[0] = 0",
  "def __iter__(self):\n\n        for idx, count in zip(self.daily_index, self.daily_count):\n            yield np.arange(idx, idx + count)",
  "def __len__(self):\n        return len(self.data_source)",
  "def __init__(\n        self,\n        d_feat=20,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        early_stop=20,\n        loss=\"mse\",\n        base_model=\"GRU\",\n        with_pretrain=True,\n        model_path=None,\n        optimizer=\"adam\",\n        GPU=\"0\",\n        n_jobs=10,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"GATs\")\n        self.logger.info(\"GATs pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.base_model = base_model\n        self.with_pretrain = with_pretrain\n        self.model_path = model_path\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.n_jobs = n_jobs\n        self.seed = seed\n\n        self.logger.info(\n            \"GATs parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\nbase_model : {}\"\n            \"\\nwith_pretrain : {}\"\n            \"\\nmodel_path : {}\"\n            \"\\nvisible_GPU : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                base_model,\n                with_pretrain,\n                model_path,\n                GPU,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.GAT_model = GATModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n            base_model=self.base_model,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.GAT_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.GAT_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.GAT_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.GAT_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.GAT_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def get_daily_inter(self, df, shuffle=False):\n        # organize the train data into daily batches\n        daily_count = df.groupby(level=0).size().values\n        daily_index = np.roll(np.cumsum(daily_count), 1)\n        daily_index[0] = 0\n        if shuffle:\n            # shuffle data\n            daily_shuffle = list(zip(daily_index, daily_count))\n            np.random.shuffle(daily_shuffle)\n            daily_index, daily_count = zip(*daily_shuffle)\n        return daily_index, daily_count",
  "def train_epoch(self, data_loader):\n\n        self.GAT_model.train()\n\n        for data in data_loader:\n\n            data = data.squeeze()\n            feature = data[:, :, 0:-1].to(self.device)\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.GAT_model(feature.float())\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.GAT_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def test_epoch(self, data_loader):\n\n        self.GAT_model.eval()\n\n        scores = []\n        losses = []\n\n        for data in data_loader:\n\n            data = data.squeeze()\n            feature = data[:, :, 0:-1].to(self.device)\n            # feature[torch.isnan(feature)] = 0\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.GAT_model(feature.float())\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def fit(\n        self,\n        dataset,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        dl_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        dl_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n\n        dl_train.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n        dl_valid.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n\n        sampler_train = DailyBatchSampler(dl_train)\n        sampler_valid = DailyBatchSampler(dl_valid)\n\n        train_loader = DataLoader(dl_train, sampler=sampler_train, num_workers=self.n_jobs, drop_last=True)\n        valid_loader = DataLoader(dl_valid, sampler=sampler_valid, num_workers=self.n_jobs, drop_last=True)\n\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # load pretrained base_model\n        if self.with_pretrain:\n            if self.model_path == None:\n                raise ValueError(\"the path of the pretrained model should be given first!\")\n            self.logger.info(\"Loading pretrained model...\")\n            if self.base_model == \"LSTM\":\n                pretrained_model = LSTMModel(\n                    d_feat=self.d_feat, hidden_size=self.hidden_size, num_layers=self.num_layers\n                )\n                pretrained_model.load_state_dict(torch.load(self.model_path))\n            elif self.base_model == \"GRU\":\n                pretrained_model = GRUModel(\n                    d_feat=self.d_feat, hidden_size=self.hidden_size, num_layers=self.num_layers\n                )\n                pretrained_model.load_state_dict(torch.load(self.model_path))\n            else:\n                raise ValueError(\"unknown base model name `%s`\" % self.base_model)\n\n            model_dict = self.GAT_model.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            self.GAT_model.load_state_dict(model_dict)\n            self.logger.info(\"Loading pretrained model Done...\")\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(train_loader)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(train_loader)\n            val_loss, val_score = self.test_epoch(valid_loader)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.GAT_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.GAT_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        dl_test = dataset.prepare(\"test\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_I)\n        dl_test.config(fillna_type=\"ffill+bfill\")\n        sampler_test = DailyBatchSampler(dl_test)\n        test_loader = DataLoader(dl_test, sampler=sampler_test, num_workers=self.n_jobs)\n        self.GAT_model.eval()\n        preds = []\n\n        for data in test_loader:\n\n            data = data.squeeze()\n            feature = data[:, :, 0:-1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.GAT_model(feature.float()).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=dl_test.get_index())",
  "def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model=\"GRU\"):\n        super().__init__()\n\n        if base_model == \"GRU\":\n            self.rnn = nn.GRU(\n                input_size=d_feat,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=dropout,\n            )\n        elif base_model == \"LSTM\":\n            self.rnn = nn.LSTM(\n                input_size=d_feat,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=dropout,\n            )\n        else:\n            raise ValueError(\"unknown base model name `%s`\" % base_model)\n\n        self.hidden_size = hidden_size\n        self.d_feat = d_feat\n        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)\n        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))\n        self.a.requires_grad = True\n        self.fc = nn.Linear(self.hidden_size, self.hidden_size)\n        self.fc_out = nn.Linear(hidden_size, 1)\n        self.leaky_relu = nn.LeakyReLU()\n        self.softmax = nn.Softmax(dim=1)",
  "def cal_attention(self, x, y):\n        x = self.transformation(x)\n        y = self.transformation(y)\n\n        sample_num = x.shape[0]\n        dim = x.shape[1]\n        e_x = x.expand(sample_num, sample_num, dim)\n        e_y = torch.transpose(e_x, 0, 1)\n        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)\n        self.a_t = torch.t(self.a)\n        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)\n        attention_out = self.leaky_relu(attention_out)\n        att_weight = self.softmax(attention_out)\n        return att_weight",
  "def forward(self, x):\n        out, _ = self.rnn(x)\n        hidden = out[:, -1, :]\n        att_weight = self.cal_attention(hidden, hidden)\n        hidden = att_weight.mm(hidden) + hidden\n        hidden = self.fc(hidden)\n        hidden = self.leaky_relu(hidden)\n        return self.fc_out(hidden).squeeze()",
  "class TabnetModel(Model):\n    def __init__(\n        self,\n        d_feat=158,\n        out_dim=64,\n        final_out_dim=1,\n        batch_size=4096,\n        n_d=64,\n        n_a=64,\n        n_shared=2,\n        n_ind=2,\n        n_steps=5,\n        n_epochs=100,\n        pretrain_n_epochs=50,\n        relax=1.3,\n        vbs=2048,\n        seed=993,\n        optimizer=\"adam\",\n        loss=\"mse\",\n        metric=\"\",\n        early_stop=20,\n        GPU=0,\n        pretrain_loss=\"custom\",\n        ps=0.3,\n        lr=0.01,\n        pretrain=True,\n        pretrain_file=None,\n    ):\n        \"\"\"\n        TabNet model for Qlib\n\n        Args\uff1a\n        ps: probability to generate the bernoulli mask\n        \"\"\"\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.out_dim = out_dim\n        self.final_out_dim = final_out_dim\n        self.lr = lr\n        self.batch_size = batch_size\n        self.optimizer = optimizer.lower()\n        self.pretrain_loss = pretrain_loss\n        self.seed = seed\n        self.ps = ps\n        self.n_epochs = n_epochs\n        self.logger = get_module_logger(\"TabNet\")\n        self.pretrain_n_epochs = pretrain_n_epochs\n        self.device = \"cuda:%s\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\"\n        self.loss = loss\n        self.metric = metric\n        self.early_stop = early_stop\n        self.pretrain = pretrain\n        self.pretrain_file = get_or_create_path(pretrain_file)\n        self.logger.info(\n            \"TabNet:\"\n            \"\\nbatch_size : {}\"\n            \"\\nvirtual bs : {}\"\n            \"\\ndevice : {}\"\n            \"\\npretrain: {}\".format(self.batch_size, vbs, self.device, self.pretrain)\n        )\n        self.fitted = False\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n\n        self.tabnet_model = TabNet(inp_dim=self.d_feat, out_dim=self.out_dim, vbs=vbs, relax=relax).to(self.device)\n        self.tabnet_decoder = TabNet_Decoder(self.out_dim, self.d_feat, n_shared, n_ind, vbs, n_steps).to(self.device)\n        self.logger.info(\"model:\\n{:}\\n{:}\".format(self.tabnet_model, self.tabnet_decoder))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters([self.tabnet_model, self.tabnet_decoder])))\n\n        if optimizer.lower() == \"adam\":\n            self.pretrain_optimizer = optim.Adam(\n                list(self.tabnet_model.parameters()) + list(self.tabnet_decoder.parameters()), lr=self.lr\n            )\n            self.train_optimizer = optim.Adam(self.tabnet_model.parameters(), lr=self.lr)\n\n        elif optimizer.lower() == \"gd\":\n            self.pretrain_optimizer = optim.SGD(\n                list(self.tabnet_model.parameters()) + list(self.tabnet_decoder.parameters()), lr=self.lr\n            )\n            self.train_optimizer = optim.SGD(self.tabnet_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def pretrain_fn(self, dataset=DatasetH, pretrain_file=\"./pretrain/best.model\"):\n        get_or_create_path(pretrain_file)\n\n        [df_train, df_valid] = dataset.prepare(\n            [\"pretrain\", \"pretrain_validation\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        df_train.fillna(df_train.mean(), inplace=True)\n        df_valid.fillna(df_valid.mean(), inplace=True)\n\n        x_train = df_train[\"feature\"]\n        x_valid = df_valid[\"feature\"]\n\n        # Early stop setup\n        stop_steps = 0\n        train_loss = 0\n        best_loss = np.inf\n\n        for epoch_idx in range(self.pretrain_n_epochs):\n            self.logger.info(\"epoch: %s\" % (epoch_idx))\n            self.logger.info(\"pre-training...\")\n            self.pretrain_epoch(x_train)\n            self.logger.info(\"evaluating...\")\n            train_loss = self.pretrain_test_epoch(x_train)\n            valid_loss = self.pretrain_test_epoch(x_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_loss, valid_loss))\n\n            if valid_loss < best_loss:\n                self.logger.info(\"Save Model...\")\n                torch.save(self.tabnet_model.state_dict(), pretrain_file)\n                best_loss = valid_loss\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n        if self.pretrain:\n            # there is a  pretrained model, load the model\n            self.logger.info(\"Pretrain...\")\n            self.pretrain_fn(dataset, self.pretrain_file)\n            self.logger.info(\"Load Pretrain model\")\n            self.tabnet_model.load_state_dict(torch.load(self.pretrain_file))\n\n        # adding one more linear layer to fit the final output dimension\n        self.tabnet_model = FinetuneModel(self.out_dim, self.final_out_dim, self.tabnet_model).to(self.device)\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n        df_train.fillna(df_train.mean(), inplace=True)\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for epoch_idx in range(self.n_epochs):\n            self.logger.info(\"epoch: %s\" % (epoch_idx))\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            valid_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = epoch_idx\n                best_param = copy.deepcopy(self.tabnet_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.tabnet_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\", data_key=DataHandlerLP.DK_I)\n        index = x_test.index\n        self.tabnet_model.eval()\n        x_values = torch.from_numpy(x_test.values)\n        x_values[torch.isnan(x_values)] = 0\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = x_values[begin:end].float().to(self.device)\n            priors = torch.ones(end - begin, self.d_feat).to(self.device)\n\n            with torch.no_grad():\n                pred = self.tabnet_model(x_batch, priors).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)\n\n    def test_epoch(self, data_x, data_y):\n        # prepare training data\n        x_values = torch.from_numpy(data_x.values)\n        y_values = torch.from_numpy(np.squeeze(data_y.values))\n        x_values[torch.isnan(x_values)] = 0\n        y_values[torch.isnan(y_values)] = 0\n        self.tabnet_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n            feature = x_values[indices[i : i + self.batch_size]].float().to(self.device)\n            label = y_values[indices[i : i + self.batch_size]].float().to(self.device)\n            priors = torch.ones(self.batch_size, self.d_feat).to(self.device)\n            with torch.no_grad():\n                pred = self.tabnet_model(feature, priors)\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def train_epoch(self, x_train, y_train):\n        x_train_values = torch.from_numpy(x_train.values)\n        y_train_values = torch.from_numpy(np.squeeze(y_train.values))\n        x_train_values[torch.isnan(x_train_values)] = 0\n        y_train_values[torch.isnan(y_train_values)] = 0\n        self.tabnet_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = x_train_values[indices[i : i + self.batch_size]].float().to(self.device)\n            label = y_train_values[indices[i : i + self.batch_size]].float().to(self.device)\n            priors = torch.ones(self.batch_size, self.d_feat).to(self.device)\n            pred = self.tabnet_model(feature, priors)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.tabnet_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def pretrain_epoch(self, x_train):\n        train_set = torch.from_numpy(x_train.values)\n        train_set[torch.isnan(train_set)] = 0\n        indices = np.arange(len(train_set))\n        np.random.shuffle(indices)\n\n        self.tabnet_model.train()\n        self.tabnet_decoder.train()\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            S_mask = torch.bernoulli(torch.empty(self.batch_size, self.d_feat).fill_(self.ps))\n            x_train_values = train_set[indices[i : i + self.batch_size]] * (1 - S_mask)\n            y_train_values = train_set[indices[i : i + self.batch_size]] * (S_mask)\n\n            S_mask = S_mask.to(self.device)\n            feature = x_train_values.float().to(self.device)\n            label = y_train_values.float().to(self.device)\n            priors = 1 - S_mask\n            (vec, sparse_loss) = self.tabnet_model(feature, priors)\n            f = self.tabnet_decoder(vec)\n            loss = self.pretrain_loss_fn(label, f, S_mask)\n\n            self.pretrain_optimizer.zero_grad()\n            loss.backward()\n            self.pretrain_optimizer.step()\n\n    def pretrain_test_epoch(self, x_train):\n        train_set = torch.from_numpy(x_train.values)\n        train_set[torch.isnan(train_set)] = 0\n        indices = np.arange(len(train_set))\n\n        self.tabnet_model.eval()\n        self.tabnet_decoder.eval()\n\n        losses = []\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            S_mask = torch.bernoulli(torch.empty(self.batch_size, self.d_feat).fill_(self.ps))\n            x_train_values = train_set[indices[i : i + self.batch_size]] * (1 - S_mask)\n            y_train_values = train_set[indices[i : i + self.batch_size]] * (S_mask)\n\n            feature = x_train_values.float().to(self.device)\n            label = y_train_values.float().to(self.device)\n            S_mask = S_mask.to(self.device)\n            priors = 1 - S_mask\n            with torch.no_grad():\n                (vec, sparse_loss) = self.tabnet_model(feature, priors)\n                f = self.tabnet_decoder(vec)\n\n                loss = self.pretrain_loss_fn(label, f, S_mask)\n            losses.append(loss.item())\n\n        return np.mean(losses)\n\n    def pretrain_loss_fn(self, f_hat, f, S):\n        \"\"\"\n        Pretrain loss function defined in the original paper, read \"Tabular self-supervised learning\" in https://arxiv.org/pdf/1908.07442.pdf\n        \"\"\"\n        down_mean = torch.mean(f, dim=0)\n        down = torch.sqrt(torch.sum(torch.square(f - down_mean), dim=0))\n        up = (f_hat - f) * S\n        return torch.sum(torch.square(up / down))\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n        mask = torch.isfinite(label)\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "class FinetuneModel(nn.Module):\n    \"\"\"\n    FinuetuneModel for adding a layer by the end\n    \"\"\"\n\n    def __init__(self, input_dim, output_dim, trained_model):\n        super().__init__()\n        self.model = trained_model\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x, priors):\n        return self.fc(self.model(x, priors)[0]).squeeze()",
  "class DecoderStep(nn.Module):\n    def __init__(self, inp_dim, out_dim, shared, n_ind, vbs):\n        super().__init__()\n        self.fea_tran = FeatureTransformer(inp_dim, out_dim, shared, n_ind, vbs)\n        self.fc = nn.Linear(out_dim, out_dim)\n\n    def forward(self, x):\n        x = self.fea_tran(x)\n        return self.fc(x)",
  "class TabNet_Decoder(nn.Module):\n    def __init__(self, inp_dim, out_dim, n_shared, n_ind, vbs, n_steps):\n        \"\"\"\n        TabNet decoder that is used in pre-training\n        \"\"\"\n        super().__init__()\n        self.out_dim = out_dim\n        if n_shared > 0:\n            self.shared = nn.ModuleList()\n            self.shared.append(nn.Linear(inp_dim, 2 * out_dim))\n            for x in range(n_shared - 1):\n                self.shared.append(nn.Linear(out_dim, 2 * out_dim))  # preset the linear function we will use\n        else:\n            self.shared = None\n        self.n_steps = n_steps\n        self.steps = nn.ModuleList()\n        for x in range(n_steps):\n            self.steps.append(DecoderStep(inp_dim, out_dim, self.shared, n_ind, vbs))\n\n    def forward(self, x):\n        out = torch.zeros(x.size(0), self.out_dim).to(x.device)\n        for step in self.steps:\n            out += step(x)\n        return out",
  "class TabNet(nn.Module):\n    def __init__(self, inp_dim=6, out_dim=6, n_d=64, n_a=64, n_shared=2, n_ind=2, n_steps=5, relax=1.2, vbs=1024):\n        \"\"\"\n        TabNet AKA the original encoder\n\n        Args:\n            n_d: dimension of the features used to calculate the final results\n            n_a: dimension of the features input to the attention transformer of the next step\n            n_shared: numbr of shared steps in feature transfomer(optional)\n            n_ind: number of independent steps in feature transformer\n            n_steps: number of steps of pass through tabbet\n            relax coefficient:\n            virtual batch size:\n        \"\"\"\n        super().__init__()\n\n        # set the number of shared step in feature transformer\n        if n_shared > 0:\n            self.shared = nn.ModuleList()\n            self.shared.append(nn.Linear(inp_dim, 2 * (n_d + n_a)))\n            for x in range(n_shared - 1):\n                self.shared.append(nn.Linear(n_d + n_a, 2 * (n_d + n_a)))  # preset the linear function we will use\n        else:\n            self.shared = None\n\n        self.first_step = FeatureTransformer(inp_dim, n_d + n_a, self.shared, n_ind, vbs)\n        self.steps = nn.ModuleList()\n        for x in range(n_steps - 1):\n            self.steps.append(DecisionStep(inp_dim, n_d, n_a, self.shared, n_ind, relax, vbs))\n        self.fc = nn.Linear(n_d, out_dim)\n        self.bn = nn.BatchNorm1d(inp_dim, momentum=0.01)\n        self.n_d = n_d\n\n    def forward(self, x, priors):\n        assert not torch.isnan(x).any()\n        x = self.bn(x)\n        x_a = self.first_step(x)[:, self.n_d :]\n        sparse_loss = []\n        out = torch.zeros(x.size(0), self.n_d).to(x.device)\n        for step in self.steps:\n            x_te, l = step(x, x_a, priors)\n            out += F.relu(x_te[:, : self.n_d])  # split the feautre from feat_transformer\n            x_a = x_te[:, self.n_d :]\n            sparse_loss.append(l)\n        return self.fc(out), sum(sparse_loss)",
  "class GBN(nn.Module):\n    \"\"\"\n    Ghost Batch Normalization\n    an efficient way of doing batch normalization\n\n    Args:\n        vbs: virtual batch size\n    \"\"\"\n\n    def __init__(self, inp, vbs=1024, momentum=0.01):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(inp, momentum=momentum)\n        self.vbs = vbs\n\n    def forward(self, x):\n        if x.size(0) <= self.vbs:  # can not be chunked\n            return self.bn(x)\n        else:\n            chunk = torch.chunk(x, x.size(0) // self.vbs, 0)\n            res = [self.bn(y) for y in chunk]\n            return torch.cat(res, 0)",
  "class GLU(nn.Module):\n    \"\"\"\n    GLU block that extracts only the most essential information\n\n    Args:\n        vbs: virtual batch size\n    \"\"\"\n\n    def __init__(self, inp_dim, out_dim, fc=None, vbs=1024):\n        super().__init__()\n        if fc:\n            self.fc = fc\n        else:\n            self.fc = nn.Linear(inp_dim, out_dim * 2)\n        self.bn = GBN(out_dim * 2, vbs=vbs)\n        self.od = out_dim\n\n    def forward(self, x):\n        x = self.bn(self.fc(x))\n        return torch.mul(x[:, : self.od], torch.sigmoid(x[:, self.od :]))",
  "class AttentionTransformer(nn.Module):\n    \"\"\"\n    Args:\n        relax: relax coefficient. The greater it is, we can\n        use the same features more. When it is set to 1\n        we can use every feature only once\n    \"\"\"\n\n    def __init__(self, d_a, inp_dim, relax, vbs=1024):\n        super().__init__()\n        self.fc = nn.Linear(d_a, inp_dim)\n        self.bn = GBN(inp_dim, vbs=vbs)\n        self.r = relax\n\n    # a:feature from previous decision step\n    def forward(self, a, priors):\n        a = self.bn(self.fc(a))\n        mask = SparsemaxFunction.apply(a * priors)\n        priors = priors * (self.r - mask)  # updating the prior\n        return mask",
  "class FeatureTransformer(nn.Module):\n    def __init__(self, inp_dim, out_dim, shared, n_ind, vbs):\n        super().__init__()\n        first = True\n        self.shared = nn.ModuleList()\n        if shared:\n            self.shared.append(GLU(inp_dim, out_dim, shared[0], vbs=vbs))\n            first = False\n            for fc in shared[1:]:\n                self.shared.append(GLU(out_dim, out_dim, fc, vbs=vbs))\n        else:\n            self.shared = None\n        self.independ = nn.ModuleList()\n        if first:\n            self.independ.append(GLU(inp, out_dim, vbs=vbs))\n        for x in range(first, n_ind):\n            self.independ.append(GLU(out_dim, out_dim, vbs=vbs))\n        self.scale = float(np.sqrt(0.5))\n\n    def forward(self, x):\n        if self.shared:\n            x = self.shared[0](x)\n            for glu in self.shared[1:]:\n                x = torch.add(x, glu(x))\n                x = x * self.scale\n        for glu in self.independ:\n            x = torch.add(x, glu(x))\n            x = x * self.scale\n        return x",
  "class DecisionStep(nn.Module):\n    \"\"\"\n    One step for the TabNet\n    \"\"\"\n\n    def __init__(self, inp_dim, n_d, n_a, shared, n_ind, relax, vbs):\n        super().__init__()\n        self.atten_tran = AttentionTransformer(n_a, inp_dim, relax, vbs)\n        self.fea_tran = FeatureTransformer(inp_dim, n_d + n_a, shared, n_ind, vbs)\n\n    def forward(self, x, a, priors):\n        mask = self.atten_tran(a, priors)\n        sparse_loss = ((-1) * mask * torch.log(mask + 1e-10)).mean()\n        x = self.fea_tran(x * mask)\n        return x, sparse_loss",
  "def make_ix_like(input, dim=0):\n    d = input.size(dim)\n    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n    view = [1] * input.dim()\n    view[0] = -1\n    return rho.view(view).transpose(0, dim)",
  "class SparsemaxFunction(Function):\n    \"\"\"\n    SparseMax function for replacing reLU\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input, dim=-1):\n        ctx.dim = dim\n        max_val, _ = input.max(dim=dim, keepdim=True)\n        input -= max_val  # same numerical stability trick as for softmax\n        tau, supp_size = SparsemaxFunction.threshold_and_support(input, dim=dim)\n        output = torch.clamp(input - tau, min=0)\n        ctx.save_for_backward(supp_size, output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output = ctx.saved_tensors\n        dim = ctx.dim\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n\n        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n        v_hat = v_hat.unsqueeze(dim)\n        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n        return grad_input, None\n\n    @staticmethod\n    def threshold_and_support(input, dim=-1):\n        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n        input_cumsum = input_srt.cumsum(dim) - 1\n        rhos = make_ix_like(input, dim)\n        support = rhos * input_srt > input_cumsum\n\n        support_size = support.sum(dim=dim).unsqueeze(dim)\n        tau = input_cumsum.gather(dim, support_size - 1)\n        tau /= support_size.to(input.dtype)\n        return tau, support_size",
  "def __init__(\n        self,\n        d_feat=158,\n        out_dim=64,\n        final_out_dim=1,\n        batch_size=4096,\n        n_d=64,\n        n_a=64,\n        n_shared=2,\n        n_ind=2,\n        n_steps=5,\n        n_epochs=100,\n        pretrain_n_epochs=50,\n        relax=1.3,\n        vbs=2048,\n        seed=993,\n        optimizer=\"adam\",\n        loss=\"mse\",\n        metric=\"\",\n        early_stop=20,\n        GPU=0,\n        pretrain_loss=\"custom\",\n        ps=0.3,\n        lr=0.01,\n        pretrain=True,\n        pretrain_file=None,\n    ):\n        \"\"\"\n        TabNet model for Qlib\n\n        Args\uff1a\n        ps: probability to generate the bernoulli mask\n        \"\"\"\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.out_dim = out_dim\n        self.final_out_dim = final_out_dim\n        self.lr = lr\n        self.batch_size = batch_size\n        self.optimizer = optimizer.lower()\n        self.pretrain_loss = pretrain_loss\n        self.seed = seed\n        self.ps = ps\n        self.n_epochs = n_epochs\n        self.logger = get_module_logger(\"TabNet\")\n        self.pretrain_n_epochs = pretrain_n_epochs\n        self.device = \"cuda:%s\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\"\n        self.loss = loss\n        self.metric = metric\n        self.early_stop = early_stop\n        self.pretrain = pretrain\n        self.pretrain_file = get_or_create_path(pretrain_file)\n        self.logger.info(\n            \"TabNet:\"\n            \"\\nbatch_size : {}\"\n            \"\\nvirtual bs : {}\"\n            \"\\ndevice : {}\"\n            \"\\npretrain: {}\".format(self.batch_size, vbs, self.device, self.pretrain)\n        )\n        self.fitted = False\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n\n        self.tabnet_model = TabNet(inp_dim=self.d_feat, out_dim=self.out_dim, vbs=vbs, relax=relax).to(self.device)\n        self.tabnet_decoder = TabNet_Decoder(self.out_dim, self.d_feat, n_shared, n_ind, vbs, n_steps).to(self.device)\n        self.logger.info(\"model:\\n{:}\\n{:}\".format(self.tabnet_model, self.tabnet_decoder))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters([self.tabnet_model, self.tabnet_decoder])))\n\n        if optimizer.lower() == \"adam\":\n            self.pretrain_optimizer = optim.Adam(\n                list(self.tabnet_model.parameters()) + list(self.tabnet_decoder.parameters()), lr=self.lr\n            )\n            self.train_optimizer = optim.Adam(self.tabnet_model.parameters(), lr=self.lr)\n\n        elif optimizer.lower() == \"gd\":\n            self.pretrain_optimizer = optim.SGD(\n                list(self.tabnet_model.parameters()) + list(self.tabnet_decoder.parameters()), lr=self.lr\n            )\n            self.train_optimizer = optim.SGD(self.tabnet_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def pretrain_fn(self, dataset=DatasetH, pretrain_file=\"./pretrain/best.model\"):\n        get_or_create_path(pretrain_file)\n\n        [df_train, df_valid] = dataset.prepare(\n            [\"pretrain\", \"pretrain_validation\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        df_train.fillna(df_train.mean(), inplace=True)\n        df_valid.fillna(df_valid.mean(), inplace=True)\n\n        x_train = df_train[\"feature\"]\n        x_valid = df_valid[\"feature\"]\n\n        # Early stop setup\n        stop_steps = 0\n        train_loss = 0\n        best_loss = np.inf\n\n        for epoch_idx in range(self.pretrain_n_epochs):\n            self.logger.info(\"epoch: %s\" % (epoch_idx))\n            self.logger.info(\"pre-training...\")\n            self.pretrain_epoch(x_train)\n            self.logger.info(\"evaluating...\")\n            train_loss = self.pretrain_test_epoch(x_train)\n            valid_loss = self.pretrain_test_epoch(x_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_loss, valid_loss))\n\n            if valid_loss < best_loss:\n                self.logger.info(\"Save Model...\")\n                torch.save(self.tabnet_model.state_dict(), pretrain_file)\n                best_loss = valid_loss\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break",
  "def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n        if self.pretrain:\n            # there is a  pretrained model, load the model\n            self.logger.info(\"Pretrain...\")\n            self.pretrain_fn(dataset, self.pretrain_file)\n            self.logger.info(\"Load Pretrain model\")\n            self.tabnet_model.load_state_dict(torch.load(self.pretrain_file))\n\n        # adding one more linear layer to fit the final output dimension\n        self.tabnet_model = FinetuneModel(self.out_dim, self.final_out_dim, self.tabnet_model).to(self.device)\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n        df_train.fillna(df_train.mean(), inplace=True)\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for epoch_idx in range(self.n_epochs):\n            self.logger.info(\"epoch: %s\" % (epoch_idx))\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            valid_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = epoch_idx\n                best_param = copy.deepcopy(self.tabnet_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.tabnet_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\", data_key=DataHandlerLP.DK_I)\n        index = x_test.index\n        self.tabnet_model.eval()\n        x_values = torch.from_numpy(x_test.values)\n        x_values[torch.isnan(x_values)] = 0\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = x_values[begin:end].float().to(self.device)\n            priors = torch.ones(end - begin, self.d_feat).to(self.device)\n\n            with torch.no_grad():\n                pred = self.tabnet_model(x_batch, priors).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "def test_epoch(self, data_x, data_y):\n        # prepare training data\n        x_values = torch.from_numpy(data_x.values)\n        y_values = torch.from_numpy(np.squeeze(data_y.values))\n        x_values[torch.isnan(x_values)] = 0\n        y_values[torch.isnan(y_values)] = 0\n        self.tabnet_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n            feature = x_values[indices[i : i + self.batch_size]].float().to(self.device)\n            label = y_values[indices[i : i + self.batch_size]].float().to(self.device)\n            priors = torch.ones(self.batch_size, self.d_feat).to(self.device)\n            with torch.no_grad():\n                pred = self.tabnet_model(feature, priors)\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def train_epoch(self, x_train, y_train):\n        x_train_values = torch.from_numpy(x_train.values)\n        y_train_values = torch.from_numpy(np.squeeze(y_train.values))\n        x_train_values[torch.isnan(x_train_values)] = 0\n        y_train_values[torch.isnan(y_train_values)] = 0\n        self.tabnet_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = x_train_values[indices[i : i + self.batch_size]].float().to(self.device)\n            label = y_train_values[indices[i : i + self.batch_size]].float().to(self.device)\n            priors = torch.ones(self.batch_size, self.d_feat).to(self.device)\n            pred = self.tabnet_model(feature, priors)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.tabnet_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def pretrain_epoch(self, x_train):\n        train_set = torch.from_numpy(x_train.values)\n        train_set[torch.isnan(train_set)] = 0\n        indices = np.arange(len(train_set))\n        np.random.shuffle(indices)\n\n        self.tabnet_model.train()\n        self.tabnet_decoder.train()\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            S_mask = torch.bernoulli(torch.empty(self.batch_size, self.d_feat).fill_(self.ps))\n            x_train_values = train_set[indices[i : i + self.batch_size]] * (1 - S_mask)\n            y_train_values = train_set[indices[i : i + self.batch_size]] * (S_mask)\n\n            S_mask = S_mask.to(self.device)\n            feature = x_train_values.float().to(self.device)\n            label = y_train_values.float().to(self.device)\n            priors = 1 - S_mask\n            (vec, sparse_loss) = self.tabnet_model(feature, priors)\n            f = self.tabnet_decoder(vec)\n            loss = self.pretrain_loss_fn(label, f, S_mask)\n\n            self.pretrain_optimizer.zero_grad()\n            loss.backward()\n            self.pretrain_optimizer.step()",
  "def pretrain_test_epoch(self, x_train):\n        train_set = torch.from_numpy(x_train.values)\n        train_set[torch.isnan(train_set)] = 0\n        indices = np.arange(len(train_set))\n\n        self.tabnet_model.eval()\n        self.tabnet_decoder.eval()\n\n        losses = []\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            S_mask = torch.bernoulli(torch.empty(self.batch_size, self.d_feat).fill_(self.ps))\n            x_train_values = train_set[indices[i : i + self.batch_size]] * (1 - S_mask)\n            y_train_values = train_set[indices[i : i + self.batch_size]] * (S_mask)\n\n            feature = x_train_values.float().to(self.device)\n            label = y_train_values.float().to(self.device)\n            S_mask = S_mask.to(self.device)\n            priors = 1 - S_mask\n            with torch.no_grad():\n                (vec, sparse_loss) = self.tabnet_model(feature, priors)\n                f = self.tabnet_decoder(vec)\n\n                loss = self.pretrain_loss_fn(label, f, S_mask)\n            losses.append(loss.item())\n\n        return np.mean(losses)",
  "def pretrain_loss_fn(self, f_hat, f, S):\n        \"\"\"\n        Pretrain loss function defined in the original paper, read \"Tabular self-supervised learning\" in https://arxiv.org/pdf/1908.07442.pdf\n        \"\"\"\n        down_mean = torch.mean(f, dim=0)\n        down = torch.sqrt(torch.sum(torch.square(f - down_mean), dim=0))\n        up = (f_hat - f) * S\n        return torch.sum(torch.square(up / down))",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n        mask = torch.isfinite(label)\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def __init__(self, input_dim, output_dim, trained_model):\n        super().__init__()\n        self.model = trained_model\n        self.fc = nn.Linear(input_dim, output_dim)",
  "def forward(self, x, priors):\n        return self.fc(self.model(x, priors)[0]).squeeze()",
  "def __init__(self, inp_dim, out_dim, shared, n_ind, vbs):\n        super().__init__()\n        self.fea_tran = FeatureTransformer(inp_dim, out_dim, shared, n_ind, vbs)\n        self.fc = nn.Linear(out_dim, out_dim)",
  "def forward(self, x):\n        x = self.fea_tran(x)\n        return self.fc(x)",
  "def __init__(self, inp_dim, out_dim, n_shared, n_ind, vbs, n_steps):\n        \"\"\"\n        TabNet decoder that is used in pre-training\n        \"\"\"\n        super().__init__()\n        self.out_dim = out_dim\n        if n_shared > 0:\n            self.shared = nn.ModuleList()\n            self.shared.append(nn.Linear(inp_dim, 2 * out_dim))\n            for x in range(n_shared - 1):\n                self.shared.append(nn.Linear(out_dim, 2 * out_dim))  # preset the linear function we will use\n        else:\n            self.shared = None\n        self.n_steps = n_steps\n        self.steps = nn.ModuleList()\n        for x in range(n_steps):\n            self.steps.append(DecoderStep(inp_dim, out_dim, self.shared, n_ind, vbs))",
  "def forward(self, x):\n        out = torch.zeros(x.size(0), self.out_dim).to(x.device)\n        for step in self.steps:\n            out += step(x)\n        return out",
  "def __init__(self, inp_dim=6, out_dim=6, n_d=64, n_a=64, n_shared=2, n_ind=2, n_steps=5, relax=1.2, vbs=1024):\n        \"\"\"\n        TabNet AKA the original encoder\n\n        Args:\n            n_d: dimension of the features used to calculate the final results\n            n_a: dimension of the features input to the attention transformer of the next step\n            n_shared: numbr of shared steps in feature transfomer(optional)\n            n_ind: number of independent steps in feature transformer\n            n_steps: number of steps of pass through tabbet\n            relax coefficient:\n            virtual batch size:\n        \"\"\"\n        super().__init__()\n\n        # set the number of shared step in feature transformer\n        if n_shared > 0:\n            self.shared = nn.ModuleList()\n            self.shared.append(nn.Linear(inp_dim, 2 * (n_d + n_a)))\n            for x in range(n_shared - 1):\n                self.shared.append(nn.Linear(n_d + n_a, 2 * (n_d + n_a)))  # preset the linear function we will use\n        else:\n            self.shared = None\n\n        self.first_step = FeatureTransformer(inp_dim, n_d + n_a, self.shared, n_ind, vbs)\n        self.steps = nn.ModuleList()\n        for x in range(n_steps - 1):\n            self.steps.append(DecisionStep(inp_dim, n_d, n_a, self.shared, n_ind, relax, vbs))\n        self.fc = nn.Linear(n_d, out_dim)\n        self.bn = nn.BatchNorm1d(inp_dim, momentum=0.01)\n        self.n_d = n_d",
  "def forward(self, x, priors):\n        assert not torch.isnan(x).any()\n        x = self.bn(x)\n        x_a = self.first_step(x)[:, self.n_d :]\n        sparse_loss = []\n        out = torch.zeros(x.size(0), self.n_d).to(x.device)\n        for step in self.steps:\n            x_te, l = step(x, x_a, priors)\n            out += F.relu(x_te[:, : self.n_d])  # split the feautre from feat_transformer\n            x_a = x_te[:, self.n_d :]\n            sparse_loss.append(l)\n        return self.fc(out), sum(sparse_loss)",
  "def __init__(self, inp, vbs=1024, momentum=0.01):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(inp, momentum=momentum)\n        self.vbs = vbs",
  "def forward(self, x):\n        if x.size(0) <= self.vbs:  # can not be chunked\n            return self.bn(x)\n        else:\n            chunk = torch.chunk(x, x.size(0) // self.vbs, 0)\n            res = [self.bn(y) for y in chunk]\n            return torch.cat(res, 0)",
  "def __init__(self, inp_dim, out_dim, fc=None, vbs=1024):\n        super().__init__()\n        if fc:\n            self.fc = fc\n        else:\n            self.fc = nn.Linear(inp_dim, out_dim * 2)\n        self.bn = GBN(out_dim * 2, vbs=vbs)\n        self.od = out_dim",
  "def forward(self, x):\n        x = self.bn(self.fc(x))\n        return torch.mul(x[:, : self.od], torch.sigmoid(x[:, self.od :]))",
  "def __init__(self, d_a, inp_dim, relax, vbs=1024):\n        super().__init__()\n        self.fc = nn.Linear(d_a, inp_dim)\n        self.bn = GBN(inp_dim, vbs=vbs)\n        self.r = relax",
  "def forward(self, a, priors):\n        a = self.bn(self.fc(a))\n        mask = SparsemaxFunction.apply(a * priors)\n        priors = priors * (self.r - mask)  # updating the prior\n        return mask",
  "def __init__(self, inp_dim, out_dim, shared, n_ind, vbs):\n        super().__init__()\n        first = True\n        self.shared = nn.ModuleList()\n        if shared:\n            self.shared.append(GLU(inp_dim, out_dim, shared[0], vbs=vbs))\n            first = False\n            for fc in shared[1:]:\n                self.shared.append(GLU(out_dim, out_dim, fc, vbs=vbs))\n        else:\n            self.shared = None\n        self.independ = nn.ModuleList()\n        if first:\n            self.independ.append(GLU(inp, out_dim, vbs=vbs))\n        for x in range(first, n_ind):\n            self.independ.append(GLU(out_dim, out_dim, vbs=vbs))\n        self.scale = float(np.sqrt(0.5))",
  "def forward(self, x):\n        if self.shared:\n            x = self.shared[0](x)\n            for glu in self.shared[1:]:\n                x = torch.add(x, glu(x))\n                x = x * self.scale\n        for glu in self.independ:\n            x = torch.add(x, glu(x))\n            x = x * self.scale\n        return x",
  "def __init__(self, inp_dim, n_d, n_a, shared, n_ind, relax, vbs):\n        super().__init__()\n        self.atten_tran = AttentionTransformer(n_a, inp_dim, relax, vbs)\n        self.fea_tran = FeatureTransformer(inp_dim, n_d + n_a, shared, n_ind, vbs)",
  "def forward(self, x, a, priors):\n        mask = self.atten_tran(a, priors)\n        sparse_loss = ((-1) * mask * torch.log(mask + 1e-10)).mean()\n        x = self.fea_tran(x * mask)\n        return x, sparse_loss",
  "def forward(ctx, input, dim=-1):\n        ctx.dim = dim\n        max_val, _ = input.max(dim=dim, keepdim=True)\n        input -= max_val  # same numerical stability trick as for softmax\n        tau, supp_size = SparsemaxFunction.threshold_and_support(input, dim=dim)\n        output = torch.clamp(input - tau, min=0)\n        ctx.save_for_backward(supp_size, output)\n        return output",
  "def backward(ctx, grad_output):\n        supp_size, output = ctx.saved_tensors\n        dim = ctx.dim\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n\n        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n        v_hat = v_hat.unsqueeze(dim)\n        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n        return grad_input, None",
  "def threshold_and_support(input, dim=-1):\n        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n        input_cumsum = input_srt.cumsum(dim) - 1\n        rhos = make_ix_like(input, dim)\n        support = rhos * input_srt > input_cumsum\n\n        support_size = support.sum(dim=dim).unsqueeze(dim)\n        tau = input_cumsum.gather(dim, support_size - 1)\n        tau /= support_size.to(input.dtype)\n        return tau, support_size",
  "class LGBModel(ModelFT):\n    \"\"\"LightGBM Model\"\"\"\n\n    def __init__(self, loss=\"mse\", **kwargs):\n        if loss not in {\"mse\", \"binary\"}:\n            raise NotImplementedError\n        self.params = {\"objective\": loss, \"verbosity\": -1}\n        self.params.update(kwargs)\n        self.model = None\n\n    def _prepare_data(self, dataset: DatasetH):\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"], col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        # Lightgbm need 1D array as its label\n        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n            y_train, y_valid = np.squeeze(y_train.values), np.squeeze(y_valid.values)\n        else:\n            raise ValueError(\"LightGBM doesn't support multi-label training\")\n\n        dtrain = lgb.Dataset(x_train.values, label=y_train)\n        dvalid = lgb.Dataset(x_valid.values, label=y_valid)\n        return dtrain, dvalid\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        num_boost_round=1000,\n        early_stopping_rounds=50,\n        verbose_eval=20,\n        evals_result=dict(),\n        **kwargs\n    ):\n        dtrain, dvalid = self._prepare_data(dataset)\n        self.model = lgb.train(\n            self.params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            valid_sets=[dtrain, dvalid],\n            valid_names=[\"train\", \"valid\"],\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=verbose_eval,\n            evals_result=evals_result,\n            **kwargs\n        )\n        evals_result[\"train\"] = list(evals_result[\"train\"].values())[0]\n        evals_result[\"valid\"] = list(evals_result[\"valid\"].values())[0]\n\n    def predict(self, dataset):\n        if self.model is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\", data_key=DataHandlerLP.DK_I)\n        return pd.Series(self.model.predict(x_test.values), index=x_test.index)\n\n    def finetune(self, dataset: DatasetH, num_boost_round=10, verbose_eval=20):\n        \"\"\"\n        finetune model\n\n        Parameters\n        ----------\n        dataset : DatasetH\n            dataset for finetuning\n        num_boost_round : int\n            number of round to finetune model\n        verbose_eval : int\n            verbose level\n        \"\"\"\n        # Based on existing model and finetune by train more rounds\n        dtrain, _ = self._prepare_data(dataset)\n        self.model = lgb.train(\n            self.params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            init_model=self.model,\n            valid_sets=[dtrain],\n            valid_names=[\"train\"],\n            verbose_eval=verbose_eval,\n        )",
  "def __init__(self, loss=\"mse\", **kwargs):\n        if loss not in {\"mse\", \"binary\"}:\n            raise NotImplementedError\n        self.params = {\"objective\": loss, \"verbosity\": -1}\n        self.params.update(kwargs)\n        self.model = None",
  "def _prepare_data(self, dataset: DatasetH):\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"], col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        # Lightgbm need 1D array as its label\n        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n            y_train, y_valid = np.squeeze(y_train.values), np.squeeze(y_valid.values)\n        else:\n            raise ValueError(\"LightGBM doesn't support multi-label training\")\n\n        dtrain = lgb.Dataset(x_train.values, label=y_train)\n        dvalid = lgb.Dataset(x_valid.values, label=y_valid)\n        return dtrain, dvalid",
  "def fit(\n        self,\n        dataset: DatasetH,\n        num_boost_round=1000,\n        early_stopping_rounds=50,\n        verbose_eval=20,\n        evals_result=dict(),\n        **kwargs\n    ):\n        dtrain, dvalid = self._prepare_data(dataset)\n        self.model = lgb.train(\n            self.params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            valid_sets=[dtrain, dvalid],\n            valid_names=[\"train\", \"valid\"],\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=verbose_eval,\n            evals_result=evals_result,\n            **kwargs\n        )\n        evals_result[\"train\"] = list(evals_result[\"train\"].values())[0]\n        evals_result[\"valid\"] = list(evals_result[\"valid\"].values())[0]",
  "def predict(self, dataset):\n        if self.model is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\", data_key=DataHandlerLP.DK_I)\n        return pd.Series(self.model.predict(x_test.values), index=x_test.index)",
  "def finetune(self, dataset: DatasetH, num_boost_round=10, verbose_eval=20):\n        \"\"\"\n        finetune model\n\n        Parameters\n        ----------\n        dataset : DatasetH\n            dataset for finetuning\n        num_boost_round : int\n            number of round to finetune model\n        verbose_eval : int\n            verbose level\n        \"\"\"\n        # Based on existing model and finetune by train more rounds\n        dtrain, _ = self._prepare_data(dataset)\n        self.model = lgb.train(\n            self.params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            init_model=self.model,\n            valid_sets=[dtrain],\n            valid_names=[\"train\"],\n            verbose_eval=verbose_eval,\n        )",
  "class GATs(Model):\n    \"\"\"GATs Model\n\n    Parameters\n    ----------\n    lr : float\n        learning rate\n    d_feat : int\n        input dimensions for each time step\n    metric : str\n        the evaluate metric used in early stop\n    optimizer : str\n        optimizer name\n    GPU : int\n        the GPU ID used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        early_stop=20,\n        loss=\"mse\",\n        base_model=\"GRU\",\n        with_pretrain=True,\n        model_path=None,\n        optimizer=\"adam\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"GATs\")\n        self.logger.info(\"GATs pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.base_model = base_model\n        self.with_pretrain = with_pretrain\n        self.model_path = model_path\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.use_gpu = torch.cuda.is_available()\n        self.seed = seed\n\n        self.logger.info(\n            \"GATs parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\nbase_model : {}\"\n            \"\\nwith_pretrain : {}\"\n            \"\\nmodel_path : {}\"\n            \"\\ndevice : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                base_model,\n                with_pretrain,\n                model_path,\n                self.device,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.GAT_model = GATModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n            base_model=self.base_model,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.GAT_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.GAT_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.GAT_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.GAT_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.GAT_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def get_daily_inter(self, df, shuffle=False):\n        # organize the train data into daily batches\n        daily_count = df.groupby(level=0).size().values\n        daily_index = np.roll(np.cumsum(daily_count), 1)\n        daily_index[0] = 0\n        if shuffle:\n            # shuffle data\n            daily_shuffle = list(zip(daily_index, daily_count))\n            np.random.shuffle(daily_shuffle)\n            daily_index, daily_count = zip(*daily_shuffle)\n        return daily_index, daily_count\n\n    def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n        self.GAT_model.train()\n\n        # organize the train data into daily batches\n        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)\n\n        for idx, count in zip(daily_index, daily_count):\n            batch = slice(idx, idx + count)\n            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[batch]).float().to(self.device)\n\n            pred = self.GAT_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.GAT_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.GAT_model.eval()\n\n        scores = []\n        losses = []\n\n        # organize the test data into daily batches\n        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)\n\n        for idx, count in zip(daily_index, daily_count):\n            batch = slice(idx, idx + count)\n            feature = torch.from_numpy(x_values[batch]).float().to(self.device)\n            label = torch.from_numpy(y_values[batch]).float().to(self.device)\n\n            pred = self.GAT_model(feature)\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid, df_test = dataset.prepare(\n            [\"train\", \"valid\", \"test\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # load pretrained base_model\n        if self.with_pretrain:\n            if self.model_path == None:\n                raise ValueError(\"the path of the pretrained model should be given first!\")\n            self.logger.info(\"Loading pretrained model...\")\n            if self.base_model == \"LSTM\":\n                pretrained_model = LSTMModel()\n                pretrained_model.load_state_dict(torch.load(self.model_path))\n            elif self.base_model == \"GRU\":\n                pretrained_model = GRUModel()\n                pretrained_model.load_state_dict(torch.load(self.model_path))\n            else:\n                raise ValueError(\"unknown base model name `%s`\" % self.base_model)\n\n            model_dict = self.GAT_model.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            self.GAT_model.load_state_dict(model_dict)\n            self.logger.info(\"Loading pretrained model Done...\")\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.GAT_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.GAT_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.GAT_model.eval()\n        x_values = x_test.values\n        preds = []\n\n        # organize the data into daily batches\n        daily_index, daily_count = self.get_daily_inter(x_test, shuffle=False)\n\n        for idx, count in zip(daily_index, daily_count):\n            batch = slice(idx, idx + count)\n            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.GAT_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "class GATModel(nn.Module):\n    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model=\"GRU\"):\n        super().__init__()\n\n        if base_model == \"GRU\":\n            self.rnn = nn.GRU(\n                input_size=d_feat,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=dropout,\n            )\n        elif base_model == \"LSTM\":\n            self.rnn = nn.LSTM(\n                input_size=d_feat,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=dropout,\n            )\n        else:\n            raise ValueError(\"unknown base model name `%s`\" % base_model)\n\n        self.hidden_size = hidden_size\n        self.d_feat = d_feat\n        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)\n        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))\n        self.a.requires_grad = True\n        self.fc = nn.Linear(self.hidden_size, self.hidden_size)\n        self.fc_out = nn.Linear(hidden_size, 1)\n        self.leaky_relu = nn.LeakyReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def cal_attention(self, x, y):\n        x = self.transformation(x)\n        y = self.transformation(y)\n\n        sample_num = x.shape[0]\n        dim = x.shape[1]\n        e_x = x.expand(sample_num, sample_num, dim)\n        e_y = torch.transpose(e_x, 0, 1)\n        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)\n        self.a_t = torch.t(self.a)\n        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)\n        attention_out = self.leaky_relu(attention_out)\n        att_weight = self.softmax(attention_out)\n        return att_weight\n\n    def forward(self, x):\n        # x: [N, F*T]\n        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n        x = x.permute(0, 2, 1)  # [N, T, F]\n        out, _ = self.rnn(x)\n        hidden = out[:, -1, :]\n        att_weight = self.cal_attention(hidden, hidden)\n        hidden = att_weight.mm(hidden) + hidden\n        hidden = self.fc(hidden)\n        hidden = self.leaky_relu(hidden)\n        return self.fc_out(hidden).squeeze()",
  "def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        early_stop=20,\n        loss=\"mse\",\n        base_model=\"GRU\",\n        with_pretrain=True,\n        model_path=None,\n        optimizer=\"adam\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"GATs\")\n        self.logger.info(\"GATs pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.base_model = base_model\n        self.with_pretrain = with_pretrain\n        self.model_path = model_path\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.use_gpu = torch.cuda.is_available()\n        self.seed = seed\n\n        self.logger.info(\n            \"GATs parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\nbase_model : {}\"\n            \"\\nwith_pretrain : {}\"\n            \"\\nmodel_path : {}\"\n            \"\\ndevice : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                base_model,\n                with_pretrain,\n                model_path,\n                self.device,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.GAT_model = GATModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n            base_model=self.base_model,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.GAT_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.GAT_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.GAT_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.GAT_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.GAT_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def get_daily_inter(self, df, shuffle=False):\n        # organize the train data into daily batches\n        daily_count = df.groupby(level=0).size().values\n        daily_index = np.roll(np.cumsum(daily_count), 1)\n        daily_index[0] = 0\n        if shuffle:\n            # shuffle data\n            daily_shuffle = list(zip(daily_index, daily_count))\n            np.random.shuffle(daily_shuffle)\n            daily_index, daily_count = zip(*daily_shuffle)\n        return daily_index, daily_count",
  "def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n        self.GAT_model.train()\n\n        # organize the train data into daily batches\n        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)\n\n        for idx, count in zip(daily_index, daily_count):\n            batch = slice(idx, idx + count)\n            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[batch]).float().to(self.device)\n\n            pred = self.GAT_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.GAT_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.GAT_model.eval()\n\n        scores = []\n        losses = []\n\n        # organize the test data into daily batches\n        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)\n\n        for idx, count in zip(daily_index, daily_count):\n            batch = slice(idx, idx + count)\n            feature = torch.from_numpy(x_values[batch]).float().to(self.device)\n            label = torch.from_numpy(y_values[batch]).float().to(self.device)\n\n            pred = self.GAT_model(feature)\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid, df_test = dataset.prepare(\n            [\"train\", \"valid\", \"test\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # load pretrained base_model\n        if self.with_pretrain:\n            if self.model_path == None:\n                raise ValueError(\"the path of the pretrained model should be given first!\")\n            self.logger.info(\"Loading pretrained model...\")\n            if self.base_model == \"LSTM\":\n                pretrained_model = LSTMModel()\n                pretrained_model.load_state_dict(torch.load(self.model_path))\n            elif self.base_model == \"GRU\":\n                pretrained_model = GRUModel()\n                pretrained_model.load_state_dict(torch.load(self.model_path))\n            else:\n                raise ValueError(\"unknown base model name `%s`\" % self.base_model)\n\n            model_dict = self.GAT_model.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            self.GAT_model.load_state_dict(model_dict)\n            self.logger.info(\"Loading pretrained model Done...\")\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.GAT_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.GAT_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.GAT_model.eval()\n        x_values = x_test.values\n        preds = []\n\n        # organize the data into daily batches\n        daily_index, daily_count = self.get_daily_inter(x_test, shuffle=False)\n\n        for idx, count in zip(daily_index, daily_count):\n            batch = slice(idx, idx + count)\n            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.GAT_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model=\"GRU\"):\n        super().__init__()\n\n        if base_model == \"GRU\":\n            self.rnn = nn.GRU(\n                input_size=d_feat,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=dropout,\n            )\n        elif base_model == \"LSTM\":\n            self.rnn = nn.LSTM(\n                input_size=d_feat,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                batch_first=True,\n                dropout=dropout,\n            )\n        else:\n            raise ValueError(\"unknown base model name `%s`\" % base_model)\n\n        self.hidden_size = hidden_size\n        self.d_feat = d_feat\n        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)\n        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))\n        self.a.requires_grad = True\n        self.fc = nn.Linear(self.hidden_size, self.hidden_size)\n        self.fc_out = nn.Linear(hidden_size, 1)\n        self.leaky_relu = nn.LeakyReLU()\n        self.softmax = nn.Softmax(dim=1)",
  "def cal_attention(self, x, y):\n        x = self.transformation(x)\n        y = self.transformation(y)\n\n        sample_num = x.shape[0]\n        dim = x.shape[1]\n        e_x = x.expand(sample_num, sample_num, dim)\n        e_y = torch.transpose(e_x, 0, 1)\n        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)\n        self.a_t = torch.t(self.a)\n        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)\n        attention_out = self.leaky_relu(attention_out)\n        att_weight = self.softmax(attention_out)\n        return att_weight",
  "def forward(self, x):\n        # x: [N, F*T]\n        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n        x = x.permute(0, 2, 1)  # [N, T, F]\n        out, _ = self.rnn(x)\n        hidden = out[:, -1, :]\n        att_weight = self.cal_attention(hidden, hidden)\n        hidden = att_weight.mm(hidden) + hidden\n        hidden = self.fc(hidden)\n        hidden = self.leaky_relu(hidden)\n        return self.fc_out(hidden).squeeze()",
  "class CatBoostModel(Model):\n    \"\"\"CatBoost Model\"\"\"\n\n    def __init__(self, loss=\"RMSE\", **kwargs):\n        # There are more options\n        if loss not in {\"RMSE\", \"Logloss\"}:\n            raise NotImplementedError\n        self._params = {\"loss_function\": loss}\n        self._params.update(kwargs)\n        self.model = None\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        num_boost_round=1000,\n        early_stopping_rounds=50,\n        verbose_eval=20,\n        evals_result=dict(),\n        **kwargs\n    ):\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        # CatBoost needs 1D array as its label\n        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n            y_train_1d, y_valid_1d = np.squeeze(y_train.values), np.squeeze(y_valid.values)\n        else:\n            raise ValueError(\"CatBoost doesn't support multi-label training\")\n\n        train_pool = Pool(data=x_train, label=y_train_1d)\n        valid_pool = Pool(data=x_valid, label=y_valid_1d)\n\n        # Initialize the catboost model\n        self._params[\"iterations\"] = num_boost_round\n        self._params[\"early_stopping_rounds\"] = early_stopping_rounds\n        self._params[\"verbose_eval\"] = verbose_eval\n        self._params[\"task_type\"] = \"GPU\" if get_gpu_device_count() > 0 else \"CPU\"\n        self.model = CatBoost(self._params, **kwargs)\n\n        # train the model\n        self.model.fit(train_pool, eval_set=valid_pool, use_best_model=True, **kwargs)\n\n        evals_result = self.model.get_evals_result()\n        evals_result[\"train\"] = list(evals_result[\"learn\"].values())[0]\n        evals_result[\"valid\"] = list(evals_result[\"validation\"].values())[0]\n\n    def predict(self, dataset):\n        if self.model is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        return pd.Series(self.model.predict(x_test.values), index=x_test.index)",
  "def __init__(self, loss=\"RMSE\", **kwargs):\n        # There are more options\n        if loss not in {\"RMSE\", \"Logloss\"}:\n            raise NotImplementedError\n        self._params = {\"loss_function\": loss}\n        self._params.update(kwargs)\n        self.model = None",
  "def fit(\n        self,\n        dataset: DatasetH,\n        num_boost_round=1000,\n        early_stopping_rounds=50,\n        verbose_eval=20,\n        evals_result=dict(),\n        **kwargs\n    ):\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        # CatBoost needs 1D array as its label\n        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n            y_train_1d, y_valid_1d = np.squeeze(y_train.values), np.squeeze(y_valid.values)\n        else:\n            raise ValueError(\"CatBoost doesn't support multi-label training\")\n\n        train_pool = Pool(data=x_train, label=y_train_1d)\n        valid_pool = Pool(data=x_valid, label=y_valid_1d)\n\n        # Initialize the catboost model\n        self._params[\"iterations\"] = num_boost_round\n        self._params[\"early_stopping_rounds\"] = early_stopping_rounds\n        self._params[\"verbose_eval\"] = verbose_eval\n        self._params[\"task_type\"] = \"GPU\" if get_gpu_device_count() > 0 else \"CPU\"\n        self.model = CatBoost(self._params, **kwargs)\n\n        # train the model\n        self.model.fit(train_pool, eval_set=valid_pool, use_best_model=True, **kwargs)\n\n        evals_result = self.model.get_evals_result()\n        evals_result[\"train\"] = list(evals_result[\"learn\"].values())[0]\n        evals_result[\"valid\"] = list(evals_result[\"validation\"].values())[0]",
  "def predict(self, dataset):\n        if self.model is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        return pd.Series(self.model.predict(x_test.values), index=x_test.index)",
  "class GRU(Model):\n    \"\"\"GRU Model\n\n    Parameters\n    ----------\n    d_feat : int\n        input dimension for each time step\n    metric: str\n        the evaluate metric used in early stop\n    optimizer : str\n        optimizer name\n    GPU : str\n        the GPU ID(s) used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"GRU\")\n        self.logger.info(\"GRU pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n\n        self.logger.info(\n            \"GRU parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\nvisible_GPU : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                GPU,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.gru_model = GRUModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.gru_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.gru_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.gru_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.gru_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.gru_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n\n        self.gru_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.gru_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.gru_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.gru_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.gru_model(feature)\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid, df_test = dataset.prepare(\n            [\"train\", \"valid\", \"test\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.gru_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.gru_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.gru_model.eval()\n        x_values = x_test.values\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.gru_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "class GRUModel(nn.Module):\n    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n        super().__init__()\n\n        self.rnn = nn.GRU(\n            input_size=d_feat,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n        )\n        self.fc_out = nn.Linear(hidden_size, 1)\n\n        self.d_feat = d_feat\n\n    def forward(self, x):\n        # x: [N, F*T]\n        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n        x = x.permute(0, 2, 1)  # [N, T, F]\n        out, _ = self.rnn(x)\n        return self.fc_out(out[:, -1, :]).squeeze()",
  "def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"GRU\")\n        self.logger.info(\"GRU pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n\n        self.logger.info(\n            \"GRU parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\nvisible_GPU : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                GPU,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.gru_model = GRUModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.gru_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.gru_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.gru_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.gru_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.gru_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n\n        self.gru_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.gru_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.gru_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.gru_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.gru_model(feature)\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid, df_test = dataset.prepare(\n            [\"train\", \"valid\", \"test\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.gru_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.gru_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.gru_model.eval()\n        x_values = x_test.values\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.gru_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n        super().__init__()\n\n        self.rnn = nn.GRU(\n            input_size=d_feat,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n        )\n        self.fc_out = nn.Linear(hidden_size, 1)\n\n        self.d_feat = d_feat",
  "def forward(self, x):\n        # x: [N, F*T]\n        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n        x = x.permute(0, 2, 1)  # [N, T, F]\n        out, _ = self.rnn(x)\n        return self.fc_out(out[:, -1, :]).squeeze()",
  "class DNNModelPytorch(Model):\n    \"\"\"DNN Model\n\n    Parameters\n    ----------\n    input_dim : int\n        input dimension\n    output_dim : int\n        output dimension\n    layers : tuple\n        layer sizes\n    lr : float\n        learning rate\n    lr_decay : float\n        learning rate decay\n    lr_decay_steps : int\n        learning rate decay steps\n    optimizer : str\n        optimizer name\n    GPU : int\n        the GPU ID used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        layers=(256,),\n        lr=0.001,\n        max_steps=300,\n        batch_size=2000,\n        early_stop_rounds=50,\n        eval_steps=20,\n        lr_decay=0.96,\n        lr_decay_steps=100,\n        optimizer=\"gd\",\n        loss=\"mse\",\n        GPU=0,\n        seed=None,\n        weight_decay=0.0,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"DNNModelPytorch\")\n        self.logger.info(\"DNN pytorch version...\")\n\n        # set hyper-parameters.\n        self.layers = layers\n        self.lr = lr\n        self.max_steps = max_steps\n        self.batch_size = batch_size\n        self.early_stop_rounds = early_stop_rounds\n        self.eval_steps = eval_steps\n        self.lr_decay = lr_decay\n        self.lr_decay_steps = lr_decay_steps\n        self.optimizer = optimizer.lower()\n        self.loss_type = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n        self.weight_decay = weight_decay\n\n        self.logger.info(\n            \"DNN parameters setting:\"\n            \"\\nlayers : {}\"\n            \"\\nlr : {}\"\n            \"\\nmax_steps : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop_rounds : {}\"\n            \"\\neval_steps : {}\"\n            \"\\nlr_decay : {}\"\n            \"\\nlr_decay_steps : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\neval_steps : {}\"\n            \"\\nseed : {}\"\n            \"\\ndevice : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nweight_decay : {}\".format(\n                layers,\n                lr,\n                max_steps,\n                batch_size,\n                early_stop_rounds,\n                eval_steps,\n                lr_decay,\n                lr_decay_steps,\n                optimizer,\n                loss,\n                eval_steps,\n                seed,\n                self.device,\n                self.use_gpu,\n                weight_decay,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        if loss not in {\"mse\", \"binary\"}:\n            raise NotImplementedError(\"loss {} is not supported!\".format(loss))\n        self._scorer = mean_squared_error if loss == \"mse\" else roc_auc_score\n\n        self.dnn_model = Net(input_dim, output_dim, layers, loss=self.loss_type)\n        self.logger.info(\"model:\\n{:}\".format(self.dnn_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.dnn_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.dnn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.dnn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        # Reduce learning rate when loss has stopped decrease\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.train_optimizer,\n            mode=\"min\",\n            factor=0.5,\n            patience=10,\n            verbose=True,\n            threshold=0.0001,\n            threshold_mode=\"rel\",\n            cooldown=0,\n            min_lr=0.00001,\n            eps=1e-08,\n        )\n\n        self.fitted = False\n        self.dnn_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        verbose=True,\n        save_path=None,\n    ):\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"], col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n        try:\n            wdf_train, wdf_valid = dataset.prepare([\"train\", \"valid\"], col_set=[\"weight\"], data_key=DataHandlerLP.DK_L)\n            w_train, w_valid = wdf_train[\"weight\"], wdf_valid[\"weight\"]\n        except KeyError as e:\n            w_train = pd.DataFrame(np.ones_like(y_train.values), index=y_train.index)\n            w_valid = pd.DataFrame(np.ones_like(y_valid.values), index=y_valid.index)\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_loss = np.inf\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n        # return\n        # prepare training data\n        x_train_values = torch.from_numpy(x_train.values).float()\n        y_train_values = torch.from_numpy(y_train.values).float()\n        w_train_values = torch.from_numpy(w_train.values).float()\n        train_num = y_train_values.shape[0]\n        # prepare validation data\n        x_val_auto = torch.from_numpy(x_valid.values).float().to(self.device)\n        y_val_auto = torch.from_numpy(y_valid.values).float().to(self.device)\n        w_val_auto = torch.from_numpy(w_valid.values).float().to(self.device)\n\n        for step in range(self.max_steps):\n            if stop_steps >= self.early_stop_rounds:\n                if verbose:\n                    self.logger.info(\"\\tearly stop\")\n                break\n            loss = AverageMeter()\n            self.dnn_model.train()\n            self.train_optimizer.zero_grad()\n            choice = np.random.choice(train_num, self.batch_size)\n            x_batch_auto = x_train_values[choice].to(self.device)\n            y_batch_auto = y_train_values[choice].to(self.device)\n            w_batch_auto = w_train_values[choice].to(self.device)\n\n            # forward\n            preds = self.dnn_model(x_batch_auto)\n            cur_loss = self.get_loss(preds, w_batch_auto, y_batch_auto, self.loss_type)\n            cur_loss.backward()\n            self.train_optimizer.step()\n            loss.update(cur_loss.item())\n            R.log_metrics(train_loss=loss.avg, step=step)\n\n            # validation\n            train_loss += loss.val\n            # for evert `eval_steps` steps or at the last steps, we will evaluate the model.\n            if step % self.eval_steps == 0 or step + 1 == self.max_steps:\n                stop_steps += 1\n                train_loss /= self.eval_steps\n\n                with torch.no_grad():\n                    self.dnn_model.eval()\n                    loss_val = AverageMeter()\n\n                    # forward\n                    preds = self.dnn_model(x_val_auto)\n                    cur_loss_val = self.get_loss(preds, w_val_auto, y_val_auto, self.loss_type)\n                    loss_val.update(cur_loss_val.item())\n                R.log_metrics(val_loss=loss_val.val, step=step)\n                if verbose:\n                    self.logger.info(\n                        \"[Epoch {}]: train_loss {:.6f}, valid_loss {:.6f}\".format(step, train_loss, loss_val.val)\n                    )\n                evals_result[\"train\"].append(train_loss)\n                evals_result[\"valid\"].append(loss_val.val)\n                if loss_val.val < best_loss:\n                    if verbose:\n                        self.logger.info(\n                            \"\\tvalid loss update from {:.6f} to {:.6f}, save checkpoint.\".format(\n                                best_loss, loss_val.val\n                            )\n                        )\n                    best_loss = loss_val.val\n                    stop_steps = 0\n                    torch.save(self.dnn_model.state_dict(), save_path)\n                train_loss = 0\n                # update learning rate\n                self.scheduler.step(cur_loss_val)\n\n        # restore the optimal parameters after training\n        self.dnn_model.load_state_dict(torch.load(save_path))\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def get_loss(self, pred, w, target, loss_type):\n        if loss_type == \"mse\":\n            sqr_loss = torch.mul(pred - target, pred - target)\n            loss = torch.mul(sqr_loss, w).mean()\n            return loss\n        elif loss_type == \"binary\":\n            loss = nn.BCELoss(weight=w)\n            return loss(pred, target)\n        else:\n            raise NotImplementedError(\"loss {} is not supported!\".format(loss_type))\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test_pd = dataset.prepare(\"test\", col_set=\"feature\")\n        x_test = torch.from_numpy(x_test_pd.values).float().to(self.device)\n        self.dnn_model.eval()\n\n        with torch.no_grad():\n            preds = self.dnn_model(x_test).detach().cpu().numpy()\n        return pd.Series(np.squeeze(preds), index=x_test_pd.index)\n\n    def save(self, filename, **kwargs):\n        with save_multiple_parts_file(filename) as model_dir:\n            model_path = os.path.join(model_dir, os.path.split(model_dir)[-1])\n            # Save model\n            torch.save(self.dnn_model.state_dict(), model_path)\n\n    def load(self, buffer, **kwargs):\n        with unpack_archive_with_buffer(buffer) as model_dir:\n            # Get model name\n            _model_name = os.path.splitext(list(filter(lambda x: x.startswith(\"model.bin\"), os.listdir(model_dir)))[0])[\n                0\n            ]\n            _model_path = os.path.join(model_dir, _model_name)\n            # Load model\n            self.dnn_model.load_state_dict(torch.load(_model_path))\n        self._fitted = True",
  "class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count",
  "class Net(nn.Module):\n    def __init__(self, input_dim, output_dim, layers=(256, 512, 768, 512, 256, 128, 64), loss=\"mse\"):\n        super(Net, self).__init__()\n        layers = [input_dim] + list(layers)\n        dnn_layers = []\n        drop_input = nn.Dropout(0.05)\n        dnn_layers.append(drop_input)\n        for i, (input_dim, hidden_units) in enumerate(zip(layers[:-1], layers[1:])):\n            fc = nn.Linear(input_dim, hidden_units)\n            activation = nn.LeakyReLU(negative_slope=0.1, inplace=False)\n            bn = nn.BatchNorm1d(hidden_units)\n            seq = nn.Sequential(fc, bn, activation)\n            dnn_layers.append(seq)\n        drop_input = nn.Dropout(0.05)\n        dnn_layers.append(drop_input)\n        if loss == \"mse\":\n            fc = nn.Linear(hidden_units, output_dim)\n            dnn_layers.append(fc)\n\n        elif loss == \"binary\":\n            fc = nn.Linear(hidden_units, output_dim)\n            sigmoid = nn.Sigmoid()\n            dnn_layers.append(nn.Sequential(fc, sigmoid))\n        else:\n            raise NotImplementedError(\"loss {} is not supported!\".format(loss))\n        # optimizer\n        self.dnn_layers = nn.ModuleList(dnn_layers)\n        self._weight_init()\n\n    def _weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, a=0.1, mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n\n    def forward(self, x):\n        cur_output = x\n        for i, now_layer in enumerate(self.dnn_layers):\n            cur_output = now_layer(cur_output)\n        return cur_output",
  "def __init__(\n        self,\n        input_dim,\n        output_dim,\n        layers=(256,),\n        lr=0.001,\n        max_steps=300,\n        batch_size=2000,\n        early_stop_rounds=50,\n        eval_steps=20,\n        lr_decay=0.96,\n        lr_decay_steps=100,\n        optimizer=\"gd\",\n        loss=\"mse\",\n        GPU=0,\n        seed=None,\n        weight_decay=0.0,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"DNNModelPytorch\")\n        self.logger.info(\"DNN pytorch version...\")\n\n        # set hyper-parameters.\n        self.layers = layers\n        self.lr = lr\n        self.max_steps = max_steps\n        self.batch_size = batch_size\n        self.early_stop_rounds = early_stop_rounds\n        self.eval_steps = eval_steps\n        self.lr_decay = lr_decay\n        self.lr_decay_steps = lr_decay_steps\n        self.optimizer = optimizer.lower()\n        self.loss_type = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n        self.weight_decay = weight_decay\n\n        self.logger.info(\n            \"DNN parameters setting:\"\n            \"\\nlayers : {}\"\n            \"\\nlr : {}\"\n            \"\\nmax_steps : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop_rounds : {}\"\n            \"\\neval_steps : {}\"\n            \"\\nlr_decay : {}\"\n            \"\\nlr_decay_steps : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\neval_steps : {}\"\n            \"\\nseed : {}\"\n            \"\\ndevice : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nweight_decay : {}\".format(\n                layers,\n                lr,\n                max_steps,\n                batch_size,\n                early_stop_rounds,\n                eval_steps,\n                lr_decay,\n                lr_decay_steps,\n                optimizer,\n                loss,\n                eval_steps,\n                seed,\n                self.device,\n                self.use_gpu,\n                weight_decay,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        if loss not in {\"mse\", \"binary\"}:\n            raise NotImplementedError(\"loss {} is not supported!\".format(loss))\n        self._scorer = mean_squared_error if loss == \"mse\" else roc_auc_score\n\n        self.dnn_model = Net(input_dim, output_dim, layers, loss=self.loss_type)\n        self.logger.info(\"model:\\n{:}\".format(self.dnn_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.dnn_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.dnn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.dnn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        # Reduce learning rate when loss has stopped decrease\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.train_optimizer,\n            mode=\"min\",\n            factor=0.5,\n            patience=10,\n            verbose=True,\n            threshold=0.0001,\n            threshold_mode=\"rel\",\n            cooldown=0,\n            min_lr=0.00001,\n            eps=1e-08,\n        )\n\n        self.fitted = False\n        self.dnn_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        verbose=True,\n        save_path=None,\n    ):\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"], col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n        try:\n            wdf_train, wdf_valid = dataset.prepare([\"train\", \"valid\"], col_set=[\"weight\"], data_key=DataHandlerLP.DK_L)\n            w_train, w_valid = wdf_train[\"weight\"], wdf_valid[\"weight\"]\n        except KeyError as e:\n            w_train = pd.DataFrame(np.ones_like(y_train.values), index=y_train.index)\n            w_valid = pd.DataFrame(np.ones_like(y_valid.values), index=y_valid.index)\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_loss = np.inf\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n        # return\n        # prepare training data\n        x_train_values = torch.from_numpy(x_train.values).float()\n        y_train_values = torch.from_numpy(y_train.values).float()\n        w_train_values = torch.from_numpy(w_train.values).float()\n        train_num = y_train_values.shape[0]\n        # prepare validation data\n        x_val_auto = torch.from_numpy(x_valid.values).float().to(self.device)\n        y_val_auto = torch.from_numpy(y_valid.values).float().to(self.device)\n        w_val_auto = torch.from_numpy(w_valid.values).float().to(self.device)\n\n        for step in range(self.max_steps):\n            if stop_steps >= self.early_stop_rounds:\n                if verbose:\n                    self.logger.info(\"\\tearly stop\")\n                break\n            loss = AverageMeter()\n            self.dnn_model.train()\n            self.train_optimizer.zero_grad()\n            choice = np.random.choice(train_num, self.batch_size)\n            x_batch_auto = x_train_values[choice].to(self.device)\n            y_batch_auto = y_train_values[choice].to(self.device)\n            w_batch_auto = w_train_values[choice].to(self.device)\n\n            # forward\n            preds = self.dnn_model(x_batch_auto)\n            cur_loss = self.get_loss(preds, w_batch_auto, y_batch_auto, self.loss_type)\n            cur_loss.backward()\n            self.train_optimizer.step()\n            loss.update(cur_loss.item())\n            R.log_metrics(train_loss=loss.avg, step=step)\n\n            # validation\n            train_loss += loss.val\n            # for evert `eval_steps` steps or at the last steps, we will evaluate the model.\n            if step % self.eval_steps == 0 or step + 1 == self.max_steps:\n                stop_steps += 1\n                train_loss /= self.eval_steps\n\n                with torch.no_grad():\n                    self.dnn_model.eval()\n                    loss_val = AverageMeter()\n\n                    # forward\n                    preds = self.dnn_model(x_val_auto)\n                    cur_loss_val = self.get_loss(preds, w_val_auto, y_val_auto, self.loss_type)\n                    loss_val.update(cur_loss_val.item())\n                R.log_metrics(val_loss=loss_val.val, step=step)\n                if verbose:\n                    self.logger.info(\n                        \"[Epoch {}]: train_loss {:.6f}, valid_loss {:.6f}\".format(step, train_loss, loss_val.val)\n                    )\n                evals_result[\"train\"].append(train_loss)\n                evals_result[\"valid\"].append(loss_val.val)\n                if loss_val.val < best_loss:\n                    if verbose:\n                        self.logger.info(\n                            \"\\tvalid loss update from {:.6f} to {:.6f}, save checkpoint.\".format(\n                                best_loss, loss_val.val\n                            )\n                        )\n                    best_loss = loss_val.val\n                    stop_steps = 0\n                    torch.save(self.dnn_model.state_dict(), save_path)\n                train_loss = 0\n                # update learning rate\n                self.scheduler.step(cur_loss_val)\n\n        # restore the optimal parameters after training\n        self.dnn_model.load_state_dict(torch.load(save_path))\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def get_loss(self, pred, w, target, loss_type):\n        if loss_type == \"mse\":\n            sqr_loss = torch.mul(pred - target, pred - target)\n            loss = torch.mul(sqr_loss, w).mean()\n            return loss\n        elif loss_type == \"binary\":\n            loss = nn.BCELoss(weight=w)\n            return loss(pred, target)\n        else:\n            raise NotImplementedError(\"loss {} is not supported!\".format(loss_type))",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test_pd = dataset.prepare(\"test\", col_set=\"feature\")\n        x_test = torch.from_numpy(x_test_pd.values).float().to(self.device)\n        self.dnn_model.eval()\n\n        with torch.no_grad():\n            preds = self.dnn_model(x_test).detach().cpu().numpy()\n        return pd.Series(np.squeeze(preds), index=x_test_pd.index)",
  "def save(self, filename, **kwargs):\n        with save_multiple_parts_file(filename) as model_dir:\n            model_path = os.path.join(model_dir, os.path.split(model_dir)[-1])\n            # Save model\n            torch.save(self.dnn_model.state_dict(), model_path)",
  "def load(self, buffer, **kwargs):\n        with unpack_archive_with_buffer(buffer) as model_dir:\n            # Get model name\n            _model_name = os.path.splitext(list(filter(lambda x: x.startswith(\"model.bin\"), os.listdir(model_dir)))[0])[\n                0\n            ]\n            _model_path = os.path.join(model_dir, _model_name)\n            # Load model\n            self.dnn_model.load_state_dict(torch.load(_model_path))\n        self._fitted = True",
  "def __init__(self):\n        self.reset()",
  "def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0",
  "def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count",
  "def __init__(self, input_dim, output_dim, layers=(256, 512, 768, 512, 256, 128, 64), loss=\"mse\"):\n        super(Net, self).__init__()\n        layers = [input_dim] + list(layers)\n        dnn_layers = []\n        drop_input = nn.Dropout(0.05)\n        dnn_layers.append(drop_input)\n        for i, (input_dim, hidden_units) in enumerate(zip(layers[:-1], layers[1:])):\n            fc = nn.Linear(input_dim, hidden_units)\n            activation = nn.LeakyReLU(negative_slope=0.1, inplace=False)\n            bn = nn.BatchNorm1d(hidden_units)\n            seq = nn.Sequential(fc, bn, activation)\n            dnn_layers.append(seq)\n        drop_input = nn.Dropout(0.05)\n        dnn_layers.append(drop_input)\n        if loss == \"mse\":\n            fc = nn.Linear(hidden_units, output_dim)\n            dnn_layers.append(fc)\n\n        elif loss == \"binary\":\n            fc = nn.Linear(hidden_units, output_dim)\n            sigmoid = nn.Sigmoid()\n            dnn_layers.append(nn.Sequential(fc, sigmoid))\n        else:\n            raise NotImplementedError(\"loss {} is not supported!\".format(loss))\n        # optimizer\n        self.dnn_layers = nn.ModuleList(dnn_layers)\n        self._weight_init()",
  "def _weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, a=0.1, mode=\"fan_in\", nonlinearity=\"leaky_relu\")",
  "def forward(self, x):\n        cur_output = x\n        for i, now_layer in enumerate(self.dnn_layers):\n            cur_output = now_layer(cur_output)\n        return cur_output",
  "class LSTM(Model):\n    \"\"\"LSTM Model\n\n    Parameters\n    ----------\n    d_feat : int\n        input dimension for each time step\n    metric: str\n        the evaluate metric used in early stop\n    optimizer : str\n        optimizer name\n    GPU : str\n        the GPU ID(s) used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"LSTM\")\n        self.logger.info(\"LSTM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n\n        self.logger.info(\n            \"LSTM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\nvisible_GPU : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                GPU,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.lstm_model = LSTMModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.lstm_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.lstm_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.lstm_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n\n        self.lstm_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.lstm_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.lstm_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.lstm_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.lstm_model(feature)\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid, df_test = dataset.prepare(\n            [\"train\", \"valid\", \"test\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.lstm_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.lstm_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.lstm_model.eval()\n        x_values = x_test.values\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.lstm_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "class LSTMModel(nn.Module):\n    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n        super().__init__()\n\n        self.rnn = nn.LSTM(\n            input_size=d_feat,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n        )\n        self.fc_out = nn.Linear(hidden_size, 1)\n\n        self.d_feat = d_feat\n\n    def forward(self, x):\n        # x: [N, F*T]\n        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n        x = x.permute(0, 2, 1)  # [N, T, F]\n        out, _ = self.rnn(x)\n        return self.fc_out(out[:, -1, :]).squeeze()",
  "def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"LSTM\")\n        self.logger.info(\"LSTM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.seed = seed\n\n        self.logger.info(\n            \"LSTM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\nvisible_GPU : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                GPU,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.lstm_model = LSTMModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.lstm_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.lstm_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.lstm_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def train_epoch(self, x_train, y_train):\n\n        x_train_values = x_train.values\n        y_train_values = np.squeeze(y_train.values)\n\n        self.lstm_model.train()\n\n        indices = np.arange(len(x_train_values))\n        np.random.shuffle(indices)\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.lstm_model(feature)\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.lstm_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def test_epoch(self, data_x, data_y):\n\n        # prepare training data\n        x_values = data_x.values\n        y_values = np.squeeze(data_y.values)\n\n        self.lstm_model.eval()\n\n        scores = []\n        losses = []\n\n        indices = np.arange(len(x_values))\n\n        for i in range(len(indices))[:: self.batch_size]:\n\n            if len(indices) - i < self.batch_size:\n                break\n\n            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)\n            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)\n\n            pred = self.lstm_model(feature)\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def fit(\n        self,\n        dataset: DatasetH,\n        evals_result=dict(),\n        save_path=None,\n    ):\n\n        df_train, df_valid, df_test = dataset.prepare(\n            [\"train\", \"valid\", \"test\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        save_path = get_or_create_path(save_path)\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(x_train, y_train)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(x_train, y_train)\n            val_loss, val_score = self.test_epoch(x_valid, y_valid)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.lstm_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.lstm_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        index = x_test.index\n        self.lstm_model.eval()\n        x_values = x_test.values\n        sample_num = x_values.shape[0]\n        preds = []\n\n        for begin in range(sample_num)[:: self.batch_size]:\n\n            if sample_num - begin < self.batch_size:\n                end = sample_num\n            else:\n                end = begin + self.batch_size\n\n            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)\n\n            with torch.no_grad():\n                pred = self.lstm_model(x_batch).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=index)",
  "def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n        super().__init__()\n\n        self.rnn = nn.LSTM(\n            input_size=d_feat,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n        )\n        self.fc_out = nn.Linear(hidden_size, 1)\n\n        self.d_feat = d_feat",
  "def forward(self, x):\n        # x: [N, F*T]\n        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n        x = x.permute(0, 2, 1)  # [N, T, F]\n        out, _ = self.rnn(x)\n        return self.fc_out(out[:, -1, :]).squeeze()",
  "class LSTM(Model):\n    \"\"\"LSTM Model\n\n    Parameters\n    ----------\n    d_feat : int\n        input dimension for each time step\n    metric: str\n        the evaluate metric used in early stop\n    optimizer : str\n        optimizer name\n    GPU : str\n        the GPU ID(s) used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        n_jobs=10,\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"LSTM\")\n        self.logger.info(\"LSTM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.n_jobs = n_jobs\n        self.seed = seed\n\n        self.logger.info(\n            \"LSTM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nn_jobs : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                self.device,\n                n_jobs,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.LSTM_model = LSTMModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        ).to(self.device)\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.LSTM_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.LSTM_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.LSTM_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def train_epoch(self, data_loader):\n\n        self.LSTM_model.train()\n\n        for data in data_loader:\n            feature = data[:, :, 0:-1].to(self.device)\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.LSTM_model(feature.float())\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.LSTM_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def test_epoch(self, data_loader):\n\n        self.LSTM_model.eval()\n\n        scores = []\n        losses = []\n\n        for data in data_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n            # feature[torch.isnan(feature)] = 0\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.LSTM_model(feature.float())\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def fit(\n        self,\n        dataset,\n        evals_result=dict(),\n        save_path=None,\n    ):\n        dl_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        dl_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n\n        dl_train.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n        dl_valid.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n\n        train_loader = DataLoader(\n            dl_train, batch_size=self.batch_size, shuffle=True, num_workers=self.n_jobs, drop_last=True\n        )\n        valid_loader = DataLoader(\n            dl_valid, batch_size=self.batch_size, shuffle=False, num_workers=self.n_jobs, drop_last=True\n        )\n\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(train_loader)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(train_loader)\n            val_loss, val_score = self.test_epoch(valid_loader)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.LSTM_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.LSTM_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        dl_test = dataset.prepare(\"test\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_I)\n        dl_test.config(fillna_type=\"ffill+bfill\")\n        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)\n        self.LSTM_model.eval()\n        preds = []\n\n        for data in test_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.LSTM_model(feature.float()).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=dl_test.get_index())",
  "class LSTMModel(nn.Module):\n    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n        super().__init__()\n\n        self.rnn = nn.LSTM(\n            input_size=d_feat,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n        )\n        self.fc_out = nn.Linear(hidden_size, 1)\n\n        self.d_feat = d_feat\n\n    def forward(self, x):\n        out, _ = self.rnn(x)\n        return self.fc_out(out[:, -1, :]).squeeze()",
  "def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        n_jobs=10,\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"LSTM\")\n        self.logger.info(\"LSTM pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.n_jobs = n_jobs\n        self.seed = seed\n\n        self.logger.info(\n            \"LSTM parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nn_jobs : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                self.device,\n                n_jobs,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.LSTM_model = LSTMModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        ).to(self.device)\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.LSTM_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.LSTM_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.LSTM_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def train_epoch(self, data_loader):\n\n        self.LSTM_model.train()\n\n        for data in data_loader:\n            feature = data[:, :, 0:-1].to(self.device)\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.LSTM_model(feature.float())\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.LSTM_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def test_epoch(self, data_loader):\n\n        self.LSTM_model.eval()\n\n        scores = []\n        losses = []\n\n        for data in data_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n            # feature[torch.isnan(feature)] = 0\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.LSTM_model(feature.float())\n            loss = self.loss_fn(pred, label)\n            losses.append(loss.item())\n\n            score = self.metric_fn(pred, label)\n            scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def fit(\n        self,\n        dataset,\n        evals_result=dict(),\n        save_path=None,\n    ):\n        dl_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        dl_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n\n        dl_train.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n        dl_valid.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n\n        train_loader = DataLoader(\n            dl_train, batch_size=self.batch_size, shuffle=True, num_workers=self.n_jobs, drop_last=True\n        )\n        valid_loader = DataLoader(\n            dl_valid, batch_size=self.batch_size, shuffle=False, num_workers=self.n_jobs, drop_last=True\n        )\n\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(train_loader)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(train_loader)\n            val_loss, val_score = self.test_epoch(valid_loader)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.LSTM_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.LSTM_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        dl_test = dataset.prepare(\"test\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_I)\n        dl_test.config(fillna_type=\"ffill+bfill\")\n        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)\n        self.LSTM_model.eval()\n        preds = []\n\n        for data in test_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.LSTM_model(feature.float()).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=dl_test.get_index())",
  "def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n        super().__init__()\n\n        self.rnn = nn.LSTM(\n            input_size=d_feat,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n        )\n        self.fc_out = nn.Linear(hidden_size, 1)\n\n        self.d_feat = d_feat",
  "def forward(self, x):\n        out, _ = self.rnn(x)\n        return self.fc_out(out[:, -1, :]).squeeze()",
  "class DEnsembleModel(Model):\n    \"\"\"Double Ensemble Model\"\"\"\n\n    def __init__(\n        self,\n        base_model=\"gbm\",\n        loss=\"mse\",\n        num_models=6,\n        enable_sr=True,\n        enable_fs=True,\n        alpha1=1.0,\n        alpha2=1.0,\n        bins_sr=10,\n        bins_fs=5,\n        decay=None,\n        sample_ratios=None,\n        sub_weights=None,\n        epochs=100,\n        **kwargs\n    ):\n        self.base_model = base_model  # \"gbm\" or \"mlp\", specifically, we use lgbm for \"gbm\"\n        self.num_models = num_models  # the number of sub-models\n        self.enable_sr = enable_sr\n        self.enable_fs = enable_fs\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.bins_sr = bins_sr\n        self.bins_fs = bins_fs\n        self.decay = decay\n        if not len(sample_ratios) == bins_fs:\n            raise ValueError(\"The length of sample_ratios should be equal to bins_fs.\")\n        self.sample_ratios = sample_ratios\n        if not len(sub_weights) == num_models:\n            raise ValueError(\"The length of sub_weights should be equal to num_models.\")\n        self.sub_weights = sub_weights\n        self.epochs = epochs\n        self.logger = get_module_logger(\"DEnsembleModel\")\n        self.logger.info(\"Double Ensemble Model...\")\n        self.ensemble = []  # the current ensemble model, a list contains all the sub-models\n        self.sub_features = []  # the features for each sub model in the form of pandas.Index\n        self.params = {\"objective\": loss}\n        self.params.update(kwargs)\n        self.loss = loss\n\n    def fit(self, dataset: DatasetH):\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"], col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        # initialize the sample weights\n        N, F = x_train.shape\n        weights = pd.Series(np.ones(N, dtype=float))\n        # initialize the features\n        features = x_train.columns\n        pred_sub = pd.DataFrame(np.zeros((N, self.num_models), dtype=float), index=x_train.index)\n        # train sub-models\n        for k in range(self.num_models):\n            self.sub_features.append(features)\n            self.logger.info(\"Training sub-model: ({}/{})\".format(k + 1, self.num_models))\n            model_k = self.train_submodel(df_train, df_valid, weights, features)\n            self.ensemble.append(model_k)\n            # no further sample re-weight and feature selection needed for the last sub-model\n            if k + 1 == self.num_models:\n                break\n\n            self.logger.info(\"Retrieving loss curve and loss values...\")\n            loss_curve = self.retrieve_loss_curve(model_k, df_train, features)\n            pred_k = self.predict_sub(model_k, df_train, features)\n            pred_sub.iloc[:, k] = pred_k\n            pred_ensemble = pred_sub.iloc[:, : k + 1].mean(axis=1)\n            loss_values = pd.Series(self.get_loss(y_train.values.squeeze(), pred_ensemble.values))\n\n            if self.enable_sr:\n                self.logger.info(\"Sample re-weighting...\")\n                weights = self.sample_reweight(loss_curve, loss_values, k + 1)\n\n            if self.enable_fs:\n                self.logger.info(\"Feature selection...\")\n                features = self.feature_selection(df_train, loss_values)\n\n    def train_submodel(self, df_train, df_valid, weights, features):\n        dtrain, dvalid = self._prepare_data_gbm(df_train, df_valid, weights, features)\n        evals_result = dict()\n        model = lgb.train(\n            self.params,\n            dtrain,\n            num_boost_round=self.epochs,\n            valid_sets=[dtrain, dvalid],\n            valid_names=[\"train\", \"valid\"],\n            verbose_eval=20,\n            evals_result=evals_result,\n        )\n        evals_result[\"train\"] = list(evals_result[\"train\"].values())[0]\n        evals_result[\"valid\"] = list(evals_result[\"valid\"].values())[0]\n        return model\n\n    def _prepare_data_gbm(self, df_train, df_valid, weights, features):\n        x_train, y_train = df_train[\"feature\"].loc[:, features], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"].loc[:, features], df_valid[\"label\"]\n\n        # Lightgbm need 1D array as its label\n        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n            y_train, y_valid = np.squeeze(y_train.values), np.squeeze(y_valid.values)\n        else:\n            raise ValueError(\"LightGBM doesn't support multi-label training\")\n\n        dtrain = lgb.Dataset(x_train.values, label=y_train, weight=weights)\n        dvalid = lgb.Dataset(x_valid.values, label=y_valid)\n        return dtrain, dvalid\n\n    def sample_reweight(self, loss_curve, loss_values, k_th):\n        \"\"\"\n        the SR module of Double Ensemble\n        :param loss_curve: the shape is NxT\n        the loss curve for the previous sub-model, where the element (i, t) if the error on the i-th sample\n        after the t-th iteration in the training of the previous sub-model.\n        :param loss_values: the shape is N\n        the loss of the current ensemble on the i-th sample.\n        :param k_th: the index of the current sub-model, starting from 1\n        :return: weights\n        the weights for all the samples.\n        \"\"\"\n        # normalize loss_curve and loss_values with ranking\n        loss_curve_norm = loss_curve.rank(axis=0, pct=True)\n        loss_values_norm = (-loss_values).rank(pct=True)\n\n        # calculate l_start and l_end from loss_curve\n        N, T = loss_curve.shape\n        part = np.maximum(int(T * 0.1), 1)\n        l_start = loss_curve_norm.iloc[:, :part].mean(axis=1)\n        l_end = loss_curve_norm.iloc[:, -part:].mean(axis=1)\n\n        # calculate h-value for each sample\n        h1 = loss_values_norm\n        h2 = (l_end / l_start).rank(pct=True)\n        h = pd.DataFrame({\"h_value\": self.alpha1 * h1 + self.alpha2 * h2})\n\n        # calculate weights\n        h[\"bins\"] = pd.cut(h[\"h_value\"], self.bins_sr)\n        h_avg = h.groupby(\"bins\")[\"h_value\"].mean()\n        weights = pd.Series(np.zeros(N, dtype=float))\n        for i_b, b in enumerate(h_avg.index):\n            weights[h[\"bins\"] == b] = 1.0 / (self.decay ** k_th * h_avg[i_b] + 0.1)\n        return weights\n\n    def feature_selection(self, df_train, loss_values):\n        \"\"\"\n        the FS module of Double Ensemble\n        :param df_train: the shape is NxF\n        :param loss_values: the shape is N\n        the loss of the current ensemble on the i-th sample.\n        :return: res_feat: in the form of pandas.Index\n\n        \"\"\"\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        features = x_train.columns\n        N, F = x_train.shape\n        g = pd.DataFrame({\"g_value\": np.zeros(F, dtype=float)})\n        M = len(self.ensemble)\n\n        # shuffle specific columns and calculate g-value for each feature\n        x_train_tmp = x_train.copy()\n        for i_f, feat in enumerate(features):\n            x_train_tmp.loc[:, feat] = np.random.permutation(x_train_tmp.loc[:, feat].values)\n            pred = pd.Series(np.zeros(N), index=x_train_tmp.index)\n            for i_s, submodel in enumerate(self.ensemble):\n                pred += (\n                    pd.Series(\n                        submodel.predict(x_train_tmp.loc[:, self.sub_features[i_s]].values), index=x_train_tmp.index\n                    )\n                    / M\n                )\n            loss_feat = self.get_loss(y_train.values.squeeze(), pred.values)\n            g.loc[i_f, \"g_value\"] = np.mean(loss_feat - loss_values) / (np.std(loss_feat - loss_values) + 1e-7)\n            x_train_tmp.loc[:, feat] = x_train.loc[:, feat].copy()\n\n        # one column in train features is all-nan # if g['g_value'].isna().any()\n        g[\"g_value\"].replace(np.nan, 0, inplace=True)\n\n        # divide features into bins_fs bins\n        g[\"bins\"] = pd.cut(g[\"g_value\"], self.bins_fs)\n\n        # randomly sample features from bins to construct the new features\n        res_feat = []\n        sorted_bins = sorted(g[\"bins\"].unique(), reverse=True)\n        for i_b, b in enumerate(sorted_bins):\n            b_feat = features[g[\"bins\"] == b]\n            num_feat = int(np.ceil(self.sample_ratios[i_b] * len(b_feat)))\n            res_feat = res_feat + np.random.choice(b_feat, size=num_feat).tolist()\n        return pd.Index(res_feat)\n\n    def get_loss(self, label, pred):\n        if self.loss == \"mse\":\n            return (label - pred) ** 2\n        else:\n            raise ValueError(\"not implemented yet\")\n\n    def retrieve_loss_curve(self, model, df_train, features):\n        if self.base_model == \"gbm\":\n            num_trees = model.num_trees()\n            x_train, y_train = df_train[\"feature\"].loc[:, features], df_train[\"label\"]\n            # Lightgbm need 1D array as its label\n            if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n                y_train = np.squeeze(y_train.values)\n            else:\n                raise ValueError(\"LightGBM doesn't support multi-label training\")\n\n            N = x_train.shape[0]\n            loss_curve = pd.DataFrame(np.zeros((N, num_trees)))\n            pred_tree = np.zeros(N, dtype=float)\n            for i_tree in range(num_trees):\n                pred_tree += model.predict(x_train.values, start_iteration=i_tree, num_iteration=1)\n                loss_curve.iloc[:, i_tree] = self.get_loss(y_train, pred_tree)\n        else:\n            raise ValueError(\"not implemented yet\")\n        return loss_curve\n\n    def predict(self, dataset):\n        if self.ensemble is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\", data_key=DataHandlerLP.DK_I)\n        pred = pd.Series(np.zeros(x_test.shape[0]), index=x_test.index)\n        for i_sub, submodel in enumerate(self.ensemble):\n            feat_sub = self.sub_features[i_sub]\n            pred += (\n                pd.Series(submodel.predict(x_test.loc[:, feat_sub].values), index=x_test.index)\n                * self.sub_weights[i_sub]\n            )\n        return pred\n\n    def predict_sub(self, submodel, df_data, features):\n        x_data, y_data = df_data[\"feature\"].loc[:, features], df_data[\"label\"]\n        pred_sub = pd.Series(submodel.predict(x_data.values), index=x_data.index)\n        return pred_sub",
  "def __init__(\n        self,\n        base_model=\"gbm\",\n        loss=\"mse\",\n        num_models=6,\n        enable_sr=True,\n        enable_fs=True,\n        alpha1=1.0,\n        alpha2=1.0,\n        bins_sr=10,\n        bins_fs=5,\n        decay=None,\n        sample_ratios=None,\n        sub_weights=None,\n        epochs=100,\n        **kwargs\n    ):\n        self.base_model = base_model  # \"gbm\" or \"mlp\", specifically, we use lgbm for \"gbm\"\n        self.num_models = num_models  # the number of sub-models\n        self.enable_sr = enable_sr\n        self.enable_fs = enable_fs\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.bins_sr = bins_sr\n        self.bins_fs = bins_fs\n        self.decay = decay\n        if not len(sample_ratios) == bins_fs:\n            raise ValueError(\"The length of sample_ratios should be equal to bins_fs.\")\n        self.sample_ratios = sample_ratios\n        if not len(sub_weights) == num_models:\n            raise ValueError(\"The length of sub_weights should be equal to num_models.\")\n        self.sub_weights = sub_weights\n        self.epochs = epochs\n        self.logger = get_module_logger(\"DEnsembleModel\")\n        self.logger.info(\"Double Ensemble Model...\")\n        self.ensemble = []  # the current ensemble model, a list contains all the sub-models\n        self.sub_features = []  # the features for each sub model in the form of pandas.Index\n        self.params = {\"objective\": loss}\n        self.params.update(kwargs)\n        self.loss = loss",
  "def fit(self, dataset: DatasetH):\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"], col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        # initialize the sample weights\n        N, F = x_train.shape\n        weights = pd.Series(np.ones(N, dtype=float))\n        # initialize the features\n        features = x_train.columns\n        pred_sub = pd.DataFrame(np.zeros((N, self.num_models), dtype=float), index=x_train.index)\n        # train sub-models\n        for k in range(self.num_models):\n            self.sub_features.append(features)\n            self.logger.info(\"Training sub-model: ({}/{})\".format(k + 1, self.num_models))\n            model_k = self.train_submodel(df_train, df_valid, weights, features)\n            self.ensemble.append(model_k)\n            # no further sample re-weight and feature selection needed for the last sub-model\n            if k + 1 == self.num_models:\n                break\n\n            self.logger.info(\"Retrieving loss curve and loss values...\")\n            loss_curve = self.retrieve_loss_curve(model_k, df_train, features)\n            pred_k = self.predict_sub(model_k, df_train, features)\n            pred_sub.iloc[:, k] = pred_k\n            pred_ensemble = pred_sub.iloc[:, : k + 1].mean(axis=1)\n            loss_values = pd.Series(self.get_loss(y_train.values.squeeze(), pred_ensemble.values))\n\n            if self.enable_sr:\n                self.logger.info(\"Sample re-weighting...\")\n                weights = self.sample_reweight(loss_curve, loss_values, k + 1)\n\n            if self.enable_fs:\n                self.logger.info(\"Feature selection...\")\n                features = self.feature_selection(df_train, loss_values)",
  "def train_submodel(self, df_train, df_valid, weights, features):\n        dtrain, dvalid = self._prepare_data_gbm(df_train, df_valid, weights, features)\n        evals_result = dict()\n        model = lgb.train(\n            self.params,\n            dtrain,\n            num_boost_round=self.epochs,\n            valid_sets=[dtrain, dvalid],\n            valid_names=[\"train\", \"valid\"],\n            verbose_eval=20,\n            evals_result=evals_result,\n        )\n        evals_result[\"train\"] = list(evals_result[\"train\"].values())[0]\n        evals_result[\"valid\"] = list(evals_result[\"valid\"].values())[0]\n        return model",
  "def _prepare_data_gbm(self, df_train, df_valid, weights, features):\n        x_train, y_train = df_train[\"feature\"].loc[:, features], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"].loc[:, features], df_valid[\"label\"]\n\n        # Lightgbm need 1D array as its label\n        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n            y_train, y_valid = np.squeeze(y_train.values), np.squeeze(y_valid.values)\n        else:\n            raise ValueError(\"LightGBM doesn't support multi-label training\")\n\n        dtrain = lgb.Dataset(x_train.values, label=y_train, weight=weights)\n        dvalid = lgb.Dataset(x_valid.values, label=y_valid)\n        return dtrain, dvalid",
  "def sample_reweight(self, loss_curve, loss_values, k_th):\n        \"\"\"\n        the SR module of Double Ensemble\n        :param loss_curve: the shape is NxT\n        the loss curve for the previous sub-model, where the element (i, t) if the error on the i-th sample\n        after the t-th iteration in the training of the previous sub-model.\n        :param loss_values: the shape is N\n        the loss of the current ensemble on the i-th sample.\n        :param k_th: the index of the current sub-model, starting from 1\n        :return: weights\n        the weights for all the samples.\n        \"\"\"\n        # normalize loss_curve and loss_values with ranking\n        loss_curve_norm = loss_curve.rank(axis=0, pct=True)\n        loss_values_norm = (-loss_values).rank(pct=True)\n\n        # calculate l_start and l_end from loss_curve\n        N, T = loss_curve.shape\n        part = np.maximum(int(T * 0.1), 1)\n        l_start = loss_curve_norm.iloc[:, :part].mean(axis=1)\n        l_end = loss_curve_norm.iloc[:, -part:].mean(axis=1)\n\n        # calculate h-value for each sample\n        h1 = loss_values_norm\n        h2 = (l_end / l_start).rank(pct=True)\n        h = pd.DataFrame({\"h_value\": self.alpha1 * h1 + self.alpha2 * h2})\n\n        # calculate weights\n        h[\"bins\"] = pd.cut(h[\"h_value\"], self.bins_sr)\n        h_avg = h.groupby(\"bins\")[\"h_value\"].mean()\n        weights = pd.Series(np.zeros(N, dtype=float))\n        for i_b, b in enumerate(h_avg.index):\n            weights[h[\"bins\"] == b] = 1.0 / (self.decay ** k_th * h_avg[i_b] + 0.1)\n        return weights",
  "def feature_selection(self, df_train, loss_values):\n        \"\"\"\n        the FS module of Double Ensemble\n        :param df_train: the shape is NxF\n        :param loss_values: the shape is N\n        the loss of the current ensemble on the i-th sample.\n        :return: res_feat: in the form of pandas.Index\n\n        \"\"\"\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        features = x_train.columns\n        N, F = x_train.shape\n        g = pd.DataFrame({\"g_value\": np.zeros(F, dtype=float)})\n        M = len(self.ensemble)\n\n        # shuffle specific columns and calculate g-value for each feature\n        x_train_tmp = x_train.copy()\n        for i_f, feat in enumerate(features):\n            x_train_tmp.loc[:, feat] = np.random.permutation(x_train_tmp.loc[:, feat].values)\n            pred = pd.Series(np.zeros(N), index=x_train_tmp.index)\n            for i_s, submodel in enumerate(self.ensemble):\n                pred += (\n                    pd.Series(\n                        submodel.predict(x_train_tmp.loc[:, self.sub_features[i_s]].values), index=x_train_tmp.index\n                    )\n                    / M\n                )\n            loss_feat = self.get_loss(y_train.values.squeeze(), pred.values)\n            g.loc[i_f, \"g_value\"] = np.mean(loss_feat - loss_values) / (np.std(loss_feat - loss_values) + 1e-7)\n            x_train_tmp.loc[:, feat] = x_train.loc[:, feat].copy()\n\n        # one column in train features is all-nan # if g['g_value'].isna().any()\n        g[\"g_value\"].replace(np.nan, 0, inplace=True)\n\n        # divide features into bins_fs bins\n        g[\"bins\"] = pd.cut(g[\"g_value\"], self.bins_fs)\n\n        # randomly sample features from bins to construct the new features\n        res_feat = []\n        sorted_bins = sorted(g[\"bins\"].unique(), reverse=True)\n        for i_b, b in enumerate(sorted_bins):\n            b_feat = features[g[\"bins\"] == b]\n            num_feat = int(np.ceil(self.sample_ratios[i_b] * len(b_feat)))\n            res_feat = res_feat + np.random.choice(b_feat, size=num_feat).tolist()\n        return pd.Index(res_feat)",
  "def get_loss(self, label, pred):\n        if self.loss == \"mse\":\n            return (label - pred) ** 2\n        else:\n            raise ValueError(\"not implemented yet\")",
  "def retrieve_loss_curve(self, model, df_train, features):\n        if self.base_model == \"gbm\":\n            num_trees = model.num_trees()\n            x_train, y_train = df_train[\"feature\"].loc[:, features], df_train[\"label\"]\n            # Lightgbm need 1D array as its label\n            if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n                y_train = np.squeeze(y_train.values)\n            else:\n                raise ValueError(\"LightGBM doesn't support multi-label training\")\n\n            N = x_train.shape[0]\n            loss_curve = pd.DataFrame(np.zeros((N, num_trees)))\n            pred_tree = np.zeros(N, dtype=float)\n            for i_tree in range(num_trees):\n                pred_tree += model.predict(x_train.values, start_iteration=i_tree, num_iteration=1)\n                loss_curve.iloc[:, i_tree] = self.get_loss(y_train, pred_tree)\n        else:\n            raise ValueError(\"not implemented yet\")\n        return loss_curve",
  "def predict(self, dataset):\n        if self.ensemble is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\", data_key=DataHandlerLP.DK_I)\n        pred = pd.Series(np.zeros(x_test.shape[0]), index=x_test.index)\n        for i_sub, submodel in enumerate(self.ensemble):\n            feat_sub = self.sub_features[i_sub]\n            pred += (\n                pd.Series(submodel.predict(x_test.loc[:, feat_sub].values), index=x_test.index)\n                * self.sub_weights[i_sub]\n            )\n        return pred",
  "def predict_sub(self, submodel, df_data, features):\n        x_data, y_data = df_data[\"feature\"].loc[:, features], df_data[\"label\"]\n        pred_sub = pd.Series(submodel.predict(x_data.values), index=x_data.index)\n        return pred_sub",
  "class LinearModel(Model):\n    \"\"\"Linear Model\n\n    Solve one of the following regression problems:\n        - `ols`: min_w |y - Xw|^2_2\n        - `nnls`: min_w |y - Xw|^2_2, s.t. w >= 0\n        - `ridge`: min_w |y - Xw|^2_2 + \\alpha*|w|^2_2\n        - `lasso`: min_w |y - Xw|^2_2 + \\alpha*|w|_1\n    where `w` is the regression coefficient.\n    \"\"\"\n\n    OLS = \"ols\"\n    NNLS = \"nnls\"\n    RIDGE = \"ridge\"\n    LASSO = \"lasso\"\n\n    def __init__(self, estimator=\"ols\", alpha=0.0, fit_intercept=False):\n        \"\"\"\n        Parameters\n        ----------\n        estimator : str\n            which estimator to use for linear regression\n        alpha : float\n            l1 or l2 regularization parameter\n        fit_intercept : bool\n            whether fit intercept\n        \"\"\"\n        assert estimator in [self.OLS, self.NNLS, self.RIDGE, self.LASSO], f\"unsupported estimator `{estimator}`\"\n        self.estimator = estimator\n\n        assert alpha == 0 or estimator in [self.RIDGE, self.LASSO], f\"alpha is only supported in `ridge`&`lasso`\"\n        self.alpha = alpha\n\n        self.fit_intercept = fit_intercept\n\n        self.coef_ = None\n\n    def fit(self, dataset: DatasetH):\n        df_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        X, y = df_train[\"feature\"].values, np.squeeze(df_train[\"label\"].values)\n\n        if self.estimator in [self.OLS, self.RIDGE, self.LASSO]:\n            self._fit(X, y)\n        elif self.estimator == self.NNLS:\n            self._fit_nnls(X, y)\n        else:\n            raise ValueError(f\"unknown estimator `{self.estimator}`\")\n\n        return self\n\n    def _fit(self, X, y):\n        if self.estimator == self.OLS:\n            model = LinearRegression(fit_intercept=self.fit_intercept, copy_X=False)\n        else:\n            model = {self.RIDGE: Ridge, self.LASSO: Lasso}[self.estimator](\n                alpha=self.alpha, fit_intercept=self.fit_intercept, copy_X=False\n            )\n        model.fit(X, y)\n        self.coef_ = model.coef_\n        self.intercept_ = model.intercept_\n\n    def _fit_nnls(self, X, y):\n        if self.fit_intercept:\n            X = np.c_[X, np.ones(len(X))]  # NOTE: mem copy\n        coef = nnls(X, y)[0]\n        if self.fit_intercept:\n            self.coef_ = coef[:-1]\n            self.intercept_ = coef[-1]\n        else:\n            self.coef_ = coef\n            self.intercept_ = 0.0\n\n    def predict(self, dataset):\n        if self.coef_ is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\", data_key=DataHandlerLP.DK_I)\n        return pd.Series(x_test.values @ self.coef_ + self.intercept_, index=x_test.index)",
  "def __init__(self, estimator=\"ols\", alpha=0.0, fit_intercept=False):\n        \"\"\"\n        Parameters\n        ----------\n        estimator : str\n            which estimator to use for linear regression\n        alpha : float\n            l1 or l2 regularization parameter\n        fit_intercept : bool\n            whether fit intercept\n        \"\"\"\n        assert estimator in [self.OLS, self.NNLS, self.RIDGE, self.LASSO], f\"unsupported estimator `{estimator}`\"\n        self.estimator = estimator\n\n        assert alpha == 0 or estimator in [self.RIDGE, self.LASSO], f\"alpha is only supported in `ridge`&`lasso`\"\n        self.alpha = alpha\n\n        self.fit_intercept = fit_intercept\n\n        self.coef_ = None",
  "def fit(self, dataset: DatasetH):\n        df_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        X, y = df_train[\"feature\"].values, np.squeeze(df_train[\"label\"].values)\n\n        if self.estimator in [self.OLS, self.RIDGE, self.LASSO]:\n            self._fit(X, y)\n        elif self.estimator == self.NNLS:\n            self._fit_nnls(X, y)\n        else:\n            raise ValueError(f\"unknown estimator `{self.estimator}`\")\n\n        return self",
  "def _fit(self, X, y):\n        if self.estimator == self.OLS:\n            model = LinearRegression(fit_intercept=self.fit_intercept, copy_X=False)\n        else:\n            model = {self.RIDGE: Ridge, self.LASSO: Lasso}[self.estimator](\n                alpha=self.alpha, fit_intercept=self.fit_intercept, copy_X=False\n            )\n        model.fit(X, y)\n        self.coef_ = model.coef_\n        self.intercept_ = model.intercept_",
  "def _fit_nnls(self, X, y):\n        if self.fit_intercept:\n            X = np.c_[X, np.ones(len(X))]  # NOTE: mem copy\n        coef = nnls(X, y)[0]\n        if self.fit_intercept:\n            self.coef_ = coef[:-1]\n            self.intercept_ = coef[-1]\n        else:\n            self.coef_ = coef\n            self.intercept_ = 0.0",
  "def predict(self, dataset):\n        if self.coef_ is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\", data_key=DataHandlerLP.DK_I)\n        return pd.Series(x_test.values @ self.coef_ + self.intercept_, index=x_test.index)",
  "def count_parameters(models_or_parameters, unit=\"m\"):\n    \"\"\"\n    This function is to obtain the storage size unit of a (or multiple) models.\n\n    Parameters\n    ----------\n    models_or_parameters : PyTorch model(s) or a list of parameters.\n    unit : the storage size unit.\n\n    Returns\n    -------\n    The number of parameters of the given model(s) or parameters.\n    \"\"\"\n    if isinstance(models_or_parameters, nn.Module):\n        counts = sum(v.numel() for v in models_or_parameters.parameters())\n    elif isinstance(models_or_parameters, nn.Parameter):\n        counts = models_or_parameters.numel()\n    elif isinstance(models_or_parameters, (list, tuple)):\n        return sum(count_parameters(x, unit) for x in models_or_parameters)\n    else:\n        counts = sum(v.numel() for v in models_or_parameters)\n    unit = unit.lower()\n    if unit == \"kb\" or unit == \"k\":\n        counts /= 2 ** 10\n    elif unit == \"mb\" or unit == \"m\":\n        counts /= 2 ** 20\n    elif unit == \"gb\" or unit == \"g\":\n        counts /= 2 ** 30\n    elif unit is not None:\n        raise ValueError(\"Unknow unit: {:}\".format(unit))\n    return counts",
  "class GRU(Model):\n    \"\"\"GRU Model\n\n    Parameters\n    ----------\n    d_feat : int\n        input dimension for each time step\n    metric: str\n        the evaluate metric used in early stop\n    optimizer : str\n        optimizer name\n    GPU : str\n        the GPU ID(s) used for training\n    \"\"\"\n\n    def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        n_jobs=10,\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"GRU\")\n        self.logger.info(\"GRU pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.n_jobs = n_jobs\n        self.seed = seed\n\n        self.logger.info(\n            \"GRU parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nn_jobs : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                self.device,\n                n_jobs,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.GRU_model = GRUModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.gru_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.gru_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.GRU_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.GRU_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.GRU_model.to(self.device)\n\n    @property\n    def use_gpu(self):\n        return self.device != torch.device(\"cpu\")\n\n    def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)\n\n    def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)\n\n    def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)\n\n    def train_epoch(self, data_loader):\n\n        self.GRU_model.train()\n\n        for data in data_loader:\n            feature = data[:, :, 0:-1].to(self.device)\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.GRU_model(feature.float())\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.GRU_model.parameters(), 3.0)\n            self.train_optimizer.step()\n\n    def test_epoch(self, data_loader):\n\n        self.GRU_model.eval()\n\n        scores = []\n        losses = []\n\n        for data in data_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n            # feature[torch.isnan(feature)] = 0\n            label = data[:, -1, -1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.GRU_model(feature.float())\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)\n\n    def fit(\n        self,\n        dataset,\n        evals_result=dict(),\n        save_path=None,\n    ):\n        dl_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        dl_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n\n        dl_train.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n        dl_valid.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n\n        train_loader = DataLoader(\n            dl_train, batch_size=self.batch_size, shuffle=True, num_workers=self.n_jobs, drop_last=True\n        )\n        valid_loader = DataLoader(\n            dl_valid, batch_size=self.batch_size, shuffle=False, num_workers=self.n_jobs, drop_last=True\n        )\n\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(train_loader)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(train_loader)\n            val_loss, val_score = self.test_epoch(valid_loader)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.GRU_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.GRU_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()\n\n    def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        dl_test = dataset.prepare(\"test\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_I)\n        dl_test.config(fillna_type=\"ffill+bfill\")\n        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)\n        self.GRU_model.eval()\n        preds = []\n\n        for data in test_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.GRU_model(feature.float()).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=dl_test.get_index())",
  "class GRUModel(nn.Module):\n    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n        super().__init__()\n\n        self.rnn = nn.GRU(\n            input_size=d_feat,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n        )\n        self.fc_out = nn.Linear(hidden_size, 1)\n\n        self.d_feat = d_feat\n\n    def forward(self, x):\n        out, _ = self.rnn(x)\n        return self.fc_out(out[:, -1, :]).squeeze()",
  "def __init__(\n        self,\n        d_feat=6,\n        hidden_size=64,\n        num_layers=2,\n        dropout=0.0,\n        n_epochs=200,\n        lr=0.001,\n        metric=\"\",\n        batch_size=2000,\n        early_stop=20,\n        loss=\"mse\",\n        optimizer=\"adam\",\n        n_jobs=10,\n        GPU=0,\n        seed=None,\n        **kwargs\n    ):\n        # Set logger.\n        self.logger = get_module_logger(\"GRU\")\n        self.logger.info(\"GRU pytorch version...\")\n\n        # set hyper-parameters.\n        self.d_feat = d_feat\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.metric = metric\n        self.batch_size = batch_size\n        self.early_stop = early_stop\n        self.optimizer = optimizer.lower()\n        self.loss = loss\n        self.device = torch.device(\"cuda:%d\" % (GPU) if torch.cuda.is_available() and GPU >= 0 else \"cpu\")\n        self.n_jobs = n_jobs\n        self.seed = seed\n\n        self.logger.info(\n            \"GRU parameters setting:\"\n            \"\\nd_feat : {}\"\n            \"\\nhidden_size : {}\"\n            \"\\nnum_layers : {}\"\n            \"\\ndropout : {}\"\n            \"\\nn_epochs : {}\"\n            \"\\nlr : {}\"\n            \"\\nmetric : {}\"\n            \"\\nbatch_size : {}\"\n            \"\\nearly_stop : {}\"\n            \"\\noptimizer : {}\"\n            \"\\nloss_type : {}\"\n            \"\\ndevice : {}\"\n            \"\\nn_jobs : {}\"\n            \"\\nuse_GPU : {}\"\n            \"\\nseed : {}\".format(\n                d_feat,\n                hidden_size,\n                num_layers,\n                dropout,\n                n_epochs,\n                lr,\n                metric,\n                batch_size,\n                early_stop,\n                optimizer.lower(),\n                loss,\n                self.device,\n                n_jobs,\n                self.use_gpu,\n                seed,\n            )\n        )\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.GRU_model = GRUModel(\n            d_feat=self.d_feat,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n        )\n        self.logger.info(\"model:\\n{:}\".format(self.gru_model))\n        self.logger.info(\"model size: {:.4f} MB\".format(count_parameters(self.gru_model)))\n\n        if optimizer.lower() == \"adam\":\n            self.train_optimizer = optim.Adam(self.GRU_model.parameters(), lr=self.lr)\n        elif optimizer.lower() == \"gd\":\n            self.train_optimizer = optim.SGD(self.GRU_model.parameters(), lr=self.lr)\n        else:\n            raise NotImplementedError(\"optimizer {} is not supported!\".format(optimizer))\n\n        self.fitted = False\n        self.GRU_model.to(self.device)",
  "def use_gpu(self):\n        return self.device != torch.device(\"cpu\")",
  "def mse(self, pred, label):\n        loss = (pred - label) ** 2\n        return torch.mean(loss)",
  "def loss_fn(self, pred, label):\n        mask = ~torch.isnan(label)\n\n        if self.loss == \"mse\":\n            return self.mse(pred[mask], label[mask])\n\n        raise ValueError(\"unknown loss `%s`\" % self.loss)",
  "def metric_fn(self, pred, label):\n\n        mask = torch.isfinite(label)\n\n        if self.metric == \"\" or self.metric == \"loss\":\n            return -self.loss_fn(pred[mask], label[mask])\n\n        raise ValueError(\"unknown metric `%s`\" % self.metric)",
  "def train_epoch(self, data_loader):\n\n        self.GRU_model.train()\n\n        for data in data_loader:\n            feature = data[:, :, 0:-1].to(self.device)\n            label = data[:, -1, -1].to(self.device)\n\n            pred = self.GRU_model(feature.float())\n            loss = self.loss_fn(pred, label)\n\n            self.train_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(self.GRU_model.parameters(), 3.0)\n            self.train_optimizer.step()",
  "def test_epoch(self, data_loader):\n\n        self.GRU_model.eval()\n\n        scores = []\n        losses = []\n\n        for data in data_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n            # feature[torch.isnan(feature)] = 0\n            label = data[:, -1, -1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.GRU_model(feature.float())\n                loss = self.loss_fn(pred, label)\n                losses.append(loss.item())\n\n                score = self.metric_fn(pred, label)\n                scores.append(score.item())\n\n        return np.mean(losses), np.mean(scores)",
  "def fit(\n        self,\n        dataset,\n        evals_result=dict(),\n        save_path=None,\n    ):\n        dl_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n        dl_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L)\n\n        dl_train.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n        dl_valid.config(fillna_type=\"ffill+bfill\")  # process nan brought by dataloader\n\n        train_loader = DataLoader(\n            dl_train, batch_size=self.batch_size, shuffle=True, num_workers=self.n_jobs, drop_last=True\n        )\n        valid_loader = DataLoader(\n            dl_valid, batch_size=self.batch_size, shuffle=False, num_workers=self.n_jobs, drop_last=True\n        )\n\n        save_path = get_or_create_path(save_path)\n\n        stop_steps = 0\n        train_loss = 0\n        best_score = -np.inf\n        best_epoch = 0\n        evals_result[\"train\"] = []\n        evals_result[\"valid\"] = []\n\n        # train\n        self.logger.info(\"training...\")\n        self.fitted = True\n\n        for step in range(self.n_epochs):\n            self.logger.info(\"Epoch%d:\", step)\n            self.logger.info(\"training...\")\n            self.train_epoch(train_loader)\n            self.logger.info(\"evaluating...\")\n            train_loss, train_score = self.test_epoch(train_loader)\n            val_loss, val_score = self.test_epoch(valid_loader)\n            self.logger.info(\"train %.6f, valid %.6f\" % (train_score, val_score))\n            evals_result[\"train\"].append(train_score)\n            evals_result[\"valid\"].append(val_score)\n\n            if val_score > best_score:\n                best_score = val_score\n                stop_steps = 0\n                best_epoch = step\n                best_param = copy.deepcopy(self.GRU_model.state_dict())\n            else:\n                stop_steps += 1\n                if stop_steps >= self.early_stop:\n                    self.logger.info(\"early stop\")\n                    break\n\n        self.logger.info(\"best score: %.6lf @ %d\" % (best_score, best_epoch))\n        self.GRU_model.load_state_dict(best_param)\n        torch.save(best_param, save_path)\n\n        if self.use_gpu:\n            torch.cuda.empty_cache()",
  "def predict(self, dataset):\n        if not self.fitted:\n            raise ValueError(\"model is not fitted yet!\")\n\n        dl_test = dataset.prepare(\"test\", col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_I)\n        dl_test.config(fillna_type=\"ffill+bfill\")\n        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)\n        self.GRU_model.eval()\n        preds = []\n\n        for data in test_loader:\n\n            feature = data[:, :, 0:-1].to(self.device)\n\n            with torch.no_grad():\n                pred = self.GRU_model(feature.float()).detach().cpu().numpy()\n\n            preds.append(pred)\n\n        return pd.Series(np.concatenate(preds), index=dl_test.get_index())",
  "def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n        super().__init__()\n\n        self.rnn = nn.GRU(\n            input_size=d_feat,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n        )\n        self.fc_out = nn.Linear(hidden_size, 1)\n\n        self.d_feat = d_feat",
  "def forward(self, x):\n        out, _ = self.rnn(x)\n        return self.fc_out(out[:, -1, :]).squeeze()",
  "class XGBModel(Model):\n    \"\"\"XGBModel Model\"\"\"\n\n    def __init__(self, **kwargs):\n        self._params = {}\n        self._params.update(kwargs)\n        self.model = None\n\n    def fit(\n        self,\n        dataset: DatasetH,\n        num_boost_round=1000,\n        early_stopping_rounds=50,\n        verbose_eval=20,\n        evals_result=dict(),\n        **kwargs\n    ):\n\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        # Lightgbm need 1D array as its label\n        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n            y_train_1d, y_valid_1d = np.squeeze(y_train.values), np.squeeze(y_valid.values)\n        else:\n            raise ValueError(\"XGBoost doesn't support multi-label training\")\n\n        dtrain = xgb.DMatrix(x_train.values, label=y_train_1d)\n        dvalid = xgb.DMatrix(x_valid.values, label=y_valid_1d)\n        self.model = xgb.train(\n            self._params,\n            dtrain=dtrain,\n            num_boost_round=num_boost_round,\n            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=verbose_eval,\n            evals_result=evals_result,\n            **kwargs\n        )\n        evals_result[\"train\"] = list(evals_result[\"train\"].values())[0]\n        evals_result[\"valid\"] = list(evals_result[\"valid\"].values())[0]\n\n    def predict(self, dataset):\n        if self.model is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        return pd.Series(self.model.predict(xgb.DMatrix(x_test.values)), index=x_test.index)",
  "def __init__(self, **kwargs):\n        self._params = {}\n        self._params.update(kwargs)\n        self.model = None",
  "def fit(\n        self,\n        dataset: DatasetH,\n        num_boost_round=1000,\n        early_stopping_rounds=50,\n        verbose_eval=20,\n        evals_result=dict(),\n        **kwargs\n    ):\n\n        df_train, df_valid = dataset.prepare(\n            [\"train\", \"valid\"],\n            col_set=[\"feature\", \"label\"],\n            data_key=DataHandlerLP.DK_L,\n        )\n        x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n        x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n        # Lightgbm need 1D array as its label\n        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:\n            y_train_1d, y_valid_1d = np.squeeze(y_train.values), np.squeeze(y_valid.values)\n        else:\n            raise ValueError(\"XGBoost doesn't support multi-label training\")\n\n        dtrain = xgb.DMatrix(x_train.values, label=y_train_1d)\n        dvalid = xgb.DMatrix(x_valid.values, label=y_valid_1d)\n        self.model = xgb.train(\n            self._params,\n            dtrain=dtrain,\n            num_boost_round=num_boost_round,\n            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=verbose_eval,\n            evals_result=evals_result,\n            **kwargs\n        )\n        evals_result[\"train\"] = list(evals_result[\"train\"].values())[0]\n        evals_result[\"valid\"] = list(evals_result[\"valid\"].values())[0]",
  "def predict(self, dataset):\n        if self.model is None:\n            raise ValueError(\"model is not fitted yet!\")\n        x_test = dataset.prepare(\"test\", col_set=\"feature\")\n        return pd.Series(self.model.predict(xgb.DMatrix(x_test.values)), index=x_test.index)",
  "def calc_ic(pred: pd.Series, label: pd.Series, date_col=\"datetime\", dropna=False) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"calc_ic.\n\n    Parameters\n    ----------\n    pred :\n        pred\n    label :\n        label\n    date_col :\n        date_col\n\n    Returns\n    -------\n    (pd.Series, pd.Series)\n        ic and rank ic\n    \"\"\"\n    df = pd.DataFrame({\"pred\": pred, \"label\": label})\n    ic = df.groupby(date_col).apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n    ric = df.groupby(date_col).apply(lambda df: df[\"pred\"].corr(df[\"label\"], method=\"spearman\"))\n    if dropna:\n        return ic.dropna(), ric.dropna()\n    else:\n        return ic, ric",
  "def calc_long_short_return(\n    pred: pd.Series,\n    label: pd.Series,\n    date_col: str = \"datetime\",\n    quantile: float = 0.2,\n    dropna: bool = False,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"\n    calculate long-short return\n\n    Note:\n        `label` must be raw stock returns.\n\n    Parameters\n    ----------\n    pred : pd.Series\n        stock predictions\n    label : pd.Series\n        stock returns\n    date_col : str\n        datetime index name\n    quantile : float\n        long-short quantile\n\n    Returns\n    ----------\n    long_short_r : pd.Series\n        daily long-short returns\n    long_avg_r : pd.Series\n        daily long-average returns\n    \"\"\"\n    df = pd.DataFrame({\"pred\": pred, \"label\": label})\n    if dropna:\n        df.dropna(inplace=True)\n    group = df.groupby(level=date_col)\n    N = lambda x: int(len(x) * quantile)\n    r_long = group.apply(lambda x: x.nlargest(N(x), columns=\"pred\").label.mean())\n    r_short = group.apply(lambda x: x.nsmallest(N(x), columns=\"pred\").label.mean())\n    r_avg = group.label.mean()\n    return (r_long - r_short) / 2, r_avg",
  "class Tuner:\n    def __init__(self, tuner_config, optim_config):\n\n        self.logger = get_module_logger(\"Tuner\", sh_level=logging.INFO)\n\n        self.tuner_config = tuner_config\n        self.optim_config = optim_config\n\n        self.max_evals = self.tuner_config.get(\"max_evals\", 10)\n        self.ex_dir = os.path.join(\n            self.tuner_config[\"experiment\"][\"dir\"],\n            self.tuner_config[\"experiment\"][\"name\"],\n        )\n\n        self.best_params = None\n        self.best_res = None\n\n        self.space = self.setup_space()\n\n    def tune(self):\n\n        TimeInspector.set_time_mark()\n        fmin(\n            fn=self.objective,\n            space=self.space,\n            algo=tpe.suggest,\n            max_evals=self.max_evals,\n        )\n        self.logger.info(\"Local best params: {} \".format(self.best_params))\n        TimeInspector.log_cost_time(\n            \"Finished searching best parameters in Tuner {}.\".format(self.tuner_config[\"experiment\"][\"id\"])\n        )\n\n        self.save_local_best_params()\n\n    @abstractmethod\n    def objective(self, params):\n        \"\"\"\n        Implement this method to give an optimization factor using parameters in space.\n        :return: {'loss': a factor for optimization, float type,\n                  'status': the status of this evaluation step, STATUS_OK or STATUS_FAIL}.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def setup_space(self):\n        \"\"\"\n        Implement this method to setup the searching space of tuner.\n        :return: searching space, dict type.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_local_best_params(self):\n        \"\"\"\n        Implement this method to save the best parameters of this tuner.\n        \"\"\"\n        pass",
  "class QLibTuner(Tuner):\n\n    ESTIMATOR_CONFIG_NAME = \"estimator_config.yaml\"\n    EXP_INFO_NAME = \"exp_info.json\"\n    EXP_RESULT_DIR = \"sacred/{}\"\n    EXP_RESULT_NAME = \"analysis.pkl\"\n    LOCAL_BEST_PARAMS_NAME = \"local_best_params.json\"\n\n    def objective(self, params):\n\n        # 1. Setup an config for a spcific estimator process\n        estimator_path = self.setup_estimator_config(params)\n        self.logger.info(\"Searching params: {} \".format(params))\n\n        # 2. Use subprocess to do the estimator program, this process will wait until subprocess finish\n        sub_fails = subprocess.call(\"estimator -c {}\".format(estimator_path), shell=True)\n        if sub_fails:\n            # If this subprocess failed, ignore this evaluation step\n            self.logger.info(\"Estimator experiment failed when using this searching parameters\")\n            return {\"loss\": np.nan, \"status\": STATUS_FAIL}\n\n        # 3. Fetch the result of subprocess, and check whether the result is Nan\n        res = self.fetch_result()\n        if np.isnan(res):\n            status = STATUS_FAIL\n        else:\n            status = STATUS_OK\n\n        # 4. Save the best score and params\n        if self.best_res is None or self.best_res > res:\n            self.best_res = res\n            self.best_params = params\n\n        # 5. Return the result as optim objective\n        return {\"loss\": res, \"status\": status}\n\n    def fetch_result(self):\n\n        # 1. Get experiment information\n        exp_info_path = os.path.join(self.ex_dir, QLibTuner.EXP_INFO_NAME)\n        with open(exp_info_path) as fp:\n            exp_info = json.load(fp)\n        estimator_ex_id = exp_info[\"id\"]\n\n        # 2. Return model result if needed\n        if self.optim_config.report_type == \"model\":\n            if self.optim_config.report_factor == \"model_score\":\n                # if estimator experiment is multi-label training, user need to process the scores by himself\n                # Default method is return the average score\n                return np.mean(exp_info[\"performance\"][\"model_score\"])\n            elif self.optim_config.report_factor == \"model_pearsonr\":\n                # pearsonr is a correlation coefficient, 1 is the best\n                return np.abs(exp_info[\"performance\"][\"model_pearsonr\"] - 1)\n\n        # 3. Get backtest results\n        exp_result_dir = os.path.join(self.ex_dir, QLibTuner.EXP_RESULT_DIR.format(estimator_ex_id))\n        exp_result_path = os.path.join(exp_result_dir, QLibTuner.EXP_RESULT_NAME)\n        with open(exp_result_path, \"rb\") as fp:\n            analysis_df = pickle.load(fp)\n\n        # 4. Get the backtest factor which user want to optimize, if user want to maximize the factor, then reverse the result\n        res = analysis_df.loc[self.optim_config.report_type].loc[self.optim_config.report_factor]\n        # res = res.values[0] if self.optim_config.optim_type == 'min' else -res.values[0]\n        if self.optim_config == \"min\":\n            return res.values[0]\n        elif self.optim_config == \"max\":\n            return -res.values[0]\n        else:\n            # self.optim_config == 'correlation'\n            return np.abs(res.values[0] - 1)\n\n    def setup_estimator_config(self, params):\n\n        estimator_config = copy.deepcopy(self.tuner_config)\n        estimator_config[\"model\"].update({\"args\": params[\"model_space\"]})\n        estimator_config[\"strategy\"].update({\"args\": params[\"strategy_space\"]})\n        if params.get(\"data_label_space\", None) is not None:\n            estimator_config[\"data\"][\"args\"].update(params[\"data_label_space\"])\n\n        estimator_path = os.path.join(\n            self.tuner_config[\"experiment\"].get(\"dir\", \"../\"),\n            QLibTuner.ESTIMATOR_CONFIG_NAME,\n        )\n\n        with open(estimator_path, \"w\") as fp:\n            yaml.dump(estimator_config, fp)\n\n        return estimator_path\n\n    def setup_space(self):\n        # 1. Setup model space\n        model_space_name = self.tuner_config[\"model\"].get(\"space\", None)\n        if model_space_name is None:\n            raise ValueError(\"Please give the search space of model.\")\n        model_space = getattr(\n            importlib.import_module(\".space\", package=\"qlib.contrib.tuner\"),\n            model_space_name,\n        )\n\n        # 2. Setup strategy space\n        strategy_space_name = self.tuner_config[\"strategy\"].get(\"space\", None)\n        if strategy_space_name is None:\n            raise ValueError(\"Please give the search space of strategy.\")\n        strategy_space = getattr(\n            importlib.import_module(\".space\", package=\"qlib.contrib.tuner\"),\n            strategy_space_name,\n        )\n\n        # 3. Setup data label space if given\n        if self.tuner_config.get(\"data_label\", None) is not None:\n            data_label_space_name = self.tuner_config[\"data_label\"].get(\"space\", None)\n            if data_label_space_name is not None:\n                data_label_space = getattr(\n                    importlib.import_module(\".space\", package=\"qlib.contrib.tuner\"),\n                    data_label_space_name,\n                )\n        else:\n            data_label_space_name = None\n\n        # 4. Combine the searching space\n        space = dict()\n        space.update({\"model_space\": model_space})\n        space.update({\"strategy_space\": strategy_space})\n        if data_label_space_name is not None:\n            space.update({\"data_label_space\": data_label_space})\n\n        return space\n\n    def save_local_best_params(self):\n\n        TimeInspector.set_time_mark()\n        local_best_params_path = os.path.join(self.ex_dir, QLibTuner.LOCAL_BEST_PARAMS_NAME)\n        with open(local_best_params_path, \"w\") as fp:\n            json.dump(self.best_params, fp)\n        TimeInspector.log_cost_time(\n            \"Finished saving local best tuner parameters to: {} .\".format(local_best_params_path)\n        )",
  "def __init__(self, tuner_config, optim_config):\n\n        self.logger = get_module_logger(\"Tuner\", sh_level=logging.INFO)\n\n        self.tuner_config = tuner_config\n        self.optim_config = optim_config\n\n        self.max_evals = self.tuner_config.get(\"max_evals\", 10)\n        self.ex_dir = os.path.join(\n            self.tuner_config[\"experiment\"][\"dir\"],\n            self.tuner_config[\"experiment\"][\"name\"],\n        )\n\n        self.best_params = None\n        self.best_res = None\n\n        self.space = self.setup_space()",
  "def tune(self):\n\n        TimeInspector.set_time_mark()\n        fmin(\n            fn=self.objective,\n            space=self.space,\n            algo=tpe.suggest,\n            max_evals=self.max_evals,\n        )\n        self.logger.info(\"Local best params: {} \".format(self.best_params))\n        TimeInspector.log_cost_time(\n            \"Finished searching best parameters in Tuner {}.\".format(self.tuner_config[\"experiment\"][\"id\"])\n        )\n\n        self.save_local_best_params()",
  "def objective(self, params):\n        \"\"\"\n        Implement this method to give an optimization factor using parameters in space.\n        :return: {'loss': a factor for optimization, float type,\n                  'status': the status of this evaluation step, STATUS_OK or STATUS_FAIL}.\n        \"\"\"\n        pass",
  "def setup_space(self):\n        \"\"\"\n        Implement this method to setup the searching space of tuner.\n        :return: searching space, dict type.\n        \"\"\"\n        pass",
  "def save_local_best_params(self):\n        \"\"\"\n        Implement this method to save the best parameters of this tuner.\n        \"\"\"\n        pass",
  "def objective(self, params):\n\n        # 1. Setup an config for a spcific estimator process\n        estimator_path = self.setup_estimator_config(params)\n        self.logger.info(\"Searching params: {} \".format(params))\n\n        # 2. Use subprocess to do the estimator program, this process will wait until subprocess finish\n        sub_fails = subprocess.call(\"estimator -c {}\".format(estimator_path), shell=True)\n        if sub_fails:\n            # If this subprocess failed, ignore this evaluation step\n            self.logger.info(\"Estimator experiment failed when using this searching parameters\")\n            return {\"loss\": np.nan, \"status\": STATUS_FAIL}\n\n        # 3. Fetch the result of subprocess, and check whether the result is Nan\n        res = self.fetch_result()\n        if np.isnan(res):\n            status = STATUS_FAIL\n        else:\n            status = STATUS_OK\n\n        # 4. Save the best score and params\n        if self.best_res is None or self.best_res > res:\n            self.best_res = res\n            self.best_params = params\n\n        # 5. Return the result as optim objective\n        return {\"loss\": res, \"status\": status}",
  "def fetch_result(self):\n\n        # 1. Get experiment information\n        exp_info_path = os.path.join(self.ex_dir, QLibTuner.EXP_INFO_NAME)\n        with open(exp_info_path) as fp:\n            exp_info = json.load(fp)\n        estimator_ex_id = exp_info[\"id\"]\n\n        # 2. Return model result if needed\n        if self.optim_config.report_type == \"model\":\n            if self.optim_config.report_factor == \"model_score\":\n                # if estimator experiment is multi-label training, user need to process the scores by himself\n                # Default method is return the average score\n                return np.mean(exp_info[\"performance\"][\"model_score\"])\n            elif self.optim_config.report_factor == \"model_pearsonr\":\n                # pearsonr is a correlation coefficient, 1 is the best\n                return np.abs(exp_info[\"performance\"][\"model_pearsonr\"] - 1)\n\n        # 3. Get backtest results\n        exp_result_dir = os.path.join(self.ex_dir, QLibTuner.EXP_RESULT_DIR.format(estimator_ex_id))\n        exp_result_path = os.path.join(exp_result_dir, QLibTuner.EXP_RESULT_NAME)\n        with open(exp_result_path, \"rb\") as fp:\n            analysis_df = pickle.load(fp)\n\n        # 4. Get the backtest factor which user want to optimize, if user want to maximize the factor, then reverse the result\n        res = analysis_df.loc[self.optim_config.report_type].loc[self.optim_config.report_factor]\n        # res = res.values[0] if self.optim_config.optim_type == 'min' else -res.values[0]\n        if self.optim_config == \"min\":\n            return res.values[0]\n        elif self.optim_config == \"max\":\n            return -res.values[0]\n        else:\n            # self.optim_config == 'correlation'\n            return np.abs(res.values[0] - 1)",
  "def setup_estimator_config(self, params):\n\n        estimator_config = copy.deepcopy(self.tuner_config)\n        estimator_config[\"model\"].update({\"args\": params[\"model_space\"]})\n        estimator_config[\"strategy\"].update({\"args\": params[\"strategy_space\"]})\n        if params.get(\"data_label_space\", None) is not None:\n            estimator_config[\"data\"][\"args\"].update(params[\"data_label_space\"])\n\n        estimator_path = os.path.join(\n            self.tuner_config[\"experiment\"].get(\"dir\", \"../\"),\n            QLibTuner.ESTIMATOR_CONFIG_NAME,\n        )\n\n        with open(estimator_path, \"w\") as fp:\n            yaml.dump(estimator_config, fp)\n\n        return estimator_path",
  "def setup_space(self):\n        # 1. Setup model space\n        model_space_name = self.tuner_config[\"model\"].get(\"space\", None)\n        if model_space_name is None:\n            raise ValueError(\"Please give the search space of model.\")\n        model_space = getattr(\n            importlib.import_module(\".space\", package=\"qlib.contrib.tuner\"),\n            model_space_name,\n        )\n\n        # 2. Setup strategy space\n        strategy_space_name = self.tuner_config[\"strategy\"].get(\"space\", None)\n        if strategy_space_name is None:\n            raise ValueError(\"Please give the search space of strategy.\")\n        strategy_space = getattr(\n            importlib.import_module(\".space\", package=\"qlib.contrib.tuner\"),\n            strategy_space_name,\n        )\n\n        # 3. Setup data label space if given\n        if self.tuner_config.get(\"data_label\", None) is not None:\n            data_label_space_name = self.tuner_config[\"data_label\"].get(\"space\", None)\n            if data_label_space_name is not None:\n                data_label_space = getattr(\n                    importlib.import_module(\".space\", package=\"qlib.contrib.tuner\"),\n                    data_label_space_name,\n                )\n        else:\n            data_label_space_name = None\n\n        # 4. Combine the searching space\n        space = dict()\n        space.update({\"model_space\": model_space})\n        space.update({\"strategy_space\": strategy_space})\n        if data_label_space_name is not None:\n            space.update({\"data_label_space\": data_label_space})\n\n        return space",
  "def save_local_best_params(self):\n\n        TimeInspector.set_time_mark()\n        local_best_params_path = os.path.join(self.ex_dir, QLibTuner.LOCAL_BEST_PARAMS_NAME)\n        with open(local_best_params_path, \"w\") as fp:\n            json.dump(self.best_params, fp)\n        TimeInspector.log_cost_time(\n            \"Finished saving local best tuner parameters to: {} .\".format(local_best_params_path)\n        )",
  "class Pipeline:\n\n    GLOBAL_BEST_PARAMS_NAME = \"global_best_params.json\"\n\n    def __init__(self, tuner_config_manager):\n\n        self.logger = get_module_logger(\"Pipeline\", sh_level=logging.INFO)\n\n        self.tuner_config_manager = tuner_config_manager\n\n        self.pipeline_ex_config = tuner_config_manager.pipeline_ex_config\n        self.optim_config = tuner_config_manager.optim_config\n        self.time_config = tuner_config_manager.time_config\n        self.pipeline_config = tuner_config_manager.pipeline_config\n        self.data_config = tuner_config_manager.data_config\n        self.backtest_config = tuner_config_manager.backtest_config\n        self.qlib_client_config = tuner_config_manager.qlib_client_config\n\n        self.global_best_res = None\n        self.global_best_params = None\n        self.best_tuner_index = None\n\n    def run(self):\n\n        TimeInspector.set_time_mark()\n        for tuner_index, tuner_config in enumerate(self.pipeline_config):\n            tuner = self.init_tuner(tuner_index, tuner_config)\n            tuner.tune()\n            if self.global_best_res is None or self.global_best_res > tuner.best_res:\n                self.global_best_res = tuner.best_res\n                self.global_best_params = tuner.best_params\n                self.best_tuner_index = tuner_index\n        TimeInspector.log_cost_time(\"Finished tuner pipeline.\")\n\n        self.save_tuner_exp_info()\n\n    def init_tuner(self, tuner_index, tuner_config):\n        \"\"\"\n        Implement this method to build the tuner by config\n        return: tuner\n        \"\"\"\n        # 1. Add experiment config in tuner_config\n        tuner_config[\"experiment\"] = {\n            \"name\": \"estimator_experiment_{}\".format(tuner_index),\n            \"id\": tuner_index,\n            \"dir\": self.pipeline_ex_config.estimator_ex_dir,\n            \"observer_type\": \"file_storage\",\n        }\n        tuner_config[\"qlib_client\"] = self.qlib_client_config\n        # 2. Add data config in tuner_config\n        tuner_config[\"data\"] = self.data_config\n        # 3. Add backtest config in tuner_config\n        tuner_config[\"backtest\"] = self.backtest_config\n        # 4. Update trainer in tuner_config\n        tuner_config[\"trainer\"].update({\"args\": self.time_config})\n\n        # 5. Import Tuner class\n        tuner_module = get_module_by_module_path(self.pipeline_ex_config.tuner_module_path)\n        tuner_class = getattr(tuner_module, self.pipeline_ex_config.tuner_class)\n        # 6. Return the specific tuner\n        return tuner_class(tuner_config, self.optim_config)\n\n    def save_tuner_exp_info(self):\n\n        TimeInspector.set_time_mark()\n        save_path = os.path.join(self.pipeline_ex_config.tuner_ex_dir, Pipeline.GLOBAL_BEST_PARAMS_NAME)\n        with open(save_path, \"w\") as fp:\n            json.dump(self.global_best_params, fp)\n        TimeInspector.log_cost_time(\"Finished save global best tuner parameters.\")\n\n        self.logger.info(\"Best Tuner id: {}.\".format(self.best_tuner_index))\n        self.logger.info(\"Global best parameters: {}.\".format(self.global_best_params))\n        self.logger.info(\"You can check the best parameters at {}.\".format(save_path))",
  "def __init__(self, tuner_config_manager):\n\n        self.logger = get_module_logger(\"Pipeline\", sh_level=logging.INFO)\n\n        self.tuner_config_manager = tuner_config_manager\n\n        self.pipeline_ex_config = tuner_config_manager.pipeline_ex_config\n        self.optim_config = tuner_config_manager.optim_config\n        self.time_config = tuner_config_manager.time_config\n        self.pipeline_config = tuner_config_manager.pipeline_config\n        self.data_config = tuner_config_manager.data_config\n        self.backtest_config = tuner_config_manager.backtest_config\n        self.qlib_client_config = tuner_config_manager.qlib_client_config\n\n        self.global_best_res = None\n        self.global_best_params = None\n        self.best_tuner_index = None",
  "def run(self):\n\n        TimeInspector.set_time_mark()\n        for tuner_index, tuner_config in enumerate(self.pipeline_config):\n            tuner = self.init_tuner(tuner_index, tuner_config)\n            tuner.tune()\n            if self.global_best_res is None or self.global_best_res > tuner.best_res:\n                self.global_best_res = tuner.best_res\n                self.global_best_params = tuner.best_params\n                self.best_tuner_index = tuner_index\n        TimeInspector.log_cost_time(\"Finished tuner pipeline.\")\n\n        self.save_tuner_exp_info()",
  "def init_tuner(self, tuner_index, tuner_config):\n        \"\"\"\n        Implement this method to build the tuner by config\n        return: tuner\n        \"\"\"\n        # 1. Add experiment config in tuner_config\n        tuner_config[\"experiment\"] = {\n            \"name\": \"estimator_experiment_{}\".format(tuner_index),\n            \"id\": tuner_index,\n            \"dir\": self.pipeline_ex_config.estimator_ex_dir,\n            \"observer_type\": \"file_storage\",\n        }\n        tuner_config[\"qlib_client\"] = self.qlib_client_config\n        # 2. Add data config in tuner_config\n        tuner_config[\"data\"] = self.data_config\n        # 3. Add backtest config in tuner_config\n        tuner_config[\"backtest\"] = self.backtest_config\n        # 4. Update trainer in tuner_config\n        tuner_config[\"trainer\"].update({\"args\": self.time_config})\n\n        # 5. Import Tuner class\n        tuner_module = get_module_by_module_path(self.pipeline_ex_config.tuner_module_path)\n        tuner_class = getattr(tuner_module, self.pipeline_ex_config.tuner_class)\n        # 6. Return the specific tuner\n        return tuner_class(tuner_config, self.optim_config)",
  "def save_tuner_exp_info(self):\n\n        TimeInspector.set_time_mark()\n        save_path = os.path.join(self.pipeline_ex_config.tuner_ex_dir, Pipeline.GLOBAL_BEST_PARAMS_NAME)\n        with open(save_path, \"w\") as fp:\n            json.dump(self.global_best_params, fp)\n        TimeInspector.log_cost_time(\"Finished save global best tuner parameters.\")\n\n        self.logger.info(\"Best Tuner id: {}.\".format(self.best_tuner_index))\n        self.logger.info(\"Global best parameters: {}.\".format(self.global_best_params))\n        self.logger.info(\"You can check the best parameters at {}.\".format(save_path))",
  "class TunerConfigManager:\n    def __init__(self, config_path):\n\n        if not config_path:\n            raise ValueError(\"Config path is invalid.\")\n        self.config_path = config_path\n\n        with open(config_path) as fp:\n            config = yaml.safe_load(fp)\n        self.config = copy.deepcopy(config)\n\n        self.pipeline_ex_config = PipelineExperimentConfig(config.get(\"experiment\", dict()), self)\n        self.pipeline_config = config.get(\"tuner_pipeline\", list())\n        self.optim_config = OptimizationConfig(config.get(\"optimization_criteria\", dict()), self)\n\n        self.time_config = config.get(\"time_period\", dict())\n        self.data_config = config.get(\"data\", dict())\n        self.backtest_config = config.get(\"backtest\", dict())\n        self.qlib_client_config = config.get(\"qlib_client\", dict())",
  "class PipelineExperimentConfig:\n    def __init__(self, config, TUNER_CONFIG_MANAGER):\n        \"\"\"\n        :param config:  The config dict for tuner experiment\n        :param TUNER_CONFIG_MANAGER:   The tuner config manager\n        \"\"\"\n        self.name = config.get(\"name\", \"tuner_experiment\")\n        # The dir of the config\n        self.global_dir = config.get(\"dir\", os.path.dirname(TUNER_CONFIG_MANAGER.config_path))\n        # The dir of the result of tuner experiment\n        self.tuner_ex_dir = config.get(\"tuner_ex_dir\", os.path.join(self.global_dir, self.name))\n        if not os.path.exists(self.tuner_ex_dir):\n            os.makedirs(self.tuner_ex_dir)\n        # The dir of the results of all estimator experiments\n        self.estimator_ex_dir = config.get(\"estimator_ex_dir\", os.path.join(self.tuner_ex_dir, \"estimator_experiment\"))\n        if not os.path.exists(self.estimator_ex_dir):\n            os.makedirs(self.estimator_ex_dir)\n        # Get the tuner type\n        self.tuner_module_path = config.get(\"tuner_module_path\", \"qlib.contrib.tuner.tuner\")\n        self.tuner_class = config.get(\"tuner_class\", \"QLibTuner\")\n        # Save the tuner experiment for further view\n        tuner_ex_config_path = os.path.join(self.tuner_ex_dir, \"tuner_config.yaml\")\n        with open(tuner_ex_config_path, \"w\") as fp:\n            yaml.dump(TUNER_CONFIG_MANAGER.config, fp)",
  "class OptimizationConfig:\n    def __init__(self, config, TUNER_CONFIG_MANAGER):\n\n        self.report_type = config.get(\"report_type\", \"pred_long\")\n        if self.report_type not in [\n            \"pred_long\",\n            \"pred_long_short\",\n            \"pred_short\",\n            \"excess_return_without_cost\",\n            \"excess_return_with_cost\",\n            \"model\",\n        ]:\n            raise ValueError(\n                \"report_type should be one of pred_long, pred_long_short, pred_short, excess_return_without_cost, excess_return_with_cost and model\"\n            )\n\n        self.report_factor = config.get(\"report_factor\", \"information_ratio\")\n        if self.report_factor not in [\n            \"annualized_return\",\n            \"information_ratio\",\n            \"max_drawdown\",\n            \"mean\",\n            \"std\",\n            \"model_score\",\n            \"model_pearsonr\",\n        ]:\n            raise ValueError(\n                \"report_factor should be one of annualized_return, information_ratio, max_drawdown, mean, std, model_pearsonr and model_score\"\n            )\n\n        self.optim_type = config.get(\"optim_type\", \"max\")\n        if self.optim_type not in [\"min\", \"max\", \"correlation\"]:\n            raise ValueError(\"optim_type should be min, max or correlation\")",
  "def __init__(self, config_path):\n\n        if not config_path:\n            raise ValueError(\"Config path is invalid.\")\n        self.config_path = config_path\n\n        with open(config_path) as fp:\n            config = yaml.safe_load(fp)\n        self.config = copy.deepcopy(config)\n\n        self.pipeline_ex_config = PipelineExperimentConfig(config.get(\"experiment\", dict()), self)\n        self.pipeline_config = config.get(\"tuner_pipeline\", list())\n        self.optim_config = OptimizationConfig(config.get(\"optimization_criteria\", dict()), self)\n\n        self.time_config = config.get(\"time_period\", dict())\n        self.data_config = config.get(\"data\", dict())\n        self.backtest_config = config.get(\"backtest\", dict())\n        self.qlib_client_config = config.get(\"qlib_client\", dict())",
  "def __init__(self, config, TUNER_CONFIG_MANAGER):\n        \"\"\"\n        :param config:  The config dict for tuner experiment\n        :param TUNER_CONFIG_MANAGER:   The tuner config manager\n        \"\"\"\n        self.name = config.get(\"name\", \"tuner_experiment\")\n        # The dir of the config\n        self.global_dir = config.get(\"dir\", os.path.dirname(TUNER_CONFIG_MANAGER.config_path))\n        # The dir of the result of tuner experiment\n        self.tuner_ex_dir = config.get(\"tuner_ex_dir\", os.path.join(self.global_dir, self.name))\n        if not os.path.exists(self.tuner_ex_dir):\n            os.makedirs(self.tuner_ex_dir)\n        # The dir of the results of all estimator experiments\n        self.estimator_ex_dir = config.get(\"estimator_ex_dir\", os.path.join(self.tuner_ex_dir, \"estimator_experiment\"))\n        if not os.path.exists(self.estimator_ex_dir):\n            os.makedirs(self.estimator_ex_dir)\n        # Get the tuner type\n        self.tuner_module_path = config.get(\"tuner_module_path\", \"qlib.contrib.tuner.tuner\")\n        self.tuner_class = config.get(\"tuner_class\", \"QLibTuner\")\n        # Save the tuner experiment for further view\n        tuner_ex_config_path = os.path.join(self.tuner_ex_dir, \"tuner_config.yaml\")\n        with open(tuner_ex_config_path, \"w\") as fp:\n            yaml.dump(TUNER_CONFIG_MANAGER.config, fp)",
  "def __init__(self, config, TUNER_CONFIG_MANAGER):\n\n        self.report_type = config.get(\"report_type\", \"pred_long\")\n        if self.report_type not in [\n            \"pred_long\",\n            \"pred_long_short\",\n            \"pred_short\",\n            \"excess_return_without_cost\",\n            \"excess_return_with_cost\",\n            \"model\",\n        ]:\n            raise ValueError(\n                \"report_type should be one of pred_long, pred_long_short, pred_short, excess_return_without_cost, excess_return_with_cost and model\"\n            )\n\n        self.report_factor = config.get(\"report_factor\", \"information_ratio\")\n        if self.report_factor not in [\n            \"annualized_return\",\n            \"information_ratio\",\n            \"max_drawdown\",\n            \"mean\",\n            \"std\",\n            \"model_score\",\n            \"model_pearsonr\",\n        ]:\n            raise ValueError(\n                \"report_factor should be one of annualized_return, information_ratio, max_drawdown, mean, std, model_pearsonr and model_score\"\n            )\n\n        self.optim_type = config.get(\"optim_type\", \"max\")\n        if self.optim_type not in [\"min\", \"max\", \"correlation\"]:\n            raise ValueError(\"optim_type should be min, max or correlation\")",
  "def run():\n    # 1. Get pipeline class.\n    tuner_pipeline_class = getattr(importlib.import_module(\".pipeline\", package=\"qlib.contrib.tuner\"), \"Pipeline\")\n    # 2. Init tuner pipeline.\n    tuner_pipeline = tuner_pipeline_class(TUNER_CONFIG_MANAGER)\n    # 3. Begin to tune\n    tuner_pipeline.run()",
  "class PortfolioOptimizer(BaseOptimizer):\n    \"\"\"Portfolio Optimizer\n\n    The following optimization algorithms are supported:\n        - `gmv`: Global Minimum Variance Portfolio\n        - `mvo`: Mean Variance Optimized Portfolio\n        - `rp`: Risk Parity\n        - `inv`: Inverse Volatility\n\n    Note:\n        This optimizer always assumes full investment and no-shorting.\n    \"\"\"\n\n    OPT_GMV = \"gmv\"\n    OPT_MVO = \"mvo\"\n    OPT_RP = \"rp\"\n    OPT_INV = \"inv\"\n\n    def __init__(\n        self,\n        method: str = \"inv\",\n        lamb: float = 0,\n        delta: float = 0,\n        alpha: float = 0.0,\n        scale_alpha: bool = True,\n        tol: float = 1e-8,\n    ):\n        \"\"\"\n        Args:\n            method (str): portfolio optimization method\n            lamb (float): risk aversion parameter (larger `lamb` means more focus on return)\n            delta (float): turnover rate limit\n            alpha (float): l2 norm regularizer\n            scale_alpha (bool): if to scale alpha to match the volatility of the covariance matrix\n            tol (float): tolerance for optimization termination\n        \"\"\"\n        assert method in [self.OPT_GMV, self.OPT_MVO, self.OPT_RP, self.OPT_INV], f\"method `{method}` is not supported\"\n        self.method = method\n\n        assert lamb >= 0, f\"risk aversion parameter `lamb` should be positive\"\n        self.lamb = lamb\n\n        assert delta >= 0, f\"turnover limit `delta` should be positive\"\n        self.delta = delta\n\n        assert alpha >= 0, f\"l2 norm regularizer `alpha` should be positive\"\n        self.alpha = alpha\n\n        self.tol = tol\n        self.scale_alpha = scale_alpha\n\n    def __call__(\n        self,\n        S: Union[np.ndarray, pd.DataFrame],\n        u: Optional[Union[np.ndarray, pd.Series]] = None,\n        w0: Optional[Union[np.ndarray, pd.Series]] = None,\n    ) -> Union[np.ndarray, pd.Series]:\n        \"\"\"\n        Args:\n            S (np.ndarray or pd.DataFrame): covariance matrix\n            u (np.ndarray or pd.Series): expected returns (a.k.a., alpha)\n            w0 (np.ndarray or pd.Series): initial weights (for turnover control)\n\n        Returns:\n            np.ndarray or pd.Series: optimized portfolio allocation\n        \"\"\"\n        # transform dataframe into array\n        index = None\n        if isinstance(S, pd.DataFrame):\n            index = S.index\n            S = S.values\n\n        # transform alpha\n        if u is not None:\n            assert len(u) == len(S), \"`u` has mismatched shape\"\n            if isinstance(u, pd.Series):\n                assert u.index.equals(index), \"`u` has mismatched index\"\n                u = u.values\n\n        # transform initial weights\n        if w0 is not None:\n            assert len(w0) == len(S), \"`w0` has mismatched shape\"\n            if isinstance(w0, pd.Series):\n                assert w0.index.equals(index), \"`w0` has mismatched index\"\n                w0 = w0.values\n\n        # scale alpha to match volatility\n        if u is not None and self.scale_alpha:\n            u = u / u.std()\n            u *= np.mean(np.diag(S)) ** 0.5\n\n        # optimize\n        w = self._optimize(S, u, w0)\n\n        # restore index if needed\n        if index is not None:\n            w = pd.Series(w, index=index)\n\n        return w\n\n    def _optimize(self, S: np.ndarray, u: Optional[np.ndarray] = None, w0: Optional[np.ndarray] = None) -> np.ndarray:\n\n        # inverse volatility\n        if self.method == self.OPT_INV:\n            if u is not None:\n                warnings.warn(\"`u` is set but will not be used for `inv` portfolio\")\n            if w0 is not None:\n                warnings.warn(\"`w0` is set but will not be used for `inv` portfolio\")\n            return self._optimize_inv(S)\n\n        # global minimum variance\n        if self.method == self.OPT_GMV:\n            if u is not None:\n                warnings.warn(\"`u` is set but will not be used for `gmv` portfolio\")\n            return self._optimize_gmv(S, w0)\n\n        # mean-variance\n        if self.method == self.OPT_MVO:\n            return self._optimize_mvo(S, u, w0)\n\n        # risk parity\n        if self.method == self.OPT_RP:\n            if u is not None:\n                warnings.warn(\"`u` is set but will not be used for `rp` portfolio\")\n            return self._optimize_rp(S, w0)\n\n    def _optimize_inv(self, S: np.ndarray) -> np.ndarray:\n        \"\"\"Inverse volatility\"\"\"\n        vola = np.diag(S) ** 0.5\n        w = 1 / vola\n        w /= w.sum()\n        return w\n\n    def _optimize_gmv(self, S: np.ndarray, w0: Optional[np.ndarray] = None) -> np.ndarray:\n        \"\"\"optimize global minimum variance portfolio\n\n        This method solves the following optimization problem\n            min_w w' S w\n            s.t. w >= 0, sum(w) == 1\n        where `S` is the covariance matrix.\n        \"\"\"\n        return self._solve(len(S), self._get_objective_gmv(S), *self._get_constrains(w0))\n\n    def _optimize_mvo(\n        self, S: np.ndarray, u: Optional[np.ndarray] = None, w0: Optional[np.ndarray] = None\n    ) -> np.ndarray:\n        \"\"\"optimize mean-variance portfolio\n\n        This method solves the following optimization problem\n            min_w   - w' u + lamb * w' S w\n            s.t.   w >= 0, sum(w) == 1\n        where `S` is the covariance matrix, `u` is the expected returns,\n        and `lamb` is the risk aversion parameter.\n        \"\"\"\n        return self._solve(len(S), self._get_objective_mvo(S, u), *self._get_constrains(w0))\n\n    def _optimize_rp(self, S: np.ndarray, w0: Optional[np.ndarray] = None) -> np.ndarray:\n        \"\"\"optimize risk parity portfolio\n\n        This method solves the following optimization problem\n            min_w sum_i [w_i - (w' S w) / ((S w)_i * N)]**2\n            s.t. w >= 0, sum(w) == 1\n        where `S` is the covariance matrix and `N` is the number of stocks.\n        \"\"\"\n        return self._solve(len(S), self._get_objective_rp(S), *self._get_constrains(w0))\n\n    def _get_objective_gmv(self, S: np.ndarray) -> Callable:\n        \"\"\"global minimum variance optimization objective\n\n        Optimization objective\n            min_w w' S w\n        \"\"\"\n\n        def func(x):\n            return x @ S @ x\n\n        return func\n\n    def _get_objective_mvo(self, S: np.ndarray, u: np.ndarray = None) -> Callable:\n        \"\"\"mean-variance optimization objective\n\n        Optimization objective\n            min_w - w' u + lamb * w' S w\n        \"\"\"\n\n        def func(x):\n            risk = x @ S @ x\n            ret = x @ u\n            return -ret + self.lamb * risk\n\n        return func\n\n    def _get_objective_rp(self, S: np.ndarray) -> Callable:\n        \"\"\"risk-parity optimization objective\n\n        Optimization objective\n            min_w sum_i [w_i - (w' S w) / ((S w)_i * N)]**2\n        \"\"\"\n\n        def func(x):\n            N = len(x)\n            Sx = S @ x\n            xSx = x @ Sx\n            return np.sum((x - xSx / Sx / N) ** 2)\n\n        return func\n\n    def _get_constrains(self, w0: Optional[np.ndarray] = None):\n        \"\"\"optimization constraints\n\n        Defines the following constraints:\n            - no shorting and leverage: 0 <= w <= 1\n            - full investment: sum(w) == 1\n            - turnover constraint: |w - w0| <= delta\n        \"\"\"\n\n        # no shorting and leverage\n        bounds = so.Bounds(0.0, 1.0)\n\n        # full investment constraint\n        cons = [{\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - 1}]  # == 0\n\n        # turnover constraint\n        if w0 is not None:\n            cons.append({\"type\": \"ineq\", \"fun\": lambda x: self.delta - np.sum(np.abs(x - w0))})  # >= 0\n\n        return bounds, cons\n\n    def _solve(self, n: int, obj: Callable, bounds: so.Bounds, cons: List) -> np.ndarray:\n        \"\"\"solve optimization\n\n        Args:\n            n (int): number of parameters\n            obj (callable): optimization objective\n            bounds (Bounds): bounds of parameters\n            cons (list): optimization constraints\n        \"\"\"\n        # add l2 regularization\n        wrapped_obj = obj\n        if self.alpha > 0:\n\n            def opt_obj(x):\n                return obj(x) + self.alpha * np.sum(np.square(x))\n\n            wrapped_obj = opt_obj\n\n        # solve\n        x0 = np.ones(n) / n  # init results\n        sol = so.minimize(wrapped_obj, x0, bounds=bounds, constraints=cons, tol=self.tol)\n        if not sol.success:\n            warnings.warn(f\"optimization not success ({sol.status})\")\n\n        return sol.x",
  "def __init__(\n        self,\n        method: str = \"inv\",\n        lamb: float = 0,\n        delta: float = 0,\n        alpha: float = 0.0,\n        scale_alpha: bool = True,\n        tol: float = 1e-8,\n    ):\n        \"\"\"\n        Args:\n            method (str): portfolio optimization method\n            lamb (float): risk aversion parameter (larger `lamb` means more focus on return)\n            delta (float): turnover rate limit\n            alpha (float): l2 norm regularizer\n            scale_alpha (bool): if to scale alpha to match the volatility of the covariance matrix\n            tol (float): tolerance for optimization termination\n        \"\"\"\n        assert method in [self.OPT_GMV, self.OPT_MVO, self.OPT_RP, self.OPT_INV], f\"method `{method}` is not supported\"\n        self.method = method\n\n        assert lamb >= 0, f\"risk aversion parameter `lamb` should be positive\"\n        self.lamb = lamb\n\n        assert delta >= 0, f\"turnover limit `delta` should be positive\"\n        self.delta = delta\n\n        assert alpha >= 0, f\"l2 norm regularizer `alpha` should be positive\"\n        self.alpha = alpha\n\n        self.tol = tol\n        self.scale_alpha = scale_alpha",
  "def __call__(\n        self,\n        S: Union[np.ndarray, pd.DataFrame],\n        u: Optional[Union[np.ndarray, pd.Series]] = None,\n        w0: Optional[Union[np.ndarray, pd.Series]] = None,\n    ) -> Union[np.ndarray, pd.Series]:\n        \"\"\"\n        Args:\n            S (np.ndarray or pd.DataFrame): covariance matrix\n            u (np.ndarray or pd.Series): expected returns (a.k.a., alpha)\n            w0 (np.ndarray or pd.Series): initial weights (for turnover control)\n\n        Returns:\n            np.ndarray or pd.Series: optimized portfolio allocation\n        \"\"\"\n        # transform dataframe into array\n        index = None\n        if isinstance(S, pd.DataFrame):\n            index = S.index\n            S = S.values\n\n        # transform alpha\n        if u is not None:\n            assert len(u) == len(S), \"`u` has mismatched shape\"\n            if isinstance(u, pd.Series):\n                assert u.index.equals(index), \"`u` has mismatched index\"\n                u = u.values\n\n        # transform initial weights\n        if w0 is not None:\n            assert len(w0) == len(S), \"`w0` has mismatched shape\"\n            if isinstance(w0, pd.Series):\n                assert w0.index.equals(index), \"`w0` has mismatched index\"\n                w0 = w0.values\n\n        # scale alpha to match volatility\n        if u is not None and self.scale_alpha:\n            u = u / u.std()\n            u *= np.mean(np.diag(S)) ** 0.5\n\n        # optimize\n        w = self._optimize(S, u, w0)\n\n        # restore index if needed\n        if index is not None:\n            w = pd.Series(w, index=index)\n\n        return w",
  "def _optimize(self, S: np.ndarray, u: Optional[np.ndarray] = None, w0: Optional[np.ndarray] = None) -> np.ndarray:\n\n        # inverse volatility\n        if self.method == self.OPT_INV:\n            if u is not None:\n                warnings.warn(\"`u` is set but will not be used for `inv` portfolio\")\n            if w0 is not None:\n                warnings.warn(\"`w0` is set but will not be used for `inv` portfolio\")\n            return self._optimize_inv(S)\n\n        # global minimum variance\n        if self.method == self.OPT_GMV:\n            if u is not None:\n                warnings.warn(\"`u` is set but will not be used for `gmv` portfolio\")\n            return self._optimize_gmv(S, w0)\n\n        # mean-variance\n        if self.method == self.OPT_MVO:\n            return self._optimize_mvo(S, u, w0)\n\n        # risk parity\n        if self.method == self.OPT_RP:\n            if u is not None:\n                warnings.warn(\"`u` is set but will not be used for `rp` portfolio\")\n            return self._optimize_rp(S, w0)",
  "def _optimize_inv(self, S: np.ndarray) -> np.ndarray:\n        \"\"\"Inverse volatility\"\"\"\n        vola = np.diag(S) ** 0.5\n        w = 1 / vola\n        w /= w.sum()\n        return w",
  "def _optimize_gmv(self, S: np.ndarray, w0: Optional[np.ndarray] = None) -> np.ndarray:\n        \"\"\"optimize global minimum variance portfolio\n\n        This method solves the following optimization problem\n            min_w w' S w\n            s.t. w >= 0, sum(w) == 1\n        where `S` is the covariance matrix.\n        \"\"\"\n        return self._solve(len(S), self._get_objective_gmv(S), *self._get_constrains(w0))",
  "def _optimize_mvo(\n        self, S: np.ndarray, u: Optional[np.ndarray] = None, w0: Optional[np.ndarray] = None\n    ) -> np.ndarray:\n        \"\"\"optimize mean-variance portfolio\n\n        This method solves the following optimization problem\n            min_w   - w' u + lamb * w' S w\n            s.t.   w >= 0, sum(w) == 1\n        where `S` is the covariance matrix, `u` is the expected returns,\n        and `lamb` is the risk aversion parameter.\n        \"\"\"\n        return self._solve(len(S), self._get_objective_mvo(S, u), *self._get_constrains(w0))",
  "def _optimize_rp(self, S: np.ndarray, w0: Optional[np.ndarray] = None) -> np.ndarray:\n        \"\"\"optimize risk parity portfolio\n\n        This method solves the following optimization problem\n            min_w sum_i [w_i - (w' S w) / ((S w)_i * N)]**2\n            s.t. w >= 0, sum(w) == 1\n        where `S` is the covariance matrix and `N` is the number of stocks.\n        \"\"\"\n        return self._solve(len(S), self._get_objective_rp(S), *self._get_constrains(w0))",
  "def _get_objective_gmv(self, S: np.ndarray) -> Callable:\n        \"\"\"global minimum variance optimization objective\n\n        Optimization objective\n            min_w w' S w\n        \"\"\"\n\n        def func(x):\n            return x @ S @ x\n\n        return func",
  "def _get_objective_mvo(self, S: np.ndarray, u: np.ndarray = None) -> Callable:\n        \"\"\"mean-variance optimization objective\n\n        Optimization objective\n            min_w - w' u + lamb * w' S w\n        \"\"\"\n\n        def func(x):\n            risk = x @ S @ x\n            ret = x @ u\n            return -ret + self.lamb * risk\n\n        return func",
  "def _get_objective_rp(self, S: np.ndarray) -> Callable:\n        \"\"\"risk-parity optimization objective\n\n        Optimization objective\n            min_w sum_i [w_i - (w' S w) / ((S w)_i * N)]**2\n        \"\"\"\n\n        def func(x):\n            N = len(x)\n            Sx = S @ x\n            xSx = x @ Sx\n            return np.sum((x - xSx / Sx / N) ** 2)\n\n        return func",
  "def _get_constrains(self, w0: Optional[np.ndarray] = None):\n        \"\"\"optimization constraints\n\n        Defines the following constraints:\n            - no shorting and leverage: 0 <= w <= 1\n            - full investment: sum(w) == 1\n            - turnover constraint: |w - w0| <= delta\n        \"\"\"\n\n        # no shorting and leverage\n        bounds = so.Bounds(0.0, 1.0)\n\n        # full investment constraint\n        cons = [{\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - 1}]  # == 0\n\n        # turnover constraint\n        if w0 is not None:\n            cons.append({\"type\": \"ineq\", \"fun\": lambda x: self.delta - np.sum(np.abs(x - w0))})  # >= 0\n\n        return bounds, cons",
  "def _solve(self, n: int, obj: Callable, bounds: so.Bounds, cons: List) -> np.ndarray:\n        \"\"\"solve optimization\n\n        Args:\n            n (int): number of parameters\n            obj (callable): optimization objective\n            bounds (Bounds): bounds of parameters\n            cons (list): optimization constraints\n        \"\"\"\n        # add l2 regularization\n        wrapped_obj = obj\n        if self.alpha > 0:\n\n            def opt_obj(x):\n                return obj(x) + self.alpha * np.sum(np.square(x))\n\n            wrapped_obj = opt_obj\n\n        # solve\n        x0 = np.ones(n) / n  # init results\n        sol = so.minimize(wrapped_obj, x0, bounds=bounds, constraints=cons, tol=self.tol)\n        if not sol.success:\n            warnings.warn(f\"optimization not success ({sol.status})\")\n\n        return sol.x",
  "def func(x):\n            return x @ S @ x",
  "def func(x):\n            risk = x @ S @ x\n            ret = x @ u\n            return -ret + self.lamb * risk",
  "def func(x):\n            N = len(x)\n            Sx = S @ x\n            xSx = x @ Sx\n            return np.sum((x - xSx / Sx / N) ** 2)",
  "def opt_obj(x):\n                return obj(x) + self.alpha * np.sum(np.square(x))",
  "class BaseOptimizer(abc.ABC):\n    \"\"\" Construct portfolio with a optimization related method \"\"\"\n\n    @abc.abstractmethod\n    def __call__(self, *args, **kwargs) -> object:\n        \"\"\" Generate a optimized portfolio allocation \"\"\"\n        pass",
  "def __call__(self, *args, **kwargs) -> object:\n        \"\"\" Generate a optimized portfolio allocation \"\"\"\n        pass",
  "class EnhancedIndexingOptimizer(BaseOptimizer):\n    \"\"\"\n    Portfolio Optimizer with Enhanced Indexing\n\n    Note:\n        This optimizer always assumes full investment and no-shorting.\n    \"\"\"\n\n    START_FROM_W0 = \"w0\"\n    START_FROM_BENCH = \"benchmark\"\n\n    def __init__(\n        self,\n        lamb: float = 10,\n        delta: float = 0.4,\n        bench_dev: float = 0.01,\n        inds_dev: float = None,\n        scale_alpha: bool = True,\n        verbose: bool = False,\n        warm_start: str = None,\n        max_iters: int = 10000,\n    ):\n        \"\"\"\n        Args:\n            lamb (float): risk aversion parameter (larger `lamb` means less focus on return)\n            delta (float): turnover rate limit\n            bench_dev (float): benchmark deviation limit\n            inds_dev (float/None): industry deviation limit, set `inds_dev` to None to ignore industry specific\n                                   restriction\n            scale_alpha (bool): if to scale alpha to match the volatility of the covariance matrix\n            verbose (bool): if print detailed information about the solver\n            warm_start (str): whether try to warm start (`w0`/`benchmark`/``)\n                              (https://www.cvxpy.org/tutorial/advanced/index.html#warm-start)\n        \"\"\"\n\n        assert lamb >= 0, \"risk aversion parameter `lamb` should be positive\"\n        self.lamb = lamb\n\n        assert delta >= 0, \"turnover limit `delta` should be positive\"\n        self.delta = delta\n\n        assert bench_dev >= 0, \"benchmark deviation limit `bench_dev` should be positive\"\n        self.bench_dev = bench_dev\n\n        assert inds_dev is None or inds_dev >= 0, \"industry deviation limit `inds_dev` should be positive or None.\"\n        self.inds_dev = inds_dev\n\n        assert warm_start in [\n            None,\n            self.START_FROM_W0,\n            self.START_FROM_BENCH,\n        ], \"illegal warm start option\"\n        self.start_from_w0 = warm_start == self.START_FROM_W0\n        self.start_from_bench = warm_start == self.START_FROM_BENCH\n\n        self.scale_alpha = scale_alpha\n        self.verbose = verbose\n        self.max_iters = max_iters\n\n    def __call__(\n        self,\n        u: Union[np.ndarray, pd.Series],\n        F: np.ndarray,\n        covB: np.ndarray,\n        varU: np.ndarray,\n        w0: np.ndarray,\n        w_bench: np.ndarray,\n        inds_onehot: np.ndarray = None,\n    ) -> Union[np.ndarray, pd.Series]:\n        \"\"\"\n        Args:\n            u (np.ndarray or pd.Series): expected returns (a.k.a., alpha)\n            F, covB, varU (np.ndarray): see StructuredCovEstimator\n            w0 (np.ndarray): initial weights (for turnover control)\n            w_bench (np.ndarray): benchmark weights\n            inds_onehot (np.ndarray): industry (onehot)\n\n        Returns:\n            np.ndarray or pd.Series: optimized portfolio allocation\n        \"\"\"\n        assert inds_onehot is not None or self.inds_dev is None, \"Industry onehot vector is required.\"\n\n        # transform dataframe into array\n        if isinstance(u, pd.Series):\n            u = u.values\n\n        # scale alpha to match volatility\n        if self.scale_alpha:\n            u = u / u.std()\n            x_variance = np.mean(np.diag(F @ covB @ F.T) + varU)\n            u *= x_variance ** 0.5\n\n        w = cp.Variable(len(u))  # num_assets\n        v = w @ F  # num_factors\n        ret = w @ u\n        risk = cp.quad_form(v, covB) + cp.sum(cp.multiply(varU, w ** 2))\n        obj = cp.Maximize(ret - self.lamb * risk)\n        d_bench = w - w_bench\n        cons = [\n            w >= 0,\n            cp.sum(w) == 1,\n            d_bench >= -self.bench_dev,\n            d_bench <= self.bench_dev,\n        ]\n\n        if self.inds_dev is not None:\n            d_inds = d_bench @ inds_onehot\n            cons.append(d_inds >= -self.inds_dev)\n            cons.append(d_inds <= self.inds_dev)\n\n        if w0 is not None:\n            turnover = cp.sum(cp.abs(w - w0))\n            cons.append(turnover <= self.delta)\n\n        warm_start = False\n        if self.start_from_w0:\n            if w0 is None:\n                print(\"Warning: try warm start with w0, but w0 is `None`.\")\n            else:\n                w.value = w0\n                warm_start = True\n        elif self.start_from_bench:\n            w.value = w_bench\n            warm_start = True\n\n        prob = cp.Problem(obj, cons)\n        prob.solve(solver=cp.SCS, verbose=self.verbose, warm_start=warm_start, max_iters=self.max_iters)\n\n        if prob.status != \"optimal\":\n            print(\"Warning: solve failed.\", prob.status)\n\n        return np.asarray(w.value)",
  "def __init__(\n        self,\n        lamb: float = 10,\n        delta: float = 0.4,\n        bench_dev: float = 0.01,\n        inds_dev: float = None,\n        scale_alpha: bool = True,\n        verbose: bool = False,\n        warm_start: str = None,\n        max_iters: int = 10000,\n    ):\n        \"\"\"\n        Args:\n            lamb (float): risk aversion parameter (larger `lamb` means less focus on return)\n            delta (float): turnover rate limit\n            bench_dev (float): benchmark deviation limit\n            inds_dev (float/None): industry deviation limit, set `inds_dev` to None to ignore industry specific\n                                   restriction\n            scale_alpha (bool): if to scale alpha to match the volatility of the covariance matrix\n            verbose (bool): if print detailed information about the solver\n            warm_start (str): whether try to warm start (`w0`/`benchmark`/``)\n                              (https://www.cvxpy.org/tutorial/advanced/index.html#warm-start)\n        \"\"\"\n\n        assert lamb >= 0, \"risk aversion parameter `lamb` should be positive\"\n        self.lamb = lamb\n\n        assert delta >= 0, \"turnover limit `delta` should be positive\"\n        self.delta = delta\n\n        assert bench_dev >= 0, \"benchmark deviation limit `bench_dev` should be positive\"\n        self.bench_dev = bench_dev\n\n        assert inds_dev is None or inds_dev >= 0, \"industry deviation limit `inds_dev` should be positive or None.\"\n        self.inds_dev = inds_dev\n\n        assert warm_start in [\n            None,\n            self.START_FROM_W0,\n            self.START_FROM_BENCH,\n        ], \"illegal warm start option\"\n        self.start_from_w0 = warm_start == self.START_FROM_W0\n        self.start_from_bench = warm_start == self.START_FROM_BENCH\n\n        self.scale_alpha = scale_alpha\n        self.verbose = verbose\n        self.max_iters = max_iters",
  "def __call__(\n        self,\n        u: Union[np.ndarray, pd.Series],\n        F: np.ndarray,\n        covB: np.ndarray,\n        varU: np.ndarray,\n        w0: np.ndarray,\n        w_bench: np.ndarray,\n        inds_onehot: np.ndarray = None,\n    ) -> Union[np.ndarray, pd.Series]:\n        \"\"\"\n        Args:\n            u (np.ndarray or pd.Series): expected returns (a.k.a., alpha)\n            F, covB, varU (np.ndarray): see StructuredCovEstimator\n            w0 (np.ndarray): initial weights (for turnover control)\n            w_bench (np.ndarray): benchmark weights\n            inds_onehot (np.ndarray): industry (onehot)\n\n        Returns:\n            np.ndarray or pd.Series: optimized portfolio allocation\n        \"\"\"\n        assert inds_onehot is not None or self.inds_dev is None, \"Industry onehot vector is required.\"\n\n        # transform dataframe into array\n        if isinstance(u, pd.Series):\n            u = u.values\n\n        # scale alpha to match volatility\n        if self.scale_alpha:\n            u = u / u.std()\n            x_variance = np.mean(np.diag(F @ covB @ F.T) + varU)\n            u *= x_variance ** 0.5\n\n        w = cp.Variable(len(u))  # num_assets\n        v = w @ F  # num_factors\n        ret = w @ u\n        risk = cp.quad_form(v, covB) + cp.sum(cp.multiply(varU, w ** 2))\n        obj = cp.Maximize(ret - self.lamb * risk)\n        d_bench = w - w_bench\n        cons = [\n            w >= 0,\n            cp.sum(w) == 1,\n            d_bench >= -self.bench_dev,\n            d_bench <= self.bench_dev,\n        ]\n\n        if self.inds_dev is not None:\n            d_inds = d_bench @ inds_onehot\n            cons.append(d_inds >= -self.inds_dev)\n            cons.append(d_inds <= self.inds_dev)\n\n        if w0 is not None:\n            turnover = cp.sum(cp.abs(w - w0))\n            cons.append(turnover <= self.delta)\n\n        warm_start = False\n        if self.start_from_w0:\n            if w0 is None:\n                print(\"Warning: try warm start with w0, but w0 is `None`.\")\n            else:\n                w.value = w0\n                warm_start = True\n        elif self.start_from_bench:\n            w.value = w_bench\n            warm_start = True\n\n        prob = cp.Problem(obj, cons)\n        prob.solve(solver=cp.SCS, verbose=self.verbose, warm_start=warm_start, max_iters=self.max_iters)\n\n        if prob.status != \"optimal\":\n            print(\"Warning: solve failed.\", prob.status)\n\n        return np.asarray(w.value)",
  "def datetime_groupby_apply(df, apply_func, axis=0, level=\"datetime\", resample_rule=\"M\", n_jobs=-1, skip_group=False):\n    \"\"\"datetime_groupby_apply\n    This function will apply the `apply_func` on the datetime level index.\n\n    Parameters\n    ----------\n    df :\n        DataFrame for processing\n    apply_func :\n        apply_func for processing the data\n    axis :\n        which axis is the datetime level located\n    level :\n        which level is the datetime level\n    resample_rule :\n        How to resample the data to calculating parallel\n    n_jobs :\n        n_jobs for joblib\n    Returns:\n        pd.DataFrame\n    \"\"\"\n\n    def _naive_group_apply(df):\n        return df.groupby(axis=axis, level=level).apply(apply_func)\n\n    if n_jobs != 1:\n        dfs = Parallel(n_jobs=n_jobs)(\n            delayed(_naive_group_apply)(sub_df) for idx, sub_df in df.resample(resample_rule, axis=axis, level=level)\n        )\n        return pd.concat(dfs, axis=axis).sort_index()\n    else:\n        return _naive_group_apply(df)",
  "def _naive_group_apply(df):\n        return df.groupby(axis=axis, level=level).apply(apply_func)",
  "class Serializable:\n    \"\"\"\n    Serializable behaves like pickle.\n    But it only saves the state whose name **does not** start with `_`\n    \"\"\"\n\n    def __init__(self):\n        self._dump_all = False\n        self._exclude = []\n\n    def __getstate__(self) -> dict:\n        return {\n            k: v for k, v in self.__dict__.items() if k not in self.exclude and (self.dump_all or not k.startswith(\"_\"))\n        }\n\n    def __setstate__(self, state: dict):\n        self.__dict__.update(state)\n\n    @property\n    def dump_all(self):\n        \"\"\"\n        will the object dump all object\n        \"\"\"\n        return getattr(self, \"_dump_all\", False)\n\n    @property\n    def exclude(self):\n        \"\"\"\n        What attribute will be dumped\n        \"\"\"\n        return getattr(self, \"_exclude\", [])\n\n    def config(self, dump_all: bool = None, exclude: list = None):\n        if dump_all is not None:\n            self._dump_all = dump_all\n\n        if exclude is not None:\n            self._exclude = exclude\n\n    def to_pickle(self, path: [Path, str], dump_all: bool = None, exclude: list = None):\n        self.config(dump_all=dump_all, exclude=exclude)\n        with Path(path).open(\"wb\") as f:\n            pickle.dump(self, f)",
  "def __init__(self):\n        self._dump_all = False\n        self._exclude = []",
  "def __getstate__(self) -> dict:\n        return {\n            k: v for k, v in self.__dict__.items() if k not in self.exclude and (self.dump_all or not k.startswith(\"_\"))\n        }",
  "def __setstate__(self, state: dict):\n        self.__dict__.update(state)",
  "def dump_all(self):\n        \"\"\"\n        will the object dump all object\n        \"\"\"\n        return getattr(self, \"_dump_all\", False)",
  "def exclude(self):\n        \"\"\"\n        What attribute will be dumped\n        \"\"\"\n        return getattr(self, \"_exclude\", [])",
  "def config(self, dump_all: bool = None, exclude: list = None):\n        if dump_all is not None:\n            self._dump_all = dump_all\n\n        if exclude is not None:\n            self._exclude = exclude",
  "def to_pickle(self, path: [Path, str], dump_all: bool = None, exclude: list = None):\n        self.config(dump_all=dump_all, exclude=exclude)\n        with Path(path).open(\"wb\") as f:\n            pickle.dump(self, f)",
  "class ObjManager:\n    def save_obj(self, obj: object, name: str):\n        \"\"\"\n        save obj as name\n\n        Parameters\n        ----------\n        obj : object\n            object to be saved\n        name : str\n            name of the object\n        \"\"\"\n        raise NotImplementedError(f\"Please implement `save_obj`\")\n\n    def save_objs(self, obj_name_l):\n        \"\"\"\n        save objects\n\n        Parameters\n        ----------\n        obj_name_l : list of <obj, name>\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `save_objs` method\")\n\n    def load_obj(self, name: str) -> object:\n        \"\"\"\n        load object by name\n\n        Parameters\n        ----------\n        name : str\n            the name of the object\n\n        Returns\n        -------\n        object:\n            loaded object\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `load_obj` method\")\n\n    def exists(self, name: str) -> bool:\n        \"\"\"\n        if the object named `name` exists\n\n        Parameters\n        ----------\n        name : str\n            name of the objecT\n\n        Returns\n        -------\n        bool:\n            If the object exists\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `exists` method\")\n\n    def list(self) -> list:\n        \"\"\"\n        list the objects\n\n        Returns\n        -------\n        list:\n            the list of returned objects\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list` method\")\n\n    def remove(self, fname=None):\n        \"\"\"remove.\n\n        Parameters\n        ----------\n        fname :\n            if file name is provided. specific file is removed\n            otherwise, The all the objects will be removed.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `remove` method\")",
  "class FileManager(ObjManager):\n    \"\"\"\n    Use file system to manage objects\n    \"\"\"\n\n    def __init__(self, path=None):\n        if path is None:\n            self.path = Path(self.create_path())\n        else:\n            self.path = Path(path).resolve()\n\n    def create_path(self) -> str:\n        try:\n            return tempfile.mkdtemp(prefix=str(C[\"file_manager_path\"]) + os.sep)\n        except AttributeError:\n            raise NotImplementedError(f\"If path is not given, the `create_path` function should be implemented\")\n\n    def save_obj(self, obj, name):\n        with (self.path / name).open(\"wb\") as f:\n            pickle.dump(obj, f)\n\n    def save_objs(self, obj_name_l):\n        for obj, name in obj_name_l:\n            self.save_obj(obj, name)\n\n    def load_obj(self, name):\n        with (self.path / name).open(\"rb\") as f:\n            return pickle.load(f)\n\n    def exists(self, name):\n        return (self.path / name).exists()\n\n    def list(self):\n        return list(self.path.iterdir())\n\n    def remove(self, fname=None):\n        if fname is None:\n            for fp in self.path.glob(\"*\"):\n                fp.unlink()\n            self.path.rmdir()\n        else:\n            (self.path / fname).unlink()",
  "def save_obj(self, obj: object, name: str):\n        \"\"\"\n        save obj as name\n\n        Parameters\n        ----------\n        obj : object\n            object to be saved\n        name : str\n            name of the object\n        \"\"\"\n        raise NotImplementedError(f\"Please implement `save_obj`\")",
  "def save_objs(self, obj_name_l):\n        \"\"\"\n        save objects\n\n        Parameters\n        ----------\n        obj_name_l : list of <obj, name>\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `save_objs` method\")",
  "def load_obj(self, name: str) -> object:\n        \"\"\"\n        load object by name\n\n        Parameters\n        ----------\n        name : str\n            the name of the object\n\n        Returns\n        -------\n        object:\n            loaded object\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `load_obj` method\")",
  "def exists(self, name: str) -> bool:\n        \"\"\"\n        if the object named `name` exists\n\n        Parameters\n        ----------\n        name : str\n            name of the objecT\n\n        Returns\n        -------\n        bool:\n            If the object exists\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `exists` method\")",
  "def list(self) -> list:\n        \"\"\"\n        list the objects\n\n        Returns\n        -------\n        list:\n            the list of returned objects\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `list` method\")",
  "def remove(self, fname=None):\n        \"\"\"remove.\n\n        Parameters\n        ----------\n        fname :\n            if file name is provided. specific file is removed\n            otherwise, The all the objects will be removed.\n        \"\"\"\n        raise NotImplementedError(f\"Please implement the `remove` method\")",
  "def __init__(self, path=None):\n        if path is None:\n            self.path = Path(self.create_path())\n        else:\n            self.path = Path(path).resolve()",
  "def create_path(self) -> str:\n        try:\n            return tempfile.mkdtemp(prefix=str(C[\"file_manager_path\"]) + os.sep)\n        except AttributeError:\n            raise NotImplementedError(f\"If path is not given, the `create_path` function should be implemented\")",
  "def save_obj(self, obj, name):\n        with (self.path / name).open(\"wb\") as f:\n            pickle.dump(obj, f)",
  "def save_objs(self, obj_name_l):\n        for obj, name in obj_name_l:\n            self.save_obj(obj, name)",
  "def load_obj(self, name):\n        with (self.path / name).open(\"rb\") as f:\n            return pickle.load(f)",
  "def exists(self, name):\n        return (self.path / name).exists()",
  "def list(self):\n        return list(self.path.iterdir())",
  "def remove(self, fname=None):\n        if fname is None:\n            for fp in self.path.glob(\"*\"):\n                fp.unlink()\n            self.path.rmdir()\n        else:\n            (self.path / fname).unlink()",
  "def get_redis_connection():\n    \"\"\"get redis connection instance.\"\"\"\n    return redis.StrictRedis(host=C.redis_host, port=C.redis_port, db=C.redis_task_db)",
  "def read_bin(file_path, start_index, end_index):\n    with open(file_path, \"rb\") as f:\n        # read start_index\n        ref_start_index = int(np.frombuffer(f.read(4), dtype=\"<f\")[0])\n        si = max(ref_start_index, start_index)\n        if si > end_index:\n            return pd.Series(dtype=np.float32)\n        # calculate offset\n        f.seek(4 * (si - ref_start_index) + 4)\n        # read nbytes\n        count = end_index - si + 1\n        data = np.frombuffer(f.read(4 * count), dtype=\"<f\")\n        series = pd.Series(data, index=pd.RangeIndex(si, si + len(data)))\n    return series",
  "def np_ffill(arr: np.array):\n    \"\"\"\n    forward fill a 1D numpy array\n\n    Parameters\n    ----------\n    arr : np.array\n        Input numpy 1D array\n    \"\"\"\n    mask = np.isnan(arr.astype(float))  # np.isnan only works on np.float\n    # get fill index\n    idx = np.where(~mask, np.arange(mask.shape[0]), 0)\n    np.maximum.accumulate(idx, out=idx)\n    return arr[idx]",
  "def lower_bound(data, val, level=0):\n    \"\"\"multi fields list lower bound.\n\n    for single field list use `bisect.bisect_left` instead\n    \"\"\"\n    left = 0\n    right = len(data)\n    while left < right:\n        mid = (left + right) // 2\n        if val <= data[mid][level]:\n            right = mid\n        else:\n            left = mid + 1\n    return left",
  "def upper_bound(data, val, level=0):\n    \"\"\"multi fields list upper bound.\n\n    for single field list use `bisect.bisect_right` instead\n    \"\"\"\n    left = 0\n    right = len(data)\n    while left < right:\n        mid = (left + right) // 2\n        if val >= data[mid][level]:\n            left = mid + 1\n        else:\n            right = mid\n    return left",
  "def requests_with_retry(url, retry=5, **kwargs):\n    while retry > 0:\n        retry -= 1\n        try:\n            res = requests.get(url, timeout=1, **kwargs)\n            assert res.status_code in {200, 206}\n            return res\n        except AssertionError:\n            continue\n        except Exception as e:\n            log.warning(\"exception encountered {}\".format(e))\n            continue\n    raise Exception(\"ERROR: requests failed!\")",
  "def parse_config(config):\n    # Check whether need parse, all object except str do not need to be parsed\n    if not isinstance(config, str):\n        return config\n    # Check whether config is file\n    if os.path.exists(config):\n        with open(config, \"r\") as f:\n            return yaml.safe_load(f)\n    # Check whether the str can be parsed\n    try:\n        return yaml.safe_load(config)\n    except BaseException:\n        raise ValueError(\"cannot parse config!\")",
  "def drop_nan_by_y_index(x, y, weight=None):\n    # x, y, weight: DataFrame\n    # Find index of rows which do not contain Nan in all columns from y.\n    mask = ~y.isna().any(axis=1)\n    # Get related rows from x, y, weight.\n    x = x[mask]\n    y = y[mask]\n    if weight is not None:\n        weight = weight[mask]\n    return x, y, weight",
  "def hash_args(*args):\n    # json.dumps will keep the dict keys always sorted.\n    string = json.dumps(args, sort_keys=True, default=str)  # frozenset\n    return hashlib.md5(string.encode()).hexdigest()",
  "def parse_field(field):\n    # Following patterns will be matched:\n    # - $close -> Feature(\"close\")\n    # - $close5 -> Feature(\"close5\")\n    # - $open+$close -> Feature(\"open\")+Feature(\"close\")\n    if not isinstance(field, str):\n        field = str(field)\n    return re.sub(r\"\\$(\\w+)\", r'Feature(\"\\1\")', re.sub(r\"(\\w+\\s*)\\(\", r\"Operators.\\1(\", field))",
  "def get_module_by_module_path(module_path):\n    \"\"\"Load module path\n\n    :param module_path:\n    :return:\n    \"\"\"\n\n    if module_path.endswith(\".py\"):\n        module_spec = importlib.util.spec_from_file_location(\"\", module_path)\n        module = importlib.util.module_from_spec(module_spec)\n        module_spec.loader.exec_module(module)\n    else:\n        module = importlib.import_module(module_path)\n\n    return module",
  "def get_cls_kwargs(config: Union[dict, str], module) -> (type, dict):\n    \"\"\"\n    extract class and kwargs from config info\n\n    Parameters\n    ----------\n    config : [dict, str]\n        similar to config\n\n    module : Python module\n        It should be a python module to load the class type\n\n    Returns\n    -------\n    (type, dict):\n        the class object and it's arguments.\n    \"\"\"\n    if isinstance(config, dict):\n        # raise AttributeError\n        klass = getattr(module, config[\"class\"])\n        kwargs = config.get(\"kwargs\", {})\n    elif isinstance(config, str):\n        klass = getattr(module, config)\n        kwargs = {}\n    else:\n        raise NotImplementedError(f\"This type of input is not supported\")\n    return klass, kwargs",
  "def init_instance_by_config(\n    config: Union[str, dict, object], module=None, accept_types: Union[type, Tuple[type]] = (), **kwargs\n) -> object:\n    \"\"\"\n    get initialized instance with config\n\n    Parameters\n    ----------\n    config : Union[str, dict, object]\n        dict example.\n            {\n                'class': 'ClassName',\n                'kwargs': dict, #  It is optional. {} will be used if not given\n                'model_path': path, # It is optional if module is given\n            }\n        str example.\n            \"ClassName\":  getattr(module, config)() will be used.\n        object example:\n            instance of accept_types\n    module : Python module\n        Optional. It should be a python module.\n        NOTE: the \"module_path\" will be override by `module` arguments\n\n    accept_types: Union[type, Tuple[type]]\n        Optional. If the config is a instance of specific type, return the config directly.\n        This will be passed into the second parameter of isinstance.\n\n    Returns\n    -------\n    object:\n        An initialized object based on the config info\n    \"\"\"\n    if isinstance(config, accept_types):\n        return config\n\n    if module is None:\n        module = get_module_by_module_path(config[\"module_path\"])\n\n    klass, cls_kwargs = get_cls_kwargs(config, module)\n    return klass(**cls_kwargs, **kwargs)",
  "def compare_dict_value(src_data: dict, dst_data: dict):\n    \"\"\"Compare dict value\n\n    :param src_data:\n    :param dst_data:\n    :return:\n    \"\"\"\n\n    class DateEncoder(json.JSONEncoder):\n        # FIXME: This class can only be accurate to the day. If it is a minute,\n        # there may be a bug\n        def default(self, o):\n            if isinstance(o, (datetime.datetime, datetime.date)):\n                return o.strftime(\"%Y-%m-%d %H:%M:%S\")\n            return json.JSONEncoder.default(self, o)\n\n    src_data = json.dumps(src_data, indent=4, sort_keys=True, cls=DateEncoder)\n    dst_data = json.dumps(dst_data, indent=4, sort_keys=True, cls=DateEncoder)\n    diff = difflib.ndiff(src_data, dst_data)\n    changes = [line for line in diff if line.startswith(\"+ \") or line.startswith(\"- \")]\n    return changes",
  "def get_or_create_path(path: Optional[Text] = None, return_dir: bool = False):\n    \"\"\"Create or get a file or directory given the path and return_dir.\n\n    Parameters\n    ----------\n    path: a string indicates the path or None indicates creating a temporary path.\n    return_dir: if True, create and return a directory; otherwise c&r a file.\n\n    \"\"\"\n    if path:\n        if return_dir and not os.path.exists(path):\n            os.makedirs(path)\n        elif not return_dir:  # return a file, thus we need to create its parent directory\n            xpath = os.path.abspath(os.path.join(path, \"..\"))\n            if not os.path.exists(xpath):\n                os.makedirs(xpath)\n    else:\n        temp_dir = os.path.expanduser(\"~/tmp\")\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir)\n        if return_dir:\n            _, path = tempfile.mkdtemp(dir=temp_dir)\n        else:\n            _, path = tempfile.mkstemp(dir=temp_dir)\n    return path",
  "def save_multiple_parts_file(filename, format=\"gztar\"):\n    \"\"\"Save multiple parts file\n\n    Implementation process:\n        1. get the absolute path to 'filename'\n        2. create a 'filename' directory\n        3. user does something with file_path('filename/')\n        4. remove 'filename' directory\n        5. make_archive 'filename' directory, and rename 'archive file' to filename\n\n    :param filename: result model path\n    :param format: archive format: one of \"zip\", \"tar\", \"gztar\", \"bztar\", or \"xztar\"\n    :return: real model path\n\n    Usage::\n\n        >>> # The following code will create an archive file('~/tmp/test_file') containing 'test_doc_i'(i is 0-10) files.\n        >>> with save_multiple_parts_file('~/tmp/test_file') as filename_dir:\n        ...   for i in range(10):\n        ...       temp_path = os.path.join(filename_dir, 'test_doc_{}'.format(str(i)))\n        ...       with open(temp_path) as fp:\n        ...           fp.write(str(i))\n        ...\n\n    \"\"\"\n\n    if filename.startswith(\"~\"):\n        filename = os.path.expanduser(filename)\n\n    file_path = os.path.abspath(filename)\n\n    # Create model dir\n    if os.path.exists(file_path):\n        raise FileExistsError(\"ERROR: file exists: {}, cannot be create the directory.\".format(file_path))\n\n    os.makedirs(file_path)\n\n    # return model dir\n    yield file_path\n\n    # filename dir to filename.tar.gz file\n    tar_file = shutil.make_archive(file_path, format=format, root_dir=file_path)\n\n    # Remove filename dir\n    if os.path.exists(file_path):\n        shutil.rmtree(file_path)\n\n    # filename.tar.gz rename to filename\n    os.rename(tar_file, file_path)",
  "def unpack_archive_with_buffer(buffer, format=\"gztar\"):\n    \"\"\"Unpack archive with archive buffer\n    After the call is finished, the archive file and directory will be deleted.\n\n    Implementation process:\n        1. create 'tempfile' in '~/tmp/' and directory\n        2. 'buffer' write to 'tempfile'\n        3. unpack archive file('tempfile')\n        4. user does something with file_path('tempfile/')\n        5. remove 'tempfile' and 'tempfile directory'\n\n    :param buffer: bytes\n    :param format: archive format: one of \"zip\", \"tar\", \"gztar\", \"bztar\", or \"xztar\"\n    :return: unpack archive directory path\n\n    Usage::\n\n        >>> # The following code is to print all the file names in 'test_unpack.tar.gz'\n        >>> with open('test_unpack.tar.gz') as fp:\n        ...     buffer = fp.read()\n        ...\n        >>> with unpack_archive_with_buffer(buffer) as temp_dir:\n        ...     for f_n in os.listdir(temp_dir):\n        ...         print(f_n)\n        ...\n\n    \"\"\"\n    temp_dir = os.path.expanduser(\"~/tmp\")\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    with tempfile.NamedTemporaryFile(\"wb\", delete=False, dir=temp_dir) as fp:\n        fp.write(buffer)\n        file_path = fp.name\n\n    try:\n        tar_file = file_path + \".tar.gz\"\n        os.rename(file_path, tar_file)\n        # Create dir\n        os.makedirs(file_path)\n        shutil.unpack_archive(tar_file, format=format, extract_dir=file_path)\n\n        # Return temp dir\n        yield file_path\n\n    except Exception as e:\n        log.error(str(e))\n    finally:\n        # Remove temp tar file\n        if os.path.exists(tar_file):\n            os.unlink(tar_file)\n\n        # Remove temp model dir\n        if os.path.exists(file_path):\n            shutil.rmtree(file_path)",
  "def get_tmp_file_with_buffer(buffer):\n    temp_dir = os.path.expanduser(\"~/tmp\")\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n    with tempfile.NamedTemporaryFile(\"wb\", delete=True, dir=temp_dir) as fp:\n        fp.write(buffer)\n        file_path = fp.name\n        yield file_path",
  "def remove_repeat_field(fields):\n    \"\"\"remove repeat field\n\n    :param fields: list; features fields\n    :return: list\n    \"\"\"\n    fields = copy.deepcopy(fields)\n    _fields = set(fields)\n    return sorted(_fields, key=fields.index)",
  "def remove_fields_space(fields: [list, str, tuple]):\n    \"\"\"remove fields space\n\n    :param fields: features fields\n    :return: list or str\n    \"\"\"\n    if isinstance(fields, str):\n        return fields.replace(\" \", \"\")\n    return [i.replace(\" \", \"\") for i in fields if isinstance(i, str)]",
  "def normalize_cache_fields(fields: [list, tuple]):\n    \"\"\"normalize cache fields\n\n    :param fields: features fields\n    :return: list\n    \"\"\"\n    return sorted(remove_repeat_field(remove_fields_space(fields)))",
  "def normalize_cache_instruments(instruments):\n    \"\"\"normalize cache instruments\n\n    :return: list or dict\n    \"\"\"\n    if isinstance(instruments, (list, tuple, pd.Index, np.ndarray)):\n        instruments = sorted(list(instruments))\n    else:\n        # dict type stockpool\n        if \"market\" in instruments:\n            pass\n        else:\n            instruments = {k: sorted(v) for k, v in instruments.items()}\n    return instruments",
  "def is_tradable_date(cur_date):\n    \"\"\"judgy whether date is a tradable date\n    ----------\n    date : pandas.Timestamp\n        current date\n    \"\"\"\n    from ..data import D\n\n    return str(cur_date.date()) == str(D.calendar(start_time=cur_date, future=True)[0].date())",
  "def get_date_range(trading_date, left_shift=0, right_shift=0, future=False):\n    \"\"\"get trading date range by shift\n\n    Parameters\n    ----------\n    trading_date: pd.Timestamp\n    left_shift: int\n    right_shift: int\n    future: bool\n\n    \"\"\"\n\n    from ..data import D\n\n    start = get_date_by_shift(trading_date, left_shift, future=future)\n    end = get_date_by_shift(trading_date, right_shift, future=future)\n\n    calendar = D.calendar(start, end, future=future)\n    return calendar",
  "def get_date_by_shift(trading_date, shift, future=False, clip_shift=True):\n    \"\"\"get trading date with shift bias wil cur_date\n        e.g. : shift == 1,  return next trading date\n               shift == -1, return previous trading date\n    ----------\n    trading_date : pandas.Timestamp\n        current date\n    shift : int\n    clip_shift: bool\n\n    \"\"\"\n    from qlib.data import D\n\n    cal = D.calendar(future=future)\n    if pd.to_datetime(trading_date) not in list(cal):\n        raise ValueError(\"{} is not trading day!\".format(str(trading_date)))\n    _index = bisect.bisect_left(cal, trading_date)\n    shift_index = _index + shift\n    if shift_index < 0 or shift_index >= len(cal):\n        if clip_shift:\n            shift_index = np.clip(shift_index, 0, len(cal) - 1)\n        else:\n            raise IndexError(f\"The shift_index({shift_index}) of the trading day ({trading_date}) is out of range\")\n    return cal[shift_index]",
  "def get_next_trading_date(trading_date, future=False):\n    \"\"\"get next trading date\n    ----------\n    cur_date : pandas.Timestamp\n        current date\n    \"\"\"\n    return get_date_by_shift(trading_date, 1, future=future)",
  "def get_pre_trading_date(trading_date, future=False):\n    \"\"\"get previous trading date\n    ----------\n    date : pandas.Timestamp\n        current date\n    \"\"\"\n    return get_date_by_shift(trading_date, -1, future=future)",
  "def transform_end_date(end_date=None, freq=\"day\"):\n    \"\"\"get previous trading date\n    If end_date is -1, None, or end_date is greater than the maximum trading day, the last trading date is returned.\n    Otherwise, returns the end_date\n    ----------\n    end_date: str\n        end trading date\n    date : pandas.Timestamp\n        current date\n    \"\"\"\n    from ..data import D\n\n    last_date = D.calendar(freq=freq)[-1]\n    if end_date is None or (str(end_date) == \"-1\") or (pd.Timestamp(last_date) < pd.Timestamp(end_date)):\n        log.warning(\n            \"\\nInfo: the end_date in the configuration file is {}, \"\n            \"so the default last date {} is used.\".format(end_date, last_date)\n        )\n        end_date = last_date\n    return end_date",
  "def get_date_in_file_name(file_name):\n    \"\"\"Get the date(YYYY-MM-DD) written in file name\n    Parameter\n            file_name : str\n       :return\n            date : str\n                'YYYY-MM-DD'\n    \"\"\"\n    pattern = \"[0-9]{4}-[0-9]{2}-[0-9]{2}\"\n    date = re.search(pattern, str(file_name)).group()\n    return date",
  "def split_pred(pred, number=None, split_date=None):\n    \"\"\"split the score file into two part\n    Parameter\n    ---------\n        pred : pd.DataFrame (index:<instrument, datetime>)\n            A score file of stocks\n        number: the number of dates for pred_left\n        split_date: the last date of the pred_left\n    Return\n    -------\n        pred_left : pd.DataFrame (index:<instrument, datetime>)\n            The first part of original score file\n        pred_right : pd.DataFrame (index:<instrument, datetime>)\n            The second part of original score file\n    \"\"\"\n    if number is None and split_date is None:\n        raise ValueError(\"`number` and `split date` cannot both be None\")\n    dates = sorted(pred.index.get_level_values(\"datetime\").unique())\n    dates = list(map(pd.Timestamp, dates))\n    if split_date is None:\n        date_left_end = dates[number - 1]\n        date_right_begin = dates[number]\n        date_left_start = None\n    else:\n        split_date = pd.Timestamp(split_date)\n        date_left_end = split_date\n        date_right_begin = split_date + pd.Timedelta(days=1)\n        if number is None:\n            date_left_start = None\n        else:\n            end_idx = bisect.bisect_right(dates, split_date)\n            date_left_start = dates[end_idx - number]\n    pred_temp = pred.sort_index()\n    pred_left = pred_temp.loc(axis=0)[:, date_left_start:date_left_end]\n    pred_right = pred_temp.loc(axis=0)[:, date_right_begin:]\n    return pred_left, pred_right",
  "def can_use_cache():\n    res = True\n    r = get_redis_connection()\n    try:\n        r.client()\n    except redis.exceptions.ConnectionError:\n        res = False\n    finally:\n        r.close()\n    return res",
  "def exists_qlib_data(qlib_dir):\n    qlib_dir = Path(qlib_dir).expanduser()\n    if not qlib_dir.exists():\n        return False\n\n    calendars_dir = qlib_dir.joinpath(\"calendars\")\n    instruments_dir = qlib_dir.joinpath(\"instruments\")\n    features_dir = qlib_dir.joinpath(\"features\")\n    # check dir\n    for _dir in [calendars_dir, instruments_dir, features_dir]:\n        if not (_dir.exists() and list(_dir.iterdir())):\n            return False\n    # check calendar bin\n    for _calendar in calendars_dir.iterdir():\n        if not list(features_dir.rglob(f\"*.{_calendar.name.split('.')[0]}.bin\")):\n            return False\n\n    # check instruments\n    code_names = set(map(lambda x: x.name.lower(), features_dir.iterdir()))\n    _instrument = instruments_dir.joinpath(\"all.txt\")\n    miss_code = set(pd.read_csv(_instrument, sep=\"\\t\", header=None).loc[:, 0].apply(str.lower)) - set(code_names)\n    if miss_code and any(map(lambda x: \"sht\" not in x, miss_code)):\n        return False\n\n    return True",
  "def check_qlib_data(qlib_config):\n    inst_dir = Path(qlib_config[\"provider_uri\"]).joinpath(\"instruments\")\n    for _p in inst_dir.glob(\"*.txt\"):\n        try:\n            assert len(pd.read_csv(_p, sep=\"\\t\", nrows=0, header=None).columns) == 3, (\n                f\"\\nThe {str(_p.resolve())} of qlib data is not equal to 3 columns:\"\n                f\"\\n\\tIf you are using the data provided by qlib: \"\n                f\"https://qlib.readthedocs.io/en/latest/component/data.html#qlib-format-dataset\"\n                f\"\\n\\tIf you are using your own data, please dump the data again: \"\n                f\"https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format\"\n            )\n        except AssertionError:\n            raise",
  "def lazy_sort_index(df: pd.DataFrame, axis=0) -> pd.DataFrame:\n    \"\"\"\n    make the df index sorted\n\n    df.sort_index() will take a lot of time even when `df.is_lexsorted() == True`\n    This function could avoid such case\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n\n    Returns\n    -------\n    pd.DataFrame:\n        sorted dataframe\n    \"\"\"\n    idx = df.index if axis == 0 else df.columns\n    if idx.is_monotonic_increasing:\n        return df\n    else:\n        return df.sort_index(axis=axis)",
  "def flatten_dict(d, parent_key=\"\", sep=\".\"):\n    \"\"\"flatten_dict.\n        >>> flatten_dict({'a': 1, 'c': {'a': 2, 'b': {'x': 5, 'y' : 10}}, 'd': [1, 2, 3]})\n        >>> {'a': 1, 'c.a': 2, 'c.b.x': 5, 'd': [1, 2, 3], 'c.b.y': 10}\n\n    Parameters\n    ----------\n    d :\n        d\n    parent_key :\n        parent_key\n    sep :\n        sep\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)",
  "class Wrapper:\n    \"\"\"Wrapper class for anything that needs to set up during qlib.init\"\"\"\n\n    def __init__(self):\n        self._provider = None\n\n    def register(self, provider):\n        self._provider = provider\n\n    def __repr__(self):\n        return \"{name}(provider={provider})\".format(name=self.__class__.__name__, provider=self._provider)\n\n    def __getattr__(self, key):\n        if self._provider is None:\n            raise AttributeError(\"Please run qlib.init() first using qlib\")\n        return getattr(self._provider, key)",
  "def register_wrapper(wrapper, cls_or_obj, module_path=None):\n    \"\"\"register_wrapper\n\n    :param wrapper: A wrapper.\n    :param cls_or_obj:  A class or class name or object instance.\n    \"\"\"\n    if isinstance(cls_or_obj, str):\n        module = get_module_by_module_path(module_path)\n        cls_or_obj = getattr(module, cls_or_obj)\n    obj = cls_or_obj() if isinstance(cls_or_obj, type) else cls_or_obj\n    wrapper.register(obj)",
  "def load_dataset(path_or_obj):\n    \"\"\"load dataset from multiple file formats\"\"\"\n    if isinstance(path_or_obj, pd.DataFrame):\n        return path_or_obj\n    if not os.path.exists(path_or_obj):\n        raise ValueError(f\"file {path_or_obj} doesn't exist\")\n    _, extension = os.path.splitext(path_or_obj)\n    if extension == \".h5\":\n        return pd.read_hdf(path_or_obj)\n    elif extension == \".pkl\":\n        return pd.read_pickle(path_or_obj)\n    elif extension == \".csv\":\n        return pd.read_csv(path_or_obj, parse_dates=True, index_col=[0, 1])\n    raise ValueError(f\"unsupported file type `{extension}`\")",
  "def code_to_fname(code: str):\n    \"\"\"stock code to file name\n\n    Parameters\n    ----------\n    code: str\n    \"\"\"\n    # NOTE: In windows, the following name is I/O device, and the file with the corresponding name cannot be created\n    # reference: https://superuser.com/questions/86999/why-cant-i-name-a-folder-or-file-con-in-windows\n    replace_names = [\"CON\", \"PRN\", \"AUX\", \"NUL\"]\n    replace_names += [f\"COM{i}\" for i in range(10)]\n    replace_names += [f\"LPT{i}\" for i in range(10)]\n\n    prefix = \"_qlib_\"\n    if str(code).upper() in replace_names:\n        code = prefix + str(code)\n\n    return code",
  "def fname_to_code(fname: str):\n    \"\"\"file name to stock code\n\n    Parameters\n    ----------\n    fname: str\n    \"\"\"\n    prefix = \"_qlib_\"\n    if fname.startswith(prefix):\n        fname = fname.lstrip(prefix)\n    return fname",
  "class DateEncoder(json.JSONEncoder):\n        # FIXME: This class can only be accurate to the day. If it is a minute,\n        # there may be a bug\n        def default(self, o):\n            if isinstance(o, (datetime.datetime, datetime.date)):\n                return o.strftime(\"%Y-%m-%d %H:%M:%S\")\n            return json.JSONEncoder.default(self, o)",
  "def __init__(self):\n        self._provider = None",
  "def register(self, provider):\n        self._provider = provider",
  "def __repr__(self):\n        return \"{name}(provider={provider})\".format(name=self.__class__.__name__, provider=self._provider)",
  "def __getattr__(self, key):\n        if self._provider is None:\n            raise AttributeError(\"Please run qlib.init() first using qlib\")\n        return getattr(self._provider, key)",
  "def default(self, o):\n            if isinstance(o, (datetime.datetime, datetime.date)):\n                return o.strftime(\"%Y-%m-%d %H:%M:%S\")\n            return json.JSONEncoder.default(self, o)",
  "class TaskGen(metaclass=abc.ABCMeta):\n    @abc.abstractmethod\n    def __call__(self, *args, **kwargs) -> typing.List[dict]:\n        \"\"\"\n        generate\n\n        Parameters\n        ----------\n        args, kwargs:\n            The info for generating tasks\n            Example 1):\n                input: a specific task template\n                output: rolling version of the tasks\n            Example 2):\n                input: a specific task template\n                output: a set of tasks with different losses\n\n        Returns\n        -------\n        typing.List[dict]:\n            A list of tasks\n        \"\"\"\n        pass",
  "def __call__(self, *args, **kwargs) -> typing.List[dict]:\n        \"\"\"\n        generate\n\n        Parameters\n        ----------\n        args, kwargs:\n            The info for generating tasks\n            Example 1):\n                input: a specific task template\n                output: rolling version of the tasks\n            Example 2):\n                input: a specific task template\n                output: a set of tasks with different losses\n\n        Returns\n        -------\n        typing.List[dict]:\n            A list of tasks\n        \"\"\"\n        pass",
  "class BaseModel(Serializable, metaclass=abc.ABCMeta):\n    \"\"\"Modeling things\"\"\"\n\n    @abc.abstractmethod\n    def predict(self, *args, **kwargs) -> object:\n        \"\"\" Make predictions after modeling things \"\"\"\n        pass\n\n    def __call__(self, *args, **kwargs) -> object:\n        \"\"\" leverage Python syntactic sugar to make the models' behaviors like functions \"\"\"\n        return self.predict(*args, **kwargs)",
  "class Model(BaseModel):\n    \"\"\"Learnable Models\"\"\"\n\n    def fit(self, dataset: Dataset):\n        \"\"\"\n        Learn model from the base model\n\n        .. note::\n\n            The attribute names of learned model should `not` start with '_'. So that the model could be\n            dumped to disk.\n\n        The following code example shows how to retrieve `x_train`, `y_train` and `w_train` from the `dataset`:\n\n            .. code-block:: Python\n\n                # get features and labels\n                df_train, df_valid = dataset.prepare(\n                    [\"train\", \"valid\"], col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L\n                )\n                x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n                x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n                # get weights\n                try:\n                    wdf_train, wdf_valid = dataset.prepare([\"train\", \"valid\"], col_set=[\"weight\"],\n                                                           data_key=DataHandlerLP.DK_L)\n                    w_train, w_valid = wdf_train[\"weight\"], wdf_valid[\"weight\"]\n                except KeyError as e:\n                    w_train = pd.DataFrame(np.ones_like(y_train.values), index=y_train.index)\n                    w_valid = pd.DataFrame(np.ones_like(y_valid.values), index=y_valid.index)\n\n        Parameters\n        ----------\n        dataset : Dataset\n            dataset will generate the processed data from model training.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def predict(self, dataset: Dataset) -> object:\n        \"\"\"give prediction given Dataset\n\n        Parameters\n        ----------\n        dataset : Dataset\n            dataset will generate the processed dataset from model training.\n\n        Returns\n        -------\n        Prediction results with certain type such as `pandas.Series`.\n        \"\"\"\n        raise NotImplementedError()",
  "class ModelFT(Model):\n    \"\"\"Model (F)ine(t)unable\"\"\"\n\n    @abc.abstractmethod\n    def finetune(self, dataset: Dataset):\n        \"\"\"finetune model based given dataset\n\n        A typical use case of finetuning model with qlib.workflow.R\n\n        .. code-block:: python\n\n            # start exp to train init model\n            with R.start(experiment_name=\"init models\"):\n                model.fit(dataset)\n                R.save_objects(init_model=model)\n                rid = R.get_recorder().id\n\n            # Finetune model based on previous trained model\n            with R.start(experiment_name=\"finetune model\"):\n                recorder = R.get_recorder(rid, experiment_name=\"init models\")\n                model = recorder.load_object(\"init_model\")\n                model.finetune(dataset, num_boost_round=10)\n\n\n        Parameters\n        ----------\n        dataset : Dataset\n            dataset will generate the processed dataset from model training.\n        \"\"\"\n        raise NotImplementedError()",
  "def predict(self, *args, **kwargs) -> object:\n        \"\"\" Make predictions after modeling things \"\"\"\n        pass",
  "def __call__(self, *args, **kwargs) -> object:\n        \"\"\" leverage Python syntactic sugar to make the models' behaviors like functions \"\"\"\n        return self.predict(*args, **kwargs)",
  "def fit(self, dataset: Dataset):\n        \"\"\"\n        Learn model from the base model\n\n        .. note::\n\n            The attribute names of learned model should `not` start with '_'. So that the model could be\n            dumped to disk.\n\n        The following code example shows how to retrieve `x_train`, `y_train` and `w_train` from the `dataset`:\n\n            .. code-block:: Python\n\n                # get features and labels\n                df_train, df_valid = dataset.prepare(\n                    [\"train\", \"valid\"], col_set=[\"feature\", \"label\"], data_key=DataHandlerLP.DK_L\n                )\n                x_train, y_train = df_train[\"feature\"], df_train[\"label\"]\n                x_valid, y_valid = df_valid[\"feature\"], df_valid[\"label\"]\n\n                # get weights\n                try:\n                    wdf_train, wdf_valid = dataset.prepare([\"train\", \"valid\"], col_set=[\"weight\"],\n                                                           data_key=DataHandlerLP.DK_L)\n                    w_train, w_valid = wdf_train[\"weight\"], wdf_valid[\"weight\"]\n                except KeyError as e:\n                    w_train = pd.DataFrame(np.ones_like(y_train.values), index=y_train.index)\n                    w_valid = pd.DataFrame(np.ones_like(y_valid.values), index=y_valid.index)\n\n        Parameters\n        ----------\n        dataset : Dataset\n            dataset will generate the processed data from model training.\n\n        \"\"\"\n        raise NotImplementedError()",
  "def predict(self, dataset: Dataset) -> object:\n        \"\"\"give prediction given Dataset\n\n        Parameters\n        ----------\n        dataset : Dataset\n            dataset will generate the processed dataset from model training.\n\n        Returns\n        -------\n        Prediction results with certain type such as `pandas.Series`.\n        \"\"\"\n        raise NotImplementedError()",
  "def finetune(self, dataset: Dataset):\n        \"\"\"finetune model based given dataset\n\n        A typical use case of finetuning model with qlib.workflow.R\n\n        .. code-block:: python\n\n            # start exp to train init model\n            with R.start(experiment_name=\"init models\"):\n                model.fit(dataset)\n                R.save_objects(init_model=model)\n                rid = R.get_recorder().id\n\n            # Finetune model based on previous trained model\n            with R.start(experiment_name=\"finetune model\"):\n                recorder = R.get_recorder(rid, experiment_name=\"init models\")\n                model = recorder.load_object(\"init_model\")\n                model.finetune(dataset, num_boost_round=10)\n\n\n        Parameters\n        ----------\n        dataset : Dataset\n            dataset will generate the processed dataset from model training.\n        \"\"\"\n        raise NotImplementedError()",
  "def task_train(task_config: dict, experiment_name):\n    \"\"\"\n    task based training\n\n    Parameters\n    ----------\n    task_config : dict\n        A dict describes a task setting.\n    \"\"\"\n\n    # model initiaiton\n    model = init_instance_by_config(task_config[\"model\"])\n    dataset = init_instance_by_config(task_config[\"dataset\"])\n\n    # start exp\n    with R.start(experiment_name=experiment_name):\n        # train model\n        R.log_params(**flatten_dict(task_config))\n        model.fit(dataset)\n        recorder = R.get_recorder()\n        R.save_objects(**{\"params.pkl\": model})\n\n        # generate records: prediction, backtest, and analysis\n        for record in task_config[\"record\"]:\n            if record[\"class\"] == SignalRecord.__name__:\n                srconf = {\"model\": model, \"dataset\": dataset, \"recorder\": recorder}\n                record[\"kwargs\"].update(srconf)\n                sr = init_instance_by_config(record)\n                sr.generate()\n            else:\n                rconf = {\"recorder\": recorder}\n                record[\"kwargs\"].update(rconf)\n                ar = init_instance_by_config(record)\n                ar.generate()",
  "class RiskModel(BaseModel):\n    \"\"\"Risk Model\n\n    A risk model is used to estimate the covariance matrix of stock returns.\n    \"\"\"\n\n    MASK_NAN = \"mask\"\n    FILL_NAN = \"fill\"\n    IGNORE_NAN = \"ignore\"\n\n    def __init__(self, nan_option: str = \"ignore\", assume_centered: bool = False, scale_return: bool = True):\n        \"\"\"\n        Args:\n            nan_option (str): nan handling option (`ignore`/`mask`/`fill`).\n            assume_centered (bool): whether the data is assumed to be centered.\n            scale_return (bool): whether scale returns as percentage.\n        \"\"\"\n        # nan\n        assert nan_option in [\n            self.MASK_NAN,\n            self.FILL_NAN,\n            self.IGNORE_NAN,\n        ], f\"`nan_option={nan_option}` is not supported\"\n        self.nan_option = nan_option\n\n        self.assume_centered = assume_centered\n        self.scale_return = scale_return\n\n    def predict(\n        self,\n        X: Union[pd.Series, pd.DataFrame, np.ndarray],\n        return_corr: bool = False,\n        is_price: bool = True,\n        return_decomposed_components=False,\n    ) -> Union[pd.DataFrame, np.ndarray, tuple]:\n        \"\"\"\n        Args:\n            X (pd.Series, pd.DataFrame or np.ndarray): data from which to estimate the covariance,\n                with variables as columns and observations as rows.\n            return_corr (bool): whether return the correlation matrix.\n            is_price (bool): whether `X` contains price (if not assume stock returns).\n            return_decomposed_components (bool): whether return decomposed components of the covariance matrix.\n\n        Returns:\n            pd.DataFrame or np.ndarray: estimated covariance (or correlation).\n        \"\"\"\n        assert (\n            not return_corr or not return_decomposed_components\n        ), \"Can only return either correlation matrix or decomposed components.\"\n\n        # transform input into 2D array\n        if not isinstance(X, (pd.Series, pd.DataFrame)):\n            columns = None\n        else:\n            if isinstance(X.index, pd.MultiIndex):\n                if isinstance(X, pd.DataFrame):\n                    X = X.iloc[:, 0].unstack(level=\"instrument\")  # always use the first column\n                else:\n                    X = X.unstack(level=\"instrument\")\n            else:\n                # X is 2D DataFrame\n                pass\n            columns = X.columns  # will be used to restore dataframe\n            X = X.values\n\n        # calculate pct_change\n        if is_price:\n            X = X[1:] / X[:-1] - 1  # NOTE: resulting `n - 1` rows\n\n        # scale return\n        if self.scale_return:\n            X *= 100\n\n        # handle nan and centered\n        X = self._preprocess(X)\n\n        # return decomposed components if needed\n        if return_decomposed_components:\n            assert (\n                \"return_decomposed_components\" in inspect.getfullargspec(self._predict).args\n            ), \"This risk model does not support return decomposed components of the covariance matrix \"\n\n            F, cov_b, var_u = self._predict(X, return_decomposed_components=True)\n            return F, cov_b, var_u\n\n        # estimate covariance\n        S = self._predict(X)\n\n        # return correlation if needed\n        if return_corr:\n            vola = np.sqrt(np.diag(S))\n            corr = S / np.outer(vola, vola)\n            if columns is None:\n                return corr\n            return pd.DataFrame(corr, index=columns, columns=columns)\n\n        # return covariance\n        if columns is None:\n            return S\n        return pd.DataFrame(S, index=columns, columns=columns)\n\n    def _predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"covariance estimation implementation\n\n        This method should be overridden by child classes.\n\n        By default, this method implements the empirical covariance estimation.\n\n        Args:\n            X (np.ndarray): data matrix containing multiple variables (columns) and observations (rows).\n\n        Returns:\n            np.ndarray: covariance matrix.\n        \"\"\"\n        xTx = np.asarray(X.T.dot(X))\n        N = len(X)\n        if isinstance(X, np.ma.MaskedArray):\n            M = 1 - X.mask\n            N = M.T.dot(M)  # each pair has distinct number of samples\n        return xTx / N\n\n    def _preprocess(self, X: np.ndarray) -> Union[np.ndarray, np.ma.MaskedArray]:\n        \"\"\"handle nan and centerize data\n\n        Note:\n            if `nan_option='mask'` then the returned array will be `np.ma.MaskedArray`.\n        \"\"\"\n        # handle nan\n        if self.nan_option == self.FILL_NAN:\n            X = np.nan_to_num(X)\n        elif self.nan_option == self.MASK_NAN:\n            X = np.ma.masked_invalid(X)\n        # centralize\n        if not self.assume_centered:\n            X = X - np.nanmean(X, axis=0)\n        return X",
  "def __init__(self, nan_option: str = \"ignore\", assume_centered: bool = False, scale_return: bool = True):\n        \"\"\"\n        Args:\n            nan_option (str): nan handling option (`ignore`/`mask`/`fill`).\n            assume_centered (bool): whether the data is assumed to be centered.\n            scale_return (bool): whether scale returns as percentage.\n        \"\"\"\n        # nan\n        assert nan_option in [\n            self.MASK_NAN,\n            self.FILL_NAN,\n            self.IGNORE_NAN,\n        ], f\"`nan_option={nan_option}` is not supported\"\n        self.nan_option = nan_option\n\n        self.assume_centered = assume_centered\n        self.scale_return = scale_return",
  "def predict(\n        self,\n        X: Union[pd.Series, pd.DataFrame, np.ndarray],\n        return_corr: bool = False,\n        is_price: bool = True,\n        return_decomposed_components=False,\n    ) -> Union[pd.DataFrame, np.ndarray, tuple]:\n        \"\"\"\n        Args:\n            X (pd.Series, pd.DataFrame or np.ndarray): data from which to estimate the covariance,\n                with variables as columns and observations as rows.\n            return_corr (bool): whether return the correlation matrix.\n            is_price (bool): whether `X` contains price (if not assume stock returns).\n            return_decomposed_components (bool): whether return decomposed components of the covariance matrix.\n\n        Returns:\n            pd.DataFrame or np.ndarray: estimated covariance (or correlation).\n        \"\"\"\n        assert (\n            not return_corr or not return_decomposed_components\n        ), \"Can only return either correlation matrix or decomposed components.\"\n\n        # transform input into 2D array\n        if not isinstance(X, (pd.Series, pd.DataFrame)):\n            columns = None\n        else:\n            if isinstance(X.index, pd.MultiIndex):\n                if isinstance(X, pd.DataFrame):\n                    X = X.iloc[:, 0].unstack(level=\"instrument\")  # always use the first column\n                else:\n                    X = X.unstack(level=\"instrument\")\n            else:\n                # X is 2D DataFrame\n                pass\n            columns = X.columns  # will be used to restore dataframe\n            X = X.values\n\n        # calculate pct_change\n        if is_price:\n            X = X[1:] / X[:-1] - 1  # NOTE: resulting `n - 1` rows\n\n        # scale return\n        if self.scale_return:\n            X *= 100\n\n        # handle nan and centered\n        X = self._preprocess(X)\n\n        # return decomposed components if needed\n        if return_decomposed_components:\n            assert (\n                \"return_decomposed_components\" in inspect.getfullargspec(self._predict).args\n            ), \"This risk model does not support return decomposed components of the covariance matrix \"\n\n            F, cov_b, var_u = self._predict(X, return_decomposed_components=True)\n            return F, cov_b, var_u\n\n        # estimate covariance\n        S = self._predict(X)\n\n        # return correlation if needed\n        if return_corr:\n            vola = np.sqrt(np.diag(S))\n            corr = S / np.outer(vola, vola)\n            if columns is None:\n                return corr\n            return pd.DataFrame(corr, index=columns, columns=columns)\n\n        # return covariance\n        if columns is None:\n            return S\n        return pd.DataFrame(S, index=columns, columns=columns)",
  "def _predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"covariance estimation implementation\n\n        This method should be overridden by child classes.\n\n        By default, this method implements the empirical covariance estimation.\n\n        Args:\n            X (np.ndarray): data matrix containing multiple variables (columns) and observations (rows).\n\n        Returns:\n            np.ndarray: covariance matrix.\n        \"\"\"\n        xTx = np.asarray(X.T.dot(X))\n        N = len(X)\n        if isinstance(X, np.ma.MaskedArray):\n            M = 1 - X.mask\n            N = M.T.dot(M)  # each pair has distinct number of samples\n        return xTx / N",
  "def _preprocess(self, X: np.ndarray) -> Union[np.ndarray, np.ma.MaskedArray]:\n        \"\"\"handle nan and centerize data\n\n        Note:\n            if `nan_option='mask'` then the returned array will be `np.ma.MaskedArray`.\n        \"\"\"\n        # handle nan\n        if self.nan_option == self.FILL_NAN:\n            X = np.nan_to_num(X)\n        elif self.nan_option == self.MASK_NAN:\n            X = np.ma.masked_invalid(X)\n        # centralize\n        if not self.assume_centered:\n            X = X - np.nanmean(X, axis=0)\n        return X",
  "class ShrinkCovEstimator(RiskModel):\n    \"\"\"Shrinkage Covariance Estimator\n\n    This estimator will shrink the sample covariance matrix towards\n    an identify matrix:\n        S_hat = (1 - alpha) * S + alpha * F\n    where `alpha` is the shrink parameter and `F` is the shrinking target.\n\n    The following shrinking parameters (`alpha`) are supported:\n        - `lw` [1][2][3]: use Ledoit-Wolf shrinking parameter.\n        - `oas` [4]: use Oracle Approximating Shrinkage shrinking parameter.\n        - float: directly specify the shrink parameter, should be between [0, 1].\n\n    The following shrinking targets (`F`) are supported:\n        - `const_var` [1][4][5]: assume stocks have the same constant variance and zero correlation.\n        - `const_corr` [2][6]: assume stocks have different variance but equal correlation.\n        - `single_factor` [3][7]: assume single factor model as the shrinking target.\n        - np.ndarray: provide the shrinking targets directly.\n\n    Note:\n        - The optimal shrinking parameter depends on the selection of the shrinking target.\n            Currently, `oas` is not supported for `const_corr` and `single_factor`.\n        - Remember to set `nan_option` to `fill` or `mask` if your data has missing values.\n\n    References:\n        [1] Ledoit, O., & Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices.\n            Journal of Multivariate Analysis, 88(2), 365\u2013411. https://doi.org/10.1016/S0047-259X(03)00096-4\n        [2] Ledoit, O., & Wolf, M. (2004). Honey, I shrunk the sample covariance matrix.\n            Journal of Portfolio Management, 30(4), 1\u201322. https://doi.org/10.3905/jpm.2004.110\n        [3] Ledoit, O., & Wolf, M. (2003). Improved estimation of the covariance matrix of stock returns\n            with an application to portfolio selection.\n            Journal of Empirical Finance, 10(5), 603\u2013621. https://doi.org/10.1016/S0927-5398(03)00007-0\n        [4] Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O. (2010). Shrinkage algorithms for MMSE covariance\n            estimation. IEEE Transactions on Signal Processing, 58(10), 5016\u20135029.\n            https://doi.org/10.1109/TSP.2010.2053029\n        [5] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-0000-00007f64e5b9/cov1para.m.zip\n        [6] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffde5e2d4e/covCor.m.zip\n        [7] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-0000-0000648dfc98/covMarket.m.zip\n    \"\"\"\n\n    SHR_LW = \"lw\"\n    SHR_OAS = \"oas\"\n\n    TGT_CONST_VAR = \"const_var\"\n    TGT_CONST_CORR = \"const_corr\"\n    TGT_SINGLE_FACTOR = \"single_factor\"\n\n    def __init__(self, alpha: Union[str, float] = 0.0, target: Union[str, np.ndarray] = \"const_var\", **kwargs):\n        \"\"\"\n        Args:\n            alpha (str or float): shrinking parameter or estimator (`lw`/`oas`)\n            target (str or np.ndarray): shrinking target (`const_var`/`const_corr`/`single_factor`)\n            kwargs: see `RiskModel` for more information\n        \"\"\"\n        super().__init__(**kwargs)\n\n        # alpha\n        if isinstance(alpha, str):\n            assert alpha in [self.SHR_LW, self.SHR_OAS], f\"shrinking method `{alpha}` is not supported\"\n        elif isinstance(alpha, (float, np.floating)):\n            assert 0 <= alpha <= 1, \"alpha should be between [0, 1]\"\n        else:\n            raise TypeError(\"invalid argument type for `alpha`\")\n        self.alpha = alpha\n\n        # target\n        if isinstance(target, str):\n            assert target in [\n                self.TGT_CONST_VAR,\n                self.TGT_CONST_CORR,\n                self.TGT_SINGLE_FACTOR,\n            ], f\"shrinking target `{target} is not supported\"\n        elif isinstance(target, np.ndarray):\n            pass\n        else:\n            raise TypeError(\"invalid argument type for `target`\")\n        if alpha == self.SHR_OAS and target != self.TGT_CONST_VAR:\n            raise NotImplementedError(\"currently `oas` can only support `const_var` as target\")\n        self.target = target\n\n    def _predict(self, X: np.ndarray) -> np.ndarray:\n        # sample covariance\n        S = super()._predict(X)\n\n        # shrinking target\n        F = self._get_shrink_target(X, S)\n\n        # get shrinking parameter\n        alpha = self._get_shrink_param(X, S, F)\n\n        # shrink covariance\n        if alpha > 0:\n            S *= 1 - alpha\n            F *= alpha\n            S += F\n\n        return S\n\n    def _get_shrink_target(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:\n        \"\"\"get shrinking target `F`\"\"\"\n        if self.target == self.TGT_CONST_VAR:\n            return self._get_shrink_target_const_var(X, S)\n        if self.target == self.TGT_CONST_CORR:\n            return self._get_shrink_target_const_corr(X, S)\n        if self.target == self.TGT_SINGLE_FACTOR:\n            return self._get_shrink_target_single_factor(X, S)\n        return self.target\n\n    def _get_shrink_target_const_var(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:\n        \"\"\"get shrinking target with constant variance\n\n        This target assumes zero pair-wise correlation and constant variance.\n        The constant variance is estimated by averaging all sample's variances.\n        \"\"\"\n        n = len(S)\n        F = np.eye(n)\n        np.fill_diagonal(F, np.mean(np.diag(S)))\n        return F\n\n    def _get_shrink_target_const_corr(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:\n        \"\"\"get shrinking target with constant correlation\n\n        This target assumes constant pair-wise correlation but keep the sample variance.\n        The constant correlation is estimated by averaging all pairwise correlations.\n        \"\"\"\n        n = len(S)\n        var = np.diag(S)\n        sqrt_var = np.sqrt(var)\n        covar = np.outer(sqrt_var, sqrt_var)\n        r_bar = (np.sum(S / covar) - n) / (n * (n - 1))\n        F = r_bar * covar\n        np.fill_diagonal(F, var)\n        return F\n\n    def _get_shrink_target_single_factor(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:\n        \"\"\"get shrinking target with single factor model\"\"\"\n        X_mkt = np.nanmean(X, axis=1)\n        cov_mkt = np.asarray(X.T.dot(X_mkt) / len(X))\n        var_mkt = np.asarray(X_mkt.dot(X_mkt) / len(X))\n        F = np.outer(cov_mkt, cov_mkt) / var_mkt\n        np.fill_diagonal(F, np.diag(S))\n        return F\n\n    def _get_shrink_param(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"get shrinking parameter `alpha`\n\n        Note:\n            The Ledoit-Wolf shrinking parameter estimator consists of three different methods.\n        \"\"\"\n        if self.alpha == self.SHR_OAS:\n            return self._get_shrink_param_oas(X, S, F)\n        elif self.alpha == self.SHR_LW:\n            if self.target == self.TGT_CONST_VAR:\n                return self._get_shrink_param_lw_const_var(X, S, F)\n            if self.target == self.TGT_CONST_CORR:\n                return self._get_shrink_param_lw_const_corr(X, S, F)\n            if self.target == self.TGT_SINGLE_FACTOR:\n                return self._get_shrink_param_lw_single_factor(X, S, F)\n        return self.alpha\n\n    def _get_shrink_param_oas(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"Oracle Approximating Shrinkage Estimator\n\n        This method uses the following formula to estimate the `alpha`\n        parameter for the shrink covariance estimator:\n            A = (1 - 2 / p) * trace(S^2) + trace^2(S)\n            B = (n + 1 - 2 / p) * (trace(S^2) - trace^2(S) / p)\n            alpha = A / B\n        where `n`, `p` are the dim of observations and variables respectively.\n        \"\"\"\n        trS2 = np.sum(S ** 2)\n        tr2S = np.trace(S) ** 2\n\n        n, p = X.shape\n\n        A = (1 - 2 / p) * (trS2 + tr2S)\n        B = (n + 1 - 2 / p) * (trS2 + tr2S / p)\n        alpha = A / B\n\n        return alpha\n\n    def _get_shrink_param_lw_const_var(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"Ledoit-Wolf Shrinkage Estimator (Constant Variance)\n\n        This method shrinks the covariance matrix towards the constand variance target.\n        \"\"\"\n        t, n = X.shape\n\n        y = X ** 2\n        phi = np.sum(y.T.dot(y) / t - S ** 2)\n\n        gamma = np.linalg.norm(S - F, \"fro\") ** 2\n\n        kappa = phi / gamma\n        alpha = max(0, min(1, kappa / t))\n\n        return alpha\n\n    def _get_shrink_param_lw_const_corr(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"Ledoit-Wolf Shrinkage Estimator (Constant Correlation)\n\n        This method shrinks the covariance matrix towards the constand correlation target.\n        \"\"\"\n        t, n = X.shape\n\n        var = np.diag(S)\n        sqrt_var = np.sqrt(var)\n        r_bar = (np.sum(S / np.outer(sqrt_var, sqrt_var)) - n) / (n * (n - 1))\n\n        y = X ** 2\n        phi_mat = y.T.dot(y) / t - S ** 2\n        phi = np.sum(phi_mat)\n\n        theta_mat = (X ** 3).T.dot(X) / t - var[:, None] * S\n        np.fill_diagonal(theta_mat, 0)\n        rho = np.sum(np.diag(phi_mat)) + r_bar * np.sum(np.outer(1 / sqrt_var, sqrt_var) * theta_mat)\n\n        gamma = np.linalg.norm(S - F, \"fro\") ** 2\n\n        kappa = (phi - rho) / gamma\n        alpha = max(0, min(1, kappa / t))\n\n        return alpha\n\n    def _get_shrink_param_lw_single_factor(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"Ledoit-Wolf Shrinkage Estimator (Single Factor Model)\n\n        This method shrinks the covariance matrix towards the single factor model target.\n        \"\"\"\n        t, n = X.shape\n\n        X_mkt = np.nanmean(X, axis=1)\n        cov_mkt = np.asarray(X.T.dot(X_mkt) / len(X))\n        var_mkt = np.asarray(X_mkt.dot(X_mkt) / len(X))\n\n        y = X ** 2\n        phi = np.sum(y.T.dot(y)) / t - np.sum(S ** 2)\n\n        rdiag = np.sum(y ** 2) / t - np.sum(np.diag(S) ** 2)\n        z = X * X_mkt[:, None]\n        v1 = y.T.dot(z) / t - cov_mkt[:, None] * S\n        roff1 = np.sum(v1 * cov_mkt[:, None].T) / var_mkt - np.sum(np.diag(v1) * cov_mkt) / var_mkt\n        v3 = z.T.dot(z) / t - var_mkt * S\n        roff3 = (\n            np.sum(v3 * np.outer(cov_mkt, cov_mkt)) / var_mkt ** 2 - np.sum(np.diag(v3) * cov_mkt ** 2) / var_mkt ** 2\n        )\n        roff = 2 * roff1 - roff3\n        rho = rdiag + roff\n\n        gamma = np.linalg.norm(S - F, \"fro\") ** 2\n\n        kappa = (phi - rho) / gamma\n        alpha = max(0, min(1, kappa / t))\n\n        return alpha",
  "def __init__(self, alpha: Union[str, float] = 0.0, target: Union[str, np.ndarray] = \"const_var\", **kwargs):\n        \"\"\"\n        Args:\n            alpha (str or float): shrinking parameter or estimator (`lw`/`oas`)\n            target (str or np.ndarray): shrinking target (`const_var`/`const_corr`/`single_factor`)\n            kwargs: see `RiskModel` for more information\n        \"\"\"\n        super().__init__(**kwargs)\n\n        # alpha\n        if isinstance(alpha, str):\n            assert alpha in [self.SHR_LW, self.SHR_OAS], f\"shrinking method `{alpha}` is not supported\"\n        elif isinstance(alpha, (float, np.floating)):\n            assert 0 <= alpha <= 1, \"alpha should be between [0, 1]\"\n        else:\n            raise TypeError(\"invalid argument type for `alpha`\")\n        self.alpha = alpha\n\n        # target\n        if isinstance(target, str):\n            assert target in [\n                self.TGT_CONST_VAR,\n                self.TGT_CONST_CORR,\n                self.TGT_SINGLE_FACTOR,\n            ], f\"shrinking target `{target} is not supported\"\n        elif isinstance(target, np.ndarray):\n            pass\n        else:\n            raise TypeError(\"invalid argument type for `target`\")\n        if alpha == self.SHR_OAS and target != self.TGT_CONST_VAR:\n            raise NotImplementedError(\"currently `oas` can only support `const_var` as target\")\n        self.target = target",
  "def _predict(self, X: np.ndarray) -> np.ndarray:\n        # sample covariance\n        S = super()._predict(X)\n\n        # shrinking target\n        F = self._get_shrink_target(X, S)\n\n        # get shrinking parameter\n        alpha = self._get_shrink_param(X, S, F)\n\n        # shrink covariance\n        if alpha > 0:\n            S *= 1 - alpha\n            F *= alpha\n            S += F\n\n        return S",
  "def _get_shrink_target(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:\n        \"\"\"get shrinking target `F`\"\"\"\n        if self.target == self.TGT_CONST_VAR:\n            return self._get_shrink_target_const_var(X, S)\n        if self.target == self.TGT_CONST_CORR:\n            return self._get_shrink_target_const_corr(X, S)\n        if self.target == self.TGT_SINGLE_FACTOR:\n            return self._get_shrink_target_single_factor(X, S)\n        return self.target",
  "def _get_shrink_target_const_var(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:\n        \"\"\"get shrinking target with constant variance\n\n        This target assumes zero pair-wise correlation and constant variance.\n        The constant variance is estimated by averaging all sample's variances.\n        \"\"\"\n        n = len(S)\n        F = np.eye(n)\n        np.fill_diagonal(F, np.mean(np.diag(S)))\n        return F",
  "def _get_shrink_target_const_corr(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:\n        \"\"\"get shrinking target with constant correlation\n\n        This target assumes constant pair-wise correlation but keep the sample variance.\n        The constant correlation is estimated by averaging all pairwise correlations.\n        \"\"\"\n        n = len(S)\n        var = np.diag(S)\n        sqrt_var = np.sqrt(var)\n        covar = np.outer(sqrt_var, sqrt_var)\n        r_bar = (np.sum(S / covar) - n) / (n * (n - 1))\n        F = r_bar * covar\n        np.fill_diagonal(F, var)\n        return F",
  "def _get_shrink_target_single_factor(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:\n        \"\"\"get shrinking target with single factor model\"\"\"\n        X_mkt = np.nanmean(X, axis=1)\n        cov_mkt = np.asarray(X.T.dot(X_mkt) / len(X))\n        var_mkt = np.asarray(X_mkt.dot(X_mkt) / len(X))\n        F = np.outer(cov_mkt, cov_mkt) / var_mkt\n        np.fill_diagonal(F, np.diag(S))\n        return F",
  "def _get_shrink_param(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"get shrinking parameter `alpha`\n\n        Note:\n            The Ledoit-Wolf shrinking parameter estimator consists of three different methods.\n        \"\"\"\n        if self.alpha == self.SHR_OAS:\n            return self._get_shrink_param_oas(X, S, F)\n        elif self.alpha == self.SHR_LW:\n            if self.target == self.TGT_CONST_VAR:\n                return self._get_shrink_param_lw_const_var(X, S, F)\n            if self.target == self.TGT_CONST_CORR:\n                return self._get_shrink_param_lw_const_corr(X, S, F)\n            if self.target == self.TGT_SINGLE_FACTOR:\n                return self._get_shrink_param_lw_single_factor(X, S, F)\n        return self.alpha",
  "def _get_shrink_param_oas(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"Oracle Approximating Shrinkage Estimator\n\n        This method uses the following formula to estimate the `alpha`\n        parameter for the shrink covariance estimator:\n            A = (1 - 2 / p) * trace(S^2) + trace^2(S)\n            B = (n + 1 - 2 / p) * (trace(S^2) - trace^2(S) / p)\n            alpha = A / B\n        where `n`, `p` are the dim of observations and variables respectively.\n        \"\"\"\n        trS2 = np.sum(S ** 2)\n        tr2S = np.trace(S) ** 2\n\n        n, p = X.shape\n\n        A = (1 - 2 / p) * (trS2 + tr2S)\n        B = (n + 1 - 2 / p) * (trS2 + tr2S / p)\n        alpha = A / B\n\n        return alpha",
  "def _get_shrink_param_lw_const_var(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"Ledoit-Wolf Shrinkage Estimator (Constant Variance)\n\n        This method shrinks the covariance matrix towards the constand variance target.\n        \"\"\"\n        t, n = X.shape\n\n        y = X ** 2\n        phi = np.sum(y.T.dot(y) / t - S ** 2)\n\n        gamma = np.linalg.norm(S - F, \"fro\") ** 2\n\n        kappa = phi / gamma\n        alpha = max(0, min(1, kappa / t))\n\n        return alpha",
  "def _get_shrink_param_lw_const_corr(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"Ledoit-Wolf Shrinkage Estimator (Constant Correlation)\n\n        This method shrinks the covariance matrix towards the constand correlation target.\n        \"\"\"\n        t, n = X.shape\n\n        var = np.diag(S)\n        sqrt_var = np.sqrt(var)\n        r_bar = (np.sum(S / np.outer(sqrt_var, sqrt_var)) - n) / (n * (n - 1))\n\n        y = X ** 2\n        phi_mat = y.T.dot(y) / t - S ** 2\n        phi = np.sum(phi_mat)\n\n        theta_mat = (X ** 3).T.dot(X) / t - var[:, None] * S\n        np.fill_diagonal(theta_mat, 0)\n        rho = np.sum(np.diag(phi_mat)) + r_bar * np.sum(np.outer(1 / sqrt_var, sqrt_var) * theta_mat)\n\n        gamma = np.linalg.norm(S - F, \"fro\") ** 2\n\n        kappa = (phi - rho) / gamma\n        alpha = max(0, min(1, kappa / t))\n\n        return alpha",
  "def _get_shrink_param_lw_single_factor(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:\n        \"\"\"Ledoit-Wolf Shrinkage Estimator (Single Factor Model)\n\n        This method shrinks the covariance matrix towards the single factor model target.\n        \"\"\"\n        t, n = X.shape\n\n        X_mkt = np.nanmean(X, axis=1)\n        cov_mkt = np.asarray(X.T.dot(X_mkt) / len(X))\n        var_mkt = np.asarray(X_mkt.dot(X_mkt) / len(X))\n\n        y = X ** 2\n        phi = np.sum(y.T.dot(y)) / t - np.sum(S ** 2)\n\n        rdiag = np.sum(y ** 2) / t - np.sum(np.diag(S) ** 2)\n        z = X * X_mkt[:, None]\n        v1 = y.T.dot(z) / t - cov_mkt[:, None] * S\n        roff1 = np.sum(v1 * cov_mkt[:, None].T) / var_mkt - np.sum(np.diag(v1) * cov_mkt) / var_mkt\n        v3 = z.T.dot(z) / t - var_mkt * S\n        roff3 = (\n            np.sum(v3 * np.outer(cov_mkt, cov_mkt)) / var_mkt ** 2 - np.sum(np.diag(v3) * cov_mkt ** 2) / var_mkt ** 2\n        )\n        roff = 2 * roff1 - roff3\n        rho = rdiag + roff\n\n        gamma = np.linalg.norm(S - F, \"fro\") ** 2\n\n        kappa = (phi - rho) / gamma\n        alpha = max(0, min(1, kappa / t))\n\n        return alpha",
  "class POETCovEstimator(RiskModel):\n    \"\"\"Principal Orthogonal Complement Thresholding Estimator (POET)\n\n    Reference:\n        [1] Fan, J., Liao, Y., & Mincheva, M. (2013). Large covariance estimation by thresholding principal orthogonal complements.\n            Journal of the Royal Statistical Society. Series B: Statistical Methodology, 75(4), 603\u2013680. https://doi.org/10.1111/rssb.12016\n        [2] http://econweb.rutgers.edu/yl1114/papers/poet/POET.m\n    \"\"\"\n\n    THRESH_SOFT = \"soft\"\n    THRESH_HARD = \"hard\"\n    THRESH_SCAD = \"scad\"\n\n    def __init__(self, num_factors: int = 0, thresh: float = 1.0, thresh_method: str = \"soft\", **kwargs):\n        \"\"\"\n        Args:\n            num_factors (int): number of factors (if set to zero, no factor model will be used).\n            thresh (float): the positive constant for thresholding.\n            thresh_method (str): thresholding method, which can be\n                - 'soft': soft thresholding.\n                - 'hard': hard thresholding.\n                - 'scad': scad thresholding.\n            kwargs: see `RiskModel` for more information.\n        \"\"\"\n        super().__init__(**kwargs)\n\n        assert num_factors >= 0, \"`num_factors` requires a positive integer\"\n        self.num_factors = num_factors\n\n        assert thresh >= 0, \"`thresh` requires a positive float number\"\n        self.thresh = thresh\n\n        assert thresh_method in [\n            self.THRESH_HARD,\n            self.THRESH_SOFT,\n            self.THRESH_SCAD,\n        ], \"`thresh_method` should be `soft`/`hard`/`scad`\"\n        self.thresh_method = thresh_method\n\n    def _predict(self, X: np.ndarray) -> np.ndarray:\n\n        Y = X.T  # NOTE: to match POET's implementation\n        p, n = Y.shape\n\n        if self.num_factors > 0:\n            Dd, V = np.linalg.eig(Y.T.dot(Y))\n            V = V[:, np.argsort(Dd)]\n            F = V[:, -self.num_factors :][:, ::-1] * np.sqrt(n)\n            LamPCA = Y.dot(F) / n\n            uhat = np.asarray(Y - LamPCA.dot(F.T))\n            Lowrank = np.asarray(LamPCA.dot(LamPCA.T))\n            rate = 1 / np.sqrt(p) + np.sqrt(np.log(p) / n)\n        else:\n            uhat = np.asarray(Y)\n            rate = np.sqrt(np.log(p) / n)\n            Lowrank = 0\n\n        lamb = rate * self.thresh\n        SuPCA = uhat.dot(uhat.T) / n\n        SuDiag = np.diag(np.diag(SuPCA))\n        R = np.linalg.inv(SuDiag ** 0.5).dot(SuPCA).dot(np.linalg.inv(SuDiag ** 0.5))\n\n        if self.thresh_method == self.THRESH_HARD:\n            M = R * (np.abs(R) > lamb)\n        elif self.thresh_method == self.THRESH_SOFT:\n            res = np.abs(R) - lamb\n            res = (res + np.abs(res)) / 2\n            M = np.sign(R) * res\n        else:\n            M1 = (np.abs(R) < 2 * lamb) * np.sign(R) * (np.abs(R) - lamb) * (np.abs(R) > lamb)\n            M2 = (np.abs(R) < 3.7 * lamb) * (np.abs(R) >= 2 * lamb) * (2.7 * R - 3.7 * np.sign(R) * lamb) / 1.7\n            M3 = (np.abs(R) >= 3.7 * lamb) * R\n            M = M1 + M2 + M3\n\n        Rthresh = M - np.diag(np.diag(M)) + np.eye(p)\n        SigmaU = (SuDiag ** 0.5).dot(Rthresh).dot(SuDiag ** 0.5)\n        SigmaY = SigmaU + Lowrank\n\n        return SigmaY",
  "def __init__(self, num_factors: int = 0, thresh: float = 1.0, thresh_method: str = \"soft\", **kwargs):\n        \"\"\"\n        Args:\n            num_factors (int): number of factors (if set to zero, no factor model will be used).\n            thresh (float): the positive constant for thresholding.\n            thresh_method (str): thresholding method, which can be\n                - 'soft': soft thresholding.\n                - 'hard': hard thresholding.\n                - 'scad': scad thresholding.\n            kwargs: see `RiskModel` for more information.\n        \"\"\"\n        super().__init__(**kwargs)\n\n        assert num_factors >= 0, \"`num_factors` requires a positive integer\"\n        self.num_factors = num_factors\n\n        assert thresh >= 0, \"`thresh` requires a positive float number\"\n        self.thresh = thresh\n\n        assert thresh_method in [\n            self.THRESH_HARD,\n            self.THRESH_SOFT,\n            self.THRESH_SCAD,\n        ], \"`thresh_method` should be `soft`/`hard`/`scad`\"\n        self.thresh_method = thresh_method",
  "def _predict(self, X: np.ndarray) -> np.ndarray:\n\n        Y = X.T  # NOTE: to match POET's implementation\n        p, n = Y.shape\n\n        if self.num_factors > 0:\n            Dd, V = np.linalg.eig(Y.T.dot(Y))\n            V = V[:, np.argsort(Dd)]\n            F = V[:, -self.num_factors :][:, ::-1] * np.sqrt(n)\n            LamPCA = Y.dot(F) / n\n            uhat = np.asarray(Y - LamPCA.dot(F.T))\n            Lowrank = np.asarray(LamPCA.dot(LamPCA.T))\n            rate = 1 / np.sqrt(p) + np.sqrt(np.log(p) / n)\n        else:\n            uhat = np.asarray(Y)\n            rate = np.sqrt(np.log(p) / n)\n            Lowrank = 0\n\n        lamb = rate * self.thresh\n        SuPCA = uhat.dot(uhat.T) / n\n        SuDiag = np.diag(np.diag(SuPCA))\n        R = np.linalg.inv(SuDiag ** 0.5).dot(SuPCA).dot(np.linalg.inv(SuDiag ** 0.5))\n\n        if self.thresh_method == self.THRESH_HARD:\n            M = R * (np.abs(R) > lamb)\n        elif self.thresh_method == self.THRESH_SOFT:\n            res = np.abs(R) - lamb\n            res = (res + np.abs(res)) / 2\n            M = np.sign(R) * res\n        else:\n            M1 = (np.abs(R) < 2 * lamb) * np.sign(R) * (np.abs(R) - lamb) * (np.abs(R) > lamb)\n            M2 = (np.abs(R) < 3.7 * lamb) * (np.abs(R) >= 2 * lamb) * (2.7 * R - 3.7 * np.sign(R) * lamb) / 1.7\n            M3 = (np.abs(R) >= 3.7 * lamb) * R\n            M = M1 + M2 + M3\n\n        Rthresh = M - np.diag(np.diag(M)) + np.eye(p)\n        SigmaU = (SuDiag ** 0.5).dot(Rthresh).dot(SuDiag ** 0.5)\n        SigmaY = SigmaU + Lowrank\n\n        return SigmaY",
  "class StructuredCovEstimator(RiskModel):\n    \"\"\"Structured Covariance Estimator\n\n    This estimator assumes observations can be predicted by multiple factors\n        X = FB + U\n    where `F` can be specified by explicit risk factors or latent factors.\n\n    Therefore the structured covariance can be estimated by\n        cov(X) = F cov(B) F.T + cov(U)\n\n    We use latent factor models to estimate the structured covariance.\n    Specifically, the following latent factor models are supported:\n        - `pca`: Principal Component Analysis\n        - `fa`: Factor Analysis\n\n    Reference: [1] Fan, J., Liao, Y., & Liu, H. (2016). An overview of the estimation of large covariance and\n    precision matrices. Econometrics Journal, 19(1), C1\u2013C32. https://doi.org/10.1111/ectj.12061\n    \"\"\"\n\n    FACTOR_MODEL_PCA = \"pca\"\n    FACTOR_MODEL_FA = \"fa\"\n    DEFAULT_NAN_OPTION = \"fill\"\n\n    def __init__(self, factor_model: str = \"pca\", num_factors: int = 10, **kwargs):\n        \"\"\"\n        Args:\n            factor_model (str): the latent factor models used to estimate the structured covariance (`pca`/`fa`).\n            num_factors (int): number of components to keep.\n            kwargs: see `RiskModel` for more information\n        \"\"\"\n        if \"nan_option\" in kwargs.keys():\n            assert kwargs[\"nan_option\"] in [self.DEFAULT_NAN_OPTION], \"nan_option={} is not supported\".format(\n                kwargs[\"nan_option\"]\n            )\n        else:\n            kwargs[\"nan_option\"] = self.DEFAULT_NAN_OPTION\n\n        super().__init__(**kwargs)\n\n        assert factor_model in [\n            self.FACTOR_MODEL_PCA,\n            self.FACTOR_MODEL_FA,\n        ], \"factor_model={} is not supported\".format(factor_model)\n        self.solver = PCA if factor_model == self.FACTOR_MODEL_PCA else FactorAnalysis\n\n        self.num_factors = num_factors\n\n    def _predict(self, X: np.ndarray, return_decomposed_components=False) -> Union[np.ndarray, tuple]:\n        \"\"\"\n        covariance estimation implementation\n\n        Args:\n            X (np.ndarray): data matrix containing multiple variables (columns) and observations (rows).\n            return_decomposed_components (bool): whether return decomposed components of the covariance matrix.\n\n        Returns:\n            tuple or np.ndarray: decomposed covariance matrix or covariance matrix.\n        \"\"\"\n\n        model = self.solver(self.num_factors, random_state=0).fit(X)\n\n        F = model.components_.T  # num_features x num_factors\n        B = model.transform(X)  # num_samples x num_factors\n        U = X - B @ F.T\n        cov_b = np.cov(B.T)  # num_factors x num_factors\n        var_u = np.var(U, axis=0)  # diagonal\n\n        if return_decomposed_components:\n            return F, cov_b, var_u\n\n        cov_x = F @ cov_b @ F.T + np.diag(var_u)\n\n        return cov_x",
  "def __init__(self, factor_model: str = \"pca\", num_factors: int = 10, **kwargs):\n        \"\"\"\n        Args:\n            factor_model (str): the latent factor models used to estimate the structured covariance (`pca`/`fa`).\n            num_factors (int): number of components to keep.\n            kwargs: see `RiskModel` for more information\n        \"\"\"\n        if \"nan_option\" in kwargs.keys():\n            assert kwargs[\"nan_option\"] in [self.DEFAULT_NAN_OPTION], \"nan_option={} is not supported\".format(\n                kwargs[\"nan_option\"]\n            )\n        else:\n            kwargs[\"nan_option\"] = self.DEFAULT_NAN_OPTION\n\n        super().__init__(**kwargs)\n\n        assert factor_model in [\n            self.FACTOR_MODEL_PCA,\n            self.FACTOR_MODEL_FA,\n        ], \"factor_model={} is not supported\".format(factor_model)\n        self.solver = PCA if factor_model == self.FACTOR_MODEL_PCA else FactorAnalysis\n\n        self.num_factors = num_factors",
  "def _predict(self, X: np.ndarray, return_decomposed_components=False) -> Union[np.ndarray, tuple]:\n        \"\"\"\n        covariance estimation implementation\n\n        Args:\n            X (np.ndarray): data matrix containing multiple variables (columns) and observations (rows).\n            return_decomposed_components (bool): whether return decomposed components of the covariance matrix.\n\n        Returns:\n            tuple or np.ndarray: decomposed covariance matrix or covariance matrix.\n        \"\"\"\n\n        model = self.solver(self.num_factors, random_state=0).fit(X)\n\n        F = model.components_.T  # num_features x num_factors\n        B = model.transform(X)  # num_samples x num_factors\n        U = X - B @ F.T\n        cov_b = np.cov(B.T)  # num_factors x num_factors\n        var_u = np.var(U, axis=0)  # diagonal\n\n        if return_decomposed_components:\n            return F, cov_b, var_u\n\n        cov_x = F @ cov_b @ F.T + np.diag(var_u)\n\n        return cov_x"
]