[
  "def video_write(fn, images, framerate=60, vcodec='libx264'):\n    \"\"\"\n    Save list of images to a video file.\n\n    Source:\n    https://github.com/kkroening/ffmpeg-python/issues/246#issuecomment-520200981\n    Modified so that framerate is given to .input(), as suggested in the\n    thread, to avoid\n    skipping frames.\n\n    Parameters\n    ----------\n    fn : string\n        filename\n    images : list or np.array\n        list of images to save to a video.\n    framerate : int\n    \"\"\"\n    global _FFMPEG_INSTALLED\n\n    try:\n        if len(images) == 0:\n            logger.warning(\"Calling video_write() with empty images.\")\n            return\n\n        if not _FFMPEG_INSTALLED:\n            logger.error(\n                \"video_write(): Unable to save video, ffmpeg-python \\\n    package required (https://github.com/kkroening/ffmpeg-python)\")\n            return\n\n        if not isinstance(images, np.ndarray):\n            images = np.asarray(images)\n        _, height, width, channels = images.shape\n        process = (\n            ffmpeg\n            .input('pipe:', format='rawvideo', pix_fmt='rgb24',\n                   s='{}x{}'.format(width, height), r=framerate)\n            .output(fn, pix_fmt='yuv420p', vcodec=vcodec)\n            .overwrite_output()\n            .run_async(pipe_stdin=True)\n        )\n        for frame in images:\n            process.stdin.write(\n                frame\n                .astype(np.uint8)\n                .tobytes()\n            )\n        process.stdin.close()\n        process.wait()\n\n    except Exception as ex:\n        logger.warning(\"Not possible to save \\\nvideo, due to exception: {}\".format(str(ex)))",
  "class OpenGLRender2D:\n    \"\"\"\n    Class to render a list of scenes using OpenGL and pygame.\n    \"\"\"\n\n    def __init__(self):\n        # parameters\n        self.window_width = 800\n        self.window_height = 800    # multiples of 16 are preferred\n        self.background_color = (0.6, 0.75, 1.0)\n        self.refresh_interval = 50\n        self.window_name = \"rlberry render\"\n        self.clipping_area = (-1.0, 1.0, -1.0, 1.0)\n\n        # time counter\n        self.time_count = 0\n\n        # background scene\n        self.background = Scene()\n        # data to be rendered (list of scenes)\n        self.data = []\n\n    def set_window_name(self, name):\n        self.window_name = name\n\n    def set_refresh_interval(self, interval):\n        self.refresh_interval = interval\n\n    def set_clipping_area(self, area):\n        \"\"\"\n        The clipping area is tuple with elements (left, right, bottom, top)\n        Default = (-1.0, 1.0, -1.0, 1.0)\n        \"\"\"\n        self.clipping_area = area\n        base_size = max(self.window_width, self.window_height)\n        width_range = area[1] - area[0]\n        height_range = area[3] - area[2]\n        base_range = max(width_range, height_range)\n        width_range /= base_range\n        height_range /= base_range\n        self.window_width = int(base_size * width_range)\n        self.window_height = int(base_size * height_range)\n\n        # width and height must be divisible by 2\n        if self.window_width % 2 == 1:\n            self.window_width += 1\n        if self.window_height % 2 == 1:\n            self.window_height += 1\n\n    def set_data(self, data):\n        self.data = data\n\n    def set_background(self, background):\n        self.background = background\n\n    def initGL(self):\n        \"\"\"\n        initialize GL\n        \"\"\"\n        glMatrixMode(GL_PROJECTION)\n        glLoadIdentity()\n        gluOrtho2D(self.clipping_area[0], self.clipping_area[1],\n                   self.clipping_area[2], self.clipping_area[3])\n\n    def display(self):\n        \"\"\"\n        Callback function, handler for window re-paint\n        \"\"\"\n        # Set background color (clear background)\n        glClearColor(self.background_color[0], self.background_color[1],\n                     self.background_color[2], 1.0)\n        glClear(GL_COLOR_BUFFER_BIT)\n\n        # Display background\n        for shape in self.background.shapes:\n            self.draw_geometric2d(shape)\n\n        # Display objects\n        if len(self.data) > 0:\n            idx = self.time_count % len(self.data)\n            for shape in self.data[idx].shapes:\n                self.draw_geometric2d(shape)\n\n        self.time_count += 1\n        glFlush()\n\n    @staticmethod\n    def draw_geometric2d(shape):\n        \"\"\"\n        Draw a 2D shape, of type GeometricPrimitive\n        \"\"\"\n        if shape.type == \"POINTS\":\n            glBegin(GL_POINTS)\n        elif shape.type == \"LINES\":\n            glBegin(GL_LINES)\n        elif shape.type == \"LINE_STRIP\":\n            glBegin(GL_LINE_STRIP)\n        elif shape.type == \"LINE_LOOP\":\n            glBegin(GL_LINE_LOOP)\n        elif shape.type == \"POLYGON\":\n            glBegin(GL_POLYGON)\n        elif shape.type == \"TRIANGLES\":\n            glBegin(GL_TRIANGLES)\n        elif shape.type == \"TRIANGLE_STRIP\":\n            glBegin(GL_TRIANGLE_STRIP)\n        elif shape.type == \"TRIANGLE_FAN\":\n            glBegin(GL_TRIANGLE_FAN)\n        elif shape.type == \"QUADS\":\n            glBegin(GL_QUADS)\n        elif shape.type == \"QUAD_STRIP\":\n            glBegin(GL_QUAD_STRIP)\n        else:\n            logger.error(\"Invalid type for geometric primitive!\")\n            raise NameError\n\n        # set color\n        glColor3f(shape.color[0], shape.color[1], shape.color[2])\n\n        # create vertices\n        for vertex in shape.vertices:\n            glVertex2f(vertex[0], vertex[1])\n        glEnd()\n\n    def run_graphics(self, loop=True):\n        \"\"\"\n        Sequentially displays scenes in self.data\n\n        If loop is True, keep rendering until user closes the window.\n        \"\"\"\n        global _IMPORT_SUCESSFUL\n\n        if _IMPORT_SUCESSFUL:\n            pg.init()\n            display = (self.window_width, self.window_height)\n            pg.display.set_mode(display, DOUBLEBUF | OPENGL)\n            pg.display.set_caption(self.window_name)\n            self.initGL()\n            while True:\n                for event in pg.event.get():\n                    if event.type == pg.QUIT:\n                        pg.quit()\n                        return\n                #\n                self.display()\n                #\n                pg.display.flip()\n                pg.time.wait(self.refresh_interval)\n\n                # if not loop, stop\n                if not loop:\n                    pg.quit()\n                    return\n        else:\n            logger.error(\"Not possible to render the environment, \\\npygame or pyopengl not installed.\")\n\n    def get_video_data(self):\n        \"\"\"\n        Stores scenes in self.data in a list of numpy arrays that can be used\n        to save a video.\n        \"\"\"\n        global _IMPORT_SUCESSFUL\n\n        if _IMPORT_SUCESSFUL:\n            video_data = []\n\n            pg.init()\n            display = (self.window_width, self.window_height)\n            screen = pg.display.set_mode(display, DOUBLEBUF | OPENGL)\n            pg.display.set_caption(self.window_name)\n            self.initGL()\n\n            self.time_count = 0\n            while self.time_count <= len(self.data):\n                #\n                self.display()\n                #\n                pg.display.flip()\n\n                #\n                # See https://stackoverflow.com/a/42754578/5691288\n                #\n                string_image = pg.image.tostring(screen, 'RGB')\n                temp_surf = pg.image.fromstring(string_image,\n                                                (self.window_width,\n                                                 self.window_height), 'RGB')\n                tmp_arr = pg.surfarray.array3d(temp_surf)\n                imgdata = np.moveaxis(tmp_arr, 0, 1)\n                video_data.append(imgdata)\n\n            pg.quit()\n            return video_data\n        else:\n            logger.error(\"Not possible to render the environment, \\\npygame or pyopengl not installed.\")\n            return []",
  "def __init__(self):\n        # parameters\n        self.window_width = 800\n        self.window_height = 800    # multiples of 16 are preferred\n        self.background_color = (0.6, 0.75, 1.0)\n        self.refresh_interval = 50\n        self.window_name = \"rlberry render\"\n        self.clipping_area = (-1.0, 1.0, -1.0, 1.0)\n\n        # time counter\n        self.time_count = 0\n\n        # background scene\n        self.background = Scene()\n        # data to be rendered (list of scenes)\n        self.data = []",
  "def set_window_name(self, name):\n        self.window_name = name",
  "def set_refresh_interval(self, interval):\n        self.refresh_interval = interval",
  "def set_clipping_area(self, area):\n        \"\"\"\n        The clipping area is tuple with elements (left, right, bottom, top)\n        Default = (-1.0, 1.0, -1.0, 1.0)\n        \"\"\"\n        self.clipping_area = area\n        base_size = max(self.window_width, self.window_height)\n        width_range = area[1] - area[0]\n        height_range = area[3] - area[2]\n        base_range = max(width_range, height_range)\n        width_range /= base_range\n        height_range /= base_range\n        self.window_width = int(base_size * width_range)\n        self.window_height = int(base_size * height_range)\n\n        # width and height must be divisible by 2\n        if self.window_width % 2 == 1:\n            self.window_width += 1\n        if self.window_height % 2 == 1:\n            self.window_height += 1",
  "def set_data(self, data):\n        self.data = data",
  "def set_background(self, background):\n        self.background = background",
  "def initGL(self):\n        \"\"\"\n        initialize GL\n        \"\"\"\n        glMatrixMode(GL_PROJECTION)\n        glLoadIdentity()\n        gluOrtho2D(self.clipping_area[0], self.clipping_area[1],\n                   self.clipping_area[2], self.clipping_area[3])",
  "def display(self):\n        \"\"\"\n        Callback function, handler for window re-paint\n        \"\"\"\n        # Set background color (clear background)\n        glClearColor(self.background_color[0], self.background_color[1],\n                     self.background_color[2], 1.0)\n        glClear(GL_COLOR_BUFFER_BIT)\n\n        # Display background\n        for shape in self.background.shapes:\n            self.draw_geometric2d(shape)\n\n        # Display objects\n        if len(self.data) > 0:\n            idx = self.time_count % len(self.data)\n            for shape in self.data[idx].shapes:\n                self.draw_geometric2d(shape)\n\n        self.time_count += 1\n        glFlush()",
  "def draw_geometric2d(shape):\n        \"\"\"\n        Draw a 2D shape, of type GeometricPrimitive\n        \"\"\"\n        if shape.type == \"POINTS\":\n            glBegin(GL_POINTS)\n        elif shape.type == \"LINES\":\n            glBegin(GL_LINES)\n        elif shape.type == \"LINE_STRIP\":\n            glBegin(GL_LINE_STRIP)\n        elif shape.type == \"LINE_LOOP\":\n            glBegin(GL_LINE_LOOP)\n        elif shape.type == \"POLYGON\":\n            glBegin(GL_POLYGON)\n        elif shape.type == \"TRIANGLES\":\n            glBegin(GL_TRIANGLES)\n        elif shape.type == \"TRIANGLE_STRIP\":\n            glBegin(GL_TRIANGLE_STRIP)\n        elif shape.type == \"TRIANGLE_FAN\":\n            glBegin(GL_TRIANGLE_FAN)\n        elif shape.type == \"QUADS\":\n            glBegin(GL_QUADS)\n        elif shape.type == \"QUAD_STRIP\":\n            glBegin(GL_QUAD_STRIP)\n        else:\n            logger.error(\"Invalid type for geometric primitive!\")\n            raise NameError\n\n        # set color\n        glColor3f(shape.color[0], shape.color[1], shape.color[2])\n\n        # create vertices\n        for vertex in shape.vertices:\n            glVertex2f(vertex[0], vertex[1])\n        glEnd()",
  "def run_graphics(self, loop=True):\n        \"\"\"\n        Sequentially displays scenes in self.data\n\n        If loop is True, keep rendering until user closes the window.\n        \"\"\"\n        global _IMPORT_SUCESSFUL\n\n        if _IMPORT_SUCESSFUL:\n            pg.init()\n            display = (self.window_width, self.window_height)\n            pg.display.set_mode(display, DOUBLEBUF | OPENGL)\n            pg.display.set_caption(self.window_name)\n            self.initGL()\n            while True:\n                for event in pg.event.get():\n                    if event.type == pg.QUIT:\n                        pg.quit()\n                        return\n                #\n                self.display()\n                #\n                pg.display.flip()\n                pg.time.wait(self.refresh_interval)\n\n                # if not loop, stop\n                if not loop:\n                    pg.quit()\n                    return\n        else:\n            logger.error(\"Not possible to render the environment, \\\npygame or pyopengl not installed.\")",
  "def get_video_data(self):\n        \"\"\"\n        Stores scenes in self.data in a list of numpy arrays that can be used\n        to save a video.\n        \"\"\"\n        global _IMPORT_SUCESSFUL\n\n        if _IMPORT_SUCESSFUL:\n            video_data = []\n\n            pg.init()\n            display = (self.window_width, self.window_height)\n            screen = pg.display.set_mode(display, DOUBLEBUF | OPENGL)\n            pg.display.set_caption(self.window_name)\n            self.initGL()\n\n            self.time_count = 0\n            while self.time_count <= len(self.data):\n                #\n                self.display()\n                #\n                pg.display.flip()\n\n                #\n                # See https://stackoverflow.com/a/42754578/5691288\n                #\n                string_image = pg.image.tostring(screen, 'RGB')\n                temp_surf = pg.image.fromstring(string_image,\n                                                (self.window_width,\n                                                 self.window_height), 'RGB')\n                tmp_arr = pg.surfarray.array3d(temp_surf)\n                imgdata = np.moveaxis(tmp_arr, 0, 1)\n                video_data.append(imgdata)\n\n            pg.quit()\n            return video_data\n        else:\n            logger.error(\"Not possible to render the environment, \\\npygame or pyopengl not installed.\")\n            return []",
  "class RenderInterface(ABC):\n    \"\"\"\n    Common interface for rendering in rlberry.\n    \"\"\"\n\n    def __init__(self):\n        self._rendering_enabled = False\n\n    def is_render_enabled(self):\n        return self._rendering_enabled\n\n    def enable_rendering(self):\n        self._rendering_enabled = True\n\n    def disable_rendering(self):\n        self._rendering_enabled = False\n\n    def save_video(self, filename, **kwargs):\n        \"\"\"\n        Save video file.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def render(self, **kwargs):\n        \"\"\"\n        Display on screen.\n        \"\"\"\n        pass",
  "class RenderInterface2D(RenderInterface):\n    \"\"\"\n    Interface for 2D rendering in rlberry.\n    \"\"\"\n\n    def __init__(self):\n        RenderInterface.__init__(self)\n        self._rendering_enabled = False\n        self._rendering_type = \"2d\"\n        self._state_history_for_rendering = []\n        self._refresh_interval = 50   # in milliseconds\n        self._clipping_area = (-1.0, 1.0, -1.0, 1.0)  # (left,right,bottom,top)\n\n        # rendering type, either 'pygame' or 'opengl'\n        self.renderer_type = 'opengl'\n\n    def get_renderer(self):\n        if self.renderer_type == 'opengl':\n            return OpenGLRender2D()\n        elif self.renderer_type == 'pygame':\n            return PyGameRender2D()\n        else:\n            raise NotImplementedError(\"Unknown renderer type.\")\n\n    @abstractmethod\n    def get_scene(self, state):\n        \"\"\"\n        Return scene (list of shapes) representing a given state\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        pass\n\n    def append_state_for_rendering(self, state):\n        self._state_history_for_rendering.append(state)\n\n    def set_refresh_interval(self, interval):\n        self._refresh_interval = interval\n\n    def clear_render_buffer(self):\n        self._state_history_for_rendering = []\n\n    def set_clipping_area(self, area):\n        self._clipping_area = area\n\n    def _get_background_and_scenes(self):\n        # background\n        background = self.get_background()\n\n        # data: convert states to scenes\n        scenes = []\n        for state in self._state_history_for_rendering:\n            scene = self.get_scene(state)\n            scenes.append(scene)\n        return background, scenes\n\n    def render(self, loop=True, **kwargs):\n        \"\"\"\n        Function to render an environment that implements the interface.\n        \"\"\"\n\n        if self.is_render_enabled():\n            # background and data\n            background, data = self._get_background_and_scenes()\n\n            if len(data) == 0:\n                logger.info(\"No data to render.\")\n                return\n\n            # render\n            renderer = self.get_renderer()\n\n            renderer.window_name = self.name\n            renderer.set_refresh_interval(self._refresh_interval)\n            renderer.set_clipping_area(self._clipping_area)\n            renderer.set_data(data)\n            renderer.set_background(background)\n            renderer.run_graphics(loop)\n            return 0\n        else:\n            logger.info(\"Rendering not enabled for the environment.\")\n            return 1\n\n    def save_video(self, filename, framerate=25, **kwargs):\n\n        # background and data\n        background, data = self._get_background_and_scenes()\n\n        if len(data) == 0:\n            logger.info(\"No data to save.\")\n            return\n\n        # get video data from renderer\n        renderer = self.get_renderer()\n        renderer.window_name = self.name\n        renderer.set_refresh_interval(self._refresh_interval)\n        renderer.set_clipping_area(self._clipping_area)\n        renderer.set_data(data)\n        renderer.set_background(background)\n\n        video_data = renderer.get_video_data()\n        video_write(filename, video_data, framerate=framerate)",
  "def __init__(self):\n        self._rendering_enabled = False",
  "def is_render_enabled(self):\n        return self._rendering_enabled",
  "def enable_rendering(self):\n        self._rendering_enabled = True",
  "def disable_rendering(self):\n        self._rendering_enabled = False",
  "def save_video(self, filename, **kwargs):\n        \"\"\"\n        Save video file.\n        \"\"\"\n        pass",
  "def render(self, **kwargs):\n        \"\"\"\n        Display on screen.\n        \"\"\"\n        pass",
  "def __init__(self):\n        RenderInterface.__init__(self)\n        self._rendering_enabled = False\n        self._rendering_type = \"2d\"\n        self._state_history_for_rendering = []\n        self._refresh_interval = 50   # in milliseconds\n        self._clipping_area = (-1.0, 1.0, -1.0, 1.0)  # (left,right,bottom,top)\n\n        # rendering type, either 'pygame' or 'opengl'\n        self.renderer_type = 'opengl'",
  "def get_renderer(self):\n        if self.renderer_type == 'opengl':\n            return OpenGLRender2D()\n        elif self.renderer_type == 'pygame':\n            return PyGameRender2D()\n        else:\n            raise NotImplementedError(\"Unknown renderer type.\")",
  "def get_scene(self, state):\n        \"\"\"\n        Return scene (list of shapes) representing a given state\n        \"\"\"\n        pass",
  "def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        pass",
  "def append_state_for_rendering(self, state):\n        self._state_history_for_rendering.append(state)",
  "def set_refresh_interval(self, interval):\n        self._refresh_interval = interval",
  "def clear_render_buffer(self):\n        self._state_history_for_rendering = []",
  "def set_clipping_area(self, area):\n        self._clipping_area = area",
  "def _get_background_and_scenes(self):\n        # background\n        background = self.get_background()\n\n        # data: convert states to scenes\n        scenes = []\n        for state in self._state_history_for_rendering:\n            scene = self.get_scene(state)\n            scenes.append(scene)\n        return background, scenes",
  "def render(self, loop=True, **kwargs):\n        \"\"\"\n        Function to render an environment that implements the interface.\n        \"\"\"\n\n        if self.is_render_enabled():\n            # background and data\n            background, data = self._get_background_and_scenes()\n\n            if len(data) == 0:\n                logger.info(\"No data to render.\")\n                return\n\n            # render\n            renderer = self.get_renderer()\n\n            renderer.window_name = self.name\n            renderer.set_refresh_interval(self._refresh_interval)\n            renderer.set_clipping_area(self._clipping_area)\n            renderer.set_data(data)\n            renderer.set_background(background)\n            renderer.run_graphics(loop)\n            return 0\n        else:\n            logger.info(\"Rendering not enabled for the environment.\")\n            return 1",
  "def save_video(self, filename, framerate=25, **kwargs):\n\n        # background and data\n        background, data = self._get_background_and_scenes()\n\n        if len(data) == 0:\n            logger.info(\"No data to save.\")\n            return\n\n        # get video data from renderer\n        renderer = self.get_renderer()\n        renderer.window_name = self.name\n        renderer.set_refresh_interval(self._refresh_interval)\n        renderer.set_clipping_area(self._clipping_area)\n        renderer.set_data(data)\n        renderer.set_background(background)\n\n        video_data = renderer.get_video_data()\n        video_write(filename, video_data, framerate=framerate)",
  "def representation_2d(state, env):\n    if isinstance(state, np.ndarray) and env.observation_space.shape == (2,):\n        return state\n    try:\n        from highway_env.envs.common.abstract import AbstractEnv\n        from highway_env.envs.common.observation import KinematicObservation\n        if isinstance(env.unwrapped, AbstractEnv) and \\\n                isinstance(env.unwrapped.observation_type, KinematicObservation):\n            return np.array([state[0, KinematicObservation.FEATURES.index(\"x\")],\n                            -state[0, KinematicObservation.FEATURES.index(\"y\")]])\n    except ImportError:\n        pass\n    return False",
  "def bar_shape(p0, p1, width):\n    shape = GeometricPrimitive(\"QUADS\")\n\n    x0, y0 = p0\n    x1, y1 = p1\n\n    direction = np.array([x1-x0, y1-y0])\n    norm = np.sqrt((direction*direction).sum())\n    direction = direction/norm\n\n    # get vector perpendicular to direction\n    u_vec = np.zeros(2)\n    u_vec[0] = -direction[1]\n    u_vec[1] = direction[0]\n\n    u_vec = u_vec*width/2\n\n    shape.add_vertex((x0+u_vec[0], y0+u_vec[1]))\n    shape.add_vertex((x0-u_vec[0], y0-u_vec[1]))\n    shape.add_vertex((x1-u_vec[0], y1-u_vec[1]))\n    shape.add_vertex((x1+u_vec[0], y1+u_vec[1]))\n    return shape",
  "def circle_shape(center, radius, n_points=50):\n    shape = GeometricPrimitive(\"POLYGON\")\n\n    x0, y0 = center\n    theta = np.linspace(0.0, 2*np.pi, n_points)\n    for tt in theta:\n        xx = radius*np.cos(tt)\n        yy = radius*np.sin(tt)\n        shape.add_vertex((x0+xx, y0+yy))\n\n    return shape",
  "class Scene:\n    \"\"\"\n    Class representing a scene, which is a vector of GeometricPrimitive objects\n    \"\"\"\n\n    def __init__(self):\n        self.shapes = []\n\n    def add_shape(self, shape):\n        self.shapes.append(shape)",
  "class GeometricPrimitive:\n    \"\"\"\n    Class representing an OpenGL geometric primitive.\n\n     Primitive type (GL_LINE_LOOP by defaut)\n\n     If using OpenGLRender2D, one of the following:\n           POINTS\n           LINES\n           LINE_STRIP\n           LINE_LOOP\n           POLYGON\n           TRIANGLES\n           TRIANGLE_STRIP\n           TRIANGLE_FAN\n           QUADS\n           QUAD_STRIP\n\n    If using PyGameRender2D:\n            POLYGON\n\n\n    TODO: Add support to more pygame shapes,\n    see https://www.pygame.org/docs/ref/draw.html\n    \"\"\"\n\n    def __init__(self, primitive_type=\"GL_LINE_LOOP\"):\n        # primitive type\n        self.type = primitive_type\n        # color in RGB\n        self.color = (0.25, 0.25, 0.25)\n        # list of vertices. each vertex is a tuple with coordinates in space\n        self.vertices = []\n\n    def add_vertex(self, vertex):\n        self.vertices.append(vertex)\n\n    def set_color(self, color):\n        self.color = color",
  "def __init__(self):\n        self.shapes = []",
  "def add_shape(self, shape):\n        self.shapes.append(shape)",
  "def __init__(self, primitive_type=\"GL_LINE_LOOP\"):\n        # primitive type\n        self.type = primitive_type\n        # color in RGB\n        self.color = (0.25, 0.25, 0.25)\n        # list of vertices. each vertex is a tuple with coordinates in space\n        self.vertices = []",
  "def add_vertex(self, vertex):\n        self.vertices.append(vertex)",
  "def set_color(self, color):\n        self.color = color",
  "class PyGameRender2D:\n    \"\"\"Class to render a list of scenes using pygame.\"\"\"\n\n    def __init__(self):\n        # parameters\n        self.window_width = 800\n        self.window_height = 800    # multiples of 16 are preferred\n        self.background_color = (150, 190, 255)\n        self.refresh_interval = 50\n        self.window_name = \"rlberry render\"\n        self.clipping_area = (-1.0, 1.0, -1.0, 1.0)\n\n        # time counter\n        self.time_count = 0\n\n        # background scene\n        self.background = Scene()\n        # data to be rendered (list of scenes)\n        self.data = []\n\n    def set_window_name(self, name):\n        self.window_name = name\n\n    def set_refresh_interval(self, interval):\n        self.refresh_interval = interval\n\n    def set_clipping_area(self, area):\n        \"\"\"\n        The clipping area is tuple with elements (left, right, bottom, top)\n        Default = (-1.0, 1.0, -1.0, 1.0)\n        \"\"\"\n        self.clipping_area = area\n        base_size = max(self.window_width, self.window_height)\n        width_range = area[1] - area[0]\n        height_range = area[3] - area[2]\n        base_range = max(width_range, height_range)\n        width_range /= base_range\n        height_range /= base_range\n        self.window_width = int(base_size * width_range)\n        self.window_height = int(base_size * height_range)\n\n        # width and height must be divisible by 2\n        if self.window_width % 2 == 1:\n            self.window_width += 1\n        if self.window_height % 2 == 1:\n            self.window_height += 1\n\n    def set_data(self, data):\n        self.data = data\n\n    def set_background(self, background):\n        self.background = background\n\n    def display(self):\n        \"\"\"\n        Callback function, handler for window re-paint\n        \"\"\"\n        # Set background color (clear background)\n        self.screen.fill(self.background_color)\n\n        # Display background\n        for shape in self.background.shapes:\n            self.draw_geometric2d(shape)\n\n        # Display objects\n        if len(self.data) > 0:\n            idx = self.time_count % len(self.data)\n            for shape in self.data[idx].shapes:\n                self.draw_geometric2d(shape)\n\n        self.time_count += 1\n\n    def draw_geometric2d(self, shape):\n        \"\"\"\n        Draw a 2D shape, of type GeometricPrimitive\n        \"\"\"\n        if shape.type in ['POLYGON']:\n            area = self.clipping_area\n            width_range = area[1] - area[0]\n            height_range = area[3] - area[2]\n\n            vertices = []\n            for vertex in shape.vertices:\n                xx = vertex[0]*self.window_width/width_range\n                yy = vertex[1]*self.window_height/height_range\n\n                # put origin at bottom left instead of top left\n                yy = self.window_height - yy\n\n                pg_vertex = (xx, yy)\n                vertices.append(pg_vertex)\n\n            color = (255*shape.color[0],\n                     255*shape.color[1],\n                     255*shape.color[2])\n            pg.draw.polygon(self.screen, color, vertices)\n\n        else:\n            raise NotImplementedError(\n                \"Shape type %s not implemented in pygame renderer.\"\n                % shape.type)\n\n    def run_graphics(self, loop=True):\n        \"\"\"\n        Sequentially displays scenes in self.data\n        \"\"\"\n        global _IMPORT_SUCESSFUL\n\n        if _IMPORT_SUCESSFUL:\n            pg.init()\n            display = (self.window_width, self.window_height)\n            self.screen = pg.display.set_mode(display)\n            pg.display.set_caption(self.window_name)\n            while True:\n                for event in pg.event.get():\n                    if event.type == pg.QUIT:\n                        pg.quit()\n                        return\n                #\n                self.display()\n                #\n                pg.display.flip()\n                pg.time.wait(self.refresh_interval)\n\n                # if not loop, stop\n                if not loop:\n                    pg.quit()\n                    return\n        else:\n            logger.error(\"Not possible to render the environment, \\\npygame or pyopengl not installed.\")\n\n    def get_video_data(self):\n        \"\"\"\n        Stores scenes in self.data in a list of numpy arrays that can be used\n        to save a video.\n        \"\"\"\n        global _IMPORT_SUCESSFUL\n\n        if _IMPORT_SUCESSFUL:\n            video_data = []\n\n            pg.init()\n            display = (self.window_width, self.window_height)\n            self.screen = pg.display.set_mode(display)\n            pg.display.set_caption(self.window_name)\n\n            self.time_count = 0\n            while self.time_count <= len(self.data):\n                #\n                self.display()\n                #\n                pg.display.flip()\n\n                #\n                # See https://stackoverflow.com/a/42754578/5691288\n                #\n                string_image = pg.image.tostring(self.screen, 'RGB')\n                temp_surf = pg.image.fromstring(string_image,\n                                                (self.window_width,\n                                                 self.window_height), 'RGB')\n                tmp_arr = pg.surfarray.array3d(temp_surf)\n                imgdata = np.moveaxis(tmp_arr, 0, 1)\n                video_data.append(imgdata)\n\n            pg.quit()\n            return video_data\n        else:\n            logger.error(\"Not possible to render the environment, pygame \\\nor pyopengl not installed.\")\n            return []",
  "def __init__(self):\n        # parameters\n        self.window_width = 800\n        self.window_height = 800    # multiples of 16 are preferred\n        self.background_color = (150, 190, 255)\n        self.refresh_interval = 50\n        self.window_name = \"rlberry render\"\n        self.clipping_area = (-1.0, 1.0, -1.0, 1.0)\n\n        # time counter\n        self.time_count = 0\n\n        # background scene\n        self.background = Scene()\n        # data to be rendered (list of scenes)\n        self.data = []",
  "def set_window_name(self, name):\n        self.window_name = name",
  "def set_refresh_interval(self, interval):\n        self.refresh_interval = interval",
  "def set_clipping_area(self, area):\n        \"\"\"\n        The clipping area is tuple with elements (left, right, bottom, top)\n        Default = (-1.0, 1.0, -1.0, 1.0)\n        \"\"\"\n        self.clipping_area = area\n        base_size = max(self.window_width, self.window_height)\n        width_range = area[1] - area[0]\n        height_range = area[3] - area[2]\n        base_range = max(width_range, height_range)\n        width_range /= base_range\n        height_range /= base_range\n        self.window_width = int(base_size * width_range)\n        self.window_height = int(base_size * height_range)\n\n        # width and height must be divisible by 2\n        if self.window_width % 2 == 1:\n            self.window_width += 1\n        if self.window_height % 2 == 1:\n            self.window_height += 1",
  "def set_data(self, data):\n        self.data = data",
  "def set_background(self, background):\n        self.background = background",
  "def display(self):\n        \"\"\"\n        Callback function, handler for window re-paint\n        \"\"\"\n        # Set background color (clear background)\n        self.screen.fill(self.background_color)\n\n        # Display background\n        for shape in self.background.shapes:\n            self.draw_geometric2d(shape)\n\n        # Display objects\n        if len(self.data) > 0:\n            idx = self.time_count % len(self.data)\n            for shape in self.data[idx].shapes:\n                self.draw_geometric2d(shape)\n\n        self.time_count += 1",
  "def draw_geometric2d(self, shape):\n        \"\"\"\n        Draw a 2D shape, of type GeometricPrimitive\n        \"\"\"\n        if shape.type in ['POLYGON']:\n            area = self.clipping_area\n            width_range = area[1] - area[0]\n            height_range = area[3] - area[2]\n\n            vertices = []\n            for vertex in shape.vertices:\n                xx = vertex[0]*self.window_width/width_range\n                yy = vertex[1]*self.window_height/height_range\n\n                # put origin at bottom left instead of top left\n                yy = self.window_height - yy\n\n                pg_vertex = (xx, yy)\n                vertices.append(pg_vertex)\n\n            color = (255*shape.color[0],\n                     255*shape.color[1],\n                     255*shape.color[2])\n            pg.draw.polygon(self.screen, color, vertices)\n\n        else:\n            raise NotImplementedError(\n                \"Shape type %s not implemented in pygame renderer.\"\n                % shape.type)",
  "def run_graphics(self, loop=True):\n        \"\"\"\n        Sequentially displays scenes in self.data\n        \"\"\"\n        global _IMPORT_SUCESSFUL\n\n        if _IMPORT_SUCESSFUL:\n            pg.init()\n            display = (self.window_width, self.window_height)\n            self.screen = pg.display.set_mode(display)\n            pg.display.set_caption(self.window_name)\n            while True:\n                for event in pg.event.get():\n                    if event.type == pg.QUIT:\n                        pg.quit()\n                        return\n                #\n                self.display()\n                #\n                pg.display.flip()\n                pg.time.wait(self.refresh_interval)\n\n                # if not loop, stop\n                if not loop:\n                    pg.quit()\n                    return\n        else:\n            logger.error(\"Not possible to render the environment, \\\npygame or pyopengl not installed.\")",
  "def get_video_data(self):\n        \"\"\"\n        Stores scenes in self.data in a list of numpy arrays that can be used\n        to save a video.\n        \"\"\"\n        global _IMPORT_SUCESSFUL\n\n        if _IMPORT_SUCESSFUL:\n            video_data = []\n\n            pg.init()\n            display = (self.window_width, self.window_height)\n            self.screen = pg.display.set_mode(display)\n            pg.display.set_caption(self.window_name)\n\n            self.time_count = 0\n            while self.time_count <= len(self.data):\n                #\n                self.display()\n                #\n                pg.display.flip()\n\n                #\n                # See https://stackoverflow.com/a/42754578/5691288\n                #\n                string_image = pg.image.tostring(self.screen, 'RGB')\n                temp_surf = pg.image.fromstring(string_image,\n                                                (self.window_width,\n                                                 self.window_height), 'RGB')\n                tmp_arr = pg.surfarray.array3d(temp_surf)\n                imgdata = np.moveaxis(tmp_arr, 0, 1)\n                video_data.append(imgdata)\n\n            pg.quit()\n            return video_data\n        else:\n            logger.error(\"Not possible to render the environment, pygame \\\nor pyopengl not installed.\")\n            return []",
  "class AutoResetWrapper(Wrapper):\n    \"\"\"\n    Auto reset the environment after \"horizon\" steps have passed.\n    \"\"\"\n    def __init__(self, env, horizon):\n        \"\"\"\n        Parameters\n        ----------\n        horizon: int\n        \"\"\"\n        Wrapper.__init__(self, env)\n        self.horizon = horizon\n        assert self.horizon >= 1\n        self.current_step = 0\n\n    def reset(self):\n        self.current_step = 0\n        return self.env.reset()\n\n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        self.current_step += 1\n        # At H, always return to the initial state.\n        # Also, set done to True.\n        if self.current_step == self.horizon:\n            self.current_step = 0\n            observation = self.env.reset()\n            done = True\n        return observation, reward, done, info",
  "def __init__(self, env, horizon):\n        \"\"\"\n        Parameters\n        ----------\n        horizon: int\n        \"\"\"\n        Wrapper.__init__(self, env)\n        self.horizon = horizon\n        assert self.horizon >= 1\n        self.current_step = 0",
  "def reset(self):\n        self.current_step = 0\n        return self.env.reset()",
  "def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        self.current_step += 1\n        # At H, always return to the initial state.\n        # Also, set done to True.\n        if self.current_step == self.horizon:\n            self.current_step = 0\n            observation = self.env.reset()\n            done = True\n        return observation, reward, done, info",
  "class UncertaintyEstimatorWrapper(Wrapper):\n    \"\"\"\n    Adds exploration bonuses to the info output of env.step(), according to an\n    instance of UncertaintyEstimator.\n\n    Example\n    -------\n\n    ```\n    observation, reward, done, info = env.step(action)\n    bonus = info['exploration_bonus']\n    ```\n\n    Parameters\n    ----------\n    uncertainty_estimator_fn : function(observation_space,\n                                        action_space, **kwargs)\n        Function that gives an instance of UncertaintyEstimator,\n        used to compute bonus.\n    uncertainty_estimator_kwargs:\n        kwargs for uncertainty_estimator_fn\n    bonus_scale_factor : double\n            Scale factor for the bonus.\n    \"\"\"\n\n    def __init__(self,\n                 env,\n                 uncertainty_estimator_fn,\n                 uncertainty_estimator_kwargs=None,\n                 bonus_scale_factor=1.0,\n                 bonus_max=np.inf):\n        Wrapper.__init__(self, env)\n\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_max = bonus_max\n        uncertainty_estimator_kwargs = uncertainty_estimator_kwargs or {}\n\n        uncertainty_estimator_fn = load(uncertainty_estimator_fn) if isinstance(uncertainty_estimator_fn, str) else \\\n            uncertainty_estimator_fn\n        self.uncertainty_estimator = uncertainty_estimator_fn(\n                                        env.observation_space,\n                                        env.action_space,\n                                        **uncertainty_estimator_kwargs)\n        self.previous_obs = None\n\n    def reset(self):\n        self.previous_obs = self.env.reset()\n        return self.previous_obs\n\n    def _update_and_get_bonus(self, state, action, next_state, reward):\n        if self.previous_obs is None:\n            return 0.0\n        #\n        self.uncertainty_estimator.update(state,\n                                          action,\n                                          next_state,\n                                          reward)\n        return self.bonus(state, action)\n\n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n\n        # update uncertainty and compute bonus\n        bonus = self._update_and_get_bonus(self.previous_obs,\n                                           action,\n                                           observation,\n                                           reward)\n        #\n        self.previous_obs = observation\n\n        # add bonus to info\n        if info is None:\n            info = {}\n        else:\n            if 'exploration_bonus' in info:\n                logger.error(\"UncertaintyEstimatorWrapper Error: info has\" +\n                             \"  already a key named exploration_bonus!\")\n\n        info['exploration_bonus'] = bonus\n\n        return observation, reward, done, info\n\n    def sample(self, state, action):\n        logger.warning(\n            '[UncertaintyEstimatorWrapper]: sample()'\n            + ' method does not consider nor update bonuses.')\n        return self.env.sample(state, action)\n\n    def bonus(self, state, action=None):\n        bonus = self.bonus_scale_factor * self.uncertainty_estimator.measure(state, action)\n        return np.clip(bonus, 0, self.bonus_max)\n\n    def bonus_batch(self, states, actions=None):\n        bonus = self.bonus_scale_factor * self.uncertainty_estimator.measure_batch(states, actions)\n        return np.clip(bonus, 0, self.bonus_max) if isinstance(bonus, np.ndarray) else torch.clamp(bonus, 0, self.bonus_max)",
  "def __init__(self,\n                 env,\n                 uncertainty_estimator_fn,\n                 uncertainty_estimator_kwargs=None,\n                 bonus_scale_factor=1.0,\n                 bonus_max=np.inf):\n        Wrapper.__init__(self, env)\n\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_max = bonus_max\n        uncertainty_estimator_kwargs = uncertainty_estimator_kwargs or {}\n\n        uncertainty_estimator_fn = load(uncertainty_estimator_fn) if isinstance(uncertainty_estimator_fn, str) else \\\n            uncertainty_estimator_fn\n        self.uncertainty_estimator = uncertainty_estimator_fn(\n                                        env.observation_space,\n                                        env.action_space,\n                                        **uncertainty_estimator_kwargs)\n        self.previous_obs = None",
  "def reset(self):\n        self.previous_obs = self.env.reset()\n        return self.previous_obs",
  "def _update_and_get_bonus(self, state, action, next_state, reward):\n        if self.previous_obs is None:\n            return 0.0\n        #\n        self.uncertainty_estimator.update(state,\n                                          action,\n                                          next_state,\n                                          reward)\n        return self.bonus(state, action)",
  "def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n\n        # update uncertainty and compute bonus\n        bonus = self._update_and_get_bonus(self.previous_obs,\n                                           action,\n                                           observation,\n                                           reward)\n        #\n        self.previous_obs = observation\n\n        # add bonus to info\n        if info is None:\n            info = {}\n        else:\n            if 'exploration_bonus' in info:\n                logger.error(\"UncertaintyEstimatorWrapper Error: info has\" +\n                             \"  already a key named exploration_bonus!\")\n\n        info['exploration_bonus'] = bonus\n\n        return observation, reward, done, info",
  "def sample(self, state, action):\n        logger.warning(\n            '[UncertaintyEstimatorWrapper]: sample()'\n            + ' method does not consider nor update bonuses.')\n        return self.env.sample(state, action)",
  "def bonus(self, state, action=None):\n        bonus = self.bonus_scale_factor * self.uncertainty_estimator.measure(state, action)\n        return np.clip(bonus, 0, self.bonus_max)",
  "def bonus_batch(self, states, actions=None):\n        bonus = self.bonus_scale_factor * self.uncertainty_estimator.measure_batch(states, actions)\n        return np.clip(bonus, 0, self.bonus_max) if isinstance(bonus, np.ndarray) else torch.clamp(bonus, 0, self.bonus_max)",
  "class ScalarizeEnvWrapper(Wrapper):\n    \"\"\"\n    Wrapper for stable_baselines VecEnv, so that they accept non-vectorized actions,\n    and return non-vectorized states.\n    \"\"\"\n\n    def __init__(self, env):\n        Wrapper.__init__(self, env)\n\n    def reset(self):\n        obs = self.env.reset()\n        return obs[0]\n\n    def step(self, action):\n        observation, reward, done, info = self.env.step([action])\n        return observation[0], reward[0], done[0], info[0]",
  "def __init__(self, env):\n        Wrapper.__init__(self, env)",
  "def reset(self):\n        obs = self.env.reset()\n        return obs[0]",
  "def step(self, action):\n        observation, reward, done, info = self.env.step([action])\n        return observation[0], reward[0], done[0], info[0]",
  "class Transition:\n    def __init__(self, raw_state, state, action, reward, n_total_visits, n_episode_visits):\n        self.raw_state = raw_state\n        self.state = state\n        self.action = action\n        self.reward = reward\n        self.n_total_visits = n_total_visits\n        self.n_episode_visits = n_episode_visits",
  "class TrajectoryMemory:\n    def __init__(self, max_size):\n        self.max_size = max_size\n        self.clear()\n\n    def clear(self):\n        self.n_trajectories = 0\n        # current trajectory\n        self.current_traj_transitions = []\n        # all trajectories\n        self.trajectories = []\n\n    def append(self, transition):\n        self.current_traj_transitions.append(transition)\n\n    def end_trajectory(self):\n        if len(self.current_traj_transitions) == 0:\n            return\n\n        self.n_trajectories += 1\n\n        # store data\n        self.trajectories.append(self.current_traj_transitions)\n\n        if self.n_trajectories > self.max_size:\n            self.trajectories.pop(0)\n            self.n_trajectories = self.max_size\n\n        # reset data\n        self.current_traj_transitions = []\n\n    def is_empty(self):\n        return self.n_trajectories > 0",
  "def identity(state, env, **kwargs):\n    return state",
  "class Vis2dWrapper(Wrapper):\n    \"\"\"\n    Stores and visualizes the trajectories environments with 2d box observation spaces\n    and discrete action spaces.\n\n    Parameters\n    ----------\n    env: gym.Env\n    n_bins_obs : int, default = 10\n        Number of intervals to discretize each dimension of the observation space.\n        Used to count number of visits.\n    memory_size : int, default = 100\n        Maximum number of trajectories to keep in memory.\n        The most recent ones are kept.\n    state_preprocess_fn : callable(state, env, **kwargs)-> np.ndarray, default: None\n        Function that converts the state to a 2d array\n    state_preprocess_kwargs : dict, default: None\n        kwargs for state_preprocess_fn\n    \"\"\"\n    def __init__(self,\n                 env,\n                 n_bins_obs=10,\n                 memory_size=100,\n                 state_preprocess_fn=None,\n                 state_preprocess_kwargs=None):\n        Wrapper.__init__(self, env)\n\n        if state_preprocess_fn is None:\n            assert isinstance(env.observation_space, spaces.Box)\n        assert isinstance(env.action_space, spaces.Discrete)\n\n        self.state_preprocess_fn = state_preprocess_fn or identity\n        self.state_preprocess_kwargs = state_preprocess_kwargs or {}\n\n        self.memory = TrajectoryMemory(memory_size)\n        self.total_visit_counter = DiscreteCounter(self.env.observation_space,\n                                                   self.env.action_space,\n                                                   n_bins_obs=n_bins_obs)\n        self.episode_visit_counter = DiscreteCounter(self.env.observation_space,\n                                                     self.env.action_space,\n                                                     n_bins_obs=n_bins_obs)\n        self.current_state = None\n        self.curret_step = 0\n\n    def reset(self):\n        self.current_step = 0\n        self.current_state = self.env.reset()\n        return self.current_state\n\n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        # initialize new trajectory\n        if self.current_step == 0:\n            self.memory.end_trajectory()\n            self.episode_visit_counter.reset()\n        self.current_step += 1\n        # update counters\n        ss, aa = self.current_state, action\n        ns = observation\n        self.total_visit_counter.update(ss, aa, ns, reward)\n        self.episode_visit_counter.update(ss, aa, ns, reward)\n        # store transition\n        transition = Transition(ss,\n                                self.state_preprocess_fn(ss, self.env, **self.state_preprocess_kwargs),\n                                aa,\n                                reward,\n                                self.total_visit_counter.count(ss, aa),\n                                self.episode_visit_counter.count(ss, aa))\n        self.memory.append(transition)\n        # update current state\n        self.current_state = observation\n        return observation, reward, done, info\n\n    def plot_trajectories(self,\n                          fignum=None,\n                          figsize=(6, 6),\n                          hide_axis=True,\n                          show=True,\n                          video_filename=None,\n                          colormap_name='cool',\n                          framerate=15,\n                          n_skip=1,\n                          dot_scale_factor=2.5,\n                          alpha=0.25,\n                          xlim=None,\n                          ylim=None,\n                          dot_size_means='episode_visits'):\n        \"\"\"\n        Plot history of trajectories in a scatter plot.\n        Colors distinguish recent and old trajectories, the size of the dots represent\n        the number of visits to a state.\n\n        If video_filename is given, a video file is saved. Otherwise,\n        plot only the final frame.\n\n        Parameters\n        ----------\n        fignum : str\n            Figure name\n        figsize : (float, float)\n            (width, height) of the image in inches.\n        hide_axis : bool\n            If True, axes are hidden.\n        show : bool\n            If True, calls plt.show()\n        video_filename : str or None\n            If not None, save a video with given filename.\n        colormap_name : str, default = 'cool'\n            Colormap name.\n            See https://matplotlib.org/tutorials/colors/colormaps.html\n        framerate : int, default: 15\n            Video framerate.\n        n_skip : int, default: 1\n            Skip period: every n_skip trajectories, one trajectory is plotted.\n        dot_scale_factor : double\n            Scale factor for scatter plot points.\n        alpha : float, default: 0.25\n            The alpha blending value, between 0 (transparent) and 1 (opaque).\n        xlim: list, default: None\n            x plot limits, set to [0, 1] if None\n        ylim: list, default: None\n            y plot limits, set to [0, 1] if None\n        dot_size_means : str, {'episode_visits' or 'total_visits'}, default: 'episode_visits'\n            Whether to scale the dot size with the number of visits in an episode\n            or the total number of visits during the whole interaction.\n        \"\"\"\n        logger.info(\"Plotting...\")\n\n        fignum = fignum or str(self)\n        colormap_fn = plt.get_cmap(colormap_name)\n\n        # discretizer\n        try:\n            discretizer = self.episode_visit_counter.state_discretizer\n            epsilon = min(discretizer._bins[0][1] - discretizer._bins[0][0],\n                          discretizer._bins[1][1] - discretizer._bins[1][0])\n        except Exception:\n            epsilon = 0.01\n\n        # figure setup\n        xlim = xlim or [0.0, 1.0]\n        ylim = ylim or [0.0, 1.0]\n\n        fig = plt.figure(fignum, figsize=figsize)\n        fig.clf()\n        canvas = FigureCanvas(fig)\n        images = []\n        ax = fig.gca()\n\n        ax.set_xlim(xlim)\n        ax.set_ylim(ylim)\n\n        if hide_axis:\n            ax.set_axis_off()\n\n        # scatter plot\n        indices = np.arange(self.memory.n_trajectories)[::n_skip]\n\n        for idx in indices:\n            traj = self.memory.trajectories[idx]\n            color_time_intensity = (idx+1)/self.memory.n_trajectories\n            color = colormap_fn(color_time_intensity)\n\n            states = np.array([traj[ii].state for ii in range(len(traj))])\n\n            if dot_size_means == 'episode_visits':\n                sizes = np.array(\n                    [traj[ii].n_episode_visits for ii in range(len(traj))]\n                )\n            elif dot_size_means == 'total_visits':\n                raw_states = [traj[ii].raw_state for ii in range(len(traj))]\n                sizes = np.array(\n                    [\n                        np.sum([self.total_visit_counter.count(ss, aa) for aa in range(self.env.action_space.n)])\n                        for ss in raw_states\n                    ]\n                )\n            else:\n                raise ValueError()\n\n            sizes = 1 + sizes\n            sizes = (dot_scale_factor**2) * 100 * epsilon * sizes / sizes.max()\n\n            ax.scatter(x=states[:, 0], y=states[:, 1], color=color, s=sizes, alpha=alpha)\n            plt.tight_layout()\n\n            if video_filename is not None:\n                canvas.draw()\n                image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n                image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n                images.append(image_from_plot)\n\n        if video_filename is not None:\n            logger.info(\"... writing video ...\")\n            video_write(video_filename, images, framerate=framerate)\n\n        logger.info(\"... done!\")\n\n        if show:\n            plt.show()\n\n    def plot_trajectory_actions(self,\n                                fignum=None,\n                                figsize=(8, 6),\n                                n_traj_to_show=10,\n                                hide_axis=True,\n                                show=True,\n                                video_filename=None,\n                                colormap_name='Paired',\n                                framerate=15,\n                                n_skip=1,\n                                dot_scale_factor=2.5,\n                                alpha=1.0,\n                                action_description=None,\n                                xlim=None,\n                                ylim=None):\n        \"\"\"\n        Plot actions (one action = one color) chosen in recent trajectories.\n\n        If video_filename is given, a video file is saved showing the evolution of\n        the actions taken in past trajectories.\n\n        Parameters\n        ----------\n        fignum : str\n            Figure name\n        figsize : (float, float)\n            (width, height) of the image in inches.\n        n_traj_to_show : int\n            Number of trajectories to visualize in each frame.\n        hide_axis : bool\n            If True, axes are hidden.\n        show : bool\n            If True, calls plt.show()\n        video_filename : str or None\n            If not None, save a video with given filename.\n        colormap_name : str, default = 'tab20b'\n            Colormap name.\n            See https://matplotlib.org/tutorials/colors/colormaps.html\n        framerate : int, default: 15\n            Video framerate.\n        n_skip : int, default: 1\n            Skip period: every n_skip trajectories, one trajectory is plotted.\n        dot_scale_factor : double\n            Scale factor for scatter plot points.\n        alpha : float, default: 1.0\n            The alpha blending value, between 0 (transparent) and 1 (opaque).\n        action_description : list or None (optional)\n            List (of strings) containing a description of each action.\n            For instance, ['left', 'right', 'up', 'down'].\n        xlim: list, default: None\n            x plot limits, set to [0, 1] if None\n        ylim: list, default: None\n            y plot limits, set to [0, 1] if None\n        \"\"\"\n        logger.info(\"Plotting...\")\n\n        fignum = fignum or (str(self)+'-actions')\n        colormap_fn = plt.get_cmap(colormap_name)\n        action_description = action_description or list(range(self.env.action_space.n))\n\n        # discretizer\n        try:\n            discretizer = self.episode_visit_counter.state_discretizer\n            epsilon = min(discretizer._bins[0][1] - discretizer._bins[0][0],\n                          discretizer._bins[1][1] - discretizer._bins[1][0])\n        except Exception:\n            epsilon = 0.01\n\n        # figure setup\n        xlim = xlim or [0.0, 1.0]\n        ylim = ylim or [0.0, 1.0]\n\n        # indices to visualize\n        if video_filename is None:\n            indices = [self.memory.n_trajectories-1]\n        else:\n            indices = np.arange(self.memory.n_trajectories)[::n_skip]\n\n        # images for video\n        images = []\n\n        # for idx in indices:\n        for init_idx in indices:\n            idx_set = range(max(0, init_idx-n_traj_to_show+1), init_idx+1)\n            # clear before showing new trajectories\n            fig = plt.figure(fignum, figsize=figsize)\n            fig.clf()\n            canvas = FigureCanvas(fig)\n            ax = fig.gca()\n\n            ax.set_xlim(xlim)\n            ax.set_ylim(ylim)\n\n            if hide_axis:\n                ax.set_axis_off()\n\n            for idx in idx_set:\n                traj = self.memory.trajectories[idx]\n\n                states = np.array([traj[ii].state for ii in range(len(traj))])\n                actions = np.array([traj[ii].action for ii in range(len(traj))])\n\n                sizes = (dot_scale_factor**2) * 750 * epsilon\n\n                for aa in range(self.env.action_space.n):\n                    states_aa = states[actions == aa]\n                    color = colormap_fn(aa/self.env.action_space.n)\n                    ax.scatter(x=states_aa[:, 0], y=states_aa[:, 1], color=color,\n                               s=sizes, alpha=alpha,\n                               label=f'action = {action_description[aa]}')\n\n            # for unique legend entries, source: https://stackoverflow.com/a/57600060\n            plt.legend(*[*zip(*{l: h for h, l in zip(*ax.get_legend_handles_labels())}.items())][::-1],\n                       loc='upper left', bbox_to_anchor=(1.00, 1.00))\n            plt.tight_layout()\n\n            if video_filename is not None:\n                canvas.draw()\n                image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n                image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n                images.append(image_from_plot)\n\n        if video_filename is not None:\n            logger.info(\"... writing video ...\")\n            video_write(video_filename, images, framerate=framerate)\n\n        logger.info(\"... done!\")\n\n        if show:\n            plt.show()",
  "def __init__(self, raw_state, state, action, reward, n_total_visits, n_episode_visits):\n        self.raw_state = raw_state\n        self.state = state\n        self.action = action\n        self.reward = reward\n        self.n_total_visits = n_total_visits\n        self.n_episode_visits = n_episode_visits",
  "def __init__(self, max_size):\n        self.max_size = max_size\n        self.clear()",
  "def clear(self):\n        self.n_trajectories = 0\n        # current trajectory\n        self.current_traj_transitions = []\n        # all trajectories\n        self.trajectories = []",
  "def append(self, transition):\n        self.current_traj_transitions.append(transition)",
  "def end_trajectory(self):\n        if len(self.current_traj_transitions) == 0:\n            return\n\n        self.n_trajectories += 1\n\n        # store data\n        self.trajectories.append(self.current_traj_transitions)\n\n        if self.n_trajectories > self.max_size:\n            self.trajectories.pop(0)\n            self.n_trajectories = self.max_size\n\n        # reset data\n        self.current_traj_transitions = []",
  "def is_empty(self):\n        return self.n_trajectories > 0",
  "def __init__(self,\n                 env,\n                 n_bins_obs=10,\n                 memory_size=100,\n                 state_preprocess_fn=None,\n                 state_preprocess_kwargs=None):\n        Wrapper.__init__(self, env)\n\n        if state_preprocess_fn is None:\n            assert isinstance(env.observation_space, spaces.Box)\n        assert isinstance(env.action_space, spaces.Discrete)\n\n        self.state_preprocess_fn = state_preprocess_fn or identity\n        self.state_preprocess_kwargs = state_preprocess_kwargs or {}\n\n        self.memory = TrajectoryMemory(memory_size)\n        self.total_visit_counter = DiscreteCounter(self.env.observation_space,\n                                                   self.env.action_space,\n                                                   n_bins_obs=n_bins_obs)\n        self.episode_visit_counter = DiscreteCounter(self.env.observation_space,\n                                                     self.env.action_space,\n                                                     n_bins_obs=n_bins_obs)\n        self.current_state = None\n        self.curret_step = 0",
  "def reset(self):\n        self.current_step = 0\n        self.current_state = self.env.reset()\n        return self.current_state",
  "def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        # initialize new trajectory\n        if self.current_step == 0:\n            self.memory.end_trajectory()\n            self.episode_visit_counter.reset()\n        self.current_step += 1\n        # update counters\n        ss, aa = self.current_state, action\n        ns = observation\n        self.total_visit_counter.update(ss, aa, ns, reward)\n        self.episode_visit_counter.update(ss, aa, ns, reward)\n        # store transition\n        transition = Transition(ss,\n                                self.state_preprocess_fn(ss, self.env, **self.state_preprocess_kwargs),\n                                aa,\n                                reward,\n                                self.total_visit_counter.count(ss, aa),\n                                self.episode_visit_counter.count(ss, aa))\n        self.memory.append(transition)\n        # update current state\n        self.current_state = observation\n        return observation, reward, done, info",
  "def plot_trajectories(self,\n                          fignum=None,\n                          figsize=(6, 6),\n                          hide_axis=True,\n                          show=True,\n                          video_filename=None,\n                          colormap_name='cool',\n                          framerate=15,\n                          n_skip=1,\n                          dot_scale_factor=2.5,\n                          alpha=0.25,\n                          xlim=None,\n                          ylim=None,\n                          dot_size_means='episode_visits'):\n        \"\"\"\n        Plot history of trajectories in a scatter plot.\n        Colors distinguish recent and old trajectories, the size of the dots represent\n        the number of visits to a state.\n\n        If video_filename is given, a video file is saved. Otherwise,\n        plot only the final frame.\n\n        Parameters\n        ----------\n        fignum : str\n            Figure name\n        figsize : (float, float)\n            (width, height) of the image in inches.\n        hide_axis : bool\n            If True, axes are hidden.\n        show : bool\n            If True, calls plt.show()\n        video_filename : str or None\n            If not None, save a video with given filename.\n        colormap_name : str, default = 'cool'\n            Colormap name.\n            See https://matplotlib.org/tutorials/colors/colormaps.html\n        framerate : int, default: 15\n            Video framerate.\n        n_skip : int, default: 1\n            Skip period: every n_skip trajectories, one trajectory is plotted.\n        dot_scale_factor : double\n            Scale factor for scatter plot points.\n        alpha : float, default: 0.25\n            The alpha blending value, between 0 (transparent) and 1 (opaque).\n        xlim: list, default: None\n            x plot limits, set to [0, 1] if None\n        ylim: list, default: None\n            y plot limits, set to [0, 1] if None\n        dot_size_means : str, {'episode_visits' or 'total_visits'}, default: 'episode_visits'\n            Whether to scale the dot size with the number of visits in an episode\n            or the total number of visits during the whole interaction.\n        \"\"\"\n        logger.info(\"Plotting...\")\n\n        fignum = fignum or str(self)\n        colormap_fn = plt.get_cmap(colormap_name)\n\n        # discretizer\n        try:\n            discretizer = self.episode_visit_counter.state_discretizer\n            epsilon = min(discretizer._bins[0][1] - discretizer._bins[0][0],\n                          discretizer._bins[1][1] - discretizer._bins[1][0])\n        except Exception:\n            epsilon = 0.01\n\n        # figure setup\n        xlim = xlim or [0.0, 1.0]\n        ylim = ylim or [0.0, 1.0]\n\n        fig = plt.figure(fignum, figsize=figsize)\n        fig.clf()\n        canvas = FigureCanvas(fig)\n        images = []\n        ax = fig.gca()\n\n        ax.set_xlim(xlim)\n        ax.set_ylim(ylim)\n\n        if hide_axis:\n            ax.set_axis_off()\n\n        # scatter plot\n        indices = np.arange(self.memory.n_trajectories)[::n_skip]\n\n        for idx in indices:\n            traj = self.memory.trajectories[idx]\n            color_time_intensity = (idx+1)/self.memory.n_trajectories\n            color = colormap_fn(color_time_intensity)\n\n            states = np.array([traj[ii].state for ii in range(len(traj))])\n\n            if dot_size_means == 'episode_visits':\n                sizes = np.array(\n                    [traj[ii].n_episode_visits for ii in range(len(traj))]\n                )\n            elif dot_size_means == 'total_visits':\n                raw_states = [traj[ii].raw_state for ii in range(len(traj))]\n                sizes = np.array(\n                    [\n                        np.sum([self.total_visit_counter.count(ss, aa) for aa in range(self.env.action_space.n)])\n                        for ss in raw_states\n                    ]\n                )\n            else:\n                raise ValueError()\n\n            sizes = 1 + sizes\n            sizes = (dot_scale_factor**2) * 100 * epsilon * sizes / sizes.max()\n\n            ax.scatter(x=states[:, 0], y=states[:, 1], color=color, s=sizes, alpha=alpha)\n            plt.tight_layout()\n\n            if video_filename is not None:\n                canvas.draw()\n                image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n                image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n                images.append(image_from_plot)\n\n        if video_filename is not None:\n            logger.info(\"... writing video ...\")\n            video_write(video_filename, images, framerate=framerate)\n\n        logger.info(\"... done!\")\n\n        if show:\n            plt.show()",
  "def plot_trajectory_actions(self,\n                                fignum=None,\n                                figsize=(8, 6),\n                                n_traj_to_show=10,\n                                hide_axis=True,\n                                show=True,\n                                video_filename=None,\n                                colormap_name='Paired',\n                                framerate=15,\n                                n_skip=1,\n                                dot_scale_factor=2.5,\n                                alpha=1.0,\n                                action_description=None,\n                                xlim=None,\n                                ylim=None):\n        \"\"\"\n        Plot actions (one action = one color) chosen in recent trajectories.\n\n        If video_filename is given, a video file is saved showing the evolution of\n        the actions taken in past trajectories.\n\n        Parameters\n        ----------\n        fignum : str\n            Figure name\n        figsize : (float, float)\n            (width, height) of the image in inches.\n        n_traj_to_show : int\n            Number of trajectories to visualize in each frame.\n        hide_axis : bool\n            If True, axes are hidden.\n        show : bool\n            If True, calls plt.show()\n        video_filename : str or None\n            If not None, save a video with given filename.\n        colormap_name : str, default = 'tab20b'\n            Colormap name.\n            See https://matplotlib.org/tutorials/colors/colormaps.html\n        framerate : int, default: 15\n            Video framerate.\n        n_skip : int, default: 1\n            Skip period: every n_skip trajectories, one trajectory is plotted.\n        dot_scale_factor : double\n            Scale factor for scatter plot points.\n        alpha : float, default: 1.0\n            The alpha blending value, between 0 (transparent) and 1 (opaque).\n        action_description : list or None (optional)\n            List (of strings) containing a description of each action.\n            For instance, ['left', 'right', 'up', 'down'].\n        xlim: list, default: None\n            x plot limits, set to [0, 1] if None\n        ylim: list, default: None\n            y plot limits, set to [0, 1] if None\n        \"\"\"\n        logger.info(\"Plotting...\")\n\n        fignum = fignum or (str(self)+'-actions')\n        colormap_fn = plt.get_cmap(colormap_name)\n        action_description = action_description or list(range(self.env.action_space.n))\n\n        # discretizer\n        try:\n            discretizer = self.episode_visit_counter.state_discretizer\n            epsilon = min(discretizer._bins[0][1] - discretizer._bins[0][0],\n                          discretizer._bins[1][1] - discretizer._bins[1][0])\n        except Exception:\n            epsilon = 0.01\n\n        # figure setup\n        xlim = xlim or [0.0, 1.0]\n        ylim = ylim or [0.0, 1.0]\n\n        # indices to visualize\n        if video_filename is None:\n            indices = [self.memory.n_trajectories-1]\n        else:\n            indices = np.arange(self.memory.n_trajectories)[::n_skip]\n\n        # images for video\n        images = []\n\n        # for idx in indices:\n        for init_idx in indices:\n            idx_set = range(max(0, init_idx-n_traj_to_show+1), init_idx+1)\n            # clear before showing new trajectories\n            fig = plt.figure(fignum, figsize=figsize)\n            fig.clf()\n            canvas = FigureCanvas(fig)\n            ax = fig.gca()\n\n            ax.set_xlim(xlim)\n            ax.set_ylim(ylim)\n\n            if hide_axis:\n                ax.set_axis_off()\n\n            for idx in idx_set:\n                traj = self.memory.trajectories[idx]\n\n                states = np.array([traj[ii].state for ii in range(len(traj))])\n                actions = np.array([traj[ii].action for ii in range(len(traj))])\n\n                sizes = (dot_scale_factor**2) * 750 * epsilon\n\n                for aa in range(self.env.action_space.n):\n                    states_aa = states[actions == aa]\n                    color = colormap_fn(aa/self.env.action_space.n)\n                    ax.scatter(x=states_aa[:, 0], y=states_aa[:, 1], color=color,\n                               s=sizes, alpha=alpha,\n                               label=f'action = {action_description[aa]}')\n\n            # for unique legend entries, source: https://stackoverflow.com/a/57600060\n            plt.legend(*[*zip(*{l: h for h, l in zip(*ax.get_legend_handles_labels())}.items())][::-1],\n                       loc='upper left', bbox_to_anchor=(1.00, 1.00))\n            plt.tight_layout()\n\n            if video_filename is not None:\n                canvas.draw()\n                image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n                image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n                images.append(image_from_plot)\n\n        if video_filename is not None:\n            logger.info(\"... writing video ...\")\n            video_write(video_filename, images, framerate=framerate)\n\n        logger.info(\"... done!\")\n\n        if show:\n            plt.show()",
  "def convert_space_from_gym(gym_space):\n    if isinstance(gym_space, gym.spaces.Discrete):\n        return Discrete(gym_space.n)\n    #\n    #\n    elif isinstance(gym_space, gym.spaces.Box):\n        return Box(gym_space.low,\n                   gym_space.high,\n                   gym_space.shape,\n                   gym_space.dtype)\n    #\n    #\n    elif isinstance(gym_space, gym.spaces.Tuple):\n        spaces = []\n        for sp in gym_space.spaces:\n            spaces.append(convert_space_from_gym(sp))\n        return Tuple(spaces)\n    #\n    #\n    elif isinstance(gym_space, gym.spaces.MultiDiscrete):\n        return MultiDiscrete(gym_space.nvec)\n    #\n    #\n    elif isinstance(gym_space, gym.spaces.MultiBinary):\n        return MultiBinary(gym_space.n)\n    #\n    #\n    elif isinstance(gym_space, gym.spaces.Dict):\n        spaces = {}\n        for key in gym_space.spaces:\n            spaces[key] = convert_space_from_gym(gym_space[key])\n        return Dict(spaces)\n    else:\n        raise ValueError(\"Unknown space class: {}\".format(type(gym_space)))",
  "class DiscretizeStateWrapper(Wrapper):\n    \"\"\"\n    Discretize an environment with continuous states and discrete actions.\n    \"\"\"\n\n    def __init__(self, _env, n_bins):\n        # initialize base class\n        super().__init__(_env)\n\n        self.n_bins = n_bins\n        # initialize bins\n        assert n_bins > 0, \"DiscretizeStateWrapper requires n_bins > 0\"\n        n_states = 1\n        tol = 1e-8\n        self.dim = len(self.env.observation_space.low)\n        n_states = n_bins ** self.dim\n        self._bins = []\n        self._open_bins = []\n        for dd in range(self.dim):\n            range_dd = self.env.observation_space.high[dd] \\\n                - self.env.observation_space.low[dd]\n            epsilon = range_dd / n_bins\n            bins_dd = []\n            for bb in range(n_bins + 1):\n                val = self.env.observation_space.low[dd] + epsilon * bb\n                bins_dd.append(val)\n            self._open_bins.append(tuple(bins_dd[1:]))\n            bins_dd[-1] += tol  # \"close\" the last interval\n            self._bins.append(tuple(bins_dd))\n\n            # set observation space\n        self.observation_space = spaces.Discrete(n_states)\n\n        # List of discretized states\n        self.discretized_states = np.zeros((self.dim, n_states))\n        for ii in range(n_states):\n            self.discretized_states[:, ii] = \\\n                self.get_continuous_state(ii, False)\n\n    def reset(self):\n        return self.get_discrete_state(self.env.reset())\n\n    def step(self, action):\n        next_state, reward, done, info = self.env.step(action)\n        next_state = binary_search_nd(next_state, self._bins)\n        return next_state, reward, done, info\n\n    def sample(self, discrete_state, action):\n        # map disctete state to continuous one\n        assert self.observation_space.contains(discrete_state)\n        continuous_state = self.get_continuous_state(discrete_state,\n                                                     randomize=True)\n        # sample in the true environment\n        next_state, reward, done, info = \\\n            self.env.sample(continuous_state, action)\n        # discretize next state\n        next_state = binary_search_nd(next_state, self._bins)\n\n        return next_state, reward, done, info\n\n    def get_discrete_state(self, continuous_state):\n        return binary_search_nd(continuous_state, self._bins)\n\n    def get_continuous_state(self, discrete_state, randomize=False):\n        assert discrete_state >= 0 \\\n            and discrete_state < self.observation_space.n, \\\n            \"invalid discrete_state\"\n        # get multi-index\n        index \\\n            = unravel_index_uniform_bin(discrete_state, self.dim, self.n_bins)\n\n        # get state\n        continuous_state = np.zeros(self.dim)\n        for dd in range(self.dim):\n            continuous_state[dd] = self._bins[dd][index[dd]]\n            if randomize:\n                range_dd = self.env.observation_space.high[dd] \\\n                    - self.env.observation_space.low[dd]\n                epsilon = range_dd / self.n_bins\n                continuous_state[dd] += epsilon * self.rng.uniform()\n        return continuous_state",
  "def __init__(self, _env, n_bins):\n        # initialize base class\n        super().__init__(_env)\n\n        self.n_bins = n_bins\n        # initialize bins\n        assert n_bins > 0, \"DiscretizeStateWrapper requires n_bins > 0\"\n        n_states = 1\n        tol = 1e-8\n        self.dim = len(self.env.observation_space.low)\n        n_states = n_bins ** self.dim\n        self._bins = []\n        self._open_bins = []\n        for dd in range(self.dim):\n            range_dd = self.env.observation_space.high[dd] \\\n                - self.env.observation_space.low[dd]\n            epsilon = range_dd / n_bins\n            bins_dd = []\n            for bb in range(n_bins + 1):\n                val = self.env.observation_space.low[dd] + epsilon * bb\n                bins_dd.append(val)\n            self._open_bins.append(tuple(bins_dd[1:]))\n            bins_dd[-1] += tol  # \"close\" the last interval\n            self._bins.append(tuple(bins_dd))\n\n            # set observation space\n        self.observation_space = spaces.Discrete(n_states)\n\n        # List of discretized states\n        self.discretized_states = np.zeros((self.dim, n_states))\n        for ii in range(n_states):\n            self.discretized_states[:, ii] = \\\n                self.get_continuous_state(ii, False)",
  "def reset(self):\n        return self.get_discrete_state(self.env.reset())",
  "def step(self, action):\n        next_state, reward, done, info = self.env.step(action)\n        next_state = binary_search_nd(next_state, self._bins)\n        return next_state, reward, done, info",
  "def sample(self, discrete_state, action):\n        # map disctete state to continuous one\n        assert self.observation_space.contains(discrete_state)\n        continuous_state = self.get_continuous_state(discrete_state,\n                                                     randomize=True)\n        # sample in the true environment\n        next_state, reward, done, info = \\\n            self.env.sample(continuous_state, action)\n        # discretize next state\n        next_state = binary_search_nd(next_state, self._bins)\n\n        return next_state, reward, done, info",
  "def get_discrete_state(self, continuous_state):\n        return binary_search_nd(continuous_state, self._bins)",
  "def get_continuous_state(self, discrete_state, randomize=False):\n        assert discrete_state >= 0 \\\n            and discrete_state < self.observation_space.n, \\\n            \"invalid discrete_state\"\n        # get multi-index\n        index \\\n            = unravel_index_uniform_bin(discrete_state, self.dim, self.n_bins)\n\n        # get state\n        continuous_state = np.zeros(self.dim)\n        for dd in range(self.dim):\n            continuous_state[dd] = self._bins[dd][index[dd]]\n            if randomize:\n                range_dd = self.env.observation_space.high[dd] \\\n                    - self.env.observation_space.low[dd]\n                epsilon = range_dd / self.n_bins\n                continuous_state[dd] += epsilon * self.rng.uniform()\n        return continuous_state",
  "class RescaleRewardWrapper(Wrapper):\n    \"\"\"\n    Rescale the reward function to a bounded range.\n\n    Parameters\n    ----------\n    reward_range: tuple (double, double)\n        tuple with the desired reward range, which needs to be bounded.\n    \"\"\"\n\n    def __init__(self, env, reward_range):\n        Wrapper.__init__(self, env)\n        self.reward_range = reward_range\n        assert reward_range[0] < reward_range[1]\n        assert reward_range[0] > -np.inf and reward_range[1] < np.inf\n\n    def _linear_rescaling(self, x, x0, x1, u0, u1):\n        \"\"\"\n        For x a value in [x0, x1], maps x linearly to the interval [u0, u1].\n        \"\"\"\n        a = (u1 - u0) / (x1 - x0)\n        b = (x1 * u0 - x0 * u1) / (x1 - x0)\n        return a * x + b\n\n    def _rescale(self, reward):\n        x0, x1 = self.env.reward_range\n        u0, u1 = self.reward_range\n        # bounded reward\n        if x0 > -np.inf and x1 < np.inf:\n            return self._linear_rescaling(reward, x0, x1, u0, u1)\n        # unbounded\n        elif x0 > -np.inf and x1 == np.inf:\n            x = reward - x0  # [0, infty]\n            x = 2.0 / (1.0 + np.exp(-x)) - 1.0  # [0, 1]\n            return self._linear_rescaling(x, 0.0, 1.0, u0, u1)\n            # unbouded below\n        elif x0 == -np.inf and x1 < np.inf:\n            x = reward - x1  # [-infty, 0]\n            x = 2.0 / (1.0 + np.exp(-x))  # [0, 1]\n            return self._linear_rescaling(x, 0.0, 1.0, u0, u1)\n            # unbounded\n        else:\n            x = 1.0 / (1.0 + np.exp(-reward))  # [0, 1]\n            return self._linear_rescaling(x, 0.0, 1.0, u0, u1)\n\n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        rescaled_reward = self._rescale(reward)\n        return observation, rescaled_reward, done, info\n\n    def sample(self, state, action):\n        observation, reward, done, info = self.env.sample(state, action)\n        rescaled_reward = self._rescale(reward)\n        return observation, rescaled_reward, done, info",
  "def __init__(self, env, reward_range):\n        Wrapper.__init__(self, env)\n        self.reward_range = reward_range\n        assert reward_range[0] < reward_range[1]\n        assert reward_range[0] > -np.inf and reward_range[1] < np.inf",
  "def _linear_rescaling(self, x, x0, x1, u0, u1):\n        \"\"\"\n        For x a value in [x0, x1], maps x linearly to the interval [u0, u1].\n        \"\"\"\n        a = (u1 - u0) / (x1 - x0)\n        b = (x1 * u0 - x0 * u1) / (x1 - x0)\n        return a * x + b",
  "def _rescale(self, reward):\n        x0, x1 = self.env.reward_range\n        u0, u1 = self.reward_range\n        # bounded reward\n        if x0 > -np.inf and x1 < np.inf:\n            return self._linear_rescaling(reward, x0, x1, u0, u1)\n        # unbounded\n        elif x0 > -np.inf and x1 == np.inf:\n            x = reward - x0  # [0, infty]\n            x = 2.0 / (1.0 + np.exp(-x)) - 1.0  # [0, 1]\n            return self._linear_rescaling(x, 0.0, 1.0, u0, u1)\n            # unbouded below\n        elif x0 == -np.inf and x1 < np.inf:\n            x = reward - x1  # [-infty, 0]\n            x = 2.0 / (1.0 + np.exp(-x))  # [0, 1]\n            return self._linear_rescaling(x, 0.0, 1.0, u0, u1)\n            # unbounded\n        else:\n            x = 1.0 / (1.0 + np.exp(-reward))  # [0, 1]\n            return self._linear_rescaling(x, 0.0, 1.0, u0, u1)",
  "def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        rescaled_reward = self._rescale(reward)\n        return observation, rescaled_reward, done, info",
  "def sample(self, state, action):\n        observation, reward, done, info = self.env.sample(state, action)\n        rescaled_reward = self._rescale(reward)\n        return observation, rescaled_reward, done, info",
  "def set_global_seed(entropy=42):\n    \"\"\"\n    rlberry has a global seed from which we can obtain different random number\n    generators with (close to) independent outcomes.\n\n    Important:\n\n    In each new process/thread, set_global_seed must be called with a new seed.\n\n    To do:\n        Check (torch seeding):\n        https://github.com/pytorch/pytorch/issues/7068#issuecomment-487907668\n\n    Parameters\n    ---------\n    entropy: int, SeedSequence optional\n        The entropy for creating the global SeedSequence, or a SeedSequence\n    \"\"\"\n    global _GLOBAL_ENTROPY, _GLOBAL_SEED_SEQ, _GLOBAL_RNG\n\n    if isinstance(entropy, SeedSequence):\n        seedseq = entropy\n        _GLOBAL_ENTROPY = seedseq.entropy\n        _GLOBAL_SEED_SEQ = seedseq\n    else:\n        _GLOBAL_ENTROPY = entropy\n        _GLOBAL_SEED_SEQ = SeedSequence(entropy)\n\n    _GLOBAL_RNG = get_rng()\n\n    # seed torch\n    if _TORCH_INSTALLED:\n        torch.manual_seed(_GLOBAL_SEED_SEQ.generate_state(1, dtype=np.uint32)[0])",
  "def generate_uniform_seed():\n    \"\"\"\n    Return a seed value using a global random number generator.\n    \"\"\"\n    return _GLOBAL_RNG.integers(2**32).item()",
  "def global_rng():\n    \"\"\"\n    Returns the global random number generator.\n    \"\"\"\n    return _GLOBAL_RNG",
  "def safe_reseed(object):\n    \"\"\"\n    Calls object.reseed() method if available;\n    If a object.seed() method is available, call object.seed(seed_val), where seed_val is returned by\n    generate_uniform_seed().\n    Otherwise, does nothing.\n\n    Returns\n    -------\n    True if reseeding was done, False otherwise.\n    \"\"\"\n    try:\n        object.reseed()\n        return True\n    except AttributeError:\n        seed_val = generate_uniform_seed()\n        try:\n            object.seed(seed_val)\n            return True\n        except AttributeError:\n            return False",
  "def spawn(n):\n    \"\"\"\n    Spawn a list of SeedSequence from the global seed sequence.\n\n    Parameters\n    ----------\n    n : int\n        Number of seed sequences to spawn\n    Returns\n    -------\n    seed_seq : list\n        List of spawned seed sequences\n    \"\"\"\n    return _GLOBAL_SEED_SEQ.spawn(n)",
  "def get_rng():\n    \"\"\"\n    Get a random number generator (rng), from the global seed sequence.\n\n    Returns\n    -------\n    rng : numpy.random._generator.Generator\n        random number generator\n    \"\"\"\n    # Spawn off 1 child SeedSequence\n    child_seed = _GLOBAL_SEED_SEQ.spawn(1)\n    rng = default_rng(child_seed[0])\n    return rng",
  "def gym_make(env_name, **kwargs):\n    \"\"\"\n    Same as gym.make, but wraps the environment\n    to ensure unified seeding with rlberry.\n    \"\"\"\n    if \"module_import\" in kwargs:\n        __import__(kwargs.pop(\"module_import\"))\n    env = gym.make(env_name)\n    try:\n        env.configure(kwargs)\n    except AttributeError:\n        pass\n    return Wrapper(env)",
  "def atari_make(env_name, scalarize=True, **kwargs):\n    from stable_baselines3.common.env_util import make_atari_env\n    from stable_baselines3.common.vec_env import VecFrameStack\n    env = make_atari_env(env_id=env_name, **kwargs)\n    env = VecFrameStack(env, n_stack=4)\n    if scalarize:\n        from rlberry.wrappers.scalarize import ScalarizeEnvWrapper\n        env = ScalarizeEnvWrapper(env)\n    return env",
  "class Wrapper(Model):\n    \"\"\"\n    Wraps a given environment, similar to OpenAI gym's wrapper [1].\n    Can also be used to wrap gym environments.\n\n    Note:\n        The input environment is not copied (Wrapper.env points\n        to the input env).\n\n    See also:\n    https://stackoverflow.com/questions/1443129/completely-wrap-an-object-in-python\n\n    [1] https://github.com/openai/gym/blob/master/gym/core.py\n    \"\"\"\n    def __init__(self, env):\n        # Init base class\n        Model.__init__(self)\n\n        # Save reference to env\n        self.env = env\n\n        self.observation_space = self.env.observation_space\n        self.action_space = self.env.action_space\n        self.metadata = self.env.metadata\n\n        try:\n            self.reward_range = self.env.reward_range\n        except AttributeError:\n            self.reward_range = (-np.inf, np.inf)\n\n        # If gym environment, reseeding is necessary here for\n        # reproducibility.\n        if isinstance(env, gym.Env):\n            self.reseed()\n\n    @property\n    def unwrapped(self):\n        return self.env.unwrapped\n\n    @property\n    def spec(self):\n        return self.env.spec\n\n    @classmethod\n    def class_name(cls):\n        return cls.__name__\n\n    def __getattr__(self, attr):\n        \"\"\"\n        The first condition is to avoid infinite recursion when deep copying.\n        See https://stackoverflow.com/a/47300262\n        \"\"\"\n        if attr[:2] == '__':\n            raise AttributeError(attr)\n        if attr in self.__dict__:\n            return getattr(self, attr)\n        return getattr(self.env, attr)\n\n    def reseed(self):\n        self.rng = seeding.get_rng()\n        # seed gym.Env that is not a rlberry Model\n        if not isinstance(self.env, Model):\n            # get a seed for gym environment\n            seeding.safe_reseed(self.env)\n            seeding.safe_reseed(self.observation_space)\n            seeding.safe_reseed(self.action_space)\n        # seed rlberry Model\n        else:\n            self.env.reseed()\n            self.observation_space.rng = self.env.rng\n            self.action_space.rng = self.env.rng\n\n    def reset(self):\n        return self.env.reset()\n\n    def step(self, action):\n        return self.env.step(action)\n\n    def sample(self, state, action):\n        return self.env.sample(state, action)\n\n    def render(self, mode='human', **kwargs):\n        return self.env.render(mode=mode, **kwargs)\n\n    def close(self):\n        return self.env.close()\n\n    def seed(self, seed=None):\n        return self.env.seed(seed)\n\n    def compute_reward(self, achieved_goal, desired_goal, info):\n        return self.env.compute_reward(achieved_goal, desired_goal, info)\n\n    def is_online(self):\n        try:\n            self.env.reset()\n            self.env.step(self.env.action_space.sample())\n            return True\n        except Exception:\n            return False\n\n    def is_generative(self):\n        try:\n            self.env.sample(self.env.observation_space.sample(),\n                            self.env.action_space.sample())\n            return True\n        except Exception:\n            return False\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        return '<{}{}>'.format(type(self).__name__, self.env)",
  "def __init__(self, env):\n        # Init base class\n        Model.__init__(self)\n\n        # Save reference to env\n        self.env = env\n\n        self.observation_space = self.env.observation_space\n        self.action_space = self.env.action_space\n        self.metadata = self.env.metadata\n\n        try:\n            self.reward_range = self.env.reward_range\n        except AttributeError:\n            self.reward_range = (-np.inf, np.inf)\n\n        # If gym environment, reseeding is necessary here for\n        # reproducibility.\n        if isinstance(env, gym.Env):\n            self.reseed()",
  "def unwrapped(self):\n        return self.env.unwrapped",
  "def spec(self):\n        return self.env.spec",
  "def class_name(cls):\n        return cls.__name__",
  "def __getattr__(self, attr):\n        \"\"\"\n        The first condition is to avoid infinite recursion when deep copying.\n        See https://stackoverflow.com/a/47300262\n        \"\"\"\n        if attr[:2] == '__':\n            raise AttributeError(attr)\n        if attr in self.__dict__:\n            return getattr(self, attr)\n        return getattr(self.env, attr)",
  "def reseed(self):\n        self.rng = seeding.get_rng()\n        # seed gym.Env that is not a rlberry Model\n        if not isinstance(self.env, Model):\n            # get a seed for gym environment\n            seeding.safe_reseed(self.env)\n            seeding.safe_reseed(self.observation_space)\n            seeding.safe_reseed(self.action_space)\n        # seed rlberry Model\n        else:\n            self.env.reseed()\n            self.observation_space.rng = self.env.rng\n            self.action_space.rng = self.env.rng",
  "def reset(self):\n        return self.env.reset()",
  "def step(self, action):\n        return self.env.step(action)",
  "def sample(self, state, action):\n        return self.env.sample(state, action)",
  "def render(self, mode='human', **kwargs):\n        return self.env.render(mode=mode, **kwargs)",
  "def close(self):\n        return self.env.close()",
  "def seed(self, seed=None):\n        return self.env.seed(seed)",
  "def compute_reward(self, achieved_goal, desired_goal, info):\n        return self.env.compute_reward(achieved_goal, desired_goal, info)",
  "def is_online(self):\n        try:\n            self.env.reset()\n            self.env.step(self.env.action_space.sample())\n            return True\n        except Exception:\n            return False",
  "def is_generative(self):\n        try:\n            self.env.sample(self.env.observation_space.sample(),\n                            self.env.action_space.sample())\n            return True\n        except Exception:\n            return False",
  "def __repr__(self):\n        return str(self)",
  "def __str__(self):\n        return '<{}{}>'.format(type(self).__name__, self.env)",
  "class SixRoom(GridWorld):\n    \"\"\"\n    GridWorld with six rooms.\n\n    Parameters\n    ----------\n    reward_free : bool, default=False\n        If true, no rewards are given to the agent.\n    array_observation:\n        If true, the observations are converted to an array (x, y)\n        instead of a discrete index.\n\n    Notes\n    -----\n    The function env.sample() does not handle conversions to array states\n    when array_observation is True. Only the functions env.reset() and\n    env.step() are covered.\n    \"\"\"\n    name = \"SixRoom\"\n\n    def __init__(self, reward_free=False, array_observation=False):\n        self.reward_free = reward_free\n        self.array_observation = array_observation\n\n        # Common parameters\n        nrows = 11\n        ncols = 17\n        start_coord = (0, 0)\n        terminal_states = ((10, 0),)\n        success_probability = 0.95\n        #\n        walls = ()\n        for ii in range(11):\n            if ii not in [2, 8]:\n                walls += ((ii, 5),)\n                walls += ((ii, 11),)\n        for jj in range(17):\n            if jj != 15:\n                walls += ((5, jj),)\n\n        # Default reward according to the difficulty\n        default_reward = -0.001\n\n        # Rewards according to the difficulty\n        if self.reward_free:\n            reward_at = {}\n        else:\n            reward_at = {\n                        (10, 0): 10.0,\n                        (4, 4): 0.1,\n                        }\n\n        # Init base class\n        GridWorld.__init__(self,\n                           nrows=nrows,\n                           ncols=ncols,\n                           start_coord=start_coord,\n                           terminal_states=terminal_states,\n                           success_probability=success_probability,\n                           reward_at=reward_at,\n                           walls=walls,\n                           default_reward=default_reward)\n\n        # spaces\n        if self.array_observation:\n            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))\n\n    def _convert_index_to_float_coord(self, state_index):\n        yy, xx = self.index2coord[state_index]\n\n        # centering\n        xx = xx + 0.5\n        yy = yy + 0.5\n        # map to [0, 1]\n        xx = xx/self.ncols\n        yy = yy/self.nrows\n        return np.array([xx, yy])\n\n    def reset(self):\n        self.state = self.coord2index[self.start_coord]\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n        return state_to_return\n\n    def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n\n        return state_to_return, reward, done, info\n\n    def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # walls\n        for wall in self.walls:\n            y, x = wall\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.25, 0.25, 0.25))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # rewards\n        for (y, x) in self.reward_at:\n            flag = GeometricPrimitive(\"POLYGON\")\n            rwd = self.reward_at[(y, x)]\n            if rwd == 10:\n                flag.set_color((0.0, 0.5, 0.0))\n            else:\n                flag.set_color((0.0, 0.0, 0.5))\n\n            x += 0.5\n            y += 0.25\n            flag.add_vertex((x, y))\n            flag.add_vertex((x + 0.25, y + 0.5))\n            flag.add_vertex((x - 0.25, y + 0.5))\n            bg.add_shape(flag)\n\n        return bg",
  "def __init__(self, reward_free=False, array_observation=False):\n        self.reward_free = reward_free\n        self.array_observation = array_observation\n\n        # Common parameters\n        nrows = 11\n        ncols = 17\n        start_coord = (0, 0)\n        terminal_states = ((10, 0),)\n        success_probability = 0.95\n        #\n        walls = ()\n        for ii in range(11):\n            if ii not in [2, 8]:\n                walls += ((ii, 5),)\n                walls += ((ii, 11),)\n        for jj in range(17):\n            if jj != 15:\n                walls += ((5, jj),)\n\n        # Default reward according to the difficulty\n        default_reward = -0.001\n\n        # Rewards according to the difficulty\n        if self.reward_free:\n            reward_at = {}\n        else:\n            reward_at = {\n                        (10, 0): 10.0,\n                        (4, 4): 0.1,\n                        }\n\n        # Init base class\n        GridWorld.__init__(self,\n                           nrows=nrows,\n                           ncols=ncols,\n                           start_coord=start_coord,\n                           terminal_states=terminal_states,\n                           success_probability=success_probability,\n                           reward_at=reward_at,\n                           walls=walls,\n                           default_reward=default_reward)\n\n        # spaces\n        if self.array_observation:\n            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))",
  "def _convert_index_to_float_coord(self, state_index):\n        yy, xx = self.index2coord[state_index]\n\n        # centering\n        xx = xx + 0.5\n        yy = yy + 0.5\n        # map to [0, 1]\n        xx = xx/self.ncols\n        yy = yy/self.nrows\n        return np.array([xx, yy])",
  "def reset(self):\n        self.state = self.coord2index[self.start_coord]\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n        return state_to_return",
  "def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n\n        return state_to_return, reward, done, info",
  "def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # walls\n        for wall in self.walls:\n            y, x = wall\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.25, 0.25, 0.25))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # rewards\n        for (y, x) in self.reward_at:\n            flag = GeometricPrimitive(\"POLYGON\")\n            rwd = self.reward_at[(y, x)]\n            if rwd == 10:\n                flag.set_color((0.0, 0.5, 0.0))\n            else:\n                flag.set_color((0.0, 0.0, 0.5))\n\n            x += 0.5\n            y += 0.25\n            flag.add_vertex((x, y))\n            flag.add_vertex((x + 0.25, y + 0.5))\n            flag.add_vertex((x - 0.25, y + 0.5))\n            bg.add_shape(flag)\n\n        return bg",
  "class FourRoom(GridWorld):\n    \"\"\"\n    GridWorld with four rooms.\n\n    Parameters\n    ----------\n    reward_free : bool, default=False\n        If true, no rewards are given to the agent.\n    difficulty: int, {0, 1 or 2}\n        Difficulty 0: reward in one location\n        Difficulty 1: easy suboptimal reward, hard optimal reward\n        Difficulty 2: easy suboptimal reward, hard optimal reward,\n            negative rewards by default.\n        Note: this parameter is ignored if reward_free is True.\n    array_observation:\n        If true, the observations are converted to an array (x, y)\n        instead of a discrete index.\n\n    Notes\n    -----\n    The function env.sample() does not handle conversions to array states\n    when array_observation is True. Only the functions env.reset() and\n    env.step() are covered.\n    \"\"\"\n    name = \"FourRoom\"\n\n    def __init__(self,\n                 reward_free=False,\n                 difficulty=0,\n                 array_observation=False):\n        self.reward_free = reward_free\n        self.difficulty = difficulty\n        self.array_observation = array_observation\n\n        if difficulty not in [0, 1, 2]:\n            raise ValueError(\"FourRoom difficulty must be in [0, 1, 2]\")\n\n        # Common parameters\n        nrows = 9\n        ncols = 9\n        start_coord = (0, 0)\n        terminal_states = ((8, 0),)\n        success_probability = 0.95\n        #\n        walls = ()\n        for ii in range(9):\n            if ii not in [2, 6]:\n                walls += ((ii, 4),)\n        for jj in range(9):\n            if jj != 7:\n                walls += ((4, jj),)\n\n        # Default reward according to the difficulty\n        if difficulty in [0, 1]:\n            default_reward = 0.0\n        elif difficulty == 2:\n            default_reward = -0.005\n\n        # Rewards according to the difficulty\n        if self.reward_free:\n            reward_at = {}\n        else:\n            if difficulty == 0:\n                reward_at = {(8, 0): 1.0}\n            elif difficulty in [1, 2]:\n                reward_at = {\n                            (8, 0): 1.0,\n                            (3, 3): 0.1,\n                            }\n\n        # Init base class\n        GridWorld.__init__(self,\n                           nrows=nrows,\n                           ncols=ncols,\n                           start_coord=start_coord,\n                           terminal_states=terminal_states,\n                           success_probability=success_probability,\n                           reward_at=reward_at,\n                           walls=walls,\n                           default_reward=default_reward)\n\n        # spaces\n        if self.array_observation:\n            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))\n\n    def _convert_index_to_float_coord(self, state_index):\n        yy, xx = self.index2coord[state_index]\n\n        # centering\n        xx = xx + 0.5\n        yy = yy + 0.5\n        # map to [0, 1]\n        xx = xx/self.ncols\n        yy = yy/self.nrows\n        return np.array([xx, yy])\n\n    def reset(self):\n        self.state = self.coord2index[self.start_coord]\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n        return state_to_return\n\n    def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n\n        return state_to_return, reward, done, info",
  "def __init__(self,\n                 reward_free=False,\n                 difficulty=0,\n                 array_observation=False):\n        self.reward_free = reward_free\n        self.difficulty = difficulty\n        self.array_observation = array_observation\n\n        if difficulty not in [0, 1, 2]:\n            raise ValueError(\"FourRoom difficulty must be in [0, 1, 2]\")\n\n        # Common parameters\n        nrows = 9\n        ncols = 9\n        start_coord = (0, 0)\n        terminal_states = ((8, 0),)\n        success_probability = 0.95\n        #\n        walls = ()\n        for ii in range(9):\n            if ii not in [2, 6]:\n                walls += ((ii, 4),)\n        for jj in range(9):\n            if jj != 7:\n                walls += ((4, jj),)\n\n        # Default reward according to the difficulty\n        if difficulty in [0, 1]:\n            default_reward = 0.0\n        elif difficulty == 2:\n            default_reward = -0.005\n\n        # Rewards according to the difficulty\n        if self.reward_free:\n            reward_at = {}\n        else:\n            if difficulty == 0:\n                reward_at = {(8, 0): 1.0}\n            elif difficulty in [1, 2]:\n                reward_at = {\n                            (8, 0): 1.0,\n                            (3, 3): 0.1,\n                            }\n\n        # Init base class\n        GridWorld.__init__(self,\n                           nrows=nrows,\n                           ncols=ncols,\n                           start_coord=start_coord,\n                           terminal_states=terminal_states,\n                           success_probability=success_probability,\n                           reward_at=reward_at,\n                           walls=walls,\n                           default_reward=default_reward)\n\n        # spaces\n        if self.array_observation:\n            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))",
  "def _convert_index_to_float_coord(self, state_index):\n        yy, xx = self.index2coord[state_index]\n\n        # centering\n        xx = xx + 0.5\n        yy = yy + 0.5\n        # map to [0, 1]\n        xx = xx/self.ncols\n        yy = yy/self.nrows\n        return np.array([xx, yy])",
  "def reset(self):\n        self.state = self.coord2index[self.start_coord]\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n        return state_to_return",
  "def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n\n        return state_to_return, reward, done, info",
  "def get_nroom_state_coord(state_index, nroom_env):\n    yy, xx = nroom_env.index2coord[state_index]\n    # centering\n    xx = xx + 0.5\n    yy = yy + 0.5\n    # map to [0, 1]\n    xx = xx/nroom_env.ncols\n    yy = yy/nroom_env.nrows\n    return np.array([xx, yy])",
  "class NRoom(GridWorld):\n    \"\"\"\n    GridWorld with N rooms of size L x L. The agent starts in the middle room.\n\n    There is one small and easy reward in the first room,\n    one big reward in the last room and zero reward elsewhere.\n\n    There is a 5% error probability in the transitions when taking an action.\n\n    Parameters\n    ----------\n    nrooms : int\n        Number of rooms.\n    reward_free : bool, default=False\n        If true, no rewards are given to the agent.\n    array_observation:\n        If true, the observations are converted to an array (x, y)\n        instead of a discrete index.\n        The underlying discrete space is saved in env.discrete_observation_space.\n    room_size : int\n        Dimension (L) of each room (L x L).\n    success_probability : double, default: 0.95\n        Sucess probability of an action. A failure is going to the wrong direction.\n    remove_walls : bool, default: False\n        If True, remove walls. Useful for debug.\n    initial_state_distribution: {'center', 'uniform'}\n        If 'center', always start at the center.\n        If 'uniform', start anywhere with uniform probability.\n    include_traps: bool, default: False\n        If true, each room will have a terminal state (a \"trap\").\n    Notes\n    -----\n    The function env.sample() does not handle conversions to array states\n    when array_observation is True. Only the functions env.reset() and\n    env.step() are covered.\n    \"\"\"\n    name = \"N-Room\"\n\n    def __init__(self,\n                 nrooms=7,\n                 reward_free=False,\n                 array_observation=False,\n                 room_size=5,\n                 success_probability=0.95,\n                 remove_walls=False,\n                 initial_state_distribution='center',\n                 include_traps=False):\n\n        assert nrooms > 0, \"nrooms must be > 0\"\n        assert initial_state_distribution in ('center', 'uniform')\n\n        self.reward_free = reward_free\n        self.array_observation = array_observation\n        self.nrooms = nrooms\n\n        # Max number of rooms/columns per row\n        self.max_rooms_per_row = 5\n\n        # Room size (default = 5x5)\n        self.room_size = room_size\n\n        # Grid size\n        self.room_nrows = math.ceil(nrooms / self.max_rooms_per_row)\n        if self.room_nrows > 1:\n            self.room_ncols = self.max_rooms_per_row\n        else:\n            self.room_ncols = nrooms\n        nrows = self.room_size*self.room_nrows + (self.room_nrows-1)\n        ncols = self.room_size*self.room_ncols + (self.room_ncols-1)\n\n        # # walls\n        walls = []\n        for room_col in range(self.room_ncols-1):\n            col = (room_col+1)*(self.room_size+1) - 1\n            for jj in range(nrows):\n                if (jj % (self.room_size+1)) != (self.room_size//2):\n                    walls.append((jj, col))\n\n        for room_row in range(self.room_nrows-1):\n            row = (room_row+1)*(self.room_size+1) - 1\n            for jj in range(ncols):\n                walls.append((row, jj))\n\n        # process each room\n        start_coord = None\n        terminal_state = None\n        self.traps = []\n        count = 0\n        for room_r in range(self.room_nrows):\n            if room_r % 2 == 0:\n                cols_iterator = range(self.room_ncols)\n            else:\n                cols_iterator = reversed(range(self.room_ncols))\n            for room_c in cols_iterator:\n                # existing rooms\n                if count < self.nrooms:\n                    # remove top wall\n                    if ((room_c == self.room_ncols-1) and (room_r % 2 == 0)) \\\n                            or ((room_c == 0) and (room_r % 2 == 1)):\n                        if room_r != self.room_nrows-1:\n                            wall_to_remove = self._convert_room_coord_to_global(\n                                                                room_r, room_c,\n                                                                self.room_size, self.room_size//2)\n                            if wall_to_remove in walls:\n                                walls.remove(wall_to_remove)\n                # rooms to remove\n                else:\n                    for ii in range(-1, self.room_size+1):\n                        for jj in range(-1, self.room_size+1):\n                            wall_to_include = self._convert_room_coord_to_global(\n                                                            room_r, room_c,\n                                                            ii, jj)\n                            if wall_to_include[0] >= 0 and wall_to_include[0] < nrows \\\n                                and wall_to_include[1] >= 0 and wall_to_include[1] < ncols \\\n                                    and (wall_to_include not in walls):\n                                walls.append(wall_to_include)\n                    pass\n\n                # start coord\n                if count == nrooms // 2:\n                    start_coord = self._convert_room_coord_to_global(\n                                                room_r, room_c,\n                                                self.room_size//2, self.room_size//2)\n                # terminal state\n                if count == nrooms - 1:\n                    terminal_state = self._convert_room_coord_to_global(\n                                                room_r, room_c,\n                                                self.room_size//2, self.room_size//2)\n                # trap\n                if include_traps:\n                    self.traps.append(\n                        self._convert_room_coord_to_global(\n                                room_r, room_c,\n                                self.room_size//2+1, self.room_size//2+1)\n                    )\n                count += 1\n\n        terminal_states = (terminal_state,) + tuple(self.traps)\n\n        if self.reward_free:\n            reward_at = {}\n        else:\n            reward_at = {\n                            terminal_state: 1.0,\n                            start_coord: 0.01,\n                            (self.room_size//2, self.room_size//2): 0.1\n                        }\n\n        # Check remove_walls\n        if remove_walls:\n            walls = ()\n\n        # Init base class\n        GridWorld.__init__(self,\n                           nrows=nrows,\n                           ncols=ncols,\n                           start_coord=start_coord,\n                           terminal_states=terminal_states,\n                           success_probability=success_probability,\n                           reward_at=reward_at,\n                           walls=walls,\n                           default_reward=0.0)\n\n        # Check initial distribution\n        if initial_state_distribution == 'uniform':\n            distr = np.ones(self.observation_space.n) / self.observation_space.n\n            self.set_initial_state_distribution(distr)\n\n        # spaces\n        if self.array_observation:\n            self.discrete_observation_space = self.observation_space\n            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))\n\n    def _convert_room_coord_to_global(self, room_row, room_col, room_coord_row, room_coord_col):\n        col_offset = (self.room_size+1)*room_col\n        row_offset = (self.room_size+1)*room_row\n\n        row = room_coord_row + row_offset\n        col = room_coord_col + col_offset\n        return (row, col)\n\n    def _convert_index_to_float_coord(self, state_index):\n        yy, xx = self.index2coord[state_index]\n\n        # centering\n        xx = xx + 0.5\n        yy = yy + 0.5\n        # map to [0, 1]\n        xx = xx/self.ncols\n        yy = yy/self.nrows\n        return np.array([xx, yy])\n\n    def reset(self):\n        self.state = GridWorld.reset(self)\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n        return state_to_return\n\n    def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n\n        return state_to_return, reward, done, info\n\n    def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # traps\n        for (y, x) in self.traps:\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.5, 0.0, 0.0))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # walls\n        for wall in self.walls:\n            y, x = wall\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.25, 0.25, 0.25))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # rewards\n        for (y, x) in self.reward_at:\n            flag = GeometricPrimitive(\"POLYGON\")\n            rwd = self.reward_at[(y, x)]\n            if rwd == 1.0:\n                flag.set_color((0.0, 0.5, 0.0))\n            elif rwd == 0.1:\n                flag.set_color((0.0, 0.0, 0.5))\n            else:\n                flag.set_color((0.5, 0.0, 0.0))\n\n            x += 0.5\n            y += 0.25\n            flag.add_vertex((x, y))\n            flag.add_vertex((x + 0.25, y + 0.5))\n            flag.add_vertex((x - 0.25, y + 0.5))\n            bg.add_shape(flag)\n\n        return bg",
  "def __init__(self,\n                 nrooms=7,\n                 reward_free=False,\n                 array_observation=False,\n                 room_size=5,\n                 success_probability=0.95,\n                 remove_walls=False,\n                 initial_state_distribution='center',\n                 include_traps=False):\n\n        assert nrooms > 0, \"nrooms must be > 0\"\n        assert initial_state_distribution in ('center', 'uniform')\n\n        self.reward_free = reward_free\n        self.array_observation = array_observation\n        self.nrooms = nrooms\n\n        # Max number of rooms/columns per row\n        self.max_rooms_per_row = 5\n\n        # Room size (default = 5x5)\n        self.room_size = room_size\n\n        # Grid size\n        self.room_nrows = math.ceil(nrooms / self.max_rooms_per_row)\n        if self.room_nrows > 1:\n            self.room_ncols = self.max_rooms_per_row\n        else:\n            self.room_ncols = nrooms\n        nrows = self.room_size*self.room_nrows + (self.room_nrows-1)\n        ncols = self.room_size*self.room_ncols + (self.room_ncols-1)\n\n        # # walls\n        walls = []\n        for room_col in range(self.room_ncols-1):\n            col = (room_col+1)*(self.room_size+1) - 1\n            for jj in range(nrows):\n                if (jj % (self.room_size+1)) != (self.room_size//2):\n                    walls.append((jj, col))\n\n        for room_row in range(self.room_nrows-1):\n            row = (room_row+1)*(self.room_size+1) - 1\n            for jj in range(ncols):\n                walls.append((row, jj))\n\n        # process each room\n        start_coord = None\n        terminal_state = None\n        self.traps = []\n        count = 0\n        for room_r in range(self.room_nrows):\n            if room_r % 2 == 0:\n                cols_iterator = range(self.room_ncols)\n            else:\n                cols_iterator = reversed(range(self.room_ncols))\n            for room_c in cols_iterator:\n                # existing rooms\n                if count < self.nrooms:\n                    # remove top wall\n                    if ((room_c == self.room_ncols-1) and (room_r % 2 == 0)) \\\n                            or ((room_c == 0) and (room_r % 2 == 1)):\n                        if room_r != self.room_nrows-1:\n                            wall_to_remove = self._convert_room_coord_to_global(\n                                                                room_r, room_c,\n                                                                self.room_size, self.room_size//2)\n                            if wall_to_remove in walls:\n                                walls.remove(wall_to_remove)\n                # rooms to remove\n                else:\n                    for ii in range(-1, self.room_size+1):\n                        for jj in range(-1, self.room_size+1):\n                            wall_to_include = self._convert_room_coord_to_global(\n                                                            room_r, room_c,\n                                                            ii, jj)\n                            if wall_to_include[0] >= 0 and wall_to_include[0] < nrows \\\n                                and wall_to_include[1] >= 0 and wall_to_include[1] < ncols \\\n                                    and (wall_to_include not in walls):\n                                walls.append(wall_to_include)\n                    pass\n\n                # start coord\n                if count == nrooms // 2:\n                    start_coord = self._convert_room_coord_to_global(\n                                                room_r, room_c,\n                                                self.room_size//2, self.room_size//2)\n                # terminal state\n                if count == nrooms - 1:\n                    terminal_state = self._convert_room_coord_to_global(\n                                                room_r, room_c,\n                                                self.room_size//2, self.room_size//2)\n                # trap\n                if include_traps:\n                    self.traps.append(\n                        self._convert_room_coord_to_global(\n                                room_r, room_c,\n                                self.room_size//2+1, self.room_size//2+1)\n                    )\n                count += 1\n\n        terminal_states = (terminal_state,) + tuple(self.traps)\n\n        if self.reward_free:\n            reward_at = {}\n        else:\n            reward_at = {\n                            terminal_state: 1.0,\n                            start_coord: 0.01,\n                            (self.room_size//2, self.room_size//2): 0.1\n                        }\n\n        # Check remove_walls\n        if remove_walls:\n            walls = ()\n\n        # Init base class\n        GridWorld.__init__(self,\n                           nrows=nrows,\n                           ncols=ncols,\n                           start_coord=start_coord,\n                           terminal_states=terminal_states,\n                           success_probability=success_probability,\n                           reward_at=reward_at,\n                           walls=walls,\n                           default_reward=0.0)\n\n        # Check initial distribution\n        if initial_state_distribution == 'uniform':\n            distr = np.ones(self.observation_space.n) / self.observation_space.n\n            self.set_initial_state_distribution(distr)\n\n        # spaces\n        if self.array_observation:\n            self.discrete_observation_space = self.observation_space\n            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))",
  "def _convert_room_coord_to_global(self, room_row, room_col, room_coord_row, room_coord_col):\n        col_offset = (self.room_size+1)*room_col\n        row_offset = (self.room_size+1)*room_row\n\n        row = room_coord_row + row_offset\n        col = room_coord_col + col_offset\n        return (row, col)",
  "def _convert_index_to_float_coord(self, state_index):\n        yy, xx = self.index2coord[state_index]\n\n        # centering\n        xx = xx + 0.5\n        yy = yy + 0.5\n        # map to [0, 1]\n        xx = xx/self.ncols\n        yy = yy/self.nrows\n        return np.array([xx, yy])",
  "def reset(self):\n        self.state = GridWorld.reset(self)\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n        return state_to_return",
  "def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n\n        return state_to_return, reward, done, info",
  "def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # traps\n        for (y, x) in self.traps:\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.5, 0.0, 0.0))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # walls\n        for wall in self.walls:\n            y, x = wall\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.25, 0.25, 0.25))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # rewards\n        for (y, x) in self.reward_at:\n            flag = GeometricPrimitive(\"POLYGON\")\n            rwd = self.reward_at[(y, x)]\n            if rwd == 1.0:\n                flag.set_color((0.0, 0.5, 0.0))\n            elif rwd == 0.1:\n                flag.set_color((0.0, 0.0, 0.5))\n            else:\n                flag.set_color((0.5, 0.0, 0.0))\n\n            x += 0.5\n            y += 0.25\n            flag.add_vertex((x, y))\n            flag.add_vertex((x + 0.25, y + 0.5))\n            flag.add_vertex((x - 0.25, y + 0.5))\n            bg.add_shape(flag)\n\n        return bg",
  "class AppleGold(GridWorld):\n    \"\"\"\n    AppleGold with six rooms: this is merely a slightly modified\n    version of SixRoom.\n\n    Parameters\n    ----------\n    reward_free : bool, default=False\n        If true, no rewards are given to the agent.\n    array_observation:\n        If true, the observations are converted to an array (x, y)\n        instead of a discrete index.\n\n    Notes\n    -----\n    The function env.sample() does not handle conversions to array states\n    when array_observation is True. Only the functions env.reset() and\n    env.step() are covered.\n\n    Reference\n    ---------\n    .. seaalso::\n        Guo et al.: Self-Imitation Learning via\n        Trajectory-Conditioned Policy\n        for Hard-Exploration Tasks\n        arXiv preprint arXiv:1907.10247\n    \"\"\"\n    name = \"AppleGold\"\n\n    def __init__(self, reward_free=False, array_observation=False):\n        self.reward_free = reward_free\n        self.array_observation = array_observation\n\n        # Common parameters\n        nrows = 13\n        ncols = 17\n        start_coord = (5, 1)\n        terminal_states = ((7, 7),)\n        success_probability = 0.95\n        #\n        walls = ()\n        for ii in range(13):\n            walls += ((ii, 0),)\n            walls += ((ii, 16),)\n        for jj in range(17):\n            walls += ((0, jj),)\n            walls += ((12, jj),)\n        for ii in range(13):\n            if ii not in [1, 11]:\n                walls += ((ii, 6),)\n                walls += ((ii, 10),)\n        walls += ((11, 6),)\n        for jj in range(17):\n            if jj not in [1, 15]:\n                walls += ((6, jj),)\n\n        # Default reward according to the difficulty\n        default_reward = 0\n\n        # Rewards according to the difficulty\n        if self.reward_free:\n            reward_at = {}\n        else:\n            reward_at = {\n                        (7, 7): 10.0,\n                        (8, 2): 1.0,\n                        (10, 3): 1.0\n                        }\n            for jj in range(7, 16):\n                for ii in range(1, 12):\n                    if (ii, jj) not in walls and (ii, jj) != (7, 7):\n                        reward_at[(ii, jj)] = -0.05\n\n        # Init base class\n        GridWorld.__init__(self,\n                           nrows=nrows,\n                           ncols=ncols,\n                           start_coord=start_coord,\n                           terminal_states=terminal_states,\n                           success_probability=success_probability,\n                           reward_at=reward_at,\n                           walls=walls,\n                           default_reward=default_reward)\n\n        # spaces\n        if self.array_observation:\n            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))\n\n    def _convert_index_to_float_coord(self, state_index):\n        yy, xx = self.index2coord[state_index]\n\n        # centering\n        xx = xx + 0.5\n        yy = yy + 0.5\n        # map to [0, 1]\n        xx = xx/self.ncols\n        yy = yy/self.nrows\n        return np.array([xx, yy])\n\n    def reset(self):\n        self.state = self.coord2index[self.start_coord]\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n        return state_to_return\n\n    def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n\n        return state_to_return, reward, done, info\n\n    def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # walls\n        for wall in self.walls:\n            y, x = wall\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.25, 0.25, 0.25))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # rewards\n        for (y, x) in self.reward_at:\n            rwd = self.reward_at[(y, x)]\n            if rwd == -0.05:\n                rock = GeometricPrimitive(\"POLYGON\")\n                rock.set_color((0.6, 0.6, 0.6))\n                rock.add_vertex((x, y))\n                rock.add_vertex((x + 1, y))\n                rock.add_vertex((x + 1, y + 1))\n                rock.add_vertex((x, y + 1))\n                bg.add_shape(rock)\n            else:\n                flag = GeometricPrimitive(\"POLYGON\")\n                if rwd == 10:\n                    flag.set_color((0.0, 0.5, 0.0))\n                elif rwd == 1:\n                    flag.set_color((0.0, 0.0, 0.5))\n\n                x += 0.5\n                y += 0.25\n                flag.add_vertex((x, y))\n                flag.add_vertex((x + 0.25, y + 0.5))\n                flag.add_vertex((x - 0.25, y + 0.5))\n                bg.add_shape(flag)\n\n        return bg",
  "def __init__(self, reward_free=False, array_observation=False):\n        self.reward_free = reward_free\n        self.array_observation = array_observation\n\n        # Common parameters\n        nrows = 13\n        ncols = 17\n        start_coord = (5, 1)\n        terminal_states = ((7, 7),)\n        success_probability = 0.95\n        #\n        walls = ()\n        for ii in range(13):\n            walls += ((ii, 0),)\n            walls += ((ii, 16),)\n        for jj in range(17):\n            walls += ((0, jj),)\n            walls += ((12, jj),)\n        for ii in range(13):\n            if ii not in [1, 11]:\n                walls += ((ii, 6),)\n                walls += ((ii, 10),)\n        walls += ((11, 6),)\n        for jj in range(17):\n            if jj not in [1, 15]:\n                walls += ((6, jj),)\n\n        # Default reward according to the difficulty\n        default_reward = 0\n\n        # Rewards according to the difficulty\n        if self.reward_free:\n            reward_at = {}\n        else:\n            reward_at = {\n                        (7, 7): 10.0,\n                        (8, 2): 1.0,\n                        (10, 3): 1.0\n                        }\n            for jj in range(7, 16):\n                for ii in range(1, 12):\n                    if (ii, jj) not in walls and (ii, jj) != (7, 7):\n                        reward_at[(ii, jj)] = -0.05\n\n        # Init base class\n        GridWorld.__init__(self,\n                           nrows=nrows,\n                           ncols=ncols,\n                           start_coord=start_coord,\n                           terminal_states=terminal_states,\n                           success_probability=success_probability,\n                           reward_at=reward_at,\n                           walls=walls,\n                           default_reward=default_reward)\n\n        # spaces\n        if self.array_observation:\n            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))",
  "def _convert_index_to_float_coord(self, state_index):\n        yy, xx = self.index2coord[state_index]\n\n        # centering\n        xx = xx + 0.5\n        yy = yy + 0.5\n        # map to [0, 1]\n        xx = xx/self.ncols\n        yy = yy/self.nrows\n        return np.array([xx, yy])",
  "def reset(self):\n        self.state = self.coord2index[self.start_coord]\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n        return state_to_return",
  "def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n\n        state_to_return = self.state\n        if self.array_observation:\n            state_to_return = self._convert_index_to_float_coord(self.state)\n\n        return state_to_return, reward, done, info",
  "def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # walls\n        for wall in self.walls:\n            y, x = wall\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.25, 0.25, 0.25))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # rewards\n        for (y, x) in self.reward_at:\n            rwd = self.reward_at[(y, x)]\n            if rwd == -0.05:\n                rock = GeometricPrimitive(\"POLYGON\")\n                rock.set_color((0.6, 0.6, 0.6))\n                rock.add_vertex((x, y))\n                rock.add_vertex((x + 1, y))\n                rock.add_vertex((x + 1, y + 1))\n                rock.add_vertex((x, y + 1))\n                bg.add_shape(rock)\n            else:\n                flag = GeometricPrimitive(\"POLYGON\")\n                if rwd == 10:\n                    flag.set_color((0.0, 0.5, 0.0))\n                elif rwd == 1:\n                    flag.set_color((0.0, 0.0, 0.5))\n\n                x += 0.5\n                y += 0.25\n                flag.add_vertex((x, y))\n                flag.add_vertex((x + 0.25, y + 0.5))\n                flag.add_vertex((x - 0.25, y + 0.5))\n                bg.add_shape(flag)\n\n        return bg",
  "def get_benchmark_env(level=1):\n    if level == 0:\n        env = _get_autoreset_env(BallLevel0())\n        return env\n    elif level == 1:\n        env = _get_autoreset_env(BallLevel1())\n        return env\n    elif level == 2:\n        env = _get_autoreset_env(BallLevel2())\n        return env\n    elif level == 3:\n        env = _get_autoreset_env(BallLevel3())\n        return env\n    elif level == 4:\n        env = _get_autoreset_env(BallLevel4())\n        return env\n    elif level == 5:\n        env = _get_autoreset_env(BallLevel5())\n        return env\n    else:\n        raise NotImplementedError(\"Invalid benchmark level.\")",
  "def _get_autoreset_env(env):\n    horizon = env.horizon\n    return AutoResetWrapper(env, horizon)",
  "class BallLevel0(PBall2D):\n    \"\"\"\n    Reward-free (0 reward)\n    \"\"\"\n    def __init__(self):\n        self.horizon = 30\n        #\n        self.p = 2\n        self.action_list = [np.array([0.0, 0.0]),\n                            0.05 * np.array([1.0, 0.0]),\n                            -0.05 * np.array([1.0, 0.0]),\n                            0.05 * np.array([0.0, 1.0]),\n                            -0.05 * np.array([0.0, 1.0])]\n\n        self.reward_amplitudes = []\n        self.reward_smoothness = []\n        self.reward_centers = []\n        self.A = np.eye(2)\n        self.B = np.eye(2)\n        self.sigma = 0.01\n        self.sigma_init = 0.001\n        self.mu_init = np.array([0.0, 0.0])\n\n        PBall2D.__init__(self,\n                         self.p,\n                         self.action_list,\n                         self.reward_amplitudes,\n                         self.reward_smoothness,\n                         self.reward_centers,\n                         self.A,\n                         self.B,\n                         self.sigma,\n                         self.sigma_init,\n                         self.mu_init)\n        self.name = \"Ball Exploration Benchmark - Level 0 (Reward-Free)\"",
  "class BallLevel1(PBall2D):\n    \"\"\"\n    Dense rewards\n    \"\"\"\n    def __init__(self):\n        self.horizon = 30\n        #\n        self.p = 2\n        self.action_list = [np.array([0.0, 0.0]),\n                            0.05 * np.array([1.0, 0.0]),\n                            -0.05 * np.array([1.0, 0.0]),\n                            0.05 * np.array([0.0, 1.0]),\n                            -0.05 * np.array([0.0, 1.0])]\n\n        self.reward_amplitudes = np.array([1.0])\n        self.reward_smoothness = np.array([0.5*np.sqrt(2)])\n        self.reward_centers = [np.array([0.5, 0.5])]\n        self.A = np.eye(2)\n        self.B = np.eye(2)\n        self.sigma = 0.01\n        self.sigma_init = 0.001\n        self.mu_init = np.array([0.0, 0.0])\n\n        PBall2D.__init__(self,\n                         self.p,\n                         self.action_list,\n                         self.reward_amplitudes,\n                         self.reward_smoothness,\n                         self.reward_centers,\n                         self.A,\n                         self.B,\n                         self.sigma,\n                         self.sigma_init,\n                         self.mu_init)\n        self.name = \"Ball Exploration Benchmark - Level 1\"",
  "class BallLevel2(BallLevel1):\n    \"\"\"\n    Sparse rewards\n    \"\"\"\n    def __init__(self):\n        BallLevel1.__init__(self)\n        self.reward_amplitudes = np.array([1.0])\n        self.reward_smoothness = np.array([0.2])\n        self.reward_centers = [np.array([0.5, 0.5])]\n        self.name = \"Ball Exploration Benchmark - Level 2\"",
  "class BallLevel3(BallLevel2):\n    \"\"\"\n    Sparse rewards, noisier\n    \"\"\"\n    def __init__(self):\n        BallLevel2.__init__(self)\n        self.sigma = 0.025\n        self.name = \"Ball Exploration Benchmark - Level 3\"",
  "class BallLevel4(BallLevel1):\n    \"\"\"\n    Far sparse reward (as lvl 2) + dense suboptimal rewards\n    \"\"\"\n    def __init__(self):\n        BallLevel1.__init__(self)\n\n        self.reward_amplitudes = np.array([1.0, 0.1])\n        self.reward_smoothness = np.array([0.2, 0.5*np.sqrt(2)])\n        self.reward_centers = [np.array([-0.5, -0.5]),     # far sparse\n                               np.array([0.5, 0.5])]       # dense\n        self.name = \"Ball Exploration Benchmark - Level 4\"",
  "class BallLevel5(BallLevel4):\n    \"\"\"\n    Far sparse reward (as lvl 2) + dense suboptimal rewards, noisier\n    \"\"\"\n    def __init__(self):\n        BallLevel4.__init__(self)\n        self.sigma = 0.025\n        self.name = \"Ball Exploration Benchmark - Level 5\"",
  "def __init__(self):\n        self.horizon = 30\n        #\n        self.p = 2\n        self.action_list = [np.array([0.0, 0.0]),\n                            0.05 * np.array([1.0, 0.0]),\n                            -0.05 * np.array([1.0, 0.0]),\n                            0.05 * np.array([0.0, 1.0]),\n                            -0.05 * np.array([0.0, 1.0])]\n\n        self.reward_amplitudes = []\n        self.reward_smoothness = []\n        self.reward_centers = []\n        self.A = np.eye(2)\n        self.B = np.eye(2)\n        self.sigma = 0.01\n        self.sigma_init = 0.001\n        self.mu_init = np.array([0.0, 0.0])\n\n        PBall2D.__init__(self,\n                         self.p,\n                         self.action_list,\n                         self.reward_amplitudes,\n                         self.reward_smoothness,\n                         self.reward_centers,\n                         self.A,\n                         self.B,\n                         self.sigma,\n                         self.sigma_init,\n                         self.mu_init)\n        self.name = \"Ball Exploration Benchmark - Level 0 (Reward-Free)\"",
  "def __init__(self):\n        self.horizon = 30\n        #\n        self.p = 2\n        self.action_list = [np.array([0.0, 0.0]),\n                            0.05 * np.array([1.0, 0.0]),\n                            -0.05 * np.array([1.0, 0.0]),\n                            0.05 * np.array([0.0, 1.0]),\n                            -0.05 * np.array([0.0, 1.0])]\n\n        self.reward_amplitudes = np.array([1.0])\n        self.reward_smoothness = np.array([0.5*np.sqrt(2)])\n        self.reward_centers = [np.array([0.5, 0.5])]\n        self.A = np.eye(2)\n        self.B = np.eye(2)\n        self.sigma = 0.01\n        self.sigma_init = 0.001\n        self.mu_init = np.array([0.0, 0.0])\n\n        PBall2D.__init__(self,\n                         self.p,\n                         self.action_list,\n                         self.reward_amplitudes,\n                         self.reward_smoothness,\n                         self.reward_centers,\n                         self.A,\n                         self.B,\n                         self.sigma,\n                         self.sigma_init,\n                         self.mu_init)\n        self.name = \"Ball Exploration Benchmark - Level 1\"",
  "def __init__(self):\n        BallLevel1.__init__(self)\n        self.reward_amplitudes = np.array([1.0])\n        self.reward_smoothness = np.array([0.2])\n        self.reward_centers = [np.array([0.5, 0.5])]\n        self.name = \"Ball Exploration Benchmark - Level 2\"",
  "def __init__(self):\n        BallLevel2.__init__(self)\n        self.sigma = 0.025\n        self.name = \"Ball Exploration Benchmark - Level 3\"",
  "def __init__(self):\n        BallLevel1.__init__(self)\n\n        self.reward_amplitudes = np.array([1.0, 0.1])\n        self.reward_smoothness = np.array([0.2, 0.5*np.sqrt(2)])\n        self.reward_centers = [np.array([-0.5, -0.5]),     # far sparse\n                               np.array([0.5, 0.5])]       # dense\n        self.name = \"Ball Exploration Benchmark - Level 4\"",
  "def __init__(self):\n        BallLevel4.__init__(self)\n        self.sigma = 0.025\n        self.name = \"Ball Exploration Benchmark - Level 5\"",
  "def projection_to_pball(x, p):\n    \"\"\"\n    Solve the problem:\n        min_z  ||x-z||_2^2\n        s.t.   ||z||_p  <= 1\n    for p = 2 or p = np.inf\n\n    If p is not 2 or np.inf, it returns x/norm_p(x) if norm_p(x) > 1\n\n    WARNING: projection_to_pball is not actually a projection for p!=2\n    or p=!np.inf\n    \"\"\"\n    if np.linalg.norm(x, ord=p) <= 1.0:\n        return x\n\n    if p == 2:\n        z = x / np.linalg.norm(x, ord=p)\n        return z\n\n    if p == np.inf:\n        z = np.minimum(1.0, np.maximum(x, -1.0))\n        return z\n\n        # below it is not a projection\n    return x / np.linalg.norm(x, ord=p)",
  "class PBall(Model):\n    \"\"\"\n    Parametric family of environments whose state space is a unit sphere\n    according to the p-norm in R^d.\n\n    Note:\n        The projection function is only a true projection for\n        p in {2, infinity}.\n\n    ----------------------------------------------------------------------\n    State space:\n        x in R^d: norm_p (x) <= 1\n\n        implemented as rlberry.spaces.Box representing [0, 1]^d\n    ----------------------------------------------------------------------\n    Action space:\n        {u_1, ..., u_m} such that u_i in R^d'  for i = 1, ..., m\n\n        implemented as rlberry.spaces.Discrete(m)\n    ----------------------------------------------------------------------\n    Reward function (independent of the actions):\n        r(x) = sum_{i=1}^n  b_i  max( 0,  1 - norm_p( x - x_i )/c_i )\n\n        requirements:\n            c_i >= 0\n            b_i in [0, 1]\n    ----------------------------------------------------------------------\n    Transitions:\n        x_{t+1} = A x_t + B u_t + N\n\n        where\n            A: square matrix of size d\n            B: matrix of size (d, d')\n            N: d-dimensional Gaussian noise with zero mean and covariance\n            matrix sigma*I\n    ----------------------------------------------------------------------\n    Initial state:\n        d-dimensional Gaussian with mean mu_init and covariance matrix\n        sigma_init*I\n    ----------------------------------------------------------------------\n\n    Default parameters are provided for a 2D environment, PBall2D\n    \"\"\"\n\n    name = \"LP-Ball\"\n\n    def __init__(self,\n                 p,\n                 action_list,\n                 reward_amplitudes,\n                 reward_smoothness,\n                 reward_centers,\n                 A,\n                 B,\n                 sigma,\n                 sigma_init,\n                 mu_init):\n        \"\"\"\n        Parameters\n        -----------\n        p : int\n            parameter of the p-norm\n        action_list : list\n            list of actions {u_1, ..., u_m}, each action u_i is a\n            d'-dimensional array\n        reward_amplitudes: list\n            list of reward amplitudes: {b_1, ..., b_n}\n        reward_smoothness : list\n            list of reward smoothness: {c_1, ..., c_n}\n        reward_centers : list\n            list of reward centers:    {x_1, ..., x_n}\n        A : numpy.ndarray\n            array A of size (d, d)\n        B : numpy.ndarray\n            array B of size (d, d')\n        sigma : double\n            transition noise sigma\n        sigma_init : double\n            initial state noise sigma_init\n        mu_init : numpy.ndarray\n            array of size (d,) containing the mean of the initial state\n        \"\"\"\n        Model.__init__(self)\n\n        assert p >= 1, \"PBall requires p>=1\"\n        if p not in [2, np.inf]:\n            logger.warning(\"For p!=2 or p!=np.inf, PBall \\\ndoes not make true projections onto the lp ball.\")\n        self.p = p\n        self.d, self.dp = B.shape  # d and d'\n        self.m = len(action_list)\n        self.action_list = action_list\n        self.reward_amplitudes = reward_amplitudes\n        self.reward_smoothness = reward_smoothness\n        self.reward_centers = reward_centers\n        self.A = A\n        self.B = B\n        self.sigma = sigma\n        self.sigma_init = sigma_init\n        self.mu_init = mu_init\n\n        # State and action spaces\n        low = -1.0 * np.ones(self.d, dtype=np.float64)\n        high = np.ones(self.d, dtype=np.float64)\n        self.observation_space = spaces.Box(low, high)\n        self.action_space = spaces.Discrete(self.m)\n\n        # reward range\n        assert len(self.reward_amplitudes) == len(self.reward_smoothness)\n        assert len(self.reward_amplitudes) == len(self.reward_centers)\n        if len(self.reward_amplitudes) > 0:\n            assert self.reward_amplitudes.max() <= 1.0 and \\\n                self.reward_amplitudes.min() >= 0.0, \\\n                \"reward amplitudes b_i must be in [0, 1]\"\n            assert self.reward_smoothness.min() > 0.0, \\\n                \"reward smoothness c_i must be > 0\"\n        self.reward_range = (0, 1.0)\n\n        #\n        self.name = \"Lp-Ball\"\n\n        # Initalize state\n        self.reset()\n\n    def reset(self, state=None):\n        if state is not None:\n            self.state = state\n        else:\n            self.state = self.mu_init \\\n                + self.sigma_init * self.rng.normal(size=self.d)\n            # projection to unit ball\n        self.state = projection_to_pball(self.state, self.p)\n        return self.state.copy()\n\n    def sample(self, state, action):\n        assert self.action_space.contains(action)\n        assert self.observation_space.contains(state)\n\n        # next state\n        action_vec = self.action_list[action]\n        next_s = self.A.dot(state) + self.B.dot(action_vec) \\\n            + self.sigma * self.rng.normal(size=self.d)\n        next_s = projection_to_pball(next_s, self.p)\n\n        # done and reward\n        done = False\n        reward = self.compute_reward_at(state)\n\n        return next_s, reward, done, {}\n\n    def step(self, action):\n        next_s, reward, done, info = self.sample(self.state, action)\n        self.state = next_s.copy()\n        return next_s, reward, done, info\n\n    def compute_reward_at(self, x):\n        reward = 0.0\n        for ii, b_ii in enumerate(self.reward_amplitudes):\n            c_ii = self.reward_smoothness[ii]\n            x_ii = self.reward_centers[ii]\n            dist = np.linalg.norm(x - x_ii, ord=self.p)\n            reward += b_ii * max(0.0, 1.0 - dist / c_ii)\n        return reward\n\n    def get_reward_lipschitz_constant(self):\n        ratios = self.reward_amplitudes / self.reward_smoothness\n        Lr = ratios.max()\n        return Lr\n\n    def get_transitions_lipschitz_constant(self):\n        \"\"\"\n        note: considers a fixed action, returns Lipschitz constant\n        w.r.t. to states.\n\n        If p!=1, p!=2 or p!=np.inf, returns an upper bound on the induced norm\n        \"\"\"\n        if self.p == 1:\n            order = np.inf\n        else:\n            order = self.p / (self.p - 1.0)\n\n        if order in [1, 2]:\n            return np.linalg.norm(self.A, ord=order)\n\n        # If p!=1, p!=2 or p!=np.inf, return upper bound on the induced norm.\n        return np.power(self.d, 1.0 / self.p) * np.linalg.norm(self.A,\n                                                               ord=np.inf)",
  "class PBall2D(RenderInterface2D, PBall):\n    def __init__(self,\n                 p=2,\n                 action_list=[0.05 * np.array([1, 0]),\n                              -0.05 * np.array([1, 0]),\n                              0.05 * np.array([0, 1]),\n                              -0.05 * np.array([0, 1])],\n                 reward_amplitudes=np.array([1.0]),\n                 reward_smoothness=np.array([0.25]),\n                 reward_centers=[np.array([0.75, 0.0])],\n                 A=np.eye(2),\n                 B=np.eye(2),\n                 sigma=0.01,\n                 sigma_init=0.001,\n                 mu_init=np.array([0.0, 0.0])\n                 ):\n        # Initialize PBall\n        PBall.__init__(self, p, action_list, reward_amplitudes,\n                       reward_smoothness,\n                       reward_centers,\n                       A, B, sigma, sigma_init, mu_init)\n\n        # Render interface\n        RenderInterface2D.__init__(self)\n\n        # rendering info\n        self.set_clipping_area((-1, 1, -1, 1))\n        self.set_refresh_interval(50)  # in milliseconds\n\n    def step(self, action):\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state.copy())\n        return PBall.step(self, action)\n\n    #\n    # Code for rendering\n    #\n\n    def _get_ball_shape(self, xcenter, radius):\n        shape = GeometricPrimitive(\"POLYGON\")\n        n_points = 200\n        theta_vals = np.linspace(0.0, 2 * np.pi, n_points)\n        for theta in theta_vals:\n            pp = np.array([2.0 * np.cos(theta), 2.0 * np.sin(theta)])\n            pp = xcenter + radius * projection_to_pball(pp, self.p)\n            # project to the main ball after translation\n            pp = projection_to_pball(pp, self.p)\n            shape.add_vertex((pp[0], pp[1]))\n        return shape\n\n    def get_background(self):\n        bg = Scene()\n\n        # ball shape\n        contour = self._get_ball_shape(np.zeros(2), 1.0)\n        contour.set_color((0.0, 0.0, 0.5))\n        bg.add_shape(contour)\n\n        # reward position\n        for ii, ampl in enumerate(self.reward_amplitudes):\n            contour = self._get_ball_shape(self.reward_centers[ii],\n                                           self.reward_smoothness[ii])\n            ampl = 1.0 - ampl  # dark violet = more reward\n            contour.set_color((0.5, 0.0, 0.5 * (1.0 + ampl)))\n            bg.add_shape(contour)\n\n        return bg\n\n    def get_scene(self, state):\n        scene = Scene()\n\n        agent = GeometricPrimitive(\"QUADS\")\n        agent.set_color((0.75, 0.0, 0.5))\n        size = 0.05\n        x = state[0]\n        y = state[1]\n        agent.add_vertex((x - size / 4.0, y - size))\n        agent.add_vertex((x + size / 4.0, y - size))\n        agent.add_vertex((x + size / 4.0, y + size))\n        agent.add_vertex((x - size / 4.0, y + size))\n\n        agent.add_vertex((x - size, y - size / 4.0))\n        agent.add_vertex((x + size, y - size / 4.0))\n        agent.add_vertex((x + size, y + size / 4.0))\n        agent.add_vertex((x - size, y + size / 4.0))\n\n        scene.add_shape(agent)\n        return scene",
  "class SimplePBallND(PBall):\n    \"\"\"\n    PBall environment in d dimensions with simple dynamics.\n    \"\"\"\n\n    def __init__(self,\n                 p=2,\n                 dim=2,\n                 action_amplitude=0.05,\n                 r_smoothness=0.25,\n                 sigma=0.01,\n                 sigma_init=0.001,\n                 mu_init=None\n                 ):\n        # Action list\n        action_list = []\n        for dd in range(dim):\n            aux = np.zeros(dim)\n            aux[dd] = action_amplitude\n            action_list.append(aux)\n            action_list.append(-1 * aux)\n\n        # Rewards\n        reward_amplitudes = np.array([1.0])\n        reward_smoothness = np.array([r_smoothness])\n        reward_centers = [np.zeros(dim)]\n        reward_centers[0][0] = 0.8\n\n        # Transitions\n        A = np.eye(dim)\n        B = np.eye(dim)\n\n        # Initial position\n        if mu_init is None:\n            mu_init = np.zeros(dim)\n\n        # Initialize PBall\n        PBall.__init__(self, p, action_list, reward_amplitudes,\n                       reward_smoothness,\n                       reward_centers,\n                       A, B, sigma, sigma_init, mu_init)",
  "def __init__(self,\n                 p,\n                 action_list,\n                 reward_amplitudes,\n                 reward_smoothness,\n                 reward_centers,\n                 A,\n                 B,\n                 sigma,\n                 sigma_init,\n                 mu_init):\n        \"\"\"\n        Parameters\n        -----------\n        p : int\n            parameter of the p-norm\n        action_list : list\n            list of actions {u_1, ..., u_m}, each action u_i is a\n            d'-dimensional array\n        reward_amplitudes: list\n            list of reward amplitudes: {b_1, ..., b_n}\n        reward_smoothness : list\n            list of reward smoothness: {c_1, ..., c_n}\n        reward_centers : list\n            list of reward centers:    {x_1, ..., x_n}\n        A : numpy.ndarray\n            array A of size (d, d)\n        B : numpy.ndarray\n            array B of size (d, d')\n        sigma : double\n            transition noise sigma\n        sigma_init : double\n            initial state noise sigma_init\n        mu_init : numpy.ndarray\n            array of size (d,) containing the mean of the initial state\n        \"\"\"\n        Model.__init__(self)\n\n        assert p >= 1, \"PBall requires p>=1\"\n        if p not in [2, np.inf]:\n            logger.warning(\"For p!=2 or p!=np.inf, PBall \\\ndoes not make true projections onto the lp ball.\")\n        self.p = p\n        self.d, self.dp = B.shape  # d and d'\n        self.m = len(action_list)\n        self.action_list = action_list\n        self.reward_amplitudes = reward_amplitudes\n        self.reward_smoothness = reward_smoothness\n        self.reward_centers = reward_centers\n        self.A = A\n        self.B = B\n        self.sigma = sigma\n        self.sigma_init = sigma_init\n        self.mu_init = mu_init\n\n        # State and action spaces\n        low = -1.0 * np.ones(self.d, dtype=np.float64)\n        high = np.ones(self.d, dtype=np.float64)\n        self.observation_space = spaces.Box(low, high)\n        self.action_space = spaces.Discrete(self.m)\n\n        # reward range\n        assert len(self.reward_amplitudes) == len(self.reward_smoothness)\n        assert len(self.reward_amplitudes) == len(self.reward_centers)\n        if len(self.reward_amplitudes) > 0:\n            assert self.reward_amplitudes.max() <= 1.0 and \\\n                self.reward_amplitudes.min() >= 0.0, \\\n                \"reward amplitudes b_i must be in [0, 1]\"\n            assert self.reward_smoothness.min() > 0.0, \\\n                \"reward smoothness c_i must be > 0\"\n        self.reward_range = (0, 1.0)\n\n        #\n        self.name = \"Lp-Ball\"\n\n        # Initalize state\n        self.reset()",
  "def reset(self, state=None):\n        if state is not None:\n            self.state = state\n        else:\n            self.state = self.mu_init \\\n                + self.sigma_init * self.rng.normal(size=self.d)\n            # projection to unit ball\n        self.state = projection_to_pball(self.state, self.p)\n        return self.state.copy()",
  "def sample(self, state, action):\n        assert self.action_space.contains(action)\n        assert self.observation_space.contains(state)\n\n        # next state\n        action_vec = self.action_list[action]\n        next_s = self.A.dot(state) + self.B.dot(action_vec) \\\n            + self.sigma * self.rng.normal(size=self.d)\n        next_s = projection_to_pball(next_s, self.p)\n\n        # done and reward\n        done = False\n        reward = self.compute_reward_at(state)\n\n        return next_s, reward, done, {}",
  "def step(self, action):\n        next_s, reward, done, info = self.sample(self.state, action)\n        self.state = next_s.copy()\n        return next_s, reward, done, info",
  "def compute_reward_at(self, x):\n        reward = 0.0\n        for ii, b_ii in enumerate(self.reward_amplitudes):\n            c_ii = self.reward_smoothness[ii]\n            x_ii = self.reward_centers[ii]\n            dist = np.linalg.norm(x - x_ii, ord=self.p)\n            reward += b_ii * max(0.0, 1.0 - dist / c_ii)\n        return reward",
  "def get_reward_lipschitz_constant(self):\n        ratios = self.reward_amplitudes / self.reward_smoothness\n        Lr = ratios.max()\n        return Lr",
  "def get_transitions_lipschitz_constant(self):\n        \"\"\"\n        note: considers a fixed action, returns Lipschitz constant\n        w.r.t. to states.\n\n        If p!=1, p!=2 or p!=np.inf, returns an upper bound on the induced norm\n        \"\"\"\n        if self.p == 1:\n            order = np.inf\n        else:\n            order = self.p / (self.p - 1.0)\n\n        if order in [1, 2]:\n            return np.linalg.norm(self.A, ord=order)\n\n        # If p!=1, p!=2 or p!=np.inf, return upper bound on the induced norm.\n        return np.power(self.d, 1.0 / self.p) * np.linalg.norm(self.A,\n                                                               ord=np.inf)",
  "def __init__(self,\n                 p=2,\n                 action_list=[0.05 * np.array([1, 0]),\n                              -0.05 * np.array([1, 0]),\n                              0.05 * np.array([0, 1]),\n                              -0.05 * np.array([0, 1])],\n                 reward_amplitudes=np.array([1.0]),\n                 reward_smoothness=np.array([0.25]),\n                 reward_centers=[np.array([0.75, 0.0])],\n                 A=np.eye(2),\n                 B=np.eye(2),\n                 sigma=0.01,\n                 sigma_init=0.001,\n                 mu_init=np.array([0.0, 0.0])\n                 ):\n        # Initialize PBall\n        PBall.__init__(self, p, action_list, reward_amplitudes,\n                       reward_smoothness,\n                       reward_centers,\n                       A, B, sigma, sigma_init, mu_init)\n\n        # Render interface\n        RenderInterface2D.__init__(self)\n\n        # rendering info\n        self.set_clipping_area((-1, 1, -1, 1))\n        self.set_refresh_interval(50)",
  "def step(self, action):\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state.copy())\n        return PBall.step(self, action)",
  "def _get_ball_shape(self, xcenter, radius):\n        shape = GeometricPrimitive(\"POLYGON\")\n        n_points = 200\n        theta_vals = np.linspace(0.0, 2 * np.pi, n_points)\n        for theta in theta_vals:\n            pp = np.array([2.0 * np.cos(theta), 2.0 * np.sin(theta)])\n            pp = xcenter + radius * projection_to_pball(pp, self.p)\n            # project to the main ball after translation\n            pp = projection_to_pball(pp, self.p)\n            shape.add_vertex((pp[0], pp[1]))\n        return shape",
  "def get_background(self):\n        bg = Scene()\n\n        # ball shape\n        contour = self._get_ball_shape(np.zeros(2), 1.0)\n        contour.set_color((0.0, 0.0, 0.5))\n        bg.add_shape(contour)\n\n        # reward position\n        for ii, ampl in enumerate(self.reward_amplitudes):\n            contour = self._get_ball_shape(self.reward_centers[ii],\n                                           self.reward_smoothness[ii])\n            ampl = 1.0 - ampl  # dark violet = more reward\n            contour.set_color((0.5, 0.0, 0.5 * (1.0 + ampl)))\n            bg.add_shape(contour)\n\n        return bg",
  "def get_scene(self, state):\n        scene = Scene()\n\n        agent = GeometricPrimitive(\"QUADS\")\n        agent.set_color((0.75, 0.0, 0.5))\n        size = 0.05\n        x = state[0]\n        y = state[1]\n        agent.add_vertex((x - size / 4.0, y - size))\n        agent.add_vertex((x + size / 4.0, y - size))\n        agent.add_vertex((x + size / 4.0, y + size))\n        agent.add_vertex((x - size / 4.0, y + size))\n\n        agent.add_vertex((x - size, y - size / 4.0))\n        agent.add_vertex((x + size, y - size / 4.0))\n        agent.add_vertex((x + size, y + size / 4.0))\n        agent.add_vertex((x - size, y + size / 4.0))\n\n        scene.add_shape(agent)\n        return scene",
  "def __init__(self,\n                 p=2,\n                 dim=2,\n                 action_amplitude=0.05,\n                 r_smoothness=0.25,\n                 sigma=0.01,\n                 sigma_init=0.001,\n                 mu_init=None\n                 ):\n        # Action list\n        action_list = []\n        for dd in range(dim):\n            aux = np.zeros(dim)\n            aux[dd] = action_amplitude\n            action_list.append(aux)\n            action_list.append(-1 * aux)\n\n        # Rewards\n        reward_amplitudes = np.array([1.0])\n        reward_smoothness = np.array([r_smoothness])\n        reward_centers = [np.zeros(dim)]\n        reward_centers[0][0] = 0.8\n\n        # Transitions\n        A = np.eye(dim)\n        B = np.eye(dim)\n\n        # Initial position\n        if mu_init is None:\n            mu_init = np.zeros(dim)\n\n        # Initialize PBall\n        PBall.__init__(self, p, action_list, reward_amplitudes,\n                       reward_smoothness,\n                       reward_centers,\n                       A, B, sigma, sigma_init, mu_init)",
  "class TwinRooms(RenderInterface2D, Model):\n    \"\"\"\n    Two continuous grid worlds, side by side, separated by a wall.\n    Both are identical (or almost identical), and the agent has equal probability to\n    start in any of the two rooms.\n\n    It can be used to test the generalization capability of agents:\n    a policy learned in one of the rooms can be used to learn faster\n    a policy in the other room.\n\n    There are 4 actions, one for each direction (left/right/up/down).\n\n    Parameters\n    ----------\n    noise_room1: double, default: 0.01\n        Noise in the transitions of the first room.\n    noise_room2: double, default: 0.01\n        Noise in the transitions of the second room.\n\n    Notes\n    -----\n    The function env.sample() does not handle conversions to array states\n    when array_observation is True. Only the functions env.reset() and\n    env.step() are covered.\n    \"\"\"\n    name = \"TwinRooms\"\n\n    def __init__(self,\n                 noise_room1=0.01,\n                 noise_room2=0.01):\n        Model.__init__(self)\n        RenderInterface2D.__init__(self)\n\n        self.observation_space = spaces.Box(\n            low=np.array([0.0, 0.0]),\n            high=np.array([2.0, 1.0]),\n            )\n        self.action_space = spaces.Discrete(4)\n        self.reward_range = (0.0, 1.0)\n\n        self.room_noises = [noise_room1, noise_room2]\n\n        # environment parameters\n        self.action_displacement = 0.1\n        self.wall_eps = 0.05\n\n        # base reward position\n        self.base_reward_pos = np.array([0.8, 0.8])\n\n        # rendering info\n        self.set_clipping_area((0, 2, 0, 1))\n        self.set_refresh_interval(100)  # in milliseconds\n        self.renderer_type = 'opengl'\n\n        # reset\n        self.reset()\n\n    def reset(self):\n        self.current_room = self.rng.integers(2)\n        if self.current_room == 0:\n            self.state = np.array([0.1, 0.1])\n        else:\n            self.state = np.array([1.1, 0.1])\n        return self.state.copy()\n\n    def _reward_fn(self, state):\n        # max reward at (x, y) = reward_pos\n        reward_pos = self.base_reward_pos\n        if self.current_room == 1:\n            reward_pos = reward_pos + np.array([1.0, 0.0])\n        xr, yr = reward_pos\n\n        dist = np.sqrt((state[0]-xr)**2.0 + (state[1]-yr)**2.0)\n        reward = max(0.0,  1.0 - dist/0.1)\n        return reward\n\n    def _clip_to_room(self, state):\n        state[1] = max(0.0, state[1])\n        state[1] = min(1.0, state[1])\n        if self.current_room == 0:\n            state[0] = max(0.0, state[0])\n            state[0] = min(1.0-self.wall_eps, state[0])\n        else:\n            state[0] = max(1.0+self.wall_eps, state[0])\n            state[0] = min(2.0, state[0])\n        return state\n\n    def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n        return self.state.copy(), reward, done, info\n\n    def sample(self, state, action):\n        delta = self.action_displacement\n        if action == 0:\n            displacement = np.array([delta, 0.0])\n        elif action == 1:\n            displacement = np.array([-delta, 0.0])\n        elif action == 2:\n            displacement = np.array([0.0, delta])\n        elif action == 3:\n            displacement = np.array([0.0, -delta])\n        else:\n            raise ValueError(\"Invalid action\")\n\n        next_state = state + displacement \\\n            + self.room_noises[self.current_room] * self.rng.normal(size=2)\n\n        # clip to room\n        next_state = self._clip_to_room(next_state)\n\n        reward = self._reward_fn(state)\n        done = False\n        info = {}\n\n        return next_state, reward, done, info\n\n    #\n    # Code for rendering\n    #\n\n    def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # wall\n        eps = self.wall_eps\n        shape = GeometricPrimitive(\"POLYGON\")\n        shape.set_color((0.25, 0.25, 0.25))\n        shape.add_vertex((1-eps, 0))\n        shape.add_vertex((1-eps, 1))\n        shape.add_vertex((1+eps, 1))\n        shape.add_vertex((1+eps, 0))\n        bg.add_shape(shape)\n\n        # rewards\n        for (x, y) in [self.base_reward_pos, self.base_reward_pos+np.array([1.0, 0.0])]:\n            reward = circle_shape((x, y), 0.1, n_points=50)\n            reward.type = \"POLYGON\"\n            reward.set_color((0.0, 0.5, 0.0))\n            bg.add_shape(reward)\n\n        return bg\n\n    def get_scene(self, state):\n        \"\"\"\n        Return scene (list of shapes) representing a given state\n        \"\"\"\n        x, y = state\n        scene = Scene()\n        agent = circle_shape((x, y), 0.02, n_points=5)\n        agent.type = \"POLYGON\"\n        agent.set_color((0.75, 0.0, 0.5))\n        scene.add_shape(agent)\n        return scene",
  "def __init__(self,\n                 noise_room1=0.01,\n                 noise_room2=0.01):\n        Model.__init__(self)\n        RenderInterface2D.__init__(self)\n\n        self.observation_space = spaces.Box(\n            low=np.array([0.0, 0.0]),\n            high=np.array([2.0, 1.0]),\n            )\n        self.action_space = spaces.Discrete(4)\n        self.reward_range = (0.0, 1.0)\n\n        self.room_noises = [noise_room1, noise_room2]\n\n        # environment parameters\n        self.action_displacement = 0.1\n        self.wall_eps = 0.05\n\n        # base reward position\n        self.base_reward_pos = np.array([0.8, 0.8])\n\n        # rendering info\n        self.set_clipping_area((0, 2, 0, 1))\n        self.set_refresh_interval(100)  # in milliseconds\n        self.renderer_type = 'opengl'\n\n        # reset\n        self.reset()",
  "def reset(self):\n        self.current_room = self.rng.integers(2)\n        if self.current_room == 0:\n            self.state = np.array([0.1, 0.1])\n        else:\n            self.state = np.array([1.1, 0.1])\n        return self.state.copy()",
  "def _reward_fn(self, state):\n        # max reward at (x, y) = reward_pos\n        reward_pos = self.base_reward_pos\n        if self.current_room == 1:\n            reward_pos = reward_pos + np.array([1.0, 0.0])\n        xr, yr = reward_pos\n\n        dist = np.sqrt((state[0]-xr)**2.0 + (state[1]-yr)**2.0)\n        reward = max(0.0,  1.0 - dist/0.1)\n        return reward",
  "def _clip_to_room(self, state):\n        state[1] = max(0.0, state[1])\n        state[1] = min(1.0, state[1])\n        if self.current_room == 0:\n            state[0] = max(0.0, state[0])\n            state[0] = min(1.0-self.wall_eps, state[0])\n        else:\n            state[0] = max(1.0+self.wall_eps, state[0])\n            state[0] = min(2.0, state[0])\n        return state",
  "def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n        return self.state.copy(), reward, done, info",
  "def sample(self, state, action):\n        delta = self.action_displacement\n        if action == 0:\n            displacement = np.array([delta, 0.0])\n        elif action == 1:\n            displacement = np.array([-delta, 0.0])\n        elif action == 2:\n            displacement = np.array([0.0, delta])\n        elif action == 3:\n            displacement = np.array([0.0, -delta])\n        else:\n            raise ValueError(\"Invalid action\")\n\n        next_state = state + displacement \\\n            + self.room_noises[self.current_room] * self.rng.normal(size=2)\n\n        # clip to room\n        next_state = self._clip_to_room(next_state)\n\n        reward = self._reward_fn(state)\n        done = False\n        info = {}\n\n        return next_state, reward, done, info",
  "def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # wall\n        eps = self.wall_eps\n        shape = GeometricPrimitive(\"POLYGON\")\n        shape.set_color((0.25, 0.25, 0.25))\n        shape.add_vertex((1-eps, 0))\n        shape.add_vertex((1-eps, 1))\n        shape.add_vertex((1+eps, 1))\n        shape.add_vertex((1+eps, 0))\n        bg.add_shape(shape)\n\n        # rewards\n        for (x, y) in [self.base_reward_pos, self.base_reward_pos+np.array([1.0, 0.0])]:\n            reward = circle_shape((x, y), 0.1, n_points=50)\n            reward.type = \"POLYGON\"\n            reward.set_color((0.0, 0.5, 0.0))\n            bg.add_shape(reward)\n\n        return bg",
  "def get_scene(self, state):\n        \"\"\"\n        Return scene (list of shapes) representing a given state\n        \"\"\"\n        x, y = state\n        scene = Scene()\n        agent = circle_shape((x, y), 0.02, n_points=5)\n        agent.type = \"POLYGON\"\n        agent.set_color((0.75, 0.0, 0.5))\n        scene.add_shape(agent)\n        return scene",
  "class Model(gym.Env):\n    \"\"\"\n    Base class for an environment model.\n\n    Attributes\n    ----------\n    name : string\n        environment identifier\n    observation_space : rlberry.spaces.Space\n        observation space\n    action_space : rlberry.spaces.Space\n        action space\n    reward_range : tuple\n        tuple (r_min, r_max) containing the minimum and the maximum reward\n    rng : numpy.random._generator.Generator\n        random number generator provided by rlberry.seeding\n\n    Methods\n    -------\n    reseed()\n        get new random number generator\n    reset()\n        puts the environment in a default state and returns this state\n    step(action)\n        returns the outcome of an action\n    sample(state, action)\n        returns a transition sampled from taking an action in a given state\n    is_online()\n        returns true if reset() and step() methods are implemented\n    is_generative()\n        returns true if sample() method is implemented\n    \"\"\"\n\n    name = \"\"\n\n    def __init__(self):\n        self.observation_space = None\n        self.action_space = None\n        self.reward_range: tuple = (-np.inf, np.inf)\n        # random number generator\n        self.rng = seeding.get_rng()\n\n    def reseed(self):\n        \"\"\"\n        Get new random number generator for the model.\n        \"\"\"\n        self.rng = seeding.get_rng()\n        self.observation_space.rng = self.rng\n        self.action_space.rng = self.rng\n\n    def sample(self, state, action):\n        \"\"\"\n        Execute a step from a state-action pair.\n\n        Parameters\n        ----------\n        state : object\n            state from which to sample\n        action : object\n            action to take in the environment\n\n        Returns\n        -------\n        observation : object\n        reward : float\n        done  : bool\n        info  : dict\n        \"\"\"\n        raise NotImplementedError(\"sample() method not implemented.\")\n\n    def is_online(self):\n        logger.warning(\"Checking if Model is\\\nonline calls reset() and step() methods.\")\n        try:\n            self.reset()\n            self.step(self.action_space.sample())\n            return True\n        except Exception as ex:\n            if isinstance(ex, NotImplementedError):\n                return False\n            else:\n                raise\n\n    def is_generative(self):\n        logger.warning(\"Checking if Model is \\\ngenerative calls sample() method.\")\n        try:\n            self.sample(self.observation_space.sample(),\n                        self.action_space.sample())\n            return True\n        except Exception as ex:\n            if isinstance(ex, NotImplementedError):\n                return False\n            else:\n                raise\n\n    @property\n    def unwrapped(self):\n        return self",
  "def __init__(self):\n        self.observation_space = None\n        self.action_space = None\n        self.reward_range: tuple = (-np.inf, np.inf)\n        # random number generator\n        self.rng = seeding.get_rng()",
  "def reseed(self):\n        \"\"\"\n        Get new random number generator for the model.\n        \"\"\"\n        self.rng = seeding.get_rng()\n        self.observation_space.rng = self.rng\n        self.action_space.rng = self.rng",
  "def sample(self, state, action):\n        \"\"\"\n        Execute a step from a state-action pair.\n\n        Parameters\n        ----------\n        state : object\n            state from which to sample\n        action : object\n            action to take in the environment\n\n        Returns\n        -------\n        observation : object\n        reward : float\n        done  : bool\n        info  : dict\n        \"\"\"\n        raise NotImplementedError(\"sample() method not implemented.\")",
  "def is_online(self):\n        logger.warning(\"Checking if Model is\\\nonline calls reset() and step() methods.\")\n        try:\n            self.reset()\n            self.step(self.action_space.sample())\n            return True\n        except Exception as ex:\n            if isinstance(ex, NotImplementedError):\n                return False\n            else:\n                raise",
  "def is_generative(self):\n        logger.warning(\"Checking if Model is \\\ngenerative calls sample() method.\")\n        try:\n            self.sample(self.observation_space.sample(),\n                        self.action_space.sample())\n            return True\n        except Exception as ex:\n            if isinstance(ex, NotImplementedError):\n                return False\n            else:\n                raise",
  "def unwrapped(self):\n        return self",
  "class MountainCar(RenderInterface2D, Model):\n    \"\"\"\n    The agent (a car) is started at the bottom of a valley. For any given\n    state the agent may choose to accelerate to the left, right or cease\n    any acceleration.\n\n    Notes\n    -----\n    Source:\n        The environment appeared first in Andrew Moore's PhD Thesis (1990).\n\n    Observation:\n        Type: Box(2)\n        Num    Observation               Min            Max\n        0      Car Position              -1.2           0.6\n        1      Car Velocity              -0.07          0.07\n\n    Actions:\n        Type: Discrete(3)\n        Num    Action\n        0      Accelerate to the Left\n        1      Don't accelerate\n        2      Accelerate to the Right\n\n        Note: This does not affect the amount of velocity affected by the\n        gravitational pull acting on the car.\n\n    Reward:\n        Reward of 1 is awarded if the agent reached the flag (position = 0.5)\n        on top of the mountain.\n        Reward of 0 is awarded if the position of the agent is less than 0.5.\n\n    Starting State:\n        The position of the car is assigned a uniform random value in\n        [-0.6 , -0.4].\n        The starting velocity of the car is always assigned to 0.\n\n    Episode Termination:\n        The car position is more than 0.5\n    \"\"\"\n    name = \"MountainCar\"\n\n    def __init__(self, goal_velocity=0):\n        # init base classes\n        Model.__init__(self)\n        RenderInterface2D.__init__(self)\n\n        self.min_position = -1.2\n        self.max_position = 0.6\n        self.max_speed = 0.07\n        self.goal_position = 0.5\n        self.goal_velocity = goal_velocity\n\n        self.force = 0.001\n        self.gravity = 0.0025\n\n        self.low = np.array([self.min_position, -self.max_speed])\n        self.high = np.array([self.max_position, self.max_speed])\n\n        self.action_space = spaces.Discrete(3)\n        self.observation_space = spaces.Box(self.low, self.high)\n\n        self.reward_range = (0.0, 1.0)\n\n        # rendering info\n        self.set_clipping_area((-1.2, 0.6, -0.2, 1.1))\n        self.set_refresh_interval(10)  # in milliseconds\n\n        # initial reset\n        self.reset()\n\n    def step(self, action):\n        assert self.action_space.contains(action), \\\n            \"%r (%s) invalid\" % (action, type(action))\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(np.array(self.state))\n\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state.copy()\n\n        return next_state, reward, done, info\n\n    def reset(self):\n        self.state = np.array([self.rng.uniform(low=-0.6, high=-0.4), 0])\n        return self.state.copy()\n\n    def sample(self, state, action):\n        if not isinstance(state, np.ndarray):\n            state = np.array(state)\n        assert self.observation_space.contains(state), \\\n            \"Invalid state as argument of reset().\"\n        assert self.action_space.contains(action), \\\n            \"%r (%s) invalid\" % (action, type(action))\n\n        position = state[0]\n        velocity = state[1]\n        velocity += (action - 1) * self.force \\\n            + math.cos(3 * position) * (-self.gravity)\n        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n        position += velocity\n        position = np.clip(position, self.min_position, self.max_position)\n        if (position == self.min_position and velocity < 0):\n            velocity = 0\n\n        done = bool(position >= self.goal_position and\n                    velocity >= self.goal_velocity)\n        reward = 0.0\n        if done:\n            reward = 1.0\n\n        next_state = np.array([position, velocity])\n        return next_state, reward, done, {}\n\n    @staticmethod\n    def _height(xs):\n        return np.sin(3 * xs) * .45 + .55\n\n    #\n    # Below: code for rendering\n    #\n\n    def get_background(self):\n        bg = Scene()\n        mountain = GeometricPrimitive(\"TRIANGLE_FAN\")\n        flag = GeometricPrimitive(\"TRIANGLES\")\n        mountain.set_color((0.6, 0.3, 0.0))\n        flag.set_color((0.0, 0.5, 0.0))\n\n        # Mountain\n        mountain.add_vertex((-0.3, -1.0))\n        mountain.add_vertex((0.6, -1.0))\n\n        n_points = 50\n        obs_range = self.observation_space.high[0] \\\n            - self.observation_space.low[0]\n        eps = obs_range / (n_points - 1)\n        for ii in reversed(range(n_points)):\n            x = self.observation_space.low[0] + ii * eps\n            y = self._height(x)\n            mountain.add_vertex((x, y))\n        mountain.add_vertex((-1.2, -1.0))\n\n        # Flag\n        goal_x = self.goal_position\n        goal_y = self._height(goal_x)\n        flag.add_vertex((goal_x, goal_y))\n        flag.add_vertex((goal_x + 0.025, goal_y + 0.075))\n        flag.add_vertex((goal_x - 0.025, goal_y + 0.075))\n\n        bg.add_shape(mountain)\n        bg.add_shape(flag)\n\n        return bg\n\n    def get_scene(self, state):\n        scene = Scene()\n\n        agent = GeometricPrimitive(\"QUADS\")\n        agent.set_color((0.0, 0.0, 0.0))\n        size = 0.025\n        x = state[0]\n        y = self._height(x)\n        agent.add_vertex((x - size, y - size))\n        agent.add_vertex((x + size, y - size))\n        agent.add_vertex((x + size, y + size))\n        agent.add_vertex((x - size, y + size))\n\n        scene.add_shape(agent)\n        return scene",
  "def __init__(self, goal_velocity=0):\n        # init base classes\n        Model.__init__(self)\n        RenderInterface2D.__init__(self)\n\n        self.min_position = -1.2\n        self.max_position = 0.6\n        self.max_speed = 0.07\n        self.goal_position = 0.5\n        self.goal_velocity = goal_velocity\n\n        self.force = 0.001\n        self.gravity = 0.0025\n\n        self.low = np.array([self.min_position, -self.max_speed])\n        self.high = np.array([self.max_position, self.max_speed])\n\n        self.action_space = spaces.Discrete(3)\n        self.observation_space = spaces.Box(self.low, self.high)\n\n        self.reward_range = (0.0, 1.0)\n\n        # rendering info\n        self.set_clipping_area((-1.2, 0.6, -0.2, 1.1))\n        self.set_refresh_interval(10)  # in milliseconds\n\n        # initial reset\n        self.reset()",
  "def step(self, action):\n        assert self.action_space.contains(action), \\\n            \"%r (%s) invalid\" % (action, type(action))\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(np.array(self.state))\n\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state.copy()\n\n        return next_state, reward, done, info",
  "def reset(self):\n        self.state = np.array([self.rng.uniform(low=-0.6, high=-0.4), 0])\n        return self.state.copy()",
  "def sample(self, state, action):\n        if not isinstance(state, np.ndarray):\n            state = np.array(state)\n        assert self.observation_space.contains(state), \\\n            \"Invalid state as argument of reset().\"\n        assert self.action_space.contains(action), \\\n            \"%r (%s) invalid\" % (action, type(action))\n\n        position = state[0]\n        velocity = state[1]\n        velocity += (action - 1) * self.force \\\n            + math.cos(3 * position) * (-self.gravity)\n        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n        position += velocity\n        position = np.clip(position, self.min_position, self.max_position)\n        if (position == self.min_position and velocity < 0):\n            velocity = 0\n\n        done = bool(position >= self.goal_position and\n                    velocity >= self.goal_velocity)\n        reward = 0.0\n        if done:\n            reward = 1.0\n\n        next_state = np.array([position, velocity])\n        return next_state, reward, done, {}",
  "def _height(xs):\n        return np.sin(3 * xs) * .45 + .55",
  "def get_background(self):\n        bg = Scene()\n        mountain = GeometricPrimitive(\"TRIANGLE_FAN\")\n        flag = GeometricPrimitive(\"TRIANGLES\")\n        mountain.set_color((0.6, 0.3, 0.0))\n        flag.set_color((0.0, 0.5, 0.0))\n\n        # Mountain\n        mountain.add_vertex((-0.3, -1.0))\n        mountain.add_vertex((0.6, -1.0))\n\n        n_points = 50\n        obs_range = self.observation_space.high[0] \\\n            - self.observation_space.low[0]\n        eps = obs_range / (n_points - 1)\n        for ii in reversed(range(n_points)):\n            x = self.observation_space.low[0] + ii * eps\n            y = self._height(x)\n            mountain.add_vertex((x, y))\n        mountain.add_vertex((-1.2, -1.0))\n\n        # Flag\n        goal_x = self.goal_position\n        goal_y = self._height(goal_x)\n        flag.add_vertex((goal_x, goal_y))\n        flag.add_vertex((goal_x + 0.025, goal_y + 0.075))\n        flag.add_vertex((goal_x - 0.025, goal_y + 0.075))\n\n        bg.add_shape(mountain)\n        bg.add_shape(flag)\n\n        return bg",
  "def get_scene(self, state):\n        scene = Scene()\n\n        agent = GeometricPrimitive(\"QUADS\")\n        agent.set_color((0.0, 0.0, 0.0))\n        size = 0.025\n        x = state[0]\n        y = self._height(x)\n        agent.add_vertex((x - size, y - size))\n        agent.add_vertex((x + size, y - size))\n        agent.add_vertex((x + size, y + size))\n        agent.add_vertex((x - size, y + size))\n\n        scene.add_shape(agent)\n        return scene",
  "class Pendulum(RenderInterface2D, Model):\n    \"\"\"\n    The inverted pendulum swingup problem is a classic problem\n    in the control literature. In this version of the problem,\n    the pendulum starts in a random position, and the goal\n    is to swing it up so it stays upright.\n    \"\"\"\n    name = \"Pendulum\"\n\n    def __init__(self):\n        # init base classes\n        Model.__init__(self)\n        RenderInterface2D.__init__(self)\n\n        # environment parameters\n        self.max_speed = 8.\n        self.max_torque = 2.\n        self.dt = 0.5\n        self.gravity = 10.\n        self.mass = 1.\n        self.length = 1.\n\n        # rendering info\n        self.set_clipping_area((-2.2, 2.2, -2.2, 2.2))\n        self.set_refresh_interval(10)\n\n        # observation and action spaces\n        high = np.array([1., 1., self.max_speed], dtype=np.float32)\n        low = -high\n        self.action_space = spaces.Box(low=-self.max_torque,\n                                       high=self.max_torque,\n                                       shape=(1,),\n                                       dtype=np.float32)\n        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n\n        # initialize\n        self.reset()\n\n    def reset(self):\n        high = np.array([np.pi, 1])\n        low = -high\n        self.state = self.rng.uniform(low=low, high=high)\n        self.last_action = None\n        return self._get_ob()\n\n    def step(self, action):\n        assert self.action_space.contains(action), \\\n                \"%r (%s) invalid\" % (action, type(action))\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(np.array(self.state))\n\n        theta, thetadot = self.state\n        gravity = self.gravity\n        mass = self.mass\n        length = self.length\n        dt = self.dt\n\n        action = np.clip(action, -self.max_torque, self.max_torque)[0]\n        self.last_action = action # for rendering\n        costs = angle_normalize(theta) ** 2 + .1 * thetadot ** 2 + .001 * (action ** 2)\n\n        # compute the next state after action\n        newthetadot = thetadot + (-3 * gravity / (2 * length) * np.sin(theta + np.pi) +\n                                  3. / (mass * length ** 2) * action) * dt\n        newtheta = theta + newthetadot * dt\n        newthetadot = np.clip(newthetadot, -self.max_speed, self.max_speed)\n\n        self.state = np.array([newtheta, newthetadot])\n        return self._get_ob(), -costs, False, {}\n\n    def _get_ob(self):\n        theta, thetadot = self.state\n        return np.array([np.cos(theta), np.sin(theta), thetadot])\n\n    #\n    # Below code for rendering\n    #\n\n    def get_background(self):\n        bg = Scene()\n        return bg\n\n    def get_scene(self, state):\n        scene = Scene()\n\n        p0 = (0.0, 0.0)\n        p1 = (self.length * np.sin(state[0]),\n              -self.length * np.cos(state[0]))\n\n        link = bar_shape(p0, p1, 0.1)\n        link.set_color((255/255, 105/255, 30/255))\n\n        joint = circle_shape(p0, 0.075)\n        joint.set_color((255/255, 215/255, 0/255))\n\n        scene.add_shape(link)\n        scene.add_shape(joint)\n\n        return scene",
  "def angle_normalize(x):\n    return (((x + np.pi) % (2 * np.pi)) - np.pi)",
  "def __init__(self):\n        # init base classes\n        Model.__init__(self)\n        RenderInterface2D.__init__(self)\n\n        # environment parameters\n        self.max_speed = 8.\n        self.max_torque = 2.\n        self.dt = 0.5\n        self.gravity = 10.\n        self.mass = 1.\n        self.length = 1.\n\n        # rendering info\n        self.set_clipping_area((-2.2, 2.2, -2.2, 2.2))\n        self.set_refresh_interval(10)\n\n        # observation and action spaces\n        high = np.array([1., 1., self.max_speed], dtype=np.float32)\n        low = -high\n        self.action_space = spaces.Box(low=-self.max_torque,\n                                       high=self.max_torque,\n                                       shape=(1,),\n                                       dtype=np.float32)\n        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n\n        # initialize\n        self.reset()",
  "def reset(self):\n        high = np.array([np.pi, 1])\n        low = -high\n        self.state = self.rng.uniform(low=low, high=high)\n        self.last_action = None\n        return self._get_ob()",
  "def step(self, action):\n        assert self.action_space.contains(action), \\\n                \"%r (%s) invalid\" % (action, type(action))\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(np.array(self.state))\n\n        theta, thetadot = self.state\n        gravity = self.gravity\n        mass = self.mass\n        length = self.length\n        dt = self.dt\n\n        action = np.clip(action, -self.max_torque, self.max_torque)[0]\n        self.last_action = action # for rendering\n        costs = angle_normalize(theta) ** 2 + .1 * thetadot ** 2 + .001 * (action ** 2)\n\n        # compute the next state after action\n        newthetadot = thetadot + (-3 * gravity / (2 * length) * np.sin(theta + np.pi) +\n                                  3. / (mass * length ** 2) * action) * dt\n        newtheta = theta + newthetadot * dt\n        newthetadot = np.clip(newthetadot, -self.max_speed, self.max_speed)\n\n        self.state = np.array([newtheta, newthetadot])\n        return self._get_ob(), -costs, False, {}",
  "def _get_ob(self):\n        theta, thetadot = self.state\n        return np.array([np.cos(theta), np.sin(theta), thetadot])",
  "def get_background(self):\n        bg = Scene()\n        return bg",
  "def get_scene(self, state):\n        scene = Scene()\n\n        p0 = (0.0, 0.0)\n        p1 = (self.length * np.sin(state[0]),\n              -self.length * np.cos(state[0]))\n\n        link = bar_shape(p0, p1, 0.1)\n        link.set_color((255/255, 105/255, 30/255))\n\n        joint = circle_shape(p0, 0.075)\n        joint.set_color((255/255, 215/255, 0/255))\n\n        scene.add_shape(link)\n        scene.add_shape(joint)\n\n        return scene",
  "class Acrobot(RenderInterface2D, Model):\n    \"\"\"\n    Acrobot is a 2-link pendulum with only the second joint actuated.\n    Initially, both links point downwards. The goal is to swing the\n    end-effector at a height at least the length of one link above the base.\n    Both links can swing freely and can pass by each other, i.e., they don't\n    collide when they have the same angle.\n\n    Notes\n    -----\n    State:\n        The state consists of the sin() and cos() of the two rotational joint\n        angles and the joint angular velocities:\n        [cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2].\n        For the first link, an angle of 0 corresponds to the link pointing\n        downwards.\n        The angle of the second link is relative to the angle of the first link.\n        An angle of 0 corresponds to having the same angle between the two links.\n        A state of [1, 0, 1, 0, ..., ...] means that both links point downwards.\n\n    Actions:\n        The action is either applying +1, 0 or -1 torque on the joint between\n        the two pendulum links.\n    .. note::\n        The dynamics equations were missing some terms in the NIPS paper which\n        are present in the book. R. Sutton confirmed in personal correspondence\n        that the experimental results shown in the paper and the book were\n        generated with the equations shown in the book.\n        However, there is the option to run the domain with the paper equations\n        by setting book_or_nips = 'nips'\n\n    Reference:\n    .. seealso::\n        R. Sutton: Generalization in Reinforcement Learning:\n        Successful Examples Using Sparse Coarse Coding (NIPS 1996)\n    .. seealso::\n        R. Sutton and A. G. Barto:\n        Reinforcement learning: An introduction.\n        Cambridge: MIT press, 1998.\n    .. warning::\n        This version of the domain uses the Runge-Kutta method for integrating\n        the system dynamics and is more realistic, but also considerably harder\n        than the original version which employs Euler integration,\n        see the AcrobotLegacy class.\n    \"\"\"\n    name = \"Acrobot\"\n\n    dt = .2\n\n    LINK_LENGTH_1 = 1.  # [m]\n    LINK_LENGTH_2 = 1.  # [m]\n    LINK_MASS_1 = 1.  #: [kg] mass of link 1\n    LINK_MASS_2 = 1.  #: [kg] mass of link 2\n    LINK_COM_POS_1 = 0.5  #: [m] position of the center of mass of link 1\n    LINK_COM_POS_2 = 0.5  #: [m] position of the center of mass of link 2\n    LINK_MOI = 1.  #: moments of inertia for both links\n\n    MAX_VEL_1 = 4 * np.pi\n    MAX_VEL_2 = 9 * np.pi\n\n    AVAIL_TORQUE = [-1., 0., +1]\n\n    torque_noise_max = 0.\n\n    #: use dynamics equations from the nips paper or the book\n    book_or_nips = \"book\"\n    action_arrow = None\n    domain_fig = None\n    actions_num = 3\n\n    def __init__(self):\n        # init base classes\n        Model.__init__(self)\n        RenderInterface2D.__init__(self)\n        self.reward_range = (-1.0, 0.0)\n\n        # rendering info\n        bound = self.LINK_LENGTH_1 + self.LINK_LENGTH_2 + 0.2\n        # (left, right, bottom, top)\n        self.set_clipping_area((-bound, bound, -bound, bound))\n        self.set_refresh_interval(10)  # in milliseconds\n\n        # observation and action spaces\n        high = np.array([1.0, 1.0, 1.0, 1.0, self.MAX_VEL_1, self.MAX_VEL_2])\n        low = -high\n        self.observation_space = spaces.Box(low=low, high=high)\n        self.action_space = spaces.Discrete(3)\n\n        # initialize\n        self.state = None\n        self.reset()\n\n    def reset(self):\n        self.state = self.rng.uniform(low=-0.1, high=0.1, size=(4,))\n        return self._get_ob()\n\n    def step(self, action):\n        assert self.action_space.contains(action), \\\n                \"%r (%s) invalid\" % (action, type(action))\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(np.array(self.state))\n\n        s = self.state\n        torque = self.AVAIL_TORQUE[action]\n\n        # Add noise to the force action\n        if self.torque_noise_max > 0:\n            torque += self.rng.uniform(-self.torque_noise_max,\n                                       self.torque_noise_max)\n\n        # Now, augment the state with our force action so it can be passed to\n        # _dsdt\n        s_augmented = np.append(s, torque)\n\n        ns = rk4(self._dsdt, s_augmented, [0, self.dt])\n        # only care about final timestep of integration returned by integrator\n        ns = ns[-1]\n        ns = ns[:4]  # omit action\n        # ODEINT IS TOO SLOW!\n        # ns_continuous = integrate.odeint(self._dsdt, self.s_continuous,\n        # [0, self.dt])\n        # self.s_continuous = ns_continuous[-1] # We only care about the state\n        # at the ''final timestep'', self.dt\n\n        ns[0] = wrap(ns[0], -np.pi, np.pi)\n        ns[1] = wrap(ns[1], -np.pi, np.pi)\n        ns[2] = bound(ns[2], -self.MAX_VEL_1, self.MAX_VEL_1)\n        ns[3] = bound(ns[3], -self.MAX_VEL_2, self.MAX_VEL_2)\n        self.state = ns\n        terminal = self._terminal()\n        reward = -1. if not terminal else 0.\n        return self._get_ob(), reward, terminal, {}\n\n    def _get_ob(self):\n        s = self.state\n        return np.array([np.cos(s[0]), np.sin(s[0]), np.cos(s[1]),\n                        np.sin(s[1]), s[2], s[3]])\n\n    def _terminal(self):\n        s = self.state\n        return bool(-np.cos(s[0]) - np.cos(s[1] + s[0]) > 1.)\n\n    def _dsdt(self, s_augmented, t):\n        m1 = self.LINK_MASS_1\n        m2 = self.LINK_MASS_2\n        l1 = self.LINK_LENGTH_1\n        lc1 = self.LINK_COM_POS_1\n        lc2 = self.LINK_COM_POS_2\n        I1 = self.LINK_MOI\n        I2 = self.LINK_MOI\n        g = 9.8\n        a = s_augmented[-1]\n        s = s_augmented[:-1]\n        theta1 = s[0]\n        theta2 = s[1]\n        dtheta1 = s[2]\n        dtheta2 = s[3]\n        d1 = m1 * lc1 ** 2 + m2 * \\\n            (l1 ** 2 + lc2 ** 2 + 2 * l1 * lc2 * np.cos(theta2)) + I1 + I2\n        d2 = m2 * (lc2 ** 2 + l1 * lc2 * np.cos(theta2)) + I2\n        phi2 = m2 * lc2 * g * np.cos(theta1 + theta2 - np.pi / 2.)\n        phi1 = - m2 * l1 * lc2 * dtheta2 ** 2 * np.sin(theta2) \\\n               - 2 * m2 * l1 * lc2 * dtheta2 * dtheta1 * np.sin(theta2)  \\\n            + (m1 * lc1 + m2 * l1) * g * np.cos(theta1 - np.pi / 2) + phi2\n        if self.book_or_nips == \"nips\":\n            # the following line is consistent with the description in the\n            # paper\n            ddtheta2 = (a + d2 / d1 * phi1 - phi2) / \\\n                (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)\n        else:\n            # the following line is consistent with the java implementation\n            # and the book\n            ddtheta2 = (a + d2 / d1 * phi1 - m2 * l1 * lc2 * dtheta1 ** 2 *\n                        np.sin(theta2) - phi2) \\\n                / (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)\n        ddtheta1 = -(d2 * ddtheta2 + phi1) / d1\n        return (dtheta1, dtheta2, ddtheta1, ddtheta2, 0.)\n\n    #\n    # Below: code for rendering\n    #\n\n    def get_background(self):\n        bg = Scene()\n        return bg\n\n    def get_scene(self, state):\n        scene = Scene()\n\n        p0 = (0.0, 0.0)\n\n        p1 = (self.LINK_LENGTH_1 * np.sin(state[0]),\n              -self.LINK_LENGTH_1 * np.cos(state[0]))\n        p2 = (p1[0] + self.LINK_LENGTH_2 * np.sin(state[0] + state[1]),\n              p1[1] - self.LINK_LENGTH_2 * np.cos(state[0] + state[1]))\n\n        link1 = bar_shape(p0, p1, 0.1)\n        link1.set_color((255/255, 140/255, 0/255))\n\n        link2 = bar_shape(p1, p2, 0.1)\n        link2.set_color((210/255, 105/255, 30/255))\n\n        joint1 = circle_shape(p0, 0.075)\n        joint1.set_color((255/255, 215/255, 0/255))\n\n        joint2 = circle_shape(p1, 0.075)\n        joint2.set_color((255/255, 215/255, 0/255))\n\n        goal_line = GeometricPrimitive(\"LINES\")\n        goal_line.add_vertex((-5, 1))\n        goal_line.add_vertex((5, 1))\n\n        scene.add_shape(link1)\n        scene.add_shape(link2)\n        scene.add_shape(joint1)\n        scene.add_shape(joint2)\n        scene.add_shape(goal_line)\n\n        return scene",
  "def wrap(x, m, M):\n    \"\"\"Wraps ``x`` so m <= x <= M; but unlike ``bound()`` which\n    truncates, ``wrap()`` wraps x around the coordinate system defined\n    by m, M.\n    For example, m = -180, M = 180 (degrees), x = 360 --> returns 0.\n\n    Parameters\n    ----------\n        x: a scalar\n        m:\n            minimum possible value in range\n        M:\n            maximum possible value in range\n\n    Returns\n    -------\n        x:\n            a scalar, wrapped\n    \"\"\"\n    diff = M - m\n    while x > M:\n        x = x - diff\n    while x < m:\n        x = x + diff\n    return x",
  "def bound(x, m, M=None):\n    \"\"\"Either have m as scalar, so bound(x,m,M) which returns m <= x <= M *OR*\n    have m as length 2 vector, bound(x,m, <IGNORED>) returns m[0] <= x <= m[1].\n\n    Parameters\n    ----------\n    x:\n        scalar\n\n    Returns\n    -------\n    x:\n        scalar, bound between min (m) and Max (M)\n    \"\"\"\n    if M is None:\n        M = m[1]\n        m = m[0]\n    # bound x between min (m) and Max (M)\n    return min(max(x, m), M)",
  "def rk4(derivs, y0, t, *args, **kwargs):\n    \"\"\"\n    Integrate 1D or ND system of ODEs using 4-th order Runge-Kutta.\n    This is a toy implementation which may be useful if you find\n    yourself stranded on a system w/o scipy.  Otherwise use\n    :func:`scipy.integrate`.\n\n    Parameters:\n    -----------\n    derivs:\n        the derivative of the system and has the signature\n        ``dy = derivs(yi, ti)``\n    y0:\n        initial state vector\n    t:\n        sample times\n    args:\n        additional arguments passed to the derivative function\n    kwargs:\n        additional keyword arguments passed to the derivative function\n\n    Returns\n    -------\n    yout:\n        Runge-Kutta approximation of the ODE\n\n    Examples\n    --------\n    Example 1::\n        ## 2D system\n        def derivs6(x,t):\n            d1 =  x[0] + 2*x[1]\n            d2 =  -3*x[0] + 4*x[1]\n            return (d1, d2)\n        dt = 0.0005\n        t = arange(0.0, 2.0, dt)\n        y0 = (1,2)\n        yout = rk4(derivs6, y0, t)\n\n    Example 2::\n        ## 1D system\n        alpha = 2\n        def derivs(x,t):\n            return -alpha*x + exp(-t)\n        y0 = 1\n        yout = rk4(derivs, y0, t)\n\n    If you have access to scipy, you should probably be using the\n    scipy.integrate tools rather than this function.\n    \"\"\"\n\n    try:\n        Ny = len(y0)\n    except TypeError:\n        yout = np.zeros((len(t),), np.float_)\n    else:\n        yout = np.zeros((len(t), Ny), np.float_)\n\n    yout[0] = y0\n\n    for i in np.arange(len(t) - 1):\n\n        thist = t[i]\n        dt = t[i + 1] - thist\n        dt2 = dt / 2.0\n        y0 = yout[i]\n\n        k1 = np.asarray(derivs(y0, thist, *args, **kwargs))\n        k2 = np.asarray(derivs(y0 + dt2 * k1, thist + dt2, *args, **kwargs))\n        k3 = np.asarray(derivs(y0 + dt2 * k2, thist + dt2, *args, **kwargs))\n        k4 = np.asarray(derivs(y0 + dt * k3, thist + dt, *args, **kwargs))\n        yout[i + 1] = y0 + dt / 6.0 * (k1 + 2 * k2 + 2 * k3 + k4)\n    return yout",
  "def __init__(self):\n        # init base classes\n        Model.__init__(self)\n        RenderInterface2D.__init__(self)\n        self.reward_range = (-1.0, 0.0)\n\n        # rendering info\n        bound = self.LINK_LENGTH_1 + self.LINK_LENGTH_2 + 0.2\n        # (left, right, bottom, top)\n        self.set_clipping_area((-bound, bound, -bound, bound))\n        self.set_refresh_interval(10)  # in milliseconds\n\n        # observation and action spaces\n        high = np.array([1.0, 1.0, 1.0, 1.0, self.MAX_VEL_1, self.MAX_VEL_2])\n        low = -high\n        self.observation_space = spaces.Box(low=low, high=high)\n        self.action_space = spaces.Discrete(3)\n\n        # initialize\n        self.state = None\n        self.reset()",
  "def reset(self):\n        self.state = self.rng.uniform(low=-0.1, high=0.1, size=(4,))\n        return self._get_ob()",
  "def step(self, action):\n        assert self.action_space.contains(action), \\\n                \"%r (%s) invalid\" % (action, type(action))\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(np.array(self.state))\n\n        s = self.state\n        torque = self.AVAIL_TORQUE[action]\n\n        # Add noise to the force action\n        if self.torque_noise_max > 0:\n            torque += self.rng.uniform(-self.torque_noise_max,\n                                       self.torque_noise_max)\n\n        # Now, augment the state with our force action so it can be passed to\n        # _dsdt\n        s_augmented = np.append(s, torque)\n\n        ns = rk4(self._dsdt, s_augmented, [0, self.dt])\n        # only care about final timestep of integration returned by integrator\n        ns = ns[-1]\n        ns = ns[:4]  # omit action\n        # ODEINT IS TOO SLOW!\n        # ns_continuous = integrate.odeint(self._dsdt, self.s_continuous,\n        # [0, self.dt])\n        # self.s_continuous = ns_continuous[-1] # We only care about the state\n        # at the ''final timestep'', self.dt\n\n        ns[0] = wrap(ns[0], -np.pi, np.pi)\n        ns[1] = wrap(ns[1], -np.pi, np.pi)\n        ns[2] = bound(ns[2], -self.MAX_VEL_1, self.MAX_VEL_1)\n        ns[3] = bound(ns[3], -self.MAX_VEL_2, self.MAX_VEL_2)\n        self.state = ns\n        terminal = self._terminal()\n        reward = -1. if not terminal else 0.\n        return self._get_ob(), reward, terminal, {}",
  "def _get_ob(self):\n        s = self.state\n        return np.array([np.cos(s[0]), np.sin(s[0]), np.cos(s[1]),\n                        np.sin(s[1]), s[2], s[3]])",
  "def _terminal(self):\n        s = self.state\n        return bool(-np.cos(s[0]) - np.cos(s[1] + s[0]) > 1.)",
  "def _dsdt(self, s_augmented, t):\n        m1 = self.LINK_MASS_1\n        m2 = self.LINK_MASS_2\n        l1 = self.LINK_LENGTH_1\n        lc1 = self.LINK_COM_POS_1\n        lc2 = self.LINK_COM_POS_2\n        I1 = self.LINK_MOI\n        I2 = self.LINK_MOI\n        g = 9.8\n        a = s_augmented[-1]\n        s = s_augmented[:-1]\n        theta1 = s[0]\n        theta2 = s[1]\n        dtheta1 = s[2]\n        dtheta2 = s[3]\n        d1 = m1 * lc1 ** 2 + m2 * \\\n            (l1 ** 2 + lc2 ** 2 + 2 * l1 * lc2 * np.cos(theta2)) + I1 + I2\n        d2 = m2 * (lc2 ** 2 + l1 * lc2 * np.cos(theta2)) + I2\n        phi2 = m2 * lc2 * g * np.cos(theta1 + theta2 - np.pi / 2.)\n        phi1 = - m2 * l1 * lc2 * dtheta2 ** 2 * np.sin(theta2) \\\n               - 2 * m2 * l1 * lc2 * dtheta2 * dtheta1 * np.sin(theta2)  \\\n            + (m1 * lc1 + m2 * l1) * g * np.cos(theta1 - np.pi / 2) + phi2\n        if self.book_or_nips == \"nips\":\n            # the following line is consistent with the description in the\n            # paper\n            ddtheta2 = (a + d2 / d1 * phi1 - phi2) / \\\n                (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)\n        else:\n            # the following line is consistent with the java implementation\n            # and the book\n            ddtheta2 = (a + d2 / d1 * phi1 - m2 * l1 * lc2 * dtheta1 ** 2 *\n                        np.sin(theta2) - phi2) \\\n                / (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)\n        ddtheta1 = -(d2 * ddtheta2 + phi1) / d1\n        return (dtheta1, dtheta2, ddtheta1, ddtheta2, 0.)",
  "def get_background(self):\n        bg = Scene()\n        return bg",
  "def get_scene(self, state):\n        scene = Scene()\n\n        p0 = (0.0, 0.0)\n\n        p1 = (self.LINK_LENGTH_1 * np.sin(state[0]),\n              -self.LINK_LENGTH_1 * np.cos(state[0]))\n        p2 = (p1[0] + self.LINK_LENGTH_2 * np.sin(state[0] + state[1]),\n              p1[1] - self.LINK_LENGTH_2 * np.cos(state[0] + state[1]))\n\n        link1 = bar_shape(p0, p1, 0.1)\n        link1.set_color((255/255, 140/255, 0/255))\n\n        link2 = bar_shape(p1, p2, 0.1)\n        link2.set_color((210/255, 105/255, 30/255))\n\n        joint1 = circle_shape(p0, 0.075)\n        joint1.set_color((255/255, 215/255, 0/255))\n\n        joint2 = circle_shape(p1, 0.075)\n        joint2.set_color((255/255, 215/255, 0/255))\n\n        goal_line = GeometricPrimitive(\"LINES\")\n        goal_line.add_vertex((-5, 1))\n        goal_line.add_vertex((5, 1))\n\n        scene.add_shape(link1)\n        scene.add_shape(link2)\n        scene.add_shape(joint1)\n        scene.add_shape(joint2)\n        scene.add_shape(goal_line)\n\n        return scene",
  "class FiniteMDP(Model):\n    \"\"\"\n    Base class for a finite MDP.\n\n    Terminal states are set to be absorbing, and\n    are determined by the is_terminal() method,\n    which can be overriden (and returns false by default).\n\n    Parameters\n    ----------\n    R : numpy.ndarray\n    P : numpy.ndarray\n    initial_state_distribution : numpy.ndarray or int\n        array of size (S,) containing the initial state distribution\n        or an integer representing the initial/default state\n\n    Attributes\n    ----------\n    R : numpy.ndarray\n        array of shape (S, A) containing the mean rewards, where\n        S = number of states;  A = number of actions.\n    P : numpy.ndarray\n        array of shape (S, A, S) containing the transition probabilities,\n        where P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a).\n    \"\"\"\n\n    def __init__(self, R, P, initial_state_distribution=0):\n        Model.__init__(self)\n        self.initial_state_distribution = initial_state_distribution\n        S, A = R.shape\n\n        self.S = S\n        self.A = A\n\n        self.R = R\n        self.P = P\n\n        self.observation_space = spaces.Discrete(S)\n        self.action_space = spaces.Discrete(A)\n        self.reward_range = (self.R.min(), self.R.max())\n\n        self.state = None\n\n        self._states = np.arange(S)\n        self._actions = np.arange(A)\n\n        self.reset()\n        self._process_terminal_states()\n        self._check()\n\n    def reset(self):\n        \"\"\"\n        Reset the environment to a default state.\n        \"\"\"\n        if isinstance(self.initial_state_distribution, np.ndarray):\n            self.state = self.rng.choice(self._states,\n                                         p=self.initial_state_distribution)\n        else:\n            self.state = self.initial_state_distribution\n        return self.state\n\n    def _process_terminal_states(self):\n        \"\"\"\n        Adapt transition array P so that terminal states\n        are absorbing.\n        \"\"\"\n        for ss in range(self.S):\n            if self.is_terminal(ss):\n                self.P[ss, :, :] = 0.0\n                self.P[ss, :, ss] = 1.0\n\n    def _check_init_distribution(self):\n        if isinstance(self.initial_state_distribution, np.ndarray):\n            assert abs(self.initial_state_distribution.sum() - 1.0) < 1e-15\n        else:\n            assert self.initial_state_distribution >= 0\n            assert self.initial_state_distribution < self.S\n\n    def _check(self):\n        \"\"\"\n        Check consistency of the MDP\n        \"\"\"\n        # Check initial_state_distribution\n        self._check_init_distribution()\n\n        # Check that P[s,a, :] is a probability distribution\n        for s in self._states:\n            for a in self._actions:\n                assert abs(self.P[s, a, :].sum() - 1.0) < 1e-15\n\n        # Check that dimensions match\n        S1, A1 = self.R.shape\n        S2, A2, S3 = self.P.shape\n        assert S1 == S2 == S3\n        assert A1 == A2\n\n    def set_initial_state_distribution(self, distribution):\n        \"\"\"\n        Parameters\n        ----------\n        distribution : numpy.ndarray or int\n            array of size (S,) containing the initial state distribution\n            or an integer representing the initial/default state\n        \"\"\"\n        self.initial_state_distribution = distribution\n        self._check_init_distribution()\n\n    def sample(self, state, action):\n        \"\"\"\n        Sample a transition s' from P(s'|state, action).\n        \"\"\"\n        prob = self.P[state, action, :]\n        next_state = self.rng.choice(self._states, p=prob)\n        reward = self.reward_fn(state, action, next_state)\n        done = self.is_terminal(self.state)\n        info = {}\n        return next_state, reward, done, info\n\n    def step(self, action):\n        assert action in self._actions, \"Invalid action!\"\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n        return next_state, reward, done, info\n\n    def is_terminal(self, state):\n        \"\"\"\n        Returns true if a state is terminal.\n        \"\"\"\n        return False\n\n    def reward_fn(self, state, action, next_state):\n        \"\"\"\n        Reward function. Returns mean reward at (state, action) by default.\n\n        Parameters\n        ----------\n        state : int\n            current state\n        action : int\n            current action\n        next_state :\n            next state\n\n        Returns:\n            reward : float\n        \"\"\"\n        return self.R[state, action]\n\n    def log(self):\n        \"\"\"\n        Print the structure of the MDP.\n        \"\"\"\n        indent = '    '\n        for s in self._states:\n            logger.info(f\"State {s} {indent}\")\n            for a in self._actions:\n                logger.info(f\"{indent} Action {a}\")\n                for ss in self._states:\n                    if self.P[s, a, ss] > 0.0:\n                        logger.info(f'{2 * indent} transition to {ss} '\n                                    f'with prob {self.P[s, a, ss]: .2f}')\n            logger.info(\"~~~~~~~~~~~~~~~~~~~~\")",
  "def __init__(self, R, P, initial_state_distribution=0):\n        Model.__init__(self)\n        self.initial_state_distribution = initial_state_distribution\n        S, A = R.shape\n\n        self.S = S\n        self.A = A\n\n        self.R = R\n        self.P = P\n\n        self.observation_space = spaces.Discrete(S)\n        self.action_space = spaces.Discrete(A)\n        self.reward_range = (self.R.min(), self.R.max())\n\n        self.state = None\n\n        self._states = np.arange(S)\n        self._actions = np.arange(A)\n\n        self.reset()\n        self._process_terminal_states()\n        self._check()",
  "def reset(self):\n        \"\"\"\n        Reset the environment to a default state.\n        \"\"\"\n        if isinstance(self.initial_state_distribution, np.ndarray):\n            self.state = self.rng.choice(self._states,\n                                         p=self.initial_state_distribution)\n        else:\n            self.state = self.initial_state_distribution\n        return self.state",
  "def _process_terminal_states(self):\n        \"\"\"\n        Adapt transition array P so that terminal states\n        are absorbing.\n        \"\"\"\n        for ss in range(self.S):\n            if self.is_terminal(ss):\n                self.P[ss, :, :] = 0.0\n                self.P[ss, :, ss] = 1.0",
  "def _check_init_distribution(self):\n        if isinstance(self.initial_state_distribution, np.ndarray):\n            assert abs(self.initial_state_distribution.sum() - 1.0) < 1e-15\n        else:\n            assert self.initial_state_distribution >= 0\n            assert self.initial_state_distribution < self.S",
  "def _check(self):\n        \"\"\"\n        Check consistency of the MDP\n        \"\"\"\n        # Check initial_state_distribution\n        self._check_init_distribution()\n\n        # Check that P[s,a, :] is a probability distribution\n        for s in self._states:\n            for a in self._actions:\n                assert abs(self.P[s, a, :].sum() - 1.0) < 1e-15\n\n        # Check that dimensions match\n        S1, A1 = self.R.shape\n        S2, A2, S3 = self.P.shape\n        assert S1 == S2 == S3\n        assert A1 == A2",
  "def set_initial_state_distribution(self, distribution):\n        \"\"\"\n        Parameters\n        ----------\n        distribution : numpy.ndarray or int\n            array of size (S,) containing the initial state distribution\n            or an integer representing the initial/default state\n        \"\"\"\n        self.initial_state_distribution = distribution\n        self._check_init_distribution()",
  "def sample(self, state, action):\n        \"\"\"\n        Sample a transition s' from P(s'|state, action).\n        \"\"\"\n        prob = self.P[state, action, :]\n        next_state = self.rng.choice(self._states, p=prob)\n        reward = self.reward_fn(state, action, next_state)\n        done = self.is_terminal(self.state)\n        info = {}\n        return next_state, reward, done, info",
  "def step(self, action):\n        assert action in self._actions, \"Invalid action!\"\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n        return next_state, reward, done, info",
  "def is_terminal(self, state):\n        \"\"\"\n        Returns true if a state is terminal.\n        \"\"\"\n        return False",
  "def reward_fn(self, state, action, next_state):\n        \"\"\"\n        Reward function. Returns mean reward at (state, action) by default.\n\n        Parameters\n        ----------\n        state : int\n            current state\n        action : int\n            current action\n        next_state :\n            next state\n\n        Returns:\n            reward : float\n        \"\"\"\n        return self.R[state, action]",
  "def log(self):\n        \"\"\"\n        Print the structure of the MDP.\n        \"\"\"\n        indent = '    '\n        for s in self._states:\n            logger.info(f\"State {s} {indent}\")\n            for a in self._actions:\n                logger.info(f\"{indent} Action {a}\")\n                for ss in self._states:\n                    if self.P[s, a, ss] > 0.0:\n                        logger.info(f'{2 * indent} transition to {ss} '\n                                    f'with prob {self.P[s, a, ss]: .2f}')\n            logger.info(\"~~~~~~~~~~~~~~~~~~~~\")",
  "class GridWorld(RenderInterface2D, FiniteMDP):\n    \"\"\"\n    Simple GridWorld environment.\n\n    Parameters\n    -----------\n    nrows : int\n        number of rows\n    ncols : int\n        number of columns\n    start_coord : tuple\n        tuple with coordinates of initial position\n    terminal_states : tuple\n        ((row_0, col_0), (row_1, col_1), ...) = coordinates of\n        terminal states\n    success_probability : double\n        probability of moving in the chosen direction\n    reward_at: dict\n        dictionary, keys = tuple containing coordinates, values = reward\n        at each coordinate\n    walls : tuple\n        ((row_0, col_0), (row_1, col_1), ...) = coordinates of walls\n    default_reward : double\n        reward received at states not in  'reward_at'\n\n    \"\"\"\n    name = \"GridWorld\"\n\n    def __init__(self,\n                 nrows=5,\n                 ncols=5,\n                 start_coord=(0, 0),\n                 terminal_states=None,\n                 success_probability=0.9,\n                 reward_at=None,\n                 walls=((1, 1), (2, 2)),\n                 default_reward=0.0):\n        # Grid dimensions\n        self.nrows = nrows\n        self.ncols = ncols\n\n        # Reward parameters\n        self.default_reward = default_reward\n\n        # Default config\n        if reward_at is not None:\n            self.reward_at = reward_at\n        else:\n            self.reward_at = {(nrows - 1, ncols - 1): 1}\n        if walls is not None:\n            self.walls = walls\n        else:\n            self.walls = ()\n        if terminal_states is not None:\n            self.terminal_states = terminal_states\n        else:\n            self.terminal_states = ((nrows - 1, ncols - 1),)\n\n        # Probability of going left/right/up/down when choosing the\n        # correspondent action\n        # The remaining probability mass is distributed uniformly to other\n        # available actions\n        self.success_probability = success_probability\n\n        # Start coordinate\n        self.start_coord = tuple(start_coord)\n\n        # Actions (string to index & index to string)\n        self.a_str2idx = {'left': 0, 'right': 1, 'up': 2, 'down': 3}\n        self.a_idx2str = {0: 'left', 1: 'right', 2: 'up', 3: 'down'}\n\n        # --------------------------------------------\n        # The variables below are defined in _build()\n        # --------------------------------------------\n\n        # Mappings (state index) <-> (state coordinate)\n        self.index2coord = {}\n        self.coord2index = {}\n\n        # MDP parameters for base class\n        self.P = None\n        self.R = None\n        self.Ns = None\n        self.Na = 4\n\n        # Build\n        self._build()\n        init_state_idx = self.coord2index[start_coord]\n        FiniteMDP.__init__(self, self.R, self.P,\n                           initial_state_distribution=init_state_idx)\n        RenderInterface2D.__init__(self)\n        self.reset()\n        self.reward_range = (self.R.min(), self.R.max())\n\n        # rendering info\n        self.set_clipping_area((0, self.ncols, 0, self.nrows))\n        self.set_refresh_interval(100)  # in milliseconds\n        self.renderer_type = 'pygame'\n\n    def is_terminal(self, state):\n        state_coord = self.index2coord[state]\n        return state_coord in self.terminal_states\n\n    def reward_fn(self, state, action, next_state):\n        row, col = self.index2coord[state]\n        if (row, col) in self.reward_at:\n            return self.reward_at[(row, col)]\n        if (row, col) in self.walls:\n            return 0.0\n        return self.default_reward\n\n    def _build(self):\n        self._build_state_mappings_and_states()\n        self._build_transition_probabilities()\n        self._build_mean_rewards()\n\n    def _build_state_mappings_and_states(self):\n        index = 0\n        for rr in range(self.nrows):\n            for cc in range(self.ncols):\n                if (rr, cc) in self.walls:\n                    self.coord2index[(rr, cc)] = -1\n                else:\n                    self.coord2index[(rr, cc)] = index\n                    self.index2coord[index] = (rr, cc)\n                    index += 1\n        states = np.arange(index).tolist()\n        self.Ns = len(states)\n\n    def _build_mean_rewards(self):\n        S = self.Ns\n        A = self.Na\n        self.R = np.zeros((S, A))\n        for ss in range(S):\n            for aa in range(A):\n                mean_r = 0\n                for ns in range(S):\n                    mean_r += self.reward_fn(ss, aa, ns) * self.P[ss, aa, ns]\n                self.R[ss, aa] = mean_r\n\n    def _build_transition_probabilities(self):\n        Ns = self.Ns\n        Na = self.Na\n        self.P = np.zeros((Ns, Na, Ns))\n        for s in range(Ns):\n            s_coord = self.index2coord[s]\n            neighbors = self._get_neighbors(*s_coord)\n            valid_neighbors = [neighbors[nn][0] for nn in neighbors\n                               if neighbors[nn][1]]\n            n_valid = len(valid_neighbors)\n            for a in range(Na):  # each action corresponds to a direction\n                for nn in neighbors:\n                    next_s_coord = neighbors[nn][0]\n                    if next_s_coord in valid_neighbors:\n                        next_s = self.coord2index[next_s_coord]\n                        if a == nn:  # action is successful\n                            self.P[s, a, next_s] = self.success_probability \\\n                                + (1 - self.success_probability) \\\n                                * (n_valid == 1)\n                        elif neighbors[a][0] not in valid_neighbors:\n                            self.P[s, a, s] = 1.0\n                        else:\n                            if n_valid > 1:\n                                self.P[s, a, next_s] = \\\n                                    (1.0 - self.success_probability) \\\n                                    / (n_valid - 1)\n\n    def _get_neighbors(self, row, col):\n        aux = {}\n        aux['left'] = (row, col - 1)  # left\n        aux['right'] = (row, col + 1)  # right\n        aux['up'] = (row - 1, col)  # up\n        aux['down'] = (row + 1, col)  # down\n        neighbors = {}\n        for direction_str in aux:\n            direction = self.a_str2idx[direction_str]\n            next_s = aux[direction_str]\n            neighbors[direction] = (next_s, self._is_valid(*next_s))\n        return neighbors\n\n    def get_transition_support(self, state):\n        row, col = self.index2coord[state]\n        neighbors = [(row, col - 1), (row, col + 1),\n                     (row - 1, col), (row + 1, col)]\n        return [self.coord2index[coord] for coord in neighbors\n                if self._is_valid(*coord)]\n\n    def _is_valid(self, row, col):\n        if (row, col) in self.walls:\n            return False\n        elif row < 0 or row >= self.nrows:\n            return False\n        elif col < 0 or col >= self.ncols:\n            return False\n        return True\n\n    def _build_ascii(self):\n        grid = [[''] * self.ncols for rr in range(self.nrows)]\n        grid_idx = [[''] * self.ncols for rr in range(self.nrows)]\n        for rr in range(self.nrows):\n            for cc in range(self.ncols):\n                if (rr, cc) in self.walls:\n                    grid[rr][cc] = 'x '\n                else:\n                    grid[rr][cc] = 'o '\n                grid_idx[rr][cc] = str(self.coord2index[(rr, cc)]).zfill(3)\n\n        for (rr, cc) in self.reward_at:\n            rwd = self.reward_at[(rr, cc)]\n            if rwd > 0:\n                grid[rr][cc] = '+ '\n            if rwd < 0:\n                grid[rr][cc] = '-'\n\n        grid[self.start_coord[0]][self.start_coord[1]] = 'I '\n\n        # current position of the agent\n        x, y = self.index2coord[self.state]\n        grid[x][y] = 'A '\n\n        #\n        grid_ascii = ''\n        for rr in range(self.nrows + 1):\n            if rr < self.nrows:\n                grid_ascii += str(rr).zfill(2) + 2 * ' '\\\n                     + ' '.join(grid[rr]) + '\\n'\n            else:\n                grid_ascii += 3 * ' ' + ' '.join([str(jj).zfill(2) for jj\n                                                  in range(self.ncols)])\n\n        self.grid_ascii = grid_ascii\n        self.grid_idx = grid_idx\n        return self.grid_ascii\n\n    def display_values(self, values):\n        assert len(values) == self.Ns\n        grid_values = [['X'.ljust(9)] * self.ncols for ii in range(self.nrows)]\n        for s_idx in range(self.Ns):\n            v = values[s_idx]\n            row, col = self.index2coord[s_idx]\n            grid_values[row][col] = (\"%0.2f\" % v).ljust(9)\n\n        grid_values_ascii = ''\n        for rr in range(self.nrows + 1):\n            if rr < self.nrows:\n                grid_values_ascii += str(rr).zfill(2) + 2 * ' ' \\\n                    + ' '.join(grid_values[rr]) + '\\n'\n            else:\n                grid_values_ascii += 4 * ' ' \\\n                    + ' '.join([str(jj).zfill(2).ljust(9) for jj\n                                in range(self.ncols)])\n        logger.info(grid_values_ascii)\n\n    def print_transition_at(self, row, col, action):\n        s_idx = self.coord2index[(row, col)]\n        if s_idx < 0:\n            logger.info(\"wall!\")\n            return\n        a_idx = self.a_str2idx[action]\n        for next_s_idx, prob in enumerate(self.P[s_idx, a_idx]):\n            if prob > 0:\n                logger.info(\"to (%d, %d) with prob %f\" %\n                            (self.index2coord[next_s_idx] + (prob,)))\n\n    def render_ascii(self):\n        logger.info(self._build_ascii())\n\n    def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n        return next_state, reward, done, info\n\n    #\n    # Code for rendering\n    #\n\n    def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # walls\n        for wall in self.walls:\n            y, x = wall\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.25, 0.25, 0.25))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # rewards\n        for (y, x) in self.reward_at:\n            flag = GeometricPrimitive(\"POLYGON\")\n            rwd = self.reward_at[(y, x)]\n            color = 0.5*np.abs(rwd)/self.reward_range[1]\n            if rwd > 0:\n                flag.set_color((0.0, color, 0.0))\n            if rwd < 0:\n                flag.set_color((color, 0.0, 0.0))\n\n            x += 0.5\n            y += 0.25\n            flag.add_vertex((x, y))\n            flag.add_vertex((x + 0.25, y + 0.5))\n            flag.add_vertex((x - 0.25, y + 0.5))\n            bg.add_shape(flag)\n\n        return bg\n\n    def get_scene(self, state):\n        \"\"\"\n        Return scene (list of shapes) representing a given state\n        \"\"\"\n        y, x = self.index2coord[state]\n        x = x + 0.5  # centering\n        y = y + 0.5  # centering\n\n        scene = Scene()\n\n        agent = circle_shape((x, y), 0.25, n_points=5)\n        agent.type = \"POLYGON\"\n        agent.set_color((0.75, 0.0, 0.5))\n\n        scene.add_shape(agent)\n        return scene",
  "def __init__(self,\n                 nrows=5,\n                 ncols=5,\n                 start_coord=(0, 0),\n                 terminal_states=None,\n                 success_probability=0.9,\n                 reward_at=None,\n                 walls=((1, 1), (2, 2)),\n                 default_reward=0.0):\n        # Grid dimensions\n        self.nrows = nrows\n        self.ncols = ncols\n\n        # Reward parameters\n        self.default_reward = default_reward\n\n        # Default config\n        if reward_at is not None:\n            self.reward_at = reward_at\n        else:\n            self.reward_at = {(nrows - 1, ncols - 1): 1}\n        if walls is not None:\n            self.walls = walls\n        else:\n            self.walls = ()\n        if terminal_states is not None:\n            self.terminal_states = terminal_states\n        else:\n            self.terminal_states = ((nrows - 1, ncols - 1),)\n\n        # Probability of going left/right/up/down when choosing the\n        # correspondent action\n        # The remaining probability mass is distributed uniformly to other\n        # available actions\n        self.success_probability = success_probability\n\n        # Start coordinate\n        self.start_coord = tuple(start_coord)\n\n        # Actions (string to index & index to string)\n        self.a_str2idx = {'left': 0, 'right': 1, 'up': 2, 'down': 3}\n        self.a_idx2str = {0: 'left', 1: 'right', 2: 'up', 3: 'down'}\n\n        # --------------------------------------------\n        # The variables below are defined in _build()\n        # --------------------------------------------\n\n        # Mappings (state index) <-> (state coordinate)\n        self.index2coord = {}\n        self.coord2index = {}\n\n        # MDP parameters for base class\n        self.P = None\n        self.R = None\n        self.Ns = None\n        self.Na = 4\n\n        # Build\n        self._build()\n        init_state_idx = self.coord2index[start_coord]\n        FiniteMDP.__init__(self, self.R, self.P,\n                           initial_state_distribution=init_state_idx)\n        RenderInterface2D.__init__(self)\n        self.reset()\n        self.reward_range = (self.R.min(), self.R.max())\n\n        # rendering info\n        self.set_clipping_area((0, self.ncols, 0, self.nrows))\n        self.set_refresh_interval(100)  # in milliseconds\n        self.renderer_type = 'pygame'",
  "def is_terminal(self, state):\n        state_coord = self.index2coord[state]\n        return state_coord in self.terminal_states",
  "def reward_fn(self, state, action, next_state):\n        row, col = self.index2coord[state]\n        if (row, col) in self.reward_at:\n            return self.reward_at[(row, col)]\n        if (row, col) in self.walls:\n            return 0.0\n        return self.default_reward",
  "def _build(self):\n        self._build_state_mappings_and_states()\n        self._build_transition_probabilities()\n        self._build_mean_rewards()",
  "def _build_state_mappings_and_states(self):\n        index = 0\n        for rr in range(self.nrows):\n            for cc in range(self.ncols):\n                if (rr, cc) in self.walls:\n                    self.coord2index[(rr, cc)] = -1\n                else:\n                    self.coord2index[(rr, cc)] = index\n                    self.index2coord[index] = (rr, cc)\n                    index += 1\n        states = np.arange(index).tolist()\n        self.Ns = len(states)",
  "def _build_mean_rewards(self):\n        S = self.Ns\n        A = self.Na\n        self.R = np.zeros((S, A))\n        for ss in range(S):\n            for aa in range(A):\n                mean_r = 0\n                for ns in range(S):\n                    mean_r += self.reward_fn(ss, aa, ns) * self.P[ss, aa, ns]\n                self.R[ss, aa] = mean_r",
  "def _build_transition_probabilities(self):\n        Ns = self.Ns\n        Na = self.Na\n        self.P = np.zeros((Ns, Na, Ns))\n        for s in range(Ns):\n            s_coord = self.index2coord[s]\n            neighbors = self._get_neighbors(*s_coord)\n            valid_neighbors = [neighbors[nn][0] for nn in neighbors\n                               if neighbors[nn][1]]\n            n_valid = len(valid_neighbors)\n            for a in range(Na):  # each action corresponds to a direction\n                for nn in neighbors:\n                    next_s_coord = neighbors[nn][0]\n                    if next_s_coord in valid_neighbors:\n                        next_s = self.coord2index[next_s_coord]\n                        if a == nn:  # action is successful\n                            self.P[s, a, next_s] = self.success_probability \\\n                                + (1 - self.success_probability) \\\n                                * (n_valid == 1)\n                        elif neighbors[a][0] not in valid_neighbors:\n                            self.P[s, a, s] = 1.0\n                        else:\n                            if n_valid > 1:\n                                self.P[s, a, next_s] = \\\n                                    (1.0 - self.success_probability) \\\n                                    / (n_valid - 1)",
  "def _get_neighbors(self, row, col):\n        aux = {}\n        aux['left'] = (row, col - 1)  # left\n        aux['right'] = (row, col + 1)  # right\n        aux['up'] = (row - 1, col)  # up\n        aux['down'] = (row + 1, col)  # down\n        neighbors = {}\n        for direction_str in aux:\n            direction = self.a_str2idx[direction_str]\n            next_s = aux[direction_str]\n            neighbors[direction] = (next_s, self._is_valid(*next_s))\n        return neighbors",
  "def get_transition_support(self, state):\n        row, col = self.index2coord[state]\n        neighbors = [(row, col - 1), (row, col + 1),\n                     (row - 1, col), (row + 1, col)]\n        return [self.coord2index[coord] for coord in neighbors\n                if self._is_valid(*coord)]",
  "def _is_valid(self, row, col):\n        if (row, col) in self.walls:\n            return False\n        elif row < 0 or row >= self.nrows:\n            return False\n        elif col < 0 or col >= self.ncols:\n            return False\n        return True",
  "def _build_ascii(self):\n        grid = [[''] * self.ncols for rr in range(self.nrows)]\n        grid_idx = [[''] * self.ncols for rr in range(self.nrows)]\n        for rr in range(self.nrows):\n            for cc in range(self.ncols):\n                if (rr, cc) in self.walls:\n                    grid[rr][cc] = 'x '\n                else:\n                    grid[rr][cc] = 'o '\n                grid_idx[rr][cc] = str(self.coord2index[(rr, cc)]).zfill(3)\n\n        for (rr, cc) in self.reward_at:\n            rwd = self.reward_at[(rr, cc)]\n            if rwd > 0:\n                grid[rr][cc] = '+ '\n            if rwd < 0:\n                grid[rr][cc] = '-'\n\n        grid[self.start_coord[0]][self.start_coord[1]] = 'I '\n\n        # current position of the agent\n        x, y = self.index2coord[self.state]\n        grid[x][y] = 'A '\n\n        #\n        grid_ascii = ''\n        for rr in range(self.nrows + 1):\n            if rr < self.nrows:\n                grid_ascii += str(rr).zfill(2) + 2 * ' '\\\n                     + ' '.join(grid[rr]) + '\\n'\n            else:\n                grid_ascii += 3 * ' ' + ' '.join([str(jj).zfill(2) for jj\n                                                  in range(self.ncols)])\n\n        self.grid_ascii = grid_ascii\n        self.grid_idx = grid_idx\n        return self.grid_ascii",
  "def display_values(self, values):\n        assert len(values) == self.Ns\n        grid_values = [['X'.ljust(9)] * self.ncols for ii in range(self.nrows)]\n        for s_idx in range(self.Ns):\n            v = values[s_idx]\n            row, col = self.index2coord[s_idx]\n            grid_values[row][col] = (\"%0.2f\" % v).ljust(9)\n\n        grid_values_ascii = ''\n        for rr in range(self.nrows + 1):\n            if rr < self.nrows:\n                grid_values_ascii += str(rr).zfill(2) + 2 * ' ' \\\n                    + ' '.join(grid_values[rr]) + '\\n'\n            else:\n                grid_values_ascii += 4 * ' ' \\\n                    + ' '.join([str(jj).zfill(2).ljust(9) for jj\n                                in range(self.ncols)])\n        logger.info(grid_values_ascii)",
  "def print_transition_at(self, row, col, action):\n        s_idx = self.coord2index[(row, col)]\n        if s_idx < 0:\n            logger.info(\"wall!\")\n            return\n        a_idx = self.a_str2idx[action]\n        for next_s_idx, prob in enumerate(self.P[s_idx, a_idx]):\n            if prob > 0:\n                logger.info(\"to (%d, %d) with prob %f\" %\n                            (self.index2coord[next_s_idx] + (prob,)))",
  "def render_ascii(self):\n        logger.info(self._build_ascii())",
  "def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n        self.state = next_state\n        return next_state, reward, done, info",
  "def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n\n        # walls\n        for wall in self.walls:\n            y, x = wall\n            shape = GeometricPrimitive(\"POLYGON\")\n            shape.set_color((0.25, 0.25, 0.25))\n            shape.add_vertex((x, y))\n            shape.add_vertex((x + 1, y))\n            shape.add_vertex((x + 1, y + 1))\n            shape.add_vertex((x, y + 1))\n            bg.add_shape(shape)\n\n        # rewards\n        for (y, x) in self.reward_at:\n            flag = GeometricPrimitive(\"POLYGON\")\n            rwd = self.reward_at[(y, x)]\n            color = 0.5*np.abs(rwd)/self.reward_range[1]\n            if rwd > 0:\n                flag.set_color((0.0, color, 0.0))\n            if rwd < 0:\n                flag.set_color((color, 0.0, 0.0))\n\n            x += 0.5\n            y += 0.25\n            flag.add_vertex((x, y))\n            flag.add_vertex((x + 0.25, y + 0.5))\n            flag.add_vertex((x - 0.25, y + 0.5))\n            bg.add_shape(flag)\n\n        return bg",
  "def get_scene(self, state):\n        \"\"\"\n        Return scene (list of shapes) representing a given state\n        \"\"\"\n        y, x = self.index2coord[state]\n        x = x + 0.5  # centering\n        y = y + 0.5  # centering\n\n        scene = Scene()\n\n        agent = circle_shape((x, y), 0.25, n_points=5)\n        agent.type = \"POLYGON\"\n        agent.set_color((0.75, 0.0, 0.5))\n\n        scene.add_shape(agent)\n        return scene",
  "class Chain(RenderInterface2D, FiniteMDP):\n    \"\"\"\n    Simple chain environment.\n    Reward 0.05 in initial state, reward 1.0 in final state.\n\n    Parameters\n    ----------\n    L : int\n        length of the chain\n    fail_prob : double\n        fail probability\n    \"\"\"\n\n    name = \"Chain\"\n\n    def __init__(self, L=5, fail_prob=0.1):\n        assert L >= 2\n        self.L = L\n\n        # transition probabilities\n        P = np.zeros((L, 2, L))\n        for ss in range(L):\n            for _ in range(2):\n                if ss == 0:\n                    P[ss, 0, ss] = 1.0 - fail_prob  # action 0 = don't move\n                    P[ss, 1, ss + 1] = 1.0 - fail_prob  # action 1 = right\n                    P[ss, 0, ss + 1] = fail_prob\n                    P[ss, 1, ss] = fail_prob\n                elif ss == L - 1:\n                    P[ss, 0, ss - 1] = 1.0 - fail_prob  # action 0 = left\n                    P[ss, 1, ss] = 1.0 - fail_prob  # action 1 = don't move\n                    P[ss, 0, ss] = fail_prob\n                    P[ss, 1, ss - 1] = fail_prob\n                else:\n                    P[ss, 0, ss - 1] = 1.0 - fail_prob  # action 0 = left\n                    P[ss, 1, ss + 1] = 1.0 - fail_prob  # action 1 = right\n                    P[ss, 0, ss + 1] = fail_prob\n                    P[ss, 1, ss - 1] = fail_prob\n\n                    # mean reward\n        S = L\n        A = 2\n        R = np.zeros((S, A))\n        R[L - 1, :] = 1.0\n        R[0, :] = 0.05\n\n        # init base classes\n        FiniteMDP.__init__(self, R, P, initial_state_distribution=0)\n        RenderInterface2D.__init__(self)\n        self.reward_range = (0.0, 1.0)\n\n        # rendering info\n        self.set_clipping_area((0, L, 0, 1))\n        self.set_refresh_interval(100)  # in milliseconds\n\n    def step(self, action):\n        assert action in self._actions, \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n\n        self.state = next_state\n        return next_state, reward, done, info\n\n    #\n    # Code for rendering\n    #\n\n    def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n        colors = [(0.8, 0.8, 0.8), (0.9, 0.9, 0.9)]\n        for ii in range(self.L):\n            shape = GeometricPrimitive(\"QUADS\")\n            shape.add_vertex((ii, 0))\n            shape.add_vertex((ii + 1, 0))\n            shape.add_vertex((ii + 1, 1))\n            shape.add_vertex((ii, 1))\n            shape.set_color(colors[ii % 2])\n            bg.add_shape(shape)\n\n        flag = GeometricPrimitive(\"TRIANGLES\")\n        flag.set_color((0.0, 0.5, 0.0))\n        x = self.L - 0.5\n        y = 0.25\n        flag.add_vertex((x, y))\n        flag.add_vertex((x + 0.25, y + 0.5))\n        flag.add_vertex((x - 0.25, y + 0.5))\n        bg.add_shape(flag)\n\n        return bg\n\n    def get_scene(self, state):\n        \"\"\"\n        Return scene (list of shapes) representing a given state\n        \"\"\"\n        scene = Scene()\n\n        agent = GeometricPrimitive(\"QUADS\")\n        agent.set_color((0.75, 0.0, 0.5))\n\n        size = 0.25\n        x = state + 0.5\n        y = 0.5\n\n        agent.add_vertex((x - size / 4.0, y - size))\n        agent.add_vertex((x + size / 4.0, y - size))\n        agent.add_vertex((x + size / 4.0, y + size))\n        agent.add_vertex((x - size / 4.0, y + size))\n\n        agent.add_vertex((x - size, y - size / 4.0))\n        agent.add_vertex((x + size, y - size / 4.0))\n        agent.add_vertex((x + size, y + size / 4.0))\n        agent.add_vertex((x - size, y + size / 4.0))\n\n        scene.add_shape(agent)\n        return scene",
  "def __init__(self, L=5, fail_prob=0.1):\n        assert L >= 2\n        self.L = L\n\n        # transition probabilities\n        P = np.zeros((L, 2, L))\n        for ss in range(L):\n            for _ in range(2):\n                if ss == 0:\n                    P[ss, 0, ss] = 1.0 - fail_prob  # action 0 = don't move\n                    P[ss, 1, ss + 1] = 1.0 - fail_prob  # action 1 = right\n                    P[ss, 0, ss + 1] = fail_prob\n                    P[ss, 1, ss] = fail_prob\n                elif ss == L - 1:\n                    P[ss, 0, ss - 1] = 1.0 - fail_prob  # action 0 = left\n                    P[ss, 1, ss] = 1.0 - fail_prob  # action 1 = don't move\n                    P[ss, 0, ss] = fail_prob\n                    P[ss, 1, ss - 1] = fail_prob\n                else:\n                    P[ss, 0, ss - 1] = 1.0 - fail_prob  # action 0 = left\n                    P[ss, 1, ss + 1] = 1.0 - fail_prob  # action 1 = right\n                    P[ss, 0, ss + 1] = fail_prob\n                    P[ss, 1, ss - 1] = fail_prob\n\n                    # mean reward\n        S = L\n        A = 2\n        R = np.zeros((S, A))\n        R[L - 1, :] = 1.0\n        R[0, :] = 0.05\n\n        # init base classes\n        FiniteMDP.__init__(self, R, P, initial_state_distribution=0)\n        RenderInterface2D.__init__(self)\n        self.reward_range = (0.0, 1.0)\n\n        # rendering info\n        self.set_clipping_area((0, L, 0, 1))\n        self.set_refresh_interval(100)",
  "def step(self, action):\n        assert action in self._actions, \"Invalid action!\"\n\n        # save state for rendering\n        if self.is_render_enabled():\n            self.append_state_for_rendering(self.state)\n\n        # take step\n        next_state, reward, done, info = self.sample(self.state, action)\n\n        self.state = next_state\n        return next_state, reward, done, info",
  "def get_background(self):\n        \"\"\"\n        Returne a scene (list of shapes) representing the background\n        \"\"\"\n        bg = Scene()\n        colors = [(0.8, 0.8, 0.8), (0.9, 0.9, 0.9)]\n        for ii in range(self.L):\n            shape = GeometricPrimitive(\"QUADS\")\n            shape.add_vertex((ii, 0))\n            shape.add_vertex((ii + 1, 0))\n            shape.add_vertex((ii + 1, 1))\n            shape.add_vertex((ii, 1))\n            shape.set_color(colors[ii % 2])\n            bg.add_shape(shape)\n\n        flag = GeometricPrimitive(\"TRIANGLES\")\n        flag.set_color((0.0, 0.5, 0.0))\n        x = self.L - 0.5\n        y = 0.25\n        flag.add_vertex((x, y))\n        flag.add_vertex((x + 0.25, y + 0.5))\n        flag.add_vertex((x - 0.25, y + 0.5))\n        bg.add_shape(flag)\n\n        return bg",
  "def get_scene(self, state):\n        \"\"\"\n        Return scene (list of shapes) representing a given state\n        \"\"\"\n        scene = Scene()\n\n        agent = GeometricPrimitive(\"QUADS\")\n        agent.set_color((0.75, 0.0, 0.5))\n\n        size = 0.25\n        x = state + 0.5\n        y = 0.5\n\n        agent.add_vertex((x - size / 4.0, y - size))\n        agent.add_vertex((x + size / 4.0, y - size))\n        agent.add_vertex((x + size / 4.0, y + size))\n        agent.add_vertex((x - size / 4.0, y + size))\n\n        agent.add_vertex((x - size, y - size / 4.0))\n        agent.add_vertex((x + size, y - size / 4.0))\n        agent.add_vertex((x + size, y + size / 4.0))\n        agent.add_vertex((x - size, y + size / 4.0))\n\n        scene.add_shape(agent)\n        return scene",
  "class AgentStats:\n    \"\"\"\n    Class to train, optimize hyperparameters, evaluate and gather\n    statistics about an agent.\n\n    Parameters\n    ----------\n    agent_class\n        Class of the agent.\n    train_env : Model or tuple (constructor, kwargs)\n        Enviroment used to initialize/train the agent.\n    eval_env : Model\n        Environment used to evaluate the agent. If None, set to a\n        reseeded deep copy of train_env.\n    init_kwargs : dict\n        Arguments required by the agent's constructor.\n    fit_kwargs : dict\n        Arguments required to call agent.fit().\n    policy_kwargs : dict\n        Arguments required to call agent.policy().\n    agent_name : str\n        Name of the agent. If None, set to agent_class.name\n    n_fit : int\n        Number of agent instances to fit.\n    n_jobs : int\n        Number of jobs to train the agents in parallel using joblib.\n    output_dir : str\n        Directory where to store data by default.\n    joblib_backend: str, {'threading', 'loky' or 'multiprocessing'}, default: 'multiprocessing'\n        Backend for joblib Parallel.\n    thread_logging_level : str, default: 'INFO'\n        Logging level in each of the threads used to fit agents.\n    \"\"\"\n\n    def __init__(self,\n                 agent_class,\n                 train_env,\n                 eval_env=None,\n                 eval_horizon=None,\n                 init_kwargs=None,\n                 fit_kwargs=None,\n                 policy_kwargs=None,\n                 agent_name=None,\n                 n_fit=4,\n                 n_jobs=4,\n                 output_dir=None,\n                 joblib_backend='loky',\n                 thread_logging_level='INFO'):\n        # agent_class should only be None when the constructor is called\n        # by the class method AgentStats.load(), since the agent class\n        # will be loaded.\n        if agent_class is not None:\n\n            self.agent_name = agent_name\n            if agent_name is None:\n                self.agent_name = agent_class.name\n\n            # create oject identifier\n            timestamp = datetime.timestamp(datetime.now())\n            self.identifier = 'stats_{}_{}'.format(self.agent_name,\n                                                   str(int(timestamp)))\n\n            self.agent_class = agent_class\n            self.train_env = train_env\n            if eval_env is None:\n                eval_env = train_env\n            try:\n                self.eval_env = deepcopy(eval_env)\n                seeding.safe_reseed(self.eval_env)\n            except Exception:\n                logger.warning('[AgentStats]: Not possible to deepcopy or reseed eval_env')\n                self.eval_env = eval_env\n\n            # preprocess eval_env, in case it is a tuple\n            self.eval_env = _preprocess_env(self.eval_env)\n\n            # check kwargs\n            init_kwargs = init_kwargs or {}\n            fit_kwargs = fit_kwargs or {}\n            policy_kwargs = policy_kwargs or {}\n\n            # evaluation horizon\n            self.eval_horizon = eval_horizon\n            if eval_horizon is None:\n                try:\n                    self.eval_horizon = init_kwargs['horizon']\n                except KeyError:\n                    pass\n\n            # init and fit kwargs are deep copied in fit()\n            self.init_kwargs = deepcopy(init_kwargs)\n            self.fit_kwargs = fit_kwargs\n            self.policy_kwargs = deepcopy(policy_kwargs)\n            self.n_fit = n_fit\n            self.n_jobs = n_jobs\n            self.joblib_backend = joblib_backend\n            self.thread_logging_level = thread_logging_level\n\n            # output dir\n            output_dir = output_dir or self.identifier\n            self.output_dir = Path(output_dir)\n\n            # Create environment copies for training\n            self.train_env_set = []\n            for _ in range(n_fit):\n                _env = deepcopy(train_env)\n                seeding.safe_reseed(_env)\n                self.train_env_set.append(_env)\n\n            # Create list of writers for each agent that will be trained\n            self.writers = [('default', None) for _ in range(n_fit)]\n\n            #\n            self.fitted_agents = None\n            self.fit_kwargs_list = None  # keep in memory for partial_fit()\n            self.fit_statistics = None\n            self.best_hyperparams = None\n\n            #  fit info is initialized at _process_fit_statistics()\n            self.fit_info = None\n\n            #\n            self.rng = seeding.get_rng()\n\n            # optuna study\n            self.study = None\n\n    def set_output_dir(self, output_dir):\n        \"\"\"\n        Change output directory.\n\n        Parameters\n        -----------\n        output_dir : str\n        \"\"\"\n        self.output_dir = Path(output_dir)\n\n    def set_writer(self, idx, writer_fn, writer_kwargs=None):\n        \"\"\"\n        Note\n        -----\n        Must be called right after creating an instance of AgentStats.\n\n        Parameters\n        ----------\n        writer_fn : callable, None or 'default'\n            Returns a writer for an agent, e.g. tensorboard SummaryWriter,\n            rlberry PeriodicWriter.\n            If 'default', use the default writer in the Agent class.\n            If None, disable any writer\n        writer_kwargs : dict or None\n            kwargs for writer_fn\n        idx : int\n            Index of the agent to set the writer (0 <= idx < `n_fit`).\n            AgentStats fits `n_fit` agents, the writer of each one of them\n            needs to be set separetely.\n        \"\"\"\n        assert idx >= 0 and idx < self.n_fit, \\\n            \"Invalid index sent to AgentStats.set_writer()\"\n        writer_kwargs = writer_kwargs or {}\n        self.writers[idx] = (writer_fn, writer_kwargs)\n\n    def disable_writers(self):\n        \"\"\"\n        Set all writers to None.\n        \"\"\"\n        self.writers = [('default', None) for _ in range(self.n_fit)]\n        if self.fitted_agents is not None:\n            for agent in self.fitted_agents:\n                agent.set_writer(None)\n\n    def fit(self):\n        \"\"\"\n        Fit the agent instances in parallel.\n        \"\"\"\n        logger.info(f\"Training AgentStats for {self.agent_name}... \")\n        seed_sequences = seeding.spawn(self.n_fit)\n        args = [(self.agent_class,\n                train_env,\n                deepcopy(self.init_kwargs),\n                deepcopy(self.fit_kwargs),\n                writer,\n                self.thread_logging_level,\n                thread_seed_seq)\n                for (thread_seed_seq, train_env, writer)\n                in zip(seed_sequences, self.train_env_set, self.writers)]\n\n        workers_output = Parallel(n_jobs=self.n_jobs,\n                                  verbose=5,\n                                  backend=self.joblib_backend)(\n            delayed(_fit_worker)(arg) for arg in args)\n\n        self.fitted_agents, stats = (\n            [i for i, j in workers_output],\n            [j for i, j in workers_output])\n\n        logger.info(\"... trained!\")\n\n        # gather all stats in a dictionary\n        self._process_fit_statistics(stats)\n\n    def partial_fit(self, fraction):\n        \"\"\"\n        Partially fit the agent instances (not parallel).\n        \"\"\"\n        assert fraction > 0.0 and fraction <= 1.0\n        assert issubclass(self.agent_class, IncrementalAgent)\n\n        # Create instances if this is the first call\n        if self.fitted_agents is None:\n            self.fitted_agents = []\n            self.fit_kwargs_list = []\n            for idx, train_env in enumerate(self.train_env_set):\n                init_kwargs = deepcopy(self.init_kwargs)\n\n                # preprocess train_env\n                train_env = _preprocess_env(train_env)\n\n                # create agent instance\n                agent = self.agent_class(train_env, copy_env=False,\n                                         reseed_env=False, **init_kwargs)\n                # set agent writer\n                if self.writers[idx][0] is None:\n                    agent.set_writer(None)\n                elif self.writers[idx][0] != 'default':\n                    writer_fn = self.writers[idx][0]\n                    writer_kwargs = self.writers[idx][1]\n                    agent.set_writer(writer_fn(**writer_kwargs))\n                #\n                self.fitted_agents.append(agent)\n                #\n                self.fit_kwargs_list.append(deepcopy(self.fit_kwargs))\n\n        # Run partial fit\n        stats = []\n        for agent, fit_kwargs in zip(self.fitted_agents, self.fit_kwargs_list):\n            info = agent.partial_fit(fraction, **fit_kwargs)\n            stats.append(info)\n        self._process_fit_statistics(stats)\n\n    def _process_fit_statistics(self, stats):\n        \"\"\"Gather stats in a dictionary\"\"\"\n        assert len(stats) > 0\n\n        ref_stats = stats[0] or {}\n        self.fit_info = tuple(ref_stats.keys())\n\n        self.fit_statistics = {}\n        for entry in self.fit_info:\n            self.fit_statistics[entry] = []\n            for stat in stats:\n                self.fit_statistics[entry].append(stat[entry])\n\n    def save_results(self, output_dir=None, **kwargs):\n        \"\"\"\n        Save the results obtained by optimize_hyperparameters(),\n        fit() and partial_fit() to a directory.\n\n        Parameters\n        ----------\n        output_dir : str or None\n            Output directory. If None, use self.output_dir.\n        \"\"\"\n        # use default self.output_dir if another one is not provided.\n        output_dir = output_dir or self.output_dir\n        output_dir = Path(output_dir)\n\n        # create dir if it does not exist\n        output_dir.mkdir(parents=True, exist_ok=True)\n        # save optimized hyperparameters\n        if self.best_hyperparams is not None:\n            fname = Path(output_dir) / 'best_hyperparams.json'\n            _safe_serialize_json(self.best_hyperparams, fname)\n        # save fit_statistics that can be aggregated in a pandas DataFrame\n        if self.fit_statistics is not None:\n            for entry in self.fit_statistics:\n                # gather data for entry\n                all_data = {}\n                for run, data in enumerate(self.fit_statistics[entry]):\n                    all_data[f'run_{run}'] = data\n                try:\n                    output = pd.DataFrame(all_data)\n                    # save\n                    fname = Path(output_dir) / f'stats_{entry}.csv'\n                    output.to_csv(fname, index=None)\n                except Exception:\n                    logger.warning(f\"Could not save entry [{entry}]\"\n                                   + \" of fit_statistics.\")\n\n    def save(self, filename='stats', **kwargs):\n        \"\"\"\n        Pickle the AgentStats object completely, so that\n        it can be loaded and continued later.\n\n        Removes writers, since they usually cannot be pickled.\n\n        This is useful, for instance:\n        * If we want to run hyperparameter optimization for\n        a few minutes/hours, save the results, then continue\n        the optimization later.\n        * If we ran some experiments and we want to reload\n        the trained agents to visualize their policies\n        policy in a rendered environment and create videos.\n\n        Parameters\n        ----------\n        filename : string\n            Filename with .pickle extension.\n            Saves to output_dir / filename\n        \"\"\"\n        # remove writers\n        self.disable_writers()\n\n        # save\n        filename = Path(filename).with_suffix('.pickle')\n        filename = self.output_dir / filename\n        filename.parent.mkdir(parents=True, exist_ok=True)\n        try:\n            with filename.open(\"wb\") as ff:\n                pickle.dump(self.__dict__, ff)\n            logger.info(\"Saved AgentStats({}) using pickle.\".format(self.agent_name))\n        except Exception:\n            try:\n                with filename.open(\"wb\") as ff:\n                    dill.dump(self.__dict__, ff)\n                logger.info(\"Saved AgentStats({}) using dill.\".format(self.agent_name))\n            except Exception as ex:\n                logger.warning(\"[AgentStats] Instance cannot be pickled: \" + str(ex))\n     \n    @classmethod\n    def load(cls, filename):\n        filename = Path(filename).with_suffix('.pickle')\n\n        obj = cls(None, None)\n        try:\n            with filename.open('rb') as ff:\n                tmp_dict = pickle.load(ff)\n            logger.info(\"Loaded AgentStats using pickle.\")\n        except Exception:\n            with filename.open('rb') as ff:\n                tmp_dict = dill.load(ff)\n            logger.info(\"Loaded AgentStats using dill.\")\n\n        obj.__dict__.clear()\n        obj.__dict__.update(tmp_dict)\n        return obj\n\n    def optimize_hyperparams(self,\n                             n_trials=5,\n                             timeout=60,\n                             n_sim=5,\n                             n_fit=2,\n                             n_jobs=2,\n                             sampler_method='random',\n                             pruner_method='halving',\n                             continue_previous=False,\n                             partial_fit_fraction=0.25,\n                             sampler_kwargs=None,\n                             evaluation_function=None,\n                             evaluation_function_kwargs=None):\n        \"\"\"\n        Run hyperparameter optimization and updates init_kwargs with the\n        best hyperparameters found.\n\n        Currently supported sampler_method:\n            'random' -> Random Search\n            'optuna_default' -> TPE\n            'grid' -> Grid Search\n            'cmaes' -> CMA-ES\n\n        Currently supported pruner_method:\n            'none'\n            'halving'\n\n        Parameters\n        ----------\n        n_trials: int\n            Number of agent evaluations\n        timeout: int\n            Stop study after the given number of second(s).\n            Set to None for unlimited time.\n        n_sim : int\n            Number of Monte Carlo simulations to evaluate a policy.\n        n_fit: int\n            Number of agents to fit for each hyperparam evaluation.\n        n_jobs: int\n            Number of jobs to fit agents for each hyperparam evaluation,\n            and also the number of jobs of Optuna.\n        sampler_method : str\n            Optuna sampling method.\n        pruner_method : str\n            Optuna pruner method.\n        continue_previous : bool\n            Set to true to continue previous Optuna study. If true,\n            sampler_method and pruner_method will be\n            the same as in the previous study.\n        partial_fit_fraction : double, in ]0, 1]\n            Fraction of the agent to fit for partial evaluation\n            (allows pruning of trials).\n            Only used for agents that implement partial_fit()\n            (IncrementalAgent interface).\n        sampler_kwargs : dict or None\n            Allows users to use different Optuna samplers with\n            personalized arguments.\n        evaluation_function : callable(agent_list, eval_env, **kwargs)->double, default: None\n            Function to maximize, that takes a list of agents and an environment as input, and returns a double.\n            If None, search for hyperparameters that maximize the mean reward.\n        evaluation_function_kwargs : dict or None\n            kwargsfor evaluation_function\n        \"\"\"\n        #\n        # setup\n        #\n        global _OPTUNA_INSTALLED\n        if not _OPTUNA_INSTALLED:\n            logging.error(\"Optuna not installed.\")\n            return\n\n        assert self.eval_horizon is not None, \\\n            \"To use optimize_hyperparams(), \" + \\\n            \"eval_horizon must be given to AgentStats.\"\n\n        assert partial_fit_fraction > 0.0 and partial_fit_fraction <= 1.0\n\n        evaluation_function_kwargs = evaluation_function_kwargs or {}\n        if evaluation_function is None:\n            evaluation_function = mc_policy_evaluation\n            evaluation_function_kwargs = {\n                'eval_horizon': self.eval_horizon,\n                'n_sim': n_sim,\n                'gamma': 1.0,\n                'policy_kwargs': self.policy_kwargs,\n                'stationary_policy': True,\n            }\n\n        #\n        # Create optuna study\n        #\n        if continue_previous:\n            assert self.study is not None\n            study = self.study\n\n        else:\n            if sampler_kwargs is None:\n                sampler_kwargs = {}\n            # get sampler\n            if sampler_method == 'random':\n                optuna_seed = self.rng.integers(2**16)\n                sampler = optuna.samplers.RandomSampler(seed=optuna_seed)\n            elif sampler_method == 'grid':\n                assert sampler_kwargs is not None, \\\n                    \"To use GridSampler, \" + \\\n                    \"a search_space dictionary must be provided.\"\n                sampler = optuna.samplers.GridSampler(**sampler_kwargs)\n            elif sampler_method == 'cmaes':\n                optuna_seed = self.rng.integers(2**16)\n                sampler_kwargs['seed'] = optuna_seed\n                sampler = optuna.samplers.CmaEsSampler(**sampler_kwargs)\n            elif sampler_method == 'optuna_default':\n                sampler = optuna.samplers.TPESampler(**sampler_kwargs)\n            else:\n                raise NotImplementedError(\n                      \"Sampler method %s is not implemented.\" % sampler_method)\n\n            # get pruner\n            if pruner_method == 'halving':\n                pruner = optuna.pruners.SuccessiveHalvingPruner(\n                            min_resource=1,\n                            reduction_factor=4,\n                            min_early_stopping_rate=0)\n            elif pruner_method == 'none':\n                pruner = None\n            else:\n                raise NotImplementedError(\n                      \"Pruner method %s is not implemented.\" % pruner_method)\n\n            # optuna study\n            study = optuna.create_study(sampler=sampler,\n                                        pruner=pruner,\n                                        direction='maximize')\n            self.study = study\n\n        def objective(trial):\n            kwargs = deepcopy(self.init_kwargs)\n\n            # will raise exception if sample_parameters() is not\n            # implemented by the agent class\n            kwargs.update(self.agent_class.sample_parameters(trial))\n\n            #\n            # fit and evaluate agents\n            #\n            # Create AgentStats with hyperparams\n            params_stats = AgentStats(\n                self.agent_class,\n                deepcopy(self.train_env),\n                init_kwargs=kwargs,   # kwargs are being optimized\n                fit_kwargs=deepcopy(self.fit_kwargs),\n                policy_kwargs=deepcopy(self.policy_kwargs),\n                agent_name='optim',\n                n_fit=n_fit,\n                n_jobs=n_jobs,\n                thread_logging_level='WARNING')\n\n            # Evaluation environment copy\n            params_eval_env = deepcopy(self.eval_env)\n            params_eval_env.reseed()\n\n            #\n            # Case 1: partial fit, that allows pruning\n            #\n            if partial_fit_fraction < 1.0 \\\n                    and issubclass(params_stats.agent_class, IncrementalAgent):\n                fraction_complete = 0.0\n                step = 0\n                while fraction_complete < 1.0:\n                    #\n                    params_stats.partial_fit(partial_fit_fraction)\n                    # Evaluate params\n                    eval_result = evaluation_function(params_stats.fitted_agents,\n                                                      params_eval_env,\n                                                      **evaluation_function_kwargs)\n\n                    eval_value = eval_result.mean()\n\n                    # Report intermediate objective value\n                    trial.report(eval_value, step)\n\n                    #\n                    fraction_complete += partial_fit_fraction\n                    step += 1\n                    #\n\n                    # Handle pruning based on the intermediate value.\n                    if trial.should_prune():\n                        raise optuna.TrialPruned()\n\n            #\n            # Case 2: full fit\n            #\n            else:\n                # Fit and evaluate params_stats\n                params_stats.fit()\n\n                # Evaluate params\n                eval_result = evaluation_function(params_stats.fitted_agents,\n                                                  params_eval_env,\n                                                  **evaluation_function_kwargs)\n\n                eval_value = eval_result.mean()\n\n            return eval_value\n\n        try:\n            study.optimize(objective,\n                           n_trials=n_trials,\n                           n_jobs=n_jobs,\n                           timeout=timeout)\n        except KeyboardInterrupt:\n            logger.warning(\"Evaluation stopped.\")\n\n        # continue\n        best_trial = study.best_trial\n\n        logger.info(f'Number of finished trials: {len(study.trials)}')\n        logger.info('Best trial:')\n        logger.info(f'Value: {best_trial.value}')\n        logger.info('Params:')\n        for key, value in best_trial.params.items():\n            logger.info(f'    {key}: {value}')\n\n        # store best parameters\n        self.best_hyperparams = best_trial.params\n\n        # update using best parameters\n        self.init_kwargs.update(best_trial.params)\n\n        return best_trial, study.trials_dataframe()",
  "def _preprocess_env(env):\n    \"\"\"\n    If env is a tuple (constructor, kwargs), creates an instance\n    and reseeds it.\n    Otherwise, does nothing.\n    \"\"\"\n    if isinstance(env, tuple):\n        constructor, kwargs = env\n        kwargs = kwargs or {}\n        env = constructor(**kwargs)\n        reseeded = safe_reseed(env)\n        assert reseeded\n    return env",
  "def _fit_worker(args):\n    \"\"\"\n    Create and fit an agent instance\n    \"\"\"\n    agent_class, train_env, init_kwargs, \\\n        fit_kwargs, writer, thread_logging_level, thread_seed_seq = args\n    # set thread seed sequence\n    seeding.set_global_seed(thread_seed_seq)\n    # preprocess train_env\n    train_env = _preprocess_env(train_env)\n\n    # logging level in thread\n    configure_logging(thread_logging_level)\n    # create agent\n    agent = agent_class(train_env, copy_env=False,\n                        reseed_env=False, **init_kwargs)\n    agent.name += f\"(spawn_key{thread_seed_seq.spawn_key})\"\n\n    # set writer\n    if writer[0] is None:\n        agent.set_writer(None)\n    elif writer[0] != 'default':\n        writer_fn = writer[0]\n        writer_kwargs = writer[1]\n        agent.set_writer(writer_fn(**writer_kwargs))\n    # fit agent\n    info = agent.fit(**fit_kwargs)\n\n    # Remove writer after fit (prevent pickle problems)\n    agent.set_writer(None)\n\n    return agent, info",
  "def _safe_serialize_json(obj, filename):\n    \"\"\"\n    Source: https://stackoverflow.com/a/56138540/5691288\n    \"\"\"\n    default = lambda o: f\"<<non-serializable: {type(o).__qualname__}>>\"\n    with open(filename, 'w') as fp:\n        json.dump(obj, fp, sort_keys=True, indent=4, default=default)",
  "def __init__(self,\n                 agent_class,\n                 train_env,\n                 eval_env=None,\n                 eval_horizon=None,\n                 init_kwargs=None,\n                 fit_kwargs=None,\n                 policy_kwargs=None,\n                 agent_name=None,\n                 n_fit=4,\n                 n_jobs=4,\n                 output_dir=None,\n                 joblib_backend='loky',\n                 thread_logging_level='INFO'):\n        # agent_class should only be None when the constructor is called\n        # by the class method AgentStats.load(), since the agent class\n        # will be loaded.\n        if agent_class is not None:\n\n            self.agent_name = agent_name\n            if agent_name is None:\n                self.agent_name = agent_class.name\n\n            # create oject identifier\n            timestamp = datetime.timestamp(datetime.now())\n            self.identifier = 'stats_{}_{}'.format(self.agent_name,\n                                                   str(int(timestamp)))\n\n            self.agent_class = agent_class\n            self.train_env = train_env\n            if eval_env is None:\n                eval_env = train_env\n            try:\n                self.eval_env = deepcopy(eval_env)\n                seeding.safe_reseed(self.eval_env)\n            except Exception:\n                logger.warning('[AgentStats]: Not possible to deepcopy or reseed eval_env')\n                self.eval_env = eval_env\n\n            # preprocess eval_env, in case it is a tuple\n            self.eval_env = _preprocess_env(self.eval_env)\n\n            # check kwargs\n            init_kwargs = init_kwargs or {}\n            fit_kwargs = fit_kwargs or {}\n            policy_kwargs = policy_kwargs or {}\n\n            # evaluation horizon\n            self.eval_horizon = eval_horizon\n            if eval_horizon is None:\n                try:\n                    self.eval_horizon = init_kwargs['horizon']\n                except KeyError:\n                    pass\n\n            # init and fit kwargs are deep copied in fit()\n            self.init_kwargs = deepcopy(init_kwargs)\n            self.fit_kwargs = fit_kwargs\n            self.policy_kwargs = deepcopy(policy_kwargs)\n            self.n_fit = n_fit\n            self.n_jobs = n_jobs\n            self.joblib_backend = joblib_backend\n            self.thread_logging_level = thread_logging_level\n\n            # output dir\n            output_dir = output_dir or self.identifier\n            self.output_dir = Path(output_dir)\n\n            # Create environment copies for training\n            self.train_env_set = []\n            for _ in range(n_fit):\n                _env = deepcopy(train_env)\n                seeding.safe_reseed(_env)\n                self.train_env_set.append(_env)\n\n            # Create list of writers for each agent that will be trained\n            self.writers = [('default', None) for _ in range(n_fit)]\n\n            #\n            self.fitted_agents = None\n            self.fit_kwargs_list = None  # keep in memory for partial_fit()\n            self.fit_statistics = None\n            self.best_hyperparams = None\n\n            #  fit info is initialized at _process_fit_statistics()\n            self.fit_info = None\n\n            #\n            self.rng = seeding.get_rng()\n\n            # optuna study\n            self.study = None",
  "def set_output_dir(self, output_dir):\n        \"\"\"\n        Change output directory.\n\n        Parameters\n        -----------\n        output_dir : str\n        \"\"\"\n        self.output_dir = Path(output_dir)",
  "def set_writer(self, idx, writer_fn, writer_kwargs=None):\n        \"\"\"\n        Note\n        -----\n        Must be called right after creating an instance of AgentStats.\n\n        Parameters\n        ----------\n        writer_fn : callable, None or 'default'\n            Returns a writer for an agent, e.g. tensorboard SummaryWriter,\n            rlberry PeriodicWriter.\n            If 'default', use the default writer in the Agent class.\n            If None, disable any writer\n        writer_kwargs : dict or None\n            kwargs for writer_fn\n        idx : int\n            Index of the agent to set the writer (0 <= idx < `n_fit`).\n            AgentStats fits `n_fit` agents, the writer of each one of them\n            needs to be set separetely.\n        \"\"\"\n        assert idx >= 0 and idx < self.n_fit, \\\n            \"Invalid index sent to AgentStats.set_writer()\"\n        writer_kwargs = writer_kwargs or {}\n        self.writers[idx] = (writer_fn, writer_kwargs)",
  "def disable_writers(self):\n        \"\"\"\n        Set all writers to None.\n        \"\"\"\n        self.writers = [('default', None) for _ in range(self.n_fit)]\n        if self.fitted_agents is not None:\n            for agent in self.fitted_agents:\n                agent.set_writer(None)",
  "def fit(self):\n        \"\"\"\n        Fit the agent instances in parallel.\n        \"\"\"\n        logger.info(f\"Training AgentStats for {self.agent_name}... \")\n        seed_sequences = seeding.spawn(self.n_fit)\n        args = [(self.agent_class,\n                train_env,\n                deepcopy(self.init_kwargs),\n                deepcopy(self.fit_kwargs),\n                writer,\n                self.thread_logging_level,\n                thread_seed_seq)\n                for (thread_seed_seq, train_env, writer)\n                in zip(seed_sequences, self.train_env_set, self.writers)]\n\n        workers_output = Parallel(n_jobs=self.n_jobs,\n                                  verbose=5,\n                                  backend=self.joblib_backend)(\n            delayed(_fit_worker)(arg) for arg in args)\n\n        self.fitted_agents, stats = (\n            [i for i, j in workers_output],\n            [j for i, j in workers_output])\n\n        logger.info(\"... trained!\")\n\n        # gather all stats in a dictionary\n        self._process_fit_statistics(stats)",
  "def partial_fit(self, fraction):\n        \"\"\"\n        Partially fit the agent instances (not parallel).\n        \"\"\"\n        assert fraction > 0.0 and fraction <= 1.0\n        assert issubclass(self.agent_class, IncrementalAgent)\n\n        # Create instances if this is the first call\n        if self.fitted_agents is None:\n            self.fitted_agents = []\n            self.fit_kwargs_list = []\n            for idx, train_env in enumerate(self.train_env_set):\n                init_kwargs = deepcopy(self.init_kwargs)\n\n                # preprocess train_env\n                train_env = _preprocess_env(train_env)\n\n                # create agent instance\n                agent = self.agent_class(train_env, copy_env=False,\n                                         reseed_env=False, **init_kwargs)\n                # set agent writer\n                if self.writers[idx][0] is None:\n                    agent.set_writer(None)\n                elif self.writers[idx][0] != 'default':\n                    writer_fn = self.writers[idx][0]\n                    writer_kwargs = self.writers[idx][1]\n                    agent.set_writer(writer_fn(**writer_kwargs))\n                #\n                self.fitted_agents.append(agent)\n                #\n                self.fit_kwargs_list.append(deepcopy(self.fit_kwargs))\n\n        # Run partial fit\n        stats = []\n        for agent, fit_kwargs in zip(self.fitted_agents, self.fit_kwargs_list):\n            info = agent.partial_fit(fraction, **fit_kwargs)\n            stats.append(info)\n        self._process_fit_statistics(stats)",
  "def _process_fit_statistics(self, stats):\n        \"\"\"Gather stats in a dictionary\"\"\"\n        assert len(stats) > 0\n\n        ref_stats = stats[0] or {}\n        self.fit_info = tuple(ref_stats.keys())\n\n        self.fit_statistics = {}\n        for entry in self.fit_info:\n            self.fit_statistics[entry] = []\n            for stat in stats:\n                self.fit_statistics[entry].append(stat[entry])",
  "def save_results(self, output_dir=None, **kwargs):\n        \"\"\"\n        Save the results obtained by optimize_hyperparameters(),\n        fit() and partial_fit() to a directory.\n\n        Parameters\n        ----------\n        output_dir : str or None\n            Output directory. If None, use self.output_dir.\n        \"\"\"\n        # use default self.output_dir if another one is not provided.\n        output_dir = output_dir or self.output_dir\n        output_dir = Path(output_dir)\n\n        # create dir if it does not exist\n        output_dir.mkdir(parents=True, exist_ok=True)\n        # save optimized hyperparameters\n        if self.best_hyperparams is not None:\n            fname = Path(output_dir) / 'best_hyperparams.json'\n            _safe_serialize_json(self.best_hyperparams, fname)\n        # save fit_statistics that can be aggregated in a pandas DataFrame\n        if self.fit_statistics is not None:\n            for entry in self.fit_statistics:\n                # gather data for entry\n                all_data = {}\n                for run, data in enumerate(self.fit_statistics[entry]):\n                    all_data[f'run_{run}'] = data\n                try:\n                    output = pd.DataFrame(all_data)\n                    # save\n                    fname = Path(output_dir) / f'stats_{entry}.csv'\n                    output.to_csv(fname, index=None)\n                except Exception:\n                    logger.warning(f\"Could not save entry [{entry}]\"\n                                   + \" of fit_statistics.\")",
  "def save(self, filename='stats', **kwargs):\n        \"\"\"\n        Pickle the AgentStats object completely, so that\n        it can be loaded and continued later.\n\n        Removes writers, since they usually cannot be pickled.\n\n        This is useful, for instance:\n        * If we want to run hyperparameter optimization for\n        a few minutes/hours, save the results, then continue\n        the optimization later.\n        * If we ran some experiments and we want to reload\n        the trained agents to visualize their policies\n        policy in a rendered environment and create videos.\n\n        Parameters\n        ----------\n        filename : string\n            Filename with .pickle extension.\n            Saves to output_dir / filename\n        \"\"\"\n        # remove writers\n        self.disable_writers()\n\n        # save\n        filename = Path(filename).with_suffix('.pickle')\n        filename = self.output_dir / filename\n        filename.parent.mkdir(parents=True, exist_ok=True)\n        try:\n            with filename.open(\"wb\") as ff:\n                pickle.dump(self.__dict__, ff)\n            logger.info(\"Saved AgentStats({}) using pickle.\".format(self.agent_name))\n        except Exception:\n            try:\n                with filename.open(\"wb\") as ff:\n                    dill.dump(self.__dict__, ff)\n                logger.info(\"Saved AgentStats({}) using dill.\".format(self.agent_name))\n            except Exception as ex:\n                logger.warning(\"[AgentStats] Instance cannot be pickled: \" + str(ex))",
  "def load(cls, filename):\n        filename = Path(filename).with_suffix('.pickle')\n\n        obj = cls(None, None)\n        try:\n            with filename.open('rb') as ff:\n                tmp_dict = pickle.load(ff)\n            logger.info(\"Loaded AgentStats using pickle.\")\n        except Exception:\n            with filename.open('rb') as ff:\n                tmp_dict = dill.load(ff)\n            logger.info(\"Loaded AgentStats using dill.\")\n\n        obj.__dict__.clear()\n        obj.__dict__.update(tmp_dict)\n        return obj",
  "def optimize_hyperparams(self,\n                             n_trials=5,\n                             timeout=60,\n                             n_sim=5,\n                             n_fit=2,\n                             n_jobs=2,\n                             sampler_method='random',\n                             pruner_method='halving',\n                             continue_previous=False,\n                             partial_fit_fraction=0.25,\n                             sampler_kwargs=None,\n                             evaluation_function=None,\n                             evaluation_function_kwargs=None):\n        \"\"\"\n        Run hyperparameter optimization and updates init_kwargs with the\n        best hyperparameters found.\n\n        Currently supported sampler_method:\n            'random' -> Random Search\n            'optuna_default' -> TPE\n            'grid' -> Grid Search\n            'cmaes' -> CMA-ES\n\n        Currently supported pruner_method:\n            'none'\n            'halving'\n\n        Parameters\n        ----------\n        n_trials: int\n            Number of agent evaluations\n        timeout: int\n            Stop study after the given number of second(s).\n            Set to None for unlimited time.\n        n_sim : int\n            Number of Monte Carlo simulations to evaluate a policy.\n        n_fit: int\n            Number of agents to fit for each hyperparam evaluation.\n        n_jobs: int\n            Number of jobs to fit agents for each hyperparam evaluation,\n            and also the number of jobs of Optuna.\n        sampler_method : str\n            Optuna sampling method.\n        pruner_method : str\n            Optuna pruner method.\n        continue_previous : bool\n            Set to true to continue previous Optuna study. If true,\n            sampler_method and pruner_method will be\n            the same as in the previous study.\n        partial_fit_fraction : double, in ]0, 1]\n            Fraction of the agent to fit for partial evaluation\n            (allows pruning of trials).\n            Only used for agents that implement partial_fit()\n            (IncrementalAgent interface).\n        sampler_kwargs : dict or None\n            Allows users to use different Optuna samplers with\n            personalized arguments.\n        evaluation_function : callable(agent_list, eval_env, **kwargs)->double, default: None\n            Function to maximize, that takes a list of agents and an environment as input, and returns a double.\n            If None, search for hyperparameters that maximize the mean reward.\n        evaluation_function_kwargs : dict or None\n            kwargsfor evaluation_function\n        \"\"\"\n        #\n        # setup\n        #\n        global _OPTUNA_INSTALLED\n        if not _OPTUNA_INSTALLED:\n            logging.error(\"Optuna not installed.\")\n            return\n\n        assert self.eval_horizon is not None, \\\n            \"To use optimize_hyperparams(), \" + \\\n            \"eval_horizon must be given to AgentStats.\"\n\n        assert partial_fit_fraction > 0.0 and partial_fit_fraction <= 1.0\n\n        evaluation_function_kwargs = evaluation_function_kwargs or {}\n        if evaluation_function is None:\n            evaluation_function = mc_policy_evaluation\n            evaluation_function_kwargs = {\n                'eval_horizon': self.eval_horizon,\n                'n_sim': n_sim,\n                'gamma': 1.0,\n                'policy_kwargs': self.policy_kwargs,\n                'stationary_policy': True,\n            }\n\n        #\n        # Create optuna study\n        #\n        if continue_previous:\n            assert self.study is not None\n            study = self.study\n\n        else:\n            if sampler_kwargs is None:\n                sampler_kwargs = {}\n            # get sampler\n            if sampler_method == 'random':\n                optuna_seed = self.rng.integers(2**16)\n                sampler = optuna.samplers.RandomSampler(seed=optuna_seed)\n            elif sampler_method == 'grid':\n                assert sampler_kwargs is not None, \\\n                    \"To use GridSampler, \" + \\\n                    \"a search_space dictionary must be provided.\"\n                sampler = optuna.samplers.GridSampler(**sampler_kwargs)\n            elif sampler_method == 'cmaes':\n                optuna_seed = self.rng.integers(2**16)\n                sampler_kwargs['seed'] = optuna_seed\n                sampler = optuna.samplers.CmaEsSampler(**sampler_kwargs)\n            elif sampler_method == 'optuna_default':\n                sampler = optuna.samplers.TPESampler(**sampler_kwargs)\n            else:\n                raise NotImplementedError(\n                      \"Sampler method %s is not implemented.\" % sampler_method)\n\n            # get pruner\n            if pruner_method == 'halving':\n                pruner = optuna.pruners.SuccessiveHalvingPruner(\n                            min_resource=1,\n                            reduction_factor=4,\n                            min_early_stopping_rate=0)\n            elif pruner_method == 'none':\n                pruner = None\n            else:\n                raise NotImplementedError(\n                      \"Pruner method %s is not implemented.\" % pruner_method)\n\n            # optuna study\n            study = optuna.create_study(sampler=sampler,\n                                        pruner=pruner,\n                                        direction='maximize')\n            self.study = study\n\n        def objective(trial):\n            kwargs = deepcopy(self.init_kwargs)\n\n            # will raise exception if sample_parameters() is not\n            # implemented by the agent class\n            kwargs.update(self.agent_class.sample_parameters(trial))\n\n            #\n            # fit and evaluate agents\n            #\n            # Create AgentStats with hyperparams\n            params_stats = AgentStats(\n                self.agent_class,\n                deepcopy(self.train_env),\n                init_kwargs=kwargs,   # kwargs are being optimized\n                fit_kwargs=deepcopy(self.fit_kwargs),\n                policy_kwargs=deepcopy(self.policy_kwargs),\n                agent_name='optim',\n                n_fit=n_fit,\n                n_jobs=n_jobs,\n                thread_logging_level='WARNING')\n\n            # Evaluation environment copy\n            params_eval_env = deepcopy(self.eval_env)\n            params_eval_env.reseed()\n\n            #\n            # Case 1: partial fit, that allows pruning\n            #\n            if partial_fit_fraction < 1.0 \\\n                    and issubclass(params_stats.agent_class, IncrementalAgent):\n                fraction_complete = 0.0\n                step = 0\n                while fraction_complete < 1.0:\n                    #\n                    params_stats.partial_fit(partial_fit_fraction)\n                    # Evaluate params\n                    eval_result = evaluation_function(params_stats.fitted_agents,\n                                                      params_eval_env,\n                                                      **evaluation_function_kwargs)\n\n                    eval_value = eval_result.mean()\n\n                    # Report intermediate objective value\n                    trial.report(eval_value, step)\n\n                    #\n                    fraction_complete += partial_fit_fraction\n                    step += 1\n                    #\n\n                    # Handle pruning based on the intermediate value.\n                    if trial.should_prune():\n                        raise optuna.TrialPruned()\n\n            #\n            # Case 2: full fit\n            #\n            else:\n                # Fit and evaluate params_stats\n                params_stats.fit()\n\n                # Evaluate params\n                eval_result = evaluation_function(params_stats.fitted_agents,\n                                                  params_eval_env,\n                                                  **evaluation_function_kwargs)\n\n                eval_value = eval_result.mean()\n\n            return eval_value\n\n        try:\n            study.optimize(objective,\n                           n_trials=n_trials,\n                           n_jobs=n_jobs,\n                           timeout=timeout)\n        except KeyboardInterrupt:\n            logger.warning(\"Evaluation stopped.\")\n\n        # continue\n        best_trial = study.best_trial\n\n        logger.info(f'Number of finished trials: {len(study.trials)}')\n        logger.info('Best trial:')\n        logger.info(f'Value: {best_trial.value}')\n        logger.info('Params:')\n        for key, value in best_trial.params.items():\n            logger.info(f'    {key}: {value}')\n\n        # store best parameters\n        self.best_hyperparams = best_trial.params\n\n        # update using best parameters\n        self.init_kwargs.update(best_trial.params)\n\n        return best_trial, study.trials_dataframe()",
  "def objective(trial):\n            kwargs = deepcopy(self.init_kwargs)\n\n            # will raise exception if sample_parameters() is not\n            # implemented by the agent class\n            kwargs.update(self.agent_class.sample_parameters(trial))\n\n            #\n            # fit and evaluate agents\n            #\n            # Create AgentStats with hyperparams\n            params_stats = AgentStats(\n                self.agent_class,\n                deepcopy(self.train_env),\n                init_kwargs=kwargs,   # kwargs are being optimized\n                fit_kwargs=deepcopy(self.fit_kwargs),\n                policy_kwargs=deepcopy(self.policy_kwargs),\n                agent_name='optim',\n                n_fit=n_fit,\n                n_jobs=n_jobs,\n                thread_logging_level='WARNING')\n\n            # Evaluation environment copy\n            params_eval_env = deepcopy(self.eval_env)\n            params_eval_env.reseed()\n\n            #\n            # Case 1: partial fit, that allows pruning\n            #\n            if partial_fit_fraction < 1.0 \\\n                    and issubclass(params_stats.agent_class, IncrementalAgent):\n                fraction_complete = 0.0\n                step = 0\n                while fraction_complete < 1.0:\n                    #\n                    params_stats.partial_fit(partial_fit_fraction)\n                    # Evaluate params\n                    eval_result = evaluation_function(params_stats.fitted_agents,\n                                                      params_eval_env,\n                                                      **evaluation_function_kwargs)\n\n                    eval_value = eval_result.mean()\n\n                    # Report intermediate objective value\n                    trial.report(eval_value, step)\n\n                    #\n                    fraction_complete += partial_fit_fraction\n                    step += 1\n                    #\n\n                    # Handle pruning based on the intermediate value.\n                    if trial.should_prune():\n                        raise optuna.TrialPruned()\n\n            #\n            # Case 2: full fit\n            #\n            else:\n                # Fit and evaluate params_stats\n                params_stats.fit()\n\n                # Evaluate params\n                eval_result = evaluation_function(params_stats.fitted_agents,\n                                                  params_eval_env,\n                                                  **evaluation_function_kwargs)\n\n                eval_value = eval_result.mean()\n\n            return eval_value",
  "def fit_stats(stats, save):\n    stats.fit()\n    if save:\n        stats.save_results()\n        stats.save()\n    return stats",
  "class MultipleStats:\n    \"\"\"\n    Class to fit multiple AgentStats instances in parallel with multiple threads.\n    \"\"\"\n    def __init__(self) -> None:\n        super().__init__()\n        self.instances = []\n\n    def append(self, agent_stats):\n        \"\"\"\n        Append new AgentStats instance.\n\n        Parameters\n        ----------\n        agent_stats : AgentStats\n        \"\"\"\n        self.instances.append(agent_stats)\n\n    def run(self, n_threads=4, save=False):\n        \"\"\"\n        Fit AgentStats instances in parallel.\n\n        Parameters\n        ----------\n        n_threads : int, default: 4\n            Number of parallel threads.\n\n        save: bool, default: False\n            If true, save AgentStats intances immediately after fitting.\n            AgentStats.save() and AgentStats.save_results() are called.\n        \"\"\"\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = []\n            for inst in self.instances:\n                futures.append(\n                    executor.submit(fit_stats, inst, save=save)\n                )\n\n            fitted_instances = []\n            for future in concurrent.futures.as_completed(futures):\n                fitted_instances.append(\n                    future.result()\n                )\n\n            self.instances = fitted_instances\n        # with ThreadPool(n_processes) as p:\n        #     self.instances = p.map(fit_stats, self.instances)\n\n    def save(self):\n        \"\"\"\n        Pickle AgentStats instances and saves fit statistics in .csv files.\n        The output folder is defined in each of the AgentStats instances.\n        \"\"\"\n        for stats in self.instances:\n            stats.save_results()\n            stats.save()\n\n    @property\n    def allstats(self):\n        return self.instances",
  "def __init__(self) -> None:\n        super().__init__()\n        self.instances = []",
  "def append(self, agent_stats):\n        \"\"\"\n        Append new AgentStats instance.\n\n        Parameters\n        ----------\n        agent_stats : AgentStats\n        \"\"\"\n        self.instances.append(agent_stats)",
  "def run(self, n_threads=4, save=False):\n        \"\"\"\n        Fit AgentStats instances in parallel.\n\n        Parameters\n        ----------\n        n_threads : int, default: 4\n            Number of parallel threads.\n\n        save: bool, default: False\n            If true, save AgentStats intances immediately after fitting.\n            AgentStats.save() and AgentStats.save_results() are called.\n        \"\"\"\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = []\n            for inst in self.instances:\n                futures.append(\n                    executor.submit(fit_stats, inst, save=save)\n                )\n\n            fitted_instances = []\n            for future in concurrent.futures.as_completed(futures):\n                fitted_instances.append(\n                    future.result()\n                )\n\n            self.instances = fitted_instances",
  "def save(self):\n        \"\"\"\n        Pickle AgentStats instances and saves fit statistics in .csv files.\n        The output folder is defined in each of the AgentStats instances.\n        \"\"\"\n        for stats in self.instances:\n            stats.save_results()\n            stats.save()",
  "def allstats(self):\n        return self.instances",
  "def mc_policy_evaluation(agent,\n                         eval_env,\n                         eval_horizon=10**5,\n                         n_sim=10,\n                         gamma=1.0,\n                         policy_kwargs=None,\n                         stationary_policy=True):\n    \"\"\"\n    Monte-Carlo Policy evaluation [1]_ of an agent to estimate the value at the initial state.\n\n    If a list of agents is provided as input, for each evaluation, one of the agents is sampled\n    uniformly at random.\n\n    Parameters\n    ----------\n    agent : Agent or list of agents.\n        Trained agent(s).\n    eval_env : Env\n        Evaluation environment.\n    eval_horizon : int, default: 10**5\n        Horizon, maximum episode length.\n    n_sim : int, default: 10\n        Number of Monte Carlo simulations.\n    gamma : double, default: 1.0\n        Discount factor.\n    policy_kwargs : dict or None\n        Optional kwargs for agent.policy() method.\n    stationary_policy : bool, default: True\n        If False, the time step h (0<= h <= eval_horizon) is sent as argument\n        to agent.policy() for policy evaluation.\n\n    Return\n    ------\n    Numpy array of shape (n_sim, ) containing the sum of rewards in each simulation.\n\n    References\n    ----------\n    .. [1] http://incompleteideas.net/book/first/ebook/node50.html\n    \"\"\"\n    rng = seeding.get_rng()\n    if not isinstance(agent, list):\n        agents = [agent]\n    else:\n        agents = agent\n\n    policy_kwargs = policy_kwargs or {}\n\n    episode_rewards = np.zeros(n_sim)\n    for sim in range(n_sim):\n        idx = rng.integers(len(agents))\n\n        observation = eval_env.reset()\n        for hh in range(eval_horizon):\n            if stationary_policy:\n                action = agents[idx].policy(observation, **policy_kwargs)\n            else:\n                action = agents[idx].policy(observation, hh, **policy_kwargs)\n            observation, reward, done, _ = eval_env.step(action)\n            episode_rewards[sim] += reward * np.power(gamma, hh)\n            if done:\n                break\n\n    return episode_rewards",
  "def plot_episode_rewards(agent_stats,\n                         cumulative=False,\n                         fignum=None,\n                         show=True,\n                         max_value=None,\n                         plot_regret=False):\n    \"\"\"\n    Given a list of AgentStats, plot the rewards obtained in each episode.\n    The dictionary returned by agents' .fit() method must contain a key 'episode_rewards'.\n\n    Parameters\n    ----------\n    agent_stats : AgentStats, or list of AgentStats\n    cumulative : bool, default: False\n        If true, plot cumulative rewards.\n    fignum: string or int\n        Identifier of plot figure.\n    show: bool\n        If true, calls plt.show().\n    max_value: double, default: None\n        Maximum reward achievable in one episode.\n    plot_regret: bool, default: False\n        If true, plots the regret. Requires max_val to be given.\n    \"\"\"\n    agent_stats_list = agent_stats\n    if not isinstance(agent_stats_list, list):\n        agent_stats_list = [agent_stats_list]\n\n    if plot_regret and max_value is None:\n        raise ValueError(\"max_value must be provided for regret plot\")\n\n    # line style\n    lines = [\"-\", \"--\", \"-.\", \":\"]\n    linecycler = cycle(lines)\n\n    plt.figure(fignum)\n    for stats in agent_stats_list:\n        # train agents if they are not already trained\n        if stats.fitted_agents is None:\n            stats.fit()\n\n        if 'episode_rewards' not in stats.fit_info:\n            logger.warning(\"episode_rewards not available for %s.\" % stats.agent_name)\n            continue\n\n        # get reward statistics and plot them\n        rewards = np.array(stats.fit_statistics['episode_rewards'])\n        if cumulative and (not plot_regret):\n            data = np.cumsum(rewards, axis=1)\n            label = \"total reward\"\n        elif plot_regret:\n            data = np.cumsum(max_value-rewards, axis=1)\n            label = \"regret\"\n        else:\n            data = rewards\n            label = \"reward in one episode\"\n\n        mean_r = data.mean(axis=0)\n        std_r = data.std(axis=0)\n        episodes = np.arange(1, data.shape[1]+1)\n\n        plt.plot(episodes, mean_r, next(linecycler), label=stats.agent_name)\n        plt.fill_between(episodes, mean_r-std_r, mean_r+std_r, alpha=0.4)\n        plt.legend()\n        plt.xlabel(\"episodes\")\n        plt.ylabel(label)\n        plt.grid(True, alpha=0.75)\n\n    if show:\n        plt.show()",
  "def compare_policies(agent_stats_list,\n                     eval_env=None,\n                     eval_horizon=None,\n                     stationary_policy=True,\n                     n_sim=10,\n                     fignum=None,\n                     show=True,\n                     plot=True,\n                     **kwargs):\n    \"\"\"\n    Compare the policies of each of the agents in agent_stats_list.\n    Each element of the agent_stats_list contains a list of fitted agents.\n    To evaluate the policy, we repeat n_sim times:\n        * choose one of the fitted agents uniformly at random\n        * run its policy in eval_env for eval_horizon time steps\n\n\n    Parameters\n    ----------\n    agent_stats_list : list of AgentStats objects.\n    eval_env : Model\n        Environment where to evaluate the policies.\n        If None, it is taken from AgentStats.\n    eval_horizon : int\n        Number of time steps for policy evaluation.\n        If None, it is taken from AgentStats.\n    stationary_policy : bool\n        If False, the time step h (0<= h <= eval_horizon) is sent as argument\n        to agent.policy() for policy evaluation.\n    n_sim : int\n        Number of simulations to evaluate each policy.\n    fignum: string or int\n        Identifier of plot figure.\n    show: bool\n        If true, calls plt.show().\n    plot: bool\n        If false, do not plot.\n    kwargs:\n        Extra parameters for sns.boxplot\n    \"\"\"\n    #\n    # evaluation\n    #\n    use_eval_from_agent_stats = (eval_env is None)\n    use_horizon_from_agent_stats = (eval_horizon is None)\n\n    rng = seeding.get_rng()\n    agents_rewards = []\n    for agent_stats in agent_stats_list:\n        # train agents if they are not already trained\n        if agent_stats.fitted_agents is None:\n            agent_stats.fit()\n\n        # eval env and horizon\n        if use_eval_from_agent_stats:\n            eval_env = agent_stats.eval_env\n            assert eval_env is not None, \\\n                \"eval_env not in AgentStats %s\" % agent_stats.agent_name\n        if use_horizon_from_agent_stats:\n            eval_horizon = agent_stats.eval_horizon\n            assert eval_horizon is not None, \\\n                \"eval_horizon not in AgentStats %s\" % agent_stats.agent_name\n\n        # evaluate agent\n        episode_rewards = np.zeros(n_sim)\n        for sim in range(n_sim):\n            # choose one of the fitted agents randomly\n            agent_idx = rng.integers(len(agent_stats.fitted_agents))\n            agent = agent_stats.fitted_agents[agent_idx]\n            # evaluate agent\n            observation = eval_env.reset()\n            for hh in range(eval_horizon):\n                if stationary_policy:\n                    action = agent.policy(observation,\n                                          **agent_stats.policy_kwargs)\n                else:\n                    action = agent.policy(observation, hh,\n                                          **agent_stats.policy_kwargs)\n                observation, reward, done, _ = eval_env.step(action)\n                episode_rewards[sim] += reward\n                if done:\n                    break\n        # store rewards\n        agents_rewards.append(episode_rewards)\n\n    #\n    # plot\n    #\n\n    # build unique agent IDs (in case there are two agents with the same ID)\n    unique_ids = []\n    id_count = {}\n    for agent_stats in agent_stats_list:\n        name = agent_stats.agent_name\n        if name not in id_count:\n            id_count[name] = 1\n        else:\n            id_count[name] += 1\n\n        unique_ids.append(name + \"*\"*(id_count[name]-1))\n\n    # convert output to DataFrame\n    data = {}\n    for agent_id, agent_rewards in zip(unique_ids, agents_rewards):\n        data[agent_id] = agent_rewards\n    output = pd.DataFrame(data)\n\n    # plot\n    if plot:\n        plt.figure(fignum)\n\n        with sns.axes_style(\"whitegrid\"):\n            ax = sns.boxplot(data=output, **kwargs)\n            ax.set_xlabel(\"agent\")\n            ax.set_ylabel(\"rewards in one episode\")\n            plt.title(\"Environment = %s\" %\n                      getattr(eval_env.unwrapped, \"name\",\n                              eval_env.unwrapped.__class__.__name__))\n            if show:\n                plt.show()\n\n    return output",
  "def plot_fit_info(agent_stats,\n                  info,\n                  fignum=None,\n                  show=True):\n    \"\"\"\n    Given a list of AgentStats, plot data (corresponding to info) obtained in each episode.\n    The dictionary returned by agents' .fit() method must contain a key equal to `info`.\n\n    Parameters\n    ----------\n    agent_stats : AgentStats, or list of AgentStats\n    info : str\n        Info (returned by Agent.fit()) to plot.\n    fignum: string or int\n        Identifier of plot figure.\n    show: bool\n        If true, calls plt.show().\n    \"\"\"\n    agent_stats_list = agent_stats\n    if not isinstance(agent_stats_list, list):\n        agent_stats_list = [agent_stats_list]\n\n    # line style\n    lines = [\"-\", \"--\", \"-.\", \":\"]\n    linecycler = cycle(lines)\n    plt.figure(fignum)\n    for stats in agent_stats_list:\n        # train agents if they are not already trained\n        if stats.fitted_agents is None:\n            stats.fit()\n\n        if info not in stats.fit_info:\n            logger.warning(\"{} not available for {}.\".format(info, stats.agent_name))\n            continue\n\n        # get data and plot them\n        data = np.array(stats.fit_statistics[info])\n        mean_data = data.mean(axis=0)\n        std_data = data.std(axis=0)\n        episodes = np.arange(1, data.shape[1]+1)\n\n        plt.plot(episodes, mean_data, next(linecycler), label=stats.agent_name)\n        plt.fill_between(episodes, mean_data-std_data, mean_data+std_data, alpha=0.4)\n        plt.legend()\n        plt.xlabel(\"episodes\")\n        plt.ylabel(info)\n        plt.grid(True, alpha=0.75)\n\n    if show:\n        plt.show()",
  "class DiscreteCounter(UncertaintyEstimator):\n    \"\"\"\n    Parameters\n    ----------\n    observation_space : spaces.Box or spaces.Discrete\n    action_space : spaces.Box or spaces.Discrete\n    n_bins_obs: int\n        number of bins to discretize observation space\n    n_bins_actions: int\n        number of bins to discretize action space\n    rate_power : float\n        Returns bonuses in 1/n ** rate_power.\n    \"\"\"\n    def __init__(self,\n                 observation_space,\n                 action_space,\n                 n_bins_obs=10,\n                 n_bins_actions=10,\n                 rate_power=0.5,\n                 **kwargs):\n        UncertaintyEstimator.__init__(self, observation_space, action_space)\n\n        self.rate_power = rate_power\n\n        self.continuous_state = False\n        self.continuous_action = False\n\n        if isinstance(observation_space, Discrete):\n            self.n_states = observation_space.n\n        else:\n            self.continuous_state = True\n            self.state_discretizer = Discretizer(self.observation_space,\n                                                 n_bins_obs)\n            self.n_states = self.state_discretizer.discrete_space.n\n\n        if isinstance(action_space, Discrete):\n            self.n_actions = action_space.n\n        else:\n            self.continuous_action = True\n            self.action_discretizer = Discretizer(self.action_space,\n                                                  n_bins_actions)\n            self.n_actions = self.action_discretizer.discrete_space.n\n\n        self.N_sa = np.zeros((self.n_states, self.n_actions))\n\n    def _preprocess(self, state, action):\n        if self.continuous_state:\n            state = self.state_discretizer.discretize(state)\n        if self.continuous_action:\n            action = self.action_discretizer.discretize(action)\n        return state, action\n\n    def reset(self):\n        self.N_sa = np.zeros((self.n_states, self.n_actions))\n\n    @preprocess_args(expected_type='numpy')\n    def update(self, state, action, next_state=None, reward=None, **kwargs):\n        state, action = self._preprocess(state, action)\n        self.N_sa[state, action] += 1\n\n    @preprocess_args(expected_type='numpy')\n    def measure(self, state, action, **kwargs):\n        state, action = self._preprocess(state, action)\n        n = np.maximum(1.0, self.N_sa[state, action])\n        return np.power(1.0/n, self.rate_power)\n\n    def count(self, state, action):\n        state, action = self._preprocess(state, action)\n        return self.N_sa[state, action]\n\n    def get_n_visited_states(self):\n        \"\"\"\n        Returns the number of different states sent to the .update() function.\n        For continuous state spaces, counts the number of different discretized states.\n        \"\"\"\n        n_visited_states = (self.N_sa.sum(axis=1) > 0).sum()\n        return n_visited_states\n\n    def get_entropy(self):\n        \"\"\"\n        Returns the entropy of the empirical distribution over states, induced by the state counts.\n        Uses log2.\n        \"\"\"\n        visited = self.N_sa.sum(axis=1) > 0\n        if visited.sum() == 0.0:\n            return 0.0\n        # number of visits of visited states only\n        n_visits = self.N_sa[visited, :].sum(axis=1)\n        # empirical distribution\n        dist = n_visits/n_visits.sum()\n        entropy = (-dist*np.log2(dist)).sum()\n        return entropy",
  "def __init__(self,\n                 observation_space,\n                 action_space,\n                 n_bins_obs=10,\n                 n_bins_actions=10,\n                 rate_power=0.5,\n                 **kwargs):\n        UncertaintyEstimator.__init__(self, observation_space, action_space)\n\n        self.rate_power = rate_power\n\n        self.continuous_state = False\n        self.continuous_action = False\n\n        if isinstance(observation_space, Discrete):\n            self.n_states = observation_space.n\n        else:\n            self.continuous_state = True\n            self.state_discretizer = Discretizer(self.observation_space,\n                                                 n_bins_obs)\n            self.n_states = self.state_discretizer.discrete_space.n\n\n        if isinstance(action_space, Discrete):\n            self.n_actions = action_space.n\n        else:\n            self.continuous_action = True\n            self.action_discretizer = Discretizer(self.action_space,\n                                                  n_bins_actions)\n            self.n_actions = self.action_discretizer.discrete_space.n\n\n        self.N_sa = np.zeros((self.n_states, self.n_actions))",
  "def _preprocess(self, state, action):\n        if self.continuous_state:\n            state = self.state_discretizer.discretize(state)\n        if self.continuous_action:\n            action = self.action_discretizer.discretize(action)\n        return state, action",
  "def reset(self):\n        self.N_sa = np.zeros((self.n_states, self.n_actions))",
  "def update(self, state, action, next_state=None, reward=None, **kwargs):\n        state, action = self._preprocess(state, action)\n        self.N_sa[state, action] += 1",
  "def measure(self, state, action, **kwargs):\n        state, action = self._preprocess(state, action)\n        n = np.maximum(1.0, self.N_sa[state, action])\n        return np.power(1.0/n, self.rate_power)",
  "def count(self, state, action):\n        state, action = self._preprocess(state, action)\n        return self.N_sa[state, action]",
  "def get_n_visited_states(self):\n        \"\"\"\n        Returns the number of different states sent to the .update() function.\n        For continuous state spaces, counts the number of different discretized states.\n        \"\"\"\n        n_visited_states = (self.N_sa.sum(axis=1) > 0).sum()\n        return n_visited_states",
  "def get_entropy(self):\n        \"\"\"\n        Returns the entropy of the empirical distribution over states, induced by the state counts.\n        Uses log2.\n        \"\"\"\n        visited = self.N_sa.sum(axis=1) > 0\n        if visited.sum() == 0.0:\n            return 0.0\n        # number of visits of visited states only\n        n_visits = self.N_sa[visited, :].sum(axis=1)\n        # empirical distribution\n        dist = n_visits/n_visits.sum()\n        entropy = (-dist*np.log2(dist)).sum()\n        return entropy",
  "def map_to_representative(state,\n                          lp_metric,\n                          representative_states,\n                          n_representatives,\n                          min_dist,\n                          scaling,\n                          accept_new_repr):\n    \"\"\"\n    Map state to representative state.\n    \"\"\"\n    dist_to_closest = np.inf\n    argmin = -1\n    for ii in range(n_representatives):\n        dist = metric_lp(state, representative_states[ii, :],\n                         lp_metric, scaling)\n        if dist < dist_to_closest:\n            dist_to_closest = dist\n            argmin = ii\n\n    max_representatives = representative_states.shape[0]\n    if dist_to_closest > min_dist \\\n        and n_representatives < max_representatives \\\n            and accept_new_repr:\n        new_index = n_representatives\n        representative_states[new_index, :] = state\n        return new_index, 0.0\n    return argmin, dist_to_closest",
  "class OnlineDiscretizationCounter(UncertaintyEstimator):\n    \"\"\"\n    Note: currently, only implemented for continuous (Box) states and\n    discrete actions.\n\n    Parameters\n    ----------\n    observation_space : spaces.Box\n    action_space : spaces.Discrete\n    lp_metric: int\n        The metric on the state space is the one induced by the p-norm,\n        where p = lp_metric. Default = 2, for the Euclidean metric.\n    scaling: numpy.ndarray\n        Must have the same size as state array, used to scale the states\n        before computing the metric.\n        If None, set to:\n        - (env.observation_space.high - env.observation_space.low) if high\n        and low are bounded\n        - np.ones(env.observation_space.shape[0]) if high or low are\n        unbounded\n    min_dist: double\n        Minimum distance between two representative states\n    max_repr: int\n        Maximum number of representative states.\n        If None, it is set to  (sqrt(d)/min_dist)**d, where d\n        is the dimension of the state space\n    rate_power : float\n        returns bonuses in n^power.\n    \"\"\"\n    def __init__(self,\n                 observation_space,\n                 action_space,\n                 lp_metric=2,\n                 min_dist=0.1,\n                 max_repr=1000,\n                 scaling=None,\n                 rate_power=1,\n                 **kwargs):\n        UncertaintyEstimator.__init__(self, observation_space, action_space)\n\n        assert isinstance(action_space, Discrete)\n        assert isinstance(observation_space, Box)\n\n        self.lp_metric = lp_metric\n        self.min_dist = min_dist\n        self.max_repr = max_repr\n        self.state_dim = self.observation_space.shape[0]\n        self.n_actions = self.action_space.n\n        self.rate_power = rate_power\n\n        # compute scaling, if it is None\n        if scaling is None:\n            # if high and low are bounded\n            if self.observation_space.is_bounded():\n                scaling = self.observation_space.high \\\n                    - self.observation_space.low\n                # if high or low are unbounded\n            else:\n                scaling = np.ones(self.state_dim)\n        else:\n            assert scaling.ndim == 1\n            assert scaling.shape[0] == self.state_dim\n        self.scaling = scaling\n\n        # initialize\n        self.n_representatives = None\n        self.representative_states = None\n        self.N_sa = None\n        self.reset()\n\n    def reset(self):\n        self.n_representatives = 0\n        self.representative_states = np.zeros((self.max_repr, self.state_dim))\n        self.N_sa = np.zeros((self.max_repr, self.n_actions))\n\n        self._overflow_warning = False\n\n    def _get_representative_state(self, state, accept_new_repr=True):\n        state_idx, dist_to_closest \\\n            = map_to_representative(state,\n                                    self.lp_metric,\n                                    self.representative_states,\n                                    self.n_representatives,\n                                    self.min_dist,\n                                    self.scaling,\n                                    accept_new_repr)\n        # check if new representative state\n        if state_idx == self.n_representatives:\n            self.n_representatives += 1\n\n        if self.n_representatives >= self.max_repr \\\n                and (not self._overflow_warning):\n            logger.warning(\"OnlineDiscretizationCounter reached \\\nthe maximum number of representative states.\")\n            self._overflow_warning = True\n\n        return state_idx, dist_to_closest\n\n    @preprocess_args(expected_type='numpy')\n    def update(self, state, action, next_state=None, reward=None, **kwargs):\n        state_idx, _ = self._get_representative_state(state)\n        self.N_sa[state_idx, action] += 1\n\n    @preprocess_args(expected_type='numpy')\n    def measure(self, state, action, **kwargs):\n        n = np.maximum(1.0, self.count(state, action))\n        return np.power(1/n, self.rate_power)\n\n    def count(self, state, action):\n        state_idx, dist_to_closest = self._get_representative_state(\n                                            state,\n                                            accept_new_repr=False)\n        # if state is too far from the closest representative,\n        # its count is zero.\n        if dist_to_closest > self.min_dist:\n            return 0.0\n        return self.N_sa[state_idx, action]\n\n    def get_n_visited_states(self):\n        \"\"\"\n        Returns the number of different states sent to the .update() function.\n        For continuous state spaces, counts the number of different discretized states.\n        \"\"\"\n        n_visited_states = (self.N_sa.sum(axis=1) > 0).sum()\n        return n_visited_states\n\n    def get_entropy(self):\n        \"\"\"\n        Returns the entropy of the empirical distribution over states, induced by the state counts.\n        Uses log2.\n        \"\"\"\n        visited = self.N_sa.sum(axis=1) > 0\n        if visited.sum() == 0.0:\n            return 0.0\n        # number of visits of visited states only\n        n_visits = self.N_sa[visited, :].sum(axis=1)\n        # empirical distribution\n        dist = n_visits/n_visits.sum()\n        entropy = (-dist*np.log2(dist)).sum()\n        return entropy",
  "def __init__(self,\n                 observation_space,\n                 action_space,\n                 lp_metric=2,\n                 min_dist=0.1,\n                 max_repr=1000,\n                 scaling=None,\n                 rate_power=1,\n                 **kwargs):\n        UncertaintyEstimator.__init__(self, observation_space, action_space)\n\n        assert isinstance(action_space, Discrete)\n        assert isinstance(observation_space, Box)\n\n        self.lp_metric = lp_metric\n        self.min_dist = min_dist\n        self.max_repr = max_repr\n        self.state_dim = self.observation_space.shape[0]\n        self.n_actions = self.action_space.n\n        self.rate_power = rate_power\n\n        # compute scaling, if it is None\n        if scaling is None:\n            # if high and low are bounded\n            if self.observation_space.is_bounded():\n                scaling = self.observation_space.high \\\n                    - self.observation_space.low\n                # if high or low are unbounded\n            else:\n                scaling = np.ones(self.state_dim)\n        else:\n            assert scaling.ndim == 1\n            assert scaling.shape[0] == self.state_dim\n        self.scaling = scaling\n\n        # initialize\n        self.n_representatives = None\n        self.representative_states = None\n        self.N_sa = None\n        self.reset()",
  "def reset(self):\n        self.n_representatives = 0\n        self.representative_states = np.zeros((self.max_repr, self.state_dim))\n        self.N_sa = np.zeros((self.max_repr, self.n_actions))\n\n        self._overflow_warning = False",
  "def _get_representative_state(self, state, accept_new_repr=True):\n        state_idx, dist_to_closest \\\n            = map_to_representative(state,\n                                    self.lp_metric,\n                                    self.representative_states,\n                                    self.n_representatives,\n                                    self.min_dist,\n                                    self.scaling,\n                                    accept_new_repr)\n        # check if new representative state\n        if state_idx == self.n_representatives:\n            self.n_representatives += 1\n\n        if self.n_representatives >= self.max_repr \\\n                and (not self._overflow_warning):\n            logger.warning(\"OnlineDiscretizationCounter reached \\\nthe maximum number of representative states.\")\n            self._overflow_warning = True\n\n        return state_idx, dist_to_closest",
  "def update(self, state, action, next_state=None, reward=None, **kwargs):\n        state_idx, _ = self._get_representative_state(state)\n        self.N_sa[state_idx, action] += 1",
  "def measure(self, state, action, **kwargs):\n        n = np.maximum(1.0, self.count(state, action))\n        return np.power(1/n, self.rate_power)",
  "def count(self, state, action):\n        state_idx, dist_to_closest = self._get_representative_state(\n                                            state,\n                                            accept_new_repr=False)\n        # if state is too far from the closest representative,\n        # its count is zero.\n        if dist_to_closest > self.min_dist:\n            return 0.0\n        return self.N_sa[state_idx, action]",
  "def get_n_visited_states(self):\n        \"\"\"\n        Returns the number of different states sent to the .update() function.\n        For continuous state spaces, counts the number of different discretized states.\n        \"\"\"\n        n_visited_states = (self.N_sa.sum(axis=1) > 0).sum()\n        return n_visited_states",
  "def get_entropy(self):\n        \"\"\"\n        Returns the entropy of the empirical distribution over states, induced by the state counts.\n        Uses log2.\n        \"\"\"\n        visited = self.N_sa.sum(axis=1) > 0\n        if visited.sum() == 0.0:\n            return 0.0\n        # number of visits of visited states only\n        n_visits = self.N_sa[visited, :].sum(axis=1)\n        # empirical distribution\n        dist = n_visits/n_visits.sum()\n        entropy = (-dist*np.log2(dist)).sum()\n        return entropy",
  "class UncertaintyEstimator(ABC):\n    def __init__(self, observation_space, action_space, **kwargs):\n        super().__init__()\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    def reset(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def update(self, state, action, next_state, reward, **kwargs):\n        pass\n\n    @abstractmethod\n    def measure(self, state, action, **kwargs):\n        pass\n\n    def measure_batch(self, states, actions, **kwargs):\n        batch = [self.measure(s, a, **kwargs) for s, a in zip(states, actions)]\n        if _get_type(batch[0]) == 'torch':\n            import torch\n            return torch.FloatTensor(batch)\n        return np.array(batch)\n\n    def measure_batch_all_actions(self, states):\n        return np.array([[self.measure(s, a) for a in range(self.action_space.n)] for s in states])",
  "def __init__(self, observation_space, action_space, **kwargs):\n        super().__init__()\n        self.observation_space = observation_space\n        self.action_space = action_space",
  "def reset(self, **kwargs):\n        pass",
  "def update(self, state, action, next_state, reward, **kwargs):\n        pass",
  "def measure(self, state, action, **kwargs):\n        pass",
  "def measure_batch(self, states, actions, **kwargs):\n        batch = [self.measure(s, a, **kwargs) for s, a in zip(states, actions)]\n        if _get_type(batch[0]) == 'torch':\n            import torch\n            return torch.FloatTensor(batch)\n        return np.array(batch)",
  "def measure_batch_all_actions(self, states):\n        return np.array([[self.measure(s, a) for a in range(self.action_space.n)] for s in states])",
  "def _get_type(arg):\n    if _TORCH_INSTALLED and isinstance(arg, torch.Tensor):\n        return 'torch'\n    elif isinstance(arg, np.ndarray):\n        return 'numpy'\n    else:\n        return type(arg)",
  "def process_type(arg, expected_type):\n    \"\"\"\n    Utility function to preprocess numpy/torch arguments,\n    according to a expected type.\n\n    For instance, if arg is numpy and expected_type is torch,\n    converts arg to torch.tensor.\n\n    Parameters\n    ----------\n    expected_type: {'torch', 'numpy'}\n        Desired type for output.\n    \"\"\"\n    if arg is None:\n        return None\n\n    if expected_type == 'torch':\n        assert _TORCH_INSTALLED, \"expected_type is 'torch', but torch is not installed!\"\n        if isinstance(arg, torch.Tensor):\n            return arg\n        elif isinstance(arg, np.ndarray):\n            return torch.from_numpy(arg)\n        elif np.issubdtype(type(arg), np.number):\n            return torch.tensor(arg)\n        else:\n            return arg\n    elif expected_type == 'numpy':\n        if isinstance(arg, np.ndarray):\n            return arg\n        elif isinstance(arg, torch.Tensor):\n            return arg.detach().cpu().numpy()\n        else:\n            return arg\n    else:\n        return arg",
  "def preprocess_args(expected_type):\n    \"\"\"\n    Utility decorator for methods to preprocess numpy/torch arguments,\n    according to an expected type.\n\n    Output type = input type of the first argument.\n\n    For instance, if function args are numpy and expected_type is torch,\n    converts function args to torch.tensor.\n\n    Parameters\n    ----------\n    expected_type: {'torch', 'numpy'}\n        Desired type for output.\n    \"\"\"\n    def decorator(func):\n        def inner(self, *args, **kwargs):\n            processed_args = ()\n            for ii, arg in enumerate(args):\n                processed_args += (process_type(arg, expected_type),)\n            output = func(self, *processed_args, **kwargs)\n            # Process output according to first argument\n            ouput_expected_type = _get_type(args[0])\n            processed_output = process_type(output, ouput_expected_type)\n            return processed_output\n        return inner\n    return decorator",
  "def decorator(func):\n        def inner(self, *args, **kwargs):\n            processed_args = ()\n            for ii, arg in enumerate(args):\n                processed_args += (process_type(arg, expected_type),)\n            output = func(self, *processed_args, **kwargs)\n            # Process output according to first argument\n            ouput_expected_type = _get_type(args[0])\n            processed_output = process_type(output, ouput_expected_type)\n            return processed_output\n        return inner",
  "def inner(self, *args, **kwargs):\n            processed_args = ()\n            for ii, arg in enumerate(args):\n                processed_args += (process_type(arg, expected_type),)\n            output = func(self, *processed_args, **kwargs)\n            # Process output according to first argument\n            ouput_expected_type = _get_type(args[0])\n            processed_output = process_type(output, ouput_expected_type)\n            return processed_output",
  "def get_network(shape, embedding_dim):\n    if len(shape) == 3:\n        if shape[2] < shape[0] and shape[2] < shape[1]:\n            W, H, C = shape\n            transpose_obs = True\n        elif shape[0] < shape[1] and shape[0] < shape[2]:\n            C, H, W = shape\n            transpose_obs = False\n        else:\n            raise ValueError(\"Unknown image convention\")\n\n        return ConvolutionalNetwork(in_channels=C,\n                                    in_width=W,\n                                    in_height=H,\n                                    out_size=embedding_dim,\n                                    activation=\"ELU\",\n                                    transpose_obs=transpose_obs,\n                                    is_policy=False)\n    elif len(shape) == 2:\n        H, W = shape\n        return ConvolutionalNetwork(in_channels=1,\n                                    in_width=W,\n                                    in_height=H,\n                                    activation=\"ELU\",\n                                    out_size=embedding_dim)\n\n    elif len(shape) == 1:\n        return MultiLayerPerceptron(in_size=shape[0],\n                                    activation=\"RELU\",\n                                    layer_sizes=[64, 64],\n                                    out_size=embedding_dim)\n    else:\n        raise ValueError(\"Incompatible observation shape: {}\"\n                         .format(shape))",
  "class RandomNetworkDistillation(UncertaintyEstimator):\n    \"\"\"\n    References\n    ----------\n    Burda Yuri, Harrison Edwards, Amos Storkey, and Oleg Klimov. 2018.\n    \"Exploration by random network distillation.\"\n    In International Conference on Learning Representations.\n    \"\"\"\n\n    def __init__(self,\n                 observation_space,\n                 action_space,\n                 learning_rate=0.001,\n                 update_period=100,\n                 embedding_dim=10,\n                 net_fn=None,\n                 net_kwargs=None,\n                 device=\"cuda:best\",\n                 rate_power=0.5,\n                 batch_size=10,\n                 memory_size=10000,\n                 with_action=False,\n                 **kwargs):\n        assert isinstance(observation_space, spaces.Box)\n        UncertaintyEstimator.__init__(self, observation_space, action_space)\n        self.learning_rate = learning_rate\n        self.loss_fn = F.mse_loss\n        self.update_period = update_period\n        self.embedding_dim = embedding_dim\n        out_size = embedding_dim * action_space.n if with_action else embedding_dim\n        self.net_fn = load(net_fn) if isinstance(net_fn, str) else \\\n            net_fn or partial(get_network, shape=observation_space.shape, embedding_dim=out_size)\n        self.net_kwargs = net_kwargs or {}\n        if \"out_size\" in self.net_kwargs:\n            self.net_kwargs[\"out_size\"] = out_size\n        self.device = choose_device(device)\n        self.rate_power = rate_power\n        self.batch_size = batch_size\n        self.memory = ReplayMemory(capacity=memory_size)\n        self.with_action = with_action\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.random_target_network = self.net_fn(**self.net_kwargs).to(self.device)\n        self.predictor_network = self.net_fn(**self.net_kwargs).to(self.device)\n        self.rnd_optimizer = torch.optim.Adam(\n                                self.predictor_network.parameters(),\n                                lr=self.learning_rate,\n                                betas=(0.9, 0.999))\n\n        self.count = 0\n        self.loss = torch.tensor(0.0).to(self.device)\n\n    def _get_embeddings(self, state, action=None, batch=False, all_actions=False):\n        state = state.to(self.device)\n        if not batch:\n            state = state.unsqueeze(0)\n\n        random_embedding = self.random_target_network(state)\n        predicted_embedding = self.predictor_network(state)\n\n        if self.with_action:\n            random_embedding = random_embedding.view((state.shape[0], self.action_space.n, -1))\n            predicted_embedding = predicted_embedding.view((state.shape[0], self.action_space.n, -1))\n            if not all_actions:\n                action = action.long().to(self.device)\n                if not batch:\n                    action = action.unsqueeze(0)\n                action = action.unsqueeze(1).repeat(1, random_embedding.shape[-1]).unsqueeze(1)\n                random_embedding = random_embedding.gather(1, action).squeeze(1)\n                predicted_embedding = predicted_embedding.gather(1, action).squeeze(1)\n        return random_embedding, predicted_embedding\n\n    @preprocess_args(expected_type='torch')\n    def update(self,\n               state,\n               action=None,\n               next_state=None,\n               reward=None,\n               **kwargs):\n\n        batch = [(state, action)]\n        if self.batch_size > 0 and not self.memory.is_empty():\n            batch += self.memory.sample(self.batch_size)\n            self.memory.push((state, action))\n        states, actions = zip(*batch)\n        states = torch.stack(states)\n        if self.with_action:\n            actions = torch.stack(actions)\n\n        random_embedding, predicted_embedding = self._get_embeddings(states, actions, batch=True)\n\n        self.loss += self.loss_fn(random_embedding.detach(),\n                                  predicted_embedding)\n\n        self.count += 1\n        if self.count % self.update_period == 0:\n            self.loss /= self.update_period\n            self.rnd_optimizer.zero_grad()\n            self.loss.backward()\n            self.rnd_optimizer.step()\n            self.loss = torch.tensor(0.0).to(self.device)\n\n    @preprocess_args(expected_type='torch')\n    def measure(self, state, action=None, **kwargs):\n        random_embedding, predicted_embedding = self._get_embeddings(state, action, batch=False)\n        error = torch.norm(predicted_embedding.detach() - random_embedding.detach(), p=2, dim=-1)\n        return error.pow(2 * self.rate_power).item()\n\n    @preprocess_args(expected_type='torch')\n    def measure_batch(self, states, actions, **kwargs):\n        random_embedding, predicted_embedding = self._get_embeddings(states, actions, batch=True)\n        error = torch.norm(predicted_embedding.detach() - random_embedding.detach(), p=2, dim=-1)\n        return error.pow(2 * self.rate_power)\n\n    @preprocess_args(expected_type='torch')\n    def measure_batch_all_actions(self, states, **kwargs):\n        \"\"\"\n        Measure N(s,a) for all a in A.\n\n        Parameters\n        ----------\n        states: a batch of states, of shape [B x <state_shape>]\n\n        Returns\n        -------\n        N(s,a): an array of shape B x A\n        \"\"\"\n        assert self.with_action\n        random_embedding, predicted_embedding = self._get_embeddings(states, None, batch=True, all_actions=True)\n        error = torch.norm(predicted_embedding.detach() - random_embedding.detach(), p=2, dim=-1)\n        return error.pow(2 * self.rate_power)",
  "def __init__(self,\n                 observation_space,\n                 action_space,\n                 learning_rate=0.001,\n                 update_period=100,\n                 embedding_dim=10,\n                 net_fn=None,\n                 net_kwargs=None,\n                 device=\"cuda:best\",\n                 rate_power=0.5,\n                 batch_size=10,\n                 memory_size=10000,\n                 with_action=False,\n                 **kwargs):\n        assert isinstance(observation_space, spaces.Box)\n        UncertaintyEstimator.__init__(self, observation_space, action_space)\n        self.learning_rate = learning_rate\n        self.loss_fn = F.mse_loss\n        self.update_period = update_period\n        self.embedding_dim = embedding_dim\n        out_size = embedding_dim * action_space.n if with_action else embedding_dim\n        self.net_fn = load(net_fn) if isinstance(net_fn, str) else \\\n            net_fn or partial(get_network, shape=observation_space.shape, embedding_dim=out_size)\n        self.net_kwargs = net_kwargs or {}\n        if \"out_size\" in self.net_kwargs:\n            self.net_kwargs[\"out_size\"] = out_size\n        self.device = choose_device(device)\n        self.rate_power = rate_power\n        self.batch_size = batch_size\n        self.memory = ReplayMemory(capacity=memory_size)\n        self.with_action = with_action\n        self.reset()",
  "def reset(self, **kwargs):\n        self.random_target_network = self.net_fn(**self.net_kwargs).to(self.device)\n        self.predictor_network = self.net_fn(**self.net_kwargs).to(self.device)\n        self.rnd_optimizer = torch.optim.Adam(\n                                self.predictor_network.parameters(),\n                                lr=self.learning_rate,\n                                betas=(0.9, 0.999))\n\n        self.count = 0\n        self.loss = torch.tensor(0.0).to(self.device)",
  "def _get_embeddings(self, state, action=None, batch=False, all_actions=False):\n        state = state.to(self.device)\n        if not batch:\n            state = state.unsqueeze(0)\n\n        random_embedding = self.random_target_network(state)\n        predicted_embedding = self.predictor_network(state)\n\n        if self.with_action:\n            random_embedding = random_embedding.view((state.shape[0], self.action_space.n, -1))\n            predicted_embedding = predicted_embedding.view((state.shape[0], self.action_space.n, -1))\n            if not all_actions:\n                action = action.long().to(self.device)\n                if not batch:\n                    action = action.unsqueeze(0)\n                action = action.unsqueeze(1).repeat(1, random_embedding.shape[-1]).unsqueeze(1)\n                random_embedding = random_embedding.gather(1, action).squeeze(1)\n                predicted_embedding = predicted_embedding.gather(1, action).squeeze(1)\n        return random_embedding, predicted_embedding",
  "def update(self,\n               state,\n               action=None,\n               next_state=None,\n               reward=None,\n               **kwargs):\n\n        batch = [(state, action)]\n        if self.batch_size > 0 and not self.memory.is_empty():\n            batch += self.memory.sample(self.batch_size)\n            self.memory.push((state, action))\n        states, actions = zip(*batch)\n        states = torch.stack(states)\n        if self.with_action:\n            actions = torch.stack(actions)\n\n        random_embedding, predicted_embedding = self._get_embeddings(states, actions, batch=True)\n\n        self.loss += self.loss_fn(random_embedding.detach(),\n                                  predicted_embedding)\n\n        self.count += 1\n        if self.count % self.update_period == 0:\n            self.loss /= self.update_period\n            self.rnd_optimizer.zero_grad()\n            self.loss.backward()\n            self.rnd_optimizer.step()\n            self.loss = torch.tensor(0.0).to(self.device)",
  "def measure(self, state, action=None, **kwargs):\n        random_embedding, predicted_embedding = self._get_embeddings(state, action, batch=False)\n        error = torch.norm(predicted_embedding.detach() - random_embedding.detach(), p=2, dim=-1)\n        return error.pow(2 * self.rate_power).item()",
  "def measure_batch(self, states, actions, **kwargs):\n        random_embedding, predicted_embedding = self._get_embeddings(states, actions, batch=True)\n        error = torch.norm(predicted_embedding.detach() - random_embedding.detach(), p=2, dim=-1)\n        return error.pow(2 * self.rate_power)",
  "def measure_batch_all_actions(self, states, **kwargs):\n        \"\"\"\n        Measure N(s,a) for all a in A.\n\n        Parameters\n        ----------\n        states: a batch of states, of shape [B x <state_shape>]\n\n        Returns\n        -------\n        N(s,a): an array of shape B x A\n        \"\"\"\n        assert self.with_action\n        random_embedding, predicted_embedding = self._get_embeddings(states, None, batch=True, all_actions=True)\n        error = torch.norm(predicted_embedding.detach() - random_embedding.detach(), p=2, dim=-1)\n        return error.pow(2 * self.rate_power)",
  "def show_video(filename=None, directory='./videos'):\n    \"\"\"\n    Either show all videos in a directory (if filename is None) or\n    show video corresponding to filename.\n    \"\"\"\n    html = []\n    if filename is not None:\n        files = Path('./').glob(filename)\n    else:\n        files = Path(directory).glob(\"*.mp4\")\n    for mp4 in files:\n        video_b64 = base64.b64encode(mp4.read_bytes())\n        html.append('''<video alt=\"{}\" autoplay\n                      loop controls style=\"height: 400px;\">\n                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n                 </video>'''.format(mp4, video_b64.decode('ascii')))\n    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))",
  "class IncrementalAgent(Agent):\n    \"\"\"Basic interface for agents that can be trained incrementally.\"\"\"\n\n    name = \"\"\n\n    def __init__(self, env, **kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        env : Model\n            Environment used to fit the agent.\n        \"\"\"\n        Agent.__init__(self, env, **kwargs)\n\n    def fit(self, **kwargs):\n        return self.partial_fit(1.0, **kwargs)\n\n    @abstractmethod\n    def partial_fit(self, fraction, **kwargs):\n        \"\"\"Partially fits the agent, according to the fraction parameter.\n\n        For instance, if the agent requires N episodes for a \"full\" fit,\n        calling partial_fit(0.5) will fit the agent for 0.5*N episodes.\n\n        Also, calling partial_fit(0.5) twice must be equivalent to\n        a single call to fit().\n\n        Parameters\n        ---------\n        fraction: double, in [0,1]\n            Fraction of the agent to fit.\n\n        Returns\n        -------\n        info : dict\n        \"\"\"\n        raise NotImplementedError(\"agent.partial_fit() not implemented.\")",
  "def __init__(self, env, **kwargs):\n        \"\"\"\n\n        Parameters\n        ----------\n        env : Model\n            Environment used to fit the agent.\n        \"\"\"\n        Agent.__init__(self, env, **kwargs)",
  "def fit(self, **kwargs):\n        return self.partial_fit(1.0, **kwargs)",
  "def partial_fit(self, fraction, **kwargs):\n        \"\"\"Partially fits the agent, according to the fraction parameter.\n\n        For instance, if the agent requires N episodes for a \"full\" fit,\n        calling partial_fit(0.5) will fit the agent for 0.5*N episodes.\n\n        Also, calling partial_fit(0.5) twice must be equivalent to\n        a single call to fit().\n\n        Parameters\n        ---------\n        fraction: double, in [0,1]\n            Fraction of the agent to fit.\n\n        Returns\n        -------\n        info : dict\n        \"\"\"\n        raise NotImplementedError(\"agent.partial_fit() not implemented.\")",
  "class Agent(ABC):\n    \"\"\" Basic interface for agents.\n\n    Parameters\n    ----------\n    env : Model\n        Environment used to fit the agent.\n    copy_env : bool\n        If true, makes a deep copy of the environment.\n    reseed_env : bool\n        If true, reseeds the environment.\n\n\n    .. note::\n        Classes that implement this interface should send ``**kwargs`` to :code:`Agent.__init__()`\n\n\n    Attributes\n    ----------\n    name : string\n        Agent identifier\n    env : Model\n        Environment on which to train the agent.\n    writer : object, default: None\n        Writer object (e.g. tensorboard SummaryWriter).\n    \"\"\"\n\n    name = \"\"\n\n    def __init__(self,\n                 env,\n                 copy_env=True,\n                 reseed_env=True,\n                 **kwargs):\n        # Check if wrong parameters have been sent to an agent.\n        assert kwargs == {}, \\\n            'Unknown parameters sent to agent:' + str(kwargs.keys())\n\n        self.env = env\n\n        if copy_env:\n            try:\n                self.env = deepcopy(env)\n            except Exception as ex:\n                logger.warning(\"[Agent] Not possible to deepcopy env: \" + str(ex))\n\n        if reseed_env:\n            reseeded = seeding.safe_reseed(self.env)\n            if not reseeded:\n                logger.warning(\"[Agent] Not possible to reseed env, seed() and reseed() are not available.\")\n\n        self.writer = None\n\n    @abstractmethod\n    def fit(self, **kwargs):\n        \"\"\"Train the agent using the provided environment.\n\n        Returns\n        -------\n        info: dict\n            Dictionary with useful info.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def policy(self, observation, **kwargs):\n        \"\"\"Returns an action, given an observation.\"\"\"\n        pass\n\n    def reset(self, **kwargs):\n        \"\"\"Put the agent in default setup.\"\"\"\n        pass\n\n    def save(self, filename, **kwargs):\n        \"\"\"Save agent object.\"\"\"\n        raise NotImplementedError(\"agent.save() not implemented.\")\n\n    def load(self, filename, **kwargs):\n        \"\"\"Load agent object.\"\"\"\n        raise NotImplementedError(\"agent.load() not implemented.\")\n\n    def set_writer(self, writer):\n        self.writer = writer\n\n        if self.writer:\n            init_args = signature(self.__init__).parameters\n            kwargs = [f\"| {key} | {getattr(self, key, None)} |\" for key in init_args]\n            writer.add_text(\n                \"Hyperparameters\",\n                \"| Parameter | Value |\\n|-------|-------|\\n\" + \"\\n\".join(kwargs),\n            )\n\n    @classmethod\n    def sample_parameters(cls, trial):\n        \"\"\"\n        Sample hyperparameters for hyperparam optimization using\n        Optuna (https://optuna.org/)\n\n        Note: only the kwargs sent to __init__ are optimized. Make sure to\n        include in the Agent constructor all \"optimizable\" parameters.\n\n        Parameters\n        ----------\n        trial: optuna.trial\n        \"\"\"\n        raise NotImplementedError(\"agent.sample_parameters() not implemented.\")",
  "def __init__(self,\n                 env,\n                 copy_env=True,\n                 reseed_env=True,\n                 **kwargs):\n        # Check if wrong parameters have been sent to an agent.\n        assert kwargs == {}, \\\n            'Unknown parameters sent to agent:' + str(kwargs.keys())\n\n        self.env = env\n\n        if copy_env:\n            try:\n                self.env = deepcopy(env)\n            except Exception as ex:\n                logger.warning(\"[Agent] Not possible to deepcopy env: \" + str(ex))\n\n        if reseed_env:\n            reseeded = seeding.safe_reseed(self.env)\n            if not reseeded:\n                logger.warning(\"[Agent] Not possible to reseed env, seed() and reseed() are not available.\")\n\n        self.writer = None",
  "def fit(self, **kwargs):\n        \"\"\"Train the agent using the provided environment.\n\n        Returns\n        -------\n        info: dict\n            Dictionary with useful info.\n        \"\"\"\n        pass",
  "def policy(self, observation, **kwargs):\n        \"\"\"Returns an action, given an observation.\"\"\"\n        pass",
  "def reset(self, **kwargs):\n        \"\"\"Put the agent in default setup.\"\"\"\n        pass",
  "def save(self, filename, **kwargs):\n        \"\"\"Save agent object.\"\"\"\n        raise NotImplementedError(\"agent.save() not implemented.\")",
  "def load(self, filename, **kwargs):\n        \"\"\"Load agent object.\"\"\"\n        raise NotImplementedError(\"agent.load() not implemented.\")",
  "def set_writer(self, writer):\n        self.writer = writer\n\n        if self.writer:\n            init_args = signature(self.__init__).parameters\n            kwargs = [f\"| {key} | {getattr(self, key, None)} |\" for key in init_args]\n            writer.add_text(\n                \"Hyperparameters\",\n                \"| Parameter | Value |\\n|-------|-------|\\n\" + \"\\n\".join(kwargs),\n            )",
  "def sample_parameters(cls, trial):\n        \"\"\"\n        Sample hyperparameters for hyperparam optimization using\n        Optuna (https://optuna.org/)\n\n        Note: only the kwargs sent to __init__ are optimized. Make sure to\n        include in the Agent constructor all \"optimizable\" parameters.\n\n        Parameters\n        ----------\n        trial: optuna.trial\n        \"\"\"\n        raise NotImplementedError(\"agent.sample_parameters() not implemented.\")",
  "class FeatureMap(ABC):\n    \"\"\"\n    Class representing a feature map, from (observation, action) pairs\n    to numpy arrays.\n\n    Attributes\n    ----------\n    shape : tuple\n        Shape of feature array.\n\n    Methods\n    --------\n    map()\n        Maps a (observation, action) pair to a numpy array.\n    \"\"\"\n    def __init__(self):\n        ABC.__init__(self)\n        self.shape = ()\n\n    @abstractmethod\n    def map(self, observation, action):\n        \"\"\"\n        Maps a (observation, action) pair to a numpy array.\n        \"\"\"\n        pass",
  "def __init__(self):\n        ABC.__init__(self)\n        self.shape = ()",
  "def map(self, observation, action):\n        \"\"\"\n        Maps a (observation, action) pair to a numpy array.\n        \"\"\"\n        pass",
  "class AVECPPOAgent(IncrementalAgent):\n    \"\"\"\n    AVEC uses a modification of the training objective for the critic in\n    actor-critic algorithms to better approximate the value function (critic).\n    The new state-value function approximation learns the *relative* value of\n    the states rather than their *absolute* value as in conventional\n    actor-critic. This modification is:\n    - well-motivated by recent studies [1,2];\n    - theoretically sound;\n    - intuitively supported by the need to improve the approximation error\n    of the critic.\n\n    The application of Actor with Variance Estimated Critic (AVEC) to\n    state-of-the-art policy gradient methods produces considerable\n    gains in performance (on average +26% for SAC and +40% for PPO)\n    over the standard actor-critic training.\n\n    Parameters\n    ----------\n    env : Model\n        model with continuous (Box) state space and discrete actions\n    n_episodes : int\n        Number of episodes\n    batch_size : int\n        Number of episodes to wait before updating the policy.\n    horizon : int\n        Horizon of the objective function. If None and gamma<1,\n        set to 1/(1-gamma).\n    gamma : double\n        Discount factor in [0, 1]. If gamma is 1.0, the problem is set\n        to be finite-horizon.\n    entr_coef : double\n        Entropy coefficient.\n    vf_coef : double\n        Value function loss coefficient.\n    learning_rate : double\n        Learning rate.\n    optimizer_type: str\n        Type of optimizer. 'ADAM' by defaut.\n    eps_clip : double\n        PPO clipping range (epsilon).\n    k_epochs : int\n        Number of epochs per update.\n    policy_net_fn : function(env, **kwargs)\n        Function that returns an instance of a policy network (pytorch).\n        If None, a default net is used.\n    value_net_fn : function(env, **kwargs)\n        Function that returns an instance of a value network (pytorch).\n        If None, a default net is used.\n    policy_net_kwargs : dict\n        kwargs for policy_net_fn\n    value_net_kwargs : dict\n        kwargs for value_net_fn\n    use_bonus : bool, default = False\n        If true, compute an 'exploration_bonus' and add it to the reward.\n        See also UncertaintyEstimatorWrapper.\n    uncertainty_estimator_kwargs : dict\n        Arguments for the UncertaintyEstimatorWrapper\n    device : str\n        Device to put the tensors on\n\n    References\n    ----------\n    Flet-Berliac, Y., Ouhamma, R., Maillard, O. A., & Preux, P. (2021).\n    \"Is Standard Deviation the New Standard? Revisiting the Critic in Deep\n    Policy Gradients.\"\n    In International Conference on Learning Representations.\n\n    [1] Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos, F.,\n    Rudolph, L. & Madry, A. (2020).\n    \"A closer look at deep policy gradients.\"\n    In International Conference on Learning Representations.\n\n    [2] Tucker, G., Bhupatiraju, S., Gu, S., Turner, R., Ghahramani, Z. &\n    Levine, S. (2018).\n    \"The mirage of action-dependent baselines in reinforcement learning.\"\n    In International Conference on Machine Learning, pp. 5015\u20135024.\n    \"\"\"\n\n    name = \"AVECPPO\"\n\n    def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 vf_coef=0.,\n                 avec_coef=1.,\n                 learning_rate=0.0003,\n                 optimizer_type='ADAM',\n                 eps_clip=0.2,\n                 k_epochs=10,\n                 policy_net_fn=None,\n                 value_net_fn=None,\n                 policy_net_kwargs=None,\n                 value_net_kwargs=None,\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 device=\"cuda:best\",\n                 **kwargs):\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env,\n                                              **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.learning_rate = learning_rate\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.vf_coef = vf_coef\n        self.avec_coef = avec_coef\n        self.eps_clip = eps_clip\n        self.k_epochs = k_epochs\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.device = choose_device(device)\n\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.value_net_kwargs = value_net_kwargs or {}\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.value_net_fn = value_net_fn or default_value_net_fn\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.cat_policy = None  # categorical policy function\n\n        # initialize\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.cat_policy = self.policy_net_fn(\n                            self.env,\n                            **self.policy_net_kwargs\n                            ).to(self.device)\n        self.policy_optimizer = optimizer_factory(\n                                    self.cat_policy.parameters(),\n                                    **self.optimizer_kwargs)\n\n        self.value_net = self.value_net_fn(\n                            self.env,\n                            **self.value_net_kwargs\n                            ).to(self.device)\n        self.value_optimizer = optimizer_factory(\n                                    self.value_net.parameters(),\n                                    **self.optimizer_kwargs)\n\n        self.cat_policy_old = self.policy_net_fn(\n                                self.env,\n                                **self.policy_net_kwargs\n                                ).to(self.device)\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n        self.MseLoss = nn.MSELoss()\n\n        self.memory = Memory()\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, state, **kwargs):\n        assert self.cat_policy is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample().item()\n\n        return action\n\n    def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction * self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info\n\n    def _select_action(self, state):\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample()\n        action_logprob = action_dist.log_prob(action)\n\n        self.memory.states.append(state)\n        self.memory.actions.append(action)\n        self.memory.logprobs.append(action_logprob)\n\n        return action.item()\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # running policy_old\n            action = self._select_action(state)\n            next_state, reward, done, info = self.env.step(action)\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save in batch\n            self.memory.rewards.append(reward+bonus)  # add bonus here\n            self.memory.is_terminals.append(done)\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # update\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        #\n        if self.episode % self.batch_size == 0:\n            self._update()\n            self.memory.clear_memory()\n\n        return episode_rewards\n\n    def _update(self):\n        # monte carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.memory.rewards),\n                                       reversed(self.memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # normalizing the rewards\n        rewards = torch.tensor(rewards).to(self.device).float()\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n        # convert list to tensor\n        old_states = torch.stack(self.memory.states).to(self.device).detach()\n        old_actions = torch.stack(self.memory.actions).to(self.device).detach()\n        old_logprobs = torch.stack(self.memory.logprobs).to(self.device).detach()\n\n        # optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # evaluate old actions and values\n            action_dist = self.cat_policy(old_states)\n            logprobs = action_dist.log_prob(old_actions)\n            state_values = torch.squeeze(self.value_net(old_states))\n            dist_entropy = action_dist.entropy()\n\n            # find ratio (pi_theta / pi_theta__old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # normalize the advantages\n            advantages = rewards - state_values.detach()\n            advantages = (advantages - advantages.mean()) / \\\n                         (advantages.std() + 1e-8)\n            # find surrogate loss\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1\n                                + self.eps_clip) * advantages\n            loss = -torch.min(surr1, surr2) \\\n                + self.avec_coef * self._avec_loss(state_values, rewards) \\\n                + self.vf_coef * self.MseLoss(state_values, rewards) \\\n                - self.entr_coef * dist_entropy\n\n            # take gradient step\n            self.policy_optimizer.zero_grad()\n            self.value_optimizer.zero_grad()\n\n            loss.mean().backward()\n\n            self.policy_optimizer.step()\n            self.value_optimizer.step()\n\n        # copy new weights into old policy\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n    def _avec_loss(self, y_pred, y_true):\n        \"\"\"\n        Computes the objective function used in AVEC for the learning\n        of the value function:\n        the residual variance between the state-values and the\n        empirical returns.\n\n        Returns Var[y-ypred]\n        :param y_pred: (np.ndarray) the prediction\n        :param y_true: (np.ndarray) the expected value\n        :return: (float) residual variance of ypred and y\n        \"\"\"\n        assert y_true.ndim == 1 and y_pred.ndim == 1\n\n        return torch.var(y_true - y_pred)\n\n    #\n    # For hyperparameter optimization\n    #\n    @classmethod\n    def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        eps_clip = trial.suggest_categorical('eps_clip',\n                                             [0.1, 0.2, 0.3])\n\n        k_epochs = trial.suggest_categorical('k_epochs',\n                                             [1, 5, 10, 20])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'eps_clip': eps_clip,\n                'k_epochs': k_epochs,\n                }",
  "def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 vf_coef=0.,\n                 avec_coef=1.,\n                 learning_rate=0.0003,\n                 optimizer_type='ADAM',\n                 eps_clip=0.2,\n                 k_epochs=10,\n                 policy_net_fn=None,\n                 value_net_fn=None,\n                 policy_net_kwargs=None,\n                 value_net_kwargs=None,\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 device=\"cuda:best\",\n                 **kwargs):\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env,\n                                              **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.learning_rate = learning_rate\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.vf_coef = vf_coef\n        self.avec_coef = avec_coef\n        self.eps_clip = eps_clip\n        self.k_epochs = k_epochs\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.device = choose_device(device)\n\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.value_net_kwargs = value_net_kwargs or {}\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.value_net_fn = value_net_fn or default_value_net_fn\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.cat_policy = None  # categorical policy function\n\n        # initialize\n        self.reset()",
  "def reset(self, **kwargs):\n        self.cat_policy = self.policy_net_fn(\n                            self.env,\n                            **self.policy_net_kwargs\n                            ).to(self.device)\n        self.policy_optimizer = optimizer_factory(\n                                    self.cat_policy.parameters(),\n                                    **self.optimizer_kwargs)\n\n        self.value_net = self.value_net_fn(\n                            self.env,\n                            **self.value_net_kwargs\n                            ).to(self.device)\n        self.value_optimizer = optimizer_factory(\n                                    self.value_net.parameters(),\n                                    **self.optimizer_kwargs)\n\n        self.cat_policy_old = self.policy_net_fn(\n                                self.env,\n                                **self.policy_net_kwargs\n                                ).to(self.device)\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n        self.MseLoss = nn.MSELoss()\n\n        self.memory = Memory()\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())",
  "def policy(self, state, **kwargs):\n        assert self.cat_policy is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample().item()\n\n        return action",
  "def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction * self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def _select_action(self, state):\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample()\n        action_logprob = action_dist.log_prob(action)\n\n        self.memory.states.append(state)\n        self.memory.actions.append(action)\n        self.memory.logprobs.append(action_logprob)\n\n        return action.item()",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # running policy_old\n            action = self._select_action(state)\n            next_state, reward, done, info = self.env.step(action)\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save in batch\n            self.memory.rewards.append(reward+bonus)  # add bonus here\n            self.memory.is_terminals.append(done)\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # update\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        #\n        if self.episode % self.batch_size == 0:\n            self._update()\n            self.memory.clear_memory()\n\n        return episode_rewards",
  "def _update(self):\n        # monte carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.memory.rewards),\n                                       reversed(self.memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # normalizing the rewards\n        rewards = torch.tensor(rewards).to(self.device).float()\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n        # convert list to tensor\n        old_states = torch.stack(self.memory.states).to(self.device).detach()\n        old_actions = torch.stack(self.memory.actions).to(self.device).detach()\n        old_logprobs = torch.stack(self.memory.logprobs).to(self.device).detach()\n\n        # optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # evaluate old actions and values\n            action_dist = self.cat_policy(old_states)\n            logprobs = action_dist.log_prob(old_actions)\n            state_values = torch.squeeze(self.value_net(old_states))\n            dist_entropy = action_dist.entropy()\n\n            # find ratio (pi_theta / pi_theta__old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # normalize the advantages\n            advantages = rewards - state_values.detach()\n            advantages = (advantages - advantages.mean()) / \\\n                         (advantages.std() + 1e-8)\n            # find surrogate loss\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1\n                                + self.eps_clip) * advantages\n            loss = -torch.min(surr1, surr2) \\\n                + self.avec_coef * self._avec_loss(state_values, rewards) \\\n                + self.vf_coef * self.MseLoss(state_values, rewards) \\\n                - self.entr_coef * dist_entropy\n\n            # take gradient step\n            self.policy_optimizer.zero_grad()\n            self.value_optimizer.zero_grad()\n\n            loss.mean().backward()\n\n            self.policy_optimizer.step()\n            self.value_optimizer.step()\n\n        # copy new weights into old policy\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())",
  "def _avec_loss(self, y_pred, y_true):\n        \"\"\"\n        Computes the objective function used in AVEC for the learning\n        of the value function:\n        the residual variance between the state-values and the\n        empirical returns.\n\n        Returns Var[y-ypred]\n        :param y_pred: (np.ndarray) the prediction\n        :param y_true: (np.ndarray) the expected value\n        :return: (float) residual variance of ypred and y\n        \"\"\"\n        assert y_true.ndim == 1 and y_pred.ndim == 1\n\n        return torch.var(y_true - y_pred)",
  "def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        eps_clip = trial.suggest_categorical('eps_clip',\n                                             [0.1, 0.2, 0.3])\n\n        k_epochs = trial.suggest_categorical('k_epochs',\n                                             [1, 5, 10, 20])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'eps_clip': eps_clip,\n                'k_epochs': k_epochs,\n                }",
  "class DiscreteDistribution(ABC):\n    def __init__(self, **kwargs):\n        self.np_random = None\n\n    @abstractmethod\n    def get_distribution(self):\n        \"\"\"\n        Returns\n        -------\n        A distribution over actions {action:probability}\n        \"\"\"\n        raise NotImplementedError()\n\n    def sample(self):\n        \"\"\"\n        Returns\n        -------\n        An action sampled from the distribution\n        \"\"\"\n        distribution = self.get_distribution()\n        return self.np_random.choice(\n                    list(distribution.keys()), 1,\n                    p=np.array(list(distribution.values())))[0]\n\n    def seed(self):\n        \"\"\"\n        Seed the policy randomness source\n        \"\"\"\n        self.np_random = seeding.get_rng()\n\n    def set_time(self, time):\n        \"\"\"\n        Set the local time, allowing to schedule the distribution temperature.\n        \"\"\"\n        pass\n\n    def step_time(self):\n        \"\"\"\n        Step the local time, allowing to schedule the distribution temperature.\n        \"\"\"\n        pass",
  "class EpsilonGreedy(DiscreteDistribution):\n    \"\"\"\n    Uniform distribution with probability epsilon, and optimal action with\n    probability 1-epsilon.\n    \"\"\"\n\n    def __init__(self,\n                 action_space,\n                 temperature=1.0,\n                 final_temperature=0.1,\n                 tau=5000,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.action_space = action_space\n        self.temperature = temperature\n        self.final_temperature = final_temperature\n        self.tau = tau\n        if isinstance(self.action_space, spaces.Tuple):\n            self.action_space = self.action_space.spaces[0]\n        if not isinstance(self.action_space, spaces.Discrete):\n            raise TypeError(\"The action space should be discrete\")\n        self.final_temperature = min(self.temperature, self.final_temperature)\n        self.optimal_action = None\n        self.epsilon = 0\n        self.time = 0\n        self.writer = None\n        self.seed()\n\n    def get_distribution(self):\n        distribution = {action: self.epsilon / self.action_space.n\n                        for action in range(self.action_space.n)}\n        distribution[self.optimal_action] += 1 - self.epsilon\n        return distribution\n\n    def update(self, values):\n        \"\"\"\n        Update the action distribution parameters\n\n        Parameters\n        -----------\n        values\n            The state-action values\n        step_time\n            Whether to update epsilon schedule\n        \"\"\"\n        self.optimal_action = np.argmax(values)\n        self.epsilon = self.final_temperature \\\n            + (self.temperature - self.final_temperature) * \\\n            np.exp(- self.time / self.tau)\n        if self.writer:\n            self.writer.add_scalar('exploration/epsilon',\n                                   self.epsilon,\n                                   self.time)\n\n    def step_time(self):\n        self.time += 1\n\n    def set_time(self, time):\n        self.time = time\n\n    def set_writer(self, writer):\n        self.writer = writer",
  "class Greedy(DiscreteDistribution):\n    \"\"\"\n    Always use the optimal action\n    \"\"\"\n\n    def __init__(self, action_space, **kwargs):\n        super().__init__()\n        self.action_space = action_space\n        if isinstance(self.action_space, spaces.Tuple):\n            self.action_space = self.action_space.spaces[0]\n        if not isinstance(self.action_space, spaces.Discrete):\n            raise TypeError(\"The action space should be discrete\")\n        self.values = None\n        self.seed()\n\n    def get_distribution(self):\n        optimal_action = np.argmax(self.values)\n        return {action: 1 if action == optimal_action\n                else 0 for action in range(self.action_space.n)}\n\n    def update(self, values):\n        self.values = values",
  "def exploration_factory(action_space, method=\"EpsilonGreedy\", **kwargs):\n    \"\"\"\n    Handles creation of exploration policies\n\n    Parameters\n    ----------\n    exploration_config : dict\n        Configuration dictionary of the policy, must contain a \"method\" key.\n    action_space : gym.spaces.Space\n        The environment action space\n\n    Returns\n    -------\n    A new exploration policy.\n    \"\"\"\n    if method == 'Greedy':\n        return Greedy(action_space, **kwargs)\n    elif method == 'EpsilonGreedy':\n        return EpsilonGreedy(action_space, **kwargs)\n    else:\n        raise ValueError(\"Unknown exploration method\")",
  "def __init__(self, **kwargs):\n        self.np_random = None",
  "def get_distribution(self):\n        \"\"\"\n        Returns\n        -------\n        A distribution over actions {action:probability}\n        \"\"\"\n        raise NotImplementedError()",
  "def sample(self):\n        \"\"\"\n        Returns\n        -------\n        An action sampled from the distribution\n        \"\"\"\n        distribution = self.get_distribution()\n        return self.np_random.choice(\n                    list(distribution.keys()), 1,\n                    p=np.array(list(distribution.values())))[0]",
  "def seed(self):\n        \"\"\"\n        Seed the policy randomness source\n        \"\"\"\n        self.np_random = seeding.get_rng()",
  "def set_time(self, time):\n        \"\"\"\n        Set the local time, allowing to schedule the distribution temperature.\n        \"\"\"\n        pass",
  "def step_time(self):\n        \"\"\"\n        Step the local time, allowing to schedule the distribution temperature.\n        \"\"\"\n        pass",
  "def __init__(self,\n                 action_space,\n                 temperature=1.0,\n                 final_temperature=0.1,\n                 tau=5000,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.action_space = action_space\n        self.temperature = temperature\n        self.final_temperature = final_temperature\n        self.tau = tau\n        if isinstance(self.action_space, spaces.Tuple):\n            self.action_space = self.action_space.spaces[0]\n        if not isinstance(self.action_space, spaces.Discrete):\n            raise TypeError(\"The action space should be discrete\")\n        self.final_temperature = min(self.temperature, self.final_temperature)\n        self.optimal_action = None\n        self.epsilon = 0\n        self.time = 0\n        self.writer = None\n        self.seed()",
  "def get_distribution(self):\n        distribution = {action: self.epsilon / self.action_space.n\n                        for action in range(self.action_space.n)}\n        distribution[self.optimal_action] += 1 - self.epsilon\n        return distribution",
  "def update(self, values):\n        \"\"\"\n        Update the action distribution parameters\n\n        Parameters\n        -----------\n        values\n            The state-action values\n        step_time\n            Whether to update epsilon schedule\n        \"\"\"\n        self.optimal_action = np.argmax(values)\n        self.epsilon = self.final_temperature \\\n            + (self.temperature - self.final_temperature) * \\\n            np.exp(- self.time / self.tau)\n        if self.writer:\n            self.writer.add_scalar('exploration/epsilon',\n                                   self.epsilon,\n                                   self.time)",
  "def step_time(self):\n        self.time += 1",
  "def set_time(self, time):\n        self.time = time",
  "def set_writer(self, writer):\n        self.writer = writer",
  "def __init__(self, action_space, **kwargs):\n        super().__init__()\n        self.action_space = action_space\n        if isinstance(self.action_space, spaces.Tuple):\n            self.action_space = self.action_space.spaces[0]\n        if not isinstance(self.action_space, spaces.Discrete):\n            raise TypeError(\"The action space should be discrete\")\n        self.values = None\n        self.seed()",
  "def get_distribution(self):\n        optimal_action = np.argmax(self.values)\n        return {action: 1 if action == optimal_action\n                else 0 for action in range(self.action_space.n)}",
  "def update(self, values):\n        self.values = values",
  "def default_qvalue_net_fn(env):\n    \"\"\"\n    Returns a default Q value network.\n    \"\"\"\n    model_config = {\"type\": \"DuelingNetwork\"}\n    model_config = size_model_config(env, **model_config)\n    return model_factory(**model_config)",
  "class DQNAgent(IncrementalAgent):\n    \"\"\"\n    Deep Q Learning Agent.\n\n    Parameters\n    ----------\n    env: gym.Env\n        Environment\n    n_episodes : int\n        Number of episodes to train the algorithm\n    horizon : int\n        Maximum lenght of an episode.\n    gamma : double\n        Discount factor\n    qvalue_net_fn : function(env, **kwargs)\n        Function that returns an instance of a network representing\n        the Q function.\n        If none, a default network is used.\n    qvalue_net_kwargs:\n        kwargs for qvalue_net_fn\n    loss_function : str\n        Type of loss function. Possibilities: 'l2', 'l1', 'smooth_l1'\n    batch_size : int\n        Batch size\n    device : str\n        Device used by pytorch.\n    target_update : int\n        Number of steps to wait before updating the target network.\n    double : bool\n        If true, use double Q-learning.\n    learning_rate : double\n        Optimizer learning rate.\n    epsilon_init : double\n        Initial value of epsilon in epsilon-greedy exploration\n    epsilon_final : double\n        Final value of epsilon in epsilon-greedy exploration\n    epsilon_decay : int\n        After `epsilon_decay` steps, epsilon approaches `epsilon_final`.\n    optimizer_type: str\n        Type of optimizer. 'ADAM' by defaut.\n    memory_capacity : int\n        Capacity of the replay buffer (in number of transitions).\n    use_bonus : bool, default = False\n        If true, compute an 'exploration_bonus' and add it to the reward.\n        See also UncertaintyEstimatorWrapper.\n    uncertainty_estimator_kwargs : dict\n        Arguments for the UncertaintyEstimatorWrapper\n    prioritized_replay: bool\n        Use prioritized replay.\n    \"\"\"\n    name = 'DQN'\n\n    def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 horizon=256,\n                 gamma=0.99,\n                 loss_function=\"l2\",\n                 batch_size=100,\n                 device=\"cuda:best\",\n                 target_update=1,\n                 learning_rate=0.001,\n                 epsilon_init=1.0,\n                 epsilon_final=0.1,\n                 epsilon_decay=5000,\n                 optimizer_type='ADAM',\n                 qvalue_net_fn=None,\n                 qvalue_net_kwargs=None,\n                 double=True,\n                 memory_capacity=10000,\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 prioritized_replay=True,\n                 update_frequency=1,\n                 **kwargs):\n        # Wrap arguments and initialize base class\n        memory_kwargs = {\n                        'capacity': memory_capacity,\n                        'n_steps': 1,\n                        'gamma': gamma\n                        }\n        exploration_kwargs = {\n                             'method': \"EpsilonGreedy\",\n                             'temperature': epsilon_init,\n                             'final_temperature': epsilon_final,\n                             'tau': epsilon_decay,\n                            }\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env,\n                                              **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n        self.horizon = horizon\n        self.exploration_kwargs = exploration_kwargs or {}\n        self.memory_kwargs = memory_kwargs or {}\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.target_update = target_update\n        self.double = double\n\n        assert isinstance(env.action_space, spaces.Discrete), \\\n            \"Only compatible with Discrete action spaces.\"\n\n        self.prioritized_replay = prioritized_replay\n        memory_class = PrioritizedReplayMemory if prioritized_replay else TransitionReplayMemory\n        self.memory = memory_class(**self.memory_kwargs)\n        self.exploration_policy = \\\n            exploration_factory(self.env.action_space,\n                                **self.exploration_kwargs)\n        self.training = True\n        self.steps = 0\n        self.episode = 0\n        self.writer = None\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n        self.device = choose_device(device)\n        self.loss_function = loss_function\n        self.gamma = gamma\n\n        qvalue_net_kwargs = qvalue_net_kwargs or {}\n        qvalue_net_fn = load(qvalue_net_fn) if isinstance(qvalue_net_fn, str) else \\\n            qvalue_net_fn or default_qvalue_net_fn\n        self.value_net = qvalue_net_fn(self.env, **qvalue_net_kwargs)\n        self.target_net = qvalue_net_fn(self.env, **qvalue_net_kwargs)\n\n        self.target_net.load_state_dict(self.value_net.state_dict())\n        self.target_net.eval()\n        logger.info(\"Number of trainable parameters: {}\"\n                    .format(trainable_parameters(self.value_net)))\n        self.value_net.to(self.device)\n        self.target_net.to(self.device)\n        self.loss_function = loss_function_factory(self.loss_function)\n        self.optimizer = optimizer_factory(self.value_net.parameters(),\n                                           **self.optimizer_kwargs)\n        self.update_frequency = update_frequency\n        self.steps = 0\n\n    def fit(self, **kwargs):\n        return self.partial_fit(fraction=1, **kwargs)\n\n    def partial_fit(self, fraction, **kwargs):\n        episode_rewards = []\n        for self.episode in range(int(fraction * self.n_episodes)):\n            if self.writer:\n                state = self.env.reset()\n                values = self.get_state_action_values(state)\n                for i, value in enumerate(values):\n                    self.writer.add_scalar(f\"agent/action_value_{i}\", value, self.episode)\n            total_reward, total_bonus, total_success, length = self._run_episode()\n            if self.episode % 20 == 0:\n                logger.info(f\"Episode {self.episode + 1}/{self.n_episodes}, total reward {total_reward}\")\n            self.plot_memory()\n            self.plot_bonuses()\n            self.plot_value()\n            if self.writer:\n                self.writer.add_scalar(\"episode/total_reward\", total_reward, self.episode)\n                self.writer.add_scalar(\"episode/total_bonus\", total_bonus, self.episode)\n                self.writer.add_scalar(\"episode/total_success\", total_success, self.episode)\n                self.writer.add_scalar(\"episode/length\", length, self.episode)\n                if self.use_bonus and \\\n                        (isinstance(self.env.uncertainty_estimator, OnlineDiscretizationCounter) or\n                         isinstance(self.env.uncertainty_estimator, DiscreteCounter)):\n                    n_visited_states = (self.env.uncertainty_estimator.N_sa.sum(axis=1) > 0).sum()\n                    self.writer.add_scalar(\"debug/n_visited_states\", n_visited_states, self.episode)\n\n            episode_rewards.append(total_reward)\n        return {\n            \"n_episodes\": int(fraction * self.n_episodes),\n            \"episode_rewards\": episode_rewards\n        }\n\n    def _run_episode(self):\n        total_reward = total_bonus = total_success = time = 0\n        state = self.env.reset()\n        for time in range(self.horizon):\n            self.exploration_policy.step_time()\n            action = self.policy(state)\n            next_state, reward, done, info = self.env.step(action)\n\n            # bonus used only for logging, here\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            self.record(state, action, reward, next_state, done, info)\n            state = next_state\n            total_reward += reward\n            total_bonus += bonus\n            total_success += info.get(\"is_success\", 0)\n            if done:\n                break\n        return total_reward, total_bonus, total_success, time+1\n\n    def record(self, state, action, reward, next_state, done, info):\n        \"\"\"\n        Record a transition by performing a Deep Q-Network iteration\n\n        - push the transition into memory\n        - sample a minibatch\n        - compute the bellman residual loss over the minibatch\n        - perform one gradient descent step\n        - slowly track the policy network with the target network\n\n        Parameters\n        ----------\n        state : object\n        action : object\n        reward : double\n        next_state : object\n        done : bool\n        \"\"\"\n        if not self.training:\n            return\n        self.memory.push(state, action, reward, next_state, done, info)\n        if self.memory.position % self.update_frequency == 0:\n            self.update()\n\n    def update(self):\n        batch, weights, indexes = self.sample_minibatch()\n        if batch:\n            losses, target = self.compute_bellman_residual(batch)\n            self.step_optimizer(losses.mean())\n            if self.prioritized_replay:\n                new_priorities = losses.abs().detach().cpu().numpy() + 1e-6\n                self.memory.update_priorities(indexes, new_priorities)\n            self.update_target_network()\n\n    def policy(self, observation, **kwargs):\n        \"\"\"\n        Act according to the state-action value model and an exploration\n        policy\n\n        Parameters\n\n        :param observation: current obs\n        :return: an action\n        \"\"\"\n        values = self.get_state_action_values(observation)\n        self.exploration_policy.update(values)\n        return self.exploration_policy.sample()\n\n    def sample_minibatch(self):\n        if len(self.memory) < self.batch_size:\n            return None, None, None\n        if self.prioritized_replay:\n            transitions, weights, indexes = self.memory.sample(self.batch_size)\n        else:\n            transitions, indexes = self.memory.sample(self.batch_size)\n            weights = np.ones((self.batch_size,))\n        return transitions, weights, indexes\n\n    def update_target_network(self):\n        self.steps += 1\n        if self.steps % self.target_update == 0:\n            self.target_net.load_state_dict(self.value_net.state_dict())\n\n    def step_optimizer(self, loss):\n        # Optimize the model\n        self.optimizer.zero_grad()\n        loss.backward()\n        for param in self.value_net.parameters():\n            param.grad.data.clamp_(-1, 1)\n        self.optimizer.step()\n\n    def compute_bellman_residual(self, batch):\n        \"\"\"\n        Compute the Bellman Residuals over a batch\n\n        Parameters\n        ----------\n        batch\n            batch of transitions\n        target_state_action_value\n            if provided, acts as a target (s,a)-value\n            if not, it will be computed from batch and model\n            (Double DQN target)\n\n        Returns\n        -------\n        The residuals over the batch, and the computed target.\n        \"\"\"\n        # Concatenate the batch elements\n        state = torch.cat(tuple(torch.tensor([batch.state],\n                          dtype=torch.float))).to(self.device)\n        action = torch.tensor(batch.action,\n                              dtype=torch.long).to(self.device)\n        reward = torch.tensor(batch.reward,\n                              dtype=torch.float).to(self.device)\n        if self.use_bonus:\n            bonus = self.env.bonus_batch(state, action).to(self.device) * self.exploration_policy.epsilon\n            if self.writer:\n                self.writer.add_scalar(\"debug/minibatch_mean_bonus\", bonus.mean().item(), self.episode)\n                self.writer.add_scalar(\"debug/minibatch_mean_reward\", reward.mean().item(), self.episode)\n            reward += bonus\n        next_state = torch.cat(tuple(torch.tensor([batch.next_state],\n                               dtype=torch.float))).to(self.device)\n        terminal = torch.tensor(batch.terminal,\n                                dtype=torch.bool).to(self.device)\n        batch = Transition(state, action, reward, next_state, terminal, batch.info)\n\n        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n        # columns of actions taken\n        state_action_values = self.value_net(batch.state)\n        state_action_values = \\\n            state_action_values.gather(1, batch.action.unsqueeze(1)).squeeze(1)\n\n        with torch.no_grad():\n            # Compute V(s_{t+1}) for all next states.\n            next_state_values = \\\n                torch.zeros(batch.reward.shape).to(self.device)\n            if self.double:\n                # Double Q-learning: pick best actions from policy network\n                _, best_actions = self.value_net(batch.next_state).max(1)\n                # Double Q-learning: estimate action values\n                # from target network\n                best_values = self.target_net(\n                                batch.next_state\n                                ).gather(1, best_actions.unsqueeze(1))\\\n                                 .squeeze(1)\n            else:\n                best_values, _ = self.target_net(batch.next_state).max(1)\n            next_state_values[~batch.terminal] \\\n                = best_values[~batch.terminal]\n            # Compute the expected Q values\n            target_state_action_value = batch.reward \\\n                + self.gamma * next_state_values\n\n        # Compute residuals\n        residuals = self.loss_function(state_action_values, target_state_action_value, reduction='none')\n        return residuals, target_state_action_value\n\n    def get_batch_state_values(self, states):\n        \"\"\"\n        Get the state values of several states\n\n        Parameters\n        ----------\n        states : array\n            [s1; ...; sN] an array of states\n\n        Returns\n        -------\n        values, actions:\n            * [V1; ...; VN] the array of the state values for each state\n            * [a1*; ...; aN*] the array of corresponding optimal action\n            indexes for each state\n        \"\"\"\n        values, actions = self.value_net(torch.tensor(states,\n                                         dtype=torch.float)\n                                         .to(self.device)).max(1)\n        return values.data.cpu().numpy(), actions.data.cpu().numpy()\n\n    def get_batch_state_action_values(self, states):\n        \"\"\"\n        Get the state-action values of several states\n\n        Parameters\n        ----------\n        states : array\n            [s1; ...; sN] an array of states\n\n        Returns\n        -------\n        values:[[Q11, ..., Q1n]; ...] the array of all action values\n        for each state\n        \"\"\"\n        return self.value_net(torch.tensor(states,\n                              dtype=torch.float)\n                              .to(self.device)).data.cpu().numpy()\n\n    def get_state_value(self, state):\n        \"\"\"\n        Parameters\n        ----------\n        state : object\n            s, an environment state\n        Returns\n        -------\n        V, its state-value\n        \"\"\"\n        values, actions = self.get_batch_state_values([state])\n        return values[0], actions[0]\n\n    def get_state_action_values(self, state):\n        \"\"\"\n        Parameters\n        ----------\n        state : object\n            s, an environment state\n\n        Returns\n        -------\n            The array of its action-values for each actions.\n        \"\"\"\n        return self.get_batch_state_action_values([state])[0]\n\n    def seed(self, seed=None):\n        return self.exploration_policy.seed(seed)\n\n    def reset(self, **kwargs):\n        self.episode = 0\n\n    def action_distribution(self, state):\n        values = self.get_state_action_values(state)\n        self.exploration_policy.update(values)\n        return self.exploration_policy.get_distribution()\n\n    def set_time(self, time):\n        self.exploration_policy.set_time(time)\n\n    def eval(self):\n        self.training = False\n        self.exploration_kwargs['method'] = \"Greedy\"\n        self.exploration_policy = \\\n            exploration_factory(self.env.action_space,\n                                **self.exploration_kwargs)\n\n    def save(self, filename, **kwargs):\n        state = {'state_dict': self.value_net.state_dict(),\n                 'optimizer': self.optimizer.state_dict()}\n        torch.save(state, filename)\n        return filename\n\n    def load(self, filename, **kwargs):\n        checkpoint = torch.load(filename, map_location=self.device)\n        self.value_net.load_state_dict(checkpoint['state_dict'])\n        self.target_net.load_state_dict(checkpoint['state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        return filename\n\n    def initialize_model(self):\n        self.value_net.reset()\n\n    def set_writer(self, writer):\n        self.writer = writer\n        try:\n            self.exploration_policy.set_writer(writer)\n        except AttributeError:\n            pass\n        if self.writer:\n            obs_shape = self.env.observation_space.shape \\\n                if isinstance(self.env.observation_space, spaces.Box) else \\\n                self.env.observation_space.spaces[0].shape\n            model_input = torch.zeros((1, *obs_shape), dtype=torch.float,\n                                      device=self.device)\n            self.writer.add_graph(self.value_net, input_to_model=(model_input,))\n            self.writer.add_scalar(\"agent/trainable_parameters\",\n                                   trainable_parameters(self.value_net), 0)\n\n    def plot_memory(self, frequency=100, states_count=20000):\n        if not self.writer:\n            return\n        if not self.episode % frequency == 0:\n            return\n        if self.memory.is_empty():\n            return\n        if representation_2d(self.memory.memory[0].state, self.env) is False:\n            return\n        import matplotlib.pyplot as plt\n        fig = plt.figure()\n        res = self.memory.sample(states_count)\n        samples, idx = res if not self.prioritized_replay else (res[0], res[2])\n        positions = np.array([representation_2d(state, self.env) for state in samples.state])\n        next_positions = np.array([representation_2d(next_state, self.env) for next_state in samples.next_state])\n        delta = next_positions - positions\n        color = np.mod(idx - self.memory.position, len(self.memory))\n        # plt.scatter(positions[:, 0], positions[:, 1], c=color, cmap=\"plasma\", alpha=0.3)\n        plt.quiver(positions[:, 0], positions[:, 1], delta[:, 0], delta[:, 1], color, cmap=\"plasma\",\n                   angles='xy', scale_units='xy', scale=1, alpha=0.3)\n        self.writer.add_figure(\"episode/memory\", fig, self.episode, close=True)\n\n    def plot_bonuses(self, frequency=100, states_count=20000):\n        if not self.writer:\n            return\n        if not self.use_bonus:\n            return\n        if not self.episode % frequency == 0:\n            return\n        from rlberry.envs.benchmarks.grid_exploration.nroom import NRoom\n        import matplotlib\n        import matplotlib.pyplot as plt\n        if isinstance(self.env.unwrapped, NRoom):\n            states = [self.env.unwrapped._convert_index_to_float_coord(idx)\n                          for idx in range(max(self.env.unwrapped.index2coord.keys()))]\n            actions = np.repeat(np.arange(self.env.action_space.n), len(states))\n            states = states * self.env.action_space.n\n        else:\n            sample = self.memory.sample(states_count)[0]\n            states, actions = sample.state, sample.action\n        states = torch.from_numpy(np.array(states)).to(self.device)\n        actions = torch.from_numpy(np.array(actions)).to(self.device)\n        bonuses = self.env.bonus_batch(states, actions).cpu().numpy()\n        positions = np.array([representation_2d(state, self.env) for state in states.cpu().numpy()])\n        fig = plt.figure()\n        plt.scatter(positions[:, 0], positions[:, 1],\n                    c=bonuses, norm=matplotlib.colors.LogNorm(), alpha=0.3)\n        plt.colorbar()\n        self.writer.add_figure(\"episode/bonuses\", fig, self.episode, close=True)\n\n    def plot_value(self, frequency=100, states_count=20000):\n        if not self.writer:\n            return\n        if not self.episode % frequency == 0:\n            return\n        from rlberry.envs.benchmarks.grid_exploration.nroom import NRoom\n        import matplotlib\n        import matplotlib.pyplot as plt\n        if isinstance(self.env.unwrapped, NRoom):\n            states = [self.env.unwrapped._convert_index_to_float_coord(idx)\n                          for idx in range(max(self.env.unwrapped.index2coord.keys()))]\n        else:\n            states = self.memory.sample(states_count)[0].state\n        states = torch.from_numpy(np.array(states)).to(self.device)\n        values = self.value_net(states).max(1)[0].detach().cpu().numpy()\n        fig = plt.figure()\n        positions = np.array([representation_2d(state, self.env) for state in states.cpu().numpy()])\n        plt.scatter(positions[:, 0], positions[:, 1],\n                    c=values, norm=matplotlib.colors.LogNorm(), alpha=0.3)\n        plt.colorbar()\n        self.writer.add_figure(\"episode/values\", fig, self.episode, close=True)\n\n    #\n    # For hyperparameter optimization\n    #\n    @classmethod\n    def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [32, 64, 128, 256, 512])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        target_update = trial.suggest_categorical('target_update',\n                                                  [1, 250, 500, 1000])\n\n        epsilon_final = trial.suggest_loguniform('epsilon_final', 1e-2, 1e-1)\n\n        epsilon_decay = trial.suggest_categorical('target_update',\n                                                  [1000, 5000, 10000])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'target_update': target_update,\n                'epsilon_final': epsilon_final,\n                'epsilon_decay': epsilon_decay,\n                }",
  "def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 horizon=256,\n                 gamma=0.99,\n                 loss_function=\"l2\",\n                 batch_size=100,\n                 device=\"cuda:best\",\n                 target_update=1,\n                 learning_rate=0.001,\n                 epsilon_init=1.0,\n                 epsilon_final=0.1,\n                 epsilon_decay=5000,\n                 optimizer_type='ADAM',\n                 qvalue_net_fn=None,\n                 qvalue_net_kwargs=None,\n                 double=True,\n                 memory_capacity=10000,\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 prioritized_replay=True,\n                 update_frequency=1,\n                 **kwargs):\n        # Wrap arguments and initialize base class\n        memory_kwargs = {\n                        'capacity': memory_capacity,\n                        'n_steps': 1,\n                        'gamma': gamma\n                        }\n        exploration_kwargs = {\n                             'method': \"EpsilonGreedy\",\n                             'temperature': epsilon_init,\n                             'final_temperature': epsilon_final,\n                             'tau': epsilon_decay,\n                            }\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env,\n                                              **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n        self.horizon = horizon\n        self.exploration_kwargs = exploration_kwargs or {}\n        self.memory_kwargs = memory_kwargs or {}\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.target_update = target_update\n        self.double = double\n\n        assert isinstance(env.action_space, spaces.Discrete), \\\n            \"Only compatible with Discrete action spaces.\"\n\n        self.prioritized_replay = prioritized_replay\n        memory_class = PrioritizedReplayMemory if prioritized_replay else TransitionReplayMemory\n        self.memory = memory_class(**self.memory_kwargs)\n        self.exploration_policy = \\\n            exploration_factory(self.env.action_space,\n                                **self.exploration_kwargs)\n        self.training = True\n        self.steps = 0\n        self.episode = 0\n        self.writer = None\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n        self.device = choose_device(device)\n        self.loss_function = loss_function\n        self.gamma = gamma\n\n        qvalue_net_kwargs = qvalue_net_kwargs or {}\n        qvalue_net_fn = load(qvalue_net_fn) if isinstance(qvalue_net_fn, str) else \\\n            qvalue_net_fn or default_qvalue_net_fn\n        self.value_net = qvalue_net_fn(self.env, **qvalue_net_kwargs)\n        self.target_net = qvalue_net_fn(self.env, **qvalue_net_kwargs)\n\n        self.target_net.load_state_dict(self.value_net.state_dict())\n        self.target_net.eval()\n        logger.info(\"Number of trainable parameters: {}\"\n                    .format(trainable_parameters(self.value_net)))\n        self.value_net.to(self.device)\n        self.target_net.to(self.device)\n        self.loss_function = loss_function_factory(self.loss_function)\n        self.optimizer = optimizer_factory(self.value_net.parameters(),\n                                           **self.optimizer_kwargs)\n        self.update_frequency = update_frequency\n        self.steps = 0",
  "def fit(self, **kwargs):\n        return self.partial_fit(fraction=1, **kwargs)",
  "def partial_fit(self, fraction, **kwargs):\n        episode_rewards = []\n        for self.episode in range(int(fraction * self.n_episodes)):\n            if self.writer:\n                state = self.env.reset()\n                values = self.get_state_action_values(state)\n                for i, value in enumerate(values):\n                    self.writer.add_scalar(f\"agent/action_value_{i}\", value, self.episode)\n            total_reward, total_bonus, total_success, length = self._run_episode()\n            if self.episode % 20 == 0:\n                logger.info(f\"Episode {self.episode + 1}/{self.n_episodes}, total reward {total_reward}\")\n            self.plot_memory()\n            self.plot_bonuses()\n            self.plot_value()\n            if self.writer:\n                self.writer.add_scalar(\"episode/total_reward\", total_reward, self.episode)\n                self.writer.add_scalar(\"episode/total_bonus\", total_bonus, self.episode)\n                self.writer.add_scalar(\"episode/total_success\", total_success, self.episode)\n                self.writer.add_scalar(\"episode/length\", length, self.episode)\n                if self.use_bonus and \\\n                        (isinstance(self.env.uncertainty_estimator, OnlineDiscretizationCounter) or\n                         isinstance(self.env.uncertainty_estimator, DiscreteCounter)):\n                    n_visited_states = (self.env.uncertainty_estimator.N_sa.sum(axis=1) > 0).sum()\n                    self.writer.add_scalar(\"debug/n_visited_states\", n_visited_states, self.episode)\n\n            episode_rewards.append(total_reward)\n        return {\n            \"n_episodes\": int(fraction * self.n_episodes),\n            \"episode_rewards\": episode_rewards\n        }",
  "def _run_episode(self):\n        total_reward = total_bonus = total_success = time = 0\n        state = self.env.reset()\n        for time in range(self.horizon):\n            self.exploration_policy.step_time()\n            action = self.policy(state)\n            next_state, reward, done, info = self.env.step(action)\n\n            # bonus used only for logging, here\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            self.record(state, action, reward, next_state, done, info)\n            state = next_state\n            total_reward += reward\n            total_bonus += bonus\n            total_success += info.get(\"is_success\", 0)\n            if done:\n                break\n        return total_reward, total_bonus, total_success, time+1",
  "def record(self, state, action, reward, next_state, done, info):\n        \"\"\"\n        Record a transition by performing a Deep Q-Network iteration\n\n        - push the transition into memory\n        - sample a minibatch\n        - compute the bellman residual loss over the minibatch\n        - perform one gradient descent step\n        - slowly track the policy network with the target network\n\n        Parameters\n        ----------\n        state : object\n        action : object\n        reward : double\n        next_state : object\n        done : bool\n        \"\"\"\n        if not self.training:\n            return\n        self.memory.push(state, action, reward, next_state, done, info)\n        if self.memory.position % self.update_frequency == 0:\n            self.update()",
  "def update(self):\n        batch, weights, indexes = self.sample_minibatch()\n        if batch:\n            losses, target = self.compute_bellman_residual(batch)\n            self.step_optimizer(losses.mean())\n            if self.prioritized_replay:\n                new_priorities = losses.abs().detach().cpu().numpy() + 1e-6\n                self.memory.update_priorities(indexes, new_priorities)\n            self.update_target_network()",
  "def policy(self, observation, **kwargs):\n        \"\"\"\n        Act according to the state-action value model and an exploration\n        policy\n\n        Parameters\n\n        :param observation: current obs\n        :return: an action\n        \"\"\"\n        values = self.get_state_action_values(observation)\n        self.exploration_policy.update(values)\n        return self.exploration_policy.sample()",
  "def sample_minibatch(self):\n        if len(self.memory) < self.batch_size:\n            return None, None, None\n        if self.prioritized_replay:\n            transitions, weights, indexes = self.memory.sample(self.batch_size)\n        else:\n            transitions, indexes = self.memory.sample(self.batch_size)\n            weights = np.ones((self.batch_size,))\n        return transitions, weights, indexes",
  "def update_target_network(self):\n        self.steps += 1\n        if self.steps % self.target_update == 0:\n            self.target_net.load_state_dict(self.value_net.state_dict())",
  "def step_optimizer(self, loss):\n        # Optimize the model\n        self.optimizer.zero_grad()\n        loss.backward()\n        for param in self.value_net.parameters():\n            param.grad.data.clamp_(-1, 1)\n        self.optimizer.step()",
  "def compute_bellman_residual(self, batch):\n        \"\"\"\n        Compute the Bellman Residuals over a batch\n\n        Parameters\n        ----------\n        batch\n            batch of transitions\n        target_state_action_value\n            if provided, acts as a target (s,a)-value\n            if not, it will be computed from batch and model\n            (Double DQN target)\n\n        Returns\n        -------\n        The residuals over the batch, and the computed target.\n        \"\"\"\n        # Concatenate the batch elements\n        state = torch.cat(tuple(torch.tensor([batch.state],\n                          dtype=torch.float))).to(self.device)\n        action = torch.tensor(batch.action,\n                              dtype=torch.long).to(self.device)\n        reward = torch.tensor(batch.reward,\n                              dtype=torch.float).to(self.device)\n        if self.use_bonus:\n            bonus = self.env.bonus_batch(state, action).to(self.device) * self.exploration_policy.epsilon\n            if self.writer:\n                self.writer.add_scalar(\"debug/minibatch_mean_bonus\", bonus.mean().item(), self.episode)\n                self.writer.add_scalar(\"debug/minibatch_mean_reward\", reward.mean().item(), self.episode)\n            reward += bonus\n        next_state = torch.cat(tuple(torch.tensor([batch.next_state],\n                               dtype=torch.float))).to(self.device)\n        terminal = torch.tensor(batch.terminal,\n                                dtype=torch.bool).to(self.device)\n        batch = Transition(state, action, reward, next_state, terminal, batch.info)\n\n        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n        # columns of actions taken\n        state_action_values = self.value_net(batch.state)\n        state_action_values = \\\n            state_action_values.gather(1, batch.action.unsqueeze(1)).squeeze(1)\n\n        with torch.no_grad():\n            # Compute V(s_{t+1}) for all next states.\n            next_state_values = \\\n                torch.zeros(batch.reward.shape).to(self.device)\n            if self.double:\n                # Double Q-learning: pick best actions from policy network\n                _, best_actions = self.value_net(batch.next_state).max(1)\n                # Double Q-learning: estimate action values\n                # from target network\n                best_values = self.target_net(\n                                batch.next_state\n                                ).gather(1, best_actions.unsqueeze(1))\\\n                                 .squeeze(1)\n            else:\n                best_values, _ = self.target_net(batch.next_state).max(1)\n            next_state_values[~batch.terminal] \\\n                = best_values[~batch.terminal]\n            # Compute the expected Q values\n            target_state_action_value = batch.reward \\\n                + self.gamma * next_state_values\n\n        # Compute residuals\n        residuals = self.loss_function(state_action_values, target_state_action_value, reduction='none')\n        return residuals, target_state_action_value",
  "def get_batch_state_values(self, states):\n        \"\"\"\n        Get the state values of several states\n\n        Parameters\n        ----------\n        states : array\n            [s1; ...; sN] an array of states\n\n        Returns\n        -------\n        values, actions:\n            * [V1; ...; VN] the array of the state values for each state\n            * [a1*; ...; aN*] the array of corresponding optimal action\n            indexes for each state\n        \"\"\"\n        values, actions = self.value_net(torch.tensor(states,\n                                         dtype=torch.float)\n                                         .to(self.device)).max(1)\n        return values.data.cpu().numpy(), actions.data.cpu().numpy()",
  "def get_batch_state_action_values(self, states):\n        \"\"\"\n        Get the state-action values of several states\n\n        Parameters\n        ----------\n        states : array\n            [s1; ...; sN] an array of states\n\n        Returns\n        -------\n        values:[[Q11, ..., Q1n]; ...] the array of all action values\n        for each state\n        \"\"\"\n        return self.value_net(torch.tensor(states,\n                              dtype=torch.float)\n                              .to(self.device)).data.cpu().numpy()",
  "def get_state_value(self, state):\n        \"\"\"\n        Parameters\n        ----------\n        state : object\n            s, an environment state\n        Returns\n        -------\n        V, its state-value\n        \"\"\"\n        values, actions = self.get_batch_state_values([state])\n        return values[0], actions[0]",
  "def get_state_action_values(self, state):\n        \"\"\"\n        Parameters\n        ----------\n        state : object\n            s, an environment state\n\n        Returns\n        -------\n            The array of its action-values for each actions.\n        \"\"\"\n        return self.get_batch_state_action_values([state])[0]",
  "def seed(self, seed=None):\n        return self.exploration_policy.seed(seed)",
  "def reset(self, **kwargs):\n        self.episode = 0",
  "def action_distribution(self, state):\n        values = self.get_state_action_values(state)\n        self.exploration_policy.update(values)\n        return self.exploration_policy.get_distribution()",
  "def set_time(self, time):\n        self.exploration_policy.set_time(time)",
  "def eval(self):\n        self.training = False\n        self.exploration_kwargs['method'] = \"Greedy\"\n        self.exploration_policy = \\\n            exploration_factory(self.env.action_space,\n                                **self.exploration_kwargs)",
  "def save(self, filename, **kwargs):\n        state = {'state_dict': self.value_net.state_dict(),\n                 'optimizer': self.optimizer.state_dict()}\n        torch.save(state, filename)\n        return filename",
  "def load(self, filename, **kwargs):\n        checkpoint = torch.load(filename, map_location=self.device)\n        self.value_net.load_state_dict(checkpoint['state_dict'])\n        self.target_net.load_state_dict(checkpoint['state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        return filename",
  "def initialize_model(self):\n        self.value_net.reset()",
  "def set_writer(self, writer):\n        self.writer = writer\n        try:\n            self.exploration_policy.set_writer(writer)\n        except AttributeError:\n            pass\n        if self.writer:\n            obs_shape = self.env.observation_space.shape \\\n                if isinstance(self.env.observation_space, spaces.Box) else \\\n                self.env.observation_space.spaces[0].shape\n            model_input = torch.zeros((1, *obs_shape), dtype=torch.float,\n                                      device=self.device)\n            self.writer.add_graph(self.value_net, input_to_model=(model_input,))\n            self.writer.add_scalar(\"agent/trainable_parameters\",\n                                   trainable_parameters(self.value_net), 0)",
  "def plot_memory(self, frequency=100, states_count=20000):\n        if not self.writer:\n            return\n        if not self.episode % frequency == 0:\n            return\n        if self.memory.is_empty():\n            return\n        if representation_2d(self.memory.memory[0].state, self.env) is False:\n            return\n        import matplotlib.pyplot as plt\n        fig = plt.figure()\n        res = self.memory.sample(states_count)\n        samples, idx = res if not self.prioritized_replay else (res[0], res[2])\n        positions = np.array([representation_2d(state, self.env) for state in samples.state])\n        next_positions = np.array([representation_2d(next_state, self.env) for next_state in samples.next_state])\n        delta = next_positions - positions\n        color = np.mod(idx - self.memory.position, len(self.memory))\n        # plt.scatter(positions[:, 0], positions[:, 1], c=color, cmap=\"plasma\", alpha=0.3)\n        plt.quiver(positions[:, 0], positions[:, 1], delta[:, 0], delta[:, 1], color, cmap=\"plasma\",\n                   angles='xy', scale_units='xy', scale=1, alpha=0.3)\n        self.writer.add_figure(\"episode/memory\", fig, self.episode, close=True)",
  "def plot_bonuses(self, frequency=100, states_count=20000):\n        if not self.writer:\n            return\n        if not self.use_bonus:\n            return\n        if not self.episode % frequency == 0:\n            return\n        from rlberry.envs.benchmarks.grid_exploration.nroom import NRoom\n        import matplotlib\n        import matplotlib.pyplot as plt\n        if isinstance(self.env.unwrapped, NRoom):\n            states = [self.env.unwrapped._convert_index_to_float_coord(idx)\n                          for idx in range(max(self.env.unwrapped.index2coord.keys()))]\n            actions = np.repeat(np.arange(self.env.action_space.n), len(states))\n            states = states * self.env.action_space.n\n        else:\n            sample = self.memory.sample(states_count)[0]\n            states, actions = sample.state, sample.action\n        states = torch.from_numpy(np.array(states)).to(self.device)\n        actions = torch.from_numpy(np.array(actions)).to(self.device)\n        bonuses = self.env.bonus_batch(states, actions).cpu().numpy()\n        positions = np.array([representation_2d(state, self.env) for state in states.cpu().numpy()])\n        fig = plt.figure()\n        plt.scatter(positions[:, 0], positions[:, 1],\n                    c=bonuses, norm=matplotlib.colors.LogNorm(), alpha=0.3)\n        plt.colorbar()\n        self.writer.add_figure(\"episode/bonuses\", fig, self.episode, close=True)",
  "def plot_value(self, frequency=100, states_count=20000):\n        if not self.writer:\n            return\n        if not self.episode % frequency == 0:\n            return\n        from rlberry.envs.benchmarks.grid_exploration.nroom import NRoom\n        import matplotlib\n        import matplotlib.pyplot as plt\n        if isinstance(self.env.unwrapped, NRoom):\n            states = [self.env.unwrapped._convert_index_to_float_coord(idx)\n                          for idx in range(max(self.env.unwrapped.index2coord.keys()))]\n        else:\n            states = self.memory.sample(states_count)[0].state\n        states = torch.from_numpy(np.array(states)).to(self.device)\n        values = self.value_net(states).max(1)[0].detach().cpu().numpy()\n        fig = plt.figure()\n        positions = np.array([representation_2d(state, self.env) for state in states.cpu().numpy()])\n        plt.scatter(positions[:, 0], positions[:, 1],\n                    c=values, norm=matplotlib.colors.LogNorm(), alpha=0.3)\n        plt.colorbar()\n        self.writer.add_figure(\"episode/values\", fig, self.episode, close=True)",
  "def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [32, 64, 128, 256, 512])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        target_update = trial.suggest_categorical('target_update',\n                                                  [1, 250, 500, 1000])\n\n        epsilon_final = trial.suggest_loguniform('epsilon_final', 1e-2, 1e-1)\n\n        epsilon_decay = trial.suggest_categorical('target_update',\n                                                  [1000, 5000, 10000])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'target_update': target_update,\n                'epsilon_final': epsilon_final,\n                'epsilon_decay': epsilon_decay,\n                }",
  "class KOVIAgent(Agent):\n    \"\"\"\n    A version of Least-Squares Value Iteration with UCB (LSVI-UCB),\n    proposed by Jin et al. (2020).\n\n    If bonus_scale_factor is 0.0, performs random exploration.\n\n    Parameters\n    ----------\n    env : Model\n        Online model of an environment.\n    horizon : int\n        Maximum length of each episode.\n    feature_map_fn : function(env, kwargs)\n        Function that returns a feature map instance\n        (rlberry.agents.features.FeatureMap class).\n    feature_map_kwargs:\n        kwargs for feature_map_fn\n    n_episodes : int\n        number of episodes\n    gamma : double\n        Discount factor.\n    bonus_scale_factor : double\n        Constant by which to multiply the exploration bonus.\n    reg_factor : double\n        Linear regression regularization factor.\n\n    References\n    ----------\n    Jin, C., Yang, Z., Wang, Z., & Jordan, M. I. (2020, July).\n    Provably efficient reinforcement learning with linear\n    function approximation. In Conference on Learning Theory (pp. 2137-2143).\n    \"\"\"\n\n    name = 'KOVI'\n\n    def __init__(self,\n                 env,\n                 horizon,\n                 pd_kernel_fn,\n                 pd_kernel_kwargs=None,\n                 n_episodes=100,\n                 gamma=0.99,\n                 bonus_scale_factor=1.0,\n                 reg_factor=0.1,\n                 **kwargs):\n        Agent.__init__(self, env, **kwargs)\n\n        self.use_jit = True\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.bonus_scale_factor = bonus_scale_factor\n        self.reg_factor = reg_factor\n        self.total_time_steps = 0\n\n        pd_kernel_kwargs = pd_kernel_kwargs or {}\n        self.pd_kernel = pd_kernel_fn\n\n        #\n        if self.bonus_scale_factor == 0.0:\n            self.name = 'KOVI-Random-Expl'\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf:\n            logger.warning(\"{}: Reward range is infinity. \".format(self.name)\n                           + \"Clipping it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon))\\\n                 / (1.0 - self.gamma)\n\n        #\n        assert isinstance(self.env.action_space, Discrete), \\\n            \"KOVI requires discrete actions.\"\n\n        # attributes initialized in reset()\n        self.episode = None\n        self.gram_mat = None        # Gram matrix\n        self.gram_mat_inv = None    # inverse of Gram matrix\n        self.alphas = None          # vector representations of Q\n        self.reward_hist = None     # reward history\n        self.state_hist = None      # state history\n        self.action_hist = None     # action history\n        self.nstate_hist = None     # next state history\n        self.rkhs_norm_hist = None  # norm history\n\n        self.feat_hist = None             # feature history\n        self.feat_ns_all_actions = None   # next state features for all actions\n\n        self.new_gram_mat = None\n        self.new_gram_mat_inv = None\n        #\n\n        # aux variables (init in reset() too)\n        self._rewards = None\n\n        # default writer\n        self.writer = PeriodicWriter(self.name, log_every=15)\n        # 5*logger.getEffectiveLevel()\n\n        #\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.episode = 0\n        self.total_time_steps = 0\n        self.gram_mat = [np.array([self.reg_factor]).reshape(1, 1) for _ in range(self.horizon)]\n        self.gram_mat_inv = [np.array([(1.0 / self.reg_factor)]).reshape(1, 1) for _ in range(self.horizon)]\n\n        self.new_gram_mat = [np.array([self.reg_factor]).reshape(1, 1) for _ in range(self.horizon)]\n        self.new_gram_mat_inv = [np.array([(1.0 / self.reg_factor)]).reshape(1, 1) for _ in range(self.horizon)]\n\n        self.reward_hist = np.zeros(self.n_episodes * self.horizon)\n        self.state_hist = []\n        self.action_hist = []\n        self.nstate_hist = []\n        self.rkhs_norm_hist = []\n\n        self.hist_reward = [[] for _ in range(self.horizon)]\n        self.hist_rkhs_norm = np.zeros(\n            (self.horizon, self.n_episodes, self.env.action_space.n)\n        )\n        self.hist_nstate = [[] for _ in range(self.horizon)]\n        self.nstate_feat_hist = np.zeros(\n            (self.horizon, self.n_episodes, self.n_episodes, self.env.action_space.n)\n        )\n\n        # episode rewards\n        self._rewards = np.zeros(self.n_episodes)\n\n        #\n        self.feat_hist = [[] for _ in range(self.n_episodes * self.horizon)]\n        self.feat_ns_all_actions = [\n            [[] for _ in range(self.env.action_space.n)] for _ in range(self.n_episodes * self.horizon)\n        ]\n\n        #\n        self.alphas = [np.array([0.]) for _ in range(self.horizon)]\n\n    def fit(self, **kwargs):\n        info = {}\n        for ep in range(self.n_episodes):\n            self.run_episode()\n            if self.bonus_scale_factor > 0.0 or ep == self.n_episodes - 1:\n                # update Q function representation\n\n                for step in range(self.horizon):\n                    self.gram_mat[step] = self.new_gram_mat[step]\n                    self.gram_mat_inv[step] = self.new_gram_mat_inv[step]\n\n                if self.use_jit:\n                    self.alphas = self._run_kovi(self.bonus_scale_factor)\n                else:\n                    self._run_kovi(self.bonus_scale_factor)\n\n        info['n_episodes'] = self.n_episodes\n        info['episode_rewards'] = self._rewards\n        return info\n\n    def run_episode(self):\n        # print(f'Episode: {self.episode}')\n\n        state = self.env.reset()\n        episode_rewards = 0\n\n        self.new_gram_mat = np.zeros((self.horizon, self.episode + 1, self.episode + 1))\n        self.new_gram_mat_inv = np.zeros((self.horizon, self.episode + 1, self.episode + 1))\n\n        for step in range(self.horizon):\n            # print(f'Step {step}')\n            if self.bonus_scale_factor == 0.0 or self.episode == 0:\n                action = self.env.action_space.sample()\n            else:\n                action = self._optimistic_policy(step, state)\n\n            next_state, reward, is_terminal, _ = self.env.step(action)\n            # if is_terminal:\n            #     reward = 0.\n\n            # update Gram matrix and its inverse\n            if self.episode == 0:\n                self.new_gram_mat[step] += self.pd_kernel(state, action)\n                self.new_gram_mat_inv[step] = 1. / self.new_gram_mat[step]\n\n            elif self.episode >= 1:\n                feat = self._score_vector(step, state, action)\n                outer_prod = np.outer(feat, feat)\n\n                # update Gram matrix\n                self.new_gram_mat[step, :-1, :-1] = self.gram_mat[step]\n                self.new_gram_mat[step, -1, :-1] = feat\n                self.new_gram_mat[step, :-1, -1] = feat\n                self.new_gram_mat[step, -1, -1] = self.pd_kernel(state, action) + self.reg_factor\n\n                # update Gram inverse\n                K22 = 1.0 / (self.gram_mat[step][-1, -1] + self.reg_factor - feat.T @ self.gram_mat_inv[step] @ feat)\n                K11 = self.gram_mat_inv[step] + K22 * self.gram_mat_inv[step] @ outer_prod @ self.gram_mat_inv[step]\n                K12 = - K22 * self.gram_mat_inv[step] @ feat\n                K21 = - K22 * feat.T @ self.gram_mat_inv[step]\n\n                self.new_gram_mat_inv[step, :-1, :-1] = K11\n                self.new_gram_mat_inv[step, :-1, -1] = K12\n                self.new_gram_mat_inv[step, -1, :-1] = K21\n                self.new_gram_mat_inv[step, -1, -1] = K22\n\n                # store next features\n                # tt = self.total_time_steps\n                # self.feat_hist[tt] = feat\n                # for aa in range(self.env.action_space.n):\n                #     self.feat_ns_all_actions[tt][aa] = self._score_vector(step + 1, next_state, aa)\n\n                for tau in range(self.episode):\n                    pns = self.hist_nstate[step][tau]\n                    for a in range(self.env.action_space.n):\n                        similarity = self.pd_kernel(next_state, action, pns, a)\n                        self.nstate_feat_hist[step, tau, self.episode, a] = similarity\n\n            # update history\n            self.reward_hist[self.total_time_steps] = reward\n            self.state_hist.append(state)\n            self.action_hist.append(action)\n            self.nstate_hist.append(next_state)\n            self.rkhs_norm_hist.append(self.pd_kernel(state, action))\n\n            for a in range(self.env.action_space.n):\n                self.hist_rkhs_norm[step, self.episode, a] = self.pd_kernel(next_state, a)\n\n            self.hist_nstate[step].append(next_state)\n            self.hist_reward[step].append(reward)\n\n            # increments\n            self.total_time_steps += 1\n            episode_rewards += reward\n\n            # update current state\n            state = next_state\n\n            # if is_terminal:\n            #     break\n\n        # store data\n        self._rewards[self.episode] = episode_rewards\n\n        # update episode\n        self.episode += 1\n\n        # log data\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        return episode_rewards\n\n    def policy(self, observation, **kwargs):\n        # return self._compute_q_vec(self.alphas[0], 0, observation, self.bonus_scale_factor).argmax()\n        return self._compute_q_vec(0, observation, 0.0).argmax()\n\n    def _optimistic_policy(self, step, observation):\n        # return self._compute_q_vec(self.alphas[step], step, observation, self.bonus_scale_factor).argmax()\n        return self._compute_q_vec(step, observation, self.bonus_scale_factor).argmax()\n\n    def _compute_q_vec(self, step, state, bonus_factor):\n        n_actions = self.env.action_space.n\n        q_vec = np.zeros(n_actions)\n\n        if step == self.horizon:\n            return q_vec\n\n        else:\n            for aa in range(n_actions):\n                feat = self._score_vector(step, state, aa)\n\n                inverse_counts = self.pd_kernel(state, aa) - feat @ self.gram_mat_inv[step] @ feat\n                bonus = bonus_factor * np.sqrt(inverse_counts) + self.v_max * inverse_counts * (bonus_factor > 0.0)\n\n                q_vec[aa] = feat.T @ self.alphas[step] + bonus\n                q_vec[aa] = min(q_vec[aa], self.v_max)\n\n            return q_vec\n\n    def _score_vector(self, step, state, action):\n        # k_h^t (s, a)\n        prev_states = self._get_previous_states(step, by='episode')\n        prev_actions = self._get_previous_actions(step, by='episode')\n\n        similarities = [self.pd_kernel(s, a, state2=state, action2=action) for s, a in zip(prev_states, prev_actions)]\n        return np.array(similarities)\n\n    def _get_previous_states(self, index, by='episode'):\n        if by == 'episode':\n            return [self.state_hist[index + ep * self.horizon] for ep in range(self.episode)]\n        elif by == 'step':\n            return [self.state_hist[step + index * self.horizon] for step in range(self.horizon)]\n\n    def _get_previous_actions(self, index, by='episode'):\n        if by == 'episode':\n            return [self.action_hist[index + ep * self.horizon] for ep in range(self.episode)]\n        elif by == 'step':\n            return [self.action_hist[step + index * self.horizon] for step in range(self.horizon)]\n\n    def _get_previous_rewards(self, index, by='episode'):\n        if by == 'episode':\n            return [self.reward_hist[index + ep * self.horizon] for ep in range(self.episode)]\n        elif by == 'step':\n            return [self.reward_hist[step + index * self.horizon] for step in range(self.horizon)]\n\n    def _run_kovi(self, bonus_factor):\n\n        if self.use_jit:\n\n            # print(self.episode)\n            # print(self.env.action_space.n)\n            # print(self.horizon)\n            # print(self.nstate_feat_hist)\n            # print(self.hist_reward)\n            # print(self.hist_rkhs_norm)\n            # print(self.gram_mat_inv)\n            # print(bonus_factor)\n            # print(self.v_max)\n            # print(self.gamma)\n\n            gram_mat_inv = List()\n            for mat in self.gram_mat_inv:\n                gram_mat_inv.append(mat)\n\n            reward_hist = List()\n            for lst in self.hist_reward:\n                reward_hist.append(lst)\n\n            return run_kovi_jit(self.episode,\n                                self.env.action_space.n,\n                                self.horizon,\n                                self.nstate_feat_hist,\n                                reward_hist,\n                                self.hist_rkhs_norm,\n                                gram_mat_inv,\n                                bonus_factor,\n                                self.v_max,\n                                self.gamma)\n\n        else:\n            q_ns = np.zeros((self.episode, self.env.action_space.n))\n            alphas = np.zeros((self.horizon + 1, self.episode))\n\n            for step in range(self.horizon - 1, -1, -1):\n                # build targets\n                prev_rewards = self._get_previous_rewards(step, by='episode')\n                targets = prev_rewards + self.gamma * q_ns.max(axis=1)\n\n                # update parameters solving kernel ridge regression\n                alphas[step] = self.gram_mat_inv[step] @ targets\n                self.alphas[step] = alphas[step]\n\n                # update value function\n                # prev_ns = self._get_previous_states(step + 1, by='episode')\n                # prev_ns = [self.nstate_hist[step + ep * self.horizon] for ep in range(self.episode)]\n                # q_ns = np.array([self._compute_q_vec(step + 1, ns, self.bonus_scale_factor) for ns in prev_ns])\n\n                for tau in range(self.episode):\n                    pns = self.hist_nstate[step][tau]\n                    for a in range(self.env.action_space.n):\n                        feat = self.nstate_feat_hist[step, tau, :self.episode, a]\n\n                        # if a == 0:\n                            # print(f'Episode {self.episode}, step {step}, tau {tau}, feat: {feat.shape}, gram_mat_inv[step]: {self.gram_mat_inv[step].shape}')\n                        inv_counts = self.pd_kernel(pns, a) - feat @ self.gram_mat_inv[step] @ feat\n                        bonus = bonus_factor * np.sqrt(inv_counts) + self.v_max * inv_counts * (bonus_factor > 0.0)\n                        q_ns[tau, a] = feat.T @ self.alphas[step] + bonus\n                        q_ns[tau, a] = min(q_ns[tau, a], self.v_max)\n\n            return alphas",
  "def run_kovi_jit(episode,\n                 n_actions,\n                 horizon,\n                 nstate_feat_hist,\n                 reward_hist,\n                 rkhs_norm_hist,\n                 gram_mat_inv,\n                 bonus_scale_factor,\n                 v_max,\n                 gamma):\n\n    q_ns = np.zeros((episode, n_actions))\n    alphas = np.zeros((horizon + 1, episode))\n    targets = np.zeros(episode)\n\n    for step in range(horizon - 1, -1, -1):\n        prev_rewards = np.array(reward_hist[step])\n        for ep in range(episode):\n            targets[ep] = prev_rewards[ep] + gamma * max(q_ns[ep])\n\n        alphas[step] = gram_mat_inv[step].dot(targets)\n\n        for tau in range(episode):\n            for a in range(n_actions):\n                feat = nstate_feat_hist[step, tau, :episode, a]\n\n                inv_counts = rkhs_norm_hist[step, tau, a] - feat.dot(gram_mat_inv[step].T).dot(feat)\n                bonus = bonus_scale_factor * np.sqrt(inv_counts) + v_max * inv_counts * (bonus_scale_factor > 0.0)\n\n                q_ns[tau, a] = feat.dot(alphas[step]) + bonus\n                q_ns[tau, a] = min(q_ns[tau, a], v_max)\n\n    return alphas",
  "def __init__(self,\n                 env,\n                 horizon,\n                 pd_kernel_fn,\n                 pd_kernel_kwargs=None,\n                 n_episodes=100,\n                 gamma=0.99,\n                 bonus_scale_factor=1.0,\n                 reg_factor=0.1,\n                 **kwargs):\n        Agent.__init__(self, env, **kwargs)\n\n        self.use_jit = True\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.bonus_scale_factor = bonus_scale_factor\n        self.reg_factor = reg_factor\n        self.total_time_steps = 0\n\n        pd_kernel_kwargs = pd_kernel_kwargs or {}\n        self.pd_kernel = pd_kernel_fn\n\n        #\n        if self.bonus_scale_factor == 0.0:\n            self.name = 'KOVI-Random-Expl'\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf:\n            logger.warning(\"{}: Reward range is infinity. \".format(self.name)\n                           + \"Clipping it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon))\\\n                 / (1.0 - self.gamma)\n\n        #\n        assert isinstance(self.env.action_space, Discrete), \\\n            \"KOVI requires discrete actions.\"\n\n        # attributes initialized in reset()\n        self.episode = None\n        self.gram_mat = None        # Gram matrix\n        self.gram_mat_inv = None    # inverse of Gram matrix\n        self.alphas = None          # vector representations of Q\n        self.reward_hist = None     # reward history\n        self.state_hist = None      # state history\n        self.action_hist = None     # action history\n        self.nstate_hist = None     # next state history\n        self.rkhs_norm_hist = None  # norm history\n\n        self.feat_hist = None             # feature history\n        self.feat_ns_all_actions = None   # next state features for all actions\n\n        self.new_gram_mat = None\n        self.new_gram_mat_inv = None\n        #\n\n        # aux variables (init in reset() too)\n        self._rewards = None\n\n        # default writer\n        self.writer = PeriodicWriter(self.name, log_every=15)\n        # 5*logger.getEffectiveLevel()\n\n        #\n        self.reset()",
  "def reset(self, **kwargs):\n        self.episode = 0\n        self.total_time_steps = 0\n        self.gram_mat = [np.array([self.reg_factor]).reshape(1, 1) for _ in range(self.horizon)]\n        self.gram_mat_inv = [np.array([(1.0 / self.reg_factor)]).reshape(1, 1) for _ in range(self.horizon)]\n\n        self.new_gram_mat = [np.array([self.reg_factor]).reshape(1, 1) for _ in range(self.horizon)]\n        self.new_gram_mat_inv = [np.array([(1.0 / self.reg_factor)]).reshape(1, 1) for _ in range(self.horizon)]\n\n        self.reward_hist = np.zeros(self.n_episodes * self.horizon)\n        self.state_hist = []\n        self.action_hist = []\n        self.nstate_hist = []\n        self.rkhs_norm_hist = []\n\n        self.hist_reward = [[] for _ in range(self.horizon)]\n        self.hist_rkhs_norm = np.zeros(\n            (self.horizon, self.n_episodes, self.env.action_space.n)\n        )\n        self.hist_nstate = [[] for _ in range(self.horizon)]\n        self.nstate_feat_hist = np.zeros(\n            (self.horizon, self.n_episodes, self.n_episodes, self.env.action_space.n)\n        )\n\n        # episode rewards\n        self._rewards = np.zeros(self.n_episodes)\n\n        #\n        self.feat_hist = [[] for _ in range(self.n_episodes * self.horizon)]\n        self.feat_ns_all_actions = [\n            [[] for _ in range(self.env.action_space.n)] for _ in range(self.n_episodes * self.horizon)\n        ]\n\n        #\n        self.alphas = [np.array([0.]) for _ in range(self.horizon)]",
  "def fit(self, **kwargs):\n        info = {}\n        for ep in range(self.n_episodes):\n            self.run_episode()\n            if self.bonus_scale_factor > 0.0 or ep == self.n_episodes - 1:\n                # update Q function representation\n\n                for step in range(self.horizon):\n                    self.gram_mat[step] = self.new_gram_mat[step]\n                    self.gram_mat_inv[step] = self.new_gram_mat_inv[step]\n\n                if self.use_jit:\n                    self.alphas = self._run_kovi(self.bonus_scale_factor)\n                else:\n                    self._run_kovi(self.bonus_scale_factor)\n\n        info['n_episodes'] = self.n_episodes\n        info['episode_rewards'] = self._rewards\n        return info",
  "def run_episode(self):\n        # print(f'Episode: {self.episode}')\n\n        state = self.env.reset()\n        episode_rewards = 0\n\n        self.new_gram_mat = np.zeros((self.horizon, self.episode + 1, self.episode + 1))\n        self.new_gram_mat_inv = np.zeros((self.horizon, self.episode + 1, self.episode + 1))\n\n        for step in range(self.horizon):\n            # print(f'Step {step}')\n            if self.bonus_scale_factor == 0.0 or self.episode == 0:\n                action = self.env.action_space.sample()\n            else:\n                action = self._optimistic_policy(step, state)\n\n            next_state, reward, is_terminal, _ = self.env.step(action)\n            # if is_terminal:\n            #     reward = 0.\n\n            # update Gram matrix and its inverse\n            if self.episode == 0:\n                self.new_gram_mat[step] += self.pd_kernel(state, action)\n                self.new_gram_mat_inv[step] = 1. / self.new_gram_mat[step]\n\n            elif self.episode >= 1:\n                feat = self._score_vector(step, state, action)\n                outer_prod = np.outer(feat, feat)\n\n                # update Gram matrix\n                self.new_gram_mat[step, :-1, :-1] = self.gram_mat[step]\n                self.new_gram_mat[step, -1, :-1] = feat\n                self.new_gram_mat[step, :-1, -1] = feat\n                self.new_gram_mat[step, -1, -1] = self.pd_kernel(state, action) + self.reg_factor\n\n                # update Gram inverse\n                K22 = 1.0 / (self.gram_mat[step][-1, -1] + self.reg_factor - feat.T @ self.gram_mat_inv[step] @ feat)\n                K11 = self.gram_mat_inv[step] + K22 * self.gram_mat_inv[step] @ outer_prod @ self.gram_mat_inv[step]\n                K12 = - K22 * self.gram_mat_inv[step] @ feat\n                K21 = - K22 * feat.T @ self.gram_mat_inv[step]\n\n                self.new_gram_mat_inv[step, :-1, :-1] = K11\n                self.new_gram_mat_inv[step, :-1, -1] = K12\n                self.new_gram_mat_inv[step, -1, :-1] = K21\n                self.new_gram_mat_inv[step, -1, -1] = K22\n\n                # store next features\n                # tt = self.total_time_steps\n                # self.feat_hist[tt] = feat\n                # for aa in range(self.env.action_space.n):\n                #     self.feat_ns_all_actions[tt][aa] = self._score_vector(step + 1, next_state, aa)\n\n                for tau in range(self.episode):\n                    pns = self.hist_nstate[step][tau]\n                    for a in range(self.env.action_space.n):\n                        similarity = self.pd_kernel(next_state, action, pns, a)\n                        self.nstate_feat_hist[step, tau, self.episode, a] = similarity\n\n            # update history\n            self.reward_hist[self.total_time_steps] = reward\n            self.state_hist.append(state)\n            self.action_hist.append(action)\n            self.nstate_hist.append(next_state)\n            self.rkhs_norm_hist.append(self.pd_kernel(state, action))\n\n            for a in range(self.env.action_space.n):\n                self.hist_rkhs_norm[step, self.episode, a] = self.pd_kernel(next_state, a)\n\n            self.hist_nstate[step].append(next_state)\n            self.hist_reward[step].append(reward)\n\n            # increments\n            self.total_time_steps += 1\n            episode_rewards += reward\n\n            # update current state\n            state = next_state\n\n            # if is_terminal:\n            #     break\n\n        # store data\n        self._rewards[self.episode] = episode_rewards\n\n        # update episode\n        self.episode += 1\n\n        # log data\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        return episode_rewards",
  "def policy(self, observation, **kwargs):\n        # return self._compute_q_vec(self.alphas[0], 0, observation, self.bonus_scale_factor).argmax()\n        return self._compute_q_vec(0, observation, 0.0).argmax()",
  "def _optimistic_policy(self, step, observation):\n        # return self._compute_q_vec(self.alphas[step], step, observation, self.bonus_scale_factor).argmax()\n        return self._compute_q_vec(step, observation, self.bonus_scale_factor).argmax()",
  "def _compute_q_vec(self, step, state, bonus_factor):\n        n_actions = self.env.action_space.n\n        q_vec = np.zeros(n_actions)\n\n        if step == self.horizon:\n            return q_vec\n\n        else:\n            for aa in range(n_actions):\n                feat = self._score_vector(step, state, aa)\n\n                inverse_counts = self.pd_kernel(state, aa) - feat @ self.gram_mat_inv[step] @ feat\n                bonus = bonus_factor * np.sqrt(inverse_counts) + self.v_max * inverse_counts * (bonus_factor > 0.0)\n\n                q_vec[aa] = feat.T @ self.alphas[step] + bonus\n                q_vec[aa] = min(q_vec[aa], self.v_max)\n\n            return q_vec",
  "def _score_vector(self, step, state, action):\n        # k_h^t (s, a)\n        prev_states = self._get_previous_states(step, by='episode')\n        prev_actions = self._get_previous_actions(step, by='episode')\n\n        similarities = [self.pd_kernel(s, a, state2=state, action2=action) for s, a in zip(prev_states, prev_actions)]\n        return np.array(similarities)",
  "def _get_previous_states(self, index, by='episode'):\n        if by == 'episode':\n            return [self.state_hist[index + ep * self.horizon] for ep in range(self.episode)]\n        elif by == 'step':\n            return [self.state_hist[step + index * self.horizon] for step in range(self.horizon)]",
  "def _get_previous_actions(self, index, by='episode'):\n        if by == 'episode':\n            return [self.action_hist[index + ep * self.horizon] for ep in range(self.episode)]\n        elif by == 'step':\n            return [self.action_hist[step + index * self.horizon] for step in range(self.horizon)]",
  "def _get_previous_rewards(self, index, by='episode'):\n        if by == 'episode':\n            return [self.reward_hist[index + ep * self.horizon] for ep in range(self.episode)]\n        elif by == 'step':\n            return [self.reward_hist[step + index * self.horizon] for step in range(self.horizon)]",
  "def _run_kovi(self, bonus_factor):\n\n        if self.use_jit:\n\n            # print(self.episode)\n            # print(self.env.action_space.n)\n            # print(self.horizon)\n            # print(self.nstate_feat_hist)\n            # print(self.hist_reward)\n            # print(self.hist_rkhs_norm)\n            # print(self.gram_mat_inv)\n            # print(bonus_factor)\n            # print(self.v_max)\n            # print(self.gamma)\n\n            gram_mat_inv = List()\n            for mat in self.gram_mat_inv:\n                gram_mat_inv.append(mat)\n\n            reward_hist = List()\n            for lst in self.hist_reward:\n                reward_hist.append(lst)\n\n            return run_kovi_jit(self.episode,\n                                self.env.action_space.n,\n                                self.horizon,\n                                self.nstate_feat_hist,\n                                reward_hist,\n                                self.hist_rkhs_norm,\n                                gram_mat_inv,\n                                bonus_factor,\n                                self.v_max,\n                                self.gamma)\n\n        else:\n            q_ns = np.zeros((self.episode, self.env.action_space.n))\n            alphas = np.zeros((self.horizon + 1, self.episode))\n\n            for step in range(self.horizon - 1, -1, -1):\n                # build targets\n                prev_rewards = self._get_previous_rewards(step, by='episode')\n                targets = prev_rewards + self.gamma * q_ns.max(axis=1)\n\n                # update parameters solving kernel ridge regression\n                alphas[step] = self.gram_mat_inv[step] @ targets\n                self.alphas[step] = alphas[step]\n\n                # update value function\n                # prev_ns = self._get_previous_states(step + 1, by='episode')\n                # prev_ns = [self.nstate_hist[step + ep * self.horizon] for ep in range(self.episode)]\n                # q_ns = np.array([self._compute_q_vec(step + 1, ns, self.bonus_scale_factor) for ns in prev_ns])\n\n                for tau in range(self.episode):\n                    pns = self.hist_nstate[step][tau]\n                    for a in range(self.env.action_space.n):\n                        feat = self.nstate_feat_hist[step, tau, :self.episode, a]\n\n                        # if a == 0:\n                            # print(f'Episode {self.episode}, step {step}, tau {tau}, feat: {feat.shape}, gram_mat_inv[step]: {self.gram_mat_inv[step].shape}')\n                        inv_counts = self.pd_kernel(pns, a) - feat @ self.gram_mat_inv[step] @ feat\n                        bonus = bonus_factor * np.sqrt(inv_counts) + self.v_max * inv_counts * (bonus_factor > 0.0)\n                        q_ns[tau, a] = feat.T @ self.alphas[step] + bonus\n                        q_ns[tau, a] = min(q_ns[tau, a], self.v_max)\n\n            return alphas",
  "class KOVIAgent(Agent):\n    \"\"\"\n    A version of Kernelized Optimistic Least-Squares Value Iteration (KOVI),\n    proposed by Jin et al. (2020).\n\n    If bonus_scale_factor is 0.0, performs random exploration.\n\n    Parameters\n    ----------\n    env : Model\n        Online model of an environment.\n    horizon : int\n        Maximum length of each episode.\n    feature_map_fn : function(env, kwargs)\n        Function that returns a feature map instance\n        (rlberry.agents.features.FeatureMap class).\n    feature_map_kwargs:\n        kwargs for feature_map_fn\n    n_episodes : int\n        number of episodes\n    gamma : double\n        Discount factor.\n    bonus_scale_factor : double\n        Constant by which to multiply the exploration bonus.\n    reg_factor : double\n        Linear regression regularization factor.\n\n    References\n    ----------\n\n    \"\"\"\n\n    name = 'KOVI'\n\n    def __init__(self,\n                 env,\n                 horizon,\n                 pd_kernel_fn,\n                 pd_kernel_kwargs=None,\n                 n_episodes=100,\n                 gamma=0.99,\n                 bonus_scale_factor=1.0,\n                 reg_factor=0.1,\n                 **kwargs):\n        Agent.__init__(self, env, **kwargs)\n\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.bonus_scale_factor = bonus_scale_factor\n        self.reg_factor = reg_factor\n        pd_kernel_kwargs = pd_kernel_kwargs or {}\n        self.pd_kernel = pd_kernel_fn\n\n        #\n        if self.bonus_scale_factor == 0.0:\n            self.name = 'KOVI-Random-Expl'\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf:\n            logger.warning(\"{}: Reward range is infinity. \".format(self.name)\n                           + \"Clipping it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon))\\\n                 / (1.0 - self.gamma)\n\n        #\n        assert isinstance(self.env.action_space, Discrete), \\\n            \"KOVI requires discrete actions.\"\n\n        # attributes initialized in reset()\n        self.episode = None\n        self.gram_mat = None        # Gram matrix\n        self.gram_mat_inv = None    # inverse of Gram matrix\n        self.alphas = None          #\n\n        self.reward_hist = None     # reward history\n        self.state_hist = None      # state history\n        self.action_hist = None     # action history\n        self.nstate_hist = None     # next state history\n\n        # aux variables (init in reset() too)\n        self._rewards = None\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=15)\n\n        #\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.episode = 0\n        self.total_time_steps = 0\n        self.targets = [[] for _ in range(self.horizon)]\n        self.gram_mat = [np.array([self.reg_factor]) for _ in range(self.horizon)]  # self.reg_factor * np.eye(self.dim)\n        self.gram_mat_inv = [np.array([1.0 / self.reg_factor]) for _ in range(self.horizon)]\n        self.alphas = [np.array([1]) for _ in range(self.horizon)]\n\n        self.reward_hist = [[] for _ in range(self.horizon)]\n        self.state_hist = [[] for _ in range(self.horizon + 1)]\n        self.action_hist = [[] for _ in range(self.horizon)]\n        self.nstate_hist = []\n\n        # episode rewards\n        self._rewards = np.zeros(self.n_episodes)\n\n    def bonus(self, step, state, action):\n        # b_h^t (s, a)\n        kh = self.score_vector(step, state, action)\n\n        tmp1 = self.pd_kernel((state, action))\n\n        print('kh', kh)\n        print('gram inv', self.gram_mat_inv[step])\n        if self.episode == 0:\n            tmp2 = kh * self.gram_mat_inv[step] * kh\n        else:\n            tmp2 = kh.T @ self.gram_mat_inv[step] @ kh\n\n        bonus = np.sqrt((tmp1 - tmp2) / self.reg_factor)\n        # bonus = np.sqrt((self.pd_kernel((state, action)) - kh.T @ self.gram_mat_inv[step] @ kh) / self.reg_factor)\n        return bonus\n\n\n    def score_vector(self, step, state, action):\n        # k_h^t (s, a)\n        previous_states = self.state_hist[step]\n        previous_actions = self.action_hist[step]\n\n        similarities = [self.pd_kernel((s, a), (state, action)) for s, a in zip(previous_states, previous_actions)]\n        return np.array(similarities)\n\n    def q_function(self, step, state, action):\n        # Q_h^t (s, a)\n        mean = self.score_vector(step, state, action).T @ self.alphas[step]\n        bonus = self.bonus(step, state, action)\n        Q = mean + self.bonus_scale_factor * bonus\n\n        return max(min(Q, self.horizon - step + 1), 0)\n\n    def value_function(self, step, state):\n        # V_h^t (s)\n        if isinstance(state, list):\n            n_states = len(state)\n\n            if step == self.horizon:\n                return np.zeros(n_states)\n            else:\n                return np.array([np.max([self.q_function(step, s, a) for a in range(self.env.action_space.n)]) for s in state])\n\n        else:\n            if step == self.horizon:\n                return 0\n            else:\n                return np.max([self.q_function(step, state, a) for a in range(self.env.action_space.n)])\n\n    def policy(self, state, **kwargs):\n        return\n\n    def _optimistic_policy(self, step, state):\n        return np.argmax([self.q_function(step, state, a) for a in range(self.env.action_space.n)])\n\n    def fit(self, **kwargs):\n        info = {}\n        for ep in range(self.n_episodes):\n            state = self.env.reset()\n\n            print('Episode: {}'.format(ep))\n            self.episode = ep\n            # run episode\n            print('Run episode --')\n            for step in range(self.horizon):\n\n                print('Step: {}'.format(step))\n\n                if ep == 0:\n                    action = 0\n                else:\n                    action = self._optimistic_policy(step, state)\n                next_state, reward, is_terminal, _ = self.env.step(action)\n\n                # update history\n                self.reward_hist[step].append(reward)\n                self.state_hist[step].append(state)\n                self.action_hist[step].append(action)\n\n                self._rewards[ep] += reward\n\n                state = next_state\n\n            # run kovi\n            print('Run KOVI --')\n            for step in range(self.horizon - 1, -1, -1):\n                print('Step: {}'.format(step))\n\n                # y_h^t\n                self.targets[step] = self.reward_hist[step] + self.value_function(step + 1, self.state_hist[step])\n\n                if ep > 0:\n                    # K_h^t\n                    Kaa = self.gram_mat[step]\n                    Kab = self.score_vector(step, self.state_hist[step][-1], self.action_hist[step][-1])[:-1].reshape(-1, 1)\n                    Kba = Kab.reshape(1, -1)\n                    Kbb = np.array([self.pd_kernel((self.state_hist[step][-1], self.action_hist[step][-1]))])\n\n                    tmp1 = np.hstack((Kaa, Kab))\n                    tmp2 = np.hstack((Kba, Kbb))\n\n                    self.gram_mat[step] = np.vstack((tmp1, tmp2))\n\n                    # (K_h^t)^-1\n                    Kdd = 1.0 / (Kbb + self.reg_factor - Kba @ self.gram_mat_inv[step] @ Kab)\n                    Kcc = self.gram_mat_inv[step] + Kdd * self.gram_mat_inv[step] @ np.outer(Kab, Kab) @ self.gram_mat_inv[step]\n                    Kcd = - Kdd * self.gram_mat_inv[step] @ Kab\n                    Kdc = - Kdd * Kba @ self.gram_mat_inv[step]\n                    self.gram_mat_inv[step] = np.vstack((np.hstack((Kcc, Kcd)), np.hstack((Kdc, Kdd))))\n\n                # alpha_h^t\n                self.alphas[step] = self.gram_mat_inv[step] @ self.targets[step]\n                if ep == 0:\n                    self.alphas[step] = np.array([self.alphas[step]])\n\n\n        info['n_episodes'] = self.n_episodes\n        info['episode_rewards'] = self._rewards\n        return info",
  "def __init__(self,\n                 env,\n                 horizon,\n                 pd_kernel_fn,\n                 pd_kernel_kwargs=None,\n                 n_episodes=100,\n                 gamma=0.99,\n                 bonus_scale_factor=1.0,\n                 reg_factor=0.1,\n                 **kwargs):\n        Agent.__init__(self, env, **kwargs)\n\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.bonus_scale_factor = bonus_scale_factor\n        self.reg_factor = reg_factor\n        pd_kernel_kwargs = pd_kernel_kwargs or {}\n        self.pd_kernel = pd_kernel_fn\n\n        #\n        if self.bonus_scale_factor == 0.0:\n            self.name = 'KOVI-Random-Expl'\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf:\n            logger.warning(\"{}: Reward range is infinity. \".format(self.name)\n                           + \"Clipping it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon))\\\n                 / (1.0 - self.gamma)\n\n        #\n        assert isinstance(self.env.action_space, Discrete), \\\n            \"KOVI requires discrete actions.\"\n\n        # attributes initialized in reset()\n        self.episode = None\n        self.gram_mat = None        # Gram matrix\n        self.gram_mat_inv = None    # inverse of Gram matrix\n        self.alphas = None          #\n\n        self.reward_hist = None     # reward history\n        self.state_hist = None      # state history\n        self.action_hist = None     # action history\n        self.nstate_hist = None     # next state history\n\n        # aux variables (init in reset() too)\n        self._rewards = None\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=15)\n\n        #\n        self.reset()",
  "def reset(self, **kwargs):\n        self.episode = 0\n        self.total_time_steps = 0\n        self.targets = [[] for _ in range(self.horizon)]\n        self.gram_mat = [np.array([self.reg_factor]) for _ in range(self.horizon)]  # self.reg_factor * np.eye(self.dim)\n        self.gram_mat_inv = [np.array([1.0 / self.reg_factor]) for _ in range(self.horizon)]\n        self.alphas = [np.array([1]) for _ in range(self.horizon)]\n\n        self.reward_hist = [[] for _ in range(self.horizon)]\n        self.state_hist = [[] for _ in range(self.horizon + 1)]\n        self.action_hist = [[] for _ in range(self.horizon)]\n        self.nstate_hist = []\n\n        # episode rewards\n        self._rewards = np.zeros(self.n_episodes)",
  "def bonus(self, step, state, action):\n        # b_h^t (s, a)\n        kh = self.score_vector(step, state, action)\n\n        tmp1 = self.pd_kernel((state, action))\n\n        print('kh', kh)\n        print('gram inv', self.gram_mat_inv[step])\n        if self.episode == 0:\n            tmp2 = kh * self.gram_mat_inv[step] * kh\n        else:\n            tmp2 = kh.T @ self.gram_mat_inv[step] @ kh\n\n        bonus = np.sqrt((tmp1 - tmp2) / self.reg_factor)\n        # bonus = np.sqrt((self.pd_kernel((state, action)) - kh.T @ self.gram_mat_inv[step] @ kh) / self.reg_factor)\n        return bonus",
  "def score_vector(self, step, state, action):\n        # k_h^t (s, a)\n        previous_states = self.state_hist[step]\n        previous_actions = self.action_hist[step]\n\n        similarities = [self.pd_kernel((s, a), (state, action)) for s, a in zip(previous_states, previous_actions)]\n        return np.array(similarities)",
  "def q_function(self, step, state, action):\n        # Q_h^t (s, a)\n        mean = self.score_vector(step, state, action).T @ self.alphas[step]\n        bonus = self.bonus(step, state, action)\n        Q = mean + self.bonus_scale_factor * bonus\n\n        return max(min(Q, self.horizon - step + 1), 0)",
  "def value_function(self, step, state):\n        # V_h^t (s)\n        if isinstance(state, list):\n            n_states = len(state)\n\n            if step == self.horizon:\n                return np.zeros(n_states)\n            else:\n                return np.array([np.max([self.q_function(step, s, a) for a in range(self.env.action_space.n)]) for s in state])\n\n        else:\n            if step == self.horizon:\n                return 0\n            else:\n                return np.max([self.q_function(step, state, a) for a in range(self.env.action_space.n)])",
  "def policy(self, state, **kwargs):\n        return",
  "def _optimistic_policy(self, step, state):\n        return np.argmax([self.q_function(step, state, a) for a in range(self.env.action_space.n)])",
  "def fit(self, **kwargs):\n        info = {}\n        for ep in range(self.n_episodes):\n            state = self.env.reset()\n\n            print('Episode: {}'.format(ep))\n            self.episode = ep\n            # run episode\n            print('Run episode --')\n            for step in range(self.horizon):\n\n                print('Step: {}'.format(step))\n\n                if ep == 0:\n                    action = 0\n                else:\n                    action = self._optimistic_policy(step, state)\n                next_state, reward, is_terminal, _ = self.env.step(action)\n\n                # update history\n                self.reward_hist[step].append(reward)\n                self.state_hist[step].append(state)\n                self.action_hist[step].append(action)\n\n                self._rewards[ep] += reward\n\n                state = next_state\n\n            # run kovi\n            print('Run KOVI --')\n            for step in range(self.horizon - 1, -1, -1):\n                print('Step: {}'.format(step))\n\n                # y_h^t\n                self.targets[step] = self.reward_hist[step] + self.value_function(step + 1, self.state_hist[step])\n\n                if ep > 0:\n                    # K_h^t\n                    Kaa = self.gram_mat[step]\n                    Kab = self.score_vector(step, self.state_hist[step][-1], self.action_hist[step][-1])[:-1].reshape(-1, 1)\n                    Kba = Kab.reshape(1, -1)\n                    Kbb = np.array([self.pd_kernel((self.state_hist[step][-1], self.action_hist[step][-1]))])\n\n                    tmp1 = np.hstack((Kaa, Kab))\n                    tmp2 = np.hstack((Kba, Kbb))\n\n                    self.gram_mat[step] = np.vstack((tmp1, tmp2))\n\n                    # (K_h^t)^-1\n                    Kdd = 1.0 / (Kbb + self.reg_factor - Kba @ self.gram_mat_inv[step] @ Kab)\n                    Kcc = self.gram_mat_inv[step] + Kdd * self.gram_mat_inv[step] @ np.outer(Kab, Kab) @ self.gram_mat_inv[step]\n                    Kcd = - Kdd * self.gram_mat_inv[step] @ Kab\n                    Kdc = - Kdd * Kba @ self.gram_mat_inv[step]\n                    self.gram_mat_inv[step] = np.vstack((np.hstack((Kcc, Kcd)), np.hstack((Kdc, Kdd))))\n\n                # alpha_h^t\n                self.alphas[step] = self.gram_mat_inv[step] @ self.targets[step]\n                if ep == 0:\n                    self.alphas[step] = np.array([self.alphas[step]])\n\n\n        info['n_episodes'] = self.n_episodes\n        info['episode_rewards'] = self._rewards\n        return info",
  "class MBQVIAgent(Agent):\n    \"\"\"\n    Model-Basel Q-Value iteration (MBQVI).\n\n    Builds an empirical MDP and runs value iteration on it.\n    Corresponds to the \"indirect\" algorithm studied by Kearns and Singh (1999).\n\n    Parameters\n    -----------\n    env : Model\n        generative model with finite state-action space\n    n_samples : int\n        number of samples *per state-action pair* used to estimate\n        the empirical MDP.\n    gamma : double\n        discount factor in [0, 1]\n    horizon : int\n        horizon, if the problem is finite-horizon. if None, the discounted\n        problem is solved. default = None\n    epsilon : double\n        precision of value iteration, only used in discounted problems\n        (when horizon is None).\n\n\n    References\n    ----------\n    Kearns, Michael J., and Satinder P. Singh.\n    \"Finite-sample convergence rates for Q-learning and indirect algorithms.\"\n    Advances in neural information processing systems. 1999.\n    \"\"\"\n\n    name = \"MBQVI\"\n\n    def __init__(self, env,\n                 n_samples=10,\n                 gamma=0.95,\n                 horizon=None,\n                 epsilon=1e-6,\n                 **kwargs):\n        # initialize base class\n        assert env.is_generative(), \\\n            \"MBQVI requires a generative model.\"\n        assert isinstance(env.observation_space, Discrete), \\\n            \"MBQVI requires a finite state space.\"\n        assert isinstance(env.action_space, Discrete), \\\n            \"MBQVI requires a finite action space.\"\n        Agent.__init__(self, env, **kwargs)\n\n        #\n        self.n_samples = n_samples\n        self.gamma = gamma\n        self.horizon = horizon\n        self.epsilon = epsilon\n\n        # empirical MDP, created in fit()\n        self.R_hat = None\n        self.P_hat = None\n\n        # value functions\n        self.V = None\n        self.Q = None\n\n    def _update(self, state, action, next_state, reward):\n        \"\"\"Update model statistics.\"\"\"\n        self.N_sa[state, action] += 1\n        self.N_sas[state, action, next_state] += 1\n        self.S_sa[state, action] += reward\n\n    def fit(self, **kwargs):\n        \"\"\"Build empirical MDP and run value iteration.\"\"\"\n        S = self.env.observation_space.n\n        A = self.env.action_space.n\n        self.N_sa = np.zeros((S, A))\n        self.N_sas = np.zeros((S, A, S))\n        self.S_sa = np.zeros((S, A))\n\n        # collect data\n        total_samples = S * A * self.n_samples\n        count = 0\n        logger.debug(\n            f\"[{self.name}] collecting {self.n_samples} samples per (s,a)\"\n            f\", total = {total_samples} samples.\"\n        )\n        for ss in range(S):\n            for aa in range(A):\n                for _ in range(self.n_samples):\n                    next_state, reward, _, _ = self.env.sample(ss, aa)\n                    self._update(ss, aa, next_state, reward)\n\n                    count += 1\n                    if count % 10000 == 0:\n                        completed = 100 * count / total_samples\n                        logger.debug(\"[{}] ... {}/{} ({:0.0f}%)\".format(\n                                                                 self.name,\n                                                                 count,\n                                                                 total_samples,\n                                                                 completed))\n\n        # build model and run VI\n        logger.debug(\n            f\"{self.name} building model and running backward induction...\")\n\n        N_sa = np.maximum(self.N_sa, 1)\n        self.R_hat = self.S_sa / N_sa\n        self.P_hat = np.zeros((S, A, S))\n        for ss in range(S):\n            self.P_hat[:, :, ss] = self.N_sas[:, :, ss] / N_sa\n\n        info = {}\n        info[\"n_samples\"] = self.n_samples\n        info[\"total_samples\"] = total_samples\n        if self.horizon is None:\n            assert self.gamma < 1.0, \\\n                \"The discounted setting requires gamma < 1.0\"\n            self.Q, self.V, n_it = value_iteration(self.R_hat, self.P_hat,\n                                                   self.gamma, self.epsilon)\n            info[\"n_iterations\"] = n_it\n            info[\"precision\"] = self.epsilon\n        else:\n            self.Q, self.V = backward_induction(self.R_hat, self.P_hat,\n                                                self.horizon, self.gamma)\n            info[\"n_iterations\"] = self.horizon\n            info[\"precision\"] = 0.0\n        return info\n\n    def policy(self, state, hh=0, **kwargs):\n        \"\"\"\n        Parameters\n        -----------\n        state : int\n        hh : int\n            Stage when action is taken (for finite horizon problems,\n            the optimal policy depends on hh).\n            Not used if horizon is None.\n        \"\"\"\n        assert self.env.observation_space.contains(state)\n        if self.horizon is None:\n            return self.Q[state, :].argmax()\n        else:\n            assert hh >= 0 and hh < self.horizon\n            return self.Q[hh, state, :].argmax()",
  "def __init__(self, env,\n                 n_samples=10,\n                 gamma=0.95,\n                 horizon=None,\n                 epsilon=1e-6,\n                 **kwargs):\n        # initialize base class\n        assert env.is_generative(), \\\n            \"MBQVI requires a generative model.\"\n        assert isinstance(env.observation_space, Discrete), \\\n            \"MBQVI requires a finite state space.\"\n        assert isinstance(env.action_space, Discrete), \\\n            \"MBQVI requires a finite action space.\"\n        Agent.__init__(self, env, **kwargs)\n\n        #\n        self.n_samples = n_samples\n        self.gamma = gamma\n        self.horizon = horizon\n        self.epsilon = epsilon\n\n        # empirical MDP, created in fit()\n        self.R_hat = None\n        self.P_hat = None\n\n        # value functions\n        self.V = None\n        self.Q = None",
  "def _update(self, state, action, next_state, reward):\n        \"\"\"Update model statistics.\"\"\"\n        self.N_sa[state, action] += 1\n        self.N_sas[state, action, next_state] += 1\n        self.S_sa[state, action] += reward",
  "def fit(self, **kwargs):\n        \"\"\"Build empirical MDP and run value iteration.\"\"\"\n        S = self.env.observation_space.n\n        A = self.env.action_space.n\n        self.N_sa = np.zeros((S, A))\n        self.N_sas = np.zeros((S, A, S))\n        self.S_sa = np.zeros((S, A))\n\n        # collect data\n        total_samples = S * A * self.n_samples\n        count = 0\n        logger.debug(\n            f\"[{self.name}] collecting {self.n_samples} samples per (s,a)\"\n            f\", total = {total_samples} samples.\"\n        )\n        for ss in range(S):\n            for aa in range(A):\n                for _ in range(self.n_samples):\n                    next_state, reward, _, _ = self.env.sample(ss, aa)\n                    self._update(ss, aa, next_state, reward)\n\n                    count += 1\n                    if count % 10000 == 0:\n                        completed = 100 * count / total_samples\n                        logger.debug(\"[{}] ... {}/{} ({:0.0f}%)\".format(\n                                                                 self.name,\n                                                                 count,\n                                                                 total_samples,\n                                                                 completed))\n\n        # build model and run VI\n        logger.debug(\n            f\"{self.name} building model and running backward induction...\")\n\n        N_sa = np.maximum(self.N_sa, 1)\n        self.R_hat = self.S_sa / N_sa\n        self.P_hat = np.zeros((S, A, S))\n        for ss in range(S):\n            self.P_hat[:, :, ss] = self.N_sas[:, :, ss] / N_sa\n\n        info = {}\n        info[\"n_samples\"] = self.n_samples\n        info[\"total_samples\"] = total_samples\n        if self.horizon is None:\n            assert self.gamma < 1.0, \\\n                \"The discounted setting requires gamma < 1.0\"\n            self.Q, self.V, n_it = value_iteration(self.R_hat, self.P_hat,\n                                                   self.gamma, self.epsilon)\n            info[\"n_iterations\"] = n_it\n            info[\"precision\"] = self.epsilon\n        else:\n            self.Q, self.V = backward_induction(self.R_hat, self.P_hat,\n                                                self.horizon, self.gamma)\n            info[\"n_iterations\"] = self.horizon\n            info[\"precision\"] = 0.0\n        return info",
  "def policy(self, state, hh=0, **kwargs):\n        \"\"\"\n        Parameters\n        -----------\n        state : int\n        hh : int\n            Stage when action is taken (for finite horizon problems,\n            the optimal policy depends on hh).\n            Not used if horizon is None.\n        \"\"\"\n        assert self.env.observation_space.contains(state)\n        if self.horizon is None:\n            return self.Q[state, :].argmax()\n        else:\n            assert hh >= 0 and hh < self.horizon\n            return self.Q[hh, state, :].argmax()",
  "def run_lsvi_jit(dim, horizon, bonus_factor, lambda_mat_inv,\n                 reward_hist, gamma, feat_hist, n_actions,\n                 feat_ns_all_actions, v_max,\n                 total_time_steps):\n    \"\"\"\n    Parameters\n    ----------\n    dim : int\n        Dimension of the features\n    horiton : int\n\n    bonus_factor : int\n\n    lambda_mat_inv : numpy array (dim, dim)\n        Inverse of the design matrix\n\n    reward_hist : numpy array (time,)\n\n    gamma : double\n\n    feat_hist : numpy array (time, dim)\n\n    n_actions : int\n\n    feat_ns_all_actions : numpy array (time, n_actions, dim)\n        History of next state features for all actions\n\n    vmax : double\n        Maximum value of the value function\n\n    total_time_steps : int\n        Current step count\n    \"\"\"\n    # run value iteration\n    q_w = np.zeros((horizon+1, dim))\n    for hh in range(horizon - 1, -1, -1):\n        T = total_time_steps\n        b = np.zeros(dim)\n        for tt in range(T):\n            # compute q function at next state, q_ns\n            q_ns = np.zeros(n_actions)\n            for aa in range(n_actions):\n                #\n                feat_ns_aa = feat_ns_all_actions[tt, aa, :]\n                inverse_counts = \\\n                    feat_ns_aa.dot(lambda_mat_inv.T.dot(feat_ns_aa))\n                bonus = bonus_factor * np.sqrt(inverse_counts) \\\n                    + v_max * inverse_counts * (bonus_factor > 0.0)\n                #\n                q_ns[aa] = feat_ns_aa.dot(q_w[hh+1, :]) + bonus\n                q_ns[aa] = min(q_ns[aa], v_max)\n\n            # compute regretion targets\n            target = reward_hist[tt] + gamma*q_ns.max()\n            feat = feat_hist[tt, :]\n            b = b + target*feat\n\n        # solve M x = b, where x = q_w, and M = self.lambda_mat\n        q_w[hh, :] = lambda_mat_inv.T @ b\n    return q_w",
  "class LSVIUCBAgent(Agent):\n    \"\"\"\n    A version of Least-Squares Value Iteration with UCB (LSVI-UCB),\n    proposed by Jin et al. (2020).\n\n    If bonus_scale_factor is 0.0, performs random exploration.\n\n    Parameters\n    ----------\n    env : Model\n        Online model of an environment.\n    horizon : int\n        Maximum length of each episode.\n    feature_map_fn : function(env, kwargs)\n        Function that returns a feature map instance\n        (rlberry.agents.features.FeatureMap class).\n    feature_map_kwargs:\n        kwargs for feature_map_fn\n    n_episodes : int\n        number of episodes\n    gamma : double\n        Discount factor.\n    bonus_scale_factor : double\n        Constant by which to multiply the exploration bonus.\n    reg_factor : double\n        Linear regression regularization factor.\n\n    References\n    ----------\n    Jin, C., Yang, Z., Wang, Z., & Jordan, M. I. (2020, July).\n    Provably efficient reinforcement learning with linear\n    function approximation. In Conference on Learning Theory (pp. 2137-2143).\n    \"\"\"\n\n    name = 'LSVI-UCB'\n\n    def __init__(self,\n                 env,\n                 horizon,\n                 feature_map_fn,\n                 feature_map_kwargs=None,\n                 n_episodes=100,\n                 gamma=0.99,\n                 bonus_scale_factor=1.0,\n                 reg_factor=0.1,\n                 **kwargs):\n        Agent.__init__(self, env, **kwargs)\n\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.bonus_scale_factor = bonus_scale_factor\n        self.reg_factor = reg_factor\n        feature_map_kwargs = feature_map_kwargs or {}\n        self.feature_map = feature_map_fn(self.env, **feature_map_kwargs)\n\n        #\n        if self.bonus_scale_factor == 0.0:\n            self.name = 'LSVI-Random-Expl'\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf:\n            logger.warning(\"{}: Reward range is infinity. \".format(self.name)\n                           + \"Clipping it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon))\\\n                 / (1.0 - self.gamma)\n\n        #\n        assert isinstance(self.env.action_space, Discrete), \\\n            \"LSVI-UCB requires discrete actions.\"\n\n        #\n        assert len(self.feature_map.shape) == 1\n        self.dim = self.feature_map.shape[0]\n\n        # attributes initialized in reset()\n        self.episode = None\n        self.lambda_mat = None      # lambda matrix\n        self.lambda_mat_inv = None  # inverse of lambda matrix\n        self.w_vec = None           # vector representation of Q\n        self.w_policy = None        # representation of Q for final policy\n        self.reward_hist = None     # reward history\n        self.state_hist = None      # state history\n        self.action_hist = None     # action history\n        self.nstate_hist = None     # next state history\n\n        self.feat_hist = None             # feature history\n        self.feat_ns_all_actions = None   # next state features for all actions\n        #\n\n        # aux variables (init in reset() too)\n        self._rewards = None\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=15)\n        # 5*logger.getEffectiveLevel()\n\n        #\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.episode = 0\n        self.total_time_steps = 0\n        self.lambda_mat = self.reg_factor * np.eye(self.dim)\n        self.lambda_mat_inv = (1.0/self.reg_factor) * np.eye(self.dim)\n        self.w_vec = np.zeros((self.horizon+1, self.dim))\n        self.reward_hist = np.zeros(self.n_episodes*self.horizon)\n        self.state_hist = []\n        self.action_hist = []\n        self.nstate_hist = []\n        # episode rewards\n        self._rewards = np.zeros(self.n_episodes)\n        #\n        self.feat_hist = np.zeros((self.n_episodes*self.horizon, self.dim))\n        self.feat_ns_all_actions = np.zeros((self.n_episodes*self.horizon,\n                                             self.env.action_space.n,\n                                             self.dim))\n        #\n        self.w_policy = None\n\n    def fit(self, **kwargs):\n        info = {}\n        for ep in range(self.n_episodes):\n            self.run_episode()\n            if self.bonus_scale_factor > 0.0 or ep == self.n_episodes - 1:\n                # update Q function representation\n                self.w_vec = self._run_lsvi(self.bonus_scale_factor)\n\n        self.w_policy = self._run_lsvi(bonus_factor=0.0)[0, :]\n\n        info['n_episodes'] = self.n_episodes\n        info['episode_rewards'] = self._rewards\n        return info\n\n    def policy(self, observation, **kwargs):\n        q_w = self.w_policy\n        assert q_w is not None\n        #\n        q_vec = self._compute_q_vec(q_w, observation, 0.0)\n        return q_vec.argmax()\n\n    def _optimistic_policy(self, observation, hh):\n        q_w = self.w_vec[hh, :]\n        q_vec = self._compute_q_vec(q_w, observation, self.bonus_scale_factor)\n        return q_vec.argmax()\n\n    def run_episode(self):\n        state = self.env.reset()\n        episode_rewards = 0\n        for hh in range(self.horizon):\n            if self.bonus_scale_factor == 0.0:\n                action = self.env.action_space.sample()\n            else:\n                action = self._optimistic_policy(state, hh)\n\n            next_state, reward, is_terminal, _ = self.env.step(action)\n\n            feat = self.feature_map.map(state, action)\n            outer_prod = np.outer(feat, feat)\n            inv = self.lambda_mat_inv\n\n            #\n            self.lambda_mat += np.outer(feat, feat)\n            # update inverse\n            self.lambda_mat_inv -= \\\n                (inv @ outer_prod @ inv) / (1 + feat @ inv.T @ feat)\n\n            # update history\n            self.reward_hist[self.total_time_steps] = reward\n            self.state_hist.append(state)\n            self.action_hist.append(action)\n            self.nstate_hist.append(next_state)\n\n            #\n            tt = self.total_time_steps\n            self.feat_hist[tt, :] = self.feature_map.map(state, action)\n            for aa in range(self.env.action_space.n):\n                self.feat_ns_all_actions[tt, aa, :] = \\\n                    self.feature_map.map(next_state, aa)\n\n            # increments\n            self.total_time_steps += 1\n            episode_rewards += reward\n\n            #\n            state = next_state\n            if is_terminal:\n                break\n\n        # store data\n        self._rewards[self.episode] = episode_rewards\n\n        # update ep\n        self.episode += 1\n\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        return episode_rewards\n\n    def _compute_q(self, q_w, state, action, bonus_factor):\n        \"\"\"q_w is the vector representation of the Q function.\"\"\"\n        feat = self.feature_map.map(state, action)\n        inverse_counts = feat @ (self.lambda_mat_inv.T @ feat)\n        bonus = bonus_factor * np.sqrt(inverse_counts) \\\n            + self.v_max * inverse_counts * (bonus_factor > 0.0)\n        q = feat.dot(q_w) + bonus\n        return q\n\n    def _compute_q_vec(self, q_w, state, bonus_factor):\n        A = self.env.action_space.n\n        q_vec = np.zeros(A)\n        for aa in range(A):\n            # q_vec[aa] = self._compute_q(q_w, state, aa, bonus_factor)\n            feat = self.feature_map.map(state, aa)\n            inverse_counts = feat @ (self.lambda_mat_inv.T @ feat)\n            bonus = bonus_factor * np.sqrt(inverse_counts) \\\n                + self.v_max * inverse_counts * (bonus_factor > 0.0)\n            q_vec[aa] = feat.dot(q_w) + bonus\n            # q_vec[aa] = min(q_vec[aa], self.v_max)   # !!!!!!!!!\n        return q_vec\n\n    def _run_lsvi(self, bonus_factor):\n        # run value iteration\n        q_w = run_lsvi_jit(self.dim,\n                           self.horizon,\n                           bonus_factor,\n                           self.lambda_mat_inv,\n                           self.reward_hist,\n                           self.gamma,\n                           self.feat_hist,\n                           self.env.action_space.n,\n                           self.feat_ns_all_actions,\n                           self.v_max,\n                           self.total_time_steps)\n        return q_w",
  "def __init__(self,\n                 env,\n                 horizon,\n                 feature_map_fn,\n                 feature_map_kwargs=None,\n                 n_episodes=100,\n                 gamma=0.99,\n                 bonus_scale_factor=1.0,\n                 reg_factor=0.1,\n                 **kwargs):\n        Agent.__init__(self, env, **kwargs)\n\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.bonus_scale_factor = bonus_scale_factor\n        self.reg_factor = reg_factor\n        feature_map_kwargs = feature_map_kwargs or {}\n        self.feature_map = feature_map_fn(self.env, **feature_map_kwargs)\n\n        #\n        if self.bonus_scale_factor == 0.0:\n            self.name = 'LSVI-Random-Expl'\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf:\n            logger.warning(\"{}: Reward range is infinity. \".format(self.name)\n                           + \"Clipping it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon))\\\n                 / (1.0 - self.gamma)\n\n        #\n        assert isinstance(self.env.action_space, Discrete), \\\n            \"LSVI-UCB requires discrete actions.\"\n\n        #\n        assert len(self.feature_map.shape) == 1\n        self.dim = self.feature_map.shape[0]\n\n        # attributes initialized in reset()\n        self.episode = None\n        self.lambda_mat = None      # lambda matrix\n        self.lambda_mat_inv = None  # inverse of lambda matrix\n        self.w_vec = None           # vector representation of Q\n        self.w_policy = None        # representation of Q for final policy\n        self.reward_hist = None     # reward history\n        self.state_hist = None      # state history\n        self.action_hist = None     # action history\n        self.nstate_hist = None     # next state history\n\n        self.feat_hist = None             # feature history\n        self.feat_ns_all_actions = None   # next state features for all actions\n        #\n\n        # aux variables (init in reset() too)\n        self._rewards = None\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=15)\n        # 5*logger.getEffectiveLevel()\n\n        #\n        self.reset()",
  "def reset(self, **kwargs):\n        self.episode = 0\n        self.total_time_steps = 0\n        self.lambda_mat = self.reg_factor * np.eye(self.dim)\n        self.lambda_mat_inv = (1.0/self.reg_factor) * np.eye(self.dim)\n        self.w_vec = np.zeros((self.horizon+1, self.dim))\n        self.reward_hist = np.zeros(self.n_episodes*self.horizon)\n        self.state_hist = []\n        self.action_hist = []\n        self.nstate_hist = []\n        # episode rewards\n        self._rewards = np.zeros(self.n_episodes)\n        #\n        self.feat_hist = np.zeros((self.n_episodes*self.horizon, self.dim))\n        self.feat_ns_all_actions = np.zeros((self.n_episodes*self.horizon,\n                                             self.env.action_space.n,\n                                             self.dim))\n        #\n        self.w_policy = None",
  "def fit(self, **kwargs):\n        info = {}\n        for ep in range(self.n_episodes):\n            self.run_episode()\n            if self.bonus_scale_factor > 0.0 or ep == self.n_episodes - 1:\n                # update Q function representation\n                self.w_vec = self._run_lsvi(self.bonus_scale_factor)\n\n        self.w_policy = self._run_lsvi(bonus_factor=0.0)[0, :]\n\n        info['n_episodes'] = self.n_episodes\n        info['episode_rewards'] = self._rewards\n        return info",
  "def policy(self, observation, **kwargs):\n        q_w = self.w_policy\n        assert q_w is not None\n        #\n        q_vec = self._compute_q_vec(q_w, observation, 0.0)\n        return q_vec.argmax()",
  "def _optimistic_policy(self, observation, hh):\n        q_w = self.w_vec[hh, :]\n        q_vec = self._compute_q_vec(q_w, observation, self.bonus_scale_factor)\n        return q_vec.argmax()",
  "def run_episode(self):\n        state = self.env.reset()\n        episode_rewards = 0\n        for hh in range(self.horizon):\n            if self.bonus_scale_factor == 0.0:\n                action = self.env.action_space.sample()\n            else:\n                action = self._optimistic_policy(state, hh)\n\n            next_state, reward, is_terminal, _ = self.env.step(action)\n\n            feat = self.feature_map.map(state, action)\n            outer_prod = np.outer(feat, feat)\n            inv = self.lambda_mat_inv\n\n            #\n            self.lambda_mat += np.outer(feat, feat)\n            # update inverse\n            self.lambda_mat_inv -= \\\n                (inv @ outer_prod @ inv) / (1 + feat @ inv.T @ feat)\n\n            # update history\n            self.reward_hist[self.total_time_steps] = reward\n            self.state_hist.append(state)\n            self.action_hist.append(action)\n            self.nstate_hist.append(next_state)\n\n            #\n            tt = self.total_time_steps\n            self.feat_hist[tt, :] = self.feature_map.map(state, action)\n            for aa in range(self.env.action_space.n):\n                self.feat_ns_all_actions[tt, aa, :] = \\\n                    self.feature_map.map(next_state, aa)\n\n            # increments\n            self.total_time_steps += 1\n            episode_rewards += reward\n\n            #\n            state = next_state\n            if is_terminal:\n                break\n\n        # store data\n        self._rewards[self.episode] = episode_rewards\n\n        # update ep\n        self.episode += 1\n\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        return episode_rewards",
  "def _compute_q(self, q_w, state, action, bonus_factor):\n        \"\"\"q_w is the vector representation of the Q function.\"\"\"\n        feat = self.feature_map.map(state, action)\n        inverse_counts = feat @ (self.lambda_mat_inv.T @ feat)\n        bonus = bonus_factor * np.sqrt(inverse_counts) \\\n            + self.v_max * inverse_counts * (bonus_factor > 0.0)\n        q = feat.dot(q_w) + bonus\n        return q",
  "def _compute_q_vec(self, q_w, state, bonus_factor):\n        A = self.env.action_space.n\n        q_vec = np.zeros(A)\n        for aa in range(A):\n            # q_vec[aa] = self._compute_q(q_w, state, aa, bonus_factor)\n            feat = self.feature_map.map(state, aa)\n            inverse_counts = feat @ (self.lambda_mat_inv.T @ feat)\n            bonus = bonus_factor * np.sqrt(inverse_counts) \\\n                + self.v_max * inverse_counts * (bonus_factor > 0.0)\n            q_vec[aa] = feat.dot(q_w) + bonus\n            # q_vec[aa] = min(q_vec[aa], self.v_max)   # !!!!!!!!!\n        return q_vec",
  "def _run_lsvi(self, bonus_factor):\n        # run value iteration\n        q_w = run_lsvi_jit(self.dim,\n                           self.horizon,\n                           bonus_factor,\n                           self.lambda_mat_inv,\n                           self.reward_hist,\n                           self.gamma,\n                           self.feat_hist,\n                           self.env.action_space.n,\n                           self.feat_ns_all_actions,\n                           self.v_max,\n                           self.total_time_steps)\n        return q_w",
  "def backward_induction(R, P, horizon, gamma=1.0, vmax=np.inf):\n    \"\"\"Backward induction to compute Q and V functions in the finite horizon\n    setting.\n\n    Parameters\n    ----------\n    R : numpy.ndarray\n        array of shape (S, A) contaning the rewards, where S is the number\n        of states and A is the number of actions\n    P : numpy.ndarray\n        array of shape (S, A, S) such that P[s,a,ns] is the probability of\n        arriving at ns by taking action a in state s.\n    horizon : int\n        problem horizon\n    gamma : double, default: 1.0\n        discount factor\n    vmax : double, default: np.inf\n        maximum possible value in V\n\n    Returns\n    --------\n    tuple (Q, V) containing the Q and V functions, of shapes (horizon, S, A)\n    and (horizon, S), respectively.\n    \"\"\"\n    S, A = R.shape\n    V = np.zeros((horizon, S))\n    Q = np.zeros((horizon, S, A))\n    for hh in range(horizon - 1, -1, -1):\n        for ss in range(S):\n            max_q = -np.inf\n            for aa in range(A):\n                q_aa = R[ss, aa]\n                if hh < horizon - 1:\n                    # not using .dot instead of loop to avoid scipy dependency\n                    # (numba seems to require scipy for linear\n                    # algebra operations in numpy)\n                    for ns in range(S):\n                        q_aa += gamma * P[ss, aa, ns] * V[hh + 1, ns]\n                if q_aa > max_q:\n                    max_q = q_aa\n                Q[hh, ss, aa] = q_aa\n            V[hh, ss] = max_q\n            if V[hh, ss] > vmax:\n                V[hh, ss] = vmax\n    return Q, V",
  "def backward_induction_in_place(Q, V, R, P, horizon, gamma=1.0, vmax=np.inf):\n    \"\"\"\n    Backward induction to compute Q and V functions in\n    the finite horizon setting.\n    Takes as input the arrays where to store Q and V.\n\n    Parameters\n    ----------\n    Q:  numpy.ndarray\n        array of shape (horizon, S, A) where to store the Q function\n    V:  numpy.ndarray\n        array of shape (horizon, S) where to store the V function\n    R : numpy.ndarray\n        array of shape (S, A) contaning the rewards, where S is the number\n        of states and A is the number of actions\n    P : numpy.ndarray\n        array of shape (S, A, S) such that P[s,a,ns] is the probability of\n        arriving at ns by taking action a in state s.\n    horizon : int\n        problem horizon\n    gamma : double\n        discount factor, default = 1.0\n    vmax : double\n        maximum possible value in V\n        default = np.inf\n    \"\"\"\n    S, A = R.shape\n    for hh in range(horizon - 1, -1, -1):\n        for ss in range(S):\n            max_q = -np.inf\n            for aa in range(A):\n                q_aa = R[ss, aa]\n                if hh < horizon - 1:\n                    # not using .dot instead of loop to avoid scipy dependency\n                    # (numba seems to require scipy for linear algebra\n                    #  operations in numpy)\n                    for ns in range(S):\n                        q_aa += gamma * P[ss, aa, ns] * V[hh + 1, ns]\n                if q_aa > max_q:\n                    max_q = q_aa\n                Q[hh, ss, aa] = q_aa\n            V[hh, ss] = max_q\n            if V[hh, ss] > vmax:\n                V[hh, ss] = vmax",
  "def backward_induction_sd(Q, V, R, P, gamma=1.0, vmax=np.inf):\n    \"\"\"\n    In-place implementation of backward induction to compute Q and V functions\n    in the finite horizon setting.\n\n    Assumes R and P are stage-dependent.\n\n    Parameters\n    ----------\n    Q:  numpy.ndarray\n        array of shape (H, S, A) where to store the Q function\n    V:  numpy.ndarray\n        array of shape (H, S) where to store the V function\n    R : numpy.ndarray\n        array of shape (H, S, A) contaning the rewards, where S is the number\n        of states and A is the number of actions\n    P : numpy.ndarray\n        array of shape (H, S, A, S) such that P[h, s, a, ns] is the probability of\n        arriving at ns by taking action a in state s at stage h.\n    gamma : double, default: 1.0\n        discount factor\n    vmax : double, default: np.inf\n        maximum possible value in V\n\n    \"\"\"\n    H, S, A = R.shape\n    for hh in range(H - 1, -1, -1):\n        for ss in range(S):\n            max_q = -np.inf\n            for aa in range(A):\n                q_aa = R[hh, ss, aa]\n                if hh < H - 1:\n                    # not using .dot instead of loop to avoid scipy dependency\n                    # (numba seems to require scipy for linear\n                    # algebra operations in numpy)\n                    for ns in range(S):\n                        q_aa += gamma * P[hh, ss, aa, ns] * V[hh + 1, ns]\n                if q_aa > max_q:\n                    max_q = q_aa\n                Q[hh, ss, aa] = q_aa\n            V[hh, ss] = max_q\n            # clip V\n            if V[hh, ss] > vmax:\n                V[hh, ss] = vmax",
  "def value_iteration(R, P, gamma, epsilon=1e-6):\n    \"\"\"\n    Value iteration for discounted problems.\n\n    Parameters\n    ----------\n    R : numpy.ndarray\n        array of shape (S, A) contaning the rewards, where S is the number\n        of states and A is the number of actions\n    P : numpy.ndarray\n        array of shape (S, A, S) such that P[s,a,ns] is the probability of\n        arriving at ns by taking action a in state s.\n    gamma : double\n        discount factor\n    epsilon : double\n        precision\n\n    Returns\n    --------\n    tuple (Q, V, n_it) containing the epsilon-optimal Q and V functions,\n    of shapes (S, A) and (S,), respectively, and n_it, the number of iterations\n    \"\"\"\n    S, A = R.shape\n    Q = np.zeros((S, A))\n    Q_aux = np.full((S, A), np.inf)\n    n_it = 0\n    while np.abs(Q - Q_aux).max() > epsilon:\n        Q_aux = Q\n        Q = bellman_operator(Q, R, P, gamma)\n        n_it += 1\n    V = np.zeros(S)\n    # numba does not support np.max(Q, axis=1)\n    for ss in range(S):\n        V[ss] = Q[ss, :].max()\n    return Q, V, n_it",
  "def bellman_operator(Q, R, P, gamma):\n    \"\"\"\n    Bellman optimality operator for Q functions\n\n    Parameters\n    ----------\n    Q : numpy.ndarray\n        array of shape (S, A) containing the Q function to which apply\n        the operator\n    R : numpy.ndarray\n        array of shape (S, A) contaning the rewards, where S is the number\n        of states and A is the number of actions\n    P : numpy.ndarray\n        array of shape (S, A, S) such that P[s,a,ns] is the probability of\n        arriving at ns by taking action a in state s.\n    gamma : double\n        discount factor\n\n    Returns\n    --------\n    TQ, array of shape (S, A) containing the result of the Bellman operator\n    applied to the input Q\n    \"\"\"\n    S, A = Q.shape\n    TQ = np.zeros((S, A))\n    V = np.zeros(S)\n    # numba does not support np.max(Q, axis=1)\n    for ss in range(S):\n        V[ss] = Q[ss, :].max()\n    #\n    for ss in range(S):\n        for aa in range(A):\n            TQ[ss, aa] = R[ss, aa]\n            for ns in range(S):\n                TQ[ss, aa] += gamma * P[ss, aa, ns] * V[ns]\n    return TQ",
  "class ValueIterationAgent(Agent):\n    \"\"\"\n    Value iteration for enviroments of type FiniteMDP\n    (rlberry.envs.finite.finite_mdp.FiniteMDP)\n\n    Important: the discount gamma is also used if the problem is\n    finite horizon, but, in this case, gamma can be set to 1.0.\n\n    Parameters\n    -----------\n    env : rlberry.envs.finite.finite_mdp.FiniteMDP\n    gamma : double\n        discount factor in [0, 1]\n    horizon : int\n        horizon, if the problem is finite-horizon. if None, the discounted\n        problem is solved\n        default = None\n    epsilon : double\n        precision of value iteration, only used in discounted problems\n        (when horizon is None).\n    \"\"\"\n\n    name = \"ValueIteration\"\n\n    def __init__(self, env, gamma=0.95, horizon=None, epsilon=1e-6, **kwargs):\n        # initialize base class\n        assert isinstance(env, FiniteMDP), \\\n            \"Value iteration requires a FiniteMDP model.\"\n        Agent.__init__(self, env, **kwargs)\n\n        #\n        self.gamma = gamma\n        self.horizon = horizon\n        self.epsilon = epsilon\n\n        # value functions\n        self.Q = None\n        self.V = None\n\n    def fit(self, **kwargs):\n        \"\"\"\n        Run value iteration.\n        \"\"\"\n        info = {}\n        if self.horizon is None:\n            assert self.gamma < 1.0, \\\n                \"The discounted setting requires gamma < 1.0\"\n            self.Q, self.V, n_it = value_iteration(self.env.R, self.env.P,\n                                                   self.gamma, self.epsilon)\n            info[\"n_iterations\"] = n_it\n            info[\"precision\"] = self.epsilon\n        else:\n            self.Q, self.V = backward_induction(self.env.R, self.env.P,\n                                                self.horizon, self.gamma)\n            info[\"n_iterations\"] = self.horizon\n            info[\"precision\"] = 0.0\n        return info\n\n    def policy(self, state, hh=0, **kwargs):\n        \"\"\"\n        Parameters\n        -----------\n        state : int\n        hh : int\n            stage when action is taken (for finite horizon problems,\n            the optimal policy depends on hh) not used if horizon is None.\n        \"\"\"\n        # assert self.env.observation_space.contains(state)\n        if self.horizon is None:\n            return self.Q[state, :].argmax()\n        else:\n            assert hh >= 0 and hh < self.horizon\n            return self.Q[hh, state, :].argmax()",
  "def __init__(self, env, gamma=0.95, horizon=None, epsilon=1e-6, **kwargs):\n        # initialize base class\n        assert isinstance(env, FiniteMDP), \\\n            \"Value iteration requires a FiniteMDP model.\"\n        Agent.__init__(self, env, **kwargs)\n\n        #\n        self.gamma = gamma\n        self.horizon = horizon\n        self.epsilon = epsilon\n\n        # value functions\n        self.Q = None\n        self.V = None",
  "def fit(self, **kwargs):\n        \"\"\"\n        Run value iteration.\n        \"\"\"\n        info = {}\n        if self.horizon is None:\n            assert self.gamma < 1.0, \\\n                \"The discounted setting requires gamma < 1.0\"\n            self.Q, self.V, n_it = value_iteration(self.env.R, self.env.P,\n                                                   self.gamma, self.epsilon)\n            info[\"n_iterations\"] = n_it\n            info[\"precision\"] = self.epsilon\n        else:\n            self.Q, self.V = backward_induction(self.env.R, self.env.P,\n                                                self.horizon, self.gamma)\n            info[\"n_iterations\"] = self.horizon\n            info[\"precision\"] = 0.0\n        return info",
  "def policy(self, state, hh=0, **kwargs):\n        \"\"\"\n        Parameters\n        -----------\n        state : int\n        hh : int\n            stage when action is taken (for finite horizon problems,\n            the optimal policy depends on hh) not used if horizon is None.\n        \"\"\"\n        # assert self.env.observation_space.contains(state)\n        if self.horizon is None:\n            return self.Q[state, :].argmax()\n        else:\n            assert hh >= 0 and hh < self.horizon\n            return self.Q[hh, state, :].argmax()",
  "class TRPOAgent(IncrementalAgent):\n    \"\"\"\n    Parameters\n    ----------\n    env : Model\n        Online model with continuous (Box) state space and discrete actions\n    n_episodes : int\n        Number of episodes\n    batch_size : int\n        Number of *episodes* to wait before updating the policy.\n    horizon : int\n        Horizon.\n    gamma : double\n        Discount factor in [0, 1].\n    entr_coef : double\n        Entropy coefficient.\n    vf_coef : double\n        Value function loss coefficient.\n    learning_rate : double\n        Learning rate.\n    optimizer_type: str\n        Type of optimizer. 'ADAM' by defaut.\n    eps_clip : double\n        PPO clipping range (epsilon).\n    k_epochs : int\n        Number of epochs per update.\n    policy_net_fn : function(env, **kwargs)\n        Function that returns an instance of a policy network (pytorch).\n        If None, a default net is used.\n    value_net_fn : function(env, **kwargs)\n        Function that returns an instance of a value network (pytorch).\n        If None, a default net is used.\n    policy_net_kwargs : dict\n        kwargs for policy_net_fn\n    value_net_kwargs : dict\n        kwargs for value_net_fn\n    device: str\n        Device to put the tensors on\n    use_bonus : bool, default = False\n        If true, compute the environment 'exploration_bonus'\n        and add it to the reward. See also UncertaintyEstimatorWrapper.\n    uncertainty_estimator_kwargs : dict\n        kwargs for UncertaintyEstimatorWrapper\n\n    References\n    ----------\n    Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015).\n    \"Trust region policy optimization.\"\n    In International Conference on Machine Learning (pp. 1889-1897).\n    \"\"\"\n\n    name = \"TRPO\"\n\n    def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 vf_coef=0.5,\n                 learning_rate=0.01,\n                 optimizer_type='ADAM',\n                 k_epochs=5,\n                 use_gae=True,\n                 gae_lambda=0.95,\n                 policy_net_fn=None,\n                 value_net_fn=None,\n                 policy_net_kwargs=None,\n                 value_net_kwargs=None,\n                 device=\"cuda:best\",\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 **kwargs):\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env, **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.horizon = horizon\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.vf_coef = vf_coef\n        self.learning_rate = learning_rate\n        self.k_epochs = k_epochs\n        self.use_gae = use_gae\n        self.gae_lambda = gae_lambda\n        self.damping = 0  # TODO: turn into argument\n        self.max_kl = 0.1  # TODO: turn into argument\n        self.use_entropy = False  # TODO: test, and eventually turn into argument\n        self.normalize_advantage = True  # TODO: turn into argument\n        self.normalize_reward = False  # TODO: turn into argument\n\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.value_net_kwargs = value_net_kwargs or {}\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.value_net_fn = value_net_fn or default_value_net_fn\n\n        self.device = choose_device(device)\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # TODO: check\n        self.cat_policy = None  # categorical policy function\n        self.policy_optimizer = None\n\n        self.value_net = None\n        self.value_optimizer = None\n\n        self.cat_policy_old = None\n\n        self.value_loss_fn = None\n\n        self.memory = None\n\n        self.episode = 0\n\n        self._rewards = None\n        self._cumul_rewards = None\n\n        # initialize\n        self.reset()\n\n    @classmethod\n    def from_config(cls, **kwargs):\n        kwargs[\"policy_net_fn\"] = eval(kwargs[\"policy_net_fn\"])\n        kwargs[\"value_net_fn\"] = eval(kwargs[\"value_net_fn\"])\n        return cls(**kwargs)\n\n    def reset(self, **kwargs):\n        self.cat_policy = self.policy_net_fn(self.env, **self.policy_net_kwargs).to(self.device)\n        self.policy_optimizer = optimizer_factory(self.cat_policy.parameters(), **self.optimizer_kwargs)\n\n        self.value_net = self.value_net_fn(self.env, **self.value_net_kwargs).to(self.device)\n        self.value_optimizer = optimizer_factory(self.value_net.parameters(), **self.optimizer_kwargs)\n\n        self.cat_policy_old = self.policy_net_fn(self.env, **self.policy_net_kwargs).to(self.device)\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n        self.value_loss_fn = nn.MSELoss()  # TODO: turn into argument\n\n        self.memory = Memory()\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name, log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, state, **kwargs):\n        assert self.cat_policy is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample().item()\n        return action\n\n    def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction * self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode, \"episode_rewards\": self._rewards[:self.episode]}\n        return info\n\n    def _select_action(self, state):\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample()\n        action_logprob = action_dist.log_prob(action)\n\n        return action, action_logprob\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # running policy_old\n            action, log_prob = self._select_action(state)\n            next_state, reward, done, info = self.env.step(action.item())\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save in batch\n            self.memory.states.append(torch.from_numpy(state).float().to(self.device))\n            self.memory.actions.append(action)\n            self.memory.logprobs.append(log_prob)\n            self.memory.rewards.append(reward + bonus)  # bonus added here\n            self.memory.is_terminals.append(done)\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # update\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"fit/total_reward\", episode_rewards, self.episode)\n\n        #\n        if self.episode % self.batch_size == 0:\n            self._update()\n            self.memory.clear_memory()\n\n        return episode_rewards\n\n    def _update(self):\n        # monte carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # convert list to tensor\n        # TODO: shuffle samples for each epoch\n        old_states = torch.stack(self.memory.states).to(self.device).detach()\n        old_actions = torch.stack(self.memory.actions).to(self.device).detach()\n        old_logprobs = torch.stack(self.memory.logprobs).to(self.device).detach()\n\n        old_action_dist = self.cat_policy_old(old_states)\n\n        # optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # evaluate old actions and values\n            action_dist = self.cat_policy(old_states)\n            logprobs = action_dist.log_prob(old_actions)\n            state_values = torch.squeeze(self.value_net(old_states))\n            dist_entropy = action_dist.entropy()\n\n            # find ratio (pi_theta / pi_theta__old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # compute returns and advantages\n            rewards = torch.tensor(rewards).to(self.device).float()\n            returns = torch.zeros(rewards.shape).to(self.device)\n            advantages = torch.zeros(rewards.shape).to(self.device)\n\n            if not self.use_gae:\n                for t in reversed(range(self.horizon)):\n                    if t == self.horizon - 1:\n                        returns[t] = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * state_values[-1]\n                    else:\n                        returns[t] = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * returns[t + 1]\n                    advantages[t] = returns[t] - state_values[t]\n            else:\n                for t in reversed(range(self.horizon)):\n                    if t == self.horizon - 1:\n                        returns[t] = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * state_values[-1]\n                        td_error = returns[t] - state_values[t]\n                    else:\n                        returns[t] = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * returns[t + 1]\n                        td_error = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * state_values[t + 1] - state_values[t]\n                    advantages[t] = advantages[t] * self.gae_lambda * self.gamma * (1 - self.memory.is_terminals[t]) + td_error\n\n            # normalizing the rewards\n            if self.normalize_reward:\n                rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n            # convert to pytorch tensors and move to gpu if available\n            advantages = advantages.view(-1, )\n\n            # normalize the advantages\n            if self.normalize_advantage:\n                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n\n            # estimate policy gradient\n            loss = - ratios * advantages\n\n            if self.use_entropy:\n                loss += - self.entr_coef * dist_entropy\n\n            # TODO: Check gradient's sign, conjugate_gradients function, fisher_vp function, linesearch function\n            # TODO: Check the gradients and if they flow correctly\n            grads = torch.autograd.grad(loss.mean(), self.cat_policy.parameters(), retain_graph=True)\n            loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n\n            # conjugate gradient algorithm\n            step_dir = self.conjugate_gradients(- loss_grad, old_action_dist, old_states, nsteps=10)\n\n            # update the policy by backtracking line search\n            shs = 0.5 * (step_dir * self.fisher_vp(step_dir, old_action_dist, old_states)).sum(0, keepdim=True)\n\n            lagrange_mult = torch.sqrt(shs / self.max_kl).item()\n            full_step = step_dir / lagrange_mult\n\n            neggdotstepdir = (- loss_grad * step_dir).sum(0, keepdim=True)\n            # print(f'Lagrange multiplier: {lm[0]}, grad norm: {loss_grad.norm()}')\n\n            prev_params = self.get_flat_params_from(self.cat_policy)\n            success, new_params = self.linesearch(old_states,\n                                                  old_actions,\n                                                  old_logprobs,\n                                                  advantages,\n                                                  prev_params,\n                                                  full_step,\n                                                  neggdotstepdir / lagrange_mult\n                                                  )\n\n            # fit value function by regression\n            value_loss = self.vf_coef * self.value_loss_fn(state_values, rewards)\n\n            self.value_optimizer.zero_grad()\n            value_loss.mean().backward()\n            self.value_optimizer.step()\n\n        # log\n        self.writer.add_scalar(\"fit/value_loss\", value_loss.mean().cpu().detach().numpy(), self.episode)\n        self.writer.add_scalar(\"fit/entropy_loss\", dist_entropy.mean().cpu().detach().numpy(), self.episode)\n\n        # copy new weights into old policy\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n    def conjugate_gradients(self, b, old_action_dist, old_states, nsteps, residual_tol=1e-10):\n        x = torch.zeros(b.size())\n        r = b.clone()\n        p = b.clone()\n        rdotr = torch.dot(r, r)\n        for i in range(nsteps):\n            _Avp = self.fisher_vp(p, old_action_dist, old_states)\n            alpha = rdotr / torch.dot(p, _Avp)\n            x += alpha * p\n            r -= alpha * _Avp\n            new_rdotr = torch.dot(r, r)\n            betta = new_rdotr / rdotr\n            p = r + betta * p\n            rdotr = new_rdotr\n            if rdotr < residual_tol:\n                break\n        return x\n\n    def fisher_vp(self, v, old_action_dist, old_states):\n\n        action_dist = self.cat_policy(old_states)\n        kl = kl_divergence(old_action_dist, action_dist)\n        kl = kl.mean()\n\n        grads = torch.autograd.grad(kl, self.cat_policy.parameters(), create_graph=True)\n        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n\n        kl_v = (flat_grad_kl * v).sum()\n        grads = torch.autograd.grad(kl_v, self.cat_policy.parameters(), allow_unused=True)\n        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).data\n\n        return flat_grad_grad_kl + v * self.damping\n\n    def linesearch(self, old_states, old_actions, old_logprobs, advantages, params, fullstep, expected_improve_rate, max_backtracks=10, accept_ratio=.1):\n\n        with torch.no_grad():\n            action_dist = self.cat_policy(old_states)\n            logprobs = action_dist.log_prob(old_actions)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n            loss = (ratios * advantages).data\n\n        for stepfrac in .5 ** np.arange(max_backtracks):\n            new_params = params + stepfrac * fullstep\n            self.set_flat_params_to(self.cat_policy, new_params)\n\n            with torch.no_grad():\n                action_dist = self.cat_policy(old_states)\n                logprobs = action_dist.log_prob(old_actions)\n                ratios = torch.exp(logprobs - old_logprobs.detach())\n                new_loss = (ratios * advantages).data\n\n            actual_improve = (loss - new_loss).mean()\n            expected_improve = expected_improve_rate * stepfrac\n            ratio = actual_improve / expected_improve\n            # print(\"a/e/r\", actual_improve.item(), expected_improve.item(), ratio.item())\n\n            if ratio.item() > accept_ratio and actual_improve.item() > 0:\n                # print(\"fval after\", newfval.item())\n                return True, new_params\n        return False, params\n\n    def get_flat_params_from(self, model):\n        params = []\n        for param in model.parameters():\n            params.append(param.data.view(-1))\n\n        flat_params = torch.cat(params)\n        return flat_params\n\n    def set_flat_params_to(self, model, flat_params):\n        prev_ind = 0\n        for param in model.parameters():\n            flat_size = int(np.prod(list(param.size())))\n            param.data.copy_(\n                flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n            prev_ind += flat_size\n\n    #\n    # For hyperparameter optimization\n    #\n    @classmethod\n    def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        eps_clip = trial.suggest_categorical('eps_clip',\n                                             [0.1, 0.2, 0.3])\n\n        k_epochs = trial.suggest_categorical('k_epochs',\n                                             [1, 5, 10, 20])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'eps_clip': eps_clip,\n                'k_epochs': k_epochs,\n                }",
  "def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 vf_coef=0.5,\n                 learning_rate=0.01,\n                 optimizer_type='ADAM',\n                 k_epochs=5,\n                 use_gae=True,\n                 gae_lambda=0.95,\n                 policy_net_fn=None,\n                 value_net_fn=None,\n                 policy_net_kwargs=None,\n                 value_net_kwargs=None,\n                 device=\"cuda:best\",\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 **kwargs):\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env, **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.horizon = horizon\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.vf_coef = vf_coef\n        self.learning_rate = learning_rate\n        self.k_epochs = k_epochs\n        self.use_gae = use_gae\n        self.gae_lambda = gae_lambda\n        self.damping = 0  # TODO: turn into argument\n        self.max_kl = 0.1  # TODO: turn into argument\n        self.use_entropy = False  # TODO: test, and eventually turn into argument\n        self.normalize_advantage = True  # TODO: turn into argument\n        self.normalize_reward = False  # TODO: turn into argument\n\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.value_net_kwargs = value_net_kwargs or {}\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.value_net_fn = value_net_fn or default_value_net_fn\n\n        self.device = choose_device(device)\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # TODO: check\n        self.cat_policy = None  # categorical policy function\n        self.policy_optimizer = None\n\n        self.value_net = None\n        self.value_optimizer = None\n\n        self.cat_policy_old = None\n\n        self.value_loss_fn = None\n\n        self.memory = None\n\n        self.episode = 0\n\n        self._rewards = None\n        self._cumul_rewards = None\n\n        # initialize\n        self.reset()",
  "def from_config(cls, **kwargs):\n        kwargs[\"policy_net_fn\"] = eval(kwargs[\"policy_net_fn\"])\n        kwargs[\"value_net_fn\"] = eval(kwargs[\"value_net_fn\"])\n        return cls(**kwargs)",
  "def reset(self, **kwargs):\n        self.cat_policy = self.policy_net_fn(self.env, **self.policy_net_kwargs).to(self.device)\n        self.policy_optimizer = optimizer_factory(self.cat_policy.parameters(), **self.optimizer_kwargs)\n\n        self.value_net = self.value_net_fn(self.env, **self.value_net_kwargs).to(self.device)\n        self.value_optimizer = optimizer_factory(self.value_net.parameters(), **self.optimizer_kwargs)\n\n        self.cat_policy_old = self.policy_net_fn(self.env, **self.policy_net_kwargs).to(self.device)\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n        self.value_loss_fn = nn.MSELoss()  # TODO: turn into argument\n\n        self.memory = Memory()\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name, log_every=5*logger.getEffectiveLevel())",
  "def policy(self, state, **kwargs):\n        assert self.cat_policy is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample().item()\n        return action",
  "def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction * self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode, \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def _select_action(self, state):\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample()\n        action_logprob = action_dist.log_prob(action)\n\n        return action, action_logprob",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # running policy_old\n            action, log_prob = self._select_action(state)\n            next_state, reward, done, info = self.env.step(action.item())\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save in batch\n            self.memory.states.append(torch.from_numpy(state).float().to(self.device))\n            self.memory.actions.append(action)\n            self.memory.logprobs.append(log_prob)\n            self.memory.rewards.append(reward + bonus)  # bonus added here\n            self.memory.is_terminals.append(done)\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # update\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"fit/total_reward\", episode_rewards, self.episode)\n\n        #\n        if self.episode % self.batch_size == 0:\n            self._update()\n            self.memory.clear_memory()\n\n        return episode_rewards",
  "def _update(self):\n        # monte carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # convert list to tensor\n        # TODO: shuffle samples for each epoch\n        old_states = torch.stack(self.memory.states).to(self.device).detach()\n        old_actions = torch.stack(self.memory.actions).to(self.device).detach()\n        old_logprobs = torch.stack(self.memory.logprobs).to(self.device).detach()\n\n        old_action_dist = self.cat_policy_old(old_states)\n\n        # optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # evaluate old actions and values\n            action_dist = self.cat_policy(old_states)\n            logprobs = action_dist.log_prob(old_actions)\n            state_values = torch.squeeze(self.value_net(old_states))\n            dist_entropy = action_dist.entropy()\n\n            # find ratio (pi_theta / pi_theta__old)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n\n            # compute returns and advantages\n            rewards = torch.tensor(rewards).to(self.device).float()\n            returns = torch.zeros(rewards.shape).to(self.device)\n            advantages = torch.zeros(rewards.shape).to(self.device)\n\n            if not self.use_gae:\n                for t in reversed(range(self.horizon)):\n                    if t == self.horizon - 1:\n                        returns[t] = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * state_values[-1]\n                    else:\n                        returns[t] = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * returns[t + 1]\n                    advantages[t] = returns[t] - state_values[t]\n            else:\n                for t in reversed(range(self.horizon)):\n                    if t == self.horizon - 1:\n                        returns[t] = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * state_values[-1]\n                        td_error = returns[t] - state_values[t]\n                    else:\n                        returns[t] = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * returns[t + 1]\n                        td_error = rewards[t] + self.gamma * (1 - self.memory.is_terminals[t]) * state_values[t + 1] - state_values[t]\n                    advantages[t] = advantages[t] * self.gae_lambda * self.gamma * (1 - self.memory.is_terminals[t]) + td_error\n\n            # normalizing the rewards\n            if self.normalize_reward:\n                rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n            # convert to pytorch tensors and move to gpu if available\n            advantages = advantages.view(-1, )\n\n            # normalize the advantages\n            if self.normalize_advantage:\n                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n\n            # estimate policy gradient\n            loss = - ratios * advantages\n\n            if self.use_entropy:\n                loss += - self.entr_coef * dist_entropy\n\n            # TODO: Check gradient's sign, conjugate_gradients function, fisher_vp function, linesearch function\n            # TODO: Check the gradients and if they flow correctly\n            grads = torch.autograd.grad(loss.mean(), self.cat_policy.parameters(), retain_graph=True)\n            loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n\n            # conjugate gradient algorithm\n            step_dir = self.conjugate_gradients(- loss_grad, old_action_dist, old_states, nsteps=10)\n\n            # update the policy by backtracking line search\n            shs = 0.5 * (step_dir * self.fisher_vp(step_dir, old_action_dist, old_states)).sum(0, keepdim=True)\n\n            lagrange_mult = torch.sqrt(shs / self.max_kl).item()\n            full_step = step_dir / lagrange_mult\n\n            neggdotstepdir = (- loss_grad * step_dir).sum(0, keepdim=True)\n            # print(f'Lagrange multiplier: {lm[0]}, grad norm: {loss_grad.norm()}')\n\n            prev_params = self.get_flat_params_from(self.cat_policy)\n            success, new_params = self.linesearch(old_states,\n                                                  old_actions,\n                                                  old_logprobs,\n                                                  advantages,\n                                                  prev_params,\n                                                  full_step,\n                                                  neggdotstepdir / lagrange_mult\n                                                  )\n\n            # fit value function by regression\n            value_loss = self.vf_coef * self.value_loss_fn(state_values, rewards)\n\n            self.value_optimizer.zero_grad()\n            value_loss.mean().backward()\n            self.value_optimizer.step()\n\n        # log\n        self.writer.add_scalar(\"fit/value_loss\", value_loss.mean().cpu().detach().numpy(), self.episode)\n        self.writer.add_scalar(\"fit/entropy_loss\", dist_entropy.mean().cpu().detach().numpy(), self.episode)\n\n        # copy new weights into old policy\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())",
  "def conjugate_gradients(self, b, old_action_dist, old_states, nsteps, residual_tol=1e-10):\n        x = torch.zeros(b.size())\n        r = b.clone()\n        p = b.clone()\n        rdotr = torch.dot(r, r)\n        for i in range(nsteps):\n            _Avp = self.fisher_vp(p, old_action_dist, old_states)\n            alpha = rdotr / torch.dot(p, _Avp)\n            x += alpha * p\n            r -= alpha * _Avp\n            new_rdotr = torch.dot(r, r)\n            betta = new_rdotr / rdotr\n            p = r + betta * p\n            rdotr = new_rdotr\n            if rdotr < residual_tol:\n                break\n        return x",
  "def fisher_vp(self, v, old_action_dist, old_states):\n\n        action_dist = self.cat_policy(old_states)\n        kl = kl_divergence(old_action_dist, action_dist)\n        kl = kl.mean()\n\n        grads = torch.autograd.grad(kl, self.cat_policy.parameters(), create_graph=True)\n        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n\n        kl_v = (flat_grad_kl * v).sum()\n        grads = torch.autograd.grad(kl_v, self.cat_policy.parameters(), allow_unused=True)\n        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).data\n\n        return flat_grad_grad_kl + v * self.damping",
  "def linesearch(self, old_states, old_actions, old_logprobs, advantages, params, fullstep, expected_improve_rate, max_backtracks=10, accept_ratio=.1):\n\n        with torch.no_grad():\n            action_dist = self.cat_policy(old_states)\n            logprobs = action_dist.log_prob(old_actions)\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n            loss = (ratios * advantages).data\n\n        for stepfrac in .5 ** np.arange(max_backtracks):\n            new_params = params + stepfrac * fullstep\n            self.set_flat_params_to(self.cat_policy, new_params)\n\n            with torch.no_grad():\n                action_dist = self.cat_policy(old_states)\n                logprobs = action_dist.log_prob(old_actions)\n                ratios = torch.exp(logprobs - old_logprobs.detach())\n                new_loss = (ratios * advantages).data\n\n            actual_improve = (loss - new_loss).mean()\n            expected_improve = expected_improve_rate * stepfrac\n            ratio = actual_improve / expected_improve\n            # print(\"a/e/r\", actual_improve.item(), expected_improve.item(), ratio.item())\n\n            if ratio.item() > accept_ratio and actual_improve.item() > 0:\n                # print(\"fval after\", newfval.item())\n                return True, new_params\n        return False, params",
  "def get_flat_params_from(self, model):\n        params = []\n        for param in model.parameters():\n            params.append(param.data.view(-1))\n\n        flat_params = torch.cat(params)\n        return flat_params",
  "def set_flat_params_to(self, model, flat_params):\n        prev_ind = 0\n        for param in model.parameters():\n            flat_size = int(np.prod(list(param.size())))\n            param.data.copy_(\n                flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n            prev_ind += flat_size",
  "def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        eps_clip = trial.suggest_categorical('eps_clip',\n                                             [0.1, 0.2, 0.3])\n\n        k_epochs = trial.suggest_categorical('k_epochs',\n                                             [1, 5, 10, 20])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'eps_clip': eps_clip,\n                'k_epochs': k_epochs,\n                }",
  "class A2CAgent(IncrementalAgent):\n    \"\"\"\n    Parameters\n    ----------\n    env : Model\n        Online model with continuous (Box) state space and discrete actions\n    n_episodes : int\n        Number of episodes\n    batch_size : int\n        Number of episodes to wait before updating the policy.\n    horizon : int\n        Horizon.\n    gamma : double\n        Discount factor in [0, 1].\n    entr_coef : double\n        Entropy coefficient.\n    learning_rate : double\n        Learning rate.\n    optimizer_type: str\n        Type of optimizer. 'ADAM' by defaut.\n    k_epochs : int\n        Number of epochs per update.\n    policy_net_fn : function(env, **kwargs)\n        Function that returns an instance of a policy network (pytorch).\n        If None, a default net is used.\n    value_net_fn : function(env, **kwargs)\n        Function that returns an instance of a value network (pytorch).\n        If None, a default net is used.\n    policy_net_kwargs : dict\n        kwargs for policy_net_fn\n    value_net_kwargs : dict\n        kwargs for value_net_fn\n    use_bonus : bool, default = False\n        If true, compute an 'exploration_bonus' and add it to the reward.\n        See also UncertaintyEstimatorWrapper.\n    uncertainty_estimator_kwargs : dict\n        Arguments for the UncertaintyEstimatorWrapper\n    device : str\n        Device to put the tensors on\n\n    References\n    ----------\n    Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,\n    Silver, D. & Kavukcuoglu, K. (2016).\n    \"Asynchronous methods for deep reinforcement learning.\"\n    In International Conference on Machine Learning (pp. 1928-1937).\n    \"\"\"\n\n    name = \"A2C\"\n\n    def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 learning_rate=0.01,\n                 optimizer_type='ADAM',\n                 k_epochs=5,\n                 policy_net_fn=None,\n                 value_net_fn=None,\n                 policy_net_kwargs=None,\n                 value_net_kwargs=None,\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 device=\"cuda:best\",\n                 **kwargs):\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env,\n                                              **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.horizon = horizon\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.learning_rate = learning_rate\n        self.k_epochs = k_epochs\n        self.device = choose_device(device)\n\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.value_net_kwargs = value_net_kwargs or {}\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.value_net_fn = value_net_fn or default_value_net_fn\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.cat_policy = None  # categorical policy function\n\n        # initialize\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.cat_policy = self.policy_net_fn(\n                            self.env,\n                            **self.policy_net_kwargs).to(self.device)\n        self.policy_optimizer = optimizer_factory(\n                                    self.cat_policy.parameters(),\n                                    **self.optimizer_kwargs)\n\n        self.value_net = self.value_net_fn(\n                                    self.env,\n                                    **self.value_net_kwargs).to(self.device)\n\n        self.value_optimizer = optimizer_factory(\n                                self.value_net.parameters(),\n                                **self.optimizer_kwargs)\n\n        self.cat_policy_old = self.policy_net_fn(\n                                self.env,\n                                **self.policy_net_kwargs).to(self.device)\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n        self.MseLoss = nn.MSELoss()\n\n        self.memory = Memory()\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        log_every = 5*logger.getEffectiveLevel()\n        self.writer = PeriodicWriter(self.name, log_every=log_every)\n\n    def policy(self, state, **kwargs):\n        assert self.cat_policy is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample().item()\n        return action\n\n    def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info\n\n    def _select_action(self, state):\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample()\n        action_logprob = action_dist.log_prob(action)\n\n        self.memory.states.append(state)\n        self.memory.actions.append(action)\n        self.memory.logprobs.append(action_logprob)\n\n        return action.item()\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # running policy_old\n            action = self._select_action(state)\n            next_state, reward, done, info = self.env.step(action)\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save in batch\n            self.memory.rewards.append(reward+bonus)   # add bonus here\n            self.memory.is_terminals.append(done)\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # update\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        #\n        if self.episode % self.batch_size == 0:\n            self._update()\n            self.memory.clear_memory()\n\n        return episode_rewards\n\n    def _update(self):\n        # monte carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.memory.rewards),\n                                       reversed(self.memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # normalize the rewards\n        rewards = torch.tensor(rewards).to(self.device).float()\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n        # convert list to tensor\n        old_states = torch.stack(self.memory.states).to(self.device).detach()\n        old_actions = torch.stack(self.memory.actions).to(self.device).detach()\n\n        # optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # evaluate old actions and values\n            action_dist = self.cat_policy(old_states)\n            logprobs = action_dist.log_prob(old_actions)\n            state_values = torch.squeeze(self.value_net(old_states))\n            dist_entropy = action_dist.entropy()\n\n            # normalize the advantages\n            advantages = rewards - state_values.detach()\n            advantages = (advantages - advantages.mean()) \\\n                / (advantages.std() + 1e-8)\n            # find pg loss\n            pg_loss = - logprobs * advantages\n            loss = pg_loss \\\n                + 0.5 * self.MseLoss(state_values, rewards) \\\n                - self.entr_coef * dist_entropy\n\n            # take gradient step\n            self.policy_optimizer.zero_grad()\n            self.value_optimizer.zero_grad()\n\n            loss.mean().backward()\n\n            self.policy_optimizer.step()\n            self.value_optimizer.step()\n\n        # copy new weights into old policy\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n    #\n    # For hyperparameter optimization\n    #\n    @classmethod\n    def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        k_epochs = trial.suggest_categorical('k_epochs',\n                                             [1, 5, 10, 20])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'k_epochs': k_epochs,\n                }",
  "def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 learning_rate=0.01,\n                 optimizer_type='ADAM',\n                 k_epochs=5,\n                 policy_net_fn=None,\n                 value_net_fn=None,\n                 policy_net_kwargs=None,\n                 value_net_kwargs=None,\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 device=\"cuda:best\",\n                 **kwargs):\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env,\n                                              **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.horizon = horizon\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.learning_rate = learning_rate\n        self.k_epochs = k_epochs\n        self.device = choose_device(device)\n\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.value_net_kwargs = value_net_kwargs or {}\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.value_net_fn = value_net_fn or default_value_net_fn\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.cat_policy = None  # categorical policy function\n\n        # initialize\n        self.reset()",
  "def reset(self, **kwargs):\n        self.cat_policy = self.policy_net_fn(\n                            self.env,\n                            **self.policy_net_kwargs).to(self.device)\n        self.policy_optimizer = optimizer_factory(\n                                    self.cat_policy.parameters(),\n                                    **self.optimizer_kwargs)\n\n        self.value_net = self.value_net_fn(\n                                    self.env,\n                                    **self.value_net_kwargs).to(self.device)\n\n        self.value_optimizer = optimizer_factory(\n                                self.value_net.parameters(),\n                                **self.optimizer_kwargs)\n\n        self.cat_policy_old = self.policy_net_fn(\n                                self.env,\n                                **self.policy_net_kwargs).to(self.device)\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n        self.MseLoss = nn.MSELoss()\n\n        self.memory = Memory()\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        log_every = 5*logger.getEffectiveLevel()\n        self.writer = PeriodicWriter(self.name, log_every=log_every)",
  "def policy(self, state, **kwargs):\n        assert self.cat_policy is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample().item()\n        return action",
  "def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def _select_action(self, state):\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample()\n        action_logprob = action_dist.log_prob(action)\n\n        self.memory.states.append(state)\n        self.memory.actions.append(action)\n        self.memory.logprobs.append(action_logprob)\n\n        return action.item()",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # running policy_old\n            action = self._select_action(state)\n            next_state, reward, done, info = self.env.step(action)\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save in batch\n            self.memory.rewards.append(reward+bonus)   # add bonus here\n            self.memory.is_terminals.append(done)\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # update\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        #\n        if self.episode % self.batch_size == 0:\n            self._update()\n            self.memory.clear_memory()\n\n        return episode_rewards",
  "def _update(self):\n        # monte carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.memory.rewards),\n                                       reversed(self.memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # normalize the rewards\n        rewards = torch.tensor(rewards).to(self.device).float()\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n        # convert list to tensor\n        old_states = torch.stack(self.memory.states).to(self.device).detach()\n        old_actions = torch.stack(self.memory.actions).to(self.device).detach()\n\n        # optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # evaluate old actions and values\n            action_dist = self.cat_policy(old_states)\n            logprobs = action_dist.log_prob(old_actions)\n            state_values = torch.squeeze(self.value_net(old_states))\n            dist_entropy = action_dist.entropy()\n\n            # normalize the advantages\n            advantages = rewards - state_values.detach()\n            advantages = (advantages - advantages.mean()) \\\n                / (advantages.std() + 1e-8)\n            # find pg loss\n            pg_loss = - logprobs * advantages\n            loss = pg_loss \\\n                + 0.5 * self.MseLoss(state_values, rewards) \\\n                - self.entr_coef * dist_entropy\n\n            # take gradient step\n            self.policy_optimizer.zero_grad()\n            self.value_optimizer.zero_grad()\n\n            loss.mean().backward()\n\n            self.policy_optimizer.step()\n            self.value_optimizer.step()\n\n        # copy new weights into old policy\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())",
  "def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        k_epochs = trial.suggest_categorical('k_epochs',\n                                             [1, 5, 10, 20])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'k_epochs': k_epochs,\n                }",
  "class OptQLAgent(IncrementalAgent):\n    \"\"\"\n    Optimistic Q-Learning [1]_ with custom exploration bonuses.\n\n    Parameters\n    ----------\n    env : gym.Env\n        Environment with discrete states and actions.\n    n_episodes : int\n        Number of episodes\n    gamma : double, default: 1.0\n        Discount factor in [0, 1].\n    horizon : int\n        Horizon of the objective function.\n    bonus_scale_factor : double, default: 1.0\n        Constant by which to multiply the exploration bonus, controls\n        the level of exploration.\n    bonus_type : {\"simplified_bernstein\"}\n        Type of exploration bonus. Currently, only \"simplified_bernstein\"\n        is implemented.\n    add_bonus_after_update : bool, default: False\n        If True, add bonus to the Q function after performing the update,\n        instead of adding it to the update target.\n\n    References\n    ----------\n    .. [1] Jin et al., 2018\n           Is Q-Learning Provably Efficient?\n           https://arxiv.org/abs/1807.03765\n    \"\"\"\n    name = \"OptQL\"\n\n    def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 gamma=1.0,\n                 horizon=100,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 add_bonus_after_update=False,\n                 **kwargs):\n        # init base class\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n        self.add_bonus_after_update = add_bonus_after_update\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Discrete)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        self.v_max = np.zeros(self.horizon)\n        self.v_max[-1] = r_range\n        for hh in reversed(range(self.horizon-1)):\n            self.v_max[hh] = r_range + self.gamma*self.v_max[hh+1]\n\n        # initialize\n        self.reset()\n\n    def reset(self, **kwargs):\n        H = self.horizon\n        S = self.env.observation_space.n\n        A = self.env.action_space.n\n\n        # (s, a) visit counter\n        self.N_sa = np.zeros((H, S, A))\n\n        # Value functions\n        self.V = np.ones((H+1, S))\n        self.V[H, :] = 0\n        self.Q = np.ones((H, S, A))\n        self.Q_bar = np.ones((H, S, A))\n        for hh in range(self.horizon):\n            self.V[hh, :] *= (self.horizon-hh)\n            self.Q[hh, :, :] *= (self.horizon-hh)\n            self.Q_bar[hh, :, :] *= (self.horizon-hh)\n\n        if self.add_bonus_after_update:\n            self.Q *= 0.0\n\n        # ep counter\n        self.episode = 0\n\n        # useful object to compute total number of visited states & entropy of visited states\n        self.counter = DiscreteCounter(self.env.observation_space,\n                                       self.env.action_space)\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, state, hh=0, **kwargs):\n        \"\"\" Recommended policy. \"\"\"\n        return self.Q_bar[hh, state, :].argmax()\n\n    def _get_action(self, state, hh=0):\n        \"\"\" Sampling policy. \"\"\"\n        return self.Q_bar[hh, state, :].argmax()\n\n    def _compute_bonus(self, n, hh):\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max[hh] / n\n            bonus = min(bonus, self.v_max[hh])\n            return bonus\n        else:\n            raise ValueError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))\n\n    def _update(self, state, action, next_state, reward, hh):\n        self.N_sa[hh, state, action] += 1\n        nn = self.N_sa[hh, state, action]\n\n        # learning rate\n        alpha = (self.horizon+1.0)/(self.horizon + nn)\n        bonus = self._compute_bonus(nn, hh)\n\n        # bonus in the update\n        if not self.add_bonus_after_update:\n            target = reward + bonus + self.gamma*self.V[hh+1, next_state]\n            self.Q[hh, state, action] = (1-alpha)*self.Q[hh, state, action] + alpha * target\n            self.V[hh, state] = min(self.v_max[hh], self.Q[hh, state, :].max())\n            self.Q_bar[hh, state, action] = self.Q[hh, state, action]\n        # bonus outside the update\n        else:\n            target = reward + self.gamma*self.V[hh+1, next_state]                  # bonus not here\n            self.Q[hh, state, action] = (1-alpha)*self.Q[hh, state, action] + alpha * target\n            self.Q_bar[hh, state, action] = self.Q[hh, state, action] + bonus      # bonus here\n            self.V[hh, state] = min(self.v_max[hh], self.Q_bar[hh, state, :].max())\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = self._get_action(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward  # used for logging only\n\n            self.counter.update(state, action)\n\n            self._update(state, action, next_state, reward, hh)\n\n            state = next_state\n            if done:\n                break\n\n        # update info\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self.episode += 1\n\n        # writer\n        if self.writer is not None:\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n            self.writer.add_scalar(\"total reward\", self._rewards[:ep].sum(), self.episode)\n            self.writer.add_scalar(\"n_visited_states\", self.counter.get_n_visited_states(), self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards\n\n    def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 gamma=1.0,\n                 horizon=100,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 add_bonus_after_update=False,\n                 **kwargs):\n        # init base class\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n        self.add_bonus_after_update = add_bonus_after_update\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Discrete)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        self.v_max = np.zeros(self.horizon)\n        self.v_max[-1] = r_range\n        for hh in reversed(range(self.horizon-1)):\n            self.v_max[hh] = r_range + self.gamma*self.v_max[hh+1]\n\n        # initialize\n        self.reset()",
  "def reset(self, **kwargs):\n        H = self.horizon\n        S = self.env.observation_space.n\n        A = self.env.action_space.n\n\n        # (s, a) visit counter\n        self.N_sa = np.zeros((H, S, A))\n\n        # Value functions\n        self.V = np.ones((H+1, S))\n        self.V[H, :] = 0\n        self.Q = np.ones((H, S, A))\n        self.Q_bar = np.ones((H, S, A))\n        for hh in range(self.horizon):\n            self.V[hh, :] *= (self.horizon-hh)\n            self.Q[hh, :, :] *= (self.horizon-hh)\n            self.Q_bar[hh, :, :] *= (self.horizon-hh)\n\n        if self.add_bonus_after_update:\n            self.Q *= 0.0\n\n        # ep counter\n        self.episode = 0\n\n        # useful object to compute total number of visited states & entropy of visited states\n        self.counter = DiscreteCounter(self.env.observation_space,\n                                       self.env.action_space)\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())",
  "def policy(self, state, hh=0, **kwargs):\n        \"\"\" Recommended policy. \"\"\"\n        return self.Q_bar[hh, state, :].argmax()",
  "def _get_action(self, state, hh=0):\n        \"\"\" Sampling policy. \"\"\"\n        return self.Q_bar[hh, state, :].argmax()",
  "def _compute_bonus(self, n, hh):\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max[hh] / n\n            bonus = min(bonus, self.v_max[hh])\n            return bonus\n        else:\n            raise ValueError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))",
  "def _update(self, state, action, next_state, reward, hh):\n        self.N_sa[hh, state, action] += 1\n        nn = self.N_sa[hh, state, action]\n\n        # learning rate\n        alpha = (self.horizon+1.0)/(self.horizon + nn)\n        bonus = self._compute_bonus(nn, hh)\n\n        # bonus in the update\n        if not self.add_bonus_after_update:\n            target = reward + bonus + self.gamma*self.V[hh+1, next_state]\n            self.Q[hh, state, action] = (1-alpha)*self.Q[hh, state, action] + alpha * target\n            self.V[hh, state] = min(self.v_max[hh], self.Q[hh, state, :].max())\n            self.Q_bar[hh, state, action] = self.Q[hh, state, action]\n        # bonus outside the update\n        else:\n            target = reward + self.gamma*self.V[hh+1, next_state]                  # bonus not here\n            self.Q[hh, state, action] = (1-alpha)*self.Q[hh, state, action] + alpha * target\n            self.Q_bar[hh, state, action] = self.Q[hh, state, action] + bonus      # bonus here\n            self.V[hh, state] = min(self.v_max[hh], self.Q_bar[hh, state, :].max())",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = self._get_action(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward  # used for logging only\n\n            self.counter.update(state, action)\n\n            self._update(state, action, next_state, reward, hh)\n\n            state = next_state\n            if done:\n                break\n\n        # update info\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self.episode += 1\n\n        # writer\n        if self.writer is not None:\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n            self.writer.add_scalar(\"total reward\", self._rewards[:ep].sum(), self.episode)\n            self.writer.add_scalar(\"n_visited_states\", self.counter.get_n_visited_states(), self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards",
  "def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "class PPOAgent(IncrementalAgent):\n    \"\"\"\n    Parameters\n    ----------\n    env : Model\n        Online model with continuous (Box) state space and discrete actions\n    n_episodes : int\n        Number of episodes\n    batch_size : int\n        Number of *episodes* to wait before updating the policy.\n    horizon : int\n        Horizon.\n    gamma : double\n        Discount factor in [0, 1].\n    entr_coef : double\n        Entropy coefficient.\n    vf_coef : double\n        Value function loss coefficient.\n    learning_rate : double\n        Learning rate.\n    optimizer_type: str\n        Type of optimizer. 'ADAM' by defaut.\n    eps_clip : double\n        PPO clipping range (epsilon).\n    k_epochs : int\n        Number of epochs per update.\n    policy_net_fn : function(env, \\*\\*kwargs)\n        Function that returns an instance of a policy network (pytorch).\n        If None, a default net is used.\n    value_net_fn : function(env, \\*\\*kwargs)\n        Function that returns an instance of a value network (pytorch).\n        If None, a default net is used.\n    policy_net_kwargs : dict\n        kwargs for policy_net_fn\n    value_net_kwargs : dict\n        kwargs for value_net_fn\n    device: str\n        Device to put the tensors on\n    use_bonus : bool, default = False\n        If true, compute the environment 'exploration_bonus'\n        and add it to the reward. See also UncertaintyEstimatorWrapper.\n    uncertainty_estimator_kwargs : dict\n        kwargs for UncertaintyEstimatorWrapper\n\n    References\n    ----------\n    Schulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. (2017).\n    \"Proximal Policy Optimization Algorithms.\"\n    arXiv preprint arXiv:1707.06347.\n\n    Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015).\n    \"Trust region policy optimization.\"\n    In International Conference on Machine Learning (pp. 1889-1897).\n    \"\"\"\n\n    name = \"PPO\"\n\n    def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=64,\n                 update_frequency=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 vf_coef=0.5,\n                 learning_rate=0.01,\n                 optimizer_type='ADAM',\n                 eps_clip=0.2,\n                 k_epochs=5,\n                 use_gae=True,\n                 gae_lambda=0.95,\n                 policy_net_fn=None,\n                 value_net_fn=None,\n                 policy_net_kwargs=None,\n                 value_net_kwargs=None,\n                 device=\"cuda:best\",\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 **kwargs):  # TODO: sort arguments\n\n        # bonus\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env, **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        # algorithm parameters\n        self.gamma = gamma\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.k_epochs = k_epochs\n        self.update_frequency = update_frequency\n\n        self.eps_clip = eps_clip\n        self.vf_coef = vf_coef\n        self.entr_coef = entr_coef\n\n        # options\n        # TODO: add reward normalization option\n        #       add observation normalization option\n        #       add orthogonal weight initialization option\n        #       add value function clip option\n        #       add ... ?\n        self.normalize_advantages = True  # TODO: turn into argument\n\n        self.use_gae = use_gae\n        self.gae_lambda = gae_lambda\n\n        # function approximators\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.value_net_kwargs = value_net_kwargs or {}\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.value_net_fn = value_net_fn or default_value_net_fn\n\n        self.device = choose_device(device)\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.cat_policy = None  # categorical policy function\n\n        # initialize\n        self.reset()\n\n    @classmethod\n    def from_config(cls, **kwargs):\n        kwargs[\"policy_net_fn\"] = eval(kwargs[\"policy_net_fn\"])\n        kwargs[\"value_net_fn\"] = eval(kwargs[\"value_net_fn\"])\n        return cls(**kwargs)\n\n    def reset(self, **kwargs):\n        self.cat_policy = self.policy_net_fn(self.env, **self.policy_net_kwargs).to(self.device)\n        self.policy_optimizer = optimizer_factory(self.cat_policy.parameters(), **self.optimizer_kwargs)\n\n        self.value_net = self.value_net_fn(self.env, **self.value_net_kwargs).to(self.device)\n        self.value_optimizer = optimizer_factory(self.value_net.parameters(), **self.optimizer_kwargs)\n\n        self.cat_policy_old = self.policy_net_fn(self.env, **self.policy_net_kwargs).to(self.device)\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n        self.MseLoss = nn.MSELoss()  # TODO: turn into argument\n\n        self.memory = Memory()  # TODO: Improve memory to include returns and advantages\n        self.returns = []  # TODO: add to memory\n        self.advantages = []  # TODO: add to memory\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name, log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, state, **kwargs):\n        assert self.cat_policy is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample().item()\n        return action\n\n    def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info\n\n    def _run_episode(self):\n\n        # to store transitions\n        states = []\n        actions = []\n        action_logprobs = []\n        rewards = []\n        is_terminals = []\n\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n\n        for _ in range(self.horizon):\n            # running policy_old\n            state = torch.from_numpy(state).float().to(self.device)\n\n            action_dist = self.cat_policy_old(state)\n            action = action_dist.sample()\n            action_logprob = action_dist.log_prob(action)\n            action = action\n\n            next_state, reward, done, info = self.env.step(action.item())\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save transition\n            states.append(state)\n            actions.append(action)\n            action_logprobs.append(action_logprob)\n            rewards.append(reward + bonus)  # bonus added here\n            is_terminals.append(done)\n\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # compute returns and advantages\n        state_values = self.value_net(torch.stack(states).to(self.device)).detach()\n        state_values = torch.squeeze(state_values).tolist()\n\n        # TODO: add the option to normalize before computing returns/advantages?\n        returns, advantages = self._compute_returns_avantages(rewards, is_terminals, state_values)\n\n        # save in batch\n        self.memory.states.extend(states)\n        self.memory.actions.extend(actions)\n        self.memory.logprobs.extend(action_logprobs)\n        self.memory.rewards.extend(rewards)\n        self.memory.is_terminals.extend(is_terminals)\n\n        self.returns.extend(returns)  # TODO: add to memory (cf reset)\n        self.advantages.extend(advantages)  # TODO: add to memory (cf reset)\n\n        # save episodic rewards and cumulative rewards\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n\n        # log\n        if self.writer is not None:\n            self.writer.add_scalar(\"fit/total_reward\", episode_rewards, self.episode)\n\n        # update\n        if self.episode % self.update_frequency == 0:  # TODO: maybe change to update in function of n_steps instead\n            self._update()\n            self.memory.clear_memory()\n            del self.returns[:]  # TODO: add to memory (cf reset)\n            del self.advantages[:]  # TODO: add to memory (cf reset)\n\n        return episode_rewards\n\n    def _update(self):\n\n        # convert list to tensor\n        full_old_states = torch.stack(self.memory.states).to(self.device).detach()\n        full_old_actions = torch.stack(self.memory.actions).to(self.device).detach()\n        full_old_logprobs = torch.stack(self.memory.logprobs).to(self.device).detach()\n        full_old_returns = torch.stack(self.returns).to(self.device).detach()\n        full_old_advantages = torch.stack(self.advantages).to(self.device).detach()\n\n        # optimize policy for K epochs\n        n_samples = full_old_actions.size(0)\n        n_batches = n_samples // self.batch_size\n\n        for _ in range(self.k_epochs):\n\n            # shuffle samples\n            rd_indices = np.random.choice(n_samples, size=n_samples, replace=False)\n            shuffled_states = full_old_states[rd_indices]\n            shuffled_actions = full_old_actions[rd_indices]\n            shuffled_logprobs = full_old_logprobs[rd_indices]\n            shuffled_returns = full_old_returns[rd_indices]\n            shuffled_advantages = full_old_advantages[rd_indices]\n\n            for k in range(n_batches):\n\n                # sample batch\n                batch_idx = np.arange(k * self.batch_size, min((k + 1) * self.batch_size, n_samples))\n                old_states = shuffled_states[batch_idx]\n                old_actions = shuffled_actions[batch_idx]\n                old_logprobs = shuffled_logprobs[batch_idx]\n                old_returns = shuffled_returns[batch_idx]\n                old_advantages = shuffled_advantages[batch_idx]\n\n                # evaluate old actions and values\n                action_dist = self.cat_policy(old_states)\n                logprobs = action_dist.log_prob(old_actions)\n                state_values = torch.squeeze(self.value_net(old_states))\n                dist_entropy = action_dist.entropy()\n\n                # find ratio (pi_theta / pi_theta__old)\n                ratios = torch.exp(logprobs - old_logprobs)\n\n                # TODO: add this option\n                # normalizing the rewards\n                # rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n                # normalize the advantages\n                old_advantages = old_advantages.view(-1, )\n\n                if self.normalize_advantages:\n                    old_advantages = (old_advantages - old_advantages.mean()) / (old_advantages.std() + 1e-10)\n\n                # compute surrogate loss\n                surr1 = ratios * old_advantages\n                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * old_advantages\n                surr_loss = torch.min(surr1, surr2)\n\n                # compute value function loss\n                loss_vf = self.vf_coef * self.MseLoss(state_values, old_returns)\n\n                # compute entropy loss\n                loss_entropy = self.entr_coef * dist_entropy\n\n                # compute total loss\n                loss = - surr_loss + loss_vf - loss_entropy\n\n                # take gradient step\n                self.policy_optimizer.zero_grad()\n                self.value_optimizer.zero_grad()\n\n                loss.mean().backward()\n\n                self.policy_optimizer.step()\n                self.value_optimizer.step()\n\n        # log\n        self.writer.add_scalar(\"fit/surrogate_loss\", surr_loss.mean().cpu().detach().numpy(), self.episode)\n        self.writer.add_scalar(\"fit/entropy_loss\", dist_entropy.mean().cpu().detach().numpy(), self.episode)\n\n        # copy new weights into old policy\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n    def _compute_returns_avantages(self, rewards, is_terminals, state_values):\n\n        returns = torch.zeros(self.horizon).to(self.device)\n        advantages = torch.zeros(self.horizon).to(self.device)\n\n        if not self.use_gae:\n            for t in reversed(range(self.horizon)):\n                if t == self.horizon - 1:\n                    returns[t] = rewards[t] + self.gamma * (1 - is_terminals[t]) * state_values[-1]\n                else:\n                    returns[t] = rewards[t] + self.gamma * (1 - is_terminals[t]) * returns[t + 1]\n\n                advantages[t] = returns[t] - state_values[t]\n        else:\n            last_adv = 0\n            for t in reversed(range(self.horizon)):\n                if t == self.horizon - 1:\n                    returns[t] = rewards[t] + self.gamma * (1 - is_terminals[t]) * state_values[-1]\n                    td_error = returns[t] - state_values[t]\n                else:\n                    returns[t] = rewards[t] + self.gamma * (1 - is_terminals[t]) * returns[t + 1]\n                    td_error = rewards[t] + self.gamma * (1 - is_terminals[t]) * state_values[t + 1] - state_values[t]\n\n                last_adv = self.gae_lambda * self.gamma * (1 - is_terminals[t]) * last_adv + td_error\n                advantages[t] = last_adv\n\n        return returns, advantages\n\n    #\n    # For hyperparameter optimization\n    #\n    @classmethod\n    def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        eps_clip = trial.suggest_categorical('eps_clip',\n                                             [0.1, 0.2, 0.3])\n\n        k_epochs = trial.suggest_categorical('k_epochs',\n                                             [1, 5, 10, 20])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'eps_clip': eps_clip,\n                'k_epochs': k_epochs,\n                }",
  "def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=64,\n                 update_frequency=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 vf_coef=0.5,\n                 learning_rate=0.01,\n                 optimizer_type='ADAM',\n                 eps_clip=0.2,\n                 k_epochs=5,\n                 use_gae=True,\n                 gae_lambda=0.95,\n                 policy_net_fn=None,\n                 value_net_fn=None,\n                 policy_net_kwargs=None,\n                 value_net_kwargs=None,\n                 device=\"cuda:best\",\n                 use_bonus=False,\n                 uncertainty_estimator_kwargs=None,\n                 **kwargs):  # TODO: sort arguments\n\n        # bonus\n        self.use_bonus = use_bonus\n        if self.use_bonus:\n            env = UncertaintyEstimatorWrapper(env, **uncertainty_estimator_kwargs)\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        # algorithm parameters\n        self.gamma = gamma\n        self.horizon = horizon\n        self.n_episodes = n_episodes\n\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.k_epochs = k_epochs\n        self.update_frequency = update_frequency\n\n        self.eps_clip = eps_clip\n        self.vf_coef = vf_coef\n        self.entr_coef = entr_coef\n\n        # options\n        # TODO: add reward normalization option\n        #       add observation normalization option\n        #       add orthogonal weight initialization option\n        #       add value function clip option\n        #       add ... ?\n        self.normalize_advantages = True  # TODO: turn into argument\n\n        self.use_gae = use_gae\n        self.gae_lambda = gae_lambda\n\n        # function approximators\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.value_net_kwargs = value_net_kwargs or {}\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.value_net_fn = value_net_fn or default_value_net_fn\n\n        self.device = choose_device(device)\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.cat_policy = None  # categorical policy function\n\n        # initialize\n        self.reset()",
  "def from_config(cls, **kwargs):\n        kwargs[\"policy_net_fn\"] = eval(kwargs[\"policy_net_fn\"])\n        kwargs[\"value_net_fn\"] = eval(kwargs[\"value_net_fn\"])\n        return cls(**kwargs)",
  "def reset(self, **kwargs):\n        self.cat_policy = self.policy_net_fn(self.env, **self.policy_net_kwargs).to(self.device)\n        self.policy_optimizer = optimizer_factory(self.cat_policy.parameters(), **self.optimizer_kwargs)\n\n        self.value_net = self.value_net_fn(self.env, **self.value_net_kwargs).to(self.device)\n        self.value_optimizer = optimizer_factory(self.value_net.parameters(), **self.optimizer_kwargs)\n\n        self.cat_policy_old = self.policy_net_fn(self.env, **self.policy_net_kwargs).to(self.device)\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())\n\n        self.MseLoss = nn.MSELoss()  # TODO: turn into argument\n\n        self.memory = Memory()  # TODO: Improve memory to include returns and advantages\n        self.returns = []  # TODO: add to memory\n        self.advantages = []  # TODO: add to memory\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name, log_every=5*logger.getEffectiveLevel())",
  "def policy(self, state, **kwargs):\n        assert self.cat_policy is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.cat_policy_old(state)\n        action = action_dist.sample().item()\n        return action",
  "def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def _run_episode(self):\n\n        # to store transitions\n        states = []\n        actions = []\n        action_logprobs = []\n        rewards = []\n        is_terminals = []\n\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n\n        for _ in range(self.horizon):\n            # running policy_old\n            state = torch.from_numpy(state).float().to(self.device)\n\n            action_dist = self.cat_policy_old(state)\n            action = action_dist.sample()\n            action_logprob = action_dist.log_prob(action)\n            action = action\n\n            next_state, reward, done, info = self.env.step(action.item())\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save transition\n            states.append(state)\n            actions.append(action)\n            action_logprobs.append(action_logprob)\n            rewards.append(reward + bonus)  # bonus added here\n            is_terminals.append(done)\n\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # compute returns and advantages\n        state_values = self.value_net(torch.stack(states).to(self.device)).detach()\n        state_values = torch.squeeze(state_values).tolist()\n\n        # TODO: add the option to normalize before computing returns/advantages?\n        returns, advantages = self._compute_returns_avantages(rewards, is_terminals, state_values)\n\n        # save in batch\n        self.memory.states.extend(states)\n        self.memory.actions.extend(actions)\n        self.memory.logprobs.extend(action_logprobs)\n        self.memory.rewards.extend(rewards)\n        self.memory.is_terminals.extend(is_terminals)\n\n        self.returns.extend(returns)  # TODO: add to memory (cf reset)\n        self.advantages.extend(advantages)  # TODO: add to memory (cf reset)\n\n        # save episodic rewards and cumulative rewards\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n\n        # log\n        if self.writer is not None:\n            self.writer.add_scalar(\"fit/total_reward\", episode_rewards, self.episode)\n\n        # update\n        if self.episode % self.update_frequency == 0:  # TODO: maybe change to update in function of n_steps instead\n            self._update()\n            self.memory.clear_memory()\n            del self.returns[:]  # TODO: add to memory (cf reset)\n            del self.advantages[:]  # TODO: add to memory (cf reset)\n\n        return episode_rewards",
  "def _update(self):\n\n        # convert list to tensor\n        full_old_states = torch.stack(self.memory.states).to(self.device).detach()\n        full_old_actions = torch.stack(self.memory.actions).to(self.device).detach()\n        full_old_logprobs = torch.stack(self.memory.logprobs).to(self.device).detach()\n        full_old_returns = torch.stack(self.returns).to(self.device).detach()\n        full_old_advantages = torch.stack(self.advantages).to(self.device).detach()\n\n        # optimize policy for K epochs\n        n_samples = full_old_actions.size(0)\n        n_batches = n_samples // self.batch_size\n\n        for _ in range(self.k_epochs):\n\n            # shuffle samples\n            rd_indices = np.random.choice(n_samples, size=n_samples, replace=False)\n            shuffled_states = full_old_states[rd_indices]\n            shuffled_actions = full_old_actions[rd_indices]\n            shuffled_logprobs = full_old_logprobs[rd_indices]\n            shuffled_returns = full_old_returns[rd_indices]\n            shuffled_advantages = full_old_advantages[rd_indices]\n\n            for k in range(n_batches):\n\n                # sample batch\n                batch_idx = np.arange(k * self.batch_size, min((k + 1) * self.batch_size, n_samples))\n                old_states = shuffled_states[batch_idx]\n                old_actions = shuffled_actions[batch_idx]\n                old_logprobs = shuffled_logprobs[batch_idx]\n                old_returns = shuffled_returns[batch_idx]\n                old_advantages = shuffled_advantages[batch_idx]\n\n                # evaluate old actions and values\n                action_dist = self.cat_policy(old_states)\n                logprobs = action_dist.log_prob(old_actions)\n                state_values = torch.squeeze(self.value_net(old_states))\n                dist_entropy = action_dist.entropy()\n\n                # find ratio (pi_theta / pi_theta__old)\n                ratios = torch.exp(logprobs - old_logprobs)\n\n                # TODO: add this option\n                # normalizing the rewards\n                # rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n                # normalize the advantages\n                old_advantages = old_advantages.view(-1, )\n\n                if self.normalize_advantages:\n                    old_advantages = (old_advantages - old_advantages.mean()) / (old_advantages.std() + 1e-10)\n\n                # compute surrogate loss\n                surr1 = ratios * old_advantages\n                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * old_advantages\n                surr_loss = torch.min(surr1, surr2)\n\n                # compute value function loss\n                loss_vf = self.vf_coef * self.MseLoss(state_values, old_returns)\n\n                # compute entropy loss\n                loss_entropy = self.entr_coef * dist_entropy\n\n                # compute total loss\n                loss = - surr_loss + loss_vf - loss_entropy\n\n                # take gradient step\n                self.policy_optimizer.zero_grad()\n                self.value_optimizer.zero_grad()\n\n                loss.mean().backward()\n\n                self.policy_optimizer.step()\n                self.value_optimizer.step()\n\n        # log\n        self.writer.add_scalar(\"fit/surrogate_loss\", surr_loss.mean().cpu().detach().numpy(), self.episode)\n        self.writer.add_scalar(\"fit/entropy_loss\", dist_entropy.mean().cpu().detach().numpy(), self.episode)\n\n        # copy new weights into old policy\n        self.cat_policy_old.load_state_dict(self.cat_policy.state_dict())",
  "def _compute_returns_avantages(self, rewards, is_terminals, state_values):\n\n        returns = torch.zeros(self.horizon).to(self.device)\n        advantages = torch.zeros(self.horizon).to(self.device)\n\n        if not self.use_gae:\n            for t in reversed(range(self.horizon)):\n                if t == self.horizon - 1:\n                    returns[t] = rewards[t] + self.gamma * (1 - is_terminals[t]) * state_values[-1]\n                else:\n                    returns[t] = rewards[t] + self.gamma * (1 - is_terminals[t]) * returns[t + 1]\n\n                advantages[t] = returns[t] - state_values[t]\n        else:\n            last_adv = 0\n            for t in reversed(range(self.horizon)):\n                if t == self.horizon - 1:\n                    returns[t] = rewards[t] + self.gamma * (1 - is_terminals[t]) * state_values[-1]\n                    td_error = returns[t] - state_values[t]\n                else:\n                    returns[t] = rewards[t] + self.gamma * (1 - is_terminals[t]) * returns[t + 1]\n                    td_error = rewards[t] + self.gamma * (1 - is_terminals[t]) * state_values[t + 1] - state_values[t]\n\n                last_adv = self.gae_lambda * self.gamma * (1 - is_terminals[t]) * last_adv + td_error\n                advantages[t] = last_adv\n\n        return returns, advantages",
  "def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        eps_clip = trial.suggest_categorical('eps_clip',\n                                             [0.1, 0.2, 0.3])\n\n        k_epochs = trial.suggest_categorical('k_epochs',\n                                             [1, 5, 10, 20])\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'eps_clip': eps_clip,\n                'k_epochs': k_epochs,\n                }",
  "def bounds_contains(bounds, x):\n    \"\"\"\n    Returns True if `x` is contained in the bounds, and False otherwise.\n\n    Parameters\n    ----------\n    bounds : numpy.ndarray\n        Array of shape (d, 2).\n        Bounds of each dimension [ [x0, y0], [x1, y1], ..., [xd, yd] ],\n        representing the following cartesian product in R^d:\n        [x0, y0] X [x1, y1] X ... X [xd, yd].\n    x : numpy.ndarray\n        Array of shape (d,)\n    \"\"\"\n    dim = x.shape[0]\n    for dd in range(dim):\n        if x[dd] < bounds[dd, 0] or x[dd] > bounds[dd, 1]:\n            return False\n    return True",
  "def split_bounds(bounds, dim=0):\n    \"\"\"\n    Split an array representing an l-infinity ball in R^d in R^d\n    into a list of 2^d arrays representing the ball split.\n\n    Parameters\n    ----------\n    bounds : numpy.ndarray\n        Array of shape (d, 2).\n        Bounds of each dimension [ [x0, y0], [x1, y1], ..., [xd, yd] ],\n        representing the cartesian product in R^d:\n        [x0, y0] X [x1, y1] X ... X [xd, yd].\n\n    dim : int, default: 0\n        Dimension from which to start splitting.\n\n    Returns\n    -------\n    List of arrays of shape (d, 2) containing the bounds to be split.\n    \"\"\"\n    if dim == bounds.shape[0]:\n        return [bounds]\n    left = bounds[dim, 0]\n    right = bounds[dim, 1]\n    middle = (left+right)/2.0\n\n    left_interval = bounds.copy()\n    right_interval = bounds.copy()\n\n    left_interval[dim, 0] = left\n    left_interval[dim, 1] = middle\n\n    right_interval[dim, 0] = middle\n    right_interval[dim, 1] = right\n\n    return split_bounds(left_interval, dim+1) + split_bounds(right_interval, dim+1)",
  "class AdaMBAgent(IncrementalAgent):\n    \"\"\"\n    Model-Based Reinforcement Learning with Adaptive Partitioning  [1]_\n\n    .. warning::\n        TO BE IMPLEMENTED, initially for enviroments with continuous (Box) states\n        and **discrete actions**.\n\n    Parameters\n    ----------\n    env : gym.Env\n        Environment with discrete states and actions.\n    n_episodes : int\n        Number of episodes\n    gamma : double, default: 1.0\n        Discount factor in [0, 1].\n    horizon : int\n        Horizon of the objective function.\n    bonus_scale_factor : double, default: 1.0\n        Constant by which to multiply the exploration bonus, controls\n        the level of exploration.\n    bonus_type : {\"simplified_bernstein\"}\n        Type of exploration bonus. Currently, only \"simplified_bernstein\"\n        is implemented.\n\n\n    References\n    ----------\n\n    .. [1]  Sinclair, S. R., Wang, T., Jain, G., Banerjee, S., & Yu, C. L. (2020).\n           Adaptive Discretization for Model-Based Reinforcement Learning.\n           arXiv preprint arXiv:2007.00717.\n\n    Notes\n    ------\n\n    Uses the metric induced by the l-infinity norm.\n    \"\"\"\n\n    name = 'AdaMBAgent'\n\n    def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 gamma=1.0,\n                 horizon=50,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 **kwargs):\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        self.v_max = np.zeros(self.horizon)\n        self.v_max[-1] = r_range\n        for hh in reversed(range(self.horizon-1)):\n            self.v_max[hh] = r_range + self.gamma*self.v_max[hh+1]\n\n        self.reset()\n\n    def reset(self):\n        # stores Q function and MDP model.\n        self.model = MDPTreePartition(self.env.observation_space,\n                                      self.env.action_space,\n                                      self.horizon)\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n        self.episode = 0\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, observation, hh=0, **kwargs):\n        return 0\n\n    def _update(self, node, state, action, next_state, reward, hh):\n        pass\n\n    def _compute_bonus(self, n, hh):\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max[hh] / n\n            bonus = min(bonus, self.v_max[hh])\n            return bonus\n        else:\n            raise ValueError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = 0    # TODO\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward\n\n            # self._update(node, state, action, next_state, reward, hh)\n\n            state = next_state\n            if done:\n                break\n\n        # update info\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self.episode += 1\n\n        # writer\n        if self.writer is not None:\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards\n\n    def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 gamma=1.0,\n                 horizon=50,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 **kwargs):\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        self.v_max = np.zeros(self.horizon)\n        self.v_max[-1] = r_range\n        for hh in reversed(range(self.horizon-1)):\n            self.v_max[hh] = r_range + self.gamma*self.v_max[hh+1]\n\n        self.reset()",
  "def reset(self):\n        # stores Q function and MDP model.\n        self.model = MDPTreePartition(self.env.observation_space,\n                                      self.env.action_space,\n                                      self.horizon)\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n        self.episode = 0\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())",
  "def policy(self, observation, hh=0, **kwargs):\n        return 0",
  "def _update(self, node, state, action, next_state, reward, hh):\n        pass",
  "def _compute_bonus(self, n, hh):\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max[hh] / n\n            bonus = min(bonus, self.v_max[hh])\n            return bonus\n        else:\n            raise ValueError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = 0    # TODO\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward\n\n            # self._update(node, state, action, next_state, reward, hh)\n\n            state = next_state\n            if done:\n                break\n\n        # update info\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self.episode += 1\n\n        # writer\n        if self.writer is not None:\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards",
  "def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "class TreeNode:\n    \"\"\"\n    Node representing an l-infinity ball in R^d, that points\n    to sub-balls (node children).\n    Stores a value, a number of visits, and (possibly) rewards and transition probability\n    to a list of other nodes.\n\n    This class is used to represent (and store data about)\n    a tuple (state, action, stage) = (x, a, h).\n\n    Parameters\n    ----------\n    bounds : numpy.ndarray\n        Bounds of each dimension [ [x0, y0], [x1, y1], ..., [xd, yd] ],\n        representing the cartesian product in R^d:\n        [x0, y0] X [x1, y1] X ... X [xd, yd]\n    depth: int\n        Node depth, root is at depth 0.\n    qvalue : double, default: 0\n        Initial node Q value\n    n_visits : int, default = 0\n        Number of visits to the node.\n\n    \"\"\"\n    def __init__(self, bounds, depth, qvalue=0.0, n_visits=0):\n        self.dim = len(bounds)\n\n        self.radius = (bounds[:, 1] - bounds[:, 0]).max() / 2.0\n        assert self.radius > 0.0\n\n        self.bounds = bounds\n        self.depth = depth\n        self.qvalue = qvalue\n        self.n_visits = n_visits\n        self.children = []\n\n        #\n        # For AdaMB\n        #\n\n        # Value V, initialized as Q\n        self.vvalue = qvalue\n        # Reward estimate\n        self.reward_est = 0.0\n        # Dictionary node_id -> transition_prob\n        # node_id = id(node), where id() is a built-in python function\n        self.transition_probs = {}\n        # Dictionary node_id -> node\n        self.transition_nodes = {}\n\n    def is_leaf(self):\n        return len(self.children) == 0\n\n    def contains(self, x):\n        \"\"\"Check if `x` is contained in the node/ball.\"\"\"\n        return bounds_contains(self.bounds, x)\n\n    def split(self):\n        \"\"\"Spawn children nodes by splitting the ball.\"\"\"\n        child_bounds = split_bounds(self.bounds)\n        for bounds in child_bounds:\n            self.children.append(\n                TreeNode(bounds, self.depth+1, self.qvalue, self.n_visits)\n            )",
  "class TreePartition:\n    \"\"\"\n    Tree-based partition of an l-infinity ball in R^d.\n\n    Each node is of type TreeNode.\n\n    Parameters\n    ----------\n    space: gym.spaces.Box\n        Domain of the function.\n    initial_value: double\n        Value to initialize the root node.\n    \"\"\"\n    def __init__(self, space, initial_value=0.0):\n        assert isinstance(space, spaces.Box)\n        assert space.is_bounded()\n\n        bounds = np.vstack((space.low, space.high)).T\n        self.root = TreeNode(bounds, depth=0, qvalue=initial_value)\n        self.dim = bounds.shape[0]\n        self.dmax = self.root.radius\n\n    def traverse(self, x, update=False):\n        \"\"\"\n        Returns leaf node containing x.\n\n        If `update=true`, increments number of visits of each\n        node in the path.\n\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Array of shape (d,)\n        \"\"\"\n        node = self.root\n\n        # traverse the tree until leaf\n        while True:\n            if update:\n                node.n_visits += 1\n            if node.is_leaf():\n                break\n            for cc in node.children:\n                if cc.contains(x):\n                    node = cc\n                    break\n\n        # return value at leaf\n        return node\n\n    def plot(self,\n             fignum=\"tree plot\",\n             colormap_name='cool',\n             max_value=10,\n             node=None,\n             root=True,):\n        \"\"\"\n        Visualize the function (2d domain only).\n        Shows the hierarchical partition.\n        \"\"\"\n        if root:\n            assert self.dim == 2, \"TreePartition plot only available for 2-dimensional spaces.\"\n            node = self.root\n            plt.figure(fignum)\n\n        # draw region corresponding to the leaf\n        if node.is_leaf():\n            x0, x1 = node.bounds[0, :]\n            y0, y1 = node.bounds[1, :]\n\n            colormap_fn = plt.get_cmap(colormap_name)\n            color = colormap_fn(node.qvalue/max_value)\n            rectangle = plt.Rectangle((x0, y0), x1-x0, y1-y0, ec=\"black\", color=color)\n            plt.gca().add_patch(rectangle)\n            plt.axis('scaled')\n\n        else:\n            for cc in node.children:\n                self.plot(max_value=max_value, colormap_name=colormap_name, node=cc, root=False)",
  "class MDPTreePartition:\n    \"\"\"\n    Set of H x A TreePartition instances.\n\n    Used to store/manipulate a Q function, a reward function and a transition model.\n    \"\"\"\n    def __init__(self, observation_space, action_space, horizon):\n        self.horizon = horizon\n        self.n_actions = action_space.n\n        self.trees = []\n        for hh in range(horizon):\n            self.trees.append({})\n            for aa in range(self.n_actions):\n                self.trees[hh][aa] = TreePartition(observation_space,\n                                                   initial_value=horizon-hh)\n\n        self.dmax = self.trees[0][0].dmax\n\n    def get_argmax_and_node(self, x, hh):\n        \"\"\"\n        Returns a* = argmax_a Q_h(x, a) and the node corresponding to (x, a*).\n        \"\"\"\n        # trees for each action at hh\n        trees_hh = self.trees[hh]\n\n        best_action = 0\n        best_node = trees_hh[0].traverse(x, update=False)\n        best_val = best_node.qvalue\n        for aa in range(1, self.n_actions):\n            node = trees_hh[aa].traverse(x, update=False)\n            val = node.qvalue\n            if val > best_val:\n                best_val = val\n                best_action = aa\n                best_node = node\n\n        return best_action, best_node\n\n    def update_counts(self, x, aa, hh):\n        \"\"\"\n        Increment counters associated to (x, aa, hh) and returns the node.\n        \"\"\"\n        tree = self.trees[hh][aa]\n        node = tree.traverse(x, update=True)\n        return node\n\n    def plot(self, a, h):\n        \"\"\"\n        Visualize Q_h(x, a)\n        \"\"\"\n        self.trees[h][a].plot(max_value=self.horizon-h)",
  "def __init__(self, bounds, depth, qvalue=0.0, n_visits=0):\n        self.dim = len(bounds)\n\n        self.radius = (bounds[:, 1] - bounds[:, 0]).max() / 2.0\n        assert self.radius > 0.0\n\n        self.bounds = bounds\n        self.depth = depth\n        self.qvalue = qvalue\n        self.n_visits = n_visits\n        self.children = []\n\n        #\n        # For AdaMB\n        #\n\n        # Value V, initialized as Q\n        self.vvalue = qvalue\n        # Reward estimate\n        self.reward_est = 0.0\n        # Dictionary node_id -> transition_prob\n        # node_id = id(node), where id() is a built-in python function\n        self.transition_probs = {}\n        # Dictionary node_id -> node\n        self.transition_nodes = {}",
  "def is_leaf(self):\n        return len(self.children) == 0",
  "def contains(self, x):\n        \"\"\"Check if `x` is contained in the node/ball.\"\"\"\n        return bounds_contains(self.bounds, x)",
  "def split(self):\n        \"\"\"Spawn children nodes by splitting the ball.\"\"\"\n        child_bounds = split_bounds(self.bounds)\n        for bounds in child_bounds:\n            self.children.append(\n                TreeNode(bounds, self.depth+1, self.qvalue, self.n_visits)\n            )",
  "def __init__(self, space, initial_value=0.0):\n        assert isinstance(space, spaces.Box)\n        assert space.is_bounded()\n\n        bounds = np.vstack((space.low, space.high)).T\n        self.root = TreeNode(bounds, depth=0, qvalue=initial_value)\n        self.dim = bounds.shape[0]\n        self.dmax = self.root.radius",
  "def traverse(self, x, update=False):\n        \"\"\"\n        Returns leaf node containing x.\n\n        If `update=true`, increments number of visits of each\n        node in the path.\n\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Array of shape (d,)\n        \"\"\"\n        node = self.root\n\n        # traverse the tree until leaf\n        while True:\n            if update:\n                node.n_visits += 1\n            if node.is_leaf():\n                break\n            for cc in node.children:\n                if cc.contains(x):\n                    node = cc\n                    break\n\n        # return value at leaf\n        return node",
  "def plot(self,\n             fignum=\"tree plot\",\n             colormap_name='cool',\n             max_value=10,\n             node=None,\n             root=True,):\n        \"\"\"\n        Visualize the function (2d domain only).\n        Shows the hierarchical partition.\n        \"\"\"\n        if root:\n            assert self.dim == 2, \"TreePartition plot only available for 2-dimensional spaces.\"\n            node = self.root\n            plt.figure(fignum)\n\n        # draw region corresponding to the leaf\n        if node.is_leaf():\n            x0, x1 = node.bounds[0, :]\n            y0, y1 = node.bounds[1, :]\n\n            colormap_fn = plt.get_cmap(colormap_name)\n            color = colormap_fn(node.qvalue/max_value)\n            rectangle = plt.Rectangle((x0, y0), x1-x0, y1-y0, ec=\"black\", color=color)\n            plt.gca().add_patch(rectangle)\n            plt.axis('scaled')\n\n        else:\n            for cc in node.children:\n                self.plot(max_value=max_value, colormap_name=colormap_name, node=cc, root=False)",
  "def __init__(self, observation_space, action_space, horizon):\n        self.horizon = horizon\n        self.n_actions = action_space.n\n        self.trees = []\n        for hh in range(horizon):\n            self.trees.append({})\n            for aa in range(self.n_actions):\n                self.trees[hh][aa] = TreePartition(observation_space,\n                                                   initial_value=horizon-hh)\n\n        self.dmax = self.trees[0][0].dmax",
  "def get_argmax_and_node(self, x, hh):\n        \"\"\"\n        Returns a* = argmax_a Q_h(x, a) and the node corresponding to (x, a*).\n        \"\"\"\n        # trees for each action at hh\n        trees_hh = self.trees[hh]\n\n        best_action = 0\n        best_node = trees_hh[0].traverse(x, update=False)\n        best_val = best_node.qvalue\n        for aa in range(1, self.n_actions):\n            node = trees_hh[aa].traverse(x, update=False)\n            val = node.qvalue\n            if val > best_val:\n                best_val = val\n                best_action = aa\n                best_node = node\n\n        return best_action, best_node",
  "def update_counts(self, x, aa, hh):\n        \"\"\"\n        Increment counters associated to (x, aa, hh) and returns the node.\n        \"\"\"\n        tree = self.trees[hh][aa]\n        node = tree.traverse(x, update=True)\n        return node",
  "def plot(self, a, h):\n        \"\"\"\n        Visualize Q_h(x, a)\n        \"\"\"\n        self.trees[h][a].plot(max_value=self.horizon-h)",
  "class AdaptiveQLAgent(IncrementalAgent):\n    \"\"\"\n    Adaptive Q-Learning algorithm [1]_ implemented for enviroments\n    with continuous (Box) states and **discrete actions**.\n\n    .. todo:: Handle continuous actios too.\n\n    Parameters\n    ----------\n    env : gym.Env\n        Environment with discrete states and actions.\n    n_episodes : int\n        Number of episodes.\n    gamma : double, default: 1.0\n        Discount factor in [0, 1].\n    horizon : int\n        Horizon of the objective function.\n    bonus_scale_factor : double, default: 1.0\n        Constant by which to multiply the exploration bonus, controls\n        the level of exploration.\n    bonus_type : {\"simplified_bernstein\"}\n        Type of exploration bonus. Currently, only \"simplified_bernstein\"\n        is implemented.\n\n\n    References\n    ----------\n\n    .. [1] Sinclair, Sean R., Siddhartha Banerjee, and Christina Lee Yu.\n    \"Adaptive Discretization for Episodic Reinforcement Learning in Metric Spaces.\"\n     Proceedings of the ACM on Measurement and Analysis of Computing Systems 3.3 (2019): 1-44.\n\n    Notes\n    ------\n\n    Uses the metric induced by the l-infinity norm.\n    \"\"\"\n\n    name = 'AdaptiveQLearning'\n\n    def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 gamma=1.0,\n                 horizon=50,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 **kwargs):\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        self.v_max = np.zeros(self.horizon)\n        self.v_max[-1] = r_range\n        for hh in reversed(range(self.horizon-1)):\n            self.v_max[hh] = r_range + self.gamma*self.v_max[hh+1]\n\n        self.reset()\n\n    def reset(self):\n        self.Qtree = MDPTreePartition(self.env.observation_space,\n                                      self.env.action_space,\n                                      self.horizon)\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n        self.episode = 0\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, observation, hh=0, **kwargs):\n        action, _ = self.Qtree.get_argmax_and_node(observation, hh)\n        return action\n\n    def _get_action_and_node(self, observation, hh):\n        action, node = self.Qtree.get_argmax_and_node(observation, hh)\n        return action, node\n\n    def _update(self, node, state, action, next_state, reward, hh):\n        # split node if necessary\n        node_to_check = self.Qtree.update_counts(state, action, hh)\n        if node_to_check.n_visits >= (self.Qtree.dmax/node_to_check.radius)**2.0:\n            node_to_check.split()\n        assert id(node_to_check) == id(node)\n\n        tt = node.n_visits  # number of visits to the selected state-action node\n\n        # value at next_state\n        value_next_state = 0\n        if hh < self.horizon-1:\n            value_next_state = min(\n                self.v_max[hh+1],\n                self.Qtree.get_argmax_and_node(next_state, hh+1)[1].qvalue\n            )\n\n        # learning rate\n        alpha = (self.horizon+1.0)/(self.horizon + tt)\n\n        bonus = self._compute_bonus(tt, hh)\n        target = reward + bonus + self.gamma*value_next_state\n\n        # update Q\n        node.qvalue = (1-alpha)*node.qvalue + alpha*target\n\n    def _compute_bonus(self, n, hh):\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max[hh] / n\n            bonus = min(bonus, self.v_max[hh])\n            return bonus\n        else:\n            raise ValueError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action, node = self._get_action_and_node(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward\n\n            self._update(node, state, action, next_state, reward, hh)\n\n            state = next_state\n            if done:\n                break\n\n        # update info\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self.episode += 1\n\n        # writer\n        if self.writer is not None:\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards\n\n    def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 gamma=1.0,\n                 horizon=50,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 **kwargs):\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        self.v_max = np.zeros(self.horizon)\n        self.v_max[-1] = r_range\n        for hh in reversed(range(self.horizon-1)):\n            self.v_max[hh] = r_range + self.gamma*self.v_max[hh+1]\n\n        self.reset()",
  "def reset(self):\n        self.Qtree = MDPTreePartition(self.env.observation_space,\n                                      self.env.action_space,\n                                      self.horizon)\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n        self.episode = 0\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())",
  "def policy(self, observation, hh=0, **kwargs):\n        action, _ = self.Qtree.get_argmax_and_node(observation, hh)\n        return action",
  "def _get_action_and_node(self, observation, hh):\n        action, node = self.Qtree.get_argmax_and_node(observation, hh)\n        return action, node",
  "def _update(self, node, state, action, next_state, reward, hh):\n        # split node if necessary\n        node_to_check = self.Qtree.update_counts(state, action, hh)\n        if node_to_check.n_visits >= (self.Qtree.dmax/node_to_check.radius)**2.0:\n            node_to_check.split()\n        assert id(node_to_check) == id(node)\n\n        tt = node.n_visits  # number of visits to the selected state-action node\n\n        # value at next_state\n        value_next_state = 0\n        if hh < self.horizon-1:\n            value_next_state = min(\n                self.v_max[hh+1],\n                self.Qtree.get_argmax_and_node(next_state, hh+1)[1].qvalue\n            )\n\n        # learning rate\n        alpha = (self.horizon+1.0)/(self.horizon + tt)\n\n        bonus = self._compute_bonus(tt, hh)\n        target = reward + bonus + self.gamma*value_next_state\n\n        # update Q\n        node.qvalue = (1-alpha)*node.qvalue + alpha*target",
  "def _compute_bonus(self, n, hh):\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max[hh] / n\n            bonus = min(bonus, self.v_max[hh])\n            return bonus\n        else:\n            raise ValueError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action, node = self._get_action_and_node(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward\n\n            self._update(node, state, action, next_state, reward, hh)\n\n            state = next_state\n            if done:\n                break\n\n        # update info\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self.episode += 1\n\n        # writer\n        if self.writer is not None:\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards",
  "def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "class REINFORCEAgent(IncrementalAgent):\n    \"\"\"\n    REINFORCE with entropy regularization.\n\n    Parameters\n    ----------\n    env : Model\n        Online model with continuous (Box) state space and discrete actions\n    n_episodes : int\n        Number of episodes\n    batch_size : int\n        Number of episodes to wait before updating the policy.\n    horizon : int\n        Horizon.\n    gamma : double\n        Discount factor in [0, 1].\n    entr_coef : double\n        Entropy coefficient.\n    learning_rate : double\n        Learning rate.\n    normalize: bool\n        If True normalize rewards\n    optimizer_type: str\n        Type of optimizer. 'ADAM' by defaut.\n    policy_net_fn : function(env, **kwargs)\n        Function that returns an instance of a policy network (pytorch).\n        If None, a default net is used.\n    policy_net_kwargs : dict\n        kwargs for policy_net_fn\n    use_bonus_if_available : bool, default = False\n        If true, check if environment info has entry 'exploration_bonus'\n        and add it to the reward. See also UncertaintyEstimatorWrapper.\n    device: str\n        Device to put the tensors on\n\n    References\n    ----------\n    Williams, Ronald J.,\n    \"Simple statistical gradient-following algorithms for connectionist\n    reinforcement learning.\"\n    ReinforcementLearning.Springer,Boston,MA,1992.5-3\n    \"\"\"\n\n    name = \"REINFORCE\"\n\n    def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 learning_rate=0.0001,\n                 normalize=True,\n                 optimizer_type='ADAM',\n                 policy_net_fn=None,\n                 policy_net_kwargs=None,\n                 use_bonus_if_available=False,\n                 device=\"cuda:best\",\n                 **kwargs):\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.horizon = horizon\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.learning_rate = learning_rate\n        self.normalize = normalize\n        self.use_bonus_if_available = use_bonus_if_available\n        self.device = choose_device(device)\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        self.policy_net_kwargs = policy_net_kwargs or {}\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.policy_net = None  # policy network\n\n        # initialize\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.policy_net = self.policy_net_fn(\n                            self.env,\n                            **self.policy_net_kwargs,\n                        ).to(self.device)\n\n        self.policy_optimizer = optimizer_factory(\n                                    self.policy_net.parameters(),\n                                    **self.optimizer_kwargs)\n\n        self.memory = Memory()\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        log_every = 5*logger.getEffectiveLevel()\n        self.writer = PeriodicWriter(self.name, log_every=log_every)\n\n    def policy(self, state, **kwargs):\n        assert self.policy_net is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.policy_net(state)\n        action = action_dist.sample().item()\n        return action\n\n    def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # running policy\n            action = self.policy(state)\n            next_state, reward, done, info = self.env.step(action)\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus_if_available:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save in batch\n            self.memory.states.append(state)\n            self.memory.actions.append(action)\n            self.memory.rewards.append(reward+bonus)  # add bonus here\n            self.memory.is_terminals.append(done)\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # update\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        #\n        if self.episode % self.batch_size == 0:\n            self._update()\n            self.memory.clear_memory()\n\n        return episode_rewards\n\n    def _normalize(self, x):\n        return (x-x.mean())/(x.std()+1e-5)\n\n    def _update(self):\n        # monte carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.memory.rewards),\n                                       reversed(self.memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # convert list to tensor\n        states = torch.FloatTensor(self.memory.states).to(self.device)\n        actions = torch.LongTensor(self.memory.actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        if self.normalize:\n            rewards = self._normalize(rewards)\n\n        # evaluate logprobs\n        action_dist = self.policy_net(states)\n        logprobs = action_dist.log_prob(actions)\n        dist_entropy = action_dist.entropy()\n\n        # compute loss\n        loss = -logprobs * rewards - self.entr_coef * dist_entropy\n\n        # take gradient step\n        self.policy_optimizer.zero_grad()\n\n        loss.mean().backward()\n\n        self.policy_optimizer.step()\n\n    #\n    # For hyperparameter optimization\n    #\n    @classmethod\n    def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                }",
  "def __init__(self, env,\n                 n_episodes=4000,\n                 batch_size=8,\n                 horizon=256,\n                 gamma=0.99,\n                 entr_coef=0.01,\n                 learning_rate=0.0001,\n                 normalize=True,\n                 optimizer_type='ADAM',\n                 policy_net_fn=None,\n                 policy_net_kwargs=None,\n                 use_bonus_if_available=False,\n                 device=\"cuda:best\",\n                 **kwargs):\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.batch_size = batch_size\n        self.horizon = horizon\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.learning_rate = learning_rate\n        self.normalize = normalize\n        self.use_bonus_if_available = use_bonus_if_available\n        self.device = choose_device(device)\n\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        self.policy_net_kwargs = policy_net_kwargs or {}\n\n        #\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        self.policy_net = None  # policy network\n\n        # initialize\n        self.reset()",
  "def reset(self, **kwargs):\n        self.policy_net = self.policy_net_fn(\n                            self.env,\n                            **self.policy_net_kwargs,\n                        ).to(self.device)\n\n        self.policy_optimizer = optimizer_factory(\n                                    self.policy_net.parameters(),\n                                    **self.optimizer_kwargs)\n\n        self.memory = Memory()\n\n        self.episode = 0\n\n        # useful data\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        log_every = 5*logger.getEffectiveLevel()\n        self.writer = PeriodicWriter(self.name, log_every=log_every)",
  "def policy(self, state, **kwargs):\n        assert self.policy_net is not None\n        state = torch.from_numpy(state).float().to(self.device)\n        action_dist = self.policy_net(state)\n        action = action_dist.sample().item()\n        return action",
  "def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # running policy\n            action = self.policy(state)\n            next_state, reward, done, info = self.env.step(action)\n\n            # check whether to use bonus\n            bonus = 0.0\n            if self.use_bonus_if_available:\n                if info is not None and 'exploration_bonus' in info:\n                    bonus = info['exploration_bonus']\n\n            # save in batch\n            self.memory.states.append(state)\n            self.memory.actions.append(action)\n            self.memory.rewards.append(reward+bonus)  # add bonus here\n            self.memory.is_terminals.append(done)\n            episode_rewards += reward\n\n            if done:\n                break\n\n            # update state\n            state = next_state\n\n        # update\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n        self.episode += 1\n\n        #\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        #\n        if self.episode % self.batch_size == 0:\n            self._update()\n            self.memory.clear_memory()\n\n        return episode_rewards",
  "def _normalize(self, x):\n        return (x-x.mean())/(x.std()+1e-5)",
  "def _update(self):\n        # monte carlo estimate of rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(self.memory.rewards),\n                                       reversed(self.memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n\n        # convert list to tensor\n        states = torch.FloatTensor(self.memory.states).to(self.device)\n        actions = torch.LongTensor(self.memory.actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        if self.normalize:\n            rewards = self._normalize(rewards)\n\n        # evaluate logprobs\n        action_dist = self.policy_net(states)\n        logprobs = action_dist.log_prob(actions)\n        dist_entropy = action_dist.entropy()\n\n        # compute loss\n        loss = -logprobs * rewards - self.entr_coef * dist_entropy\n\n        # take gradient step\n        self.policy_optimizer.zero_grad()\n\n        loss.mean().backward()\n\n        self.policy_optimizer.step()",
  "def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [1, 4, 8, 16, 32])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                }",
  "def update_model(repr_state, action, repr_next_state, reward,\n                 n_representatives, repr_states,\n                 lp_metric, scaling, bandwidth,\n                 bonus_scale_factor, beta, v_max, bonus_type,\n                 kernel_type, N_sa, B_sa, P_hat, R_hat):\n    \"\"\"\n    Model update function, lots of arguments so we can use JIT :)\n    \"\"\"\n    # aux var for transition update\n    dirac_next_s = np.zeros(n_representatives)\n    dirac_next_s[repr_next_state] = 1.0\n\n    for u_repr_state in range(n_representatives):\n        # compute weight\n        dist = metric_lp(repr_states[repr_state, :],\n                         repr_states[u_repr_state, :],\n                         lp_metric,\n                         scaling)\n        weight = kernel_func(dist/bandwidth, kernel_type=kernel_type)\n\n        # aux variables\n        prev_N_sa = beta + N_sa[u_repr_state, action]  # regularization beta\n        current_N_sa = prev_N_sa + weight\n\n        # update weights\n        N_sa[u_repr_state, action] += weight\n\n        # update transitions\n        P_hat[u_repr_state, action, :n_representatives] =\\\n            dirac_next_s*weight / current_N_sa + \\\n            (prev_N_sa/current_N_sa) * \\\n            P_hat[u_repr_state, action, :n_representatives]\n\n        # update rewards\n        R_hat[u_repr_state, action] = weight*reward/current_N_sa + \\\n            (prev_N_sa/current_N_sa)*R_hat[u_repr_state, action]\n\n        # update bonus\n        B_sa[u_repr_state, action] = compute_bonus(N_sa[u_repr_state, action],\n                                                   beta, bonus_scale_factor,\n                                                   v_max, bonus_type)",
  "def compute_bonus(sum_weights, beta, bonus_scale_factor, v_max, bonus_type):\n    n = beta + sum_weights\n    if bonus_type == \"simplified_bernstein\":\n        return bonus_scale_factor * np.sqrt(1.0/n) + (1+beta)*(v_max)/n\n    else:\n        raise NotImplementedError(\"Error: unknown bonus type.\")",
  "class RSKernelUCBVIAgent(Agent):\n    \"\"\"\n    Implements KernelUCBVI [1] with representative states [2, 3].\n\n    Value iteration with exploration bonuses for continuous-state environments,\n    using a online discretization strategy + kernel smoothing:\n    - Build (online) a set of representative states\n    - Using smoothing kernels, estimate transtions an rewards on the\n    finite set of representative states and actions.\n\n    Criterion: finite-horizon with discount factor gamma.\n    If the discount is not 1, only the Q function at h=0 is used.\n\n    The recommended policy after all the episodes is computed without\n    exploration bonuses.\n\n\n    Parameters\n    ----------\n    env : Model\n        Online model with continuous (Box) state space and discrete actions\n    n_episodes : int\n        number of episodes\n    gamma : double\n        Discount factor in [0, 1]. If gamma is 1.0, the problem is set to\n        be finite-horizon.\n    horizon : int\n        Horizon of the objective function. If None and gamma<1, set to\n        1/(1-gamma).\n    lp_metric: int\n        The metric on the state space is the one induced by the p-norm,\n        where p = lp_metric. Default = 2, for the Euclidean metric.\n    kernel_type : string\n        See rlberry.agents.kernel_based.kernels.kernel_func for\n        possible kernel types.\n    scaling: numpy.ndarray\n        Must have the same size as state array, used to scale the states\n        before computing the metric.\n        If None, set to:\n        - (env.observation_space.high - env.observation_space.low) if high\n            and low are bounded\n        - np.ones(env.observation_space.shape[0]) if high or low\n        are unbounded\n    bandwidth : double\n        Kernel bandwidth.\n    min_dist : double\n        Minimum distance between two representative states\n    max_repr : int\n        Maximum number of representative states.\n        If None, it is set to  (sqrt(d)/min_dist)**d, where d\n        is the dimension of the state space\n    bonus_scale_factor : double\n        Constant by which to multiply the exploration bonus,\n        controls the level of exploration.\n    beta : double\n        Regularization constant.\n    bonus_type : string\n            Type of exploration bonus. Currently, only \"simplified_bernstein\"\n            is implemented.\n\n\n    References\n    ----------\n    [1] Domingues et al., 2020\n        Regret Bounds for Kernel-Based Reinforcement Learning\n        https://arxiv.org/abs/2004.05599\n    [2] Domingues et al., 2020\n        A Kernel-Based Approach to Non-Stationary Reinforcement Learning\n        in Metric Spaces\n        https://arxiv.org/abs/2007.05078\n    [3] Kveton & Theocharous, 2012\n        Kernel-Based Reinforcement Learning on Representative States\n        https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewFile/4967/5509\n    \"\"\"\n\n    name = \"RSKernelUCBVI\"\n\n    def __init__(self, env,\n                 n_episodes=1000,\n                 gamma=0.95,\n                 horizon=None,\n                 lp_metric=2,\n                 kernel_type=\"epanechnikov\",\n                 scaling=None,\n                 bandwidth=0.05,\n                 min_dist=0.1,\n                 max_repr=1000,\n                 bonus_scale_factor=1.0,\n                 beta=0.01,\n                 bonus_type=\"simplified_bernstein\",\n                 **kwargs):\n        # init base class\n        Agent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.lp_metric = lp_metric\n        self.kernel_type = kernel_type\n        self.bandwidth = bandwidth\n        self.min_dist = min_dist\n        self.bonus_scale_factor = bonus_scale_factor\n        self.beta = beta\n        self.bonus_type = bonus_type\n\n        # check environment\n        assert self.env.is_online()\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # other checks\n        assert gamma >= 0 and gamma <= 1.0\n        if self.horizon is None:\n            assert gamma < 1.0, \\\n                \"If no horizon is given, gamma must be smaller than 1.\"\n            self.horizon = int(np.ceil(1.0 / (1.0 - gamma)))\n\n        # state dimension\n        self.state_dim = self.env.observation_space.shape[0]\n\n        # compute scaling, if it is None\n        if scaling is None:\n            # if high and low are bounded\n            if (self.env.observation_space.high == np.inf).sum() == 0 \\\n                    and (self.env.observation_space.low == -np.inf).sum() == 0:\n                scaling = self.env.observation_space.high \\\n                    - self.env.observation_space.low\n                # if high or low are unbounded\n            else:\n                scaling = np.ones(self.state_dim)\n        else:\n            assert scaling.ndim == 1\n            assert scaling.shape[0] == self.state_dim\n        self.scaling = scaling\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon)) \\\n                                                        / (1.0 - self.gamma)\n\n        # number of representative states and number of actions\n        if max_repr is None:\n            max_repr = int(np.ceil((1.0 * np.sqrt(self.state_dim)\n                                    / self.min_dist) ** self.state_dim))\n        self.max_repr = max_repr\n\n        # current number of representative states\n        self.M = None\n        self.A = self.env.action_space.n\n\n        # declaring variables\n        self.episode = None  # current episode\n        self.representative_states = None  # coordinates of all repr states\n        self.N_sa = None   # sum of weights at (s, a)\n        self.B_sa = None   # bonus at (s, a)\n        self.R_hat = None  # reward  estimate\n        self.P_hat = None  # transitions estimate\n        self.Q = None  # Q function\n        self.V = None  # V function\n\n        self.Q_policy = None  # Q function for recommended policy\n\n        # initialize\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.M = 0\n        self.representative_states = np.zeros((self.max_repr, self.state_dim))\n        self.N_sa = np.zeros((self.max_repr, self.A))\n        self.B_sa = self.v_max * np.ones((self.max_repr, self.A))\n\n        self.R_hat = np.zeros((self.max_repr, self.A))\n        self.P_hat = np.zeros((self.max_repr, self.A, self.max_repr))\n\n        self.V = np.zeros((self.horizon, self.max_repr))\n        self.Q = np.zeros((self.horizon, self.max_repr, self.A))\n        self.Q_policy = None\n\n        self.episode = 0\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, state, hh=0, **kwargs):\n        assert self.Q_policy is not None\n        repr_state = self._map_to_repr(state, False)\n\n        # no discount\n        if self.gamma == 1.0:\n            return self.Q_policy[hh, repr_state, :].argmax()\n        # discounted\n        return self.Q_policy[0, repr_state, :].argmax()\n\n    def fit(self, **kwargs):\n        info = {}\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n        for _ in range(self.n_episodes):\n            self._run_episode()\n\n        # compute Q function for the recommended policy\n        self.Q_policy, _ = backward_induction(self.R_hat[:self.M, :],\n                                              self.P_hat[:self.M, :, :self.M],\n                                              self.horizon, self.gamma)\n\n        info[\"n_episodes\"] = self.n_episodes\n        info[\"episode_rewards\"] = self._rewards\n        return info\n\n    def _map_to_repr(self, state, accept_new_repr=True):\n        repr_state = map_to_representative(state,\n                                           self.lp_metric,\n                                           self.representative_states,\n                                           self.M,\n                                           self.min_dist,\n                                           self.scaling,\n                                           accept_new_repr)\n        # check if new representative state\n        if repr_state == self.M:\n            self.M += 1\n        return repr_state\n\n    def _update(self, state, action, next_state, reward):\n        repr_state = self._map_to_repr(state)\n        repr_next_state = self._map_to_repr(next_state)\n\n        update_model(repr_state, action, repr_next_state, reward,\n                     self.M,\n                     self.representative_states,\n                     self.lp_metric,\n                     self.scaling,\n                     self.bandwidth,\n                     self.bonus_scale_factor,\n                     self.beta,\n                     self.v_max,\n                     self.bonus_type,\n                     self.kernel_type,\n                     self.N_sa,\n                     self.B_sa,\n                     self.P_hat,\n                     self.R_hat)\n\n    def _get_action(self, state, hh=0):\n        assert self.Q is not None\n        repr_state = self._map_to_repr(state, False)\n\n        # no discount\n        if self.gamma == 1.0:\n            return self.Q[hh, repr_state, :].argmax()\n        # discounted\n        return self.Q[0, repr_state, :].argmax()\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = self._get_action(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            self._update(state, action, next_state, reward)\n            state = next_state\n            episode_rewards += reward\n\n            if done:\n                break\n\n        # run backward induction\n        backward_induction_in_place(\n                                self.Q[:, :self.M, :], self.V[:, :self.M],\n                                self.R_hat[:self.M, :]+self.B_sa[:self.M, :],\n                                self.P_hat[:self.M, :, :self.M],\n                                self.horizon, self.gamma, self.v_max)\n\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n\n        self.episode += 1\n        #\n        if self.writer is not None:\n            avg_reward = self._cumul_rewards[ep]/max(1, ep)\n\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n            self.writer.add_scalar(\"avg reward\", avg_reward)\n            self.writer.add_scalar(\"representative states\", self.M)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards",
  "def __init__(self, env,\n                 n_episodes=1000,\n                 gamma=0.95,\n                 horizon=None,\n                 lp_metric=2,\n                 kernel_type=\"epanechnikov\",\n                 scaling=None,\n                 bandwidth=0.05,\n                 min_dist=0.1,\n                 max_repr=1000,\n                 bonus_scale_factor=1.0,\n                 beta=0.01,\n                 bonus_type=\"simplified_bernstein\",\n                 **kwargs):\n        # init base class\n        Agent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.lp_metric = lp_metric\n        self.kernel_type = kernel_type\n        self.bandwidth = bandwidth\n        self.min_dist = min_dist\n        self.bonus_scale_factor = bonus_scale_factor\n        self.beta = beta\n        self.bonus_type = bonus_type\n\n        # check environment\n        assert self.env.is_online()\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # other checks\n        assert gamma >= 0 and gamma <= 1.0\n        if self.horizon is None:\n            assert gamma < 1.0, \\\n                \"If no horizon is given, gamma must be smaller than 1.\"\n            self.horizon = int(np.ceil(1.0 / (1.0 - gamma)))\n\n        # state dimension\n        self.state_dim = self.env.observation_space.shape[0]\n\n        # compute scaling, if it is None\n        if scaling is None:\n            # if high and low are bounded\n            if (self.env.observation_space.high == np.inf).sum() == 0 \\\n                    and (self.env.observation_space.low == -np.inf).sum() == 0:\n                scaling = self.env.observation_space.high \\\n                    - self.env.observation_space.low\n                # if high or low are unbounded\n            else:\n                scaling = np.ones(self.state_dim)\n        else:\n            assert scaling.ndim == 1\n            assert scaling.shape[0] == self.state_dim\n        self.scaling = scaling\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon)) \\\n                                                        / (1.0 - self.gamma)\n\n        # number of representative states and number of actions\n        if max_repr is None:\n            max_repr = int(np.ceil((1.0 * np.sqrt(self.state_dim)\n                                    / self.min_dist) ** self.state_dim))\n        self.max_repr = max_repr\n\n        # current number of representative states\n        self.M = None\n        self.A = self.env.action_space.n\n\n        # declaring variables\n        self.episode = None  # current episode\n        self.representative_states = None  # coordinates of all repr states\n        self.N_sa = None   # sum of weights at (s, a)\n        self.B_sa = None   # bonus at (s, a)\n        self.R_hat = None  # reward  estimate\n        self.P_hat = None  # transitions estimate\n        self.Q = None  # Q function\n        self.V = None  # V function\n\n        self.Q_policy = None  # Q function for recommended policy\n\n        # initialize\n        self.reset()",
  "def reset(self, **kwargs):\n        self.M = 0\n        self.representative_states = np.zeros((self.max_repr, self.state_dim))\n        self.N_sa = np.zeros((self.max_repr, self.A))\n        self.B_sa = self.v_max * np.ones((self.max_repr, self.A))\n\n        self.R_hat = np.zeros((self.max_repr, self.A))\n        self.P_hat = np.zeros((self.max_repr, self.A, self.max_repr))\n\n        self.V = np.zeros((self.horizon, self.max_repr))\n        self.Q = np.zeros((self.horizon, self.max_repr, self.A))\n        self.Q_policy = None\n\n        self.episode = 0\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())",
  "def policy(self, state, hh=0, **kwargs):\n        assert self.Q_policy is not None\n        repr_state = self._map_to_repr(state, False)\n\n        # no discount\n        if self.gamma == 1.0:\n            return self.Q_policy[hh, repr_state, :].argmax()\n        # discounted\n        return self.Q_policy[0, repr_state, :].argmax()",
  "def fit(self, **kwargs):\n        info = {}\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n        for _ in range(self.n_episodes):\n            self._run_episode()\n\n        # compute Q function for the recommended policy\n        self.Q_policy, _ = backward_induction(self.R_hat[:self.M, :],\n                                              self.P_hat[:self.M, :, :self.M],\n                                              self.horizon, self.gamma)\n\n        info[\"n_episodes\"] = self.n_episodes\n        info[\"episode_rewards\"] = self._rewards\n        return info",
  "def _map_to_repr(self, state, accept_new_repr=True):\n        repr_state = map_to_representative(state,\n                                           self.lp_metric,\n                                           self.representative_states,\n                                           self.M,\n                                           self.min_dist,\n                                           self.scaling,\n                                           accept_new_repr)\n        # check if new representative state\n        if repr_state == self.M:\n            self.M += 1\n        return repr_state",
  "def _update(self, state, action, next_state, reward):\n        repr_state = self._map_to_repr(state)\n        repr_next_state = self._map_to_repr(next_state)\n\n        update_model(repr_state, action, repr_next_state, reward,\n                     self.M,\n                     self.representative_states,\n                     self.lp_metric,\n                     self.scaling,\n                     self.bandwidth,\n                     self.bonus_scale_factor,\n                     self.beta,\n                     self.v_max,\n                     self.bonus_type,\n                     self.kernel_type,\n                     self.N_sa,\n                     self.B_sa,\n                     self.P_hat,\n                     self.R_hat)",
  "def _get_action(self, state, hh=0):\n        assert self.Q is not None\n        repr_state = self._map_to_repr(state, False)\n\n        # no discount\n        if self.gamma == 1.0:\n            return self.Q[hh, repr_state, :].argmax()\n        # discounted\n        return self.Q[0, repr_state, :].argmax()",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = self._get_action(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            self._update(state, action, next_state, reward)\n            state = next_state\n            episode_rewards += reward\n\n            if done:\n                break\n\n        # run backward induction\n        backward_induction_in_place(\n                                self.Q[:, :self.M, :], self.V[:, :self.M],\n                                self.R_hat[:self.M, :]+self.B_sa[:self.M, :],\n                                self.P_hat[:self.M, :, :self.M],\n                                self.horizon, self.gamma, self.v_max)\n\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n\n        self.episode += 1\n        #\n        if self.writer is not None:\n            avg_reward = self._cumul_rewards[ep]/max(1, ep)\n\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n            self.writer.add_scalar(\"avg reward\", avg_reward)\n            self.writer.add_scalar(\"representative states\", self.M)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards",
  "def kernel_func(z, kernel_type):\n    \"\"\"\n    Returns a kernel function to the real value z.\n\n    Kernel types:\n\n    \"uniform\"      : 1.0*(abs(z) <= 1)\n    \"triangular\"   : max(0, 1 - abs(z))\n    \"gaussian\"     : exp(-z^2/2)\n    \"epanechnikov\" : max(0, 1-z^2)\n    \"quartic\"      : (1-z^2)^2 *(abs(z) <= 1)\n    \"triweight\"    : (1-z^2)^3 *(abs(z) <= 1)\n    \"tricube\"      : (1-abs(z)^3)^3 *(abs(z) <= 1)\n    \"cosine\"       : cos( z * (pi/2) ) *(abs(z) <= 1)\n    \"exp-n\"        : exp(-abs(z)^n/2), for n integer\n\n    Parameters\n    ----------\n    z : double\n    kernel_type : string\n    \"\"\"\n    if kernel_type == \"uniform\":\n        return 1.0 * (np.abs(z) <= 1)\n    elif kernel_type == \"triangular\":\n        return (1.0 - np.abs(z)) * (np.abs(z) <= 1)\n    elif kernel_type == \"gaussian\":\n        return np.exp(-np.power(z, 2.0) / 2.0)\n    elif kernel_type == \"epanechnikov\":\n        return (1.0 - np.power(z, 2.0)) * (np.abs(z) <= 1)\n    elif kernel_type == \"quartic\":\n        return np.power((1.0 - np.power(z, 2.0)), 2.0)*(np.abs(z) <= 1)\n    elif kernel_type == \"triweight\":\n        return np.power((1.0 - np.power(z, 2.0)), 3.0)*(np.abs(z) <= 1)\n    elif kernel_type == \"tricube\":\n        return np.power((1.0 - np.power(np.abs(z), 3.0)), 3.0)*(np.abs(z) <= 1)\n    elif kernel_type == \"cosine\":\n        return np.cos(z*np.pi/2)*(np.abs(z) <= 1)\n    elif \"exp-\" in kernel_type:\n        exponent = _str_to_int(kernel_type.split(\"-\")[1])\n        return np.exp(-np.power(np.abs(z), exponent) / 2.0)\n    else:\n        raise NotImplementedError(\"Unknown kernel type.\")",
  "def _str_to_int(s):\n    \"\"\"\n    Source: https://github.com/numba/numba/issues/5650#issuecomment-623511109\n    \"\"\"\n    final_index, result = len(s) - 1, 0\n    for i, v in enumerate(s):\n        result += (ord(v) - 48) * (10 ** (final_index - i))\n    return result",
  "class RSUCBVIAgent(IncrementalAgent):\n    \"\"\"\n    Value iteration with exploration bonuses for continuous-state environments,\n    using a online discretization strategy:\n    - Build (online) a set of representative states\n    - Estimate transtions an rewards on the finite set of representative states\n    and actions.\n\n    Criterion: finite-horizon with discount factor gamma.\n    If the discount is not 1, only the Q function at h=0 is used.\n\n    The recommended policy after all the episodes is computed without\n    exploration bonuses.\n\n    Parameters\n    ----------\n    env : Model\n        Online model with continuous (Box) state space and discrete actions\n    n_episodes : int\n        number of episodes\n    gamma : double\n        Discount factor in [0, 1]. If gamma is 1.0, the problem is set to\n        be finite-horizon.\n    horizon : int\n        Horizon of the objective function. If None and gamma<1, set to\n        1/(1-gamma).\n    lp_metric: int\n        The metric on the state space is the one induced by the p-norm,\n        where p = lp_metric. Default = 2, for the Euclidean metric.\n    scaling: numpy.ndarray\n        Must have the same size as state array, used to scale the states\n        before computing the metric.\n        If None, set to:\n        - (env.observation_space.high - env.observation_space.low) if high\n        and low are bounded\n        - np.ones(env.observation_space.shape[0]) if high or low are\n        unbounded\n    min_dist: double\n        Minimum distance between two representative states\n    max_repr: int\n        Maximum number of representative states.\n        If None, it is set to  (sqrt(d)/min_dist)**d, where d\n        is the dimension of the state space\n    bonus_scale_factor : double\n        Constant by which to multiply the exploration bonus, controls\n        the level of exploration.\n    bonus_type : string\n        Type of exploration bonus. Currently, only \"simplified_bernstein\"\n        is implemented. If `reward_free` is true, this parameter is ignored\n        and the algorithm uses 1/n bonuses.\n    reward_free : bool\n        If true, ignores rewards and uses only 1/n bonuses.\n\n    References\n    ----------\n    .. [1] Azar, Mohammad Gheshlaghi, Ian Osband, and R\u00e9mi Munos.\n    \"Minimax regret bounds for reinforcement learning.\"\n    Proceedings of the 34th ICML, 2017.\n\n    .. [2] Strehl, Alexander L., and Michael L. Littman.\n    \"An analysis of model-based interval estimation for Markov decision\n    processes.\"\n     Journal of Computer and System Sciences 74.8 (2008): 1309-1331.\n\n    .. [3] Kveton, Branislav, and Georgios Theocharous.\n    \"Kernel-Based Reinforcement Learning on Representative States.\"\n    AAAI, 2012.\n\n    .. [4] Domingues, O. D., M\u00e9nard, P., Pirotta, M., Kaufmann, E., & Valko, M.(2020).\n    A kernel-based approach to non-stationary reinforcement learning in metric\n    spaces.\n    arXiv preprint arXiv:2007.05078.\n    \"\"\"\n\n    name = \"RSUCBVI\"\n\n    def __init__(self, env,\n                 n_episodes=1000,\n                 gamma=0.99,\n                 horizon=100,\n                 lp_metric=2,\n                 scaling=None,\n                 min_dist=0.1,\n                 max_repr=1000,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 reward_free=False,\n                 **kwargs):\n        # init base class\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.lp_metric = lp_metric\n        self.min_dist = min_dist\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n        self.reward_free = reward_free\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # other checks\n        assert gamma >= 0 and gamma <= 1.0\n        if self.horizon is None:\n            assert gamma < 1.0, \\\n                \"If no horizon is given, gamma must be smaller than 1.\"\n            self.horizon = int(np.ceil(1.0 / (1.0 - gamma)))\n\n        # state dimension\n        self.state_dim = self.env.observation_space.shape[0]\n\n        # compute scaling, if it is None\n        if scaling is None:\n            # if high and low are bounded\n            if (self.env.observation_space.high == np.inf).sum() == 0 \\\n                    and (self.env.observation_space.low == -np.inf).sum() == 0:\n                scaling = self.env.observation_space.high \\\n                    - self.env.observation_space.low\n                # if high or low are unbounded\n            else:\n                scaling = np.ones(self.state_dim)\n        else:\n            assert scaling.ndim == 1\n            assert scaling.shape[0] == self.state_dim\n        self.scaling = scaling\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon)) \\\n                                                        / (1.0 - self.gamma)\n\n        # number of representative states and number of actions\n        if max_repr is None:\n            max_repr = int(np.ceil((1.0 * np.sqrt(self.state_dim) /\n                                    self.min_dist) ** self.state_dim))\n        self.max_repr = max_repr\n\n        # current number of representative states\n        self.M = None\n        self.A = self.env.action_space.n\n\n        # declaring variables\n        self.episode = None  # current episode\n        self.representative_states = None  # coordinates of all repr states\n        self.N_sa = None  # visits to (s, a)\n        self.N_sas = None  # visits to (s, a, s')\n        self.S_sa = None  # sum of rewards at (s, a)\n        self.B_sa = None  # bonus at (s, a)\n        self.Q = None  # Q function\n        self.V = None  # V function\n\n        self.Q_policy = None  # Q function for recommended policy\n\n        # initialize\n        self.reset()\n\n    def reset(self, **kwargs):\n        self.M = 0\n        self.representative_states = np.zeros((self.max_repr, self.state_dim))\n        self.N_sa = np.zeros((self.max_repr, self.A))\n        self.N_sas = np.zeros((self.max_repr, self.A, self.max_repr))\n        self.S_sa = np.zeros((self.max_repr, self.A))\n        self.B_sa = self.v_max * np.ones((self.max_repr, self.A))\n\n        self.R_hat = np.zeros((self.max_repr, self.A))\n        self.P_hat = np.zeros((self.max_repr, self.A, self.max_repr))\n\n        self.V = np.zeros((self.horizon, self.max_repr))\n        self.Q = np.zeros((self.horizon, self.max_repr, self.A))\n        self.Q_policy = None\n\n        self.episode = 0\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, state, hh=0, **kwargs):\n        assert self.Q_policy is not None\n        repr_state = self._map_to_repr(state, False)\n\n        # no discount\n        if self.gamma == 1.0:\n            return self.Q_policy[hh, repr_state, :].argmax()\n        # discounted\n        return self.Q_policy[0, repr_state, :].argmax()\n\n    def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        # compute Q function for the recommended policy\n        self.Q_policy, _ = backward_induction(self.R_hat[:self.M, :],\n                                              self.P_hat[:self.M, :, :self.M],\n                                              self.horizon, self.gamma)\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info\n\n    def _map_to_repr(self, state, accept_new_repr=True):\n        repr_state = map_to_representative(state,\n                                           self.lp_metric,\n                                           self.representative_states,\n                                           self.M,\n                                           self.min_dist,\n                                           self.scaling,\n                                           accept_new_repr)\n        # check if new representative state\n        if repr_state == self.M:\n            self.M += 1\n        return repr_state\n\n    def _update(self, state, action, next_state, reward):\n        repr_state = self._map_to_repr(state)\n        repr_next_state = self._map_to_repr(next_state)\n\n        self.N_sa[repr_state, action] += 1\n        self.N_sas[repr_state, action, repr_next_state] += 1\n        self.S_sa[repr_state, action] += reward\n\n        self.R_hat[repr_state, action] = self.S_sa[repr_state, action] \\\n            / self.N_sa[repr_state, action]\n        self.P_hat[repr_state, action, :] = self.N_sas[repr_state, action, :] \\\n            / self.N_sa[repr_state, action]\n        self.B_sa[repr_state, action] = \\\n            self._compute_bonus(self.N_sa[repr_state, action])\n\n    def _compute_bonus(self, n):\n        # reward-free\n        if self.reward_free:\n            bonus = 1.0 / n\n            return bonus\n\n        # not reward-free\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max / n\n            bonus = min(bonus, self.v_max)\n            return bonus\n        else:\n            raise NotImplementedError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))\n\n    def _get_action(self, state, hh=0):\n        assert self.Q is not None\n        repr_state = self._map_to_repr(state, False)\n\n        # no discount\n        if self.gamma == 1.0:\n            return self.Q[hh, repr_state, :].argmax()\n        # discounted\n        return self.Q[0, repr_state, :].argmax()\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = self._get_action(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward  # used for logging only\n\n            if self.reward_free:\n                reward = 0.0  # set to zero before update if reward_free\n\n            self._update(state, action, next_state, reward)\n\n            state = next_state\n            if done:\n                break\n\n        # run backward induction\n        backward_induction_in_place(\n            self.Q[:, :self.M, :], self.V[:, :self.M],\n            self.R_hat[:self.M, :] + self.B_sa[:self.M, :],\n            self.P_hat[:self.M, :, :self.M],\n            self.horizon, self.gamma, self.v_max)\n\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n\n        self.episode += 1\n        #\n        if self.writer is not None:\n            avg_reward = self._cumul_rewards[ep]/max(1, ep)\n\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n            self.writer.add_scalar(\"avg reward\", avg_reward, self.episode)\n            self.writer.add_scalar(\"representative states\", self.M, self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards",
  "def __init__(self, env,\n                 n_episodes=1000,\n                 gamma=0.99,\n                 horizon=100,\n                 lp_metric=2,\n                 scaling=None,\n                 min_dist=0.1,\n                 max_repr=1000,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 reward_free=False,\n                 **kwargs):\n        # init base class\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.lp_metric = lp_metric\n        self.min_dist = min_dist\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n        self.reward_free = reward_free\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # other checks\n        assert gamma >= 0 and gamma <= 1.0\n        if self.horizon is None:\n            assert gamma < 1.0, \\\n                \"If no horizon is given, gamma must be smaller than 1.\"\n            self.horizon = int(np.ceil(1.0 / (1.0 - gamma)))\n\n        # state dimension\n        self.state_dim = self.env.observation_space.shape[0]\n\n        # compute scaling, if it is None\n        if scaling is None:\n            # if high and low are bounded\n            if (self.env.observation_space.high == np.inf).sum() == 0 \\\n                    and (self.env.observation_space.low == -np.inf).sum() == 0:\n                scaling = self.env.observation_space.high \\\n                    - self.env.observation_space.low\n                # if high or low are unbounded\n            else:\n                scaling = np.ones(self.state_dim)\n        else:\n            assert scaling.ndim == 1\n            assert scaling.shape[0] == self.state_dim\n        self.scaling = scaling\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        if self.gamma == 1.0:\n            self.v_max = r_range * horizon\n        else:\n            self.v_max = r_range * (1.0 - np.power(self.gamma, self.horizon)) \\\n                                                        / (1.0 - self.gamma)\n\n        # number of representative states and number of actions\n        if max_repr is None:\n            max_repr = int(np.ceil((1.0 * np.sqrt(self.state_dim) /\n                                    self.min_dist) ** self.state_dim))\n        self.max_repr = max_repr\n\n        # current number of representative states\n        self.M = None\n        self.A = self.env.action_space.n\n\n        # declaring variables\n        self.episode = None  # current episode\n        self.representative_states = None  # coordinates of all repr states\n        self.N_sa = None  # visits to (s, a)\n        self.N_sas = None  # visits to (s, a, s')\n        self.S_sa = None  # sum of rewards at (s, a)\n        self.B_sa = None  # bonus at (s, a)\n        self.Q = None  # Q function\n        self.V = None  # V function\n\n        self.Q_policy = None  # Q function for recommended policy\n\n        # initialize\n        self.reset()",
  "def reset(self, **kwargs):\n        self.M = 0\n        self.representative_states = np.zeros((self.max_repr, self.state_dim))\n        self.N_sa = np.zeros((self.max_repr, self.A))\n        self.N_sas = np.zeros((self.max_repr, self.A, self.max_repr))\n        self.S_sa = np.zeros((self.max_repr, self.A))\n        self.B_sa = self.v_max * np.ones((self.max_repr, self.A))\n\n        self.R_hat = np.zeros((self.max_repr, self.A))\n        self.P_hat = np.zeros((self.max_repr, self.A, self.max_repr))\n\n        self.V = np.zeros((self.horizon, self.max_repr))\n        self.Q = np.zeros((self.horizon, self.max_repr, self.A))\n        self.Q_policy = None\n\n        self.episode = 0\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())",
  "def policy(self, state, hh=0, **kwargs):\n        assert self.Q_policy is not None\n        repr_state = self._map_to_repr(state, False)\n\n        # no discount\n        if self.gamma == 1.0:\n            return self.Q_policy[hh, repr_state, :].argmax()\n        # discounted\n        return self.Q_policy[0, repr_state, :].argmax()",
  "def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        # compute Q function for the recommended policy\n        self.Q_policy, _ = backward_induction(self.R_hat[:self.M, :],\n                                              self.P_hat[:self.M, :, :self.M],\n                                              self.horizon, self.gamma)\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def _map_to_repr(self, state, accept_new_repr=True):\n        repr_state = map_to_representative(state,\n                                           self.lp_metric,\n                                           self.representative_states,\n                                           self.M,\n                                           self.min_dist,\n                                           self.scaling,\n                                           accept_new_repr)\n        # check if new representative state\n        if repr_state == self.M:\n            self.M += 1\n        return repr_state",
  "def _update(self, state, action, next_state, reward):\n        repr_state = self._map_to_repr(state)\n        repr_next_state = self._map_to_repr(next_state)\n\n        self.N_sa[repr_state, action] += 1\n        self.N_sas[repr_state, action, repr_next_state] += 1\n        self.S_sa[repr_state, action] += reward\n\n        self.R_hat[repr_state, action] = self.S_sa[repr_state, action] \\\n            / self.N_sa[repr_state, action]\n        self.P_hat[repr_state, action, :] = self.N_sas[repr_state, action, :] \\\n            / self.N_sa[repr_state, action]\n        self.B_sa[repr_state, action] = \\\n            self._compute_bonus(self.N_sa[repr_state, action])",
  "def _compute_bonus(self, n):\n        # reward-free\n        if self.reward_free:\n            bonus = 1.0 / n\n            return bonus\n\n        # not reward-free\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max / n\n            bonus = min(bonus, self.v_max)\n            return bonus\n        else:\n            raise NotImplementedError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))",
  "def _get_action(self, state, hh=0):\n        assert self.Q is not None\n        repr_state = self._map_to_repr(state, False)\n\n        # no discount\n        if self.gamma == 1.0:\n            return self.Q[hh, repr_state, :].argmax()\n        # discounted\n        return self.Q[0, repr_state, :].argmax()",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = self._get_action(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward  # used for logging only\n\n            if self.reward_free:\n                reward = 0.0  # set to zero before update if reward_free\n\n            self._update(state, action, next_state, reward)\n\n            state = next_state\n            if done:\n                break\n\n        # run backward induction\n        backward_induction_in_place(\n            self.Q[:, :self.M, :], self.V[:, :self.M],\n            self.R_hat[:self.M, :] + self.B_sa[:self.M, :],\n            self.P_hat[:self.M, :, :self.M],\n            self.horizon, self.gamma, self.v_max)\n\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards \\\n            + self._cumul_rewards[max(0, ep - 1)]\n\n        self.episode += 1\n        #\n        if self.writer is not None:\n            avg_reward = self._cumul_rewards[ep]/max(1, ep)\n\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n            self.writer.add_scalar(\"avg reward\", avg_reward, self.episode)\n            self.writer.add_scalar(\"representative states\", self.M, self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards",
  "def map_to_representative(state,\n                          lp_metric,\n                          representative_states,\n                          n_representatives,\n                          min_dist,\n                          scaling,\n                          accept_new_repr):\n    \"\"\"Map state to representative state. \"\"\"\n    dist_to_closest = np.inf\n    argmin = -1\n    for ii in range(n_representatives):\n        dist = metric_lp(state, representative_states[ii, :],\n                         lp_metric,\n                         scaling)\n        if dist < dist_to_closest:\n            dist_to_closest = dist\n            argmin = ii\n\n    max_representatives = representative_states.shape[0]\n    if (dist_to_closest > min_dist) \\\n        and (n_representatives < max_representatives) \\\n            and accept_new_repr:\n        new_index = n_representatives\n        representative_states[new_index, :] = state\n        return new_index\n    return argmin",
  "class CEMAgent(IncrementalAgent):\n    \"\"\"\n    Parameters\n    ----------\n    env : Model\n        Environment for training.\n    n_episodes : int\n        Number of training episodes.\n    horizon : int\n        Maximum length of a trajectory.\n    gamma : double\n        Discount factor in [0, 1].\n    entr_coef : double\n        Entropy coefficient.\n    batch_size : int\n        Number of trajectories to sample at each iteration.\n    percentile : int\n        Percentile used to remove trajectories with low rewards.\n    learning_rate : double\n        Optimizer learning rate\n    optimizer_type: str\n        Type of optimizer. 'ADAM' by defaut.\n    on_policy : bool\n        If True, updates are done only with on-policy data.\n    policy_net_fn : function(env, **kwargs)\n        Function that returns an instance of a policy network (pytorch).\n        If None, a default net is used.\n    policy_net_kwargs : dict\n        kwargs for policy_net_fn\n    device : str\n        Device to put the tensors on\n    \"\"\"\n\n    name = \"CrossEntropyAgent\"\n\n    def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 horizon=100,\n                 gamma=0.99,\n                 entr_coef=0.1,\n                 batch_size=16,\n                 percentile=70,\n                 learning_rate=0.01,\n                 optimizer_type='ADAM',\n                 on_policy=False,\n                 policy_net_fn=None,\n                 policy_net_kwargs=None,\n                 device=\"cuda:best\",\n                 **kwargs):\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # parameters\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.batch_size = batch_size\n        self.n_episodes = n_episodes\n        self.percentile = percentile\n        self.learning_rate = learning_rate\n        self.horizon = horizon\n        self.on_policy = on_policy\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n        self.device = choose_device(device)\n        self.reset()\n\n    def reset(self, **kwargs):\n        # policy net\n        self.policy_net = self.policy_net_fn(\n                            self.env,\n                            **self.policy_net_kwargs\n                            ).to(self.device)\n\n        # loss function and optimizer\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.optimizer = optimizer_factory(\n                                    self.policy_net.parameters(),\n                                    **self.optimizer_kwargs)\n\n        # memory\n        self.memory = CEMMemory(self.batch_size)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n        #\n        self.episode = 0\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)\n\n    def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction * self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info\n\n    def policy(self, observation, **kwargs):\n        state = torch.from_numpy(observation).float().to(self.device)\n        action_dist = self.policy_net(state)\n        action = action_dist.sample().item()\n        return action\n\n    def _process_batch(self):\n        rewards = np.array(self.memory.rewards)\n\n        reward_bound = np.percentile(rewards, self.percentile)\n        reward_mean = float(np.mean(rewards))\n\n        train_states = []\n        train_actions = []\n\n        for ii in range(self.memory.size):\n            if rewards[ii] < reward_bound:\n                continue\n            train_states.extend(self.memory.states[ii])\n            train_actions.extend(self.memory.actions[ii])\n\n        train_states_tensor = torch.FloatTensor(train_states).to(self.device)\n        train_actions_tensor = torch.LongTensor(train_actions).to(self.device)\n\n        # states in last trajectory\n        last_states = self.memory.states[-1]\n        last_states_tensor = torch.FloatTensor(last_states).to(self.device)\n\n        return train_states_tensor, train_actions_tensor, \\\n            reward_bound, reward_mean, last_states_tensor\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        episode_states = []\n        episode_actions = []\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # take action according to policy_net\n            action = self.policy(state)\n            next_state, reward, done, _ = self.env.step(action)\n\n            # save\n            episode_states.append(state)\n            episode_actions.append(action)\n\n            # increment rewards\n            episode_rewards += reward\n\n            if done:\n                break\n            state = next_state\n\n        self.memory.append(episode_states, episode_actions, episode_rewards)\n\n        # update\n        if (self.episode % self.batch_size == 0) or (not self.on_policy):\n            self._update()\n\n        #\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards + \\\n            self._cumul_rewards[max(0, ep - 1)]\n\n        # increment ep and write\n        self.episode += 1\n\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        return episode_rewards\n\n    def _update(self):\n        train_states_tensor, train_actions_tensor,\\\n            reward_bound, reward_mean, \\\n            last_states_tensor = self._process_batch()\n        self.optimizer.zero_grad()\n        action_scores = self.policy_net.action_scores(train_states_tensor)\n        loss = self.loss_fn(action_scores, train_actions_tensor)\n\n        # entropy in last trajectory\n        action_dist_last_traj = self.policy_net(last_states_tensor)\n        entropy = action_dist_last_traj.entropy().mean()\n\n        loss = loss - self.entr_coef*entropy\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.item(), reward_mean, reward_bound\n\n    #\n    # For hyperparameter optimization\n    #\n    @classmethod\n    def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [10, 20, 50, 100, 200])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        on_policy = trial.suggest_categorical('on_policy',\n                                              [False, True])\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'on_policy': on_policy\n                }",
  "def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 horizon=100,\n                 gamma=0.99,\n                 entr_coef=0.1,\n                 batch_size=16,\n                 percentile=70,\n                 learning_rate=0.01,\n                 optimizer_type='ADAM',\n                 on_policy=False,\n                 policy_net_fn=None,\n                 policy_net_kwargs=None,\n                 device=\"cuda:best\",\n                 **kwargs):\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Box)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # parameters\n        self.gamma = gamma\n        self.entr_coef = entr_coef\n        self.batch_size = batch_size\n        self.n_episodes = n_episodes\n        self.percentile = percentile\n        self.learning_rate = learning_rate\n        self.horizon = horizon\n        self.on_policy = on_policy\n        self.policy_net_kwargs = policy_net_kwargs or {}\n        self.policy_net_fn = policy_net_fn or default_policy_net_fn\n        self.optimizer_kwargs = {'optimizer_type': optimizer_type,\n                                 'lr': learning_rate}\n        self.device = choose_device(device)\n        self.reset()",
  "def reset(self, **kwargs):\n        # policy net\n        self.policy_net = self.policy_net_fn(\n                            self.env,\n                            **self.policy_net_kwargs\n                            ).to(self.device)\n\n        # loss function and optimizer\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.optimizer = optimizer_factory(\n                                    self.policy_net.parameters(),\n                                    **self.optimizer_kwargs)\n\n        # memory\n        self.memory = CEMMemory(self.batch_size)\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n        #\n        self.episode = 0\n        self._rewards = np.zeros(self.n_episodes)\n        self._cumul_rewards = np.zeros(self.n_episodes)",
  "def partial_fit(self, fraction: float, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction * self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def policy(self, observation, **kwargs):\n        state = torch.from_numpy(observation).float().to(self.device)\n        action_dist = self.policy_net(state)\n        action = action_dist.sample().item()\n        return action",
  "def _process_batch(self):\n        rewards = np.array(self.memory.rewards)\n\n        reward_bound = np.percentile(rewards, self.percentile)\n        reward_mean = float(np.mean(rewards))\n\n        train_states = []\n        train_actions = []\n\n        for ii in range(self.memory.size):\n            if rewards[ii] < reward_bound:\n                continue\n            train_states.extend(self.memory.states[ii])\n            train_actions.extend(self.memory.actions[ii])\n\n        train_states_tensor = torch.FloatTensor(train_states).to(self.device)\n        train_actions_tensor = torch.LongTensor(train_actions).to(self.device)\n\n        # states in last trajectory\n        last_states = self.memory.states[-1]\n        last_states_tensor = torch.FloatTensor(last_states).to(self.device)\n\n        return train_states_tensor, train_actions_tensor, \\\n            reward_bound, reward_mean, last_states_tensor",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        episode_states = []\n        episode_actions = []\n        state = self.env.reset()\n        for _ in range(self.horizon):\n            # take action according to policy_net\n            action = self.policy(state)\n            next_state, reward, done, _ = self.env.step(action)\n\n            # save\n            episode_states.append(state)\n            episode_actions.append(action)\n\n            # increment rewards\n            episode_rewards += reward\n\n            if done:\n                break\n            state = next_state\n\n        self.memory.append(episode_states, episode_actions, episode_rewards)\n\n        # update\n        if (self.episode % self.batch_size == 0) or (not self.on_policy):\n            self._update()\n\n        #\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self._cumul_rewards[ep] = episode_rewards + \\\n            self._cumul_rewards[max(0, ep - 1)]\n\n        # increment ep and write\n        self.episode += 1\n\n        if self.writer is not None:\n            self.writer.add_scalar(\"episode\", self.episode, None)\n            self.writer.add_scalar(\"ep reward\", episode_rewards)\n\n        return episode_rewards",
  "def _update(self):\n        train_states_tensor, train_actions_tensor,\\\n            reward_bound, reward_mean, \\\n            last_states_tensor = self._process_batch()\n        self.optimizer.zero_grad()\n        action_scores = self.policy_net.action_scores(train_states_tensor)\n        loss = self.loss_fn(action_scores, train_actions_tensor)\n\n        # entropy in last trajectory\n        action_dist_last_traj = self.policy_net(last_states_tensor)\n        entropy = action_dist_last_traj.entropy().mean()\n\n        loss = loss - self.entr_coef*entropy\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.item(), reward_mean, reward_bound",
  "def sample_parameters(cls, trial):\n        batch_size = trial.suggest_categorical('batch_size',\n                                               [10, 20, 50, 100, 200])\n        gamma = trial.suggest_categorical('gamma',\n                                          [0.9, 0.95, 0.99])\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1)\n\n        entr_coef = trial.suggest_loguniform('entr_coef', 1e-8, 0.1)\n\n        on_policy = trial.suggest_categorical('on_policy',\n                                              [False, True])\n        return {\n                'batch_size': batch_size,\n                'gamma': gamma,\n                'learning_rate': learning_rate,\n                'entr_coef': entr_coef,\n                'on_policy': on_policy\n                }",
  "def default_policy_net_fn(env):\n    \"\"\"\n    Returns a default value network.\n    \"\"\"\n    if isinstance(env.observation_space, spaces.Box):\n        obs_shape = env.observation_space.shape\n    elif isinstance(env.observation_space, spaces.Tuple):\n        obs_shape = env.observation_space.spaces[0].shape\n    else:\n        raise ValueError(\"Incompatible observation space: {}\".format(env.observation_space))\n\n    if len(obs_shape) == 3:\n        if obs_shape[0] < obs_shape[1] and obs_shape[0] < obs_shape[1]:\n            # Assume CHW observation space\n            model_config = {\"type\": \"ConvolutionalNetwork\",\n                            \"is_policy\": True,\n                            \"in_channels\": int(obs_shape[0]),\n                            \"in_height\": int(obs_shape[1]),\n                            \"in_width\": int(obs_shape[2])}\n        elif obs_shape[2] < obs_shape[0] and obs_shape[2] < obs_shape[1]:\n            # Assume WHC observation space\n            model_config = {\"type\": \"ConvolutionalNetwork\",\n                            \"is_policy\": True,\n                            \"transpose_obs\": True,\n                            \"in_channels\": int(obs_shape[2]),\n                            \"in_height\": int(obs_shape[1]),\n                            \"in_width\": int(obs_shape[0])}\n    elif len(obs_shape) == 2:\n        model_config = {\"type\": \"ConvolutionalNetwork\",\n                        \"is_policy\": True,\n                        \"in_channels\": int(1),\n                        \"in_height\": int(obs_shape[0]),\n                        \"in_width\": int(obs_shape[1])}\n    elif len(obs_shape) == 1:\n        model_config = {\"type\": \"MultiLayerPerceptron\", \"in_size\": int(obs_shape[0]),\n                        \"layer_sizes\": [64, 64], \"reshape\": False, \"is_policy\": True}\n    else:\n        raise ValueError(\"Incompatible observation shape: {}\".format(env.observation_space.shape))\n\n    if isinstance(env.action_space, spaces.Discrete):\n        model_config[\"out_size\"] = env.action_space.n\n    elif isinstance(env.action_space, spaces.Tuple):\n        model_config[\"out_size\"] = env.action_space.spaces[0].n\n\n    return model_factory(**model_config)",
  "def default_value_net_fn(env):\n    \"\"\"\n    Returns a default value network.\n    \"\"\"\n    if isinstance(env.observation_space, spaces.Box):\n        obs_shape = env.observation_space.shape\n    elif isinstance(env.observation_space, spaces.Tuple):\n        obs_shape = env.observation_space.spaces[0].shape\n    else:\n        raise ValueError(\"Incompatible observation space: {}\".format(env.observation_space))\n    # Assume CHW observation space\n    if len(obs_shape) == 3:\n        model_config = {\"type\": \"ConvolutionalNetwork\", \"in_channels\": int(obs_shape[0]),\n                        \"in_height\": int(obs_shape[1]),\n                        \"in_width\": int(obs_shape[2])}\n    elif len(obs_shape) == 2:\n        model_config = {\"type\": \"ConvolutionalNetwork\", \"in_channels\": int(1),\n                        \"in_height\": int(obs_shape[0]),\n                        \"in_width\": int(obs_shape[1])}\n    elif len(obs_shape) == 1:\n        model_config = {\"type\": \"MultiLayerPerceptron\", \"in_size\": int(obs_shape[0]),\n                        \"layer_sizes\": [64, 64]}\n    else:\n        raise ValueError(\"Incompatible observation shape: {}\".format(env.observation_space.shape))\n\n    model_config[\"out_size\"] = 1\n\n    return model_factory(**model_config)",
  "class Net(nn.Module):\n    def __init__(self, obs_size, hidden_size, n_actions):\n        super(Net, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(obs_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_actions)\n        )\n\n    def forward(self, x):\n        return self.net(x)",
  "class BaseModule(torch.nn.Module):\n    \"\"\"\n    Base torch.nn.Module implementing basic features:\n        - initialization factory\n        - normalization parameters\n    \"\"\"\n\n    def __init__(self, activation_type=\"RELU\", reset_type=\"XAVIER\"):\n        super().__init__()\n        self.activation = activation_factory(activation_type)\n        self.reset_type = reset_type\n\n    def _init_weights(self, m):\n        if hasattr(m, 'weight'):\n            if self.reset_type == \"XAVIER\":\n                torch.nn.init.xavier_uniform_(m.weight.data)\n            elif self.reset_type == \"ZEROS\":\n                torch.nn.init.constant_(m.weight.data, 0.)\n            else:\n                raise ValueError(\"Unknown reset type\")\n        if hasattr(m, 'bias') and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.)\n\n    def reset(self):\n        self.apply(self._init_weights)",
  "class Table(torch.nn.Module):\n    def __init__(self, state_size, action_size):\n        super().__init__()\n        self.policy = nn.Embedding.from_pretrained(torch.zeros(state_size, action_size), freeze=False)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        action_probs = self.softmax(self.action_scores(x))\n        return Categorical(action_probs)\n\n    def action_scores(self, x):\n        return self.policy(x.long())",
  "class MultiLayerPerceptron(BaseModule):\n    def __init__(self,\n                 in_size=None,\n                 layer_sizes=None,\n                 reshape=True,\n                 out_size=None,\n                 activation=\"RELU\",\n                 is_policy=False,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.reshape = reshape\n        self.layer_sizes = layer_sizes or [64, 64]\n        self.out_size = out_size\n        self.activation = activation_factory(activation)\n        self.is_policy = is_policy\n        self.softmax = nn.Softmax(dim=-1)\n        sizes = [in_size] + self.layer_sizes\n        layers_list = [nn.Linear(sizes[i], sizes[i + 1])\n                       for i in range(len(sizes) - 1)]\n        self.layers = nn.ModuleList(layers_list)\n        if out_size:\n            self.predict = nn.Linear(sizes[-1], out_size)\n\n    def forward(self, x):\n        if self.reshape:\n            x = x.reshape(x.shape[0], -1)  # We expect a batch of vectors\n        for layer in self.layers:\n            x = self.activation(layer(x.float()))\n        if self.out_size:\n            x = self.predict(x)\n        if self.is_policy:\n            action_probs = self.softmax(x)\n            dist = Categorical(action_probs)\n            return dist\n        return x\n\n    def action_scores(self, x):\n        if self.is_policy:\n            if self.reshape:\n                x = x.reshape(x.shape[0], -1)  # We expect a batch of vectors\n            for layer in self.layers:\n                x = self.activation(layer(x.float()))\n            if self.out_size:\n                action_scores = self.predict(x)\n            return action_scores",
  "class DuelingNetwork(BaseModule):\n    def __init__(self,\n                 in_size=None,\n                 base_module_kwargs=None,\n                 value_kwargs=None,\n                 advantage_kwargs=None,\n                 out_size=None):\n        super().__init__()\n        self.out_size = out_size\n        base_module_kwargs = base_module_kwargs or {}\n        base_module_kwargs[\"in_size\"] = in_size\n        self.base_module = model_factory(**base_module_kwargs)\n        value_kwargs = value_kwargs or {}\n        value_kwargs[\"in_size\"] = self.base_module.layer_sizes[-1]\n        value_kwargs[\"out_size\"] = 1\n        self.value = model_factory(**value_kwargs)\n        advantage_kwargs = advantage_kwargs or {}\n        advantage_kwargs[\"in_size\"] = self.base_module.layer_sizes[-1]\n        advantage_kwargs[\"out_size\"] = out_size\n        self.advantage = model_factory(**advantage_kwargs)\n\n    def forward(self, x):\n        x = self.base_module(x)\n        value = self.value(x).expand(-1, self.out_size)\n        advantage = self.advantage(x)\n        return value + advantage \\\n               - advantage.mean(1).unsqueeze(1).expand(-1, self.out_size)",
  "class ConvolutionalNetwork(nn.Module):\n    def __init__(self,\n                 activation=\"RELU\",\n                 in_channels=None,\n                 in_height=None,\n                 in_width=None,\n                 head_mlp_kwargs=None,\n                 out_size=None,\n                 is_policy=False,\n                 transpose_obs=False,\n                 **kwargs):\n        super().__init__()\n        self.activation = activation_factory(activation)\n        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=2, stride=2)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=2, stride=2)\n\n        # MLP Head\n        # Number of Linear input connections depends on output of conv2d layers\n        # and therefore the input image size, so compute it.\n        def conv2d_size_out(size, kernel_size=2, stride=2):\n            return (size - (kernel_size - 1) - 1) // stride + 1\n\n        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(in_width)))\n        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(in_height)))\n        assert convh > 0 and convw > 0\n        self.head_mlp_kwargs = head_mlp_kwargs or {}\n        self.head_mlp_kwargs[\"in_size\"] = convw * convh * 64\n        self.head_mlp_kwargs[\"out_size\"] = out_size\n        self.head_mlp_kwargs[\"is_policy\"] = is_policy\n        self.head = model_factory(**self.head_mlp_kwargs)\n\n        self.is_policy = is_policy\n        self.transpose_obs = transpose_obs\n\n    def convolutions(self, x):\n        x = x.float()\n        if len(x.shape) == 3:\n            x = x.unsqueeze(0)\n        if self.transpose_obs:\n            x = torch.transpose(x, -1, -3)\n        x = self.activation((self.conv1(x)))\n        x = self.activation((self.conv2(x)))\n        x = self.activation((self.conv3(x)))\n        return x\n\n    def forward(self, x):\n        \"\"\"\n            Forward convolutional network\n        :param x: tensor of shape BCHW\n        \"\"\"\n        return self.head(self.convolutions(x))\n\n    def action_scores(self, x):\n        return self.head.action_scores(self.convolutions(x))",
  "def __init__(self, obs_size, hidden_size, n_actions):\n        super(Net, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(obs_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_actions)\n        )",
  "def forward(self, x):\n        return self.net(x)",
  "def __init__(self, activation_type=\"RELU\", reset_type=\"XAVIER\"):\n        super().__init__()\n        self.activation = activation_factory(activation_type)\n        self.reset_type = reset_type",
  "def _init_weights(self, m):\n        if hasattr(m, 'weight'):\n            if self.reset_type == \"XAVIER\":\n                torch.nn.init.xavier_uniform_(m.weight.data)\n            elif self.reset_type == \"ZEROS\":\n                torch.nn.init.constant_(m.weight.data, 0.)\n            else:\n                raise ValueError(\"Unknown reset type\")\n        if hasattr(m, 'bias') and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.)",
  "def reset(self):\n        self.apply(self._init_weights)",
  "def __init__(self, state_size, action_size):\n        super().__init__()\n        self.policy = nn.Embedding.from_pretrained(torch.zeros(state_size, action_size), freeze=False)\n        self.softmax = nn.Softmax(dim=-1)",
  "def forward(self, x):\n        action_probs = self.softmax(self.action_scores(x))\n        return Categorical(action_probs)",
  "def action_scores(self, x):\n        return self.policy(x.long())",
  "def __init__(self,\n                 in_size=None,\n                 layer_sizes=None,\n                 reshape=True,\n                 out_size=None,\n                 activation=\"RELU\",\n                 is_policy=False,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.reshape = reshape\n        self.layer_sizes = layer_sizes or [64, 64]\n        self.out_size = out_size\n        self.activation = activation_factory(activation)\n        self.is_policy = is_policy\n        self.softmax = nn.Softmax(dim=-1)\n        sizes = [in_size] + self.layer_sizes\n        layers_list = [nn.Linear(sizes[i], sizes[i + 1])\n                       for i in range(len(sizes) - 1)]\n        self.layers = nn.ModuleList(layers_list)\n        if out_size:\n            self.predict = nn.Linear(sizes[-1], out_size)",
  "def forward(self, x):\n        if self.reshape:\n            x = x.reshape(x.shape[0], -1)  # We expect a batch of vectors\n        for layer in self.layers:\n            x = self.activation(layer(x.float()))\n        if self.out_size:\n            x = self.predict(x)\n        if self.is_policy:\n            action_probs = self.softmax(x)\n            dist = Categorical(action_probs)\n            return dist\n        return x",
  "def action_scores(self, x):\n        if self.is_policy:\n            if self.reshape:\n                x = x.reshape(x.shape[0], -1)  # We expect a batch of vectors\n            for layer in self.layers:\n                x = self.activation(layer(x.float()))\n            if self.out_size:\n                action_scores = self.predict(x)\n            return action_scores",
  "def __init__(self,\n                 in_size=None,\n                 base_module_kwargs=None,\n                 value_kwargs=None,\n                 advantage_kwargs=None,\n                 out_size=None):\n        super().__init__()\n        self.out_size = out_size\n        base_module_kwargs = base_module_kwargs or {}\n        base_module_kwargs[\"in_size\"] = in_size\n        self.base_module = model_factory(**base_module_kwargs)\n        value_kwargs = value_kwargs or {}\n        value_kwargs[\"in_size\"] = self.base_module.layer_sizes[-1]\n        value_kwargs[\"out_size\"] = 1\n        self.value = model_factory(**value_kwargs)\n        advantage_kwargs = advantage_kwargs or {}\n        advantage_kwargs[\"in_size\"] = self.base_module.layer_sizes[-1]\n        advantage_kwargs[\"out_size\"] = out_size\n        self.advantage = model_factory(**advantage_kwargs)",
  "def forward(self, x):\n        x = self.base_module(x)\n        value = self.value(x).expand(-1, self.out_size)\n        advantage = self.advantage(x)\n        return value + advantage \\\n               - advantage.mean(1).unsqueeze(1).expand(-1, self.out_size)",
  "def __init__(self,\n                 activation=\"RELU\",\n                 in_channels=None,\n                 in_height=None,\n                 in_width=None,\n                 head_mlp_kwargs=None,\n                 out_size=None,\n                 is_policy=False,\n                 transpose_obs=False,\n                 **kwargs):\n        super().__init__()\n        self.activation = activation_factory(activation)\n        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=2, stride=2)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=2, stride=2)\n\n        # MLP Head\n        # Number of Linear input connections depends on output of conv2d layers\n        # and therefore the input image size, so compute it.\n        def conv2d_size_out(size, kernel_size=2, stride=2):\n            return (size - (kernel_size - 1) - 1) // stride + 1\n\n        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(in_width)))\n        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(in_height)))\n        assert convh > 0 and convw > 0\n        self.head_mlp_kwargs = head_mlp_kwargs or {}\n        self.head_mlp_kwargs[\"in_size\"] = convw * convh * 64\n        self.head_mlp_kwargs[\"out_size\"] = out_size\n        self.head_mlp_kwargs[\"is_policy\"] = is_policy\n        self.head = model_factory(**self.head_mlp_kwargs)\n\n        self.is_policy = is_policy\n        self.transpose_obs = transpose_obs",
  "def convolutions(self, x):\n        x = x.float()\n        if len(x.shape) == 3:\n            x = x.unsqueeze(0)\n        if self.transpose_obs:\n            x = torch.transpose(x, -1, -3)\n        x = self.activation((self.conv1(x)))\n        x = self.activation((self.conv2(x)))\n        x = self.activation((self.conv3(x)))\n        return x",
  "def forward(self, x):\n        \"\"\"\n            Forward convolutional network\n        :param x: tensor of shape BCHW\n        \"\"\"\n        return self.head(self.convolutions(x))",
  "def action_scores(self, x):\n        return self.head.action_scores(self.convolutions(x))",
  "def conv2d_size_out(size, kernel_size=2, stride=2):\n            return (size - (kernel_size - 1) - 1) // stride + 1",
  "class ReplayMemory(object):\n    \"\"\"\n    Container that stores and samples transitions.\n    \"\"\"\n    def __init__(self,\n                 capacity=10000,\n                 **kwargs):\n        self.capacity = int(capacity)\n        self.memory = []\n        self.position = 0\n\n    def push(self, item):\n        \"\"\"Saves a thing.\"\"\"\n        if len(self.memory) < self.capacity:\n            self.memory.append(item)\n        else:\n            self.memory[self.position] = item\n        # Faster than append and pop\n        self.position = (self.position + 1) % self.capacity\n\n    def _encode_sample(self, idxes):\n        return [self.memory[idx] for idx in idxes]\n\n    def sample(self, batch_size):\n        batch_size = min(batch_size, len(self))\n        idxes = np.random.choice(len(self.memory), size=batch_size)\n        return self._encode_sample(idxes), idxes\n\n    def __len__(self):\n        return len(self.memory)\n\n    def is_full(self):\n        return len(self.memory) == self.capacity\n\n    def is_empty(self):\n        return len(self.memory) == 0",
  "class TransitionReplayMemory(ReplayMemory):\n    def push(self, *args):\n        super().push(Transition(*args))\n\n    def _encode_sample(self, idxes):\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n        for i in idxes:\n            data = self.memory[i]\n            state, action, reward, next_state, done, info = data\n            states.append(np.array(state, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            next_states.append(np.array(next_state, copy=False))\n            dones.append(done)\n        return Transition(np.array(states),\n                          np.array(actions),\n                          np.array(rewards),\n                          np.array(next_states),\n                          np.array(dones),\n                          {})",
  "class PrioritizedReplayMemory(TransitionReplayMemory):\n    \"\"\"Code from https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\"\"\"\n    def __init__(self,\n                 capacity=10000,\n                 alpha=0.5,\n                 beta=0.5,\n                 **kwargs):\n        \"\"\"Create Prioritized Replay buffer.\n        Parameters\n        ----------\n        alpha: float\n            how much prioritization is used\n            (0 - no prioritization, 1 - full prioritization)\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n        See Also\n        --------\n        ReplayMemory.__init__\n        \"\"\"\n        super().__init__(capacity, **kwargs)\n        assert alpha >= 0\n        self._alpha = alpha\n        self._beta = beta\n\n        it_capacity = 1\n        while it_capacity < self.capacity:\n            it_capacity *= 2\n\n        self._it_sum = SumSegmentTree(it_capacity)\n        self._it_min = MinSegmentTree(it_capacity)\n        self._max_priority = 1.0\n\n    def push(self, *args):\n        idx = self.position\n        super().push(*args)\n        self._it_sum[idx] = self._max_priority ** self._alpha\n        self._it_min[idx] = self._max_priority ** self._alpha\n\n    def _sample_proportional(self, batch_size):\n        res = []\n        p_total = self._it_sum.sum(0, len(self.memory) - 1)\n        every_range_len = p_total / batch_size\n        for i in range(batch_size):\n            mass = np.random.random() * every_range_len + i * every_range_len\n            idx = self._it_sum.find_prefixsum_idx(mass)\n            res.append(idx)\n        return np.array(res)\n\n    def sample(self, batch_size):\n        \"\"\"Sample a batch of experiences.\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        weights: np.array\n            Array of shape (batch_size,) and dtype np.float32\n            denoting importance weight of each sampled transition\n        idxes: np.array\n            Array of shape (batch_size,) and dtype np.int32\n            idexes in buffer of sampled experiences\n        \"\"\"\n        assert self._beta > 0\n\n        batch_size = min(batch_size, len(self))\n        idxes = self._sample_proportional(batch_size)\n\n        weights = []\n        p_min = self._it_min.min() / self._it_sum.sum()\n        max_weight = (p_min * len(self.memory)) ** (-self._beta)\n\n        for idx in idxes:\n            p_sample = self._it_sum[idx] / self._it_sum.sum()\n            weight = (p_sample * len(self.memory)) ** (-self._beta)\n            weights.append(weight / max_weight)\n        weights = np.array(weights)\n        return self._encode_sample(idxes), weights, idxes\n\n    def update_priorities(self, idxes, priorities):\n        \"\"\"Update priorities of sampled transitions.\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`.\n        \"\"\"\n        assert np.size(idxes) == np.size(priorities)\n        for idx, priority in zip(idxes, priorities):\n            assert priority > 0\n            assert 0 <= idx < len(self.memory)\n            self._it_sum[idx] = priority ** self._alpha\n            self._it_min[idx] = priority ** self._alpha\n\n            self._max_priority = max(self._max_priority, priority)",
  "class Memory:\n    def __init__(self):\n        self.actions = []\n        self.states = []\n        self.logprobs = []\n        self.rewards = []\n        self.is_terminals = []\n\n    def clear_memory(self):\n        del self.actions[:]\n        del self.states[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.is_terminals[:]",
  "class CEMMemory:\n    def __init__(self, max_size):\n        self.max_size = max_size\n        self.clear()\n\n    def clear(self):\n        self.size = 0\n        self.states = []\n        self.actions = []\n        self.rewards = []\n\n    def append(self, state, action, reward):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.size += 1\n        if self.size == self.max_size+1:\n            self.states.pop(0)\n            self.actions.pop(0)\n            self.rewards.pop(0)\n            self.size = self.max_size",
  "class SegmentTree(object):\n    def __init__(self, capacity, operation, neutral_element):\n        \"\"\"Build a Segment Tree data structure.\n\n        https://en.wikipedia.org/wiki/Segment_tree\n\n        Can be used as regular array, but with two\n        important differences:\n\n            a) setting item's value is slightly slower.\n               It is O(lg capacity) instead of O(1).\n            b) user has access to an efficient ( O(log segment size) )\n               `reduce` operation which reduces `operation` over\n               a contiguous subsequence of items in the array.\n\n        Paramters\n        ---------\n        capacity: int\n            Total size of the array - must be a power of two.\n        operation: lambda obj, obj -> obj\n            and operation for combining elements (eg. sum, max)\n            must form a mathematical group together with the set of\n            possible values for array elements (i.e. be associative)\n        neutral_element: obj\n            neutral element for the operation above. eg. float('-inf')\n            for max and 0 for sum.\n        \"\"\"\n        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n        self._capacity = capacity\n        self._value = [neutral_element for _ in range(2 * capacity)]\n        self._operation = operation\n\n    def _reduce_helper(self, start, end, node, node_start, node_end):\n        if start == node_start and end == node_end:\n            return self._value[node]\n        mid = (node_start + node_end) // 2\n        if end <= mid:\n            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n        else:\n            if mid + 1 <= start:\n                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n            else:\n                return self._operation(\n                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n                )\n\n    def reduce(self, start=0, end=None):\n        \"\"\"Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n\n            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n\n        Parameters\n        ----------\n        start: int\n            beginning of the subsequence\n        end: int\n            end of the subsequences\n\n        Returns\n        -------\n        reduced: obj\n            result of reducing self.operation over the specified range of array elements.\n        \"\"\"\n        if end is None:\n            end = self._capacity\n        if end < 0:\n            end += self._capacity\n        end -= 1\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n\n    def __setitem__(self, idx, val):\n        # index of the leaf\n        idx += self._capacity\n        self._value[idx] = val\n        idx //= 2\n        while idx >= 1:\n            self._value[idx] = self._operation(\n                self._value[2 * idx],\n                self._value[2 * idx + 1]\n            )\n            idx //= 2\n\n    def __getitem__(self, idx):\n        assert 0 <= idx < self._capacity\n        return self._value[self._capacity + idx]",
  "class SumSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super(SumSegmentTree, self).__init__(\n            capacity=capacity,\n            operation=operator.add,\n            neutral_element=0.0\n        )\n\n    def sum(self, start=0, end=None):\n        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n        return super(SumSegmentTree, self).reduce(start, end)\n\n    def find_prefixsum_idx(self, prefixsum):\n        \"\"\"Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n\n        Parameters\n        ----------\n        perfixsum: float\n            upperbound on the sum of array prefix\n\n        Returns\n        -------\n        idx: int\n            highest index satisfying the prefixsum constraint\n        \"\"\"\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        idx = 1\n        while idx < self._capacity:  # while non-leaf\n            if self._value[2 * idx] > prefixsum:\n                idx = 2 * idx\n            else:\n                prefixsum -= self._value[2 * idx]\n                idx = 2 * idx + 1\n        return idx - self._capacity",
  "class MinSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super(MinSegmentTree, self).__init__(\n            capacity=capacity,\n            operation=min,\n            neutral_element=float('inf')\n        )\n\n    def min(self, start=0, end=None):\n        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n\n        return super(MinSegmentTree, self).reduce(start, end)",
  "def __init__(self,\n                 capacity=10000,\n                 **kwargs):\n        self.capacity = int(capacity)\n        self.memory = []\n        self.position = 0",
  "def push(self, item):\n        \"\"\"Saves a thing.\"\"\"\n        if len(self.memory) < self.capacity:\n            self.memory.append(item)\n        else:\n            self.memory[self.position] = item\n        # Faster than append and pop\n        self.position = (self.position + 1) % self.capacity",
  "def _encode_sample(self, idxes):\n        return [self.memory[idx] for idx in idxes]",
  "def sample(self, batch_size):\n        batch_size = min(batch_size, len(self))\n        idxes = np.random.choice(len(self.memory), size=batch_size)\n        return self._encode_sample(idxes), idxes",
  "def __len__(self):\n        return len(self.memory)",
  "def is_full(self):\n        return len(self.memory) == self.capacity",
  "def is_empty(self):\n        return len(self.memory) == 0",
  "def push(self, *args):\n        super().push(Transition(*args))",
  "def _encode_sample(self, idxes):\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n        for i in idxes:\n            data = self.memory[i]\n            state, action, reward, next_state, done, info = data\n            states.append(np.array(state, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            next_states.append(np.array(next_state, copy=False))\n            dones.append(done)\n        return Transition(np.array(states),\n                          np.array(actions),\n                          np.array(rewards),\n                          np.array(next_states),\n                          np.array(dones),\n                          {})",
  "def __init__(self,\n                 capacity=10000,\n                 alpha=0.5,\n                 beta=0.5,\n                 **kwargs):\n        \"\"\"Create Prioritized Replay buffer.\n        Parameters\n        ----------\n        alpha: float\n            how much prioritization is used\n            (0 - no prioritization, 1 - full prioritization)\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n        See Also\n        --------\n        ReplayMemory.__init__\n        \"\"\"\n        super().__init__(capacity, **kwargs)\n        assert alpha >= 0\n        self._alpha = alpha\n        self._beta = beta\n\n        it_capacity = 1\n        while it_capacity < self.capacity:\n            it_capacity *= 2\n\n        self._it_sum = SumSegmentTree(it_capacity)\n        self._it_min = MinSegmentTree(it_capacity)\n        self._max_priority = 1.0",
  "def push(self, *args):\n        idx = self.position\n        super().push(*args)\n        self._it_sum[idx] = self._max_priority ** self._alpha\n        self._it_min[idx] = self._max_priority ** self._alpha",
  "def _sample_proportional(self, batch_size):\n        res = []\n        p_total = self._it_sum.sum(0, len(self.memory) - 1)\n        every_range_len = p_total / batch_size\n        for i in range(batch_size):\n            mass = np.random.random() * every_range_len + i * every_range_len\n            idx = self._it_sum.find_prefixsum_idx(mass)\n            res.append(idx)\n        return np.array(res)",
  "def sample(self, batch_size):\n        \"\"\"Sample a batch of experiences.\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        weights: np.array\n            Array of shape (batch_size,) and dtype np.float32\n            denoting importance weight of each sampled transition\n        idxes: np.array\n            Array of shape (batch_size,) and dtype np.int32\n            idexes in buffer of sampled experiences\n        \"\"\"\n        assert self._beta > 0\n\n        batch_size = min(batch_size, len(self))\n        idxes = self._sample_proportional(batch_size)\n\n        weights = []\n        p_min = self._it_min.min() / self._it_sum.sum()\n        max_weight = (p_min * len(self.memory)) ** (-self._beta)\n\n        for idx in idxes:\n            p_sample = self._it_sum[idx] / self._it_sum.sum()\n            weight = (p_sample * len(self.memory)) ** (-self._beta)\n            weights.append(weight / max_weight)\n        weights = np.array(weights)\n        return self._encode_sample(idxes), weights, idxes",
  "def update_priorities(self, idxes, priorities):\n        \"\"\"Update priorities of sampled transitions.\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`.\n        \"\"\"\n        assert np.size(idxes) == np.size(priorities)\n        for idx, priority in zip(idxes, priorities):\n            assert priority > 0\n            assert 0 <= idx < len(self.memory)\n            self._it_sum[idx] = priority ** self._alpha\n            self._it_min[idx] = priority ** self._alpha\n\n            self._max_priority = max(self._max_priority, priority)",
  "def __init__(self):\n        self.actions = []\n        self.states = []\n        self.logprobs = []\n        self.rewards = []\n        self.is_terminals = []",
  "def clear_memory(self):\n        del self.actions[:]\n        del self.states[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.is_terminals[:]",
  "def __init__(self, max_size):\n        self.max_size = max_size\n        self.clear()",
  "def clear(self):\n        self.size = 0\n        self.states = []\n        self.actions = []\n        self.rewards = []",
  "def append(self, state, action, reward):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.size += 1\n        if self.size == self.max_size+1:\n            self.states.pop(0)\n            self.actions.pop(0)\n            self.rewards.pop(0)\n            self.size = self.max_size",
  "def __init__(self, capacity, operation, neutral_element):\n        \"\"\"Build a Segment Tree data structure.\n\n        https://en.wikipedia.org/wiki/Segment_tree\n\n        Can be used as regular array, but with two\n        important differences:\n\n            a) setting item's value is slightly slower.\n               It is O(lg capacity) instead of O(1).\n            b) user has access to an efficient ( O(log segment size) )\n               `reduce` operation which reduces `operation` over\n               a contiguous subsequence of items in the array.\n\n        Paramters\n        ---------\n        capacity: int\n            Total size of the array - must be a power of two.\n        operation: lambda obj, obj -> obj\n            and operation for combining elements (eg. sum, max)\n            must form a mathematical group together with the set of\n            possible values for array elements (i.e. be associative)\n        neutral_element: obj\n            neutral element for the operation above. eg. float('-inf')\n            for max and 0 for sum.\n        \"\"\"\n        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n        self._capacity = capacity\n        self._value = [neutral_element for _ in range(2 * capacity)]\n        self._operation = operation",
  "def _reduce_helper(self, start, end, node, node_start, node_end):\n        if start == node_start and end == node_end:\n            return self._value[node]\n        mid = (node_start + node_end) // 2\n        if end <= mid:\n            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n        else:\n            if mid + 1 <= start:\n                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n            else:\n                return self._operation(\n                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n                )",
  "def reduce(self, start=0, end=None):\n        \"\"\"Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n\n            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n\n        Parameters\n        ----------\n        start: int\n            beginning of the subsequence\n        end: int\n            end of the subsequences\n\n        Returns\n        -------\n        reduced: obj\n            result of reducing self.operation over the specified range of array elements.\n        \"\"\"\n        if end is None:\n            end = self._capacity\n        if end < 0:\n            end += self._capacity\n        end -= 1\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)",
  "def __setitem__(self, idx, val):\n        # index of the leaf\n        idx += self._capacity\n        self._value[idx] = val\n        idx //= 2\n        while idx >= 1:\n            self._value[idx] = self._operation(\n                self._value[2 * idx],\n                self._value[2 * idx + 1]\n            )\n            idx //= 2",
  "def __getitem__(self, idx):\n        assert 0 <= idx < self._capacity\n        return self._value[self._capacity + idx]",
  "def __init__(self, capacity):\n        super(SumSegmentTree, self).__init__(\n            capacity=capacity,\n            operation=operator.add,\n            neutral_element=0.0\n        )",
  "def sum(self, start=0, end=None):\n        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n        return super(SumSegmentTree, self).reduce(start, end)",
  "def find_prefixsum_idx(self, prefixsum):\n        \"\"\"Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n\n        Parameters\n        ----------\n        perfixsum: float\n            upperbound on the sum of array prefix\n\n        Returns\n        -------\n        idx: int\n            highest index satisfying the prefixsum constraint\n        \"\"\"\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        idx = 1\n        while idx < self._capacity:  # while non-leaf\n            if self._value[2 * idx] > prefixsum:\n                idx = 2 * idx\n            else:\n                prefixsum -= self._value[2 * idx]\n                idx = 2 * idx + 1\n        return idx - self._capacity",
  "def __init__(self, capacity):\n        super(MinSegmentTree, self).__init__(\n            capacity=capacity,\n            operation=min,\n            neutral_element=float('inf')\n        )",
  "def min(self, start=0, end=None):\n        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n\n        return super(MinSegmentTree, self).reduce(start, end)",
  "def loss_function_factory(loss_function):\n    if loss_function == \"l2\":\n        return F.mse_loss\n    elif loss_function == \"l1\":\n        return F.l1_loss\n    elif loss_function == \"smooth_l1\":\n        return F.smooth_l1_loss\n    elif loss_function == \"bce\":\n        return F.binary_cross_entropy\n    else:\n        raise ValueError(\"Unknown loss function : {}\".format(loss_function))",
  "def optimizer_factory(params, optimizer_type=\"ADAM\", **kwargs):\n    if optimizer_type == \"ADAM\":\n        return torch.optim.Adam(params=params, **kwargs)\n    elif optimizer_type == \"RMS_PROP\":\n        return torch.optim.RMSprop(params=params, **kwargs)\n    else:\n        raise ValueError(\"Unknown optimizer type: {}\".format(optimizer_type))",
  "def model_factory(type=\"MultiLayerPerceptron\", **kwargs) -> nn.Module:\n    from rlberry.agents.utils.torch_attention_models import EgoAttentionNetwork\n    from rlberry.agents.utils.torch_models import MultiLayerPerceptron, DuelingNetwork, ConvolutionalNetwork, \\\n        Table\n    if type == \"MultiLayerPerceptron\":\n        return MultiLayerPerceptron(**kwargs)\n    elif type == \"DuelingNetwork\":\n        return DuelingNetwork(**kwargs)\n    elif type == \"ConvolutionalNetwork\":\n        return ConvolutionalNetwork(**kwargs)\n    elif type == \"EgoAttentionNetwork\":\n        return EgoAttentionNetwork(**kwargs)\n    elif type == \"Table\":\n        return Table(**kwargs)\n    else:\n        raise ValueError(\"Unknown model type\")",
  "def model_factory_from_env(env, **kwargs):\n    kwargs = size_model_config(env, **kwargs)\n    return model_factory(**kwargs)",
  "def size_model_config(env,\n                      **model_config):\n    \"\"\"\n    Update the configuration of a model depending on the environment\n    observation/action spaces.\n\n    Typically, the input/output sizes.\n\n    Parameters\n    ----------\n    env : gym.Env\n        An environment.\n    model_config : dict\n        A model configuration.\n    \"\"\"\n\n    if isinstance(env.observation_space, spaces.Box):\n        obs_shape = env.observation_space.shape\n    elif isinstance(env.observation_space, spaces.Tuple):\n        obs_shape = env.observation_space.spaces[0].shape\n    elif isinstance(env.observation_space, spaces.Discrete):\n        return model_config\n\n    # Assume CHW observation space\n    if model_config[\"type\"] == \"ConvolutionalNetwork\":\n        model_config[\"in_channels\"] = int(obs_shape[0])\n        model_config[\"in_height\"] = int(obs_shape[1])\n        model_config[\"in_width\"] = int(obs_shape[2])\n    else:\n        model_config[\"in_size\"] = int(np.prod(obs_shape))\n\n    if isinstance(env.action_space, spaces.Discrete):\n        model_config[\"out_size\"] = env.action_space.n\n    elif isinstance(env.action_space, spaces.Tuple):\n        model_config[\"out_size\"] = env.action_space.spaces[0].n\n    return model_config",
  "def activation_factory(activation_type):\n    if activation_type == \"RELU\":\n        return F.relu\n    elif activation_type == \"TANH\":\n        return torch.tanh\n    elif activation_type == \"ELU\":\n        return nn.ELU()\n    else:\n        raise ValueError(\"Unknown activation_type: {}\".format(activation_type))",
  "def trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)",
  "class EgoAttention(BaseModule):\n    def __init__(self,\n                 feature_size=64,\n                 heads=4,\n                 dropout_factor=0):\n        super().__init__()\n        self.feature_size = feature_size\n        self.heads = heads\n        self.dropout_factor = dropout_factor\n        self.features_per_head = int(self.feature_size / self.heads)\n\n        self.value_all = nn.Linear(self.feature_size,\n                                   self.feature_size,\n                                   bias=False)\n        self.key_all = nn.Linear(self.feature_size,\n                                 self.feature_size,\n                                 bias=False)\n        self.query_ego = nn.Linear(self.feature_size,\n                                   self.feature_size,\n                                   bias=False)\n        self.attention_combine = nn.Linear(self.feature_size,\n                                           self.feature_size,\n                                           bias=False)\n\n    @classmethod\n    def default_config(cls):\n        return {\n        }\n\n    def forward(self, ego, others, mask=None):\n        batch_size = others.shape[0]\n        n_entities = others.shape[1] + 1\n        input_all = torch.cat((ego.view(batch_size, 1,\n                               self.feature_size), others), dim=1)\n        # Dimensions: Batch, entity, head, feature_per_head\n        key_all = self.key_all(input_all).view(batch_size,\n                                               n_entities,\n                                               self.heads,\n                                               self.features_per_head)\n        value_all = self.value_all(input_all).view(batch_size,\n                                                   n_entities,\n                                                   self.heads,\n                                                   self.features_per_head)\n        query_ego = self.query_ego(ego).view(batch_size, 1,\n                                             self.heads,\n                                             self.features_per_head)\n\n        # Dimensions: Batch, head, entity, feature_per_head\n        key_all = key_all.permute(0, 2, 1, 3)\n        value_all = value_all.permute(0, 2, 1, 3)\n        query_ego = query_ego.permute(0, 2, 1, 3)\n        if mask is not None:\n            mask = mask.view((batch_size, 1, 1,\n                              n_entities)).repeat((1, self.heads, 1, 1))\n        value, attention_matrix = attention(query_ego,\n                                            key_all,\n                                            value_all,\n                                            mask,\n                                            nn.Dropout(self.dropout_factor))\n        result = (self.attention_combine(\n                    value.reshape((batch_size,\n                                   self.feature_size))) + ego.squeeze(1))/2\n        return result, attention_matrix",
  "class SelfAttention(BaseModule):\n    def __init__(self,\n                 feature_size=64,\n                 heads=4,\n                 dropout_factor=0,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.feature_size = feature_size\n        self.heads = heads\n        self.dropout_factor = dropout_factor\n        self.features_per_head = int(self.feature_size / self.heads)\n\n        self.value_all = nn.Linear(self.feature_size,\n                                   self.feature_size,\n                                   bias=False)\n        self.key_all = nn.Linear(self.feature_size,\n                                 self.feature_size,\n                                 bias=False)\n        self.query_all = nn.Linear(self.feature_size,\n                                   self.feature_size,\n                                   bias=False)\n        self.attention_combine = nn.Linear(self.feature_size,\n                                           self.feature_size,\n                                           bias=False)\n\n    def forward(self, ego, others, mask=None):\n        batch_size = others.shape[0]\n        n_entities = others.shape[1] + 1\n        input_all = torch.cat((ego.view(batch_size, 1,\n                                        self.feature_size),\n                               others), dim=1)\n        # Dimensions: Batch, entity, head, feature_per_head\n        key_all = self.key_all(input_all).view(batch_size, n_entities,\n                                               self.heads,\n                                               self.features_per_head)\n        value_all = self.value_all(input_all).view(batch_size, n_entities,\n                                                   self.heads,\n                                                   self.features_per_head)\n        query_all = self.query_all(input_all).view(batch_size,\n                                                   n_entities,\n                                                   self.heads,\n                                                   self.features_per_head)\n\n        # Dimensions: Batch, head, entity, feature_per_head\n        key_all = key_all.permute(0, 2, 1, 3)\n        value_all = value_all.permute(0, 2, 1, 3)\n        query_all = query_all.permute(0, 2, 1, 3)\n        if mask is not None:\n            mask = mask.view((batch_size, 1, 1,\n                              n_entities)).repeat((1, self.heads, 1, 1))\n        value, attention_matrix = attention(query_all, key_all, value_all,\n                                            mask,\n                                            nn.Dropout(self.dropout_factor))\n        result = (self.attention_combine(\n            value.reshape((batch_size, n_entities, self.feature_size)))\n            + input_all)/2\n        return result, attention_matrix",
  "class EgoAttentionNetwork(BaseModule):\n    def __init__(self,\n                 in_size=None,\n                 out_size=None,\n                 presence_feature_idx=0,\n                 embedding_layer_kwargs=None,\n                 attention_layer_kwargs=None,\n                 output_layer_kwargs=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.out_size = out_size\n        self.presence_feature_idx = presence_feature_idx\n        embedding_layer_kwargs = embedding_layer_kwargs or {}\n        if not embedding_layer_kwargs.get(\"in_size\", None):\n            embedding_layer_kwargs[\"in_size\"] = in_size\n        self.ego_embedding = model_factory(**embedding_layer_kwargs)\n        self.embedding = model_factory(**embedding_layer_kwargs)\n\n        attention_layer_kwargs = attention_layer_kwargs or {}\n        self.attention_layer = EgoAttention(**attention_layer_kwargs)\n\n        output_layer_kwargs = output_layer_kwargs or {}\n        output_layer_kwargs[\"in_size\"] = self.attention_layer.feature_size\n        output_layer_kwargs[\"out_size\"] = self.out_size\n        self.output_layer = model_factory(**output_layer_kwargs)\n\n    def forward(self, x):\n        ego_embedded_att, _ = self.forward_attention(x)\n        return self.output_layer(ego_embedded_att)\n\n    def split_input(self, x, mask=None):\n        # Dims: batch, entities, features\n        if len(x.shape) == 2:\n            x = x.unsqueeze(axis=0)\n        ego = x[:, 0:1, :]\n        others = x[:, 1:, :]\n        if mask is None:\n            aux = self.presence_feature_idx\n            mask = x[:, :, aux:aux + 1] < 0.5\n        return ego, others, mask\n\n    def forward_attention(self, x):\n        ego, others, mask = self.split_input(x)\n        ego = self.ego_embedding(ego)\n        others = self.embedding(others)\n        return self.attention_layer(ego, others, mask)\n\n    def get_attention_matrix(self, x):\n        _, attention_matrix = self.forward_attention(x)\n        return attention_matrix\n\n    def action_scores(self, x):\n        ego_embedded_att, _ = self.forward_attention(x)\n        return self.output_layer.action_scores(ego_embedded_att)",
  "def attention(query, key, value, mask=None, dropout=None):\n    \"\"\"\n    Compute a Scaled Dot Product Attention.\n\n    Parameters\n    ----------\n    query\n        size: batch, head, 1 (ego-entity), features\n    key\n        size: batch, head, entities, features\n    value\n        size: batch, head, entities, features\n    mask\n        size: batch,  head, 1 (absence feature), 1 (ego-entity)\n    dropout\n\n    Returns\n    -------\n    The attention softmax(QK^T/sqrt(dk))V\n    \"\"\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask, -1e9)\n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    output = torch.matmul(p_attn, value)\n    return output, p_attn",
  "def __init__(self,\n                 feature_size=64,\n                 heads=4,\n                 dropout_factor=0):\n        super().__init__()\n        self.feature_size = feature_size\n        self.heads = heads\n        self.dropout_factor = dropout_factor\n        self.features_per_head = int(self.feature_size / self.heads)\n\n        self.value_all = nn.Linear(self.feature_size,\n                                   self.feature_size,\n                                   bias=False)\n        self.key_all = nn.Linear(self.feature_size,\n                                 self.feature_size,\n                                 bias=False)\n        self.query_ego = nn.Linear(self.feature_size,\n                                   self.feature_size,\n                                   bias=False)\n        self.attention_combine = nn.Linear(self.feature_size,\n                                           self.feature_size,\n                                           bias=False)",
  "def default_config(cls):\n        return {\n        }",
  "def forward(self, ego, others, mask=None):\n        batch_size = others.shape[0]\n        n_entities = others.shape[1] + 1\n        input_all = torch.cat((ego.view(batch_size, 1,\n                               self.feature_size), others), dim=1)\n        # Dimensions: Batch, entity, head, feature_per_head\n        key_all = self.key_all(input_all).view(batch_size,\n                                               n_entities,\n                                               self.heads,\n                                               self.features_per_head)\n        value_all = self.value_all(input_all).view(batch_size,\n                                                   n_entities,\n                                                   self.heads,\n                                                   self.features_per_head)\n        query_ego = self.query_ego(ego).view(batch_size, 1,\n                                             self.heads,\n                                             self.features_per_head)\n\n        # Dimensions: Batch, head, entity, feature_per_head\n        key_all = key_all.permute(0, 2, 1, 3)\n        value_all = value_all.permute(0, 2, 1, 3)\n        query_ego = query_ego.permute(0, 2, 1, 3)\n        if mask is not None:\n            mask = mask.view((batch_size, 1, 1,\n                              n_entities)).repeat((1, self.heads, 1, 1))\n        value, attention_matrix = attention(query_ego,\n                                            key_all,\n                                            value_all,\n                                            mask,\n                                            nn.Dropout(self.dropout_factor))\n        result = (self.attention_combine(\n                    value.reshape((batch_size,\n                                   self.feature_size))) + ego.squeeze(1))/2\n        return result, attention_matrix",
  "def __init__(self,\n                 feature_size=64,\n                 heads=4,\n                 dropout_factor=0,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.feature_size = feature_size\n        self.heads = heads\n        self.dropout_factor = dropout_factor\n        self.features_per_head = int(self.feature_size / self.heads)\n\n        self.value_all = nn.Linear(self.feature_size,\n                                   self.feature_size,\n                                   bias=False)\n        self.key_all = nn.Linear(self.feature_size,\n                                 self.feature_size,\n                                 bias=False)\n        self.query_all = nn.Linear(self.feature_size,\n                                   self.feature_size,\n                                   bias=False)\n        self.attention_combine = nn.Linear(self.feature_size,\n                                           self.feature_size,\n                                           bias=False)",
  "def forward(self, ego, others, mask=None):\n        batch_size = others.shape[0]\n        n_entities = others.shape[1] + 1\n        input_all = torch.cat((ego.view(batch_size, 1,\n                                        self.feature_size),\n                               others), dim=1)\n        # Dimensions: Batch, entity, head, feature_per_head\n        key_all = self.key_all(input_all).view(batch_size, n_entities,\n                                               self.heads,\n                                               self.features_per_head)\n        value_all = self.value_all(input_all).view(batch_size, n_entities,\n                                                   self.heads,\n                                                   self.features_per_head)\n        query_all = self.query_all(input_all).view(batch_size,\n                                                   n_entities,\n                                                   self.heads,\n                                                   self.features_per_head)\n\n        # Dimensions: Batch, head, entity, feature_per_head\n        key_all = key_all.permute(0, 2, 1, 3)\n        value_all = value_all.permute(0, 2, 1, 3)\n        query_all = query_all.permute(0, 2, 1, 3)\n        if mask is not None:\n            mask = mask.view((batch_size, 1, 1,\n                              n_entities)).repeat((1, self.heads, 1, 1))\n        value, attention_matrix = attention(query_all, key_all, value_all,\n                                            mask,\n                                            nn.Dropout(self.dropout_factor))\n        result = (self.attention_combine(\n            value.reshape((batch_size, n_entities, self.feature_size)))\n            + input_all)/2\n        return result, attention_matrix",
  "def __init__(self,\n                 in_size=None,\n                 out_size=None,\n                 presence_feature_idx=0,\n                 embedding_layer_kwargs=None,\n                 attention_layer_kwargs=None,\n                 output_layer_kwargs=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.out_size = out_size\n        self.presence_feature_idx = presence_feature_idx\n        embedding_layer_kwargs = embedding_layer_kwargs or {}\n        if not embedding_layer_kwargs.get(\"in_size\", None):\n            embedding_layer_kwargs[\"in_size\"] = in_size\n        self.ego_embedding = model_factory(**embedding_layer_kwargs)\n        self.embedding = model_factory(**embedding_layer_kwargs)\n\n        attention_layer_kwargs = attention_layer_kwargs or {}\n        self.attention_layer = EgoAttention(**attention_layer_kwargs)\n\n        output_layer_kwargs = output_layer_kwargs or {}\n        output_layer_kwargs[\"in_size\"] = self.attention_layer.feature_size\n        output_layer_kwargs[\"out_size\"] = self.out_size\n        self.output_layer = model_factory(**output_layer_kwargs)",
  "def forward(self, x):\n        ego_embedded_att, _ = self.forward_attention(x)\n        return self.output_layer(ego_embedded_att)",
  "def split_input(self, x, mask=None):\n        # Dims: batch, entities, features\n        if len(x.shape) == 2:\n            x = x.unsqueeze(axis=0)\n        ego = x[:, 0:1, :]\n        others = x[:, 1:, :]\n        if mask is None:\n            aux = self.presence_feature_idx\n            mask = x[:, :, aux:aux + 1] < 0.5\n        return ego, others, mask",
  "def forward_attention(self, x):\n        ego, others, mask = self.split_input(x)\n        ego = self.ego_embedding(ego)\n        others = self.embedding(others)\n        return self.attention_layer(ego, others, mask)",
  "def get_attention_matrix(self, x):\n        _, attention_matrix = self.forward_attention(x)\n        return attention_matrix",
  "def action_scores(self, x):\n        ego_embedded_att, _ = self.forward_attention(x)\n        return self.output_layer.action_scores(ego_embedded_att)",
  "def update_value_and_get_action(state,\n                                hh,\n                                V,\n                                R_hat,\n                                P_hat,\n                                B_sa,\n                                gamma,\n                                v_max):\n    \"\"\"\n    state : int\n    hh : int\n    V : np.ndarray\n        shape (H, S)\n    R_hat : np.ndarray\n        shape (S, A)\n    P_hat : np.ndarray\n        shape (S, A, S)\n    B_sa : np.ndarray\n        shape (S, A)\n    gamma : double\n    v_max : np.ndarray\n        shape (H,)\n    \"\"\"\n    H = V.shape[0]\n    S, A = R_hat.shape[-2:]\n    best_action = 0\n    max_val = 0\n    previous_value = V[hh, state]\n\n    for aa in range(A):\n        q_aa = R_hat[state, aa] + B_sa[state, aa]\n\n        if hh < H-1:\n            for sn in range(S):\n                q_aa += gamma*P_hat[state, aa, sn]*V[hh+1, sn]\n\n        if aa == 0 or q_aa > max_val:\n            max_val = q_aa\n            best_action = aa\n\n    V[hh, state] = max_val\n    V[hh, state] = min(v_max[hh], V[hh, state])\n    V[hh, state] = min(previous_value, V[hh, state])\n\n    return best_action",
  "def update_value_and_get_action_sd(state,\n                                   hh,\n                                   V,\n                                   R_hat,\n                                   P_hat,\n                                   B_sa,\n                                   gamma,\n                                   v_max):\n    \"\"\"\n    state : int\n    hh : int\n    V : np.ndarray\n        shape (H, S)\n    R_hat : np.ndarray\n        shape (H, S, A)\n    P_hat : np.ndarray\n        shape (H, S, A, S)\n    B_sa : np.ndarray\n        shape (S, A)\n    gamma : double\n    v_max : np.ndarray\n        shape (H,)\n    \"\"\"\n    H = V.shape[0]\n    S, A = R_hat.shape[-2:]\n    best_action = 0\n    max_val = 0\n    previous_value = V[hh, state]\n\n    for aa in range(A):\n        q_aa = R_hat[hh, state, aa] + B_sa[hh, state, aa]\n\n        if hh < H-1:\n            for sn in range(S):\n                q_aa += gamma*P_hat[hh, state, aa, sn]*V[hh+1, sn]\n\n        if aa == 0 or q_aa > max_val:\n            max_val = q_aa\n            best_action = aa\n\n    V[hh, state] = max_val\n    V[hh, state] = min(v_max[hh], V[hh, state])\n    V[hh, state] = min(previous_value, V[hh, state])\n\n    return best_action",
  "class UCBVIAgent(IncrementalAgent):\n    \"\"\"\n    UCBVI [1]_ with custom exploration bonus.\n\n    Notes\n    -----\n    The recommended policy after all the episodes is computed without\n    exploration bonuses.\n\n    Parameters\n    ----------\n    env : gym.Env\n        Environment with discrete states and actions.\n    n_episodes : int\n        Number of episodes.\n    gamma : double, default: 1.0\n        Discount factor in [0, 1]. If gamma is 1.0, the problem is set to\n        be finite-horizon.\n    horizon : int\n        Horizon of the objective function. If None and gamma<1, set to\n        1/(1-gamma).\n    bonus_scale_factor : double, default: 1.0\n        Constant by which to multiply the exploration bonus, controls\n        the level of exploration.\n    bonus_type : {\"simplified_bernstein\"}\n        Type of exploration bonus. Currently, only \"simplified_bernstein\"\n        is implemented. If `reward_free` is true, this parameter is ignored\n        and the algorithm uses 1/n bonuses.\n    reward_free : bool, default: False\n        If true, ignores rewards and uses only 1/n bonuses.\n    stage_dependent : bool, default: False\n        If true, assume that transitions and rewards can change with the stage h.\n    real_time_dp : bool, default: False\n        If true, uses real-time dynamic programming [2]_ instead of full backward induction\n        for the sampling policy.\n\n    References\n    ----------\n    .. [1] Azar et al., 2017\n        Minimax Regret Bounds for Reinforcement Learning\n        https://arxiv.org/abs/1703.05449\n\n    .. [2] Efroni, Yonathan, et al.\n          Tight regret bounds for model-based reinforcement learning with greedy policies.\n          Advances in Neural Information Processing Systems. 2019.\n          https://papers.nips.cc/paper/2019/file/25caef3a545a1fff2ff4055484f0e758-Paper.pdf\n    \"\"\"\n    name = \"UCBVI\"\n\n    def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 gamma=1.0,\n                 horizon=100,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 reward_free=False,\n                 stage_dependent=False,\n                 real_time_dp=False,\n                 **kwargs):\n        # init base class\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n        self.reward_free = reward_free\n        self.stage_dependent = stage_dependent\n        self.real_time_dp = real_time_dp\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Discrete)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # other checks\n        assert gamma >= 0 and gamma <= 1.0\n        if self.horizon is None:\n            assert gamma < 1.0, \\\n                \"If no horizon is given, gamma must be smaller than 1.\"\n            self.horizon = int(np.ceil(1.0 / (1.0 - gamma)))\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        self.v_max = np.zeros(self.horizon)\n        self.v_max[-1] = r_range\n        for hh in reversed(range(self.horizon-1)):\n            self.v_max[hh] = r_range + self.gamma*self.v_max[hh+1]\n\n        # initialize\n        self.reset()\n\n    def reset(self, **kwargs):\n        H = self.horizon\n        S = self.env.observation_space.n\n        A = self.env.action_space.n\n\n        if self.stage_dependent:\n            shape_hsa = (H, S, A)\n            shape_hsas = (H, S, A, S)\n        else:\n            shape_hsa = (S, A)\n            shape_hsas = (S, A, S)\n\n        # (s, a) visit counter\n        self.N_sa = np.zeros(shape_hsa)\n        # (s, a) bonus\n        self.B_sa = np.ones(shape_hsa)\n\n        # MDP estimator\n        self.R_hat = np.zeros(shape_hsa)\n        self.P_hat = np.ones(shape_hsas) * 1.0/S\n\n        # Value functions\n        self.V = np.ones((H, S))\n        self.Q = np.zeros((H, S, A))\n        # for rec. policy\n        self.V_policy = np.zeros((H, S))\n        self.Q_policy = np.zeros((H, S, A))\n\n        # Init V and bonus\n        if not self.stage_dependent:\n            self.B_sa *= self.v_max[0]\n            self.V *= self.v_max[0]\n        else:\n            for hh in range(self.horizon):\n                self.B_sa[hh, :, :] = self.v_max[hh]\n                self.V[hh, :] = self.v_max[hh]\n\n        # ep counter\n        self.episode = 0\n\n        # useful object to compute total number of visited states & entropy of visited states\n        self.counter = DiscreteCounter(self.env.observation_space,\n                                       self.env.action_space)\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n\n        # update name\n        if self.real_time_dp:\n            self.name = 'UCBVI-RTDP'\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())\n\n    def policy(self, state, hh=0, **kwargs):\n        \"\"\" Recommended policy. \"\"\"\n        assert self.Q_policy is not None\n        return self.Q_policy[hh, state, :].argmax()\n\n    def _get_action(self, state, hh=0):\n        \"\"\" Sampling policy. \"\"\"\n        if not self.real_time_dp:\n            assert self.Q is not None\n            return self.Q[hh, state, :].argmax()\n        else:\n            if self.stage_dependent:\n                update_fn = update_value_and_get_action_sd\n            else:\n                update_fn = update_value_and_get_action\n            return update_fn(\n                state,\n                hh,\n                self.V,\n                self.R_hat,\n                self.P_hat,\n                self.B_sa,\n                self.gamma,\n                self.v_max,\n                )\n\n    def _compute_bonus(self, n, hh):\n        # reward-free\n        if self.reward_free:\n            bonus = 1.0 / n\n            return bonus\n\n        # not reward-free\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max[hh] / n\n            bonus = min(bonus, self.v_max[hh])\n            return bonus\n        else:\n            raise ValueError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))\n\n    def _update(self, state, action, next_state, reward, hh):\n        if self.stage_dependent:\n            self.N_sa[hh, state, action] += 1\n\n            nn = self.N_sa[hh, state, action]\n            prev_r = self.R_hat[hh, state, action]\n            prev_p = self.P_hat[hh, state, action, :]\n\n            self.R_hat[hh, state, action] = (1.0-1.0/nn)*prev_r + reward*1.0/nn\n\n            self.P_hat[hh, state, action, :] = (1.0-1.0/nn)*prev_p\n            self.P_hat[hh, state, action, next_state] += 1.0/nn\n\n            self.B_sa[hh, state, action] = self._compute_bonus(nn, hh)\n\n        else:\n            self.N_sa[state, action] += 1\n\n            nn = self.N_sa[state, action]\n            prev_r = self.R_hat[state, action]\n            prev_p = self.P_hat[state, action, :]\n\n            self.R_hat[state, action] = (1.0-1.0/nn)*prev_r + reward*1.0/nn\n\n            self.P_hat[state, action, :] = (1.0-1.0/nn)*prev_p\n            self.P_hat[state, action, next_state] += 1.0/nn\n\n            self.B_sa[state, action] = self._compute_bonus(nn, 0)\n\n    def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = self._get_action(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward  # used for logging only\n\n            self.counter.update(state, action)\n\n            if self.reward_free:\n                reward = 0.0  # set to zero before update if reward_free\n\n            self._update(state, action, next_state, reward, hh)\n\n            state = next_state\n            if done:\n                break\n\n        # run backward induction\n        if not self.real_time_dp:\n            if self.stage_dependent:\n                backward_induction_sd(\n                    self.Q,\n                    self.V,\n                    self.R_hat+self.B_sa,\n                    self.P_hat,\n                    self.gamma,\n                    self.v_max[0])\n            else:\n                backward_induction_in_place(\n                    self.Q,\n                    self.V,\n                    self.R_hat + self.B_sa,\n                    self.P_hat,\n                    self.horizon,\n                    self.gamma,\n                    self.v_max[0])\n\n        # update info\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self.episode += 1\n\n        # writer\n        if self.writer is not None:\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n            self.writer.add_scalar(\"n_visited_states\", self.counter.get_n_visited_states(), self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards\n\n    def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        # compute Q function for the recommended policy\n        if self.stage_dependent:\n            backward_induction_sd(\n                self.Q_policy,\n                self.V_policy,\n                self.R_hat,\n                self.P_hat,\n                self.gamma,\n                self.v_max[0])\n        else:\n            backward_induction_in_place(\n                self.Q_policy,\n                self.V_policy,\n                self.R_hat,\n                self.P_hat,\n                self.horizon,\n                self.gamma,\n                self.v_max[0])\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "def __init__(self,\n                 env,\n                 n_episodes=1000,\n                 gamma=1.0,\n                 horizon=100,\n                 bonus_scale_factor=1.0,\n                 bonus_type=\"simplified_bernstein\",\n                 reward_free=False,\n                 stage_dependent=False,\n                 real_time_dp=False,\n                 **kwargs):\n        # init base class\n        IncrementalAgent.__init__(self, env, **kwargs)\n\n        self.n_episodes = n_episodes\n        self.gamma = gamma\n        self.horizon = horizon\n        self.bonus_scale_factor = bonus_scale_factor\n        self.bonus_type = bonus_type\n        self.reward_free = reward_free\n        self.stage_dependent = stage_dependent\n        self.real_time_dp = real_time_dp\n\n        # check environment\n        assert isinstance(self.env.observation_space, spaces.Discrete)\n        assert isinstance(self.env.action_space, spaces.Discrete)\n\n        # other checks\n        assert gamma >= 0 and gamma <= 1.0\n        if self.horizon is None:\n            assert gamma < 1.0, \\\n                \"If no horizon is given, gamma must be smaller than 1.\"\n            self.horizon = int(np.ceil(1.0 / (1.0 - gamma)))\n\n        # maximum value\n        r_range = self.env.reward_range[1] - self.env.reward_range[0]\n        if r_range == np.inf or r_range == 0.0:\n            logger.warning(\"{}: Reward range is  zero or infinity. \".format(self.name)\n                           + \"Setting it to 1.\")\n            r_range = 1.0\n\n        self.v_max = np.zeros(self.horizon)\n        self.v_max[-1] = r_range\n        for hh in reversed(range(self.horizon-1)):\n            self.v_max[hh] = r_range + self.gamma*self.v_max[hh+1]\n\n        # initialize\n        self.reset()",
  "def reset(self, **kwargs):\n        H = self.horizon\n        S = self.env.observation_space.n\n        A = self.env.action_space.n\n\n        if self.stage_dependent:\n            shape_hsa = (H, S, A)\n            shape_hsas = (H, S, A, S)\n        else:\n            shape_hsa = (S, A)\n            shape_hsas = (S, A, S)\n\n        # (s, a) visit counter\n        self.N_sa = np.zeros(shape_hsa)\n        # (s, a) bonus\n        self.B_sa = np.ones(shape_hsa)\n\n        # MDP estimator\n        self.R_hat = np.zeros(shape_hsa)\n        self.P_hat = np.ones(shape_hsas) * 1.0/S\n\n        # Value functions\n        self.V = np.ones((H, S))\n        self.Q = np.zeros((H, S, A))\n        # for rec. policy\n        self.V_policy = np.zeros((H, S))\n        self.Q_policy = np.zeros((H, S, A))\n\n        # Init V and bonus\n        if not self.stage_dependent:\n            self.B_sa *= self.v_max[0]\n            self.V *= self.v_max[0]\n        else:\n            for hh in range(self.horizon):\n                self.B_sa[hh, :, :] = self.v_max[hh]\n                self.V[hh, :] = self.v_max[hh]\n\n        # ep counter\n        self.episode = 0\n\n        # useful object to compute total number of visited states & entropy of visited states\n        self.counter = DiscreteCounter(self.env.observation_space,\n                                       self.env.action_space)\n\n        # info\n        self._rewards = np.zeros(self.n_episodes)\n\n        # update name\n        if self.real_time_dp:\n            self.name = 'UCBVI-RTDP'\n\n        # default writer\n        self.writer = PeriodicWriter(self.name,\n                                     log_every=5*logger.getEffectiveLevel())",
  "def policy(self, state, hh=0, **kwargs):\n        \"\"\" Recommended policy. \"\"\"\n        assert self.Q_policy is not None\n        return self.Q_policy[hh, state, :].argmax()",
  "def _get_action(self, state, hh=0):\n        \"\"\" Sampling policy. \"\"\"\n        if not self.real_time_dp:\n            assert self.Q is not None\n            return self.Q[hh, state, :].argmax()\n        else:\n            if self.stage_dependent:\n                update_fn = update_value_and_get_action_sd\n            else:\n                update_fn = update_value_and_get_action\n            return update_fn(\n                state,\n                hh,\n                self.V,\n                self.R_hat,\n                self.P_hat,\n                self.B_sa,\n                self.gamma,\n                self.v_max,\n                )",
  "def _compute_bonus(self, n, hh):\n        # reward-free\n        if self.reward_free:\n            bonus = 1.0 / n\n            return bonus\n\n        # not reward-free\n        if self.bonus_type == \"simplified_bernstein\":\n            bonus = self.bonus_scale_factor * np.sqrt(1.0 / n) + self.v_max[hh] / n\n            bonus = min(bonus, self.v_max[hh])\n            return bonus\n        else:\n            raise ValueError(\n                \"Error: bonus type {} not implemented\".format(self.bonus_type))",
  "def _update(self, state, action, next_state, reward, hh):\n        if self.stage_dependent:\n            self.N_sa[hh, state, action] += 1\n\n            nn = self.N_sa[hh, state, action]\n            prev_r = self.R_hat[hh, state, action]\n            prev_p = self.P_hat[hh, state, action, :]\n\n            self.R_hat[hh, state, action] = (1.0-1.0/nn)*prev_r + reward*1.0/nn\n\n            self.P_hat[hh, state, action, :] = (1.0-1.0/nn)*prev_p\n            self.P_hat[hh, state, action, next_state] += 1.0/nn\n\n            self.B_sa[hh, state, action] = self._compute_bonus(nn, hh)\n\n        else:\n            self.N_sa[state, action] += 1\n\n            nn = self.N_sa[state, action]\n            prev_r = self.R_hat[state, action]\n            prev_p = self.P_hat[state, action, :]\n\n            self.R_hat[state, action] = (1.0-1.0/nn)*prev_r + reward*1.0/nn\n\n            self.P_hat[state, action, :] = (1.0-1.0/nn)*prev_p\n            self.P_hat[state, action, next_state] += 1.0/nn\n\n            self.B_sa[state, action] = self._compute_bonus(nn, 0)",
  "def _run_episode(self):\n        # interact for H steps\n        episode_rewards = 0\n        state = self.env.reset()\n        for hh in range(self.horizon):\n            action = self._get_action(state, hh)\n            next_state, reward, done, _ = self.env.step(action)\n            episode_rewards += reward  # used for logging only\n\n            self.counter.update(state, action)\n\n            if self.reward_free:\n                reward = 0.0  # set to zero before update if reward_free\n\n            self._update(state, action, next_state, reward, hh)\n\n            state = next_state\n            if done:\n                break\n\n        # run backward induction\n        if not self.real_time_dp:\n            if self.stage_dependent:\n                backward_induction_sd(\n                    self.Q,\n                    self.V,\n                    self.R_hat+self.B_sa,\n                    self.P_hat,\n                    self.gamma,\n                    self.v_max[0])\n            else:\n                backward_induction_in_place(\n                    self.Q,\n                    self.V,\n                    self.R_hat + self.B_sa,\n                    self.P_hat,\n                    self.horizon,\n                    self.gamma,\n                    self.v_max[0])\n\n        # update info\n        ep = self.episode\n        self._rewards[ep] = episode_rewards\n        self.episode += 1\n\n        # writer\n        if self.writer is not None:\n            self.writer.add_scalar(\"ep reward\", episode_rewards, self.episode)\n            self.writer.add_scalar(\"n_visited_states\", self.counter.get_n_visited_states(), self.episode)\n\n        # return sum of rewards collected in the episode\n        return episode_rewards",
  "def partial_fit(self, fraction, **kwargs):\n        assert 0.0 < fraction <= 1.0\n        n_episodes_to_run = int(np.ceil(fraction*self.n_episodes))\n        count = 0\n        while count < n_episodes_to_run and self.episode < self.n_episodes:\n            self._run_episode()\n            count += 1\n\n        # compute Q function for the recommended policy\n        if self.stage_dependent:\n            backward_induction_sd(\n                self.Q_policy,\n                self.V_policy,\n                self.R_hat,\n                self.P_hat,\n                self.gamma,\n                self.v_max[0])\n        else:\n            backward_induction_in_place(\n                self.Q_policy,\n                self.V_policy,\n                self.R_hat,\n                self.P_hat,\n                self.horizon,\n                self.gamma,\n                self.v_max[0])\n\n        info = {\"n_episodes\": self.episode,\n                \"episode_rewards\": self._rewards[:self.episode]}\n        return info",
  "class Discrete(gym.spaces.Discrete, SpaceSeeder):\n    \"\"\"\n    Class that represents discrete spaces.\n\n\n    Inherited from gym.spaces.Discrete for compatibility with gym.\n\n    rlberry wraps gym.spaces to make sure the seeding\n    mechanism is unified in the library (rlberry.seeding)\n\n    Attributes\n    ----------\n    rng : numpy.random._generator.Generator\n        random number generator provided by rlberry.seeding\n\n    Methods\n    -------\n    reseed()\n        get new random number generator\n    \"\"\"\n\n    def __init__(self, n):\n        \"\"\"\n        Parameters\n        ----------\n        n : int\n            number of elements in the space\n        \"\"\"\n        assert n >= 0, \"The number of elements in Discrete must be >= 0\"\n        SpaceSeeder.__init__(self)\n        gym.spaces.Discrete.__init__(self, n)\n\n    def sample(self):\n        return self.rng.integers(0, self.n)\n\n    def __str__(self):\n        objstr = \"%d-element Discrete space\" % self.n\n        return objstr",
  "def __init__(self, n):\n        \"\"\"\n        Parameters\n        ----------\n        n : int\n            number of elements in the space\n        \"\"\"\n        assert n >= 0, \"The number of elements in Discrete must be >= 0\"\n        SpaceSeeder.__init__(self)\n        gym.spaces.Discrete.__init__(self, n)",
  "def sample(self):\n        return self.rng.integers(0, self.n)",
  "def __str__(self):\n        objstr = \"%d-element Discrete space\" % self.n\n        return objstr",
  "class SpaceSeeder:\n    \"\"\"\n    Base class to define observation and action spaces.\n\n    Inherited from gym.spaces.Space for compatibility.\n\n    rlberry wraps gym.spaces to make sure the seeding\n    mechanism is unified in the library.\n\n    Attributes\n    ----------\n    rng : numpy.random._generator.Generator\n        random number generator provided by rlberry.seeding\n\n    Methods\n    -------\n    reseed()\n        get new random number generator\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # random number generator\n        self.rng = seeding.get_rng()\n\n    def reseed(self):\n        self.rng = seeding.get_rng()",
  "def __init__(self):\n        super().__init__()\n        # random number generator\n        self.rng = seeding.get_rng()",
  "def reseed(self):\n        self.rng = seeding.get_rng()",
  "class MultiDiscrete(gym.spaces.MultiDiscrete, SpaceSeeder):\n    \"\"\"\n\n    Inherited from gym.spaces.MultiDiscrete for compatibility with gym.\n\n    rlberry wraps gym.spaces to make sure the seeding\n    mechanism is unified in the library (rlberry.seeding)\n\n    Attributes\n    ----------\n    rng : numpy.random._generator.Generator\n        random number generator provided by rlberry.seeding\n\n    Methods\n    -------\n    reseed()\n        get new random number generator\n    \"\"\"\n    def __init__(self, nvec):\n        gym.spaces.MultiDiscrete.__init__(self, nvec)\n        SpaceSeeder.__init__(self)\n\n    def sample(self):\n        sample = self.rng.random(self.nvec.shape)*self.nvec\n        return sample.astype(self.dtype)",
  "def __init__(self, nvec):\n        gym.spaces.MultiDiscrete.__init__(self, nvec)\n        SpaceSeeder.__init__(self)",
  "def sample(self):\n        sample = self.rng.random(self.nvec.shape)*self.nvec\n        return sample.astype(self.dtype)",
  "class Box(gym.spaces.Box, SpaceSeeder):\n    \"\"\"\n    Class that represents a space that is a cartesian product in R^n:\n\n    [a_1, b_1] x [a_2, b_2] x ... x [a_n, b_n]\n\n\n    Inherited from gym.spaces.Box for compatibility with gym.\n\n    rlberry wraps gym.spaces to make sure the seeding\n    mechanism is unified in the library (rlberry.seeding)\n\n    Attributes\n    ----------\n    rng : numpy.random._generator.Generator\n        random number generator provided by rlberry.seeding\n\n    Methods\n    -------\n    reseed()\n        get new random number generator\n    \"\"\"\n    def __init__(self, low, high, shape=None, dtype=np.float64):\n        gym.spaces.Box.__init__(self, low, high, shape=shape, dtype=dtype)\n        SpaceSeeder.__init__(self)\n\n    def sample(self):\n        \"\"\"\n        Adapted from:\n        https://raw.githubusercontent.com/openai/gym/master/gym/spaces/box.py\n\n\n        Generates a single random sample inside of the Box.\n\n        In creating a sample of the box, each coordinate is sampled according\n        to the form of the interval:\n\n        * [a, b] : uniform distribution\n        * [a, oo) : shifted exponential distribution\n        * (-oo, b] : shifted negative exponential distribution\n        * (-oo, oo) : normal distribution\n        \"\"\"\n        high = self.high if self.dtype.kind == 'f' \\\n            else self.high.astype('int64') + 1\n        sample = np.empty(self.shape)\n\n        # Masking arrays which classify the coordinates according to interval\n        # type\n        unbounded = ~self.bounded_below & ~self.bounded_above\n        upp_bounded = ~self.bounded_below & self.bounded_above\n        low_bounded = self.bounded_below & ~self.bounded_above\n        bounded = self.bounded_below & self.bounded_above\n\n        # Vectorized sampling by interval type\n        sample[unbounded] = self.rng.normal(\n                size=unbounded[unbounded].shape)\n\n        sample[low_bounded] = self.rng.exponential(\n            size=low_bounded[low_bounded].shape) + self.low[low_bounded]\n\n        sample[upp_bounded] = -self.rng.exponential(\n            size=upp_bounded[upp_bounded].shape) + self.high[upp_bounded]\n\n        sample[bounded] = self.rng.uniform(low=self.low[bounded],\n                                           high=high[bounded],\n                                           size=bounded[bounded].shape)\n        if self.dtype.kind == 'i':\n            sample = np.floor(sample)\n\n        return sample.astype(self.dtype)",
  "def __init__(self, low, high, shape=None, dtype=np.float64):\n        gym.spaces.Box.__init__(self, low, high, shape=shape, dtype=dtype)\n        SpaceSeeder.__init__(self)",
  "def sample(self):\n        \"\"\"\n        Adapted from:\n        https://raw.githubusercontent.com/openai/gym/master/gym/spaces/box.py\n\n\n        Generates a single random sample inside of the Box.\n\n        In creating a sample of the box, each coordinate is sampled according\n        to the form of the interval:\n\n        * [a, b] : uniform distribution\n        * [a, oo) : shifted exponential distribution\n        * (-oo, b] : shifted negative exponential distribution\n        * (-oo, oo) : normal distribution\n        \"\"\"\n        high = self.high if self.dtype.kind == 'f' \\\n            else self.high.astype('int64') + 1\n        sample = np.empty(self.shape)\n\n        # Masking arrays which classify the coordinates according to interval\n        # type\n        unbounded = ~self.bounded_below & ~self.bounded_above\n        upp_bounded = ~self.bounded_below & self.bounded_above\n        low_bounded = self.bounded_below & ~self.bounded_above\n        bounded = self.bounded_below & self.bounded_above\n\n        # Vectorized sampling by interval type\n        sample[unbounded] = self.rng.normal(\n                size=unbounded[unbounded].shape)\n\n        sample[low_bounded] = self.rng.exponential(\n            size=low_bounded[low_bounded].shape) + self.low[low_bounded]\n\n        sample[upp_bounded] = -self.rng.exponential(\n            size=upp_bounded[upp_bounded].shape) + self.high[upp_bounded]\n\n        sample[bounded] = self.rng.uniform(low=self.low[bounded],\n                                           high=high[bounded],\n                                           size=bounded[bounded].shape)\n        if self.dtype.kind == 'i':\n            sample = np.floor(sample)\n\n        return sample.astype(self.dtype)",
  "class Dict(gym.spaces.Dict, SpaceSeeder):\n    \"\"\"\n\n    Inherited from gym.spaces.Dict for compatibility with gym.\n\n    rlberry wraps gym.spaces to make sure the seeding\n    mechanism is unified in the library (rlberry.seeding)\n\n    Attributes\n    ----------\n    rng : numpy.random._generator.Generator\n        random number generator provided by rlberry.seeding\n\n    Methods\n    -------\n    reseed()\n        get new random number generator\n    \"\"\"\n    def __init__(self, spaces=None, **spaces_kwargs):\n        gym.spaces.Dict.__init__(self, spaces, **spaces_kwargs)\n        SpaceSeeder.__init__(self)\n\n    def reseed(self):\n        _ = [space.reseed() for space in self.spaces.values()]",
  "def __init__(self, spaces=None, **spaces_kwargs):\n        gym.spaces.Dict.__init__(self, spaces, **spaces_kwargs)\n        SpaceSeeder.__init__(self)",
  "def reseed(self):\n        _ = [space.reseed() for space in self.spaces.values()]",
  "class MultiBinary(gym.spaces.MultiBinary, SpaceSeeder):\n    \"\"\"\n\n    Inherited from gym.spaces.MultiBinary for compatibility with gym.\n\n    rlberry wraps gym.spaces to make sure the seeding\n    mechanism is unified in the library (rlberry.seeding)\n\n    Attributes\n    ----------\n    rng : numpy.random._generator.Generator\n        random number generator provided by rlberry.seeding\n\n    Methods\n    -------\n    reseed()\n        get new random number generator\n    \"\"\"\n    def __init__(self, n):\n        gym.spaces.MultiBinary.__init__(self, n)\n        SpaceSeeder.__init__(self)\n\n    def sample(self):\n        return self.rng.integers(low=0, high=2,\n                                 size=self.n, dtype=self.dtype)",
  "def __init__(self, n):\n        gym.spaces.MultiBinary.__init__(self, n)\n        SpaceSeeder.__init__(self)",
  "def sample(self):\n        return self.rng.integers(low=0, high=2,\n                                 size=self.n, dtype=self.dtype)",
  "class Tuple(gym.spaces.Tuple, SpaceSeeder):\n    \"\"\"\n\n    Inherited from gym.spaces.Tuple for compatibility with gym.\n\n    rlberry wraps gym.spaces to make sure the seeding\n    mechanism is unified in the library (rlberry.seeding)\n\n    Attributes\n    ----------\n    rng : numpy.random._generator.Generator\n        random number generator provided by rlberry.seeding\n\n    Methods\n    -------\n    reseed()\n        get new random number generator\n    \"\"\"\n    def __init__(self, spaces):\n        gym.spaces.Tuple.__init__(self, spaces)\n        SpaceSeeder.__init__(self)\n\n    def reseed(self):\n        _ = [space.reseed() for space in self.spaces]",
  "def __init__(self, spaces):\n        gym.spaces.Tuple.__init__(self, spaces)\n        SpaceSeeder.__init__(self)",
  "def reseed(self):\n        _ = [space.reseed() for space in self.spaces]",
  "def numba_jit(func, **options):\n        \"\"\"This decorator does not modify the decorated function.\"\"\"\n        return func",
  "def metric_lp(x, y, p, scaling):\n    \"\"\"\n    Returns the p-norm:  || (x-y)/scaling||_p\n\n    Parameters\n    ----------\n    x : numpy.ndarray\n        1d array\n    y : numpy.ndarray\n        1d array\n    p : int\n        norm parameter\n    scaling : numpy.ndarray\n        1d array\n    \"\"\"\n    assert p >= 1\n    assert x.ndim == 1\n    assert y.ndim == 1\n    assert scaling.ndim == 1\n\n    d = len(x)\n    diff = np.abs((x - y) / scaling)\n    # p = infinity\n    if p == np.inf:\n        return diff.max()\n    # p < infinity\n    tmp = 0\n    for ii in range(d):\n        tmp += np.power(diff[ii], p)\n    return np.power(tmp, 1.0 / p)",
  "def load(path: str) -> Callable:\n    module_name, class_name = path.rsplit(\".\", 1)\n    return getattr(importlib.import_module(module_name), class_name)",
  "class Discretizer:\n    def __init__(self, space, n_bins):\n        assert isinstance(space, Box), \\\n            \"Discretization is only implemented for Box spaces.\"\n        assert space.is_bounded()\n        self.space = space\n        self.n_bins = n_bins\n\n        # initialize bins\n        assert n_bins > 0, \"Discretizer requires n_bins > 0\"\n        n_elements = 1\n        tol = 1e-8\n        self.dim = len(self.space.low)\n        n_elements = n_bins ** self.dim\n        self._bins = []\n        self._open_bins = []\n        for dd in range(self.dim):\n            range_dd = self.space.high[dd] - self.space.low[dd]\n            epsilon = range_dd / n_bins\n            bins_dd = []\n            for bb in range(n_bins + 1):\n                val = self.space.low[dd] + epsilon * bb\n                bins_dd.append(val)\n            self._open_bins.append(tuple(bins_dd[1:]))\n            bins_dd[-1] += tol  # \"close\" the last interval\n            self._bins.append(tuple(bins_dd))\n\n        # set observation space\n        self.discrete_space = Discrete(n_elements)\n\n        # List of discretized elements\n        self.discretized_elements = np.zeros((self.dim, n_elements))\n        for ii in range(n_elements):\n            self.discretized_elements[:, ii] = self.get_coordinates(ii, False)\n\n    def discretize(self, coordinates):\n        return binary_search_nd(coordinates, self._bins)\n\n    def get_coordinates(self, flat_index, randomize=False):\n        assert self.discrete_space.contains(flat_index), \"invalid flat_index\"\n        # get multi-index\n        index = unravel_index_uniform_bin(flat_index, self.dim, self.n_bins)\n\n        # get coordinates\n        coordinates = np.zeros(self.dim)\n        for dd in range(self.dim):\n            coordinates[dd] = self._bins[dd][index[dd]]\n            if randomize:\n                range_dd = self.space.high[dd] - self.space.low[dd]\n                epsilon = range_dd / self.n_bins\n                coordinates[dd] += epsilon * self.space.rng.uniform()\n        return coordinates",
  "def __init__(self, space, n_bins):\n        assert isinstance(space, Box), \\\n            \"Discretization is only implemented for Box spaces.\"\n        assert space.is_bounded()\n        self.space = space\n        self.n_bins = n_bins\n\n        # initialize bins\n        assert n_bins > 0, \"Discretizer requires n_bins > 0\"\n        n_elements = 1\n        tol = 1e-8\n        self.dim = len(self.space.low)\n        n_elements = n_bins ** self.dim\n        self._bins = []\n        self._open_bins = []\n        for dd in range(self.dim):\n            range_dd = self.space.high[dd] - self.space.low[dd]\n            epsilon = range_dd / n_bins\n            bins_dd = []\n            for bb in range(n_bins + 1):\n                val = self.space.low[dd] + epsilon * bb\n                bins_dd.append(val)\n            self._open_bins.append(tuple(bins_dd[1:]))\n            bins_dd[-1] += tol  # \"close\" the last interval\n            self._bins.append(tuple(bins_dd))\n\n        # set observation space\n        self.discrete_space = Discrete(n_elements)\n\n        # List of discretized elements\n        self.discretized_elements = np.zeros((self.dim, n_elements))\n        for ii in range(n_elements):\n            self.discretized_elements[:, ii] = self.get_coordinates(ii, False)",
  "def discretize(self, coordinates):\n        return binary_search_nd(coordinates, self._bins)",
  "def get_coordinates(self, flat_index, randomize=False):\n        assert self.discrete_space.contains(flat_index), \"invalid flat_index\"\n        # get multi-index\n        index = unravel_index_uniform_bin(flat_index, self.dim, self.n_bins)\n\n        # get coordinates\n        coordinates = np.zeros(self.dim)\n        for dd in range(self.dim):\n            coordinates[dd] = self._bins[dd][index[dd]]\n            if randomize:\n                range_dd = self.space.high[dd] - self.space.low[dd]\n                epsilon = range_dd / self.n_bins\n                coordinates[dd] += epsilon * self.space.rng.uniform()\n        return coordinates",
  "def configure_logging(level: str = \"INFO\",\n                      file_path: Path = None,\n                      file_level: str = \"DEBUG\",\n                      default_msg: str = \"\") -> None:\n    \"\"\"\n    Set the logging configuration\n\n    This default config can be further edited to only enable logging\n    in specific modules, by providing the name of its logger.\n\n    Parameters\n    ----------\n    level\n        Level of verbosity for the default (console) handler\n    file_path\n        Path to a log file\n    file_level\n        Level of verbosity for the file handler\n    default_msg\n        Message to append to the beginning all logs (e.g. thread id).\n    \"\"\"\n    config = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"formatters\": {\n            \"standard\": {\n                \"format\": default_msg+\"[%(levelname)s] %(message)s \"\n            },\n            \"detailed\": {\n                \"format\": default_msg+\"[%(name)s:%(levelname)s] %(message)s \"\n            }\n        },\n        \"handlers\": {\n            \"default\": {\n                \"level\": level,\n                \"formatter\": \"standard\",\n                \"class\": \"logging.StreamHandler\"\n            }\n        },\n        \"loggers\": {\n            \"\": {\n                \"handlers\": [\n                    \"default\"\n                ],\n                \"level\": \"DEBUG\",\n                \"propagate\": True\n            }\n        }\n    }\n    if file_path:\n        config[\"handlers\"][file_path.name] = {\n            \"class\": \"logging.FileHandler\",\n            \"filename\": file_path,\n            \"level\": file_level,\n            \"formatter\": \"detailed\",\n            \"mode\": 'w'\n        }\n        config[\"loggers\"][\"\"][\"handlers\"].append(file_path.name)\n    logging.config.dictConfig(config)\n    gym.logger.set_level(logging.getLevelName(level))\n    numba_logger = logging.getLogger('numba')\n    numba_logger.setLevel(logging.WARNING)",
  "def get_gpu_memory_map():\n    result = check_output(['nvidia-smi',\n                           '--query-gpu=memory.used',\n                           '--format=csv,nounits,noheader'])\n    return [int(x) for x in result.split()]",
  "def least_used_device():\n    \"\"\" Get the  GPU device with most available memory. \"\"\"\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"cuda unavailable\")\n\n    if shutil.which('nvidia-smi') is None:\n        raise RuntimeError(\"nvidia-smi unavailable: \\\ncannot select device with most least memory used.\")\n\n    memory_map = get_gpu_memory_map()\n    device_id = np.argmin(memory_map)\n    logger.info(f\"Choosing GPU device: {device_id}, \"\n                f\"memory used: {memory_map[device_id]}\")\n    return torch.device(\"cuda:{}\".format(device_id))",
  "def choose_device(preferred_device, default_device=\"cpu\"):\n    if preferred_device == \"cuda:best\":\n        try:\n            preferred_device = least_used_device()\n        except RuntimeError:\n            logger.info(f\"Could not find least used device (nvidia-smi might be missing), use cuda:0 instead\")\n            if torch.cuda.is_available():\n                return choose_device(\"cuda:0\")\n            else:\n                return choose_device(\"cpu\")\n    try:\n        torch.zeros((1,), device=preferred_device)  # Test availability\n    except (RuntimeError, AssertionError) as e:\n        logger.info(f\"Preferred device {preferred_device} unavailable ({e}).\"\n                    f\"Switching to default {default_device}\")\n        return default_device\n    return preferred_device",
  "def get_memory(pid=None):\n    if not pid:\n        pid = os.getpid()\n    command = \"nvidia-smi\"\n    result = run(command, stdout=PIPE, stderr=PIPE,\n                 universal_newlines=True, shell=True).stdout\n    m = re.findall(\"\\| *[0-9] *\"\n                   + str(pid)\n                   + \" *C *.*python.*? +([0-9]+).*\\|\", result, re.MULTILINE)\n    return [int(mem) for mem in m]",
  "class PeriodicWriter:\n    \"\"\"\n    Writes info to stdout periodically.\n\n    Useful for simple experiments where tensorboard is too much.\n\n    Parameters\n    ----------\n    name : str\n        Name of the writer, printed in the logs.\n    log_every : int\n        Interval used to log data. Measured in the number of calls\n        to add_scalar()\n    \"\"\"\n    def __init__(self, name, log_every=10):\n        self.name = name\n        self.log_every = log_every\n        #\n        self.data = None\n        self._calls_since_last_log = None\n        self._time_last_log = None\n        self.reset()\n\n    def reset(self):\n        self.data = {}\n        self._calls_since_last_log = 0\n        self._time_last_log = time.process_time()\n\n    def add_scalar(self, tag, scalar_value, global_step=None):\n        self._calls_since_last_log += 1\n        if tag not in self.data:\n            self.data[tag] = []\n        self.data[tag].append((scalar_value, global_step))\n\n        if self.log_every > 0 and \\\n                self._calls_since_last_log % self.log_every == 0:\n            self.log()\n            self._calls_since_last_log = 0\n\n    def log(self):\n        # time since last log\n        t_now = time.process_time()\n        time_elapsed = t_now - self._time_last_log\n        self._time_last_log = t_now\n\n        # write message\n        message = \"[{}]\".format(self.name)\n\n        for tag in self.data:\n            last_value, last_step = self.data[tag][-1]\n            if last_step is not None:\n                message += \\\n                    \" | step = {}, {} = {:0.3f}\".format(last_step,\n                                                        tag,\n                                                        last_value)\n            else:\n                message += \\\n                    \" | {} = {:0.3f}\".format(tag,\n                                             last_value)\n        # append time per log\n        if time_elapsed > 0:\n            logs_per_ms = self.log_every / time_elapsed\n            message += \" | freq = {:0.3f} logs/ms\".format(logs_per_ms)\n        else:\n            message += \" | freq = N.A. logs/ms\"\n\n        logger.info(message)\n\n    def __getattr__(self, attr):\n        \"\"\"\n        Avoid raising exceptions when invalid method is called, so\n        that PeriodicWriter does not raise exceptions when\n        the code expects a tensorboard writer.\n        \"\"\"\n        if attr[:2] == '__':\n            raise AttributeError(attr)\n\n        def method(*args, **kwargs):\n            pass\n        return method",
  "def __init__(self, name, log_every=10):\n        self.name = name\n        self.log_every = log_every\n        #\n        self.data = None\n        self._calls_since_last_log = None\n        self._time_last_log = None\n        self.reset()",
  "def reset(self):\n        self.data = {}\n        self._calls_since_last_log = 0\n        self._time_last_log = time.process_time()",
  "def add_scalar(self, tag, scalar_value, global_step=None):\n        self._calls_since_last_log += 1\n        if tag not in self.data:\n            self.data[tag] = []\n        self.data[tag].append((scalar_value, global_step))\n\n        if self.log_every > 0 and \\\n                self._calls_since_last_log % self.log_every == 0:\n            self.log()\n            self._calls_since_last_log = 0",
  "def log(self):\n        # time since last log\n        t_now = time.process_time()\n        time_elapsed = t_now - self._time_last_log\n        self._time_last_log = t_now\n\n        # write message\n        message = \"[{}]\".format(self.name)\n\n        for tag in self.data:\n            last_value, last_step = self.data[tag][-1]\n            if last_step is not None:\n                message += \\\n                    \" | step = {}, {} = {:0.3f}\".format(last_step,\n                                                        tag,\n                                                        last_value)\n            else:\n                message += \\\n                    \" | {} = {:0.3f}\".format(tag,\n                                             last_value)\n        # append time per log\n        if time_elapsed > 0:\n            logs_per_ms = self.log_every / time_elapsed\n            message += \" | freq = {:0.3f} logs/ms\".format(logs_per_ms)\n        else:\n            message += \" | freq = N.A. logs/ms\"\n\n        logger.info(message)",
  "def __getattr__(self, attr):\n        \"\"\"\n        Avoid raising exceptions when invalid method is called, so\n        that PeriodicWriter does not raise exceptions when\n        the code expects a tensorboard writer.\n        \"\"\"\n        if attr[:2] == '__':\n            raise AttributeError(attr)\n\n        def method(*args, **kwargs):\n            pass\n        return method",
  "def method(*args, **kwargs):\n            pass",
  "def lmap(v: np.ndarray, x: Interval, y: Interval) -> np.ndarray:\n    \"\"\"Linear map of value v with range x to desired range y.\"\"\"\n    return y[0] + (v - x[0]) * (y[1] - y[0]) / (x[1] - x[0])",
  "def binary_search_nd(x_vec, bins):\n    \"\"\"n-dimensional binary search\n\n    Parameters\n    -----------\n    x_vec : numpy.ndarray\n        numpy 1d array to be searched in the bins\n    bins : list\n        list of numpy 1d array, bins[d] = bins of the d-th dimension\n\n\n    Returns\n    --------\n    index (int) corresponding to the position of x in the partition\n    defined by the bins.\n    \"\"\"\n    dim = len(bins)\n    flat_index = 0\n    aux = 1\n    assert dim == len(x_vec), \"dimension mismatch in binary_search_nd()\"\n    for dd in range(dim):\n        index_dd = np.searchsorted(bins[dd], x_vec[dd], side='right') - 1\n        assert index_dd != -1, \"error in binary_search_nd()\"\n        flat_index += aux * index_dd\n        aux *= (len(bins[dd]) - 1)\n    return flat_index",
  "def unravel_index_uniform_bin(flat_index, dim, n_per_dim):\n    index = []\n    aux_index = flat_index\n    for _ in range(dim):\n        index.append(aux_index % n_per_dim)\n        aux_index = aux_index // n_per_dim\n    return tuple(index)",
  "def _get_most_recent_path(path_list):\n    \"\"\"\n    get most recend result for each agent_name\n    \"\"\"\n    most_recent_path = None\n    most_recent_id = None\n    for ii, dd in enumerate(path_list):\n        try:\n            run_id = int(dd.name)\n            if ii == 0 or run_id > most_recent_id:\n                most_recent_path = dd\n                most_recent_id = run_id\n        except Exception:\n            continue\n    return most_recent_path",
  "def load_experiment_results(output_dir, experiment_name):\n    \"\"\"\n    Parameters\n    ----------\n    output_dir : str or Path, or list\n        directory (or list of directories) where experiment results are stored\n        (command line argument --output_dir when running the eperiment)\n    experiment_name : str or Path, or list\n        name of yaml file describing the experiment.\n\n    Returns\n    -------\n    output_data: dict\n        dictionary such that\n\n        output_data['experiment_dirs'] = list of paths to experiment directory (output_dir/experiment_name)\n        output_data['agent_list'] = list containing the names of the agents in the experiment\n        output_data['stats'][agent_name] = fitted AgentStats for agent_name\n        output_data['dataframes'][agent_name] = dict of pandas data frames from the last run of the experiment\n        output_data['data_dir'][agent_name] = directory from which the results were loaded\n    \"\"\"\n    output_data = {}\n    output_data['agent_list'] = []\n    output_data['stats'] = {}\n    output_data['dataframes'] = {}\n    output_data['data_dir'] = {}\n\n    # preprocess input\n    if not isinstance(output_dir, list):\n        output_dir = [output_dir]\n    if not isinstance(experiment_name, list):\n        experiment_name = [experiment_name]\n    ndirs = len(output_dir)\n\n    if ndirs > 1:\n        assert len(experiment_name) == ndirs, \"Number of experiment names must match the number of output_dirs \"\n    else:\n        output_dir = len(experiment_name)*output_dir\n\n    results_dirs = []\n    for dd, exper in zip(output_dir, experiment_name):\n        results_dirs.append(Path(dd) / Path(exper).stem)\n    output_data['experiment_dirs'] = results_dirs\n\n    # Subdirectories with data for each agent\n    subdirs = []\n    for dd in results_dirs:\n        subdirs.extend([f for f in dd.iterdir() if f.is_dir()])\n\n    # Create dictionary dict[agent_name] = most recent result dir\n    data_dirs = {}\n    for dd in subdirs:\n        data_dirs[dd.name] = _get_most_recent_path([f for f in dd.iterdir() if f.is_dir()])\n\n    # Load data from each subdir\n    for agent_name in data_dirs:\n        output_data['agent_list'].append(agent_name)\n\n        # store data_dir\n        output_data['data_dir'][agent_name] = data_dirs[agent_name]\n\n        # store AgentStats\n        output_data['stats'][agent_name] = None\n        fname = data_dirs[agent_name] / 'stats.pickle'\n        try:\n            output_data['stats'][agent_name] = AgentStats.load(fname)\n        except Exception:\n            pass\n        logger.info(\"... loaded \" + str(fname))\n\n        # store data frames\n        dataframes = {}\n        csv_files = [f for f in data_dirs[agent_name].iterdir() if f.suffix == '.csv']\n        for ff in csv_files:\n            dataframes[ff.stem] = pd.read_csv(ff)\n            logger.info(\"... loaded \" + str(ff))\n        output_data['dataframes'][agent_name] = dataframes\n\n    return output_data",
  "def experiment_generator():\n    \"\"\"\n    Parse command line arguments and yields AgentStats instances.\n    \"\"\"\n    args = docopt(__doc__)\n    for (_, agent_stats) in parse_experiment_config(\n                Path(args[\"<experiment_path>\"]),\n                n_fit=int(args[\"--n_fit\"]),\n                n_jobs=int(args[\"--n_jobs\"]),\n                output_base_dir=args[\"--output_dir\"],\n                joblib_backend=args[\"--joblib_backend\"]):\n        if args[\"--writer\"]:\n            if _TENSORBOARD_INSTALLED:\n                for idx in range(agent_stats.n_fit):\n                    logdir = agent_stats.output_dir / f\"run_{idx + 1}_{datetime.now().strftime('%b%d_%H-%M-%S')}\"\n                    agent_stats.set_writer(idx=idx, writer_fn=SummaryWriter, writer_kwargs={'log_dir': logdir})\n            else:\n                logger.warning('Option --writer is not available: tensorboard is not installed.')\n\n        yield agent_stats",
  "def read_yaml(path):\n    with open(path) as file:\n        return yaml.safe_load(file)",
  "def process_agent_yaml(path):\n    config = read_yaml(path)\n    for key in _AGENT_KEYS:\n        if key not in config:\n            config[key] = {}\n    return config",
  "def read_agent_config(config_path):\n    \"\"\"\n    Read .yaml config file for an Agent instance.\n\n    The file contains the agent class and its parameters.\n\n    TODO: recursive update of base_config.\n\n    Example:\n\n    ``` myagent.yaml\n        agent_class: 'rlberry.agents.kernel_based.rs_ucbvi.RSUCBVIAgent'\n        gamma: 1.0\n        lp_metric: 2\n        min_dist: 0.0\n        max_repr: 800\n        bonus_scale_factor: 1.0\n    ```\n\n    Parameters\n    ----------\n    config_path : str\n        yaml file name containing the agent config\n\n    Returns\n    -------\n    agent_class\n    base_config : dict\n        dictionary whose keys are ('init_kwargs', 'fit_kwargs', 'policy_kwargs')\n    \"\"\"\n    agent_config = process_agent_yaml(config_path)\n    base_config_yaml = agent_config.pop(\"base_config\", None)\n\n    # TODO: recursive update\n    if base_config_yaml is None:\n        base_config = agent_config\n    else:\n        base_config = process_agent_yaml(base_config_yaml)\n        for key in _AGENT_KEYS:\n            try:\n                base_config[key].update(agent_config[key])\n            except KeyError:\n                base_config[key] = agent_config[key]\n\n    agent_class = load(base_config.pop(\"agent_class\"))\n    return agent_class, base_config",
  "def read_env_config(config_path):\n    \"\"\"\n    Read .yaml config file for an environment instance.\n\n    The file contains the environment constructor and its params.\n\n    Example:\n\n    ``` env.yaml\n        constructor: 'rlberry.envs.benchmarks.grid_exploration.nroom.NRoom'\n        params:\n            reward_free: false\n            array_observation: true\n            nrooms: 5\n    ```\n\n    Parameters\n    ----------\n    config_path : str\n        yaml file name containing the env config\n\n    Returns\n    -------\n    Tuple (constructor, kwargs) for the env\n    \"\"\"\n    with open(config_path) as file:\n        env_config = yaml.safe_load(file)\n        return load(env_config[\"constructor\"]), env_config[\"params\"]",
  "def parse_experiment_config(path: Path,\n                            n_fit: int = 4,\n                            n_jobs: int = 4,\n                            output_base_dir: str = 'results',\n                            joblib_backend: str = 'loky') -> Generator[Tuple[int, AgentStats], None, None]:\n    \"\"\"\n    Read .yaml files. set global seed and convert to AgentStats instances.\n\n    Exemple of experiment config:\n\n    ```experiment.yaml\n        description: 'My cool experiment'\n        seed: 42\n        n_episodes: 1000\n        horizon: 50\n        train_env: 'env_train.yaml'     # see read_env_config()\n        eval_env: 'env_eval.yaml'\n        agents:\n        - 'agent1.yaml'                 # see read_agent_config()\n        - 'agent2.yaml'\n    ```\n\n    Parameters\n    ----------\n    path : Path\n        Path to an experiment config\n    n_fit : int\n        Number of instances of each agent to fit\n    n_jobs : int\n        Number of parallel jobs\n    output_base_dir : str\n        Directory where to save AgentStats results.\n\n    Returns\n    -------\n    seed: int\n        global seed\n    agent_stats: AgentStats\n        the Agent Stats to fit\n    \"\"\"\n    with path.open() as file:\n        config = yaml.safe_load(file)\n        train_env = read_env_config(config[\"train_env\"])\n        eval_env = read_env_config(config[\"eval_env\"])\n        n_fit = n_fit\n        n_jobs = n_jobs\n\n        for agent_path in config[\"agents\"]:\n            # set seed before creating AgentStats\n            seed = config[\"seed\"]\n            set_global_seed(seed)\n\n            agent_name = Path(agent_path).stem\n            agent_class, agent_config = read_agent_config(agent_path)\n\n            # Process output dir, avoid erasing previous results\n            output_dir = Path(output_base_dir) / path.stem / agent_name\n            last = 0\n\n            try:\n                subdirs = [f for f in output_dir.iterdir() if f.is_dir()]\n            except FileNotFoundError:\n                subdirs = []\n\n            for dd in subdirs:\n                try:\n                    idx = int(dd.stem)\n                except ValueError:\n                    continue\n                if idx > last:\n                    last = idx\n\n            # kwargs\n            init_kwargs = agent_config['init_kwargs']\n            fit_kwargs = agent_config['fit_kwargs']\n            policy_kwargs = agent_config['policy_kwargs']\n\n            # check if there are global kwargs\n            if 'global_init_kwargs' in config:\n                init_kwargs.update(config['global_init_kwargs'])\n            if 'global_fit_kwargs' in config:\n                init_kwargs.update(config['global_fit_kwargs'])\n            if 'global_policy_kwargs' in config:\n                init_kwargs.update(config['global_policy_kwargs'])\n\n            # check eval_horizon\n            if 'eval_horizon' in config:\n                eval_horizon = config['eval_horizon']\n            else:\n                eval_horizon = None\n\n            # append run index to dir\n            output_dir = output_dir / str(last+1)\n\n            yield seed, AgentStats(agent_class=agent_class,\n                                   init_kwargs=init_kwargs,\n                                   fit_kwargs=fit_kwargs,\n                                   policy_kwargs=policy_kwargs,\n                                   agent_name=agent_name,\n                                   train_env=train_env,\n                                   eval_env=eval_env,\n                                   eval_horizon=eval_horizon,\n                                   n_fit=n_fit,\n                                   n_jobs=n_jobs,\n                                   output_dir=output_dir,\n                                   joblib_backend=joblib_backend)"
]