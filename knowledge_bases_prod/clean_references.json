[
  "\nfrom seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.compiler import Docker\nfrom seedemu.services import DomainNameCachingService\nfrom seedemu.core import Emulator, Binding, Filter, Node\nfrom typing import List\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\nldns = DomainNameCachingService()\n\ndef make_stub_as(asn: int, exchange: str):\n    stub_as = base.createAutonomousSystem(asn)\n    host = stub_as.createHost('host0')\n    host1 = stub_as.createHost('host1')\n    host2 = stub_as.createHost('host2')\n    host3 = stub_as.createHost('host3')\n    host4 = stub_as.createHost('host4')\n    host5 = stub_as.createHost('host5')\n    ldns_host = stub_as.createHost('ldns') \n\n    router = stub_as.createRouter('router0')\n    net = stub_as.createNetwork('net0')\n\n    router.joinNetwork('net0')\n    host.joinNetwork('net0')\n    host1.joinNetwork('net0')\n    host2.joinNetwork('net0')\n    host3.joinNetwork('net0')\n    host4.joinNetwork('net0')\n    host5.joinNetwork('net0')\n    ldns_host.joinNetwork('net0')\n\n    router.joinNetwork(exchange)\n\nldns.install('local-dns-150').setConfigureResolvconf(True)\nldns.install('local-dns-151').setConfigureResolvconf(True)\nldns.install('local-dns-152').setConfigureResolvconf(True)\nldns.install('local-dns-153').setConfigureResolvconf(True)\nldns.install('local-dns-154').setConfigureResolvconf(True)\nldns.install('local-dns-160').setConfigureResolvconf(True)\nldns.install('local-dns-161').setConfigureResolvconf(True)\n\nsim.addBinding(Binding('local-dns-150', filter = Filter(asn=150, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-151', filter = Filter(asn=151, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-152', filter = Filter(asn=152, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-153', filter = Filter(asn=153, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-154', filter = Filter(asn=154, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-160', filter = Filter(asn=160, nodeName=\"ldns\")))\nsim.addBinding(Binding('local-dns-161', filter = Filter(asn=161, nodeName=\"ldns\")))\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\nbase.createInternetExchange(102)\n\nmake_stub_as(150, 'ix100')\nmake_stub_as(151, 'ix100')\n\nmake_stub_as(152, 'ix101')\nmake_stub_as(153, 'ix101')\nmake_stub_as(154, 'ix101')\n\nmake_stub_as(160, 'ix102')\nmake_stub_as(161, 'ix102')\n\nas2 = base.createAutonomousSystem(2)\n\nas2_100 = as2.createRouter('r0')\nas2_101 = as2.createRouter('r1')\nas2_102 = as2.createRouter('r2')\n\nas2_100.joinNetwork('ix100')\nas2_101.joinNetwork('ix101')\nas2_102.joinNetwork('ix102')\n\nas2_net_100_101 = as2.createNetwork('n01')\nas2_net_101_102 = as2.createNetwork('n12')\nas2_net_102_100 = as2.createNetwork('n20')\n\nas2_100.joinNetwork('n01')\nas2_101.joinNetwork('n01')\n\nas2_101.joinNetwork('n12')\nas2_102.joinNetwork('n12')\n\nas2_102.joinNetwork('n20')\nas2_100.joinNetwork('n20')\n\nas3 = base.createAutonomousSystem(3)\n\nas3_101 = as3.createRouter('r1')\nas3_102 = as3.createRouter('r2')\n\nas3_101.joinNetwork('ix101')\nas3_102.joinNetwork('ix102')\n\nas3_net_101_102 = as3.createNetwork('n12')\n\nas3_101.joinNetwork('n12')\nas3_102.joinNetwork('n12')\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 154, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 154, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ebgp)\nsim.addLayer(ibgp)\nsim.addLayer(ospf)\nsim.addLayer(ldns)\n\nsim.dump('base-component.bin')",
  "\nimport argparse\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc('text', usetex=True)\n\nfrom fealpy.decorator import cartesian\n\nfrom fealpy.mesh import MeshFactory as MF\nfrom fealpy.mesh import HalfEdgeMesh2d\n\nfrom fealpy.timeintegratoralg import UniformTimeLine\n\nfrom fealpy.pde.heatequation_model_2d import ExpExpData\n\nfrom fealpy.functionspace import LagrangeFiniteElementSpace\n\nfrom fealpy.boundarycondition import DirichletBC \nfrom fealpy.tools.show import showmultirate\n\nfrom scipy.sparse.linalg import spsolve\n\nimport copy\n\nparser = argparse.ArgumentParser(description=\n        \"\")\n\nparser.add_argument('--ns',\n        default=10, type=int,\n        help='空间各个方向剖分段数， 默认剖分 10 段.')\n\nparser.add_argument('--nt',\n        default=100, type=int,\n        help='时间剖分段数，默认剖分 100 段.')\n\nparser.add_argument('--tol',\n        default=0.05, type=float,\n        help='自适应加密停止阈值，默认设定为 0.05.')\n\nparser.add_argument('--rtheta',\n        default=0.7, type=float,\n        help='自适应加密参数，默认设定为 0.7.')\n\nparser.add_argument('--ctheta',\n        default=0.3, type=float,\n        help='自适应粗化参数，默认设定为 0.3.')\n\nargs = parser.parse_args()\n\nns = args.ns\nnt = args.nt\ntol = args.tol\n\nrtheta = args.rtheta \nctheta = args.ctheta \n\npde = ExpExpData()\ndomain = pde.domain()\nc = pde.diffusionCoefficient\n\ntmesh = UniformTimeLine(0, 1, nt) \n\nsmesh = MF.boxmesh2d(domain, nx=ns, ny=ns, meshtype='tri')\nsmesh = HalfEdgeMesh2d.from_mesh(smesh, NV=3) \n\nsmesh.add_plot(plt)\nplt.savefig('./test-' + str(0) + '.png')\nplt.close()\ni = 0   \nwhile True:\n\n    space = LagrangeFiniteElementSpace(smesh, p=1) \n    \n    uh0 = space.interpolation(pde.init_value)\n    eta = space.recovery_estimate(uh0, method='area_harmonic')\n    err = np.sqrt(np.sum(eta**2))\n    if err < tol:\n        break\n    isMarkedCell = smesh.refine_marker(eta, rtheta, method='L2')\n    smesh.refine_triangle_rg(isMarkedCell)\n    i += 1\n    smesh.add_plot(plt)\n    plt.savefig('./test-' + str(i+1) + '.png')\n    plt.close()\n\nspace = LagrangeFiniteElementSpace(smesh, p=1)\nuh0 = space.interpolation(pde.init_value)\n\nfor j in range(0, nt): \n\n    t1 = tmesh.next_time_level()\n    print(\"t1=\", t1)\n\n    while True:\n        \n        uh1 = space.function()\n        A = c*space.stiff_matrix() \n        M = space.mass_matrix() \n        dt = tmesh.current_time_step_length() \n        G = M + dt*A \n\n        @cartesian\n        def source(p):\n            return pde.source(p, t1)\n        F = space.source_vector(source)\n        F *= dt\n        F += M@uh0\n\n        @cartesian\n        def dirichlet(p):\n            return pde.dirichlet(p, t1)\n        bc = DirichletBC(space, dirichlet)\n        GD, F = bc.apply(G, F, uh1)\n        \n        uh1[:] = spsolve(GD, F)\n        eta = space.recovery_estimate(uh1, method='area_harmonic')\n        err = np.sqrt(np.sum(eta**2))\n        print('errrefine', err)\n        if err < tol:\n            break\n        else:\n            \n            NN0 = smesh.number_of_nodes()\n            edge = smesh.entity('edge')\n            isMarkedCell = smesh.refine_marker(eta, rtheta, method='L2')\n            smesh.refine_triangle_rg(isMarkedCell)\n            i += 1\n            smesh.add_plot(plt)\n            plt.savefig('./test-'+str(i+1)+'.png')\n            plt.close()\n            space = LagrangeFiniteElementSpace(smesh, p=1)\n            print('refinedof', space.number_of_global_dofs())\n            uh00 = space.function()\n            nn2e = smesh.newnode2edge\n            uh00[:NN0] = uh0\n            uh00[NN0:] = np.average(uh0[edge[nn2e]], axis=-1)\n            uh0 = space.function()\n            uh0[:] = uh00\n    \n    isMarkedCell = smesh.refine_marker(eta, ctheta, 'COARSEN')\n    smesh.coarsen_triangle_rg(isMarkedCell)\n    i += 1\n    smesh.add_plot(plt)\n    plt.savefig('./test-'+str(i+1)+'.png')\n    plt.close()\n    space = LagrangeFiniteElementSpace(smesh, p=1)\n    print('coarsendof', space.number_of_global_dofs())\n    uh2 = space.function()\n    retain = smesh.retainnode\n    uh2[:] = uh1[retain]\n    uh1 = space.function()\n    uh0 = space.function()\n    uh1[:] = uh2\n\n    @cartesian\n    def solution(p):\n        return pde.solution(p, t1)\n    error = space.integralalg.error(solution, uh1)\n    print(\"error:\", error)\n\n    if (t1 ==0.01) | (t1 == 0.49) | (t1==0.99):\n        fig = plt.figure()\n        axes = fig.add_subplot(1, 1, 1, projection='3d')\n        uh1.add_plot(axes, cmap='rainbow')\n    uh0[:] = uh1\n    uh1[:] = 0.0\n\n    tmesh.advance()\n\nplt.show()",
  "\nimport sys\nimport argparse\nimport numpy as np\nfrom fealpy.pde.adi_2d import ADI_2d as PDE\nfrom fealpy.mesh import TriangleMesh\nfrom fealpy.mesh import MeshFactory as mf\nfrom fealpy.decorator import cartesian, barycentric\nfrom numpy.linalg import inv\nimport matplotlib.pyplot as plt\nfrom fealpy.functionspace import FirstKindNedelecFiniteElementSpace2d\nfrom fealpy.functionspace import ScaledMonomialSpace2d\nfrom fealpy.quadrature import  GaussLegendreQuadrature\nfrom scipy.sparse import csr_matrix, coo_matrix\nfrom numpy.linalg import inv\nfrom scipy.sparse import csr_matrix, spdiags, eye, bmat \t\t\nfrom scipy.sparse.linalg import spsolve\nfrom fealpy.tools.show import showmultirate, show_error_table\n\nparser = argparse.ArgumentParser(description=\n        \"\")\n\nparser.add_argument('--nt',\n        default=100, type=int,\n        help='时间剖分段数，默认剖分 100 段.')\n        \nparser.add_argument('--ns',\n        default=5, type=int,\n        help='空间各个方向初始剖分段数， 默认剖分 10 段.')\n        \nparser.add_argument('--nmax',\n        default=5, type=int,\n        help='空间迭代次数， 默认迭代5次.')\n\t\t\t\t\t\nargs = parser.parse_args()\nns = args.ns\nnt = args.nt\nnmax = args.nmax\n\nbox = [0, 1, 0, 1]\nmesh = mf.boxmesh2d(box, nx=ns, ny=ns, meshtype='tri')  \n\nsigma = 3*np.pi\nepsilon = 1.0\nmu = 1.0\npde = PDE(sigma, epsilon, mu)\n\ntau = 1.0e-5\n\"\"\nerrorType = ['$|| E - E_h||_{\\Omega,0}$'\n             ]\n\nerrorMatrix = np.zeros((1, nmax), dtype=np.float64)\nNDof = np.zeros(nmax, dtype=np.float64)\n\nfor n in range(nmax):\n\t\n\tspace = FirstKindNedelecFiniteElementSpace2d(mesh, p=0)\n\tdef init_E_value(p):\n\t\treturn pde.Efield(p, 0.5*tau)\t\n\tEh0 = space.interpolation(init_E_value)\n\tEh1 = space.function()\n\tgdof = space.number_of_global_dofs()\n\tNDof[n] = gdof \n\n\tsmspace = ScaledMonomialSpace2d(mesh, p=0)  \n\t\n\tdef init_H_value(p):\n\t\treturn pde.Hz(p, tau)    \n\tHh0 = smspace.local_projection(init_H_value)\n\tHh1 = smspace.function()\n\t\n\tdef get_phi_curl_matrix():\n\t\tqf = mesh.integrator(q=9, etype='cell')\n\t\tbcs, ws = qf.get_quadrature_points_and_weights()\n\t\tcellmeasure = mesh.entity_measure('cell')\n\t\tps= mesh.bc_to_point(bcs) \n\t\n\t\tcurlpsi = space.curl_basis(bcs) \n\t\tgdof = space.number_of_global_dofs()\n\t\tcell2dof = space.cell_to_dof() \n\t\t\n\t\tphi = smspace.basis(ps) \n\t\tsmsgdof = smspace.number_of_global_dofs() \n\t\tsmscell2dof = smspace.cell_to_dof() \n\t\t\n\t\tM = np.einsum('i, imd, ijk, j->jkd', ws, phi, curlpsi, cellmeasure, optimize=True)\n\t\t\n\t\tI = cell2dof[:, :, None]\n\t\t\n\t\tJ = np.broadcast_to(smscell2dof[:, :, None], shape=M.shape)\n\t\tM = csr_matrix((M.flat, (I.flat, J.flat)), shape=(gdof, smsgdof))\n\t\t\n\t\treturn M\n\t\n\tM_EMat = space.mass_matrix(epsilon)\n\tM_sigMat = space.mass_matrix(sigma)\n\tM_SMat = space.curl_matrix(1.0/mu)\n\tM_HMat = smspace.mass_matrix()\n\t\n\tM_CMat = get_phi_curl_matrix()\n\tTM_CMat = M_CMat.T \n\t\n\tLMat = M_EMat + tau/2*M_sigMat + (tau**2/4)*M_SMat\n\tRMat = M_EMat - tau/2*M_sigMat + (tau**2/4)*M_SMat\n\t\n\tfor i in range(nt):\n\t\t\n\t\tx = Hh0.T.flat \n\t\tRtH1 = tau*M_CMat@x\n\t\t\n\t\ty = Eh0.T.flat \n\t\tRtE1 = RMat@y\n\t\t@cartesian\n\t\tdef sol_g(p):\n\t\t\treturn pde.gsource(p, (i+1)*tau) \n\t\tRt1 = space.source_vector(sol_g)\n\t\tRt1 = tau*Rt1\t\n\t\tF1 = RtH1 + RtE1 + Rt1\n\t\t\n\t\tedge2dof = space.dof.edge_to_dof()\n\t\tgdof = space.number_of_global_dofs()\n\t\tisDDof = np.zeros(gdof, dtype=np.bool_)\n\t\tindex = mesh.ds.boundary_edge_index()\n\t\tisDDof[edge2dof[index]] = True\n\t\n\t\tbdIdx = np.zeros(LMat.shape[0], dtype=np.int_)\n\t\tbdIdx[isDDof] = 1\n\t\tTbd = spdiags(bdIdx, 0, LMat.shape[0], LMat.shape[0])\n\t\tT = spdiags(1-bdIdx, 0, LMat.shape[0], LMat.shape[0])\t\n\t\tA1 = T@LMat@T + Tbd\n\t\tF1[isDDof] = y[isDDof]\n\t\tEh1[:] = spsolve(A1, F1)\n\t\t\n\t\tA2 = mu*M_HMat\n\t\t@cartesian\n\t\tdef sol_fz(p):\n\t\t\treturn pde.fzsource(p, (i+1.5)*tau)\n\t\t\n\t\tRt2 = smspace.source_vector(sol_fz)\t\n\t\tRt2 = tau*Rt2\n\t\ty1 = Eh1.T.flat \n\t\tF2 = mu*M_HMat@x - tau*TM_CMat@y1 + Rt2\n\t\tHh1[:] = spsolve(A2, F2)\n\t\n\t\tEh0 = Eh1 \n\t\tHh0 = Hh1\n\t\n\t@cartesian\n\tdef solutionE(p):\n\t\treturn pde.Efield(p, (nt + 1.5)*tau)\n\t\t\t\n\terrorMatrix[0, n] = space.integralalg.error(solutionE, Eh0)\t\n\t\n\t@cartesian\n\tdef solutionH(p):\n\t\treturn pde.Hz(p, (nt + 2)*tau)\n\t\t\n\tif n < nmax - 1:\n\t\tmesh.uniform_refine()\n\t\t\nprint(\"errorMatrix = \", errorMatrix)\n\nshowmultirate(plt, 0, NDof, errorMatrix,  errorType, propsize=20)   \nplt.show()\t\n\t",
  "\n\"\"\n\nimport os\nimport numpy\nfrom pyscf import gto\nfrom pyscf import scf\nfrom pyscf import dft\n\nmol = gto.Mole()\nmol.verbose = 3\nmol.atom = [\n [\"C\",  ( 0.000000,  0.418626, 0.000000)],\n [\"H\",  (-0.460595,  1.426053, 0.000000)],\n [\"O\",  ( 1.196516,  0.242075, 0.000000)],\n [\"N\",  (-0.936579, -0.568753, 0.000000)],\n [\"H\",  (-0.634414, -1.530889, 0.000000)],\n [\"H\",  (-1.921071, -0.362247, 0.000000)]\n]\nmol.basis = {\"H\": '6-311++g**',\n             \"O\": '6-311++g**',\n             \"N\": '6-311++g**',\n             \"C\": '6-311++g**',\n             }\nmol.build()\n\na = dft.UKS(mol)\na.xc='b3lyp'\n\na.chkfile='nh2cho_s0.chkfile'\na.scf()\nmo0 = a.mo_coeff\nocc0 = a.mo_occ\n\nocc0[0][11] = 0.0\nocc0[0][12] = 1.0\n\nb = dft.UKS(mol)\nb.xc='b3lyp'\n\nb.chkfile='nh2cho_s1.chkfile'\ndm = b.make_rdm1(mo0, occ0)\n\nscf.addons.mom_occ_(b, mo0, occ0)\nb.scf(dm)\n\nmo0 = scf.chkfile.load('nh2cho_s0.chkfile', 'scf/mo_coeff')\nocc0 = scf.chkfile.load('nh2cho_s0.chkfile', 'scf/mo_occ')\nmo1 = scf.chkfile.load('nh2cho_s1.chkfile', 'scf/mo_coeff')\nocc1 = scf.chkfile.load('nh2cho_s1.chkfile', 'scf/mo_occ')\n\nmf = scf.UHF(mol)\n\ns, x = mf.det_ovlp(mo0, mo1, occ0, occ1)\n\ndm_s0 = mf.make_rdm1(mo0, occ0)\ndm_s1 = mf.make_rdm1(mo1, occ1)\ndm_01 = mf.make_asym_dm(mo0, mo1, occ0, occ1, x)\n\nh1e = mf.get_hcore(mol)\ne1_s0 = numpy.einsum('ji,ji', h1e.conj(), dm_s0[0]+dm_s0[1])\ne1_s1 = numpy.einsum('ji,ji', h1e.conj(), dm_s1[0]+dm_s1[1])\ne1_01 = numpy.einsum('ji,ji', h1e.conj(), dm_01[0]+dm_01[1])\n\nvhf_s0 = mf.get_veff(mol, dm_s0)\nvhf_s1 = mf.get_veff(mol, dm_s1)\nvhf_01 = mf.get_veff(mol, dm_01, hermi=0)\n\ne_s0 = mf.energy_elec(dm_s0, h1e, vhf_s0)\ne_s1 = mf.energy_elec(dm_s1, h1e, vhf_s1)\ne_01 = mf.energy_elec(dm_01, h1e, vhf_01)\n\nprint('The overlap between these two determiants is: %12.8f' % s)\nprint('E_1e(I),  E_JK(I),  E_tot(I):  %15.7f, %13.7f, %15.7f' % (e1_s0, e_s0[1], e_s0[0]))\nprint('E_1e(F),  E_JK(F),  E_tot(I):  %15.7f, %13.7f, %15.7f' % (e1_s1, e_s1[1], e_s1[0]))\nprint('E_1e(IF), E_JK(IF), E_tot(IF): %15.7f, %13.7f, %15.7f' % (e1_01, e_01[1], e_01[0]))\nprint(' <I|H|F> coupling is: %12.7f a.u.' % (e_01[0]*s))\nprint('(0.5*s*H_II+H_FF) is: %12.7f a.u.' % (0.5*s*(e_s0[0]+e_s1[0])))\n\nv01 = s*(e_01[0]-(e_s0[0]+e_s1[0])*0.5)/(1.0 - s*s)\nprint('The effective coupling is: %7.5f eV' % (numpy.abs(v01)*27.211385) )\n\nos.remove('nh2cho_s0.chkfile')\nos.remove('nh2cho_s1.chkfile')",
  "\nimport logging\n\nimport capytaine as cpt\nfrom capytaine.bem.airy_waves import airy_waves_free_surface_elevation\nfrom capytaine.ui.vtk.animation import Animation\n\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s:\\t%(message)s\")\n\nfull_mesh = cpt.mesh_sphere(radius=3, center=(0, 0, 0), resolution=(20, 20))\nfull_sphere = cpt.FloatingBody(mesh=full_mesh)\nfull_sphere.add_translation_dof(name=\"Heave\")\n\nsphere = full_sphere.immersed_part()\n\nsolver = cpt.BEMSolver()\n\ndiffraction_problem = cpt.DiffractionProblem(body=sphere, wave_direction=0.0, omega=2.0)\ndiffraction_result = solver.solve(diffraction_problem)\n\nradiation_problem = cpt.RadiationProblem(body=sphere, radiating_dof=\"Heave\", omega=2.0)\nradiation_result = solver.solve(radiation_problem)\n\nfree_surface = cpt.FreeSurface(x_range=(-50, 50), y_range=(-50, 50), nx=150, ny=150)\ndiffraction_elevation_at_faces = solver.compute_free_surface_elevation(free_surface, diffraction_result)\nradiation_elevation_at_faces = solver.compute_free_surface_elevation(free_surface, radiation_result)\n\ndiffraction_elevation_at_faces = diffraction_elevation_at_faces + airy_waves_free_surface_elevation(free_surface, diffraction_problem)\n\nanimation = Animation(loop_duration=diffraction_result.period)\nanimation.add_body(full_sphere, faces_motion=None)\nanimation.add_free_surface(free_surface, faces_elevation=0.5*diffraction_elevation_at_faces)\nanimation.run(camera_position=(-30, -30, 30))  \n\nanimation = Animation(loop_duration=radiation_result.period)\nanimation.add_body(full_sphere, faces_motion=full_sphere.dofs[\"Heave\"])\nanimation.add_free_surface(free_surface, faces_elevation=3.0*radiation_elevation_at_faces)\nanimation.run(camera_position=(-30, -30, 30))\n",
  "from seedemu.layers import Base, Routing, Ebgp\nfrom seedemu.services import WebService\nfrom seedemu.compiler import Docker\nfrom seedemu.core import Emulator, Binding, Filter\n\nemu = Emulator()\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nweb = WebService()\n\nix100 = base.createInternetExchange(100)\n\nix100_net = ix100.getPeeringLan()\nix100_net.setDisplayName('Seattle Internet Exchange')\nix100_net.setDescription('The largest IX in Seattle.')\n\nas150 = base.createAutonomousSystem(150)\n\nnet150 = as150.createNetwork('net0')\n\nnet150.setDisplayName('AS150 Backbone')\nnet150.setDescription('This is the main network of AS150.')\n\nas150_router = as150.createRouter('router0')\nas150_router.joinNetwork('net0')\nas150_router.joinNetwork('ix100')\n\nas150_router.setDisplayName('AS150 Core Router')\nas150_router.setDescription('The core router of AS150.')\n\nas150.createHost('web').joinNetwork('net0')\n\nweb.install('web150')\n\nemu.addBinding(Binding('web150', filter = Filter(nodeName = 'web', asn = 150)))\n\nas151 = base.createAutonomousSystem(151)\nas151.createNetwork('net0')\n\nas151.createHost('web').joinNetwork('net0')\nweb.install('web151')\nemu.addBinding(Binding('web151', filter = Filter(nodeName = 'web', asn = 151)))\n\nas151_router = as151.createRouter('router0')\nas151_router.joinNetwork('net0')\nas151_router.joinNetwork('ix100')\n\nas152 = base.createAutonomousSystem(152)\nas152.createNetwork('net0')\n\nas152.createHost('web').joinNetwork('net0')\nweb.install('web152')\nemu.addBinding(Binding('web152', filter = Filter(nodeName = 'web', asn = 152)))\n\nas152_router = as152.createRouter('router0')\nas152_router.joinNetwork('net0')\nas152_router.joinNetwork('ix100')\n\nebgp.addRsPeer(100, 150)\nebgp.addRsPeer(100, 151)\nebgp.addRsPeer(100, 152)\n\nemu.addLayer(base)\nemu.addLayer(routing)\nemu.addLayer(ebgp)\nemu.addLayer(web)\n\nemu.render()\n\nemu.compile(Docker(internetMapEnabled = True), './output')",
  "\n\"\"\n\nfrom __future__ import annotations\n\nimport sys\n\nimport urwid.lcd_display\n\nCGRAM = \"\"\n\ndef program_cgram(screen):\n    \"\"\n    \n    cbuf = [list() for x in range(8)]\n    for row in CGRAM.strip().split('\\n'):\n        rowsegments = row.strip().split()\n        for num, r in enumerate(rowsegments):\n            accum = 0\n            for c in r:\n                accum = (accum << 1) + (c == 'X')\n            cbuf[num].append(accum)\n\n    for num, cdata in enumerate(cbuf):\n        screen.program_cgram(num, cdata)\n\nclass LCDCheckBox(urwid.CheckBox):\n    \"\"\n    states = {\n        True: urwid.SelectableIcon('\\xd0'),\n        False: urwid.SelectableIcon('\\x05'),\n    }\n    reserve_columns = 1\n\nclass LCDRadioButton(urwid.RadioButton):\n    \"\"\n    states = {\n        True: urwid.SelectableIcon('\\xbb'),\n        False: urwid.SelectableIcon('\\x06'),\n    }\n    reserve_columns = 1\n\nclass LCDProgressBar(urwid.FlowWidget):\n    \"\"\n    segments = '\\x00\\x01\\x02\\x03'\n    def __init__(self, range, value):\n        self.range = range\n        self.value = value\n\n    def rows(self, size, focus=False):\n        return 1\n\n    def render(self, size, focus=False):\n        \"\"\n        (maxcol,) = size\n        steps = self.get_steps(size)\n        filled = urwid.int_scale(self.value, self.range, steps)\n        full_segments = int(filled / (len(self.segments) - 1))\n        last_char = filled % (len(self.segments) - 1) + 1\n        s = (self.segments[-1] * full_segments +\n            self.segments[last_char] +\n            self.segments[0] * (maxcol -full_segments - 1))\n        return urwid.Text(s).render(size)\n\n    def move_position(self, size, direction):\n        \"\"\n        steps = self.get_steps(size)\n        filled = urwid.int_scale(self.value, self.range, steps)\n        filled += 2 * direction - 1\n        value = urwid.int_scale(filled, steps, self.range)\n        value = max(0, min(self.range - 1, value))\n        if value != self.value:\n            self.value = value\n            self._invalidate()\n        return value\n\n    def get_steps(self, size):\n        \"\"\n        (maxcol,) = size\n        return maxcol * (len(self.segments) - 1)\n\nclass LCDHorizontalSlider(urwid.WidgetWrap):\n    \"\"\n    def __init__(self, range, value, callback):\n        self.bar = LCDProgressBar(range, value)\n        cols = urwid.Columns([\n            ('fixed', 1, urwid.SelectableIcon('\\x11')),\n            self.bar,\n            ('fixed', 1, urwid.SelectableIcon('\\x04')),\n            ])\n        super().__init__(cols)\n        self.callback = callback\n\n    def keypress(self, size, key):\n        \n        if key == 'enter':\n            \n            self.bar.move_position((self._w.column_widths(size)[1],),\n                self._w.get_focus_column() != 0)\n            self.callback(self.bar.value)\n        else:\n            return super().keypress(size, key)\n\nclass MenuOption(urwid.Button):\n    \"\"\n    def __init__(self, label, submenu):\n        super().__init__(\"\")\n        \n        self._label = urwid.Text(\"\")\n        self.set_label(label)\n\n        self._w = urwid.Columns([\n            ('fixed', 1, urwid.SelectableIcon('\\xdf')),\n            self._label])\n\n        urwid.connect_signal(self, 'click',\n            lambda option: show_menu(submenu))\n\n    def keypress(self, size, key):\n        if key == 'right':\n            key = 'enter'\n        return super().keypress(size, key)\n\nclass Menu(urwid.ListBox):\n    def __init__(self, widgets):\n        self.menu_parent = None\n        super().__init__(urwid.SimpleListWalker(widgets))\n\n    def keypress(self, size, key):\n        \"\"\n        key = super().keypress(size, key)\n        if key in ('left', 'esc') and self.menu_parent:\n            show_menu(self.menu_parent)\n        else:\n            return key\n\ndef build_menus():\n    cursor_option_group = []\n    def cursor_option(label, style):\n        \"a radio button that sets the cursor style\"\n        def on_change(b, state):\n            if state: screen.set_cursor_style(style)\n        b = LCDRadioButton(cursor_option_group, label,\n            screen.cursor_style == style)\n        urwid.connect_signal(b, 'change', on_change)\n        return b\n\n    def display_setting(label, range, fn):\n        slider = LCDHorizontalSlider(range, range/2, fn)\n        return urwid.Columns([\n            urwid.Text(label),\n            ('fixed', 10, slider),\n            ])\n\n    def led_custom(index):\n        def exp_scale_led(rg):\n            \"\"\n            return lambda value: screen.set_led_pin(index, rg,\n                [0, 1, 2, 3, 4, 5, 6, 8, 11, 14, 18,\n                23, 29, 38, 48, 61, 79, 100][value])\n\n        return urwid.Columns([\n            ('fixed', 2, urwid.Text('%dR' % index)),\n            LCDHorizontalSlider(18, 0, exp_scale_led(0)),\n            ('fixed', 2, urwid.Text(' G')),\n            LCDHorizontalSlider(18, 0, exp_scale_led(1)),\n            ])\n\n    menu_structure = [\n        ('Display Settings', [\n            display_setting('Brightness', 101, screen.set_backlight),\n            display_setting('Contrast', 76,\n                lambda x: screen.set_lcd_contrast(x + 75)),\n            ]),\n        ('Cursor Settings', [\n            cursor_option('Block', screen.CURSOR_BLINKING_BLOCK),\n            cursor_option('Underscore', screen.CURSOR_UNDERSCORE),\n            cursor_option('Block + Underscore',\n                screen.CURSOR_BLINKING_BLOCK_UNDERSCORE),\n            cursor_option('Inverting Block',\n                screen.CURSOR_INVERTING_BLINKING_BLOCK),\n            ]),\n        ('LEDs', [\n            led_custom(0),\n            led_custom(1),\n            led_custom(2),\n            led_custom(3),\n            ]),\n        ('About this Demo', [\n            urwid.Text(\"This is a demo of Urwid's CF635Display \"\n                \"module. If you need an interface for a limited \"\n                \"character display device this should serve as a \"\n                \"good example for implementing your own display \"\n                \"module and menu-driven application.\"),\n            ])\n        ]\n\n    def build_submenu(ms):\n        \"\"\n        options = []\n        submenus = []\n        for opt in ms:\n            \n            if type(opt) == tuple:\n                name, sub = opt\n                submenu = build_submenu(sub)\n                opt = MenuOption(name, submenu)\n                submenus.append(submenu)\n            options.append(opt)\n        menu = Menu(options)\n        for s in submenus:\n            s.menu_parent = menu\n        return menu\n    return build_submenu(menu_structure)\n\nscreen = urwid.lcd_display.CF635Screen(sys.argv[1])\n\nprogram_cgram(screen)\nloop = urwid.MainLoop(build_menus(), screen=screen)\n\nurwid.set_encoding('narrow')\n\ndef show_menu(menu):\n    loop.widget = menu\n\nloop.run()\n",
  "\n\"\"\n\nimport argparse\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import SGD\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor, RandomCrop\n\nfrom avalanche.benchmarks import nc_benchmark\nfrom avalanche.benchmarks.datasets.dataset_utils import default_dataset_location\nfrom avalanche.evaluation.metrics import (\n    forgetting_metrics,\n    accuracy_metrics,\n    labels_repartition_metrics,\n    loss_metrics,\n    cpu_usage_metrics,\n    timing_metrics,\n    gpu_usage_metrics,\n    ram_usage_metrics,\n    disk_usage_metrics,\n    MAC_metrics,\n    bwt_metrics,\n    forward_transfer_metrics,\n    class_accuracy_metrics,\n    amca_metrics,\n)\nfrom avalanche.models import SimpleMLP\nfrom avalanche.logging import (\n    InteractiveLogger,\n    TextLogger,\n    CSVLogger,\n    TensorboardLogger,\n)\nfrom avalanche.training.plugins import EvaluationPlugin\nfrom avalanche.training.supervised import Naive\n\ndef main(args):\n    \n    device = torch.device(\n        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n    )\n    \n    train_transform = transforms.Compose(\n        [\n            RandomCrop(28, padding=4),\n            ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ]\n    )\n    test_transform = transforms.Compose(\n        [ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    \n    mnist_train = MNIST(\n        root=default_dataset_location(\"mnist\"),\n        train=True,\n        download=True,\n        transform=train_transform,\n    )\n    mnist_test = MNIST(\n        root=default_dataset_location(\"mnist\"),\n        train=False,\n        download=True,\n        transform=test_transform,\n    )\n    benchmark = nc_benchmark(mnist_train, mnist_test, 5, task_labels=False, seed=1234)\n    \n    model = SimpleMLP(num_classes=benchmark.n_classes)\n\n    text_logger = TextLogger(open(\"log.txt\", \"a\"))\n\n    interactive_logger = InteractiveLogger()\n\n    csv_logger = CSVLogger()\n\n    tb_logger = TensorboardLogger()\n\n    eval_plugin = EvaluationPlugin(\n        accuracy_metrics(\n            minibatch=True,\n            epoch=True,\n            epoch_running=True,\n            experience=True,\n            stream=True,\n        ),\n        loss_metrics(\n            minibatch=True,\n            epoch=True,\n            epoch_running=True,\n            experience=True,\n            stream=True,\n        ),\n        class_accuracy_metrics(\n            epoch=True, stream=True, classes=list(range(benchmark.n_classes))\n        ),\n        amca_metrics(),\n        forgetting_metrics(experience=True, stream=True),\n        bwt_metrics(experience=True, stream=True),\n        forward_transfer_metrics(experience=True, stream=True),\n        cpu_usage_metrics(\n            minibatch=True,\n            epoch=True,\n            epoch_running=True,\n            experience=True,\n            stream=True,\n        ),\n        timing_metrics(\n            minibatch=True,\n            epoch=True,\n            epoch_running=True,\n            experience=True,\n            stream=True,\n        ),\n        ram_usage_metrics(\n            every=0.5, minibatch=True, epoch=True, experience=True, stream=True\n        ),\n        gpu_usage_metrics(\n            args.cuda,\n            every=0.5,\n            minibatch=True,\n            epoch=True,\n            experience=True,\n            stream=True,\n        ),\n        disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        MAC_metrics(minibatch=True, epoch=True, experience=True),\n        labels_repartition_metrics(on_train=True, on_eval=True),\n        loggers=[interactive_logger, text_logger, csv_logger, tb_logger],\n        collect_all=True,\n    )  \n\n    cl_strategy = Naive(\n        model,\n        SGD(model.parameters(), lr=0.001, momentum=0.9),\n        CrossEntropyLoss(),\n        train_mb_size=500,\n        train_epochs=1,\n        eval_mb_size=100,\n        device=device,\n        evaluator=eval_plugin,\n        eval_every=1,\n    )\n\n    print(\"Starting experiment...\")\n    results = []\n    for i, experience in enumerate(benchmark.train_stream):\n        print(\"Start of experience: \", experience.current_experience)\n        print(\"Current Classes: \", experience.classes_in_this_experience)\n\n        res = cl_strategy.train(experience, eval_streams=[benchmark.test_stream])\n        print(\"Training completed\")\n\n        print(\"Computing accuracy on the whole test set\")\n        \n        results.append(cl_strategy.eval(benchmark.test_stream))\n\n    print(f\"Test metrics:\\n{results}\")\n\n    all_metrics = cl_strategy.evaluator.get_all_metrics()\n    print(f\"Stored metrics: {list(all_metrics.keys())}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--cuda\",\n        type=int,\n        default=0,\n        help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n    )\n    args = parser.parse_args()\n    main(args)",
  "import blenderproc as bproc\nimport random\nimport os\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('scene_net_obj_path', help=\"Path to the used scene net `.obj` file, download via scripts/download_scenenet.py\")\nparser.add_argument('scene_texture_path', help=\"Path to the downloaded texture files, you can find them at http://tinyurl.com/zpc9ppb\")\nparser.add_argument('cc_material_path', nargs='?', default=\"resources/cctextures\", help=\"Path to CCTextures folder, see the /scripts for the download script.\")\nparser.add_argument('output_dir', nargs='?', default=\"examples/datasets/scenenet_with_cctextures/output\", help=\"Path to where the final files, will be saved\")\nargs = parser.parse_args()\n\nbproc.init()\n\nlabel_mapping = bproc.utility.LabelIdMapping.from_csv(bproc.utility.resolve_resource(os.path.join('id_mappings', 'nyu_idset.csv')))\nobjs = bproc.loader.load_scenenet(args.scene_net_obj_path, args.scene_texture_path, label_mapping)\n\ncc_materials = bproc.loader.load_ccmaterials(args.cc_material_path, preload=True)\n\nfor obj in objs:\n    \n    for i in range(len(obj.get_materials())):\n        \n        if np.random.uniform(0, 1) <= 0.4:\n            \n            obj.set_material(i, random.choice(cc_materials))\n\nbproc.loader.load_ccmaterials(args.cc_material_path, fill_used_empty_materials=True)\n\nwalls = bproc.filter.by_cp(objs, \"category_id\", label_mapping.id_from_label(\"wall\"))\n\nnew_floors = bproc.object.extract_floor(walls, new_name_for_object=\"floor\", should_skip_if_object_is_already_there=True)\n\nfor floor in new_floors:\n    floor.set_cp(\"category_id\", label_mapping.id_from_label(\"floor\"))\n\nobjs += new_floors\n\nnew_ceilings = bproc.object.extract_floor(walls, new_name_for_object=\"ceiling\", up_vector_upwards=False, should_skip_if_object_is_already_there=True)\n\nfor ceiling in new_ceilings:\n    ceiling.set_cp(\"category_id\", label_mapping.id_from_label(\"ceiling\"))\n\nobjs += new_ceilings\n\nlamps = bproc.filter.by_attr(objs, \"name\", \".*[l|L]amp.*\", regex=True)\nbproc.lighting.light_surface(lamps, emission_strength=15)\n\nceilings = bproc.filter.by_attr(objs, \"name\", \".*[c|C]eiling.*\", regex=True)\nbproc.lighting.light_surface(ceilings, emission_strength=2, emission_color=[1,1,1,1])\n\nbvh_tree = bproc.object.create_bvh_tree_multi_objects(objs)\n\nfloors = bproc.filter.by_cp(objs, \"category_id\", label_mapping.id_from_label(\"floor\"))\nposes = 0\ntries = 0\nwhile tries < 10000 and poses < 5:\n    tries += 1\n    \n    location = bproc.sampler.upper_region(floors, min_height=1.5, max_height=1.8)\n    \n    _, _, _, _, hit_object, _ = bproc.object.scene_ray_cast(location, [0, 0, -1])\n    if hit_object not in floors:\n        continue\n\n    rotation = np.random.uniform([1.2217, 0, 0], [1.2217, 0, 2 * np.pi])\n    cam2world_matrix = bproc.math.build_transformation_mat(location, rotation)\n\n    if not bproc.camera.perform_obstacle_in_view_check(cam2world_matrix, {\"min\": 1.0}, bvh_tree):\n        continue\n\n    if bproc.camera.scene_coverage_score(cam2world_matrix) < 0.1:\n        continue\n\n    bproc.camera.add_camera_pose(cam2world_matrix)\n    poses += 1\n\nbproc.renderer.enable_normals_output()\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\nbproc.renderer.enable_segmentation_output(map_by=[\"category_id\"])\n\ndata = bproc.renderer.render()\n\nbproc.writer.write_hdf5(args.output_dir, data)",
  "\nfrom __future__ import absolute_import, unicode_literals\nimport logging\n\nlogging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\"\"\n\ndef run():\n  import os\n  import DDG4\n  from DDG4 import OutputLevel as Output\n  from g4units import keV\n\n  args = DDG4.CommandLine()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  if args.help:\n    import sys\n    logger.info(\"\" + install_dir + \"\")\n    sys.exit(0)\n\n  kernel = DDG4.Kernel()\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/DDG4/compact/Channeling.xml\"))\n\n  DDG4.importConstants(kernel.detectorDescription(), debug=False)\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  geant4.printDetectors()\n  \n  if args.macro:\n    ui = geant4.setupCshUI(macro=args.macro, vis=args.vis)\n  else:\n    ui = geant4.setupCshUI(vis=args.vis)\n\n  if args.batch:\n    ui.Commands = ['/run/beamOn ' + str(args.events), '/ddg4/UI/terminate']\n\n  geant4.setupTrackingField(prt=True)\n  \n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputLevel = Output.DEBUG\n  prt.OutputType = 3  \n  kernel.eventAction().adopt(prt)\n\n  generator_output_level = Output.INFO\n\n  seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n  act.DebugMaterials = True\n  act.DebugElements = False\n  act.DebugVolumes = True\n  act.DebugShapes = True\n  act.DebugSurfaces = True\n\n  gun = geant4.setupGun(\"Gun\", particle='gamma', energy=5 * keV, multiplicity=1)\n  gun.OutputLevel = generator_output_level\n\n  geant4.setupTracker('ChannelingDevice')\n\n  phys = geant4.setupPhysics('QGSP_BERT')\n  ph = DDG4.PhysicsList(kernel, 'Channeling')\n  ph.addPhysicsConstructor(str('Geant4ChannelingPhysics'))\n  ph.enableUI()\n  phys.adopt(ph)\n  phys.dump()\n\n  phys.dump()\n\n  geant4.execute()\n\nif __name__ == \"__main__\":\n  run()",
  "\"\"\nfrom SimPEG import (\n    Mesh, Maps, Utils, DataMisfit, Regularization,\n    Optimization, Inversion, InvProblem, Directives\n)\nfrom SimPEG.EM import FDEM\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\ntry:\n    from pymatsolver import PardisoSolver as Solver\nexcept ImportError:\n    from SimPEG import SolverLU as Solver\n\ndef run(plotIt=True):\n\n    cs, ncx, ncz, npad = 10., 15, 25, 13  \n    hx = [(cs, ncx), (cs, npad, 1.3)]\n    hz = [(cs, npad, -1.3), (cs, ncz), (cs, npad, 1.3)]\n    mesh = Mesh.CylMesh([hx, 1, hz], '00C')\n\n    layerz = np.r_[-100., -50.]\n    layer = (mesh.vectorCCz >= layerz[0]) & (mesh.vectorCCz <= layerz[1])\n    active = mesh.vectorCCz < 0.\n\n    sig_half = 1e-2  \n    sig_air = 1e-8  \n    sig_layer = 1e-2  \n    sigma = np.ones(mesh.nCz)*sig_air\n    sigma[active] = sig_half\n    sigma[layer] = sig_layer\n\n    mur_half = 1.\n    mur_air = 1.\n    mur_layer = 2.\n    mur = np.ones(mesh.nCz)*mur_air\n    mur[active] = mur_half\n    mur[layer] = mur_layer\n\n    mtrue = mur[active]\n\n    actMap = Maps.InjectActiveCells(mesh, active, mur_air, nC=mesh.nCz)\n    surj1Dmap = Maps.SurjectVertical1D(mesh)\n    murMap = Maps.MuRelative(mesh)\n\n    muMap = murMap * surj1Dmap * actMap\n\n    rxlocs = Utils.ndgrid([np.r_[10.], np.r_[0], np.r_[30.]])\n    bzr = FDEM.Rx.Point_bSecondary(rxlocs, 'z', 'real')\n    \n    freqs = np.linspace(2000, 10000, 10)  \n    srcLoc = np.array([0., 0., 30.])\n\n    print(\n        'min skin depth = ', 500./np.sqrt(freqs.max() * sig_half),\n        'max skin depth = ', 500./np.sqrt(freqs.min() * sig_half)\n    )\n    print(\n        'max x ', mesh.vectorCCx.max(), 'min z ', mesh.vectorCCz.min(),\n        'max z ', mesh.vectorCCz.max()\n    )\n\n    srcList = [\n        FDEM.Src.MagDipole([bzr], freq, srcLoc, orientation='Z')\n        for freq in freqs\n    ]\n\n    surveyFD = FDEM.Survey(srcList)\n    prbFD = FDEM.Problem3D_b(\n        mesh, sigma=surj1Dmap * sigma, muMap=muMap, Solver=Solver\n    )\n    prbFD.pair(surveyFD)\n    std = 0.03\n    surveyFD.makeSyntheticData(mtrue, std)\n    surveyFD.eps = np.linalg.norm(surveyFD.dtrue)*1e-6\n\n    np.random.seed(13472)\n    dmisfit = DataMisfit.l2_DataMisfit(surveyFD)\n    regMesh = Mesh.TensorMesh([mesh.hz[muMap.maps[-1].indActive]])\n    reg = Regularization.Simple(regMesh)\n    opt = Optimization.InexactGaussNewton(maxIterCG=10)\n    invProb = InvProblem.BaseInvProblem(dmisfit, reg, opt)\n\n    beta = Directives.BetaSchedule(coolingFactor=4, coolingRate=3)\n    betaest = Directives.BetaEstimate_ByEig(beta0_ratio=2.)\n    target = Directives.TargetMisfit()\n    directiveList = [beta, betaest, target]\n\n    inv = Inversion.BaseInversion(invProb, directiveList=directiveList)\n    m0 = mur_half * np.ones(mtrue.size)\n    reg.alpha_s = 2e-2\n    reg.alpha_x = 1.\n    prbFD.counter = opt.counter = Utils.Counter()\n    opt.remember('xc')\n    moptFD = inv.run(m0)\n\n    dpredFD = surveyFD.dpred(moptFD)\n\n    if plotIt:\n        fig, ax = plt.subplots(1, 3, figsize=(10, 6))\n\n        fs = 13  \n        matplotlib.rcParams['font.size'] = fs\n\n        ax[0].semilogx(sigma[active], mesh.vectorCCz[active], 'k-', lw=2)\n        ax[0].set_ylim(-500, 0)\n        ax[0].set_xlim(5e-3, 1e-1)\n\n        ax[0].set_xlabel('Conductivity (S/m)', fontsize=fs)\n        ax[0].set_ylabel('Depth (m)', fontsize=fs)\n        ax[0].grid(\n            which='both', color='k', alpha=0.5, linestyle='-', linewidth=0.2\n        )\n        ax[0].legend(['Conductivity Model'], fontsize=fs, loc=4)\n\n        ax[1].plot(mur[active], mesh.vectorCCz[active], 'k-', lw=2)\n        ax[1].plot(moptFD, mesh.vectorCCz[active], 'b-', lw=2)\n        ax[1].set_ylim(-500, 0)\n        ax[1].set_xlim(0.5, 2.1)\n\n        ax[1].set_xlabel('Relative Permeability', fontsize=fs)\n        ax[1].set_ylabel('Depth (m)', fontsize=fs)\n        ax[1].grid(\n            which='both', color='k', alpha=0.5, linestyle='-', linewidth=0.2\n        )\n        ax[1].legend(['True', 'Predicted'], fontsize=fs, loc=4)\n\n        ax[2].plot(freqs, -surveyFD.dobs, 'k-', lw=2)\n        \n        ax[2].loglog(freqs, -dpredFD, 'bo', ms=6)\n        \n        ax[2].grid(which='both', alpha=0.5, linestyle='-', linewidth=0.2)\n        ax[2].grid(which='both', alpha=0.5, linestyle='-', linewidth=0.2)\n\n        ax[2].set_xlabel('Frequency (Hz)', fontsize=fs)\n        ax[2].set_ylabel('Vertical magnetic field (-T)', fontsize=fs)\n        ax[2].legend(\n            (\"z-Obs (real)\", \"z-Pred (real)\"),\n            fontsize=fs\n        )\n        ax[2].set_xlim(freqs.max(), freqs.min())\n\n        ax[0].set_title(\"(a) Conductivity Model\", fontsize=fs)\n        ax[1].set_title(\"(b) $\\mu_r$ Model\", fontsize=fs)\n        ax[2].set_title(\"(c) FDEM observed vs. predicted\", fontsize=fs)\n\n        plt.tight_layout(pad=1.5)\n\nif __name__ == '__main__':\n    run(plotIt=True)\n    plt.show()",
  "\nfrom seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.compiler import Docker\nfrom seedemu.core import Emulator\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\n\ndef make_stub_as(asn: int, exchange: str):\n    stub_as = base.createAutonomousSystem(asn)\n    host = stub_as.createHost('host0')\n    host1 = stub_as.createHost('host1')\n    host2 = stub_as.createHost('host2')\n    host3 = stub_as.createHost('host3')\n    host4 = stub_as.createHost('host4')\n    router = stub_as.createRouter('router0')\n\n    net = stub_as.createNetwork('net0')\n\n    router.joinNetwork('net0')\n    host.joinNetwork('net0')\n    host1.joinNetwork('net0')\n    host2.joinNetwork('net0')\n    host3.joinNetwork('net0')\n    host4.joinNetwork('net0')\n\n    router.joinNetwork(exchange)\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\nbase.createInternetExchange(102)\n\nmake_stub_as(150, 'ix100')\nmake_stub_as(151, 'ix100')\n\nmake_stub_as(152, 'ix101')\nmake_stub_as(153, 'ix101')\nmake_stub_as(154, 'ix101')\n\nmake_stub_as(160, 'ix102')\nmake_stub_as(161, 'ix102')\n\nas2 = base.createAutonomousSystem(2)\n\nas2_100 = as2.createRouter('r0')\nas2_101 = as2.createRouter('r1')\nas2_102 = as2.createRouter('r2')\n\nas2_100.joinNetwork('ix100')\nas2_101.joinNetwork('ix101')\nas2_102.joinNetwork('ix102')\n\nas2_net_100_101 = as2.createNetwork('n01')\nas2_net_101_102 = as2.createNetwork('n12')\nas2_net_102_100 = as2.createNetwork('n20')\n\nas2_100.joinNetwork('n01')\nas2_101.joinNetwork('n01')\n\nas2_101.joinNetwork('n12')\nas2_102.joinNetwork('n12')\n\nas2_102.joinNetwork('n20')\nas2_100.joinNetwork('n20')\n\nas3 = base.createAutonomousSystem(3)\n\nas3_101 = as3.createRouter('r1')\nas3_102 = as3.createRouter('r2')\n\nas3_101.joinNetwork('ix101')\nas3_102.joinNetwork('ix102')\n\nas3_net_101_102 = as3.createNetwork('n12')\n\nas3_101.joinNetwork('n12')\nas3_102.joinNetwork('n12')\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 154, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 154, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ebgp)\nsim.addLayer(ibgp)\nsim.addLayer(ospf)\n\nsim.dump('base-component.bin')",
  "\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom fealpy.mesh import TetrahedronMesh\n\nnode = np.array([\n    [0.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0],\n    [0.0, 1.0, 0.0],\n    [0.0, 0.0, 1.0],\n    [0.0, 1.0, 1.0]], dtype=np.float) \n\ncell = np.array([[1, 2, 0, 3], [2, 4, 3, 1]], dtype=np.int) \n\nmesh = TetrahedronMesh(node, cell)\n\nNN = mesh.number_of_nodes() \nNE = mesh.number_of_edges() \nNF = mesh.number_of_faces() \nNC = mesh.number_of_cells() \n\nnode = mesh.entity('node') \nedge = mesh.entity('edge') \nface = mesh.entity('face') \ncell = mesh.entity('cell') \n\nebc = mesh.entity_barycenter('edge') \nfbc = mesh.entity_barycenter('face') \ncbc = mesh.entity_barycenter('cell') \n\narea = mesh.entity_measure('cell') \nface = mesh.entity_measure('face') \neh = mesh.entity_measure('edge') \n\ncell2cell = mesh.ds.cell_to_cell() \ncell2face = mesh.ds.cell_to_face() \ncell2edge = mesh.ds.cell_to_edge() \ncell2node = mesh.ds.cell_to_node() \nprint('cell2cell:\\n', cell2cell)\nprint('cell2face:\\n', cell2face)\nprint('cell2edge:\\n', cell2edge)\nprint('cell2node:\\n', cell2node)\n\nface2cell = mesh.ds.face_to_cell() \nface2face = mesh.ds.face_to_face() \nface2edge = mesh.ds.face_to_edge() \nface2node = mesh.ds.face_to_node() \nprint('face2cell:\\n', face2cell)\nprint('face2face:\\n', face2face)\nprint(\"face2edge:\\n\", face2edge)\nprint('face2node:\\n', face2node)\n\nedge2cell = mesh.ds.edge_to_cell() \nedge2face = mesh.ds.edge_to_face() \nedge2node = mesh.ds.edge_to_node() \nedge2edge = mesh.ds.edge_to_edge() \nprint('edge2cell:\\n',edge2cell)\nprint('edge2face:\\n',edge2face)\nprint(\"edge2edge:\\n\",edge2edge)\nprint('edge2node:\\n',edge2face)\n\nnode2cell = mesh.ds.node_to_cell() \nnode2face = mesh.ds.node_to_face() \nnode2edge = mesh.ds.node_to_edge() \nnode2node = mesh.ds.node_to_node() \n\nprint('node2cell:\\n',node2cell)\nprint('node2face:\\n',node2face)\nprint('node2edge:\\n',node2edge)\nprint(\"node2node:\\n\",node2node)\n\nisBdNode = mesh.ds.boundary_node_flag() \nisBdEdge = mesh.ds.boundary_edge_flag() \nisBdFace = mesh.ds.boundary_face_flag() \nisBdCell = mesh.ds.boundary_cell_flag() \n\nbdNodeIdx = mesh.ds.boundary_node_index() \nbdEdgeIdx = mesh.ds.boundary_edge_index() \nbdFaceIdx = mesh.ds.boundary_face_index() \nbdCellIdx = mesh.ds.boundary_cell_index() \n\nmesh.print()\n\nfig = plt.figure()\naxes = Axes3D(fig)\nmesh.add_plot(axes, alpha=0, showedge=True)\nmesh.find_node(axes,showindex=True,fontsize=40)\nmesh.find_edge(axes, showindex=True,fontsize=40)\nmesh.find_cell(axes, showindex=True,fontsize=40)\nplt.show()\n",
  "\n\"\"\n\nfrom pprint import pprint\nimport fire\nimport qlib\nfrom qlib.model.trainer import DelayTrainerR, DelayTrainerRM, TrainerR, TrainerRM\nfrom qlib.workflow import R\nfrom qlib.workflow.online.manager import OnlineManager\nfrom qlib.workflow.online.strategy import RollingStrategy\nfrom qlib.workflow.task.gen import RollingGen\nfrom qlib.workflow.task.manage import TaskManager\nfrom qlib.tests.config import CSI100_RECORD_LGB_TASK_CONFIG_ONLINE, CSI100_RECORD_XGBOOST_TASK_CONFIG_ONLINE\nimport pandas as pd\nfrom qlib.contrib.evaluate import backtest_daily\nfrom qlib.contrib.evaluate import risk_analysis\nfrom qlib.contrib.strategy import TopkDropoutStrategy\n\nclass OnlineSimulationExample:\n    def __init__(\n        self,\n        provider_uri=\"~/.qlib/qlib_data/cn_data\",\n        region=\"cn\",\n        exp_name=\"rolling_exp\",\n        task_url=\"mongodb://10.0.0.4:27017/\",  \n        task_db_name=\"rolling_db\",  \n        task_pool=\"rolling_task\",\n        rolling_step=80,\n        start_time=\"2018-09-10\",\n        end_time=\"2018-10-31\",\n        tasks=None,\n        trainer=\"TrainerR\",\n    ):\n        \"\"\n        if tasks is None:\n            tasks = [CSI100_RECORD_XGBOOST_TASK_CONFIG_ONLINE, CSI100_RECORD_LGB_TASK_CONFIG_ONLINE]\n        self.exp_name = exp_name\n        self.task_pool = task_pool\n        self.start_time = start_time\n        self.end_time = end_time\n        mongo_conf = {\n            \"task_url\": task_url,\n            \"task_db_name\": task_db_name,\n        }\n        qlib.init(provider_uri=provider_uri, region=region, mongo=mongo_conf)\n        self.rolling_gen = RollingGen(\n            step=rolling_step, rtype=RollingGen.ROLL_SD, ds_extra_mod_func=None\n        )  \n        if trainer == \"TrainerRM\":\n            self.trainer = TrainerRM(self.exp_name, self.task_pool)\n        elif trainer == \"TrainerR\":\n            self.trainer = TrainerR(self.exp_name)\n        else:\n            \n            raise NotImplementedError(f\"This type of input is not supported\")\n        self.rolling_online_manager = OnlineManager(\n            RollingStrategy(exp_name, task_template=tasks, rolling_gen=self.rolling_gen),\n            trainer=self.trainer,\n            begin_time=self.start_time,\n        )\n        self.tasks = tasks\n\n    def reset(self):\n        if isinstance(self.trainer, TrainerRM):\n            TaskManager(self.task_pool).remove()\n        exp = R.get_exp(experiment_name=self.exp_name)\n        for rid in exp.list_recorders():\n            exp.delete_recorder(rid)\n\n    def main(self):\n        print(\"========== reset ==========\")\n        self.reset()\n        print(\"========== simulate ==========\")\n        self.rolling_online_manager.simulate(end_time=self.end_time)\n        print(\"========== collect results ==========\")\n        print(self.rolling_online_manager.get_collector()())\n        print(\"========== signals ==========\")\n        signals = self.rolling_online_manager.get_signals()\n        print(signals)\n        \n        CSI300_BENCH = \"SH000903\"\n        STRATEGY_CONFIG = {\n            \"topk\": 30,\n            \"n_drop\": 3,\n            \"signal\": signals.to_frame(\"score\"),\n        }\n        strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)\n        report_normal, positions_normal = backtest_daily(\n            start_time=signals.index.get_level_values(\"datetime\").min(),\n            end_time=signals.index.get_level_values(\"datetime\").max(),\n            strategy=strategy_obj,\n        )\n        analysis = dict()\n        analysis[\"excess_return_without_cost\"] = risk_analysis(report_normal[\"return\"] - report_normal[\"bench\"])\n        analysis[\"excess_return_with_cost\"] = risk_analysis(\n            report_normal[\"return\"] - report_normal[\"bench\"] - report_normal[\"cost\"]\n        )\n\n        analysis_df = pd.concat(analysis)  \n        pprint(analysis_df)\n\n    def worker(self):\n        \n        print(\"========== worker ==========\")\n        if isinstance(self.trainer, TrainerRM):\n            self.trainer.worker()\n        else:\n            print(f\"{type(self.trainer)} is not supported for worker.\")\n\nif __name__ == \"__main__\":\n    \n    fire.Fire(OnlineSimulationExample)",
  "\nfrom __future__ import print_function\n\nimport numpy as np\nimport os\nfrom mpi4py import MPI\nfrom tacs import TACS, elements, constitutive, functions\n\nbdfFile = os.path.join(os.path.dirname(__file__), \"CRM_box_2nd.bdf\")\ntacs_comm = MPI.COMM_WORLD\nstruct_mesh = TACS.MeshLoader(tacs_comm)\nstruct_mesh.scanBDFFile(bdfFile)\n\nrho = 2500.0  \nE = 70e9  \nnu = 0.3  \nkcorr = 5.0 / 6.0  \nys = 350e6  \nmin_thickness = 0.002\nmax_thickness = 0.20\nthickness = 0.02\n\nnum_components = struct_mesh.getNumComponents()\nfor i in range(num_components):\n    descriptor = struct_mesh.getElementDescript(i)\n    \n    prop = constitutive.MaterialProperties(rho=rho, E=E, nu=nu, ys=ys)\n    \n    stiff = constitutive.IsoShellConstitutive(\n        prop, t=thickness, tMin=min_thickness, tMax=max_thickness, tNum=i\n    )\n\n    element = None\n    transform = None\n    if descriptor in [\"CQUAD\", \"CQUADR\", \"CQUAD4\"]:\n        element = elements.Quad4Shell(transform, stiff)\n    struct_mesh.setElement(i, element)\n\ntacs = struct_mesh.createTACS(6)\n\nksWeight = 100.0\nfuncs = [functions.KSFailure(tacs, ksWeight=ksWeight)]\n\nx = tacs.createDesignVec()\nx_array = x.getArray()\ntacs.getDesignVars(x)\n\nX = tacs.createNodeVec()\ntacs.getNodes(X)\ntacs.setNodes(X)\n\nforces = tacs.createVec()\nforce_array = forces.getArray()\nforce_array[2::6] += 100.0  \ntacs.applyBCs(forces)\n\nres = tacs.createVec()\nans = tacs.createVec()\nu = tacs.createVec()\nmat = tacs.createSchurMat()\npc = TACS.Pc(mat)\nsubspace = 100\nrestarts = 2\ngmres = TACS.KSM(mat, pc, subspace, restarts)\n\nalpha = 1.0\nbeta = 0.0\ngamma = 0.0\ntacs.zeroVariables()\ntacs.assembleJacobian(alpha, beta, gamma, res, mat)\npc.factor()\n\ngmres.solve(forces, ans)\ntacs.setVariables(ans)\n\nfvals1 = tacs.evalFunctions(funcs)\n\nadjoint = tacs.createVec()\nres.zeroEntries()\ntacs.addSVSens([funcs[0]], [res])\ngmres.solve(res, adjoint)\n\nfdv_sens = tacs.createDesignVec()\nfdv_sens_array = fdv_sens.getArray()\ntacs.addDVSens([funcs[0]], [fdv_sens])\ntacs.addAdjointResProducts([adjoint], [fdv_sens], -1)\n\nfdv_sens.beginSetValues()\nfdv_sens.endSetValues()\n\npert = tacs.createNodeVec()\nX_array = X.getArray()\npert_array = pert.getArray()\npert_array[0::3] = X_array[1::3]\npert_array[1::3] = X_array[0::3]\npert_array[2::3] = X_array[2::3]\n\nfXptSens = tacs.createNodeVec()\ntacs.addXptSens([funcs[0]], [fXptSens])\ntacs.addAdjointResXptSensProducts([adjoint], [fXptSens], -1)\n\nfXptSens.beginSetValues()\nfXptSens.endSetValues()\n\nxpert = tacs.createDesignVec()\nxpert.setRand()\nxpert_array = xpert.getArray()\nxnew = tacs.createDesignVec()\nxnew.copyValues(x)\nif TACS.dtype is complex:\n    dh = 1e-30\n    xnew.axpy(dh * 1j, xpert)\nelse:\n    dh = 1e-6\n    xnew.axpy(dh, xpert)\n\ntacs.setDesignVars(xnew)\n\ntacs.zeroVariables()\ntacs.assembleJacobian(alpha, beta, gamma, res, mat)\npc.factor()\ngmres.solve(forces, u)\ntacs.setVariables(u)\n\nfvals2 = tacs.evalFunctions(funcs)\n\nif TACS.dtype is complex:\n    fd = fvals2.imag / dh\nelse:\n    fd = (fvals2 - fvals1) / dh\n\nresult = xpert.dot(fdv_sens)\nif tacs_comm.rank == 0:\n    print(\"FD:      \", fd[0])\n    print(\"Result:  \", result)\n    print(\"Rel err: \", (result - fd[0]) / result)\n\ntacs.setDesignVars(x)\n\nif TACS.dtype is complex:\n    dh = 1e-30\n    X.axpy(dh * 1j, pert)\nelse:\n    dh = 1e-6\n    X.axpy(dh, pert)\n\ntacs.setNodes(X)\n\ntacs.zeroVariables()\ntacs.assembleJacobian(alpha, beta, gamma, res, mat)\npc.factor()\ngmres.solve(forces, u)\ntacs.setVariables(u)\n\nfvals2 = tacs.evalFunctions(funcs)\n\nif TACS.dtype is complex:\n    fd = fvals2.imag / dh\nelse:\n    fd = (fvals2 - fvals1) / dh\n\nresult = pert.dot(fXptSens)\n\nif tacs_comm.rank == 0:\n    print(\"FD:      \", fd[0])\n    print(\"Result:  \", result)\n    print(\"Rel err: \", (result - fd[0]) / result)\n\nflag = (\n    TACS.OUTPUT_CONNECTIVITY\n    | TACS.OUTPUT_NODES\n    | TACS.OUTPUT_DISPLACEMENTS\n    | TACS.OUTPUT_STRAINS\n    | TACS.OUTPUT_STRESSES\n    | TACS.OUTPUT_EXTRAS\n    | TACS.OUTPUT_LOADS\n)\nf5 = TACS.ToFH5(tacs, TACS.BEAM_OR_SHELL_ELEMENT, flag)\nf5.writeToFile(\"ucrm.f5\")",
  "\nimport sys\nimport argparse\n\nimport numpy as np\nfrom numpy.linalg import inv\nfrom scipy.sparse import spdiags\nfrom scipy.sparse.linalg import spsolve\nimport matplotlib.pyplot as plt\n\nfrom fealpy.mesh import MeshFactory\nfrom fealpy.pde.timeharmonic_2d import CosSinData\nfrom fealpy.functionspace import FirstKindNedelecFiniteElementSpace2d \nfrom fealpy.functionspace import LagrangeFiniteElementSpace\nfrom fealpy.boundarycondition import DirichletBC \n\nfrom fealpy.mesh.adaptive_tools import mark\nfrom fealpy.tools.show import showmultirate\nfrom fealpy.tools.show import show_error_table\n\ndef curl_recover(uh):\n\n    mesh = uh.space.mesh\n    space = LagrangeFiniteElementSpace(mesh, p=1)\n    ruh = space.function() \n\n    bc = np.array([1/3, 1/3, 1/3], dtype=mesh.ftype)\n    val = uh.curl_value(bc) \n    w = 1/mesh.entity_measure('cell')\n    val *= w\n\n    NN = mesh.number_of_nodes() \n    NC = mesh.number_of_cells()\n    cell = mesh.entity('cell')\n    w = np.broadcast_to(w.reshape(-1, 1), shape=cell.shape)\n    W = np.zeros(NN, dtype=mesh.ftype)\n    np.add.at(W, cell, w)\n\n    val = np.broadcast_to(val.reshape(-1, 1), shape=cell.shape)\n    np.add.at(ruh, cell, val)\n    ruh /= W\n\n    return ruh\n\ndef spr_edge(mesh, h, edgeVal):\n\n    \"\"\n\n    NN = mesh.number_of_nodes() \n    NE = mesh.number_of_edges()\n    NC = mesh.number_of_cells()\n\n    edge = mesh.entity('edge')\n    v = mesh.edge_tangent()/2 \n    phi = np.ones((NE, 3), dtype=mesh.ftype)\n\n    A = np.zeros((NN, 3, 3), dtype=mesh.ftype)\n    b = np.zeros((NN, 3), dtype=mesh.ftype)\n\n    phi[:, 1:] = v/h[edge[:, 0], None] \n    val = phi[:, :, None]*phi[:, None, :]\n    np.add.at(A, (edge[:, 0], np.s_[:], np.s_[:]), val) \n    val = phi*edgeVal[:, None]\n    np.add.at(b, (edge[:, 0], np.s_[:]), val)\n\n    phi[:, 1:] = -v/h[edge[:, 1], None] \n    val = phi[:, :, None]*phi[:, None, :]\n    np.add.at(A, (edge[:, 1], np.s_[:], np.s_[:]), val) \n    val = phi*edgeVal[:, None]\n    np.add.at(b, (edge[:, 1], np.s_[:]), val)\n    return A, b\n\ndef spr_curl(uh):\n\n    \"\"\n    mesh = uh.space.mesh\n\n    NN = mesh.number_of_nodes()\n    NE = mesh.number_of_edges()\n    NC = mesh.number_of_cells()\n\n    node = mesh.entity('node')\n    edge = mesh.entity('edge')\n    cell = mesh.entity('cell')\n\n    bc = np.array([1/3, 1/3, 1/3], dtype=mesh.ftype)\n    cellVal = uh.curl_value(bc) \n\n    edge2cell = mesh.ds.edge_to_cell()\n    edgeVal = (cellVal[edge2cell[:, 0]] + cellVal[edge2cell[:, 1]])/2.0\n\n    h = mesh.node_size()\n    A, b = spr_edge(mesh, h, edgeVal) \n\n    isBdNode = mesh.ds.boundary_node_flag()\n    idxMap = np.arange(NN, dtype=mesh.itype) \n\n    flag = isBdNode[edge[:, 0]] & (~isBdNode[edge[:, 1]])\n    idxMap[edge[flag, 0]] = edge[flag, 1]\n    flag = isBdNode[edge[:, 1]] & (~isBdNode[edge[:, 0]])\n    idxMap[edge[flag, 1]] = edge[flag, 0]\n\n    isCEdge = edge2cell[:, 0] != edge2cell[:, 1]\n    isCEdge = isCEdge & isBdNode[edge[:, 0]] & isBdNode[edge[:, 1]]\n\n    idxMap[cell[edge2cell[isCEdge, 0], edge2cell[isCEdge, 2]]] = cell[edge2cell[isCEdge, 1], edge2cell[isCEdge, 3]] \n    idxMap[cell[edge2cell[isCEdge, 1], edge2cell[isCEdge, 3]]] = cell[edge2cell[isCEdge, 0], edge2cell[isCEdge, 2]] \n\n    c = h[idxMap[isBdNode]]/h[isBdNode] \n    xe = (node[idxMap[isBdNode]] - node[isBdNode])/h[isBdNode, None]\n\n    A[isBdNode, 0, 0] = A[idxMap[isBdNode], 0, 0]\n\n    A[isBdNode, 0, 1] = A[idxMap[isBdNode], 0, 0]*xe[:, 0] \n    A[isBdNode, 0, 1]+= A[idxMap[isBdNode], 0, 1]*c\n    A[isBdNode, 1, 0] = A[isBdNode, 0, 1]\n\n    A[isBdNode, 0, 2] = A[idxMap[isBdNode], 0, 0]*xe[:, 1] \n    A[isBdNode, 0, 2]+= A[idxMap[isBdNode], 0, 2]*c\n    A[isBdNode, 2, 0] = A[isBdNode, 0, 2]\n\n    A[isBdNode, 1, 1] = A[idxMap[isBdNode], 0, 0]*xe[:, 0]**2 \n    A[isBdNode, 1, 1]+= A[idxMap[isBdNode], 0, 1]*xe[:, 0]*2*c\n    A[isBdNode, 1, 1]+= A[idxMap[isBdNode], 1, 1]*c**2\n\n    A[isBdNode, 1, 2] = A[idxMap[isBdNode], 0, 0]*xe[:, 0]*xe[:, 1] \n    A[isBdNode, 1, 2]+= A[idxMap[isBdNode], 0, 1]*xe[:, 1]*c\n    A[isBdNode, 1, 2]+= A[idxMap[isBdNode], 0, 2]*xe[:, 0]*c\n    A[isBdNode, 1, 2]+= A[idxMap[isBdNode], 1, 2]*c**2\n    A[isBdNode, 2, 1] = A[isBdNode, 1, 2]\n\n    A[isBdNode, 2, 2] = A[idxMap[isBdNode], 0, 0]*xe[:, 1]**2\n    A[isBdNode, 2, 2]+= A[idxMap[isBdNode], 0, 2]*xe[:, 1]*2*c\n    A[isBdNode, 2, 2]+= A[idxMap[isBdNode], 2, 2]*c**2\n\n    b[isBdNode, 0] = b[idxMap[isBdNode], 0]\n\n    b[isBdNode, 1] = b[idxMap[isBdNode], 0]*xe[:, 0]\n    b[isBdNode, 1]+= b[idxMap[isBdNode], 1]*c\n\n    b[isBdNode, 2] = b[idxMap[isBdNode], 0]*xe[:, 1]\n    b[isBdNode, 2]+= b[idxMap[isBdNode], 2]*c\n\n    A = inv(A)\n    val = (A@b[:, :, None]).reshape(-1, 3)\n\n    space = LagrangeFiniteElementSpace(mesh, p=1)\n    ruh = space.function() \n    ruh[:] = val[:, 0]\n\n    return ruh\n\nparser = argparse.ArgumentParser(description=\n        \"\")\n\nparser.add_argument('--degree', \n        default=0, type=int,\n        help='第一类 Nedlec 元的次数, 默认为 0!')\n\nparser.add_argument('--size', \n        default=5, type=int,\n        help='初始网格的 x 和 y 方向剖分段数, 默认为 5 段')\n\nparser.add_argument('--maxit', \n        default=40, type=int,\n        help='自适应迭代次数, 默认自适应迭代 40 次')\n\nparser.add_argument('--theta', \n        default=0.3, type=float,\n        help='自适应迭代的 theta 参数, 默认为  0.3')\n\nparser.print_help()\nargs = parser.parse_args()\nprint('程序参数为:', args)\n\npde = CosSinData()\n\nbox = [-1, 1, -1, 1]\nmesh = MeshFactory.boxmesh2d(box, nx=args.size, ny=args.size, meshtype='tri') \n\nmesh.delete_cell(threshold=lambda x: (x[..., 0] > 0) & (x[..., 1] < 0)) \n\nerrorType = ['$|| u - u_h||_{\\Omega,0}$',\n             '$||\\\\nabla\\\\times u - \\\\nabla\\\\times u_h||_{\\Omega, 0}$',\n             '$||\\\\nabla\\\\times u - G(\\\\nabla\\\\times u_h)||_{\\Omega, 0}$',\n             ]\nerrorMatrix = np.zeros((len(errorType), args.maxit), dtype=np.float64)\nNDof = np.zeros(args.maxit, dtype=np.float64)\n\nfor i in range(args.maxit):\n    space = FirstKindNedelecFiniteElementSpace2d(mesh, p=args.degree)\n    bc = DirichletBC(space, pde.dirichlet) \n\n    gdof = space.number_of_global_dofs()\n    NDof[i] = gdof \n\n    uh = space.function()\n    A = space.curl_matrix() - space.mass_matrix()\n    F = space.source_vector(pde.source)\n\n    A, F = bc.apply(A, F, uh)\n\n    uh[:] = spsolve(A, F)\n\n    ruh = spr_curl(uh) \n\n    errorMatrix[0, i] = space.integralalg.L2_error(pde.solution, uh)\n    errorMatrix[1, i] = space.integralalg.L2_error(pde.curl, uh.curl_value)\n    errorMatrix[2, i] = space.integralalg.L2_error(pde.curl, ruh)\n    eta = space.integralalg.error(uh.curl_value, ruh, power=2, celltype=True) \n\n    if i < args.maxit - 1:\n        isMarkedCell = mark(eta, theta=args.theta)\n        mesh.bisect(isMarkedCell)\n        mesh.add_plot(plt)\n        plt.savefig('./test-' + str(i+1) + '.png')\n        plt.close()\n\nshowmultirate(plt, args.maxit-10, NDof, errorMatrix,  errorType, propsize=20)\nplt.show()",
  "\"\"\n\nfrom docopt import docopt\nfrom os import path\nimport sirf.STIR as pet\nimport sirf.Reg as reg\nfrom sirf.Utilities import error, show_3D_array, examples_data_path, existing_filepath\n\n__version__ = '0.1.0'\nargs = docopt(__doc__, version=__version__)\n\ndef check_file_exists(filename):\n    \"\"\n    if not path.isfile(filename):\n        raise error('File not found: %s' % filename)\n\ndata_path = args['--path']\nif data_path is None:\n    \n    data_path = examples_data_path('PET') + '/mMR'\nprint('Finding files in %s' % data_path)\n\nsino_file = existing_filepath(data_path, args['--sino'])\n\nattn_im_file = existing_filepath(data_path, args['--attn'])\n\nnorm_e8_file = existing_filepath(data_path, args['--norm'])\n\ntrans = args['--trans']\nif trans:\n    check_file_exists(trans)\ntrans_type = args['--trans_type']\n\noutp_file = args['--outp']\n\ndef resample_attn_image(image):\n    \"\"\n    if trans_type == 'tm':\n        transformation = reg.AffineTransformation(trans)\n    elif trans_type == 'disp':\n        transformation = reg.NiftiImageData3DDisplacement(trans)\n    elif trans_type == 'def':\n        transformation = reg.NiftiImageData3DDeformation(trans)\n    else:\n        raise ValueError(\"Unknown transformation type.\")\n\n    resampler = reg.NiftyResampler()\n    resampler.set_reference_image(image)\n    resampler.set_floating_image(image)\n    resampler.set_interpolation_type_to_linear()\n    resampler.set_padding_value(0.0)\n    resampler.add_transformation(transformation)\n    return resampler.forward(image)\n\ndef main():\n    \"\"\n    \n    acq_model = pet.AcquisitionModelUsingRayTracingMatrix()\n    acq_data = pet.AcquisitionData(sino_file)\n\n    asm_norm = None\n    if norm_e8_file:\n        \n        asm_norm = pet.AcquisitionSensitivityModel(norm_e8_file)\n\n    asm_attn = None\n    if attn_im_file:\n        attn_image = pet.ImageData(attn_im_file)\n        if trans:\n            attn_image = resample_attn_image(attn_image)\n        asm_attn = pet.AcquisitionSensitivityModel(attn_image, acq_model)\n        \n        asm_attn.set_up(acq_data)\n        bin_eff = pet.AcquisitionData(acq_data)\n        bin_eff.fill(1.0)\n        print('applying attenuation (please wait, may take a while)...')\n        asm_attn.unnormalise(bin_eff)\n        asm_attn = pet.AcquisitionSensitivityModel(bin_eff)\n\n    if asm_norm and asm_attn:\n        print(\"AcquisitionSensitivityModel contains norm and attenuation...\")\n        asm = pet.AcquisitionSensitivityModel(asm_norm, asm_attn)\n    elif asm_norm:\n        print(\"AcquisitionSensitivityModel contains norm...\")\n        asm = asm_norm\n    elif asm_attn:\n        print(\"AcquisitionSensitivityModel contains attenuation...\")\n        asm = asm_attn\n    else:\n        raise ValueError(\"Need norm and/or attn\")\n\n    if asm_norm:\n        asm_attn.set_up(acq_data)\n        bin_eff = pet.AcquisitionData(acq_data)\n        bin_eff.fill(1.0)\n        print('getting sinograms for multiplicative factors...')\n        asm.set_up(acq_data)\n        asm.unnormalise(bin_eff)\n\n    print('writing multiplicative sinogram: ' + outp_file)\n    bin_eff.write(outp_file)\n\nif __name__ == \"__main__\":\n    main()",
  "\n\"\"\n\nimport os\n\nimport dolfin\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport bluemira.geometry.tools as tools\nimport bluemira.magnetostatics.greens as greens\nfrom bluemira.base.components import Component, PhysicalComponent\nfrom bluemira.base.file import get_bluemira_path\nfrom bluemira.geometry.face import BluemiraFace\nfrom bluemira.magnetostatics.finite_element_2d import (\n    Bz_coil_axis,\n    FemMagnetostatic2d,\n    ScalarSubFunc,\n)\nfrom bluemira.mesh import meshing\nfrom bluemira.mesh.tools import import_mesh, msh_to_xdmf\n\nr_enclo = 100\nlcar_enclo = 1.0\n\nrc = 5\ndrc = 0.025\nlcar_coil = 0.05\n\npoly_coil = tools.make_polygon(\n    [[rc - drc, rc + drc, rc + drc, rc - drc], [0, 0, 0, 0], [-drc, -drc, +drc, +drc]],\n    closed=True,\n    label=\"poly_enclo\",\n)\n\npoly_coil.mesh_options = {\"lcar\": lcar_coil, \"physical_group\": \"poly_coil\"}\ncoil = BluemiraFace(poly_coil)\ncoil.mesh_options = {\"lcar\": lcar_coil, \"physical_group\": \"coil\"}\n\npoly_enclo = tools.make_polygon(\n    [[0, r_enclo, r_enclo, 0], [0, 0, 0, 0], [-r_enclo, -r_enclo, r_enclo, r_enclo]],\n    closed=True,\n    label=\"poly_enclo\",\n)\n\npoly_enclo.mesh_options = {\"lcar\": lcar_enclo, \"physical_group\": \"poly_enclo\"}\nenclosure = BluemiraFace([poly_enclo, poly_coil])\nenclosure.mesh_options = {\"lcar\": lcar_enclo, \"physical_group\": \"enclo\"}\n\nc_universe = Component(name=\"universe\")\nc_enclo = PhysicalComponent(name=\"enclosure\", shape=enclosure, parent=c_universe)\nc_coil = PhysicalComponent(name=\"coil\", shape=coil, parent=c_universe)\n\ndirectory = get_bluemira_path(\"\", subfolder=\"generated_data\")\nmeshfiles = [os.path.join(directory, p) for p in [\"Mesh.geo_unrolled\", \"Mesh.msh\"]]\n\nmeshing.Mesh(meshfile=meshfiles)(c_universe, dim=2)\n\nmsh_to_xdmf(\"Mesh.msh\", dimensions=(0, 2), directory=directory)\n\nmesh, boundaries, subdomains, labels = import_mesh(\n    \"Mesh\",\n    directory=directory,\n    subdomains=True,\n)\ndolfin.plot(mesh)\nplt.show()\n\nem_solver = FemMagnetostatic2d(2)\nem_solver.set_mesh(mesh, boundaries)\n\nIc = 1e6\njc = Ic / coil.area\nmarkers = [labels[\"coil\"]]\nfunctions = [jc]\njtot = ScalarSubFunc(functions, markers, subdomains)\n\nf_space = dolfin.FunctionSpace(mesh, \"DG\", 0)\nf = dolfin.Function(f_space)\nf.interpolate(jtot)\ndolfin.plot(f, title=\"Source term\")\nplt.show()\n\nem_solver.define_g(jtot)\nem_solver.solve()\nem_solver.calculate_b()\n\nz_points_axis = np.linspace(0, r_enclo, 200)\nr_points_axis = np.zeros(z_points_axis.shape)\nBz_axis = np.array(\n    [em_solver.B(x) for x in np.array([r_points_axis, z_points_axis]).T]\n).T[1]\nB_teo = np.array([Bz_coil_axis(rc, 0, z, Ic) for z in z_points_axis])\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, Bz_axis, label=\"B_calc\")\nax.plot(z_points_axis, B_teo, label=\"B_teo\")\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"B (T)\")\nplt.legend()\nplt.show()\n\ndiff = Bz_axis - B_teo\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, diff, label=\"B_calc - B_teo\")\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"error (T)\")\nplt.legend()\nplt.show()\n\nz_offset = 40 * drc\n\npoints_x = np.linspace(0, r_enclo, 200)\npoints_z = np.zeros(z_points_axis.shape) + z_offset\n\ng_psi, g_bx, g_bz = greens.greens_all(rc, 0, points_x, points_z)\ng_psi *= Ic\ng_bx *= Ic\ng_bz *= Ic\nB_fem = np.array([em_solver.B(x) for x in np.array([points_x, points_z]).T])\nBx_fem = B_fem.T[0]\nBz_fem = B_fem.T[1]\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, Bx_fem, label=\"Bx_fem\")\nax.plot(z_points_axis, g_bx, label=\"Green Bx\")\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"Bx (T)\")\nplt.legend()\nplt.show()\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, Bz_fem, label=\"Bz_fem\")\nax.plot(z_points_axis, g_bz, label=\"Green Bz\")\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"Bz (T)\")\nplt.legend()\nplt.show()\n\ndiff1 = Bx_fem - g_bx\ndiff2 = Bz_fem - g_bz\n\nfig, ax = plt.subplots()\nax.plot(z_points_axis, diff1, label=\"B_calc - GreenBx\")\nax.plot(z_points_axis, diff2, label=\"B_calc - GreenBz\")\nplt.legend()\nplt.xlabel(\"r (m)\")\nplt.ylabel(\"error (T)\")\nplt.show()",
  "\n\"\"\n\n__authors__ = [\"V. Valls\"]\n__license__ = \"MIT\"\n__date__ = \"02/08/2018\"\n\nimport sys\nimport functools\nimport numpy\n\nfrom silx.gui import qt\nfrom silx.gui.colors import Colormap\nfrom silx.gui.widgets.WaitingPushButton import WaitingPushButton\nfrom silx.gui.widgets.ThreadPoolPushButton import ThreadPoolPushButton\nfrom silx.gui.widgets.RangeSlider import RangeSlider\nfrom silx.gui.widgets.LegendIconWidget import LegendIconWidget\nfrom silx.gui.widgets.ElidedLabel import ElidedLabel\n\nclass SimpleWidgetExample(qt.QMainWindow):\n    \"\"\n\n    def __init__(self):\n        \"\"\n        qt.QMainWindow.__init__(self)\n        self.setWindowTitle(\"Silx simple widget example\")\n\n        main_panel = qt.QWidget(self)\n        main_panel.setLayout(qt.QVBoxLayout())\n\n        layout = main_panel.layout()\n        layout.addWidget(qt.QLabel(\"WaitingPushButton\"))\n        layout.addWidget(self.createWaitingPushButton())\n        layout.addWidget(self.createWaitingPushButton2())\n\n        layout.addWidget(qt.QLabel(\"ThreadPoolPushButton\"))\n        layout.addWidget(self.createThreadPoolPushButton())\n\n        layout.addWidget(qt.QLabel(\"RangeSlider\"))\n        layout.addWidget(self.createRangeSlider())\n        layout.addWidget(self.createRangeSliderWithBackground())\n\n        panel = self.createLegendIconPanel(self)\n        layout.addWidget(qt.QLabel(\"LegendIconWidget\"))\n        layout.addWidget(panel)\n\n        panel = self.createElidedLabelPanel(self)\n        layout.addWidget(qt.QLabel(\"ElidedLabel\"))\n        layout.addWidget(panel)\n\n        self.setCentralWidget(main_panel)\n\n    def createWaitingPushButton(self):\n        widget = WaitingPushButton(text=\"Push me and wait for ever\")\n        widget.clicked.connect(widget.swapWaiting)\n        return widget\n\n    def createWaitingPushButton2(self):\n        widget = WaitingPushButton(text=\"Push me\")\n        widget.setDisabledWhenWaiting(False)\n        widget.clicked.connect(widget.swapWaiting)\n        return widget\n\n    def printResult(self, result):\n        print(result)\n\n    def printError(self, result):\n        print(\"Error\")\n        print(result)\n\n    def printEvent(self, eventName, *args):\n        print(\"Event %s: %s\" % (eventName, args))\n\n    def takesTimeToComputePow(self, a, b):\n        qt.QThread.sleep(2)\n        return a ** b\n\n    def createThreadPoolPushButton(self):\n        widget = ThreadPoolPushButton(text=\"Compute 2^16\")\n        widget.setCallable(self.takesTimeToComputePow, 2, 16)\n        widget.succeeded.connect(self.printResult)\n        widget.failed.connect(self.printError)\n        return widget\n\n    def createRangeSlider(self):\n        widget = RangeSlider(self)\n        widget.setRange(0, 500)\n        widget.setValues(100, 400)\n        widget.sigValueChanged.connect(functools.partial(self.printEvent, \"sigValueChanged\"))\n        widget.sigPositionChanged.connect(functools.partial(self.printEvent, \"sigPositionChanged\"))\n        widget.sigPositionCountChanged.connect(functools.partial(self.printEvent, \"sigPositionCountChanged\"))\n        return widget\n\n    def createRangeSliderWithBackground(self):\n        widget = RangeSlider(self)\n        widget.setRange(0, 500)\n        widget.setValues(100, 400)\n        background = numpy.sin(numpy.arange(250) / 250.0)\n        background[0], background[-1] = background[-1], background[0]\n        colormap = Colormap(\"viridis\")\n        widget.setGroovePixmapFromProfile(background, colormap)\n        return widget\n\n    def createLegendIconPanel(self, parent):\n        panel = qt.QWidget(parent)\n        layout = qt.QVBoxLayout(panel)\n\n        legend = LegendIconWidget(panel)\n        layout.addWidget(legend)\n\n        legend = LegendIconWidget(panel)\n        legend.setLineStyle(\"-\")\n        legend.setLineColor(\"blue\")\n        legend.setLineWidth(2)\n        layout.addWidget(legend)\n\n        legend = LegendIconWidget(panel)\n        legend.setSymbol(\"o\")\n        legend.setSymbolColor(\"red\")\n        layout.addWidget(legend)\n\n        legend = LegendIconWidget(panel)\n        legend.setLineStyle(\":\")\n        legend.setLineColor(\"green\")\n        legend.setLineWidth(2)\n        legend.setSymbol(\"x\")\n        legend.setSymbolColor(\"violet\")\n        layout.addWidget(legend)\n\n        legend = LegendIconWidget(panel)\n        legend.setColormap(\"viridis\")\n        layout.addWidget(legend)\n\n        legend = LegendIconWidget(panel)\n        legend.setSymbol(\"o\")\n        legend.setSymbolColormap(\"viridis\")\n        layout.addWidget(legend)\n\n        legend = LegendIconWidget(panel)\n        legend.setSymbol(\"+\")\n        legend.setSymbolColormap(\"plasma\")\n        layout.addWidget(legend)\n\n        legend = LegendIconWidget(panel)\n        legend.setColormap(\"gray\")\n        legend.setLineStyle(\"-\")\n        legend.setLineColor(\"white\")\n        legend.setLineWidth(3)\n        legend.setSymbol(\".\")\n        legend.setSymbolColormap(\"red\")\n        layout.addWidget(legend)\n\n        return panel\n\n    def createElidedLabelPanel(self, parent):\n        panel = qt.QWidget(parent)\n        layout = qt.QVBoxLayout(panel)\n\n        label = ElidedLabel(parent)\n        label.setText(\"A very long text which is far too long.\")\n        layout.addWidget(label)\n\n        label = ElidedLabel(parent)\n        label.setText(\"A very long text which is far too long.\")\n        label.setElideMode(qt.Qt.ElideMiddle)\n        layout.addWidget(label)\n\n        label = ElidedLabel(parent)\n        label.setText(\"Basically nothing.\")\n        layout.addWidget(label)\n\n        return panel\n\ndef main():\n    \"\"\n    app = qt.QApplication([])\n    sys.excepthook = qt.exceptionHandler\n    window = SimpleWidgetExample()\n    window.show()\n    result = app.exec()\n    \n    app.deleteLater()\n    sys.excepthook = sys.__excepthook__\n    sys.exit(result)\n\nmain()",
  "\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nimport DREAM.Settings.Equations.DistributionFunction as DistFunc\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Equations.RunawayElectrons as Runaways\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\n\nds = DREAMSettings()\n\nds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_PARTIALLY_SCREENED\n\nE = 6       \nn = 5e19    \nT = 100     \n\npMax = 1    \nNp   = 300  \nNxi  = 20   \ntMax = 1e-3 \nNt   = 20   \n\nds.eqsys.E_field.setPrescribedData(E)\n\nds.eqsys.T_cold.setPrescribedData(T)\n\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=n)\n\nds.eqsys.n_re.setAvalanche(avalanche=Runaways.AVALANCHE_MODE_NEGLECT)\n\nds.hottailgrid.setNxi(Nxi)\nds.hottailgrid.setNp(Np)\nds.hottailgrid.setPmax(pMax)\n\nds.eqsys.f_hot.setInitialProfiles(n0=n, T0=T)\n\nds.eqsys.f_hot.setBoundaryCondition(DistFunc.BC_F_0) \nds.eqsys.f_hot.setSynchrotronMode(DistFunc.SYNCHROTRON_MODE_NEGLECT)\nds.eqsys.f_hot.setAdvectionInterpolationMethod(DistFunc.AD_INTERP_UPWIND)\n\nds.runawaygrid.setEnabled(False)\n\nds.radialgrid.setB0(5)\nds.radialgrid.setMinorRadius(0.22)\nds.radialgrid.setWallRadius(0.22)\nds.radialgrid.setNr(1)\n\nds.solver.setType(Solver.LINEAR_IMPLICIT) \nds.solver.preconditioner.setEnabled(False)\n\nds.other.include('fluid','nu_s','nu_D')\n\nds.timestep.setTmax(tMax)\nds.timestep.setNt(Nt)\n\nds.output.setTiming(stdout=True, file=True)\nds.output.setFilename('output.h5')\n\nds.save('dream_settings.h5')\n",
  "\nr\"\"\n\nimport inspect\nimport os\nimport shutil\n\nfilename = inspect.getframeinfo(inspect.currentframe()).filename\nfileNameString = os.path.basename(os.path.splitext(__file__)[0])\npath = os.path.dirname(os.path.abspath(filename))\n\nfrom Basilisk import __path__\nbskPath = __path__[0]\n\nfrom Basilisk.utilities import SimulationBaseClass\nfrom Basilisk.utilities import macros\nfrom Basilisk.topLevelModules import pyswice\nfrom Basilisk.utilities.pyswice_spk_utilities import spkRead\n\nfrom Basilisk.simulation import spacecraft\n\nfrom Basilisk.utilities.MonteCarlo.Controller import Controller\n\nclass MyController(Controller):\n    def __init__(self):  \n        Controller.__init__(self)\n\nclass MySimulation(SimulationBaseClass.SimBaseClass):\n    def __init__(self):\n        SimulationBaseClass.SimBaseClass.__init__(self)\n        \n        simTaskName = \"simTask\"\n        simProcessName = \"simProcess\"\n\n        self.dynProcess = self.CreateNewProcess(simProcessName)\n\n        self.dynProcess.addTask(self.CreateNewTask(simTaskName, macros.sec2nano(10.)))\n\n        scObject = spacecraft.Spacecraft()\n        self.AddModelToTask(simTaskName, scObject, 1)\n        scObject.hub.r_CN_NInit = [7000000.0, 0.0, 0.0]     \n        scObject.hub.v_CN_NInit = [0.0, 7500.0, 0.0]        \n\n        dataPath = bskPath + \"/supportData/EphemerisData/\"\n        self.scSpiceName = 'HUBBLE SPACE TELESCOPE'\n        pyswice.furnsh_c(dataPath + 'naif0011.tls')\n        pyswice.furnsh_c(dataPath + 'pck00010.tpc')\n        pyswice.furnsh_c(dataPath + 'de-403-masses.tpc')\n        pyswice.furnsh_c(dataPath + 'de430.bsp')\n        pyswice.furnsh_c(dataPath + 'hst_edited.bsp')\n\n        self.accessSpiceKernel()\n\n        self.additionalReferences = [scObject]\n\n    def accessSpiceKernel(self):\n        startCalendarTime = '2012 APR 29 15:18:14.907 (UTC)'\n        zeroBase = 'Sun'\n        integFrame = 'j2000'\n        stateOut = spkRead(self.scSpiceName, startCalendarTime, integFrame, zeroBase)\n        print(stateOut)\n\ndef run():\n    \"\"\n\n    monteCarlo = MyController()\n    monteCarlo.setSimulationFunction(MySimulation)\n    monteCarlo.setExecutionFunction(executeScenario)\n    monteCarlo.setExecutionCount(12)\n    monteCarlo.setShouldDisperseSeeds(True)\n    monteCarlo.setThreadCount(6)\n    monteCarlo.setVerbose(False)\n\n    dirName = \"montecarlo_test\" + str(os.getpid())\n    monteCarlo.setArchiveDir(dirName)\n\n    failures = monteCarlo.executeSimulations()\n\n    shutil.rmtree(dirName)\n\n    return\n\ndef executeScenario(sim):\n    sim.ConfigureStopTime(macros.sec2nano(100.))\n    sim.InitializeSimulation()\n\n    sim.ExecuteSimulation()\n\nif __name__ == \"__main__\":\n    run()",
  "\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rlberry.envs.bandits import BernoulliBandit\nfrom rlberry.manager import ExperimentManager, plot_writer_data\nfrom rlberry.wrappers import WriterWrapper\nfrom rlberry.agents.bandits import (\n    IndexAgent,\n    RandomizedAgent,\n    makeBoundedIMEDIndex,\n    makeBoundedMOSSIndex,\n    makeBoundedNPTSIndex,\n    makeBoundedUCBIndex,\n    makeBoundedUCBVIndex,\n    makeETCIndex,\n    makeEXP3Index,\n)\n\nmeans = np.array([0.6, 0.6, 0.6, 0.9])  \nA = len(means)\nT = 2000  \nM = 10  \n\nenv_ctor = BernoulliBandit\nenv_kwargs = {\"p\": means}\n\nclass UCBAgent(IndexAgent):\n    name = \"UCB\"\n\n    def __init__(self, env, **kwargs):\n        index, _ = makeBoundedUCBIndex()\n        IndexAgent.__init__(self, env, index, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\nclass UCBVAgent(IndexAgent):\n    name = \"UCBV\"\n\n    def __init__(self, env, **kwargs):\n        index, params = makeBoundedUCBVIndex()\n        IndexAgent.__init__(self, env, index, tracker_params=params, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\nclass ETCAgent(IndexAgent):\n    name = \"ETC\"\n\n    def __init__(self, env, m=20, **kwargs):\n        index, _ = makeETCIndex(A, m)\n        IndexAgent.__init__(self, env, index, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\nclass MOSSAgent(IndexAgent):\n    name = \"MOSS\"\n\n    def __init__(self, env, **kwargs):\n        index, _ = makeBoundedMOSSIndex(T, A)\n        IndexAgent.__init__(self, env, index, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\nclass IMEDAgent(IndexAgent):\n    name = \"IMED\"\n\n    def __init__(self, env, **kwargs):\n        index, tracker_params = makeBoundedIMEDIndex()\n        IndexAgent.__init__(self, env, index, tracker_params=tracker_params, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\nclass NPTSAgent(IndexAgent):\n    name = \"NPTS\"\n\n    def __init__(self, env, **kwargs):\n        index, tracker_params = makeBoundedNPTSIndex()\n        IndexAgent.__init__(self, env, index, tracker_params=tracker_params, **kwargs)\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\nclass EXP3Agent(RandomizedAgent):\n    name = \"EXP3\"\n\n    def __init__(self, env, **kwargs):\n        prob, tracker_params = makeEXP3Index()\n        RandomizedAgent.__init__(\n            self, env, prob, tracker_params=tracker_params, **kwargs\n        )\n        self.env = WriterWrapper(\n            self.env, self.writer, write_scalar=\"action_and_reward\"\n        )\n\nAgents_class = [\n    ETCAgent,\n    EXP3Agent,\n    IMEDAgent,\n    MOSSAgent,\n    NPTSAgent,\n    UCBAgent,\n    UCBVAgent,\n]\n\nagents = [\n    ExperimentManager(\n        Agent,\n        (env_ctor, env_kwargs),\n        fit_budget=T,\n        n_fit=M,\n        parallelization=\"process\",\n        mp_context=\"fork\",\n    )\n    for Agent in Agents_class\n]\n\nfor agent in agents:\n    agent.fit()\n\ndef compute_regret(rewards):\n    return np.cumsum(np.max(means) - rewards)\n\ndef compute_pseudo_regret(actions):\n    return np.cumsum(np.max(means) - means[actions.astype(int)])\n\noutput = plot_writer_data(\n    agents,\n    tag=\"action\",\n    preprocess_func=compute_pseudo_regret,\n    title=\"Cumulative Pseudo-Regret\",\n    sns_kwargs={\"style\": \"name\"},  \n)\n\noutput = plot_writer_data(\n    agents,\n    tag=\"reward\",\n    preprocess_func=compute_regret,\n    title=\"Cumulative Regret\",\n    sns_kwargs={\"style\": \"name\"},  \n)\n\ndef compute_na(actions, a):\n    return np.cumsum(actions == a)\n\nfig, axes = plt.subplots(2, 2, sharey=True, figsize=(6, 6))\naxes = axes.ravel()\nfor arm in range(A):\n    output = plot_writer_data(\n        agents,\n        tag=\"action\",\n        preprocess_func=lambda actions: compute_na(actions, arm),\n        title=\"Na for arm \" + str(arm) + \", mean=\" + str(means[arm]),\n        ax=axes[arm],\n        show=False,\n        sns_kwargs={\"style\": \"name\"},  \n    )\nfig.tight_layout()\nplt.show()",
  "import numpy as np\nfrom loguru import logger\n\nfrom sc2 import maps\nfrom sc2.bot_ai import BotAI\nfrom sc2.data import Difficulty, Race, Result\nfrom sc2.ids.ability_id import AbilityId\nfrom sc2.ids.buff_id import BuffId\nfrom sc2.ids.unit_typeid import UnitTypeId\nfrom sc2.ids.upgrade_id import UpgradeId\nfrom sc2.main import run_game\nfrom sc2.player import Bot, Computer\nfrom sc2.position import Point2, Point3\nfrom sc2.unit import Unit\nfrom sc2.units import Units\n\nclass ZergRushBot(BotAI):\n\n    def __init__(self):\n        self.on_end_called = False\n\n    async def on_start(self):\n        self.client.game_step = 2\n\n    async def on_step(self, iteration):\n        if iteration == 0:\n            await self.chat_send(\"(glhf)\")\n\n        if not self.townhalls:\n            for unit in self.units.exclude_type({UnitTypeId.EGG, UnitTypeId.LARVA}):\n                unit.attack(self.enemy_start_locations[0])\n            return\n\n        hatch: Unit = self.townhalls[0]\n\n        target: Point2 = self.enemy_structures.not_flying.random_or(self.enemy_start_locations[0]).position\n\n        for zergling in self.units(UnitTypeId.ZERGLING):\n            zergling.attack(target)\n\n        for queen in self.units(UnitTypeId.QUEEN):\n            if queen.energy >= 25 and not hatch.has_buff(BuffId.QUEENSPAWNLARVATIMER):\n                queen(AbilityId.EFFECT_INJECTLARVA, hatch)\n\n        if self.vespene >= 88 or self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) > 0:\n            gas_drones: Units = self.workers.filter(lambda w: w.is_carrying_vespene and len(w.orders) < 2)\n            drone: Unit\n            for drone in gas_drones:\n                minerals: Units = self.mineral_field.closer_than(10, hatch)\n                if minerals:\n                    mineral: Unit = minerals.closest_to(drone)\n                    drone.gather(mineral, queue=True)\n\n        if self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED\n                                        ) == 0 and self.can_afford(UpgradeId.ZERGLINGMOVEMENTSPEED):\n            spawning_pools_ready: Units = self.structures(UnitTypeId.SPAWNINGPOOL).ready\n            if spawning_pools_ready:\n                self.research(UpgradeId.ZERGLINGMOVEMENTSPEED)\n\n        if self.supply_left < 2 and self.already_pending(UnitTypeId.OVERLORD) < 1:\n            self.train(UnitTypeId.OVERLORD, 1)\n\n        if (\n            self.gas_buildings.ready and self.vespene < 88\n            and self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) == 0\n        ):\n            extractor: Unit = self.gas_buildings.first\n            if extractor.surplus_harvesters < 0:\n                self.workers.random.gather(extractor)\n\n        if self.minerals > 500:\n            for d in range(4, 15):\n                pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                if await self.can_place_single(UnitTypeId.HATCHERY, pos):\n                    self.workers.random.build(UnitTypeId.HATCHERY, pos)\n                    break\n\n        if self.can_afford(UnitTypeId.DRONE) and self.supply_workers < 16:\n            self.train(UnitTypeId.DRONE)\n\n        if self.structures(UnitTypeId.SPAWNINGPOOL).ready and self.larva and self.can_afford(UnitTypeId.ZERGLING):\n            _amount_trained: int = self.train(UnitTypeId.ZERGLING, self.larva.amount)\n\n        if (\n            self.gas_buildings.amount + self.already_pending(UnitTypeId.EXTRACTOR) == 0\n            and self.can_afford(UnitTypeId.EXTRACTOR) and self.workers\n        ):\n            drone: Unit = self.workers.random\n            target: Unit = self.vespene_geyser.closest_to(drone)\n            drone.build_gas(target)\n\n        elif self.structures(UnitTypeId.SPAWNINGPOOL).amount + self.already_pending(UnitTypeId.SPAWNINGPOOL) == 0:\n            if self.can_afford(UnitTypeId.SPAWNINGPOOL):\n                for d in range(4, 15):\n                    pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                    if await self.can_place_single(UnitTypeId.SPAWNINGPOOL, pos):\n                        drone: Unit = self.workers.closest_to(pos)\n                        drone.build(UnitTypeId.SPAWNINGPOOL, pos)\n\n        elif (\n            self.units(UnitTypeId.QUEEN).amount + self.already_pending(UnitTypeId.QUEEN) < self.townhalls.amount\n            and self.structures(UnitTypeId.SPAWNINGPOOL).ready\n        ):\n            if self.can_afford(UnitTypeId.QUEEN):\n                self.train(UnitTypeId.QUEEN)\n\n    def draw_creep_pixelmap(self):\n        for (y, x), value in np.ndenumerate(self.state.creep.data_numpy):\n            p = Point2((x, y))\n            h2 = self.get_terrain_z_height(p)\n            pos = Point3((p.x, p.y, h2))\n            \n            color = Point3((255, 0, 0))\n            if value == 1:\n                \n                color = Point3((0, 255, 0))\n            self.client.debug_box2_out(pos, half_vertex_length=0.25, color=color)\n\n    async def on_end(self, game_result: Result):\n        self.on_end_called = True\n        logger.info(f\"{self.time_formatted} On end was called\")\n\ndef main():\n    run_game(\n        maps.get(\"AcropolisLE\"),\n        [Bot(Race.Zerg, ZergRushBot()), Computer(Race.Terran, Difficulty.Medium)],\n        realtime=False,\n        save_replay_as=\"ZvT.SC2Replay\",\n    )\n\nif __name__ == \"__main__\":\n    main()",
  "from kivy.uix.gridlayout import GridLayout\nfrom kivy.uix.button import Button\nfrom kivy.uix.behaviors import CompoundSelectionBehavior\nfrom kivy.uix.behaviors import FocusBehavior\nfrom kivy.app import runTouchApp\n\nclass SelectableGrid(FocusBehavior, CompoundSelectionBehavior, GridLayout):\n\n    def __init__(self, **kwargs):\n        super(SelectableGrid, self).__init__(**kwargs)\n\n        def print_selection(*l):\n            print('selected: ', [x.text for x in self.selected_nodes])\n        self.bind(selected_nodes=print_selection)\n\n    def keyboard_on_key_down(self, window, keycode, text, modifiers):\n        if super(SelectableGrid, self).keyboard_on_key_down(\n                window, keycode, text, modifiers):\n            return True\n        if self.select_with_key_down(window, keycode, text, modifiers):\n            return True\n        return False\n\n    def keyboard_on_key_up(self, window, keycode):\n        if super(SelectableGrid, self).keyboard_on_key_up(window, keycode):\n            return True\n        if self.select_with_key_up(window, keycode):\n            return True\n        return False\n\n    def goto_node(self, key, last_node, last_node_idx):\n        \"\"\n        node, idx = super(SelectableGrid, self).goto_node(key, last_node,\n                                                          last_node_idx)\n        if node != last_node:\n            return node, idx\n\n        items = list(enumerate(self.get_selectable_nodes()))\n        \"\"\n        \n        if not self.nodes_order_reversed:\n            items = items[last_node_idx + 1:] + items[:last_node_idx + 1]\n        else:\n            items = items[:last_node_idx][::-1] + items[last_node_idx:][::-1]\n\n        for i, child in items:\n            if child.text.startswith(key):\n                return child, i\n        return node, idx\n\n    def select_node(self, node):\n        node.background_color = (1, 0, 0, 1)\n        return super(SelectableGrid, self).select_node(node)\n\n    def deselect_node(self, node):\n        node.background_color = (1, 1, 1, 1)\n        super(SelectableGrid, self).deselect_node(node)\n\n    def do_touch(self, instance, touch):\n        if ('button' in touch.profile and touch.button in\n                ('scrollup', 'scrolldown', 'scrollleft', 'scrollright')) or\\\n                instance.collide_point(*touch.pos):\n            self.select_with_touch(instance, touch)\n        else:\n            return False\n        return True\n\nroot = SelectableGrid(cols=5, up_count=5, multiselect=True, scroll_count=1)\nfor i in range(40):\n    c = Button(text=str(i))\n    c.bind(on_touch_down=root.do_touch)\n    root.add_widget(c)\n\nrunTouchApp(root)",
  "import argparse\nimport sys\nimport numpy as np\nimport matplotlib\n\nfrom scipy.sparse import spdiags, bmat\nfrom scipy.sparse.linalg import spsolve\nfrom scipy.sparse import csr_matrix,hstack,vstack,spdiags,bmat\nfrom mumps import DMumpsContext\n\nimport matplotlib.pyplot as plt\nfrom fealpy.functionspace import LagrangeFiniteElementSpace\nfrom fealpy.boundarycondition import DirichletBC \nfrom fealpy.timeintegratoralg import UniformTimeLine\n\nfrom navier_stokes_mold_2d import Poisuille as PDE\n\nfrom fealpy.mesh import TriangleMesh\nfrom fealpy.functionspace import LagrangeFESpace\n\nfrom fealpy.fem import ScalarDiffusionIntegrator, VectorMassIntegrator\nfrom fealpy.fem import VectorDiffusionIntegrator\nfrom fealpy.fem import VectorViscousWorkIntegrator, PressWorkIntegrator\nfrom fealpy.fem import ScalarConvectionIntegrator\nfrom fealpy.fem import BilinearForm, MixedBilinearForm\nfrom fealpy.fem import LinearForm\nfrom fealpy.fem import VectorSourceIntegrator, ScalarSourceIntegrator\nfrom fealpy.fem import DirichletBC\n\nparser = argparse.ArgumentParser(description=\n        \"\")\n\nparser.add_argument('--udegree',\n        default=2, type=int,\n        help='运动有限元空间的次数, 默认为 2 次.')\n\nparser.add_argument('--pdegree',\n        default=1, type=int,\n        help='压力有限元空间的次数, 默认为 1 次.')\n\nparser.add_argument('--nt',\n        default=100, type=int,\n        help='时间剖分段数，默认剖分 5000 段.')\n\nparser.add_argument('--T',\n        default=10, type=float,\n        help='演化终止时间, 默认为 5')\n\nparser.add_argument('--output',\n        default='./', type=str,\n        help='结果输出目录, 默认为 ./')\n\nparser.add_argument('--step',\n        default=10, type=int,\n        help='隔多少步输出一次')\n\nparser.add_argument('--method',\n        default='Netwon', type=str,\n        help='非线性化方法')\n\nargs = parser.parse_args()\nudegree = args.udegree\npdegree = args.pdegree\nnt = args.nt\nT = args.T\noutput = args.output\nstep = args.step\nmethod = args.method\nns = 8\n\nmu= 1\nrho = 1\nudim = 2\ndoforder = 'sdofs'\n\npde = PDE()\nmesh = TriangleMesh.from_unit_square(nx=ns, ny=ns)\nsmesh = TriangleMesh.from_unit_square(nx=ns, ny=ns)\ntmesh = UniformTimeLine(0,T,nt)\ndt = tmesh.dt\nuspace = LagrangeFiniteElementSpace(smesh,p=udegree)\npspace = LagrangeFiniteElementSpace(smesh,p=pdegree)\nnuspace = LagrangeFESpace(mesh,p=2,doforder=doforder)\nnpspace = LagrangeFESpace(mesh,p=1,doforder=doforder)\n\nu0 = uspace.function(dim=udim)\nu1 = uspace.function(dim=udim)\n\np1 = pspace.function()\n\nugdof = uspace.number_of_global_dofs()\npgdof = pspace.number_of_global_dofs()\n\nugdof = uspace.number_of_global_dofs()\npgdof = pspace.number_of_global_dofs()\ngdof = pgdof+2*ugdof\n\nVbform0 = BilinearForm(nuspace)\nVbform0.add_domain_integrator(ScalarDiffusionIntegrator())\nVbform0.assembly()\nA = Vbform0.get_matrix()\n\nVbform1 = MixedBilinearForm((npspace,), 2*(nuspace, ))\nVbform1.add_domain_integrator(PressWorkIntegrator()) \nVbform1.assembly()\nB = Vbform1.get_matrix()\nB1 = B[:B.shape[0]//2,:]\nB2 = B[B.shape[0]//2:,:]\n\nE = (1/dt)*uspace.mass_matrix()\n\nerrorMatrix = np.zeros((4,nt),dtype=np.float64)\n\nfor i in range(0,3):\n\n    t1 = tmesh.next_time_level()\n    print(\"t1=\",t1)\n\n    Vbform2 = BilinearForm(nuspace)\n    Vbform2.add_domain_integrator(ScalarConvectionIntegrator(c=u0)) \n    Vbform2.assembly()\n    DD = Vbform2.get_matrix()\n    \n    D1,D2 = uspace.div_matrix(uspace)\n    D = D1 * np.broadcast_to(u0[...,0],D1.shape)+\\\n        D2 * np.broadcast_to(u0[...,1],D1.shape) \n    \n    print(\"asd\",np.abs(D1).sum())\n    print(\"asd\",np.abs(D).sum())\n    print(np.sum(np.abs(D-DD))) \n    M = bmat([[E+A+D,None,-B1],[None,E+A+D,-B2],[-B1.T,-B2.T,None]],format='csr')\n    \"\"\n    \n    F = uspace.source_vector(pde.source,dim=udim) + E@u0\n    FF = np.r_['0', F.T.flat, np.zeros(pgdof)]\n    \"\"\n    u_isBdDof = uspace.is_boundary_dof()\n    \n    p_isBdDof = pspace.is_boundary_dof(threshold=pde.is_p_boundary)\n    \n    x = np.zeros(gdof,np.float64)\n    ipoint = uspace.interpolation_points()\n    uso = pde.u_dirichlet(ipoint)\n    x[0:ugdof][u_isBdDof] = uso[:,0][u_isBdDof]\n    x[ugdof:2*ugdof][u_isBdDof] = uso[u_isBdDof][:,1]\n    ipoint = pspace.interpolation_points()\n    pso = pde.p_dirichlet(ipoint)\n    x[-pgdof:][p_isBdDof] = pso[p_isBdDof]\n\n    isBdDof = np.hstack([u_isBdDof, u_isBdDof, p_isBdDof])\n    \n    FF -= M@x\n    bdIdx = np.zeros(gdof, dtype=np.int_)\n    bdIdx[isBdDof] = 1\n    Tbd = spdiags(bdIdx, 0, gdof, gdof)\n    T = spdiags(1-bdIdx, 0, gdof, gdof)\n    M = T@M@T + Tbd\n    FF[isBdDof] = x[isBdDof]\n\n    x[:] = spsolve(M, FF)\n    u1[:, 0] = x[:ugdof]\n    u1[:, 1] = x[ugdof:2*ugdof]\n    p1[:] = x[2*ugdof:]\n    \n    uc1 = pde.velocity(smesh.node)\n    NN = smesh.number_of_nodes()\n    uc2 = u1[:NN]\n    up1 = pde.pressure(smesh.node)\n    up2 = p1[:NN]\n   \n    errorMatrix[0,i] = uspace.integralalg.L2_error(pde.velocity,u1)\n    errorMatrix[1,i] = pspace.integralalg.error(pde.pressure,p1)\n    errorMatrix[2,i] = np.abs(uc1-uc2).max()\n    errorMatrix[3,i] = np.abs(up1-up2).max()\n\n    u0[:] = u1 \n\n    tmesh.advance()\nprint(np.sum(np.abs(u1)))\n\"\"",
  "\nr\"\"\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom scipy.optimize import minimize\n\nfrom simsopt.field import BiotSavart, Current, coils_via_symmetries\nfrom simsopt.geo import (SurfaceRZFourier, curves_to_vtk, create_equally_spaced_curves,\n                         CurveLength, CurveCurveDistance, MeanSquaredCurvature,\n                         LpCurveCurvature, CurveSurfaceDistance)\nfrom simsopt.objectives import Weight, SquaredFlux, QuadraticPenalty\nfrom simsopt.util import in_github_actions\n\nncoils = 4\n\nR0 = 1.0\n\nR1 = 0.5\n\norder = 5\n\nLENGTH_WEIGHT = Weight(1e-6)\n\nCC_THRESHOLD = 0.1\nCC_WEIGHT = 1000\n\nCS_THRESHOLD = 0.3\nCS_WEIGHT = 10\n\nCURVATURE_THRESHOLD = 5.\nCURVATURE_WEIGHT = 1e-6\n\nMSC_THRESHOLD = 5\nMSC_WEIGHT = 1e-6\n\nMAXITER = 50 if in_github_actions else 400\n\nTEST_DIR = (Path(__file__).parent / \"..\" / \"..\" / \"tests\" / \"test_files\").resolve()\nfilename = TEST_DIR / 'input.LandremanPaul2021_QA'\n\nOUT_DIR = \"./output/\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nnphi = 32\nntheta = 32\ns = SurfaceRZFourier.from_vmec_input(filename, range=\"half period\", nphi=nphi, ntheta=ntheta)\n\nbase_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order)\nbase_currents = [Current(1e5) for i in range(ncoils)]\n\nbase_currents[0].fix_all()\n\ncoils = coils_via_symmetries(base_curves, base_currents, s.nfp, True)\nbs = BiotSavart(coils)\nbs.set_points(s.gamma().reshape((-1, 3)))\n\ncurves = [c.curve for c in coils]\ncurves_to_vtk(curves, OUT_DIR + \"curves_init\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + \"surf_init\", extra_data=pointData)\n\nJf = SquaredFlux(s, bs)\nJls = [CurveLength(c) for c in base_curves]\nJccdist = CurveCurveDistance(curves, CC_THRESHOLD, num_basecurves=ncoils)\nJcsdist = CurveSurfaceDistance(curves, s, CS_THRESHOLD)\nJcs = [LpCurveCurvature(c, 2, CURVATURE_THRESHOLD) for c in base_curves]\nJmscs = [MeanSquaredCurvature(c) for c in base_curves]\n\nJF = Jf \\\n    + LENGTH_WEIGHT * sum(Jls) \\\n    + CC_WEIGHT * Jccdist \\\n    + CS_WEIGHT * Jcsdist \\\n    + CURVATURE_WEIGHT * sum(Jcs) \\\n    + MSC_WEIGHT * sum(QuadraticPenalty(J, MSC_THRESHOLD, \"max\") for J in Jmscs)\n\ndef fun(dofs):\n    JF.x = dofs\n    J = JF.J()\n    grad = JF.dJ()\n    jf = Jf.J()\n    BdotN = np.mean(np.abs(np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)))\n    outstr = f\"J={J:.1e}, Jf={jf:.1e}, ⟨B·n⟩={BdotN:.1e}\"\n    cl_string = \", \".join([f\"{J.J():.1f}\" for J in Jls])\n    kap_string = \", \".join(f\"{np.max(c.kappa()):.1f}\" for c in base_curves)\n    msc_string = \", \".join(f\"{J.J():.1f}\" for J in Jmscs)\n    outstr += f\", Len=sum([{cl_string}])={sum(J.J() for J in Jls):.1f}, ϰ=[{kap_string}], ∫ϰ²/L=[{msc_string}]\"\n    outstr += f\", C-C-Sep={Jccdist.shortest_distance():.2f}, C-S-Sep={Jcsdist.shortest_distance():.2f}\"\n    outstr += f\", ║∇J║={np.linalg.norm(grad):.1e}\"\n    print(outstr)\n    return J, grad\n\nprint(\"\")\nf = fun\ndofs = JF.x\nnp.random.seed(1)\nh = np.random.uniform(size=dofs.shape)\nJ0, dJ0 = f(dofs)\ndJh = sum(dJ0 * h)\nfor eps in [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\n    J1, _ = f(dofs + eps*h)\n    J2, _ = f(dofs - eps*h)\n    print(\"err\", (J1-J2)/(2*eps) - dJh)\n\nprint(\"\")\nres = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 300}, tol=1e-15)\ncurves_to_vtk(curves, OUT_DIR + f\"curves_opt_short\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + \"surf_opt_short\", extra_data=pointData)\n\ndofs = res.x\nLENGTH_WEIGHT *= 0.1\nres = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 300}, tol=1e-15)\ncurves_to_vtk(curves, OUT_DIR + f\"curves_opt_long\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + \"surf_opt_long\", extra_data=pointData)\n\nbs.save(OUT_DIR + \"biot_savart_opt.json\")",
  "\nfrom seedemu.layers import Base, Routing, Ebgp, Ibgp, Ospf, PeerRelationship, Dnssec\nfrom seedemu.services import WebService, DomainNameService, DomainNameCachingService\nfrom seedemu.services import CymruIpOriginService, ReverseDomainNameService, BgpLookingGlassService\nfrom seedemu.compiler import Docker, Graphviz\nfrom seedemu.hooks import ResolvConfHook\nfrom seedemu.core import Emulator, Service, Binding, Filter\nfrom seedemu.layers import Router\nfrom seedemu.raps import OpenVpnRemoteAccessProvider\nfrom seedemu.utilities import Makers\n\nfrom typing import List, Tuple, Dict\n\nemu     = Emulator()\nbase    = Base()\nrouting = Routing()\nebgp    = Ebgp()\nibgp    = Ibgp()\nospf    = Ospf()\nweb     = WebService()\novpn    = OpenVpnRemoteAccessProvider()\n\nix100 = base.createInternetExchange(100)\nix101 = base.createInternetExchange(101)\nix102 = base.createInternetExchange(102)\nix103 = base.createInternetExchange(103)\nix104 = base.createInternetExchange(104)\nix105 = base.createInternetExchange(105)\n\nix100.getPeeringLan().setDisplayName('NYC-100')\nix101.getPeeringLan().setDisplayName('San Jose-101')\nix102.getPeeringLan().setDisplayName('Chicago-102')\nix103.getPeeringLan().setDisplayName('Miami-103')\nix104.getPeeringLan().setDisplayName('Boston-104')\nix105.getPeeringLan().setDisplayName('Huston-105')\n\nMakers.makeTransitAs(base, 2, [100, 101, 102, 105], \n       [(100, 101), (101, 102), (100, 105)] \n)\n\nMakers.makeTransitAs(base, 3, [100, 103, 104, 105], \n       [(100, 103), (100, 105), (103, 105), (103, 104)]\n)\n\nMakers.makeTransitAs(base, 4, [100, 102, 104], \n       [(100, 104), (102, 104)]\n)\n\nMakers.makeTransitAs(base, 11, [102, 105], [(102, 105)])\nMakers.makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\nMakers.makeStubAs(emu, base, 150, 100, [web, None])\nMakers.makeStubAs(emu, base, 151, 100, [web, None])\n\nMakers.makeStubAs(emu, base, 152, 101, [None, None])\nMakers.makeStubAs(emu, base, 153, 101, [web, None, None])\n\nMakers.makeStubAs(emu, base, 154, 102, [None, web])\n\nMakers.makeStubAs(emu, base, 160, 103, [web, None])\nMakers.makeStubAs(emu, base, 161, 103, [web, None])\nMakers.makeStubAs(emu, base, 162, 103, [web, None])\n\nMakers.makeStubAs(emu, base, 163, 104, [web, None])\nMakers.makeStubAs(emu, base, 164, 104, [None, None])\n\nMakers.makeStubAs(emu, base, 170, 105, [web, None])\nMakers.makeStubAs(emu, base, 171, 105, [None])\n\nas154 = base.getAutonomousSystem(154)\nas154.createHost('host_2').joinNetwork('net0', address = '10.154.0.129')\n\nas11872 = base.createAutonomousSystem(11872)\nas11872.createRealWorldRouter('rw').joinNetwork('ix102', '10.102.0.118')\n\nas152 = base.getAutonomousSystem(152)\nas152.getNetwork('net0').enableRemoteAccess(ovpn)\n\nebgp.addRsPeers(100, [2, 3, 4])\nebgp.addRsPeers(102, [2, 4])\nebgp.addRsPeers(104, [3, 4])\nebgp.addRsPeers(105, [2, 3])\n\nebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\nebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\nebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(102, [2, 4],  [11, 154], PeerRelationship.Provider)\nebgp.addPrivatePeerings(102, [11], [154, 11872], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(103, [3],  [160, 161, 162], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(104, [3, 4], [12], PeerRelationship.Provider)\nebgp.addPrivatePeerings(104, [4],  [163], PeerRelationship.Provider)\nebgp.addPrivatePeerings(104, [12], [164], PeerRelationship.Provider)\n\nebgp.addPrivatePeerings(105, [3],  [11, 170], PeerRelationship.Provider)\nebgp.addPrivatePeerings(105, [11], [171], PeerRelationship.Provider)\n\nemu.addLayer(base)\nemu.addLayer(routing)\nemu.addLayer(ebgp)\nemu.addLayer(ibgp)\nemu.addLayer(ospf)\nemu.addLayer(web)\n\nemu.dump('base-component.bin')\n\nemu.render()\nemu.compile(Docker(), './output')\n",
  "\nimport numpy as np\nimport sys\nimport matplotlib.pyplot as plt\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nfrom DREAM.DREAMOutput import DREAMOutput\nfrom DREAM import runiface\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.ElectricField as Efield\nimport DREAM.Settings.Equations.RunawayElectrons as RE\nimport DREAM.Settings.Equations.HotElectronDistribution as FHot\nimport DREAM.Settings.Equations.ColdElectronTemperature as T_cold\n\nfrom DREAM.Settings.Equations.ElectricField import ElectricField\nfrom DREAM.Settings.Equations.ColdElectronTemperature import ColdElectronTemperature\n\nds = DREAMSettings()\n\nds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_FULL\nds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_PARTIALLY_SCREENED\n\nds.collisions.bremsstrahlung_mode = Collisions.BREMSSTRAHLUNG_MODE_STOPPING_POWER\n\nds.collisions.lnlambda = Collisions.LNLAMBDA_ENERGY_DEPENDENT\nds.collisions.pstar_mode = Collisions.PSTAR_MODE_COLLISIONAL\n\nn_D = 1e20 \nn_Z = 0.1e20 \n\nJ=1.69e6 \n\nB0 = 5.3            \n\nTmax_init = 1e-11   \nNt_init = 2         \n\nTmax_restart_ioniz = 2e-6\nNt_restart_ioniz = 500\n\nTmax_restart_eq = 30e-3\nNt_restart_eq = 1000\n\nTmax_restart_rad=1e-11\nNt_restart_rad=2\n\nNr = 151             \ntimes  = [0]        \nradius = [0, 2]     \nradialgrid = np.linspace(radius[0],radius[-1],Nr)\nradius_wall = 2.15  \n\nE_initial = 0.001 \nE_wall = 0.0001        \n\nT_initial = np.logspace(np.log10(0.7),np.log10(2e3),Nr)    \n\nds.radialgrid.setB0(B0)\nds.radialgrid.setMinorRadius(radius[-1])\nds.radialgrid.setNr(Nr)\nds.radialgrid.setWallRadius(radius_wall)\n\nds.timestep.setTmax(Tmax_init)\nds.timestep.setNt(Nt_init)\n\nZ0=1\nZ=10\n\n\"\"\n\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_DYNAMIC_FULLY_IONIZED, n=n_D, opacity_mode=Ions.ION_OPACITY_MODE_GROUND_STATE_OPAQUE)\nds.eqsys.n_i.addIon(name='Ne', Z=Z, iontype=Ions.IONS_DYNAMIC_NEUTRAL, n=n_Z)\n\n\"\"\n\ntemperature = T_initial * np.ones((len(times), len(radialgrid)))\nds.eqsys.T_cold.setPrescribedData(temperature=temperature, times=times, radius=radialgrid)\n\nefield = E_initial*np.ones((len(times), len(radius)))\nds.eqsys.E_field.setPrescribedData(efield=efield, times=times, radius=radius)\nds.eqsys.E_field.setBoundaryCondition()\n\nds.runawaygrid.setEnabled(False)\nds.hottailgrid.setEnabled(False)\n\nds.solver.setType(Solver.NONLINEAR)\nds.solver.setLinearSolver(linsolv=Solver.LINEAR_SOLVER_LU)\n\nds.other.include('fluid')\n\nds.save('init_settings.h5')\nruniface(ds, 'output_init.h5', quiet=False)\n\nds2 = DREAMSettings(ds)\n\nds2.timestep.setTmax(Tmax_restart_ioniz)\nds2.timestep.setNt(Nt_restart_ioniz)\n\nds2.save('ioniz_restart_settings.h5')\n\nruniface(ds2, 'output_restart_ioniz.h5', quiet=False)\n\nds3 = DREAMSettings(ds2)\n\nds3.timestep.setTmax(Tmax_restart_eq)\nds3.timestep.setNt(Nt_restart_eq)\n\nds3.save('eq_restart_settings.h5')\n\nruniface(ds3, 'output_restart_eq.h5', quiet=False)\n\nds4 = DREAMSettings(ds3)\n\nds4.eqsys.T_cold.setType(ttype=T_cold.TYPE_SELFCONSISTENT)\n\nds4.timestep.setTmax(Tmax_restart_rad)\nds4.timestep.setNt(Nt_restart_rad)\n\nds4.save('rad_restart_settings.h5')\n\nruniface(ds4, 'output_restart_rad.h5', quiet=False)\n\ndo=DREAMOutput(ds4.output.filename)\nsigma=do.other.fluid.conductivity[0,:]\nrad=do.other.fluid.Tcold_radiation[0,:]\nT=do.eqsys.T_cold[0,:]\nplt.loglog(T,J**2/sigma/1e6)\nplt.loglog(T,rad/1e6)\nplt.show()",
  "\"\"\n\n__version__ = '0.1.0'\nfrom docopt import docopt\nargs = docopt(__doc__, version=__version__)\n\nfrom sirf.Utilities import show_2D_array\n\nexec('from sirf.' + args['--engine'] + ' import *')\n\nnum_subsets = int(args['--subs'])\nnum_subiterations = int(args['--subiter'])\ndata_file = args['--file']\ndata_path = args['--path']\nif data_path is None:\n    data_path = examples_data_path('PET')\nraw_data_file = existing_filepath(data_path, data_file)\nshow_plot = not args['--non-interactive']\n\ndef my_osmaposl(image, obj_fun, prior, filter, num_subsets, num_subiterations):\n\n    for sub_iter in range(1, num_subiterations + 1):\n        print('\\n------------- Subiteration %d' % sub_iter) \n\n        subset = (sub_iter - 1) % num_subsets\n\n        sens_image = obj_fun.get_subset_sensitivity(subset)\n\n        grad_image = obj_fun.get_backprojection_of_acquisition_ratio\\\n                     (image, subset)\n\n        prior_grad_image = prior.get_gradient(image)\n\n        denom = sens_image + prior_grad_image/num_subsets\n        update = grad_image/denom\n        image = image*update\n\n        filter.apply(image)\n\n    return image\n\ndef main():\n\n    msg_red = MessageRedirector('info.txt', 'warn.txt', 'errr.txt')\n\n    acq_model = AcquisitionModelUsingRayTracingMatrix()\n\n    print('raw data: %s' % raw_data_file)\n    acq_data = AcquisitionData(raw_data_file)\n\n    filter = TruncateToCylinderProcessor()\n\n    image_size = (31, 111, 111)\n    voxel_size = (3.375, 3, 3) \n    image = ImageData()\n    image.initialise(image_size, voxel_size)\n    image.fill(1.0)\n\n    prior = QuadraticPrior()\n    prior.set_penalisation_factor(0.5)\n    prior.set_up(image)\n\n    obj_fun = make_Poisson_loglikelihood(acq_data)\n    obj_fun.set_acquisition_model(acq_model)\n    obj_fun.set_num_subsets(num_subsets)\n    obj_fun.set_up(image)\n\n    image = my_osmaposl \\\n        (image, obj_fun, prior, filter, num_subsets, num_subiterations)\n\n    if show_plot:\n        \n        image_array = image.as_array()\n        show_2D_array('Reconstructed image at z = 20', image_array[20,:,:])\n\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    \n    print('%s' % err.value)",
  "\"\"\nimport pyvista\nfrom pyvista import examples\n\nlight = pyvista.Light()\nlight.set_direction_angle(30, -20)\n\nmercury = examples.planets.load_mercury(radius=2439.0)\nmercury_texture = examples.planets.download_mercury_surface(texture=True)\nvenus = examples.planets.load_venus(radius=6052.0)\nvenus_texture = examples.planets.download_venus_surface(texture=True)\nearth = examples.planets.load_earth(radius=6378.1)\nearth_texture = examples.load_globe_texture()\nmars = examples.planets.load_mars(radius=3397.2)\nmars_texture = examples.planets.download_mars_surface(texture=True)\njupiter = examples.planets.load_jupiter(radius=71492.0)\njupiter_texture = examples.planets.download_jupiter_surface(texture=True)\nsaturn = examples.planets.load_saturn(radius=60268.0)\nsaturn_texture = examples.planets.download_saturn_surface(texture=True)\n\ninner = 60268.0 + 7000.0\nouter = 60268.0 + 80000.0\nsaturn_rings = examples.planets.load_saturn_rings(inner=inner, outer=outer, c_res=50)\nsaturn_rings_texture = examples.planets.download_saturn_rings(texture=True)\nuranus = examples.planets.load_uranus(radius=25559.0)\nuranus_texture = examples.planets.download_uranus_surface(texture=True)\nneptune = examples.planets.load_neptune(radius=24764.0)\nneptune_texture = examples.planets.download_neptune_surface(texture=True)\npluto = examples.planets.load_pluto(radius=1151.0)\npluto_texture = examples.planets.download_pluto_surface(texture=True)\n\nmercury.translate((0.0, 0.0, 0.0), inplace=True)\nvenus.translate((-15000.0, 0.0, 0.0), inplace=True)\nearth.translate((-30000.0, 0.0, 0.0), inplace=True)\nmars.translate((-45000.0, 0.0, 0.0), inplace=True)\njupiter.translate((-150000.0, 0.0, 0.0), inplace=True)\nsaturn.translate((-400000.0, 0.0, 0.0), inplace=True)\nsaturn_rings.translate((-400000.0, 0.0, 0.0), inplace=True)\nuranus.translate((-600000.0, 0.0, 0.0), inplace=True)\nneptune.translate((-700000.0, 0.0, 0.0), inplace=True)\n\npl = pyvista.Plotter(lighting=\"none\")\ncubemap = examples.download_cubemap_space_16k()\n_ = pl.add_actor(cubemap.to_skybox())\npl.set_environment_texture(cubemap, True)\npl.add_light(light)\npl.add_mesh(mercury, texture=mercury_texture, smooth_shading=True)\npl.add_mesh(venus, texture=venus_texture, smooth_shading=True)\npl.add_mesh(earth, texture=earth_texture, smooth_shading=True)\npl.add_mesh(mars, texture=mars_texture, smooth_shading=True)\npl.add_mesh(jupiter, texture=jupiter_texture, smooth_shading=True)\npl.add_mesh(saturn, texture=saturn_texture, smooth_shading=True)\npl.add_mesh(saturn_rings, texture=saturn_rings_texture, smooth_shading=True)\npl.add_mesh(uranus, texture=uranus_texture, smooth_shading=True)\npl.add_mesh(neptune, texture=neptune_texture, smooth_shading=True)\npl.add_mesh(pluto, texture=pluto_texture, smooth_shading=True)\npl.show()\n\npl = pyvista.Plotter(shape=(3, 2))\npl.subplot(0, 0)\npl.add_text(\"Mercury\")\npl.add_mesh(examples.planets.download_mercury_surface(), rgb=True)\npl.subplot(0, 1)\npl.add_mesh(mercury, texture=mercury_texture)\npl.subplot(1, 0)\npl.add_text(\"Venus\")\npl.add_mesh(examples.planets.download_venus_surface(atmosphere=True), rgb=True)\npl.subplot(1, 1)\npl.add_mesh(venus, texture=venus_texture)\npl.subplot(2, 0)\npl.add_text(\"Mars\")\npl.add_mesh(examples.planets.download_mars_surface(), rgb=True)\npl.subplot(2, 1)\npl.add_mesh(mars, texture=mars_texture)\npl.show(cpos=\"xy\")\n\nvenus = examples.planets.load_venus()\natmosphere_texture = examples.planets.download_venus_surface(atmosphere=True, texture=True)\nsurface_texture = examples.planets.download_venus_surface(atmosphere=False, texture=True)\n\npl = pyvista.Plotter(shape=(1, 2))\npl.subplot(0, 0)\npl.add_text(\"Venus Atmosphere\")\npl.add_mesh(venus, texture=atmosphere_texture, smooth_shading=True)\npl.subplot(0, 1)\npl.add_text(\"Venus Surface\")\npl.add_mesh(venus, texture=surface_texture, smooth_shading=True)\npl.link_views()\npl.show(cpos=\"xy\")",
  "\"\"\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Callable\nimport torch\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModel\nimport thinc\nfrom thinc.api import PyTorchWrapper, Softmax, chain, with_array, Model, Config\nfrom thinc.api import torch2xp, xp2torch, SequenceCategoricalCrossentropy\nfrom thinc.api import prefer_gpu, use_pytorch_for_gpu_memory\nfrom thinc.types import Floats2d, ArgsKwargs\nimport ml_datasets\nimport tqdm\nimport typer\n\nCONFIG = \"\"\n\ndef main(path: Optional[Path] = None, out_dir: Optional[Path] = None):\n    if prefer_gpu():\n        print(\"Using gpu!\")\n        use_pytorch_for_gpu_memory()\n    \n    if path is None:\n        config = Config().from_str(CONFIG)\n    else:\n        config = Config().from_disk(path)\n    \n    C = thinc.registry.resolve(config)\n    words_per_subbatch = C[\"training\"][\"words_per_subbatch\"]\n    n_epoch = C[\"training\"][\"n_epoch\"]\n    batch_size = C[\"training\"][\"batch_size\"]\n    model = C[\"model\"]\n    optimizer = C[\"optimizer\"]\n    calculate_loss = SequenceCategoricalCrossentropy()\n\n    (train_X, train_Y), (dev_X, dev_Y) = ml_datasets.ud_ancora_pos_tags()\n    \n    train_Y = list(map(model.ops.asarray, train_Y))\n    dev_Y = list(map(model.ops.asarray, dev_Y))\n    \n    model.initialize(X=train_X[:5], Y=train_Y[:5])\n    for epoch in range(n_epoch):\n        \n        batches = model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True)\n        for outer_batch in tqdm.tqdm(batches, leave=False):\n            \n            for batch in minibatch_by_words(outer_batch, words_per_subbatch):\n                inputs, truths = zip(*batch)\n                guesses, backprop = model(inputs, is_train=True)\n                backprop(calculate_loss.get_grad(guesses, truths))\n            \n            model.finish_update(optimizer)\n            optimizer.step_schedules()\n        \n        score = evaluate_sequences(model, dev_X, dev_Y, 128)\n        print(epoch, f\"{score:.3f}\")\n        if out_dir:\n            model.to_disk(out_dir / f\"{epoch}.bin\")\n\n@dataclass\nclass TokensPlus:\n    \"\"\n\n    input_ids: torch.Tensor\n    token_type_ids: torch.Tensor\n    attention_mask: torch.Tensor\n    input_len: List[int]\n    overflowing_tokens: Optional[torch.Tensor] = None\n    num_truncated_tokens: Optional[torch.Tensor] = None\n    special_tokens_mask: Optional[torch.Tensor] = None\n\n@thinc.registry.layers(\"TransformersTagger.v1\")\ndef TransformersTagger(\n    starter: str, n_tags: int = 17\n) -> Model[List[List[str]], List[Floats2d]]:\n    return chain(\n        TransformersTokenizer(starter),\n        Transformer(starter),\n        with_array(Softmax(nO=n_tags)),\n    )\n\n@thinc.registry.layers(\"transformers_tokenizer.v1\")\ndef TransformersTokenizer(name: str) -> Model[List[List[str]], TokensPlus]:\n    def forward(\n        model, texts: List[List[str]], is_train: bool\n    ) -> Tuple[TokensPlus, Callable]:\n        tokenizer = model.attrs[\"tokenizer\"]\n        token_data = tokenizer.batch_encode_plus(\n            [(text, None) for text in texts],\n            add_special_tokens=True,\n            return_token_type_ids=True,\n            return_attention_masks=True,\n            return_input_lengths=True,\n            return_tensors=\"pt\",\n        )\n        return TokensPlus(**token_data), lambda d_tokens: []\n\n    return Model(\n        \"tokenizer\",\n        forward,\n        attrs={\"tokenizer\": AutoTokenizer.from_pretrained(name)},\n    )\n\n@thinc.registry.layers(\"transformers_model.v1\")\ndef Transformer(name: str) -> Model[TokensPlus, List[Floats2d]]:\n    return PyTorchWrapper(\n        AutoModel.from_pretrained(name),\n        convert_inputs=convert_transformer_inputs,\n        convert_outputs=convert_transformer_outputs,\n    )\n\ndef convert_transformer_inputs(model, tokens: TokensPlus, is_train):\n    kwargs = {\n        \"input_ids\": tokens.input_ids,\n        \"attention_mask\": tokens.attention_mask,\n        \"token_type_ids\": tokens.token_type_ids,\n    }\n    return ArgsKwargs(args=(), kwargs=kwargs), lambda dX: []\n\ndef convert_transformer_outputs(model, inputs_outputs, is_train):\n    layer_inputs, torch_outputs = inputs_outputs\n    torch_tokvecs: torch.Tensor = torch_outputs[0]\n    \n    torch_outputs = None\n    lengths = list(layer_inputs.input_len)\n    tokvecs: List[Floats2d] = model.ops.unpad(torch2xp(torch_tokvecs), lengths)\n    \n    tokvecs = [arr[1:-1] for arr in tokvecs]\n\n    def backprop(d_tokvecs: List[Floats2d]) -> ArgsKwargs:\n        \n        shim = model.shims[0]\n        row = model.ops.alloc2f(1, d_tokvecs[0].shape[1])\n        d_tokvecs = [model.ops.xp.vstack((row, arr, row)) for arr in d_tokvecs]\n        return ArgsKwargs(\n            args=(torch_tokvecs,),\n            kwargs={\n                \"grad_tensors\": xp2torch(model.ops.pad(d_tokvecs, device=shim.device))\n            },\n        )\n\n    return tokvecs, backprop\n\ndef evaluate_sequences(\n    model, Xs: List[Floats2d], Ys: List[Floats2d], batch_size: int\n) -> float:\n    correct = 0.0\n    total = 0.0\n    for X, Y in model.ops.multibatch(batch_size, Xs, Ys):\n        Yh = model.predict(X)\n        for yh, y in zip(Yh, Y):\n            correct += (y.argmax(axis=1) == yh.argmax(axis=1)).sum()\n            total += y.shape[0]\n    return float(correct / total)\n\ndef minibatch_by_words(pairs, max_words):\n    \"\"\n    pairs = list(zip(*pairs))\n    pairs.sort(key=lambda xy: len(xy[0]), reverse=True)\n    batch = []\n    for X, Y in pairs:\n        batch.append((X, Y))\n        n_words = max(len(xy[0]) for xy in batch) * len(batch)\n        if n_words >= max_words:\n            \n            yield batch[:-1]\n            batch = [(X, Y)]\n    if batch:\n        yield batch\n\nif __name__ == \"__main__\":\n    typer.run(main)",
  "\"\"\n\nimport os\nfrom math import radians, sin, cos, sqrt\nimport pyaedt\n\nnon_graphical = False\n\nhfss = pyaedt.Hfss(specified_version=\"2023.2\",\n                   solution_type=\"DrivenTerminal\",\n                   new_desktop_session=True,\n                   non_graphical=non_graphical)\nhfss.change_material_override(True)\nhfss.change_automatically_use_causal_materials(True)\nhfss.create_open_region(\"100GHz\")\nhfss.modeler.model_units = \"mil\"\nhfss.mesh.assign_initial_mesh_from_slider(applycurvilinear=True)\n\ntotal_length = 300\ntheta = 120\nr = 100\nwidth = 3\nheight = 0.1\nspacing = 1.53\ngnd_width = 10\ngnd_thickness = 2\n\nxt = (total_length - r * radians(theta)) / 2\n\ndef create_bending(radius, extension=0):\n    position_list = [(-xt, 0, -radius), (0, 0, -radius)]\n\n    for i in [radians(i) for i in range(theta)] + [radians(theta + 0.000000001)]:\n        position_list.append((radius * sin(i), 0, -radius * cos(i)))\n\n    x1, y1, z1 = position_list[-1]\n    x0, y0, z0 = position_list[-2]\n\n    scale = (xt + extension) / sqrt((x1 - x0) ** 2 + (z1 - z0) ** 2)\n    x, y, z = (x1 - x0) * scale + x0, 0, (z1 - z0) * scale + z0\n\n    position_list[-1] = (x, y, z)\n    return position_list\n\nposition_list = create_bending(r, 1)\nline = hfss.modeler.create_polyline(\n    position_list=position_list,\n    xsection_type=\"Rectangle\",\n    xsection_width=height,\n    xsection_height=width,\n    matname=\"copper\",\n)\n\ngnd_r = [(x, spacing + width / 2 + gnd_width / 2, z) for x, y, z in position_list]\ngnd_l = [(x, -y, z) for x, y, z in gnd_r]\n\ngnd_objs = []\nfor gnd in [gnd_r, gnd_l]:\n    x = hfss.modeler.create_polyline(\n        position_list=gnd, xsection_type=\"Rectangle\", xsection_width=height, xsection_height=gnd_width, matname=\"copper\"\n    )\n    x.color = (255, 0, 0)\n    gnd_objs.append(x)\n\nposition_list = create_bending(r + (height + gnd_thickness) / 2)\n\nfr4 = hfss.modeler.create_polyline(\n    position_list=position_list,\n    xsection_type=\"Rectangle\",\n    xsection_width=gnd_thickness,\n    xsection_height=width + 2 * spacing + 2 * gnd_width,\n    matname=\"FR4_epoxy\",\n)\n\nposition_list = create_bending(r + height + gnd_thickness, 1)\n\nbot = hfss.modeler.create_polyline(\n    position_list=position_list,\n    xsection_type=\"Rectangle\",\n    xsection_width=height,\n    xsection_height=width + 2 * spacing + 2 * gnd_width,\n    matname=\"copper\",\n)\n\nport_faces = []\nfor face, blockname in zip([fr4.top_face_z, fr4.bottom_face_x], [\"b1\", \"b2\"]):\n    xc, yc, zc = face.center\n    positions = [i.position for i in face.vertices]\n\n    port_sheet_list = [((x - xc) * 10 + xc, (y - yc) + yc, (z - zc) * 10 + zc) for x, y, z in positions]\n    s = hfss.modeler.create_polyline(port_sheet_list, close_surface=True, cover_surface=True)\n    center = [round(i, 6) for i in s.faces[0].center]\n\n    port_block = hfss.modeler.thicken_sheet(s.name, -5)\n    port_block.name = blockname\n    for f in port_block.faces:\n\n        if [round(i, 6) for i in f.center] == center:\n            port_faces.append(f)\n\n    port_block.material_name = \"PEC\"\n\n    for i in [line, bot] + gnd_objs:\n        i.subtract([port_block], True)\n\n    print(port_faces)\n\nboundary = []\nfor face in [fr4.top_face_y, fr4.bottom_face_y]:\n    s = hfss.modeler.create_object_from_face(face)\n    boundary.append(s)\n    hfss.assign_perfecte_to_sheets(s)\n\nfor s, port_name in zip(port_faces, [\"1\", \"2\"]):\n    reference = [i.name for i in gnd_objs + boundary + [bot]] + [\"b1\", \"b2\"]\n\n    hfss.wave_port(s.id, name=port_name, reference=reference)\n\nsetup = hfss.create_setup(\"setup1\")\nsetup[\"Frequency\"] = \"2GHz\"\nsetup.props[\"MaximumPasses\"] = 10\nsetup.props[\"MinimumConvergedPasses\"] = 2\nhfss.create_linear_count_sweep(\n    setupname=\"setup1\",\n    unit=\"GHz\",\n    freqstart=1e-1,\n    freqstop=4,\n    num_of_freq_points=101,\n    sweepname=\"sweep1\",\n    save_fields=False,\n    sweep_type=\"Interpolating\",\n)\n\nmy_plot = hfss.plot(show=False, plot_air_objects=False)\nmy_plot.show_axes = False\nmy_plot.show_grid = False\nmy_plot.plot(\n    os.path.join(hfss.working_directory, \"Image.jpg\"),\n)\n\nhfss.release_desktop()",
  "\"\"\n\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nn_samples = 500\nseed = 30\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\nplt.figure(figsize=(9 * 2 + 3, 13))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    \n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    X = StandardScaler().fit_transform(X)\n\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    \n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" > 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"\n                            \"\n                            \"\n                            \"\n                            \"\n                            \"\n                            \"\n                            \"\n                            \"\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        \n        colors = np.append(colors, [\"\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()",
  "\n\"\"\n\nimport initExample\n\nimport numpy as np\nfrom pyqtgraph.Qt import QtWidgets, mkQApp\nimport pyqtgraph as pg\n\nclass MainWindow(QtWidgets.QMainWindow):\n    \"\"\n    def __init__(self, *args, **kwargs):\n        super(MainWindow, self).__init__(*args, **kwargs)\n        gr_wid = pg.GraphicsLayoutWidget(show=True)\n        self.setCentralWidget(gr_wid)\n        self.setWindowTitle('pyqtgraph example: Interactive color bar')\n        self.resize(800,700)\n        self.show()\n\n        data = np.fromfunction(lambda i, j: (1+0.3*np.sin(i)) * (i)**2 + (j)**2, (100, 100))\n        noisy_data = data * (1 + 0.2 * np.random.random(data.shape) )\n        noisy_transposed = noisy_data.transpose()\n\n        i1 = pg.ImageItem(image=data)\n        p1 = gr_wid.addPlot(title=\"non-interactive\")\n        p1.addItem( i1 )\n        p1.setMouseEnabled( x=False, y=False)\n        p1.disableAutoRange()\n        p1.hideButtons()\n        p1.setRange(xRange=(0,100), yRange=(0,100), padding=0)\n        for key in ['left','right','top','bottom']:\n            p1.showAxis(key)\n            axis = p1.getAxis(key)\n            axis.setZValue(1)\n            if key in ['top', 'right']: \n                p1.getAxis(key).setStyle( showValues=False )\n\n        cmap = pg.colormap.get('CET-L9')\n        bar = pg.ColorBarItem(\n            interactive=False, values= (0, 30_000), cmap=cmap,\n            label='vertical fixed color bar'\n        )\n        bar.setImageItem( i1, insert_in=p1 )\n\n        i2 = pg.ImageItem(image=noisy_data)\n        p2 = gr_wid.addPlot(1,0, 1,1, title=\"interactive\")\n        p2.addItem( i2, title='' )\n        \n        p2.showAxis('right')\n        p2.getAxis('left').setStyle( showValues=False )\n        p2.getAxis('bottom').setLabel('bottom axis label')\n        p2.getAxis('right').setLabel('right axis label')\n\n        cmap = pg.colormap.get('CET-L4')\n        bar = pg.ColorBarItem(\n            values = (0, 30_000),\n            cmap=cmap,\n            label='horizontal color bar',\n            limits = (0, None),\n            rounding=1000,\n            orientation = 'horizontal',\n            pen='\n        )\n        bar.setImageItem( i2, insert_in=p2 )\n\n        i3 = pg.ImageItem(image=noisy_data)\n        p3 = gr_wid.addPlot(0,1, 1,1, title=\"shared 1\")\n        p3.addItem( i3 )\n\n        i4 = pg.ImageItem(image=noisy_transposed)\n        p4 = gr_wid.addPlot(1,1, 1,1, title=\"shared 2\")\n        p4.addItem( i4 )\n\n        cmap = pg.colormap.get('CET-L8')\n        bar = pg.ColorBarItem(\n            \n            limits = (-30_000, 30_000), \n            rounding=1000,\n            width = 10,\n            cmap=cmap )\n        bar.setImageItem( [i3, i4] )\n        bar.setLevels( low=-5_000, high=15_000) \n\n        bar.getAxis('bottom').setHeight(21)\n        bar.getAxis('top').setHeight(31)\n        gr_wid.addItem(bar, 0,2, 2,1) \n\nmkQApp(\"ColorBarItem Example\")\nmain_window = MainWindow()\n\nif __name__ == '__main__':\n    pg.exec()",
  "\nimport argparse\n\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation, DataStatistics, HeteroPearson\nfrom pipeline.component import HeteroLR, OneHotEncoder\nfrom pipeline.component import HeteroFeatureBinning\nfrom pipeline.component import HeteroFeatureSelection\nfrom pipeline.component import FeatureScale\nfrom pipeline.component import Intersection\nfrom pipeline.component import Reader\nfrom pipeline.interface import Data\nfrom pipeline.interface import Model\n\nfrom pipeline.utils.tools import load_job_config\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    \n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n    arbiter = parties.arbiter[0]\n\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": f\"experiment_sid{namespace}\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": f\"experiment_sid{namespace}\"}\n\n    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).\\\n        set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    reader_0 = Reader(name=\"reader_0\")\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    data_transform_0 = DataTransform(name=\"data_transform_0\", with_match_id=True)\n    data_transform_0.get_party_instance(role='guest', party_id=guest).component_param(with_label=True)\n    data_transform_0.get_party_instance(role='host', party_id=host).component_param(with_label=False)\n\n    intersection_0 = Intersection(name=\"intersection_0\")\n    feature_scale_0 = FeatureScale(name='feature_scale_0', method=\"standard_scale\",\n                                   need_run=True)\n\n    binning_param = {\n        \"method\": \"quantile\",\n        \"compress_thres\": 10000,\n        \"head_size\": 10000,\n        \"error\": 0.001,\n        \"bin_num\": 10,\n        \"bin_indexes\": -1,\n        \"adjustment_factor\": 0.5,\n        \"local_only\": False,\n        \"need_run\": True,\n        \"transform_param\": {\n            \"transform_cols\": -1,\n            \"transform_type\": \"bin_num\"\n        }\n    }\n    hetero_feature_binning_0 = HeteroFeatureBinning(name='hetero_feature_binning_0',\n                                                    **binning_param)\n\n    statistic_0 = DataStatistics(name='statistic_0', statistics=[\"95%\"])\n    pearson_0 = HeteroPearson(name='pearson_0', column_indexes=-1)\n    onehot_0 = OneHotEncoder(name='onehot_0')\n    selection_param = {\n        \"name\": \"hetero_feature_selection_0\",\n        \"select_col_indexes\": -1,\n        \"select_names\": [],\n        \"filter_methods\": [\n            \"manually\",\n            \"unique_value\",\n            \"iv_filter\",\n            \"coefficient_of_variation_value_thres\",\n            \"outlier_cols\"\n        ],\n        \"manually_param\": {\n            \"filter_out_indexes\": [\n                0,\n                1,\n                2\n            ],\n            \"filter_out_names\": [\n                \"x3\"\n            ]\n        },\n        \"unique_param\": {\n            \"eps\": 1e-06\n        },\n        \"iv_param\": {\n            \"metrics\": [\"iv\", \"iv\", \"iv\"],\n            \"filter_type\": [\"threshold\", \"top_k\", \"top_percentile\"],\n            \"threshold\": [0.001, 100, 0.99]\n        },\n        \"variance_coe_param\": {\n            \"value_threshold\": 0.3\n        },\n        \"outlier_param\": {\n            \"percentile\": 0.95,\n            \"upper_threshold\": 2.0\n        }}\n    hetero_feature_selection_0 = HeteroFeatureSelection(**selection_param)\n\n    lr_param = {\n        \"name\": \"hetero_lr_0\",\n        \"penalty\": \"L2\",\n        \"optimizer\": \"rmsprop\",\n        \"tol\": 0.0001,\n        \"alpha\": 0.01,\n        \"max_iter\": 30,\n        \"early_stop\": \"diff\",\n        \"batch_size\": 320,\n        \"learning_rate\": 0.15,\n        \"init_param\": {\n            \"init_method\": \"zeros\"\n        },\n        \"sqn_param\": {\n            \"update_interval_L\": 3,\n            \"memory_M\": 5,\n            \"sample_size\": 5000,\n            \"random_seed\": None\n        },\n        \"cv_param\": {\n            \"n_splits\": 5,\n            \"shuffle\": False,\n            \"random_seed\": 103,\n            \"need_cv\": False\n        }\n    }\n\n    hetero_lr_0 = HeteroLR(**lr_param)\n    evaluation_0 = Evaluation(name='evaluation_0')\n\n    pipeline.add_component(reader_0)\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    pipeline.add_component(intersection_0, data=Data(data=data_transform_0.output.data))\n    pipeline.add_component(feature_scale_0, data=Data(data=intersection_0.output.data))\n    pipeline.add_component(hetero_feature_binning_0, data=Data(data=feature_scale_0.output.data))\n    pipeline.add_component(statistic_0, data=Data(data=feature_scale_0.output.data))\n    pipeline.add_component(pearson_0, data=Data(data=feature_scale_0.output.data))\n\n    pipeline.add_component(hetero_feature_selection_0, data=Data(data=hetero_feature_binning_0.output.data),\n                           model=Model(isometric_model=[hetero_feature_binning_0.output.model,\n                                                        statistic_0.output.model]))\n    pipeline.add_component(onehot_0, data=Data(data=hetero_feature_selection_0.output.data))\n\n    pipeline.add_component(hetero_lr_0, data=Data(train_data=onehot_0.output.data))\n    pipeline.add_component(evaluation_0, data=Data(data=hetero_lr_0.output.data))\n\n    pipeline.compile()\n\n    pipeline.fit()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()",
  "from loguru import logger\n\nfrom sc2 import maps\nfrom sc2.bot_ai import BotAI\nfrom sc2.data import Difficulty, Race\nfrom sc2.ids.ability_id import AbilityId\nfrom sc2.ids.buff_id import BuffId\nfrom sc2.ids.unit_typeid import UnitTypeId\nfrom sc2.ids.upgrade_id import UpgradeId\nfrom sc2.main import run_game\nfrom sc2.player import Bot, Computer\n\nclass WarpGateBot(BotAI):\n\n    def __init__(self):\n        \n        self.proxy_built = False\n\n    async def warp_new_units(self, proxy):\n        for warpgate in self.structures(UnitTypeId.WARPGATE).ready:\n            abilities = await self.get_available_abilities(warpgate)\n            \n            if AbilityId.WARPGATETRAIN_STALKER in abilities:\n                pos = proxy.position.to2.random_on_distance(4)\n                placement = await self.find_placement(AbilityId.WARPGATETRAIN_STALKER, pos, placement_step=1)\n                if placement is None:\n                    \n                    logger.info(\"can't place\")\n                    return\n                warpgate.warp_in(UnitTypeId.STALKER, placement)\n\n    async def on_step(self, iteration):\n        await self.distribute_workers()\n\n        if not self.townhalls.ready:\n            \n            for worker in self.workers:\n                worker.attack(self.enemy_start_locations[0])\n            return\n\n        nexus = self.townhalls.ready.random\n\n        if self.supply_left < 2 and self.already_pending(UnitTypeId.PYLON) == 0:\n            \n            if self.can_afford(UnitTypeId.PYLON):\n                await self.build(UnitTypeId.PYLON, near=nexus)\n            return\n\n        if self.workers.amount < self.townhalls.amount * 22 and nexus.is_idle:\n            if self.can_afford(UnitTypeId.PROBE):\n                nexus.train(UnitTypeId.PROBE)\n\n        elif self.structures(UnitTypeId.PYLON).amount < 5 and self.already_pending(UnitTypeId.PYLON) == 0:\n            if self.can_afford(UnitTypeId.PYLON):\n                await self.build(UnitTypeId.PYLON, near=nexus.position.towards(self.game_info.map_center, 5))\n\n        proxy = None\n        if self.structures(UnitTypeId.PYLON).ready:\n            proxy = self.structures(UnitTypeId.PYLON).closest_to(self.enemy_start_locations[0])\n            pylon = self.structures(UnitTypeId.PYLON).ready.random\n            if self.structures(UnitTypeId.GATEWAY).ready:\n                \n                if not self.structures(UnitTypeId.CYBERNETICSCORE):\n                    if (\n                        self.can_afford(UnitTypeId.CYBERNETICSCORE)\n                        and self.already_pending(UnitTypeId.CYBERNETICSCORE) == 0\n                    ):\n                        await self.build(UnitTypeId.CYBERNETICSCORE, near=pylon)\n            \n            if (\n                self.can_afford(UnitTypeId.GATEWAY)\n                and self.structures(UnitTypeId.WARPGATE).amount + self.structures(UnitTypeId.GATEWAY).amount < 4\n            ):\n                await self.build(UnitTypeId.GATEWAY, near=pylon)\n\n        for nexus in self.townhalls.ready:\n            vgs = self.vespene_geyser.closer_than(15, nexus)\n            for vg in vgs:\n                if not self.can_afford(UnitTypeId.ASSIMILATOR):\n                    break\n                worker = self.select_build_worker(vg.position)\n                if worker is None:\n                    break\n                if not self.gas_buildings or not self.gas_buildings.closer_than(1, vg):\n                    worker.build_gas(vg)\n                    worker.stop(queue=True)\n\n        if (\n            self.structures(UnitTypeId.CYBERNETICSCORE).ready and self.can_afford(AbilityId.RESEARCH_WARPGATE)\n            and self.already_pending_upgrade(UpgradeId.WARPGATERESEARCH) == 0\n        ):\n            ccore = self.structures(UnitTypeId.CYBERNETICSCORE).ready.first\n            ccore.research(UpgradeId.WARPGATERESEARCH)\n\n        for gateway in self.structures(UnitTypeId.GATEWAY).ready.idle:\n            if self.already_pending_upgrade(UpgradeId.WARPGATERESEARCH) == 1:\n                gateway(AbilityId.MORPH_WARPGATE)\n\n        if self.proxy_built and proxy:\n            await self.warp_new_units(proxy)\n\n        if self.units(UnitTypeId.STALKER).amount > 3:\n            for stalker in self.units(UnitTypeId.STALKER).ready.idle:\n                targets = (self.enemy_units | self.enemy_structures).filter(lambda unit: unit.can_be_attacked)\n                if targets:\n                    target = targets.closest_to(stalker)\n                    stalker.attack(target)\n                else:\n                    stalker.attack(self.enemy_start_locations[0])\n\n        if (\n            self.structures(UnitTypeId.CYBERNETICSCORE).amount >= 1 and not self.proxy_built\n            and self.can_afford(UnitTypeId.PYLON)\n        ):\n            p = self.game_info.map_center.towards(self.enemy_start_locations[0], 20)\n            await self.build(UnitTypeId.PYLON, near=p)\n            self.proxy_built = True\n\n        if not self.structures(UnitTypeId.CYBERNETICSCORE).ready:\n            if not nexus.has_buff(BuffId.CHRONOBOOSTENERGYCOST) and not nexus.is_idle:\n                if nexus.energy >= 50:\n                    nexus(AbilityId.EFFECT_CHRONOBOOSTENERGYCOST, nexus)\n        else:\n            ccore = self.structures(UnitTypeId.CYBERNETICSCORE).ready.first\n            if not ccore.has_buff(BuffId.CHRONOBOOSTENERGYCOST) and not ccore.is_idle:\n                if nexus.energy >= 50:\n                    nexus(AbilityId.EFFECT_CHRONOBOOSTENERGYCOST, ccore)\n\ndef main():\n    run_game(\n        maps.get(\"(2)CatalystLE\"),\n        [Bot(Race.Protoss, WarpGateBot()), Computer(Race.Protoss, Difficulty.Easy)],\n        realtime=False,\n    )\n\nif __name__ == \"__main__\":\n    main()",
  "\nr\"\"\n\nimport inspect\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfilename = inspect.getframeinfo(inspect.currentframe()).filename\nfileNameString = os.path.basename(os.path.splitext(__file__)[0])\npath = os.path.dirname(os.path.abspath(filename))\n\nfrom Basilisk import __path__\nbskPath = __path__[0]\n\nimport sys\nfrom Basilisk.utilities.MonteCarlo.Controller import Controller\nfrom Basilisk.utilities.MonteCarlo.RetentionPolicy import RetentionPolicy\nfrom Basilisk.utilities.MonteCarlo.Dispersions import (UniformEulerAngleMRPDispersion, UniformDispersion,\n                                                       NormalVectorCartDispersion)\n\nsys.path.append(path+\"/../BskSim/scenarios/\")\nimport scenario_AttFeedback\n\nsNavTransName = \"sNavTransMsg\"\nattGuidName = \"attGuidMsg\"\n\ndef run(show_plots):\n    \"\"\n\n    monteCarlo = Controller()\n    monteCarlo.setSimulationFunction(scenario_AttFeedback.scenario_AttFeedback)  \n    monteCarlo.setExecutionFunction(scenario_AttFeedback.runScenario)  \n    monteCarlo.setExecutionCount(4)  \n\n    monteCarlo.setArchiveDir(path + \"/scenario_AttFeedbackMC\")  \n    monteCarlo.setShouldDisperseSeeds(True)  \n    monteCarlo.setThreadCount(2)  \n    monteCarlo.setVerbose(True)  \n    monteCarlo.setVarCast('float')  \n    monteCarlo.setDispMagnitudeFile(True)  \n\n    dispMRPInit = 'TaskList[0].TaskModels[0].hub.sigma_BNInit'\n    dispOmegaInit = 'TaskList[0].TaskModels[0].hub.omega_BN_BInit'\n    dispMass = 'TaskList[0].TaskModels[0].hub.mHub'\n    dispCoMOff = 'TaskList[0].TaskModels[0].hub.r_BcB_B'\n    dispInertia = 'hubref.IHubPntBc_B'\n    dispList = [dispMRPInit, dispOmegaInit, dispMass, dispCoMOff, dispInertia]\n\n    monteCarlo.addDispersion(UniformEulerAngleMRPDispersion('TaskList[0].TaskModels[0].hub.sigma_BNInit'))\n    monteCarlo.addDispersion(NormalVectorCartDispersion('TaskList[0].TaskModels[0].hub.omega_BN_BInit', 0.0, 0.75 / 3.0 * np.pi / 180))\n    monteCarlo.addDispersion(UniformDispersion('TaskList[0].TaskModels[0].hub.mHub', ([750.0 - 0.05*750, 750.0 + 0.05*750])))\n    monteCarlo.addDispersion(NormalVectorCartDispersion('TaskList[0].TaskModels[0].hub.r_BcB_B', [0.0, 0.0, 1.0], [0.05 / 3.0, 0.05 / 3.0, 0.1 / 3.0]))\n\n    retentionPolicy = RetentionPolicy()\n    samplingTime = int(2E9)\n    retentionPolicy.addMessageLog(sNavTransName, [\"r_BN_N\"])\n    retentionPolicy.addMessageLog(attGuidName, [\"sigma_BR\", \"omega_BR_B\"])\n    retentionPolicy.setDataCallback(displayPlots)\n    monteCarlo.addRetentionPolicy(retentionPolicy)\n\n    failures = monteCarlo.executeSimulations()\n\n    if show_plots:\n        monteCarlo.executeCallbacks()\n        plt.show()\n\n    return\n\ndef displayPlots(data, retentionPolicy):\n    states = data[\"messages\"][attGuidName + \".sigma_BR\"]\n    time = states[:, 0]\n    plt.figure(1)\n    plt.plot(time, states[:,1],\n             time, states[:,2],\n             time, states[:,3])\n\nif __name__ == \"__main__\":\n    run(True)",
  "import blenderproc as bproc\nimport numpy as np\nfrom mathutils import Euler\nimport argparse\nimport os\n\nparser = argparse.ArgumentParser()\nparser.add_argument('house', help=\"Path to the house.json file of the SUNCG scene to load\")\nparser.add_argument('object_path', help='Path to the chair object which will be used to replace others.')\nparser.add_argument('output_dir', nargs='?', default=\"examples/datasets/suncg_with_object_replacer/output\",\n                    help=\"Path to where the final files, will be saved\")\nargs = parser.parse_args()\n\nbproc.init()\n\nlabel_mapping = bproc.utility.LabelIdMapping.from_csv(bproc.utility.resolve_resource(os.path.join('id_mappings', 'nyu_idset.csv')))\nobjs = bproc.loader.load_suncg(args.house, label_mapping)\n\nchair_obj = bproc.loader.load_obj(args.object_path)\nif len(chair_obj) != 1:\n    raise Exception(f\"There should only be one chair object not: {len(chair_obj)}\")\nchair_obj = chair_obj[0]\n\ndef relative_pose_sampler(obj):\n    \n    obj.blender_obj.rotation_euler.rotate(Euler((0, 0, np.random.uniform(0.0, 6.283185307))))\n\nreplace_ratio = 1.0\nbproc.object.replace_objects(\n    objects_to_be_replaced=bproc.filter.by_cp(objs, \"coarse_grained_class\", \"chair\"),\n    objects_to_replace_with=[chair_obj],\n    ignore_collision_with=bproc.filter.by_cp(objs, \"suncg_type\", \"Floor\"),\n    replace_ratio=replace_ratio,\n    copy_properties=True,\n    relative_pose_sampler=relative_pose_sampler\n)\n\nobjs = [obj for obj in objs if obj.is_valid()]\n\nbproc.lighting.light_suncg_scene()\n\npoint_sampler = bproc.sampler.SuncgPointInRoomSampler(objs)\n\nbvh_tree = bproc.object.create_bvh_tree_multi_objects([o for o in objs if isinstance(o, bproc.types.MeshObject)])\n\nposes = 0\ntries = 0\nwhile tries < 10000 and poses < 5:\n    \n    height = np.random.uniform(0.5, 2)\n    location, _ = point_sampler.sample(height)\n    \n    euler_rotation = np.random.uniform([1.2217, 0, 0], [1.2217, 0, 6.283185307])\n    cam2world_matrix = bproc.math.build_transformation_mat(location, euler_rotation)\n\n    if bproc.camera.perform_obstacle_in_view_check(cam2world_matrix, {\"min\": 1.0},\n                                                       bvh_tree) and bproc.camera.scene_coverage_score(\n            cam2world_matrix) > 0.4:\n        bproc.camera.add_camera_pose(cam2world_matrix)\n        poses += 1\n    tries += 1\n\nbproc.renderer.enable_normals_output()\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\nbproc.material.add_alpha_channel_to_textures(blurry_edges=True)\nbproc.renderer.enable_segmentation_output(map_by=[\"category_id\"])\n\ndata = bproc.renderer.render()\n\nbproc.writer.write_hdf5(args.output_dir, data)",
  "\nfrom seedemu import *\nimport os\n\nemu = Emulator()\n\nbase = Base()\n\nix100 = base.createInternetExchange(100)\nix101 = base.createInternetExchange(101)\nix100.getPeeringLan().setDisplayName('New York-100')\nix101.getPeeringLan().setDisplayName('Chicago-101')\n\nas3 = base.createAutonomousSystem(3)\n\nas3.createNetwork('net0')\nas3.createNetwork('net1')\nas3.createNetwork('net2')\n\nas3.createRouter('r1').joinNetwork('net0').joinNetwork('ix100')\nas3.createRouter('r2').joinNetwork('net0').joinNetwork('net1')\nas3.createRouter('r3').joinNetwork('net1').joinNetwork('net2')\nas3.createRouter('r4').joinNetwork('net2').joinNetwork('ix101')\n\nas151 = base.createAutonomousSystem(151)\n\nas151.createNetwork('net0')\nas151.createRouter('router0').joinNetwork('net0').joinNetwork('ix100')\n\nas151.createHost('host0').joinNetwork('net0')\nas151.createHost('host1').joinNetwork('net0', address = '10.151.0.80')\n\nhost0 = as151.getHost('host0')\nhost0.addSoftware('telnetd').addSoftware('telnet')\n\nhost0.addBuildCommand('useradd -m -s /bin/bash seed && echo \"seed:dees\" | chpasswd')\n\nas152 = base.createAutonomousSystem(152)\nas152.createNetwork('net0')\nas152.createRouter('router0').joinNetwork('net0').joinNetwork('ix101')\nas152.createHost('host0').joinNetwork('net0')\nas152.createHost('host1').joinNetwork('net0')\nas152.createHost('host2').joinNetwork('net0')\n\nas152.getHost('host0').addSoftware('telnet')\n\nMakers.makeStubAs(emu, base, 153, 101, [None, None])\n\nas153 = base.getAutonomousSystem(153)\nas153.getHost('host_1').addSoftware('telnet')\n\nebgp = Ebgp()\n\nebgp.addPrivatePeering (100, 3,   151, abRelationship = PeerRelationship.Provider)\nebgp.addPrivatePeerings(101, [3], [152, 153], abRelationship = PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 152, 153, abRelationship = PeerRelationship.Peer)\n\nweb = WebService()\n\nweb01 = web.install('web01').appendClassName(\"SEEDWeb\")\nweb02 = web.install('web02').appendClassName(\"SyrWeb\")\n\nemu.addBinding(Binding('web01', filter = Filter(nodeName = 'host0', asn = 151)))\nemu.addBinding(Binding('web02', filter = Filter(nodeName = 'host0', asn = 152)))\n\nemu.addLayer(base)\nemu.addLayer(ebgp)\nemu.addLayer(web)\n\nemu.addLayer(Routing())\nemu.addLayer(Ibgp())\nemu.addLayer(Ospf())\n\nemu.dump('base-component.bin')\n\nemu.render()\n\nemu.getBindingFor('web01').setDisplayName('Web-1')\nemu.getBindingFor('web02').setDisplayName('Web-2')\n\ndocker = Docker()\n\ndocker.addImage(DockerImage('handsonsecurity/seed-ubuntu:small', [], local = False), priority=-1)\ndocker.setImageOverride(as152.getHost('host1'), 'handsonsecurity/seed-ubuntu:small')\n\ndocker.addImage(DockerImage('seed-ubuntu-large', [], local = True), priority=-1)\ndocker.setImageOverride(as152.getHost('host2'), 'seed-ubuntu-large')\n\nemu.compile(docker, './output')\n\nos.system('cp -r seed-ubuntu-large ./output')\n",
  "\n\"\"\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport burnman\nfrom burnman import minerals\nfrom burnman.classes.solution import Solution\nfrom burnman.classes.solutionmodel import IdealSolution\n\nif __name__ == \"__main__\":\n    \n    if True:\n        amount_perovskite = 0.95\n        rock = burnman.Composite(\n            [minerals.SLB_2011.mg_perovskite(), minerals.SLB_2011.periclase()],\n            [amount_perovskite, 1 - amount_perovskite],\n        )\n\n    if False:\n        rock = burnman.Composite(\n            [\n                minerals.SLB_2011.fe_perovskite(),\n                minerals.SLB_2011.periclase(),\n                minerals.SLB_2011.stishovite(),\n            ],\n            [0.7, 0.2, 0.1],\n        )\n\n    if False:\n        \n        preset_solution = minerals.SLB_2011.mg_fe_perovskite()\n        \n        preset_solution.set_composition([0.9, 0.1, 0.0])\n        rock = burnman.Composite(\n            [preset_solution, minerals.SLB_2011.periclase()], [0.8, 0.2]\n        )\n\n    if False:\n        \n        mpv = minerals.SLB_2011.mg_perovskite()\n        fpv = minerals.SLB_2011.fe_perovskite()\n        new_solution = Solution(\n            name=\"New Mg-Fe bridgmanite\",\n            solution_model=IdealSolution(\n                endmembers=[[mpv, \"[Mg]SiO3\"], [fpv, \"[Fe]SiO3\"]]\n            ),\n        )\n\n        new_solution.set_composition([0.9, 0.1])\n        rock = burnman.Composite(\n            [new_solution, minerals.SLB_2011.periclase()], [0.8, 0.2]\n        )\n\n    seismic_model = burnman.seismic.PREM()\n    \n    number_of_points = 20\n    \n    depths = np.linspace(700e3, 2800e3, number_of_points)\n    \n    seis_p, seis_rho, seis_vp, seis_vs, seis_vphi = seismic_model.evaluate(\n        [\"pressure\", \"density\", \"v_p\", \"v_s\", \"v_phi\"], depths\n    )\n\n    temperature = burnman.geotherm.brown_shankland(depths)\n\n    print(\"Calculations are done for:\")\n    rock.debug_print()\n\n    mat_rho, mat_vp, mat_vphi, mat_vs, mat_K, mat_G = rock.evaluate(\n        [\"density\", \"v_p\", \"v_phi\", \"v_s\", \"K_S\", \"G\"], seis_p, temperature\n    )\n\n    [vs_err, vphi_err, rho_err] = burnman.utils.math.compare_chifactor(\n        [mat_vs, mat_vphi, mat_rho], [seis_vs, seis_vphi, seis_rho]\n    )\n\n    plt.subplot(2, 2, 1)\n    plt.plot(\n        seis_p / 1.0e9,\n        mat_vs / 1.0e3,\n        color=\"b\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"b\",\n        markersize=4,\n        label=\"computation\",\n    )\n    plt.plot(\n        seis_p / 1.0e9,\n        seis_vs / 1.0e3,\n        color=\"k\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"k\",\n        markersize=4,\n        label=\"reference\",\n    )\n    plt.title(\"Vs (km/s)\")\n    plt.xlim(min(seis_p) / 1.0e9, max(seis_p) / 1.0e9)\n    plt.ylim(5.1, 7.6)\n    plt.legend(loc=\"lower right\")\n    plt.text(40, 7.3, \"misfit= %3.3f\" % vs_err)\n\n    plt.subplot(2, 2, 2)\n    plt.plot(\n        seis_p / 1.0e9,\n        mat_vphi / 1.0e3,\n        color=\"b\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"b\",\n        markersize=4,\n    )\n    plt.plot(\n        seis_p / 1.0e9,\n        seis_vphi / 1.0e3,\n        color=\"k\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"k\",\n        markersize=4,\n    )\n    plt.title(\"Vphi (km/s)\")\n    plt.xlim(min(seis_p) / 1.0e9, max(seis_p) / 1.0e9)\n    plt.ylim(7, 12)\n    plt.text(40, 11.5, \"misfit= %3.3f\" % vphi_err)\n\n    plt.subplot(2, 2, 3)\n    plt.plot(\n        seis_p / 1.0e9,\n        mat_rho / 1.0e3,\n        color=\"b\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"b\",\n        markersize=4,\n    )\n    plt.plot(\n        seis_p / 1.0e9,\n        seis_rho / 1.0e3,\n        color=\"k\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"k\",\n        markersize=4,\n    )\n    plt.title(\"density ($\\\\cdot 10^3$ kg/m$^3$)\")\n    plt.xlim(min(seis_p) / 1.0e9, max(seis_p) / 1.0e9)\n    plt.text(40, 4.3, \"misfit= %3.3f\" % rho_err)\n    plt.xlabel(\"Pressure (GPa)\")\n\n    plt.subplot(2, 2, 4)\n    plt.plot(\n        seis_p / 1e9,\n        temperature,\n        color=\"r\",\n        linestyle=\"-\",\n        marker=\"o\",\n        markerfacecolor=\"r\",\n        markersize=4,\n    )\n    plt.title(\"Geotherm (K)\")\n    plt.xlim(min(seis_p) / 1.0e9, max(seis_p) / 1.0e9)\n    plt.xlabel(\"Pressure (GPa)\")\n\n    plt.savefig(\"output_figures/example_composition.png\")\n    plt.show()",
  "\n\"\"\n\nimport pickle\n\nimport torch\n\nfrom examples.compression.models import (\n    build_resnet18,\n    prepare_dataloader,\n    prepare_optimizer,\n    train,\n    training_step,\n    evaluate,\n    device\n)\n\nfrom nni.compression import TorchEvaluator\nfrom nni.compression.base.compressor import Quantizer\nfrom nni.compression.distillation import DynamicLayerwiseDistiller\nfrom nni.compression.pruning import TaylorPruner, AGPPruner\nfrom nni.compression.quantization import QATQuantizer\nfrom nni.compression.utils import auto_set_denpendency_group_ids\nfrom nni.compression.speedup import ModelSpeedup\n\nif __name__ == '__main__':\n    \n    model = build_resnet18()\n    optimizer = prepare_optimizer(model)\n    train(model, optimizer, training_step, lr_scheduler=None, max_steps=None, max_epochs=30)\n    _, test_loader = prepare_dataloader()\n    print('Original model paramater number: ', sum([param.numel() for param in model.parameters()]))\n    print('Original model after 10 epochs finetuning acc: ', evaluate(model, test_loader), '%')\n\n    teacher_model = build_resnet18()\n    teacher_model.load_state_dict(pickle.loads(pickle.dumps(model.state_dict())))\n\n    bn_list = [module_name for module_name, module in model.named_modules() if isinstance(module, torch.nn.BatchNorm2d)]\n    p_config_list = [{\n        'op_types': ['Conv2d'],\n        'sparse_ratio': 0.5\n    }, *[{\n        'op_names': [name],\n        'target_names': ['_output_'],\n        'target_settings': {\n            '_output_': {\n                'align': {\n                    'module_name': name.replace('bn', 'conv') if 'bn' in name else name.replace('downsample.1', 'downsample.0'),\n                    'target_name': 'weight',\n                    'dims': [0],\n                },\n                'granularity': 'per_channel'\n            }\n        }\n    } for name in bn_list]]\n    dummy_input = torch.rand(8, 3, 224, 224).to(device)\n    p_config_list = auto_set_denpendency_group_ids(model, p_config_list, dummy_input)\n\n    optimizer = prepare_optimizer(model)\n    evaluator = TorchEvaluator(train, optimizer, training_step)\n    sub_pruner = TaylorPruner(model, p_config_list, evaluator, training_steps=100)\n    scheduled_pruner = AGPPruner(sub_pruner, interval_steps=100, total_times=30)\n\n    q_config_list = [{\n        'op_types': ['Conv2d'],\n        'quant_dtype': 'int8',\n        'target_names': ['_input_'],\n        'granularity': 'per_channel'\n    }, {\n        'op_types': ['Conv2d'],\n        'quant_dtype': 'int8',\n        'target_names': ['weight'],\n        'granularity': 'out_channel'\n    }, {\n        'op_types': ['BatchNorm2d'],\n        'quant_dtype': 'int8',\n        'target_names': ['_output_'],\n        'granularity': 'per_channel'\n    }]\n\n    quantizer = QATQuantizer.from_compressor(scheduled_pruner, q_config_list, quant_start_step=100)\n\n    def teacher_predict(batch, teacher_model):\n        return teacher_model(batch[0])\n\n    d_config_list = [{\n        'op_types': ['Conv2d'],\n        'lambda': 0.1,\n        'apply_method': 'mse',\n    }]\n    distiller = DynamicLayerwiseDistiller.from_compressor(quantizer, d_config_list, teacher_model, teacher_predict, 0.1)\n\n    distiller.compress(max_steps=100 * 60, max_epochs=None)\n    distiller.unwrap_model()\n    distiller.unwrap_teacher_model()\n\n    masks = scheduled_pruner.get_masks()\n    speedup = ModelSpeedup(model, dummy_input, masks)\n    model = speedup.speedup_model()\n\n    print('Compressed model paramater number: ', sum([param.numel() for param in model.parameters()]))\n    print('Compressed model without finetuning & qsim acc: ', evaluate(model, test_loader), '%')\n\n    calibration_config = quantizer.get_calibration_config()\n\n    def trans(calibration_config, speedup: ModelSpeedup):\n        for node, node_info in speedup.node_infos.items():\n            if node.op == 'call_module' and node.target in calibration_config:\n                \n                input_mask = speedup.node_infos[node.args[0]].output_masks\n                param_mask = node_info.param_masks\n                output_mask = node_info.output_masks\n\n                module_cali_config = calibration_config[node.target]\n                if '_input_0' in module_cali_config:\n                    reduce_dims = list(range(len(input_mask.shape)))\n                    reduce_dims.remove(1)\n                    idxs = torch.nonzero(input_mask.sum(reduce_dims), as_tuple=True)[0].cpu()\n                    module_cali_config['_input_0']['scale'] = module_cali_config['_input_0']['scale'].index_select(1, idxs)\n                    module_cali_config['_input_0']['zero_point'] = module_cali_config['_input_0']['zero_point'].index_select(1, idxs)\n                if '_output_0' in module_cali_config:\n                    reduce_dims = list(range(len(output_mask.shape)))\n                    reduce_dims.remove(1)\n                    idxs = torch.nonzero(output_mask.sum(reduce_dims), as_tuple=True)[0].cpu()\n                    module_cali_config['_output_0']['scale'] = module_cali_config['_output_0']['scale'].index_select(1, idxs)\n                    module_cali_config['_output_0']['zero_point'] = module_cali_config['_output_0']['zero_point'].index_select(1, idxs)\n                if 'weight' in module_cali_config:\n                    reduce_dims = list(range(len(param_mask['weight'].shape)))\n                    reduce_dims.remove(0)\n                    idxs = torch.nonzero(param_mask['weight'].sum(reduce_dims), as_tuple=True)[0].cpu()\n                    module_cali_config['weight']['scale'] = module_cali_config['weight']['scale'].index_select(0, idxs)\n                    module_cali_config['weight']['zero_point'] = module_cali_config['weight']['zero_point'].index_select(0, idxs)\n                if 'bias' in module_cali_config:\n                    idxs = torch.nonzero(param_mask['bias'], as_tuple=True)[0].cpu()\n                    module_cali_config['bias']['scale'] = module_cali_config['bias']['scale'].index_select(0, idxs)\n                    module_cali_config['bias']['zero_point'] = module_cali_config['bias']['zero_point'].index_select(0, idxs)\n        return calibration_config\n\n    calibration_config = trans(calibration_config, speedup)\n\n    sim_quantizer = Quantizer(model, q_config_list)\n    sim_quantizer.update_calibration_config(calibration_config)\n\n    print('Compressed model without finetuning acc: ', evaluate(model, test_loader), '%')",
  "\n\"\"\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport metpy.calc as mpcalc\nfrom metpy.cbook import get_test_data\nfrom metpy.plots import add_metpy_logo, SkewT\nfrom metpy.units import units\n\ncol_names = ['pressure', 'height', 'temperature', 'dewpoint', 'direction', 'speed']\n\ndf = pd.read_fwf(get_test_data('may4_sounding.txt', as_file_obj=False),\n                 skiprows=5, usecols=[0, 1, 2, 3, 6, 7], names=col_names)\n\ndf = df.dropna(subset=('temperature', 'dewpoint', 'direction', 'speed'), how='all'\n               ).reset_index(drop=True)\n\np = df['pressure'].values * units.hPa\nT = df['temperature'].values * units.degC\nTd = df['dewpoint'].values * units.degC\nwind_speed = df['speed'].values * units.knots\nwind_dir = df['direction'].values * units.degrees\nu, v = mpcalc.wind_components(wind_speed, wind_dir)\n\nfig = plt.figure(figsize=(9, 9))\nadd_metpy_logo(fig, 115, 100)\nskew = SkewT(fig, rotation=45)\n\nskew.plot(p, T, 'r')\nskew.plot(p, Td, 'g')\nskew.plot_barbs(p, u, v)\nskew.ax.set_ylim(1000, 100)\nskew.ax.set_xlim(-40, 60)\n\nskew.ax.set_xlabel(f'Temperature ({T.units:~P})')\nskew.ax.set_ylabel(f'Pressure ({p.units:~P})')\n\nlcl_pressure, lcl_temperature = mpcalc.lcl(p[0], T[0], Td[0])\nskew.plot(lcl_pressure, lcl_temperature, 'ko', markerfacecolor='black')\n\nprof = mpcalc.parcel_profile(p, T[0], Td[0]).to('degC')\nskew.plot(p, prof, 'k', linewidth=2)\n\nskew.shade_cin(p, T, prof, Td)\nskew.shade_cape(p, T, prof)\n\nskew.ax.axvline(0, color='c', linestyle='--', linewidth=2)\n\nskew.plot_dry_adiabats()\nskew.plot_moist_adiabats()\nskew.plot_mixing_lines()\n\nplt.show()",
  "\n\"\"\n\nfrom absl import app\nfrom absl import flags\nimport acme\nfrom acme import specs\nfrom acme.agents.jax import actor_core as actor_core_lib\nfrom acme.agents.jax import actors\nfrom acme.agents.jax import crr\nfrom acme.datasets import tfds\nfrom acme.examples.offline import helpers as gym_helpers\nfrom acme.jax import variable_utils\nfrom acme.types import Transition\nfrom acme.utils import loggers\nimport haiku as hk\nimport jax\nimport optax\nimport rlds\n\nflags.DEFINE_integer('batch_size', 64, 'Batch size.')\nflags.DEFINE_integer('evaluate_every', 20, 'Evaluation period.')\nflags.DEFINE_integer('evaluation_episodes', 10, 'Evaluation episodes.')\nflags.DEFINE_integer(\n    'num_demonstrations', 10,\n    'Number of demonstration episodes to load from the dataset. If None, loads the full dataset.'\n)\nflags.DEFINE_integer('seed', 0, 'Random seed for learner and evaluator.')\n\nflags.DEFINE_float('policy_learning_rate', 3e-5, 'Policy learning rate.')\nflags.DEFINE_float('critic_learning_rate', 3e-4, 'Critic learning rate.')\nflags.DEFINE_float('discount', 0.99, 'Discount.')\nflags.DEFINE_integer('target_update_period', 100, 'Target update periode.')\nflags.DEFINE_integer('grad_updates_per_batch', 1, 'Grad updates per batch.')\nflags.DEFINE_bool(\n    'use_sarsa_target', True,\n    'Compute on-policy target using iterator actions rather than sampled '\n    'actions.'\n)\n\nflags.DEFINE_string('env_name', 'HalfCheetah-v2',\n                    'Gym mujoco environment name.')\nflags.DEFINE_string(\n    'dataset_name', 'd4rl_mujoco_halfcheetah/v2-medium',\n    'D4rl dataset name. Can be any locomotion dataset from '\n    'https://www.tensorflow.org/datasets/catalog/overview\n\nFLAGS = flags.FLAGS\n\ndef _add_next_action_extras(double_transitions: Transition) -> Transition:\n  return Transition(\n      observation=double_transitions.observation[0],\n      action=double_transitions.action[0],\n      reward=double_transitions.reward[0],\n      discount=double_transitions.discount[0],\n      next_observation=double_transitions.next_observation[0],\n      extras={'next_action': double_transitions.action[1]})\n\ndef main(_):\n  key = jax.random.PRNGKey(FLAGS.seed)\n  key_demonstrations, key_learner = jax.random.split(key, 2)\n\n  environment = gym_helpers.make_environment(task=FLAGS.env_name)\n  environment_spec = specs.make_environment_spec(environment)\n\n  transitions = tfds.get_tfds_dataset(\n      FLAGS.dataset_name, FLAGS.num_demonstrations)\n  double_transitions = rlds.transformations.batch(\n      transitions, size=2, shift=1, drop_remainder=True)\n  transitions = double_transitions.map(_add_next_action_extras)\n  demonstrations = tfds.JaxInMemoryRandomSampleIterator(\n      transitions, key=key_demonstrations, batch_size=FLAGS.batch_size)\n\n  networks = crr.make_networks(environment_spec)\n\n  policy_loss_coeff_fn = crr.policy_loss_coeff_advantage_exp\n\n  learner = crr.CRRLearner(\n      networks=networks,\n      random_key=key_learner,\n      discount=FLAGS.discount,\n      target_update_period=FLAGS.target_update_period,\n      policy_loss_coeff_fn=policy_loss_coeff_fn,\n      iterator=demonstrations,\n      policy_optimizer=optax.adam(FLAGS.policy_learning_rate),\n      critic_optimizer=optax.adam(FLAGS.critic_learning_rate),\n      grad_updates_per_batch=FLAGS.grad_updates_per_batch,\n      use_sarsa_target=FLAGS.use_sarsa_target)\n\n  def evaluator_network(\n      params: hk.Params, key: jax.Array, observation: jax.Array\n  ) -> jax.Array:\n    dist_params = networks.policy_network.apply(params, observation)\n    return networks.sample_eval(dist_params, key)\n\n  actor_core = actor_core_lib.batched_feed_forward_to_actor_core(\n      evaluator_network)\n  variable_client = variable_utils.VariableClient(\n      learner, 'policy', device='cpu')\n  evaluator = actors.GenericActor(\n      actor_core, key, variable_client, backend='cpu')\n\n  eval_loop = acme.EnvironmentLoop(\n      environment=environment,\n      actor=evaluator,\n      logger=loggers.TerminalLogger('evaluation', time_delta=0.))\n\n  while True:\n    for _ in range(FLAGS.evaluate_every):\n      learner.step()\n    eval_loop.run(FLAGS.evaluation_episodes)\n\nif __name__ == '__main__':\n  app.run(main)",
  "\n\"\"\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pprint import pprint\nfrom pysteps import io, nowcasts, rcparams, verification\nfrom pysteps.motion.lucaskanade import dense_lucaskanade\nfrom pysteps.postprocessing import ensemblestats\nfrom pysteps.utils import conversion, dimension, transformation\nfrom pysteps.visualization import plot_precip_field\n\ndate = datetime.strptime(\"201607112100\", \"%Y%m%d%H%M\")\ndata_source = rcparams.data_sources[\"mch\"]\nn_ens_members = 20\nn_leadtimes = 6\nseed = 24\n\nroot_path = data_source[\"root_path\"]\npath_fmt = data_source[\"path_fmt\"]\nfn_pattern = data_source[\"fn_pattern\"]\nfn_ext = data_source[\"fn_ext\"]\nimporter_name = data_source[\"importer\"]\nimporter_kwargs = data_source[\"importer_kwargs\"]\ntimestep = data_source[\"timestep\"]\n\nfns = io.find_by_date(\n    date, root_path, path_fmt, fn_pattern, fn_ext, timestep, num_prev_files=2\n)\n\nimporter = io.get_method(importer_name, \"importer\")\nR, _, metadata = io.read_timeseries(fns, importer, **importer_kwargs)\n\nR, metadata = conversion.to_rainrate(R, metadata)\n\nR, metadata = dimension.aggregate_fields_space(R, metadata, 2000)\n\nplot_precip_field(R[-1, :, :], geodata=metadata)\nplt.show()\n\nR, metadata = transformation.dB_transform(R, metadata, threshold=0.1, zerovalue=-15.0)\n\nR[~np.isfinite(R)] = -15.0\n\npprint(metadata)\n\nV = dense_lucaskanade(R)\n\nnowcast_method = nowcasts.get_method(\"steps\")\nR_f = nowcast_method(\n    R[-3:, :, :],\n    V,\n    n_leadtimes,\n    n_ens_members,\n    n_cascade_levels=6,\n    R_thr=-10.0,\n    kmperpixel=2.0,\n    timestep=timestep,\n    decomp_method=\"fft\",\n    bandpass_filter_method=\"gaussian\",\n    noise_method=\"nonparametric\",\n    vel_pert_method=\"bps\",\n    mask_method=\"incremental\",\n    seed=seed,\n)\n\nR_f = transformation.dB_transform(R_f, threshold=-10.0, inverse=True)[0]\n\nfig = plt.figure()\nfor i in range(4):\n    ax = fig.add_subplot(221 + i)\n    ax.set_title(\"Member %02d\" % i)\n    plot_precip_field(R_f[i, -1, :, :], geodata=metadata, colorbar=False, axis=\"off\")\nplt.tight_layout()\nplt.show()\n\nfns = io.archive.find_by_date(\n    date,\n    root_path,\n    path_fmt,\n    fn_pattern,\n    fn_ext,\n    timestep,\n    0,\n    num_next_files=n_leadtimes,\n)\n\nR_o, _, metadata_o = io.read_timeseries(fns, importer, **importer_kwargs)\n\nR_o, metadata_o = conversion.to_rainrate(R_o, metadata_o)\n\nR_o, metadata_o = dimension.aggregate_fields_space(R_o, metadata_o, 2000)\n\nP_f = ensemblestats.excprob(R_f[:, -1, :, :], 0.1, ignore_nan=True)\n\nroc = verification.ROC_curve_init(0.1, n_prob_thrs=10)\nverification.ROC_curve_accum(roc, P_f, R_o[-1, :, :])\nfig, ax = plt.subplots()\nverification.plot_ROC(roc, ax, opt_prob_thr=True)\nax.set_title(\"ROC curve (+%i min)\" % (n_leadtimes * timestep))\nplt.show()\n\nreldiag = verification.reldiag_init(0.1)\nverification.reldiag_accum(reldiag, P_f, R_o[-1, :, :])\nfig, ax = plt.subplots()\nverification.plot_reldiag(reldiag, ax)\nax.set_title(\"Reliability diagram (+%i min)\" % (n_leadtimes * timestep))\nplt.show()\n\nrankhist = verification.rankhist_init(R_f.shape[0], 0.1)\nverification.rankhist_accum(rankhist, R_f[:, -1, :, :], R_o[-1, :, :])\nfig, ax = plt.subplots()\nverification.plot_rankhist(rankhist, ax)\nax.set_title(\"Rank histogram (+%i min)\" % (n_leadtimes * timestep))\nplt.show()\n",
  "\"\"\n\nimport os\nimport tempfile\nimport pyaedt\n\nproject_dir =  pyaedt.generate_unique_folder_name()\naedb_project = pyaedt.downloads.download_file('edb/ANSYS-HSD_V1.aedb',destination=project_dir)\n\nproject_name = pyaedt.generate_unique_name(\"HSD\")\noutput_edb = os.path.join(project_dir, project_name + '.aedb')\noutput_q3d = os.path.join(project_dir, project_name + '_q3d.aedt')\n\nedb = pyaedt.Edb(aedb_project, edbversion=\"2023.2\")\nedb.cutout([\"CLOCK_I2C_SCL\", \"CLOCK_I2C_SDA\"], [\"GND\"], output_aedb_path=output_edb,\n                              use_pyaedt_extent_computing=True, )\n\npin_u13_scl = [i for i in edb.components[\"U13\"].pins.values() if i.net_name == \"CLOCK_I2C_SCL\"]\npin_u1_scl = [i for i in edb.components[\"U1\"].pins.values() if i.net_name == \"CLOCK_I2C_SCL\"]\npin_u13_sda = [i for i in edb.components[\"U13\"].pins.values() if i.net_name == \"CLOCK_I2C_SDA\"]\npin_u1_sda = [i for i in edb.components[\"U1\"].pins.values() if i.net_name == \"CLOCK_I2C_SDA\"]\n\nlocation_u13_scl = [i * 1000 for i in pin_u13_scl[0].position]\nlocation_u13_scl.append(edb.components[\"U13\"].upper_elevation * 1000)\n\nlocation_u1_scl = [i * 1000 for i in pin_u1_scl[0].position]\nlocation_u1_scl.append(edb.components[\"U1\"].upper_elevation * 1000)\n\nlocation_u13_sda = [i * 1000 for i in pin_u13_sda[0].position]\nlocation_u13_sda.append(edb.components[\"U13\"].upper_elevation * 1000)\n\nlocation_u1_sda = [i * 1000 for i in pin_u1_sda[0].position]\nlocation_u1_sda.append(edb.components[\"U1\"].upper_elevation * 1000)\n\nedb.save_edb()\nedb.close_edb()\n\nh3d = pyaedt.Hfss3dLayout(output_edb, specified_version=\"2023.2\", non_graphical=True, new_desktop_session=True)\n\nsetup = h3d.create_setup()\nsetup.export_to_q3d(output_q3d, keep_net_name=True)\nh3d.close_project()\n\nq3d = pyaedt.Q3d(output_q3d)\nq3d.plot(show=False, objects=[\"CLOCK_I2C_SCL\", \"CLOCK_I2C_SDA\"],\n         export_path=os.path.join(q3d.working_directory, \"Q3D.jpg\"), plot_air_objects=False)\n\nf1 = q3d.modeler.get_faceid_from_position(location_u13_scl, obj_name=\"CLOCK_I2C_SCL\")\nq3d.source(f1, net_name=\"CLOCK_I2C_SCL\")\nf1 = q3d.modeler.get_faceid_from_position(location_u13_sda, obj_name=\"CLOCK_I2C_SDA\")\nq3d.source(f1, net_name=\"CLOCK_I2C_SDA\")\nf1 = q3d.modeler.get_faceid_from_position(location_u1_scl, obj_name=\"CLOCK_I2C_SCL\")\nq3d.sink(f1, net_name=\"CLOCK_I2C_SCL\")\nf1 = q3d.modeler.get_faceid_from_position(location_u1_sda, obj_name=\"CLOCK_I2C_SDA\")\nq3d.sink(f1, net_name=\"CLOCK_I2C_SDA\")\n\nsetup = q3d.create_setup()\nsetup.dc_enabled = True\nsetup.capacitance_enabled = False\nsweep = setup.add_sweep()\nsweep.add_subrange(\"LinearStep\", 0, end=2, count=0.05, unit=\"GHz\", save_single_fields=False, clear=True)\nsetup.analyze()\n\ntraces_acl = q3d.post.available_report_quantities(quantities_category=\"ACL Matrix\")\nsolution = q3d.post.get_solution_data(traces_acl)\nsolution.plot()\n\ntraces_acr = q3d.post.available_report_quantities(quantities_category=\"ACR Matrix\")\nsolution2 = q3d.post.get_solution_data(traces_acr)\nsolution2.plot()\n\nq3d.release_desktop()",
  "import blenderproc as bproc\nimport argparse\nimport os\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument('bop_parent_path', help=\"Path to the bop datasets parent directory\")\nparser.add_argument('cc_textures_path', default=\"resources/cctextures\", help=\"Path to downloaded cc textures\")\nparser.add_argument('output_dir', help=\"Path to where the final files will be saved \")\nparser.add_argument('--num_scenes', type=int, default=2000, help=\"How many scenes with 25 images each to generate\")\nargs = parser.parse_args()\n\nbproc.init()\n\ntarget_bop_objs = bproc.loader.load_bop_objs(bop_dataset_path = os.path.join(args.bop_parent_path, 'itodd'), mm2m = True)\n\ntless_dist_bop_objs = bproc.loader.load_bop_objs(bop_dataset_path = os.path.join(args.bop_parent_path, 'tless'), model_type = 'cad', mm2m = True)\n\nbproc.loader.load_bop_intrinsics(bop_dataset_path = os.path.join(args.bop_parent_path, 'itodd'))\n\nfor obj in (target_bop_objs + tless_dist_bop_objs):\n    obj.set_shading_mode('auto')\n    obj.hide(True)\n    \nroom_planes = [bproc.object.create_primitive('PLANE', scale=[2, 2, 1]),\n               bproc.object.create_primitive('PLANE', scale=[2, 2, 1], location=[0, -2, 2], rotation=[-1.570796, 0, 0]),\n               bproc.object.create_primitive('PLANE', scale=[2, 2, 1], location=[0, 2, 2], rotation=[1.570796, 0, 0]),\n               bproc.object.create_primitive('PLANE', scale=[2, 2, 1], location=[2, 0, 2], rotation=[0, -1.570796, 0]),\n               bproc.object.create_primitive('PLANE', scale=[2, 2, 1], location=[-2, 0, 2], rotation=[0, 1.570796, 0])]\nfor plane in room_planes:\n    plane.enable_rigidbody(False, collision_shape='BOX', mass=1.0, friction = 100.0, linear_damping = 0.99, angular_damping = 0.99)\n\nlight_plane = bproc.object.create_primitive('PLANE', scale=[3, 3, 1], location=[0, 0, 10])\nlight_plane.set_name('light_plane')\nlight_plane_material = bproc.material.create('light_material')\n\nlight_point = bproc.types.Light()\nlight_point.set_energy(20)\n\ncc_textures = bproc.loader.load_ccmaterials(args.cc_textures_path)\n\ndef sample_pose_func(obj: bproc.types.MeshObject):\n    min = np.random.uniform([-0.3, -0.3, 0.0], [-0.2, -0.2, 0.0])\n    max = np.random.uniform([0.2, 0.2, 0.4], [0.3, 0.3, 0.6])\n    obj.set_location(np.random.uniform(min, max))\n    obj.set_rotation_euler(bproc.sampler.uniformSO3())\n    \nbproc.renderer.enable_depth_output(activate_antialiasing=False)\nbproc.renderer.set_max_amount_of_samples(50)\n\nfor i in range(args.num_scenes):\n\n    sampled_target_bop_objs = list(np.random.choice(target_bop_objs, size=25, replace=False))\n    sampled_distractor_bop_objs = list(np.random.choice(tless_dist_bop_objs, size=5, replace=False))\n\n    for obj in (sampled_target_bop_objs + sampled_distractor_bop_objs):        \n        mat = obj.get_materials()[0]\n        if obj.get_cp(\"bop_dataset_name\") in ['itodd', 'tless']:\n            grey_col = np.random.uniform(0.1, 0.7)   \n            mat.set_principled_shader_value(\"Base Color\", [grey_col, grey_col, grey_col, 1])      \n        mat.set_principled_shader_value(\"Roughness\", np.random.uniform(0, 0.5))\n        if obj.get_cp(\"bop_dataset_name\") == 'itodd':  \n            mat.set_principled_shader_value(\"Specular\", np.random.uniform(0.3, 1.0))\n            mat.set_principled_shader_value(\"Metallic\", np.random.uniform(0, 1.0))\n        if obj.get_cp(\"bop_dataset_name\") == 'tless':\n            mat.set_principled_shader_value(\"Metallic\", np.random.uniform(0, 0.5))\n            \n        obj.enable_rigidbody(True, mass=1.0, friction = 100.0, linear_damping = 0.99, angular_damping = 0.99, collision_margin=0.0005)\n        obj.hide(False)\n    \n    light_plane_material.make_emissive(emission_strength=np.random.uniform(0.1,0.5), \n                                    emission_color=np.random.uniform([0.5, 0.5, 0.5, 1.0], [1.0, 1.0, 1.0, 1.0]))  \n    light_plane.replace_materials(light_plane_material)\n    light_point.set_color(np.random.uniform([0.5,0.5,0.5],[1,1,1]))\n    location = bproc.sampler.shell(center = [0, 0, 0], radius_min = 0.5, radius_max = 1.5,\n                            elevation_min = 5, elevation_max = 89)\n    light_point.set_location(location)\n\n    random_cc_texture = np.random.choice(cc_textures)\n    for plane in room_planes:\n        plane.replace_materials(random_cc_texture)\n\n    bproc.object.sample_poses(objects_to_sample = sampled_target_bop_objs + sampled_distractor_bop_objs,\n                            sample_pose_func = sample_pose_func, \n                            max_tries = 1000)\n            \n    bproc.object.simulate_physics_and_fix_final_poses(min_simulation_time=3,\n                                                      max_simulation_time=10,\n                                                      check_object_interval=1,\n                                                      substeps_per_frame = 50,\n                                                      solver_iters=25)\n\n    bop_bvh_tree = bproc.object.create_bvh_tree_multi_objects(sampled_target_bop_objs + sampled_distractor_bop_objs)\n\n    cam_poses = 0\n    while cam_poses < 25:\n        \n        location = bproc.sampler.shell(center = [0, 0, 0],\n                                radius_min = 0.64,\n                                radius_max = 0.78,\n                                elevation_min = 5,\n                                elevation_max = 89)\n        \n        poi = bproc.object.compute_poi(np.random.choice(sampled_target_bop_objs, size=15, replace=False))\n        \n        rotation_matrix = bproc.camera.rotation_from_forward_vec(poi - location, inplane_rot=np.random.uniform(-3.14159, 3.14159))\n        \n        cam2world_matrix = bproc.math.build_transformation_mat(location, rotation_matrix)\n        \n        if bproc.camera.perform_obstacle_in_view_check(cam2world_matrix, {\"min\": 0.3}, bop_bvh_tree):\n            \n            bproc.camera.add_camera_pose(cam2world_matrix, frame=cam_poses)\n            cam_poses += 1\n\n    data = bproc.renderer.render()\n\n    bproc.writer.write_bop(os.path.join(args.output_dir, 'bop_data'),\n                           target_objects = sampled_target_bop_objs,\n                           dataset = 'itodd',\n                           depth_scale = 0.1,\n                           depths = data[\"depth\"],\n                           colors = data[\"colors\"], \n                           color_file_format = \"JPEG\",\n                           ignore_dist_thres = 10)\n    \n    for obj in (sampled_target_bop_objs + sampled_distractor_bop_objs):      \n        obj.disable_rigidbody()\n        obj.hide(True)",
  "\n\"\"\n\nimport numpy as np\nimport pyscf\nfrom pyscf import fci, lib\n\nmyhf1 = pyscf.M(atom='H 0 0 0; F 0 0 1.1', basis='6-31g', verbose=0).RHF().run()\ne1, ci1 = fci.FCI(myhf1.mol, myhf1.mo_coeff).kernel()\nprint('FCI energy of mol1', e1)\n\nmyhf2 = pyscf.M(atom='H 0 0 0; F 0 0 1.2', basis='6-31g', verbose=0).RHF().run()\n\ns12 = pyscf.gto.intor_cross('cint1e_ovlp_sph', myhf1.mol, myhf2.mol)\ns12 = myhf1.mo_coeff.T.dot(s12).dot(myhf2.mo_coeff)\nnelec = myhf2.mol.nelectron\nci2 = fci.addons.transform_ci(ci1, nelec, s12)\n\nprint('alpha-string, beta-string,  CI coefficients')\nnorb = myhf2.mo_coeff.shape[1]\nfor c,stra,strb in fci.addons.large_ci(ci2, norb, nelec):\n    print(stra, strb, c)\n\nmol = pyscf.M(atom=['H 0 0 %f'%x for x in range(6)], basis='6-31g')\nmf = mol.RHF().run()\nh1 = mf.mo_coeff.T.dot(mf.get_hcore()).dot(mf.mo_coeff)\nh2 = pyscf.lib.einsum('pqrs,pi,qj,rk,sl->ijkl', mf.mol.intor('int2e'),\n                      mf.mo_coeff, mf.mo_coeff, mf.mo_coeff, mf.mo_coeff)\n\nnorb = 6\nnelec = (3, 3)\ncivec = fci.FCI(mol).kernel(h1[:norb,:norb], h2[:norb,:norb,:norb,:norb], norb, nelec,\n                            ecore=mol.energy_nuc())[1]\n\nu = np.zeros((6, 8))\nfor i in range(6):\n    u[i, i] = 1\ncivec1 = fci.addons.transform_ci(civec, nelec, u)\nprint(civec1.shape)  \n\nnorb = 8\nnelec = (3, 3)\ncivec2 = fci.FCI(mol).kernel(h1[:norb,:norb], h2[:norb,:norb,:norb,:norb], norb, nelec,\n                              ecore=mol.energy_nuc())[1]\nprint(np.dot(civec1.ravel(), civec2.ravel()))\n\nstrsa = fci.cistring.make_strings([0,1,2,4,5,7], nelec[0])\nstrsb = fci.cistring.make_strings([0,1,2,4,5,7], nelec[1])\naddrsa = fci.cistring.strs2addr(8, nelec[0], strsa)\naddrsb = fci.cistring.strs2addr(8, nelec[1], strsa)\ncivec1 = np.zeros_like(civec2)\ncivec1[addrsa[:,None], addrsb] = civec\n\nu = np.zeros((6, 8))\nu[0,0] = 1\nu[1,1] = 1\nu[2,2] = 1\nu[3,4] = 1\nu[4,5] = 1\nu[5,7] = 1\ncivec1_ref = fci.addons.transform_ci(civec, nelec, u)\nprint(np.allclose(civec1, civec1_ref))\n",
  "\"\"\n\nfrom SimPEG import (\n    Mesh, Maps, Utils,\n    DataMisfit, Regularization, Optimization,\n    InvProblem, Directives, Inversion\n)\nfrom SimPEG.EM.Static import DC, Utils as DCUtils\nimport numpy as np\nimport matplotlib.pyplot as plt\ntry:\n    from pymatsolver import Pardiso as Solver\nexcept ImportError:\n    from SimPEG import SolverLU as Solver\n\nnp.random.seed(12345)\n\ncsx, csy, csz = 1., 1., 0.5\n\nncx, ncy, ncz = 41, 31, 21\n\nnpad = 7\n\nhx = [(csx, npad, -1.5), (csx, ncx), (csx, npad, 1.5)]\nhy = [(csy, npad, -1.5), (csy, ncy), (csy, npad, 1.5)]\nhz = [(csz, npad, -1.5), (csz, ncz)]\n\nmesh = Mesh.TensorMesh([hx, hy, hz], x0=\"CCN\")\n\nx0, y0, z0, r0 = -6., 0., -3.5, 3.\nx1, y1, z1, r1 = 6., 0., -3.5, 3.\n\nln_sigback = -5.\nln_sigc = -3.\nln_sigr = -6.\n\nmtrue = ln_sigback * np.ones(mesh.nC)\n\ncsph = (np.sqrt((mesh.gridCC[:, 0] - x0)**2. + (mesh.gridCC[:, 1] - y0)**2. +\n                (mesh.gridCC[:, 2] - z0)**2.)) < r0\nmtrue[csph] = ln_sigc * np.ones_like(mtrue[csph])\n\nrsph = (np.sqrt((mesh.gridCC[:, 0] - x1)**2. + (mesh.gridCC[:, 1] - y1)**2. +\n                (mesh.gridCC[:, 2] - z1)**2.)) < r1\nmtrue[rsph] = ln_sigr * np.ones_like(mtrue[rsph])\n\nxmin, xmax = -20., 20.\nymin, ymax = -15., 15.\nzmin, zmax = -10., 0.\nxyzlim = np.r_[[[xmin, xmax], [ymin, ymax], [zmin, zmax]]]\nactind, meshCore = Utils.meshutils.ExtractCoreMesh(xyzlim, mesh)\n\ndef getCylinderPoints(xc, zc, r):\n    xLocOrig1 = np.arange(-r, r + r / 10., r / 10.)\n    xLocOrig2 = np.arange(r, -r - r / 10., -r / 10.)\n    \n    zLoc1 = np.sqrt(-xLocOrig1**2. + r**2.) + zc\n    \n    zLoc2 = -np.sqrt(-xLocOrig2**2. + r**2.) + zc\n    \n    xLoc1 = xLocOrig1 + xc * np.ones_like(xLocOrig1)\n    xLoc2 = xLocOrig2 + xc * np.ones_like(xLocOrig2)\n\n    topHalf = np.vstack([xLoc1, zLoc1]).T\n    topHalf = topHalf[0:-1, :]\n    bottomHalf = np.vstack([xLoc2, zLoc2]).T\n    bottomHalf = bottomHalf[0:-1, :]\n\n    cylinderPoints = np.vstack([topHalf, bottomHalf])\n    cylinderPoints = np.vstack([cylinderPoints, topHalf[0, :]])\n    return cylinderPoints\n\nxmin, xmax = -15., 15.\nymin, ymax = 0., 0.\nzmin, zmax = 0, 0\nendl = np.array([[xmin, ymin, zmin], [xmax, ymax, zmax]])\nsurvey1 = DCUtils.gen_DCIPsurvey(endl, \"dipole-dipole\", dim=mesh.dim,\n                                 a=3, b=3, n=8)\n\nxmin, xmax = -15., 15.\nymin, ymax = 5., 5.\nzmin, zmax = 0, 0\nendl = np.array([[xmin, ymin, zmin], [xmax, ymax, zmax]])\nsurvey2 = DCUtils.gen_DCIPsurvey(endl, \"dipole-dipole\", dim=mesh.dim,\n                                 a=3, b=3, n=8)\n\nxmin, xmax = -15., 15.\nymin, ymax = -5., -5.\nzmin, zmax = 0, 0\nendl = np.array([[xmin, ymin, zmin], [xmax, ymax, zmax]])\nsurvey3 = DCUtils.gen_DCIPsurvey(endl, \"dipole-dipole\", dim=mesh.dim,\n                                 a=3, b=3, n=8)\n\nsurvey = DC.Survey(survey1.srcList + survey2.srcList + survey3.srcList)\n\nexpmap = Maps.ExpMap(mesh)\nmapactive = Maps.InjectActiveCells(mesh=mesh, indActive=actind,\n                                   valInactive=-5.)\nmapping = expmap * mapactive\nproblem = DC.Problem3D_CC(mesh, sigmaMap=mapping)\nproblem.pair(survey)\nproblem.Solver = Solver\n\nsurvey.dpred(mtrue[actind])\nsurvey.makeSyntheticData(mtrue[actind], std=0.05, force=True)\n\nm0 = np.median(ln_sigback) * np.ones(mapping.nP)\n\ndmis = DataMisfit.l2_DataMisfit(survey)\n\nregT = Regularization.Simple(mesh, indActive=actind, alpha_s=1e-6,\n                             alpha_x=1., alpha_y=1., alpha_z=1.)\n\nopt = Optimization.InexactGaussNewton(maxIter=10)\n\nopt.remember('xc')\ninvProb = InvProblem.BaseInvProblem(dmis, regT, opt)\n\nbeta = Directives.BetaEstimate_ByEig(beta0_ratio=1e+1)\nTarget = Directives.TargetMisfit()\nbetaSched = Directives.BetaSchedule(coolingFactor=5., coolingRate=2)\n\ninv = Inversion.BaseInversion(invProb, directiveList=[beta, Target,\n                                                      betaSched])\n\nminv = inv.run(m0)\n\nfig, ax = plt.subplots(2, 2, figsize=(12, 6))\nax = Utils.mkvc(ax)\n\ncyl0v = getCylinderPoints(x0, z0, r0)\ncyl1v = getCylinderPoints(x1, z1, r1)\n\ncyl0h = getCylinderPoints(x0, y0, r0)\ncyl1h = getCylinderPoints(x1, y1, r1)\n\nclim = [(mtrue[actind]).min(), (mtrue[actind]).max()]\n\ndat = meshCore.plotSlice(((mtrue[actind])), ax=ax[0], normal='Y', clim=clim,\n                         ind=int(ncy / 2))\nax[0].set_title('Ground Truth, Vertical')\nax[0].set_aspect('equal')\n\nmeshCore.plotSlice((minv), ax=ax[1], normal='Y', clim=clim, ind=int(ncy / 2))\nax[1].set_aspect('equal')\nax[1].set_title('Inverted Model, Vertical')\n\nmeshCore.plotSlice(((mtrue[actind])), ax=ax[2], normal='Z', clim=clim,\n                   ind=int(ncz / 2))\nax[2].set_title('Ground Truth, Horizontal')\nax[2].set_aspect('equal')\n\nmeshCore.plotSlice((minv), ax=ax[3], normal='Z', clim=clim, ind=int(ncz / 2))\nax[3].set_title('Inverted Model, Horizontal')\nax[3].set_aspect('equal')\n\nfor i in range(2):\n    ax[i].plot(cyl0v[:, 0], cyl0v[:, 1], 'k--')\n    ax[i].plot(cyl1v[:, 0], cyl1v[:, 1], 'k--')\nfor i in range(2, 4):\n    ax[i].plot(cyl1h[:, 0], cyl1h[:, 1], 'k--')\n    ax[i].plot(cyl0h[:, 0], cyl0h[:, 1], 'k--')\n\nfig.subplots_adjust(right=0.8)\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\ncb = plt.colorbar(dat[0], ax=cbar_ax)\ncb.set_label('ln conductivity')\n\ncbar_ax.axis('off')\n\nplt.show()",
  "\"\"\n\n__version__ = '0.1.0'\nfrom docopt import docopt\nargs = docopt(__doc__, version=__version__)\n\nfrom ast import literal_eval\n\nfrom pUtilities import show_3D_array\n\nimport numpy as np\n\nexec('from sirf.' + args['--engine'] + ' import *')\n\ndata_path = args['--path']\nif data_path is None:\n    \n    data_path = examples_data_path('PET') + '/mMR'\nlist_file = args['--list']\nsino_file = args['--sino']\nrand_file = args['--rand']\ntmpl_file = args['--tmpl']\nlist_file = existing_filepath(data_path, list_file)\ntmpl_file = existing_filepath(data_path, tmpl_file)\ninterval = literal_eval(args['--interval'])\nstorage = args['--storage']\nshow_plot = not args['--non-interactive']\n\ndef main():\n\n    msg_red = MessageRedirector('info.txt', 'warn.txt', 'errr.txt')\n\n    AcquisitionData.set_storage_scheme(storage)\n\n    lm2sino = ListmodeToSinograms()\n\n    lm2sino.set_input(list_file)\n    lm2sino.set_output_prefix(sino_file)\n    lm2sino.set_template(tmpl_file)\n\n    lm2sino.set_time_interval(interval[0], interval[1])\n\n    lm2sino.flag_on('store_delayeds')\n    lm2sino.flag_off('store_prompts')\n    \n    lm2sino.set_up()\n\n    lm2sino.process()\n\n    delayeds_acq_data = lm2sino.get_output()\n    \n    randoms_estimate_acq_data = lm2sino.estimate_randoms();\n    randoms_estimate_acq_data.write(rand_file)\n    \n    delayeds_acq_array = delayeds_acq_data.as_array()\n    randoms_estimate_acq_array = randoms_estimate_acq_data.as_array()\n    acq_dim = delayeds_acq_array.shape\n    print('acquisition data dimensions: %dx%dx%dx%d' % acq_dim)\n    print('The total number of delayed coincidences and estimated randoms have to be very similar.')\n    print('Let us check this:')\n    print('total delayeds: %.1f, total estimated randoms: %.1f' % (delayeds_acq_array.sum(), randoms_estimate_acq_array.sum()))\n    print('Max values should be somewhat similar, but this depends on statistics of course.')\n    print('max delayeds: %f, max estimated randoms: %f' % (delayeds_acq_array.max(), randoms_estimate_acq_array.max()))\n\n    print('A single sinogram (this will look very different for noisy data)')\n    z = acq_dim[1]//2\n    if show_plot:\n        show_3D_array(np.stack((delayeds_acq_array[0,z,:,:], randoms_estimate_acq_array[0,z,:,:])), titles=('raw delayeds', ' estimated randoms'))\n        pylab.show()\n\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    print('%s' % err.value)",
  "\n\"\"\n\nimport initExample \n\nimport pyqtgraph as pg\nfrom pyqtgraph.Qt import QtCore, QtGui\nimport numpy as np\n\npg.setConfigOptions(antialias=True)\n\nw = pg.GraphicsLayoutWidget(show=True)\nw.setWindowTitle('pyqtgraph example: CustomGraphItem')\nv = w.addViewBox()\nv.setAspectLocked()\n\nclass Graph(pg.GraphItem):\n    def __init__(self):\n        self.dragPoint = None\n        self.dragOffset = None\n        self.textItems = []\n        pg.GraphItem.__init__(self)\n        self.scatter.sigClicked.connect(self.clicked)\n        \n    def setData(self, **kwds):\n        self.text = kwds.pop('text', [])\n        self.data = kwds\n        if 'pos' in self.data:\n            npts = self.data['pos'].shape[0]\n            self.data['data'] = np.empty(npts, dtype=[('index', int)])\n            self.data['data']['index'] = np.arange(npts)\n        self.setTexts(self.text)\n        self.updateGraph()\n        \n    def setTexts(self, text):\n        for i in self.textItems:\n            i.scene().removeItem(i)\n        self.textItems = []\n        for t in text:\n            item = pg.TextItem(t)\n            self.textItems.append(item)\n            item.setParentItem(self)\n        \n    def updateGraph(self):\n        pg.GraphItem.setData(self, **self.data)\n        for i,item in enumerate(self.textItems):\n            item.setPos(*self.data['pos'][i])\n        \n    def mouseDragEvent(self, ev):\n        if ev.button() != QtCore.Qt.LeftButton:\n            ev.ignore()\n            return\n        \n        if ev.isStart():\n            \n            pos = ev.buttonDownPos()\n            pts = self.scatter.pointsAt(pos)\n            if len(pts) == 0:\n                ev.ignore()\n                return\n            self.dragPoint = pts[0]\n            ind = pts[0].data()[0]\n            self.dragOffset = self.data['pos'][ind] - pos\n        elif ev.isFinish():\n            self.dragPoint = None\n            return\n        else:\n            if self.dragPoint is None:\n                ev.ignore()\n                return\n        \n        ind = self.dragPoint.data()[0]\n        self.data['pos'][ind] = ev.pos() + self.dragOffset\n        self.updateGraph()\n        ev.accept()\n        \n    def clicked(self, pts):\n        print(\"clicked: %s\" % pts)\n\ng = Graph()\nv.addItem(g)\n\npos = np.array([\n    [0,0],\n    [10,0],\n    [0,10],\n    [10,10],\n    [5,5],\n    [15,5]\n    ], dtype=float)\n    \nadj = np.array([\n    [0,1],\n    [1,3],\n    [3,2],\n    [2,0],\n    [1,5],\n    [3,5],\n    ])\n    \nsymbols = ['o','o','o','o','t','+']\n\nlines = np.array([\n    (255,0,0,255,1),\n    (255,0,255,255,2),\n    (255,0,255,255,3),\n    (255,255,0,255,2),\n    (255,0,0,255,1),\n    (255,255,255,255,4),\n    ], dtype=[('red',np.ubyte),('green',np.ubyte),('blue',np.ubyte),('alpha',np.ubyte),('width',float)])\n\ntexts = [\"Point %d\" % i for i in range(6)]\n\ng.setData(pos=pos, adj=adj, pen=lines, size=1, symbol=symbols, pxMode=False, text=texts)\n\nif __name__ == '__main__':\n    pg.exec()",
  "\n\"\"\n\nfrom typing import Callable, Iterator, Tuple\n\nfrom absl import flags\nfrom acme import specs\nfrom acme import types\nfrom acme.agents.jax import actor_core as actor_core_lib\nfrom acme.agents.jax import bc\nfrom acme.datasets import tfds\nimport helpers\nfrom absl import app\nfrom acme.jax import experiments\nfrom acme.jax import types as jax_types\nfrom acme.jax import utils\nfrom acme.utils import lp_utils\nimport dm_env\nimport haiku as hk\nimport launchpad as lp\nimport numpy as np\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_bool(\n    'run_distributed', True, 'Should an agent be executed in a distributed '\n    'way. If False, will run single-threaded.')\n\nflags.DEFINE_string('env_name', 'HalfCheetah-v2', 'What environment to run')\nflags.DEFINE_integer('num_demonstrations', 11,\n                     'Number of demonstration trajectories.')\nflags.DEFINE_integer('num_bc_steps', 100_000, 'Number of bc learning steps.')\nflags.DEFINE_integer('num_steps', 0, 'Number of environment steps.')\nflags.DEFINE_integer('batch_size', 64, 'Batch size.')\nflags.DEFINE_float('learning_rate', 1e-4, 'Optimizer learning rate.')\nflags.DEFINE_float('dropout_rate', 0.1, 'Dropout rate of bc network.')\nflags.DEFINE_integer('num_layers', 3, 'Num layers of bc network.')\nflags.DEFINE_integer('num_units', 256, 'Num units of bc network layers.')\nflags.DEFINE_integer('eval_every', 5000, 'Evaluation period.')\nflags.DEFINE_integer('evaluation_episodes', 10, 'Evaluation episodes.')\nflags.DEFINE_integer('seed', 0, 'Random seed for learner and evaluator.')\n\ndef _make_demonstration_dataset_factory(\n    dataset_name: str, num_demonstrations: int,\n    environment_spec: specs.EnvironmentSpec, batch_size: int\n) -> Callable[[jax_types.PRNGKey], Iterator[types.Transition]]:\n  \"\"\n\n  def demonstration_dataset_factory(\n      random_key: jax_types.PRNGKey) -> Iterator[types.Transition]:\n    \"\"\n\n    transitions_iterator = tfds.get_tfds_dataset(\n        dataset_name, num_demonstrations, env_spec=environment_spec)\n    return tfds.JaxInMemoryRandomSampleIterator(\n        transitions_iterator, key=random_key, batch_size=batch_size)\n\n  return demonstration_dataset_factory\n\ndef _make_environment_factory(env_name: str) -> jax_types.EnvironmentFactory:\n  \"\"\n\n  def environment_factory(seed: int) -> dm_env.Environment:\n    del seed\n    return helpers.make_environment(task=env_name)\n\n  return environment_factory\n\ndef _make_network_factory(\n    shift: Tuple[np.float64], scale: Tuple[np.float64], num_layers: int,\n    num_units: int,\n    dropout_rate: float) -> Callable[[specs.EnvironmentSpec], bc.BCNetworks]:\n  \"\"\n\n  def network_factory(spec: specs.EnvironmentSpec) -> bc.BCNetworks:\n    \"\"\n\n    action_spec = spec.actions\n    num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n    def actor_fn(obs, is_training=False, key=None):\n      obs += shift\n      obs *= scale\n      hidden_layers = [num_units] * num_layers\n      mlp = hk.Sequential([\n          hk.nets.MLP(hidden_layers + [num_dimensions]),\n      ])\n      if is_training:\n        return mlp(obs, dropout_rate=dropout_rate, rng=key)\n      else:\n        return mlp(obs)\n\n    policy = hk.without_apply_rng(hk.transform(actor_fn))\n\n    dummy_obs = utils.zeros_like(spec.observations)\n    dummy_obs = utils.add_batch_dim(dummy_obs)\n\n    policy_network = bc.BCPolicyNetwork(lambda key: policy.init(key, dummy_obs),\n                                        policy.apply)\n\n    return bc.BCNetworks(policy_network=policy_network)\n\n  return network_factory\n\ndef build_experiment_config() -> experiments.OfflineExperimentConfig[\n    bc.BCNetworks, actor_core_lib.FeedForwardPolicy, types.Transition]:\n  \"\"\n\n  environment = helpers.make_environment(task=FLAGS.env_name)\n  environment_spec = specs.make_environment_spec(environment)\n\n  dataset_name = helpers.get_dataset_name(FLAGS.env_name)\n  demonstration_dataset_factory = _make_demonstration_dataset_factory(\n      dataset_name, FLAGS.num_demonstrations, environment_spec,\n      FLAGS.batch_size)\n\n  dataset = tfds.get_tfds_dataset(\n      dataset_name, FLAGS.num_demonstrations, env_spec=environment_spec)\n  shift, scale = helpers.get_observation_stats(dataset)\n\n  network_factory = _make_network_factory(  \n      shift=shift,\n      scale=scale,\n      num_layers=FLAGS.num_layers,\n      num_units=FLAGS.num_units,\n      dropout_rate=FLAGS.dropout_rate)\n\n  bc_config = bc.BCConfig(learning_rate=FLAGS.learning_rate)\n  bc_builder = bc.BCBuilder(bc_config, loss_fn=bc.mse())\n\n  environment_factory = _make_environment_factory(FLAGS.env_name)\n\n  return experiments.OfflineExperimentConfig(\n      builder=bc_builder,\n      network_factory=network_factory,\n      demonstration_dataset_factory=demonstration_dataset_factory,\n      environment_factory=environment_factory,\n      max_num_learner_steps=FLAGS.num_bc_steps,\n      seed=FLAGS.seed,\n      environment_spec=environment_spec,\n  )\n\ndef main(_):\n  config = build_experiment_config()\n  if FLAGS.run_distributed:\n    program = experiments.make_distributed_offline_experiment(experiment=config)\n    lp.launch(program, xm_resources=lp_utils.make_xm_docker_resources(program))\n  else:\n    experiments.run_offline_experiment(\n        experiment=config,\n        eval_every=FLAGS.eval_every,\n        num_eval_episodes=FLAGS.evaluation_episodes)\n\nif __name__ == '__main__':\n  app.run(main)",
  "\n\"\"\n\nfrom functools import reduce\nimport numpy\nfrom pyscf.pbc import gto, scf, cc\n\ncell = gto.Cell()\ncell.atom=\"\"\ncell.basis = 'gth-szv'\ncell.pseudo = 'gth-pade'\ncell.a = \"\"\ncell.unit = 'B'\ncell.verbose = 5\ncell.build()\n\nkpts = cell.make_kpts([2,2,2])\nkmf = scf.KRHF(cell)\nkmf.kpts = kpts\nehf = kmf.kernel()\n\nmycc = cc.KCCSD(kmf)\nmycc.kernel()\nprint(\"KRCCSD energy (per unit cell) =\", mycc.e_tot)\n\nkpts = cell.get_abs_kpts([0.25, 0.25, 0.25])\nkmf = scf.KRHF(cell)\nkmf.kpts = kpts\nehf = kmf.kernel()\n\nmycc = cc.KRCCSD(kmf)\nmycc.kernel()\nprint(\"KRCCSD energy (per unit cell) =\", mycc.e_tot)\n\nkpt = cell.get_abs_kpts([0.25, 0.25, 0.25])\nmf = scf.RHF(cell, kpt=kpt)\nehf = mf.kernel()\n\nmycc = cc.RCCSD(mf).run()\nprint(\"RCCSD energy (per unit cell) at k-point =\", mycc.e_tot)\ndm1 = mycc.make_rdm1()\ndm2 = mycc.make_rdm2()\nnmo = mf.mo_coeff.shape[1]\neri_mo = mf.with_df.ao2mo(mf.mo_coeff, kpts=kpt).reshape([nmo]*4)\nh1 = reduce(numpy.dot, (mf.mo_coeff.conj().T, mf.get_hcore(), mf.mo_coeff))\ne_tot = numpy.einsum('ij,ji', h1, dm1) + numpy.einsum('ijkl,jilk', eri_mo, dm2)*.5 + mf.energy_nuc()\nprint(\"RCCSD energy based on CCSD density matrices =\", e_tot.real)\n\nmf = scf.addons.convert_to_uhf(mf)\nmycc = cc.UCCSD(mf).run()\nprint(\"UCCSD energy (per unit cell) at k-point =\", mycc.e_tot)\ndm1a, dm1b = mycc.make_rdm1()\ndm2aa, dm2ab, dm2bb = mycc.make_rdm2()\nnmo = dm1a.shape[0]\neri_aa = mf.with_df.ao2mo(mf.mo_coeff[0], kpts=kpt).reshape([nmo]*4)\neri_bb = mf.with_df.ao2mo(mf.mo_coeff[1], kpts=kpt).reshape([nmo]*4)\neri_ab = mf.with_df.ao2mo((mf.mo_coeff[0],mf.mo_coeff[0],mf.mo_coeff[1],mf.mo_coeff[1]), kpts=kpt).reshape([nmo]*4)\nhcore = mf.get_hcore()\nh1a = reduce(numpy.dot, (mf.mo_coeff[0].conj().T, hcore, mf.mo_coeff[0]))\nh1b = reduce(numpy.dot, (mf.mo_coeff[1].conj().T, hcore, mf.mo_coeff[1]))\ne_tot = (numpy.einsum('ij,ji', h1a, dm1a) +\n         numpy.einsum('ij,ji', h1b, dm1b) +\n         numpy.einsum('ijkl,jilk', eri_aa, dm2aa)*.5 +\n         numpy.einsum('ijkl,jilk', eri_ab, dm2ab)    +\n         numpy.einsum('ijkl,jilk', eri_bb, dm2bb)*.5 + mf.energy_nuc())\nprint(\"UCCSD energy based on CCSD density matrices =\", e_tot.real)\n\nmf = scf.addons.convert_to_ghf(mf)\nmycc = cc.GCCSD(mf).run()\nprint(\"GCCSD energy (per unit cell) at k-point =\", mycc.e_tot)\ndm1 = mycc.make_rdm1()\ndm2 = mycc.make_rdm2()\nnao = cell.nao_nr()\nnmo = mf.mo_coeff.shape[1]\nmo = mf.mo_coeff[:nao] + mf.mo_coeff[nao:]\neri_mo = mf.with_df.ao2mo(mo, kpts=kpt).reshape([nmo]*4)\norbspin = mf.mo_coeff.orbspin\neri_mo[orbspin[:,None]!=orbspin] = 0\neri_mo[:,:,orbspin[:,None]!=orbspin] = 0\nh1 = reduce(numpy.dot, (mf.mo_coeff.conj().T, mf.get_hcore(), mf.mo_coeff))\ne_tot = numpy.einsum('ij,ji', h1, dm1) + numpy.einsum('ijkl,jilk', eri_mo, dm2)*.5 + mf.energy_nuc()\nprint(\"GCCSD energy based on CCSD density matrices =\", e_tot.real)",
  "\n\"\"\nimport numpy as np\n\nfrom vispy import app\nfrom vispy.gloo import gl\n\nvertex_code = \"\"\n\nfragment_code = \"\"\n\nclass Canvas(app.Canvas):\n    def __init__(self):\n        app.Canvas.__init__(self, size=(800, 600), title='GL Fireworks',\n                            keys='interactive')\n\n    def on_initialize(self, event):\n        \n        self.program = gl.glCreateProgram()\n        vertex = gl.glCreateShader(gl.GL_VERTEX_SHADER)\n        fragment = gl.glCreateShader(gl.GL_FRAGMENT_SHADER)\n        gl.glShaderSource(vertex, vertex_code)\n        gl.glShaderSource(fragment, fragment_code)\n        gl.glCompileShader(vertex)\n        gl.glCompileShader(fragment)\n        gl.glAttachShader(self.program, vertex)\n        gl.glAttachShader(self.program, fragment)\n        gl.glLinkProgram(self.program)\n        gl.glDetachShader(self.program, vertex)\n        gl.glDetachShader(self.program, fragment)\n        gl.glUseProgram(self.program)\n\n        n = 10000\n        self.data = np.zeros(n, dtype=[('lifetime', np.float32),\n                                       ('start', np.float32, 3),\n                                       ('end', np.float32, 3)])\n        vbuffer = gl.glCreateBuffer()\n        gl.glBindBuffer(gl.GL_ARRAY_BUFFER, vbuffer)\n        gl.glBufferData(gl.GL_ARRAY_BUFFER, self.data, gl.GL_DYNAMIC_DRAW)\n\n        stride = self.data.strides[0]\n\n        offset = 0\n        loc = gl.glGetAttribLocation(self.program, \"lifetime\")\n        gl.glEnableVertexAttribArray(loc)\n        gl.glVertexAttribPointer(loc, 1, gl.GL_FLOAT, False, stride, offset)\n\n        offset = self.data.dtype[\"lifetime\"].itemsize\n        loc = gl.glGetAttribLocation(self.program, \"start\")\n        gl.glEnableVertexAttribArray(loc)\n        gl.glVertexAttribPointer(loc, 3, gl.GL_FLOAT, False, stride, offset)\n\n        offset = self.data.dtype[\"start\"].itemsize\n        loc = gl.glGetAttribLocation(self.program, \"end\")\n        gl.glEnableVertexAttribArray(loc)\n        gl.glVertexAttribPointer(loc, 3, gl.GL_FLOAT, False, stride, offset)\n\n        self.elapsed_time = 0\n        gl.glClearColor(0, 0, 0, 1)\n        gl.glDisable(gl.GL_DEPTH_TEST)\n        gl.glEnable(gl.GL_BLEND)\n        gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE)\n        gl.glEnable(34370)  \n        gl.glEnable(34913)  \n        gl.glViewport(0, 0, *self.physical_size)\n        self.new_explosion()\n        self.timer = app.Timer('auto', self.on_timer, start=True)\n\n    def on_draw(self, event):\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        gl.glDrawArrays(gl.GL_POINTS, 0, len(self.data))\n\n    def on_resize(self, event):\n        gl.glViewport(0, 0, *event.physical_size)\n\n    def on_timer(self, event):\n        self.elapsed_time += 1. / 60.\n        if self.elapsed_time > 1.5:\n            self.new_explosion()\n            self.elapsed_time = 0.0\n\n        loc = gl.glGetUniformLocation(self.program, \"time\")\n        gl.glUniform1f(loc, self.elapsed_time)\n        self.update()\n\n    def new_explosion(self):\n        n = len(self.data)\n        color = np.random.uniform(0.1, 0.9, 4).astype(np.float32)\n        color[3] = 1.0 / n ** 0.08\n        loc = gl.glGetUniformLocation(self.program, \"color\")\n        gl.glUniform4f(loc, *color)\n\n        center = np.random.uniform(-0.5, 0.5, 3)\n        loc = gl.glGetUniformLocation(self.program, \"center\")\n        gl.glUniform3f(loc, *center)\n\n        self.data['lifetime'] = np.random.normal(2.0, 0.5, (n,))\n        self.data['start'] = np.random.normal(0.0, 0.2, (n, 3))\n        self.data['end'] = np.random.normal(0.0, 1.2, (n, 3))\n        gl.glBufferData(gl.GL_ARRAY_BUFFER, self.data, gl.GL_DYNAMIC_DRAW)\n\nif __name__ == '__main__':\n    c = Canvas()\n    c.show()\n    app.run()",
  "\nimport os\n\nimport numpy as np\n\nimport simpa as sp\nfrom simpa import Tags\nfrom simpa.visualisation.matplotlib_data_visualisation import visualise_data\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\npath_manager = sp.PathManager()\n\nVOLUME_TRANSDUCER_DIM_IN_MM = 75\nVOLUME_PLANAR_DIM_IN_MM = 20\nVOLUME_HEIGHT_IN_MM = 25\nSPACING = 0.25\nRANDOM_SEED = 471\nVOLUME_NAME = \"LinearUnmixingExample_\" + str(RANDOM_SEED)\n\nWAVELENGTHS = [750, 800, 850]\n\ndef create_example_tissue():\n    \"\"\n    background_dictionary = sp.Settings()\n    background_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.constant(1e-4, 1e-4, 0.9)\n    background_dictionary[Tags.STRUCTURE_TYPE] = Tags.BACKGROUND\n\n    muscle_dictionary = sp.Settings()\n    muscle_dictionary[Tags.PRIORITY] = 1\n    muscle_dictionary[Tags.STRUCTURE_START_MM] = [0, 0, 0]\n    muscle_dictionary[Tags.STRUCTURE_END_MM] = [0, 0, 100]\n    muscle_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.muscle()\n    muscle_dictionary[Tags.CONSIDER_PARTIAL_VOLUME] = True\n    muscle_dictionary[Tags.ADHERE_TO_DEFORMATION] = True\n    muscle_dictionary[Tags.STRUCTURE_TYPE] = Tags.HORIZONTAL_LAYER_STRUCTURE\n\n    vessel_1_dictionary = sp.Settings()\n    vessel_1_dictionary[Tags.PRIORITY] = 3\n    vessel_1_dictionary[Tags.STRUCTURE_START_MM] = [VOLUME_TRANSDUCER_DIM_IN_MM/2,\n                                                    10,\n                                                    5]\n    vessel_1_dictionary[Tags.STRUCTURE_END_MM] = [VOLUME_TRANSDUCER_DIM_IN_MM/2,\n                                                  12,\n                                                  5]\n    vessel_1_dictionary[Tags.STRUCTURE_RADIUS_MM] = 3\n    vessel_1_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.blood(oxygenation=0.99)\n    vessel_1_dictionary[Tags.CONSIDER_PARTIAL_VOLUME] = True\n    vessel_1_dictionary[Tags.STRUCTURE_TYPE] = Tags.CIRCULAR_TUBULAR_STRUCTURE\n\n    vessel_2_dictionary = sp.Settings()\n    vessel_2_dictionary[Tags.PRIORITY] = 3\n    vessel_2_dictionary[Tags.STRUCTURE_START_MM] = [VOLUME_TRANSDUCER_DIM_IN_MM/3,\n                                                    10,\n                                                    5]\n    vessel_2_dictionary[Tags.STRUCTURE_END_MM] = [VOLUME_TRANSDUCER_DIM_IN_MM/3,\n                                                  12,\n                                                  5]\n    vessel_2_dictionary[Tags.STRUCTURE_RADIUS_MM] = 2\n    vessel_2_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.blood(oxygenation=0.75)\n    vessel_2_dictionary[Tags.CONSIDER_PARTIAL_VOLUME] = True\n    vessel_2_dictionary[Tags.STRUCTURE_TYPE] = Tags.CIRCULAR_TUBULAR_STRUCTURE\n\n    epidermis_dictionary = sp.Settings()\n    epidermis_dictionary[Tags.PRIORITY] = 8\n    epidermis_dictionary[Tags.STRUCTURE_START_MM] = [0, 0, 0]\n    epidermis_dictionary[Tags.STRUCTURE_END_MM] = [0, 0, 0.1]\n    epidermis_dictionary[Tags.MOLECULE_COMPOSITION] = sp.TISSUE_LIBRARY.epidermis()\n    epidermis_dictionary[Tags.CONSIDER_PARTIAL_VOLUME] = True\n    epidermis_dictionary[Tags.ADHERE_TO_DEFORMATION] = True\n    epidermis_dictionary[Tags.STRUCTURE_TYPE] = Tags.HORIZONTAL_LAYER_STRUCTURE\n\n    tissue_dict = sp.Settings()\n    tissue_dict[Tags.BACKGROUND] = background_dictionary\n    tissue_dict[\"muscle\"] = muscle_dictionary\n    tissue_dict[\"epidermis\"] = epidermis_dictionary\n    tissue_dict[\"vessel_1\"] = vessel_1_dictionary\n    tissue_dict[\"vessel_2\"] = vessel_2_dictionary\n    return tissue_dict\n\nnp.random.seed(RANDOM_SEED)\n\ngeneral_settings = {\n    \n    Tags.RANDOM_SEED: RANDOM_SEED,\n    Tags.VOLUME_NAME: VOLUME_NAME,\n    Tags.SIMULATION_PATH: path_manager.get_hdf5_file_save_path(),\n    Tags.SPACING_MM: SPACING,\n    Tags.DIM_VOLUME_Z_MM: VOLUME_HEIGHT_IN_MM,\n    Tags.DIM_VOLUME_X_MM: VOLUME_TRANSDUCER_DIM_IN_MM,\n    Tags.DIM_VOLUME_Y_MM: VOLUME_PLANAR_DIM_IN_MM,\n    Tags.WAVELENGTHS: WAVELENGTHS,\n    Tags.GPU: True,\n    Tags.DO_FILE_COMPRESSION: True\n}\nsettings = sp.Settings(general_settings)\nsettings.set_volume_creation_settings({\n    Tags.SIMULATE_DEFORMED_LAYERS: True,\n    Tags.STRUCTURES: create_example_tissue()\n})\nsettings.set_optical_settings({\n    Tags.OPTICAL_MODEL_NUMBER_PHOTONS: 1e7,\n    Tags.OPTICAL_MODEL_BINARY_PATH: path_manager.get_mcx_binary_path(),\n    Tags.OPTICAL_MODEL: Tags.OPTICAL_MODEL_MCX,\n    Tags.LASER_PULSE_ENERGY_IN_MILLIJOULE: 50\n})\n\nsettings[\"linear_unmixing\"] = {\n    Tags.DATA_FIELD: Tags.DATA_FIELD_INITIAL_PRESSURE,\n    Tags.WAVELENGTHS: WAVELENGTHS,\n    Tags.LINEAR_UNMIXING_SPECTRA: sp.get_simpa_internal_absorption_spectra_by_names(\n        [Tags.SIMPA_NAMED_ABSORPTION_SPECTRUM_OXYHEMOGLOBIN, Tags.SIMPA_NAMED_ABSORPTION_SPECTRUM_DEOXYHEMOGLOBIN]\n    ),\n    Tags.LINEAR_UNMIXING_COMPUTE_SO2: True,\n    Tags.LINEAR_UNMIXING_NON_NEGATIVE: True\n}\n\ndevice = sp.MSOTAcuityEcho(device_position_mm=np.array([VOLUME_TRANSDUCER_DIM_IN_MM/2,\n                                                        VOLUME_PLANAR_DIM_IN_MM/2,\n                                                        0]))\ndevice.update_settings_for_use_of_model_based_volume_creator(settings)\n\npipeline = [\n    sp.ModelBasedVolumeCreationAdapter(settings),\n    sp.MCXAdapter(settings),\n    sp.FieldOfViewCropping(settings),\n]\nsp.simulate(pipeline, settings, device)\n\nsp.LinearUnmixing(settings, \"linear_unmixing\").run()\n\nfile_path = path_manager.get_hdf5_file_save_path() + \"/\" + VOLUME_NAME + \".hdf5\"\nlu_results = sp.load_data_field(file_path, Tags.LINEAR_UNMIXING_RESULT)\nsO2 = lu_results[\"sO2\"]\n\nmua = sp.load_data_field(file_path, Tags.DATA_FIELD_ABSORPTION_PER_CM, wavelength=WAVELENGTHS[0])\np0 = sp.load_data_field(file_path, Tags.DATA_FIELD_INITIAL_PRESSURE, wavelength=WAVELENGTHS[0])\ngt_oxy = sp.load_data_field(file_path, Tags.DATA_FIELD_OXYGENATION, wavelength=WAVELENGTHS[0])\n\nvisualise_data(path_to_hdf5_file=path_manager.get_hdf5_file_save_path() + \"/\" + VOLUME_NAME + \".hdf5\",\n               wavelength=WAVELENGTHS[0],\n               show_initial_pressure=True,\n               show_oxygenation=True,\n               show_linear_unmixing_sO2=True)",
  "\n\"\"\n\nimport argparse\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor, RandomCrop\nimport torch.optim.lr_scheduler\nfrom avalanche.benchmarks import nc_benchmark\nfrom avalanche.benchmarks.datasets.dataset_utils import default_dataset_location\nfrom avalanche.models import SimpleMLP\nfrom avalanche.training.supervised.strategy_wrappers_online import OnlineNaive\nfrom avalanche.training.plugins import ReplayPlugin\nfrom avalanche.training.storage_policy import ReservoirSamplingBuffer\nfrom avalanche.benchmarks.scenarios.online_scenario import OnlineCLScenario\nfrom avalanche.evaluation.metrics import (\n    forgetting_metrics,\n    accuracy_metrics,\n    loss_metrics,\n)\nfrom avalanche.logging import InteractiveLogger\nfrom avalanche.training.plugins import EvaluationPlugin\n\ndef main(args):\n    \n    device = torch.device(\n        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n    )\n    n_batches = 5\n    \n    train_transform = transforms.Compose(\n        [\n            RandomCrop(28, padding=4),\n            ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ]\n    )\n    test_transform = transforms.Compose(\n        [ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    \n    mnist_train = MNIST(\n        root=default_dataset_location(\"mnist\"),\n        train=True,\n        download=True,\n        transform=train_transform,\n    )\n    mnist_test = MNIST(\n        root=default_dataset_location(\"mnist\"),\n        train=False,\n        download=True,\n        transform=test_transform,\n    )\n    benchmark = nc_benchmark(\n        mnist_train, mnist_test, n_batches, task_labels=False, seed=1234\n    )\n    \n    model = SimpleMLP(num_classes=benchmark.n_classes)\n\n    interactive_logger = InteractiveLogger()\n\n    eval_plugin = EvaluationPlugin(\n        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        forgetting_metrics(experience=True),\n        loggers=[interactive_logger],\n    )\n\n    storage_policy = ReservoirSamplingBuffer(max_size=100)\n    replay_plugin = ReplayPlugin(\n        mem_size=100, batch_size=1, storage_policy=storage_policy\n    )\n\n    cl_strategy = OnlineNaive(\n        model,\n        torch.optim.Adam(model.parameters(), lr=0.1),\n        CrossEntropyLoss(),\n        train_passes=1,\n        train_mb_size=10,\n        eval_mb_size=32,\n        device=device,\n        evaluator=eval_plugin,\n        plugins=[replay_plugin],\n    )\n\n    print(\"Starting experiment...\")\n    results = []\n\n    batch_streams = benchmark.streams.values()\n    \n    for i, exp in enumerate(benchmark.train_stream):\n        \n        ocl_benchmark = OnlineCLScenario(\n            original_streams=batch_streams, experiences=exp, experience_size=10\n        )\n        \n        cl_strategy.train(ocl_benchmark.train_stream)\n        results.append(cl_strategy.eval(benchmark.test_stream))\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--cuda\",\n        type=int,\n        default=0,\n        help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n    )\n    args = parser.parse_args()\n    main(args)",
  "\nfrom __future__ import absolute_import, unicode_literals\nimport os\nimport sys\nimport time\nimport logging\nimport DDG4\nfrom DDG4 import OutputLevel as Output\nfrom g4units import MeV, GeV, m, mm\n\nlogging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\"\"\n\ndef show_help():\n  logging.info(\"Check_shape.py -option [-option]                           \")\n  logging.info(\"       -vis                          Enable visualization  \")\n  logging.info(\"       -batch                        Batch execution       \")\n\ndef run():\n  hlp = False\n  vis = False\n  dump = False\n  batch = False\n  install_dir = os.environ['DD4hepINSTALL']\n  \n  for i in list(range(len(sys.argv))):\n    c = sys.argv[i].upper()\n    if c.find('BATCH') < 2 and c.find('BATCH') >= 0:\n      batch = True\n    elif c[:4] == '-VIS':\n      vis = True\n    elif c[:4] == '-DUM':\n      dump = True\n    elif c[:2] == '-H':\n      hlp = True\n\n  if hlp:\n    show_help()\n    sys.exit(1)\n\n  kernel = DDG4.Kernel()\n  description = kernel.detectorDescription()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  \n  logger.info(\"\n  ui = None\n  if batch:\n    geant4.setupCshUI(ui=None, vis=None)\n    kernel.UI = 'UI'\n  else:\n    ui = geant4.setupCshUI(vis=vis)\n\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/ClientTests/compact/NestedBoxReflection.xml\"))\n  DDG4.importConstants(description)\n\n  geant4.printDetectors()\n  if dump:\n    seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n    act.DebugReflections = True\n    act.DebugMaterials = False\n    act.DebugElements = False\n    act.DebugVolumes = False\n    act.DebugShapes = False\n    act.DumpHierarchy = ~0x0\n\n  logger.info(\"\n  geant4.setupTrackingField()\n\n  logger.info(\"\n  rndm = DDG4.Action(kernel, 'Geant4Random/Random')\n  rndm.Seed = 987654321\n  rndm.initialize()\n  \n  logger.info(\"\n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputType = 3  \n  prt.OutputLevel = Output.INFO\n  kernel.eventAction().adopt(prt)\n  \n  logger.info(\"\n  geant4.setupROOTOutput('RootOutput', 'BoxReflect_' + time.strftime('%Y-%m-%d_%H-%M'))\n  \n  gen = DDG4.GeneratorAction(kernel, \"Geant4GeneratorActionInit/GenerationInit\")\n  gen.enableUI()\n  kernel.generatorAction().adopt(gen)\n  \n  logger.info(\"\n  gen = DDG4.GeneratorAction(kernel, \"Geant4ParticleGun/IsotropE+\")\n  gen.mask = 4\n  gen.isotrop = True\n  gen.particle = 'e+'\n  gen.Energy = 100 * GeV\n  gen.multiplicity = 200\n  gen.position = (0 * m, 0 * m, 0 * m)\n  gen.direction = (0, 0, 1.)\n  gen.distribution = 'uniform'\n  gen.standalone = False\n  \n  gen.enableUI()\n  kernel.generatorAction().adopt(gen)\n  \n  logger.info(\"\n  gen = DDG4.GeneratorAction(kernel, \"Geant4InteractionMerger/InteractionMerger\")\n  gen.OutputLevel = 4  \n  gen.enableUI()\n  kernel.generatorAction().adopt(gen)\n  \n  logger.info(\"\n  gen = DDG4.GeneratorAction(kernel, \"Geant4PrimaryHandler/PrimaryHandler\")\n  gen.OutputLevel = 4  \n  gen.enableUI()\n  kernel.generatorAction().adopt(gen)\n  \n  logger.info(\"\n  part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n  kernel.generatorAction().adopt(part)\n  \n  part.SaveProcesses = ['Decay']\n  part.MinimalKineticEnergy = 100 * MeV\n  part.OutputLevel = 5  \n  part.enableUI()\n  user = DDG4.Action(kernel, \"Geant4TCUserParticleHandler/UserParticleHandler\")\n  user.TrackingVolume_Zmax = 3.0 * m\n  user.TrackingVolume_Rmax = 3.0 * m\n  user.enableUI()\n  part.adopt(user)\n  \n  logger.info(\"\n  seq, actions = geant4.setupDetectors()\n  \n  logger.info(\"\n  geant4.setupPhysics('QGSP_BERT')\n  ph = geant4.addPhysics(str('Geant4PhysicsList/Myphysics'))\n  ph.addPhysicsConstructor(str('G4StepLimiterPhysics'))\n  \n  part = geant4.addPhysics('Geant4ExtraParticles/ExtraParticles')\n  part.pdgfile = os.path.join(install_dir, 'examples/DDG4/examples/particle.tbl')\n  \n  rg = geant4.addPhysics('Geant4DefaultRangeCut/GlobalRangeCut')\n  rg.RangeCut = 0.7 * mm\n  \n  if ui and vis:\n    cmds = []\n    cmds.append('/control/verbose 2')\n    cmds.append('/run/initialize')\n    cmds.append('/vis/open OGL')\n    cmds.append('/vis/verbose errors')\n    cmds.append('/vis/drawVolume')\n    cmds.append('/vis/viewer/set/viewpointThetaPhi 55. 45.')\n    cmds.append('/vis/scene/add/axes 0 0 0 3 m')\n    ui.Commands = cmds\n\n  kernel.configure()\n  kernel.initialize()\n\n  kernel.run()\n  kernel.terminate()\n\nif __name__ == \"__main__\":\n  run()",
  "from seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.services import WebService\nfrom seedemu.core import Emulator, Binding, Filter\nfrom seedemu.compiler import Docker\n\nemu = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\nweb = WebService()\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\n\nas150 = base.createAutonomousSystem(150)\n\nas150.createNetwork('net0')\nas150.createNetwork('net1')\nas150.createNetwork('net2')\n\nr1 = as150.createRouter('r1')\nr2 = as150.createRouter('r2')\nr3 = as150.createRouter('r3')\nr4 = as150.createRouter('r4')\n\nr1.joinNetwork('ix100')\nr1.joinNetwork('net0')\n\nr2.joinNetwork('net0')\nr2.joinNetwork('net1')\n\nr3.joinNetwork('net1')\nr3.joinNetwork('net2')\n\nr4.joinNetwork('net2')\nr4.joinNetwork('ix101')\n\nas151 = base.createAutonomousSystem(151)\n\nas151_web = as151.createHost('web')\nweb.install('web151')\nemu.addBinding(Binding('web151', filter = Filter(nodeName = 'web', asn = 151)))\n\nas151_router = as151.createRouter('router0')\n\nas151_net = as151.createNetwork('net0')\n\nas151_web.joinNetwork('net0')\nas151_router.joinNetwork('net0')\n\nas151_router.joinNetwork('ix100')\n\nas152 = base.createAutonomousSystem(152)\n\nas152_web = as152.createHost('web')\nweb.install('web152')\nemu.addBinding(Binding('web152', filter = Filter(nodeName = 'web', asn = 152)))\n\nas152_router = as152.createRouter('router0')\n\nas152_net = as152.createNetwork('net0')\n\nas152_web.joinNetwork('net0')\nas152_router.joinNetwork('net0')\n\nas152_router.joinNetwork('ix101')\n\nebgp.addPrivatePeering(100, 150, 151, abRelationship = PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 150, 152, abRelationship = PeerRelationship.Provider)\n\nemu.addLayer(base)\nemu.addLayer(routing)\nemu.addLayer(ebgp)\nemu.addLayer(ibgp)\nemu.addLayer(ospf)\nemu.addLayer(web)\n\nemu.dump('component-dump.bin')",
  "from seedemu.core import Emulator\nfrom seedemu.layers import Base, Routing, Ebgp, Ospf, Ibgp, PeerRelationship\nfrom seedemu.components import BgpAttackerComponent\nfrom seedemu.compiler import Docker\nfrom seedemu.mergers import DEFAULT_MERGERS\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nospf = Ospf()\nibgp = Ibgp()\n\nbgp_attack = BgpAttackerComponent(attackerAsn = 66)\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\n\nas150 = base.createAutonomousSystem(150)\n\nas150_r0 = as150.createRouter('r0')\n\nas150_n0 = as150.createNetwork('n0')\n\nas150_r0.joinNetwork('n0')\nas150_r0.joinNetwork('ix100')\n\nas2 = base.createAutonomousSystem(2)\n\nas2_r0 = as2.createRouter('r0')\nas2_r1 = as2.createRouter('r1')\n\nas2_n0 = as2.createNetwork('n0')\n\nas2_r0.joinNetwork('n0')\nas2_r1.joinNetwork('n0')\n\nas2_r0.joinNetwork('ix100')\nas2_r1.joinNetwork('ix101')\n\nas151 = base.createAutonomousSystem(151)\n\nas151_r0 = as151.createRouter('r0')\n\nas151_n0 = as151.createNetwork('n0')\n\nas151_r0.joinNetwork('n0')\nas151_r0.joinNetwork('ix101')\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ibgp)\nsim.addLayer(ebgp)\nsim.addLayer(ospf)\n\nbgp_attack.addHijackedPrefix(as151_n0.getPrefix())\nbgp_attack.joinInternetExchange('ix100', '10.100.0.66')\n\nhijack_component = bgp_attack.get()\n\nsim_with_attack = sim.merge(bgp_attack.get(), DEFAULT_MERGERS)\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(100, 2, 66, PeerRelationship.Unfiltered)\n\nsim_with_attack.render()\nsim_with_attack.compile(Docker(selfManagedNetwork = True), 'bgp-attacker-component')",
  "\nimport os\nfrom typing import cast, List, Optional\n\nimport torch\nfrom fbgemm_gpu.split_embedding_configs import EmbOptimType as OptimType\nfrom torch import distributed as dist, nn\nfrom torch.utils.data import DataLoader\nfrom torchrec.datasets.criteo import DEFAULT_CAT_NAMES, DEFAULT_INT_NAMES\nfrom torchrec.datasets.random import RandomRecDataset\nfrom torchrec.distributed import TrainPipelineSparseDist\nfrom torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\nfrom torchrec.distributed.model_parallel import DistributedModelParallel\nfrom torchrec.distributed.types import ModuleSharder\nfrom torchrec.models.dlrm import DLRM, DLRMTrain\nfrom torchrec.modules.embedding_configs import EmbeddingBagConfig\nfrom torchrec.modules.embedding_modules import EmbeddingBagCollection\nfrom torchrec.optim.keyed import KeyedOptimizerWrapper\nfrom torchrec.optim.optimizers import in_backward_optimizer_filter\nfrom tqdm import tqdm\n\ndef _get_random_dataloader(\n    num_embeddings: int, batch_size: int = 32, pin_memory: bool = False\n) -> DataLoader:\n    return DataLoader(\n        RandomRecDataset(\n            keys=DEFAULT_CAT_NAMES,\n            batch_size=batch_size,\n            hash_size=num_embeddings,\n            ids_per_feature=1,\n            num_dense=len(DEFAULT_INT_NAMES),\n        ),\n        batch_size=None,\n        batch_sampler=None,\n        pin_memory=pin_memory,\n        num_workers=0,\n    )\n\ndef train(\n    num_embeddings: int = 1024**2,\n    embedding_dim: int = 128,\n    dense_arch_layer_sizes: Optional[List[int]] = None,\n    over_arch_layer_sizes: Optional[List[int]] = None,\n    learning_rate: float = 0.1,\n) -> None:\n    \"\"\n    if dense_arch_layer_sizes is None:\n        dense_arch_layer_sizes = [64, 128]\n    if over_arch_layer_sizes is None:\n        over_arch_layer_sizes = [64, 1]\n\n    rank = int(os.environ[\"LOCAL_RANK\"])\n    if torch.cuda.is_available():\n        device: torch.device = torch.device(f\"cuda:{rank}\")\n        backend = \"nccl\"\n        torch.cuda.set_device(device)\n    else:\n        device: torch.device = torch.device(\"cpu\")\n        backend = \"gloo\"\n    dist.init_process_group(backend=backend)\n\n    eb_configs = [\n        EmbeddingBagConfig(\n            name=f\"t_{feature_name}\",\n            embedding_dim=embedding_dim,\n            num_embeddings=num_embeddings,\n            feature_names=[feature_name],\n        )\n        for feature_idx, feature_name in enumerate(DEFAULT_CAT_NAMES)\n    ]\n    dlrm_model = DLRM(\n        embedding_bag_collection=EmbeddingBagCollection(\n            tables=eb_configs, device=torch.device(\"meta\")\n        ),\n        dense_in_features=len(DEFAULT_INT_NAMES),\n        dense_arch_layer_sizes=dense_arch_layer_sizes,\n        over_arch_layer_sizes=over_arch_layer_sizes,\n        dense_device=device,\n    )\n    train_model = DLRMTrain(dlrm_model)\n\n    fused_params = {\n        \"learning_rate\": learning_rate,\n        \"optimizer\": OptimType.EXACT_ROWWISE_ADAGRAD,\n    }\n    sharders = [\n        EmbeddingBagCollectionSharder(fused_params=fused_params),\n    ]\n    \n    model = DistributedModelParallel(\n        module=train_model,\n        device=device,\n        sharders=cast(List[ModuleSharder[nn.Module]], sharders),\n    )\n\n    non_fused_optimizer = KeyedOptimizerWrapper(\n        dict(in_backward_optimizer_filter(model.named_parameters())),\n        lambda params: torch.optim.Adagrad(params, lr=learning_rate),\n    )\n    train_pipeline = TrainPipelineSparseDist(\n        model,\n        non_fused_optimizer,\n        device,\n    )\n\n    train_iterator = iter(\n        _get_random_dataloader(\n            num_embeddings=num_embeddings, pin_memory=backend == \"nccl\"\n        )\n    )\n    for _ in tqdm(range(int(1e4)), mininterval=5.0):\n        train_pipeline.progress(train_iterator)\n\nif __name__ == \"__main__\":\n    train()",
  "from __future__ import print_function\nimport gunpowder as gp\nimport json\nimport math\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef train(iterations):\n\n    raw = gp.ArrayKey('RAW')\n\n    gt_labels = gp.ArrayKey('LABELS')\n\n    gt_affs= gp.ArrayKey('AFFINITIES')\n\n    loss_weights = gp.ArrayKey('LOSS_WEIGHTS')\n\n    pred_affs = gp.ArrayKey('PRED_AFFS')\n\n    pred_affs_gradients = gp.ArrayKey('PRED_AFFS_GRADIENTS')\n\n    with open('train_net_config.json', 'r') as f:\n        net_config = json.load(f)\n\n    voxel_size = gp.Coordinate((40, 4, 4))\n    input_size = gp.Coordinate(net_config['input_shape'])*voxel_size\n    output_size = gp.Coordinate(net_config['output_shape'])*voxel_size\n\n    request = gp.BatchRequest()\n    request.add(raw, input_size)\n    request.add(gt_affs, output_size)\n    request.add(loss_weights, output_size)\n\n    snapshot_request = gp.BatchRequest()\n    snapshot_request[pred_affs] = request[gt_affs]\n    snapshot_request[pred_affs_gradients] = request[gt_affs]\n\n    pipeline = (\n\n        tuple(\n\n            gp.Hdf5Source(\n                'sample_'+s+'_padded_20160501.hdf',\n                datasets = {\n                    raw: 'volumes/raw',\n                    gt_labels: 'volumes/labels/neuron_ids'\n                }\n            ) +\n\n            gp.Normalize(raw) +\n\n            gp.RandomLocation()\n\n            for s in ['A', 'B', 'C']\n        ) +\n\n        gp.RandomProvider() +\n\n        gp.ElasticAugment(\n            [4,40,40],\n            [0,2,2],\n            [0,math.pi/2.0],\n            prob_slip=0.05,\n            prob_shift=0.05,\n            max_misalign=25) +\n\n        gp.SimpleAugment(transpose_only=[1, 2]) +\n\n        gp.IntensityAugment(\n            raw,\n            scale_min=0.9,\n            scale_max=1.1,\n            shift_min=-0.1,\n            shift_max=0.1,\n            z_section_wise=True) +\n\n        gp.GrowBoundary(\n            gt_labels,\n            steps=3,\n            only_xy=True) +\n\n        gp.AddAffinities(\n            [[-1, 0, 0], [0, -1, 0], [0, 0, -1]],\n            gt_labels,\n            gt_affs) +\n\n        gp.BalanceLabels(\n            gt_affs,\n            loss_weights) +\n\n        gp.PreCache(\n            cache_size=10,\n            num_workers=5) +\n\n        gp.tensorflow.Train(\n            'train_net',\n            net_config['optimizer'],\n            net_config['loss'],\n            inputs={\n                net_config['raw']: raw,\n                net_config['gt_affs']: gt_affs,\n                net_config['loss_weights']: loss_weights\n            },\n            outputs={\n                net_config['pred_affs']: pred_affs\n            },\n            gradients={\n                net_config['pred_affs']: pred_affs_gradients\n            },\n            save_every=1) +\n\n        gp.Snapshot(\n            {\n                raw: '/volumes/raw',\n                gt_labels: '/volumes/labels/neuron_ids',\n                gt_affs: '/volumes/labels/affs',\n                pred_affs: '/volumes/pred_affs',\n                pred_affs_gradients: '/volumes/pred_affs_gradients'\n            },\n            output_dir='snapshots',\n            output_filename='batch_{iteration}.hdf',\n            every=100,\n            additional_request=snapshot_request,\n            compression_type='gzip') +\n\n        gp.PrintProfilingStats(every=10)\n    )\n\n    print(\"Training for\", iterations, \"iterations\")\n\n    with gp.build(pipeline):\n        for i in range(iterations):\n            pipeline.request_batch(request)\n\n    print(\"Finished\")\n\nif __name__ == \"__main__\":\n    train(200000)\n    ",
  "\nr\"\"\n\nimport inspect\nimport os\nimport sys\n\nfrom Basilisk.utilities.MonteCarlo.Controller import Controller\nfrom Basilisk.utilities.MonteCarlo.RetentionPolicy import RetentionPolicy\n\nfilename = inspect.getframeinfo(inspect.currentframe()).filename\nfileNameString = os.path.basename(os.path.splitext(__file__)[0])\npath = os.path.dirname(os.path.abspath(filename))\n\nfrom Basilisk import __path__\nbskPath = __path__[0]\n\nsys.path.append(path+\"/../BskSim/scenarios/\")\n\ndef run(time=None):\n    \"\"\n\n    scenarioName = \"scenario_AttFeedback\"\n\n    monteCarlo = Controller()\n    monteCarlo.numProcess = 3 \n    runsList = [1]  \n\n    icName = path + \"/\" + scenarioName + \"MC/\"\n    newDataDir = path + \"/\" + scenarioName + \"MC/rerun\"\n\n    exec('import '+ scenarioName)\n    simulationModule = eval(scenarioName + \".\" + scenarioName) \n    if time is not None:\n        exec (scenarioName + '.' + scenarioName + '.simBaseTime = time')  \n    executionModule = eval(scenarioName + \".runScenario\") \n\n    monteCarlo.setSimulationFunction(simulationModule)\n    monteCarlo.setExecutionFunction(executionModule)\n    monteCarlo.setICDir(icName)\n    monteCarlo.setICRunFlag(True)\n    monteCarlo.setArchiveDir(newDataDir)\n    monteCarlo.setExecutionCount(len(runsList))\n    monteCarlo.setShouldDisperseSeeds(False)\n    monteCarlo.shouldArchiveParameters = False\n\n    retentionPolicy = RetentionPolicy()\n    retentionPolicy.logRate = int(2E9)\n    retentionPolicy.addMessageLog(\"attGuidMsg\", [\"sigma_BR\"]) \n    monteCarlo.addRetentionPolicy(retentionPolicy)\n\n    failed = monteCarlo.runInitialConditions(runsList)\n    assert len(failed) == 0, \"Should run ICs successfully\"\n\nif __name__ == \"__main__\":\n    run()\n",
  "\"\"\n\n__version__ = '0.1.0'\nfrom docopt import docopt\n\nargs = docopt(__doc__, version=__version__)\n\nexec('from sirf.' + args['--engine'] + ' import *')\n\ndata_file = args['--file']\ndata_path = args['--path']\nif data_path is None:\n    data_path = examples_data_path('MR') + '/zenodo/'\noutput_file = args['--output']\nshow_plot = not args['--non-interactive']\ntrajtype = args['--traj']\nrun_recon = str(args['--recon']) == 'True'\n\nimport numpy\n\ndef EhE(E, image):\n    return E.backward( E.forward(image) )\n\ndef ConjugateGradient(rawdata, num_iter = 10, stop_criterion = 1e-7):\n\n    print('---\\n computing coil sensitivity maps...')\n    csms = CoilSensitivityData()\n    csms.smoothness = 10\n    csms.calculate(rawdata)\n    \n    print('---\\n Setting up Acquisition Model...')\n\n    E = AcquisitionModel()\n    E.set_up(rawdata, csms.copy())\n    E.set_coil_sensitivity_maps(csms)\n\n    print('---\\n Backward projection ...')\n    recon_img = E.backward(rawdata)\n    recon_img.fill(0+0j) \n\n    x = recon_img\n    y = rawdata\n\n    r = E.backward( y ) - EhE(E,x)\n\n    rr = r.norm() ** 2\n    rr0 = rr\n\n    p = r\n    \n    print('Cost for k = 0: '  + str( rr/ rr0) )\n    \n    for k in range(num_iter):\n\n        Ap = EhE(E, p )\n\n        alpha = rr / Ap.dot(p)\n\n        x = x + alpha * p\n\n        r = r - alpha * Ap\n\n        beta  = r.norm()**2 / rr\n        rr = r.norm()**2\n\n        p = r + beta * p\n\n        relative_residual = numpy.sqrt(rr/rr0)\n\n        print('Cost at step  {} = {}'.format(k+1, relative_residual))\n        \n        if( relative_residual  < stop_criterion ):\n            print('We achieved our desired accuracy. Stopping iterative reconstruction')\n            break\n\n        if k is num_iter-1:\n            print('Reached maximum number of iterations. Stopping reconstruction.')\n\n    return x\n\ndef main():\n    \n    input_file = existing_filepath(data_path, data_file)\n\n    acq_data = AcquisitionData(input_file)\n    \n    print('---\\n acquisition data norm: %e' % acq_data.norm())\n\n    if trajtype != 'radial' and trajtype != 'goldenangle':\n        print('---\\n pre-processing acquisition data...')\n        processed_data  = preprocess_acquisition_data(acq_data)\n    else:\n        processed_data = acq_data\n\n    print('---\\n setting the trajectory...')\n    if trajtype == 'cartesian':\n        pass\n    elif trajtype == 'grpe':\n        processed_data = set_grpe_trajectory(processed_data)\n    elif trajtype == 'radial':\n        processed_data = set_radial2D_trajectory(processed_data)\n    elif trajtype == 'goldenangle':\n        processed_data = set_goldenangle2D_trajectory(processed_data)\n    else:\n        raise NameError('Please submit a trajectory name of the following list: (cartesian, grpe, radial). You gave {}'\\\n                        .format(trajtype))\n\n    print('---\\n sorting acquisition data...')\n    processed_data.sort()\n    \n    if run_recon:\n        recon = ConjugateGradient(processed_data, num_iter = 20, stop_criterion = 1e-7)\n        \n        if show_plot:\n            recon.show(title = 'Reconstructed images using CG() (magnitude)')\n            \n    else:\n        print('---\\n Skipping non-cartesian code...')\n\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    \n    print('??? %s' % err.value)\n    exit(1)\n",
  "\nfrom functools import reduce\nimport numpy\nimport scipy.linalg\nfrom pyscf import scf\nfrom pyscf import gto\nfrom pyscf import mcscf\nfrom pyscf import dmrgscf\nfrom pyscf import mrpt\n\ndmrgscf.settings.MPIPREFIX = 'mpirun -np 8'\n\n\"\"\n\ndef dmet_cas(mc, dm, implst):\n    from pyscf import lo\n    nao = mc.mol.nao_nr()\n    ncore = mc.ncore\n    ncas = mc.ncas\n    nocc = ncore + ncas\n    nimp = len(implst)\n    nbath = ncas - nimp\n    corth = lo.orth.orth_ao(mol, method='meta_lowdin')\n    s = mol.intor_symmetric('cint1e_ovlp_sph')\n    cinv = numpy.dot(corth.T, s)\n    \n    dm = reduce(numpy.dot, (cinv, dm[0]+dm[1], cinv.T))\n\n    implst = numpy.asarray(implst)\n    notimp = numpy.asarray([i for i in range(nao) if i not in implst])\n    occi, ui = scipy.linalg.eigh(-dm[implst][:,implst])\n    occb, ub = scipy.linalg.eigh(-dm[notimp][:,notimp])\n    bathorb = numpy.dot(corth[:,notimp], ub)\n    imporb = numpy.dot(corth[:,implst], ui)\n    mocore = bathorb[:,:ncore]\n    mocas  = numpy.hstack((imporb, bathorb[:,ncore:ncore+nbath]))\n    moext  = bathorb[:,ncore+nbath:]\n\n    hf_orb = mc._scf.mo_coeff\n    fock = reduce(numpy.dot, (s, hf_orb*mc._scf.mo_energy, hf_orb.T, s))\n\n    fockc = reduce(numpy.dot, (mocore.T, fock, mocore))\n    e, u = scipy.linalg.eigh(fockc)\n    mocore = numpy.dot(mocore, u)\n    focka = reduce(numpy.dot, (mocas.T, fock, mocas))\n    e, u = scipy.linalg.eigh(focka)\n    mocas = numpy.dot(mocas, u)\n    focke = reduce(numpy.dot, (moext.T, fock, moext))\n    e, u = scipy.linalg.eigh(focke)\n    moext = numpy.dot(moext, u)\n\n    mo_init = numpy.hstack((mocore, mocas, moext))\n    return mo_init\n\nmol = gto.Mole()\nmol.atom = [\n    ['Fe', (0.      , 0.0000  , 0.0000)],\n    ['N' , (1.9764  , 0.0000  , 0.0000)],\n    ['N' , (0.0000  , 1.9884  , 0.0000)],\n    ['N' , (-1.9764 , 0.0000  , 0.0000)],\n    ['N' , (0.0000  , -1.9884 , 0.0000)],\n    ['C' , (2.8182  , -1.0903 , 0.0000)],\n    ['C' , (2.8182  , 1.0903  , 0.0000)],\n    ['C' , (1.0918  , 2.8249  , 0.0000)],\n    ['C' , (-1.0918 , 2.8249  , 0.0000)],\n    ['C' , (-2.8182 , 1.0903  , 0.0000)],\n    ['C' , (-2.8182 , -1.0903 , 0.0000)],\n    ['C' , (-1.0918 , -2.8249 , 0.0000)],\n    ['C' , (1.0918  , -2.8249 , 0.0000)],\n    ['C' , (4.1961  , -0.6773 , 0.0000)],\n    ['C' , (4.1961  , 0.6773  , 0.0000)],\n    ['C' , (0.6825  , 4.1912  , 0.0000)],\n    ['C' , (-0.6825 , 4.1912  , 0.0000)],\n    ['C' , (-4.1961 , 0.6773  , 0.0000)],\n    ['C' , (-4.1961 , -0.6773 , 0.0000)],\n    ['C' , (-0.6825 , -4.1912 , 0.0000)],\n    ['C' , (0.6825  , -4.1912 , 0.0000)],\n    ['H' , (5.0441  , -1.3538 , 0.0000)],\n    ['H' , (5.0441  , 1.3538  , 0.0000)],\n    ['H' , (1.3558  , 5.0416  , 0.0000)],\n    ['H' , (-1.3558 , 5.0416  , 0.0000)],\n    ['H' , (-5.0441 , 1.3538  , 0.0000)],\n    ['H' , (-5.0441 , -1.3538 , 0.0000)],\n    ['H' , (-1.3558 , -5.0416 , 0.0000)],\n    ['H' , (1.3558  , -5.0416 , 0.0000)],\n    ['C' , (2.4150  , 2.4083  , 0.0000)],\n    ['C' , (-2.4150 , 2.4083  , 0.0000)],\n    ['C' , (-2.4150 , -2.4083 , 0.0000)],\n    ['C' , (2.4150  , -2.4083 , 0.0000)],\n    ['H' , (3.1855  , 3.1752  , 0.0000)],\n    ['H' , (-3.1855 , 3.1752  , 0.0000)],\n    ['H' , (-3.1855 , -3.1752 , 0.0000)],\n    ['H' , (3.1855  , -3.1752 , 0.0000)],\n]\nmol.basis = 'ccpvdz'\nmol.verbose = 4\nmol.output = 'fepor-dmrgscf.out'\nmol.spin = 4\nmol.symmetry = True\nmol.build()\n\nmf = scf.ROHF(mol)\nmf = scf.fast_newton(mf)\n\nmc = mcscf.approx_hessian(dmrgscf.dmrgci.DMRGSCF(mf, 20, 16))\n\nidx = mol.search_ao_label(['Fe 3d', 'Fe 4d', 'Fe 4s', 'N 2pz'])\nmo = dmet_cas(mc, mf.make_rdm1(), idx)\n\nmc.fcisolver.wfnsym = 'Ag'\nmc.kernel(mo)\n\ne_q = mc.e_tot  \ncas_q = mc.mo_coeff[:,mc.ncore:mc.ncore+mc.ncas]\n\nept2_q = mrpt.NEVPT(mc).kernel()\n\nmol.spin = 2\nmol.build(0, 0)\n\nmf = scf.ROHF(mol)\nmf = scf.fast_newton(mf)\n\nmc = mcscf.approx_hessian(dmrgscf.dmrgci.DMRGSCF(mf, 20, 16))\nidx = mol.search_ao_label(['Fe 3d', 'Fe 4d', 'Fe 4s', 'N 2pz'])\nmo = dmet_cas(mc, mf.make_rdm1(), idx3d)\nmc.fcisolver.wfnsym = 'B1g'\nmc.kernel(mo)\nmo = mc.mo_coeff\n\ne_t = mc.e_tot  \ncas_t = mc.mo_coeff[:,mc.ncore:mc.ncore+mc.ncas]\n\nept2_t = mrpt.NEVPT(mc).kernel()\n\nprint('E(T) = %.15g  E(Q) = %.15g  gap = %.15g' % (e_t, e_q, e_t-e_q))\n\ns = reduce(numpy.dot, (cas_t.T, mol.intor('cint1e_ovlp_sph'), cas_q))\nprint('Active space overlpa <T|Q> ~ %f' % numpy.linalg.det(s))\n\nprint('NEVPT2: E(T) = %.15g  E(Q) = %.15g' % (ept2_t, ept2_q))\n\nfrom pyscf import tools\ntools.molden.from_mo(mol, 'triplet-cas.molden', cas_t)\ntools.molden.from_mo(mol, 'quintet-cas.molden', cas_q)",
  "\"\"\nfrom ansys.dpf import core as dpf\nfrom ansys.dpf.core import examples\nfrom ansys.dpf.core import operators as ops\n\nmodel = dpf.Model(examples.find_complex_rst())\nmesh = model.metadata.meshed_region\n\nvolume_check = 4.0e-11\n\nnodes = mesh.nodes.scoping\nnodes_ids = nodes.ids\nnodes_ids_to_compute = []\nfor i in range(0, 400):\n    nodes_ids_to_compute.append(nodes_ids[i])\nelements = mesh.elements.scoping\nelements_ids = elements.ids\n\nvol_op = ops.result.elemental_volume()\nvol_op.inputs.streams_container(model.metadata.streams_provider)\nvol_field = vol_op.outputs.fields_container()[0]\n\nconnectivity_field = mesh.elements.connectivities_field\nnodal_connectivity_field = mesh.nodes.nodal_connectivity_field\n\nnode_index_to_el_ids = {}\nnode_index_to_found_volume = {}\n\nwith connectivity_field.as_local_field() as connectivity, \\\n    nodal_connectivity_field.as_local_field() as nodal_connectivity,\\\n        vol_field.as_local_field() as vol:  \n    for i, node in enumerate(nodes_ids_to_compute):\n\n        current_node_indexes = [i]\n        volume = 0.0\n        \n        while volume_check > volume:\n            volume = 0.0\n            elements_indexes = []\n\n            for current_node_index in current_node_indexes:\n                elements_indexes.extend(nodal_connectivity.get_entity_data(i).flatten())\n\n            current_node_indexes = []\n            for index in elements_indexes:\n                \n                volume += vol.get_entity_data(index)[0]\n                \n                current_node_indexes.extend(connectivity.get_entity_data(index))\n\n        node_index_to_el_ids[i] = [elements_ids[index] for index in elements_indexes]\n        node_index_to_found_volume[i] = volume\n\ns = model.results.stress()\nto_elemental = ops.averaging.to_elemental_fc(s)\neqv = ops.invariant.von_mises_eqv_fc(to_elemental)\nvalues_to_sum_field = eqv.outputs.fields_container()[0]\n\nseqvsum = dpf.fields_factory.create_scalar_field(len(nodes), dpf.locations.nodal)\ndataseqvsum = []\nvolsum = dpf.fields_factory.create_scalar_field(len(nodes), dpf.locations.nodal)\ndatavolsum = []\n\nwith values_to_sum_field.as_local_field() as values_to_sum:\n    with vol_field.as_local_field() as vol:\n        for key in node_index_to_el_ids:\n            ssum = 0.0\n            for id in node_index_to_el_ids[key]:\n                ssum += (\n                    values_to_sum.get_entity_data_by_id(id)[0] * vol.get_entity_data_by_id(id)[0]\n                )\n            dataseqvsum.append(ssum)\n            datavolsum.append(node_index_to_found_volume[key])\n\nseqvsum.data = dataseqvsum\nseqvsum.scoping.ids = nodes_ids_to_compute\n\nvolsum.data = datavolsum\nvolsum.scoping.ids = nodes_ids_to_compute\n\ndivide = ops.math.component_wise_divide(seqvsum, volsum)\ndivide.run()\n\nmesh.plot(values_to_sum_field)\nmesh.plot(divide.outputs.field())\n\ns_fc = s.outputs.fields_container()\nsingle_field_vol_fc = dpf.fields_container_factory.over_time_freq_fields_container([vol_field])\n\nsingle_field_fc = dpf.fields_container_factory.over_time_freq_fields_container(\n    [values_to_sum_field]\n)\n\nop = dpf.Operator(\"volume_stress\")\nop.inputs.scoping.connect(nodes)\nop.inputs.stress_fields.connect(single_field_fc)\nop.inputs.volume_fields(single_field_vol_fc)\nop.inputs.volume(volume_check * 10.0)\n\nout = op.get_output(0, dpf.types.field)\nmesh.plot(out)",
  "\"\"\nimport numpy as np\n\nfrom ansys.mapdl.core import launch_mapdl\n\nmapdl = launch_mapdl()\n\nmapdl.clear()\nmapdl.prep7()\n\nmapdl.units(\"BIN\")\n\nmapdl.et(1, \"SOLID285\")\nmapdl.mp(\"EX\", 1, 10e6)\nmapdl.mp(\"PRXY\", 1, 0.3)\nmapdl.mp(\"DENS\", 1, 0.1)\nprint(mapdl.mplist())\n\nheight = 10\ninner_width = 2.5\nouter_width = 3.5\nmapdl.rectng(inner_width, outer_width, 0, height)\nmapdl.cyl4(0, height, inner_width, 0, outer_width, 90)\n\na_comb = mapdl.aadd(1, 2)\nmapdl.aplot(color=\"grey\", background=\"w\", show_area_numbering=True)\n\nmapdl.vrotat(a_comb, pax1=6, arc=90)\nmapdl.vplot(background=\"w\")\n\nmapdl.smrtsize(1)\nmapdl.esize(0.25, 0)\nmapdl.mshape(1, \"3D\")\nmapdl.mshkey(0)\nmapdl.vmesh(\"ALL\")\nmapdl.eplot(color=\"grey\", background=\"w\")\n\nmapdl.geometry.area_select([3, 5, 7])\nmapdl.da(\"ALL\", \"SYMM\")\nmapdl.allsel()\n\nmapdl.geometry.area_select([1, 6])\nmapdl.sfa(\"ALL\", 1, \"PRES\", 1000)\nmapdl.allsel()\n\nmapdl.run(\"/SOL\")\nmapdl.antype(0)\nmapdl.outres(\"ALL\", \"ALL\")\nmapdl.run(\"/STATUS,SOLU\")\nsol_output = mapdl.solve()\nmapdl.finish()\n\nmapdl.post1()\nmapdl.set(1, 1)\n\nnnum = mapdl.mesh.nnum\nvon_mises_mapdl = mapdl.post_processing.nodal_eqv_stress()\n\nprint(f\"\\nNode  Stress (psi)\")\nfor node_num, stress_value in zip(nnum[:5], von_mises_mapdl[:5]):\n    print(f\"{node_num:<5d} {stress_value:.3f}\")\nprint(\"...\")\n\nidx = np.argmax(von_mises_mapdl)\nnode_num = nnum[idx]\nstress_value = von_mises_mapdl[idx]\nprint(f\"\\nMaximum Stress\")\nprint(f\"Node  Stress (psi)\")\nprint(f\"{node_num:<5d} {stress_value:.3f}\")\n\nmapdl.post_processing.plot_nodal_eqv_stress(cpos=\"zy\")\n\nresult = mapdl.result\n\nnnum, stress = result.principal_nodal_stress(0)\nvon_mises = stress[:, -1]  \nmin_von_mises, max_von_mises = np.min(von_mises), np.max(von_mises)\nprint(\"All close:\", np.allclose(von_mises, von_mises_mapdl))\n\nmapdl.header(\"OFF\", \"OFF\", \"OFF\", \"OFF\", \"OFF\", \"OFF\")\ntable = mapdl.prnsol(\"S\", \"PRIN\").splitlines()[1:]\nprnsol_eqv = np.genfromtxt(table)[:, -1]  \n\nprint(\"All close:\", np.allclose(von_mises, prnsol_eqv, rtol=1e-4))\n\nprint(f\"LEGACY Reader and MAPDL VGET Min: {min_von_mises}\")\nprint(f\"PRNSOL MAPDL Min:                 {prnsol_eqv.min()}\")\nprint()\nprint(f\"LEGACY Reader and MAPDL VGET Min: {max_von_mises}\")\nprint(f\"PRNSOL MAPDL Min:                 {prnsol_eqv.max()}\")\n\nmapdl.exit()",
  "\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nfrom DREAM.DREAMOutput import DREAMOutput\nfrom DREAM import runiface\n\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.TransportSettings as Transport\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.ElectricField as Efield\nimport DREAM.Settings.Equations.RunawayElectrons as RE\nimport DREAM.Settings.Equations.HotElectronDistribution as FHot\nimport DREAM.Settings.Equations.ColdElectronTemperature as T_cold\n\nfrom DREAM.Settings.Equations.ElectricField import ElectricField\nfrom DREAM.Settings.Equations.ColdElectronTemperature import ColdElectronTemperature\n\nfrom DREAM import DREAMIO\n\nds = DREAMSettings()\n\nds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_SUPERTHERMAL \nds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_PARTIALLY_SCREENED \nds.collisions.bremsstrahlung_mode = Collisions.BREMSSTRAHLUNG_MODE_STOPPING_POWER\nds.collisions.lnlambda = Collisions.LNLAMBDA_ENERGY_DEPENDENT\nds.collisions.pstar_mode = Collisions.PSTAR_MODE_COLLISIONAL\n\nrun_init = True\nrun_exp = True\n\ntransport_mode = Transport.TRANSPORT_RECHESTER_ROSENBLUTH\n\nTmax_init = 1e-11 \nNt_init = 2       \n\nTfinal_exp = 50 \nt0_exp = .5e-3 \nTmax_exp = 10e-3 \nNt_exp = 3000\ntimes_exp = np.linspace(0,Tmax_exp,Nt_exp) \n\nn_D = 1e20 \n\nB0 = 5              \nE_initial = 5e-3    \nE_wall = 0.0        \nT_initial = 20e3    \nT_in_back = 10      \njTot = 1.69e6\n\nNr_kin  = 15        \nNp      = 100        \nNxi     = 1         \npMax    = 3         \ntimes   = [0]       \nradius  = [0, 2]    \nradius_wall = 2.15  \n\ndiffusion_coeff = 100 \n\nhotTailGrid_enabled = True\nif hotTailGrid_enabled == False and transport_mode == Transport.TRANSPORT_RECHESTER_ROSENBLUTH:\n    print('WARNING: Using Rechester-Rosenbluth transport requires f_hot. Enabling hot-tail grid...')\n\nds.radialgrid.setB0(B0)\nds.radialgrid.setMinorRadius(radius[-1])\nds.radialgrid.setWallRadius(radius_wall)\nds.radialgrid.setNr(Nr_kin)\n\nds.timestep.setTmax(Tmax_init)\nds.timestep.setNt(Nt_init)\n\ndensity_D = n_D\n\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=density_D)\n\nefield = E_initial*np.ones((len(times), len(radius)))\nds.eqsys.E_field.setPrescribedData(efield=efield, times=times, radius=radius)\n\nradialgrid = np.linspace(radius[0],radius[-1],Nr_kin)\ntemp_prof=(1-0.99*(radialgrid/radialgrid[-1])**2).reshape(1,-1)\ntemperature_init = Tfinal_exp+(T_initial*temp_prof - Tfinal_exp)\n\nds.eqsys.T_cold.setPrescribedData(temperature=temperature_init, times=[0], radius=radialgrid)\n\nds.eqsys.n_re.setAvalanche(RE.AVALANCHE_MODE_FLUID)\nds.hottailgrid.setEnabled(False) \n\nds.runawaygrid.setEnabled(False)\n\nds.solver.setType(Solver.NONLINEAR)\nds.solver.tolerance.set(reltol=1e-4)\nds.solver.setMaxIterations(maxiter = 100)\nds.solver.setVerbose(False)\nds.output.setTiming(False)\nds.other.include('fluid', 'transport')\n\nif not hotTailGrid_enabled:\n    ds.hottailgrid.setEnabled(False)\nelse:\n    ds.hottailgrid.setEnabled(True)\n    ds.hottailgrid.setNxi(Nxi)\n    ds.hottailgrid.setNp(Np)\n    ds.hottailgrid.setPmax(pMax)\n    nfree_initial, rn0 = ds.eqsys.n_i.getFreeElectronDensity()\n    \n    ds.eqsys.f_hot.setInitialProfiles(rn0=rn0, n0=nfree_initial*.99, rT0=radialgrid, T0=temperature_init[0,:])\n    ds.eqsys.f_hot.setBoundaryCondition(bc=FHot.BC_F_0)\n    ds.eqsys.f_hot.setAdvectionInterpolationMethod(ad_int=FHot.AD_INTERP_TCDF)\n\nif transport_mode == Transport.TRANSPORT_PRESCRIBED:\n    ds.eqsys.n_re.transport.prescribeDiffusion(drr=diffusion_coeff)\n    ds.eqsys.n_re.transport.setBoundaryCondition(Transport.BC_F_0)\n    \nelif transport_mode  == Transport.TRANSPORT_RECHESTER_ROSENBLUTH and hotTailGrid_enabled:\n    ds.eqsys.f_hot.transport.setMagneticPerturbation(1e-5)\n    ds.eqsys.f_hot.transport.setBoundaryCondition(Transport.BC_F_0)\n\nif run_init:\n    ds.save('initsim.h5')\n    runiface(ds,f'out_1init.h5')\n\ndo=DREAMOutput(f'out_1init.h5')\nconductivity=do.other.fluid.conductivity.getData()\njprof=(1-(1-0.001**(1/0.41))*(radialgrid/radialgrid[-1])**2)**0.41\nefield=jTot*jprof/conductivity[-1,:]\nds.eqsys.E_field.setPrescribedData(efield=efield, radius=radialgrid)\n\nds.save(f'settings_1init.h5')        \nif run_init:\n    runiface(ds,f'out_1init.h5')\n\nds3 = DREAMSettings(ds)\nif run_exp:\n    ds3.fromOutput(f'out_1init.h5')\n\ntemperature_exp = Tfinal_exp+(T_initial*temp_prof - Tfinal_exp) * np.exp(-times_exp/t0_exp).reshape(-1,1)\nds3.eqsys.T_cold.setPrescribedData(temperature=temperature_exp, times=times_exp, radius=radialgrid)\nds3.eqsys.E_field.setType(Efield.TYPE_SELFCONSISTENT)\nds3.eqsys.E_field.setBoundaryCondition(bctype = Efield.BC_TYPE_PRESCRIBED, inverse_wall_time = 0, V_loop_wall_R0 = E_wall*2*np.pi)\n\nds3.timestep.setTmax(Tmax_exp)\nds3.timestep.setNt(Nt_exp)\n\nds3.save(f'settings_2exp.h5')\nif run_exp:\n    runiface(ds3,f'out_2exp.h5')\n        ",
  "\nfrom pprint import pprint\nfrom hpeOneView.oneview_client import OneViewClient\nfrom config_loader import try_load_from_file\n\nconfig = {\n    \"ip\": \"\",\n    \"credentials\": {\n        \"userName\": \"\",\n        \"password\": \"\"\n    }\n}\n\nconfig = try_load_from_file(config)\n\noneview_client = OneViewClient(config)\nusers = oneview_client.users\nscopes = oneview_client.scopes\n\nscope_options = {\n    \"name\": \"SampleScopeForTest\",\n    \"description\": \"Sample Scope description\"\n}\nscope = scopes.get_by_name(scope_options['name'])\nif not scope:\n    scope = scopes.create(scope_options)\nscope_uri = scope.data['uri']\n\noptions = {\n    'emailAddress': 'testUser@example.com',\n    'enabled': 'true',\n    'fullName': 'testUser101',\n    'mobilePhone': '555-2121',\n    'officePhone': '555-1212',\n    'password': 'myPass1234',\n    'permissions': [\n        {\n            'roleName': 'Infrastructure administrator',\n            'scopeUri': scope_uri\n        }\n    ],\n    'type': 'UserAndPermissions',\n    'userName': 'testUser'\n}\n\nmulti_users = [\n    {\n        'emailAddress': 'testUser@example.com',\n        'enabled': 'true',\n        'fullName': 'testUser101',\n        'mobilePhone': '555-2121',\n        'officePhone': '555-1212',\n        'password': 'myPass1234',\n        'permissions': [\n            {\n                'roleName': 'Read only',\n            }\n        ],\n        'type': 'UserAndPermissions',\n        'userName': 'testUser1'\n    },\n    {\n        'emailAddress': 'testUser@example.com',\n        'enabled': 'true',\n        'fullName': 'testUser101',\n        'mobilePhone': '555-2121',\n        'officePhone': '555-1212',\n        'password': 'myPass1234',\n        'permissions': [\n            {\n                'roleName': 'Read only',\n            }\n        ],\n        'type': 'UserAndPermissions',\n        'userName': 'testUser2'\n    }\n]\n\nuser = users.create(options)\nprint(\"Created user '%s' successfully.\\n  uri = '%s'\\n\" % (user.data['userName'], user.data['uri']))\nprint(user.data)\n\nmulti_user = users.create_multiple_user(multi_users)\nprint(\"\\nCreated multiple users successfully.\\n\")\nprint(multi_user.data)\n\ndata = user.data.copy()\ndata[\"password\"] = \"change1234\"\nupdated_user = user.update(data)\nprint(\"\\nThe users is updated successfully....\\n\")\nprint(updated_user.data)\n\nrole_options = [\n    {\n        \"roleName\": \"Backup administrator\"\n    }\n]\nrole = users.add_role_to_userName(\"testUser1\", role_options)\nprint(\"\\nSuccessfully added new role to existing one....\\n\")\nprint(role.data)\n\nrole_options = [\n    {\n        \"roleName\": \"Scope administrator\"\n    },\n    {\n        \"roleName\": \"Backup administrator\"\n    },\n    {\n        \"roleName\": \"Infrastructure administrator\"\n    }\n]\n\nrole = users.update_role_to_userName(\"testUser1\", role_options)\nprint(\"\\nSuccessfully updated the role to the username....\\n\")\nprint(role)\n\nrole = users.remove_role_from_username(\"testUser1\", [\"Scope administrator\", \"Backup administrator\"])\nprint(\"\\nRemoved role from the user successfully...\\n\")\nprint(role)\n\nuser = users.get_by_userName(options['userName'])\nif user:\n    print(\"\\nFound user by uri = '%s'\\n\" % user.data['uri'])\n\nprint(\"\\nGet all users\")\nall_users = users.get_all()\npprint(all_users)\n\nbol = users.validate_full_name(options['fullName'])\nprint(\"Is full name already in use? %s\" % (bol.data))\n\nbol = users.validate_user_name(options['userName'])\nprint(\"Is user name already in use? %s\" % (bol.data))\n\nrolelist = users.get_role_associated_with_userName(\"testUser\")\nprint(\"\\n>> Got all the roles for the users\\n\")\nprint(rolelist)\n\nrole = users.get_user_by_role(\"Infrastructure administrator\")\nprint(\"\\n>> Got the users by role name\\n\")\nprint(role)\n\nuser_to_delete = users.get_by_userName(\"testUser\")\nif user_to_delete:\n    user_to_delete.delete()\n    print(\"\\nSuccessfully deleted the testuser2 user.....\\n\")\n\nuser_name = [\"testUser1\", \"testUser2\"]\nusers.delete_multiple_user(user_name)\nprint(\"\\nDeleted multiple users successfully...\\n\")\n\n\"\"",
  "\"\"\n\nimport os\nimport pyaedt\n\nnon_graphical = False\ndesktop_version = \"2023.2\"\n\nq = pyaedt.Q2d(specified_version=desktop_version,\n               non_graphical=non_graphical,\n               new_desktop_session=True,\n               projectname=pyaedt.generate_unique_name(\"pyaedt_q2d_example\"),\n               designname=\"coplanar_waveguide\")\n\ne_factor = \"e_factor\"\nsig_bot_w = \"sig_bot_w\"\nco_gnd_w = \"gnd_w\"\nclearance = \"clearance\"\ncond_h = \"cond_h\"\nd_h = \"d_h\"\nsm_h = \"sm_h\"\n\nfor var_name, var_value in {\n    \"sig_bot_w\": \"150um\",\n    \"e_factor\": \"2\",\n    \"gnd_w\": \"500um\",\n    \"clearance\": \"150um\",\n    \"cond_h\": \"50um\",\n    \"d_h\": \"150um\",\n    \"sm_h\": \"20um\",\n}.items():\n    q[var_name] = var_value\n\ndelta_w_half = \"({0}/{1})\".format(cond_h, e_factor)\nsig_top_w = \"({1}-{0}*2)\".format(delta_w_half, sig_bot_w)\nco_gnd_top_w = \"({1}-{0}*2)\".format(delta_w_half, co_gnd_w)\nmodel_w = \"{}*2+{}*2+{}\".format(co_gnd_w, clearance, sig_bot_w)\n\nlayer_1_lh = 0\nlayer_1_uh = cond_h\nlayer_2_lh = layer_1_uh + \"+\" + d_h\nlayer_2_uh = layer_2_lh + \"+\" + cond_h\n\nbase_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_lh, 0], [sig_bot_w, layer_2_lh, 0]], name=\"signal\")\ntop_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_uh, 0], [sig_top_w, layer_2_uh, 0]])\nq.modeler.move(objid=[top_line_obj], vector=[delta_w_half, 0, 0])\nq.modeler.connect([base_line_obj, top_line_obj])\nq.modeler.move(objid=[base_line_obj], vector=[\"{}+{}\".format(co_gnd_w, clearance), 0, 0])\n\nbase_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_lh, 0], [co_gnd_w, layer_2_lh, 0]],\n                                          name=\"co_gnd_left\")\ntop_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_uh, 0], [co_gnd_top_w, layer_2_uh, 0]])\nq.modeler.move(objid=[top_line_obj], vector=[delta_w_half, 0, 0])\nq.modeler.connect([base_line_obj, top_line_obj])\n\nbase_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_lh, 0], [co_gnd_w, layer_2_lh, 0]],\n                                          name=\"co_gnd_right\")\ntop_line_obj = q.modeler.create_polyline(position_list=[[0, layer_2_uh, 0], [co_gnd_top_w, layer_2_uh, 0]])\nq.modeler.move(objid=[top_line_obj], vector=[delta_w_half, 0, 0])\nq.modeler.connect([base_line_obj, top_line_obj])\nq.modeler.move(objid=[base_line_obj], vector=[\"{}+{}*2+{}\".format(co_gnd_w, clearance, sig_bot_w), 0, 0])\n\nq.modeler.create_rectangle(position=[0, layer_1_lh, 0], dimension_list=[model_w, cond_h], name=\"ref_gnd\")\n\nq.modeler.create_rectangle(\n    position=[0, layer_1_uh, 0], dimension_list=[model_w, d_h], name=\"Dielectric\", matname=\"FR4_epoxy\"\n)\n\nsm_obj_list = []\nids = [1,2,3]\nif desktop_version >= \"2023.1\":\n    ids = [0,1,2]\n\nfor obj_name in [\"signal\", \"co_gnd_left\", \"co_gnd_right\"]:\n    obj = q.modeler.get_object_from_name(obj_name)\n    e_obj_list = []\n    for i in ids:\n        e_obj = q.modeler.create_object_from_edge(obj.edges[i])\n        e_obj_list.append(e_obj)\n    e_obj_1 = e_obj_list[0]\n    q.modeler.unite(e_obj_list)\n    new_obj = q.modeler.sweep_along_vector(e_obj_1.id, [0, sm_h, 0])\n    sm_obj_list.append(e_obj_1)\n\nnew_obj = q.modeler.create_rectangle(position=[co_gnd_w, layer_2_lh, 0], dimension_list=[clearance, sm_h])\nsm_obj_list.append(new_obj)\n\nnew_obj = q.modeler.create_rectangle(position=[co_gnd_w, layer_2_lh, 0], dimension_list=[clearance, sm_h])\nq.modeler.move([new_obj], [sig_bot_w + \"+\" + clearance, 0, 0])\nsm_obj_list.append(new_obj)\n\nsm_obj = sm_obj_list[0]\nq.modeler.unite(sm_obj_list)\nsm_obj.material_name = \"SolderMask\"\nsm_obj.color = (0, 150, 100)\nsm_obj.name = \"solder_mask\"\n\nobj = q.modeler.get_object_from_name(\"signal\")\nq.assign_single_conductor(\n    name=obj.name, target_objects=[obj], conductor_type=\"SignalLine\", solve_option=\"SolveOnBoundary\", unit=\"mm\"\n)\n\nobj = [q.modeler.get_object_from_name(i) for i in [\"co_gnd_left\", \"co_gnd_right\", \"ref_gnd\"]]\nq.assign_single_conductor(\n    name=\"gnd\", target_objects=obj, conductor_type=\"ReferenceGround\", solve_option=\"SolveOnBoundary\", unit=\"mm\"\n)\n\nobj = q.modeler.get_object_from_name(\"signal\")\nq.assign_huray_finitecond_to_edges(obj.edges, radius=\"0.5um\", ratio=3, name=\"b_\" + obj.name)\n\nsetup = q.create_setup(setupname=\"new_setup\")\n\nsweep = setup.add_sweep(sweepname=\"sweep1\", sweeptype=\"Discrete\")\nsweep.props[\"RangeType\"] = \"LinearStep\"\nsweep.props[\"RangeStart\"] = \"1GHz\"\nsweep.props[\"RangeStep\"] = \"100MHz\"\nsweep.props[\"RangeEnd\"] = \"5GHz\"\nsweep.props[\"SaveFields\"] = False\nsweep.props[\"SaveRadFields\"] = False\nsweep.props[\"Type\"] = \"Interpolating\"\n\nsweep.update()\n\nq.analyze()\n\na = q.post.get_solution_data(expressions=\"Z0(signal,signal)\", context=\"Original\")\na.plot()\n\nhome = os.path.expanduser(\"~\")\nq.save_project(os.path.join(home, \"Downloads\", \"pyaedt_example\", q.project_name + \".aedt\"))\nq.release_desktop()",
  "\n\"\"\n\nfrom pyscf import gto, scf, agf2\nimport numpy as np\nfrom functools import reduce\n\nmol = gto.M(atom='O 0 0 0; H 0 0 1; H 0 1 0', basis='cc-pvdz')\n\nmf = scf.RHF(mol).density_fit(auxbasis='cc-pv5z-ri')\nmf.conv_tol = 1e-12\nmf.run()\n\ngf2 = agf2.AGF2(mf)\ngf2.conv_tol = 1e-7\ngf2.run(verbose=4)\n\ngf2.ipagf2(nroots=3)\n\ngf2.eaagf2(nroots=3)\n\ndm = gf2.make_rdm1()\n\ndipole = [0.0, 0.0, 0.0]\n\nmol.set_common_origin([0,0,0])\nr_ints_ao = mol.intor('cint1e_r_sph', comp=3)\nr_ints_mo = np.empty_like(r_ints_ao)\nfor i in range(3):\n    r_ints_mo[i] = reduce(np.dot,(mf.mo_coeff.T, r_ints_ao[i], mf.mo_coeff))\n    dipole[i] = -np.trace(np.dot(dm, r_ints_mo[i]))\n    \n    for j in range(mol.natm):\n        dipole[i] += mol.atom_charge(j) * mol.atom_coord(j)[i]\n\nprint('Dipole moment from AGF2: {} {} {}'.format(dipole[0], dipole[1], dipole[2]))",
  "import argparse \nimport numpy as np\nfrom scipy.sparse.linalg import spsolve\nimport matplotlib.pyplot as plt\n\nfrom fealpy.pde.poisson_2d import CosCosData\nfrom fealpy.pde.poisson_2d import LShapeRSinData\n\nfrom fealpy.mesh import PolygonMesh\nfrom fealpy.mesh import HalfEdgeMesh2d\n\nfrom fealpy.functionspace import NonConformingScalarVESpace2d\n\nfrom fealpy.vem import ScaledMonomialSpaceMassIntegrator2d\nfrom fealpy.vem import NonConformingVEMDoFIntegrator2d\nfrom fealpy.vem import NonConformingScalarVEMH1Projector2d\nfrom fealpy.vem import NonConformingScalarVEML2Projector2d \nfrom fealpy.vem import NonConformingScalarVEMLaplaceIntegrator2d\nfrom fealpy.vem import NonConformingVEMScalarSourceIntegrator2d\nfrom fealpy.vem import PoissonCVEMEstimator\nfrom fealpy.mesh.adaptive_tools import mark\n\nfrom fealpy.vem import BilinearForm\n\nfrom fealpy.vem import LinearForm\n\nfrom fealpy.boundarycondition import DirichletBC \nfrom fealpy.tools.show import showmultirate\n\nparser = argparse.ArgumentParser(description=\n        \"\")\n\nparser.add_argument('--degree',\n        default=4, type=int,\n        help='虚单元空间的次数, 默认为 1 次.')\n\nparser.add_argument('--maxit',\n        default=400, type=int,\n        help='默认网格加密求解的次数, 默认加密求解 4 次')\n\nparser.add_argument('--theta',\n        default=0.2, type=int,\n        help='自适应参数， 默认0.2')\nargs = parser.parse_args()\n\ndegree = args.degree\nmaxit = args.maxit\ntheta = args.theta\n\npde = LShapeRSinData()\ndomain = pde.domain()\n\nerrorType = ['$|| u - \\Pi u_h||_{0,\\Omega}$',\n             '$||\\\\nabla u -  \\\\nabla \\Pi u_h||_{0, \\Omega}$',\n             '$\\eta$']\n\nerrorMatrix = np.zeros((3, maxit), dtype=np.float64)\nNDof = np.zeros(maxit, dtype=np.float64)\n\nmesh = pde.init_mesh(n = 1, meshtype='quad')\nmesh = PolygonMesh.from_mesh(mesh)\nHmesh = HalfEdgeMesh2d.from_mesh(mesh)\n\nfig = plt.figure()\naxes  = fig.gca()\nmesh.add_plot(axes)\nplt.show()\n\nfor i in range(maxit):\n    space = NonConformingScalarVESpace2d(mesh, p=degree)\n    uh = space.function()\n    \n    NDof[i] = space.number_of_global_dofs()\n    \n    m = ScaledMonomialSpaceMassIntegrator2d()\n    M = m.assembly_cell_matrix(space.smspace)\n\n    d = NonConformingVEMDoFIntegrator2d()\n    D = d.assembly_cell_matrix(space, M)\n\n    h1 = NonConformingScalarVEMH1Projector2d(D)\n    PI1 = h1.assembly_cell_matrix(space)\n    G = h1.G\n\n    li = NonConformingScalarVEMLaplaceIntegrator2d(PI1, G, D)\n    bform = BilinearForm(space)\n    bform.add_domain_integrator(li)\n    A = bform.assembly()\n\n    l2 = NonConformingScalarVEML2Projector2d(M, PI1)\n    PI0 = l2.assembly_cell_matrix(space)\n\n    si = NonConformingVEMScalarSourceIntegrator2d(pde.source, PI0)\n    lform = LinearForm(space)\n    lform.add_domain_integrator(si)\n    F = lform.assembly()\n\n    bc = DirichletBC(space, pde.dirichlet)\n    A, F = bc.apply(A, F, uh)\n\n    uh[:] = spsolve(A, F)\n    sh = space.project_to_smspace(uh, PI1)\n\n    estimator = PoissonCVEMEstimator(space, M, PI1)\n    eta = estimator.residual_estimate(uh, pde.source)\n    \n    print(i,\":\",NDof[i],\",\",np.sqrt(np.sum(eta))) \n    errorMatrix[0, i] = mesh.error(pde.solution, sh.value)\n    errorMatrix[1, i] = mesh.error(pde.gradient, sh.grad_value)\n    errorMatrix[2, i] = np.sqrt(np.sum(eta))\n    \n    isMarkedCell = mark(eta, theta, 'L2')\n    Hmesh.adaptive_refine(isMarkedCell, method='poly')\n    newcell, cellocation = Hmesh.entity('cell')\n    newnode = Hmesh.entity(\"node\")[:]\n    mesh = PolygonMesh(newnode, newcell, cellocation)\n    if NDof[i] > 1e4 :\n        iterations = i \n        break\n    \"\"\n    \n    \"\"\n\nshowmultirate(plt, iterations-20, NDof[:iterations], errorMatrix[:,:iterations], \n        errorType, propsize=20, lw=2, ms=4)\nnp.savetxt(\"Ndof.txt\", NDof[:iterations], delimiter=',')\nnp.savetxt(\"errorMatrix.txt\", errorMatrix[:,:iterations], delimiter=',')\n\nplt.xlabel('Number of d.o.f', fontdict={'family' : 'Times New Roman', 'size'   : 16})\nplt.ylabel('Error', fontdict={'family' : 'Times New Roman', 'size'   : 16})\nplt.savefig('error.jpg')\nplt.show()\n\nfig1 = plt.figure()\naxes  = fig1.gca()\nmesh.add_plot(axes)\nplt.show()\n",
  "import numpy as np\nimport scipy.sparse as sps\n\nfrom porepy.viz.exporter import Exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import vem_dual, vem_source\n\ndef add_data(gb, domain, kf):\n    \"\"\n    gb.add_node_props(['param'])\n    tol = 1e-5\n    a = 1e-4\n\n    for g, d in gb:\n        param = Parameters(g)\n\n        kxx = np.ones(g.num_cells) * np.power(kf, g.dim < gb.dim_max())\n        if g.dim == 2:\n            perm = tensor.SecondOrder(g.dim, kxx=kxx, kyy=kxx, kzz=1)\n        else:\n            perm = tensor.SecondOrder(g.dim, kxx=kxx, kyy=1, kzz=1)\n        param.set_tensor(\"flow\", perm)\n\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        aperture = np.power(a, gb.dim_max() - g.dim)\n        param.set_aperture(np.ones(g.num_cells) * aperture)\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[right] = 'dir'\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = -aperture \\\n                * g.face_areas[bound_faces[left]]\n            bc_val[bound_faces[right]] = 1\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        gn = gb.sorted_nodes_of_edge(e)\n        aperture = np.power(a, gb.dim_max() - gn[0].dim)\n        d['kn'] = np.ones(gn[0].num_cells) * kf / aperture\n\ndef write_network(file_name):\n    network = \"FID,START_X,START_Y,END_X,END_Y\\n\"\n    network += \"0,0,0.5,1,0.5\\n\"\n    network += \"1,0.5,0,0.5,1\\n\"\n    network += \"2,0.5,0.75,1,0.75\\n\"\n    network += \"3,0.75,0.5,0.75,1\\n\"\n    network += \"4,0.5,0.625,0.75,0.625\\n\"\n    network += \"5,0.625,0.5,0.625,0.75\\n\"\n    with open(file_name, \"w\") as text_file:\n        text_file.write(network)\n\ndef main(kf, description, is_coarse=False, if_export=False):\n    mesh_kwargs = {}\n    mesh_kwargs['mesh_size'] = {'mode': 'constant',\n                                'value': 0.045, 'bound_value': 0.045}\n\n    domain = {'xmin': 0, 'xmax': 1, 'ymin': 0, 'ymax': 1}\n\n    file_name = 'network_geiger.csv'\n    write_network(file_name)\n    gb = importer.dfm_2d_from_csv(file_name, mesh_kwargs, domain)\n    gb.compute_geometry()\n    if is_coarse:\n        co.coarsen(gb, 'by_volume')\n    gb.assign_node_ordering()\n\n    add_data(gb, domain, kf)\n\n    solver_flow = vem_dual.DualVEMMixedDim('flow')\n    A_flow, b_flow = solver_flow.matrix_rhs(gb)\n\n    solver_source = vem_source.IntegralMixedDim('flow')\n    A_source, b_source = solver_source.matrix_rhs(gb)\n\n    up = sps.linalg.spsolve(A_flow + A_source, b_flow + b_source)\n    solver_flow.split(gb, \"up\", up)\n\n    gb.add_node_props([\"discharge\", 'pressure', \"P0u\"])\n    solver_flow.extract_u(gb, \"up\", \"discharge\")\n    solver_flow.extract_p(gb, \"up\", 'pressure')\n    solver_flow.project_u(gb, \"discharge\", \"P0u\")\n\n    if if_export:\n        save = Exporter(gb, \"vem\", folder=\"vem_\" + description)\n        save.write_vtk(['pressure', \"P0u\"])\n\ndef test_vem_blocking():\n    kf = 1e-4\n    main(kf, \"blocking\")\n\ndef test_vem_permeable():\n    kf = 1e4\n    main(kf, \"permeable\")\n",
  "\nimport os\nimport distutils.util\n\nimport numpy as np\nimport fast_tokenizer\nfrom paddlenlp.transformers import AutoTokenizer\nimport fastdeploy as fd\n\ndef parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\", required=True, help=\"The directory of model.\")\n    parser.add_argument(\n        \"--vocab_path\",\n        type=str,\n        default=\"\",\n        help=\"The path of tokenizer vocab.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        choices=['gpu', 'cpu', 'kunlunxin'],\n        help=\"Type of inference device, support 'cpu', 'kunlunxin' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default='onnx_runtime',\n        choices=[\n            'onnx_runtime', 'paddle', 'openvino', 'tensorrt', 'paddle_tensorrt'\n        ],\n        help=\"The inference runtime backend.\")\n    parser.add_argument(\n        \"--batch_size\", type=int, default=1, help=\"The batch size of data.\")\n    parser.add_argument(\n        \"--max_length\",\n        type=int,\n        default=128,\n        help=\"The max length of sequence.\")\n    parser.add_argument(\n        \"--log_interval\",\n        type=int,\n        default=10,\n        help=\"The interval of logging.\")\n    parser.add_argument(\n        \"--use_fp16\",\n        type=distutils.util.strtobool,\n        default=False,\n        help=\"Wheter to use FP16 mode\")\n    parser.add_argument(\n        \"--use_fast\",\n        type=distutils.util.strtobool,\n        default=False,\n        help=\"Whether to use fast_tokenizer to accelarate the tokenization.\")\n    return parser.parse_args()\n\ndef batchfy_text(texts, batch_size):\n    batch_texts = []\n    batch_start = 0\n    while batch_start < len(texts):\n        batch_texts += [\n            texts[batch_start:min(batch_start + batch_size, len(texts))]\n        ]\n        batch_start += batch_size\n    return batch_texts\n\nclass ErnieForSequenceClassificationPredictor(object):\n    def __init__(self, args):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            'ernie-3.0-medium-zh', use_faster=args.use_fast)\n        self.runtime = self.create_fd_runtime(args)\n        self.batch_size = args.batch_size\n        self.max_length = args.max_length\n\n    def create_fd_runtime(self, args):\n        option = fd.RuntimeOption()\n        model_path = os.path.join(args.model_dir, \"infer.pdmodel\")\n        params_path = os.path.join(args.model_dir, \"infer.pdiparams\")\n        option.set_model_path(model_path, params_path)\n        if args.device == 'kunlunxin':\n            option.use_kunlunxin()\n            option.use_paddle_lite_backend()\n            return fd.Runtime(option)\n        if args.device == 'cpu':\n            option.use_cpu()\n        else:\n            option.use_gpu()\n        if args.backend == 'paddle':\n            option.use_paddle_infer_backend()\n        elif args.backend == 'onnx_runtime':\n            option.use_ort_backend()\n        elif args.backend == 'openvino':\n            option.use_openvino_backend()\n        else:\n            option.use_trt_backend()\n            if args.backend == 'paddle_tensorrt':\n                option.enable_paddle_to_trt()\n                option.enable_paddle_trt_collect_shape()\n            trt_file = os.path.join(args.model_dir, \"infer.trt\")\n            option.set_trt_input_shape(\n                'input_ids',\n                min_shape=[1, args.max_length],\n                opt_shape=[args.batch_size, args.max_length],\n                max_shape=[args.batch_size, args.max_length])\n            option.set_trt_input_shape(\n                'token_type_ids',\n                min_shape=[1, args.max_length],\n                opt_shape=[args.batch_size, args.max_length],\n                max_shape=[args.batch_size, args.max_length])\n            if args.use_fp16:\n                option.enable_trt_fp16()\n                trt_file = trt_file + \".fp16\"\n            option.set_trt_cache_file(trt_file)\n        return fd.Runtime(option)\n\n    def preprocess(self, texts, texts_pair):\n        data = self.tokenizer(\n            texts,\n            texts_pair,\n            max_length=self.max_length,\n            padding=True,\n            truncation=True)\n        input_ids_name = self.runtime.get_input_info(0).name\n        token_type_ids_name = self.runtime.get_input_info(1).name\n        input_map = {\n            input_ids_name: np.array(\n                data[\"input_ids\"], dtype=\"int64\"),\n            token_type_ids_name: np.array(\n                data[\"token_type_ids\"], dtype=\"int64\")\n        }\n        return input_map\n\n    def infer(self, input_map):\n        results = self.runtime.infer(input_map)\n        return results\n\n    def postprocess(self, infer_data):\n        logits = np.array(infer_data[0])\n        max_value = np.max(logits, axis=1, keepdims=True)\n        exp_data = np.exp(logits - max_value)\n        probs = exp_data / np.sum(exp_data, axis=1, keepdims=True)\n        out_dict = {\n            \"label\": probs.argmax(axis=-1),\n            \"confidence\": probs.max(axis=-1)\n        }\n        return out_dict\n\n    def predict(self, texts, texts_pair=None):\n        input_map = self.preprocess(texts, texts_pair)\n        infer_result = self.infer(input_map)\n        output = self.postprocess(infer_result)\n        return output\n\nif __name__ == \"__main__\":\n    args = parse_arguments()\n    predictor = ErnieForSequenceClassificationPredictor(args)\n    texts_ds = [\"花呗收款额度限制\", \"花呗支持高铁票支付吗\"]\n    texts_pair_ds = [\"收钱码，对花呗支付的金额有限制吗\", \"为什么友付宝不支持花呗付款\"]\n    batch_texts = batchfy_text(texts_ds, args.batch_size)\n    batch_texts_pair = batchfy_text(texts_pair_ds, args.batch_size)\n\n    for bs, (texts,\n             texts_pair) in enumerate(zip(batch_texts, batch_texts_pair)):\n        outputs = predictor.predict(texts, texts_pair)\n        for i, (sentence1, sentence2) in enumerate(zip(texts, texts_pair)):\n            print(\n                f\"Batch id:{bs}, example id:{i}, sentence1:{sentence1}, sentence2:{sentence2}, label:{outputs['label'][i]}, similarity:{outputs['confidence'][i]:.4f}\"\n            )",
  "from seedemu.layers import Base, Routing, Ebgp, PeerRelationship\nfrom seedemu.services import WebService\nfrom seedemu.compiler import Docker\nfrom seedemu.core import Emulator, Binding, Filter\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nweb = WebService()\n\nbase.createInternetExchange(100)\n\nas150 = base.createAutonomousSystem(150)\n\nas150_web = as150.createHost('web')\n\nweb.install('web150')\nsim.addBinding(Binding('web150', filter = Filter(asn = 150, nodeName = 'web')))\n\nas150_router = as150.createRouter('router0')\nas150_net = as150.createNetwork('net0')\n\nas150_web.joinNetwork('net0')\nas150_router.joinNetwork('net0')\n\nas150_router.joinNetwork('ix100')\n\nas150_router.crossConnect(152, 'router0', '10.50.0.1/30')\n\nas151 = base.createAutonomousSystem(151)\n\nas151_web = as151.createHost('web')\n\nweb.install('web151')\nsim.addBinding(Binding('web151', filter = Filter(asn = 151, nodeName = 'web')))\n\nas151_router = as151.createRouter('router0')\n\nas151_net = as151.createNetwork('net0')\n\nas151_web.joinNetwork('net0')\nas151_router.joinNetwork('net0')\n\nas151_router.joinNetwork('ix100')\n\nas152 = base.createAutonomousSystem(152)\n\nas152_web = as152.createHost('web')\n\nweb.install('web152')\nsim.addBinding(Binding('web152', filter = Filter(asn = 152, nodeName = 'web')))\n\nas152_router = as152.createRouter('router0')\n\nas152_net = as152.createNetwork('net0')\n\nas152_web.joinNetwork('net0')\nas152_router.joinNetwork('net0')\n\nas152_router.crossConnect(150, 'router0', '10.50.0.2/30')\n\nebgp.addRsPeer(100, 150)\nebgp.addRsPeer(100, 151)\n\nebgp.addCrossConnectPeering(150, 152, PeerRelationship.Provider)\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ebgp)\nsim.addLayer(web)\n\nsim.render()\n\nsim.compile(Docker(selfManagedNetwork = True), './cross-connect')",
  "\"\"\n\n__version__ = '1.0.0'\nfrom docopt import docopt\nargs = docopt(__doc__, version=__version__)\n\nfrom ast import literal_eval\n\nfrom sirf.Utilities import show_2D_array\n\nexec('from sirf.' + args['--engine'] + ' import *')\n\ndata_path = args['--path']\nif data_path is None:\n    \n    data_path = examples_data_path('PET') + '/mMR'\nprefix = data_path + '/'\nlist_file = args['--list']\nsino_file = args['--sino']\ntmpl_file = args['--tmpl']\nlist_file = existing_filepath(data_path, list_file)\ntmpl_file = existing_filepath(data_path, tmpl_file)\ninterval = literal_eval(args['--interval'])\nstorage = args['--storage']\nshow_plot = not args['--non-interactive']\n\ndef main():\n\n    AcquisitionData.set_storage_scheme(storage)\n\n    acq_data_template = AcquisitionData(tmpl_file)\n\n    lm2sino = ListmodeToSinograms()\n\n    lm2sino.set_input(list_file)\n    lm2sino.set_output_prefix(sino_file)\n    \n    lm2sino.set_template(acq_data_template)\n    \n    lm2sino.set_time_interval(interval[0], interval[1])\n\n    lm2sino.flag_on('store_prompts')\n    lm2sino.flag_off('interactive')\n\n    lm2sino.set_up()\n\n    lm2sino.process()\n\n    acq_data = lm2sino.get_output()\n    \n    acq_array = acq_data.as_array()\n    acq_dim = acq_array.shape\n    print('acquisition data dimensions: %dx%dx%dx%d' % acq_dim)\n    z = acq_dim[1]//2\n    if show_plot:\n        show_2D_array('Acquisition data', acq_array[0,z,:,:])\n\n    print('estimating randoms, please wait...')\n    randoms = lm2sino.estimate_randoms()\n    rnd_array = randoms.as_array()\n    if show_plot:\n        show_2D_array('Randoms', rnd_array[0,z,:,:])\n\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    print('%s' % err.value)",
  "\n\"\"\n\nfrom absl import app\nfrom absl import flags\nimport acme\nfrom acme import specs\nfrom acme.agents.jax import actor_core as actor_core_lib\nfrom acme.agents.jax import actors\nfrom acme.agents.jax import cql\nfrom acme.datasets import tfds\nfrom acme.examples.offline import helpers as gym_helpers\nfrom acme.jax import variable_utils\nfrom acme.utils import loggers\nimport haiku as hk\nimport jax\nimport optax\n\nflags.DEFINE_integer('batch_size', 64, 'Batch size.')\nflags.DEFINE_integer('evaluate_every', 20, 'Evaluation period.')\nflags.DEFINE_integer('evaluation_episodes', 10, 'Evaluation episodes.')\nflags.DEFINE_integer(\n    'num_demonstrations', 10,\n    'Number of demonstration episodes to load from the dataset. If None, loads the full dataset.'\n)\nflags.DEFINE_integer('seed', 0, 'Random seed for learner and evaluator.')\n\nflags.DEFINE_float('policy_learning_rate', 3e-5, 'Policy learning rate.')\nflags.DEFINE_float('critic_learning_rate', 3e-4, 'Critic learning rate.')\nflags.DEFINE_float('fixed_cql_coefficient', None,\n                   'Fixed CQL coefficient. If None, an adaptive one is used.')\nflags.DEFINE_float('cql_lagrange_threshold', 10.,\n                   'Lagrange threshold for the adaptive CQL coefficient.')\n\nflags.DEFINE_string('env_name', 'HalfCheetah-v2',\n                    'Gym mujoco environment name.')\nflags.DEFINE_string(\n    'dataset_name', 'd4rl_mujoco_halfcheetah/v2-medium',\n    'D4rl dataset name. Can be any locomotion dataset from '\n    'https://www.tensorflow.org/datasets/catalog/overview\n\nFLAGS = flags.FLAGS\n\ndef main(_):\n  key = jax.random.PRNGKey(FLAGS.seed)\n  key_demonstrations, key_learner = jax.random.split(key, 2)\n\n  environment = gym_helpers.make_environment(task=FLAGS.env_name)\n  environment_spec = specs.make_environment_spec(environment)\n\n  transitions_iterator = tfds.get_tfds_dataset(FLAGS.dataset_name,\n                                               FLAGS.num_demonstrations)\n  demonstrations = tfds.JaxInMemoryRandomSampleIterator(\n      transitions_iterator, key=key_demonstrations, batch_size=FLAGS.batch_size)\n\n  networks = cql.make_networks(environment_spec)\n\n  learner = cql.CQLLearner(\n      batch_size=FLAGS.batch_size,\n      networks=networks,\n      random_key=key_learner,\n      policy_optimizer=optax.adam(FLAGS.policy_learning_rate),\n      critic_optimizer=optax.adam(FLAGS.critic_learning_rate),\n      fixed_cql_coefficient=FLAGS.fixed_cql_coefficient,\n      cql_lagrange_threshold=FLAGS.cql_lagrange_threshold,\n      demonstrations=demonstrations,\n      num_sgd_steps_per_step=1)\n\n  def evaluator_network(\n      params: hk.Params, key: jax.Array, observation: jax.Array\n  ) -> jax.Array:\n    dist_params = networks.policy_network.apply(params, observation)\n    return networks.sample_eval(dist_params, key)\n\n  actor_core = actor_core_lib.batched_feed_forward_to_actor_core(\n      evaluator_network)\n  variable_client = variable_utils.VariableClient(\n      learner, 'policy', device='cpu')\n  evaluator = actors.GenericActor(\n      actor_core, key, variable_client, backend='cpu')\n\n  eval_loop = acme.EnvironmentLoop(\n      environment=environment,\n      actor=evaluator,\n      logger=loggers.TerminalLogger('evaluation', time_delta=0.))\n\n  while True:\n    for _ in range(FLAGS.evaluate_every):\n      learner.step()\n    eval_loop.run(FLAGS.evaluation_episodes)\n\nif __name__ == '__main__':\n  app.run(main)",
  "from paz import processors as pr\nfrom paz.abstract import SequentialProcessor, Processor\nfrom processors import MatchBoxes\nimport numpy as np\nimport os\n\nclass PreprocessBoxes(SequentialProcessor):\n    \"\"\n    def __init__(self, num_classes, prior_boxes, IOU, variances):\n        super(PreprocessBoxes, self).__init__()\n        self.add(MatchBoxes(prior_boxes, IOU),)\n        self.add(pr.EncodeBoxes(prior_boxes, variances))\n        self.add(pr.BoxClassToOneHotVector(num_classes))\n\nclass PreprocessImage(SequentialProcessor):\n    \"\"\n    def __init__(self, shape, mean=pr.BGR_IMAGENET_MEAN):\n        super(PreprocessImage, self).__init__()\n        self.add(pr.ResizeImage(shape))\n        self.add(pr.CastImage(float))\n        if mean is None:\n            self.add(pr.NormalizeImage())\n        else:\n            self.add(pr.SubtractMeanImage(mean))\n\nclass AugmentImage(SequentialProcessor):\n    \"\"\n    def __init__(self, shape, bkg_paths, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentImage, self).__init__()\n        \n        self.add(pr.ResizeImage(shape))\n        self.add(pr.BlendRandomCroppedBackground(bkg_paths))\n        self.add(pr.RandomContrast())\n        self.add(pr.RandomBrightness())\n        self.add(pr.RandomSaturation(0.7))\n        self.add(pr.RandomHue())\n        self.add(pr.ConvertColorSpace(pr.RGB2BGR))\n\nclass AugmentBoxes(SequentialProcessor):\n    \"\"\n    def __init__(self, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentBoxes, self).__init__()\n        self.add(pr.ToImageBoxCoordinates())\n        self.add(pr.Expand(mean=mean))\n        self.add(pr.RandomSampleCrop())\n        self.add(pr.RandomFlipBoxesLeftRight())\n        self.add(pr.ToNormalizedBoxCoordinates())\n\nclass DrawBoxData2D(Processor):\n    def __init__(self, class_names, preprocess=None, colors=None):\n        super(DrawBoxData2D, self).__init__()\n        self.class_names, self.colors = class_names, colors\n        self.to_boxes2D = pr.ToBoxes2D(self.class_names)\n        self.draw_boxes2D = pr.DrawBoxes2D(self.class_names, self.colors)\n        self.preprocess = preprocess\n\n    def call(self, image, boxes):\n        if self.preprocess is not None:\n            image, boxes = self.preprocess(image, boxes)\n            boxes = boxes.astype('int')\n        boxes = self.to_boxes2D(boxes)\n        print(boxes)\n        image = self.draw_boxes2D(image, boxes)\n        return image, boxes\n\nclass ShowBoxes(Processor):\n    def __init__(self, class_names, prior_boxes,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(ShowBoxes, self).__init__()\n        self.deprocess_boxes = SequentialProcessor([\n            pr.DecodeBoxes(prior_boxes, variances),\n            pr.ToBoxes2D(class_names, True),\n            pr.FilterClassBoxes2D(class_names[1:])])\n        self.denormalize_boxes2D = pr.DenormalizeBoxes2D()\n        self.draw_boxes2D = pr.DrawBoxes2D(class_names)\n        self.show_image = pr.ShowImage()\n        self.resize_image = pr.ResizeImage((600, 600))\n\n    def call(self, image, boxes):\n        image = self.resize_image(image)\n        boxes2D = self.deprocess_boxes(boxes)\n        boxes2D = self.denormalize_boxes2D(image, boxes2D)\n        image = self.draw_boxes2D(image, boxes2D)\n        image = (image + pr.BGR_IMAGENET_MEAN).astype(np.uint8)\n        image = image[..., ::-1]\n        self.show_image(image)\n        return image, boxes2D\n\nclass AugmentDetection(SequentialProcessor):\n    \"\"\n    def __init__(self, prior_boxes, bkg_paths, split=pr.TRAIN, num_classes=2,\n                 size=300, mean=pr.BGR_IMAGENET_MEAN, IOU=.5,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(AugmentDetection, self).__init__()\n        \n        self.augment_image = AugmentImage((size, size), bkg_paths, mean)\n        self.preprocess_image = PreprocessImage((size, size), mean)\n\n        self.augment_boxes = AugmentBoxes()\n        args = (num_classes, prior_boxes, IOU, variances)\n        self.preprocess_boxes = PreprocessBoxes(*args)\n\n        self.add(pr.UnpackDictionary(['image', 'boxes']))\n        self.add(pr.ControlMap(pr.LoadImage(4), [0], [0]))\n        if split == pr.TRAIN:\n            self.add(pr.ControlMap(self.augment_image, [0], [0]))\n            self.add(pr.ControlMap(self.augment_boxes, [0, 1], [0, 1]))\n        self.add(pr.ControlMap(self.preprocess_image, [0], [0]))\n        self.add(pr.ControlMap(self.preprocess_boxes, [1], [1]))\n        self.add(pr.SequenceWrapper(\n            {0: {'image': [size, size, 3]}},\n            {1: {'boxes': [len(prior_boxes), 4 + num_classes]}}))\n\ndraw_boxes2D = DrawBoxData2D(['background', 'solar_panel'],\n                             pr.ToImageBoxCoordinates())\nif __name__ == \"__main__\":\n    import tensorflow as tf\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(gpus[0], True)\n\n    from data_manager import CSVLoader\n    from paz.models import SSD300\n    import glob\n    model = SSD300()\n    prior_boxes = model.prior_boxes\n\n    path = 'datasets/solar_panel/BoundingBox.txt'\n    class_names = ['background', 'solar_panel']\n    data_manager = CSVLoader(path, class_names)\n    dataset = data_manager.load_data()\n    home = os.path.expanduser('~')\n    bkg_path = os.path.join(home, '.keras/paz/datasets/voc-backgrounds/')\n    wild_card = bkg_path + '*.png'\n    bkg_paths = glob.glob(wild_card)\n    process = AugmentDetection(prior_boxes, bkg_paths)\n    show_boxes2D = ShowBoxes(class_names, prior_boxes)\n    for sample_arg in range(len(dataset)):\n        sample = dataset[sample_arg]\n        wrapped_outputs = process(sample)\n        image = wrapped_outputs['inputs']['image']\n        boxes = wrapped_outputs['labels']['boxes']\n        image, boxes = show_boxes2D(image, boxes)",
  "\nimport pybamm\nimport numpy as np\n\npybamm.set_logging_level(\"INFO\")\n\nmodel = pybamm.lithium_ion.BaseModel(name=\"my li-ion model\")\n\nmodel.submodels[\"external circuit\"] = pybamm.external_circuit.ExplicitCurrentControl(\n    model.param, model.options\n)\nmodel.submodels[\"current collector\"] = pybamm.current_collector.Uniform(model.param)\nmodel.submodels[\"thermal\"] = pybamm.thermal.isothermal.Isothermal(model.param)\nmodel.submodels[\"porosity\"] = pybamm.porosity.Constant(model.param, model.options)\nmodel.submodels[\n    \"electrolyte diffusion\"\n] = pybamm.electrolyte_diffusion.ConstantConcentration(model.param)\nmodel.submodels[\n    \"electrolyte conductivity\"\n] = pybamm.electrolyte_conductivity.LeadingOrder(model.param)\n\nmodel.submodels[\"sei\"] = pybamm.sei.NoSEI(model.param, model.options)\nmodel.submodels[\"sei on cracks\"] = pybamm.sei.NoSEI(\n    model.param, model.options, cracks=True\n)\nmodel.submodels[\"lithium plating\"] = pybamm.lithium_plating.NoPlating(model.param)\n\nfor domain in [\"negative\", \"positive\"]:\n    model.submodels[f\"{domain} active material\"] = pybamm.active_material.Constant(\n        model.param, domain, model.options\n    )\n    model.submodels[\n        f\"{domain} electrode potential\"\n    ] = pybamm.electrode.ohm.LeadingOrder(model.param, domain)\n    model.submodels[f\"{domain} particle\"] = pybamm.particle.XAveragedPolynomialProfile(\n        model.param,\n        domain,\n        options={**model.options, \"particle\": \"uniform profile\"},\n        phase=\"primary\",\n    )\n    model.submodels[\n        f\"{domain} total particle concentration\"\n    ] = pybamm.particle.TotalConcentration(\n        model.param, domain, model.options, phase=\"primary\"\n    )\n\n    model.submodels[\n        f\"{domain} open-circuit potential\"\n    ] = pybamm.open_circuit_potential.SingleOpenCircuitPotential(\n        model.param,\n        domain,\n        \"lithium-ion main\",\n        options=model.options,\n        phase=\"primary\",\n    )\n    model.submodels[f\"{domain} interface\"] = pybamm.kinetics.InverseButlerVolmer(\n        model.param, domain, \"lithium-ion main\", options=model.options\n    )\n    model.submodels[\n        f\"{domain} interface utilisation\"\n    ] = pybamm.interface_utilisation.Full(model.param, domain, model.options)\n    model.submodels[\n        f\"{domain} interface current\"\n    ] = pybamm.kinetics.CurrentForInverseButlerVolmer(\n        model.param, domain, \"lithium-ion main\"\n    )\n    model.submodels[\n        f\"{domain} surface potential difference [V]\"\n    ] = pybamm.electrolyte_conductivity.surface_potential_form.Explicit(\n        model.param, domain, model.options\n    )\n    model.submodels[\n        f\"{domain} particle mechanics\"\n    ] = pybamm.particle_mechanics.NoMechanics(model.param, domain, model.options)\n\nmodel.build_model()\n\ngeometry = pybamm.battery_geometry()\n\nparam = model.default_parameter_values\nparam.process_model(model)\nparam.process_geometry(geometry)\n\nmesh = pybamm.Mesh(geometry, model.default_submesh_types, model.default_var_pts)\n\ndisc = pybamm.Discretisation(mesh, model.default_spatial_methods)\ndisc.process_model(model)\n\nt_eval = np.linspace(0, 3600, 100)\nsolver = pybamm.ScipySolver()\nsolution = solver.solve(model, t_eval)\n\nplot = pybamm.QuickPlot(solution)\nplot.dynamic_plot()",
  "\nfrom seedemu import *\n\nCustomGenesisFileContent = \"\"\n\nemu = Makers.makeEmulatorBaseWith10StubASAndHosts(1)\n\neth = EthereumService(saveState = True, override=True)\n\nblockchain1 = eth.createBlockchain(chainName=\"POW\", consensus=ConsensusMechanism.POW)\n\nblockchain2 = eth.createBlockchain(chainName=\"POA\", consensus=ConsensusMechanism.POA)\n\ne1 = blockchain1.createNode(\"pow-eth1\")\ne2 = blockchain1.createNode(\"pow-eth2\")\ne3 = blockchain1.createNode(\"pow-eth3\")\ne4 = blockchain1.createNode(\"pow-eth4\")\n\ne5 = blockchain2.createNode(\"poa-eth5\")\ne6 = blockchain2.createNode(\"poa-eth6\")\ne7 = blockchain2.createNode(\"poa-eth7\")\ne8 = blockchain2.createNode(\"poa-eth8\")\n\ne1.setBootNode(True).setBootNodeHttpPort(8090).startMiner()\ne2.startMiner()\ne5.setBootNode(True).unlockAccounts().startMiner()\ne6.unlockAccounts().startMiner()\n\ne3.createAccount(balance=20, unit=EthUnit.ETHER, password=\"admin\").unlockAccounts()\ne7.createAccounts(total=3, balance=30, unit=EthUnit.ETHER, password=\"admin\")\n\ne2.importAccount(keyfilePath='./resources/keyfile_to_import', password=\"admin\", balance=10)\n\ne3.enableGethHttp().setGethHttpPort(8540)\n\ne4.setCustomGethCommandOption(\"--http --http.addr 0.0.0.0\")\n\ne5.enableGethWs().setGethWsPort(8541)\ne5.enableGethHttp()\n\ne8.setNoDiscover()\n\ne3.setCustomGeth(\"./resources/custom_geth\")\n\nemu.getVirtualNode('pow-eth1').setDisplayName('Ethereum-POW-1')\nemu.getVirtualNode('pow-eth2').setDisplayName('Ethereum-POW-2')\nemu.getVirtualNode('pow-eth3').setDisplayName('Ethereum-POW-3').addPortForwarding(8545, 8540)\nemu.getVirtualNode('pow-eth4').setDisplayName('Ethereum-POW-4')\n\nemu.getVirtualNode('poa-eth5').setDisplayName('Ethereum-POA-5')\nemu.getVirtualNode('poa-eth6').setDisplayName('Ethereum-POA-6')\nemu.getVirtualNode('poa-eth7').setDisplayName('Ethereum-POA-7')\nemu.getVirtualNode('poa-eth8').setDisplayName('Ethereum-POA-8')\n\nemu.addBinding(Binding('pow-eth1', filter = Filter(asn = 150, nodeName='host_0')))\nemu.addBinding(Binding('pow-eth2', filter = Filter(asn = 151, nodeName='host_0')))\nemu.addBinding(Binding('pow-eth3', filter = Filter(asn = 152, nodeName='host_0')))\nemu.addBinding(Binding('pow-eth4', filter = Filter(asn = 153, nodeName='host_0')))\n\nemu.addBinding(Binding('poa-eth5', filter = Filter(asn = 160, nodeName='host_0')))\nemu.addBinding(Binding('poa-eth6', filter = Filter(asn = 161, nodeName='host_0')))\nemu.addBinding(Binding('poa-eth7', filter = Filter(asn = 162, nodeName='host_0')))\nemu.addBinding(Binding('poa-eth8', filter = Filter(asn = 163, nodeName='host_0')))\n\nemu.addLayer(eth)\nemu.dump('component-blockchain.bin')\n\nemu.render()\n\ndocker = Docker(etherViewEnabled=True)\n\nemu.compile(docker, './output')",
  "\nimport argparse\nfrom pipeline.utils.tools import load_job_config\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation\nfrom pipeline.component import HomoOneHotEncoder\nfrom pipeline.component.homo_feature_binning import HomoFeatureBinning\nfrom pipeline.component import FederatedSample\nfrom pipeline.component import HomoLR\nfrom pipeline.component import HomoSecureBoost\nfrom pipeline.component import LocalBaseline\nfrom pipeline.component import Reader\nfrom pipeline.interface import Data\nfrom pipeline.interface import Model\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    \n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n    arbiter = parties.arbiter[0]\n\n    guest_train_data = {\"name\": \"breast_homo_guest\", \"namespace\": f\"experiment{namespace}\"}\n    host_train_data = {\"name\": \"breast_homo_host\", \"namespace\": f\"experiment{namespace}\"}\n\n    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    reader_0 = Reader(name=\"reader_0\")\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_1.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    data_transform_0 = DataTransform(name=\"data_transform_0\", with_label=True)\n    data_transform_1 = DataTransform(name=\"data_transform_1\")\n\n    federated_sample_0 = FederatedSample(name=\"federated_sample_0\", mode=\"stratified\", method=\"downsample\",\n                                         fractions=[[0, 1.0], [1, 1.0]], task_type=\"homo\")\n\n    homo_binning_0 = HomoFeatureBinning(name='homo_binning_0', sample_bins=10, method=\"recursive_query\")\n    homo_binning_1 = HomoFeatureBinning(name='homo_binning_1')\n\n    homo_onehot_0 = HomoOneHotEncoder(name='homo_onehot_0', need_alignment=True)\n    homo_onehot_1 = HomoOneHotEncoder(name='homo_onehot_1')\n\n    homo_lr_0 = HomoLR(name=\"homo_lr_0\", penalty=\"L2\", tol=0.0001, alpha=1.0,\n                       optimizer=\"rmsprop\", max_iter=5)\n    homo_lr_1 = HomoLR(name=\"homo_lr_1\")\n\n    local_baseline_0 = LocalBaseline(name=\"local_baseline_0\", model_name=\"LogisticRegression\",\n                                     model_opts={\"penalty\": \"l2\", \"tol\": 0.0001, \"C\": 1.0, \"fit_intercept\": True,\n                                                 \"solver\": \"lbfgs\", \"max_iter\": 5, \"multi_class\": \"ovr\"})\n    local_baseline_0.get_party_instance(role='guest', party_id=guest).component_param(need_run=True)\n    local_baseline_0.get_party_instance(role='host', party_id=host).component_param(need_run=True)\n    local_baseline_1 = LocalBaseline(name=\"local_baseline_1\")\n\n    homo_secureboost_0 = HomoSecureBoost(name=\"homo_secureboost_0\", num_trees=3)\n    homo_secureboost_1 = HomoSecureBoost(name=\"homo_secureboost_1\", num_trees=3)\n\n    evaluation_0 = Evaluation(name=\"evaluation_0\")\n    evaluation_1 = Evaluation(name=\"evaluation_1\")\n\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    pipeline.add_component(data_transform_1, data=Data(data=reader_1.output.data),\n                           model=Model(model=data_transform_0.output.model))\n\n    pipeline.add_component(federated_sample_0, data=Data(data=data_transform_0.output.data))\n\n    pipeline.add_component(homo_binning_0, data=Data(data=federated_sample_0.output.data))\n    pipeline.add_component(homo_binning_1, data=Data(data=data_transform_1.output.data),\n                           model=Model(model=homo_binning_0.output.model))\n\n    pipeline.add_component(homo_onehot_0, data=Data(data=homo_binning_0.output.data))\n    pipeline.add_component(homo_onehot_1, data=Data(data=homo_binning_1.output.data),\n                           model=Model(model=homo_onehot_0.output.model))\n\n    pipeline.add_component(homo_lr_0, data=Data(data=homo_onehot_0.output.data))\n    pipeline.add_component(homo_lr_1, data=Data(data=homo_onehot_1.output.data),\n                           model=Model(model=homo_lr_0.output.model))\n\n    pipeline.add_component(local_baseline_0, data=Data(data=homo_onehot_0.output.data))\n    pipeline.add_component(local_baseline_1, data=Data(data=homo_onehot_1.output.data),\n                           model=Model(model=local_baseline_0.output.model))\n\n    pipeline.add_component(homo_secureboost_0, data=Data(data=homo_onehot_0.output.data))\n    pipeline.add_component(homo_secureboost_1, data=Data(data=homo_onehot_1.output.data),\n                           model=Model(model=homo_secureboost_0.output.model))\n\n    pipeline.add_component(evaluation_0,\n                           data=Data(\n                               data=[homo_lr_0.output.data, homo_lr_1.output.data,\n                                     local_baseline_0.output.data, local_baseline_1.output.data]))\n    pipeline.add_component(evaluation_1,\n                           data=Data(\n                               data=[homo_secureboost_0.output.data, homo_secureboost_1.output.data]))\n\n    pipeline.compile()\n\n    pipeline.fit()\n\n    print(pipeline.get_component(\"evaluation_0\").get_summary())\n    print(pipeline.get_component(\"evaluation_1\").get_summary())\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()",
  "\nfrom __future__ import absolute_import, unicode_literals\nimport os\nimport sys\nimport DDG4\nfrom DDG4 import OutputLevel as Output\nfrom g4units import keV\n\n\"\"\n\ndef run():\n  kernel = DDG4.Kernel()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/OpticalSurfaces/compact/ReadMaterialProperties.xml\"))\n\n  DDG4.importConstants(kernel.detectorDescription(), debug=False)\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  geant4.printDetectors()\n  \n  if len(sys.argv) > 1:\n    geant4.setupCshUI(macro=sys.argv[1])\n  else:\n    geant4.setupCshUI()\n\n  geant4.setupTrackingField(prt=True)\n  \n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputLevel = Output.DEBUG\n  prt.OutputType = 3  \n  kernel.eventAction().adopt(prt)\n\n  generator_output_level = Output.INFO\n\n  seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n  act.DebugMaterials = True\n  act.DebugVolumes = True\n  act.DebugShapes = True\n\n  gun = geant4.setupGun(\"Gun\", particle='gamma', energy=5 * keV, multiplicity=1)\n  gun.OutputLevel = generator_output_level\n\n  geant4.setupTracker('MaterialTester')\n\n  phys = geant4.setupPhysics('QGSP_BERT')\n  phys.dump()\n\n  geant4.execute()\n\nif __name__ == \"__main__\":\n  run()",
  "\"\"\n\nfrom avalanche.benchmarks.utils import DataAttribute, ConstantSequence\nfrom avalanche.training.plugins import ReplayPlugin\n\nfrom dataclasses import dataclass\n\nfrom transformers import PreTrainedTokenizerBase\nfrom typing import Optional, Union, Any\n\nfrom transformers.utils import PaddingStrategy\nimport torch\n\nimport avalanche\nimport torch.nn\n\nfrom avalanche.benchmarks import CLScenario, CLStream, CLExperience\nfrom avalanche.evaluation.metrics import accuracy_metrics\nimport avalanche.training.templates.base\nfrom avalanche.benchmarks.utils import AvalancheDataset\nfrom transformers import AutoTokenizer\nfrom transformers import T5ForConditionalGeneration\nfrom datasets import load_dataset\nimport numpy as np\n\n@dataclass\nclass CustomDataCollatorSeq2SeqBeta:\n    \"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    model: Optional[Any] = None\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    label_pad_token_id: int = -100\n    return_tensors: str = \"pt\"\n\n    def __call__(self, features, return_tensors=None):\n        if return_tensors is None:\n            return_tensors = self.return_tensors\n        labels = (\n            [feature[\"labels\"] for feature in features]\n            if \"labels\" in features[0].keys()\n            else None\n        )\n        \n        if labels is not None:\n            max_label_length = max(len(lab) for lab in labels)\n            if self.pad_to_multiple_of is not None:\n                max_label_length = (\n                    (max_label_length + self.pad_to_multiple_of - 1)\n                    // self.pad_to_multiple_of\n                    * self.pad_to_multiple_of\n                )\n\n            padding_side = self.tokenizer.padding_side\n            for feature in features:\n                remainder = [self.label_pad_token_id] * (\n                    max_label_length - len(feature[\"labels\"])\n                )\n                if isinstance(feature[\"labels\"], list):\n                    feature[\"labels\"] = (\n                        feature[\"labels\"] + remainder\n                        if padding_side == \"right\"\n                        else remainder + feature[\"labels\"]\n                    )\n                elif padding_side == \"right\":\n                    feature[\"labels\"] = np.concatenate(\n                        [feature[\"labels\"], remainder]\n                    ).astype(np.int64)\n                else:\n                    feature[\"labels\"] = np.concatenate(\n                        [remainder, feature[\"labels\"]]\n                    ).astype(np.int64)\n\n        features = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=return_tensors,\n        )\n\n        if (\n            labels is not None\n            and self.model is not None\n            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n        ):\n            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(\n                labels=features[\"labels\"]\n            )\n            features[\"decoder_input_ids\"] = decoder_input_ids\n\n        return features\n\nclass HGNaive(avalanche.training.Naive):\n    \"\"\n\n    @property\n    def mb_attention_mask(self):\n        return self.mbatch[\"attention_mask\"]\n\n    @property\n    def mb_x(self):\n        \"\"\n        return self.mbatch[\"input_ids\"]\n\n    @property\n    def mb_y(self):\n        \"\"\n        return self.mbatch[\"labels\"]\n\n    @property\n    def mb_decoder_in_ids(self):\n        \"\"\n        return self.mbatch[\"decoder_input_ids\"]\n\n    @property\n    def mb_token_type_ids(self):\n        return self.mbatch[3]\n\n    def _unpack_minibatch(self):\n        \"\"\n        for k in self.mbatch.keys():\n            self.mbatch[k] = self.mbatch[k].to(self.device)\n\n    def forward(self):\n        out = self.model(\n            input_ids=self.mb_x,\n            attention_mask=self.mb_attention_mask,\n            labels=self.mb_y,\n        )\n        return out.logits\n\n    def criterion(self):\n        mb_output = self.mb_output.view(-1, self.mb_output.size(-1))\n        ll = self._criterion(mb_output, self.mb_y.view(-1))\n        return ll\n\ndef main():\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", padding=True)\n    tokenizer.save_pretrained(\"./MLDATA/NLP/hf_tokenizers\")  \n\n    prefix = \"<2en>\"\n    source_lang = \"de\"\n    target_lang = \"en\"\n    remote_data = load_dataset(\"news_commentary\", \"de-en\")\n\n    def preprocess_function(examples):\n        inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n        targets = [example[target_lang] for example in examples[\"translation\"]]\n        model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(targets, max_length=128, truncation=True)\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n\n    remote_data = remote_data.map(preprocess_function, batched=True)\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n    remote_data = remote_data.remove_columns([\"id\", \"translation\"])\n    remote_data.set_format(type=\"torch\")\n    data_collator = CustomDataCollatorSeq2SeqBeta(tokenizer=tokenizer, model=model)\n\n    train_exps = []\n    for i in range(0, 2):\n        \n        exp_data = remote_data[\"train\"].select(range(30 * i, 30 * (i + 1)))\n        tl = DataAttribute(ConstantSequence(i, len(exp_data)), \"targets_task_labels\")\n\n        exp = CLExperience()\n        exp.dataset = AvalancheDataset(\n            [exp_data], data_attributes=[tl], collate_fn=data_collator\n        )\n        train_exps.append(exp)\n\n    benchmark = CLScenario(\n        [\n            CLStream(\"train\", train_exps),\n            \n        ]\n    )\n    eval_plugin = avalanche.training.plugins.EvaluationPlugin(\n        avalanche.evaluation.metrics.loss_metrics(\n            epoch=True, experience=True, stream=True\n        ),\n        loggers=[avalanche.logging.InteractiveLogger()],\n        strict_checks=False,\n    )\n    plugins = [ReplayPlugin(mem_size=200)]\n    optimizer = torch.optim.Adam(model.parameters(), lr=2)\n    strategy = HGNaive(\n        model,\n        optimizer,\n        torch.nn.CrossEntropyLoss(ignore_index=-100),\n        evaluator=eval_plugin,\n        train_epochs=1,\n        train_mb_size=10,\n        plugins=plugins,\n    )\n    for experience in benchmark.train_stream:\n        strategy.train(experience, collate_fn=data_collator)\n        strategy.eval(benchmark.train_stream)\n\nif __name__ == \"__main__\":\n    main()",
  "\n\"\"\nimport numpy as np\nimport pandas as pd\n\nimport metpy.calc as mpcalc\nfrom metpy.cbook import get_test_data\nfrom metpy.units import units\n\ndef effective_layer(p, t, td, h, height_layer=False):\n    \"\"\n    from metpy.calc import cape_cin, parcel_profile\n    from metpy.units import units\n\n    pbot = None\n\n    for i in range(p.shape[0]):\n        prof = parcel_profile(p[i:], t[i], td[i])\n        sbcape, sbcin = cape_cin(p[i:], t[i:], td[i:], prof)\n        if sbcape >= 100 * units('J/kg') and sbcin > -250 * units('J/kg'):\n            pbot = p[i]\n            hbot = h[i]\n            bot_idx = i\n            break\n    if not pbot:\n        return None, None\n\n    for i in range(bot_idx + 1, p.shape[0]):\n        prof = parcel_profile(p[i:], t[i], td[i])\n        sbcape, sbcin = cape_cin(p[i:], t[i:], td[i:], prof)\n        if sbcape < 100 * units('J/kg') or sbcin < -250 * units('J/kg'):\n            ptop = p[i]\n            htop = h[i]\n            break\n\n    if height_layer:\n        return hbot, htop\n    else:\n        return pbot, ptop\n\ncol_names = ['pressure', 'height', 'temperature', 'dewpoint', 'direction', 'speed']\n\ndf = pd.read_fwf(get_test_data('20110522_OUN_12Z.txt', as_file_obj=False),\n                 skiprows=7, usecols=[0, 1, 2, 3, 6, 7], names=col_names)\n\ndf = df.dropna(subset=('temperature', 'dewpoint', 'direction', 'speed'\n                       ), how='all').reset_index(drop=True)\n\np = df['pressure'].values * units.hPa\nT = df['temperature'].values * units.degC\nTd = df['dewpoint'].values * units.degC\nwdir = df['direction'].values * units.degree\nsped = df['speed'].values * units.knot\nheight = df['height'].values * units.meter\n\nu, v = mpcalc.wind_components(sped, wdir)\n\nctotals = mpcalc.cross_totals(p, T, Td)\nkindex = mpcalc.k_index(p, T, Td)\nshowalter = mpcalc.showalter_index(p, T, Td)\ntotal_totals = mpcalc.total_totals_index(p, T, Td)\nvert_totals = mpcalc.vertical_totals(p, T)\n\nprof = mpcalc.parcel_profile(p, T[0], Td[0])\n\nlift_index = mpcalc.lifted_index(p, T, prof)\ncape, cin = mpcalc.cape_cin(p, T, Td, prof)\n\nlclp, lclt = mpcalc.lcl(p[0], T[0], Td[0])\nlfcp, _ = mpcalc.lfc(p, T, Td)\nel_pressure, _ = mpcalc.el(p, T, Td, prof)\n\nml_t, ml_td = mpcalc.mixed_layer(p, T, Td, depth=50 * units.hPa)\nml_p, _, _ = mpcalc.mixed_parcel(p, T, Td, depth=50 * units.hPa)\nmlcape, mlcin = mpcalc.mixed_layer_cape_cin(p, T, prof, depth=50 * units.hPa)\n\nmu_p, mu_t, mu_td, _ = mpcalc.most_unstable_parcel(p, T, Td, depth=50 * units.hPa)\nmucape, mucin = mpcalc.most_unstable_cape_cin(p, T, Td, depth=50 * units.hPa)\n\n(u_storm, v_storm), *_ = mpcalc.bunkers_storm_motion(p, u, v, height)\ncritical_angle = mpcalc.critical_angle(p, u, v, height, u_storm, v_storm)\n\nnew_p = np.append(p[p > lclp], lclp)\nnew_t = np.append(T[p > lclp], lclt)\nlcl_height = mpcalc.thickness_hydrostatic(new_p, new_t)\n\nsbcape, _ = mpcalc.surface_based_cape_cin(p, T, Td)\n\n*_, total_helicity = mpcalc.storm_relative_helicity(height, u, v, depth=1 * units.km,\n                                                    storm_u=u_storm, storm_v=v_storm)\n\nubshr, vbshr = mpcalc.bulk_shear(p, u, v, height=height, depth=6 * units.km)\nbshear = mpcalc.wind_speed(ubshr, vbshr)\n\nsig_tor = mpcalc.significant_tornado(sbcape, lcl_height,\n                                     total_helicity, bshear).to_base_units()\n\nhbot, htop = effective_layer(p, T, Td, height, height_layer=True)\n\nif hbot:\n    esrh = mpcalc.storm_relative_helicity(height, u, v, depth=htop - hbot, bottom=hbot)\n    eubshr, evbshr = mpcalc.bulk_shear(p, u, v, height=height, depth=htop - hbot, bottom=hbot)\n    ebshear = mpcalc.wind_speed(eubshr, evbshr)\n\n    super_comp = mpcalc.supercell_composite(mucape, esrh[0], ebshear)\nelse:\n    super_comp = np.nan\n\nprint('Important Sounding Parameters for KOUN on 22 Mary 2011 12 UTC')\nprint()\nprint(f'        CAPE: {cape:.2f}')\nprint(f'         CIN: {cin:.2f}')\nprint(f'LCL Pressure: {lclp:.2f}')\nprint(f'LFC Pressure: {lfcp:.2f}')\nprint(f' EL Pressure: {el_pressure:.2f}')\nprint()\nprint(f'   Lifted Index: {lift_index:.2f}')\nprint(f'        K-Index: {kindex:.2f}')\nprint(f'Showalter Index: {showalter:.2f}')\nprint(f'   Cross Totals: {ctotals:.2f}')\nprint(f'   Total Totals: {total_totals:.2f}')\nprint(f'Vertical Totals: {vert_totals:.2f}')\nprint()\nprint('Mixed Layer - Lowest 50-hPa')\nprint(f'     ML Temp: {ml_t:.2f}')\nprint(f'     ML Dewp: {ml_td:.2f}')\nprint(f'     ML CAPE: {mlcape:.2f}')\nprint(f'      ML CIN: {mlcin:.2f}')\nprint()\nprint('Most Unstable - Lowest 50-hPa')\nprint(f'     MU Temp: {mu_t:.2f}')\nprint(f'     MU Dewp: {mu_td:.2f}')\nprint(f' MU Pressure: {mu_p:.2f}')\nprint(f'     MU CAPE: {mucape:.2f}')\nprint(f'      MU CIN: {mucin:.2f}')\nprint()\nprint('Bunkers Storm Motion Vector')\nprint(f'  u_storm: {u_storm:.2f}')\nprint(f'  v_storm: {v_storm:.2f}')\nprint(f'Critical Angle: {critical_angle:.2f}')\nprint()\nprint(f'Storm Relative Helicity: {total_helicity:.2f}')\nprint(f'Significant Tornado Parameter: {sig_tor:.2f}')\nprint(f'Supercell Composite Parameter: {super_comp:.2f}')",
  "\nimport copy\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom helpers.supervised_pt_ditto import SupervisedPTDittoHelper\nfrom learners.supervised_monai_prostate_learner import SupervisedMonaiProstateLearner\nfrom monai.losses import DiceLoss\nfrom monai.networks.nets.unet import UNet\n\nfrom nvflare.apis.dxo import DXO, DataKind, MetaKey, from_shareable\nfrom nvflare.apis.fl_constant import ReturnCode\nfrom nvflare.apis.fl_context import FLContext\nfrom nvflare.apis.shareable import Shareable, make_reply\nfrom nvflare.apis.signal import Signal\nfrom nvflare.app_common.app_constant import AppConstants\n\nclass SupervisedMonaiProstateDittoLearner(SupervisedMonaiProstateLearner):\n    def __init__(\n        self,\n        train_config_filename,\n        aggregation_epochs: int = 1,\n        ditto_model_epochs: int = 1,\n        train_task_name: str = AppConstants.TASK_TRAIN,\n    ):\n        \"\"\n        SupervisedMonaiProstateLearner.__init__(\n            self,\n            train_config_filename=train_config_filename,\n            aggregation_epochs=aggregation_epochs,\n            train_task_name=train_task_name,\n        )\n        self.ditto_helper = None\n        self.ditto_model_epochs = ditto_model_epochs\n\n    def train_config(self, fl_ctx: FLContext):\n        \n        SupervisedMonaiProstateLearner.train_config(self, fl_ctx)\n\n        engine = fl_ctx.get_engine()\n        ws = engine.get_workspace()\n        app_dir = ws.get_app_dir(fl_ctx.get_job_id())\n\n        ditto_model = UNet(\n            spatial_dims=3,\n            in_channels=1,\n            out_channels=1,\n            channels=(16, 32, 64, 128, 256),\n            strides=(2, 2, 2, 2),\n            num_res_units=2,\n        ).to(self.device)\n        ditto_optimizer = optim.SGD(\n            ditto_model.parameters(),\n            lr=self.config_info[\"ditto_learning_rate\"],\n            momentum=0.9,\n        )\n        self.ditto_helper = SupervisedPTDittoHelper(\n            criterion=DiceLoss(sigmoid=True),\n            model=ditto_model,\n            optimizer=ditto_optimizer,\n            device=self.device,\n            app_dir=app_dir,\n            ditto_lambda=self.config_info[\"ditto_lambda\"],\n            model_epochs=self.ditto_model_epochs,\n        )\n\n    def train(\n        self,\n        shareable: Shareable,\n        fl_ctx: FLContext,\n        abort_signal: Signal,\n    ) -> Shareable:\n        \"\"\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n\n        current_round = shareable.get_header(AppConstants.CURRENT_ROUND)\n        total_rounds = shareable.get_header(AppConstants.NUM_ROUNDS)\n        self.log_info(fl_ctx, f\"Current/Total Round: {current_round + 1}/{total_rounds}\")\n        self.log_info(fl_ctx, f\"Client identity: {fl_ctx.get_identity_name()}\")\n\n        dxo = from_shareable(shareable)\n        global_weights = dxo.data\n\n        local_var_dict = self.model.state_dict()\n        model_keys = global_weights.keys()\n        for var_name in local_var_dict:\n            if var_name in model_keys:\n                weights = global_weights[var_name]\n                try:\n                    \n                    global_weights[var_name] = np.reshape(weights, local_var_dict[var_name].shape)\n                    \n                    local_var_dict[var_name] = torch.as_tensor(global_weights[var_name])\n                except Exception as e:\n                    raise ValueError(\"Convert weight from {} failed with error: {}\".format(var_name, str(e)))\n        self.model.load_state_dict(local_var_dict)\n\n        self.ditto_helper.load_model(local_var_dict)\n\n        epoch_len = len(self.train_loader)\n        self.log_info(fl_ctx, f\"Local steps per epoch: {epoch_len}\")\n\n        model_global = copy.deepcopy(self.model)\n        for param in model_global.parameters():\n            param.requires_grad = False\n\n        self.local_train(\n            fl_ctx=fl_ctx,\n            train_loader=self.train_loader,\n            model_global=model_global,\n            abort_signal=abort_signal,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n        self.epoch_of_start_time += self.aggregation_epochs\n\n        self.ditto_helper.local_train(\n            train_loader=self.train_loader,\n            model_global=model_global,\n            abort_signal=abort_signal,\n            writer=self.writer,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n\n        metric = self.local_valid(\n            self.ditto_helper.model,\n            self.valid_loader,\n            abort_signal,\n            tb_id=\"val_metric_per_model\",\n            record_epoch=self.ditto_helper.epoch_global,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n        self.log_info(fl_ctx, f\"val_metric_per_model: {metric:.4f}\")\n        \n        self.ditto_helper.update_metric_save_model(metric=metric)\n\n        local_weights = self.model.state_dict()\n        model_diff = {}\n        for name in global_weights:\n            if name not in local_weights:\n                continue\n            model_diff[name] = np.subtract(local_weights[name].cpu().numpy(), global_weights[name], dtype=np.float32)\n            if np.any(np.isnan(model_diff[name])):\n                self.system_panic(f\"{name} weights became NaN...\", fl_ctx)\n                return make_reply(ReturnCode.EXECUTION_EXCEPTION)\n\n        self.writer.flush()\n\n        dxo = DXO(data_kind=DataKind.WEIGHT_DIFF, data=model_diff)\n        dxo.set_meta_prop(MetaKey.NUM_STEPS_CURRENT_ROUND, epoch_len)\n\n        self.log_info(fl_ctx, \"Local epochs finished. Returning shareable\")\n        return dxo.to_shareable()",
  "\nimport os\nimport numpy as np\nfrom tensorflow.keras.utils import get_file\n\nfrom paz.abstract import SequentialProcessor, ProcessingSequence\nfrom paz.models.detection.utils import create_prior_boxes\nimport paz.processors as pr\nimport paz.backend as P\n\nIMAGE_URL = ('https://github.com/oarriaga/altamira-data/releases/download'\n             '/v0.9/object_detection_augmentation.png')\nfilename = os.path.basename(IMAGE_URL)\nimage_fullpath = get_file(filename, IMAGE_URL, cache_subdir='paz/tutorials')\n\nclass AugmentImage(SequentialProcessor):\n    def __init__(self):\n        super(AugmentImage, self).__init__()\n        self.add(pr.RandomContrast())\n        self.add(pr.RandomBrightness())\n        self.add(pr.RandomSaturation())\n        self.add(pr.RandomHue())\n\nclass PreprocessImage(SequentialProcessor):\n    def __init__(self, shape, mean=pr.BGR_IMAGENET_MEAN):\n        super(PreprocessImage, self).__init__()\n        self.add(pr.ResizeImage(shape))\n        self.add(pr.CastImage(float))\n        if mean is None:\n            self.add(pr.NormalizeImage())\n        else:\n            self.add(pr.SubtractMeanImage(mean))\n\npreprocess_image, augment_image = PreprocessImage((300, 300)), AugmentImage()\nprint('Image pre-processing examples')\nfor _ in range(10):\n    image = P.image.load_image(image_fullpath)\n    image = preprocess_image(augment_image(image))\n    P.image.show_image(image.astype('uint8'))\n\nH, W = P.image.load_image(image_fullpath).shape[:2]\nclass_names = ['background', 'human', 'horse']\nbox_data = np.array([[200 / W, 60 / H, 300 / W, 200 / H, 1],\n                     [100 / W, 90 / H, 400 / W, 300 / H, 2]])\n\nclass AugmentBoxes(SequentialProcessor):\n    def __init__(self, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentBoxes, self).__init__()\n        self.add(pr.ToImageBoxCoordinates())\n        self.add(pr.Expand(mean=mean))\n        self.add(pr.RandomSampleCrop())\n        self.add(pr.RandomFlipBoxesLeftRight())\n        self.add(pr.ToNormalizedBoxCoordinates())\n\ndraw_boxes = SequentialProcessor([\n    pr.ControlMap(pr.ToBoxes2D(class_names, False), [1], [1]),\n    pr.ControlMap(pr.DenormalizeBoxes2D(), [0, 1], [1], {0: 0}),\n    pr.DrawBoxes2D(class_names),\n    pr.ShowImage()])\n\naugment_boxes = AugmentBoxes()\nprint('Box augmentation examples')\nfor _ in range(10):\n    image = P.image.load_image(image_fullpath)\n    image, boxes = augment_boxes(image, box_data.copy())\n    draw_boxes(P.image.resize_image(image, (300, 300)), boxes)\n\nclass PreprocessBoxes(SequentialProcessor):\n    def __init__(self, num_classes, prior_boxes, IOU, variances):\n        super(PreprocessBoxes, self).__init__()\n        self.add(pr.MatchBoxes(prior_boxes, IOU),)\n        self.add(pr.EncodeBoxes(prior_boxes, variances))\n        self.add(pr.BoxClassToOneHotVector(num_classes))\n\nclass AugmentDetection(SequentialProcessor):\n    def __init__(self, prior_boxes, split=pr.TRAIN, num_classes=21, size=300,\n                 mean=pr.BGR_IMAGENET_MEAN, IOU=.5,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(AugmentDetection, self).__init__()\n\n        self.augment_image = AugmentImage()\n        self.augment_image.add(pr.ConvertColorSpace(pr.RGB2BGR))\n        self.preprocess_image = PreprocessImage((size, size), mean)\n\n        self.augment_boxes = AugmentBoxes()\n        args = (num_classes, prior_boxes, IOU, variances)\n        self.preprocess_boxes = PreprocessBoxes(*args)\n\n        self.add(pr.UnpackDictionary(['image', 'boxes']))\n        self.add(pr.ControlMap(pr.LoadImage(), [0], [0]))\n        if split == pr.TRAIN:\n            self.add(pr.ControlMap(self.augment_image, [0], [0]))\n            self.add(pr.ControlMap(self.augment_boxes, [0, 1], [0, 1]))\n        self.add(pr.ControlMap(self.preprocess_image, [0], [0]))\n        self.add(pr.ControlMap(self.preprocess_boxes, [1], [1]))\n        self.add(pr.SequenceWrapper(\n            {0: {'image': [size, size, 3]}},\n            {1: {'boxes': [len(prior_boxes), 4 + num_classes]}}))\n\nprior_boxes = create_prior_boxes()\ndraw_boxes.processors[0].processor.one_hot_encoded = True\ndraw_boxes.insert(0, pr.ControlMap(pr.DecodeBoxes(prior_boxes), [1], [1]))\ndraw_boxes.insert(2, pr.ControlMap(\n    pr.FilterClassBoxes2D(class_names[1:]), [1], [1]))\n\ndef deprocess_image(image):\n    image = (image + pr.BGR_IMAGENET_MEAN).astype('uint8')\n    return P.image.convert_color_space(image, pr.BGR2RGB)\n\naugmentator = AugmentDetection(prior_boxes, num_classes=len(class_names))\nprint('Image and boxes augmentations')\nfor _ in range(10):\n    sample = {'image': image_fullpath, 'boxes': box_data.copy()}\n    data = augmentator(sample)\n    image, boxes = data['inputs']['image'], data['labels']['boxes']\n    image = deprocess_image(image)\n    draw_boxes(image, boxes)\n\ndata = [{'image': image_fullpath, 'boxes': box_data}]\nprint('Image and boxes augmentations with generator')\nbatch_size = 1\nsequence = ProcessingSequence(augmentator, batch_size, data)\nfor _ in range(10):\n    batch = sequence.__getitem__(0)\n    batch_images, batch_boxes = batch[0]['image'], batch[1]['boxes']\n    image, boxes = batch_images[0], batch_boxes[0]\n    image = deprocess_image(image)\n    draw_boxes(image, boxes)",
  "\n\"\"\n\nimport numpy as np\nfrom pyscf.pbc import gto, scf, cc, df\nfrom pyscf.pbc.tools import pywannier90\n\ncell = gto.Cell()\ncell.atom = [['Si',[0.0,0.0,0.0]], ['Si',[1.35775, 1.35775, 1.35775]]]\ncell.a = [[0.0, 2.7155, 2.7155], [2.7155, 0.0, 2.7155], [2.7155, 2.7155, 0.0]]\ncell.basis = 'gth-dzv'\ncell.pseudo = 'gth-pade'\ncell.exp_to_discard = 0.1\ncell.build()\n\nkmesh = [3, 3, 3]\nkpts = cell.make_kpts(kmesh)\nnkpts = kpts.shape[0]\nkks = scf.KKS(cell, kpts)\nkks.xc = 'PBE'\nkks.chkfile = 'chkfile'\nkks.init_guess = 'chkfile'\nkks.run()\n\npywannier90.save_kmf(kks, 'chk_PBE')\nkmf = pywannier90.load_kmf('chk_PBE')\n\nnum_wann = 8\nkeywords = \\\n\"\"\nw90 = pywannier90.W90(kmf, cell, kmesh, num_wann, other_keywords=keywords)\nw90.kernel()    \n\nw90.plot_wf(supercell=kmesh, grid=[20,20,20])\n\nw90.export_AME()\nw90.kernel(external_AME='wannier90')\n\nband_kpts = kpts + 0.5 * kpts[1]\nfrac_kpts = cell.get_scaled_kpts(band_kpts)\ninterpolated_fock = w90.interpolate_ham_kpts(frac_kpts)     \nbands = w90.interpolate_band(frac_kpts)                     \nbands_ref = kks.get_bands(band_kpts)                        \n\nprint(\"Difference in the eigenvalues interpolated by scf.get_bands function and by pywannier90: %10.8e\" % \\\n(abs(bands[0] -bands_ref[0]).max()))\n\nimport mcu\npyscf_plot = mcu.PYSCF(cell)\nkpath = \"\"\nfrac_kpts, abs_kpts = pyscf_plot.make_kpts(kpath, 11)\nbands = w90.interpolate_band(frac_kpts, use_ws_distance=True)\npyscf_plot.set_kpts_bands([frac_kpts, bands])\npyscf_plot.get_bandgap()\npyscf_plot.plot_band(ylim=[-17,17], klabel=kpath)",
  "\n\"\"\n\nfrom expyfun._utils import _TempDir\nfrom expyfun import ExperimentController, analyze, building_doc\nfrom expyfun.stimuli import (crm_prepare_corpus, crm_sentence, crm_info,\n                             crm_response_menu, add_pad, CRMPreload)\n\nimport numpy as np\n\nprint(__doc__)\n\ncrm_path = _TempDir()\nfs = 40000\n\ncrm_prepare_corpus(fs, path_out=crm_path, overwrite=True,\n                   talker_list=[dict(sex=0, talker_num=0),\n                                dict(sex=1, talker_num=0)])\n\nprint('Valid callsigns are {0}'.format(crm_info()['callsign']))\n\nx1 = 0.5 * crm_sentence(fs, 'm', '0', 'c', 'r', '5', path=crm_path)\n\ncrm = CRMPreload(fs, path=crm_path)\nx2 = crm.sentence('f', '0', 'ringo', 'green', '6')\n\nx = add_pad([x1, x2], alignment='start')\n\nmax_wait = 0.01 if building_doc else 3\nwith ExperimentController(\n        exp_name='CRM corpus example', window_size=(720, 480),\n        full_screen=False, participant='foo', session='foo', version='dev',\n        output_dir=None, stim_fs=40000) as ec:\n    ec.screen_text('Report the color and number spoken by the female '\n                   'talker.', wrap=True)\n    screenshot = ec.screenshot()\n    ec.flip()\n    ec.wait_secs(max_wait)\n\n    ec.load_buffer(x)\n    ec.identify_trial(ec_id='', ttl_id=[])\n    ec.start_stimulus()\n    ec.wait_secs(x.shape[-1] / float(fs))\n\n    resp = crm_response_menu(ec, max_wait=0.01 if building_doc else np.inf)\n    if resp == ('g', '6'):\n        ec.screen_prompt('Correct!', max_wait=max_wait)\n    else:\n        ec.screen_prompt('Incorrect.', max_wait=max_wait)\n    ec.trial_ok()\n\nanalyze.plot_screen(screenshot)",
  "\n\"\"\n\nimport initExample\n\nfrom pyqtgraph.Qt import QtGui, QtCore\nimport pyqtgraph as pg\nimport numpy as np\nfrom collections import namedtuple\nfrom itertools import chain\n\napp = pg.mkQApp(\"Scatter Plot Item Example\") \nmw = QtGui.QMainWindow()\nmw.resize(800,800)\nview = pg.GraphicsLayoutWidget()  \nmw.setCentralWidget(view)\nmw.show()\nmw.setWindowTitle('pyqtgraph example: ScatterPlot')\n\nw1 = view.addPlot()\nw2 = view.addViewBox()\nw2.setAspectLocked(True)\nview.nextRow()\nw3 = view.addPlot()\nw4 = view.addPlot()\nprint(\"Generating data, this takes a few seconds...\")\n\nclickedPen = pg.mkPen('b', width=2)\nlastClicked = []\ndef clicked(plot, points):\n    global lastClicked\n    for p in lastClicked:\n        p.resetPen()\n    print(\"clicked points\", points)\n    for p in points:\n        p.setPen(clickedPen)\n    lastClicked = points\n\nn = 300\ns1 = pg.ScatterPlotItem(size=10, pen=pg.mkPen(None), brush=pg.mkBrush(255, 255, 255, 120))\npos = np.random.normal(size=(2,n), scale=1e-5)\nspots = [{'pos': pos[:,i], 'data': 1} for i in range(n)] + [{'pos': [0,0], 'data': 1}]\ns1.addPoints(spots)\nw1.addItem(s1)\ns1.sigClicked.connect(clicked)\n\nTextSymbol = namedtuple(\"TextSymbol\", \"label symbol scale\")\n\ndef createLabel(label, angle):\n    symbol = QtGui.QPainterPath()\n    \n    f = QtGui.QFont()\n    f.setPointSize(10)\n    symbol.addText(0, 0, f, label)\n    br = symbol.boundingRect()\n    scale = min(1. / br.width(), 1. / br.height())\n    tr = QtGui.QTransform()\n    tr.scale(scale, scale)\n    tr.rotate(angle)\n    tr.translate(-br.x() - br.width()/2., -br.y() - br.height()/2.)\n    return TextSymbol(label, tr.map(symbol), 0.1 / scale)\n\nrandom_str = lambda : (''.join([chr(np.random.randint(ord('A'),ord('z'))) for i in range(np.random.randint(1,5))]), np.random.randint(0, 360))\n\ns2 = pg.ScatterPlotItem(size=10, pen=pg.mkPen('w'), pxMode=True)\npos = np.random.normal(size=(2,n), scale=1e-5)\nspots = [{'pos': pos[:,i], 'data': 1, 'brush':pg.intColor(i, n), 'symbol': i%10, 'size': 5+i/10.} for i in range(n)]\ns2.addPoints(spots)\nspots = [{'pos': pos[:,i], 'data': 1, 'brush':pg.intColor(i, n), 'symbol': label[1], 'size': label[2]*(5+i/10.)} for (i, label) in [(i, createLabel(*random_str())) for i in range(n)]]\ns2.addPoints(spots)\nw2.addItem(s2)\ns2.sigClicked.connect(clicked)\n\ns3 = pg.ScatterPlotItem(\n    pxMode=False,  \n    hoverable=True,\n    hoverPen=pg.mkPen('g'),\n    hoverSize=1e-6\n)\nspots3 = []\nfor i in range(10):\n    for j in range(10):\n        spots3.append({'pos': (1e-6*i, 1e-6*j), 'size': 1e-6, 'pen': {'color': 'w', 'width': 2}, 'brush':pg.intColor(i*10+j, 100)})\ns3.addPoints(spots3)\nw3.addItem(s3)\ns3.sigClicked.connect(clicked)\n\ns4 = pg.ScatterPlotItem(\n    size=10,\n    pen=pg.mkPen(None),\n    brush=pg.mkBrush(255, 255, 255, 20),\n    hoverable=True,\n    hoverSymbol='s',\n    hoverSize=15,\n    hoverPen=pg.mkPen('r', width=2),\n    hoverBrush=pg.mkBrush('g'),\n)\nn = 10000\npos = np.random.normal(size=(2, n), scale=1e-9)\ns4.addPoints(\n    x=pos[0],\n    y=pos[1],\n    \n    data=np.arange(n)\n)\nw4.addItem(s4)\ns4.sigClicked.connect(clicked)\n\nif __name__ == '__main__':\n    pg.exec()",
  "\"\"\nfrom ansys.mapdl import core as pymapdl\n\nmapdl = pymapdl.launch_mapdl()\n\nmapdl.prep7()\n\nvnum0 = mapdl.block(0, 1, 0, 1, 0, 0.5)\n\nmapdl.et(1, 187)\nmapdl.esize(0.1)\n\nmapdl.vmesh(vnum0)\nmapdl.eplot()\n\nmapdl.esize(0.09)\nmapdl.et(2, 186)\nmapdl.type(2)\nvnum1 = mapdl.block(0, 1, 0, 1, 0.50001, 1)\nmapdl.vmesh(vnum1)\nmapdl.eplot()\n\nmapdl.nsel(\"s\", \"loc\", \"z\", 0.5, 0.50001)\nmapdl.esln(\"s\")\noutput = mapdl.gcgen(\"NEW\", splitkey=\"SPLIT\", selopt=\"SELECT\")\nprint(output)\n\nmapdl.esel(\"S\", \"SEC\", vmin=5, vmax=6)\nmapdl.eplot(style=\"wireframe\", line_width=3)\n\nmapdl.exit()",
  "\nfrom seedemu import *\n\nhosts_per_stub_as = 3\nemu = Makers.makeEmulatorBaseWith10StubASAndHosts(hosts_per_stub_as = hosts_per_stub_as)\n\neth = EthereumService()\n\nblockchain = eth.createBlockchain(chainName=\"pos\", consensus=ConsensusMechanism.POS)\n\nblockchain.setTerminalTotalDifficulty(30)\n\nasns = [150, 151, 152, 153, 154, 160, 161, 162, 163, 164]\n\ni = 1\nfor asn in asns:\n    for id in range(hosts_per_stub_as):        \n        \n        e:EthereumServer = blockchain.createNode(\"eth{}\".format(i))   \n        \n        e.appendClassName('Ethereum-POS-{}'.format(i))\n\n        e.enableGethHttp()\n\n        if asn == 150 and id == 0:\n                e.setBeaconSetupNode()\n\n        if asn == 150 and id == 1:\n                e.setBootNode(True)\n\n        if asn in [151]:\n            if id == 0:\n                e.enablePOSValidatorAtRunning()\n            if id == 1:\n                e.enablePOSValidatorAtRunning(is_manual=True)\n        \n        if asn in [152,153,154,160,161,162,163,164]:\n            e.enablePOSValidatorAtGenesis()\n            e.startMiner()\n\n        if e.isBeaconSetupNode():\n            emu.getVirtualNode('eth{}'.format(i)).setDisplayName('Ethereum-BeaconSetup')\n        else:\n            emu.getVirtualNode('eth{}'.format(i)).setDisplayName('Ethereum-POS-{}'.format(i))\n\n        emu.addBinding(Binding('eth{}'.format(i), filter=Filter(asn=asn, nodeName='host_{}'.format(id))))\n\n        i = i+1\n\nemu.addLayer(eth)\n\nemu.render()\n\ndocker = Docker(internetMapEnabled=True, etherViewEnabled=True)\n\nemu.compile(docker, './output', override = True)",
  "\nimport argparse\nimport json\n\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation\nfrom pipeline.component import HomoLR\nfrom pipeline.component import Reader\nfrom pipeline.component import FeatureScale\nfrom pipeline.interface import Data\nfrom pipeline.utils.tools import load_job_config\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    \n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n    arbiter = parties.arbiter[0]\n\n    guest_train_data = {\"name\": \"breast_homo_guest\", \"namespace\": f\"experiment{namespace}\"}\n    host_train_data = {\"name\": \"breast_homo_host\", \"namespace\": f\"experiment{namespace}\"}\n\n    pipeline = PipeLine()\n    \n    pipeline.set_initiator(role='guest', party_id=guest)\n    \n    pipeline.set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    reader_0 = Reader(name=\"reader_0\")\n    \n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    \n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    data_transform_0 = DataTransform(\n        name=\"data_transform_0\",\n        with_label=True,\n        output_format=\"dense\")  \n\n    scale_0 = FeatureScale(name='scale_0')\n    param = {\n        \"penalty\": \"L2\",\n        \"optimizer\": \"sgd\",\n        \"tol\": 1e-05,\n        \"alpha\": 0.01,\n        \"max_iter\": 30,\n        \"early_stop\": \"diff\",\n        \"batch_size\": -1,\n        \"learning_rate\": 0.15,\n        \"decay\": 1,\n        \"decay_sqrt\": True,\n        \"init_param\": {\n            \"init_method\": \"zeros\"\n        },\n        \"cv_param\": {\n            \"n_splits\": 4,\n            \"shuffle\": True,\n            \"random_seed\": 33,\n            \"need_cv\": False\n        }\n    }\n\n    homo_lr_0 = HomoLR(name='homo_lr_0', **param)\n\n    pipeline.add_component(reader_0)\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    \n    pipeline.add_component(scale_0, data=Data(data=data_transform_0.output.data))\n    pipeline.add_component(homo_lr_0, data=Data(train_data=scale_0.output.data))\n    evaluation_0 = Evaluation(name=\"evaluation_0\", eval_type=\"binary\")\n    evaluation_0.get_party_instance(role='host', party_id=host).component_param(need_run=False)\n    pipeline.add_component(evaluation_0, data=Data(data=homo_lr_0.output.data))\n\n    pipeline.compile()\n\n    pipeline.fit()\n\n    deploy_components = [data_transform_0, scale_0, homo_lr_0]\n    pipeline.deploy_component(components=deploy_components)\n    \n    predict_pipeline = PipeLine()\n    \n    predict_pipeline.add_component(reader_0)\n    \n    predict_pipeline.add_component(\n        pipeline, data=Data(\n            predict_input={\n                pipeline.data_transform_0.input.data: reader_0.output.data}))\n    predict_pipeline.compile()\n    predict_pipeline.predict()\n\n    dsl_json = predict_pipeline.get_predict_dsl()\n    conf_json = predict_pipeline.get_predict_conf()\n    \n    json.dump(dsl_json, open('./homo-lr-normal-predict-dsl.json', 'w'), indent=4)\n    json.dump(conf_json, open('./homo-lr-normal-predict-conf.json', 'w'), indent=4)\n\n    print(json.dumps(pipeline.get_component(\"homo_lr_0\").get_summary(), indent=4, ensure_ascii=False))\n    print(json.dumps(pipeline.get_component(\"evaluation_0\").get_summary(), indent=4, ensure_ascii=False))\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()",
  "\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nfrom DREAM import runiface\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.ElectricField as Efield\nimport DREAM.Settings.Equations.HotElectronDistribution as FHot\nimport DREAM.Settings.Equations.ColdElectronTemperature as T_cold\n\nfrom DREAM.Settings.Equations.ElectricField import ElectricField\nfrom DREAM.Settings.Equations.ColdElectronTemperature import ColdElectronTemperature\n\nds = DREAMSettings()\n\nds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_FULL\nds.collisions.collfreq_type = Collisions.COLLFREQ_TYPE_PARTIALLY_SCREENED\n\nds.collisions.bremsstrahlung_mode = Collisions.BREMSSTRAHLUNG_MODE_STOPPING_POWER\n\nds.collisions.lnlambda = Collisions.LNLAMBDA_ENERGY_DEPENDENT\n\nTmax_restart2 = 1e-4\nNt_restart2 = 50\n\nTmax_restart = 1e-5 \nNt_restart = 20     \n\nn_D = 1e20\nn_Z = 0.3e20\n\nB0 = 5.3            \nE_initial = 0.00032 \nE_wall = 0.0        \nT_initial = 25e3    \n\nTmax_init = 1e-11   \nNt_init = 2         \nNr = 5              \nNp = 200            \nNxi = 5             \npMax = 2.0          \ntimes  = [0]        \nradius = [0, 2]     \nradius_wall = 2.15  \n\nT_selfconsistent    = True\nhotTailGrid_enabled = True\n\nds.radialgrid.setB0(B0)\nds.radialgrid.setMinorRadius(radius[-1])\nds.radialgrid.setWallRadius(radius_wall)\nds.radialgrid.setNr(Nr)\n\nds.timestep.setTmax(Tmax_init)\nds.timestep.setNt(Nt_init)\n\ndensity_D = n_D\ndensity_Z = n_Z\n\nds.eqsys.n_i.addIon(name='D', T=T_initial, Z=1, iontype=Ions.IONS_DYNAMIC_FULLY_IONIZED, n=density_D)\nds.eqsys.n_i.addIon(name='Ar', Z=18, iontype=Ions.IONS_DYNAMIC_NEUTRAL, n=density_Z)\n\nefield = E_initial*np.ones((len(times), len(radius)))\nds.eqsys.E_field.setPrescribedData(efield=efield, times=times, radius=radius)\n\ntemperature = T_initial * np.ones((len(times), len(radius)))\nds.eqsys.T_cold.setPrescribedData(temperature=temperature, times=times, radius=radius)\n\nif not hotTailGrid_enabled:\n    ds.hottailgrid.setEnabled(False)\nelse:\n    ds.hottailgrid.setNxi(Nxi)\n    ds.hottailgrid.setNp(Np)\n    ds.hottailgrid.setPmax(pMax)\n    nfree_initial, rn0 = ds.eqsys.n_i.getFreeElectronDensity()\n    ds.eqsys.f_hot.setBoundaryCondition(bc=FHot.BC_F_0)\n    ds.eqsys.f_hot.setInitialProfiles(rn0=rn0, n0=nfree_initial, rT0=0, T0=T_initial)\n    ds.eqsys.f_hot.setAdvectionInterpolationMethod(ad_int=FHot.AD_INTERP_TCDF,ad_jac=FHot.AD_INTERP_JACOBIAN_FULL) \n    ds.eqsys.f_hot.enableIonJacobian(False)\n\nds.runawaygrid.setEnabled(False)\n\nds.solver.setType(Solver.NONLINEAR)\nds.solver.setTolerance(reltol=1e-4)\nds.solver.setMaxIterations(maxiter = 100)\nds.solver.setVerbose(False)\n\nds.other.include('fluid', 'scalar')\n\nds.save('init_settings.h5')\nruniface(ds, 'output_init.h5', quiet=False)\n\nds2 = DREAMSettings(ds)\n\nds2.fromOutput('output_init.h5')\n\nds2.eqsys.E_field.setType(Efield.TYPE_SELFCONSISTENT)\nds2.eqsys.E_field.setBoundaryCondition(bctype = Efield.BC_TYPE_PRESCRIBED, inverse_wall_time = 0, V_loop_wall_R0 = E_wall*2*np.pi)\nif T_selfconsistent:\n    ds2.eqsys.T_cold.setType(ttype=T_cold.TYPE_SELFCONSISTENT)\n    ds.eqsys.T_cold.setRecombinationRadiation(False)\n\nds2.timestep.setTmax(Tmax_restart)\nds2.timestep.setNt(Nt_restart)\n\nds2.save('restart_settings.h5')\nruniface(ds2, 'output_restart.h5', quiet=False)\n\nds3 = DREAMSettings(ds2)\nds3.fromOutput('output_restart.h5')\n\nds3.timestep.setTmax(Tmax_restart2)\nds3.timestep.setNt(Nt_restart2)\n\nds3.save('second_restart_settings.h5')\nruniface(ds3, 'output.h5', quiet=False)",
  "import sys\n\nif \"pyodide\" in sys.modules:\n    \n    from fakepsutil import cpu_count, cpu_percent\nelse:\n    from psutil import cpu_count, cpu_percent\n\nfrom math import ceil\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom shiny import App, Inputs, Outputs, Session, reactive, render, ui\n\nmatplotlib.use(\"agg\")\n\nMAX_SAMPLES = 1000\n\nSAMPLE_PERIOD = 1\n\nncpu = cpu_count(logical=True)\n\napp_ui = ui.page_fluid(\n    ui.tags.style(\n        \"\"\n        % f\"{ncpu*4}em\"\n    ),\n    ui.h3(\"CPU Usage %\", class_=\"mt-2\"),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_select(\n                \"cmap\",\n                \"Colormap\",\n                {\n                    \"inferno\": \"inferno\",\n                    \"viridis\": \"viridis\",\n                    \"copper\": \"copper\",\n                    \"prism\": \"prism (not recommended)\",\n                },\n            ),\n            ui.p(ui.input_action_button(\"reset\", \"Clear history\", class_=\"btn-sm\")),\n            ui.input_switch(\"hold\", \"Freeze output\", value=False),\n            class_=\"mb-3\",\n        ),\n        ui.panel_main(\n            ui.div(\n                {\"class\": \"card mb-3\"},\n                ui.div(\n                    {\"class\": \"card-body\"},\n                    ui.h5({\"class\": \"card-title mt-0\"}, \"Graphs\"),\n                    ui.output_plot(\"plot\", height=f\"{ncpu * 40}px\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-footer\"},\n                    ui.input_numeric(\"sample_count\", \"Number of samples per graph\", 50),\n                ),\n            ),\n            ui.div(\n                {\"class\": \"card\"},\n                ui.div(\n                    {\"class\": \"card-body\"},\n                    ui.h5({\"class\": \"card-title m-0\"}, \"Heatmap\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-body overflow-auto pt-0\"},\n                    ui.output_table(\"table\"),\n                ),\n                ui.div(\n                    {\"class\": \"card-footer\"},\n                    ui.input_numeric(\"table_rows\", \"Rows to display\", 5),\n                ),\n            ),\n        ),\n    ),\n)\n\n@reactive.Calc\ndef cpu_current():\n    reactive.invalidate_later(SAMPLE_PERIOD)\n    return cpu_percent(percpu=True)\n\ndef server(input: Inputs, output: Outputs, session: Session):\n    cpu_history = reactive.Value(None)\n\n    @reactive.Calc\n    def cpu_history_with_hold():\n        \n        if not input.hold():\n            return cpu_history()\n        else:\n            \n            input.reset()\n            with reactive.isolate():\n                return cpu_history()\n\n    @reactive.Effect\n    def collect_cpu_samples():\n        \"\"\n\n        new_data = np.vstack(cpu_current())\n        with reactive.isolate():\n            if cpu_history() is None:\n                cpu_history.set(new_data)\n            else:\n                combined_data = np.hstack([cpu_history(), new_data])\n                \n                if combined_data.shape[1] > MAX_SAMPLES:\n                    combined_data = combined_data[:, -MAX_SAMPLES:]\n                cpu_history.set(combined_data)\n\n    @reactive.Effect(priority=100)\n    @reactive.event(input.reset)\n    def reset_history():\n        cpu_history.set(None)\n\n    @output\n    @render.plot\n    def plot():\n        history = cpu_history_with_hold()\n\n        if history is None:\n            history = np.array([])\n            history.shape = (ncpu, 0)\n\n        nsamples = input.sample_count()\n\n        if history.shape[1] > nsamples:\n            history = history[:, -nsamples:]\n\n        ncols = 2\n        nrows = int(ceil(ncpu / ncols))\n        fig, axeses = plt.subplots(\n            nrows=nrows,\n            ncols=ncols,\n            squeeze=False,\n        )\n        for i in range(0, ncols * nrows):\n            row = i // ncols\n            col = i % ncols\n            axes = axeses[row, col]\n            if i >= len(history):\n                axes.set_visible(False)\n                continue\n            data = history[i]\n            axes.yaxis.set_label_position(\"right\")\n            axes.yaxis.tick_right()\n            axes.set_xlim(-(nsamples - 1), 0)\n            axes.set_ylim(0, 100)\n\n            assert len(data) <= nsamples\n\n            x = np.arange(0, len(data))\n            x = np.flip(-x)\n\n            color = plt.get_cmap(input.cmap())(data / 100)\n            axes.bar(x, data, color=color, linewidth=0, width=1.0)\n\n            axes.set_yticks([25, 50, 75])\n            for ytl in axes.get_yticklabels():\n                if col == ncols - 1 or i == ncpu - 1 or True:\n                    ytl.set_fontsize(7)\n                else:\n                    ytl.set_visible(False)\n                    hide_ticks(axes.yaxis)\n            for xtl in axes.get_xticklabels():\n                xtl.set_visible(False)\n            hide_ticks(axes.xaxis)\n            axes.grid(True, linewidth=0.25)\n\n        return fig\n\n    @output\n    @render.table\n    def table():\n        history = cpu_history_with_hold()\n        latest = pd.DataFrame(history).transpose().tail(input.table_rows())\n        if latest.shape[0] == 0:\n            return latest\n        return (\n            latest.style.format(precision=0)\n            .hide(axis=\"index\")\n            .set_table_attributes(\n                'class=\"dataframe shiny-table table table-borderless font-monospace\"'\n            )\n            .background_gradient(cmap=input.cmap(), vmin=0, vmax=100)\n        )\n\ndef hide_ticks(axis):\n    for ticks in [axis.get_major_ticks(), axis.get_minor_ticks()]:\n        for tick in ticks:\n            tick.tick1line.set_visible(False)\n            tick.tick2line.set_visible(False)\n            tick.label1.set_visible(False)\n            tick.label2.set_visible(False)\n\napp = App(app_ui, server)",
  "\"\"\n\nimport os\nimport pyaedt\n\nnon_graphical = False\n\nexample_path = pyaedt.downloads.download_3dcomponent()\n\nproject_name = pyaedt.generate_unique_project_name(project_name=\"array\")\nhfss = pyaedt.Hfss(projectname=project_name,\n                   specified_version=\"2023.2\",\n                   designname=\"Array_Simple\",\n                   non_graphical=non_graphical,\n                   new_desktop_session=True)\n\nprint(\"Project name \" + project_name)\n\ndict_in = pyaedt.data_handler.json_to_dict(os.path.join(example_path, \"array_simple.json\"))\ndict_in[\"Circ_Patch_5GHz1\"] = os.path.join(example_path, \"Circ_Patch_5GHz.a3dcomp\")\ndict_in[\"cells\"][(3, 3)] = {\"name\": \"Circ_Patch_5GHz1\"}\nhfss.add_3d_component_array_from_json(dict_in)\n\nsetup = hfss.create_setup()\nsetup.props[\"Frequency\"] = \"5GHz\"\nsetup.props[\"MaximumPasses\"] = 3\n\nhfss.analyze(num_cores=4)\n\nffdata = hfss.get_antenna_ffd_solution_data(sphere_name=\"Infinite Sphere1\", setup_name=hfss.nominal_adaptive,\n                                            frequencies=[5e9])\n\nffdata.plot_farfield_contour(qty_str='RealizedGain', convert_to_db=True,\n                             title='Contour at {}Hz'.format(ffdata.frequency))\n\nffdata.plot_2d_cut(primary_sweep='theta', secondary_sweep_value=[-180, -75, 75],\n                   qty_str='RealizedGain',\n                   title='Azimuth at {}Hz'.format(ffdata.frequency),\n                   convert_to_db=True)\n\nffdata.plot_2d_cut(primary_sweep=\"phi\", secondary_sweep_value=30,\n                   qty_str='RealizedGain',\n                   title='Elevation',\n                   convert_to_db=True)\n\nffdata.polar_plot_3d(qty_str='RealizedGain',\n                     convert_to_db=True)\n\nffdata.polar_plot_3d_pyvista(qty_str='RealizedGain',\n                             convert_to_db=True,\n                             export_image_path=os.path.join(hfss.working_directory, \"picture.jpg\"),\n                             show=False)\n\nhfss.release_desktop()",
  "from sc2 import maps\nfrom sc2.bot_ai import BotAI\nfrom sc2.data import Difficulty, Race\nfrom sc2.ids.ability_id import AbilityId\nfrom sc2.ids.buff_id import BuffId\nfrom sc2.ids.unit_typeid import UnitTypeId\nfrom sc2.main import run_game\nfrom sc2.player import Bot, Computer\n\nclass ThreebaseVoidrayBot(BotAI):\n\n    async def on_step(self, iteration):\n        target_base_count = 3\n        target_stargate_count = 3\n\n        if iteration == 0:\n            await self.chat_send(\"(glhf)\")\n\n        if not self.townhalls.ready:\n            \n            for worker in self.workers:\n                worker.attack(self.enemy_start_locations[0])\n            return\n\n        nexus = self.townhalls.ready.random\n\n        if not nexus.is_idle and not nexus.has_buff(BuffId.CHRONOBOOSTENERGYCOST):\n            nexuses = self.structures(UnitTypeId.NEXUS)\n            abilities = await self.get_available_abilities(nexuses)\n            for loop_nexus, abilities_nexus in zip(nexuses, abilities):\n                if AbilityId.EFFECT_CHRONOBOOSTENERGYCOST in abilities_nexus:\n                    loop_nexus(AbilityId.EFFECT_CHRONOBOOSTENERGYCOST, nexus)\n                    break\n\n        if self.units(UnitTypeId.VOIDRAY).amount > 5:\n            for vr in self.units(UnitTypeId.VOIDRAY):\n                \n                if vr.weapon_cooldown > 0:\n                    vr(AbilityId.EFFECT_VOIDRAYPRISMATICALIGNMENT)\n                \n                targets = (self.enemy_units | self.enemy_structures).filter(lambda unit: unit.can_be_attacked)\n                if targets:\n                    target = targets.closest_to(vr)\n                    vr.attack(target)\n                else:\n                    vr.attack(self.enemy_start_locations[0])\n\n        await self.distribute_workers()\n\n        if (\n            self.supply_left < 2 and self.already_pending(UnitTypeId.PYLON) == 0\n            or self.supply_used > 15 and self.supply_left < 4 and self.already_pending(UnitTypeId.PYLON) < 2\n        ):\n            \n            if self.can_afford(UnitTypeId.PYLON):\n                await self.build(UnitTypeId.PYLON, near=nexus)\n\n        if self.supply_workers + self.already_pending(UnitTypeId.PROBE) < self.townhalls.amount * 22 and nexus.is_idle:\n            if self.can_afford(UnitTypeId.PROBE):\n                nexus.train(UnitTypeId.PROBE)\n\n        if self.townhalls.ready.amount + self.already_pending(UnitTypeId.NEXUS) < 3:\n            if self.can_afford(UnitTypeId.NEXUS):\n                await self.expand_now()\n\n        if self.structures(UnitTypeId.PYLON).ready:\n            pylon = self.structures(UnitTypeId.PYLON).ready.random\n            if self.structures(UnitTypeId.GATEWAY).ready:\n                \n                if not self.structures(UnitTypeId.CYBERNETICSCORE):\n                    if (\n                        self.can_afford(UnitTypeId.CYBERNETICSCORE)\n                        and self.already_pending(UnitTypeId.CYBERNETICSCORE) == 0\n                    ):\n                        await self.build(UnitTypeId.CYBERNETICSCORE, near=pylon)\n            else:\n                \n                if self.can_afford(UnitTypeId.GATEWAY) and self.already_pending(UnitTypeId.GATEWAY) == 0:\n                    await self.build(UnitTypeId.GATEWAY, near=pylon)\n\n        if self.structures(UnitTypeId.CYBERNETICSCORE):\n            for nexus in self.townhalls.ready:\n                vgs = self.vespene_geyser.closer_than(15, nexus)\n                for vg in vgs:\n                    if not self.can_afford(UnitTypeId.ASSIMILATOR):\n                        break\n\n                    worker = self.select_build_worker(vg.position)\n                    if worker is None:\n                        break\n\n                    if not self.gas_buildings or not self.gas_buildings.closer_than(1, vg):\n                        worker.build_gas(vg)\n                        worker.stop(queue=True)\n\n        if self.structures(UnitTypeId.PYLON).ready and self.structures(UnitTypeId.CYBERNETICSCORE).ready:\n            pylon = self.structures(UnitTypeId.PYLON).ready.random\n            if (\n                self.townhalls.ready.amount + self.already_pending(UnitTypeId.NEXUS) >= target_base_count\n                and self.structures(UnitTypeId.STARGATE).ready.amount + self.already_pending(UnitTypeId.STARGATE) <\n                target_stargate_count\n            ):\n                if self.can_afford(UnitTypeId.STARGATE):\n                    await self.build(UnitTypeId.STARGATE, near=pylon)\n\n        if self.townhalls.amount >= 3:\n            for sg in self.structures(UnitTypeId.STARGATE).ready.idle:\n                if self.can_afford(UnitTypeId.VOIDRAY):\n                    sg.train(UnitTypeId.VOIDRAY)\n\ndef main():\n    run_game(\n        maps.get(\"(2)CatalystLE\"),\n        [Bot(Race.Protoss, ThreebaseVoidrayBot()),\n         Computer(Race.Protoss, Difficulty.Easy)],\n        realtime=False,\n    )\n\nif __name__ == \"__main__\":\n    main()",
  "\"\"\n\nimport os\nimport tempfile\nimport pyaedt\n\ntemp_dir = tempfile.gettempdir()\ndst_dir = os.path.join(temp_dir, pyaedt.generate_unique_name(\"pyaedt_dcir\"))\nos.mkdir(dst_dir)\nlocal_path = pyaedt.downloads.download_aedb(dst_dir)\n\nedbversion = \"2023.2\"\nappedb = pyaedt.Edb(local_path, edbversion=edbversion)\n\ngnd_name = \"GND\"\nappedb.siwave.create_pin_group_on_net(\n    reference_designator=\"U3A1\",\n    net_name=\"BST_V3P3_S5\",\n    group_name=\"U3A1-BST_V3P3_S5\")\n\nappedb.siwave.create_pin_group_on_net(\n    reference_designator=\"U3A1\",\n    net_name=\"GND\",\n    group_name=\"U3A1-GND\")\n\nappedb.siwave.create_voltage_source_on_pin_group(\n    pos_pin_group_name=\"U3A1-BST_V3P3_S5\",\n    neg_pin_group_name=\"U3A1-GND\",\n    magnitude=3.3,\n    name=\"U3A1-BST_V3P3_S5\"\n)\n\nappedb.siwave.create_pin_group_on_net(\n    reference_designator=\"U2A5\",\n    net_name=\"V3P3_S5\",\n    group_name=\"U2A5-V3P3_S5\")\n\nappedb.siwave.create_pin_group_on_net(\n    reference_designator=\"U2A5\",\n    net_name=\"GND\",\n    group_name=\"U2A5-GND\")\n\nappedb.siwave.create_current_source_on_pin_group(\n    pos_pin_group_name=\"U2A5-V3P3_S5\",\n    neg_pin_group_name=\"U2A5-GND\",\n    magnitude=1,\n    name=\"U2A5-V3P3_S5\"\n)\n\nappedb.siwave.add_siwave_dc_analysis(name=\"my_setup\")\n\nappedb.save_edb()\nappedb.close_edb()\n\ndesktop = pyaedt.Desktop(edbversion, non_graphical=False, new_desktop_session=True)\nhfss3dl = pyaedt.Hfss3dLayout(local_path)\nhfss3dl.analyze()\nhfss3dl.save_project()\n\nloop_resistance = hfss3dl.get_dcir_element_data_loop_resistance(setup_name=\"my_setup\")\nprint(loop_resistance)\n\ncurrent_source = hfss3dl.get_dcir_element_data_current_source(setup_name=\"my_setup\")\nprint(current_source)\n\nvia = hfss3dl.get_dcir_element_data_via(setup_name=\"my_setup\")\nprint(via)\n\nvoltage = hfss3dl.get_dcir_solution_data(\n    setup_name=\"my_setup\",\n    show=\"Sources\",\n    category=\"Voltage\")\nprint({expression: voltage.data_magnitude(expression) for  expression in voltage.expressions})\n\nhfss3dl.close_project()\ndesktop.release_desktop()",
  "\"\"\nfrom avalanche.benchmarks.utils import DataAttribute, ConstantSequence\nfrom avalanche.training.plugins import ReplayPlugin\nfrom transformers import DataCollatorForSeq2Seq\nimport torch\nimport avalanche\nimport torch.nn\nfrom avalanche.benchmarks import CLScenario, CLStream, CLExperience\nimport avalanche.training.templates.base\nfrom avalanche.benchmarks.utils import AvalancheDataset\nfrom transformers import AutoTokenizer\nfrom transformers import T5ForConditionalGeneration\nfrom datasets import load_dataset\nimport numpy as np\n\nclass HGNaive(avalanche.training.Naive):\n    \"\"\n\n    @property\n    def mb_attention_mask(self):\n        return self.mbatch[\"attention_mask\"]\n\n    @property\n    def mb_x(self):\n        \"\"\n        return self.mbatch[\"input_ids\"]\n\n    @property\n    def mb_y(self):\n        \"\"\n        return self.mbatch[\"labels\"]\n\n    @property\n    def mb_decoder_in_ids(self):\n        \"\"\n        return self.mbatch[\"decoder_input_ids\"]\n\n    @property\n    def mb_token_type_ids(self):\n        return self.mbatch[3]\n\n    def _unpack_minibatch(self):\n        \"\"\n        for k in self.mbatch.keys():\n            self.mbatch[k] = self.mbatch[k].to(self.device)\n\n    def forward(self):\n        out = self.model(\n            input_ids=self.mb_x,\n            attention_mask=self.mb_attention_mask,\n            labels=self.mb_y,\n        )\n        return out.logits\n\n    def criterion(self):\n        mb_output = self.mb_output.view(-1, self.mb_output.size(-1))\n        ll = self._criterion(mb_output, self.mb_y.view(-1))\n        return ll\n\ndef main():\n    \n    squad_tr = load_dataset(\"squad\", split=\"train\")\n    squad_val = load_dataset(\"squad\", split=\"validation\")\n\n    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n    encoder_max_len = tokenizer.model_max_length\n    decoder_max_len = 60\n\n    \"\"\n\n    def t2t_converter(example):\n        example[\"input_text\"] = f\"question: {example['question']}\"\n        +f\"context: {example['context']} </s>\"\n        example[\"target_text\"] = f\"{example['answers']['text'][0]} </s>\"\n        return example\n\n    def preprocess_function(\n        examples, encoder_max_len=encoder_max_len, decoder_max_len=decoder_max_len\n    ):\n        encoder_inputs = tokenizer(\n            examples[\"input_text\"],\n            truncation=True,\n            return_tensors=\"pt\",\n            max_length=encoder_max_len,\n            pad_to_max_length=True,\n        )\n\n        decoder_inputs = tokenizer(\n            examples[\"target_text\"],\n            truncation=True,\n            return_tensors=\"pt\",\n            max_length=decoder_max_len,\n            pad_to_max_length=True,\n        )\n\n        input_ids = encoder_inputs[\"input_ids\"]\n        input_attention = encoder_inputs[\"attention_mask\"]\n        target_ids = decoder_inputs[\"input_ids\"]\n        target_attention = decoder_inputs[\"attention_mask\"]\n\n        outputs = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": input_attention,\n            \"labels\": target_ids,\n            \"decoder_attention_mask\": target_attention,\n        }\n        return outputs\n\n    squad_tr = squad_tr.map(t2t_converter)\n    squad_tr = squad_tr.map(preprocess_function, batched=True)\n    squad_tr = squad_tr.remove_columns(\n        [\"id\", \"title\", \"context\", \"question\", \"answers\", \"input_text\", \"target_text\"]\n    )\n    squad_val = squad_val.map(t2t_converter)\n    squad_val = squad_val.map(preprocess_function, batched=True)\n    \n    squad_val = squad_val.remove_columns(\n        [\"id\", \"title\", \"context\", \"question\", \"answers\"]\n    )\n\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n    \n    data_collator = DataCollatorForSeq2Seq(tokenizer)\n\n    train_exps = []\n    for i in range(0, 2):\n        \n        exp_data = squad_tr.select(range(30 * i, 30 * (i + 1)))\n        tl = DataAttribute(ConstantSequence(i, len(exp_data)), \"targets_task_labels\")\n\n        exp = CLExperience()\n        exp.dataset = AvalancheDataset(\n            [exp_data], data_attributes=[tl], collate_fn=data_collator\n        )\n        train_exps.append(exp)\n    tl = DataAttribute(ConstantSequence(2, len(squad_val)), \"targets_task_labels\")\n    val_exp = CLExperience()\n    val_exp.dataset = AvalancheDataset(\n        [squad_val], data_attributes=[tl], collate_fn=data_collator\n    )\n    val_exp = [val_exp]\n\n    benchmark = CLScenario(\n        [\n            CLStream(\"train\", train_exps),\n            CLStream(\"valid\", val_exp)\n            \n        ]\n    )\n    eval_plugin = avalanche.training.plugins.EvaluationPlugin(\n        avalanche.evaluation.metrics.loss_metrics(\n            epoch=True, experience=True, stream=True\n        ),\n        loggers=[avalanche.logging.InteractiveLogger()],\n        strict_checks=False,\n    )\n    plugins = [ReplayPlugin(mem_size=200)]\n    optimizer = torch.optim.Adam(model.parameters(), lr=2)\n    strategy = HGNaive(\n        model,\n        optimizer,\n        torch.nn.CrossEntropyLoss(ignore_index=-100),\n        evaluator=eval_plugin,\n        train_epochs=1,\n        train_mb_size=10,\n        plugins=plugins,\n    )\n    for experience in benchmark.train_stream:\n        strategy.train(experience, collate_fn=data_collator)\n        strategy.eval(benchmark.train_stream)\n\n    model.eval()\n    question = \"Which libraries is Avalanche based upon?\"\n    context = \"\"\n\n    input_text = f\"answer_me: {question} context: {context} </s>\"\n    encoded_query = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        pad_to_max_length=True,\n        truncation=True,\n        max_length=250,\n    )\n    input_ids = encoded_query[\"input_ids\"]\n    attention_mask = encoded_query[\"attention_mask\"]\n    generated_answer = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        max_length=50,\n        top_p=0.95,\n        top_k=50,\n        repetition_penalty=2.0,\n    )\n\n    decoded_answer = tokenizer.batch_decode(generated_answer, skip_special_tokens=True)\n    print(f\"Answer: {decoded_answer}\")\n\nif __name__ == \"__main__\":\n    main()",
  "import numpy as np\nimport scipy.sparse as sps\nimport os\nimport sys\n\nfrom porepy.viz import exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import dual\nfrom porepy.numerics.fv.transport import upwind\nfrom porepy.numerics.fv import tpfa, mass_matrix\n\ndef add_data_darcy(gb, domain, tol):\n    gb.add_node_props(['param', 'is_tangent'])\n\n    apert = 1e-2\n\n    km = 7.5 * 1e-11\n    kf_t = 1e-5 * km\n    kf_n = 1e-5 * km\n\n    for g, d in gb:\n        param = Parameters(g)\n\n        rock = g.dim == gb.dim_max()\n        kxx = km if rock else kf_t\n        d['is_tangential'] = True\n        perm = tensor.SecondOrder(g.dim, kxx * np.ones(g.num_cells))\n        param.set_tensor(\"flow\", perm)\n\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        param.set_aperture(np.power(apert, gb.dim_max() - g.dim))\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            top = bound_face_centers[1, :] > domain['ymax'] - tol\n            bottom = bound_face_centers[1, :] < domain['ymin'] + tol\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n            boundary = np.logical_or(left, right)\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[boundary] = ['dir']\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = 30 * 1e6\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        g = gb.sorted_nodes_of_edge(e)[0]\n        d['kn'] = kf_n / gb.node_prop(g, 'param').get_aperture()\n\ndef add_data_advection(gb, domain, tol):\n\n    phi_m = 1e-1\n    phi_f = 9 * 1e-1\n\n    rho_w = 1e3  \n    rho_s = 2 * 1e3  \n\n    c_w = 4 * 1e3  \n    c_s = 8 * 1e2  \n\n    c_m = phi_m * rho_w * c_w + (1 - phi_m) * rho_s * c_s\n    c_f = phi_f * rho_w * c_w + (1 - phi_f) * rho_s * c_s\n\n    for g, d in gb:\n        param = d['param']\n\n        rock = g.dim == gb.dim_max()\n        source = np.zeros(g.num_cells)\n        param.set_source(\"transport\", source)\n\n        param.set_porosity(1)\n        param.set_discharge(d['discharge'])\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            top = bound_face_centers[1, :] > domain['ymax'] - tol\n            bottom = bound_face_centers[1, :] < domain['ymin'] + tol\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n            boundary = np.logical_or(left, right)\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[boundary] = ['dir']\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = 1\n\n            param.set_bc(\"transport\", BoundaryCondition(\n                g, bound_faces, labels))\n            param.set_bc_val(\"transport\", bc_val)\n        else:\n            param.set_bc(\"transport\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n        d['param'] = param\n\n    gb.add_edge_prop('param')\n    for e, d in gb.edges_props():\n        g = gb.sorted_nodes_of_edge(e)[1]\n        discharge = gb.node_prop(g, 'param').get_discharge()\n        d['param'] = Parameters(g)\n        d['param'].set_discharge(discharge)\n\ntol = 1e-4\nexport_folder = 'example_5_2_2'\n\nT = 40 * np.pi * 1e7\nNt = 20  \ndeltaT = T / Nt\nexport_every = 1\nif_coarse = True\n\nmesh_kwargs = {}\nmesh_kwargs['mesh_size'] = {'mode': 'weighted',\n                            'value': 500,\n                            'bound_value': 500,\n                            'tol': tol}\n\ndomain = {'xmin': 0, 'xmax': 700, 'ymin': 0, 'ymax': 600}\ngb = importer.from_csv('network.csv', mesh_kwargs, domain)\ngb.compute_geometry()\nif if_coarse:\n    co.coarsen(gb, 'by_volume')\ngb.assign_node_ordering()\n\ndarcy = dual.DualVEMMixDim(\"flow\")\n\nadd_data_darcy(gb, domain, tol)\n\nA, b = darcy.matrix_rhs(gb)\n\nup = sps.linalg.spsolve(A, b)\ndarcy.split(gb, \"up\", up)\n\ngb.add_node_props(['pressure', \"P0u\", \"discharge\"])\ndarcy.extract_u(gb, \"up\", \"discharge\")\ndarcy.extract_p(gb, \"up\", 'pressure')\ndarcy.project_u(gb, \"discharge\", \"P0u\")\n\ntotal_flow_rate = 0\nfor g, d in gb:\n    bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n    if bound_faces.size != 0:\n        bound_face_centers = g.face_centers[:, bound_faces]\n        left = bound_face_centers[0, :] < domain['xmin'] + tol\n        flow_rate = d['discharge'][bound_faces[left]]\n        total_flow_rate += np.sum(flow_rate)\n\nexporter.export_vtk(gb, 'darcy', ['pressure', \"P0u\"], folder=export_folder,\n                    binary=False)\n\nphysics = 'transport'\nadvection = upwind.UpwindMixDim(physics)\nmass = mass_matrix.MassMatrixMixDim(physics)\ninvMass = mass_matrix.InvMassMatrixMixDim(physics)\n\nadd_data_advection(gb, domain, tol)\n\ngb.add_node_prop('deltaT', prop=deltaT)\n\nU, rhs_u = advection.matrix_rhs(gb)\nM, _ = mass.matrix_rhs(gb)\nOF = advection.outflow(gb)\nM_U = M + U\n\nrhs = rhs_u\n\nIE_solver = sps.linalg.factorized((M_U).tocsc())\n\ntheta = np.zeros(rhs.shape[0])\n\ntime = np.empty(Nt)\nfile_name = \"theta\"\ni_export = 0\nstep_to_export = np.empty(0)\n\nproduction = np.zeros(Nt)\n\nfor i in np.arange(Nt):\n    print(\"Time step\", i, \" of \", Nt, \" time \", i * deltaT, \" deltaT \", deltaT)\n    \n    production[i] = np.sum(OF.dot(theta)) / total_flow_rate\n    theta = IE_solver(M.dot(theta) + rhs)\n\n    if i % export_every == 0:\n        print(\"Export solution at\", i)\n        advection.split(gb, \"theta\", theta)\n\n        exporter.export_vtk(gb, file_name, [\"theta\"], time_step=i_export,\n                            binary=False, folder=export_folder)\n        step_to_export = np.r_[step_to_export, i]\n        i_export += 1\n\nexporter.export_pvd(gb, file_name, step_to_export *\n                    deltaT, folder=export_folder)\n\ntimes = deltaT * np.arange(Nt)\nnp.savetxt(export_folder + '/production.txt', (times, np.abs(production)),\n           delimiter=',')",
  "\"\"\n\n__version__ = '1.0.0'\nfrom docopt import docopt\n\nargs = docopt(__doc__, version=__version__)\n\nimport sirf.STIR as PET\n\nfrom sirf.Utilities import show_2D_array\nimport PET_plot_functions\n\nraw_data_file = args['--file']\nrandoms_data_file = args['--randoms']\nacf_file = args['--attenuation_correction_factors']\ndata_path = args['--path']\nif data_path is None:\n    data_path = PET.examples_data_path('PET') + '/mMR'\nnorm_file = PET.existing_filepath(data_path, args['--norm'])\nmu_map_file = PET.existing_filepath(data_path, args['--attenuation_image'])\noutput_prefix = args['--output']\ninteractive = not args['--non-interactive']\n\ndef main():\n\n    msg_red = PET.MessageRedirector('info.txt', 'warn.txt', 'errr.txt')\n\n    PET.AcquisitionData.set_storage_scheme('memory')\n\n    se = PET.ScatterEstimator()\n\n    prompts = PET.AcquisitionData(raw_data_file)\n    se.set_input(prompts)\n    se.set_attenuation_image(PET.ImageData(mu_map_file))\n    if randoms_data_file is None:\n        randoms = None\n    else:\n        randoms = PET.AcquisitionData(randoms_data_file)\n        se.set_randoms(randoms)\n    if not(norm_file is None):\n        se.set_asm(PET.AcquisitionSensitivityModel(norm_file))\n    if not(acf_file is None):\n        se.set_attenuation_correction_factors(PET.AcquisitionData(acf_file))\n    \n    se.set_num_iterations(1)\n    print(\"number of scatter iterations that will be used: %d\" % se.get_num_iterations())\n    se.set_output_prefix(output_prefix)\n    se.set_up()\n    se.process()\n    scatter_estimate = se.get_output()\n\n    if not interactive:\n        return\n\n    scatter_estimate_as_array = scatter_estimate.as_array()\n    show_2D_array('Scatter estimate', scatter_estimate_as_array[0, 0, :, :])\n\n    PET_plot_functions.plot_sinogram_profile(prompts, randoms=randoms, scatter=scatter_estimate)\n\ntry:\n    main()\n    print('done')\nexcept PET.error as err:\n    print('%s' % err.value)",
  "\nimport hydro\n\nimport mpisppy.utils.sputils as sputils\n\nfrom mpisppy.spin_the_wheel import WheelSpinner\nfrom mpisppy.utils import config\nimport mpisppy.utils.cfg_vanilla as vanilla\n\nimport mpisppy.cylinders as cylinders\n\nwrite_solution = True\n\ndef _parse_args():\n    \n    cfg = config.Config()\n    cfg.multistage()\n    cfg.ph_args()\n    cfg.two_sided_args()\n    cfg.xhatlooper_args()\n    cfg.xhatshuffle_args()\n    cfg.lagrangian_args()\n    cfg.xhatspecific_args()\n\n    cfg.add_to_config(name =\"stage2EFsolvern\",\n                         description=\"Solver to use for xhatlooper stage2ef option (default None)\",\n                         domain = str,\n                         default=None)\n\n    cfg.parse_command_line(\"farmer_cylinders\")\n    return cfg\n\ndef main():\n    cfg = _parse_args()\n\n    _parse_args()  \n\n    BFs = cfg[\"branching_factors\"]\n    if len(BFs) != 2:\n        raise RuntimeError(\"Hydro is a three stage problem, so it needs 2 BFs\")\n\n    xhatshuffle = cfg[\"xhatshuffle\"]\n    lagrangian = cfg[\"lagrangian\"]\n\n    all_nodenames = sputils.create_nodenames_from_branching_factors(BFs)\n\n    ScenCount = BFs[0] * BFs[1]\n    scenario_creator_kwargs = {\"branching_factors\": BFs}\n    all_scenario_names = [f\"Scen{i+1}\" for i in range(ScenCount)]\n    scenario_creator = hydro.scenario_creator\n    scenario_denouement = hydro.scenario_denouement\n    rho_setter = None\n    \n    beans = (cfg, scenario_creator, scenario_denouement, all_scenario_names)\n    \n    hub_dict = vanilla.ph_hub(*beans,\n                              scenario_creator_kwargs=scenario_creator_kwargs,\n                              ph_extensions=None,\n                              rho_setter = rho_setter,\n                              all_nodenames = all_nodenames,\n                             )\n\n    if lagrangian:\n        lagrangian_spoke = vanilla.lagrangian_spoke(*beans,\n                                                    scenario_creator_kwargs=scenario_creator_kwargs,\n                                                    rho_setter = rho_setter,\n                                                    all_nodenames = all_nodenames,\n                                                   )\n\n    if xhatshuffle:\n        xhatshuffle_spoke = vanilla.xhatshuffle_spoke(*beans,\n                                                      all_nodenames=all_nodenames,\n                                                      scenario_creator_kwargs=scenario_creator_kwargs,\n                                                     )\n\n    list_of_spoke_dict = list()\n    if lagrangian:\n        list_of_spoke_dict.append(lagrangian_spoke)\n    if xhatshuffle:\n        list_of_spoke_dict.append(xhatshuffle_spoke)\n\n    if cfg.stage2EFsolvern is not None:\n        xhatshuffle_spoke[\"opt_kwargs\"][\"options\"][\"stage2EFsolvern\"] = cfg[\"stage2EFsolvern\"]\n        xhatshuffle_spoke[\"opt_kwargs\"][\"options\"][\"branching_factors\"] = cfg[\"branching_factors\"]\n\n    wheel = WheelSpinner(hub_dict, list_of_spoke_dict)\n    wheel.spin()\n\n    if wheel.global_rank == 0:  \n        print(f\"BestInnerBound={wheel.BestInnerBound} and BestOuterBound={wheel.BestOuterBound}\")\n    \n    if write_solution:\n        wheel.write_first_stage_solution('hydro_first_stage.csv')\n        wheel.write_tree_solution('hydro_full_solution')\n\nif __name__ == \"__main__\":\n    main()",
  "\"\"\n\nimport os\nimport sys\n\nfrom moviepy import *\nfrom moviepy.audio.tools.cuts import find_audio_period\nfrom moviepy.video.tools.cuts import find_video_period\n\nif not os.path.exists(\"knights.mp4\") or not os.path.exists(\"frontier.webm\"):\n    retcode1 = os.system(\"youtube-dl zvCvOC2VwDc -o knights\")\n    retcode2 = os.system(\"youtube-dl lkY3Ek9VPtg -o frontier\")\n    if retcode1 != 0 or retcode2 != 0:\n        sys.stderr.write(\n            \"Error downloading videos. Check that you've installed youtube-dl.\\n\"\n        )\n        sys.exit(1)\n\naudio = (\n    AudioFileClip(\"frontier.webm\")\n    .subclip((4, 7), (4, 18))\n    .audio_fadein(1)\n    .audio_fadeout(1)\n)\n\naudio_period = find_audio_period(audio)\nprint(\"Analyzed the audio, found a period of %.02f seconds\" % audio_period)\n\nclip = (\n    VideoFileClip(\"knights.mp4\", audio=False)\n    .subclip((1, 24.15), (1, 26))\n    .crop(x1=500, x2=1350)\n)\n\nvideo_period = find_video_period(clip, start_time=0.3)\nprint(\"Analyzed the video, found a period of %.02f seconds\" % video_period)\n\nedited_right = (\n    clip.subclip(0, video_period)\n    .speedx(final_duration=2 * audio_period)\n    .fx(vfx.loop, duration=audio.duration)\n    .subclip(0.25)\n)\n\nedited_left = edited_right.fx(vfx.mirror_x)\n\ndancing_knights = (\n    clips_array([[edited_left, edited_right]])\n    .fadein(1)\n    .fadeout(1)\n    .with_audio(audio)\n    .subclip(0.3)\n)\n\ntxt_title = (\n    TextClip(\n        \"15th century dancing\\n(hypothetical)\",\n        font_size=70,\n        font=\"Century-Schoolbook-Roman\",\n        color=\"white\",\n    )\n    .margin(top=15, opacity=0)\n    .with_position((\"center\", \"top\"))\n)\n\ntitle = (\n    CompositeVideoClip([dancing_knights.to_ImageClip(), txt_title])\n    .fadein(0.5)\n    .with_duration(3.5)\n)\n\ntxt_credits = \"\"\n\ncredits = (\n    TextClip(\n        txt_credits,\n        color=\"white\",\n        font=\"Century-Schoolbook-Roman\",\n        font_size=35,\n        kerning=-2,\n        interline=-1,\n        bg_color=\"black\",\n        size=title.size,\n    )\n    .with_duration(2.5)\n    .fadein(0.5)\n    .fadeout(0.5)\n)\n\nfinal = concatenate_videoclips([title, dancing_knights, credits])\n\nfinal.write_videofile(\n    \"dancing_knights.mp4\", fps=clip.fps, audio_bitrate=\"1000k\", bitrate=\"4000k\"\n)",
  "\nfrom __future__ import absolute_import, unicode_literals\nimport os\nimport sys\nimport DDG4\nfrom DDG4 import OutputLevel as Output\nfrom g4units import keV\n\n\"\"\n\ndef run():\n  kernel = DDG4.Kernel()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/OpticalSurfaces/compact/OpNovice.xml\"))\n\n  DDG4.importConstants(kernel.detectorDescription(), debug=False)\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  geant4.printDetectors()\n  \n  if len(sys.argv) > 1:\n    geant4.setupCshUI(macro=sys.argv[1])\n  else:\n    geant4.setupCshUI()\n\n  geant4.setupTrackingField(prt=True)\n  \n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputLevel = Output.DEBUG\n  prt.OutputType = 3  \n  kernel.eventAction().adopt(prt)\n\n  generator_output_level = Output.INFO\n\n  seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n  act.DebugMaterials = True\n  act.DebugElements = False\n  act.DebugVolumes = True\n  act.DebugShapes = True\n  act.DebugSurfaces = True\n\n  gun = geant4.setupGun(\"Gun\", particle='gamma', energy=5 * keV, multiplicity=1)\n  gun.OutputLevel = generator_output_level\n\n  \"\"\n  geant4.setupTracker('BubbleDevice')\n\n  phys = geant4.setupPhysics('')\n  ph = DDG4.PhysicsList(kernel, 'Geant4OpticalPhotonPhysics/OpticalGammaPhys')\n  ph.VerboseLevel = 2\n  ph.addParticleGroup('G4BosonConstructor')\n  ph.addParticleGroup('G4LeptonConstructor')\n  ph.addParticleGroup('G4MesonConstructor')\n  ph.addParticleGroup('G4BaryonConstructor')\n  ph.addParticleGroup('G4IonConstructor')\n  ph.addParticleConstructor('G4OpticalPhoton')\n\n  ph.addDiscreteParticleProcess('gamma', 'G4GammaConversion')\n  ph.addDiscreteParticleProcess('gamma', 'G4ComptonScattering')\n  ph.addDiscreteParticleProcess('gamma', 'G4PhotoElectricEffect')\n  ph.addParticleProcess(str('e[+-]'), str('G4eMultipleScattering'), -1, 1, 1)\n  ph.addParticleProcess(str('e[+-]'), str('G4eIonisation'), -1, 2, 2)\n  ph.addParticleProcess(str('e[+-]'), str('G4eBremsstrahlung'), -1, 3, 3)\n  ph.addParticleProcess(str('e+'), str('G4eplusAnnihilation'), 0, -1, 4)\n  ph.addParticleProcess(str('mu[+-]'), str('G4MuMultipleScattering'), -1, 1, 1)\n  ph.addParticleProcess(str('mu[+-]'), str('G4MuIonisation'), -1, 2, 2)\n  ph.addParticleProcess(str('mu[+-]'), str('G4MuBremsstrahlung'), -1, 3, 3)\n  ph.addParticleProcess(str('mu[+-]'), str('G4MuPairProduction'), -1, 4, 4)\n  ph.enableUI()\n  phys.adopt(ph)\n\n  ph = DDG4.PhysicsList(kernel, 'Geant4ScintillationPhysics/ScintillatorPhys')\n  ph.ScintillationYieldFactor = 1.0\n  ph.ScintillationExcitationRatio = 1.0\n  ph.TrackSecondariesFirst = False\n  ph.VerboseLevel = 2\n  ph.enableUI()\n  phys.adopt(ph)\n\n  ph = DDG4.PhysicsList(kernel, 'Geant4CerenkovPhysics/CerenkovPhys')\n  ph.MaxNumPhotonsPerStep = 10\n  ph.MaxBetaChangePerStep = 10.0\n  ph.TrackSecondariesFirst = True\n  ph.VerboseLevel = 2\n  ph.enableUI()\n  phys.adopt(ph)\n\n  phys.dump()\n\n  geant4.execute()\n\nif __name__ == \"__main__\":\n  run()",
  "import numpy as np\nimport scipy.sparse as sps\n\nfrom porepy.viz import exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import dual\n\nfrom porepy.utils import comp_geom as cg\nfrom porepy.utils import sort_points\n\ndef add_data(gb, domain):\n    \"\"\n    gb.add_node_props(['param', 'is_tangent'])\n    tol = 1e-3\n    a = 1e-2\n\n    for g, d in gb:\n        param = Parameters(g)\n\n        d['is_tangential'] = True\n        if g.dim == 2:\n            kxx = 1e-14 * np.ones(g.num_cells)\n        else:\n            kxx = 1e-8 * np.ones(g.num_cells)\n\n        perm = tensor.SecondOrder(g.dim, kxx)\n        param.set_tensor(\"flow\", perm)\n\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        aperture = np.power(a, gb.dim_max() - g.dim)\n        param.set_aperture(np.ones(g.num_cells) * aperture)\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[np.logical_or(left, right)] = 'dir'\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = 1013250\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        gn = gb.sorted_nodes_of_edge(e)\n        aperture = np.power(a, gb.dim_max() - gn[0].dim)\n        d['kn'] = 1e-10 * np.ones(gn[0].num_cells) / aperture\n\ndef plot_over_line(gb, pts, name, tol):\n\n    values = np.zeros(pts.shape[1])\n    is_found = np.zeros(pts.shape[1], dtype=np.bool)\n\n    for g, d in gb:\n        if g.dim < gb.dim_max():\n            continue\n\n        if not cg.is_planar(np.hstack((g.nodes, pts)), tol=1e-4):\n            continue\n\n        faces_cells, _, _ = sps.find(g.cell_faces)\n        nodes_faces, _, _ = sps.find(g.face_nodes)\n\n        normal = cg.compute_normal(g.nodes)\n        for c in np.arange(g.num_cells):\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c + 1])\n            pts_id_c = np.array([nodes_faces[g.face_nodes.indptr[f]:\n                                             g.face_nodes.indptr[f + 1]]\n                                 for f in faces_cells[loc]]).T\n            pts_id_c = sort_points.sort_point_pairs(pts_id_c)[0, :]\n            pts_c = g.nodes[:, pts_id_c]\n\n            mask = np.where(np.logical_not(is_found))[0]\n            if mask.size == 0:\n                break\n            check = np.zeros(mask.size, dtype=np.bool)\n            last = False\n            for i, pt in enumerate(pts[:, mask].T):\n                check[i] = cg.is_point_in_cell(pts_c, pt)\n                if last and not check[i]:\n                    break\n            is_found[mask] = check\n            values[mask[check]] = d[name][c]\n\n    return values\n\ntol = 1e-4\nmesh_kwargs = {}\nmesh_kwargs['mesh_size'] = {'mode': 'weighted',\n                            'value': 500,\n                            'bound_value': 500,\n                            'tol': tol}\n\ndomain = {'xmin': 0, 'xmax': 700, 'ymin': 0, 'ymax': 600}\ngb = importer.from_csv('network.csv', mesh_kwargs, domain)\ngb.compute_geometry()\nco.coarsen(gb, 'by_volume')\ngb.assign_node_ordering()\n\nadd_data(gb, domain)\n\nsolver = dual.DualVEMMixDim(\"flow\")\nA, b = solver.matrix_rhs(gb)\n\nup = sps.linalg.spsolve(A, b)\nsolver.split(gb, \"up\", up)\n\ngb.add_node_props([\"discharge\", 'pressure', \"P0u\"])\nsolver.extract_u(gb, \"up\", \"discharge\")\nsolver.extract_p(gb, \"up\", 'pressure')\nsolver.project_u(gb, \"discharge\", \"P0u\")\n\nexporter.export_vtk(gb, 'vem', ['pressure', \"P0u\"], folder='example_5_1_2')\n\nb_box = gb.bounding_box()\nN_pts = 1000\ny_range = np.linspace(b_box[0][1] + tol, b_box[1][1] - tol, N_pts)\npts = np.stack((625 * np.ones(N_pts), y_range, np.zeros(N_pts)))\nvalues = plot_over_line(gb, pts, 'pressure', tol)\n\narc_length = y_range - b_box[0][1]\nnp.savetxt(\"example_5_1_2/vem_x_625.csv\", (arc_length, values))\n\nx_range = np.linspace(b_box[0][0] + tol, b_box[1][0] - tol, N_pts)\npts = np.stack((x_range, 500 * np.ones(N_pts), np.zeros(N_pts)))\nvalues = plot_over_line(gb, pts, 'pressure', tol)\n\narc_length = x_range - b_box[0][0]\nnp.savetxt(\"example_5_1_2/vem_y_500.csv\", (arc_length, values))\n\nprint(\"diam\", gb.diameter(lambda g: g.dim == gb.dim_max()))\nprint(\"num_cells 2d\", gb.num_cells(lambda g: g.dim == 2))\nprint(\"num_cells 1d\", gb.num_cells(lambda g: g.dim == 1))",
  "\nfrom seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.compiler import Docker\nfrom seedemu.services import DomainNameCachingService\nfrom seedemu.core import Emulator, Binding, Filter, Node\nfrom typing import List\n\nsim = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\n\ndef make_stub_as(asn: int, exchange: str):\n    stub_as = base.createAutonomousSystem(asn)\n    host = stub_as.createHost('host0')\n    host1 = stub_as.createHost('host1')\n    host2 = stub_as.createHost('host2')\n    \n    router = stub_as.createRouter('router0')\n    net = stub_as.createNetwork('net0')\n\n    router.joinNetwork('net0')\n    host.joinNetwork('net0')\n    host1.joinNetwork('net0')\n    host2.joinNetwork('net0')\n    \n    router.joinNetwork(exchange)\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\nbase.createInternetExchange(102)\n\nmake_stub_as(150, 'ix100')\nmake_stub_as(151, 'ix100')\n\nmake_stub_as(152, 'ix101')\nmake_stub_as(153, 'ix101')\nmake_stub_as(154, 'ix101')\n\nmake_stub_as(160, 'ix102')\nmake_stub_as(161, 'ix102')\n\nas2 = base.createAutonomousSystem(2)\n\nas2_100 = as2.createRouter('r0')\nas2_101 = as2.createRouter('r1')\nas2_102 = as2.createRouter('r2')\n\nas2_100.joinNetwork('ix100')\nas2_101.joinNetwork('ix101')\nas2_102.joinNetwork('ix102')\n\nas2_net_100_101 = as2.createNetwork('n01')\nas2_net_101_102 = as2.createNetwork('n12')\nas2_net_102_100 = as2.createNetwork('n20')\n\nas2_100.joinNetwork('n01')\nas2_101.joinNetwork('n01')\n\nas2_101.joinNetwork('n12')\nas2_102.joinNetwork('n12')\n\nas2_102.joinNetwork('n20')\nas2_100.joinNetwork('n20')\n\nas3 = base.createAutonomousSystem(3)\n\nas3_101 = as3.createRouter('r1')\nas3_102 = as3.createRouter('r2')\n\nas3_101.joinNetwork('ix101')\nas3_102.joinNetwork('ix102')\n\nas3_net_101_102 = as3.createNetwork('n12')\n\nas3_101.joinNetwork('n12')\nas3_102.joinNetwork('n12')\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 153, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 154, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 154, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n\nsim.addLayer(base)\nsim.addLayer(routing)\nsim.addLayer(ebgp)\nsim.addLayer(ibgp)\nsim.addLayer(ospf)\n\nsim.dump('base-component.bin')",
  "\"\"\n\nimport os\nimport pyaedt\n\nnon_graphical = False\n\nmaxwell_2d = pyaedt.Maxwell2d(solution_type=\"TransientXY\", specified_version=\"2023.2\", non_graphical=non_graphical,\n                              new_desktop_session=True, projectname=pyaedt.generate_unique_project_name())\n\nrect1 = maxwell_2d.modeler.create_rectangle([0, 0, 0], [10, 20], name=\"winding\", matname=\"copper\")\nadded = rect1.duplicate_along_line([14, 0, 0])\nrect2 = maxwell_2d.modeler[added[0]]\n\nregion = maxwell_2d.modeler.create_region([100, 100, 100, 100, 100, 100])\n\nmaxwell_2d.assign_winding([rect1.name, rect2.name], name=\"PHA\")\nmaxwell_2d.assign_balloon(region.edges)\n\nmaxwell_2d.plot(show=False, export_path=os.path.join(maxwell_2d.working_directory, \"Image.jpg\"), plot_air_objects=True)\n\nsetup = maxwell_2d.create_setup()\nsetup.props[\"StopTime\"] = \"0.02s\"\nsetup.props[\"TimeStep\"] = \"0.0002s\"\nsetup.props[\"SaveFieldsType\"] = \"Every N Steps\"\nsetup.props[\"N Steps\"] = \"1\"\nsetup.props[\"Steps From\"] = \"0s\"\nsetup.props[\"Steps To\"] = \"0.002s\"\n\nmaxwell_2d.post.create_report(\n    \"InputCurrent(PHA)\", domain=\"Time\", primary_sweep_variable=\"Time\", plotname=\"Winding Plot 1\"\n)\n\nmaxwell_2d.analyze(use_auto_settings=False)\n\ncutlist = [\"Global:XY\"]\nface_lists = rect1.faces\nface_lists += rect2.faces\ntimesteps = [str(i * 2e-4) + \"s\" for i in range(11)]\nid_list = [f.id for f in face_lists]\n\nanimatedGif = maxwell_2d.post.plot_animated_field(\n    \"Mag_B\",\n    id_list,\n    \"Surface\",\n    intrinsics={\"Time\": \"0s\"},\n    variation_variable=\"Time\",\n    variation_list=timesteps,\n    show=False,\n    export_gif=False,\n)\nanimatedGif.isometric_view = False\nanimatedGif.camera_position = [15, 15, 80]\nanimatedGif.focal_point = [15, 15, 0]\nanimatedGif.roll_angle = 0\nanimatedGif.elevation_angle = 0\nanimatedGif.azimuth_angle = 0\n\nanimatedGif.animate()\n\nsolutions = maxwell_2d.post.get_solution_data(\"InputCurrent(PHA)\", primary_sweep_variable=\"Time\")\nsolutions.plot()\n\nmaxwell_2d.release_desktop()",
  "\"\"\nimport os\nimport numpy as np\nfrom pytransform3d.urdf import UrdfTransformManager\nimport pytransform3d.transformations as pt\nimport pytransform3d.visualizer as pv\n\ndef plot_screw(figure, q=np.zeros(3), s_axis=np.array([1.0, 0.0, 0.0]),\n               h=1.0, theta=1.0, A2B=None, s=1.0):\n    \"\"\n    from pytransform3d.rotations import (\n        vector_projection, angle_between_vectors, perpendicular_to_vectors,\n        slerp_weights)\n    from pytransform3d.transformations import (\n        check_screw_parameters, transform, translate_transform,\n        vector_to_point, vector_to_direction, vectors_to_points)\n\n    if A2B is None:\n        A2B = np.eye(4)\n\n    q, s_axis, h = check_screw_parameters(q, s_axis, h)\n\n    origin_projected_on_screw_axis = q + vector_projection(-q, s_axis)\n\n    pure_translation = np.isinf(h)\n\n    if not pure_translation:\n        screw_axis_to_old_frame = -origin_projected_on_screw_axis\n        screw_axis_to_rotated_frame = perpendicular_to_vectors(\n            s_axis, screw_axis_to_old_frame)\n        screw_axis_to_translated_frame = h * s_axis\n\n        arc = np.empty((100, 3))\n        angle = angle_between_vectors(screw_axis_to_old_frame,\n                                      screw_axis_to_rotated_frame)\n        for i, t in enumerate(zip(np.linspace(0, 2 * theta / np.pi, len(arc)),\n                                  np.linspace(0.0, 1.0, len(arc)))):\n            t1, t2 = t\n            w1, w2 = slerp_weights(angle, t1)\n            arc[i] = (origin_projected_on_screw_axis\n                      + w1 * screw_axis_to_old_frame\n                      + w2 * screw_axis_to_rotated_frame\n                      + screw_axis_to_translated_frame * t2 * theta)\n\n    q = transform(A2B, vector_to_point(q))[:3]\n    s_axis = transform(A2B, vector_to_direction(s_axis))[:3]\n    if not pure_translation:\n        arc = transform(A2B, vectors_to_points(arc))[:, :3]\n        origin_projected_on_screw_axis = transform(\n            A2B, vector_to_point(origin_projected_on_screw_axis))[:3]\n\n    Q = translate_transform(np.eye(4), q)\n    fig.plot_sphere(radius=s * 0.02, A2B=Q, c=[1, 0, 0])\n    if pure_translation:\n        s_axis *= theta\n        Q_plus_S_axis = translate_transform(np.eye(4), q + s_axis)\n        fig.plot_sphere(radius=s * 0.02, A2B=Q_plus_S_axis, c=[0, 1, 0])\n    P = np.array([\n        [q[0] - s * s_axis[0], q[1] - s * s_axis[1], q[2] - s * s_axis[2]],\n        [q[0] + (1 + s) * s_axis[0],\n         q[1] + (1 + s) * s_axis[1], q[2] + (1 + s) * s_axis[2]]\n    ])\n    figure.plot(P=P, c=[0, 0, 0])\n\n    if not pure_translation:\n        \n        figure.plot(arc, c=[0, 0, 0])\n\n        for i, c in zip([0, -1], [[1, 0, 0], [0, 1, 0]]):\n            arc_bound = np.vstack((origin_projected_on_screw_axis, arc[i]))\n            figure.plot(arc_bound, c=c)\n\nBASE_DIR = \"test/test_data/\"\ndata_dir = BASE_DIR\nsearch_path = \".\"\nwhile (not os.path.exists(data_dir) and\n       os.path.dirname(search_path) != \"pytransform3d\"):\n    search_path = os.path.join(search_path, \"..\")\n    data_dir = os.path.join(search_path, BASE_DIR)\n\ntm = UrdfTransformManager()\nfilename = os.path.join(data_dir, \"robot_with_visuals.urdf\")\nwith open(filename, \"r\") as f:\n    robot_urdf = f.read()\n    tm.load_urdf(robot_urdf, mesh_path=data_dir)\ntm.set_joint(\"joint2\", 0.2 * np.pi)\ntm.set_joint(\"joint3\", 0.2 * np.pi)\ntm.set_joint(\"joint5\", 0.1 * np.pi)\ntm.set_joint(\"joint6\", 0.5 * np.pi)\n\nee2base = tm.get_transform(\"tcp\", \"robot_arm\")\nbase2ee = tm.get_transform(\"robot_arm\", \"tcp\")\n\nmass = 1.0\nwrench_in_ee = np.array([0.0, 0.0, 0.0, 0.0, -9.81, 0.0]) * mass\nwrench_in_base = np.dot(pt.adjoint_from_transform(base2ee).T, wrench_in_ee)\n\nfig = pv.figure()\n\nfig.plot_graph(tm, \"robot_arm\", s=0.1, show_visuals=True)\n\nfig.plot_transform(s=0.4)\nfig.plot_transform(A2B=ee2base, s=0.1)\n\nmass2base = np.copy(ee2base)\nmass2base[2, 3] += 0.075\nfig.plot_sphere(radius=0.025, A2B=mass2base)\n\nS, theta = pt.screw_axis_from_exponential_coordinates(wrench_in_base)\nq, s, h = pt.screw_parameters_from_screw_axis(S)\nplot_screw(fig, q, s, h, theta * 0.05)\n\nS, theta = pt.screw_axis_from_exponential_coordinates(wrench_in_ee)\nq, s, h = pt.screw_parameters_from_screw_axis(S)\nplot_screw(fig, q, s, h, theta * 0.05, A2B=ee2base)\n\nfig.view_init()\nif \"__file__\" in globals():\n    fig.show()\nelse:\n    fig.save_image(\"__open3d_rendered_image.jpg\")",
  "\"\"\n\nfrom math import pi\n\nimport pyvista as pv\n\nsupertoroid = pv.ParametricSuperToroid(n1=0.5)\nsupertoroid.plot(color='lightblue', smooth_shading=True)\n\nellipsoid = pv.ParametricEllipsoid(10, 5, 5)\nellipsoid.plot(color='lightblue')\n\ncpos = [\n    (21.9930, 21.1810, -30.3780),\n    (-1.1640, -1.3098, -0.1061),\n    (0.8498, -0.2515, 0.4631),\n]\n\npart_ellipsoid = pv.ParametricEllipsoid(10, 5, 5, max_v=pi / 2)\npart_ellipsoid.plot(color='lightblue', smooth_shading=True, cpos=cpos)\n\npseudosphere = pv.ParametricPseudosphere()\npseudosphere.plot(color='lightblue', smooth_shading=True)\n\nbohemiandome = pv.ParametricBohemianDome()\nbohemiandome.plot(color='lightblue')\n\nbour = pv.ParametricBour()\nbour.plot(color='lightblue')\n\nboy = pv.ParametricBoy()\nboy.plot(color='lightblue')\n\ncatalanminimal = pv.ParametricCatalanMinimal()\ncatalanminimal.plot(color='lightblue')\n\nconicspiral = pv.ParametricConicSpiral()\nconicspiral.plot(color='lightblue')\n\ncrosscap = pv.ParametricCrossCap()\ncrosscap.plot(color='lightblue')\n\ndini = pv.ParametricDini()\ndini.plot(color='lightblue')\n\nenneper = pv.ParametricEnneper()\nenneper.plot(cpos=\"yz\")\n\nfigure8klein = pv.ParametricFigure8Klein()\nfigure8klein.plot()\n\nhenneberg = pv.ParametricHenneberg()\nhenneberg.plot(color='lightblue')\n\nklein = pv.ParametricKlein()\nklein.plot(color='lightblue')\n\nkuen = pv.ParametricKuen()\nkuen.plot(color='lightblue')\n\nmobius = pv.ParametricMobius()\nmobius.plot(color='lightblue')\n\npluckerconoid = pv.ParametricPluckerConoid()\npluckerconoid.plot(color='lightblue')\n\nrandomhills = pv.ParametricRandomHills()\nrandomhills.plot(color='lightblue')\n\nroman = pv.ParametricRoman()\nroman.plot(color='lightblue')\n\nsuperellipsoid = pv.ParametricSuperEllipsoid(n1=0.1, n2=2)\nsuperellipsoid.plot(color='lightblue')\n\ntorus = pv.ParametricTorus()\ntorus.plot(color='lightblue')\n\npointa = [-1, 0, 0]\npointb = [0, 1, 0]\ncenter = [0, 0, 0]\nresolution = 100\n\narc = pv.CircularArc(pointa, pointb, center, resolution)\n\npl = pv.Plotter()\npl.add_mesh(arc, color='k', line_width=4)\npl.show_bounds()\npl.view_xy()\npl.show()\n\npointa = [-1, 0, 0]\npointb = [1, 0, 0]\ncenter = [0, 0, 0]\nresolution = 100\n\narc = pv.CircularArc(pointa, pointb, center, resolution)\npoly = arc.extrude([0, 0, 1])\npoly.plot(color='lightblue', cpos='iso', show_edges=True)",
  "from seedemu.layers import Base, Routing, Ebgp, PeerRelationship, Ibgp, Ospf\nfrom seedemu.services import WebService\nfrom seedemu.compiler import Docker\nfrom seedemu.core import Emulator, Filter, Binding\nfrom seedemu.components import BgpAttackerComponent\nfrom seedemu.mergers import DEFAULT_MERGERS\n\nemu = Emulator()\n\nbase = Base()\nrouting = Routing()\nebgp = Ebgp()\nibgp = Ibgp()\nospf = Ospf()\nweb = WebService()\n\ndef make_stub_as(asn: int, exchange: str):\n    stub_as = base.createAutonomousSystem(asn)\n\n    web_server = stub_as.createHost('web')\n    web.install('web{}'.format(asn))\n    emu.addBinding(Binding('web{}'.format(asn), filter = Filter(asn = asn, nodeName = 'web')))\n\n    router = stub_as.createRouter('router0')\n\n    net = stub_as.createNetwork('net0')\n\n    web_server.joinNetwork('net0')\n    router.joinNetwork('net0')\n\n    router.joinNetwork(exchange)\n\nbase.createInternetExchange(100)\nbase.createInternetExchange(101)\nbase.createInternetExchange(102)\n\nmake_stub_as(150, 'ix100')\nmake_stub_as(151, 'ix100')\n\nmake_stub_as(152, 'ix101')\n\nmake_stub_as(160, 'ix102')\nmake_stub_as(161, 'ix102')\n\nas2 = base.createAutonomousSystem(2)\n\nas2_100 = as2.createRouter('r0')\nas2_101 = as2.createRouter('r1')\nas2_102 = as2.createRouter('r2')\n\nas2_100.joinNetwork('ix100')\nas2_101.joinNetwork('ix101')\nas2_102.joinNetwork('ix102')\n\nas2_net_100_101 = as2.createNetwork('n01')\nas2_net_101_102 = as2.createNetwork('n12')\nas2_net_102_100 = as2.createNetwork('n20')\n\nas2_100.joinNetwork('n01')\nas2_101.joinNetwork('n01')\n\nas2_101.joinNetwork('n12')\nas2_102.joinNetwork('n12')\n\nas2_102.joinNetwork('n20')\nas2_100.joinNetwork('n20')\n\nas3 = base.createAutonomousSystem(3)\n\nas3_101 = as3.createRouter('r1')\nas3_102 = as3.createRouter('r2')\n\nas3_101.joinNetwork('ix101')\nas3_102.joinNetwork('ix102')\n\nas3_net_101_102 = as3.createNetwork('n12')\n\nas3_101.joinNetwork('n12')\nas3_102.joinNetwork('n12')\n\nebgp.addPrivatePeering(100, 2, 150, PeerRelationship.Provider)\nebgp.addPrivatePeering(100, 150, 151, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(101, 2, 3, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 2, 152, PeerRelationship.Provider)\nebgp.addPrivatePeering(101, 3, 152, PeerRelationship.Provider)\n\nebgp.addPrivatePeering(102, 2, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 160, PeerRelationship.Provider)\nebgp.addPrivatePeering(102, 3, 161, PeerRelationship.Provider)\n\nemu.addLayer(base)\nemu.addLayer(routing)\nemu.addLayer(ebgp)\nemu.addLayer(ibgp)\nemu.addLayer(ospf)\nemu.addLayer(web)\n\nbgp_attacker = BgpAttackerComponent(attackerAsn = 66)\n\nbgp_attacker.addHijackedPrefix('10.151.0.0/25')\nbgp_attacker.addHijackedPrefix('10.151.0.128/25')\nbgp_attacker.joinInternetExchange('ix101', '10.101.0.66')\n\nebgp.addPrivatePeering(101, 2, 66, PeerRelationship.Unfiltered)\n\nemu_new = emu.merge(bgp_attacker.get(), DEFAULT_MERGERS)\nemu_new.render()\n\nemu_new.compile(Docker(selfManagedNetwork = True), './output2')",
  "import numpy as np\nimport scipy.sparse as sps\n\nfrom porepy.viz import exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import dual\nfrom porepy.utils import comp_geom as cg\n\ndef add_data(gb, domain, kf):\n    \"\"\n    gb.add_node_props(['param'])\n    tol = 1e-5\n    a = 1e-4\n\n    for g, d in gb:\n        param = Parameters(g)\n\n        kxx = np.ones(g.num_cells) * np.power(kf, g.dim < gb.dim_max())\n        if g.dim == 2:\n            perm = tensor.SecondOrder(3, kxx=kxx, kyy=kxx, kzz=1)\n        else:\n            perm = tensor.SecondOrder(3, kxx=kxx, kyy=1, kzz=1)\n            if g.dim == 1:\n                R = cg.project_line_matrix(g.nodes, reference=[1, 0, 0])\n                perm.rotate(R)\n\n        param.set_tensor(\"flow\", perm)\n\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        aperture = np.power(a, gb.dim_max() - g.dim)\n        param.set_aperture(np.ones(g.num_cells) * aperture)\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            left = bound_face_centers[0, :] < domain['xmin'] + tol\n            right = bound_face_centers[0, :] > domain['xmax'] - tol\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[right] = 'dir'\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[left]] = -aperture \\\n                * g.face_areas[bound_faces[left]]\n            bc_val[bound_faces[right]] = 1\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        gn = gb.sorted_nodes_of_edge(e)\n        aperture = np.power(a, gb.dim_max() - gn[0].dim)\n        d['kn'] = np.ones(gn[0].num_cells) * kf / aperture\n\ndef write_network(file_name):\n    network = \"FID,START_X,START_Y,END_X,END_Y\\n\"\n    network += \"0,0,0.5,1,0.5\\n\"\n    network += \"1,0.5,0,0.5,1\\n\"\n    network += \"2,0.5,0.75,1,0.75\\n\"\n    network += \"3,0.75,0.5,0.75,1\\n\"\n    network += \"4,0.5,0.625,0.75,0.625\\n\"\n    network += \"5,0.625,0.5,0.625,0.75\\n\"\n    with open(file_name, \"w\") as text_file:\n        text_file.write(network)\n\ndef main(kf, description, mesh_size):\n    mesh_kwargs = {}\n    mesh_kwargs['mesh_size'] = {'mode': 'constant',\n                                'value': mesh_size, 'bound_value': mesh_size}\n\n    domain = {'xmin': 0, 'xmax': 1, 'ymin': 0, 'ymax': 1}\n    if_coarse = True\n\n    folder = 'example_5_1_1_' + description\n\n    file_name = 'network_geiger.csv'\n    write_network(file_name)\n    gb = importer.from_csv(file_name, mesh_kwargs, domain)\n    gb.compute_geometry()\n\n    g_fine = gb.get_grids(lambda g: g.dim == gb.dim_max())[0].copy()\n\n    if if_coarse:\n        partition = co.create_aggregations(gb)\n        partition = co.reorder_partition(partition)\n        co.generate_coarse_grid(gb, partition)\n\n    gb.assign_node_ordering()\n\n    add_data(gb, domain, kf)\n\n    solver = dual.DualVEMMixDim('flow')\n    A, b = solver.matrix_rhs(gb)\n\n    up = sps.linalg.spsolve(A, b)\n    solver.split(gb, \"up\", up)\n\n    gb.add_node_props([\"discharge\", 'pressure', \"P0u\"])\n    solver.extract_u(gb, \"up\", \"discharge\")\n    solver.extract_p(gb, \"up\", 'pressure')\n    solver.project_u(gb, \"discharge\", \"P0u\")\n\n    exporter.export_vtk(\n        gb, 'vem', ['pressure', \"P0u\"], folder=folder, binary=False)\n\n    if if_coarse:\n        partition = partition[gb.grids_of_dimension(gb.dim_max())[0]]\n        p = np.array([d['pressure']\n                      for g, d in gb if g.dim == gb.dim_max()]).ravel()\n        data = {'partition': partition, 'pressure': p[partition]}\n        exporter.export_vtk(g_fine, 'sub_grid', data, binary=False,\n                            folder=folder)\n\n    print(\"diam\", gb.diameter(lambda g: g.dim == gb.dim_max()))\n    print(\"num_cells 2d\", gb.num_cells(lambda g: g.dim == 2))\n    print(\"num_cells 1d\", gb.num_cells(lambda g: g.dim == 1))\n\ndef vem_blocking():\n    kf = 1e-4\n    mesh_size = 0.035 / np.array([1, 2, 4])\n\n    for i in np.arange(mesh_size.size):\n        main(kf, \"blocking_\" + str(i), mesh_size[i])\n\ndef vem_permeable():\n    kf = 1e4\n    mesh_size = 0.035 / np.array([1, 2, 4])\n\n    for i in np.arange(mesh_size.size):\n        main(kf, \"permeable_\" + str(i), mesh_size[i])\n\nvem_blocking()\nvem_permeable()",
  "\n\"\"\n\nimport nplab\nimport nplab.utils.gui \nfrom nplab.instrument.spectrometer import Spectrometer\nfrom nplab.instrument.shutter import Shutter\nfrom nplab.experiment import Experiment, ExperimentStopped\nfrom nplab.utils.notified_property import DumbNotifiedProperty\nfrom nplab.ui.ui_tools import QuickControlBox\nfrom nplab.utils.gui import show_guis\nfrom nplab.utils.gui import QtWidgets, QtCore, QtGui, get_qt_app, uic\nfrom nplab.ui.ui_tools import UiTools\n\nclass DumbIrradiationExperiment(Experiment):\n    \"\"\n    irradiation_time = DumbNotifiedProperty(1.0)\n    wait_time = DumbNotifiedProperty(0.5)\n    log_to_console = True\n    \n    def __init__(self):\n        super(DumbIrradiationExperiment, self).__init__()\n        \n        self.shutter = Shutter.get_instance()\n        self.spectrometer = Spectrometer.get_instance()\n        \n    def run(self):\n        try:\n            dg = self.create_data_group(\"irradiation_%d\")\n            while True:\n                self.log(\"opening shutter\")\n                self.shutter.open_shutter()\n                self.wait_or_stop(self.irradiation_time)\n                self.shutter.close_shutter()\n                self.log(\"closed shutter\")\n                self.wait_or_stop(self.wait_time)\n                spectrum = self.spectrometer.read_spectrum(bundle_metadata=True)\n                dg.create_dataset(\"spectrum_%d\", data=spectrum)\n        except ExperimentStopped:\n            pass \n        finally:\n            self.shutter.close_shutter() \n            \n    def get_qt_ui(self):\n        \"\"\n        gb = QuickControlBox(\"Irradiation Experiment\")\n        gb.add_doublespinbox(\"irradiation_time\")\n        gb.add_doublespinbox(\"wait_time\")\n        gb.add_button(\"start\")\n        gb.add_button(\"stop\")\n        gb.auto_connect_by_name(self)\n        return gb\n\nclass DumbIrradiationExperiment_Gui(QtWidgets.QMainWindow, UiTools):\n    \"\"\n    \n    def __init__(self, spec,shutter, experiment, parent=None):\n        super(DumbIrradiationExperiment_Gui, self).__init__(parent)\n        \n        uic.loadUi('DumbIrradiationExperimentGui.ui', self)\n        \n        self.data_file = nplab.current_datafile()\n        self.data_file_tab = self.replace_widget(self.DataBrowser_tab_layout,self.DataBrowser_widget,self.data_file.get_qt_ui())\n        \n        self.spectrometer = spec\n        self.Spectrometer_widget = self.replace_widget(self.Spectrometer_Layout,self.Spectrometer_widget,self.spectrometer.get_qt_ui(display_only = True))\n        self.spectrometer_tab = self.replace_widget(self.Spectrometer_tab_Layout,self.Spectrometer_tab_widget,self.spectrometer.get_qt_ui())\n        \n        self.Experiment = experiment\n        self.Experiment_controls_widget = self.replace_widget(self.Main_layout,self.Experiment_controls_widget,self.Experiment.get_qt_ui())\n            \n        self.shutter = shutter\n        self.StageControls_widget = self.replace_widget(self.Main_layout,self.shutter_controls_widget,self.shutter.get_qt_ui())\n\nif __name__ == '__main__':\n    from nplab.instrument.spectrometer import DummySpectrometer\n    from nplab.instrument.shutter import DummyShutter    \n    \n    spectrometer = DummySpectrometer()\n    shutter = DummyShutter()\n    \n    experiment = DumbIrradiationExperiment()\n    \n    df = nplab.current_datafile()\n    \n    app = get_qt_app()\n    gui = DumbIrradiationExperiment_Gui(spec = spectrometer, shutter = shutter, experiment = experiment)\n    gui.show()    ",
  "\n\"\"\n\nimport numpy as np\nfrom pyscf.pbc import cc as pbccc\nfrom pyscf.pbc import scf as pbchf\nfrom pyscf.pbc import gto\nfrom pyscf.pbc.tools.pbc import super_cell\n\nnmp = [1, 1, 2]\ncell = gto.M(\n    unit='B',\n    a=[[0., 3.37013733, 3.37013733],\n       [3.37013733, 0., 3.37013733],\n       [3.37013733, 3.37013733, 0.]],\n    mesh=[24,]*3,\n    atom=\"\",\n    basis='gth-szv',\n    pseudo='gth-pade',\n    verbose=4\n)\n\nsupcell = super_cell(cell, nmp)\nmf = pbchf.RHF(supcell)\nmf.kernel()\nsupcell_energy = mf.energy_tot() / np.prod(nmp)\n\nmycc = pbccc.RCCSD(mf)\ngccsd_energy = mycc.ccsd()[0] / np.prod(nmp)\neip, wip = mycc.ipccsd(nroots=2)\neea, wea = mycc.eaccsd(nroots=2)\n\nkpts = cell.make_kpts(nmp)\nkpts -= kpts[0]\nkmf = pbchf.KRHF(cell, kpts)\nkpoint_energy = kmf.kernel()\n\nmykcc = pbccc.KRCCSD(kmf)\nkccsd_energy = mykcc.ccsd()[0]\nekcc = mykcc.ecc\n\nekip, wkip = mykcc.ipccsd(nroots=2, kptlist=[0])\nekea, wkea = mykcc.eaccsd(nroots=2, kptlist=[0])\n\nprint('Difference between gamma/k-point mean-field calculation = %.15g' % (\n    abs(supcell_energy-kpoint_energy)))\nprint('Difference between gamma/k-point ccsd calculation = %.15g' % (\n    abs(gccsd_energy - kccsd_energy)))\nprint('Difference between gamma/k-point ip-eomccsd calculation = %.15g' % (\n    np.linalg.norm(np.array(eip) - np.array(ekip))))\nprint('Difference between gamma/k-point ea-eomccsd calculation = %.15g' % (\n    np.linalg.norm(np.array(eea) - np.array(ekea))))",
  "\"\"\n\n__version__ = '0.1.0'\nfrom docopt import docopt\nargs = docopt(__doc__, version=__version__)\n\nexec('from sirf.' + args['--engine'] + ' import *')\n\nsteps = int(args['--steps'])\nopt = args['--optimal']\nverbose = args['--verbose']\ndata_file = args['--file']\ndata_path = args['--path']\nif data_path is None:\n    data_path = examples_data_path('PET')\nraw_data_file = existing_filepath(data_path, data_file)\nshow_plot = not args['--non-interactive']\n\nif opt:\n    import scipy.optimize\n\ndef trunc(image):\n    arr = image.as_array()\n    arr[arr < 0 ] = 0\n    out = image.copy()\n    out.fill(arr)\n    return out\n\ndef main():\n\n    msg_red = MessageRedirector('info.txt', 'warn.txt', 'errr.txt')\n\n    acq_model = AcquisitionModelUsingRayTracingMatrix()\n\n    print('raw data: %s' % raw_data_file)\n    acq_data = AcquisitionData(raw_data_file)\n\n    filter = TruncateToCylinderProcessor()\n\n    nx = 111\n    ny = 111\n    nz = 31\n    image_size = (nz, ny, nx)\n    voxel_size = (3.375, 3, 3) \n    image = ImageData()\n    image.initialise(image_size, voxel_size)\n    image.fill(1.0)\n    \n    filter.apply(image)\n\n    obj_fun = make_Poisson_loglikelihood(acq_data)\n    obj_fun.set_acquisition_model(acq_model)\n    obj_fun.set_num_subsets(12)\n    obj_fun.set_up(image)\n\n    if show_plot:\n        \n        image.show(20)\n\n    print('computing initial objective function value...')\n    print('objective function value: %e' % (obj_fun.value(image)))\n\n    if verbose:\n        disp = 3\n        if opt:\n            print('NOTE: below f(x) is the negative of the objective function value')\n    else:\n        disp = 0\n    eps = 1e-6 \n\n    for iter in range(steps):\n\n        grad = obj_fun.get_subset_gradient(image, iter % 12)\n        \n        filter.apply(grad)\n\n        if iter == 0:\n            image0 = image\n            grad0 = grad\n            \n            lmd_max = 2*grad.norm()/image.norm()\n            tau = 1/lmd_max\n            maxstep = tau\n        else:\n            di = image - image0\n            dg = grad - grad0 \n            \n            lmd_max = 2*dg.norm()/di.norm()\n            \n            tau = min(maxstep, 1/lmd_max)\n\n        if opt:\n            \n            fun = lambda x: -obj_fun.value(image + x*grad)\n            tau = scipy.optimize.fminbound \\\n                (fun, 0, 2*maxstep, xtol = 1e-4, maxfun = 4, disp = disp)\n\n        print('using step size %f' % tau)\n\n        new_image = trunc(image + tau*grad)\n        diff = new_image - image\n        rc = diff.norm()/image.norm()\n        print('step %d, change in image %e' % (iter, rc))\n        image = new_image\n        \n        filter.apply(image)\n\n        if show_plot:\n            \n            image.show(20)\n\n    if not opt or disp == 0:\n        print('computing attained objective function value...')\n        print('objective function value: %e' % (obj_fun.value(image)))\n\ntry:\n    main()\n    print('\\n=== done with %s' % __file__)\n\nexcept error as err:\n    \n    print('%s' % err.value)",
  "\nr\"\"\n\nimport os\nimport numpy as np\nfrom pathlib import Path\nfrom scipy.optimize import minimize\nfrom simsopt.field import BiotSavart, Current, Coil, apply_symmetries_to_curves, apply_symmetries_to_currents\nfrom simsopt.geo import (curves_to_vtk, create_equally_spaced_curves, create_multifilament_grid,\n                         CurveLength, CurveCurveDistance, SurfaceRZFourier)\nfrom simsopt.objectives import QuadraticPenalty, SquaredFlux\nfrom simsopt.util import in_github_actions\n\nncoils = 4\n\nR0 = 1.00\n\nR1 = 0.70\n\norder = 5\n\nLENGTH_PEN = 1e-2\n\nDIST_MIN = 0.1\nDIST_PEN = 10\n\nnumfilaments_n = 2  \nnumfilaments_b = 3  \ngapsize_n = 0.02  \ngapsize_b = 0.04  \nrot_order = 1  \n\nMAXITER = 50 if in_github_actions else 400\n\nTEST_DIR = (Path(__file__).parent / \"..\" / \"..\" / \"tests\" / \"test_files\").resolve()\nfilename = TEST_DIR / 'input.LandremanPaul2021_QA'\n\nOUT_DIR = \"./output/\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nconfig_str = f\"rot_order_{rot_order}_nfn_{numfilaments_n}_nfb_{numfilaments_b}\"\n\nnphi = 32\nntheta = 32\ns = SurfaceRZFourier.from_vmec_input(filename, range=\"half period\", nphi=nphi, ntheta=ntheta)\n\nnfil = numfilaments_n * numfilaments_b\nbase_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order)\nbase_currents = []\nfor i in range(ncoils):\n    curr = Current(1.)\n    \n    if i == 0:\n        curr.fix_all()\n    base_currents.append(curr * (1e5/nfil))\n\nbase_curves_finite_build = sum([\n    create_multifilament_grid(c, numfilaments_n, numfilaments_b, gapsize_n, gapsize_b, rotation_order=rot_order) for c in base_curves], [])\nbase_currents_finite_build = sum([[c]*nfil for c in base_currents], [])\n\ncurves_fb = apply_symmetries_to_curves(base_curves_finite_build, s.nfp, True)\ncurrents_fb = apply_symmetries_to_currents(base_currents_finite_build, s.nfp, True)\n\ncurves = apply_symmetries_to_curves(base_curves, s.nfp, True)\n\ncoils_fb = [Coil(c, curr) for (c, curr) in zip(curves_fb, currents_fb)]\nbs = BiotSavart(coils_fb)\nbs.set_points(s.gamma().reshape((-1, 3)))\n\ncurves_to_vtk(curves, OUT_DIR + \"curves_init\")\ncurves_to_vtk(curves_fb, OUT_DIR + f\"curves_init_fb_{config_str}\")\n\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + f\"surf_init_fb_{config_str}\", extra_data=pointData)\n\nJf = SquaredFlux(s, bs)\nJls = [CurveLength(c) for c in base_curves]\nJdist = CurveCurveDistance(curves, DIST_MIN)\n\nJF = Jf \\\n    + LENGTH_PEN * sum(QuadraticPenalty(Jls[i], Jls[i].J(), \"max\") for i in range(len(base_curves))) \\\n    + DIST_PEN * Jdist\n\ndef fun(dofs):\n    JF.x = dofs\n    J = JF.J()\n    grad = JF.dJ()\n    cl_string = \", \".join([f\"{J.J():.3f}\" for J in Jls])\n    mean_AbsB = np.mean(bs.AbsB())\n    jf = Jf.J()\n    kap_string = \", \".join(f\"{np.max(c.kappa()):.1f}\" for c in base_curves)\n    print(f\"J={J:.3e}, Jflux={jf:.3e}, sqrt(Jflux)/Mean(|B|)={np.sqrt(jf)/mean_AbsB:.3e}, CoilLengths=[{cl_string}], [{kap_string}], ||∇J||={np.linalg.norm(grad):.3e}\")\n    return 1e-4*J, 1e-4*grad\n\nprint(\"\")\nf = fun\ndofs = JF.x\nnp.random.seed(1)\nh = np.random.uniform(size=dofs.shape)\nJ0, dJ0 = f(dofs)\ndJh = sum(dJ0 * h)\nfor eps in [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]:\n    J1, _ = f(dofs + eps*h)\n    J2, _ = f(dofs - eps*h)\n    print(\"err\", (J1-J2)/(2*eps) - dJh)\n\nprint(\"\")\n\nres = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 400, 'gtol': 1e-20, 'ftol': 1e-20}, tol=1e-20)\n\ncurves_to_vtk(curves_fb, OUT_DIR + f\"curves_opt_fb_{config_str}\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(OUT_DIR + f\"surf_opt_fb_{config_str}\", extra_data=pointData)",
  "\nfrom __future__ import absolute_import, unicode_literals\nimport logging\n\nlogging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\"\"\n\ndef run():\n  import os\n  import DDG4\n  from DDG4 import OutputLevel as Output\n  from g4units import GeV, keV\n\n  kernel = DDG4.Kernel()\n  install_dir = os.environ['DD4hepExamplesINSTALL']\n  kernel.loadGeometry(str(\"file:\" + install_dir + \"/examples/ClientTests/compact/SiliconBlock.xml\"))\n\n  DDG4.importConstants(kernel.detectorDescription(), debug=False)\n  geant4 = DDG4.Geant4(kernel, tracker='Geant4TrackerCombineAction')\n  geant4.printDetectors()\n  \n  geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=False)\n\n  geant4.setupTrackingField(prt=True)\n  \n  prt = DDG4.EventAction(kernel, 'Geant4ParticlePrint/ParticlePrint')\n  prt.OutputLevel = Output.DEBUG\n  prt.OutputType = 3  \n  kernel.eventAction().adopt(prt)\n\n  generator_output_level = Output.INFO\n\n  seq, act = geant4.addDetectorConstruction(\"Geant4DetectorGeometryConstruction/ConstructGeo\")\n  act.DebugMaterials = True\n  act.DebugElements = False\n  act.DebugVolumes = True\n  act.DebugShapes = True\n  act.DebugSurfaces = True\n\n  gun = geant4.setupGun(\"Gun\", particle='gamma', energy=1 * GeV, multiplicity=1)\n  gun.direction = (0.0, 0.0, 1.0)\n  gun.OutputLevel = generator_output_level\n  kernel.NumEvents = 10\n  \n  stepping = DDG4.SteppingAction(kernel, 'TestSteppingAction/MyStepper')\n  kernel.steppingAction().add(stepping)\n\n  part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n  kernel.generatorAction().adopt(part)\n  part.SaveProcesses = ['conv', 'Decay']\n  part.MinimalKineticEnergy = 1 * keV\n  part.KeepAllParticles = False\n  part.PrintEndTracking = True\n  part.enableUI()\n\n  phys = geant4.setupPhysics('QGSP_BERT')\n  phys.dump()\n  \n  geant4.execute()\n\nif __name__ == \"__main__\":\n  run()",
  "import blenderproc as bproc\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('blend_path', nargs='?', default=\"resources/haven/models/ArmChair_01/ArmChair_01.blend\", help=\"Path to the blend file, from the haven dataset, browse the model folder, for all possible options\")\nparser.add_argument('haven_path', nargs='?', default=\"resources/haven\", help=\"The folder where the `hdri` folder can be found, to load an world environment\")\nparser.add_argument('output_dir', nargs='?', default=\"examples/datasets/haven/output\", help=\"Path to where the final files will be saved\")\nargs = parser.parse_args()\n\nbproc.init()\n\nobjs = bproc.loader.load_blend(args.blend_path)\n\nhaven_hdri_path = bproc.loader.get_random_world_background_hdr_img_path_from_haven(args.haven_path)\nbproc.world.set_world_background_hdr_img(haven_hdri_path)\n\nlight = bproc.types.Light()\nlight.set_type(\"POINT\")\nlight.set_location([5, -5, 5])\nlight.set_energy(1000)\n\npoi = bproc.object.compute_poi(objs)\n\nfor i in range(5):\n    \n    location = bproc.sampler.part_sphere([0, 0, 0], radius=3, part_sphere_dir_vector=[1, 0, 0], mode=\"SURFACE\")\n    \n    rotation_matrix = bproc.camera.rotation_from_forward_vec(poi - location)\n    \n    cam2world_matrix = bproc.math.build_transformation_mat(location, rotation_matrix)\n    bproc.camera.add_camera_pose(cam2world_matrix)\n\nbproc.renderer.enable_normals_output()\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\n\ndata = bproc.renderer.render()\n\nbproc.writer.write_hdf5(args.output_dir, data)",
  "\"\"\nimport base64\nimport datetime\nimport json\nimport os\nimport shutil\nfrom getpass import getpass\nfrom pathlib import Path\n\nimport maya\n\nfrom nucypher.blockchain.eth.signers import Signer\nfrom nucypher.characters.lawful import Bob, Alice\nfrom nucypher.policy.payment import SubscriptionManagerPayment\nfrom nucypher.utilities.ethereum import connect_web3_provider\nfrom nucypher.utilities.logging import GlobalLoggerSettings\n\nLOG_LEVEL = 'info'\nGlobalLoggerSettings.set_log_level(log_level_name=LOG_LEVEL)\nGlobalLoggerSettings.start_console_logging()\n\nTEMP_ALICE_DIR = Path('/', 'tmp', 'heartbeat-demo-alice')\nPOLICY_FILENAME = \"policy-metadata.json\"\nshutil.rmtree(TEMP_ALICE_DIR, ignore_errors=True)\n\ntry:\n\n    L1_PROVIDER = os.environ['DEMO_L1_PROVIDER_URI']\n    L2_PROVIDER = os.environ['DEMO_L2_PROVIDER_URI']\n\n    WALLET_FILEPATH = os.environ['DEMO_L2_WALLET_FILEPATH']\n    SIGNER_URI = f'keystore://{WALLET_FILEPATH}'\n\n    ALICE_ADDRESS = os.environ['DEMO_ALICE_ADDRESS']\n\nexcept KeyError:\n    raise RuntimeError('Missing environment variables to run demo.')\n\nL1_NETWORK = 'mainnet'  \nL2_NETWORK = 'polygon'  \n\nconnect_web3_provider(eth_provider_uri=L1_PROVIDER)  \nconnect_web3_provider(eth_provider_uri=L2_PROVIDER)  \n\nwallet = Signer.from_signer_uri(SIGNER_URI)\npassword = os.environ.get('DEMO_ALICE_PASSWORD') or getpass(f\"Enter password to unlock Alice's wallet ({ALICE_ADDRESS[:8]}): \")\nwallet.unlock_account(account=ALICE_ADDRESS, password=password)\n\npayment_method = SubscriptionManagerPayment(\n    network=L2_NETWORK,\n    eth_provider=L2_PROVIDER\n)\n\nalicia = Alice(\n    checksum_address=ALICE_ADDRESS,\n    signer=wallet,\n    domain=L1_NETWORK,\n    eth_provider_uri=L1_PROVIDER,\n    payment_method=payment_method\n)\n\nalice_verifying_key = alicia.stamp.as_umbral_pubkey()\n\nalicia.start_learning_loop(now=True)\n\nlabel = \"heart-data-❤️-\"+os.urandom(4).hex()\nlabel = label.encode()\n\npolicy_pubkey = alicia.get_policy_encrypting_key_from_label(label)\n\nprint(\"The policy public key for \"\n      \"label '{}' is {}\".format(label.decode(\"utf-8\"), bytes(policy_pubkey).hex()))\n\nimport heart_monitor\nheart_monitor.generate_heart_rate_samples(policy_pubkey,\n                                          samples=50,\n                                          save_as_file=True)\n\nfrom doctor_keys import get_doctor_pubkeys\ndoctor_pubkeys = get_doctor_pubkeys()\n\ndoctor_strange = Bob.from_public_keys(verifying_key=doctor_pubkeys['sig'],\n                                      encrypting_key=doctor_pubkeys['enc'],\n                                      federated_only=True)\n\npolicy_end_datetime = maya.now() + datetime.timedelta(days=1)\n\nthreshold, shares = 2, 3\n\nprint(\"Creating access policy for the Doctor...\")\npolicy = alicia.grant(bob=doctor_strange,\n                      label=label,\n                      threshold=threshold,\n                      shares=shares,\n                      expiration=policy_end_datetime)\nprint(\"Done!\")\n\npolicy_info = {\n    \"policy_pubkey\": bytes(policy.public_key).hex(),\n    \"alice_sig_pubkey\": bytes(alicia.stamp).hex(),\n    \"label\": label.decode(\"utf-8\"),\n    \"treasure_map\": base64.b64encode(bytes(policy.treasure_map)).decode()\n}\n\nfilename = POLICY_FILENAME\nwith open(filename, 'w') as f:\n    json.dump(policy_info, f)",
  "\nfrom bitcoinlib.encoding import *\n\nexamples = [\n    ('4c52127a72fb42b82439ab18697dcfcfb96ac63ba8209833b2e29f2302b8993f45e743412d65c7a571da70259d4f6795e98af20e6e'\n     '57603314a662a49c198199', 16, 256),\n    ('LR\u0012zrûB¸$9«\u0018i}ÏÏ¹jÆ;¨ 3²â\n    \n    ('L1odb1uUozbfK2NrsMyhJfvRsxGM2AxixgPL8vG9BUBnE6W1VyTX', 58, 16),\n    ('FF', 16, 10),\n    ('AF', 16, 2),\n    (200, 10, 16, 2),\n    (200, 10, 16, 4),\n    ('thisisfunny', 32, 3),\n    ('1LeNnaRtV52nNtZXvtw6PaGKpk46hU1Xmx', 58, 16),\n    ('1LeNnaRtV52nNtZXvtw6PaGKpk46hU1Xmx', 58, 32),\n    ('1LeNnaRtV52nNtZXvtw6PaGKpk46hU1Xmx', 58, 256),\n    ('1LeNnaRtV52nNtZXvtw6PaGKpk46hU1Xmx', 58, 2048),\n    ([b'\\0', b'\\x12', b'L'], 256, 16, 6),\n    (\"為 宋 暴 治 伯 及 灘 冶 忙 逃 湘 艇 例 讓 忠\", 256, 16),\n    (b'\\x00\\t\\xc6\\xe7\\x11\\x18\\xd8\\xf1+\\xeck\\\\a\\x88K5g|\\n\\n\\xe3*\\x02\\x1f\\x87', 256, 58),\n    (b'\\0', 256, 10),\n    (\"\\x00\\x01\\tfw`\\x06\\x95=UgC\\x9e^9\\xf8j\\r';\\xee\\xd6\\x19g\\xf6\", 256, 58),\n    (b'LR\\x12zr\\xfbB\\xb8$9\\xab\\x18i}\\xcf\\xcf\\xb9j\\xc6;\\xa8 \\x983\\xb2\\xe2\\x9f\n     b'\\xdap%\\x9dOg\\x95\\xe9\\x8a\\xf2\\x0enW`3\\x14\\xa6b\\xa4\\x9c\\x19\\x81\\x99', 256, 16),\n]\n\nprint(\"\\n=== Change base: convert from base N to base M ===\")\nfor example in examples:\n    print(\"\\n>>> change_base%s     \n          (example, example[1], example[2]))\n    print(\"%s\" % change_base(*example))\n\nprint(\"\\n=== Conversion of Bitcoin Addresses to Public Key Hashes ===\")\naddrs = ['1KcBA4i4Qqu1oRjobyWU3R5UXUorLQ3jUg', '1111111111111111111114oLvT2',\n         '1QLbz7JHiBTspS962RLKV8GndWFwi5j6Qr']\nfor addr in addrs:\n    print(\"Public Key Hash of address '%s' is '%s'\" % (addr, addr_to_pubkeyhash(addr, True)))\n\nprint(\"\\n=== From Public Key Hashes to address ===\")\nprint(pubkeyhash_to_addr('13d215d212cd5188ae02c5635faabdc4d7d4ec91'))\nprint(pubkeyhash_to_addr('00' * 20))\n\nprint(\"\\n=== Create PKH from redeemscript ===\")\nredeemscript = '5221023dd6aeaa2acb92cbea35820361e5fd07af10f4b01c985adec30848b424756a6c210381cd2bb2a38d939fa677a5dcc' \\\n               '981ee0630b32b956b2e6dc3e1c028e6d09db5a72103d2c6d31cabe4025c25879010465f501194b352823c553660d303adfa' \\\n               '9a26ad3c53ae'\nprint(to_hexstring(hash160(to_bytes(redeemscript))))\n\nder_signature = '3045022100f952ff1b290c54d8b9fd35573b50f1af235632c595bb2f10b34127fb82f66d18022068b59150f825a81032c' \\\n                '22ce2db091d6fd47294c9e2144fa0291949402e3003ce'\nprint(\"\\n=== Convert DER encoded signature ===\")\nprint(convert_der_sig(to_bytes(der_signature)))\n\nprint(\"\\n=== Varbyte Int conversions ===\")\nprint(\"Number 1000 as Varbyte Integer (hexstring): %s\" % to_hexstring(int_to_varbyteint(1000)))\nprint(\"Converted back (3 is size in bytes: 1 size byte + integer in bytes): \", varbyteint_to_int(to_bytes('fde803')))\n\nprint(\"\\n=== Normalizations ===\")\ndata = [\n    u\"guion cruz envío papel otoño percha hazaña salir joya gorra íntimo actriz\",\n    u'\\u2167',\n    u'\\uFDFA',\n    \"あじわう　ちしき　たわむれる　おくさま　しゃそう　うんこう　ひてい　みほん　たいほ　てのひら　りこう　わかれる　かいすいよく　こもん　ねもと\",\n    '12345',\n]\n\nfor dt in data:\n    print(\"\\nInput data\", dt)\n    print(\"Normalized unicode string (normalize_string): \", normalize_string(dt))\n    print(\"Normalized variable (normalize_var): \", normalize_var(dt))\n\naddress = \"BC1QW508D6QEJXTDG4Y5R3ZARVARY0C5XW7KV8F3T4\"\npkh = \"0014751e76e8199196d454941c45d1b3a323f1433bd6\"\npkh_converted = addr_bech32_to_pubkeyhash(address, prefix='bc', include_witver=True, as_hex=True)\nprint(pkh, \" == \", pkh_converted)\naddr = pubkeyhash_to_addr_bech32(pkh_converted, address[:2].lower())",
  "\n\"\"\n\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport torchvision\n\nfrom avalanche.evaluation.metrics import (\n    forgetting_metrics,\n    accuracy_metrics,\n    loss_metrics,\n    timing_metrics,\n    cpu_usage_metrics,\n    confusion_matrix_metrics,\n    disk_usage_metrics,\n)\nfrom avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\nfrom avalanche.training.plugins import EvaluationPlugin\nfrom avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\nfrom avalanche.training.supervised import Naive\nfrom avalanche.benchmarks.classic.clear import CLEAR, CLEARMetric\n\nDATASET_NAME = \"clear100_cvpr2022\"\nNUM_CLASSES = {\n    \"clear10_neurips_2021\": 11,\n    \"clear100_cvpr2022\": 100,\n    \"clear10\": 11,\n    \"clear100\": 100,\n}\nassert DATASET_NAME in NUM_CLASSES.keys()\n\nEVALUATION_PROTOCOL = \"streaming\"  \n\nROOT = Path(\"..\")\nDATA_ROOT = ROOT / DATASET_NAME\nMODEL_ROOT = ROOT / \"models\"\nDATA_ROOT.mkdir(parents=True, exist_ok=True)\nMODEL_ROOT.mkdir(parents=True, exist_ok=True)\n\nHPARAM = {\n    \"batch_size\": 256,\n    \"num_epoch\": 100,\n    \"step_scheduler_decay\": 30,\n    \"scheduler_step\": 0.1,\n    \"start_lr\": 0.01,\n    \"weight_decay\": 1e-5,\n    \"momentum\": 0.9,\n}\n\ndef make_scheduler(optimizer, step_size, gamma=0.1):\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=step_size, gamma=gamma\n    )\n    return scheduler\n\ndef main():\n    model = torchvision.models.resnet18(pretrained=False)\n\n    normalize = torchvision.transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    )\n    train_transform = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.Resize(224),\n            torchvision.transforms.RandomCrop(224),\n            torchvision.transforms.ToTensor(),\n            normalize,\n        ]\n    )\n    test_transform = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.Resize(224),\n            torchvision.transforms.CenterCrop(224),\n            torchvision.transforms.ToTensor(),\n            normalize,\n        ]\n    )\n\n    tb_logger = TensorboardLogger(ROOT)\n\n    text_logger = TextLogger(open(ROOT / \"log.txt\", \"w+\"))\n\n    interactive_logger = InteractiveLogger()\n\n    eval_plugin = EvaluationPlugin(\n        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        timing_metrics(epoch=True, epoch_running=True),\n        forgetting_metrics(experience=True, stream=True),\n        cpu_usage_metrics(experience=True),\n        confusion_matrix_metrics(\n            num_classes=NUM_CLASSES[DATASET_NAME], save_image=False, stream=True\n        ),\n        disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n        loggers=[interactive_logger, text_logger, tb_logger],\n    )\n\n    if EVALUATION_PROTOCOL == \"streaming\":\n        seed = None\n    else:\n        seed = 0\n\n    benchmark = CLEAR(\n        data_name=DATASET_NAME,\n        evaluation_protocol=EVALUATION_PROTOCOL,\n        feature_type=None,\n        seed=seed,\n        train_transform=train_transform,\n        eval_transform=test_transform,\n        dataset_root=DATA_ROOT,\n    )\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = torch.optim.SGD(\n        model.parameters(),\n        lr=HPARAM[\"start_lr\"],\n        weight_decay=HPARAM[\"weight_decay\"],\n        momentum=HPARAM[\"momentum\"],\n    )\n    scheduler = make_scheduler(\n        optimizer,\n        HPARAM[\"step_scheduler_decay\"],\n        HPARAM[\"scheduler_step\"],\n    )\n\n    plugin_list = [LRSchedulerPlugin(scheduler)]\n    cl_strategy = Naive(\n        model,\n        optimizer,\n        torch.nn.CrossEntropyLoss(),\n        train_mb_size=HPARAM[\"batch_size\"],\n        train_epochs=HPARAM[\"num_epoch\"],\n        eval_mb_size=HPARAM[\"batch_size\"],\n        evaluator=eval_plugin,\n        device=device,\n        plugins=plugin_list,\n    )\n\n    print(\"Starting experiment...\")\n    results = []\n    print(\"Current protocol : \", EVALUATION_PROTOCOL)\n    for index, experience in enumerate(benchmark.train_stream):\n        print(\"Start of experience: \", experience.current_experience)\n        print(\"Current Classes: \", experience.classes_in_this_experience)\n        res = cl_strategy.train(experience)\n        torch.save(\n            model.state_dict(), str(MODEL_ROOT / f\"model{str(index).zfill(2)}.pth\")\n        )\n        print(\"Training completed\")\n        print(\n            \"Computing accuracy on the whole test set with\"\n            f\" {EVALUATION_PROTOCOL} evaluation protocol\"\n        )\n        results.append(cl_strategy.eval(benchmark.test_stream))\n    \n    num_timestamp = len(results)\n    accuracy_matrix = np.zeros((num_timestamp, num_timestamp))\n    for train_idx in range(num_timestamp):\n        for test_idx in range(num_timestamp):\n            accuracy_matrix[train_idx][test_idx] = results[train_idx][\n                f\"Top1_Acc_Stream/eval_phase/test_stream\"\n                f\"/Task00{test_idx}/Exp00{test_idx}\"\n            ]\n    print(\"Accuracy_matrix : \")\n    print(accuracy_matrix)\n    metric = CLEARMetric().get_metrics(accuracy_matrix)\n    print(metric)\n\n    metric_log = open(ROOT / \"metric_log.txt\", \"w+\")\n    metric_log.write(f\"Protocol: {EVALUATION_PROTOCOL} \" f\"Seed: {seed} \")\n    json.dump(accuracy_matrix.tolist(), metric_log, indent=6)\n    json.dump(metric, metric_log, indent=6)\n    metric_log.close()\n\nif __name__ == \"__main__\":\n    main()",
  "import blenderproc as bproc\nimport os\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('scene_net_obj_path', help=\"Path to the used scene net `.obj` file, download via scripts/download_scenenet.py\")\nparser.add_argument('scene_texture_path', help=\"Path to the downloaded texture files, you can find them at http://tinyurl.com/zpc9ppb\")\nparser.add_argument('output_dir', nargs='?', default=\"examples/datasets/scenenet/output\", help=\"Path to where the final files, will be saved\")\nargs = parser.parse_args()\n\nbproc.init()\n\nlabel_mapping = bproc.utility.LabelIdMapping.from_csv(bproc.utility.resolve_resource(os.path.join('id_mappings', 'nyu_idset.csv')))\nobjs = bproc.loader.load_scenenet(args.scene_net_obj_path, args.scene_texture_path, label_mapping)\n\nwalls = bproc.filter.by_cp(objs, \"category_id\", label_mapping.id_from_label(\"wall\"))\n\nnew_floors = bproc.object.extract_floor(walls, new_name_for_object=\"floor\", should_skip_if_object_is_already_there=True)\n\nfor floor in new_floors:\n    floor.set_cp(\"category_id\", label_mapping.id_from_label(\"floor\"))\n\nobjs += new_floors\n\nnew_ceilings = bproc.object.extract_floor(walls, new_name_for_object=\"ceiling\", up_vector_upwards=False, should_skip_if_object_is_already_there=True)\n\nfor ceiling in new_ceilings:\n    ceiling.set_cp(\"category_id\", label_mapping.id_from_label(\"ceiling\"))\n\nobjs += new_ceilings\n\nlamps = bproc.filter.by_attr(objs, \"name\", \".*[l|L]amp.*\", regex=True)\nbproc.lighting.light_surface(lamps, emission_strength=15)\n\nceilings = bproc.filter.by_attr(objs, \"name\", \".*[c|C]eiling.*\", regex=True)\nbproc.lighting.light_surface(ceilings, emission_strength=2, emission_color=[1,1,1,1])\n\nbvh_tree = bproc.object.create_bvh_tree_multi_objects(objs)\n\nfloors = bproc.filter.by_cp(objs, \"category_id\", label_mapping.id_from_label(\"floor\"))\nposes = 0\ntries = 0\nwhile tries < 10000 and poses < 5:\n    tries += 1\n    \n    location = bproc.sampler.upper_region(floors, min_height=1.5, max_height=1.8)\n    \n    _, _, _, _, hit_object, _ = bproc.object.scene_ray_cast(location, [0, 0, -1])\n    if hit_object not in floors:\n        continue\n\n    rotation = np.random.uniform([1.2217, 0, 0], [1.2217, 0, 2 * np.pi])\n    cam2world_matrix = bproc.math.build_transformation_mat(location, rotation)\n\n    if not bproc.camera.perform_obstacle_in_view_check(cam2world_matrix, {\"min\": 1.0}, bvh_tree):\n        continue\n\n    if bproc.camera.scene_coverage_score(cam2world_matrix) < 0.1:\n        continue\n\n    bproc.camera.add_camera_pose(cam2world_matrix)\n    poses += 1\n\nbproc.renderer.enable_normals_output()\nbproc.renderer.enable_depth_output(activate_antialiasing=False)\nbproc.renderer.enable_segmentation_output(map_by=[\"category_id\"])\n\ndata = bproc.renderer.render()\n\nbproc.writer.write_hdf5(args.output_dir, data)",
  "\n\"\"\nfrom psyclone.domain.common.transformations import KernelModuleInlineTrans\nfrom psyclone.domain.lfric import LFRicConstants\nfrom psyclone.dynamo0p3 import DynHaloExchange, DynHaloExchangeStart\nfrom psyclone.psyir.transformations import Matmul2CodeTrans\nfrom psyclone.psyir.nodes import BinaryOperation, Container, KernelSchedule\nfrom psyclone.transformations import Dynamo0p3ColourTrans, \\\n                                     Dynamo0p3OMPLoopTrans, \\\n                                     OMPParallelTrans, \\\n                                     Dynamo0p3RedundantComputationTrans, \\\n                                     Dynamo0p3AsyncHaloExchangeTrans, \\\n                                     MoveTrans, \\\n                                     TransformationError\n\nENABLE_REDUNDANT_COMPUTATION = True\nENABLE_ASYNC_HALOS = True\nENABLE_OMP_COLOURING = True\nENABLE_INTRINSIC_INLINING = True\n\ndef trans(psy):\n    \"\"\n    rtrans = Dynamo0p3RedundantComputationTrans()\n    ctrans = Dynamo0p3ColourTrans()\n    otrans = Dynamo0p3OMPLoopTrans()\n    oregtrans = OMPParallelTrans()\n    inline_trans = KernelModuleInlineTrans()\n    matmul_trans = Matmul2CodeTrans()\n    const = LFRicConstants()\n    ahex_trans = Dynamo0p3AsyncHaloExchangeTrans()\n    mtrans = MoveTrans()\n\n    for invoke in psy.invokes.invoke_list:\n        schedule = invoke.schedule\n\n        if ENABLE_REDUNDANT_COMPUTATION:\n            \n            for loop in schedule.loops():\n                if loop.iteration_space == \"dof\":\n                    if len(loop.kernels()) == 1:\n                        if loop.kernels()[0].name in [\"setval_c\", \"setval_x\"]:\n                            rtrans.apply(loop, options={\"depth\": 1})\n\n        if ENABLE_ASYNC_HALOS:\n            \n            for h_ex in schedule.walk(DynHaloExchange):\n                ahex_trans.apply(h_ex)\n\n            location_cursor = 0\n            for ahex in schedule.walk(DynHaloExchangeStart):\n                if ahex.position <= location_cursor:\n                    continue\n                try:\n                    mtrans.apply(ahex, schedule.children[location_cursor])\n                    location_cursor += 1\n                except TransformationError:\n                    pass\n\n        if ENABLE_OMP_COLOURING:\n            \n            for loop in schedule.loops():\n                if loop.iteration_space == \"cell_column\" \\\n                    and loop.field_space.orig_name \\\n                        not in const.VALID_DISCONTINUOUS_NAMES:\n                    ctrans.apply(loop)\n\n            for loop in schedule.loops():\n                if loop.loop_type not in [\"colours\", \"null\"]:\n                    oregtrans.apply(loop)\n                    otrans.apply(loop, options={\"reprod\": True})\n\n        if ENABLE_INTRINSIC_INLINING:\n            for kernel in schedule.coded_kernels():\n                try:\n                    inline_trans.apply(kernel)\n                except TransformationError:\n                    pass\n\n    if psy.invokes.invoke_list:\n        root = psy.invokes.invoke_list[0].schedule.ancestor(Container)\n        for kschedule in root.walk(KernelSchedule):\n            if ENABLE_INTRINSIC_INLINING:\n                \n                for bop in kschedule.walk(BinaryOperation):\n                    if bop.operator == BinaryOperation.Operator.MATMUL:\n                        try:\n                            matmul_trans.apply(bop)\n                        except TransformationError:\n                            pass\n\n    return psy",
  "\nfrom Lib.RansomwareService import RansomwareClientService, RansomwareService, RansomwareServerFileTemplates\nfrom Lib.TorService import *\n\nfrom seedemu.core.Emulator import *\nfrom seedemu.services.DomainNameService import *\nfrom seedemu.services.DomainNameCachingService import *\nfrom seedemu.core.Binding import Action, Filter, Binding\nfrom seedemu.layers.Base import Base\nfrom seedemu.core.Node import *\nfrom seedemu.compiler.Docker import *\nimport random\nimport os\n\nemu = Emulator()\n\nemu.load('./base-component.bin')\nbase: Base = emu.getLayer(\"Base\")\n\nransomware = RansomwareService()\nransomware.install('ransom-attacker').supportBotnet(False).supportTor(False)\nemu.getVirtualNode('ransom-attacker').setDisplayName('Ransom-Attacker')\nbase.getAutonomousSystem(170).createHost('ransom-attacker').joinNetwork('net0', address='10.170.0.99')\nemu.addBinding(Binding(\"ransom-attacker\", filter=Filter(asn=170, nodeName='ransom-attacker')))\n\nvictim = RansomwareClientService()\n\nfor i in range(1, 17):\n   victim_name =  'victim-{}'.format(i)\n   display_name = 'Ransom-Victim-{}'.format(i)\n   victim.install(victim_name).supportBotnet(False)\n   emu.getVirtualNode(victim_name).setDisplayName(display_name)\n   emu.addBinding(Binding(victim_name, filter=Filter(nodeName=\"host\"), action=Action.RANDOM))\n\ntor = TorService()\n\nvnodes = {\n   \"da-1\":     TorNodeType.DA,\n   \"da-2\":     TorNodeType.DA,\n   \"da-3\":     TorNodeType.DA,\n   \"da-4\":     TorNodeType.DA,\n   \"da-5\":     TorNodeType.DA,\n   \"client-1\": TorNodeType.CLIENT,\n   \"client-2\": TorNodeType.CLIENT,\n   \"relay-1\":  TorNodeType.RELAY,\n   \"relay-2\":  TorNodeType.RELAY,\n   \"relay-3\":  TorNodeType.RELAY,\n   \"relay-4\":  TorNodeType.RELAY,\n   \"exit-1\":   TorNodeType.EXIT,\n   \"exit-2\":   TorNodeType.EXIT,\n   \"hidden-service\": TorNodeType.HS\n}\n\nfor i, (name, nodeType) in enumerate(vnodes.items()):\n   if nodeType == TorNodeType.HS: \n      \n      tor.install(name).setRole(nodeType).linkByVnode(\"ransom-attacker\", [445, 446, 447, 448])\n   else:\n      tor.install(name).setRole(nodeType)\n\n   emu.getVirtualNode(name).setDisplayName(\"Tor-{}\".format(name))\n\nas_list = [150, 151, 152, 153, 154, 160, 161, 162, 163, 164, 170, 171]\nfor i, (name, nodeType) in enumerate(vnodes.items()):\n    \n    asn = random.choice(as_list)\n    emu.addBinding(Binding(name, filter=Filter(asn=asn, nodeName=name), action=Action.NEW))\n\ndns = DomainNameService()\n\ndns.install('root-server').addZone('.')\n\ndns.install('com-server').addZone('com.')\ndns.install('edu-server').addZone('edu.')\n\ndns.install('ns-syr-edu').addZone('syr.edu.')\ndns.install('killswitch').addZone('iuqerfsodp9ifjaposdfjhgosurijfaewrwergwea.com.')\n\ndns.getZone('syr.edu.').addRecord('@ A 128.230.18.63')\n\nemu.getVirtualNode('root-server').setDisplayName('Root')\nemu.getVirtualNode('com-server').setDisplayName('COM')\nemu.getVirtualNode('edu-server').setDisplayName('EDU')\nemu.getVirtualNode('ns-syr-edu').setDisplayName('syr.edu')\nemu.getVirtualNode('killswitch').setDisplayName('killswitch')\n\nemu.addBinding(Binding('root-server', filter=Filter(asn=171), action=Action.NEW))\nemu.addBinding(Binding('com-server', filter=Filter(asn=150), action=Action.NEW))\nemu.addBinding(Binding('edu-server', filter=Filter(asn=152), action=Action.NEW))\nemu.addBinding(Binding('ns-syr-edu', filter=Filter(asn=154), action=Action.NEW))\nemu.addBinding(Binding('killswitch', filter=Filter(asn=161), action=Action.NEW))\n\nldns = DomainNameCachingService()\nldns.install('global-dns')\n\nemu.getVirtualNode('global-dns').setDisplayName('Global DNS')\n\nas153 = base.getAutonomousSystem(153)\nas153.createHost('local-dns').joinNetwork('net0', address = '10.153.0.53')\n\nemu.addBinding(Binding('global-dns', filter=Filter(asn=153, nodeName='local-dns')))\n\nbase.setNameServers(['10.153.0.53'])\n\nemu.addLayer(ldns)\nemu.addLayer(dns)\nemu.addLayer(tor)\nemu.addLayer(ransomware)\nemu.addLayer(victim)\nemu.render()\n\ndocker = Docker()\n\ndocker.addImage(DockerImage('morris-worm-base', [], local = True))\ndocker.addImage(DockerImage('handsonsecurity/seed-ubuntu:large', [], local=False))\n\nvictim_nodes = base.getNodesByName('host')\nfor victim in victim_nodes:\n   docker.setImageOverride(victim, 'morris-worm-base')\n\nattacker_node = base.getNodesByName('ransom-attacker')\ndocker.setImageOverride(attacker_node[0], 'handsonsecurity/seed-ubuntu:large')\n\nemu.compile(docker, './output', override=True)\n\nos.system('cp -r container_files/morris-worm-base ./output')\nos.system('cp -r container_files/z_start.sh ./output')\nos.system('chmod a+x ./output/z_start.sh')",
  "\n\"\"\n\nimport numpy as np\nimport sys\nimport matplotlib.pyplot as plt\n\nfrom fealpy.pde.sfc_2d import SFCModelData1\nfrom fealpy.vem.SFCVEMModel2d import SFCVEMModel2d\nfrom fealpy.mesh import PolygonMesh, HalfEdgePolygonMesh\nfrom fealpy.mesh import TriangleMeshWithInfinityNode\nfrom fealpy.tools import showmultirate\n\nimport pickle\n\nmaxit = int(sys.argv[1])\ntheta = float(sys.argv[2])\nk = int(sys.argv[3])\n\npde = SFCModelData1()\n\nmesh = pde.init_mesh(n=4, meshtype='tri')\nmesh = TriangleMeshWithInfinityNode(mesh)\npnode, pcell, pcellLocation = mesh.to_polygonmesh()\nmesh = PolygonMesh(pnode, pcell, pcellLocation)\nmesh = HalfEdgePolygonMesh.from_polygonmesh(mesh)\n\nerrorType = ['$\\eta$', '$\\Psi_0$', '$\\Psi_1$']\nNdof = np.zeros((maxit,), dtype=np.int)\nerrorMatrix = np.zeros((len(errorType), maxit), dtype=np.float)\ndata = {}\n\nfor i in range(maxit):\n    print(i, \"-th step\")\n    vem = SFCVEMModel2d(pde, mesh, p=1, q=4)\n    if i == 0:\n        vem.solve(rho=0.7, maxit=40000)\n    else:\n        vem.solve(rho=0.9, maxit=40000, uh=data[2*(i-1)], lh=data[2*(i-1)+1])\n\n    data[2*i] = vem.uh\n    data[2*i+1] = vem.lh\n\n    eta = vem.residual_estimator()\n    psi0, psi1 = vem.high_order_term()\n    Ndof[i] = vem.space.number_of_global_dofs()\n    errorMatrix[0, i] = np.sqrt(np.sum(eta**2))\n    errorMatrix[1, i] = np.sqrt(psi0)\n    errorMatrix[2, i] = np.sqrt(psi1)\n\n    fname = sys.argv[1] + 'vem' + str(i) + '.space'\n    f = open(fname, 'wb')\n    pickle.dump([vem, data], f)\n    f.close()\n\n    fname = sys.argv[1] + 'error' + '-' + str(i) + '.data'\n    f = open(fname, 'wb')\n    pickle.dump([Ndof[:i+1], errorMatrix[:, :i+1], errorType], f)\n    f.close()\n\n    fig1 = plt.figure()\n    axes = fig1.gca()\n    mesh.add_plot(axes)\n    plt.savefig(sys.argv[1] + str(i) + '-mesh.png')\n    plt.close()\n\n    if i < maxit - 1:\n        isMarkedCell = mesh.refine_marker(eta, theta, method=\"L2\")\n        mesh.refine(isMarkedCell, data=data)\n\nfname = sys.argv[1] + 'error.data'\nf = open(fname, 'wb')\npickle.dump([Ndof, errorMatrix, errorType], f)\nshowmultirate(plt, k, Ndof, errorMatrix, errorType)\nplt.show()\n\n\"\"",
  "\"\"\nfrom __future__ import print_function\n\nimport io\n\nfrom numpy import pi\n\nfrom bokeh.client import push_session\nfrom bokeh.document import Document\nfrom bokeh.embed import autoload_server\nfrom bokeh.layouts import row, column\nfrom bokeh.models import (Plot, DataRange1d, LinearAxis, CategoricalAxis,\n                          Legend, ColumnDataSource, Grid, Line,\n                          SingleIntervalTicker, Quad, Select, FactorRange)\nfrom bokeh.sampledata.population import load_population\n\ndocument = Document()\n\nsession = push_session(document)\n\ndf = load_population()\nrevision = 2012\n\nyear = 2010\nlocation = \"World\"\n\nyears = [str(x) for x in sorted(df.Year.unique())]\nlocations = sorted(df.Location.unique())\n\nsource_pyramid = ColumnDataSource(data=dict())\n\ndef pyramid():\n    xdr = DataRange1d()\n    ydr = DataRange1d()\n\n    plot = Plot(title=None, x_range=xdr, y_range=ydr, plot_width=600, plot_height=600)\n\n    xaxis = LinearAxis()\n    plot.add_layout(xaxis, 'below')\n    yaxis = LinearAxis(ticker=SingleIntervalTicker(interval=5))\n    plot.add_layout(yaxis, 'left')\n\n    plot.add_layout(Grid(dimension=0, ticker=xaxis.ticker))\n    plot.add_layout(Grid(dimension=1, ticker=yaxis.ticker))\n\n    male_quad = Quad(left=\"male\", right=0, bottom=\"groups\", top=\"shifted\", fill_color=\"\n    male_quad_glyph = plot.add_glyph(source_pyramid, male_quad)\n\n    female_quad = Quad(left=0, right=\"female\", bottom=\"groups\", top=\"shifted\", fill_color=\"\n    female_quad_glyph = plot.add_glyph(source_pyramid, female_quad)\n\n    plot.add_layout(Legend(items=[\n        (\"Male\"   , [male_quad_glyph]),\n        (\"Female\" , [female_quad_glyph]),\n    ]))\n\n    return plot\n\nsource_known = ColumnDataSource(data=dict(x=[], y=[]))\nsource_predicted = ColumnDataSource(data=dict(x=[], y=[]))\n\ndef population():\n    xdr = FactorRange(factors=years)\n    ydr = DataRange1d()\n\n    plot = Plot(title=None, x_range=xdr, y_range=ydr, plot_width=800, plot_height=200)\n\n    plot.add_layout(CategoricalAxis(major_label_orientation=pi/4), 'below')\n\n    line_known = Line(x=\"x\", y=\"y\", line_color=\"violet\", line_width=2)\n    line_known_glyph = plot.add_glyph(source_known, line_known)\n\n    line_predicted = Line(x=\"x\", y=\"y\", line_color=\"violet\", line_width=2, line_dash=\"dashed\")\n    line_predicted_glyph = plot.add_glyph(source_predicted, line_predicted)\n\n    plot.add_layout(Legend(\n        location=\"bottom_right\",\n        items=[\n            (\"known\"     , [line_known_glyph]),\n            (\"predicted\" , [line_predicted_glyph]),\n        ])\n    )\n\n    return plot\n\ndef update_pyramid():\n    pyramid = df[(df.Location == location) & (df.Year == year)]\n\n    male = pyramid[pyramid.Sex == \"Male\"]\n    female = pyramid[pyramid.Sex == \"Female\"]\n\n    total = male.Value.sum() + female.Value.sum()\n\n    male_percent = -male.Value/total\n    female_percent = female.Value/total\n\n    groups = male.AgeGrpStart.tolist()\n    shifted = groups[1:] + [groups[-1] + 5]\n\n    source_pyramid.data = dict(\n        groups=groups,\n        shifted=shifted,\n        male=male_percent,\n        female=female_percent,\n    )\n\ndef update_population():\n    population = df[df.Location == location].groupby(df.Year).Value.sum()\n    aligned_revision = revision//10 * 10\n\n    known = population[population.index <= aligned_revision]\n    predicted = population[population.index >= aligned_revision]\n\n    source_known.data = dict(x=known.index.map(str), y=known.values)\n    source_predicted.data = dict(x=predicted.index.map(str), y=predicted.values)\n\ndef update_data():\n    update_population()\n    update_pyramid()\n\ndef on_year_change(attr, old, new):\n    global year\n    year = int(new)\n    update_data()\n\ndef on_location_change(attr, old, new):\n    global location\n    location = new\n    update_data()\n\ndef create_layout():\n    year_select = Select(title=\"Year:\", value=\"2010\", options=years)\n    location_select = Select(title=\"Location:\", value=\"World\", options=locations)\n\n    year_select.on_change('value', on_year_change)\n    location_select.on_change('value', on_location_change)\n\n    controls = row(children=[year_select, location_select])\n    layout = column(children=[controls, pyramid(), population()])\n\n    return layout\n\nlayout = create_layout()\n\nupdate_data()\n\nhtml = \"\" % autoload_server(layout, session_id=session.id)\n\nwith io.open(\"widget.html\", mode='w+', encoding='utf-8') as f:\n    f.write(html)\n\nprint(__doc__)\n\ndocument.add_root(layout)\n\nif __name__ == \"__main__\":\n    print(\"\\npress ctrl-C to exit\")\n    session.loop_until_closed()",
  "\nimport argparse\n\nimport torch as t\nfrom torch import nn\n\nfrom pipeline import fate_torch_hook\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation\nfrom pipeline.component import HeteroNN\nfrom pipeline.component import Intersection\nfrom pipeline.component import Reader\nfrom pipeline.interface import Data\nfrom pipeline.utils.tools import load_job_config\n\nfate_torch_hook(t)\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    \n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, host=host)\n\n    reader_0 = Reader(name=\"reader_0\")\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    data_transform_0 = DataTransform(name=\"data_transform_0\")\n    data_transform_0.get_party_instance(role='guest', party_id=guest).component_param(with_label=True)\n    data_transform_0.get_party_instance(role='host', party_id=host).component_param(with_label=False)\n\n    intersection_0 = Intersection(name=\"intersection_0\")\n\n    hetero_nn_0 = HeteroNN(name=\"hetero_nn_0\", epochs=5,\n                           interactive_layer_lr=0.01, batch_size=128, validation_freqs=1, task_type='classification',\n                           selector_param={\"method\": \"relative\"})\n    guest_nn_0 = hetero_nn_0.get_party_instance(role='guest', party_id=guest)\n    host_nn_0 = hetero_nn_0.get_party_instance(role='host', party_id=host)\n\n    guest_bottom = t.nn.Sequential(\n        nn.Linear(10, 4),\n        nn.ReLU(),\n        nn.Dropout(p=0.2)\n    )\n\n    guest_top = t.nn.Sequential(\n        nn.Linear(4, 1),\n        nn.Sigmoid()\n    )\n\n    host_bottom = t.nn.Sequential(\n        nn.Linear(20, 4),\n        nn.ReLU(),\n        nn.Dropout(p=0.2)\n    )\n\n    interactive_layer = t.nn.InteractiveLayer(out_dim=4, guest_dim=4, host_dim=4, host_num=1, dropout=0.2)\n\n    guest_nn_0.add_top_model(guest_top)\n    guest_nn_0.add_bottom_model(guest_bottom)\n    host_nn_0.add_bottom_model(host_bottom)\n\n    optimizer = t.optim.Adam(lr=0.01)  \n    loss = t.nn.BCELoss()\n\n    hetero_nn_0.set_interactive_layer(interactive_layer)\n    hetero_nn_0.compile(optimizer=optimizer, loss=loss)\n\n    evaluation_0 = Evaluation(name='eval_0', eval_type='binary')\n\n    pipeline.add_component(reader_0)\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    pipeline.add_component(intersection_0, data=Data(data=data_transform_0.output.data))\n    pipeline.add_component(hetero_nn_0, data=Data(train_data=intersection_0.output.data))\n    pipeline.add_component(evaluation_0, data=Data(data=hetero_nn_0.output.data))\n    pipeline.compile()\n    pipeline.fit()\n\n    print(pipeline.get_component(\"hetero_nn_0\").get_summary())\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()",
  "\nfrom manim import *\n\nclass OpeningManim(Scene):\n    def construct(self):\n        title = Tex(r\"This is some \\LaTeX\")\n        basel = MathTex(r\"\\sum_{n=1}^\\infty \\frac{1}{n^2} = \\frac{\\pi^2}{6}\")\n        VGroup(title, basel).arrange(DOWN)\n        self.play(\n            Write(title),\n            FadeIn(basel, shift=DOWN),\n        )\n        self.wait()\n\n        transform_title = Tex(\"That was a transform\")\n        transform_title.to_corner(UP + LEFT)\n        self.play(\n            Transform(title, transform_title),\n            LaggedStart(*(FadeOut(obj, shift=DOWN) for obj in basel)),\n        )\n        self.wait()\n\n        grid = NumberPlane()\n        grid_title = Tex(\"This is a grid\", font_size=72)\n        grid_title.move_to(transform_title)\n\n        self.add(grid, grid_title)  \n        self.play(\n            FadeOut(title),\n            FadeIn(grid_title, shift=UP),\n            Create(grid, run_time=3, lag_ratio=0.1),\n        )\n        self.wait()\n\n        grid_transform_title = Tex(\n            r\"That was a non-linear function \\\\ applied to the grid\",\n        )\n        grid_transform_title.move_to(grid_title, UL)\n        grid.prepare_for_nonlinear_transform()\n        self.play(\n            grid.animate.apply_function(\n                lambda p: p\n                + np.array(\n                    [\n                        np.sin(p[1]),\n                        np.sin(p[0]),\n                        0,\n                    ],\n                ),\n            ),\n            run_time=3,\n        )\n        self.wait()\n        self.play(Transform(grid_title, grid_transform_title))\n        self.wait()\n\nclass SquareToCircle(Scene):\n    def construct(self):\n        circle = Circle()\n        square = Square()\n        square.flip(RIGHT)\n        square.rotate(-3 * TAU / 8)\n        circle.set_fill(PINK, opacity=0.5)\n\n        self.play(Create(square))\n        self.play(Transform(square, circle))\n        self.play(FadeOut(square))\n\nclass WarpSquare(Scene):\n    def construct(self):\n        square = Square()\n        self.play(\n            ApplyPointwiseFunction(\n                lambda point: complex_to_R3(np.exp(R3_to_complex(point))),\n                square,\n            ),\n        )\n        self.wait()\n\nclass WriteStuff(Scene):\n    def construct(self):\n        example_text = Tex(\"This is a some text\", tex_to_color_map={\"text\": YELLOW})\n        example_tex = MathTex(\n            \"\\\\sum_{k=1}^\\\\infty {1 \\\\over k^2} = {\\\\pi^2 \\\\over 6}\",\n        )\n        group = VGroup(example_text, example_tex)\n        group.arrange(DOWN)\n        group.width = config[\"frame_width\"] - 2 * LARGE_BUFF\n\n        self.play(Write(example_text))\n        self.play(Write(example_tex))\n        self.wait()\n\nclass UpdatersExample(Scene):\n    def construct(self):\n        decimal = DecimalNumber(\n            0,\n            show_ellipsis=True,\n            num_decimal_places=3,\n            include_sign=True,\n        )\n        square = Square().to_edge(UP)\n\n        decimal.add_updater(lambda d: d.next_to(square, RIGHT))\n        decimal.add_updater(lambda d: d.set_value(square.get_center()[1]))\n        self.add(square, decimal)\n        self.play(\n            square.animate.to_edge(DOWN),\n            rate_func=there_and_back,\n            run_time=5,\n        )\n        self.wait()\n\nclass SpiralInExample(Scene):\n    def construct(self):\n        logo_green = \"\n        logo_blue = \"\n        logo_red = \"\n\n        font_color = \"\n\n        pi = MathTex(r\"\\pi\").scale(7).set_color(font_color)\n        pi.shift(2.25 * LEFT + 1.5 * UP)\n\n        circle = Circle(color=logo_green, fill_opacity=0.7, stroke_width=0).shift(LEFT)\n        square = Square(color=logo_blue, fill_opacity=0.8, stroke_width=0).shift(UP)\n        triangle = Triangle(color=logo_red, fill_opacity=0.9, stroke_width=0).shift(\n            RIGHT\n        )\n        pentagon = Polygon(\n            *[\n                [np.cos(2 * np.pi / 5 * i), np.sin(2 * np.pi / 5 * i), 0]\n                for i in range(5)\n            ],\n            color=PURPLE_B,\n            fill_opacity=1,\n            stroke_width=0\n        ).shift(UP + 2 * RIGHT)\n        shapes = VGroup(triangle, square, circle, pentagon, pi)\n        self.play(SpiralIn(shapes, fade_in_fraction=0.9))\n        self.wait()\n        self.play(FadeOut(shapes))\n\nTriangle.set_default(stroke_width=20)\n\nclass LineJoints(Scene):\n    def construct(self):\n        t1 = Triangle()\n        t2 = Triangle(line_join=LineJointType.ROUND)\n        t3 = Triangle(line_join=LineJointType.BEVEL)\n\n        grp = VGroup(t1, t2, t3).arrange(RIGHT)\n        grp.set(width=config.frame_width - 1)\n\n        self.add(grp)\n",
  "\"\"\n\nimport os\nimport tempfile\nimport pyaedt\n\ntmpfold = tempfile.gettempdir()\ntemp_folder = os.path.join(tmpfold, pyaedt.generate_unique_name(\"Example\"))\nif not os.path.exists(temp_folder):\n    os.makedirs(temp_folder)\nprint(temp_folder)\n\ntargetfile = pyaedt.downloads.download_aedb()\nprint(targetfile)\naedt_file = targetfile[:-12] + \"aedt\"\n\nnon_graphical = False\nNewThread = True\n\ndesktopVersion = \"2023.2\"\n\nd = pyaedt.launch_desktop(desktopVersion, non_graphical, NewThread)\nif os.path.exists(aedt_file):\n    os.remove(aedt_file)\nh3d = pyaedt.Hfss3dLayout(targetfile)\nh3d.save_project(os.path.join(temp_folder, \"edb_demo.aedt\"))\n\nh3d.boundaries\n\nh3d.modeler.change_net_visibility(visible=False)\n\nh3d.modeler.change_net_visibility([\"A0_GPIO\", \"A0_MUX\"], visible=True)\nedb = h3d.modeler.edb\nedb.nets.plot([\"A0_GPIO\", \"A0_MUX\"])\n\nfor layer in h3d.modeler.layers.all_signal_layers:\n    layer.is_visible = True\n\nlayer = h3d.modeler.layers.layers[h3d.modeler.layers.layer_id(\"TOP\")]\nlayer.set_layer_color(0, 255, 0)\nh3d.modeler.fit_all()\n\ntop = h3d.modeler.layers.layers[h3d.modeler.layers.layer_id(\"TOP\")]\ntop.is_visible_component = False\n\nbot = h3d.modeler.layers.layers[h3d.modeler.layers.layer_id(\"BOTTOM\")]\nbot.is_visible_component = False\n\nh3d.modeler.fit_all()\n\nh3d.close_project()\nd.release_desktop()",
  "\n\"\"\n\nfrom functools import reduce\nimport numpy\nfrom pyscf.pbc import gto, scf, mp\n\ncell = gto.Cell()\ncell.atom=\"\"\ncell.basis = 'gth-szv'\ncell.pseudo = 'gth-pade'\ncell.a = \"\"\ncell.unit = 'B'\ncell.verbose = 5\ncell.build()\n\nkpts = cell.make_kpts([2,2,2])\nkmf = scf.KRHF(cell)\nkmf.kpts = kpts\nehf = kmf.kernel()\n\nmypt = mp.KMP2(kmf)\nmypt.kernel()\nprint(\"KMP2 energy (per unit cell) =\", mypt.e_tot)\n\nkpts = cell.get_abs_kpts([0.25, 0.25, 0.25])\nkmf = scf.KRHF(cell)\nkmf.kpts = kpts\nehf = kmf.kernel()\n\nmypt = mp.KMP2(kmf)\nmypt.kernel()\nprint(\"KMP2 energy (per unit cell) =\", mypt.e_tot)\n\nkpt = cell.get_abs_kpts([0.25, 0.25, 0.25])\nmf = scf.RHF(cell, kpt=kpt)\nehf = mf.kernel()\n\nmypt = mp.RMP2(mf).run()\nprint(\"RMP2 energy (per unit cell) at k-point =\", mypt.e_tot)\ndm1 = mypt.make_rdm1()\ndm2 = mypt.make_rdm2()\nnmo = mf.mo_coeff.shape[1]\neri_mo = mf.with_df.ao2mo(mf.mo_coeff, kpts=kpt).reshape([nmo]*4)\nh1 = reduce(numpy.dot, (mf.mo_coeff.conj().T, mf.get_hcore(), mf.mo_coeff))\ne_tot = numpy.einsum('ij,ji', h1, dm1) + numpy.einsum('ijkl,jilk', eri_mo, dm2)*.5 + mf.energy_nuc()\nprint(\"RMP2 energy based on MP2 density matrices =\", e_tot.real)\n\nmf = scf.addons.convert_to_uhf(mf)\nmypt = mp.UMP2(mf).run()\nprint(\"UMP2 energy (per unit cell) at k-point =\", mypt.e_tot)\ndm1a, dm1b = mypt.make_rdm1()\ndm2aa, dm2ab, dm2bb = mypt.make_rdm2()\nnmo = dm1a.shape[0]\neri_aa = mf.with_df.ao2mo(mf.mo_coeff[0], kpts=kpt).reshape([nmo]*4)\neri_bb = mf.with_df.ao2mo(mf.mo_coeff[1], kpts=kpt).reshape([nmo]*4)\neri_ab = mf.with_df.ao2mo((mf.mo_coeff[0],mf.mo_coeff[0],mf.mo_coeff[1],mf.mo_coeff[1]), kpts=kpt).reshape([nmo]*4)\nhcore = mf.get_hcore()\nh1a = reduce(numpy.dot, (mf.mo_coeff[0].conj().T, hcore, mf.mo_coeff[0]))\nh1b = reduce(numpy.dot, (mf.mo_coeff[1].conj().T, hcore, mf.mo_coeff[1]))\ne_tot = (numpy.einsum('ij,ji', h1a, dm1a) +\n         numpy.einsum('ij,ji', h1b, dm1b) +\n         numpy.einsum('ijkl,jilk', eri_aa, dm2aa)*.5 +\n         numpy.einsum('ijkl,jilk', eri_ab, dm2ab)    +\n         numpy.einsum('ijkl,jilk', eri_bb, dm2bb)*.5 + mf.energy_nuc())\nprint(\"UMP2 energy based on MP2 density matrices =\", e_tot.real)\n\nmf = scf.addons.convert_to_ghf(mf)\nmypt = mp.GMP2(mf).run()\nprint(\"GMP2 energy (per unit cell) at k-point =\", mypt.e_tot)\ndm1 = mypt.make_rdm1()\ndm2 = mypt.make_rdm2()\nnao = cell.nao_nr()\nnmo = mf.mo_coeff.shape[1]\nmo = mf.mo_coeff[:nao] + mf.mo_coeff[nao:]\neri_mo = mf.with_df.ao2mo(mo, kpts=kpt).reshape([nmo]*4)\norbspin = mf.mo_coeff.orbspin\neri_mo[orbspin[:,None]!=orbspin] = 0\neri_mo[:,:,orbspin[:,None]!=orbspin] = 0\nh1 = reduce(numpy.dot, (mf.mo_coeff.conj().T, mf.get_hcore(), mf.mo_coeff))\ne_tot = numpy.einsum('ij,ji', h1, dm1) + numpy.einsum('ijkl,jilk', eri_mo, dm2)*.5 + mf.energy_nuc()\nprint(\"GMP2 energy based on MP2 density matrices =\", e_tot.real)\n",
  "\nimport copy\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom helpers.supervised_pt_ditto import SupervisedPTDittoHelper\nfrom learners.supervised_monai_prostate_learner import SupervisedMonaiProstateLearner\nfrom monai.losses import DiceLoss\nfrom monai.networks.nets.unet import UNet\n\nfrom nvflare.apis.dxo import DXO, DataKind, MetaKey, from_shareable\nfrom nvflare.apis.fl_constant import ReturnCode\nfrom nvflare.apis.fl_context import FLContext\nfrom nvflare.apis.shareable import Shareable, make_reply\nfrom nvflare.apis.signal import Signal\nfrom nvflare.app_common.app_constant import AppConstants\n\nclass SupervisedMonaiProstateDittoLearner(SupervisedMonaiProstateLearner):\n    def __init__(\n        self,\n        train_config_filename,\n        aggregation_epochs: int = 1,\n        ditto_model_epochs: int = 1,\n        train_task_name: str = AppConstants.TASK_TRAIN,\n    ):\n        \"\"\n        SupervisedMonaiProstateLearner.__init__(\n            self,\n            train_config_filename=train_config_filename,\n            aggregation_epochs=aggregation_epochs,\n            train_task_name=train_task_name,\n        )\n        self.ditto_helper = None\n        self.ditto_model_epochs = ditto_model_epochs\n\n    def train_config(self, fl_ctx: FLContext):\n        \n        SupervisedMonaiProstateLearner.train_config(self, fl_ctx)\n\n        engine = fl_ctx.get_engine()\n        ws = engine.get_workspace()\n        app_dir = ws.get_app_dir(fl_ctx.get_job_id())\n\n        ditto_model = UNet(\n            spatial_dims=2,\n            in_channels=1,\n            out_channels=1,\n            channels=(16, 32, 64, 128, 256),\n            strides=(2, 2, 2, 2),\n            num_res_units=2,\n        ).to(self.device)\n        ditto_optimizer = optim.Adam(ditto_model.parameters(), lr=self.config_info[\"ditto_learning_rate\"])\n        self.ditto_helper = SupervisedPTDittoHelper(\n            criterion=DiceLoss(sigmoid=True),\n            model=ditto_model,\n            optimizer=ditto_optimizer,\n            device=self.device,\n            app_dir=app_dir,\n            ditto_lambda=self.config_info[\"ditto_lambda\"],\n            model_epochs=self.ditto_model_epochs,\n        )\n\n    def train(\n        self,\n        shareable: Shareable,\n        fl_ctx: FLContext,\n        abort_signal: Signal,\n    ) -> Shareable:\n        \"\"\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n\n        current_round = shareable.get_header(AppConstants.CURRENT_ROUND)\n        total_rounds = shareable.get_header(AppConstants.NUM_ROUNDS)\n        self.log_info(fl_ctx, f\"Current/Total Round: {current_round + 1}/{total_rounds}\")\n        self.log_info(fl_ctx, f\"Client identity: {fl_ctx.get_identity_name()}\")\n\n        dxo = from_shareable(shareable)\n        global_weights = dxo.data\n\n        local_var_dict = self.model.state_dict()\n        model_keys = global_weights.keys()\n        for var_name in local_var_dict:\n            if var_name in model_keys:\n                weights = global_weights[var_name]\n                try:\n                    \n                    global_weights[var_name] = np.reshape(weights, local_var_dict[var_name].shape)\n                    \n                    local_var_dict[var_name] = torch.as_tensor(global_weights[var_name])\n                except Exception as e:\n                    raise ValueError(\"Convert weight from {} failed with error: {}\".format(var_name, str(e)))\n        self.model.load_state_dict(local_var_dict)\n\n        self.ditto_helper.load_model(local_var_dict)\n\n        epoch_len = len(self.train_loader)\n        self.log_info(fl_ctx, f\"Local steps per epoch: {epoch_len}\")\n\n        model_global = copy.deepcopy(self.model)\n        for param in model_global.parameters():\n            param.requires_grad = False\n\n        self.local_train(\n            fl_ctx=fl_ctx,\n            train_loader=self.train_loader,\n            model_global=model_global,\n            abort_signal=abort_signal,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n        self.epoch_of_start_time += self.aggregation_epochs\n\n        self.ditto_helper.local_train(\n            train_loader=self.train_loader,\n            model_global=model_global,\n            abort_signal=abort_signal,\n            writer=self.writer,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n\n        metric = self.local_valid(\n            self.ditto_helper.model,\n            self.valid_loader,\n            abort_signal,\n            tb_id=\"val_metric_per_model\",\n            record_epoch=self.ditto_helper.epoch_global,\n        )\n        if abort_signal.triggered:\n            return make_reply(ReturnCode.TASK_ABORTED)\n        self.log_info(fl_ctx, f\"val_metric_per_model: {metric:.4f}\")\n        \n        self.ditto_helper.update_metric_save_model(metric=metric)\n\n        local_weights = self.model.state_dict()\n        model_diff = {}\n        for name in global_weights:\n            if name not in local_weights:\n                continue\n            model_diff[name] = np.subtract(local_weights[name].cpu().numpy(), global_weights[name], dtype=np.float32)\n            if np.any(np.isnan(model_diff[name])):\n                self.system_panic(f\"{name} weights became NaN...\", fl_ctx)\n                return make_reply(ReturnCode.EXECUTION_EXCEPTION)\n\n        self.writer.flush()\n\n        dxo = DXO(data_kind=DataKind.WEIGHT_DIFF, data=model_diff)\n        dxo.set_meta_prop(MetaKey.NUM_STEPS_CURRENT_ROUND, epoch_len)\n\n        self.log_info(fl_ctx, \"Local epochs finished. Returning shareable\")\n        return dxo.to_shareable()",
  "\n\"\"\n\nimport numpy as np\nimport scipy.linalg\nfrom pyscf import gto, scf, tdscf, lib\n\nmolA = gto.M(atom='H 0.5 0.2 0.1; F 0 -0.1 -0.2', basis='ccpvdz')\nmfA = scf.RHF(molA).run()\nmoA = mfA.mo_coeff\no_A = moA[:,mfA.mo_occ!=0]\nv_A = moA[:,mfA.mo_occ==0]\ntdA = mfA.TDA().run()\n\nmolB = gto.M(atom='C 0.9 0.2 0; O 0.1 .2 .1', basis='ccpvtz')\nmfB = scf.RHF(molB).run()\nmoB = mfB.mo_coeff\no_B = moB[:,mfB.mo_occ!=0]\nv_B = moB[:,mfB.mo_occ==0]\ntdB = mfB.TDA().run()\n\nstate_id = 2  \nt1_A = tdA.xy[state_id][0]\nt1_B = tdB.xy[state_id][0]\n\nmolAB = molA + molB\nnaoA = molA.nao\neri = molAB.intor('int2e')\neri_AABB = eri[:naoA,:naoA,naoA:,naoA:]\neri_ABBA = eri[:naoA,naoA:,naoA:,:naoA]\n\neri_iabj = lib.einsum('pqrs,pi,qa,rb,sj->iabj', eri_AABB, o_A, v_A, v_B, o_B)\neri_ijba = lib.einsum('pqrs,pi,qj,rb,sa->ijba', eri_ABBA, o_A, o_B, v_B, v_A)\n\ncJ = np.einsum('iabj,ia,jb->', eri_iabj, t1_A, t1_B)\ncK = np.einsum('ijba,ia,jb->', eri_ijba, t1_A, t1_B)\nprint(cJ * 2 - cK)\n\ndef jk_ints(molA, molB, dm_ia, dm_jb):\n    \"\"\n    from pyscf.scf import jk, _vhf\n    naoA = molA.nao\n    naoB = molB.nao\n    assert(dm_ia.shape == (naoA, naoA))\n    assert(dm_jb.shape == (naoB, naoB))\n\n    molAB = molA + molB\n    vhfopt = _vhf.VHFOpt(molAB, 'int2e', 'CVHFnrs8_prescreen',\n                         'CVHFsetnr_direct_scf',\n                         'CVHFsetnr_direct_scf_dm')\n    dmAB = scipy.linalg.block_diag(dm_ia, dm_jb)\n    \n    vhfopt.set_dm(dmAB, molAB._atm, molAB._bas, molAB._env)\n    \n    vhfopt._dmcondname = None\n    \n    with lib.temporary_env(vhfopt._this.contents,\n                           fprescreen=_vhf._fpointer('CVHFnrs8_vj_prescreen')):\n        shls_slice = (0        , molA.nbas , 0        , molA.nbas,\n                      molA.nbas, molAB.nbas, molA.nbas, molAB.nbas)  \n        vJ = jk.get_jk(molAB, dm_jb, 'ijkl,lk->s2ij', shls_slice=shls_slice,\n                       vhfopt=vhfopt, aosym='s4', hermi=1)\n        cJ = np.einsum('ia,ia->', vJ, dm_ia)\n\n    with lib.temporary_env(vhfopt._this.contents,\n                           fprescreen=_vhf._fpointer('CVHFnrs8_vk_prescreen')):\n        shls_slice = (0        , molA.nbas , molA.nbas, molAB.nbas,\n                      molA.nbas, molAB.nbas, 0        , molA.nbas)  \n        vK = jk.get_jk(molAB, dm_jb, 'ijkl,jk->il', shls_slice=shls_slice,\n                       vhfopt=vhfopt, aosym='s1', hermi=0)\n        cK = np.einsum('ia,ia->', vK, dm_ia)\n\n    return cJ, cK\n\ndef eval_coupling(molA, molB, dmA, dmB, dm_ia, dm_jb, xc=None):\n    \"\"\n    from pyscf import dft\n    from pyscf.scf import jk, _vhf\n    from pyscf.dft import numint\n    molAB = molA + molB\n    naoA = molA.nao\n    naoB = molB.nao\n    nao = naoA + naoB\n    assert(dm_ia.shape == (naoA, naoA))\n    assert(dm_jb.shape == (naoB, naoB))\n\n    vhfopt = _vhf.VHFOpt(molAB, 'int2e', 'CVHFnrs8_prescreen',\n                         'CVHFsetnr_direct_scf',\n                         'CVHFsetnr_direct_scf_dm')\n    dmAB = scipy.linalg.block_diag(dm_ia, dm_jb)\n    \n    vhfopt.set_dm(dmAB, molAB._atm, molAB._bas, molAB._env)\n    \n    vhfopt._dmcondname = None\n    \n    with lib.temporary_env(vhfopt._this.contents,\n                           fprescreen=_vhf._fpointer('CVHFnrs8_vj_prescreen')):\n        shls_slice = (0        , molA.nbas , 0        , molA.nbas,\n                      molA.nbas, molAB.nbas, molA.nbas, molAB.nbas)  \n        vJ = jk.get_jk(molAB, dm_jb, 'ijkl,lk->s2ij', shls_slice=shls_slice,\n                       vhfopt=vhfopt, aosym='s4', hermi=1)\n        cJ = np.einsum('ia,ia->', vJ, dm_ia)\n\n    with lib.temporary_env(vhfopt._this.contents,\n                           fprescreen=_vhf._fpointer('CVHFnrs8_vk_prescreen')):\n        shls_slice = (0        , molA.nbas , molA.nbas, molAB.nbas,\n                      molA.nbas, molAB.nbas, 0        , molA.nbas)  \n        vK = jk.get_jk(molAB, dm_jb, 'ijkl,jk->il', shls_slice=shls_slice,\n                       vhfopt=vhfopt, aosym='s1', hermi=0)\n        cK = np.einsum('ia,ia->', vK, dm_ia)\n\n    if xc is None:  \n        return cJ * 2 - cK\n\n    else:\n        ni = numint.NumInt()\n        omega, alpha, hyb = ni.rsh_and_hybrid_coeff(xc)\n\n        cK *= hyb\n\n        if omega > 1e-10:  \n            with lib.temporary_env(vhfopt._this.contents,\n                                   fprescreen=_vhf._fpointer('CVHFnrs8_vk_prescreen')):\n                with molAB.with_range_coulomb(omega):\n                    vK = jk.get_jk(molAB, dm_jb, 'ijkl,jk->il', shls_slice=shls_slice,\n                                   vhfopt=vhfopt, aosym='s1', hermi=0)\n                cK += np.einsum('ia,ia->', vK, dm_ia) * (alpha - hyb)\n\n    grids = dft.Grids(molAB)\n    xctype = ni._xc_type(xc)\n\n    def make_rhoA(ao, dmA):\n        return ni.eval_rho(molA, ao[...,:naoA], dmA, xctype=xctype)\n    def make_rhoB(ao, dmB):\n        return ni.eval_rho(molB, ao[...,naoA:], dmB, xctype=xctype)\n\n    cXC = 0\n    if xctype == 'LDA':\n        ao_deriv = 0\n        for ao, mask, weight, coords in ni.block_loop(molAB, grids, nao, ao_deriv):\n            \n            rho0 = make_rhoA(ao, dmA) + make_rhoB(ao, dmB)\n            fxc = ni.eval_xc(xc, rho0, 0, deriv=2)[2]\n            frr = fxc[0]\n\n            rhoA = make_rhoA(ao, dm_ia)\n            rhoB = make_rhoB(ao, dm_jb)\n            cXC += np.einsum('i,i,i,i->', weight, frr, rhoA, rhoB)\n\n    elif xctype == 'GGA':\n        ao_deriv = 1\n        for ao, mask, weight, coords in ni.block_loop(molAB, grids, nao, ao_deriv):\n            \n            rho0 = make_rhoA(ao, dmA) + make_rhoB(ao, dmB)\n            vxc, fxc = ni.eval_xc(xc, rho0, 0, deriv=2)[1:3]\n            vgamma = vxc[1]\n            frho, frhogamma, fgg = fxc[:3]\n\n            rhoA = make_rhoA(ao, dm_ia)\n            rhoB = make_rhoB(ao, dm_jb)\n            sigmaA = np.einsum('xi,xi->i', rho0[1:4], rhoA[1:4])\n            sigmaB = np.einsum('xi,xi->i', rho0[1:4], rhoB[1:4])\n            cXC += np.einsum('i,i,i,i->', weight, frho, rhoA[0], rhoB[0])\n            cXC += np.einsum('i,i,i,i->', weight, frhogamma, sigmaA, rhoB[0]) * 2\n            cXC += np.einsum('i,i,i,i->', weight, frhogamma, sigmaB, rhoA[0]) * 2\n            cXC += np.einsum('i,i,i,i->', weight, fgg, sigmaA, sigmaB) * 4\n            cXC += np.einsum('i,i,xi,xi->', weight, vgamma, rhoA[1:4], rhoB[1:4]) * 2\n\n    return cJ * 2 - cK + cXC\n\ndm_ia = o_A.dot(t1_A).dot(v_A.T)\ndm_jb = o_B.dot(t1_B).dot(v_B.T)\ncJ, cK = jk_ints(molA, molB, dm_ia, dm_jb)\nprint(cJ * 2 - cK)\n\nprint(eval_coupling(molA, molB, mfA.make_rdm1(), mfB.make_rdm1(), dm_ia, dm_jb))\nprint(eval_coupling(molA, molB, mfA.make_rdm1(), mfB.make_rdm1(), dm_ia, dm_jb, 'b3lyp'))",
  "\ndef zoom_in(graph):\n    \"\"\n    zoom = graph.get_zoom() + 0.1\n    graph.set_zoom(zoom)\n\ndef zoom_out(graph):\n    \"\"\n    zoom = graph.get_zoom() - 0.2\n    graph.set_zoom(zoom)\n\ndef reset_zoom(graph):\n    \"\"\n    graph.reset_zoom()\n\ndef layout_h_mode(graph):\n    \"\"\n    graph.set_layout_direction(0)\n\ndef layout_v_mode(graph):\n    \"\"\n    graph.set_layout_direction(1)\n\ndef open_session(graph):\n    \"\"\n    current = graph.current_session()\n    file_path = graph.load_dialog(current)\n    if file_path:\n        graph.load_session(file_path)\n\ndef import_session(graph):\n    \"\"\n    current = graph.current_session()\n    file_path = graph.load_dialog(current)\n    if file_path:\n        graph.import_session(file_path)\n\ndef save_session(graph):\n    \"\"\n    current = graph.current_session()\n    if current:\n        graph.save_session(current)\n        msg = 'Session layout saved:\\n{}'.format(current)\n        viewer = graph.viewer()\n        viewer.message_dialog(msg, title='Session Saved')\n    else:\n        save_session_as(graph)\n\ndef save_session_as(graph):\n    \"\"\n    current = graph.current_session()\n    file_path = graph.save_dialog(current)\n    if file_path:\n        graph.save_session(file_path)\n\ndef clear_session(graph):\n    \"\"\n    if graph.question_dialog('Clear Current Session?', 'Clear Session'):\n        graph.clear_session()\n\ndef clear_undo(graph):\n    \"\"\n    viewer = graph.viewer()\n    msg = 'Clear all undo history, Are you sure?'\n    if viewer.question_dialog('Clear Undo History', msg):\n        graph.clear_undo_stack()\n\ndef copy_nodes(graph):\n    \"\"\n    graph.copy_nodes()\n\ndef cut_nodes(graph):\n    \"\"\n    graph.cut_nodes()\n\ndef paste_nodes(graph):\n    \"\"\n    graph.paste_nodes()\n\ndef delete_nodes(graph):\n    \"\"\n    graph.delete_nodes(graph.selected_nodes())\n\ndef extract_nodes(graph):\n    \"\"\n    graph.extract_nodes(graph.selected_nodes())\n\ndef clear_node_connections(graph):\n    \"\"\n    graph.undo_stack().beginMacro('clear selected node connections')\n    for node in graph.selected_nodes():\n        for port in node.input_ports() + node.output_ports():\n            port.clear_connections()\n    graph.undo_stack().endMacro()\n\ndef select_all_nodes(graph):\n    \"\"\n    graph.select_all()\n\ndef clear_node_selection(graph):\n    \"\"\n    graph.clear_selection()\n\ndef invert_node_selection(graph):\n    \"\"\n    graph.invert_selection()\n\ndef disable_nodes(graph):\n    \"\"\n    graph.disable_nodes(graph.selected_nodes())\n\ndef duplicate_nodes(graph):\n    \"\"\n    graph.duplicate_nodes(graph.selected_nodes())\n\ndef expand_group_node(graph):\n    \"\"\n    selected_nodes = graph.selected_nodes()\n    if not selected_nodes:\n        graph.message_dialog('Please select a \"GroupNode\" to expand.')\n        return\n    graph.expand_group_node(selected_nodes[0])\n\ndef fit_to_selection(graph):\n    \"\"\n    graph.fit_to_selection()\n\ndef show_undo_view(graph):\n    \"\"\n    graph.undo_view.show()\n\ndef curved_pipe(graph):\n    \"\"\n    from NodeGraphQt.constants import PipeLayoutEnum\n    graph.set_pipe_style(PipeLayoutEnum.CURVED.value)\n\ndef straight_pipe(graph):\n    \"\"\n    from NodeGraphQt.constants import PipeLayoutEnum\n    graph.set_pipe_style(PipeLayoutEnum.STRAIGHT.value)\n\ndef angle_pipe(graph):\n    \"\"\n    from NodeGraphQt.constants import PipeLayoutEnum\n    graph.set_pipe_style(PipeLayoutEnum.ANGLE.value)\n\ndef bg_grid_none(graph):\n    \"\"\n    from NodeGraphQt.constants import ViewerEnum\n    graph.set_grid_mode(ViewerEnum.GRID_DISPLAY_NONE.value)\n\ndef bg_grid_dots(graph):\n    \"\"\n    from NodeGraphQt.constants import ViewerEnum\n    graph.set_grid_mode(ViewerEnum.GRID_DISPLAY_DOTS.value)\n\ndef bg_grid_lines(graph):\n    \"\"\n    from NodeGraphQt.constants import ViewerEnum\n    graph.set_grid_mode(ViewerEnum.GRID_DISPLAY_LINES.value)\n\ndef layout_graph_down(graph):\n    \"\"\n    nodes = graph.selected_nodes() or graph.all_nodes()\n    graph.auto_layout_nodes(nodes=nodes, down_stream=True)\n\ndef layout_graph_up(graph):\n    \"\"\n    nodes = graph.selected_nodes() or graph.all_nodes()\n    graph.auto_layout_nodes(nodes=nodes, down_stream=False)\n\ndef toggle_node_search(graph):\n    \"\"\n    graph.toggle_node_search()",
  "\nimport select\nimport socket\nimport time\nimport sys\n\nfrom impacket import ImpactDecoder, IP6, ICMP6, version\n\nprint(version.BANNER)\n\nif len(sys.argv) < 3:\n    print(\"Use: %s <src ip> <dst ip>\" % sys.argv[0])\n    sys.exit(1)\n\nsrc = sys.argv[1]\ndst = sys.argv[2]\n\nip = IP6.IP6()\nip.set_ip_src(src)\nip.set_ip_dst(dst)\nip.set_traffic_class(0)\nip.set_flow_label(0)\nip.set_hop_limit(64)\n\ns = socket.socket(socket.AF_INET6, socket.SOCK_RAW, socket.IPPROTO_ICMPV6)\n\npayload = b\"A\"*156\n\nprint(\"PING %s %d data bytes\" % (dst, len(payload)))\nseq_id = 0\nwhile 1:\n    \n    seq_id += 1\n    icmp = ICMP6.ICMP6.Echo_Request(1, seq_id, payload)\n\n    ip.contains(icmp)\n    ip.set_next_header(ip.child().get_ip_protocol_number())\n    ip.set_payload_length(ip.child().get_size())\n    icmp.calculate_checksum()\n\n    s.sendto(icmp.get_packet(), (dst, 0))\n\n    if s in select.select([s], [], [], 1)[0]:\n        reply = s.recvfrom(2000)[0]\n\n        rip = ImpactDecoder.ICMP6Decoder().decode(reply)\n\n        if ICMP6.ICMP6.ECHO_REPLY == rip.get_type():\n            print(\"%d bytes from %s: icmp_seq=%d \" % (rip.child().get_size()-4, dst, rip.get_echo_sequence_number()))\n\n        time.sleep(1)",
  "\nimport sys\nimport json\nimport uc_funcs as uc\n\nimport mpisppy.utils.sputils as sputils\nfrom mpisppy.spin_the_wheel import WheelSpinner\n\nfrom mpisppy.extensions.extension import MultiExtension\nfrom mpisppy.extensions.fixer import Fixer\nfrom mpisppy.extensions.mipgapper import Gapper\nfrom mpisppy.extensions.xhatclosest import XhatClosest\nfrom mpisppy.utils import config\nimport mpisppy.utils.cfg_vanilla as vanilla\nfrom mpisppy.extensions.cross_scen_extension import CrossScenarioExtension\n\ndef _parse_args():\n    cfg = config.Config()\n    cfg.popular_args()\n    cfg.num_scens_required()\n    cfg.ph_args()\n    cfg.two_sided_args()\n    cfg.aph_args()\n    cfg.fixer_args()\n    cfg.fwph_args()\n    cfg.lagrangian_args()\n    cfg.xhatlooper_args()\n    cfg.xhatshuffle_args()\n    cfg.cross_scenario_cuts_args()\n    cfg.add_to_config(\"ph_mipgaps_json\",\n                         description=\"json file with mipgap schedule (default None)\",\n                         domain=str,\n                         default=None)\n    cfg.add_to_config(\"solution_dir\",\n                         description=\"writes a tree solution to the provided directory\"\n                                      \" (default None)\",\n                         domain=str,\n                         default=None)\n    cfg.add_to_config(\"xhat_closest_tree\",\n                         description=\"Uses XhatClosest to compute a tree solution after\"\n                                     \" PH termination (default False)\",\n                         domain=bool,\n                         default=False)\n    cfg.add_to_config(\"run_aph\",\n                         description=\"Run with async projective hedging instead of progressive hedging\",\n                         domain=bool,\n                         default=False)\n    cfg.parse_command_line(\"uc_cylinders\")\n    return cfg\n\ndef main():\n\n    cfg = _parse_args()\n\n    num_scen = cfg.num_scens\n\n    fwph = cfg.fwph\n    xhatlooper = cfg.xhatlooper\n    xhatshuffle = cfg.xhatshuffle\n    lagrangian = cfg.lagrangian\n    fixer = cfg.fixer\n    fixer_tol = cfg.fixer_tol\n    cross_scenario_cuts = cfg.cross_scenario_cuts\n\n    scensavail = [3,5,10,25,50,100]\n    if num_scen not in scensavail:\n        raise RuntimeError(\"num-scen was {}, but must be in {}\".\\\n                           format(num_scen, scensavail))\n\n    scenario_creator_kwargs = {\n        \"scenario_count\": num_scen,\n        \"path\": str(num_scen) + \"scenarios_r1\",\n    }\n    scenario_creator = uc.scenario_creator\n    scenario_denouement = uc.scenario_denouement\n    all_scenario_names = [f\"Scenario{i+1}\" for i in range(num_scen)]\n    rho_setter = uc._rho_setter\n\n    beans = (cfg, scenario_creator, scenario_denouement, all_scenario_names)\n\n    if cfg.run_aph:\n        hub_dict = vanilla.aph_hub(*beans,\n                                   scenario_creator_kwargs=scenario_creator_kwargs,\n                                   ph_extensions=MultiExtension,\n                                   rho_setter = rho_setter)\n    else:\n        hub_dict = vanilla.ph_hub(*beans,\n                                  scenario_creator_kwargs=scenario_creator_kwargs,\n                                  ph_extensions=MultiExtension,\n                                  rho_setter = rho_setter)\n\n    ext_classes =  [Gapper]\n    if fixer:\n        ext_classes.append(Fixer)\n    if cross_scenario_cuts:\n        ext_classes.append(CrossScenarioExtension)\n    if cfg.xhat_closest_tree:\n        ext_classes.append(XhatClosest)\n\n    hub_dict[\"opt_kwargs\"][\"extension_kwargs\"] = {\"ext_classes\" : ext_classes}\n    if cross_scenario_cuts:\n        hub_dict[\"opt_kwargs\"][\"options\"][\"cross_scen_options\"]\\\n            = {\"check_bound_improve_iterations\" : cfg.cross_scenario_iter_cnt}\n\n    if fixer:\n        hub_dict[\"opt_kwargs\"][\"options\"][\"fixeroptions\"] = {\n            \"verbose\": cfg.verbose,\n            \"boundtol\": fixer_tol,\n            \"id_fix_list_fct\": uc.id_fix_list_fct,\n        }\n    if cfg.xhat_closest_tree:\n        hub_dict[\"opt_kwargs\"][\"options\"][\"xhat_closest_options\"] = {\n            \"xhat_solver_options\" : dict(),\n            \"keep_solution\" : True\n        }\n\n    if cfg.ph_mipgaps_json is not None:\n        with open(cfg.ph_mipgaps_json) as fin:\n            din = json.load(fin)\n        mipgapdict = {int(i): din[i] for i in din}\n    else:\n        mipgapdict = None\n    hub_dict[\"opt_kwargs\"][\"options\"][\"gapperoptions\"] = {\n        \"verbose\": cfg.verbose,\n        \"mipgapdict\": mipgapdict\n        }\n\n    if cfg.default_rho is None:\n        \n        hub_dict[\"opt_kwargs\"][\"options\"][\"defaultPHrho\"] = 1\n    \n    if fwph:\n        fw_spoke = vanilla.fwph_spoke(*beans, scenario_creator_kwargs=scenario_creator_kwargs)\n\n    if lagrangian:\n        lagrangian_spoke = vanilla.lagrangian_spoke(*beans,\n                                              scenario_creator_kwargs=scenario_creator_kwargs,\n                                              rho_setter = rho_setter)\n\n    if xhatlooper:\n        xhatlooper_spoke = vanilla.xhatlooper_spoke(*beans, scenario_creator_kwargs=scenario_creator_kwargs)\n\n    if xhatshuffle:\n        xhatshuffle_spoke = vanilla.xhatshuffle_spoke(*beans, scenario_creator_kwargs=scenario_creator_kwargs)\n\n    if cross_scenario_cuts:\n        cross_scenario_cuts_spoke = vanilla.cross_scenario_cuts_spoke(*beans, scenario_creator_kwargs=scenario_creator_kwargs)\n\n    list_of_spoke_dict = list()\n    if fwph:\n        list_of_spoke_dict.append(fw_spoke)\n    if lagrangian:\n        list_of_spoke_dict.append(lagrangian_spoke)\n    if xhatlooper:\n        list_of_spoke_dict.append(xhatlooper_spoke)\n    if xhatshuffle:\n        list_of_spoke_dict.append(xhatshuffle_spoke)\n    if cross_scenario_cuts:\n        list_of_spoke_dict.append(cross_scenario_cuts_spoke)\n\n    wheel = WheelSpinner(hub_dict, list_of_spoke_dict)\n    wheel.spin()\n\n    if cfg.solution_dir is not None:\n        wheel.write_tree_solution(cfg.solution_dir, uc.scenario_tree_solution_writer)\n\n    wheel.write_first_stage_solution('uc_cyl_nonants.npy',\n            first_stage_solution_writer=sputils.first_stage_nonant_npy_serializer)\n\nif __name__ == \"__main__\":\n    main()",
  "from __future__ import (absolute_import, division, print_function)\n\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nbmap = Basemap(projection='ortho',lat_0=45,lon_0=-100,resolution='l')\n\nbmap.drawcoastlines(linewidth=0.25)\nbmap.drawcountries(linewidth=0.25)\nbmap.fillcontinents(color='coral',lake_color='aqua')\n\nbmap.drawmapboundary(fill_color='aqua')\n\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n\nlats=[40.02,32.73,38.55,48.25,17.29]\nlons=[-105.16,-117.16,-77.00,-114.21,-88.10]\ncities=['Boulder, CO','San Diego, CA',\n        'Washington, DC','Whitefish, MT','Belize City, Belize']\n\nxc,yc = bmap(lons,lats)\n\nbmap.plot(xc,yc,'bo')\n\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9)\n\nnlats = 73; nlons = 145; delta = 2.*np.pi/(nlons-1)\nlats = (0.5*np.pi-delta*np.indices((nlats,nlons))[0,:,:])\nlons = (delta*np.indices((nlats,nlons))[1,:,:])\nwave = 0.75*(np.sin(2.*lats)**8*np.cos(4.*lons))\nmean = 0.5*np.cos(2.*lats)*((np.sin(2.*lats))**2 + 2.)\n\nx, y = bmap(lons*180./np.pi, lats*180./np.pi)\n\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('filled continent background')\n\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n\nbmap.plot(xc,yc,'wo')\n\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('land-sea mask background')\nbmap.drawlsmask(ocean_color='aqua',land_color='coral')\n\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n\nbmap.plot(xc,yc,'wo')\n\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('blue marble background')\nbmap.bluemarble()\n\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n\nbmap.plot(xc,yc,'wo')\n\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('shaded relief background')\nbmap.shadedrelief()\n\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n\nbmap.plot(xc,yc,'wo')\n\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('etopo background')\nbmap.etopo()\n\nfig = plt.figure()\nbmap.drawmapboundary()\nbmap.drawmeridians(np.arange(0,360,30))\nbmap.drawparallels(np.arange(-90,90,30))\n\nbmap.plot(xc,yc,'wo')\n\nfor name,xpt,ypt in zip(cities,xc,yc):\n    plt.text(xpt+50000,ypt+50000,name,fontsize=9,color='w')\n\ncs = bmap.contour(x,y,wave+mean,15,linewidths=1.5)\nplt.title('etopo background with oceans masked')\nbmap.etopo()\nbmap.drawlsmask(ocean_color='DarkBlue',land_color=(255,255,255,1))\n\nplt.show()",
  "\nfrom __future__ import print_function\nfrom bcc import BPF\nfrom bcc.libbcc import lib, bcc_symbol, bcc_symbol_option\n\nimport ctypes as ct\nimport sys\nimport time\n\nNAME = 'c'\nSYMBOL = 'strlen'\nSTT_GNU_IFUNC = 1 << 10\n\nHIST_BPF_TEXT = \"\"\n\nSUBMIT_FUNC_ADDR_BPF_TEXT = \"\"\n\ndef get_indirect_function_sym(module, symname):\n    sym = bcc_symbol()\n    sym_op = bcc_symbol_option()\n    sym_op.use_debug_file = 1\n    sym_op.check_debug_file_crc = 1\n    sym_op.lazy_symbolize = 1\n    sym_op.use_symbol_type = STT_GNU_IFUNC\n    if lib.bcc_resolve_symname(\n            module.encode(),\n            symname.encode(),\n            0x0,\n            0,\n            ct.byref(sym_op),\n            ct.byref(sym),\n    ) < 0:\n        return None\n    else:\n        return sym\n\ndef set_impl_func_addr(cpu, data, size):\n    addr = ct.cast(data, ct.POINTER(ct.c_uint64)).contents.value\n    global impl_func_addr\n    impl_func_addr = addr\n\ndef set_resolv_func_addr(cpu, data, size):\n    addr = ct.cast(data, ct.POINTER(ct.c_uint64)).contents.value\n    global resolv_func_addr\n    resolv_func_addr = addr\n\ndef find_impl_func_offset(ifunc_symbol):\n    b = BPF(text=SUBMIT_FUNC_ADDR_BPF_TEXT)\n    b.attach_uprobe(name=NAME, sym=SYMBOL, fn_name=b'submit_resolv_func_addr')\n    b['resolv_func_addr'].open_perf_buffer(set_resolv_func_addr)\n    b.attach_uretprobe(name=NAME, sym=SYMBOL, fn_name=b\"submit_impl_func_addr\")\n    b['impl_func_addr'].open_perf_buffer(set_impl_func_addr)\n\n    print('wait for the first {} call'.format(SYMBOL))\n    while True:\n        try:\n            if resolv_func_addr and impl_func_addr:\n                b.detach_uprobe(name=NAME, sym=SYMBOL)\n                b.detach_uretprobe(name=NAME, sym=SYMBOL)\n                b.cleanup()\n                break\n            b.perf_buffer_poll()\n        except KeyboardInterrupt:\n            exit()\n    print('IFUNC resolution of {} is performed'.format(SYMBOL))\n    print('resolver function address: {:\n    print('resolver function offset: {:\n    print('function implementation address: {:\n    impl_func_offset = impl_func_addr - resolv_func_addr + ifunc_symbol.offset\n    print('function implementation offset: {:\n    return impl_func_offset\n\ndef main():\n    ifunc_symbol = get_indirect_function_sym(NAME, SYMBOL)\n    if not ifunc_symbol:\n        sys.stderr.write('{} is not an indirect function. abort!\\n'.format(SYMBOL))\n        exit(1)\n\n    impl_func_offset = find_impl_func_offset(ifunc_symbol)\n\n    b = BPF(text=HIST_BPF_TEXT)\n    b.attach_uretprobe(name=ct.cast(ifunc_symbol.module, ct.c_char_p).value,\n                       addr=impl_func_offset,\n                       fn_name=b'count')\n    dist = b['dist']\n    try:\n        while True:\n            time.sleep(1)\n            print('%-8s\\n' % time.strftime('%H:%M:%S'), end='')\n            dist.print_log2_hist(SYMBOL + ' return:')\n            dist.clear()\n\n    except KeyboardInterrupt:\n        pass\n\nresolv_func_addr = 0\nimpl_func_addr = 0\n\nmain()",
  "\nimport argparse\n\nimport torch as t\nfrom torch import nn\n\nfrom pipeline import fate_torch_hook\nfrom pipeline.backend.pipeline import PipeLine\nfrom pipeline.component import DataTransform\nfrom pipeline.component import Evaluation\nfrom pipeline.component import HeteroNN\nfrom pipeline.component import Intersection\nfrom pipeline.component import Reader\nfrom pipeline.interface import Data\nfrom pipeline.utils.tools import load_job_config\n\nfate_torch_hook(t)\n\ndef main(config=\"../../config.yaml\", namespace=\"\"):\n    \n    if isinstance(config, str):\n        config = load_job_config(config)\n    parties = config.parties\n    guest = parties.guest[0]\n    host = parties.host[0]\n\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n    guest_val_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_val_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, host=host)\n\n    reader_0 = Reader(name=\"reader_0\")\n    reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n    reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role='guest', party_id=guest).component_param(table=guest_val_data)\n    reader_1.get_party_instance(role='host', party_id=host).component_param(table=host_val_data)\n\n    data_transform_0 = DataTransform(name=\"data_transform_0\")\n    data_transform_0.get_party_instance(role='guest', party_id=guest).component_param(with_label=True)\n    data_transform_0.get_party_instance(role='host', party_id=host).component_param(with_label=False)\n\n    data_transform_1 = DataTransform(name=\"data_transform_1\")\n    data_transform_1.get_party_instance(role='guest', party_id=guest).component_param(with_label=True)\n    data_transform_1.get_party_instance(role='host', party_id=host).component_param(with_label=False)\n\n    intersection_0 = Intersection(name=\"intersection_0\")\n    intersection_1 = Intersection(name=\"intersection_1\")\n\n    hetero_nn_0 = HeteroNN(name=\"hetero_nn_0\", epochs=100,\n                           interactive_layer_lr=0.01, batch_size=-1, task_type='classification',\n                           callback_param={\n                               \"callbacks\": [\"EarlyStopping\"],\n                               \"validation_freqs\": 1,\n                               \"early_stopping_rounds\": 2,\n                               \"use_first_metric_only\": True,\n                               \"metrics\": [\"AUC\"]\n                           }\n                           )\n    guest_nn_0 = hetero_nn_0.get_party_instance(role='guest', party_id=guest)\n    host_nn_0 = hetero_nn_0.get_party_instance(role='host', party_id=host)\n\n    guest_bottom = t.nn.Sequential(\n        nn.Linear(10, 2),\n        nn.ReLU()\n    )\n\n    guest_top = t.nn.Sequential(\n        nn.Linear(2, 1),\n        nn.Sigmoid()\n    )\n\n    host_bottom = t.nn.Sequential(\n        nn.Linear(20, 2),\n        nn.ReLU()\n    )\n\n    interactive_layer = t.nn.InteractiveLayer(out_dim=2, guest_dim=2, host_dim=2, host_num=1)\n\n    guest_nn_0.add_top_model(guest_top)\n    guest_nn_0.add_bottom_model(guest_bottom)\n    host_nn_0.add_bottom_model(host_bottom)\n\n    optimizer = t.optim.Adam(lr=0.01)  \n    loss = t.nn.BCELoss()\n\n    hetero_nn_0.set_interactive_layer(interactive_layer)\n    hetero_nn_0.compile(optimizer=optimizer, loss=loss)\n\n    evaluation_0 = Evaluation(name='eval_0', eval_type='binary')\n\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n    pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n    pipeline.add_component(data_transform_1, data=Data(data=reader_1.output.data))\n    pipeline.add_component(intersection_0, data=Data(data=data_transform_0.output.data))\n    pipeline.add_component(intersection_1, data=Data(data=data_transform_1.output.data))\n    pipeline.add_component(hetero_nn_0, data=Data(train_data=intersection_0.output.data,\n                                                  validate_data=intersection_1.output.data))\n    pipeline.add_component(evaluation_0, data=Data(data=hetero_nn_0.output.data))\n    pipeline.compile()\n    pipeline.fit()\n\n    print(pipeline.get_component(\"hetero_nn_0\").get_summary())\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n    parser.add_argument(\"-config\", type=str,\n                        help=\"config file\")\n    args = parser.parse_args()\n    if args.config is not None:\n        main(args.config)\n    else:\n        main()",
  "\nimport copy\n\nimport torch\nimport torchvision\nfrom sklearn.cluster import KMeans\nfrom torch import nn\n\nfrom lightly.loss.memory_bank import MemoryBankModule\nfrom lightly.models import utils\nfrom lightly.models.modules.heads import (\n    SMoGPredictionHead,\n    SMoGProjectionHead,\n    SMoGPrototypes,\n)\nfrom lightly.transforms.smog_transform import SMoGTransform\n\nclass SMoGModel(nn.Module):\n    def __init__(self, backbone):\n        super().__init__()\n\n        self.backbone = backbone\n        self.projection_head = SMoGProjectionHead(512, 2048, 128)\n        self.prediction_head = SMoGPredictionHead(128, 2048, 128)\n\n        self.backbone_momentum = copy.deepcopy(self.backbone)\n        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n\n        utils.deactivate_requires_grad(self.backbone_momentum)\n        utils.deactivate_requires_grad(self.projection_head_momentum)\n\n        self.n_groups = 300\n        self.smog = SMoGPrototypes(\n            group_features=torch.rand(self.n_groups, 128), beta=0.99\n        )\n\n    def _cluster_features(self, features: torch.Tensor) -> torch.Tensor:\n        \n        features = features.cpu().numpy()\n        kmeans = KMeans(self.n_groups).fit(features)\n        clustered = torch.from_numpy(kmeans.cluster_centers_).float()\n        clustered = torch.nn.functional.normalize(clustered, dim=1)\n        return clustered\n\n    def reset_group_features(self, memory_bank):\n        \n        features = memory_bank.bank\n        group_features = self._cluster_features(features.t())\n        self.smog.set_group_features(group_features)\n\n    def reset_momentum_weights(self):\n        \n        self.backbone_momentum = copy.deepcopy(self.backbone)\n        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n        utils.deactivate_requires_grad(self.backbone_momentum)\n        utils.deactivate_requires_grad(self.projection_head_momentum)\n\n    def forward(self, x):\n        features = self.backbone(x).flatten(start_dim=1)\n        encoded = self.projection_head(features)\n        predicted = self.prediction_head(encoded)\n        return encoded, predicted\n\n    def forward_momentum(self, x):\n        features = self.backbone_momentum(x).flatten(start_dim=1)\n        encoded = self.projection_head_momentum(features)\n        return encoded\n\nbatch_size = 256\n\nresnet = torchvision.models.resnet18()\nbackbone = nn.Sequential(*list(resnet.children())[:-1])\nmodel = SMoGModel(backbone)\n\nmemory_bank_size = 300 * batch_size\nmemory_bank = MemoryBankModule(size=memory_bank_size)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\ntransform = SMoGTransform(\n    crop_sizes=(32, 32),\n    crop_counts=(1, 1),\n    gaussian_blur_probs=(0.0, 0.0),\n    crop_min_scales=(0.2, 0.2),\n    crop_max_scales=(1.0, 1.0),\n)\ndataset = torchvision.datasets.CIFAR10(\n    \"datasets/cifar10\", download=True, transform=transform\n)\n\ndataloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=256,\n    shuffle=True,\n    drop_last=True,\n    num_workers=8,\n)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(\n    model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-6\n)\n\nglobal_step = 0\n\nprint(\"Starting Training\")\nfor epoch in range(10):\n    total_loss = 0\n    for batch_idx, batch in enumerate(dataloader):\n        (x0, x1) = batch[0]\n\n        if batch_idx % 2:\n            \n            x1, x0 = x0, x1\n\n        x0 = x0.to(device)\n        x1 = x1.to(device)\n\n        if global_step > 0 and global_step % 300 == 0:\n            \n            model.reset_group_features(memory_bank=memory_bank)\n            model.reset_momentum_weights()\n        else:\n            \n            utils.update_momentum(model.backbone, model.backbone_momentum, 0.99)\n            utils.update_momentum(\n                model.projection_head, model.projection_head_momentum, 0.99\n            )\n\n        x0_encoded, x0_predicted = model(x0)\n        x1_encoded = model.forward_momentum(x1)\n\n        assignments = model.smog.assign_groups(x1_encoded)\n        group_features = model.smog.get_updated_group_features(x0_encoded)\n        logits = model.smog(x0_predicted, group_features, temperature=0.1)\n        model.smog.set_group_features(group_features)\n\n        loss = criterion(logits, assignments)\n\n        memory_bank(x0_encoded, update=True)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        global_step += 1\n        total_loss += loss.detach()\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")",
  "\n\"\"\n\nimport initExample \n\nfrom pyqtgraph.Qt import QtGui, QtCore\nimport numpy as np\nimport pyqtgraph as pg\n\napp = pg.mkQApp(\"Plotting Example\")\n\nwin = pg.GraphicsLayoutWidget(show=True, title=\"Basic plotting examples\")\nwin.resize(1000,600)\nwin.setWindowTitle('pyqtgraph example: Plotting')\n\npg.setConfigOptions(antialias=True)\n\np1 = win.addPlot(title=\"Basic array plotting\", y=np.random.normal(size=100))\n\np2 = win.addPlot(title=\"Multiple curves\")\np2.plot(np.random.normal(size=100), pen=(255,0,0), name=\"Red curve\")\np2.plot(np.random.normal(size=110)+5, pen=(0,255,0), name=\"Green curve\")\np2.plot(np.random.normal(size=120)+10, pen=(0,0,255), name=\"Blue curve\")\n\np3 = win.addPlot(title=\"Drawing with points\")\np3.plot(np.random.normal(size=100), pen=(200,200,200), symbolBrush=(255,0,0), symbolPen='w')\n\nwin.nextRow()\n\np4 = win.addPlot(title=\"Parametric, grid enabled\")\nx = np.cos(np.linspace(0, 2*np.pi, 1000))\ny = np.sin(np.linspace(0, 4*np.pi, 1000))\np4.plot(x, y)\np4.showGrid(x=True, y=True)\n\np5 = win.addPlot(title=\"Scatter plot, axis labels, log scale\")\nx = np.random.normal(size=1000) * 1e-5\ny = x*1000 + 0.005 * np.random.normal(size=1000)\ny -= y.min()-1.0\nmask = x > 1e-15\nx = x[mask]\ny = y[mask]\np5.plot(x, y, pen=None, symbol='t', symbolPen=None, symbolSize=10, symbolBrush=(100, 100, 255, 50))\np5.setLabel('left', \"Y Axis\", units='A')\np5.setLabel('bottom', \"Y Axis\", units='s')\np5.setLogMode(x=True, y=False)\n\np6 = win.addPlot(title=\"Updating plot\")\ncurve = p6.plot(pen='y')\ndata = np.random.normal(size=(10,1000))\nptr = 0\ndef update():\n    global curve, data, ptr, p6\n    curve.setData(data[ptr%10])\n    if ptr == 0:\n        p6.enableAutoRange('xy', False)  \n    ptr += 1\ntimer = QtCore.QTimer()\ntimer.timeout.connect(update)\ntimer.start(50)\n\nwin.nextRow()\n\np7 = win.addPlot(title=\"Filled plot, axis disabled\")\ny = np.sin(np.linspace(0, 10, 1000)) + np.random.normal(size=1000, scale=0.1)\np7.plot(y, fillLevel=-0.3, brush=(50,50,200,100))\np7.showAxis('bottom', False)\n\nx2 = np.linspace(-100, 100, 1000)\ndata2 = np.sin(x2) / x2\np8 = win.addPlot(title=\"Region Selection\")\np8.plot(data2, pen=(255,255,255,200))\nlr = pg.LinearRegionItem([400,700])\nlr.setZValue(-10)\np8.addItem(lr)\n\np9 = win.addPlot(title=\"Zoom on selected region\")\np9.plot(data2)\ndef updatePlot():\n    p9.setXRange(*lr.getRegion(), padding=0)\ndef updateRegion():\n    lr.setRegion(p9.getViewBox().viewRange()[0])\nlr.sigRegionChanged.connect(updatePlot)\np9.sigXRangeChanged.connect(updateRegion)\nupdatePlot()\n\nif __name__ == '__main__':\n    pg.exec()",
  "\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.DistributionFunction as DistFunc\nimport DREAM.Settings.Equations.RunawayElectrons as Runaways\nimport DREAM.Settings.Equations.RunawayElectronDistribution as REDist\n\nds = DREAMSettings()\n\nE = 0.6     \nn = 5e19    \nT = 1e3     \n\nre_enabled = True\n\nds.eqsys.E_field.setPrescribedData(E)\n\nds.eqsys.T_cold.setPrescribedData(T)\n\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=n)\n\nds.hottailgrid.setEnabled(False)\n\nds.collisions.collfreq_mode = Collisions.COLLFREQ_MODE_ULTRA_RELATIVISTIC\n\nds.eqsys.n_re.setAvalanche(Runaways.AVALANCHE_MODE_FLUID)\nds.eqsys.n_re.setDreicer(Runaways.DREICER_RATE_NEURAL_NETWORK)\n\nds.eqsys.n_re.setInitialProfile(1e15)\n\npmax_re = 0.5\nif re_enabled:\n    ds.runawaygrid.setNxi(50)\n    ds.runawaygrid.setNp(100)\n    ds.runawaygrid.setPmax(pmax_re)\n\n    ds.eqsys.f_re.setAdvectionInterpolationMethod(ad_int=DistFunc.AD_INTERP_TCDF)\n\n    ds.eqsys.f_re.setInitType(REDist.INIT_ISOTROPIC)\n\nelse:\n    ds.runawaygrid.setEnabled(False)\n\nds.radialgrid.setB0(5)\nds.radialgrid.setMinorRadius(0.22)\nds.radialgrid.setWallRadius(0.22)\nds.radialgrid.setNr(1)\n\nds.solver.setType(Solver.NONLINEAR)\nds.solver.setVerbose(True)\n\nds.solver.tolerance.set('j_re', reltol=1e-4)\n\nds.other.include('fluid')\n\nds.timestep.setTmax(1e-1)\nds.timestep.setNt(20)\n\nds.save('dream_settings.h5')\n",
  "import numpy as np\nimport scipy.sparse as sps\nimport os\nimport sys\n\nfrom porepy.viz import exporter\nfrom porepy.fracs import importer\n\nfrom porepy.params import tensor\nfrom porepy.params.bc import BoundaryCondition\nfrom porepy.params.data import Parameters\n\nfrom porepy.grids import coarsening as co\n\nfrom porepy.numerics.vem import dual\nfrom porepy.numerics.fv.transport import upwind\nfrom porepy.numerics.fv import tpfa, mass_matrix\n\ndef add_data_darcy(gb, domain, tol):\n    gb.add_node_props(['param', 'if_tangent'])\n\n    apert = 1e-2\n\n    km = 7.5 * 1e-10  \n\n    kf = 5 * 1e-5\n\n    for g, d in gb:\n\n        param = Parameters(g)\n        d['if_tangent'] = True\n        if g.dim == gb.dim_max():\n            kxx = km\n        else:\n            kxx = kf\n\n        perm = tensor.SecondOrder(g.dim, kxx * np.ones(g.num_cells))\n        param.set_tensor(\"flow\", perm)\n\n        param.set_source(\"flow\", np.zeros(g.num_cells))\n\n        param.set_aperture(np.power(apert, gb.dim_max() - g.dim))\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            top = bound_face_centers[2, :] > domain['zmax'] - tol\n            bottom = bound_face_centers[2, :] < domain['zmin'] + tol\n\n            boundary = np.logical_or(top, bottom)\n\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[boundary] = ['dir']\n\n            bc_val = np.zeros(g.num_faces)\n            p = np.abs(domain['zmax'] - domain['zmin']) * 1e3 * 9.81\n            bc_val[bound_faces[bottom]] = p\n\n            param.set_bc(\"flow\", BoundaryCondition(g, bound_faces, labels))\n            param.set_bc_val(\"flow\", bc_val)\n        else:\n            param.set_bc(\"flow\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n\n        d['param'] = param\n\n    gb.add_edge_prop('kn')\n    for e, d in gb.edges_props():\n        g = gb.sorted_nodes_of_edge(e)[0]\n        d['kn'] = kf / gb.node_prop(g, 'param').get_aperture()\n\ndef add_data_advection(gb, domain, tol):\n\n    for g, d in gb:\n        param = d['param']\n\n        source = np.zeros(g.num_cells)\n        param.set_source(\"transport\", source)\n\n        param.set_porosity(1)\n        param.set_discharge(d['discharge'])\n\n        bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        if bound_faces.size != 0:\n            bound_face_centers = g.face_centers[:, bound_faces]\n\n            top = bound_face_centers[2, :] > domain['zmax'] - tol\n            bottom = bound_face_centers[2, :] < domain['zmin'] + tol\n            boundary = np.logical_or(top, bottom)\n            labels = np.array(['neu'] * bound_faces.size)\n            labels[boundary] = ['dir']\n\n            bc_val = np.zeros(g.num_faces)\n            bc_val[bound_faces[bottom]] = 1\n\n            param.set_bc(\"transport\", BoundaryCondition(\n                g, bound_faces, labels))\n            param.set_bc_val(\"transport\", bc_val)\n        else:\n            param.set_bc(\"transport\", BoundaryCondition(\n                g, np.empty(0), np.empty(0)))\n        d['param'] = param\n\n    gb.add_edge_prop('param')\n    for e, d in gb.edges_props():\n        g_h = gb.sorted_nodes_of_edge(e)[1]\n        discharge = gb.node_prop(g_h, 'param').get_discharge()\n        d['param'] = Parameters(g_h)\n        d['param'].set_discharge(discharge)\n\nsys.path.append('../../example3')\nimport soultz_grid\n\nexport_folder = 'example_5_3_coarse'\ntol = 1e-6\n\nT = 40 * np.pi * 1e7\nNt = 100\ndeltaT = T / Nt\nexport_every = 5\nif_coarse = True\n\nmesh_kwargs = {}\nmesh_kwargs['mesh_size'] = {'mode': 'constant',\n                            'value': 75,\n                            'bound_value': 200,\n                            'meshing_algorithm': 4,\n                            'tol': tol}\nmesh_kwargs['num_fracs'] = 20\nmesh_kwargs['num_points'] = 10\nmesh_kwargs['file_name'] = 'soultz_fracs'\ndomain = {'xmin': -1200, 'xmax': 500,\n          'ymin': -600,  'ymax': 600,\n          'zmin': 600,   'zmax': 5500}\nmesh_kwargs['domain'] = domain\n\nprint(\"create soultz grid\")\ngb = soultz_grid.create_grid(**mesh_kwargs)\ngb.compute_geometry()\nif if_coarse:\n    co.coarsen(gb, 'by_volume')\ngb.assign_node_ordering()\n\nprint(\"solve Darcy problem\")\nfor g, d in gb:\n    d['cell_id'] = np.arange(g.num_cells)\n\nexporter.export_vtk(gb, 'grid', ['cell_id'], folder=export_folder)\n\ndarcy = dual.DualVEMMixDim(\"flow\")\n\nadd_data_darcy(gb, domain, tol)\n\nA, b = darcy.matrix_rhs(gb)\n\nup = sps.linalg.spsolve(A, b)\ndarcy.split(gb, \"up\", up)\n\ngb.add_node_props(['pressure', \"P0u\", \"discharge\"])\ndarcy.extract_u(gb, \"up\", \"discharge\")\ndarcy.extract_p(gb, \"up\", 'pressure')\ndarcy.project_u(gb, \"discharge\", \"P0u\")\n\ntotal_flow_rate = 0\nfor g, d in gb:\n    bound_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n    if bound_faces.size != 0:\n        bound_face_centers = g.face_centers[:, bound_faces]\n        top = bound_face_centers[2, :] > domain['zmax'] - tol\n        flow_rate = d['discharge'][bound_faces[top]]\n        total_flow_rate += np.sum(flow_rate)\n\nprint(\"total flow rate\", total_flow_rate)\nexporter.export_vtk(gb, 'darcy', ['pressure', \"P0u\"], folder=export_folder)\n\nphysics = 'transport'\nadvection = upwind.UpwindMixedDim(physics)\nmass = mass_matrix.MassMatrixMixedDim(physics)\ninvMass = mass_matrix.InvMassMatrixMixDim(physics)\n\nadd_data_advection(gb, domain, tol)\n\ngb.add_node_prop('deltaT', prop=deltaT)\n\nU, rhs_u = advection.matrix_rhs(gb)\nM, _ = mass.matrix_rhs(gb)\nOF = advection.outflow(gb)\nM_U = M + U\n\nrhs = rhs_u\n\nIE_solver = sps.linalg.factorized((M_U).tocsc())\n\ntheta = np.zeros(rhs.shape[0])\n\ntime = np.empty(Nt)\nfile_name = \"theta\"\ni_export = 0\nstep_to_export = np.empty(0)\n\nproduction = np.zeros(Nt)\n\nfor i in np.arange(Nt):\n    print(\"Time step\", i, \" of \", Nt)\n    \n    production[i] = np.sum(OF.dot(theta)) / total_flow_rate\n    theta = IE_solver(M.dot(theta) + rhs)\n\n    if i % export_every == 0:\n        print(\"Export solution at\", i)\n        advection.split(gb, \"theta\", theta)\n        exporter.export_vtk(gb, file_name, [\"theta\"], time_step=i_export,\n                            folder=export_folder)\n        step_to_export = np.r_[step_to_export, i]\n        i_export += 1\n\nexporter.export_pvd(gb, file_name, step_to_export *\n                    deltaT, folder=export_folder)\n\nnp.savetxt(export_folder + '/production.txt', (deltaT * np.arange(Nt),\n                                               np.abs(production)),\n           delimiter=',')",
  "\nimport sys\n\nimport numpy as np\nfrom scipy.sparse.linalg import spsolve, cg, LinearOperator, spilu\nfrom scipy.sparse import spdiags\n\nimport matplotlib.pyplot as plt\n\nfrom fealpy.decorator import cartesian\nfrom fealpy.mesh import TriangleMesh\nfrom fealpy.functionspace import LagrangeFiniteElementSpace\nfrom fealpy.boundarycondition import DirichletBC, NeumannBC\n\nimport pyamg\nfrom timeit import default_timer as timer\n\nclass BoxDomain2DData():\n    def __init__(self, E=1e+5, nu=0.2):\n        self.E = E \n        self.nu = nu\n        self.lam = self.nu*self.E/((1+self.nu)*(1-2*self.nu))\n        self.mu = self.E/(2*(1+self.nu))\n\n    def domain(self):\n        return [0, 1, 0, 1]\n\n    def init_mesh(self, n=3, meshtype='tri'):\n        node = np.array([\n            (0, 0),\n            (1, 0),\n            (1, 1),\n            (0, 1)], dtype=np.float)\n        cell = np.array([(1, 2, 0), (3, 0, 2)], dtype=np.int)\n        mesh = TriangleMesh(node, cell)\n        mesh.uniform_refine(n)\n        return mesh \n\n    @cartesian\n    def displacement(self, p):\n        return 0.0\n\n    @cartesian\n    def jacobian(self, p):\n        return 0.0\n\n    @cartesian\n    def strain(self, p):\n        return 0.0\n\n    @cartesian\n    def stress(self, p):\n        return 0.0\n\n    @cartesian\n    def source(self, p):\n        val = np.array([0.0, 0.0], dtype=np.float64)\n        shape = len(p.shape[:-1])*(1, ) + (2, )\n        return val.reshape(shape)\n\n    @cartesian\n    def dirichlet(self, p):\n        val = np.array([0.0, 0.0], dtype=np.float64)\n        shape = len(p.shape[:-1])*(1, ) + (2, )\n        return val.reshape(shape)\n\n    @cartesian\n    def neumann(self, p, n):\n        val = np.array([-500, 0.0], dtype=np.float64)\n        shape = len(p.shape[:-1])*(1, ) + (2, )\n        return val.reshape(shape)\n\n    @cartesian\n    def is_dirichlet_boundary(self, p):\n        x = p[..., 0]\n        y = p[..., 1]\n        flag = np.abs(x) < 1e-13\n        return flag\n\n    @cartesian\n    def is_neumann_boundary(self, p):\n        x = p[..., 0]\n        y = p[..., 1]\n        flag = np.abs(x - 1) < 1e-13\n        return flag\n\n    @cartesian\n    def is_fracture_boundary(self, p):\n        pass\n\nclass IterationCounter(object):\n    def __init__(self, disp=True):\n        self._disp = disp\n        self.niter = 0\n    def __call__(self, rk=None):\n        self.niter += 1\n        if self._disp:\n            print('iter %3i:' % (self.niter))\n\nclass LinearElasticityLFEMFastSolver():\n    def __init__(self, A, P, isBdDof):\n        \"\"\n        self.gdof = P.shape[0]\n        self.GD = A.shape[0]//self.gdof\n\n        self.A = A\n        self.isBdDof = isBdDof\n\n        bdIdx = np.zeros(P.shape[0], dtype=np.int_)\n        bdIdx[isBdDof] = 1\n        Tbd = spdiags(bdIdx, 0, P.shape[0], P.shape[0])\n        T = spdiags(1-bdIdx, 0, P.shape[0], P.shape[0])\n        P = T@P@T + Tbd\n        self.ml = pyamg.ruge_stuben_solver(P) \n\n    def preconditioner(self, b):\n        GD = self.GD\n        b = b.reshape(GD, -1)\n        r = np.zeros_like(b)\n        for i in range(GD):\n            r[i] = self.ml.solve(b[i], tol=1e-8, accel='cg')       \n        return r.reshape(-1)\n\n    def solve(self, uh, F, tol=1e-8):\n        \"\"\n\n        GD = self.GD\n        gdof = self.gdof\n\n        counter = IterationCounter()\n        P = LinearOperator((GD*gdof, GD*gdof), matvec=self.preconditioner)\n        uh.T.flat, info = cg(self.A, F.T.flat, x0= uh.T.flat, M=P, tol=1e-8,\n                callback=counter)\n        print(\"Convergence info:\", info)\n        print(\"Number of iteration of pcg:\", counter.niter)\n\n        return uh \n\nn = int(sys.argv[1])\np = int(sys.argv[2])\nscale = float(sys.argv[3])\n\npde = BoxDomain2DData()\n\nmesh = pde.init_mesh(n=n)\n\narea = mesh.entity_measure('cell')\n\nspace = LagrangeFiniteElementSpace(mesh, p=p)\n\nbc0 = DirichletBC(space, pde.dirichlet, threshold=pde.is_dirichlet_boundary) \nbc1 = NeumannBC(space, pde.neumann, threshold=pde.is_neumann_boundary)\n\nuh = space.function(dim=2) \nP = space.stiff_matrix(c=2*pde.mu)\nA = space.linear_elasticity_matrix(pde.lam, pde.mu) \nF = space.source_vector(pde.source, dim=2) \nF = bc1.apply(F)\nA, F = bc0.apply(A, F, uh)\n\nprint(A.shape)\n\nif False:\n    uh.T.flat[:] = spsolve(A, F) \nelif False:\n    N = len(F)\n    print(N)\n    ilu = spilu(A.tocsc(), drop_tol=1e-6, fill_factor=40)\n    M = LinearOperator((N, N), lambda x: ilu.solve(x))\n    start = timer()\n    uh.T.flat[:], info = cg(A, F, tol=1e-8, M=M)   \n    print(info)\n    end = timer()\n    print('time:', end - start)\nelse:\n    isBdDof = space.set_dirichlet_bc(pde.dirichlet, uh,\n            threshold=pde.is_dirichlet_boundary)\n    solver = LinearElasticityLFEMFastSolver(A, P, isBdDof) \n    start = timer()\n    uh[:] = solver.solve(uh, F) \n    end = timer()\n    print('time:', end - start)\n\nmesh.add_plot(plt)\n",
  "\"\"\nfrom ansys.dpf import core as dpf\nfrom ansys.dpf.core import examples\nfrom ansys.dpf.core import operators as ops\n\ncyc = examples.download_multi_stage_cyclic_result()\nmodel = dpf.Model(cyc)\nprint(model)\n\nresult_info = model.metadata.result_info\nprint(result_info.has_cyclic)\nprint(result_info.cyclic_symmetry_type)\n\ncyc_support = result_info.cyclic_support\nprint(\"num stages:\", cyc_support.num_stages)\nprint(\"num_sectors stage 0:\", cyc_support.num_sectors(0))\nprint(\"num_sectors stage 1:\", cyc_support.num_sectors(1))\nprint(\n    \"num nodes in the first stage's base sector: \",\n    len(cyc_support.base_nodes_scoping(0)),\n)\n\nUCyc = dpf.operators.result.cyclic_expanded_displacement()\nUCyc.inputs.data_sources(model.metadata.data_sources)\n\nUCyc.inputs.sectors_to_expand([0, 1, 2])\n\nsectors_scopings = dpf.ScopingsContainer()\nsectors_scopings.labels = [\"stage\"]\nsectors_scopings.add_scoping({\"stage\": 0}, dpf.Scoping(ids=[0, 1, 2]))\nsectors_scopings.add_scoping({\"stage\": 1}, dpf.Scoping(ids=[0, 1, 2, 3, 4, 5, 6]))\nUCyc.inputs.sectors_to_expand(sectors_scopings)\n\nnrm = dpf.Operator(\"norm_fc\")\nnrm.inputs.connect(UCyc.outputs)\nfields = nrm.outputs.fields_container()\n\nmesh_provider = model.metadata.mesh_provider\nmesh_provider.inputs.read_cyclic(2)\nmesh = mesh_provider.outputs.mesh()\n\nmesh.plot(fields)\n\ncyc_support_provider = ops.metadata.cyclic_support_provider(\n    data_sources=model.metadata.data_sources\n)\ncyc_support_provider.inputs.sectors_to_expand(sectors_scopings)\nmesh_exp = ops.metadata.cyclic_mesh_expansion(cyclic_support=cyc_support_provider)\nselected_sectors_mesh = mesh_exp.outputs.meshed_region()\n\nselected_sectors_mesh.plot(fields)\n\nprint(model.metadata.time_freq_support)\nprint(model.metadata.time_freq_support.get_harmonic_indices(stage_num=1).data)\n\nnode_id = cyc_support.base_nodes_scoping(0)[18]\nprint(node_id)\n\nexpanded_ids = cyc_support.expand_node_id(node_id, [0, 1, 2], 0)\nprint(expanded_ids.ids)\n\nfor node in expanded_ids.ids:\n    print(fields[0].get_entity_data_by_id(node))",
  "\nimport fastdeploy as fd\nimport cv2\nimport os\n\ndef parse_arguments():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_model\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--table_model\",\n        required=True,\n        help=\"Path of Table recognition model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_label_file\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--table_char_dict_path\",\n        type=str,\n        required=True,\n        help=\"tabel recognition dict path.\")\n    parser.add_argument(\n        \"--rec_bs\",\n        type=int,\n        default=6,\n        help=\"Recognition model inference batch size\")\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--device_id\",\n        type=int,\n        default=0,\n        help=\"Define which GPU card used to run model.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"Type of inference backend, support ort/trt/paddle/openvino, default 'openvino' for cpu, 'tensorrt' for gpu\"\n    )\n\n    return parser.parse_args()\n\ndef build_option(args):\n    det_option = fd.RuntimeOption()\n    rec_option = fd.RuntimeOption()\n    table_option = fd.RuntimeOption()\n\n    if args.device.lower() == \"gpu\":\n        det_option.use_gpu(args.device_id)\n        rec_option.use_gpu(args.device_id)\n        table_option.use_gpu(args.device_id)\n\n    if args.backend.lower() == \"trt\":\n        assert args.device.lower(\n        ) == \"gpu\", \"TensorRT backend require inference on device GPU.\"\n        det_option.use_trt_backend()\n        rec_option.use_trt_backend()\n        table_option.use_trt_backend()\n\n        det_option.set_trt_input_shape(\"x\", [1, 3, 64, 64], [1, 3, 640, 640],\n                                       [1, 3, 960, 960])\n\n        rec_option.set_trt_input_shape(\"x\", [1, 3, 48, 10],\n                                       [args.rec_bs, 3, 48, 320],\n                                       [args.rec_bs, 3, 48, 2304])\n\n        table_option.set_trt_input_shape(\"x\", [1, 3, 488, 488])\n\n        det_option.set_trt_cache_file(args.det_model + \"/det_trt_cache.trt\")\n        rec_option.set_trt_cache_file(args.rec_model + \"/rec_trt_cache.trt\")\n        table_option.set_trt_cache_file(args.table_model +\n                                        \"/table_trt_cache.trt\")\n\n    elif args.backend.lower() == \"ort\":\n        det_option.use_ort_backend()\n        rec_option.use_ort_backend()\n        table_option.use_ort_backend()\n\n    elif args.backend.lower() == \"paddle\":\n        det_option.use_paddle_infer_backend()\n        rec_option.use_paddle_infer_backend()\n        table_option.use_paddle_infer_backend()\n\n    elif args.backend.lower() == \"openvino\":\n        assert args.device.lower(\n        ) == \"cpu\", \"OpenVINO backend require inference on device CPU.\"\n        det_option.use_openvino_backend()\n        rec_option.use_openvino_backend()\n        table_option.use_openvino_backend()\n\n    return det_option, rec_option, table_option\n\nargs = parse_arguments()\n\ndet_model_file = os.path.join(args.det_model, \"inference.pdmodel\")\ndet_params_file = os.path.join(args.det_model, \"inference.pdiparams\")\n\nrec_model_file = os.path.join(args.rec_model, \"inference.pdmodel\")\nrec_params_file = os.path.join(args.rec_model, \"inference.pdiparams\")\nrec_label_file = args.rec_label_file\n\ntable_model_file = os.path.join(args.table_model, \"inference.pdmodel\")\ntable_params_file = os.path.join(args.table_model, \"inference.pdiparams\")\ntable_char_dict_path = args.table_char_dict_path\n\ndet_option, rec_option, table_option = build_option(args)\n\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file, det_params_file, runtime_option=det_option)\n\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option)\n\ntable_model = fd.vision.ocr.StructureV2Table(\n    table_model_file,\n    table_params_file,\n    table_char_dict_path,\n    runtime_option=table_option)\n\ndet_model.preprocessor.max_side_len = 960\ndet_model.postprocessor.det_db_thresh = 0.3\ndet_model.postprocessor.det_db_box_thresh = 0.6\ndet_model.postprocessor.det_db_unclip_ratio = 1.5\ndet_model.postprocessor.det_db_score_mode = \"slow\"\ndet_model.postprocessor.use_dilation = False\n\nppstructurev2_table = fd.vision.ocr.PPStructureV2Table(\n    det_model=det_model, rec_model=rec_model, table_model=table_model)\n\nppstructurev2_table.rec_batch_size = args.rec_bs\n\nim = cv2.imread(args.image)\n\nresult = ppstructurev2_table.predict(im)\n\nprint(result)\n\nvis_im = fd.vision.vis_ppocr(im, result)\ncv2.imwrite(\"visualized_result.jpg\", vis_im)\nprint(\"Visualized result save in ./visualized_result.jpg\")",
  "\n\"\"\n\nfrom functools import reduce\nimport numpy\nfrom pyscf import gto, scf, mp, qmmm\n\nmol = gto.M(atom=\"\",\n            basis='3-21g')\n\nnumpy.random.seed(1)\ncoords = numpy.random.random((5,3)) * 10\ncharges = (numpy.arange(5) + 1.) * -.1\n\ndef force(dm):\n    \n    qm_coords = mol.atom_coords()\n    qm_charges = mol.atom_charges()\n    dr = qm_coords[:,None,:] - coords\n    r = numpy.linalg.norm(dr, axis=2)\n    g = numpy.einsum('r,R,rRx,rR->Rx', qm_charges, charges, dr, r**-3)\n\n    for i, q in enumerate(charges):\n        with mol.with_rinv_origin(coords[i]):\n            v = mol.intor('int1e_iprinv')\n        f =(numpy.einsum('ij,xji->x', dm, v) +\n            numpy.einsum('ij,xij->x', dm, v.conj())) * -q\n        g[i] += f\n\n    return -g\n\nmf = qmmm.mm_charge(scf.RHF(mol), coords, charges, unit='Bohr').run()\ne1_mf = mf.e_tot\ndm = mf.make_rdm1()\nmm_force_mf = force(dm)\nprint('HF force:')\nprint(mm_force_mf)\n\ncoords[0,0] += 1e-3\nmf = qmmm.mm_charge(scf.RHF(mol), coords, charges, unit='Bohr').run()\ne2_mf = mf.e_tot\nprint(-(e2_mf-e1_mf)/1e-3, '==', mm_force_mf[0,0])\n\ndef make_rdm1_with_orbital_response(mp):\n    import time\n    from pyscf import lib\n    from pyscf.grad.mp2 import _response_dm1, _index_frozen_active, _shell_prange\n    from pyscf.mp import mp2\n    from pyscf.ao2mo import _ao2mo\n    log = lib.logger.new_logger(mp)\n    time0 = time.clock(), time.time()\n    mol = mp.mol\n\n    log.debug('Build mp2 rdm1 intermediates')\n    d1 = mp2._gamma1_intermediates(mp, mp.t2)\n    doo, dvv = d1\n    time1 = log.timer_debug1('rdm1 intermediates', *time0)\n\n    with_frozen = not (mp.frozen is None or mp.frozen is 0)\n    OA, VA, OF, VF = _index_frozen_active(mp.get_frozen_mask(), mp.mo_occ)\n    orbo = mp.mo_coeff[:,OA]\n    orbv = mp.mo_coeff[:,VA]\n    nao, nocc = orbo.shape\n    nvir = orbv.shape[1]\n\n    part_dm2 = _ao2mo.nr_e2(mp.t2.reshape(nocc**2,nvir**2),\n                            numpy.asarray(orbv.T, order='F'), (0,nao,0,nao),\n                            's1', 's1').reshape(nocc,nocc,nao,nao)\n    part_dm2 = (part_dm2.transpose(0,2,3,1) * 4 -\n                part_dm2.transpose(0,3,2,1) * 2)\n\n    offsetdic = mol.offset_nr_by_atom()\n    diagidx = numpy.arange(nao)\n    diagidx = diagidx*(diagidx+1)//2 + diagidx\n    Imat = numpy.zeros((nao,nao))\n\n    max_memory = max(0, mp.max_memory - lib.current_memory()[0])\n    blksize = max(1, int(max_memory*.9e6/8/(nao**3*2.5)))\n\n    for ia in range(mol.natm):\n        shl0, shl1, p0, p1 = offsetdic[ia]\n        ip1 = p0\n        for b0, b1, nf in _shell_prange(mol, shl0, shl1, blksize):\n            ip0, ip1 = ip1, ip1 + nf\n            dm2buf = lib.einsum('pi,iqrj->pqrj', orbo[ip0:ip1], part_dm2)\n            dm2buf+= lib.einsum('qi,iprj->pqrj', orbo, part_dm2[:,ip0:ip1])\n            dm2buf = lib.einsum('pqrj,sj->pqrs', dm2buf, orbo)\n            dm2buf = dm2buf + dm2buf.transpose(0,1,3,2)\n            dm2buf = lib.pack_tril(dm2buf.reshape(-1,nao,nao)).reshape(nf,nao,-1)\n            dm2buf[:,:,diagidx] *= .5\n\n            shls_slice = (b0,b1,0,mol.nbas,0,mol.nbas,0,mol.nbas)\n            eri0 = mol.intor('int2e', aosym='s2kl', shls_slice=shls_slice)\n            Imat += lib.einsum('ipx,iqx->pq', eri0.reshape(nf,nao,-1), dm2buf)\n            eri0 = None\n            dm2buf = None\n        time1 = log.timer_debug1('2e-part grad of atom %d'%ia, *time1)\n\n    mo_coeff = mp.mo_coeff\n    mo_energy = mp._scf.mo_energy\n    nao, nmo = mo_coeff.shape\n    nocc = numpy.count_nonzero(mp.mo_occ > 0)\n    Imat = reduce(numpy.dot, (mo_coeff.T, Imat, mp._scf.get_ovlp(), mo_coeff)) * -1\n\n    dm1mo = numpy.zeros((nmo,nmo))\n    if with_frozen:\n        dco = Imat[OF[:,None],OA] / (mo_energy[OF,None] - mo_energy[OA])\n        dfv = Imat[VF[:,None],VA] / (mo_energy[VF,None] - mo_energy[VA])\n        dm1mo[OA[:,None],OA] = doo + doo.T\n        dm1mo[OF[:,None],OA] = dco\n        dm1mo[OA[:,None],OF] = dco.T\n        dm1mo[VA[:,None],VA] = dvv + dvv.T\n        dm1mo[VF[:,None],VA] = dfv\n        dm1mo[VA[:,None],VF] = dfv.T\n    else:\n        dm1mo[:nocc,:nocc] = doo + doo.T\n        dm1mo[nocc:,nocc:] = dvv + dvv.T\n\n    dm1 = reduce(numpy.dot, (mo_coeff, dm1mo, mo_coeff.T))\n    vhf = mp._scf.get_veff(mp.mol, dm1) * 2\n    Xvo = reduce(numpy.dot, (mo_coeff[:,nocc:].T, vhf, mo_coeff[:,:nocc]))\n    Xvo+= Imat[:nocc,nocc:].T - Imat[nocc:,:nocc]\n\n    dm1mo += _response_dm1(mp, Xvo)\n\n    dm1 = reduce(numpy.dot, (mo_coeff, dm1mo, mo_coeff.T))\n    dm1 += mp._scf.make_rdm1(mp.mo_coeff, mp.mo_occ)\n    return dm1\n\nm = mp.MP2(mf).run()\ne1_mp2 = m.e_tot\ndm = make_rdm1_with_orbital_response(m)\nmm_force_mp2 = force(dm)\nprint('MP2 force:')\nprint(mm_force_mp2)\n\ncoords[0,0] += 1e-3\nmf = qmmm.mm_charge(scf.RHF(mol), coords, charges, unit='Bohr').run()\nm = mp.MP2(mf).run()\ne2_mp2 = m.e_tot\nprint(-(e2_mp2-e1_mp2)/1e-3, '==', mm_force_mp2[0,0])",
  "\n\"\"\n\nfrom __future__ import annotations\n\nimport sys\n\nimport urwid\n\nclass LineWalker(urwid.ListWalker):\n    \"\"\n\n    def __init__(self, name):\n        self.file = open(name)\n        self.lines = []\n        self.focus = 0\n\n    def get_focus(self):\n        return self._get_at_pos(self.focus)\n\n    def set_focus(self, focus):\n        self.focus = focus\n        self._modified()\n\n    def get_next(self, start_from):\n        return self._get_at_pos(start_from + 1)\n\n    def get_prev(self, start_from):\n        return self._get_at_pos(start_from - 1)\n\n    def read_next_line(self):\n        \"\"\n\n        next_line = self.file.readline()\n\n        if not next_line or next_line[-1:] != '\\n':\n            \n            self.file = None\n        else:\n            \n            next_line = next_line[:-1]\n\n        expanded = next_line.expandtabs()\n\n        edit = urwid.Edit(\"\", expanded, allow_tab=True)\n        edit.set_edit_pos(0)\n        edit.original_text = next_line\n        self.lines.append(edit)\n\n        return next_line\n\n    def _get_at_pos(self, pos):\n        \"\"\n\n        if pos < 0:\n            \n            return None, None\n\n        if len(self.lines) > pos:\n            \n            return self.lines[pos], pos\n\n        if self.file is None:\n            \n            return None, None\n\n        assert pos == len(self.lines), \"out of order request?\"\n\n        self.read_next_line()\n\n        return self.lines[-1], pos\n\n    def split_focus(self):\n        \"\"\n\n        focus = self.lines[self.focus]\n        pos = focus.edit_pos\n        edit = urwid.Edit(\"\",focus.edit_text[pos:], allow_tab=True)\n        edit.original_text = \"\"\n        focus.set_edit_text(focus.edit_text[:pos])\n        edit.set_edit_pos(0)\n        self.lines.insert(self.focus+1, edit)\n\n    def combine_focus_with_prev(self):\n        \"\"\n\n        above, ignore = self.get_prev(self.focus)\n        if above is None:\n            \n            return\n\n        focus = self.lines[self.focus]\n        above.set_edit_pos(len(above.edit_text))\n        above.set_edit_text(above.edit_text + focus.edit_text)\n        del self.lines[self.focus]\n        self.focus -= 1\n\n    def combine_focus_with_next(self):\n        \"\"\n\n        below, ignore = self.get_next(self.focus)\n        if below is None:\n            \n            return\n\n        focus = self.lines[self.focus]\n        focus.set_edit_text(focus.edit_text + below.edit_text)\n        del self.lines[self.focus+1]\n\nclass EditDisplay:\n    palette = [\n        ('body','default', 'default'),\n        ('foot','dark cyan', 'dark blue', 'bold'),\n        ('key','light cyan', 'dark blue', 'underline'),\n        ]\n\n    footer_text = ('foot', [\n        \"Text Editor    \",\n        ('key', \"F5\"), \" save  \",\n        ('key', \"F8\"), \" quit\",\n        ])\n\n    def __init__(self, name):\n        self.save_name = name\n        self.walker = LineWalker(name)\n        self.listbox = urwid.ListBox(self.walker)\n        self.footer = urwid.AttrWrap(urwid.Text(self.footer_text),\n            \"foot\")\n        self.view = urwid.Frame(urwid.AttrWrap(self.listbox, 'body'),\n            footer=self.footer)\n\n    def main(self):\n        self.loop = urwid.MainLoop(self.view, self.palette,\n            unhandled_input=self.unhandled_keypress)\n        self.loop.run()\n\n    def unhandled_keypress(self, k):\n        \"\"\n\n        if k == \"f5\":\n            self.save_file()\n        elif k == \"f8\":\n            raise urwid.ExitMainLoop()\n        elif k == \"delete\":\n            \n            self.walker.combine_focus_with_next()\n        elif k == \"backspace\":\n            \n            self.walker.combine_focus_with_prev()\n        elif k == \"enter\":\n            \n            self.walker.split_focus()\n            \n            self.loop.process_input([\"down\", \"home\"])\n        elif k == \"right\":\n            w, pos = self.walker.get_focus()\n            w, pos = self.walker.get_next(pos)\n            if w:\n                self.listbox.set_focus(pos, 'above')\n                self.loop.process_input([\"home\"])\n        elif k == \"left\":\n            w, pos = self.walker.get_focus()\n            w, pos = self.walker.get_prev(pos)\n            if w:\n                self.listbox.set_focus(pos, 'below')\n                self.loop.process_input([\"end\"])\n        else:\n            return\n        return True\n\n    def save_file(self):\n        \"\"\n\n        l = []\n        walk = self.walker\n        for edit in walk.lines:\n            \n            if edit.original_text.expandtabs() == edit.edit_text:\n                l.append(edit.original_text)\n            else:\n                l.append(re_tab(edit.edit_text))\n\n        while walk.file is not None:\n            l.append(walk.read_next_line())\n\n        outfile = open(self.save_name, \"w\")\n\n        prefix = \"\"\n        for line in l:\n            outfile.write(prefix + line)\n            prefix = \"\\n\"\n\ndef re_tab(s):\n    \"\"\n    l = []\n    p = 0\n    for i in range(8, len(s), 8):\n        if s[i-2:i] == \"  \":\n            \n            l.append(f\"{s[p:i].rstrip()}\\t\")\n            p = i\n\n    if p == 0:\n        return s\n    else:\n        l.append(s[p:])\n        return \"\".join(l)\n\ndef main():\n    try:\n        name = sys.argv[1]\n        assert open(name, \"a\")\n    except:\n        sys.stderr.write(__doc__)\n        return\n    EditDisplay(name).main()\n\nif __name__==\"__main__\":\n    main()",
  "from datetime import date\n\nfrom shiny import App, Inputs, Outputs, Session, reactive, ui\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Changing the values of inputs from the server\"),\n    ui.row(\n        ui.column(\n            4,\n            ui.panel_well(\n                ui.tags.h4(\"These inputs control the other inputs on the page\"),\n                ui.input_text(\n                    \"control_label\", \"This controls some of the labels:\", \"LABEL TEXT\"\n                ),\n                ui.input_slider(\n                    \"control_num\", \"This controls values:\", min=1, max=20, value=15\n                ),\n            ),\n        ),\n        ui.column(\n            4,\n            ui.panel_well(\n                ui.tags.h4(\"These inputs are controlled by the other inputs\"),\n                ui.input_text(\"inText\", \"Text input:\", value=\"start text\"),\n                ui.input_numeric(\n                    \"inNumber\", \"Number input:\", min=1, max=20, value=5, step=0.5\n                ),\n                ui.input_numeric(\n                    \"inNumber2\", \"Number input 2:\", min=1, max=20, value=5, step=0.5\n                ),\n                ui.input_slider(\"inSlider\", \"Slider input:\", min=1, max=20, value=15),\n                ui.input_slider(\n                    \"inSlider2\", \"Slider input 2:\", min=1, max=20, value=(5, 15)\n                ),\n                ui.input_slider(\n                    \"inSlider3\", \"Slider input 3:\", min=1, max=20, value=(5, 15)\n                ),\n                ui.input_date(\"inDate\", \"Date input:\"),\n                ui.input_date_range(\"inDateRange\", \"Date range input:\"),\n            ),\n        ),\n        ui.column(\n            4,\n            ui.panel_well(\n                ui.input_checkbox(\"inCheckbox\", \"Checkbox input\", value=False),\n                ui.input_checkbox_group(\n                    \"inCheckboxGroup\",\n                    \"Checkbox group input:\",\n                    {\n                        \"option1\": \"label 1\",\n                        \"option2\": \"label 2\",\n                    },\n                ),\n                ui.input_radio_buttons(\n                    \"inRadio\",\n                    \"Radio buttons:\",\n                    {\n                        \"option1\": \"label 1\",\n                        \"option2\": \"label 2\",\n                    },\n                ),\n                ui.input_select(\n                    \"inSelect\",\n                    \"Select input:\",\n                    {\n                        \"option1\": \"label 1\",\n                        \"option2\": \"label 2\",\n                    },\n                ),\n                ui.input_select(\n                    \"inSelect2\",\n                    \"Select input 2:\",\n                    {\n                        \"option1\": \"label 1\",\n                        \"option2\": \"label 2\",\n                    },\n                    multiple=True,\n                ),\n            ),\n            ui.navset_tab(\n                ui.nav(\"panel1\", ui.h2(\"This is the first panel.\")),\n                ui.nav(\"panel2\", ui.h2(\"This is the second panel.\")),\n                id=\"inTabset\",\n            ),\n        ),\n    ),\n)\n\ndef server(input: Inputs, output: Outputs, session: Session):\n    @reactive.Effect\n    def _():\n        \n        c_label = input.control_label()\n        c_num = input.control_num()\n\n        ui.update_text(\n            \"inText\",\n            label=\"New \" + c_label,\n            value=\"New text \" + str(c_num),\n        )\n\n        ui.update_numeric(\"inNumber\", value=c_num)\n\n        ui.update_numeric(\n            \"inNumber2\",\n            label=\"Number \" + c_label,\n            value=c_num,\n            min=c_num - 10,\n            max=c_num + 10,\n            step=5,\n        )\n\n        ui.update_slider(\"inSlider\", label=\"Slider \" + c_label, value=c_num)\n\n        ui.update_slider(\"inSlider2\", value=(c_num - 1, c_num + 1))\n\n        ui.update_slider(\"inSlider3\", value=(input.inSlider3()[0], c_num + 2))\n\n        ui.update_date(\"inDate\", label=\"Date \" + c_label, value=date(2013, 4, c_num))\n\n        ui.update_date_range(\n            \"inDateRange\",\n            label=\"Date range \" + c_label,\n            start=date(2013, 1, c_num),\n            end=date(2013, 12, c_num),\n            min=date(2001, 1, c_num),\n            max=date(2030, 1, c_num),\n        )\n\n        ui.update_checkbox(\"inCheckbox\", value=c_num % 2)\n\n        opt_labels = [f\"option label {c_num} {type}\" for type in [\"A\", \"B\"]]\n        opt_vals = [f\"option-{c_num}-{type}\" for type in [\"A\", \"B\"]]\n        opts_dict = dict(zip(opt_vals, opt_labels))\n\n        ui.update_checkbox_group(\n            \"inCheckboxGroup\",\n            label=\"Checkbox group \" + c_label,\n            choices=opts_dict,\n            selected=f\"option-{c_num}-A\",\n        )\n\n        ui.update_radio_buttons(\n            \"inRadio\",\n            label=\"Radio \" + c_label,\n            choices=opts_dict,\n            selected=f\"option-{c_num}-A\",\n        )\n        \n        ui.update_select(\n            \"inSelect\",\n            label=\"Select \" + c_label,\n            choices=opts_dict,\n            selected=f\"option-{c_num}-A\",\n        )\n\n        ui.update_select(\n            \"inSelect2\",\n            label=\"Select label \" + c_label,\n            choices=opts_dict,\n            selected=f\"option-{c_num}-B\",\n        )\n\n        ui.update_navs(\"inTabset\", selected=\"panel2\" if c_num % 2 else \"panel1\")\n\napp = App(app_ui, server, debug=True)",
  "\n\"\"\n__url__ = 'https://github.com/silnrsi/pysilfont'\n__copyright__ = 'Copyright (c) 2018,2021 SIL International  (https://www.sil.org)'\n__license__ = 'Released under the MIT License (https://opensource.org/licenses/MIT)'\n__author__ = 'Bob Hallissy'\n\nimport re\nfrom silfont.core import execute\nimport silfont.ftml_builder as FB\n\nargspec = [\n    ('ifont', {'help': 'Input UFO'}, {'type': 'infont'}),\n    ('output', {'help': 'Output file ftml in XML format', 'nargs': '?'}, {'type': 'outfile', 'def': '_out.ftml'}),\n    ('-i','--input', {'help': 'Glyph info csv file'}, {'type': 'incsv', 'def': 'glyph_data.csv'}),\n    ('-f','--fontcode', {'help': 'letter to filter for glyph_data'},{}),\n    ('-l','--log', {'help': 'Set log file name'}, {'type': 'outfile', 'def': '_ftml.log'}),\n    ('--langs', {'help':'List of bcp47 language tags', 'default': None}, {}),\n    ('--rtl', {'help': 'enable right-to-left features', 'action': 'store_true'}, {}),\n    ('--norendercheck', {'help': 'do not include the RenderingUnknown check', 'action': 'store_true'}, {}),\n    ('-t', '--test', {'help': 'name of the test to generate', 'default': None}, {}),\n    ('-s','--fontsrc', {'help': 'font source: \"url()\" or \"local()\" optionally followed by \"=label\"', 'action': 'append'}, {}),\n    ('--scale', {'help': 'percentage to scale rendered text (default 100)'}, {}),\n    ('--ap', {'help': 'regular expression describing APs to examine', 'default': '.'}, {}),\n    ('-w', '--width', {'help': 'total width of all <string> column (default automatic)'}, {}),\n    ('--xsl', {'help': 'XSL stylesheet to use'}, {}),\n]\n\ndef doit(args):\n    logger = args.logger\n\n    builder = FB.FTMLBuilder(logger, incsv=args.input, fontcode=args.fontcode, font=args.ifont, ap=args.ap,\n                             rtlenable=True, langs=args.langs)\n\n    builder.diacBase = 0x0628   \n\n    test = args.test or 'AllChars (NG)'\n    widths = None\n    if args.width:\n        try:\n            width, units = re.match(r'(\\d+)(.*)$', args.width).groups()\n            if len(args.fontsrc):\n                width = int(round(int(width)/len(args.fontsrc)))\n            widths = {'string': f'{width}{units}'}\n            logger.log(f'width: {args.width} --> {widths[\"string\"]}', 'I')\n        except:\n            logger.log(f'Unable to parse width argument \"{args.width}\"', 'W')\n    \n    fontsrc = []\n    labels = []\n    for sl in args.fontsrc:\n        try:\n            s, l = sl.split('=',1)\n            fontsrc.append(s)\n            labels.append(l)\n        except ValueError:\n            fontsrc.append(sl)\n            labels.append(None)\n    ftml = FB.FTML(test, logger, rendercheck=not args.norendercheck, fontscale=args.scale,\n                   widths=widths, xslfn=args.xsl, fontsrc=fontsrc, fontlabel=labels, defaultrtl=args.rtl)\n\n    if test.lower().startswith(\"allchars\"):\n        \n        ftml.startTestGroup('Encoded characters')\n        for uid in sorted(builder.uids()):\n            if uid < 32: continue\n            c = builder.char(uid)\n            \n            for featlist in builder.permuteFeatures(uids = (uid,)):\n                ftml.setFeatures(featlist)\n                builder.render((uid,), ftml)\n                \n            ftml.clearFeatures()\n            for langID in sorted(c.langs):\n                ftml.setLang(langID)\n                builder.render((uid,), ftml)\n            ftml.clearLang()\n\n        ftml.startTestGroup('Specials & ligatures from glyph_data')\n        for basename in sorted(builder.specials()):\n            special = builder.special(basename)\n            \n            for featlist in builder.permuteFeatures(uids = special.uids):\n                ftml.setFeatures(featlist)\n                builder.render(special.uids, ftml)\n                \n                ftml.closeTest()\n            ftml.clearFeatures()\n            if len(special.langs):\n                for langID in sorted(special.langs):\n                    ftml.setLang(langID)\n                    builder.render(special.uids, ftml)\n                    ftml.closeTest()\n                ftml.clearLang()\n\n        ftml.startTestGroup('Lam-Alef')\n        \n        lamlist = list(filter(lambda x: x in builder.uids(), (0x0644, 0x06B5, 0x06B6, 0x06B7, 0x06B8, 0x076A, 0x08A6)))\n        aleflist = list(filter(lambda x: x in builder.uids(), (0x0627, 0x0622, 0x0623, 0x0625, 0x0671, 0x0672, 0x0673, 0x0675, 0x0773, 0x0774)))\n        \n        for lam in lamlist:\n            for alef in aleflist:\n                for featlist in builder.permuteFeatures(uids = (lam, alef)):\n                    ftml.setFeatures(featlist)\n                    builder.render((lam,alef), ftml)\n                    \n                    ftml.closeTest()\n                ftml.clearFeatures()\n\n    if test.lower().startswith(\"diac\"):\n        \n        repDiac = list(filter(lambda x: x in builder.uids(), (0x064E, 0x0650, 0x065E, 0x0670, 0x0616, 0x06E3, 0x08F0, 0x08F2)))\n        repBase = list(filter(lambda x: x in builder.uids(), (0x0627, 0x0628, 0x062B, 0x0647, 0x064A, 0x77F, 0x08AC)))\n\n        ftml.startTestGroup('Representative diacritics on all bases that take diacritics')\n        for uid in sorted(builder.uids()):\n            \n            if uid < 32 or uid in (0xAA, 0xBA): continue\n            c = builder.char(uid)\n            \n            if c.general == 'Lo' or c.isBase:\n                for diac in repDiac:\n                    for featlist in builder.permuteFeatures(uids = (uid,diac)):\n                        ftml.setFeatures(featlist)\n                        \n                        builder.render((uid,diac), ftml, addBreaks = False)\n                    ftml.clearFeatures()\n                ftml.closeTest()\n\n        ftml.startTestGroup('All diacritics on representative bases')\n        for uid in sorted(builder.uids()):\n            \n            if uid < 0x600 or uid in range(0xFE00, 0xFE10): continue\n            c = builder.char(uid)\n            if c.general == 'Mn':\n                for base in repBase:\n                    for featlist in builder.permuteFeatures(uids = (uid,base)):\n                        ftml.setFeatures(featlist)\n                        builder.render((base,uid), ftml, keyUID = uid, addBreaks = False)\n                    ftml.clearFeatures()\n                ftml.closeTest()\n\n        ftml.startTestGroup('Special cases')\n        builder.render((0x064A, 0x065E), ftml, comment=\"Yeh + Fatha should keep dots\")\n        builder.render((0x064A, 0x0654), ftml, comment=\"Yeh + Hamza should loose dots\")\n        ftml.closeTest()\n\n    ftml.writeFile(args.output)\n\ndef cmd() : execute(\"UFO\",doit,argspec)\nif __name__ == \"__main__\": cmd()",
  "\nimport numpy as np\nimport sys\n\nsys.path.append('../../py/')\n\nfrom DREAM.DREAMSettings import DREAMSettings\nimport DREAM.Settings.Equations.IonSpecies as Ions\nimport DREAM.Settings.Solver as Solver\nimport DREAM.Settings.CollisionHandler as Collisions\nimport DREAM.Settings.Equations.DistributionFunction as DistFunc\nimport DREAM.Settings.Equations.RunawayElectrons as Runaways\nimport DREAM.Settings.TransportSettings as Transport\n\nds = DREAMSettings()\n\nE = 0.6     \nn = 5e19    \nT = 1e3     \n\npstar=0.5\n\nNt = 3\nNr = 11; a0=0.22\nNp = 60\nNxi= 45\n\nt_data  = np.linspace(0,1e-2,Nt)\nr_data  = np.linspace(0,a0,Nr)\np_data  = np.linspace(0.0,1.5,Np)\nxi_data = np.linspace(-1.0,1.0,Nxi)\n\nAr  = 1.0 * np.ones((Nt,Nr,Nxi,Np));    \nDrr = 1.0e-2 * np.ones((Nt,Nr,Nxi,Np))\n\nAr[:,r_data<0.05,:,:]  = 0.0\nDrr[:,r_data<0.05,:,:] = 0.0\n\nre_enabled = True\n\nds.eqsys.E_field.setPrescribedData(E)\n\nds.eqsys.T_cold.setPrescribedData(T)\n\nds.eqsys.n_i.addIon(name='D', Z=1, iontype=Ions.IONS_PRESCRIBED_FULLY_IONIZED, n=n)\n\nds.hottailgrid.setEnabled(False)\n\nds.eqsys.f_hot.setInitialProfiles(n0=2*n, T0=T)\n\nds.hottailgrid.setNp(15)\nds.hottailgrid.setNxi(5)\nds.hottailgrid.setPmax(1.5)\n\nds.eqsys.n_re.setAvalanche(Runaways.AVALANCHE_MODE_FLUID)\nds.eqsys.n_re.setDreicer(Runaways.DREICER_RATE_NEURAL_NETWORK)\n\npmax_re = 0.5\nds.runawaygrid.setEnabled(False)\n\nds.radialgrid.setB0(5)\nds.radialgrid.setMinorRadius(a0)\nds.radialgrid.setNr(50)\nds.radialgrid.setWallRadius(a0*1.1)\n\nds.eqsys.n_re.transport.setSvenssonPstar(pstar)\n\nds.eqsys.n_re.transport.setSvenssonInterp1dParam(Transport.SVENSSON_INTERP1D_PARAM_IP)\n\nds.eqsys.n_re.transport.setSvenssonAdvection(Ar ,t=t_data,r=r_data,p=p_data,xi=xi_data)\nds.eqsys.n_re.transport.setSvenssonDiffusion(Drr,t=t_data,r=r_data,p=p_data,xi=xi_data,\n                                             \n                                             interp1d=Transport.INTERP1D_LINEAR)\n\nds.solver.setType(Solver.NONLINEAR)\nds.solver.setVerbose(False)\n\nds.other.include('fluid')\n\nds.timestep.setTmax(1e-3)\nds.timestep.setNt(500)\n\nds.save('dream_settings.h5')\n\nprint()\nprint(\"Done!\")",
  "\n\"\"\n\nfrom kubernetes import client, config\n\ndef create_deployment(apps_v1_api):\n    container = client.V1Container(\n        name=\"deployment\",\n        image=\"gcr.io/google-appengine/fluentd-logger\",\n        image_pull_policy=\"Never\",\n        ports=[client.V1ContainerPort(container_port=5678)],\n    )\n    \n    template = client.V1PodTemplateSpec(\n        metadata=client.V1ObjectMeta(labels={\"app\": \"deployment\"}),\n        spec=client.V1PodSpec(containers=[container]))\n    \n    spec = client.V1DeploymentSpec(\n        replicas=1,\n        template=template)\n    \n    deployment = client.V1Deployment(\n        api_version=\"apps/v1\",\n        kind=\"Deployment\",\n        metadata=client.V1ObjectMeta(name=\"deployment\"),\n        spec=spec)\n    \n    apps_v1_api.create_namespaced_deployment(\n        namespace=\"default\", body=deployment\n    )\n\ndef create_service():\n    core_v1_api = client.CoreV1Api()\n    body = client.V1Service(\n        api_version=\"v1\",\n        kind=\"Service\",\n        metadata=client.V1ObjectMeta(\n            name=\"service-example\"\n        ),\n        spec=client.V1ServiceSpec(\n            selector={\"app\": \"deployment\"},\n            ports=[client.V1ServicePort(\n                port=5678,\n                target_port=5678\n            )]\n        )\n    )\n    \n    core_v1_api.create_namespaced_service(namespace=\"default\", body=body)\n\ndef create_ingress(networking_v1_beta1_api):\n    body = client.NetworkingV1beta1Ingress(\n        api_version=\"networking.k8s.io/v1beta1\",\n        kind=\"Ingress\",\n        metadata=client.V1ObjectMeta(name=\"ingress-example\", annotations={\n            \"nginx.ingress.kubernetes.io/rewrite-target\": \"/\"\n        }),\n        spec=client.NetworkingV1beta1IngressSpec(\n            rules=[client.NetworkingV1beta1IngressRule(\n                host=\"example.com\",\n                http=client.NetworkingV1beta1HTTPIngressRuleValue(\n                    paths=[client.NetworkingV1beta1HTTPIngressPath(\n                        path=\"/\",\n                        backend=client.NetworkingV1beta1IngressBackend(\n                            service_port=5678,\n                            service_name=\"service-example\")\n\n                    )]\n                )\n            )\n            ]\n        )\n    )\n    \n    networking_v1_beta1_api.create_namespaced_ingress(\n        namespace=\"default\",\n        body=body\n    )\n\ndef main():\n    \n    config.load_kube_config()\n    apps_v1_api = client.AppsV1Api()\n    networking_v1_beta1_api = client.NetworkingV1beta1Api()\n\n    create_deployment(apps_v1_api)\n    create_service()\n    create_ingress(networking_v1_beta1_api)\n\nif __name__ == \"__main__\":\n    main()",
  "\n\"\"\n\nimport os.path as op\nimport matplotlib.pyplot as plt\n\nimport mne\nfrom mne.datasets import spm_face\nfrom mne.preprocessing import ICA, create_eog_epochs\nfrom mne import io\nfrom mne.minimum_norm import make_inverse_operator, apply_inverse\n\nprint(__doc__)\n\ndata_path = spm_face.data_path()\nsubjects_dir = data_path + '/subjects'\n\nraw_fname = data_path + '/MEG/spm/SPM_CTF_MEG_example_faces%d_3D_raw.fif'\n\nraw = io.Raw(raw_fname % 1, preload=True)  \n\npicks = mne.pick_types(raw.info, meg=True, exclude='bads')\nraw.filter(1, 30, method='iir')\n\nevents = mne.find_events(raw, stim_channel='UPPT001')\n\nmne.viz.plot_events(events, raw.info['sfreq'])\n\nevent_ids = {\"faces\": 1, \"scrambled\": 2}\n\ntmin, tmax = -0.2, 0.6\nbaseline = None  \nreject = dict(mag=5e-12)\n\nepochs = mne.Epochs(raw, events, event_ids, tmin, tmax,  picks=picks,\n                    baseline=baseline, preload=True, reject=reject)\n\nica = ICA(n_components=0.95).fit(raw, decim=6, reject=reject)\n\neog_epochs = create_eog_epochs(raw, ch_name='MRT31-2908', reject=reject)\neog_inds, eog_scores = ica.find_bads_eog(eog_epochs, ch_name='MRT31-2908')\nica.plot_scores(eog_scores, eog_inds)  \nica.plot_components(eog_inds)  \nica.exclude += eog_inds[:1]  \nica.plot_overlay(eog_epochs.average())  \nepochs_cln = ica.apply(epochs, copy=True)  \n\nevoked = [epochs_cln[k].average() for k in event_ids]\n\ncontrast = evoked[1] - evoked[0]\n\nevoked.append(contrast)\n\nfor e in evoked:\n    e.plot(ylim=dict(mag=[-400, 400]))\n\nplt.show()\n\nnoise_cov = mne.compute_covariance(epochs_cln, tmax=0)\n\ntrans_fname = data_path + ('/MEG/spm/SPM_CTF_MEG_example_faces1_3D_'\n                           'raw-trans.fif')\n\nmaps = mne.make_field_map(evoked[0], trans_fname, subject='spm',\n                          subjects_dir=subjects_dir, n_jobs=1)\n\nevoked[0].plot_field(maps, time=0.170)\n\nsrc_fname = data_path + '/subjects/spm/bem/spm-oct-6-src.fif'\nif not op.isfile(src_fname):\n    src = mne.setup_source_space('spm', src_fname, spacing='oct6',\n                                 subjects_dir=subjects_dir, overwrite=True)\nelse:\n    src = mne.read_source_spaces(src_fname)\n\nbem = data_path + '/subjects/spm/bem/spm-5120-5120-5120-bem-sol.fif'\nforward = mne.make_forward_solution(contrast.info, trans_fname, src, bem)\nforward = mne.convert_forward_solution(forward, surf_ori=True)\n\nsnr = 3.0\nlambda2 = 1.0 / snr ** 2\nmethod = 'dSPM'\n\ninverse_operator = make_inverse_operator(contrast.info, forward, noise_cov,\n                                         loose=0.2, depth=0.8)\n\nstc = apply_inverse(contrast, inverse_operator, lambda2, method, pick_ori=None)\n\nbrain = stc.plot(hemi='both', subjects_dir=subjects_dir)\nbrain.set_time(170.0)  \nbrain.show_view('ventral')\n",
  "\"\"\n\nimport os\nimport pyaedt\n\nnon_graphical = False\n\nproject_temp_name = pyaedt.downloads.download_via_wizard(pyaedt.generate_unique_folder_name())\n\nversion = \"2023.2\"\nhfss = pyaedt.Hfss(projectname=project_temp_name, specified_version=version, non_graphical=non_graphical,\n                   new_desktop_session=True)\npin_names = hfss.excitations\n\ncircuit = pyaedt.Circuit()\nhfss_comp = circuit.modeler.schematic.add_subcircuit_dynamic_link(hfss)\n\ncircuit.modeler.schematic.refresh_dynamic_link(hfss_comp.composed_name)\ncircuit.modeler.schematic.set_sim_option_on_hfss_subcircuit(hfss_comp)\nhfss_setup_name = hfss.setups[0].name + \" : \" + hfss.setups[0].sweeps[0].name\ncircuit.modeler.schematic.set_sim_solution_on_hfss_subcircuit(hfss_comp.composed_name, hfss_setup_name)\n\ncircuit.modeler.schematic.create_interface_port(\n    name=\"Excitation_1\", location=[hfss_comp.pins[0].location[0], hfss_comp.pins[0].location[1]]\n)\ncircuit.modeler.schematic.create_interface_port(\n    name=\"Excitation_2\", location=[hfss_comp.pins[1].location[0], hfss_comp.pins[1].location[1]]\n)\ncircuit.modeler.schematic.create_interface_port(\n    name=\"Port_1\", location=[hfss_comp.pins[2].location[0], hfss_comp.pins[2].location[1]]\n)\ncircuit.modeler.schematic.create_interface_port(\n    name=\"Port_2\", location=[hfss_comp.pins[3].location[0], hfss_comp.pins[3].location[1]]\n)\n\nvoltage = 1\nphase = 0\nports_list = [\"Excitation_1\", \"Excitation_2\"]\nsource = circuit.assign_voltage_sinusoidal_excitation_to_ports(ports_list)\nsource.ac_magnitude = voltage\nsource.phase = phase\n\nsetup_name = \"MySetup\"\nLNA_setup = circuit.create_setup(setupname=setup_name)\nbw_start = 4.3\nbw_stop = 4.4\nn_points = 1001\nunit = \"GHz\"\nsweep_list = [\"LINC\", str(bw_start) + unit, str(bw_stop) + unit, str(n_points)]\nLNA_setup.props[\"SweepDefinition\"][\"Data\"] = \" \".join(sweep_list)\n\ncircuit.analyze()\ncircuit.push_excitations(instance_name=\"S1\", setup_name=setup_name)\n\nmech = pyaedt.Mechanical()\nmech.copy_solid_bodies_from(hfss)\n\nmech.assign_em_losses(\n    designname=hfss.design_name,\n    setupname=hfss.setups[0].name,\n    sweepname=\"LastAdaptive\",\n    map_frequency=hfss.setups[0].props[\"Frequency\"],\n    surface_objects=hfss.get_all_conductors_names(),\n)\ndiels = [\"1_pd\", \"2_pd\", \"3_pd\", \"4_pd\", \"5_pd\"]\nfor el in diels:\n    mech.assign_uniform_convection(objects_list=[mech.modeler[el].top_face_y, mech.modeler[el].bottom_face_y],\n                                   convection_value=3)\n\nmech.plot(show=False, export_path=os.path.join(mech.working_directory, \"Mech.jpg\"), plot_air_objects=False)\n\nmech.create_setup()\nmech.save_project()\nmech.analyze()\nsurfaces = []\nfor name in mech.get_all_conductors_names():\n    surfaces.extend(mech.modeler.get_object_faces(name))\nmech.post.create_fieldplot_surface(objlist=surfaces, quantityName=\"Temperature\")\n\nmech.release_desktop(True, True)",
  "\nr\"\"\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom simsopt.field import BiotSavart, Current, coils_via_symmetries\nfrom simsopt.geo import CurveLength, curves_to_vtk, create_equally_spaced_curves, SurfaceRZFourier\nfrom simsopt.mhd import VirtualCasing, Vmec\nfrom simsopt.objectives import QuadraticPenalty, SquaredFlux\nfrom simsopt.util import in_github_actions\n\nncoils = 5\n\nR0 = 5.5\n\nR1 = 1.25\n\norder = 6\n\nLENGTH_PENALTY = 1e0\n\nMAXITER = 50 if in_github_actions else 500\n\nTEST_DIR = (Path(__file__).parent / \"..\" / \"..\" / \"tests\" / \"test_files\").resolve()\nfilename = 'wout_W7-X_without_coil_ripple_beta0p05_d23p4_tm_reference.nc'\nvmec_file = TEST_DIR / filename\n\nnphi = 32\nntheta = 32\n\nvc_src_nphi = 80\n\nout_dir = Path(\"output\")\nout_dir.mkdir(parents=True, exist_ok=True)\n\nhead, tail = os.path.split(vmec_file)\nvc_filename = os.path.join(head, tail.replace('wout', 'vcasing'))\nprint('virtual casing data file:', vc_filename)\nif os.path.isfile(vc_filename):\n    print('Loading saved virtual casing result')\n    vc = VirtualCasing.load(vc_filename)\nelse:\n    \n    print('Running the virtual casing calculation')\n    vc = VirtualCasing.from_vmec(vmec_file, src_nphi=vc_src_nphi, trgt_nphi=nphi, trgt_ntheta=ntheta)\n\ns = SurfaceRZFourier.from_wout(vmec_file, range=\"half period\", nphi=nphi, ntheta=ntheta)\ntotal_current = Vmec(vmec_file).external_current() / (2 * s.nfp)\n\nbase_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order, numquadpoints=128)\n\nbase_currents = [Current(total_current / ncoils * 1e-5) * 1e5 for _ in range(ncoils-1)]\n\ntotal_current = Current(total_current)\ntotal_current.fix_all()\nbase_currents += [total_current - sum(base_currents)]\n\ncoils = coils_via_symmetries(base_curves, base_currents, s.nfp, True)\nbs = BiotSavart(coils)\n\nbs.set_points(s.gamma().reshape((-1, 3)))\ncurves = [c.curve for c in coils]\ncurves_to_vtk(curves, out_dir / \"curves_init\")\npointData = {\"B_N\": np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)[:, :, None]}\ns.to_vtk(out_dir / \"surf_init\", extra_data=pointData)\n\nJf = SquaredFlux(s, bs, target=vc.B_external_normal)\nJls = [CurveLength(c) for c in base_curves]\n\nJF = Jf \\\n    + LENGTH_PENALTY * sum(QuadraticPenalty(Jls[i], Jls[i].J(), \"identity\") for i in range(len(base_curves)))\n\ndef fun(dofs):\n    JF.x = dofs\n    J = JF.J()\n    grad = JF.dJ()\n    jf = Jf.J()\n    Bbs = bs.B().reshape((nphi, ntheta, 3))\n    BdotN = np.abs(np.sum(Bbs * s.unitnormal(), axis=2) - vc.B_external_normal) / np.linalg.norm(Bbs, axis=2)\n    BdotN_mean = np.mean(BdotN)\n    BdotN_max = np.max(BdotN)\n    outstr = f\"J={J:.1e}, Jf={jf:.1e}, ⟨|B·n|⟩={BdotN_mean:.1e}, max(|B·n|)={BdotN_max:.1e}\"\n    cl_string = \", \".join([f\"{J.J():.1f}\" for J in Jls])\n    outstr += f\", Len=sum([{cl_string}])={sum(J.J() for J in Jls):.1f}\"\n    outstr += f\", ║∇J║={np.linalg.norm(grad):.1e}\"\n    print(outstr)\n    return 1e-4*J, 1e-4*grad\n\nprint(\"\")\nf = fun\ndofs = JF.x\nnp.random.seed(1)\nh = np.random.uniform(size=dofs.shape)\nJ0, dJ0 = f(dofs)\ndJh = sum(dJ0 * h)\nfor eps in [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\n    J1, _ = f(dofs + eps*h)\n    J2, _ = f(dofs - eps*h)\n    print(\"err\", (J1-J2)/(2*eps) - dJh)\n\nprint(\"\")\nres = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 300, 'ftol': 1e-20, 'gtol': 1e-20}, tol=1e-20)\ndofs = res.x\ncurves_to_vtk(curves, out_dir / \"curves_opt\")\nBbs = bs.B().reshape((nphi, ntheta, 3))\nBdotN = np.abs(np.sum(Bbs * s.unitnormal(), axis=2) - vc.B_external_normal) / np.linalg.norm(Bbs, axis=2)\npointData = {\"B_N\": BdotN[:, :, None]}\ns.to_vtk(out_dir / \"surf_opt\", extra_data=pointData)"
]