[
  "def read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname), encoding='utf-8').read()",
  "def read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname), encoding='utf-8').read()",
  "class OptimizeMode(Enum):\n    \"\"\"Optimize Mode class\n\n    if OptimizeMode is 'minimize', it means the tuner need to minimize the reward\n    that received from Trial.\n\n    if OptimizeMode is 'maximize', it means the tuner need to maximize the reward\n    that received from Trial.\n    \"\"\"\n    Minimize = 'minimize'\n    Maximize = 'maximize'",
  "class NodeType:\n    \"\"\"Node Type class\n    \"\"\"\n    ROOT = 'root'\n    TYPE = '_type'\n    VALUE = '_value'\n    INDEX = '_index'\n    NAME = '_name'",
  "class MetricType:\n    \"\"\"The types of metric data\n    \"\"\"\n    FINAL = 'FINAL'\n    PERIODICAL = 'PERIODICAL'\n    REQUEST_PARAMETER = 'REQUEST_PARAMETER'",
  "def split_index(params):\n    \"\"\"\n    Delete index infromation from params\n    \"\"\"\n    if isinstance(params, dict):\n        if NodeType.INDEX in params.keys():\n            return split_index(params[NodeType.VALUE])\n        result = {}\n        for key in params:\n            result[key] = split_index(params[key])\n        return result\n    else:\n        return params",
  "def extract_scalar_reward(value, scalar_key='default'):\n    \"\"\"\n    Extract scalar reward from trial result.\n\n    Parameters\n    ----------\n    value : int, float, dict\n        the reported final metric data\n    scalar_key : str\n        the key name that indicates the numeric number\n\n    Raises\n    ------\n    RuntimeError\n        Incorrect final result: the final result should be float/int,\n        or a dict which has a key named \"default\" whose value is float/int.\n    \"\"\"\n    if isinstance(value, (float, int)):\n        reward = value\n    elif isinstance(value, dict) and scalar_key in value and isinstance(value[scalar_key], (float, int)):\n        reward = value[scalar_key]\n    else:\n        raise RuntimeError('Incorrect final result: the final result should be float/int, ' \\\n            'or a dict which has a key named \"default\" whose value is float/int.')\n    return reward",
  "def extract_scalar_history(trial_history, scalar_key='default'):\n    \"\"\"\n    Extract scalar value from a list of intermediate results.\n\n    Parameters\n    ----------\n    trial_history : list\n        accumulated intermediate results of a trial\n    scalar_key : str\n        the key name that indicates the numeric number\n\n    Raises\n    ------\n    RuntimeError\n        Incorrect final result: the final result should be float/int,\n        or a dict which has a key named \"default\" whose value is float/int.\n    \"\"\"\n    return [extract_scalar_reward(ele, scalar_key) for ele in trial_history]",
  "def convert_dict2tuple(value):\n    \"\"\"\n    convert dict type to tuple to solve unhashable problem.\n    \"\"\"\n    if isinstance(value, dict):\n        for _keys in value:\n            value[_keys] = convert_dict2tuple(value[_keys])\n        return tuple(sorted(value.items()))\n    return value",
  "def init_dispatcher_logger():\n    \"\"\"\n    Initialize dispatcher logging configuration\n    \"\"\"\n    logger_file_path = 'dispatcher.log'\n    if dispatcher_env_vars.NNI_LOG_DIRECTORY is not None:\n        logger_file_path = os.path.join(dispatcher_env_vars.NNI_LOG_DIRECTORY, logger_file_path)\n    init_logger(logger_file_path, dispatcher_env_vars.NNI_LOG_LEVEL)",
  "def json2space(x, oldy=None, name=NodeType.ROOT):\n    \"\"\"\n    Change search space from json format to hyperopt format\n\n    \"\"\"\n    y = list()\n    if isinstance(x, dict):\n        if NodeType.TYPE in x.keys():\n            _type = x[NodeType.TYPE]\n            name = name + '-' + _type\n            if _type == 'choice':\n                if oldy is not None:\n                    _index = oldy[NodeType.INDEX]\n                    y += json2space(x[NodeType.VALUE][_index],\n                                    oldy[NodeType.VALUE], name=name+'[%d]' % _index)\n                else:\n                    y += json2space(x[NodeType.VALUE], None, name=name)\n            y.append(name)\n        else:\n            for key in x.keys():\n                y += json2space(x[key], oldy[key] if oldy else None, name+\"[%s]\" % str(key))\n    elif isinstance(x, list):\n        for i, x_i in enumerate(x):\n            if isinstance(x_i, dict):\n                if NodeType.NAME not in x_i.keys():\n                    raise RuntimeError('\\'_name\\' key is not found in this nested search space.')\n            y += json2space(x_i, oldy[i] if oldy else None, name + \"[%d]\" % i)\n    return y",
  "def json2parameter(x, is_rand, random_state, oldy=None, Rand=False, name=NodeType.ROOT):\n    \"\"\"\n    Json to pramaters.\n\n    \"\"\"\n    if isinstance(x, dict):\n        if NodeType.TYPE in x.keys():\n            _type = x[NodeType.TYPE]\n            _value = x[NodeType.VALUE]\n            name = name + '-' + _type\n            Rand |= is_rand[name]\n            if Rand is True:\n                if _type == 'choice':\n                    _index = random_state.randint(len(_value))\n                    y = {\n                        NodeType.INDEX: _index,\n                        NodeType.VALUE: json2parameter(\n                            x[NodeType.VALUE][_index],\n                            is_rand,\n                            random_state,\n                            None,\n                            Rand,\n                            name=name+\"[%d]\" % _index\n                        )\n                    }\n                else:\n                    y = getattr(parameter_expressions, _type)(*(_value + [random_state]))\n            else:\n                y = copy.deepcopy(oldy)\n        else:\n            y = dict()\n            for key in x.keys():\n                y[key] = json2parameter(\n                    x[key],\n                    is_rand,\n                    random_state,\n                    oldy[key] if oldy else None,\n                    Rand,\n                    name + \"[%s]\" % str(key)\n                )\n    elif isinstance(x, list):\n        y = list()\n        for i, x_i in enumerate(x):\n            if isinstance(x_i, dict):\n                if NodeType.NAME not in x_i.keys():\n                    raise RuntimeError('\\'_name\\' key is not found in this nested search space.')\n            y.append(json2parameter(\n                x_i,\n                is_rand,\n                random_state,\n                oldy[i] if oldy else None,\n                Rand,\n                name + \"[%d]\" % i\n            ))\n    else:\n        y = copy.deepcopy(x)\n    return y",
  "def merge_parameter(base_params, override_params):\n    \"\"\"\n    Update the parameters in ``base_params`` with ``override_params``.\n    Can be useful to override parsed command line arguments.\n\n    Parameters\n    ----------\n    base_params : namespace or dict\n        Base parameters. A key-value mapping.\n    override_params : dict or None\n        Parameters to override. Usually the parameters got from ``get_next_parameters()``.\n        When it is none, nothing will happen.\n\n    Returns\n    -------\n    namespace or dict\n        The updated ``base_params``. Note that ``base_params`` will be updated inplace. The return value is\n        only for convenience.\n    \"\"\"\n    if override_params is None:\n        return base_params\n    is_dict = isinstance(base_params, dict)\n    for k, v in override_params.items():\n        if is_dict:\n            if k not in base_params:\n                raise ValueError('Key \\'%s\\' not found in base parameters.' % k)\n            if type(base_params[k]) != type(v) and base_params[k] is not None:\n                raise TypeError('Expected \\'%s\\' in override parameters to have type \\'%s\\', but found \\'%s\\'.' %\n                                (k, type(base_params[k]), type(v)))\n            base_params[k] = v\n        else:\n            if not hasattr(base_params, k):\n                raise ValueError('Key \\'%s\\' not found in base parameters.' % k)\n            if type(getattr(base_params, k)) != type(v) and getattr(base_params, k) is not None:\n                raise TypeError('Expected \\'%s\\' in override parameters to have type \\'%s\\', but found \\'%s\\'.' %\n                                (k, type(getattr(base_params, k)), type(v)))\n            setattr(base_params, k, v)\n    return base_params",
  "class ClassArgsValidator(object):\n    \"\"\"\n    NNI tuners/assessors/adivisors accept a `classArgs` parameter in experiment configuration file.\n    This ClassArgsValidator interface is used to validate the classArgs section in exeperiment\n    configuration file.\n    \"\"\"\n    def validate_class_args(self, **kwargs):\n        \"\"\"\n        Validate the classArgs configuration in experiment configuration file.\n\n        Parameters\n        ----------\n        kwargs: dict\n            kwargs passed to tuner/assessor/advisor constructor\n\n        Raises:\n            Raise an execption if the kwargs is invalid.\n        \"\"\"\n        pass\n\n    def choices(self, key, *args):\n        \"\"\"\n        Utility method to create a scheme to check whether the `key` is one of the `args`.\n\n        Parameters:\n        ----------\n        key: str\n            key name of the data to be validated\n        args: list of str\n            list of the choices\n\n        Returns: Schema\n        --------\n            A scheme to check whether the `key` is one of the `args`.\n        \"\"\"\n        return And(lambda n: n in args, error='%s should be in [%s]!' % (key, str(args)))\n\n    def range(self, key, keyType, start, end):\n        \"\"\"\n        Utility method to create a schema to check whether the `key` is in the range of [start, end].\n\n        Parameters:\n        ----------\n        key: str\n            key name of the data to be validated\n        keyType: type\n            python data type, such as int, float\n        start: type is specified by keyType\n            start of the range\n        end: type is specified by keyType\n            end of the range\n\n        Returns: Schema\n        --------\n            A scheme to check whether the `key` is in the range of [start, end].\n        \"\"\"\n        return And(\n            And(keyType, error='%s should be %s type!' % (key, keyType.__name__)),\n            And(lambda n: start <= n <= end, error='%s should be in range of (%s, %s)!' % (key, start, end))\n        )",
  "def validate_class_args(self, **kwargs):\n        \"\"\"\n        Validate the classArgs configuration in experiment configuration file.\n\n        Parameters\n        ----------\n        kwargs: dict\n            kwargs passed to tuner/assessor/advisor constructor\n\n        Raises:\n            Raise an execption if the kwargs is invalid.\n        \"\"\"\n        pass",
  "def choices(self, key, *args):\n        \"\"\"\n        Utility method to create a scheme to check whether the `key` is one of the `args`.\n\n        Parameters:\n        ----------\n        key: str\n            key name of the data to be validated\n        args: list of str\n            list of the choices\n\n        Returns: Schema\n        --------\n            A scheme to check whether the `key` is one of the `args`.\n        \"\"\"\n        return And(lambda n: n in args, error='%s should be in [%s]!' % (key, str(args)))",
  "def range(self, key, keyType, start, end):\n        \"\"\"\n        Utility method to create a schema to check whether the `key` is in the range of [start, end].\n\n        Parameters:\n        ----------\n        key: str\n            key name of the data to be validated\n        keyType: type\n            python data type, such as int, float\n        start: type is specified by keyType\n            start of the range\n        end: type is specified by keyType\n            end of the range\n\n        Returns: Schema\n        --------\n            A scheme to check whether the `key` is in the range of [start, end].\n        \"\"\"\n        return And(\n            And(keyType, error='%s should be %s type!' % (key, keyType.__name__)),\n            And(lambda n: start <= n <= end, error='%s should be in range of (%s, %s)!' % (key, start, end))\n        )",
  "class Tuner(Recoverable):\n    \"\"\"\n    Tuner is an AutoML algorithm, which generates a new configuration for the next try.\n    A new trial will run with this configuration.\n\n    This is the abstract base class for all tuners.\n    Tuning algorithms should inherit this class and override :meth:`update_search_space`, :meth:`receive_trial_result`,\n    as well as :meth:`generate_parameters` or :meth:`generate_multiple_parameters`.\n\n    After initializing, NNI will first call :meth:`update_search_space` to tell tuner the feasible region,\n    and then call :meth:`generate_parameters` one or more times to request for hyper-parameter configurations.\n\n    The framework will train several models with given configuration.\n    When one of them is finished, the final accuracy will be reported to :meth:`receive_trial_result`.\n    And then another configuration will be reqeusted and trained, util the whole experiment finish.\n\n    If a tuner want's to know when a trial ends, it can also override :meth:`trial_end`.\n\n    Tuners use *parameter ID* to track trials.\n    In tuner context, there is a one-to-one mapping between parameter ID and trial.\n    When the framework ask tuner to generate hyper-parameters for a new trial,\n    an ID has already been assigned and can be recorded in :meth:`generate_parameters`.\n    Later when the trial ends, the ID will be reported to :meth:`trial_end`,\n    and :meth:`receive_trial_result` if it has a final result.\n    Parameter IDs are unique integers.\n\n    The type/format of search space and hyper-parameters are not limited,\n    as long as they are JSON-serializable and in sync with trial code.\n    For HPO tuners, however, there is a widely shared common interface,\n    which supports ``choice``, ``randint``, ``uniform``, and so on.\n    See ``docs/en_US/Tutorial/SearchSpaceSpec.md`` for details of this interface.\n\n    [WIP] For advanced tuners which take advantage of trials' intermediate results,\n    an ``Advisor`` interface is under development.\n\n    See Also\n    --------\n    Builtin tuners:\n    :class:`~nni.hyperopt_tuner.hyperopt_tuner.HyperoptTuner`\n    :class:`~nni.evolution_tuner.evolution_tuner.EvolutionTuner`\n    :class:`~nni.smac_tuner.SMACTuner`\n    :class:`~nni.gridsearch_tuner.GridSearchTuner`\n    :class:`~nni.networkmorphism_tuner.networkmorphism_tuner.NetworkMorphismTuner`\n    :class:`~nni.metis_tuner.mets_tuner.MetisTuner`\n    :class:`~nni.ppo_tuner.PPOTuner`\n    :class:`~nni.gp_tuner.gp_tuner.GPTuner`\n    \"\"\"\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Abstract method which provides a set of hyper-parameters.\n\n        This method will get called when the framework is about to launch a new trial,\n        if user does not override :meth:`generate_multiple_parameters`.\n\n        The return value of this method will be received by trials via :func:`nni.get_next_parameter`.\n        It should fit in the search space, though the framework will not verify this.\n\n        User code must override either this method or :meth:`generate_multiple_parameters`.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters. This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n\n        Returns\n        -------\n        any\n            The hyper-parameters, a dict in most cases, but could be any JSON-serializable type when needed.\n\n        Raises\n        ------\n        nni.NoMoreTrialError\n            If the search space is fully explored, tuner can raise this exception.\n        \"\"\"\n        # FIXME: some tuners raise NoMoreTrialError when they are waiting for more trial results\n        # we need to design a new exception for this purpose\n        raise NotImplementedError('Tuner: generate_parameters not implemented')\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Callback method which provides multiple sets of hyper-parameters.\n\n        This method will get called when the framework is about to launch one or more new trials.\n\n        If user does not override this method, it will invoke :meth:`generate_parameters` on each parameter ID.\n\n        See :meth:`generate_parameters` for details.\n\n        User code must override either this method or :meth:`generate_parameters`.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n\n        Returns\n        -------\n        list\n            List of hyper-parameters. An empty list indicates there are no more trials.\n        \"\"\"\n        result = []\n        for parameter_id in parameter_id_list:\n            try:\n                _logger.debug(\"generating param for %s\", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                return result\n            result.append(res)\n        return result\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Abstract method invoked when a trial reports its final result. Must override.\n\n        This method only listens to results of algorithm-generated hyper-parameters.\n        Currently customized trials added from web UI will not report result to this method.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        \"\"\"\n        raise NotImplementedError('Tuner: receive_trial_result not implemented')\n\n    def _accept_customized_trials(self, accept=True):\n        # FIXME: because Tuner is designed as interface, this API should not be here\n\n        # Enable or disable receiving results of user-added hyper-parameters.\n        # By default `receive_trial_result()` will only receive results of algorithm-generated hyper-parameters.\n        # If tuners want to receive those of customized parameters as well, they can call this function in `__init__()`.\n\n        # pylint: disable=attribute-defined-outside-init\n        self._accept_customized = accept\n\n    def trial_end(self, parameter_id, success, **kwargs):\n        \"\"\"\n        Abstract method invoked when a trial is completed or terminated. Do nothing by default.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        \"\"\"\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Abstract method for updating the search space. Must override.\n\n        Tuners are advised to support updating search space at run-time.\n        If a tuner can only set search space once before generating first hyper-parameters,\n        it should explicitly document this behaviour.\n\n        Parameters\n        ----------\n        search_space\n            JSON object defined by experiment owner.\n        \"\"\"\n        raise NotImplementedError('Tuner: update_search_space not implemented')\n\n    def load_checkpoint(self):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info('Load checkpoint ignored by tuner, checkpoint path: %s', checkpoin_path)\n\n    def save_checkpoint(self):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info('Save checkpoint ignored by tuner, checkpoint path: %s', checkpoin_path)\n\n    def import_data(self, data):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        # Import additional data for tuning\n        # data: a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n        pass\n\n    def _on_exit(self):\n        pass\n\n    def _on_error(self):\n        pass",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Abstract method which provides a set of hyper-parameters.\n\n        This method will get called when the framework is about to launch a new trial,\n        if user does not override :meth:`generate_multiple_parameters`.\n\n        The return value of this method will be received by trials via :func:`nni.get_next_parameter`.\n        It should fit in the search space, though the framework will not verify this.\n\n        User code must override either this method or :meth:`generate_multiple_parameters`.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters. This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n\n        Returns\n        -------\n        any\n            The hyper-parameters, a dict in most cases, but could be any JSON-serializable type when needed.\n\n        Raises\n        ------\n        nni.NoMoreTrialError\n            If the search space is fully explored, tuner can raise this exception.\n        \"\"\"\n        # FIXME: some tuners raise NoMoreTrialError when they are waiting for more trial results\n        # we need to design a new exception for this purpose\n        raise NotImplementedError('Tuner: generate_parameters not implemented')",
  "def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Callback method which provides multiple sets of hyper-parameters.\n\n        This method will get called when the framework is about to launch one or more new trials.\n\n        If user does not override this method, it will invoke :meth:`generate_parameters` on each parameter ID.\n\n        See :meth:`generate_parameters` for details.\n\n        User code must override either this method or :meth:`generate_parameters`.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n\n        Returns\n        -------\n        list\n            List of hyper-parameters. An empty list indicates there are no more trials.\n        \"\"\"\n        result = []\n        for parameter_id in parameter_id_list:\n            try:\n                _logger.debug(\"generating param for %s\", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                return result\n            result.append(res)\n        return result",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Abstract method invoked when a trial reports its final result. Must override.\n\n        This method only listens to results of algorithm-generated hyper-parameters.\n        Currently customized trials added from web UI will not report result to this method.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        \"\"\"\n        raise NotImplementedError('Tuner: receive_trial_result not implemented')",
  "def _accept_customized_trials(self, accept=True):\n        # FIXME: because Tuner is designed as interface, this API should not be here\n\n        # Enable or disable receiving results of user-added hyper-parameters.\n        # By default `receive_trial_result()` will only receive results of algorithm-generated hyper-parameters.\n        # If tuners want to receive those of customized parameters as well, they can call this function in `__init__()`.\n\n        # pylint: disable=attribute-defined-outside-init\n        self._accept_customized = accept",
  "def trial_end(self, parameter_id, success, **kwargs):\n        \"\"\"\n        Abstract method invoked when a trial is completed or terminated. Do nothing by default.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        \"\"\"",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Abstract method for updating the search space. Must override.\n\n        Tuners are advised to support updating search space at run-time.\n        If a tuner can only set search space once before generating first hyper-parameters,\n        it should explicitly document this behaviour.\n\n        Parameters\n        ----------\n        search_space\n            JSON object defined by experiment owner.\n        \"\"\"\n        raise NotImplementedError('Tuner: update_search_space not implemented')",
  "def load_checkpoint(self):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info('Load checkpoint ignored by tuner, checkpoint path: %s', checkpoin_path)",
  "def save_checkpoint(self):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info('Save checkpoint ignored by tuner, checkpoint path: %s', checkpoin_path)",
  "def import_data(self, data):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        # Import additional data for tuning\n        # data: a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n        pass",
  "def _on_exit(self):\n        pass",
  "def _on_error(self):\n        pass",
  "class Recoverable:\n\n    def load_checkpoint(self):\n        pass\n\n    def save_checkpoint(self):\n        pass\n\n    def get_checkpoint_path(self):\n        ckp_path = os.getenv('NNI_CHECKPOINT_DIRECTORY')\n        if ckp_path is not None and os.path.isdir(ckp_path):\n            return ckp_path\n        return None",
  "def load_checkpoint(self):\n        pass",
  "def save_checkpoint(self):\n        pass",
  "def get_checkpoint_path(self):\n        ckp_path = os.getenv('NNI_CHECKPOINT_DIRECTORY')\n        if ckp_path is not None and os.path.isdir(ckp_path):\n            return ckp_path\n        return None",
  "def get_all_builtin_names(algo_type):\n    \"\"\"Get all valid builtin names, including:\n    1. BuiltinAlgorithms which is pre-installed.\n    2. User installed packages in <nni_installation_path>/config/installed_packages.yml\n\n    Parameters\n    ----------\n    algo_type: str\n        can be one of 'tuners', 'assessors' or 'advisors'\n\n    Returns: list of string\n    -------\n    All builtin names of specified type, for example, if algo_type is 'tuners', returns\n    all builtin tuner names.\n    \"\"\"\n    assert algo_type in ALGO_TYPES\n    merged_dict = _get_merged_builtin_dict()\n\n    builtin_names = [x['name'] for x in merged_dict[algo_type]]\n    return builtin_names",
  "def get_not_installable_builtin_names(algo_type=None):\n    \"\"\"Get builtin names in BuiltinAlgorithms which do not need to be installed\n    and can be used once NNI is installed.\n\n    Parameters\n    ----------\n    algo_type: str | None\n        can be one of 'tuners', 'assessors', 'advisors' or None\n\n    Returns: list of string\n    -------\n    All builtin names of specified type, for example, if algo_type is 'tuners', returns\n    all builtin tuner names.\n    If algo_type is None, returns all builtin names of all types.\n    \"\"\"\n    if algo_type is None:\n        meta = BuiltinAlgorithms\n    else:\n        assert algo_type in ALGO_TYPES\n        meta = {\n            algo_type: BuiltinAlgorithms[algo_type]\n        }\n    names = []\n    for t in ALGO_TYPES:\n        if t in meta:\n            names.extend([x['name'] for x in meta[t]])\n    return names",
  "def get_builtin_algo_meta(algo_type=None, builtin_name=None):\n    \"\"\" Get meta information of builtin algorithms from:\n    1. Pre-installed BuiltinAlgorithms\n    2. User installed packages in <nni_installation_path>/config/installed_packages.yml\n\n    Parameters\n    ----------\n    algo_type: str | None\n        can be one of 'tuners', 'assessors', 'advisors' or None\n    builtin_name: str | None\n        builtin name.\n\n    Returns: dict | list of dict | None\n    -------\n        If builtin_name is specified, returns meta information of speicified builtin\n        alogorithms, for example:\n        {\n            'name': 'Random',\n            'class_name': 'nni.hyperopt_tuner.hyperopt_tuner.HyperoptTuner',\n            'class_args': {\n                'algorithm_name': 'random_search'\n            },\n            'accept_class_args': False,\n            'class_args_validator': 'nni.hyperopt_tuner.hyperopt_tuner.HyperoptClassArgsValidator'\n        }\n        If builtin_name is None, returns multiple meta information in a list.\n    \"\"\"\n    merged_dict = _get_merged_builtin_dict()\n\n    if algo_type is None and builtin_name is None:\n        return merged_dict\n\n    if algo_type:\n        assert algo_type in ALGO_TYPES\n        metas = merged_dict[algo_type]\n    else:\n        metas = merged_dict['tuners'] + merged_dict['assessors'] + merged_dict['advisors']\n    if builtin_name:\n        for m in metas:\n            if m['name'] == builtin_name:\n                return m\n    else:\n        return metas\n\n    return None",
  "def get_installed_package_meta(algo_type, builtin_name):\n    \"\"\" Get meta information of user installed algorithms from:\n    <nni_installation_path>/config/installed_packages.yml\n\n    Parameters\n    ----------\n    algo_type: str | None\n        can be one of 'tuners', 'assessors', 'advisors' or None\n    builtin_name: str\n        builtin name.\n\n    Returns: dict | None\n    -------\n        Returns meta information of speicified builtin alogorithms, for example:\n        {\n            'class_args_validator': 'nni.smac_tuner.smac_tuner.SMACClassArgsValidator',\n            'class_name': 'nni.smac_tuner.smac_tuner.SMACTuner',\n            'name': 'SMAC'\n        }\n    \"\"\"\n    assert builtin_name is not None\n    if algo_type:\n        assert algo_type in ALGO_TYPES\n    config = read_installed_package_meta()\n\n    candidates = []\n    if algo_type:\n        candidates = config[algo_type]\n    else:\n        for algo_type in ALGO_TYPES:\n            candidates.extend(config[algo_type])\n    for meta in candidates:\n        if meta['name'] == builtin_name:\n            return meta\n    return None",
  "def _parse_full_class_name(full_class_name):\n    if not full_class_name:\n        return None, None\n    parts = full_class_name.split('.')\n    module_name, class_name = '.'.join(parts[:-1]), parts[-1]\n    return module_name, class_name",
  "def get_builtin_module_class_name(algo_type, builtin_name):\n    \"\"\"Get module name and class name of all builtin algorithms\n\n    Parameters\n    ----------\n    algo_type: str\n        can be one of 'tuners', 'assessors', 'advisors'\n    builtin_name: str\n        builtin name.\n\n    Returns: tuple\n    -------\n        tuple of (module name, class name)\n    \"\"\"\n    assert algo_type in ALGO_TYPES\n    assert builtin_name is not None\n    meta = get_builtin_algo_meta(algo_type, builtin_name)\n    if not meta:\n        return None, None\n    return _parse_full_class_name(meta['class_name'])",
  "def create_validator_instance(algo_type, builtin_name):\n    \"\"\"Create instance of validator class\n\n    Parameters\n    ----------\n    algo_type: str\n        can be one of 'tuners', 'assessors', 'advisors'\n    builtin_name: str\n        builtin name.\n\n    Returns: object | None\n    -------\n        Returns validator class instance.\n        If specified validator class does not exist, returns None.\n    \"\"\"\n    assert algo_type in ALGO_TYPES\n    assert builtin_name is not None\n    meta = get_builtin_algo_meta(algo_type, builtin_name)\n    if not meta or 'class_args_validator' not in meta:\n        return None\n    module_name, class_name = _parse_full_class_name(meta['class_args_validator'])\n    class_module = importlib.import_module(module_name)\n    class_constructor = getattr(class_module, class_name)\n\n    return class_constructor()",
  "def create_builtin_class_instance(builtin_name, input_class_args, algo_type):\n    \"\"\"Create instance of builtin algorithms\n\n    Parameters\n    ----------\n    builtin_name: str\n        builtin name.\n    input_class_args: dict\n        kwargs for builtin class constructor\n    algo_type: str\n        can be one of 'tuners', 'assessors', 'advisors'\n\n    Returns: object\n    -------\n        Returns builtin class instance.\n    \"\"\"\n    assert algo_type in ALGO_TYPES\n    if builtin_name not in get_all_builtin_names(algo_type):\n        raise RuntimeError('Builtin name is not found: {}'.format(builtin_name))\n\n    def parse_algo_meta(algo_meta, input_class_args):\n        \"\"\"\n        1. parse class_name field in meta data into module name and class name,\n        for example:\n            parse class_name 'nni.hyperopt_tuner.hyperopt_tuner.HyperoptTuner' in meta data into:\n            module name: nni.hyperopt_tuner.hyperopt_tuner\n            class name: HyperoptTuner\n        2. merge user specified class args together with builtin class args.\n        \"\"\"\n        assert algo_meta\n        module_name, class_name = _parse_full_class_name(algo_meta['class_name'])\n\n        class_args = {}\n        if 'class_args' in algo_meta:\n            class_args = algo_meta['class_args']\n        if input_class_args is not None:\n            class_args.update(input_class_args)\n\n        return module_name, class_name, class_args\n\n    algo_meta = get_builtin_algo_meta(algo_type, builtin_name)\n    module_name, class_name, class_args = parse_algo_meta(algo_meta, input_class_args)\n\n    if importlib.util.find_spec(module_name) is None:\n        raise RuntimeError('Builtin module can not be loaded: {}'.format(module_name))\n\n    class_module = importlib.import_module(module_name)\n    class_constructor = getattr(class_module, class_name)\n\n    instance = class_constructor(**class_args)\n\n    return instance",
  "def create_customized_class_instance(class_params):\n    \"\"\"Create instance of customized algorithms\n\n    Parameters\n    ----------\n    class_params: dict\n        class_params should contains following keys:\n            codeDir: code directory\n            classFileName: python file name of the class\n            className: class name\n            classArgs (optional): kwargs pass to class constructor\n    Returns: object\n    -------\n        Returns customized class instance.\n    \"\"\"\n\n    code_dir = class_params.get('codeDir')\n    class_filename = class_params.get('classFileName')\n    class_name = class_params.get('className')\n    class_args = class_params.get('classArgs')\n\n    if not os.path.isfile(os.path.join(code_dir, class_filename)):\n        raise ValueError('Class file not found: {}'.format(\n            os.path.join(code_dir, class_filename)))\n    sys.path.append(code_dir)\n    module_name = os.path.splitext(class_filename)[0]\n    class_module = importlib.import_module(module_name)\n    class_constructor = getattr(class_module, class_name)\n\n    if class_args is None:\n        class_args = {}\n    instance = class_constructor(**class_args)\n\n    return instance",
  "def get_nni_installation_parent_dir():\n    ''' Find nni installation parent directory\n    '''\n    if os.getenv('VIRTUAL_ENV'):\n        # if 'virtualenv' package is used, `site` has not attr getsitepackages, so we will instead use VIRTUAL_ENV\n        # Note that conda venv will not have VIRTUAL_ENV\n        python_dir = os.getenv('VIRTUAL_ENV')\n    else:\n        python_sitepackage = site.getsitepackages()[0]\n        # If system-wide python is used, we will give priority to using `local sitepackage`--\"usersitepackages()\" given\n        # that nni exists there\n        if python_sitepackage.startswith('/usr') or python_sitepackage.startswith('/Library'):\n            python_dir = _try_installation_path_sequentially(site.getusersitepackages(), *site.getsitepackages())\n        else:\n            python_dir = _try_installation_path_sequentially(*site.getsitepackages(), site.getusersitepackages())\n    return python_dir",
  "def _try_installation_path_sequentially(*sitepackages):\n    '''Try different installation path sequentially util nni is found.\n    Return None if nothing is found\n    '''\n    for sitepackage in sitepackages:\n        path = Path(sitepackage)\n        if len(path.parents) > 2 and (path.parents[2] / 'nni' / 'main.js').is_file():\n            return str(path.parents[2])\n        if (path / 'nni' / 'main.js').is_file():\n            return str(path)\n    return None",
  "def get_nni_installation_path():\n    ''' Find nni installation directory\n    '''\n    parent_dir = get_nni_installation_parent_dir()\n    if parent_dir:\n        entry_file = os.path.join(parent_dir, 'nni', 'main.js')\n        if os.path.isfile(entry_file):\n            return os.path.join(parent_dir, 'nni')\n    return None",
  "def get_nni_config_dir():\n    return os.path.join(get_nni_installation_path(), 'config')",
  "def get_package_config_path():\n    config_dir = get_nni_config_dir()\n    if not os.path.exists(config_dir):\n        os.makedirs(config_dir, exist_ok=True)\n    return os.path.join(config_dir, 'installed_packages.yml')",
  "def read_installed_package_meta():\n    config_file = get_package_config_path()\n    if os.path.exists(config_file):\n        with open(config_file, 'r') as f:\n            config = yaml.load(f, Loader=yaml.Loader)\n    else:\n        config = defaultdict(list)\n    for t in ALGO_TYPES:\n        if t not in config:\n            config[t] = []\n    return config",
  "def write_package_meta(config):\n    config_file = get_package_config_path()\n    with open(config_file, 'w') as f:\n        f.write(yaml.dump(dict(config), default_flow_style=False))",
  "def _get_merged_builtin_dict():\n    def merge_meta_dict(d1, d2):\n        res = defaultdict(list)\n        for t in ALGO_TYPES:\n            res[t] = d1[t] + d2[t]\n        return res\n\n    return merge_meta_dict(BuiltinAlgorithms, read_installed_package_meta())",
  "def parse_algo_meta(algo_meta, input_class_args):\n        \"\"\"\n        1. parse class_name field in meta data into module name and class name,\n        for example:\n            parse class_name 'nni.hyperopt_tuner.hyperopt_tuner.HyperoptTuner' in meta data into:\n            module name: nni.hyperopt_tuner.hyperopt_tuner\n            class name: HyperoptTuner\n        2. merge user specified class args together with builtin class args.\n        \"\"\"\n        assert algo_meta\n        module_name, class_name = _parse_full_class_name(algo_meta['class_name'])\n\n        class_args = {}\n        if 'class_args' in algo_meta:\n            class_args = algo_meta['class_args']\n        if input_class_args is not None:\n            class_args.update(input_class_args)\n\n        return module_name, class_name, class_args",
  "def merge_meta_dict(d1, d2):\n        res = defaultdict(list)\n        for t in ALGO_TYPES:\n            res[t] = d1[t] + d2[t]\n        return res",
  "def main():\n    parser = argparse.ArgumentParser(description='Dispatcher command line parser')\n    parser.add_argument('--exp_params', type=str, required=True)\n    args, _ = parser.parse_known_args()\n\n    exp_params_decode = base64.b64decode(args.exp_params).decode('utf-8')\n    logger.debug('decoded exp_params: [%s]', exp_params_decode)\n    exp_params = json.loads(exp_params_decode)\n    logger.debug('exp_params json obj: [%s]', json.dumps(exp_params, indent=4))\n\n    if exp_params.get('multiThread'):\n        enable_multi_thread()\n    if exp_params.get('multiPhase'):\n        enable_multi_phase()\n\n    if exp_params.get('advisor') is not None:\n        # advisor is enabled and starts to run\n        _run_advisor(exp_params)\n    else:\n        # tuner (and assessor) is enabled and starts to run\n        assert exp_params.get('tuner') is not None\n        tuner = _create_tuner(exp_params)\n        if exp_params.get('assessor') is not None:\n            assessor = _create_assessor(exp_params)\n        else:\n            assessor = None\n        dispatcher = MsgDispatcher(tuner, assessor)\n\n        try:\n            dispatcher.run()\n            tuner._on_exit()\n            if assessor is not None:\n                assessor._on_exit()\n        except Exception as exception:\n            logger.exception(exception)\n            tuner._on_error()\n            if assessor is not None:\n                assessor._on_error()\n            raise",
  "def _run_advisor(exp_params):\n    if exp_params.get('advisor').get('builtinAdvisorName'):\n        dispatcher = create_builtin_class_instance(\n            exp_params.get('advisor').get('builtinAdvisorName'),\n            exp_params.get('advisor').get('classArgs'),\n            'advisors')\n    else:\n        dispatcher = create_customized_class_instance(exp_params.get('advisor'))\n    if dispatcher is None:\n        raise AssertionError('Failed to create Advisor instance')\n    try:\n        dispatcher.run()\n    except Exception as exception:\n        logger.exception(exception)\n        raise",
  "def _create_tuner(exp_params):\n    if exp_params.get('tuner').get('builtinTunerName'):\n        tuner = create_builtin_class_instance(\n            exp_params.get('tuner').get('builtinTunerName'),\n            exp_params.get('tuner').get('classArgs'),\n            'tuners')\n    else:\n        tuner = create_customized_class_instance(exp_params.get('tuner'))\n    if tuner is None:\n        raise AssertionError('Failed to create Tuner instance')\n    return tuner",
  "def _create_assessor(exp_params):\n    if exp_params.get('assessor').get('builtinAssessorName'):\n        assessor = create_builtin_class_instance(\n            exp_params.get('assessor').get('builtinAssessorName'),\n            exp_params.get('assessor').get('classArgs'),\n            'assessors')\n    else:\n        assessor = create_customized_class_instance(exp_params.get('assessor'))\n    if assessor is None:\n        raise AssertionError('Failed to create Assessor instance')\n    return assessor",
  "class AssessResult(Enum):\n    \"\"\"\n    Enum class for :meth:`Assessor.assess_trial` return value.\n    \"\"\"\n\n    Good = True\n    \"\"\"The trial works well.\"\"\"\n\n    Bad = False\n    \"\"\"The trial works poorly and should be early stopped.\"\"\"",
  "class Assessor(Recoverable):\n    \"\"\"\n    Assessor analyzes trial's intermediate results (e.g., periodically evaluated accuracy on test dataset)\n    to tell whether this trial can be early stopped or not.\n\n    This is the abstract base class for all assessors.\n    Early stopping algorithms should inherit this class and override :meth:`assess_trial` method,\n    which receives intermediate results from trials and give an assessing result.\n\n    If :meth:`assess_trial` returns :obj:`AssessResult.Bad` for a trial,\n    it hints NNI framework that the trial is likely to result in a poor final accuracy,\n    and therefore should be killed to save resource.\n\n    If an accessor want's to be notified when a trial ends, it can also override :meth:`trial_end`.\n\n    To write a new assessor, you can reference :class:`~nni.medianstop_assessor.MedianstopAssessor`'s code as an example.\n\n    See Also\n    --------\n    Builtin assessors:\n    :class:`~nni.medianstop_assessor.MedianstopAssessor`\n    :class:`~nni.curvefitting_assessor.CurvefittingAssessor`\n    \"\"\"\n\n    def assess_trial(self, trial_job_id, trial_history):\n        \"\"\"\n        Abstract method for determining whether a trial should be killed. Must override.\n\n        The NNI framework has little guarantee on ``trial_history``.\n        This method is not guaranteed to be invoked for each time ``trial_history`` get updated.\n        It is also possible that a trial's history keeps updating after receiving a bad result.\n        And if the trial failed and retried, ``trial_history`` may be inconsistent with its previous value.\n\n        The only guarantee is that ``trial_history`` is always growing.\n        It will not be empty and will always be longer than previous value.\n\n        This is an example of how :meth:`assess_trial` get invoked sequentially:\n\n        ::\n\n            trial_job_id | trial_history   | return value\n            ------------ | --------------- | ------------\n            Trial_A      | [1.0, 2.0]      | Good\n            Trial_B      | [1.5, 1.3]      | Bad\n            Trial_B      | [1.5, 1.3, 1.9] | Good\n            Trial_A      | [0.9, 1.8, 2.3] | Good\n\n        Parameters\n        ----------\n        trial_job_id : str\n            Unique identifier of the trial.\n        trial_history : list\n            Intermediate results of this trial. The element type is decided by trial code.\n\n        Returns\n        -------\n        AssessResult\n            :obj:`AssessResult.Good` or :obj:`AssessResult.Bad`.\n        \"\"\"\n        raise NotImplementedError('Assessor: assess_trial not implemented')\n\n    def trial_end(self, trial_job_id, success):\n        \"\"\"\n        Abstract method invoked when a trial is completed or terminated. Do nothing by default.\n\n        Parameters\n        ----------\n        trial_job_id : str\n            Unique identifier of the trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        \"\"\"\n\n    def load_checkpoint(self):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info('Load checkpoint ignored by assessor, checkpoint path: %s', checkpoin_path)\n\n    def save_checkpoint(self):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info('Save checkpoint ignored by assessor, checkpoint path: %s', checkpoin_path)\n\n    def _on_exit(self):\n        pass\n\n    def _on_error(self):\n        pass",
  "def assess_trial(self, trial_job_id, trial_history):\n        \"\"\"\n        Abstract method for determining whether a trial should be killed. Must override.\n\n        The NNI framework has little guarantee on ``trial_history``.\n        This method is not guaranteed to be invoked for each time ``trial_history`` get updated.\n        It is also possible that a trial's history keeps updating after receiving a bad result.\n        And if the trial failed and retried, ``trial_history`` may be inconsistent with its previous value.\n\n        The only guarantee is that ``trial_history`` is always growing.\n        It will not be empty and will always be longer than previous value.\n\n        This is an example of how :meth:`assess_trial` get invoked sequentially:\n\n        ::\n\n            trial_job_id | trial_history   | return value\n            ------------ | --------------- | ------------\n            Trial_A      | [1.0, 2.0]      | Good\n            Trial_B      | [1.5, 1.3]      | Bad\n            Trial_B      | [1.5, 1.3, 1.9] | Good\n            Trial_A      | [0.9, 1.8, 2.3] | Good\n\n        Parameters\n        ----------\n        trial_job_id : str\n            Unique identifier of the trial.\n        trial_history : list\n            Intermediate results of this trial. The element type is decided by trial code.\n\n        Returns\n        -------\n        AssessResult\n            :obj:`AssessResult.Good` or :obj:`AssessResult.Bad`.\n        \"\"\"\n        raise NotImplementedError('Assessor: assess_trial not implemented')",
  "def trial_end(self, trial_job_id, success):\n        \"\"\"\n        Abstract method invoked when a trial is completed or terminated. Do nothing by default.\n\n        Parameters\n        ----------\n        trial_job_id : str\n            Unique identifier of the trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        \"\"\"",
  "def load_checkpoint(self):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info('Load checkpoint ignored by assessor, checkpoint path: %s', checkpoin_path)",
  "def save_checkpoint(self):\n        \"\"\"\n        Internal API under revising, not recommended for end users.\n        \"\"\"\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info('Save checkpoint ignored by assessor, checkpoint path: %s', checkpoin_path)",
  "def _on_exit(self):\n        pass",
  "def _on_error(self):\n        pass",
  "class MsgDispatcherBase(Recoverable):\n    \"\"\"This is where tuners and assessors are not defined yet.\n    Inherits this class to make your own advisor.\n    \"\"\"\n\n    def __init__(self):\n        if multi_thread_enabled():\n            self.pool = ThreadPool()\n            self.thread_results = []\n        else:\n            self.stopping = False\n            self.default_command_queue = Queue()\n            self.assessor_command_queue = Queue()\n            self.default_worker = threading.Thread(target=self.command_queue_worker, args=(self.default_command_queue,))\n            self.assessor_worker = threading.Thread(target=self.command_queue_worker,\n                                                    args=(self.assessor_command_queue,))\n            self.default_worker.start()\n            self.assessor_worker.start()\n            self.worker_exceptions = []\n\n    def run(self):\n        \"\"\"Run the tuner.\n        This function will never return unless raise.\n        \"\"\"\n        _logger.info('Start dispatcher')\n        if dispatcher_env_vars.NNI_MODE == 'resume':\n            self.load_checkpoint()\n\n        while True:\n            command, data = receive()\n            if data:\n                data = json_tricks.loads(data)\n\n            if command is None or command is CommandType.Terminate:\n                break\n            if multi_thread_enabled():\n                result = self.pool.map_async(self.process_command_thread, [(command, data)])\n                self.thread_results.append(result)\n                if any([thread_result.ready() and not thread_result.successful() for thread_result in\n                        self.thread_results]):\n                    _logger.debug('Caught thread exception')\n                    break\n            else:\n                self.enqueue_command(command, data)\n                if self.worker_exceptions:\n                    break\n\n        _logger.info('Dispatcher exiting...')\n        self.stopping = True\n        if multi_thread_enabled():\n            self.pool.close()\n            self.pool.join()\n        else:\n            self.default_worker.join()\n            self.assessor_worker.join()\n\n        _logger.info('Terminated by NNI manager')\n\n    def command_queue_worker(self, command_queue):\n        \"\"\"Process commands in command queues.\n        \"\"\"\n        while True:\n            try:\n                # set timeout to ensure self.stopping is checked periodically\n                command, data = command_queue.get(timeout=3)\n                try:\n                    self.process_command(command, data)\n                except Exception as e:\n                    _logger.exception(e)\n                    self.worker_exceptions.append(e)\n                    break\n            except Empty:\n                pass\n            if self.stopping and (_worker_fast_exit_on_terminate or command_queue.empty()):\n                break\n\n    def enqueue_command(self, command, data):\n        \"\"\"Enqueue command into command queues\n        \"\"\"\n        if command == CommandType.TrialEnd or (\n                command == CommandType.ReportMetricData and data['type'] == 'PERIODICAL'):\n            self.assessor_command_queue.put((command, data))\n        else:\n            self.default_command_queue.put((command, data))\n\n        qsize = self.default_command_queue.qsize()\n        if qsize >= QUEUE_LEN_WARNING_MARK:\n            _logger.warning('default queue length: %d', qsize)\n\n        qsize = self.assessor_command_queue.qsize()\n        if qsize >= QUEUE_LEN_WARNING_MARK:\n            _logger.warning('assessor queue length: %d', qsize)\n\n    def process_command_thread(self, request):\n        \"\"\"Worker thread to process a command.\n        \"\"\"\n        command, data = request\n        if multi_thread_enabled():\n            try:\n                self.process_command(command, data)\n            except Exception as e:\n                _logger.exception(str(e))\n                raise\n        else:\n            pass\n\n    def process_command(self, command, data):\n        _logger.debug('process_command: command: [%s], data: [%s]', command, data)\n\n        command_handlers = {\n            # Tuner commands:\n            CommandType.Initialize: self.handle_initialize,\n            CommandType.RequestTrialJobs: self.handle_request_trial_jobs,\n            CommandType.UpdateSearchSpace: self.handle_update_search_space,\n            CommandType.ImportData: self.handle_import_data,\n            CommandType.AddCustomizedTrialJob: self.handle_add_customized_trial,\n\n            # Tuner/Assessor commands:\n            CommandType.ReportMetricData: self.handle_report_metric_data,\n\n            CommandType.TrialEnd: self.handle_trial_end,\n            CommandType.Ping: self.handle_ping,\n        }\n        if command not in command_handlers:\n            raise AssertionError('Unsupported command: {}'.format(command))\n        command_handlers[command](data)\n\n    def handle_ping(self, data):\n        pass\n\n    def handle_initialize(self, data):\n        \"\"\"Initialize search space and tuner, if any\n        This method is meant to be called only once for each experiment, after calling this method,\n        dispatcher should `send(CommandType.Initialized, '')`, to set the status of the experiment to be \"INITIALIZED\".\n        Parameters\n        ----------\n        data: dict\n            search space\n        \"\"\"\n        raise NotImplementedError('handle_initialize not implemented')\n\n    def handle_request_trial_jobs(self, data):\n        \"\"\"The message dispatcher is demanded to generate ``data`` trial jobs.\n        These trial jobs should be sent via ``send(CommandType.NewTrialJob, json_tricks.dumps(parameter))``,\n        where ``parameter`` will be received by NNI Manager and eventually accessible to trial jobs as \"next parameter\".\n        Semantically, message dispatcher should do this ``send`` exactly ``data`` times.\n\n        The JSON sent by this method should follow the format of\n\n        ::\n\n            {\n                \"parameter_id\": 42\n                \"parameters\": {\n                    // this will be received by trial\n                },\n                \"parameter_source\": \"algorithm\" // optional\n            }\n\n        Parameters\n        ----------\n        data: int\n            number of trial jobs\n        \"\"\"\n        raise NotImplementedError('handle_request_trial_jobs not implemented')\n\n    def handle_update_search_space(self, data):\n        \"\"\"This method will be called when search space is updated.\n        It's recommended to call this method in `handle_initialize` to initialize search space.\n        *No need to* notify NNI Manager when this update is done.\n        Parameters\n        ----------\n        data: dict\n            search space\n        \"\"\"\n        raise NotImplementedError('handle_update_search_space not implemented')\n\n    def handle_import_data(self, data):\n        \"\"\"Import previous data when experiment is resumed.\n        Parameters\n        ----------\n        data: list\n            a list of dictionaries, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        raise NotImplementedError('handle_import_data not implemented')\n\n    def handle_add_customized_trial(self, data):\n        \"\"\"Experimental API. Not recommended for usage.\n        \"\"\"\n        raise NotImplementedError('handle_add_customized_trial not implemented')\n\n    def handle_report_metric_data(self, data):\n        \"\"\"Called when metric data is reported or new parameters are requested (for multiphase).\n        When new parameters are requested, this method should send a new parameter.\n\n        Parameters\n        ----------\n        data: dict\n            a dict which contains 'parameter_id', 'value', 'trial_job_id', 'type', 'sequence'.\n            type: can be `MetricType.REQUEST_PARAMETER`, `MetricType.FINAL` or `MetricType.PERIODICAL`.\n            `REQUEST_PARAMETER` is used to request new parameters for multiphase trial job. In this case,\n            the dict will contain additional keys: `trial_job_id`, `parameter_index`. Refer to `msg_dispatcher.py`\n            as an example.\n\n        Raises\n        ------\n        ValueError\n            Data type is not supported\n        \"\"\"\n        raise NotImplementedError('handle_report_metric_data not implemented')\n\n    def handle_trial_end(self, data):\n        \"\"\"Called when the state of one of the trials is changed\n\n        Parameters\n        ----------\n        data: dict\n            a dict with keys: trial_job_id, event, hyper_params.\n            trial_job_id: the id generated by training service.\n            event: the job\u2019s state.\n            hyper_params: the string that is sent by message dispatcher during the creation of trials.\n\n        \"\"\"\n        raise NotImplementedError('handle_trial_end not implemented')",
  "def __init__(self):\n        if multi_thread_enabled():\n            self.pool = ThreadPool()\n            self.thread_results = []\n        else:\n            self.stopping = False\n            self.default_command_queue = Queue()\n            self.assessor_command_queue = Queue()\n            self.default_worker = threading.Thread(target=self.command_queue_worker, args=(self.default_command_queue,))\n            self.assessor_worker = threading.Thread(target=self.command_queue_worker,\n                                                    args=(self.assessor_command_queue,))\n            self.default_worker.start()\n            self.assessor_worker.start()\n            self.worker_exceptions = []",
  "def run(self):\n        \"\"\"Run the tuner.\n        This function will never return unless raise.\n        \"\"\"\n        _logger.info('Start dispatcher')\n        if dispatcher_env_vars.NNI_MODE == 'resume':\n            self.load_checkpoint()\n\n        while True:\n            command, data = receive()\n            if data:\n                data = json_tricks.loads(data)\n\n            if command is None or command is CommandType.Terminate:\n                break\n            if multi_thread_enabled():\n                result = self.pool.map_async(self.process_command_thread, [(command, data)])\n                self.thread_results.append(result)\n                if any([thread_result.ready() and not thread_result.successful() for thread_result in\n                        self.thread_results]):\n                    _logger.debug('Caught thread exception')\n                    break\n            else:\n                self.enqueue_command(command, data)\n                if self.worker_exceptions:\n                    break\n\n        _logger.info('Dispatcher exiting...')\n        self.stopping = True\n        if multi_thread_enabled():\n            self.pool.close()\n            self.pool.join()\n        else:\n            self.default_worker.join()\n            self.assessor_worker.join()\n\n        _logger.info('Terminated by NNI manager')",
  "def command_queue_worker(self, command_queue):\n        \"\"\"Process commands in command queues.\n        \"\"\"\n        while True:\n            try:\n                # set timeout to ensure self.stopping is checked periodically\n                command, data = command_queue.get(timeout=3)\n                try:\n                    self.process_command(command, data)\n                except Exception as e:\n                    _logger.exception(e)\n                    self.worker_exceptions.append(e)\n                    break\n            except Empty:\n                pass\n            if self.stopping and (_worker_fast_exit_on_terminate or command_queue.empty()):\n                break",
  "def enqueue_command(self, command, data):\n        \"\"\"Enqueue command into command queues\n        \"\"\"\n        if command == CommandType.TrialEnd or (\n                command == CommandType.ReportMetricData and data['type'] == 'PERIODICAL'):\n            self.assessor_command_queue.put((command, data))\n        else:\n            self.default_command_queue.put((command, data))\n\n        qsize = self.default_command_queue.qsize()\n        if qsize >= QUEUE_LEN_WARNING_MARK:\n            _logger.warning('default queue length: %d', qsize)\n\n        qsize = self.assessor_command_queue.qsize()\n        if qsize >= QUEUE_LEN_WARNING_MARK:\n            _logger.warning('assessor queue length: %d', qsize)",
  "def process_command_thread(self, request):\n        \"\"\"Worker thread to process a command.\n        \"\"\"\n        command, data = request\n        if multi_thread_enabled():\n            try:\n                self.process_command(command, data)\n            except Exception as e:\n                _logger.exception(str(e))\n                raise\n        else:\n            pass",
  "def process_command(self, command, data):\n        _logger.debug('process_command: command: [%s], data: [%s]', command, data)\n\n        command_handlers = {\n            # Tuner commands:\n            CommandType.Initialize: self.handle_initialize,\n            CommandType.RequestTrialJobs: self.handle_request_trial_jobs,\n            CommandType.UpdateSearchSpace: self.handle_update_search_space,\n            CommandType.ImportData: self.handle_import_data,\n            CommandType.AddCustomizedTrialJob: self.handle_add_customized_trial,\n\n            # Tuner/Assessor commands:\n            CommandType.ReportMetricData: self.handle_report_metric_data,\n\n            CommandType.TrialEnd: self.handle_trial_end,\n            CommandType.Ping: self.handle_ping,\n        }\n        if command not in command_handlers:\n            raise AssertionError('Unsupported command: {}'.format(command))\n        command_handlers[command](data)",
  "def handle_ping(self, data):\n        pass",
  "def handle_initialize(self, data):\n        \"\"\"Initialize search space and tuner, if any\n        This method is meant to be called only once for each experiment, after calling this method,\n        dispatcher should `send(CommandType.Initialized, '')`, to set the status of the experiment to be \"INITIALIZED\".\n        Parameters\n        ----------\n        data: dict\n            search space\n        \"\"\"\n        raise NotImplementedError('handle_initialize not implemented')",
  "def handle_request_trial_jobs(self, data):\n        \"\"\"The message dispatcher is demanded to generate ``data`` trial jobs.\n        These trial jobs should be sent via ``send(CommandType.NewTrialJob, json_tricks.dumps(parameter))``,\n        where ``parameter`` will be received by NNI Manager and eventually accessible to trial jobs as \"next parameter\".\n        Semantically, message dispatcher should do this ``send`` exactly ``data`` times.\n\n        The JSON sent by this method should follow the format of\n\n        ::\n\n            {\n                \"parameter_id\": 42\n                \"parameters\": {\n                    // this will be received by trial\n                },\n                \"parameter_source\": \"algorithm\" // optional\n            }\n\n        Parameters\n        ----------\n        data: int\n            number of trial jobs\n        \"\"\"\n        raise NotImplementedError('handle_request_trial_jobs not implemented')",
  "def handle_update_search_space(self, data):\n        \"\"\"This method will be called when search space is updated.\n        It's recommended to call this method in `handle_initialize` to initialize search space.\n        *No need to* notify NNI Manager when this update is done.\n        Parameters\n        ----------\n        data: dict\n            search space\n        \"\"\"\n        raise NotImplementedError('handle_update_search_space not implemented')",
  "def handle_import_data(self, data):\n        \"\"\"Import previous data when experiment is resumed.\n        Parameters\n        ----------\n        data: list\n            a list of dictionaries, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        raise NotImplementedError('handle_import_data not implemented')",
  "def handle_add_customized_trial(self, data):\n        \"\"\"Experimental API. Not recommended for usage.\n        \"\"\"\n        raise NotImplementedError('handle_add_customized_trial not implemented')",
  "def handle_report_metric_data(self, data):\n        \"\"\"Called when metric data is reported or new parameters are requested (for multiphase).\n        When new parameters are requested, this method should send a new parameter.\n\n        Parameters\n        ----------\n        data: dict\n            a dict which contains 'parameter_id', 'value', 'trial_job_id', 'type', 'sequence'.\n            type: can be `MetricType.REQUEST_PARAMETER`, `MetricType.FINAL` or `MetricType.PERIODICAL`.\n            `REQUEST_PARAMETER` is used to request new parameters for multiphase trial job. In this case,\n            the dict will contain additional keys: `trial_job_id`, `parameter_index`. Refer to `msg_dispatcher.py`\n            as an example.\n\n        Raises\n        ------\n        ValueError\n            Data type is not supported\n        \"\"\"\n        raise NotImplementedError('handle_report_metric_data not implemented')",
  "def handle_trial_end(self, data):\n        \"\"\"Called when the state of one of the trials is changed\n\n        Parameters\n        ----------\n        data: dict\n            a dict with keys: trial_job_id, event, hyper_params.\n            trial_job_id: the id generated by training service.\n            event: the job\u2019s state.\n            hyper_params: the string that is sent by message dispatcher during the creation of trials.\n\n        \"\"\"\n        raise NotImplementedError('handle_trial_end not implemented')",
  "def classic_mode(\n        mutable_id,\n        mutable_layer_id,\n        funcs,\n        funcs_args,\n        fixed_inputs,\n        optional_inputs,\n        optional_input_size):\n    '''Execute the chosen function and inputs directly.\n    In this mode, the trial code is only running the chosen subgraph (i.e., the chosen ops and inputs),\n    without touching the full model graph.'''\n    if trial.get_current_parameter() is None:\n        trial.get_next_parameter()\n\n    chosen_layer, chosen_inputs = _get_layer_and_inputs_from_tuner(mutable_id, mutable_layer_id,\n                                                                   list(optional_inputs.keys()))\n    real_chosen_inputs = [optional_inputs[input_name] for input_name in chosen_inputs]\n    layer_out = funcs[chosen_layer]([fixed_inputs, real_chosen_inputs], **funcs_args[chosen_layer])\n\n    return layer_out",
  "def enas_mode(\n        mutable_id,\n        mutable_layer_id,\n        funcs,\n        funcs_args,\n        fixed_inputs,\n        optional_inputs,\n        optional_input_size,\n        tf):\n    '''For enas mode, we build the full model graph in trial but only run a subgraph\u3002\n    This is implemented by masking inputs and branching ops.\n    Specifically, based on the received subgraph (through nni.get_next_parameter),\n    it can be known which inputs should be masked and which op should be executed.'''\n    name_prefix = \"{}_{}\".format(mutable_id, mutable_layer_id)\n    # store namespace\n    _namespace[mutable_id] = True\n    _namespace[name_prefix] = dict()\n    _namespace[name_prefix]['funcs'] = list(funcs)\n    _namespace[name_prefix]['optional_inputs'] = list(optional_inputs)\n    # create tensorflow variables as 1/0 signals used to form subgraph\n    name_for_optional_inputs = name_prefix + '_optional_inputs'\n    name_for_funcs = name_prefix + '_funcs'\n    _tf_variables[name_prefix] = dict()\n    _tf_variables[name_prefix]['optional_inputs'] = tf.get_variable(\n        name_for_optional_inputs,\n        [len(optional_inputs)],\n        dtype=tf.bool,\n        trainable=False\n    )\n    _tf_variables[name_prefix]['funcs'] = tf.get_variable(\n        name_for_funcs, [], dtype=tf.int64, trainable=False)\n\n    # get real values using their variable names\n    real_optional_inputs_value = [optional_inputs[name]\n                                  for name in _namespace[name_prefix]['optional_inputs']]\n    real_func_value = [funcs[name]\n                       for name in _namespace[name_prefix]['funcs']]\n    real_funcs_args = [funcs_args[name]\n                       for name in _namespace[name_prefix]['funcs']]\n    # build tensorflow graph of geting chosen inputs by masking\n    real_chosen_inputs = tf.boolean_mask(\n        real_optional_inputs_value, _tf_variables[name_prefix]['optional_inputs'])\n    # build tensorflow graph of different branches by using tf.case\n    branches = dict()\n    func_output = None\n    for func_id in range(len(funcs)):\n        func_output = real_func_value[func_id]([fixed_inputs, real_chosen_inputs], **real_funcs_args[func_id])\n        branches[tf.equal(_tf_variables[name_prefix]['funcs'], func_id)] = lambda: func_output\n    layer_out = tf.case(branches, exclusive=True, default=lambda: func_output)\n\n    return layer_out",
  "def oneshot_mode(\n        mutable_id,\n        mutable_layer_id,\n        funcs,\n        funcs_args,\n        fixed_inputs,\n        optional_inputs,\n        optional_input_size,\n        tf):\n    '''Similar to enas mode, oneshot mode also builds the full model graph.\n    The difference is that oneshot mode does not receive subgraph.\n    Instead, it uses dropout to randomly dropout inputs and ops.'''\n    # NNI requires to get_next_parameter before report a result. But the parameter will not be used in this mode\n    if trial.get_current_parameter() is None:\n        trial.get_next_parameter()\n    optional_inputs = list(optional_inputs.values())\n    inputs_num = len(optional_inputs)\n    # Calculate dropout rate according to the formular r^(1/k), where r is a hyper-parameter and k is the number of inputs\n    if inputs_num > 0:\n        rate = 0.01 ** (1 / inputs_num)\n        noise_shape = [inputs_num] + [1] * len(optional_inputs[0].get_shape())\n        optional_inputs = tf.nn.dropout(\n            optional_inputs, rate=rate, noise_shape=noise_shape)\n        optional_inputs = [optional_inputs[idx] for idx in range(inputs_num)]\n    layer_outs = [func([fixed_inputs, optional_inputs], **funcs_args[func_name])\n                  for func_name, func in funcs.items()]\n    output_num = len(layer_outs)\n    rate = 0.01 ** (1 / output_num)\n    noise_shape = [output_num] + [1] * len(layer_outs[0].get_shape())\n    layer_outs = tf.nn.dropout(layer_outs, rate=rate, noise_shape=noise_shape)\n    layer_out = tf.reduce_sum(layer_outs, axis=0)\n\n    return layer_out",
  "def darts_mode(\n        mutable_id,\n        mutable_layer_id,\n        funcs,\n        funcs_args,\n        fixed_inputs,\n        optional_inputs,\n        optional_input_size,\n        tf):\n    optional_inputs = list(optional_inputs.values())\n    layer_outs = [func([fixed_inputs, optional_inputs], **funcs_args[func_name])\n                  for func_name, func in funcs.items()]\n    # Create architecture weights for every func(op)\n    var_name = \"{}_{}_arch_weights\".format(mutable_id, mutable_layer_id)\n    arch_logits = tf.get_variable(var_name, shape=[len(funcs)], trainable=False)\n    _arch_logits_list.append(arch_logits)\n    arch_weights = tf.nn.softmax(arch_logits)\n    layer_out = tf.add_n([arch_weights[idx] * out for idx, out in enumerate(layer_outs)])\n\n    return layer_out",
  "def reload_tensorflow_variables(tf, session):\n    '''In Enas mode, this function reload every signal varaible created in `enas_mode` function so\n    the whole tensorflow graph will be changed into certain subgraph recerived from Tuner.\n    ---------------\n    session: the tensorflow session created by users\n    tf: tensorflow module\n    '''\n    subgraph_from_tuner = trial.get_next_parameter()\n    mutable_layers = set()\n    for subgraph_key in subgraph_from_tuner:\n        if \"/\" in subgraph_key:\n            # has to remove the last, could be layer_choice or whatever\n            mutable_id, mutable_layer_id = _decompose_general_key(subgraph_key[:subgraph_key.rfind(\"/\")])\n            if mutable_id is not None:\n                mutable_layers.add((mutable_id, mutable_layer_id))\n    mutable_layers = sorted(list(mutable_layers))\n    for mutable_id, mutable_layer_id in mutable_layers:\n        if mutable_id not in _namespace:\n            _logger.warning(\"%s not found in name space\", mutable_id)\n            continue\n        name_prefix = \"{}_{}\".format(mutable_id, mutable_layer_id)\n        # get optional inputs names\n        optional_inputs = _namespace[name_prefix]['optional_inputs']\n        # extract layer information from the subgraph sampled by tuner\n        chosen_layer, chosen_inputs = _get_layer_and_inputs_from_tuner(mutable_id, mutable_layer_id, optional_inputs)\n        chosen_layer = _namespace[name_prefix]['funcs'].index(chosen_layer)\n        chosen_inputs = [1 if inp in chosen_inputs else 0 for inp in optional_inputs]\n        # load these information into pre-defined tensorflow variables\n        _tf_variables[name_prefix]['funcs'].load(chosen_layer, session)\n        _tf_variables[name_prefix]['optional_inputs'].load(\n            chosen_inputs, session)",
  "def _construct_general_key(mutable_id, mutable_layer_id):\n    # Mutable layer key in a general (search space) format\n    # that is, prefix/mutable_id/mutable_layer_id\n    return _MUTABLE_LAYER_SPACE_PREFIX + \"/\" + mutable_id + \"/\" + mutable_layer_id",
  "def _decompose_general_key(key):\n    # inverse operation of above\n    if not key.startswith(_MUTABLE_LAYER_SPACE_PREFIX):\n        return None, None\n    else:\n        _, mutable_id, mutable_layer_id = key.split(\"/\", maxsplit=2)\n        return mutable_id, mutable_layer_id",
  "def darts_training(tf, session, loss, feed_dict):\n    global _optimizer, _train_op\n    if _optimizer is None:\n        _optimizer = tf.MomentumOptimizer(learning_rate=0.025)\n        # TODO: Calculate loss\n        grads_and_vars = _optimizer.compute_gradients(loss, _arch_logits_list)\n        _train_op = _optimizer.apply_gradients(grads_and_vars)\n    session.run(_train_op)",
  "def training_update(nas_mode, tf=None, session=None, loss=None, feed_dict=None):\n    if nas_mode == 'darts_mode':\n        darts_training(tf, session, loss, feed_dict)\n    elif nas_mode == 'enas_mode':\n        reload_tensorflow_variables(tf, session)",
  "def _get_layer_and_inputs_from_tuner(mutable_id, mutable_layer_id, optional_inputs):\n    # optional_inputs should be name(key)s of the optional inputs\n    try:\n        mutable_block = trial.get_current_parameter(mutable_id)\n\n        # There is a NAS tuner\n        chosen_layer = mutable_block[mutable_layer_id][\"chosen_layer\"]\n        chosen_inputs = mutable_block[mutable_layer_id][\"chosen_inputs\"]\n    except KeyError:\n        # Try to find converted NAS parameters\n        params = trial.get_current_parameter()\n        expected_prefix = _construct_general_key(mutable_id, mutable_layer_id)\n        chosen_layer = params[expected_prefix + \"/layer_choice\"]\n\n        # find how many to choose\n        optional_input_size = int(params[expected_prefix + \"/optional_input_size\"])  # convert uniform to randint\n\n        # find who to choose, can duplicate\n        optional_input_state = params[expected_prefix + \"/optional_input_chosen_state\"]\n        chosen_inputs = []\n        # make sure dict -> list produce stable result by sorting\n        optional_inputs_keys = sorted(optional_inputs)\n        for _ in range(optional_input_size):\n            chosen_inputs.append(optional_inputs_keys[optional_input_state % len(optional_inputs)])\n            optional_input_state //= len(optional_inputs)\n\n    _logger.info(\"%s_%s: layer: %s, optional inputs: %s\", mutable_id, mutable_layer_id, chosen_layer, chosen_inputs)\n    return chosen_layer, chosen_inputs",
  "def convert_nas_search_space(search_space):\n    \"\"\"\n    Args:\n        param search_space: raw search space\n        return: the new search space, mutable_layers will be converted into choice\n    \"\"\"\n    if not isinstance(search_space, dict):\n        return search_space\n    ret = dict()\n    for k, v in search_space.items():\n        if \"_type\" not in v:\n            # this should not happen\n            _logger.warning(\"There is no _type in one of your search space values with key '%s'\"\n                            \". Please check your search space\", k)\n            ret[k] = v\n        elif v[\"_type\"] != \"mutable_layer\":\n            ret[k] = v\n        else:\n            _logger.info(\"Converting mutable_layer search space with key '%s'\", k)\n            # v[\"_value\"] looks like {'mutable_layer_1': {'layer_choice': ...} ...}\n            values = v[\"_value\"]\n            for layer_name, layer_data in values.items():\n                # there should be at most layer_choice, optional_inputs, optional_input_size in layer_data\n\n                # add \"_mutable_layer\" as prefix so that they can be recovered later\n                layer_key = _construct_general_key(k, layer_name)\n\n                if layer_data.get(\"layer_choice\"):  # filter out empty choice and no choice\n                    layer_choice = layer_data[\"layer_choice\"]\n                else:\n                    raise ValueError(\"No layer choice found in %s\" % layer_key)\n\n                if layer_data.get(\"optional_input_size\"):\n                    input_size = layer_data[\"optional_input_size\"]\n                    if isinstance(input_size, int):\n                        input_size = [input_size, input_size]\n                    if input_size[0] > input_size[1] or input_size[0] < 0:\n                        _logger.error(\"Might not be able to handle optional_input_size < 0, please double check\")\n                    input_size[1] += 1\n                else:\n                    _logger.info(\"Optional input choices are set to empty by default in %s\", layer_key)\n                    input_size = [0, 1]\n\n                if layer_data.get(\"optional_inputs\"):\n                    total_state_size = len(layer_data[\"optional_inputs\"]) ** (input_size[1] - 1)\n                else:\n                    _logger.info(\"Optional inputs not found in %s\", layer_key)\n                    total_state_size = 1\n\n                converted = {\n                    layer_key + \"/layer_choice\": {\n                        \"_type\": \"choice\", \"_value\": layer_choice\n                    },\n                    layer_key + \"/optional_input_size\": {\n                        \"_type\": \"randint\", \"_value\": input_size\n                    },\n                    layer_key + \"/optional_input_chosen_state\": {\n                        \"_type\": \"randint\", \"_value\": [0, total_state_size]\n                    }\n                }\n                _logger.info(converted)\n                ret.update(converted)\n\n    return ret",
  "def rewrite_nas_space(func):\n    @functools.wraps(func)\n    def wrap(self, search_space):\n        search_space = convert_nas_search_space(search_space)\n        return func(self, search_space)\n    return wrap",
  "def wrap(self, search_space):\n        search_space = convert_nas_search_space(search_space)\n        return func(self, search_space)",
  "def _load_env_vars(env_var_names):\n    env_var_dict = {k: os.environ.get(k) for k in env_var_names}\n    return namedtuple('EnvVars', env_var_names)(**env_var_dict)",
  "class NoMoreTrialError(Exception):\n    def __init__(self, ErrorInfo):\n        super().__init__(self)\n        self.errorinfo = ErrorInfo\n\n    def __str__(self):\n        return self.errorinfo",
  "def __init__(self, ErrorInfo):\n        super().__init__(self)\n        self.errorinfo = ErrorInfo",
  "def __str__(self):\n        return self.errorinfo",
  "def choice(*options, name=None):\n        return param_exp.choice(options, np.random.RandomState())",
  "def randint(lower, upper, name=None):\n        return param_exp.randint(lower, upper, np.random.RandomState())",
  "def uniform(low, high, name=None):\n        return param_exp.uniform(low, high, np.random.RandomState())",
  "def quniform(low, high, q, name=None):\n        assert high > low, 'Upper bound must be larger than lower bound'\n        return param_exp.quniform(low, high, q, np.random.RandomState())",
  "def loguniform(low, high, name=None):\n        assert low > 0, 'Lower bound must be positive'\n        return param_exp.loguniform(low, high, np.random.RandomState())",
  "def qloguniform(low, high, q, name=None):\n        return param_exp.qloguniform(low, high, q, np.random.RandomState())",
  "def normal(mu, sigma, name=None):\n        return param_exp.normal(mu, sigma, np.random.RandomState())",
  "def qnormal(mu, sigma, q, name=None):\n        return param_exp.qnormal(mu, sigma, q, np.random.RandomState())",
  "def lognormal(mu, sigma, name=None):\n        return param_exp.lognormal(mu, sigma, np.random.RandomState())",
  "def qlognormal(mu, sigma, q, name=None):\n        return param_exp.qlognormal(mu, sigma, q, np.random.RandomState())",
  "def function_choice(*funcs, name=None):\n        return param_exp.choice(funcs, np.random.RandomState())()",
  "def mutable_layer():\n        raise RuntimeError('Cannot call nni.mutable_layer in this mode')",
  "def choice(options, name=None, key=None):\n        return options[_get_param(key)]",
  "def randint(lower, upper, name=None, key=None):\n        return _get_param(key)",
  "def uniform(low, high, name=None, key=None):\n        return _get_param(key)",
  "def quniform(low, high, q, name=None, key=None):\n        return _get_param(key)",
  "def loguniform(low, high, name=None, key=None):\n        return _get_param(key)",
  "def qloguniform(low, high, q, name=None, key=None):\n        return _get_param(key)",
  "def normal(mu, sigma, name=None, key=None):\n        return _get_param(key)",
  "def qnormal(mu, sigma, q, name=None, key=None):\n        return _get_param(key)",
  "def lognormal(mu, sigma, name=None, key=None):\n        return _get_param(key)",
  "def qlognormal(mu, sigma, q, name=None, key=None):\n        return _get_param(key)",
  "def function_choice(funcs, name=None, key=None):\n        return funcs[_get_param(key)]()",
  "def mutable_layer(\n            mutable_id,\n            mutable_layer_id,\n            funcs,\n            funcs_args,\n            fixed_inputs,\n            optional_inputs,\n            optional_input_size,\n            mode='classic_mode',\n            tf=None):\n        '''execute the chosen function and inputs.\n        Below is an example of chosen function and inputs:\n        {\n            \"mutable_id\": {\n                \"mutable_layer_id\": {\n                    \"chosen_layer\": \"pool\",\n                    \"chosen_inputs\": [\"out1\", \"out3\"]\n                }\n            }\n        }\n        Parameters:\n        ---------------\n        mutable_id: the name of this mutable_layer block (which could have multiple mutable layers)\n        mutable_layer_id: the name of a mutable layer in this block\n        funcs: dict of function calls\n        funcs_args:\n        fixed_inputs:\n        optional_inputs: dict of optional inputs\n        optional_input_size: number of candidate inputs to be chosen\n        tf: tensorflow module\n        '''\n        args = (mutable_id, mutable_layer_id, funcs, funcs_args, fixed_inputs, optional_inputs, optional_input_size)\n        if mode == 'classic_mode':\n            return classic_mode(*args)\n        assert tf is not None, 'Internal Error: Tensorflow should not be None in modes other than classic_mode'\n        if mode == 'enas_mode':\n            return enas_mode(*args, tf)\n        if mode == 'oneshot_mode':\n            return oneshot_mode(*args, tf)\n        if mode == 'darts_mode':\n            return darts_mode(*args, tf)\n        raise RuntimeError('Unrecognized mode: %s' % mode)",
  "def _get_param(key):\n        if trial.get_current_parameter() is None:\n            trial.get_next_parameter()\n        return trial.get_current_parameter(key)",
  "def get_next_parameter():\n    \"\"\"\n    Get the hyper paremeters generated by tuner. For a multiphase experiment, it returns a new group of hyper\n    parameters at each call of get_next_parameter. For a non-multiphase (multiPhase is not configured or set to False)\n    experiment, it returns hyper parameters only on the first call for each trial job, it returns None since second call.\n    This API should be called only once in each trial job of an experiment which is not specified as multiphase.\n\n    Returns\n    -------\n    dict\n        A dict object contains the hyper parameters generated by tuner, the keys of the dict are defined in\n        search space. Returns None if no more hyper parameters can be generated by tuner.\n    \"\"\"\n    global _params\n    _params = platform.get_next_parameter()\n    if _params is None:\n        return None\n    return _params['parameters']",
  "def get_current_parameter(tag=None):\n    \"\"\"\n    Get current hyper parameters generated by tuner. It returns the same group of hyper parameters as the last\n    call of get_next_parameter returns.\n\n    Parameters\n    ----------\n    tag: str\n        hyper parameter key\n    \"\"\"\n    global _params\n    if _params is None:\n        return None\n    if tag is None:\n        return _params['parameters']\n    return _params['parameters'][tag]",
  "def get_experiment_id():\n    \"\"\"\n    Get experiment ID.\n\n    Returns\n    -------\n    str\n        Identifier of current experiment\n    \"\"\"\n    return _experiment_id",
  "def get_trial_id():\n    \"\"\"\n    Get trial job ID which is string identifier of a trial job, for example 'MoXrp'. In one experiment, each trial\n    job has an unique string ID.\n\n    Returns\n    -------\n    str\n        Identifier of current trial job which is calling this API.\n    \"\"\"\n    return _trial_id",
  "def get_sequence_id():\n    \"\"\"\n    Get trial job sequence nubmer. A sequence number is an integer value assigned to each trial job base on the\n    order they are submitted, incremental starting from 0. In one experiment, both trial job ID and sequence number\n    are unique for each trial job, they are of different data types.\n\n    Returns\n    -------\n    int\n        Sequence number of current trial job which is calling this API.\n    \"\"\"\n    return _sequence_id",
  "def report_intermediate_result(metric):\n    \"\"\"\n    Reports intermediate result to NNI.\n\n    Parameters\n    ----------\n    metric:\n        serializable object.\n    \"\"\"\n    global _intermediate_seq\n    assert _params or trial_env_vars.NNI_PLATFORM is None, \\\n        'nni.get_next_parameter() needs to be called before report_intermediate_result'\n    metric = to_json({\n        'parameter_id': _params['parameter_id'] if _params else None,\n        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,\n        'type': 'PERIODICAL',\n        'sequence': _intermediate_seq,\n        'value': to_json(metric)\n    })\n    _intermediate_seq += 1\n    platform.send_metric(metric)",
  "def report_final_result(metric):\n    \"\"\"\n    Reports final result to NNI.\n\n    Parameters\n    ----------\n    metric: serializable object\n        Usually (for built-in tuners to work), it should be a number, or\n        a dict with key \"default\" (a number), and any other extra keys.\n    \"\"\"\n    assert _params or trial_env_vars.NNI_PLATFORM is None, \\\n        'nni.get_next_parameter() needs to be called before report_final_result'\n    metric = to_json({\n        'parameter_id': _params['parameter_id'] if _params else None,\n        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,\n        'type': 'FINAL',\n        'sequence': 0,\n        'value': to_json(metric)\n    })\n    platform.send_metric(metric)",
  "class _LoggerFileWrapper(TextIOBase):\n    def __init__(self, logger_file):\n        self.file = logger_file\n\n    def write(self, s):\n        if s != '\\n':\n            cur_time = datetime.now().strftime(_time_format)\n            self.file.write('[{}] PRINT '.format(cur_time) + s + '\\n')\n            self.file.flush()\n        return len(s)",
  "def init_logger(logger_file_path, log_level_name='info'):\n    \"\"\"Initialize root logger.\n    This will redirect anything from logging.getLogger() as well as stdout to specified file.\n    logger_file_path: path of logger file (path-like object).\n    \"\"\"\n    log_level = log_level_map.get(log_level_name, logging.INFO)\n    logger_file = open(logger_file_path, 'w')\n    fmt = '[%(asctime)s] %(levelname)s (%(name)s/%(threadName)s) %(message)s'\n    logging.Formatter.converter = time.localtime\n    formatter = logging.Formatter(fmt, _time_format)\n    handler = logging.StreamHandler(logger_file)\n    handler.setFormatter(formatter)\n\n    root_logger = logging.getLogger()\n    root_logger.addHandler(handler)\n    root_logger.setLevel(log_level)\n\n    # these modules are too verbose\n    logging.getLogger('matplotlib').setLevel(log_level)\n\n    sys.stdout = _LoggerFileWrapper(logger_file)",
  "def init_standalone_logger():\n    \"\"\"\n    Initialize root logger for standalone mode.\n    This will set NNI's log level to INFO and print its log to stdout.\n    \"\"\"\n    fmt = '[%(asctime)s] %(levelname)s (%(name)s) %(message)s'\n    formatter = logging.Formatter(fmt, _time_format)\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(formatter)\n    nni_logger = logging.getLogger('nni')\n    nni_logger.addHandler(handler)\n    nni_logger.setLevel(logging.INFO)\n    nni_logger.propagate = False\n\n    # Following line does not affect NNI loggers, but without this user's logger won't be able to\n    # print log even it's level is set to INFO, so we do it for user's convenience.\n    # If this causes any issue in future, remove it and use `logging.info` instead of\n    # `logging.getLogger('xxx')` in all examples.\n    logging.basicConfig()",
  "def enable_multi_thread():\n    global _multi_thread\n    _multi_thread = True",
  "def multi_thread_enabled():\n    return _multi_thread",
  "def enable_multi_phase():\n    global _multi_phase\n    _multi_phase = True",
  "def multi_phase_enabled():\n    return _multi_phase",
  "def __init__(self, logger_file):\n        self.file = logger_file",
  "def write(self, s):\n        if s != '\\n':\n            cur_time = datetime.now().strftime(_time_format)\n            self.file.write('[{}] PRINT '.format(cur_time) + s + '\\n')\n            self.file.flush()\n        return len(s)",
  "def build_module_graph(model, dummy_input):\n    return TorchModuleGraph(model, dummy_input)",
  "def build_graph(model, dummy_input, verbose=False):\n    g = TorchProtoGraph(model, dummy_input, verbose)\n    return g.graph_def, g.stepstats",
  "def parse_traced_name(module_name):\n    prefix = 'TracedModule['\n    suffix = ']'\n    if module_name.startswith(prefix) and module_name.endswith(suffix):\n        module_name = module_name[len(prefix):-len(suffix)]\n    return module_name",
  "class TorchGraph:\n    \"\"\"\n    This class is to extract pytorch model topology graph by tracing\n    \"\"\"\n\n    def __init__(self, model=None, dummy_input=None, traced_model=None):\n        \"\"\"\n        Parameters\n        ----------\n        model : pytorch model\n            The model user wants to speed up\n        dummy_input : pytorch tensor\n            The dummy input for ```jit.trace```, users should put it on right device before pass in\n        traced_model : torch._C.torch.jit.TopLevelTracedModule\n            An alredy traced model, if traced_model is not None, then TorchGraph will build the graph\n            based on this traced model and won't trace the model again.\n        \"\"\"\n        assert torch.__version__ >= '1.3.1'\n        # check if the input is legal\n        if traced_model is not None:\n            assert isinstance(traced_model, torch.jit.TopLevelTracedModule)\n            self.trace = traced_model\n            # it's ok if the graph is already unpacked\n            torch._C._jit_pass_inline(self.trace.graph)\n        elif model is not None and dummy_input is not None:\n            self.bound_model = model\n            self._trace(model, dummy_input)\n        else:\n            raise Exception(\n                'Please provide model & dummy_input or the traced_model as inputs')\n\n    def _trace(self, model, dummy_input):\n        with torch.onnx.set_training(model, False):\n            self.trace = torch.jit.trace(model, dummy_input)\n            torch._C._jit_pass_inline(self.trace.graph)",
  "class TorchProtoGraph(TorchGraph):\n    \"\"\"\n    Generates model graph for pytorch models in protobuf, this implementation\n    is borrowed from pytorch v1.4.0, and fixed following issues:\n    https://github.com/pytorch/pytorch/issues/33691\n    https://github.com/pytorch/pytorch/issues/33670\n\n    \"\"\"\n\n    def __init__(self, model, dummy_input, verbose=False):\n        super().__init__(model, dummy_input)\n\n        from tensorboard.compat.proto.config_pb2 import RunMetadata\n        from tensorboard.compat.proto.graph_pb2 import GraphDef\n        from tensorboard.compat.proto.step_stats_pb2 import StepStats, DeviceStepStats\n        from tensorboard.compat.proto.versions_pb2 import VersionDef\n\n        list_of_nodes = self.parse(self.trace.graph, self.trace, dummy_input)\n        if verbose:\n            print(self.trace.graph)\n        self.stepstats = RunMetadata(step_stats=StepStats(\n            dev_stats=[DeviceStepStats(device=\"/device:CPU:0\")]))\n        self.graph_def = GraphDef(\n            node=list_of_nodes, versions=VersionDef(producer=22))\n\n    def parse(self, graph, trace, args=None, omit_useless_nodes=True):\n        \"\"\"This method parses an optimized PyTorch model graph and produces\n        a list of nodes and node stats for eventual conversion to TensorBoard\n        protobuf format.\n\n        Args:\n        graph (PyTorch module): The model graph to be parsed.\n        trace (PyTorch JIT TracedModule): The model trace to be parsed.\n        args (tuple): input tensor[s] for the model.\n        omit_useless_nodes (boolean): Whether to remove nodes from the graph.\n        \"\"\"\n        nodes_py = GraphPy()\n        for node in graph.inputs():\n            if omit_useless_nodes:\n                if not node.uses():  # number of user of the node (= number of outputs/ fanout)\n                    continue\n\n            if node.type().kind() != CLASSTYPE_KIND:\n                nodes_py.append(NodePyIO(node, 'input'))\n\n        attr_to_scope = dict()\n\n        def node_to_name(d):\n            return str(d).split(\":\")[0].strip()\n        for node in graph.nodes():\n            if node.kind() == GETATTR_KIND:\n                attr_name = node.s('name')\n                node_name = node_to_name(node)\n                parent = node.input().node()\n                # If the parent node is not the top-level \"self\" node\n                if parent.kind() == GETATTR_KIND:\n                    parent_scope = attr_to_scope[node_to_name(parent)]\n                    attr_scope = parent_scope.split('/')[-1]\n                    attr_to_scope[node_name] = '{}/{}.{}'.format(\n                        parent_scope, attr_scope, attr_name)\n                else:\n                    attr_to_scope[node_name] = '__module.{}'.format(attr_name)\n                # We don't need classtype nodes; scope will provide this information\n                if node.output().type().kind() != CLASSTYPE_KIND:\n                    node_py = NodePyOP(node)\n                    node_py.scopeName = attr_to_scope[node_name]\n                    nodes_py.append(node_py)\n            else:\n                nodes_py.append(NodePyOP(node))\n\n        # Create sink nodes for output ops\n        for i, node in enumerate(graph.outputs()):\n            node_py = NodePyIO(node, 'output')\n            node_py.debugName = \"output.{}\".format(i + 1)\n            node_py.inputs = [node.debugName()]\n            nodes_py.append(node_py)\n\n        alias_to_name = dict()\n        base_name = parse_traced_name(trace._name)\n        for name, module in trace.named_modules(prefix='__module'):\n            mod_name = parse_traced_name(module._name)\n            attr_name = name.split('.')[-1]\n            alias_to_name[name] = '{}[{}]'.format(mod_name, attr_name)\n\n        for node in nodes_py.nodes_op:\n            module_aliases = node.scopeName.split('/')[-1].split('.')\n            module_name = ''\n            for i, alias in enumerate(module_aliases):\n                if i == 0:\n                    module_name = alias\n                    node.scopeName = base_name\n                else:\n                    module_name += '.' + alias\n                    node.scopeName += '/' + \\\n                        (alias_to_name[module_name]\n                         if module_name in alias_to_name else alias)\n\n        nodes_py.populate_namespace_from_OP_to_IO()\n        return nodes_py.to_proto()",
  "class NodePyGroup(NodePy):\n    \"\"\"\n    This class is used to represent a graph node which consists of multiple jit traced nodes. In a pytorch trace graph,\n    there are multiple nodes are traced for one torch.nn.Module object, we group them together to form a single node to\n    represent the torch.nn.Module object. We also group some functional call trace nodes together to form a new node.\n    \"\"\"\n\n    def __init__(self, name, unique_name, node_type, op_type, node_cpps, inputs=None, outputs=None, key_node=None):\n        \"\"\"\n        Parameters:\n        -----------\n        name: str\n            node name, such as `conv1`, `backbone.classifier`\n        unique_name: str\n            A global unique name for current node. Due to some modules,\n            such as relu, may be reused several times, so the scopename\n            is not suitable as the global unique identifier, so we add a\n            unique_name for each node as the global unique identifier.\n            We should use the unique_name to traverset the module graph.\n        node_type: str\n            `module` or `func`\n        op_type: str\n            operation type, such as `Conv2d`, `aten::view`\n        node_cpps: list of torch._C.Node\n            jit trace nodes which are included in this new node\n        inputs: list of str\n            All the inputs of this node, each element is debugName of one input\n        outputs: list of str\n            All the outputs of this node, each element is debugName of one output\n        key_node: torch._C.Node\n            The key node of this NodePyGroup.\n        \"\"\"\n        super(NodePyGroup, self).__init__(name, [])\n        self.node_cpps = node_cpps\n        self.name = name\n        self.unique_name = unique_name\n        self.op_type = op_type\n        self.type = node_type\n        self.nodes = []\n        self.auxiliary = None\n        self.add_nodes(node_cpps)\n        self.inputs = inputs\n        self.outputs = outputs\n        # The core node in this NodePyGroup\n        self.key_node = key_node\n\n    def add_nodes(self, node_cpps):\n        for node_cpp in node_cpps:\n            nodepy = NodePyOP(node_cpp)\n            nodepy.name = node_cpp.scopeName() + '_' + node_cpp.kind()\n            self.nodes.append(nodepy)\n\n    def sub_node_names(self):\n        return [x.name for x in self.nodes]\n\n    def __repr__(self):\n        return 'name: {}, type: {}, op_type: {}, sub_nodes: {}, inputs: {}, outputs: {}, aux: {}'.format(\n            self.name, self.type, self.op_type, self.sub_node_names(),\n            self.inputs, self.outputs, self.auxiliary\n        )",
  "class TorchModuleGraph(TorchGraph):\n    \"\"\"\n    Generates model graph, each node is created from single or multiple jit trace nodes.\n    \"\"\"\n\n    def __init__(self, model=None, dummy_input=None, traced_model=None):\n        super().__init__(model, dummy_input, traced_model)\n        self.global_count = 0\n        self.name_to_node, self.input_to_node, self.output_to_node = self._build_graph()\n        self._extract_auxiliary_info()\n\n    def _expand_key_func_node(self, node, nodes, input_to_node, output_to_node,\n                              module_type):\n        \"\"\"\n        For trace graph nodes, some nodes are not in modules, these nodes are usually generated by\n        the functions directly called in module ```forward```. For such nodes, some of them are\n        trivial op which are label by ```prim::```, some of them are not such ops which is call\n        non-prim ops. This function is to merge neighbor prim ops to a non-prim op, to construct\n        a node.\n\n        Parameters\n        ----------\n        node : trace graph node\n            The non-prim node to expand\n        nodes : list of trace graph node\n            All the trace graph nodes within the same scope as the non-prim node\n        input_to_node : dict\n            key: input name, value: a node that uses this input\n        output_to_node : dict\n            key: output name, value: a node that generates this output\n        module_type : str\n            can be 'module' or 'func'\n\n        Returns\n        -------\n        node\n            the expanded non-prim node\n        \"\"\"\n        # TODO: scope name could be empty\n        node_name = '.'.join([self._get_module_name(\n            node.scopeName()), node.kind(), str(self.global_count)])\n        unique_name = node_name\n        _logger.debug(\"expand non-prim node, node name: %s\", node_name)\n        self.global_count += 1\n        op_type = node.kind()\n        node_group = [node]\n        inputs = list()\n        outputs = list()\n        node_queue = queue.Queue()\n        node_queue.put(node)\n        while not node_queue.empty():\n            curr_node = node_queue.get()\n            for _input in curr_node.inputs():\n                input_name = _input.debugName()\n                if input_name in output_to_node and output_to_node[input_name] in nodes:\n                    predecessor_node = output_to_node[input_name]\n                    if not self._is_key_func(predecessor_node):\n                        node_group.append(predecessor_node)\n                        node_queue.put(predecessor_node)\n                    else:\n                        inputs.append(input_name)\n                else:\n                    inputs.append(input_name)\n        for output in node.outputs():\n            outputs.append(output.debugName())\n        nodepy = NodePyGroup(node_name, unique_name, module_type, op_type,\n                             node_group, inputs=inputs, outputs=outputs, key_node=node)\n        return nodepy\n\n    def _expand_module_node(self, node, node_name, unique_name, op_type, nodes,\n                            input_to_node, output_to_node, module_type):\n        \"\"\"\n        merge the adjacent nodes of the module. The difference between the\n        _expand_module_node and _expand_non_prim_node is that, the _expand_non_prim_node\n        only merge the prim:: nodes into the aten:: node, in contrast,the _expand_module_node\n        will merge all adjacent nodes into a same nodepy group.\n\n        Parameters\n        ----------\n        node : trace graph node\n            The non-prim node to expand\n        node_name : str\n            specify the node_name for NodePyGroup\n        unique_name : str\n            unique_name for the NodePyGroup\n        op_type : str\n            specify the op_type for the NodePyGroup\n        nodes : list of trace graph node\n            All the trace graph nodes within the same scope as the non-prim node\n        input_to_node : dict\n            key: input name, value: a node that uses this input\n        output_to_node : dict\n            key: output name, value: a node that generates this output\n        module_type : str\n            can be 'module' or 'func'\n        Returns\n        -------\n        node\n            the expanded non-prim node\n\n        \"\"\"\n        _logger.debug(\"expand module node, node name: %s\", node_name)\n        self.global_count += 1\n        if not op_type:\n            op_type = node.kind()\n        node_group = [node]\n        inputs = list()\n        outputs = list()\n        node_queue = queue.Queue()\n        node_queue.put(node)\n        visited = {node}\n        while not node_queue.empty():\n            curr_node = node_queue.get()\n            for _input in curr_node.inputs():\n                input_name = _input.debugName()\n                if input_name in output_to_node and output_to_node[input_name] in nodes:\n                    predecessor_node = output_to_node[input_name]\n                    if predecessor_node not in visited:\n                        node_group.append(predecessor_node)\n                        node_queue.put(predecessor_node)\n                        visited.add(predecessor_node)\n                else:\n                    inputs.append(input_name)\n            for _output in curr_node.outputs():\n                output_name = _output.debugName()\n                if output_name in input_to_node and input_to_node[output_name] in nodes:\n                    successor_node = input_to_node[output_name]\n                    if successor_node not in visited:\n                        node_group.append(successor_node)\n                        node_queue.put(successor_node)\n                        visited.add(successor_node)\n                else:\n                    outputs.append(output_name)\n\n        nodepy = NodePyGroup(node_name, unique_name, module_type, op_type,\n                             node_group, inputs=inputs, outputs=outputs)\n        return nodepy\n\n    def _extract_cat_info(self, node_group, cpp_node):\n        \"\"\"\n        Extract the detail information of the cat operation,\n        such the order of the input tensor, the shape of each\n        input tensor, the output shape, and the cat dimension.\n\n        Parameters\n        ----------\n        node_group : NodePyGroup\n        cpp_node: torch._C.Node\n            It should be ```aten::cat``` node\n\n        Returns\n        -------\n        dict\n            Include auxiliary information for the cat operation.\n            This dict objec has four keys: 'cat_dim', 'out_shape',\n            'in_order' and 'in_shape'. cat_dim is the dimension of\n            the cat operation to concat the input tensors. out_shape\n            is the shape of the output tensor of the cat operation.\n            in_order is an ordered list which contains the corresponding\n            parent operaion nodes of the input tensors. in_shape is also\n            an ordered list that contains the input shapes of the input\n            tensor.\n        \"\"\"\n        # only suport the cat operation\n        assert cpp_node.kind() == CAT_KIND\n        cat_info = {}\n        # get the shape of the output tensor\n        t_output = cpp_node.output()\n        out_shape = t_output.type().sizes()\n        cat_info['out_shape'] = out_shape\n        # get the cat dimension\n        inputs = cpp_node.inputs()\n        cat_dim = list(inputs)[1].toIValue()\n        cat_info['cat_dim'] = cat_dim\n        # get the order of the input tensors\n        # To get the order of the input tensors, we need\n        # to be aware of the topology of the model, which\n        # means we should extract the auxiliary information\n        # after the build_index function.\n        input_order = []\n        list_construct_cpp = list(cpp_node.inputs())[0].node()\n        input_tensors = list(list_construct_cpp.inputs())\n        for _tensor in input_tensors:\n            debug_name = _tensor.debugName()\n            input_order.append(self.output_to_node[debug_name].unique_name)\n        cat_info['in_order'] = input_order\n        input_shapes = [t.type().sizes() for t in input_tensors]\n        cat_info['in_shape'] = input_shapes\n        return cat_info\n\n    def _extract_shape_info(self, node):\n        \"\"\"\n        Extract the shape information of ```aten::view``` node\n\n        Parameters\n        ----------\n        node : trace graph node\n            It should be ```aten::view``` node\n\n        Returns\n        -------\n        dict\n            Include shape of input tensor and shape of output tensor\n        \"\"\"\n        t_input = None\n        for _input in node.inputs():\n            t_input = _input\n            break\n        t_output = node.output()\n        assert isinstance(t_input.type(), torch._C.TensorType)\n        assert isinstance(t_output.type(), torch._C.TensorType)\n        in_shape = t_input.type().sizes()\n        out_shape = t_output.type().sizes()\n        return {'in_shape': in_shape, 'out_shape': out_shape}\n\n    def _extract_leaf_modules(self):\n        \"\"\"\n        Extract leaf modules from the given graph. Leaf module means it does not have submodules.\n        To extract leaf modules because only leaf module can be replaced. And shape inference can\n        be done in leaf module level. Other shape inference is done in lower level i.e.,\n        operation level.\n\n        Returns\n        -------\n        list\n            a list of scope name of all the leaf modules\n        \"\"\"\n        def is_parent(name1, name2):\n            \"\"\"\n            check if name1 is parent node of name2, for example:\n            name1: aa.bb,  name2: aa.bb.cc,  return True\n            name1: aa.b,  name2: aa.bb, return False\n            \"\"\"\n            parts1, parts2 = name1.split('.'), name2.split('.')\n            if len(parts1) >= len(parts2):\n                return False\n            for i, _ in enumerate(parts1):\n                if parts2[i] != parts1[i]:\n                    return False\n            return True\n        module_names = sorted([x[0]\n                               for x in self.trace.named_modules() if x[0]])\n        leaf_nodes = []\n        for i, name in enumerate(module_names):\n            if i + 1 >= len(module_names) or not is_parent(name, module_names[i + 1]):\n                leaf_nodes.append(name)\n        return leaf_nodes\n\n    def _get_module_name(self, scope_name):\n        \"\"\"\n        Retrieve module name from scope name.\n        Parameters:\n        -----------\n        scope_name: str\n            scope_name of a graph node, for example:\n            for pytorch 1.3.1: MyModel/BackboneModel[backbone]/Conv2d[conv2]\n            for pytorch 1.4.0: __module.backbone/__module.backbone.conv2\n\n        Returns:\n        -------\n        str\n            module name, such as backbone.conv2\n        \"\"\"\n        if torch.__version__ >= '1.4.0':\n            return scope_name.split('/')[-1].replace('__module.', '')\n        else:\n            return '.'.join(re.findall(r'\\[(.*?)\\]', scope_name))\n\n    def _build_index(self, nodes_op):\n        name_to_node = dict()\n        input_to_node = defaultdict(list)\n        output_to_node = dict()\n        for node in nodes_op:\n            name_to_node[node.unique_name] = node\n            for _input in node.inputs:\n                input_to_node[_input].append(node)\n            for output in node.outputs:\n                assert not output in output_to_node, \\\n                    \"One output cannot be generated by multiple nodes\"\n                output_to_node[output] = node\n        return name_to_node, input_to_node, output_to_node\n\n    def _is_key_func(self, node_cpp):\n        \"\"\"\n        Judge if a cpp node is a key function node.\n        If so, we should not merge this node into the\n        adjacent node.\n        \"\"\"\n        if node_cpp.kind().startswith('aten::'):\n            # the nodes that start with 'aten' are key function\n            # nodes\n            return True\n        if node_cpp.kind() in [LIST_UNPACK_KIND, TUPLE_UNPACK_KIND]:\n            # We cannot merge the List/Tuple\n            # Unpack func into other nodes, else it\n            # may lead to a graph construction error.\n            # The reason why we donnot take the construct node\n            # also as a key node is that `cat` operation node need\n            # the last(previous) visited node to infer the mask. If\n            # we take the Construct node as the important node, the\n            # predecessor of the `cat` node will always be a construct\n            # node, which means we cannot infer the mask for the cat\n            # operation.\n            return True\n        return False\n\n    def unpack_manually(self):\n        \"\"\"\n        Unpack the tensor tuple or tensor list manually,\n        and remove the ListUnpack/TupleUnpack node from\n        the graph. Note: this function will change the\n        graph structure.\n        \"\"\"\n        if hasattr(self, 'unpacked'):\n            # if already unpacked the tuple/list manually\n            return\n        for node in self.nodes_py.nodes_op:\n            if node.op_type in [TUPLE_UNPACK_KIND, LIST_UNPACK_KIND]:\n                unpack_cpp = node.key_node\n                last_cpp = list(unpack_cpp.inputs())[0].node()\n                if last_cpp.kind() in [TUPLE_CONSTRUCT_KIND, LIST_CONSTRUCT_KIND]:\n                    # we need check if the tensor tuple or tensor list is produced\n                    # by a list/tuple construct node. If so, we can unpack the tuple\n                    # or list manunally.\n                    _logger.debug('List/Tuple Construct Node(cpp) %s', str(last_cpp))\n                    _logger.debug('List/Tuple Unpack Node(cpp) %s', str(unpack_cpp))\n                    assert len(list(unpack_cpp.outputs())) == len(list(last_cpp.inputs()))\n                    errmsg = '%s Input number: %d if inconsistent with the output number %d' % (unpack_cpp, \\\n                        len(node.inputs), len(list(last_cpp.inputs())))\n\n                    assert len(node.inputs) == len(list(last_cpp.inputs())), errmsg\n                    for _debug_input, _debug_output in zip(node.inputs, node.outputs):\n                        # _debug_input = _input.debugName()\n                        # _debug_output = _output.debugName()\n                        if _debug_input in self.input_to_node and _debug_output in self.input_to_node:\n                            # input_to_node[_debug_input] is a list of NodePyGroup, because\n                            # one tensor can be used as input for multiple nodes at the same time.\n\n                            # note that, in this case, the construct cpp node and unpack cpp node\n                            # will be merged into the same NodePyGroup, so we remove the `node` from\n                            # input_to_node[_debug_input] and directly connect this tensor to the\n                            # input_to_node[_debug_output]\n                            self.input_to_node[_debug_input].remove(node)\n                            # add the following nodes of _output into the input_to_node[_debug_input]\n                            self.input_to_node[_debug_input].extend(self.input_to_node[_debug_output])\n                        # just remove the _debug_output from the grapgh index. So that we can also skip\n                        # the construct and tuple\n                        if _debug_output in self.input_to_node:\n                            for following_node in self.input_to_node[_debug_output]:\n                                _tmp_index = following_node.inputs.index(_debug_output)\n                                following_node.inputs[_tmp_index] = _debug_input\n\n\n        self.unpacked = True\n\n    def _build_graph(self):\n        \"\"\"\n        Build graph using our defined format from jit trace.\n        There are basically three steps: first, construct necessary information (data structures),\n        second, extract all the modules to convert to node, Third, extract all functions to convert\n        to node.\n\n        Returns\n        -------\n        dict\n            use name to index nodes, key: node name, value: node\n        dict\n            use input (its name) to index nodes,\n            key: input, value: list of nodes that take this input\n        dict\n            use output (its name) to index nodes,\n            key: output, value: node that generates this output\n        \"\"\"\n        omit_useless_nodes = True\n        graph = self.trace.graph\n        _logger.debug(graph)\n        # build output mapping, from output debugName to its node\n        output_to_node = {x.debugName(): n for n in graph.nodes()\n                          for x in n.outputs()}\n        # build input mapping, from input debugName to its node\n        input_to_node = {x.debugName(): n for n in graph.nodes()\n                         for x in n.inputs()}\n        # build module mapping, from module name to all nodes (as list) under this module scope\n        module_to_nodes = defaultdict(list)\n        # the mapping of function (non-module in forward) to nodes, key is scope name\n        func_to_nodes = defaultdict(list)\n\n        nodes_py = GraphPy()\n        for node in graph.inputs():\n            if omit_useless_nodes:\n                if not node.uses():  # number of user of the node (= number of outputs/ fanout)\n                    continue\n\n            if node.type().kind() != 'ClassType':\n                nodes_py.append(NodePyIO(node, 'input'))\n\n        self.leaf_modules = self._extract_leaf_modules()\n        module_to_type = {name: parse_traced_name(\n            module._name) for name, module in self.trace.named_modules()}\n\n        # associate module name with their trace graph nodes\n        for node in graph.nodes():\n            module_name = self._get_module_name(node.scopeName())\n            if module_name in self.leaf_modules:\n                module_to_nodes[module_name].append(node)\n            else:\n                func_to_nodes[node.scopeName()].append(node)\n        # build node group for module\n        for module_name, node_cpps in module_to_nodes.items():\n            use_count = 0\n            merged = set()\n            for node in node_cpps:\n                if node not in merged:\n                    # modules that have same scope name may have different locations in the\n                    # graph. Futhermore, there are also lots of prim:: nodes that in node_cpps,\n                    # so we also need to call the expand_module_node.\n                    unique_name = module_name\n                    if use_count > 0:\n                        unique_name = module_name + '.%d' % use_count\n                    node_group = self._expand_module_node(\n                        node, module_name, unique_name, module_to_type[module_name],\n                        node_cpps, input_to_node, output_to_node, 'module')\n                    nodes_py.nodes_op.append(node_group)\n                    use_count += 1\n                    merged.update(node_group.node_cpps)\n\n        # each scope_name may have multiple funcs, we split them and create node for each of them\n        # build node group for torch.nn.functional\n        for _, nodes in func_to_nodes.items():\n            # extract non prim:: nodes\n            key_func_nodes = list()\n            for node in nodes:\n                if self._is_key_func(node):\n                    # find the key function nodes\n                    key_func_nodes.append(node)\n            # for each non prim node, expand it\n            for node in key_func_nodes:\n                node_group = self._expand_key_func_node(\n                    node, nodes, input_to_node, output_to_node, 'func')\n                nodes_py.nodes_op.append(node_group)\n                # get shape infor for view (aten::view) func\n                # if node_group.op_type in ['aten::view', 'aten::flatten']:\n                #     node_group.auxiliary = self._extract_shape_info(node)\n\n        for node in graph.outputs():  # Create sink nodes for output ops\n            node_py = NodePyIO(node, 'output')\n            nodes_py.append(node_py)\n\n        self.nodes_py = nodes_py\n        # build index\n        return self._build_index(self.nodes_py.nodes_op)\n\n    def _extract_auxiliary_info(self):\n        \"\"\"\n        Extract the auxiliary information for the nodegroups\n        if necessary. For example, view/flatten operations may\n        need the shape of the input tensor and output tensor.\n        \"\"\"\n        # extract the input & output shape for the view and flatten\n        for node_group in self.nodes_py.nodes_op:\n            if node_group.op_type in ['aten::view', 'aten::flatten', 'aten::mean', 'aten::reshape']:\n                # get shape infor for view (aten::view) func\n                cpp_node = list(filter(lambda x: x.kind() == node_group.op_type,\n                                       node_group.node_cpps))[0]\n                node_group.auxiliary = self._extract_shape_info(cpp_node)\n            elif node_group.op_type == CAT_KIND:\n                # get the detail information for cat func\n                cpp_node = list(filter(lambda x: x.kind() == node_group.op_type,\n                                       node_group.node_cpps))[0]\n                node_group.auxiliary = self._extract_cat_info(\n                    node_group, cpp_node)\n\n    def find_predecessors(self, unique_name):\n        \"\"\"\n        Find predecessor node of the given node\n\n        Parameters\n        ----------\n        unique_name : str\n            The unique name of the node\n\n        Returns\n        -------\n        list\n            a list of nodes who are the given node's predecessor\n        \"\"\"\n        predecessors = []\n        for _input in self.name_to_node[unique_name].inputs:\n            if not _input in self.output_to_node:\n                _logger.debug(\"cannot find node with %s as its output\", _input)\n            else:\n                node_py = self.output_to_node[_input]\n                predecessors.append(node_py.unique_name)\n        return predecessors\n\n    def find_successors(self, unique_name):\n        \"\"\"\n        Find successor nodes of the given node\n\n        Parameters\n        ----------\n        unique_name : str\n            The unique name of the node\n\n        Returns\n        -------\n        list\n            a list of nodes who are the given node's successor\n        \"\"\"\n        successors = []\n        for output in self.name_to_node[unique_name].outputs:\n            if output not in self.input_to_node:\n                # may reach the output of the whole graph\n                continue\n            nodes_py = self.input_to_node[output]\n            for node_py in nodes_py:\n                successors.append(node_py.unique_name)\n        return successors",
  "def __init__(self, model=None, dummy_input=None, traced_model=None):\n        \"\"\"\n        Parameters\n        ----------\n        model : pytorch model\n            The model user wants to speed up\n        dummy_input : pytorch tensor\n            The dummy input for ```jit.trace```, users should put it on right device before pass in\n        traced_model : torch._C.torch.jit.TopLevelTracedModule\n            An alredy traced model, if traced_model is not None, then TorchGraph will build the graph\n            based on this traced model and won't trace the model again.\n        \"\"\"\n        assert torch.__version__ >= '1.3.1'\n        # check if the input is legal\n        if traced_model is not None:\n            assert isinstance(traced_model, torch.jit.TopLevelTracedModule)\n            self.trace = traced_model\n            # it's ok if the graph is already unpacked\n            torch._C._jit_pass_inline(self.trace.graph)\n        elif model is not None and dummy_input is not None:\n            self.bound_model = model\n            self._trace(model, dummy_input)\n        else:\n            raise Exception(\n                'Please provide model & dummy_input or the traced_model as inputs')",
  "def _trace(self, model, dummy_input):\n        with torch.onnx.set_training(model, False):\n            self.trace = torch.jit.trace(model, dummy_input)\n            torch._C._jit_pass_inline(self.trace.graph)",
  "def __init__(self, model, dummy_input, verbose=False):\n        super().__init__(model, dummy_input)\n\n        from tensorboard.compat.proto.config_pb2 import RunMetadata\n        from tensorboard.compat.proto.graph_pb2 import GraphDef\n        from tensorboard.compat.proto.step_stats_pb2 import StepStats, DeviceStepStats\n        from tensorboard.compat.proto.versions_pb2 import VersionDef\n\n        list_of_nodes = self.parse(self.trace.graph, self.trace, dummy_input)\n        if verbose:\n            print(self.trace.graph)\n        self.stepstats = RunMetadata(step_stats=StepStats(\n            dev_stats=[DeviceStepStats(device=\"/device:CPU:0\")]))\n        self.graph_def = GraphDef(\n            node=list_of_nodes, versions=VersionDef(producer=22))",
  "def parse(self, graph, trace, args=None, omit_useless_nodes=True):\n        \"\"\"This method parses an optimized PyTorch model graph and produces\n        a list of nodes and node stats for eventual conversion to TensorBoard\n        protobuf format.\n\n        Args:\n        graph (PyTorch module): The model graph to be parsed.\n        trace (PyTorch JIT TracedModule): The model trace to be parsed.\n        args (tuple): input tensor[s] for the model.\n        omit_useless_nodes (boolean): Whether to remove nodes from the graph.\n        \"\"\"\n        nodes_py = GraphPy()\n        for node in graph.inputs():\n            if omit_useless_nodes:\n                if not node.uses():  # number of user of the node (= number of outputs/ fanout)\n                    continue\n\n            if node.type().kind() != CLASSTYPE_KIND:\n                nodes_py.append(NodePyIO(node, 'input'))\n\n        attr_to_scope = dict()\n\n        def node_to_name(d):\n            return str(d).split(\":\")[0].strip()\n        for node in graph.nodes():\n            if node.kind() == GETATTR_KIND:\n                attr_name = node.s('name')\n                node_name = node_to_name(node)\n                parent = node.input().node()\n                # If the parent node is not the top-level \"self\" node\n                if parent.kind() == GETATTR_KIND:\n                    parent_scope = attr_to_scope[node_to_name(parent)]\n                    attr_scope = parent_scope.split('/')[-1]\n                    attr_to_scope[node_name] = '{}/{}.{}'.format(\n                        parent_scope, attr_scope, attr_name)\n                else:\n                    attr_to_scope[node_name] = '__module.{}'.format(attr_name)\n                # We don't need classtype nodes; scope will provide this information\n                if node.output().type().kind() != CLASSTYPE_KIND:\n                    node_py = NodePyOP(node)\n                    node_py.scopeName = attr_to_scope[node_name]\n                    nodes_py.append(node_py)\n            else:\n                nodes_py.append(NodePyOP(node))\n\n        # Create sink nodes for output ops\n        for i, node in enumerate(graph.outputs()):\n            node_py = NodePyIO(node, 'output')\n            node_py.debugName = \"output.{}\".format(i + 1)\n            node_py.inputs = [node.debugName()]\n            nodes_py.append(node_py)\n\n        alias_to_name = dict()\n        base_name = parse_traced_name(trace._name)\n        for name, module in trace.named_modules(prefix='__module'):\n            mod_name = parse_traced_name(module._name)\n            attr_name = name.split('.')[-1]\n            alias_to_name[name] = '{}[{}]'.format(mod_name, attr_name)\n\n        for node in nodes_py.nodes_op:\n            module_aliases = node.scopeName.split('/')[-1].split('.')\n            module_name = ''\n            for i, alias in enumerate(module_aliases):\n                if i == 0:\n                    module_name = alias\n                    node.scopeName = base_name\n                else:\n                    module_name += '.' + alias\n                    node.scopeName += '/' + \\\n                        (alias_to_name[module_name]\n                         if module_name in alias_to_name else alias)\n\n        nodes_py.populate_namespace_from_OP_to_IO()\n        return nodes_py.to_proto()",
  "def __init__(self, name, unique_name, node_type, op_type, node_cpps, inputs=None, outputs=None, key_node=None):\n        \"\"\"\n        Parameters:\n        -----------\n        name: str\n            node name, such as `conv1`, `backbone.classifier`\n        unique_name: str\n            A global unique name for current node. Due to some modules,\n            such as relu, may be reused several times, so the scopename\n            is not suitable as the global unique identifier, so we add a\n            unique_name for each node as the global unique identifier.\n            We should use the unique_name to traverset the module graph.\n        node_type: str\n            `module` or `func`\n        op_type: str\n            operation type, such as `Conv2d`, `aten::view`\n        node_cpps: list of torch._C.Node\n            jit trace nodes which are included in this new node\n        inputs: list of str\n            All the inputs of this node, each element is debugName of one input\n        outputs: list of str\n            All the outputs of this node, each element is debugName of one output\n        key_node: torch._C.Node\n            The key node of this NodePyGroup.\n        \"\"\"\n        super(NodePyGroup, self).__init__(name, [])\n        self.node_cpps = node_cpps\n        self.name = name\n        self.unique_name = unique_name\n        self.op_type = op_type\n        self.type = node_type\n        self.nodes = []\n        self.auxiliary = None\n        self.add_nodes(node_cpps)\n        self.inputs = inputs\n        self.outputs = outputs\n        # The core node in this NodePyGroup\n        self.key_node = key_node",
  "def add_nodes(self, node_cpps):\n        for node_cpp in node_cpps:\n            nodepy = NodePyOP(node_cpp)\n            nodepy.name = node_cpp.scopeName() + '_' + node_cpp.kind()\n            self.nodes.append(nodepy)",
  "def sub_node_names(self):\n        return [x.name for x in self.nodes]",
  "def __repr__(self):\n        return 'name: {}, type: {}, op_type: {}, sub_nodes: {}, inputs: {}, outputs: {}, aux: {}'.format(\n            self.name, self.type, self.op_type, self.sub_node_names(),\n            self.inputs, self.outputs, self.auxiliary\n        )",
  "def __init__(self, model=None, dummy_input=None, traced_model=None):\n        super().__init__(model, dummy_input, traced_model)\n        self.global_count = 0\n        self.name_to_node, self.input_to_node, self.output_to_node = self._build_graph()\n        self._extract_auxiliary_info()",
  "def _expand_key_func_node(self, node, nodes, input_to_node, output_to_node,\n                              module_type):\n        \"\"\"\n        For trace graph nodes, some nodes are not in modules, these nodes are usually generated by\n        the functions directly called in module ```forward```. For such nodes, some of them are\n        trivial op which are label by ```prim::```, some of them are not such ops which is call\n        non-prim ops. This function is to merge neighbor prim ops to a non-prim op, to construct\n        a node.\n\n        Parameters\n        ----------\n        node : trace graph node\n            The non-prim node to expand\n        nodes : list of trace graph node\n            All the trace graph nodes within the same scope as the non-prim node\n        input_to_node : dict\n            key: input name, value: a node that uses this input\n        output_to_node : dict\n            key: output name, value: a node that generates this output\n        module_type : str\n            can be 'module' or 'func'\n\n        Returns\n        -------\n        node\n            the expanded non-prim node\n        \"\"\"\n        # TODO: scope name could be empty\n        node_name = '.'.join([self._get_module_name(\n            node.scopeName()), node.kind(), str(self.global_count)])\n        unique_name = node_name\n        _logger.debug(\"expand non-prim node, node name: %s\", node_name)\n        self.global_count += 1\n        op_type = node.kind()\n        node_group = [node]\n        inputs = list()\n        outputs = list()\n        node_queue = queue.Queue()\n        node_queue.put(node)\n        while not node_queue.empty():\n            curr_node = node_queue.get()\n            for _input in curr_node.inputs():\n                input_name = _input.debugName()\n                if input_name in output_to_node and output_to_node[input_name] in nodes:\n                    predecessor_node = output_to_node[input_name]\n                    if not self._is_key_func(predecessor_node):\n                        node_group.append(predecessor_node)\n                        node_queue.put(predecessor_node)\n                    else:\n                        inputs.append(input_name)\n                else:\n                    inputs.append(input_name)\n        for output in node.outputs():\n            outputs.append(output.debugName())\n        nodepy = NodePyGroup(node_name, unique_name, module_type, op_type,\n                             node_group, inputs=inputs, outputs=outputs, key_node=node)\n        return nodepy",
  "def _expand_module_node(self, node, node_name, unique_name, op_type, nodes,\n                            input_to_node, output_to_node, module_type):\n        \"\"\"\n        merge the adjacent nodes of the module. The difference between the\n        _expand_module_node and _expand_non_prim_node is that, the _expand_non_prim_node\n        only merge the prim:: nodes into the aten:: node, in contrast,the _expand_module_node\n        will merge all adjacent nodes into a same nodepy group.\n\n        Parameters\n        ----------\n        node : trace graph node\n            The non-prim node to expand\n        node_name : str\n            specify the node_name for NodePyGroup\n        unique_name : str\n            unique_name for the NodePyGroup\n        op_type : str\n            specify the op_type for the NodePyGroup\n        nodes : list of trace graph node\n            All the trace graph nodes within the same scope as the non-prim node\n        input_to_node : dict\n            key: input name, value: a node that uses this input\n        output_to_node : dict\n            key: output name, value: a node that generates this output\n        module_type : str\n            can be 'module' or 'func'\n        Returns\n        -------\n        node\n            the expanded non-prim node\n\n        \"\"\"\n        _logger.debug(\"expand module node, node name: %s\", node_name)\n        self.global_count += 1\n        if not op_type:\n            op_type = node.kind()\n        node_group = [node]\n        inputs = list()\n        outputs = list()\n        node_queue = queue.Queue()\n        node_queue.put(node)\n        visited = {node}\n        while not node_queue.empty():\n            curr_node = node_queue.get()\n            for _input in curr_node.inputs():\n                input_name = _input.debugName()\n                if input_name in output_to_node and output_to_node[input_name] in nodes:\n                    predecessor_node = output_to_node[input_name]\n                    if predecessor_node not in visited:\n                        node_group.append(predecessor_node)\n                        node_queue.put(predecessor_node)\n                        visited.add(predecessor_node)\n                else:\n                    inputs.append(input_name)\n            for _output in curr_node.outputs():\n                output_name = _output.debugName()\n                if output_name in input_to_node and input_to_node[output_name] in nodes:\n                    successor_node = input_to_node[output_name]\n                    if successor_node not in visited:\n                        node_group.append(successor_node)\n                        node_queue.put(successor_node)\n                        visited.add(successor_node)\n                else:\n                    outputs.append(output_name)\n\n        nodepy = NodePyGroup(node_name, unique_name, module_type, op_type,\n                             node_group, inputs=inputs, outputs=outputs)\n        return nodepy",
  "def _extract_cat_info(self, node_group, cpp_node):\n        \"\"\"\n        Extract the detail information of the cat operation,\n        such the order of the input tensor, the shape of each\n        input tensor, the output shape, and the cat dimension.\n\n        Parameters\n        ----------\n        node_group : NodePyGroup\n        cpp_node: torch._C.Node\n            It should be ```aten::cat``` node\n\n        Returns\n        -------\n        dict\n            Include auxiliary information for the cat operation.\n            This dict objec has four keys: 'cat_dim', 'out_shape',\n            'in_order' and 'in_shape'. cat_dim is the dimension of\n            the cat operation to concat the input tensors. out_shape\n            is the shape of the output tensor of the cat operation.\n            in_order is an ordered list which contains the corresponding\n            parent operaion nodes of the input tensors. in_shape is also\n            an ordered list that contains the input shapes of the input\n            tensor.\n        \"\"\"\n        # only suport the cat operation\n        assert cpp_node.kind() == CAT_KIND\n        cat_info = {}\n        # get the shape of the output tensor\n        t_output = cpp_node.output()\n        out_shape = t_output.type().sizes()\n        cat_info['out_shape'] = out_shape\n        # get the cat dimension\n        inputs = cpp_node.inputs()\n        cat_dim = list(inputs)[1].toIValue()\n        cat_info['cat_dim'] = cat_dim\n        # get the order of the input tensors\n        # To get the order of the input tensors, we need\n        # to be aware of the topology of the model, which\n        # means we should extract the auxiliary information\n        # after the build_index function.\n        input_order = []\n        list_construct_cpp = list(cpp_node.inputs())[0].node()\n        input_tensors = list(list_construct_cpp.inputs())\n        for _tensor in input_tensors:\n            debug_name = _tensor.debugName()\n            input_order.append(self.output_to_node[debug_name].unique_name)\n        cat_info['in_order'] = input_order\n        input_shapes = [t.type().sizes() for t in input_tensors]\n        cat_info['in_shape'] = input_shapes\n        return cat_info",
  "def _extract_shape_info(self, node):\n        \"\"\"\n        Extract the shape information of ```aten::view``` node\n\n        Parameters\n        ----------\n        node : trace graph node\n            It should be ```aten::view``` node\n\n        Returns\n        -------\n        dict\n            Include shape of input tensor and shape of output tensor\n        \"\"\"\n        t_input = None\n        for _input in node.inputs():\n            t_input = _input\n            break\n        t_output = node.output()\n        assert isinstance(t_input.type(), torch._C.TensorType)\n        assert isinstance(t_output.type(), torch._C.TensorType)\n        in_shape = t_input.type().sizes()\n        out_shape = t_output.type().sizes()\n        return {'in_shape': in_shape, 'out_shape': out_shape}",
  "def _extract_leaf_modules(self):\n        \"\"\"\n        Extract leaf modules from the given graph. Leaf module means it does not have submodules.\n        To extract leaf modules because only leaf module can be replaced. And shape inference can\n        be done in leaf module level. Other shape inference is done in lower level i.e.,\n        operation level.\n\n        Returns\n        -------\n        list\n            a list of scope name of all the leaf modules\n        \"\"\"\n        def is_parent(name1, name2):\n            \"\"\"\n            check if name1 is parent node of name2, for example:\n            name1: aa.bb,  name2: aa.bb.cc,  return True\n            name1: aa.b,  name2: aa.bb, return False\n            \"\"\"\n            parts1, parts2 = name1.split('.'), name2.split('.')\n            if len(parts1) >= len(parts2):\n                return False\n            for i, _ in enumerate(parts1):\n                if parts2[i] != parts1[i]:\n                    return False\n            return True\n        module_names = sorted([x[0]\n                               for x in self.trace.named_modules() if x[0]])\n        leaf_nodes = []\n        for i, name in enumerate(module_names):\n            if i + 1 >= len(module_names) or not is_parent(name, module_names[i + 1]):\n                leaf_nodes.append(name)\n        return leaf_nodes",
  "def _get_module_name(self, scope_name):\n        \"\"\"\n        Retrieve module name from scope name.\n        Parameters:\n        -----------\n        scope_name: str\n            scope_name of a graph node, for example:\n            for pytorch 1.3.1: MyModel/BackboneModel[backbone]/Conv2d[conv2]\n            for pytorch 1.4.0: __module.backbone/__module.backbone.conv2\n\n        Returns:\n        -------\n        str\n            module name, such as backbone.conv2\n        \"\"\"\n        if torch.__version__ >= '1.4.0':\n            return scope_name.split('/')[-1].replace('__module.', '')\n        else:\n            return '.'.join(re.findall(r'\\[(.*?)\\]', scope_name))",
  "def _build_index(self, nodes_op):\n        name_to_node = dict()\n        input_to_node = defaultdict(list)\n        output_to_node = dict()\n        for node in nodes_op:\n            name_to_node[node.unique_name] = node\n            for _input in node.inputs:\n                input_to_node[_input].append(node)\n            for output in node.outputs:\n                assert not output in output_to_node, \\\n                    \"One output cannot be generated by multiple nodes\"\n                output_to_node[output] = node\n        return name_to_node, input_to_node, output_to_node",
  "def _is_key_func(self, node_cpp):\n        \"\"\"\n        Judge if a cpp node is a key function node.\n        If so, we should not merge this node into the\n        adjacent node.\n        \"\"\"\n        if node_cpp.kind().startswith('aten::'):\n            # the nodes that start with 'aten' are key function\n            # nodes\n            return True\n        if node_cpp.kind() in [LIST_UNPACK_KIND, TUPLE_UNPACK_KIND]:\n            # We cannot merge the List/Tuple\n            # Unpack func into other nodes, else it\n            # may lead to a graph construction error.\n            # The reason why we donnot take the construct node\n            # also as a key node is that `cat` operation node need\n            # the last(previous) visited node to infer the mask. If\n            # we take the Construct node as the important node, the\n            # predecessor of the `cat` node will always be a construct\n            # node, which means we cannot infer the mask for the cat\n            # operation.\n            return True\n        return False",
  "def unpack_manually(self):\n        \"\"\"\n        Unpack the tensor tuple or tensor list manually,\n        and remove the ListUnpack/TupleUnpack node from\n        the graph. Note: this function will change the\n        graph structure.\n        \"\"\"\n        if hasattr(self, 'unpacked'):\n            # if already unpacked the tuple/list manually\n            return\n        for node in self.nodes_py.nodes_op:\n            if node.op_type in [TUPLE_UNPACK_KIND, LIST_UNPACK_KIND]:\n                unpack_cpp = node.key_node\n                last_cpp = list(unpack_cpp.inputs())[0].node()\n                if last_cpp.kind() in [TUPLE_CONSTRUCT_KIND, LIST_CONSTRUCT_KIND]:\n                    # we need check if the tensor tuple or tensor list is produced\n                    # by a list/tuple construct node. If so, we can unpack the tuple\n                    # or list manunally.\n                    _logger.debug('List/Tuple Construct Node(cpp) %s', str(last_cpp))\n                    _logger.debug('List/Tuple Unpack Node(cpp) %s', str(unpack_cpp))\n                    assert len(list(unpack_cpp.outputs())) == len(list(last_cpp.inputs()))\n                    errmsg = '%s Input number: %d if inconsistent with the output number %d' % (unpack_cpp, \\\n                        len(node.inputs), len(list(last_cpp.inputs())))\n\n                    assert len(node.inputs) == len(list(last_cpp.inputs())), errmsg\n                    for _debug_input, _debug_output in zip(node.inputs, node.outputs):\n                        # _debug_input = _input.debugName()\n                        # _debug_output = _output.debugName()\n                        if _debug_input in self.input_to_node and _debug_output in self.input_to_node:\n                            # input_to_node[_debug_input] is a list of NodePyGroup, because\n                            # one tensor can be used as input for multiple nodes at the same time.\n\n                            # note that, in this case, the construct cpp node and unpack cpp node\n                            # will be merged into the same NodePyGroup, so we remove the `node` from\n                            # input_to_node[_debug_input] and directly connect this tensor to the\n                            # input_to_node[_debug_output]\n                            self.input_to_node[_debug_input].remove(node)\n                            # add the following nodes of _output into the input_to_node[_debug_input]\n                            self.input_to_node[_debug_input].extend(self.input_to_node[_debug_output])\n                        # just remove the _debug_output from the grapgh index. So that we can also skip\n                        # the construct and tuple\n                        if _debug_output in self.input_to_node:\n                            for following_node in self.input_to_node[_debug_output]:\n                                _tmp_index = following_node.inputs.index(_debug_output)\n                                following_node.inputs[_tmp_index] = _debug_input\n\n\n        self.unpacked = True",
  "def _build_graph(self):\n        \"\"\"\n        Build graph using our defined format from jit trace.\n        There are basically three steps: first, construct necessary information (data structures),\n        second, extract all the modules to convert to node, Third, extract all functions to convert\n        to node.\n\n        Returns\n        -------\n        dict\n            use name to index nodes, key: node name, value: node\n        dict\n            use input (its name) to index nodes,\n            key: input, value: list of nodes that take this input\n        dict\n            use output (its name) to index nodes,\n            key: output, value: node that generates this output\n        \"\"\"\n        omit_useless_nodes = True\n        graph = self.trace.graph\n        _logger.debug(graph)\n        # build output mapping, from output debugName to its node\n        output_to_node = {x.debugName(): n for n in graph.nodes()\n                          for x in n.outputs()}\n        # build input mapping, from input debugName to its node\n        input_to_node = {x.debugName(): n for n in graph.nodes()\n                         for x in n.inputs()}\n        # build module mapping, from module name to all nodes (as list) under this module scope\n        module_to_nodes = defaultdict(list)\n        # the mapping of function (non-module in forward) to nodes, key is scope name\n        func_to_nodes = defaultdict(list)\n\n        nodes_py = GraphPy()\n        for node in graph.inputs():\n            if omit_useless_nodes:\n                if not node.uses():  # number of user of the node (= number of outputs/ fanout)\n                    continue\n\n            if node.type().kind() != 'ClassType':\n                nodes_py.append(NodePyIO(node, 'input'))\n\n        self.leaf_modules = self._extract_leaf_modules()\n        module_to_type = {name: parse_traced_name(\n            module._name) for name, module in self.trace.named_modules()}\n\n        # associate module name with their trace graph nodes\n        for node in graph.nodes():\n            module_name = self._get_module_name(node.scopeName())\n            if module_name in self.leaf_modules:\n                module_to_nodes[module_name].append(node)\n            else:\n                func_to_nodes[node.scopeName()].append(node)\n        # build node group for module\n        for module_name, node_cpps in module_to_nodes.items():\n            use_count = 0\n            merged = set()\n            for node in node_cpps:\n                if node not in merged:\n                    # modules that have same scope name may have different locations in the\n                    # graph. Futhermore, there are also lots of prim:: nodes that in node_cpps,\n                    # so we also need to call the expand_module_node.\n                    unique_name = module_name\n                    if use_count > 0:\n                        unique_name = module_name + '.%d' % use_count\n                    node_group = self._expand_module_node(\n                        node, module_name, unique_name, module_to_type[module_name],\n                        node_cpps, input_to_node, output_to_node, 'module')\n                    nodes_py.nodes_op.append(node_group)\n                    use_count += 1\n                    merged.update(node_group.node_cpps)\n\n        # each scope_name may have multiple funcs, we split them and create node for each of them\n        # build node group for torch.nn.functional\n        for _, nodes in func_to_nodes.items():\n            # extract non prim:: nodes\n            key_func_nodes = list()\n            for node in nodes:\n                if self._is_key_func(node):\n                    # find the key function nodes\n                    key_func_nodes.append(node)\n            # for each non prim node, expand it\n            for node in key_func_nodes:\n                node_group = self._expand_key_func_node(\n                    node, nodes, input_to_node, output_to_node, 'func')\n                nodes_py.nodes_op.append(node_group)\n                # get shape infor for view (aten::view) func\n                # if node_group.op_type in ['aten::view', 'aten::flatten']:\n                #     node_group.auxiliary = self._extract_shape_info(node)\n\n        for node in graph.outputs():  # Create sink nodes for output ops\n            node_py = NodePyIO(node, 'output')\n            nodes_py.append(node_py)\n\n        self.nodes_py = nodes_py\n        # build index\n        return self._build_index(self.nodes_py.nodes_op)",
  "def _extract_auxiliary_info(self):\n        \"\"\"\n        Extract the auxiliary information for the nodegroups\n        if necessary. For example, view/flatten operations may\n        need the shape of the input tensor and output tensor.\n        \"\"\"\n        # extract the input & output shape for the view and flatten\n        for node_group in self.nodes_py.nodes_op:\n            if node_group.op_type in ['aten::view', 'aten::flatten', 'aten::mean', 'aten::reshape']:\n                # get shape infor for view (aten::view) func\n                cpp_node = list(filter(lambda x: x.kind() == node_group.op_type,\n                                       node_group.node_cpps))[0]\n                node_group.auxiliary = self._extract_shape_info(cpp_node)\n            elif node_group.op_type == CAT_KIND:\n                # get the detail information for cat func\n                cpp_node = list(filter(lambda x: x.kind() == node_group.op_type,\n                                       node_group.node_cpps))[0]\n                node_group.auxiliary = self._extract_cat_info(\n                    node_group, cpp_node)",
  "def find_predecessors(self, unique_name):\n        \"\"\"\n        Find predecessor node of the given node\n\n        Parameters\n        ----------\n        unique_name : str\n            The unique name of the node\n\n        Returns\n        -------\n        list\n            a list of nodes who are the given node's predecessor\n        \"\"\"\n        predecessors = []\n        for _input in self.name_to_node[unique_name].inputs:\n            if not _input in self.output_to_node:\n                _logger.debug(\"cannot find node with %s as its output\", _input)\n            else:\n                node_py = self.output_to_node[_input]\n                predecessors.append(node_py.unique_name)\n        return predecessors",
  "def find_successors(self, unique_name):\n        \"\"\"\n        Find successor nodes of the given node\n\n        Parameters\n        ----------\n        unique_name : str\n            The unique name of the node\n\n        Returns\n        -------\n        list\n            a list of nodes who are the given node's successor\n        \"\"\"\n        successors = []\n        for output in self.name_to_node[unique_name].outputs:\n            if output not in self.input_to_node:\n                # may reach the output of the whole graph\n                continue\n            nodes_py = self.input_to_node[output]\n            for node_py in nodes_py:\n                successors.append(node_py.unique_name)\n        return successors",
  "def node_to_name(d):\n            return str(d).split(\":\")[0].strip()",
  "def is_parent(name1, name2):\n            \"\"\"\n            check if name1 is parent node of name2, for example:\n            name1: aa.bb,  name2: aa.bb.cc,  return True\n            name1: aa.b,  name2: aa.bb, return False\n            \"\"\"\n            parts1, parts2 = name1.split('.'), name2.split('.')\n            if len(parts1) >= len(parts2):\n                return False\n            for i, _ in enumerate(parts1):\n                if parts2[i] != parts1[i]:\n                    return False\n            return True",
  "def _sort_history(history):\n    ret = []\n    for i, _ in enumerate(history):\n        if i in history:\n            ret.append(history[i])\n        else:\n            break\n    return ret",
  "def _create_parameter_id():\n    global _next_parameter_id\n    _next_parameter_id += 1\n    return _next_parameter_id - 1",
  "def _pack_parameter(parameter_id, params, customized=False, trial_job_id=None, parameter_index=None):\n    _trial_params[parameter_id] = params\n    ret = {\n        'parameter_id': parameter_id,\n        'parameter_source': 'customized' if customized else 'algorithm',\n        'parameters': params\n    }\n    if trial_job_id is not None:\n        ret['trial_job_id'] = trial_job_id\n    if parameter_index is not None:\n        ret['parameter_index'] = parameter_index\n    else:\n        ret['parameter_index'] = 0\n    return to_json(ret)",
  "class MsgDispatcher(MsgDispatcherBase):\n    def __init__(self, tuner, assessor=None):\n        super(MsgDispatcher, self).__init__()\n        self.tuner = tuner\n        self.assessor = assessor\n        if assessor is None:\n            _logger.debug('Assessor is not configured')\n\n    def load_checkpoint(self):\n        self.tuner.load_checkpoint()\n        if self.assessor is not None:\n            self.assessor.load_checkpoint()\n\n    def save_checkpoint(self):\n        self.tuner.save_checkpoint()\n        if self.assessor is not None:\n            self.assessor.save_checkpoint()\n\n    def handle_initialize(self, data):\n        \"\"\"Data is search space\n        \"\"\"\n        self.tuner.update_search_space(data)\n        send(CommandType.Initialized, '')\n\n    def send_trial_callback(self, id_, params):\n        \"\"\"For tuner to issue trial config when the config is generated\n        \"\"\"\n        send(CommandType.NewTrialJob, _pack_parameter(id_, params))\n\n    def handle_request_trial_jobs(self, data):\n        # data: number or trial jobs\n        ids = [_create_parameter_id() for _ in range(data)]\n        _logger.debug(\"requesting for generating params of %s\", ids)\n        params_list = self.tuner.generate_multiple_parameters(ids, st_callback=self.send_trial_callback)\n\n        for i, _ in enumerate(params_list):\n            send(CommandType.NewTrialJob, _pack_parameter(ids[i], params_list[i]))\n        # when parameters is None.\n        if len(params_list) < len(ids):\n            send(CommandType.NoMoreTrialJobs, _pack_parameter(ids[0], ''))\n\n    def handle_update_search_space(self, data):\n        self.tuner.update_search_space(data)\n\n    def handle_import_data(self, data):\n        \"\"\"Import additional data for tuning\n        data: a list of dictionaries, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        for entry in data:\n            entry['value'] = json_tricks.loads(entry['value'])\n        self.tuner.import_data(data)\n\n    def handle_add_customized_trial(self, data):\n        # data: parameters\n        id_ = _create_parameter_id()\n        _customized_parameter_ids.add(id_)\n\n    def handle_report_metric_data(self, data):\n        \"\"\"\n        data: a dict received from nni_manager, which contains:\n              - 'parameter_id': id of the trial\n              - 'value': metric value reported by nni.report_final_result()\n              - 'type': report type, support {'FINAL', 'PERIODICAL'}\n        \"\"\"\n        # metrics value is dumped as json string in trial, so we need to decode it here\n        if 'value' in data:\n            data['value'] = json_tricks.loads(data['value'])\n        if data['type'] == MetricType.FINAL:\n            self._handle_final_metric_data(data)\n        elif data['type'] == MetricType.PERIODICAL:\n            if self.assessor is not None:\n                self._handle_intermediate_metric_data(data)\n        elif data['type'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data['trial_job_id'] is not None\n            assert data['parameter_index'] is not None\n            param_id = _create_parameter_id()\n            try:\n                param = self.tuner.generate_parameters(param_id, trial_job_id=data['trial_job_id'])\n            except NoMoreTrialError:\n                param = None\n            send(CommandType.SendTrialJobParameter, _pack_parameter(param_id, param, trial_job_id=data['trial_job_id'],\n                                                                    parameter_index=data['parameter_index']))\n        else:\n            raise ValueError('Data type not supported: {}'.format(data['type']))\n\n    def handle_trial_end(self, data):\n        \"\"\"\n        data: it has three keys: trial_job_id, event, hyper_params\n             - trial_job_id: the id generated by training service\n             - event: the job's state\n             - hyper_params: the hyperparameters generated and returned by tuner\n        \"\"\"\n        trial_job_id = data['trial_job_id']\n        _ended_trials.add(trial_job_id)\n        if trial_job_id in _trial_history:\n            _trial_history.pop(trial_job_id)\n            if self.assessor is not None:\n                self.assessor.trial_end(trial_job_id, data['event'] == 'SUCCEEDED')\n        if self.tuner is not None:\n            self.tuner.trial_end(json_tricks.loads(data['hyper_params'])['parameter_id'], data['event'] == 'SUCCEEDED')\n\n    def _handle_final_metric_data(self, data):\n        \"\"\"Call tuner to process final results\n        \"\"\"\n        id_ = data['parameter_id']\n        value = data['value']\n        if id_ is None or id_ in _customized_parameter_ids:\n            if not hasattr(self.tuner, '_accept_customized'):\n                self.tuner._accept_customized = False\n            if not self.tuner._accept_customized:\n                _logger.info('Customized trial job %s ignored by tuner', id_)\n                return\n            customized = True\n        else:\n            customized = False\n            self.tuner.receive_trial_result(id_, _trial_params[id_], value, customized=customized,\n                                            trial_job_id=data.get('trial_job_id'))\n\n    def _handle_intermediate_metric_data(self, data):\n        \"\"\"Call assessor to process intermediate results\n        \"\"\"\n        if data['type'] != MetricType.PERIODICAL:\n            return\n        if self.assessor is None:\n            return\n\n        trial_job_id = data['trial_job_id']\n        if trial_job_id in _ended_trials:\n            return\n\n        history = _trial_history[trial_job_id]\n        history[data['sequence']] = data['value']\n        ordered_history = _sort_history(history)\n        if len(ordered_history) < data['sequence']:  # no user-visible update since last time\n            return\n\n        try:\n            result = self.assessor.assess_trial(trial_job_id, ordered_history)\n        except Exception as e:\n            _logger.error('Assessor error')\n            _logger.exception(e)\n\n        if isinstance(result, bool):\n            result = AssessResult.Good if result else AssessResult.Bad\n        elif not isinstance(result, AssessResult):\n            msg = 'Result of Assessor.assess_trial must be an object of AssessResult, not %s'\n            raise RuntimeError(msg % type(result))\n\n        if result is AssessResult.Bad:\n            _logger.debug('BAD, kill %s', trial_job_id)\n            send(CommandType.KillTrialJob, json_tricks.dumps(trial_job_id))\n            # notify tuner\n            _logger.debug('env var: NNI_INCLUDE_INTERMEDIATE_RESULTS: [%s]',\n                          dispatcher_env_vars.NNI_INCLUDE_INTERMEDIATE_RESULTS)\n            if dispatcher_env_vars.NNI_INCLUDE_INTERMEDIATE_RESULTS == 'true':\n                self._earlystop_notify_tuner(data)\n        else:\n            _logger.debug('GOOD')\n\n    def _earlystop_notify_tuner(self, data):\n        \"\"\"Send last intermediate result as final result to tuner in case the\n        trial is early stopped.\n        \"\"\"\n        _logger.debug('Early stop notify tuner data: [%s]', data)\n        data['type'] = MetricType.FINAL\n        if multi_thread_enabled():\n            self._handle_final_metric_data(data)\n        else:\n            data['value'] = to_json(data['value'])\n            self.enqueue_command(CommandType.ReportMetricData, data)",
  "def __init__(self, tuner, assessor=None):\n        super(MsgDispatcher, self).__init__()\n        self.tuner = tuner\n        self.assessor = assessor\n        if assessor is None:\n            _logger.debug('Assessor is not configured')",
  "def load_checkpoint(self):\n        self.tuner.load_checkpoint()\n        if self.assessor is not None:\n            self.assessor.load_checkpoint()",
  "def save_checkpoint(self):\n        self.tuner.save_checkpoint()\n        if self.assessor is not None:\n            self.assessor.save_checkpoint()",
  "def handle_initialize(self, data):\n        \"\"\"Data is search space\n        \"\"\"\n        self.tuner.update_search_space(data)\n        send(CommandType.Initialized, '')",
  "def send_trial_callback(self, id_, params):\n        \"\"\"For tuner to issue trial config when the config is generated\n        \"\"\"\n        send(CommandType.NewTrialJob, _pack_parameter(id_, params))",
  "def handle_request_trial_jobs(self, data):\n        # data: number or trial jobs\n        ids = [_create_parameter_id() for _ in range(data)]\n        _logger.debug(\"requesting for generating params of %s\", ids)\n        params_list = self.tuner.generate_multiple_parameters(ids, st_callback=self.send_trial_callback)\n\n        for i, _ in enumerate(params_list):\n            send(CommandType.NewTrialJob, _pack_parameter(ids[i], params_list[i]))\n        # when parameters is None.\n        if len(params_list) < len(ids):\n            send(CommandType.NoMoreTrialJobs, _pack_parameter(ids[0], ''))",
  "def handle_update_search_space(self, data):\n        self.tuner.update_search_space(data)",
  "def handle_import_data(self, data):\n        \"\"\"Import additional data for tuning\n        data: a list of dictionaries, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        for entry in data:\n            entry['value'] = json_tricks.loads(entry['value'])\n        self.tuner.import_data(data)",
  "def handle_add_customized_trial(self, data):\n        # data: parameters\n        id_ = _create_parameter_id()\n        _customized_parameter_ids.add(id_)",
  "def handle_report_metric_data(self, data):\n        \"\"\"\n        data: a dict received from nni_manager, which contains:\n              - 'parameter_id': id of the trial\n              - 'value': metric value reported by nni.report_final_result()\n              - 'type': report type, support {'FINAL', 'PERIODICAL'}\n        \"\"\"\n        # metrics value is dumped as json string in trial, so we need to decode it here\n        if 'value' in data:\n            data['value'] = json_tricks.loads(data['value'])\n        if data['type'] == MetricType.FINAL:\n            self._handle_final_metric_data(data)\n        elif data['type'] == MetricType.PERIODICAL:\n            if self.assessor is not None:\n                self._handle_intermediate_metric_data(data)\n        elif data['type'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data['trial_job_id'] is not None\n            assert data['parameter_index'] is not None\n            param_id = _create_parameter_id()\n            try:\n                param = self.tuner.generate_parameters(param_id, trial_job_id=data['trial_job_id'])\n            except NoMoreTrialError:\n                param = None\n            send(CommandType.SendTrialJobParameter, _pack_parameter(param_id, param, trial_job_id=data['trial_job_id'],\n                                                                    parameter_index=data['parameter_index']))\n        else:\n            raise ValueError('Data type not supported: {}'.format(data['type']))",
  "def handle_trial_end(self, data):\n        \"\"\"\n        data: it has three keys: trial_job_id, event, hyper_params\n             - trial_job_id: the id generated by training service\n             - event: the job's state\n             - hyper_params: the hyperparameters generated and returned by tuner\n        \"\"\"\n        trial_job_id = data['trial_job_id']\n        _ended_trials.add(trial_job_id)\n        if trial_job_id in _trial_history:\n            _trial_history.pop(trial_job_id)\n            if self.assessor is not None:\n                self.assessor.trial_end(trial_job_id, data['event'] == 'SUCCEEDED')\n        if self.tuner is not None:\n            self.tuner.trial_end(json_tricks.loads(data['hyper_params'])['parameter_id'], data['event'] == 'SUCCEEDED')",
  "def _handle_final_metric_data(self, data):\n        \"\"\"Call tuner to process final results\n        \"\"\"\n        id_ = data['parameter_id']\n        value = data['value']\n        if id_ is None or id_ in _customized_parameter_ids:\n            if not hasattr(self.tuner, '_accept_customized'):\n                self.tuner._accept_customized = False\n            if not self.tuner._accept_customized:\n                _logger.info('Customized trial job %s ignored by tuner', id_)\n                return\n            customized = True\n        else:\n            customized = False\n            self.tuner.receive_trial_result(id_, _trial_params[id_], value, customized=customized,\n                                            trial_job_id=data.get('trial_job_id'))",
  "def _handle_intermediate_metric_data(self, data):\n        \"\"\"Call assessor to process intermediate results\n        \"\"\"\n        if data['type'] != MetricType.PERIODICAL:\n            return\n        if self.assessor is None:\n            return\n\n        trial_job_id = data['trial_job_id']\n        if trial_job_id in _ended_trials:\n            return\n\n        history = _trial_history[trial_job_id]\n        history[data['sequence']] = data['value']\n        ordered_history = _sort_history(history)\n        if len(ordered_history) < data['sequence']:  # no user-visible update since last time\n            return\n\n        try:\n            result = self.assessor.assess_trial(trial_job_id, ordered_history)\n        except Exception as e:\n            _logger.error('Assessor error')\n            _logger.exception(e)\n\n        if isinstance(result, bool):\n            result = AssessResult.Good if result else AssessResult.Bad\n        elif not isinstance(result, AssessResult):\n            msg = 'Result of Assessor.assess_trial must be an object of AssessResult, not %s'\n            raise RuntimeError(msg % type(result))\n\n        if result is AssessResult.Bad:\n            _logger.debug('BAD, kill %s', trial_job_id)\n            send(CommandType.KillTrialJob, json_tricks.dumps(trial_job_id))\n            # notify tuner\n            _logger.debug('env var: NNI_INCLUDE_INTERMEDIATE_RESULTS: [%s]',\n                          dispatcher_env_vars.NNI_INCLUDE_INTERMEDIATE_RESULTS)\n            if dispatcher_env_vars.NNI_INCLUDE_INTERMEDIATE_RESULTS == 'true':\n                self._earlystop_notify_tuner(data)\n        else:\n            _logger.debug('GOOD')",
  "def _earlystop_notify_tuner(self, data):\n        \"\"\"Send last intermediate result as final result to tuner in case the\n        trial is early stopped.\n        \"\"\"\n        _logger.debug('Early stop notify tuner data: [%s]', data)\n        data['type'] = MetricType.FINAL\n        if multi_thread_enabled():\n            self._handle_final_metric_data(data)\n        else:\n            data['value'] = to_json(data['value'])\n            self.enqueue_command(CommandType.ReportMetricData, data)",
  "class CommandType(Enum):\n    # in\n    Initialize = b'IN'\n    RequestTrialJobs = b'GE'\n    ReportMetricData = b'ME'\n    UpdateSearchSpace = b'SS'\n    ImportData = b'FD'\n    AddCustomizedTrialJob = b'AD'\n    TrialEnd = b'EN'\n    Terminate = b'TE'\n    Ping = b'PI'\n\n    # out\n    Initialized = b'ID'\n    NewTrialJob = b'TR'\n    SendTrialJobParameter = b'SP'\n    NoMoreTrialJobs = b'NO'\n    KillTrialJob = b'KI'",
  "def send(command, data):\n    \"\"\"Send command to Training Service.\n    command: CommandType object.\n    data: string payload.\n    \"\"\"\n    global _lock\n    try:\n        _lock.acquire()\n        data = data.encode('utf8')\n        msg = b'%b%014d%b' % (command.value, len(data), data)\n        logging.getLogger(__name__).debug('Sending command, data: [%s]', msg)\n        _out_file.write(msg)\n        _out_file.flush()\n    finally:\n        _lock.release()",
  "def receive():\n    \"\"\"Receive a command from Training Service.\n    Returns a tuple of command (CommandType) and payload (str)\n    \"\"\"\n    header = _in_file.read(16)\n    logging.getLogger(__name__).debug('Received command, header: [%s]', header)\n    if header is None or len(header) < 16:\n        # Pipe EOF encountered\n        logging.getLogger(__name__).debug('Pipe EOF encountered')\n        return None, None\n    length = int(header[2:])\n    data = _in_file.read(length)\n    command = CommandType(header[:2])\n    data = data.decode('utf8')\n    logging.getLogger(__name__).debug('Received command, data: [%s]', data)\n    return command, data",
  "def choice(options, random_state):\n    '''\n    options: 1-D array-like or int\n    random_state: an object of numpy.random.RandomState\n    '''\n    return random_state.choice(options)",
  "def randint(lower, upper, random_state):\n    '''\n    Generate a random integer from `lower` (inclusive) to `upper` (exclusive).\n    lower: an int that represent an lower bound\n    upper: an int that represent an upper bound\n    random_state: an object of numpy.random.RandomState\n    '''\n    return random_state.randint(lower, upper)",
  "def uniform(low, high, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    random_state: an object of numpy.random.RandomState\n    '''\n    assert high >= low, 'Upper bound must be larger than lower bound'\n    return random_state.uniform(low, high)",
  "def quniform(low, high, q, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.clip(np.round(uniform(low, high, random_state) / q) * q, low, high)",
  "def loguniform(low, high, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    random_state: an object of numpy.random.RandomState\n    '''\n    assert low > 0, 'Lower bound must be positive'\n    return np.exp(uniform(np.log(low), np.log(high), random_state))",
  "def qloguniform(low, high, q, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.clip(np.round(loguniform(low, high, random_state) / q) * q, low, high)",
  "def normal(mu, sigma, random_state):\n    '''\n    The probability density function of the normal distribution,\n    first derived by De Moivre and 200 years later by both Gauss and Laplace independently.\n    mu: float or array_like of floats\n        Mean (\u201ccentre\u201d) of the distribution.\n    sigma: float or array_like of floats\n           Standard deviation (spread or \u201cwidth\u201d) of the distribution.\n    random_state: an object of numpy.random.RandomState\n    '''\n    return random_state.normal(mu, sigma)",
  "def qnormal(mu, sigma, q, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.round(normal(mu, sigma, random_state) / q) * q",
  "def lognormal(mu, sigma, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.exp(normal(mu, sigma, random_state))",
  "def qlognormal(mu, sigma, q, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.round(lognormal(mu, sigma, random_state) / q) * q",
  "def _match_val_type(vals, bounds):\n    \"\"\"\n    Update values in the array, to match their corresponding type, make sure the value is legal.\n\n    Parameters\n    ----------\n    vals : numpy array\n        values of parameters\n    bounds : numpy array\n        list of dictionary which stores parameters names and legal values.\n\n    Returns\n    -------\n    vals_new : list\n        The closest legal value to the original value\n    \"\"\"\n    vals_new = []\n\n    for i, bound in enumerate(bounds):\n        _type = bound['_type']\n        if _type == \"choice\":\n            # Find the closest integer in the array, vals_bounds\n            # pylint: disable=cell-var-from-loop\n            vals_new.append(min(bound['_value'], key=lambda x: abs(x - vals[i])))\n        elif _type in ['quniform', 'randint']:\n            vals_new.append(np.around(vals[i]))\n        else:\n            vals_new.append(vals[i])\n\n    return vals_new",
  "def acq_max(f_acq, gp, y_max, bounds, space, num_warmup, num_starting_points):\n    \"\"\"\n    A function to find the maximum of the acquisition function\n\n    It uses a combination of random sampling (cheap) and the 'L-BFGS-B'\n    optimization method. First by sampling ``num_warmup`` points at random,\n    and then running L-BFGS-B from ``num_starting_points`` random starting points.\n\n    Parameters\n    ----------\n    f_acq : UtilityFunction.utility\n        The acquisition function object that return its point-wise value.\n\n    gp : GaussianProcessRegressor\n        A gaussian process fitted to the relevant data.\n\n    y_max : float\n        The current maximum known value of the target function.\n\n    bounds : numpy array\n        The variables bounds to limit the search of the acq max.\n\n    num_warmup : int\n        number of times to randomly sample the aquisition function\n\n    num_starting_points : int\n        number of times to run scipy.minimize\n\n    Returns\n    -------\n    numpy array\n        The parameter which achieves max of the acquisition function.\n    \"\"\"\n\n    # Warm up with random points\n    x_tries = [space.random_sample()\n               for _ in range(int(num_warmup))]\n    ys = f_acq(x_tries, gp=gp, y_max=y_max)\n    x_max = x_tries[ys.argmax()]\n    max_acq = ys.max()\n\n\n    # Explore the parameter space more throughly\n    x_seeds = [space.random_sample() for _ in range(int(num_starting_points))]\n\n    bounds_minmax = np.array(\n        [[bound['_value'][0], bound['_value'][-1]] for bound in bounds])\n\n    for x_try in x_seeds:\n        # Find the minimum of minus the acquisition function\n        res = minimize(lambda x: -f_acq(x.reshape(1, -1), gp=gp, y_max=y_max),\n                       x_try.reshape(1, -1),\n                       bounds=bounds_minmax,\n                       method=\"L-BFGS-B\")\n\n        # See if success\n        if not res.success:\n            continue\n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or -res.fun[0] >= max_acq:\n            x_max = _match_val_type(res.x, bounds)\n            max_acq = -res.fun[0]\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    return np.clip(x_max, bounds_minmax[:, 0], bounds_minmax[:, 1])",
  "class UtilityFunction():\n    \"\"\"\n    A class to compute different acquisition function values.\n\n    Parameters\n    ----------\n    kind : string\n        specification of utility function to use\n    kappa : float\n        parameter usedd for 'ucb' acquisition function\n    xi : float\n        parameter usedd for 'ei' and 'poi' acquisition function\n    \"\"\"\n\n    def __init__(self, kind, kappa, xi):\n        self._kappa = kappa\n        self._xi = xi\n\n        if kind not in ['ucb', 'ei', 'poi']:\n            err = \"The utility function \" \\\n                \"{} has not been implemented, \" \\\n                \"please choose one of ucb, ei, or poi.\".format(kind)\n            raise NotImplementedError(err)\n        self._kind = kind\n\n    def utility(self, x, gp, y_max):\n        \"\"\"\n        return utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n\n        Returns\n        -------\n        function\n            return corresponding function, return None if parameter is illegal\n        \"\"\"\n        if self._kind == 'ucb':\n            return self._ucb(x, gp, self._kappa)\n        if self._kind == 'ei':\n            return self._ei(x, gp, y_max, self._xi)\n        if self._kind == 'poi':\n            return self._poi(x, gp, y_max, self._xi)\n        return None\n\n    @staticmethod\n    def _ucb(x, gp, kappa):\n        \"\"\"\n        Upper Confidence Bound (UCB) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        kappa : float\n\n        Returns\n        -------\n        float\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            mean, std = gp.predict(x, return_std=True)\n\n        return mean + kappa * std\n\n    @staticmethod\n    def _ei(x, gp, y_max, xi):\n        \"\"\"\n        Expected Improvement (EI) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n        xi : float\n\n        Returns\n        -------\n        float\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            mean, std = gp.predict(x, return_std=True)\n\n        z = (mean - y_max - xi)/std\n        return (mean - y_max - xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    @staticmethod\n    def _poi(x, gp, y_max, xi):\n        \"\"\"\n        Possibility Of Improvement (POI) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n        xi : float\n\n        Returns\n        -------\n        float\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            mean, std = gp.predict(x, return_std=True)\n\n        z = (mean - y_max - xi)/std\n        return norm.cdf(z)",
  "def __init__(self, kind, kappa, xi):\n        self._kappa = kappa\n        self._xi = xi\n\n        if kind not in ['ucb', 'ei', 'poi']:\n            err = \"The utility function \" \\\n                \"{} has not been implemented, \" \\\n                \"please choose one of ucb, ei, or poi.\".format(kind)\n            raise NotImplementedError(err)\n        self._kind = kind",
  "def utility(self, x, gp, y_max):\n        \"\"\"\n        return utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n\n        Returns\n        -------\n        function\n            return corresponding function, return None if parameter is illegal\n        \"\"\"\n        if self._kind == 'ucb':\n            return self._ucb(x, gp, self._kappa)\n        if self._kind == 'ei':\n            return self._ei(x, gp, y_max, self._xi)\n        if self._kind == 'poi':\n            return self._poi(x, gp, y_max, self._xi)\n        return None",
  "def _ucb(x, gp, kappa):\n        \"\"\"\n        Upper Confidence Bound (UCB) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        kappa : float\n\n        Returns\n        -------\n        float\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            mean, std = gp.predict(x, return_std=True)\n\n        return mean + kappa * std",
  "def _ei(x, gp, y_max, xi):\n        \"\"\"\n        Expected Improvement (EI) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n        xi : float\n\n        Returns\n        -------\n        float\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            mean, std = gp.predict(x, return_std=True)\n\n        z = (mean - y_max - xi)/std\n        return (mean - y_max - xi) * norm.cdf(z) + std * norm.pdf(z)",
  "def _poi(x, gp, y_max, xi):\n        \"\"\"\n        Possibility Of Improvement (POI) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n        xi : float\n\n        Returns\n        -------\n        float\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            mean, std = gp.predict(x, return_std=True)\n\n        z = (mean - y_max - xi)/std\n        return norm.cdf(z)",
  "class GPClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('utility'): self.choices('utility', 'ei', 'ucb', 'poi'),\n            Optional('kappa'): float,\n            Optional('xi'): float,\n            Optional('nu'): float,\n            Optional('alpha'): float,\n            Optional('cold_start_num'): int,\n            Optional('selection_num_warm_up'):  int,\n            Optional('selection_num_starting_points'):  int,\n        }).validate(kwargs)",
  "class GPTuner(Tuner):\n    \"\"\"\n    GPTuner is a Bayesian Optimization method where Gaussian Process is used for modeling loss functions.\n\n    Parameters\n    ----------\n    optimize_mode : str\n        optimize mode, 'maximize' or 'minimize', by default 'maximize'\n    utility : str\n        utility function (also called 'acquisition funcition') to use, which can be 'ei', 'ucb' or 'poi'. By default 'ei'.\n    kappa : float\n        value used by utility function 'ucb'. The bigger kappa is, the more the tuner will be exploratory. By default 5.\n    xi : float\n        used by utility function 'ei' and 'poi'. The bigger xi is, the more the tuner will be exploratory. By default 0.\n    nu : float\n        used to specify Matern kernel. The smaller nu, the less smooth the approximated function is. By default 2.5.\n    alpha : float\n        Used to specify Gaussian Process Regressor. Larger values correspond to increased noise level in the observations.\n        By default 1e-6.\n    cold_start_num : int\n        Number of random exploration to perform before Gaussian Process. By default 10.\n    selection_num_warm_up : int\n        Number of random points to evaluate for getting the point which maximizes the acquisition function. By default 100000\n    selection_num_starting_points : int\n        Number of times to run L-BFGS-B from a random starting point after the warmup. By default 250.\n    \"\"\"\n\n    def __init__(self, optimize_mode=\"maximize\", utility='ei', kappa=5, xi=0, nu=2.5, alpha=1e-6, cold_start_num=10,\n                 selection_num_warm_up=100000, selection_num_starting_points=250):\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # utility function related\n        self._utility = utility\n        self._kappa = kappa\n        self._xi = xi\n\n        # target space\n        self._space = None\n\n        self._random_state = np.random.RandomState()\n\n        # nu, alpha are GPR related params\n        self._gp = GaussianProcessRegressor(\n            kernel=Matern(nu=nu),\n            alpha=alpha,\n            normalize_y=True,\n            n_restarts_optimizer=25,\n            random_state=self._random_state\n        )\n        # num of random evaluations before GPR\n        self._cold_start_num = cold_start_num\n\n        # params for acq_max\n        self._selection_num_warm_up = selection_num_warm_up\n        self._selection_num_starting_points = selection_num_starting_points\n\n        # num of imported data\n        self._supplement_data_num = 0\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Update the self.bounds and self.types by the search_space.json file.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        \"\"\"\n        self._space = TargetSpace(search_space, self._random_state)\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Method which provides one set of hyper-parameters.\n        If the number of trial result is lower than cold_start_number, GPTuner will first randomly generate some parameters.\n        Otherwise, choose the parameters by the Gussian Process Model.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        \"\"\"\n        if self._space.len() < self._cold_start_num:\n            results = self._space.random_sample()\n        else:\n            # Sklearn's GP throws a large number of warnings at times, but\n            # we don't really need to see them here.\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                self._gp.fit(self._space.params, self._space.target)\n\n            util = UtilityFunction(\n                kind=self._utility, kappa=self._kappa, xi=self._xi)\n\n            results = acq_max(\n                f_acq=util.utility,\n                gp=self._gp,\n                y_max=self._space.target.max(),\n                bounds=self._space.bounds,\n                space=self._space,\n                num_warmup=self._selection_num_warm_up,\n                num_starting_points=self._selection_num_starting_points\n            )\n\n        results = self._space.array_to_params(results)\n        logger.info(\"Generate paramageters:\\n %s\", results)\n        return results\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Method invoked when a trial reports its final result.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        \"\"\"\n        value = extract_scalar_reward(value)\n        if self._optimize_mode == OptimizeMode.Minimize:\n            value = -value\n\n        logger.info(\"Received trial result.\")\n        logger.info(\"value :%s\", value)\n        logger.info(\"parameter : %s\", parameters)\n        self._space.register(parameters, value)\n\n    def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\n                \"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\n                    \"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            self._supplement_data_num += 1\n            _parameter_id = '_'.join(\n                [\"ImportData\", str(self._supplement_data_num)])\n            self.receive_trial_result(\n                parameter_id=_parameter_id, parameters=_params, value=_value)\n        logger.info(\"Successfully import data to GP tuner.\")",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('utility'): self.choices('utility', 'ei', 'ucb', 'poi'),\n            Optional('kappa'): float,\n            Optional('xi'): float,\n            Optional('nu'): float,\n            Optional('alpha'): float,\n            Optional('cold_start_num'): int,\n            Optional('selection_num_warm_up'):  int,\n            Optional('selection_num_starting_points'):  int,\n        }).validate(kwargs)",
  "def __init__(self, optimize_mode=\"maximize\", utility='ei', kappa=5, xi=0, nu=2.5, alpha=1e-6, cold_start_num=10,\n                 selection_num_warm_up=100000, selection_num_starting_points=250):\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # utility function related\n        self._utility = utility\n        self._kappa = kappa\n        self._xi = xi\n\n        # target space\n        self._space = None\n\n        self._random_state = np.random.RandomState()\n\n        # nu, alpha are GPR related params\n        self._gp = GaussianProcessRegressor(\n            kernel=Matern(nu=nu),\n            alpha=alpha,\n            normalize_y=True,\n            n_restarts_optimizer=25,\n            random_state=self._random_state\n        )\n        # num of random evaluations before GPR\n        self._cold_start_num = cold_start_num\n\n        # params for acq_max\n        self._selection_num_warm_up = selection_num_warm_up\n        self._selection_num_starting_points = selection_num_starting_points\n\n        # num of imported data\n        self._supplement_data_num = 0",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Update the self.bounds and self.types by the search_space.json file.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        \"\"\"\n        self._space = TargetSpace(search_space, self._random_state)",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Method which provides one set of hyper-parameters.\n        If the number of trial result is lower than cold_start_number, GPTuner will first randomly generate some parameters.\n        Otherwise, choose the parameters by the Gussian Process Model.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        \"\"\"\n        if self._space.len() < self._cold_start_num:\n            results = self._space.random_sample()\n        else:\n            # Sklearn's GP throws a large number of warnings at times, but\n            # we don't really need to see them here.\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                self._gp.fit(self._space.params, self._space.target)\n\n            util = UtilityFunction(\n                kind=self._utility, kappa=self._kappa, xi=self._xi)\n\n            results = acq_max(\n                f_acq=util.utility,\n                gp=self._gp,\n                y_max=self._space.target.max(),\n                bounds=self._space.bounds,\n                space=self._space,\n                num_warmup=self._selection_num_warm_up,\n                num_starting_points=self._selection_num_starting_points\n            )\n\n        results = self._space.array_to_params(results)\n        logger.info(\"Generate paramageters:\\n %s\", results)\n        return results",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Method invoked when a trial reports its final result.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        \"\"\"\n        value = extract_scalar_reward(value)\n        if self._optimize_mode == OptimizeMode.Minimize:\n            value = -value\n\n        logger.info(\"Received trial result.\")\n        logger.info(\"value :%s\", value)\n        logger.info(\"parameter : %s\", parameters)\n        self._space.register(parameters, value)",
  "def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\n                \"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\n                    \"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            self._supplement_data_num += 1\n            _parameter_id = '_'.join(\n                [\"ImportData\", str(self._supplement_data_num)])\n            self.receive_trial_result(\n                parameter_id=_parameter_id, parameters=_params, value=_value)\n        logger.info(\"Successfully import data to GP tuner.\")",
  "def _hashable(params):\n    \"\"\"\n    Transform list params to tuple format. Ensure that an point is hashable by a python dict.\n\n    Parameters\n    ----------\n    params : numpy array\n        array format of parameters\n\n    Returns\n    -------\n    tuple\n        tuple format of parameters\n    \"\"\"\n    return tuple(map(float, params))",
  "class TargetSpace():\n    \"\"\"\n    Holds the param-space coordinates (X) and target values (Y)\n\n    Parameters\n    ----------\n    pbounds : dict\n        Dictionary with parameters names and legal values.\n\n    random_state : int, RandomState, or None\n        optionally specify a seed for a random number generator, by default None.\n    \"\"\"\n\n    def __init__(self, pbounds, random_state=None):\n        self._random_state = random_state\n\n        # Get the name of the parameters\n        self._keys = sorted(pbounds)\n\n        # Create an array with parameters bounds\n        self._bounds = np.array(\n            [item[1] for item in sorted(pbounds.items(), key=lambda x: x[0])]\n        )\n\n        # check values type\n        for _bound in self._bounds:\n            if _bound['_type'] == 'choice':\n                try:\n                    [float(val) for val in _bound['_value']]\n                except ValueError:\n                    raise ValueError(\"GP Tuner supports only numerical values\")\n\n        # preallocated memory for X and Y points\n        self._params = np.empty(shape=(0, self.dim))\n        self._target = np.empty(shape=(0))\n\n        # keep track of unique points we have seen so far\n        self._cache = {}\n\n    def __contains__(self, params):\n        \"\"\"\n        check if a parameter is already registered\n\n        Parameters\n        ----------\n        params : numpy array\n\n        Returns\n        -------\n        bool\n            True if the parameter is already registered, else false\n        \"\"\"\n        return _hashable(params) in self._cache\n\n    def len(self):\n        \"\"\"\n        length of registered params and targets\n\n        Returns\n        -------\n        int\n        \"\"\"\n        assert len(self._params) == len(self._target)\n        return len(self._target)\n\n    @property\n    def params(self):\n        \"\"\"\n        registered parameters\n\n        Returns\n        -------\n        numpy array\n        \"\"\"\n        return self._params\n\n    @property\n    def target(self):\n        \"\"\"\n        registered target values\n\n        Returns\n        -------\n        numpy array\n        \"\"\"\n        return self._target\n\n    @property\n    def dim(self):\n        \"\"\"\n        dimension of parameters\n\n        Returns\n        -------\n        int\n        \"\"\"\n        return len(self._keys)\n\n    @property\n    def keys(self):\n        \"\"\"\n        keys of parameters\n\n        Returns\n        -------\n        numpy array\n        \"\"\"\n        return self._keys\n\n    @property\n    def bounds(self):\n        \"\"\"\n        bounds of parameters\n\n        Returns\n        -------\n        numpy array\n        \"\"\"\n        return self._bounds\n\n    def params_to_array(self, params):\n        \"\"\"\n        dict to array\n\n        Parameters\n        ----------\n        params : dict\n            dict format of parameters\n\n        Returns\n        -------\n        numpy array\n            array format of parameters\n        \"\"\"\n        try:\n            assert set(params) == set(self.keys)\n        except AssertionError:\n            raise ValueError(\n                \"Parameters' keys ({}) do \".format(sorted(params)) +\n                \"not match the expected set of keys ({}).\".format(self.keys)\n            )\n        return np.asarray([params[key] for key in self.keys])\n\n    def array_to_params(self, x):\n        \"\"\"\n        array to dict\n\n        maintain int type if the paramters is defined as int in search_space.json\n        Parameters\n        ----------\n        x : numpy array\n            array format of parameters\n\n        Returns\n        -------\n        dict\n            dict format of parameters\n        \"\"\"\n        try:\n            assert len(x) == len(self.keys)\n        except AssertionError:\n            raise ValueError(\n                \"Size of array ({}) is different than the \".format(len(x)) +\n                \"expected number of parameters ({}).\".format(self.dim)\n            )\n\n        params = {}\n        for i, _bound in enumerate(self._bounds):\n            if _bound['_type'] == 'choice' and all(isinstance(val, int) for val in _bound['_value']):\n                params.update({self.keys[i]: int(x[i])})\n            elif _bound['_type'] in ['randint']:\n                params.update({self.keys[i]: int(x[i])})\n            else:\n                params.update({self.keys[i]:  x[i]})\n\n        return params\n\n    def register(self, params, target):\n        \"\"\"\n        Append a point and its target value to the known data.\n\n        Parameters\n        ----------\n        params : dict\n            parameters\n\n        target : float\n            target function value\n        \"\"\"\n\n        x = self.params_to_array(params)\n        if x in self:\n            print('Data point {} is not unique'.format(x))\n\n        # Insert data into unique dictionary\n        self._cache[_hashable(x.ravel())] = target\n\n        self._params = np.concatenate([self._params, x.reshape(1, -1)])\n        self._target = np.concatenate([self._target, [target]])\n\n    def random_sample(self):\n        \"\"\"\n        Creates a random point within the bounds of the space.\n\n        Returns\n        -------\n        numpy array\n            one groupe of parameter\n        \"\"\"\n        params = np.empty(self.dim)\n        for col, _bound in enumerate(self._bounds):\n            if _bound['_type'] == 'choice':\n                params[col] = parameter_expressions.choice(\n                    _bound['_value'], self._random_state)\n            elif _bound['_type'] == 'randint':\n                params[col] = self._random_state.randint(\n                    _bound['_value'][0], _bound['_value'][1], size=1)\n            elif _bound['_type'] == 'uniform':\n                params[col] = parameter_expressions.uniform(\n                    _bound['_value'][0], _bound['_value'][1], self._random_state)\n            elif _bound['_type'] == 'quniform':\n                params[col] = parameter_expressions.quniform(\n                    _bound['_value'][0], _bound['_value'][1], _bound['_value'][2], self._random_state)\n            elif _bound['_type'] == 'loguniform':\n                params[col] = parameter_expressions.loguniform(\n                    _bound['_value'][0], _bound['_value'][1], self._random_state)\n            elif _bound['_type'] == 'qloguniform':\n                params[col] = parameter_expressions.qloguniform(\n                    _bound['_value'][0], _bound['_value'][1], _bound['_value'][2], self._random_state)\n\n        return params\n\n    def max(self):\n        \"\"\"\n        Get maximum target value found and its corresponding parameters.\n\n        Returns\n        -------\n        dict\n            target value and parameters, empty dict if nothing registered\n        \"\"\"\n        try:\n            res = {\n                'target': self.target.max(),\n                'params': dict(\n                    zip(self.keys, self.params[self.target.argmax()])\n                )\n            }\n        except ValueError:\n            res = {}\n        return res\n\n    def res(self):\n        \"\"\"\n        Get all target values found and corresponding parameters.\n\n        Returns\n        -------\n        list\n            a list of target values and their corresponding parameters\n        \"\"\"\n        params = [dict(zip(self.keys, p)) for p in self.params]\n\n        return [\n            {\"target\": target, \"params\": param}\n            for target, param in zip(self.target, params)\n        ]",
  "def __init__(self, pbounds, random_state=None):\n        self._random_state = random_state\n\n        # Get the name of the parameters\n        self._keys = sorted(pbounds)\n\n        # Create an array with parameters bounds\n        self._bounds = np.array(\n            [item[1] for item in sorted(pbounds.items(), key=lambda x: x[0])]\n        )\n\n        # check values type\n        for _bound in self._bounds:\n            if _bound['_type'] == 'choice':\n                try:\n                    [float(val) for val in _bound['_value']]\n                except ValueError:\n                    raise ValueError(\"GP Tuner supports only numerical values\")\n\n        # preallocated memory for X and Y points\n        self._params = np.empty(shape=(0, self.dim))\n        self._target = np.empty(shape=(0))\n\n        # keep track of unique points we have seen so far\n        self._cache = {}",
  "def __contains__(self, params):\n        \"\"\"\n        check if a parameter is already registered\n\n        Parameters\n        ----------\n        params : numpy array\n\n        Returns\n        -------\n        bool\n            True if the parameter is already registered, else false\n        \"\"\"\n        return _hashable(params) in self._cache",
  "def len(self):\n        \"\"\"\n        length of registered params and targets\n\n        Returns\n        -------\n        int\n        \"\"\"\n        assert len(self._params) == len(self._target)\n        return len(self._target)",
  "def params(self):\n        \"\"\"\n        registered parameters\n\n        Returns\n        -------\n        numpy array\n        \"\"\"\n        return self._params",
  "def target(self):\n        \"\"\"\n        registered target values\n\n        Returns\n        -------\n        numpy array\n        \"\"\"\n        return self._target",
  "def dim(self):\n        \"\"\"\n        dimension of parameters\n\n        Returns\n        -------\n        int\n        \"\"\"\n        return len(self._keys)",
  "def keys(self):\n        \"\"\"\n        keys of parameters\n\n        Returns\n        -------\n        numpy array\n        \"\"\"\n        return self._keys",
  "def bounds(self):\n        \"\"\"\n        bounds of parameters\n\n        Returns\n        -------\n        numpy array\n        \"\"\"\n        return self._bounds",
  "def params_to_array(self, params):\n        \"\"\"\n        dict to array\n\n        Parameters\n        ----------\n        params : dict\n            dict format of parameters\n\n        Returns\n        -------\n        numpy array\n            array format of parameters\n        \"\"\"\n        try:\n            assert set(params) == set(self.keys)\n        except AssertionError:\n            raise ValueError(\n                \"Parameters' keys ({}) do \".format(sorted(params)) +\n                \"not match the expected set of keys ({}).\".format(self.keys)\n            )\n        return np.asarray([params[key] for key in self.keys])",
  "def array_to_params(self, x):\n        \"\"\"\n        array to dict\n\n        maintain int type if the paramters is defined as int in search_space.json\n        Parameters\n        ----------\n        x : numpy array\n            array format of parameters\n\n        Returns\n        -------\n        dict\n            dict format of parameters\n        \"\"\"\n        try:\n            assert len(x) == len(self.keys)\n        except AssertionError:\n            raise ValueError(\n                \"Size of array ({}) is different than the \".format(len(x)) +\n                \"expected number of parameters ({}).\".format(self.dim)\n            )\n\n        params = {}\n        for i, _bound in enumerate(self._bounds):\n            if _bound['_type'] == 'choice' and all(isinstance(val, int) for val in _bound['_value']):\n                params.update({self.keys[i]: int(x[i])})\n            elif _bound['_type'] in ['randint']:\n                params.update({self.keys[i]: int(x[i])})\n            else:\n                params.update({self.keys[i]:  x[i]})\n\n        return params",
  "def register(self, params, target):\n        \"\"\"\n        Append a point and its target value to the known data.\n\n        Parameters\n        ----------\n        params : dict\n            parameters\n\n        target : float\n            target function value\n        \"\"\"\n\n        x = self.params_to_array(params)\n        if x in self:\n            print('Data point {} is not unique'.format(x))\n\n        # Insert data into unique dictionary\n        self._cache[_hashable(x.ravel())] = target\n\n        self._params = np.concatenate([self._params, x.reshape(1, -1)])\n        self._target = np.concatenate([self._target, [target]])",
  "def random_sample(self):\n        \"\"\"\n        Creates a random point within the bounds of the space.\n\n        Returns\n        -------\n        numpy array\n            one groupe of parameter\n        \"\"\"\n        params = np.empty(self.dim)\n        for col, _bound in enumerate(self._bounds):\n            if _bound['_type'] == 'choice':\n                params[col] = parameter_expressions.choice(\n                    _bound['_value'], self._random_state)\n            elif _bound['_type'] == 'randint':\n                params[col] = self._random_state.randint(\n                    _bound['_value'][0], _bound['_value'][1], size=1)\n            elif _bound['_type'] == 'uniform':\n                params[col] = parameter_expressions.uniform(\n                    _bound['_value'][0], _bound['_value'][1], self._random_state)\n            elif _bound['_type'] == 'quniform':\n                params[col] = parameter_expressions.quniform(\n                    _bound['_value'][0], _bound['_value'][1], _bound['_value'][2], self._random_state)\n            elif _bound['_type'] == 'loguniform':\n                params[col] = parameter_expressions.loguniform(\n                    _bound['_value'][0], _bound['_value'][1], self._random_state)\n            elif _bound['_type'] == 'qloguniform':\n                params[col] = parameter_expressions.qloguniform(\n                    _bound['_value'][0], _bound['_value'][1], _bound['_value'][2], self._random_state)\n\n        return params",
  "def max(self):\n        \"\"\"\n        Get maximum target value found and its corresponding parameters.\n\n        Returns\n        -------\n        dict\n            target value and parameters, empty dict if nothing registered\n        \"\"\"\n        try:\n            res = {\n                'target': self.target.max(),\n                'params': dict(\n                    zip(self.keys, self.params[self.target.argmax()])\n                )\n            }\n        except ValueError:\n            res = {}\n        return res",
  "def res(self):\n        \"\"\"\n        Get all target values found and corresponding parameters.\n\n        Returns\n        -------\n        list\n            a list of target values and their corresponding parameters\n        \"\"\"\n        params = [dict(zip(self.keys, p)) for p in self.params]\n\n        return [\n            {\"target\": target, \"params\": param}\n            for target, param in zip(self.target, params)\n        ]",
  "class FeatureSelector():\n\n    def __init__(self, **kwargs):\n        self.selected_features_ = None\n        self.X = None\n        self.y = None\n\n\n    def fit(self, X, y, **kwargs):\n        \"\"\"\n        Fit the training data to FeatureSelector\n\n        Paramters\n        ---------\n        X : array-like numpy matrix\n            The training input samples, which shape is [n_samples, n_features].\n        y: array-like numpy matrix\n            The target values (class labels in classification, real numbers in\n            regression). Which shape is [n_samples].\n        \"\"\"\n        self.X = X\n        self.y = y\n\n\n    def get_selected_features(self):\n        \"\"\"\n        Fit the training data to FeatureSelector\n\n        Returns\n        -------\n        list :\n                Return the index of imprtant feature.\n        \"\"\"\n        return self.selected_features_",
  "def __init__(self, **kwargs):\n        self.selected_features_ = None\n        self.X = None\n        self.y = None",
  "def fit(self, X, y, **kwargs):\n        \"\"\"\n        Fit the training data to FeatureSelector\n\n        Paramters\n        ---------\n        X : array-like numpy matrix\n            The training input samples, which shape is [n_samples, n_features].\n        y: array-like numpy matrix\n            The target values (class labels in classification, real numbers in\n            regression). Which shape is [n_samples].\n        \"\"\"\n        self.X = X\n        self.y = y",
  "def get_selected_features(self):\n        \"\"\"\n        Fit the training data to FeatureSelector\n\n        Returns\n        -------\n        list :\n                Return the index of imprtant feature.\n        \"\"\"\n        return self.selected_features_",
  "class GBDTSelector(FeatureSelector):\n\n    def __init__(self, **kwargs):\n        self.selected_features_ = None\n        self.X = None\n        self.y = None\n        self.feature_importance = None\n        self.lgb_params = None\n        self.eval_ratio = None\n        self.early_stopping_rounds = None\n        self.importance_type = None\n        self.num_boost_round = None\n        self.model = None\n\n\n    def fit(self, X, y, **kwargs):\n        \"\"\"\n        Fit the training data to FeatureSelector\n\n        Paramters\n        ---------\n        X : array-like numpy matrix\n            The training input samples, which shape is [n_samples, n_features].\n        y : array-like numpy matrix\n            The target values (class labels in classification, real numbers in\n            regression). Which shape is [n_samples].\n        lgb_params : dict\n            Parameters of lightgbm\n        eval_ratio : float\n            The ratio of data size. It's used for split the eval data and train data from self.X.\n        early_stopping_rounds : int\n            The early stopping setting in lightgbm.\n        importance_type : str\n            Supporting type is 'gain' or 'split'.\n        num_boost_round : int\n            num_boost_round in lightgbm.\n        \"\"\"\n        assert kwargs['lgb_params']\n        assert kwargs['eval_ratio']\n        assert kwargs['early_stopping_rounds']\n        assert kwargs['importance_type']\n        assert kwargs['num_boost_round']\n\n        self.X = X\n        self.y = y\n        self.lgb_params = kwargs['lgb_params']\n        self.eval_ratio = kwargs['eval_ratio']\n        self.early_stopping_rounds = kwargs['early_stopping_rounds']\n        self.importance_type = kwargs['importance_type']\n        self.num_boost_round = kwargs['num_boost_round']\n\n        X_train, X_test, y_train, y_test = train_test_split(self.X,\n                                                            self.y,\n                                                            test_size=self.eval_ratio,\n                                                            random_state=random.seed(41))\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n        self.model = lgb.train(self.lgb_params,\n                               lgb_train,\n                               num_boost_round=self.num_boost_round,\n                               valid_sets=lgb_eval,\n                               early_stopping_rounds=self.early_stopping_rounds)\n\n        self.feature_importance = self.model.feature_importance(self.importance_type)\n\n\n    def get_selected_features(self, topk):\n        \"\"\"\n        Fit the training data to FeatureSelector\n\n        Returns\n        -------\n        list :\n                Return the index of imprtant feature.\n        \"\"\"\n        assert topk > 0\n\n        self.selected_features_ = self.feature_importance.argsort()[-topk:][::-1]\n\n        return self.selected_features_",
  "def __init__(self, **kwargs):\n        self.selected_features_ = None\n        self.X = None\n        self.y = None\n        self.feature_importance = None\n        self.lgb_params = None\n        self.eval_ratio = None\n        self.early_stopping_rounds = None\n        self.importance_type = None\n        self.num_boost_round = None\n        self.model = None",
  "def fit(self, X, y, **kwargs):\n        \"\"\"\n        Fit the training data to FeatureSelector\n\n        Paramters\n        ---------\n        X : array-like numpy matrix\n            The training input samples, which shape is [n_samples, n_features].\n        y : array-like numpy matrix\n            The target values (class labels in classification, real numbers in\n            regression). Which shape is [n_samples].\n        lgb_params : dict\n            Parameters of lightgbm\n        eval_ratio : float\n            The ratio of data size. It's used for split the eval data and train data from self.X.\n        early_stopping_rounds : int\n            The early stopping setting in lightgbm.\n        importance_type : str\n            Supporting type is 'gain' or 'split'.\n        num_boost_round : int\n            num_boost_round in lightgbm.\n        \"\"\"\n        assert kwargs['lgb_params']\n        assert kwargs['eval_ratio']\n        assert kwargs['early_stopping_rounds']\n        assert kwargs['importance_type']\n        assert kwargs['num_boost_round']\n\n        self.X = X\n        self.y = y\n        self.lgb_params = kwargs['lgb_params']\n        self.eval_ratio = kwargs['eval_ratio']\n        self.early_stopping_rounds = kwargs['early_stopping_rounds']\n        self.importance_type = kwargs['importance_type']\n        self.num_boost_round = kwargs['num_boost_round']\n\n        X_train, X_test, y_train, y_test = train_test_split(self.X,\n                                                            self.y,\n                                                            test_size=self.eval_ratio,\n                                                            random_state=random.seed(41))\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n        self.model = lgb.train(self.lgb_params,\n                               lgb_train,\n                               num_boost_round=self.num_boost_round,\n                               valid_sets=lgb_eval,\n                               early_stopping_rounds=self.early_stopping_rounds)\n\n        self.feature_importance = self.model.feature_importance(self.importance_type)",
  "def get_selected_features(self, topk):\n        \"\"\"\n        Fit the training data to FeatureSelector\n\n        Returns\n        -------\n        list :\n                Return the index of imprtant feature.\n        \"\"\"\n        assert topk > 0\n\n        self.selected_features_ = self.feature_importance.argsort()[-topk:][::-1]\n\n        return self.selected_features_",
  "class EMA():\n    \"\"\"\n    maintains an exponential moving average\n    \"\"\"\n\n    def __init__(self, f=np.nan, discount_factor=0.1, valid_after=None,\n                 n_iters_relchange=3):\n\n        self.f_ma = [f]\n        self.fs = [f]\n        self.gamma = discount_factor\n        self.rel_change = [np.nan]\n        if valid_after is None:\n            self.valid_after = int(1/discount_factor)\n        else:\n            self.valid_after = valid_after\n        self.n_iters_relchange = n_iters_relchange\n        self.initialized = False\n\n    def reset(self, f):\n\n        self.f_ma = [f]\n        self.fs = [f]\n        self.rel_change = [np.nan]\n        self.initialized = True\n\n    def relchange(self):\n\n        if self.num_updates() > np.max([self.valid_after,\n                                        self.n_iters_relchange]):\n            return np.max(self.rel_change[-self.n_iters_relchange:])\n        else:\n            return np.nan\n\n    def update(self, f_new):\n\n        if not self.initialized:\n            self.reset(f_new)\n        else:\n            self.fs.append(f_new)\n            self.f_ma.append(self.f_ma[-1]*(1-self.gamma) + self.gamma*f_new)\n            if self.num_updates() > self.valid_after:\n                self.rel_change.append(np.abs((self.f_ma[-1]-self.f_ma[-2])\n                                              / self.f_ma[-2]))\n\n    def num_updates(self):\n\n        return len(self.f_ma)\n\n    def __call__(self):\n\n        if self.num_updates() > self.valid_after:\n            return self.f_ma[-1]\n        else:\n            return np.nan",
  "def __init__(self, f=np.nan, discount_factor=0.1, valid_after=None,\n                 n_iters_relchange=3):\n\n        self.f_ma = [f]\n        self.fs = [f]\n        self.gamma = discount_factor\n        self.rel_change = [np.nan]\n        if valid_after is None:\n            self.valid_after = int(1/discount_factor)\n        else:\n            self.valid_after = valid_after\n        self.n_iters_relchange = n_iters_relchange\n        self.initialized = False",
  "def reset(self, f):\n\n        self.f_ma = [f]\n        self.fs = [f]\n        self.rel_change = [np.nan]\n        self.initialized = True",
  "def relchange(self):\n\n        if self.num_updates() > np.max([self.valid_after,\n                                        self.n_iters_relchange]):\n            return np.max(self.rel_change[-self.n_iters_relchange:])\n        else:\n            return np.nan",
  "def update(self, f_new):\n\n        if not self.initialized:\n            self.reset(f_new)\n        else:\n            self.fs.append(f_new)\n            self.f_ma.append(self.f_ma[-1]*(1-self.gamma) + self.gamma*f_new)\n            if self.num_updates() > self.valid_after:\n                self.rel_change.append(np.abs((self.f_ma[-1]-self.f_ma[-2])\n                                              / self.f_ma[-2]))",
  "def num_updates(self):\n\n        return len(self.f_ma)",
  "def __call__(self):\n\n        if self.num_updates() > self.valid_after:\n            return self.f_ma[-1]\n        else:\n            return np.nan",
  "class PrepareData(Dataset):\n\n    def __init__(self,\n                 path_data=None,\n                 data_format=constants.DataFormat.NUMPY,\n                 D=None, N=None,\n                 classification=True,\n                 ordinal=False,\n                 balanced=True,\n                 preprocess=None,\n                 n_to_estimate=None,\n                 MAXMEMGB=syssettings.MAXMEMGB,\n                 set_params=True,\n                 path_mappings=None,\n                 X=None,\n                 y=None,\n                 verbose=0,\n                 n_classes=None,\n                 device=constants.Device.CPU):\n        \"\"\"\n        Dataset class with helpful features and functions for being included in a dataloader\n        and managing memory usage.\n        can read following formats:\n            svm:        svm light format (sklearn.datasets.load_svmlight_file)\n            numpy:      Pass X and y as numpy or sparse arrays\n\n        assumes\n            1. if classification, y is in {-1, 1} or continuous and 0 indexed\n            2. y can fit into memory\n            3. consecutive calls to __getitem__() have consecutive idx values\n\n        notes:\n            1. this implementation is not careful wrt/ precise memory reqts. for\n            example, being able to store one dense row in memory is necessary,\n            but not sufficient.\n            2. for y with 4.2 billion elements, 31.3 GB of memory is  necessary\n            @ 8 bytes/scalar. Use partial fit to avoid loading the entire dataset\n            at once\n            3. disk_size always refer to size of complete data file, even after\n            a split().\n\n\n        Parameters\n        ----------\n        path_data : str\n            Path to load data from\n        data_format : str\n            File ending for path data.\n            \"numpy\" is the default when passing in X and y\n        D : int\n            Number of features.\n        N : int\n            Number of rows.\n        classification : bool\n            If True, problem is classification, else regression.\n        ordinal: bool\n            If True, problem is ordinal classification. Requires classification to be True.\n        balanced : bool\n            If true, each class is weighted equally in optimization, otherwise\n            weighted is done via support of each class. Requires classification to be True.\n        prerocess : str\n            'zscore' which refers to centering and normalizing data to unit variance or\n            'center' which only centers the data to 0 mean\n        n_to_estimate : int\n            Number of rows of data to estimate\n        MAXMEMGB : float\n            Maximum allowable size for a minibatch\n        set_params : bool\n            Whether or not to determine the statistics of the dataset\n        path_mappings : str\n            Used when streaming from disk\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        verbose : int\n            Controls the verbosity when fitting. Set to 0 for no printing\n            1 or higher for printing every verbose number of gradient steps.\n        device : str\n            'cpu' to run on CPU and 'cuda' to run on GPU. Runs much faster on GPU\n        n_classes : int\n            number of classes\n        \"\"\"\n\n        self.path_data = path_data\n        if self.path_data:\n            self.disk_size = os.path.getsize(path_data)\n        else:\n            assert X is not None, 'X must be specified if no path data'\n            self.disk_size = X.nbytes if not scipy.sparse.issparse(\n                X) else X.data.nbytes\n        assert data_format in constants.DataFormat.ALL_FORMATS, 'Format must in {0}.'.format(\n            \", \".join(constants.DataFormat.ALL_FORMATS))\n        self.format = data_format\n        self.classification = classification\n        self.ordinal = ordinal\n        self.balanced = balanced\n        self.MAXMEMGB = MAXMEMGB\n        self.preprocess = preprocess\n        self.set_params = set_params\n        self.verbose = verbose\n        self.n_classes = n_classes\n        self.device = device\n\n        self.path_data_stats = None\n\n        if D is None:\n            assert self.disk_size / BYTESPERGB <= self.MAXMEMGB, \\\n                'Cannot load data into memory. Supply D.'\n\n            if self.format == constants.DataFormat.SVM:\n                self.X, self.y = load_svmlight_file(path_data)\n            elif self.format == constants.DataFormat.NUMPY:\n                assert X is not None, 'X must be specified in numpy mode'\n                assert y is not None, 'y must be specified in numpy mode'\n                self.X = X\n                self.y = y\n                if self.n_classes is None:\n                    self.n_classes = np.unique(y).shape[0]\n                elif self.classification:\n                    assert self.n_classes >= np.unique(y).shape[0], \\\n                        'n_classes given must be greater than or equal to the number of classes in y'\n            else:\n                raise NotImplementedError\n            self.y = torch.as_tensor(self.y, dtype=torch.get_default_dtype())\n\n            self.N, self.D = self.X.shape\n\n            # assumes X was returned as a sparse array\n            self.storage_level = (constants.StorageLevel.SPARSE\n                                  if scipy.sparse.issparse(self.X)\n                                  else constants.StorageLevel.DENSE)\n\n        else:\n            assert N is not None, 'Supply N.'\n            self.N, self.D = N, D\n\n            # assume sparse matrix cannot fit into memory\n            self.storage_level = constants.StorageLevel.DISK\n\n        self.dense_size_gb = self.get_dense_size()\n\n        # check dense size\n        self.set_dense_X()\n\n        self.max_rows = int(self.MAXMEMGB * BYTESPERGB / BYTESPERREAL / self.D)\n        assert self.max_rows, \\\n            'Cannot fit one dense row into %d GB memory.' % self.MAXMEMGB\n        self.max_rows = self.max_batch_size()\n        sys.stdout.flush()\n\n        if n_to_estimate is None:\n            self.n_to_estimate = self.max_batch_size()\n        else:\n            assert n_to_estimate <= self.N, 'n_to_estimate must be <= N.'\n            self.n_to_estimate = n_to_estimate\n\n        # initialize disk loader\n        if self.storage_level == constants.StorageLevel.DISK and self.set_params:\n            if self.format == constants.DataFormat.SVM:\n                raise NotImplementedError(\n                    'Please use partial fit to train on datasets that do not fit in memory')\n            else:\n                raise NotImplementedError\n\n        # TODO: use a passed-in RNG here\n        self.ix_statistics = np.random.permutation(self.N)[:self.n_to_estimate]\n        self.n_features = self.D\n        if self.set_params:\n            if self.verbose:\n                print('Finding data statistics...', end='')\n                sys.stdout.flush()\n            Xmn, sv1, Xsd, ymn, ysd = self.compute_data_stats()\n            self.set_data_stats(Xmn, sv1, Xsd, ymn, ysd)\n            if self.verbose:\n                print()\n            self.set_return_raw(False)\n        else:\n            self.set_return_raw(True)\n\n        self.set_return_np(False)\n\n        # this needs to occur after setting preprocessing params\n        if (self.storage_level == constants.StorageLevel.DISK and\n                self.format == constants.DataFormat.SVM and self.set_params):\n            self.loader.batchsize = 1\n\n    def get_dense_size(self):\n        return self.N * self.D * BYTESPERREAL / BYTESPERGB\n\n    def set_dense_X(self):\n        if self.storage_level != constants.StorageLevel.DISK:\n            if self.dense_size_gb <= self.MAXMEMGB:\n                if self.storage_level == constants.StorageLevel.SPARSE:\n                    self.X = self.X.toarray()\n                self.X = torch.as_tensor(\n                    self.X, dtype=torch.get_default_dtype())\n                self.storage_level = constants.StorageLevel.DENSE\n\n    def set_return_np(self, boolean):\n\n        self.return_np = boolean\n\n    def set_return_raw(self, boolean):\n\n        self.return_raw = boolean\n\n    def save_data_stats(self, path_data_stats):\n        \"\"\"\n        Dumps dataset statistics to pickle file.\n        \"\"\"\n\n        data_stats = {\n            'Xmn': self.Xmn,\n            'sv1': self.sv1,\n            'Xsd': self.Xsd,\n            'ymn': self.ymn,\n            'ysd': self.ysd,\n            'ix_statistics': self.ix_statistics,\n        }\n        pickle.dump(data_stats, open(path_data_stats, 'wb'))\n\n    def load_data_stats(self, path_data_stats):\n\n        stats = pickle.load(open(path_data_stats, 'rb'))\n        self.path_data_stats = path_data_stats\n\n        self.set_data_stats(np.asarray(stats['Xmn']), stats['sv1'],\n                            stats['Xsd'], stats['ymn'], stats['ysd'])\n\n        if self.storage_level == constants.StorageLevel.DISK and hasattr(\n                self, 'path_mappings'):\n            if 'ix_statistics' in stats:\n                self.ix_statistics = stats['ix_statistics']\n            else:\n                self.ix_statistics = range(self.N)\n\n        self.set_return_raw(False)\n\n    def reset(self):\n        \"\"\"\n        Resets the dataloader. Only implemented for disk StorageLevel.\n        \"\"\"\n\n        if self.storage_level == constants.StorageLevel.DENSE:\n            pass\n        elif self.storage_level == constants.StorageLevel.SPARSE:\n            pass\n        elif self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                self.loader.reset()\n            else:\n                raise NotImplementedError\n\n    def todense(self):\n\n        assert hasattr(self, 'Xmn'), 'Set preprocess params first.'\n        assert len(self) <= self.max_batch_size(\n        ), 'N must be <= max_batch_size().'\n\n        with torch.no_grad():\n            dense, _ = self.split(range(len(self)))\n            Braw = self.return_raw\n            Bnp = self.return_np\n            self.set_return_raw(True)\n            self.set_return_np(True)\n            dense.X, dense.y = [], []\n\n            def f_Xy(X, y):\n                dense.X.append(X)\n                dense.y.append(y)\n            self.apply(f_Xy=f_Xy)\n            dense.X = dense.X[-1]\n            dense.y = dense.y[-1]\n            self.set_return_raw(Braw)\n            self.set_return_np(Bnp)\n            dense.storage_level = constants.StorageLevel.DENSE\n\n            return dense\n\n    def split(self, ix):\n\n        assert hasattr(self, 'Xmn'), 'Run set_preprocess_params() first.'\n\n        first = type(self)(\n            self.path_data,\n            self.format,\n            self.D,\n            N=len(ix),\n            classification=self.classification,\n            preprocess=self.preprocess,\n            n_to_estimate=None,\n            MAXMEMGB=self.MAXMEMGB,\n            set_params=False)\n        second = type(self)(\n            self.path_data,\n            self.format,\n            self.D,\n            N=self.N - len(ix),\n            classification=self.classification,\n            preprocess=self.preprocess,\n            n_to_estimate=None,\n            MAXMEMGB=self.MAXMEMGB,\n            set_params=False)\n\n        first.storage_level = self.storage_level\n        second.storage_level = self.storage_level\n\n        # copy preprocess params\n        if not self.classification:\n            first.ymn = self.ymn\n            second.ymn = self.ymn\n            first.ysd = self.ysd\n            second.ysd = self.ysd\n\n        first.Xmn = self.Xmn\n        second.Xmn = self.Xmn\n        first.sv1 = self.sv1\n        second.sv1 = self.sv1\n\n        if self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                first.Xsd = self.Xsd\n                second.Xsd = self.Xsd\n            else:\n                raise NotImplementedError\n\n        # initialize data structures\n        if self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                raise NotImplementedError\n            raise NotImplementedError\n        elif self.storage_level in [constants.StorageLevel.SPARSE,\n                                    constants.StorageLevel.DENSE]:\n            first.X, first.y = self.X[ix], self.y[ix]\n            ixsec = list(set(range(self.N)).difference(set(ix)))\n            second.X, second.y = self.X[ixsec], self.y[ixsec]\n\n        return first, second\n\n    @staticmethod\n    def sparse_std(X, X_mean):\n        \"\"\"\n        Calculate the column wise standard deviations of a sparse matrix.\n        \"\"\"\n        X_copy = X.copy()\n        X_copy.data **= 2  # square non zero elements\n        E_x_squared = np.array(X_copy.mean(axis=0)).ravel()\n        Xsd = np.sqrt(E_x_squared - X_mean**2)\n        return Xsd\n\n    def compute_data_stats(self):\n        \"\"\"\n        1. computes/estimates feature means\n        2. if preprocess == 'zscore', computes/estimates feature standard devs\n        3. if not classification, computes/estimates target mean/standard dev\n        4. estimates largest singular value of data matrix\n        \"\"\"\n        t = time.time()\n        X, y = self.X[self.ix_statistics], self.y[self.ix_statistics]\n        preprocess = self.preprocess\n        classification = self.classification\n\n        Xmn = (X.mean(dim=0)\n               if not scipy.sparse.issparse(X)\n               else np.array(X.mean(axis=0)).ravel())\n\n        if preprocess == constants.Preprocess.ZSCORE:\n            Xsd = (X.std(dim=0)\n                   if not scipy.sparse.issparse(X)\n                   else PrepareData.sparse_std(X, Xmn))\n            Xsd[Xsd == 0] = 1.\n        else:\n            Xsd = 1.\n\n        if preprocess is not None and preprocess:\n            if preprocess == constants.Preprocess.ZSCORE:\n                Xc = (X - Xmn) / Xsd\n            else:\n                Xc = X - Xmn\n        else:\n            Xc = X - Xmn\n\n        sv1 = scipy.sparse.linalg.svds(Xc / (\n            torch.sqrt(torch.prod(torch.as_tensor(y.size(), dtype=torch.get_default_dtype())))\n            if not scipy.sparse.issparse(X) else y.numpy().size),\n                                       k=1,\n                                       which='LM',\n                                       return_singular_vectors=False)\n        # avoid runaway sv1\n        sv1 = np.array([min(np.finfo(np.float32).max,\n                            sv1[0])])\n\n        if not classification:\n            ymn = y.mean()\n            ysd = y.std()\n        else:\n            # TODO: set these, for each class?\n            ymn = 0.\n            ysd = 1.\n        if self.verbose:\n            print(\" computing data statistics took: \", time.time() - t)\n\n        return Xmn, sv1, Xsd, ymn, ysd\n\n\n    def set_data_stats(self, Xmn, sv1, Xsd=1., ymn=0., ysd=1.):\n        \"\"\"\n        Saves dataset stats to self to be used for preprocessing.\n        \"\"\"\n\n        self.Xmn = torch.as_tensor(\n            Xmn, dtype=torch.get_default_dtype()).to(self.device)\n        self.sv1 = torch.as_tensor(\n            sv1, dtype=torch.get_default_dtype()).to(self.device)\n        self.Xsd = torch.as_tensor(\n            Xsd, dtype=torch.get_default_dtype()).to(self.device)\n        self.ymn = torch.as_tensor(\n            ymn, dtype=torch.get_default_dtype()).to(self.device)\n        self.ysd = torch.as_tensor(\n            ysd, dtype=torch.get_default_dtype()).to(self.device)\n\n\n    def apply_preprocess(self, X, y):\n        \"\"\"\n        Faster on gpu device, while dataloading takes up a large portion of the time.\n        \"\"\"\n\n        with torch.no_grad():\n            if not self.classification:\n                y = (y.reshape((-1, 1)) - self.ymn) / self.ysd\n            else:\n                y = y.reshape((-1, 1))\n            X = (X - self.Xmn) / self.sv1\n\n            if self.preprocess == constants.Preprocess.ZSCORE:\n                X /= self.Xsd\n\n            return X, y\n\n\n    def max_batch_size(self):\n        \"\"\"\n        Return the maximum batchsize for the dataset.\n        \"\"\"\n\n        return int(np.min([self.max_rows, self.N]))\n\n\n    def apply(self, ix_rows=None, ix_cols=None, f_Xy=None):\n\n        if f_Xy is None:\n            return\n\n        if ix_rows is None:\n            ix_rows = range(self.N)\n\n        if ix_cols is None:\n            ix_cols = range(self.n_features)\n\n        f_Xy((self.X[ix_rows, ix_cols]\n              if not self.storage_level == constants.StorageLevel.SPARSE\n              else self.X[ix_rows, ix_cols].toarray()), self.y[ix_rows])\n\n\n    def get_dense_data(self, ix_cols=None, ix_rows=None):\n\n        if ix_cols is None:\n            ix_cols = range(self.n_features)\n\n        X = [np.zeros((0, len(ix_cols)))]\n        y = [np.zeros((0, 1))]\n        Bnp = self.return_np\n\n        def f_Xy(Xb, yb, n):\n            X[-1] = np.concatenate((X[-1], Xb), axis=0)\n            y[-1] = np.concatenate((y[-1], yb), axis=0)\n        self.apply(f_Xy=f_Xy, ix_rows=ix_rows, ix_cols=ix_cols)\n        self.set_return_np(Bnp)\n\n        return X[-1], y[-1]\n\n\n    def __len__(self):\n\n        return self.N\n\n\n    def getXy(self, idx):\n\n        if self.storage_level == constants.StorageLevel.DENSE:\n            X, y = self.X[idx], self.y[idx]\n        elif self.storage_level == constants.StorageLevel.SPARSE:\n            # assume subset can fit into memory even if whole matrix cant\n            X, y = self.X[idx].toarray(), self.y[idx]\n        else:\n            raise NotImplementedError\n\n        return X, y\n\n\n    def __getitem__(self, idx):\n\n        with torch.no_grad():\n            X, y = self.getXy(idx)\n            X = X.toarray() if scipy.sparse.issparse(X) else X\n\n            X = torch.as_tensor(\n                X, dtype=torch.get_default_dtype()).to(self.device)\n            y = torch.as_tensor(\n                y, dtype=torch.get_default_dtype()).to(self.device)\n\n            if not self.return_raw:\n                X, y = self.apply_preprocess(X, y)\n\n            if self.classification and (\n                    self.n_classes is None or self.n_classes == 2):\n                y[y == 0] = -1\n\n            if self.return_np:\n                if constants.Device.CPU not in self.device:\n                    X = X.cpu()\n                    y = y.cpu()\n                X = X.numpy()\n                y = y.numpy()\n                return X, y\n\n            return X, y",
  "class ChunkDataLoader(DataLoader):\n    \"\"\"\n    DataLoader class used to more quickly load a batch of indices at once.\n    \"\"\"\n\n    def __iter__(self):\n        return _ChunkDataLoaderIter(self)",
  "class _ChunkDataLoaderIter:\n    \"\"\"\n    DataLoaderIter class used to more quickly load a batch of indices at once.\n    \"\"\"\n    def __init__(self, dataloader):\n        if dataloader.num_workers == 0:\n            self.iter = _SingleProcessDataLoaderIter(dataloader)\n        else:\n            self.iter = _MultiProcessingDataLoaderIter(dataloader)\n\n    def __next__(self):\n        # only chunk that is edited from base\n        if self.iter._num_workers == 0:  # same-process loading\n            indices = next(self.iter._sampler_iter)  # may raise StopIteration\n            if len(indices) > 1:\n                batch = self.iter._dataset[np.array(indices)]\n            else:\n                batch = self.iter._collate_fn([self.iter._dataset[i] for i in indices])\n\n            if self.iter._pin_memory:\n                batch = _utils.pin_memory.pin_memory_batch(batch)\n            return batch\n        else:\n            return next(self.iter)",
  "def __init__(self,\n                 path_data=None,\n                 data_format=constants.DataFormat.NUMPY,\n                 D=None, N=None,\n                 classification=True,\n                 ordinal=False,\n                 balanced=True,\n                 preprocess=None,\n                 n_to_estimate=None,\n                 MAXMEMGB=syssettings.MAXMEMGB,\n                 set_params=True,\n                 path_mappings=None,\n                 X=None,\n                 y=None,\n                 verbose=0,\n                 n_classes=None,\n                 device=constants.Device.CPU):\n        \"\"\"\n        Dataset class with helpful features and functions for being included in a dataloader\n        and managing memory usage.\n        can read following formats:\n            svm:        svm light format (sklearn.datasets.load_svmlight_file)\n            numpy:      Pass X and y as numpy or sparse arrays\n\n        assumes\n            1. if classification, y is in {-1, 1} or continuous and 0 indexed\n            2. y can fit into memory\n            3. consecutive calls to __getitem__() have consecutive idx values\n\n        notes:\n            1. this implementation is not careful wrt/ precise memory reqts. for\n            example, being able to store one dense row in memory is necessary,\n            but not sufficient.\n            2. for y with 4.2 billion elements, 31.3 GB of memory is  necessary\n            @ 8 bytes/scalar. Use partial fit to avoid loading the entire dataset\n            at once\n            3. disk_size always refer to size of complete data file, even after\n            a split().\n\n\n        Parameters\n        ----------\n        path_data : str\n            Path to load data from\n        data_format : str\n            File ending for path data.\n            \"numpy\" is the default when passing in X and y\n        D : int\n            Number of features.\n        N : int\n            Number of rows.\n        classification : bool\n            If True, problem is classification, else regression.\n        ordinal: bool\n            If True, problem is ordinal classification. Requires classification to be True.\n        balanced : bool\n            If true, each class is weighted equally in optimization, otherwise\n            weighted is done via support of each class. Requires classification to be True.\n        prerocess : str\n            'zscore' which refers to centering and normalizing data to unit variance or\n            'center' which only centers the data to 0 mean\n        n_to_estimate : int\n            Number of rows of data to estimate\n        MAXMEMGB : float\n            Maximum allowable size for a minibatch\n        set_params : bool\n            Whether or not to determine the statistics of the dataset\n        path_mappings : str\n            Used when streaming from disk\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        verbose : int\n            Controls the verbosity when fitting. Set to 0 for no printing\n            1 or higher for printing every verbose number of gradient steps.\n        device : str\n            'cpu' to run on CPU and 'cuda' to run on GPU. Runs much faster on GPU\n        n_classes : int\n            number of classes\n        \"\"\"\n\n        self.path_data = path_data\n        if self.path_data:\n            self.disk_size = os.path.getsize(path_data)\n        else:\n            assert X is not None, 'X must be specified if no path data'\n            self.disk_size = X.nbytes if not scipy.sparse.issparse(\n                X) else X.data.nbytes\n        assert data_format in constants.DataFormat.ALL_FORMATS, 'Format must in {0}.'.format(\n            \", \".join(constants.DataFormat.ALL_FORMATS))\n        self.format = data_format\n        self.classification = classification\n        self.ordinal = ordinal\n        self.balanced = balanced\n        self.MAXMEMGB = MAXMEMGB\n        self.preprocess = preprocess\n        self.set_params = set_params\n        self.verbose = verbose\n        self.n_classes = n_classes\n        self.device = device\n\n        self.path_data_stats = None\n\n        if D is None:\n            assert self.disk_size / BYTESPERGB <= self.MAXMEMGB, \\\n                'Cannot load data into memory. Supply D.'\n\n            if self.format == constants.DataFormat.SVM:\n                self.X, self.y = load_svmlight_file(path_data)\n            elif self.format == constants.DataFormat.NUMPY:\n                assert X is not None, 'X must be specified in numpy mode'\n                assert y is not None, 'y must be specified in numpy mode'\n                self.X = X\n                self.y = y\n                if self.n_classes is None:\n                    self.n_classes = np.unique(y).shape[0]\n                elif self.classification:\n                    assert self.n_classes >= np.unique(y).shape[0], \\\n                        'n_classes given must be greater than or equal to the number of classes in y'\n            else:\n                raise NotImplementedError\n            self.y = torch.as_tensor(self.y, dtype=torch.get_default_dtype())\n\n            self.N, self.D = self.X.shape\n\n            # assumes X was returned as a sparse array\n            self.storage_level = (constants.StorageLevel.SPARSE\n                                  if scipy.sparse.issparse(self.X)\n                                  else constants.StorageLevel.DENSE)\n\n        else:\n            assert N is not None, 'Supply N.'\n            self.N, self.D = N, D\n\n            # assume sparse matrix cannot fit into memory\n            self.storage_level = constants.StorageLevel.DISK\n\n        self.dense_size_gb = self.get_dense_size()\n\n        # check dense size\n        self.set_dense_X()\n\n        self.max_rows = int(self.MAXMEMGB * BYTESPERGB / BYTESPERREAL / self.D)\n        assert self.max_rows, \\\n            'Cannot fit one dense row into %d GB memory.' % self.MAXMEMGB\n        self.max_rows = self.max_batch_size()\n        sys.stdout.flush()\n\n        if n_to_estimate is None:\n            self.n_to_estimate = self.max_batch_size()\n        else:\n            assert n_to_estimate <= self.N, 'n_to_estimate must be <= N.'\n            self.n_to_estimate = n_to_estimate\n\n        # initialize disk loader\n        if self.storage_level == constants.StorageLevel.DISK and self.set_params:\n            if self.format == constants.DataFormat.SVM:\n                raise NotImplementedError(\n                    'Please use partial fit to train on datasets that do not fit in memory')\n            else:\n                raise NotImplementedError\n\n        # TODO: use a passed-in RNG here\n        self.ix_statistics = np.random.permutation(self.N)[:self.n_to_estimate]\n        self.n_features = self.D\n        if self.set_params:\n            if self.verbose:\n                print('Finding data statistics...', end='')\n                sys.stdout.flush()\n            Xmn, sv1, Xsd, ymn, ysd = self.compute_data_stats()\n            self.set_data_stats(Xmn, sv1, Xsd, ymn, ysd)\n            if self.verbose:\n                print()\n            self.set_return_raw(False)\n        else:\n            self.set_return_raw(True)\n\n        self.set_return_np(False)\n\n        # this needs to occur after setting preprocessing params\n        if (self.storage_level == constants.StorageLevel.DISK and\n                self.format == constants.DataFormat.SVM and self.set_params):\n            self.loader.batchsize = 1",
  "def get_dense_size(self):\n        return self.N * self.D * BYTESPERREAL / BYTESPERGB",
  "def set_dense_X(self):\n        if self.storage_level != constants.StorageLevel.DISK:\n            if self.dense_size_gb <= self.MAXMEMGB:\n                if self.storage_level == constants.StorageLevel.SPARSE:\n                    self.X = self.X.toarray()\n                self.X = torch.as_tensor(\n                    self.X, dtype=torch.get_default_dtype())\n                self.storage_level = constants.StorageLevel.DENSE",
  "def set_return_np(self, boolean):\n\n        self.return_np = boolean",
  "def set_return_raw(self, boolean):\n\n        self.return_raw = boolean",
  "def save_data_stats(self, path_data_stats):\n        \"\"\"\n        Dumps dataset statistics to pickle file.\n        \"\"\"\n\n        data_stats = {\n            'Xmn': self.Xmn,\n            'sv1': self.sv1,\n            'Xsd': self.Xsd,\n            'ymn': self.ymn,\n            'ysd': self.ysd,\n            'ix_statistics': self.ix_statistics,\n        }\n        pickle.dump(data_stats, open(path_data_stats, 'wb'))",
  "def load_data_stats(self, path_data_stats):\n\n        stats = pickle.load(open(path_data_stats, 'rb'))\n        self.path_data_stats = path_data_stats\n\n        self.set_data_stats(np.asarray(stats['Xmn']), stats['sv1'],\n                            stats['Xsd'], stats['ymn'], stats['ysd'])\n\n        if self.storage_level == constants.StorageLevel.DISK and hasattr(\n                self, 'path_mappings'):\n            if 'ix_statistics' in stats:\n                self.ix_statistics = stats['ix_statistics']\n            else:\n                self.ix_statistics = range(self.N)\n\n        self.set_return_raw(False)",
  "def reset(self):\n        \"\"\"\n        Resets the dataloader. Only implemented for disk StorageLevel.\n        \"\"\"\n\n        if self.storage_level == constants.StorageLevel.DENSE:\n            pass\n        elif self.storage_level == constants.StorageLevel.SPARSE:\n            pass\n        elif self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                self.loader.reset()\n            else:\n                raise NotImplementedError",
  "def todense(self):\n\n        assert hasattr(self, 'Xmn'), 'Set preprocess params first.'\n        assert len(self) <= self.max_batch_size(\n        ), 'N must be <= max_batch_size().'\n\n        with torch.no_grad():\n            dense, _ = self.split(range(len(self)))\n            Braw = self.return_raw\n            Bnp = self.return_np\n            self.set_return_raw(True)\n            self.set_return_np(True)\n            dense.X, dense.y = [], []\n\n            def f_Xy(X, y):\n                dense.X.append(X)\n                dense.y.append(y)\n            self.apply(f_Xy=f_Xy)\n            dense.X = dense.X[-1]\n            dense.y = dense.y[-1]\n            self.set_return_raw(Braw)\n            self.set_return_np(Bnp)\n            dense.storage_level = constants.StorageLevel.DENSE\n\n            return dense",
  "def split(self, ix):\n\n        assert hasattr(self, 'Xmn'), 'Run set_preprocess_params() first.'\n\n        first = type(self)(\n            self.path_data,\n            self.format,\n            self.D,\n            N=len(ix),\n            classification=self.classification,\n            preprocess=self.preprocess,\n            n_to_estimate=None,\n            MAXMEMGB=self.MAXMEMGB,\n            set_params=False)\n        second = type(self)(\n            self.path_data,\n            self.format,\n            self.D,\n            N=self.N - len(ix),\n            classification=self.classification,\n            preprocess=self.preprocess,\n            n_to_estimate=None,\n            MAXMEMGB=self.MAXMEMGB,\n            set_params=False)\n\n        first.storage_level = self.storage_level\n        second.storage_level = self.storage_level\n\n        # copy preprocess params\n        if not self.classification:\n            first.ymn = self.ymn\n            second.ymn = self.ymn\n            first.ysd = self.ysd\n            second.ysd = self.ysd\n\n        first.Xmn = self.Xmn\n        second.Xmn = self.Xmn\n        first.sv1 = self.sv1\n        second.sv1 = self.sv1\n\n        if self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                first.Xsd = self.Xsd\n                second.Xsd = self.Xsd\n            else:\n                raise NotImplementedError\n\n        # initialize data structures\n        if self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                raise NotImplementedError\n            raise NotImplementedError\n        elif self.storage_level in [constants.StorageLevel.SPARSE,\n                                    constants.StorageLevel.DENSE]:\n            first.X, first.y = self.X[ix], self.y[ix]\n            ixsec = list(set(range(self.N)).difference(set(ix)))\n            second.X, second.y = self.X[ixsec], self.y[ixsec]\n\n        return first, second",
  "def sparse_std(X, X_mean):\n        \"\"\"\n        Calculate the column wise standard deviations of a sparse matrix.\n        \"\"\"\n        X_copy = X.copy()\n        X_copy.data **= 2  # square non zero elements\n        E_x_squared = np.array(X_copy.mean(axis=0)).ravel()\n        Xsd = np.sqrt(E_x_squared - X_mean**2)\n        return Xsd",
  "def compute_data_stats(self):\n        \"\"\"\n        1. computes/estimates feature means\n        2. if preprocess == 'zscore', computes/estimates feature standard devs\n        3. if not classification, computes/estimates target mean/standard dev\n        4. estimates largest singular value of data matrix\n        \"\"\"\n        t = time.time()\n        X, y = self.X[self.ix_statistics], self.y[self.ix_statistics]\n        preprocess = self.preprocess\n        classification = self.classification\n\n        Xmn = (X.mean(dim=0)\n               if not scipy.sparse.issparse(X)\n               else np.array(X.mean(axis=0)).ravel())\n\n        if preprocess == constants.Preprocess.ZSCORE:\n            Xsd = (X.std(dim=0)\n                   if not scipy.sparse.issparse(X)\n                   else PrepareData.sparse_std(X, Xmn))\n            Xsd[Xsd == 0] = 1.\n        else:\n            Xsd = 1.\n\n        if preprocess is not None and preprocess:\n            if preprocess == constants.Preprocess.ZSCORE:\n                Xc = (X - Xmn) / Xsd\n            else:\n                Xc = X - Xmn\n        else:\n            Xc = X - Xmn\n\n        sv1 = scipy.sparse.linalg.svds(Xc / (\n            torch.sqrt(torch.prod(torch.as_tensor(y.size(), dtype=torch.get_default_dtype())))\n            if not scipy.sparse.issparse(X) else y.numpy().size),\n                                       k=1,\n                                       which='LM',\n                                       return_singular_vectors=False)\n        # avoid runaway sv1\n        sv1 = np.array([min(np.finfo(np.float32).max,\n                            sv1[0])])\n\n        if not classification:\n            ymn = y.mean()\n            ysd = y.std()\n        else:\n            # TODO: set these, for each class?\n            ymn = 0.\n            ysd = 1.\n        if self.verbose:\n            print(\" computing data statistics took: \", time.time() - t)\n\n        return Xmn, sv1, Xsd, ymn, ysd",
  "def set_data_stats(self, Xmn, sv1, Xsd=1., ymn=0., ysd=1.):\n        \"\"\"\n        Saves dataset stats to self to be used for preprocessing.\n        \"\"\"\n\n        self.Xmn = torch.as_tensor(\n            Xmn, dtype=torch.get_default_dtype()).to(self.device)\n        self.sv1 = torch.as_tensor(\n            sv1, dtype=torch.get_default_dtype()).to(self.device)\n        self.Xsd = torch.as_tensor(\n            Xsd, dtype=torch.get_default_dtype()).to(self.device)\n        self.ymn = torch.as_tensor(\n            ymn, dtype=torch.get_default_dtype()).to(self.device)\n        self.ysd = torch.as_tensor(\n            ysd, dtype=torch.get_default_dtype()).to(self.device)",
  "def apply_preprocess(self, X, y):\n        \"\"\"\n        Faster on gpu device, while dataloading takes up a large portion of the time.\n        \"\"\"\n\n        with torch.no_grad():\n            if not self.classification:\n                y = (y.reshape((-1, 1)) - self.ymn) / self.ysd\n            else:\n                y = y.reshape((-1, 1))\n            X = (X - self.Xmn) / self.sv1\n\n            if self.preprocess == constants.Preprocess.ZSCORE:\n                X /= self.Xsd\n\n            return X, y",
  "def max_batch_size(self):\n        \"\"\"\n        Return the maximum batchsize for the dataset.\n        \"\"\"\n\n        return int(np.min([self.max_rows, self.N]))",
  "def apply(self, ix_rows=None, ix_cols=None, f_Xy=None):\n\n        if f_Xy is None:\n            return\n\n        if ix_rows is None:\n            ix_rows = range(self.N)\n\n        if ix_cols is None:\n            ix_cols = range(self.n_features)\n\n        f_Xy((self.X[ix_rows, ix_cols]\n              if not self.storage_level == constants.StorageLevel.SPARSE\n              else self.X[ix_rows, ix_cols].toarray()), self.y[ix_rows])",
  "def get_dense_data(self, ix_cols=None, ix_rows=None):\n\n        if ix_cols is None:\n            ix_cols = range(self.n_features)\n\n        X = [np.zeros((0, len(ix_cols)))]\n        y = [np.zeros((0, 1))]\n        Bnp = self.return_np\n\n        def f_Xy(Xb, yb, n):\n            X[-1] = np.concatenate((X[-1], Xb), axis=0)\n            y[-1] = np.concatenate((y[-1], yb), axis=0)\n        self.apply(f_Xy=f_Xy, ix_rows=ix_rows, ix_cols=ix_cols)\n        self.set_return_np(Bnp)\n\n        return X[-1], y[-1]",
  "def __len__(self):\n\n        return self.N",
  "def getXy(self, idx):\n\n        if self.storage_level == constants.StorageLevel.DENSE:\n            X, y = self.X[idx], self.y[idx]\n        elif self.storage_level == constants.StorageLevel.SPARSE:\n            # assume subset can fit into memory even if whole matrix cant\n            X, y = self.X[idx].toarray(), self.y[idx]\n        else:\n            raise NotImplementedError\n\n        return X, y",
  "def __getitem__(self, idx):\n\n        with torch.no_grad():\n            X, y = self.getXy(idx)\n            X = X.toarray() if scipy.sparse.issparse(X) else X\n\n            X = torch.as_tensor(\n                X, dtype=torch.get_default_dtype()).to(self.device)\n            y = torch.as_tensor(\n                y, dtype=torch.get_default_dtype()).to(self.device)\n\n            if not self.return_raw:\n                X, y = self.apply_preprocess(X, y)\n\n            if self.classification and (\n                    self.n_classes is None or self.n_classes == 2):\n                y[y == 0] = -1\n\n            if self.return_np:\n                if constants.Device.CPU not in self.device:\n                    X = X.cpu()\n                    y = y.cpu()\n                X = X.numpy()\n                y = y.numpy()\n                return X, y\n\n            return X, y",
  "def __iter__(self):\n        return _ChunkDataLoaderIter(self)",
  "def __init__(self, dataloader):\n        if dataloader.num_workers == 0:\n            self.iter = _SingleProcessDataLoaderIter(dataloader)\n        else:\n            self.iter = _MultiProcessingDataLoaderIter(dataloader)",
  "def __next__(self):\n        # only chunk that is edited from base\n        if self.iter._num_workers == 0:  # same-process loading\n            indices = next(self.iter._sampler_iter)  # may raise StopIteration\n            if len(indices) > 1:\n                batch = self.iter._dataset[np.array(indices)]\n            else:\n                batch = self.iter._collate_fn([self.iter._dataset[i] for i in indices])\n\n            if self.iter._pin_memory:\n                batch = _utils.pin_memory.pin_memory_batch(batch)\n            return batch\n        else:\n            return next(self.iter)",
  "def f_Xy(Xb, yb, n):\n            X[-1] = np.concatenate((X[-1], Xb), axis=0)\n            y[-1] = np.concatenate((y[-1], yb), axis=0)",
  "def f_Xy(X, y):\n                dense.X.append(X)\n                dense.y.append(y)",
  "class FeatureGradientSelector(FeatureSelector, BaseEstimator, SelectorMixin):\n    def __init__(self,\n                 order=4,\n                 penalty=1,\n                 n_features=None,\n                 max_features=None,\n                 learning_rate=1e-1,\n                 init='zero',\n                 n_epochs=1,\n                 shuffle=True,\n                 batch_size=1000,\n                 target_batch_size=1000,\n                 max_time=np.inf,\n                 classification=True,\n                 ordinal=False,\n                 balanced=True,\n                 preprocess='zscore',\n                 soft_grouping=False,\n                 verbose=0,\n                 device='cpu'):\n        \"\"\"\n            FeatureGradientSelector is a class that selects features for a machine\n            learning model using a gradient based search.\n\n            Parameters\n            ----------\n            order : int\n                What order of interactions to include. Higher orders\n                may be more accurate but increase the run time. 12 is the maximum allowed order.\n            penatly : int\n                Constant that multiplies the regularization term.\n            n_features: int\n                If None, will automatically choose number of features based on search.\n                Otherwise, number of top features to select.\n            max_features : int\n                If not None, will use the 'elbow method' to determine the number of features\n                with max_features as the upper limit.\n            learning_rate : float\n            init : str\n                How to initialize the vector of scores. 'zero' is the default.\n                Options: {'zero', 'on', 'off', 'onhigh', 'offhigh', 'sklearn'}\n            n_epochs : int\n                number of epochs to run\n            shuffle : bool\n                Shuffle \"rows\" prior to an epoch.\n            batch_size : int\n                Nnumber of \"rows\" to process at a time\n            target_batch_size : int\n                Number of \"rows\" to accumulate gradients over.\n                Useful when many rows will not fit into memory but are needed for accurate estimation.\n            classification : bool\n                If True, problem is classification, else regression.\n            ordinal : bool\n                If True, problem is ordinal classification. Requires classification to be True.\n            balanced : bool\n                If true, each class is weighted equally in optimization, otherwise\n                weighted is done via support of each class. Requires classification to be True.\n            prerocess : str\n                'zscore' which refers to centering and normalizing data to unit variance or\n                'center' which only centers the data to 0 mean\n            soft_grouping : bool\n                if True, groups represent features that come from the same source.\n                Used to encourage sparsity of groups and features within groups.\n            verbose : int\n                Controls the verbosity when fitting. Set to 0 for no printing\n                1 or higher for printing every verbose number of gradient steps.\n            device : str\n                'cpu' to run on CPU and 'cuda' to run on GPU. Runs much faster on GPU\n        \"\"\"\n        assert order <= 12 and order >= 1, 'order must be an integer between 1 and 12, inclusive'\n        assert n_features is None or max_features is None, \\\n            'only specify one of n_features and max_features at a time'\n\n        self.order = order\n        self.penalty = penalty\n        self.n_features = n_features\n        self.max_features = max_features\n        self.learning_rate = learning_rate\n        self.init = init\n        self.n_epochs = n_epochs\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.target_batch_size = target_batch_size\n        self.max_time = max_time\n        self.dftol_stop = -1\n        self.freltol_stop = -1\n        self.classification = classification\n        self.ordinal = ordinal\n        self.balanced = balanced\n        self.preprocess = preprocess\n        self.soft_grouping = soft_grouping\n        self.verbose = verbose\n        self.device = device\n\n        self.model_ = None\n        self.scores_ = None\n        self._prev_checkpoint = None\n        self._data_train = None\n\n    def partial_fit(self, X, y,\n                    n_classes=None,\n                    groups=None):\n        \"\"\"\n        Select Features via a gradient based search on (X, y) on the given samples.\n        Can be called repeatedly with different X and y to handle streaming datasets.\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y :  array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        n_classes : int\n            Number of classes\n            Classes across all calls to partial_fit.\n            Can be obtained by via `np.unique(y_all).shape[0]`, where y_all is the\n            target vector of the entire dataset.\n            This argument is expected for the first call to partial_fit,\n            otherwise will assume all classes are present in the batch of y given.\n            It will be ignored in the subsequent calls.\n            Note that y doesn't need to contain all labels in `classes`.\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n            This argument is expected for the first call to partial_fit,\n            otherwise will assume all classes are present in the batch of y given.\n            It will be ignored in the subsequent calls.\n        \"\"\"\n        try:\n            self._partial_fit(X, y, n_classes=n_classes, groups=groups)\n        except constants.NanError:\n            if hasattr(self, '_prev_checkpoint'):\n                # if it's already done some batches successfully just ignore it\n                print('failed fitting this batch, loss was nan')\n            else:\n                # if this is the first batch, reset and try with doubles\n                if self.verbose:\n                    print('Loss was nan, trying with Doubles')\n                self._reset()\n                torch.set_default_tensor_type(torch.DoubleTensor)\n                self._partial_fit(X, y, n_classes=n_classes, groups=groups)\n\n        return self\n\n    def _partial_fit(self, X, y, n_classes=None, groups=None):\n        \"\"\"\n        Private function for partial_fit to enable trying floats before doubles.\n        \"\"\"\n        # pass in X and y in chunks\n        if hasattr(self, '_data_train'):\n            # just overwrite the X and y from the new chunk but make them tensors\n            # keep dataset stats from previous\n            self._data_train.X = X.values if isinstance(X, pd.DataFrame) else X\n            self._data_train.N, self._data_train.D = self._data_train.X.shape\n            self._data_train.dense_size_gb = self._data_train.get_dense_size()\n            self._data_train.set_dense_X()\n\n            self._data_train.y = y.values if isinstance(y, pd.Series) else y\n            self._data_train.y = torch.as_tensor(\n                y, dtype=torch.get_default_dtype())\n        else:\n            data_train = self._prepare_data(X, y, n_classes=n_classes)\n            self._data_train = data_train\n\n        batch_size, _, accum_steps, max_iter = self._set_batch_size(\n            self._data_train)\n\n        rng = None  # not used\n        debug = 0  # {0,1} print messages and do other stuff?\n        dn_logs = None  # tensorboard logs; only specify if debug=1\n        path_save = None  # intermediate models saves; only specify if debug=1\n        m, solver = _train(self._data_train,\n                           batch_size,\n                           self.order,\n                           self.penalty,\n                           rng,\n                           self.learning_rate,\n                           debug,\n                           max_iter,\n                           self.max_time,\n                           self.init,\n                           self.dftol_stop,\n                           self.freltol_stop,\n                           dn_logs,\n                           accum_steps,\n                           path_save,\n                           self.shuffle,\n                           device=self.device,\n                           verbose=self.verbose,\n                           prev_checkpoint=self._prev_checkpoint if hasattr(\n                               self, '_prev_checkpoint') else None,\n                           groups=groups if not self.soft_grouping else None,\n                           soft_groups=groups if self.soft_grouping else None)\n\n        self._prev_checkpoint = m\n        self._process_results(m, solver, X, groups=groups)\n        return self\n\n    def fit(self, X, y,\n            groups=None):\n        \"\"\"\n        Select Features via a gradient based search on (X, y).\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        \"\"\"\n        try:\n            self._fit(X, y, groups=groups)\n        except constants.NanError:\n            if self.verbose:\n                print('Loss was nan, trying with Doubles')\n            torch.set_default_tensor_type(torch.DoubleTensor)\n            self._fit(X, y, groups=groups)\n        return self\n\n    def get_selected_features(self):\n        return self.selected_features_\n\n    def _prepare_data(self, X, y, n_classes=None):\n        \"\"\"\n        Returns a PrepareData object.\n        \"\"\"\n        return PrepareData(X=X.values if isinstance(X, pd.DataFrame) else X,\n                           y=y.values if isinstance(y, pd.Series) else y,\n                           data_format=constants.DataFormat.NUMPY,\n                           classification=int(self.classification),\n                           ordinal=self.ordinal,\n                           balanced=self.balanced,\n                           preprocess=self.preprocess,\n                           verbose=self.verbose,\n                           device=self.device,\n                           n_classes=n_classes)\n\n    def _fit(self, X, y, groups=None):\n        \"\"\"\n        Private function for fit to enable trying floats before doubles.\n        \"\"\"\n        data_train = self._prepare_data(X, y)\n\n        batch_size, _, accum_steps, max_iter = self._set_batch_size(\n            data_train)\n\n        rng = None  # not used\n        debug = 0  # {0,1} print messages and log to tensorboard\n        dn_logs = None  # tensorboard logs; only specify if debug=1\n        path_save = None  # intermediate models saves; only specify if debug=1\n        m, solver = _train(data_train,\n                           batch_size,\n                           self.order,\n                           self.penalty,\n                           rng,\n                           self.learning_rate,\n                           debug,\n                           max_iter,\n                           self.max_time,\n                           self.init,\n                           self.dftol_stop,\n                           self.freltol_stop,\n                           dn_logs,\n                           accum_steps,\n                           path_save,\n                           self.shuffle,\n                           device=self.device,\n                           verbose=self.verbose,\n                           groups=groups if not self.soft_grouping else None,\n                           soft_groups=groups if self.soft_grouping else None)\n\n        self._process_results(m, solver, X, groups=groups)\n        return self\n\n    def _process_torch_scores(self, scores):\n        \"\"\"\n        Convert scores into flat numpy arrays.\n        \"\"\"\n        if constants.Device.CUDA in scores.device.type:\n            scores = scores.cpu()\n        return scores.numpy().ravel()\n\n    def _set_batch_size(self, data_train):\n        \"\"\"\n        Ensures that batch_size is less than the number of rows.\n        \"\"\"\n        batch_size = min(self.batch_size, data_train.N)\n        target_batch_size = min(max(\n            self.batch_size, self.target_batch_size), data_train.N)\n        accum_steps = max(int(np.ceil(target_batch_size / self.batch_size)), 1)\n        max_iter = self.n_epochs * (data_train.N // batch_size)\n        return batch_size, target_batch_size, accum_steps, max_iter\n\n    def _process_results(self, m, solver, X, groups=None):\n        \"\"\"\n        Process the results of a run into something suitable for transform().\n        \"\"\"\n        self.scores_ = self._process_torch_scores(\n            torch.sigmoid(m[constants.Checkpoint.MODEL]['x'] * 2))\n        if self.max_features:\n            self.max_features = min([self.max_features, self.scores_.shape[0]])\n            n_features = self._recommend_number_features(solver)\n            self.set_n_features(n_features, groups=groups)\n        elif self.n_features:\n            self.set_n_features(self.n_features, groups=groups)\n        else:\n            self.selected_features_ = m['feats']\n\n        # subtract elapsed time from max_time\n        self.max_time -= m['t']\n\n        self.model_ = m\n\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Returns selected features from X.\n\n        Paramters\n        ---------\n        X: array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        \"\"\"\n\n        self._get_support_mask()\n        if self.selected_features_.shape[0] == 0:\n            raise ValueError(\n                'No Features selected, consider lowering the penalty or specifying n_features')\n        return (X.iloc[:, self.selected_features_]\n                if isinstance(X, pd.DataFrame)\n                else X[:, self.selected_features_])\n\n    def get_support(self, indices=False):\n        \"\"\"\n        Get a mask, or integer index, of the features selected.\n\n        Parameters\n        ----------\n        indices : bool\n            Default False\n            If True, the return value will be an array of integers, rather than a boolean mask.\n\n        Returns\n        -------\n        list :\n            returns support: An index that selects the retained features from a feature vector.\n            If indices is False, this is a boolean array of shape [# input features],\n            in which an element is True iff its corresponding feature is selected for retention.\n            If indices is True, this is an integer array of shape [# output features] whose values\n            are indices into the input feature vector.\n        \"\"\"\n        self._get_support_mask()\n        if indices:\n            return self.selected_features_\n\n        mask = np.zeros_like(self.scores_, dtype=bool)\n        # pylint: disable=E1137\n        mask[self.selected_features_] = True\n        return mask\n\n    def inverse_transform(self, X):\n        \"\"\"\n        Returns transformed X to the original number of column.\n        This operation is lossy and all columns not in the transformed data\n        will be returned as columns of 0s.\n        \"\"\"\n        self._get_support_mask()\n        X_new = np.zeros((X.shape[0], self.scores_.shape[0]))\n        X_new[self.selected_features_] = X\n        return X_new\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n        \"\"\"\n        params = self.__dict__\n        params = {key: val for (key, val) in params.items()\n                  if not key.endswith('_')}\n        return params\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n        \"\"\"\n        for param in params:\n            if hasattr(self, param):\n                setattr(self, param, params[param])\n        return self\n\n    def fit_transform(self, X, y):\n        \"\"\"\n        Select features and then return X with the selected features.\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        \"\"\"\n        self.fit(X, y)\n        return self.transform(X)\n\n    def _get_support_mask(self):\n        \"\"\"\n        Check if it is fitted.\n        \"\"\"\n        check_is_fitted(self, 'scores_')\n\n    def _generate_scores(self, solver, xsub, ysub, step_size, feature_order):\n        \"\"\"\n        Generate forward passes to determine the number of features when max_features is set.\n        \"\"\"\n        scores = []\n        for i in np.arange(1, self.max_features + 1, step_size):\n            # optimization possible since xsub is growing?\n            i = int(np.ceil(i))\n            # pylint: disable=E1102\n            score = solver.f_train(torch.tensor(np.ones(i),\n                                                dtype=torch.get_default_dtype()\n                                                ).unsqueeze(1).to(self.device),\n                                   xsub[:, feature_order[:i]],\n                                   ysub)\n            if constants.Device.CUDA in score.device.type:\n                score = score.cpu()\n            # score.numpy()[0][0]\n            scores.append(score)\n        return scores\n\n    def set_n_features(self, n, groups=None):\n        \"\"\"\n        Set the number of features to return after fitting.\n        \"\"\"\n        self._get_support_mask()\n        self.n_features = n\n        return self._set_top_features(groups=groups)\n\n    def _set_top_features(self, groups=None):\n        \"\"\"\n        Set the selected features after a run.\n\n        With groups, ensures that if any member of a group is selected, all members are selected\n        \"\"\"\n        self._get_support_mask()\n        assert self.n_features <= self.scores_.shape[0], \\\n            'n_features must be less than or equal to the number of columns in X'\n        # pylint: disable=E1130\n        self.selected_features_ = np.argpartition(\n            self.scores_, -self.n_features)[-self.n_features:]\n        if groups is not None and not self.soft_grouping:\n            selected_feature_set = set(self.selected_features_.tolist())\n            for _ in np.unique(groups):\n                group_members = np.where(groups == groups)[0].tolist()\n                if selected_feature_set.intersection(group_members):\n                    selected_feature_set.update(group_members)\n            self.selected_features_ = np.array(list(selected_feature_set))\n        self.selected_features_ = np.sort(self.selected_features_)\n        return self\n\n    def set_top_percentile(self, percentile, groups=None):\n        \"\"\"\n        Set the percentile of features to return after fitting.\n        \"\"\"\n        self._get_support_mask()\n        assert percentile <= 1 and percentile >= 0, \\\n            'percentile must between 0 and 1 inclusive'\n        self.n_features = int(self.scores_.shape[0] * percentile)\n        return self._set_top_features(groups=groups)\n\n    def _recommend_number_features(self, solver, max_time=None):\n        \"\"\"\n        Get the recommended number of features by doing forward passes when max_features is set.\n        \"\"\"\n        max_time = max_time if max_time else self.max_time\n        if max_time < 0:\n            max_time = 60  # allow 1 minute extra if we already spent max_time\n        MAX_FORWARD_PASS = 200\n        MAX_FULL_BATCHES = 3  # the forward passes can take longer than the fitting\n        # if we allow a full epoch of data to be included. By only doing 3 full batches at most\n        # we get enough accuracy without increasing the time too much. This\n        # constant may not be optimal\n        accum_steps = solver.accum_steps\n        step_size = max(self.max_features / MAX_FORWARD_PASS, 1)\n        # pylint: disable=E1130\n        feature_order = np.argsort(-self.scores_)  # note the negative\n        t = time.time()\n\n        dataloader_iterator = iter(solver.ds_train)\n        full_scores = []\n        # keep_going = True\n        with torch.no_grad():\n            # might want to only consider a batch valid if there are at least\n            # two classes\n            for _ in range(accum_steps * MAX_FULL_BATCHES):\n                scores = []\n                try:\n                    xsub, ysub = next(dataloader_iterator)\n                except StopIteration:\n                    # done with epoch, don't do more than one epoch\n                    break\n                except Exception as e:\n                    print(e)\n                    break\n                if max_time and time.time() - t > max_time:\n                    if self.verbose:\n                        print(\n                            \"Stoppinn forward passes because they reached max_time: \",\n                            max_time)\n                    if not full_scores:\n                        # no forward passes worked, return half of max_features\n                        return self.max_features // 2\n                    break\n                if solver.multiclass:\n                    for target_class in range(solver.n_classes):\n                        ysub_binary = solver.transform_y_into_binary(\n                            ysub, target_class)\n                        scaling_value = solver._get_scaling_value(\n                            ysub, target_class)\n                        if not solver._skip_y_forward(ysub_binary):\n                            scores = self._generate_scores(\n                                solver, xsub, ysub_binary, step_size, feature_order)\n                            # one row will represent one class that is present in the data\n                            # all classes are weighted equally\n                            full_scores.append(\n                                [score * scaling_value for score in scores])\n                else:\n                    if not solver._skip_y_forward(ysub):\n                        scores = self._generate_scores(\n                            solver, xsub, ysub, step_size, feature_order)\n                        full_scores.append(scores)\n        best_index = FeatureGradientSelector._find_best_index_elbow(\n            full_scores)\n        if self.verbose:\n            print(\"Forward passes took: \", time.time() - t)\n        # account for step size and off by one (n_features is 1 indexed, not 0\n        # )\n        return int(\n            np.ceil(\n                np.arange(\n                    1,\n                    self.max_features +\n                    1,\n                    step_size))[best_index])\n\n    @staticmethod\n    def _find_best_index_elbow(full_scores):\n        \"\"\"\n        Finds the point on the curve that maximizes distance from the line determined by the endpoints.\n        \"\"\"\n        scores = pd.DataFrame(full_scores).mean(0).values.tolist()\n        first_point = np.array([0, scores[0]])\n        last_point = np.array([len(scores) - 1, scores[-1]])\n        elbow_metric = []\n        for i in range(len(scores)):\n            elbow_metric.append(\n                FeatureGradientSelector._distance_to_line(\n                    first_point, last_point, np.array([i, scores[i]])))\n        return np.argmax(elbow_metric)\n\n    @staticmethod\n    def _distance_to_line(start_point, end_point, new_point):\n        \"\"\"\n        Calculates the shortest distance from new_point to the line determined by start_point and end_point.\n        \"\"\"\n        # for calculating elbow method\n        return np.cross(new_point - start_point,\n                        end_point - start_point) / np.linalg.norm(\n                            end_point - start_point)\n\n    def _reset(self):\n        \"\"\"\n        Reset the estimator by deleting all private and fit parameters.\n        \"\"\"\n        params = self.__dict__\n        for key, _ in params.items():\n            if key.endswith('_') or key.startswith('_'):\n                delattr(self, key)\n        return self",
  "def __init__(self,\n                 order=4,\n                 penalty=1,\n                 n_features=None,\n                 max_features=None,\n                 learning_rate=1e-1,\n                 init='zero',\n                 n_epochs=1,\n                 shuffle=True,\n                 batch_size=1000,\n                 target_batch_size=1000,\n                 max_time=np.inf,\n                 classification=True,\n                 ordinal=False,\n                 balanced=True,\n                 preprocess='zscore',\n                 soft_grouping=False,\n                 verbose=0,\n                 device='cpu'):\n        \"\"\"\n            FeatureGradientSelector is a class that selects features for a machine\n            learning model using a gradient based search.\n\n            Parameters\n            ----------\n            order : int\n                What order of interactions to include. Higher orders\n                may be more accurate but increase the run time. 12 is the maximum allowed order.\n            penatly : int\n                Constant that multiplies the regularization term.\n            n_features: int\n                If None, will automatically choose number of features based on search.\n                Otherwise, number of top features to select.\n            max_features : int\n                If not None, will use the 'elbow method' to determine the number of features\n                with max_features as the upper limit.\n            learning_rate : float\n            init : str\n                How to initialize the vector of scores. 'zero' is the default.\n                Options: {'zero', 'on', 'off', 'onhigh', 'offhigh', 'sklearn'}\n            n_epochs : int\n                number of epochs to run\n            shuffle : bool\n                Shuffle \"rows\" prior to an epoch.\n            batch_size : int\n                Nnumber of \"rows\" to process at a time\n            target_batch_size : int\n                Number of \"rows\" to accumulate gradients over.\n                Useful when many rows will not fit into memory but are needed for accurate estimation.\n            classification : bool\n                If True, problem is classification, else regression.\n            ordinal : bool\n                If True, problem is ordinal classification. Requires classification to be True.\n            balanced : bool\n                If true, each class is weighted equally in optimization, otherwise\n                weighted is done via support of each class. Requires classification to be True.\n            prerocess : str\n                'zscore' which refers to centering and normalizing data to unit variance or\n                'center' which only centers the data to 0 mean\n            soft_grouping : bool\n                if True, groups represent features that come from the same source.\n                Used to encourage sparsity of groups and features within groups.\n            verbose : int\n                Controls the verbosity when fitting. Set to 0 for no printing\n                1 or higher for printing every verbose number of gradient steps.\n            device : str\n                'cpu' to run on CPU and 'cuda' to run on GPU. Runs much faster on GPU\n        \"\"\"\n        assert order <= 12 and order >= 1, 'order must be an integer between 1 and 12, inclusive'\n        assert n_features is None or max_features is None, \\\n            'only specify one of n_features and max_features at a time'\n\n        self.order = order\n        self.penalty = penalty\n        self.n_features = n_features\n        self.max_features = max_features\n        self.learning_rate = learning_rate\n        self.init = init\n        self.n_epochs = n_epochs\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.target_batch_size = target_batch_size\n        self.max_time = max_time\n        self.dftol_stop = -1\n        self.freltol_stop = -1\n        self.classification = classification\n        self.ordinal = ordinal\n        self.balanced = balanced\n        self.preprocess = preprocess\n        self.soft_grouping = soft_grouping\n        self.verbose = verbose\n        self.device = device\n\n        self.model_ = None\n        self.scores_ = None\n        self._prev_checkpoint = None\n        self._data_train = None",
  "def partial_fit(self, X, y,\n                    n_classes=None,\n                    groups=None):\n        \"\"\"\n        Select Features via a gradient based search on (X, y) on the given samples.\n        Can be called repeatedly with different X and y to handle streaming datasets.\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y :  array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        n_classes : int\n            Number of classes\n            Classes across all calls to partial_fit.\n            Can be obtained by via `np.unique(y_all).shape[0]`, where y_all is the\n            target vector of the entire dataset.\n            This argument is expected for the first call to partial_fit,\n            otherwise will assume all classes are present in the batch of y given.\n            It will be ignored in the subsequent calls.\n            Note that y doesn't need to contain all labels in `classes`.\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n            This argument is expected for the first call to partial_fit,\n            otherwise will assume all classes are present in the batch of y given.\n            It will be ignored in the subsequent calls.\n        \"\"\"\n        try:\n            self._partial_fit(X, y, n_classes=n_classes, groups=groups)\n        except constants.NanError:\n            if hasattr(self, '_prev_checkpoint'):\n                # if it's already done some batches successfully just ignore it\n                print('failed fitting this batch, loss was nan')\n            else:\n                # if this is the first batch, reset and try with doubles\n                if self.verbose:\n                    print('Loss was nan, trying with Doubles')\n                self._reset()\n                torch.set_default_tensor_type(torch.DoubleTensor)\n                self._partial_fit(X, y, n_classes=n_classes, groups=groups)\n\n        return self",
  "def _partial_fit(self, X, y, n_classes=None, groups=None):\n        \"\"\"\n        Private function for partial_fit to enable trying floats before doubles.\n        \"\"\"\n        # pass in X and y in chunks\n        if hasattr(self, '_data_train'):\n            # just overwrite the X and y from the new chunk but make them tensors\n            # keep dataset stats from previous\n            self._data_train.X = X.values if isinstance(X, pd.DataFrame) else X\n            self._data_train.N, self._data_train.D = self._data_train.X.shape\n            self._data_train.dense_size_gb = self._data_train.get_dense_size()\n            self._data_train.set_dense_X()\n\n            self._data_train.y = y.values if isinstance(y, pd.Series) else y\n            self._data_train.y = torch.as_tensor(\n                y, dtype=torch.get_default_dtype())\n        else:\n            data_train = self._prepare_data(X, y, n_classes=n_classes)\n            self._data_train = data_train\n\n        batch_size, _, accum_steps, max_iter = self._set_batch_size(\n            self._data_train)\n\n        rng = None  # not used\n        debug = 0  # {0,1} print messages and do other stuff?\n        dn_logs = None  # tensorboard logs; only specify if debug=1\n        path_save = None  # intermediate models saves; only specify if debug=1\n        m, solver = _train(self._data_train,\n                           batch_size,\n                           self.order,\n                           self.penalty,\n                           rng,\n                           self.learning_rate,\n                           debug,\n                           max_iter,\n                           self.max_time,\n                           self.init,\n                           self.dftol_stop,\n                           self.freltol_stop,\n                           dn_logs,\n                           accum_steps,\n                           path_save,\n                           self.shuffle,\n                           device=self.device,\n                           verbose=self.verbose,\n                           prev_checkpoint=self._prev_checkpoint if hasattr(\n                               self, '_prev_checkpoint') else None,\n                           groups=groups if not self.soft_grouping else None,\n                           soft_groups=groups if self.soft_grouping else None)\n\n        self._prev_checkpoint = m\n        self._process_results(m, solver, X, groups=groups)\n        return self",
  "def fit(self, X, y,\n            groups=None):\n        \"\"\"\n        Select Features via a gradient based search on (X, y).\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        \"\"\"\n        try:\n            self._fit(X, y, groups=groups)\n        except constants.NanError:\n            if self.verbose:\n                print('Loss was nan, trying with Doubles')\n            torch.set_default_tensor_type(torch.DoubleTensor)\n            self._fit(X, y, groups=groups)\n        return self",
  "def get_selected_features(self):\n        return self.selected_features_",
  "def _prepare_data(self, X, y, n_classes=None):\n        \"\"\"\n        Returns a PrepareData object.\n        \"\"\"\n        return PrepareData(X=X.values if isinstance(X, pd.DataFrame) else X,\n                           y=y.values if isinstance(y, pd.Series) else y,\n                           data_format=constants.DataFormat.NUMPY,\n                           classification=int(self.classification),\n                           ordinal=self.ordinal,\n                           balanced=self.balanced,\n                           preprocess=self.preprocess,\n                           verbose=self.verbose,\n                           device=self.device,\n                           n_classes=n_classes)",
  "def _fit(self, X, y, groups=None):\n        \"\"\"\n        Private function for fit to enable trying floats before doubles.\n        \"\"\"\n        data_train = self._prepare_data(X, y)\n\n        batch_size, _, accum_steps, max_iter = self._set_batch_size(\n            data_train)\n\n        rng = None  # not used\n        debug = 0  # {0,1} print messages and log to tensorboard\n        dn_logs = None  # tensorboard logs; only specify if debug=1\n        path_save = None  # intermediate models saves; only specify if debug=1\n        m, solver = _train(data_train,\n                           batch_size,\n                           self.order,\n                           self.penalty,\n                           rng,\n                           self.learning_rate,\n                           debug,\n                           max_iter,\n                           self.max_time,\n                           self.init,\n                           self.dftol_stop,\n                           self.freltol_stop,\n                           dn_logs,\n                           accum_steps,\n                           path_save,\n                           self.shuffle,\n                           device=self.device,\n                           verbose=self.verbose,\n                           groups=groups if not self.soft_grouping else None,\n                           soft_groups=groups if self.soft_grouping else None)\n\n        self._process_results(m, solver, X, groups=groups)\n        return self",
  "def _process_torch_scores(self, scores):\n        \"\"\"\n        Convert scores into flat numpy arrays.\n        \"\"\"\n        if constants.Device.CUDA in scores.device.type:\n            scores = scores.cpu()\n        return scores.numpy().ravel()",
  "def _set_batch_size(self, data_train):\n        \"\"\"\n        Ensures that batch_size is less than the number of rows.\n        \"\"\"\n        batch_size = min(self.batch_size, data_train.N)\n        target_batch_size = min(max(\n            self.batch_size, self.target_batch_size), data_train.N)\n        accum_steps = max(int(np.ceil(target_batch_size / self.batch_size)), 1)\n        max_iter = self.n_epochs * (data_train.N // batch_size)\n        return batch_size, target_batch_size, accum_steps, max_iter",
  "def _process_results(self, m, solver, X, groups=None):\n        \"\"\"\n        Process the results of a run into something suitable for transform().\n        \"\"\"\n        self.scores_ = self._process_torch_scores(\n            torch.sigmoid(m[constants.Checkpoint.MODEL]['x'] * 2))\n        if self.max_features:\n            self.max_features = min([self.max_features, self.scores_.shape[0]])\n            n_features = self._recommend_number_features(solver)\n            self.set_n_features(n_features, groups=groups)\n        elif self.n_features:\n            self.set_n_features(self.n_features, groups=groups)\n        else:\n            self.selected_features_ = m['feats']\n\n        # subtract elapsed time from max_time\n        self.max_time -= m['t']\n\n        self.model_ = m\n\n        return self",
  "def transform(self, X):\n        \"\"\"\n        Returns selected features from X.\n\n        Paramters\n        ---------\n        X: array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        \"\"\"\n\n        self._get_support_mask()\n        if self.selected_features_.shape[0] == 0:\n            raise ValueError(\n                'No Features selected, consider lowering the penalty or specifying n_features')\n        return (X.iloc[:, self.selected_features_]\n                if isinstance(X, pd.DataFrame)\n                else X[:, self.selected_features_])",
  "def get_support(self, indices=False):\n        \"\"\"\n        Get a mask, or integer index, of the features selected.\n\n        Parameters\n        ----------\n        indices : bool\n            Default False\n            If True, the return value will be an array of integers, rather than a boolean mask.\n\n        Returns\n        -------\n        list :\n            returns support: An index that selects the retained features from a feature vector.\n            If indices is False, this is a boolean array of shape [# input features],\n            in which an element is True iff its corresponding feature is selected for retention.\n            If indices is True, this is an integer array of shape [# output features] whose values\n            are indices into the input feature vector.\n        \"\"\"\n        self._get_support_mask()\n        if indices:\n            return self.selected_features_\n\n        mask = np.zeros_like(self.scores_, dtype=bool)\n        # pylint: disable=E1137\n        mask[self.selected_features_] = True\n        return mask",
  "def inverse_transform(self, X):\n        \"\"\"\n        Returns transformed X to the original number of column.\n        This operation is lossy and all columns not in the transformed data\n        will be returned as columns of 0s.\n        \"\"\"\n        self._get_support_mask()\n        X_new = np.zeros((X.shape[0], self.scores_.shape[0]))\n        X_new[self.selected_features_] = X\n        return X_new",
  "def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n        \"\"\"\n        params = self.__dict__\n        params = {key: val for (key, val) in params.items()\n                  if not key.endswith('_')}\n        return params",
  "def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n        \"\"\"\n        for param in params:\n            if hasattr(self, param):\n                setattr(self, param, params[param])\n        return self",
  "def fit_transform(self, X, y):\n        \"\"\"\n        Select features and then return X with the selected features.\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        \"\"\"\n        self.fit(X, y)\n        return self.transform(X)",
  "def _get_support_mask(self):\n        \"\"\"\n        Check if it is fitted.\n        \"\"\"\n        check_is_fitted(self, 'scores_')",
  "def _generate_scores(self, solver, xsub, ysub, step_size, feature_order):\n        \"\"\"\n        Generate forward passes to determine the number of features when max_features is set.\n        \"\"\"\n        scores = []\n        for i in np.arange(1, self.max_features + 1, step_size):\n            # optimization possible since xsub is growing?\n            i = int(np.ceil(i))\n            # pylint: disable=E1102\n            score = solver.f_train(torch.tensor(np.ones(i),\n                                                dtype=torch.get_default_dtype()\n                                                ).unsqueeze(1).to(self.device),\n                                   xsub[:, feature_order[:i]],\n                                   ysub)\n            if constants.Device.CUDA in score.device.type:\n                score = score.cpu()\n            # score.numpy()[0][0]\n            scores.append(score)\n        return scores",
  "def set_n_features(self, n, groups=None):\n        \"\"\"\n        Set the number of features to return after fitting.\n        \"\"\"\n        self._get_support_mask()\n        self.n_features = n\n        return self._set_top_features(groups=groups)",
  "def _set_top_features(self, groups=None):\n        \"\"\"\n        Set the selected features after a run.\n\n        With groups, ensures that if any member of a group is selected, all members are selected\n        \"\"\"\n        self._get_support_mask()\n        assert self.n_features <= self.scores_.shape[0], \\\n            'n_features must be less than or equal to the number of columns in X'\n        # pylint: disable=E1130\n        self.selected_features_ = np.argpartition(\n            self.scores_, -self.n_features)[-self.n_features:]\n        if groups is not None and not self.soft_grouping:\n            selected_feature_set = set(self.selected_features_.tolist())\n            for _ in np.unique(groups):\n                group_members = np.where(groups == groups)[0].tolist()\n                if selected_feature_set.intersection(group_members):\n                    selected_feature_set.update(group_members)\n            self.selected_features_ = np.array(list(selected_feature_set))\n        self.selected_features_ = np.sort(self.selected_features_)\n        return self",
  "def set_top_percentile(self, percentile, groups=None):\n        \"\"\"\n        Set the percentile of features to return after fitting.\n        \"\"\"\n        self._get_support_mask()\n        assert percentile <= 1 and percentile >= 0, \\\n            'percentile must between 0 and 1 inclusive'\n        self.n_features = int(self.scores_.shape[0] * percentile)\n        return self._set_top_features(groups=groups)",
  "def _recommend_number_features(self, solver, max_time=None):\n        \"\"\"\n        Get the recommended number of features by doing forward passes when max_features is set.\n        \"\"\"\n        max_time = max_time if max_time else self.max_time\n        if max_time < 0:\n            max_time = 60  # allow 1 minute extra if we already spent max_time\n        MAX_FORWARD_PASS = 200\n        MAX_FULL_BATCHES = 3  # the forward passes can take longer than the fitting\n        # if we allow a full epoch of data to be included. By only doing 3 full batches at most\n        # we get enough accuracy without increasing the time too much. This\n        # constant may not be optimal\n        accum_steps = solver.accum_steps\n        step_size = max(self.max_features / MAX_FORWARD_PASS, 1)\n        # pylint: disable=E1130\n        feature_order = np.argsort(-self.scores_)  # note the negative\n        t = time.time()\n\n        dataloader_iterator = iter(solver.ds_train)\n        full_scores = []\n        # keep_going = True\n        with torch.no_grad():\n            # might want to only consider a batch valid if there are at least\n            # two classes\n            for _ in range(accum_steps * MAX_FULL_BATCHES):\n                scores = []\n                try:\n                    xsub, ysub = next(dataloader_iterator)\n                except StopIteration:\n                    # done with epoch, don't do more than one epoch\n                    break\n                except Exception as e:\n                    print(e)\n                    break\n                if max_time and time.time() - t > max_time:\n                    if self.verbose:\n                        print(\n                            \"Stoppinn forward passes because they reached max_time: \",\n                            max_time)\n                    if not full_scores:\n                        # no forward passes worked, return half of max_features\n                        return self.max_features // 2\n                    break\n                if solver.multiclass:\n                    for target_class in range(solver.n_classes):\n                        ysub_binary = solver.transform_y_into_binary(\n                            ysub, target_class)\n                        scaling_value = solver._get_scaling_value(\n                            ysub, target_class)\n                        if not solver._skip_y_forward(ysub_binary):\n                            scores = self._generate_scores(\n                                solver, xsub, ysub_binary, step_size, feature_order)\n                            # one row will represent one class that is present in the data\n                            # all classes are weighted equally\n                            full_scores.append(\n                                [score * scaling_value for score in scores])\n                else:\n                    if not solver._skip_y_forward(ysub):\n                        scores = self._generate_scores(\n                            solver, xsub, ysub, step_size, feature_order)\n                        full_scores.append(scores)\n        best_index = FeatureGradientSelector._find_best_index_elbow(\n            full_scores)\n        if self.verbose:\n            print(\"Forward passes took: \", time.time() - t)\n        # account for step size and off by one (n_features is 1 indexed, not 0\n        # )\n        return int(\n            np.ceil(\n                np.arange(\n                    1,\n                    self.max_features +\n                    1,\n                    step_size))[best_index])",
  "def _find_best_index_elbow(full_scores):\n        \"\"\"\n        Finds the point on the curve that maximizes distance from the line determined by the endpoints.\n        \"\"\"\n        scores = pd.DataFrame(full_scores).mean(0).values.tolist()\n        first_point = np.array([0, scores[0]])\n        last_point = np.array([len(scores) - 1, scores[-1]])\n        elbow_metric = []\n        for i in range(len(scores)):\n            elbow_metric.append(\n                FeatureGradientSelector._distance_to_line(\n                    first_point, last_point, np.array([i, scores[i]])))\n        return np.argmax(elbow_metric)",
  "def _distance_to_line(start_point, end_point, new_point):\n        \"\"\"\n        Calculates the shortest distance from new_point to the line determined by start_point and end_point.\n        \"\"\"\n        # for calculating elbow method\n        return np.cross(new_point - start_point,\n                        end_point - start_point) / np.linalg.norm(\n                            end_point - start_point)",
  "def _reset(self):\n        \"\"\"\n        Reset the estimator by deleting all private and fit parameters.\n        \"\"\"\n        params = self.__dict__\n        for key, _ in params.items():\n            if key.endswith('_') or key.startswith('_'):\n                delattr(self, key)\n        return self",
  "class StorageLevel:\n    DISK = 'disk'\n    SPARSE = 'sparse'\n    DENSE = 'dense'",
  "class DataFormat:\n    SVM = 'svm'\n    NUMPY = 'numpy'\n    ALL_FORMATS = [SVM, NUMPY]",
  "class Preprocess:\n    \"\"\"\n    center the data to mean 0 and create unit variance\n    center the data to mean 0\n    \"\"\"\n    ZSCORE = 'zscore'\n    CENTER = 'center'",
  "class Device:\n    CUDA = 'cuda'\n    CPU = 'cpu'",
  "class Checkpoint:\n    MODEL = 'model_state_dict'\n    OPT = 'optimizer_state_dict'\n    RNG = 'torch_rng_state'",
  "class NanError(ValueError):\n    pass",
  "class Initialization:\n    ZERO = 'zero'\n    ON = 'on'\n    OFF = 'off'\n    ON_HIGH = 'onhigh'\n    OFF_HIGH = 'offhigh'\n    SKLEARN = 'sklearn'\n    RANDOM = 'random'\n    VALUE_DICT = {ZERO: 0,\n                  ON: 1,\n                  OFF: -1,\n                  ON_HIGH: 5,\n                  OFF_HIGH: -1,\n                  SKLEARN: None,\n                  RANDOM: None}",
  "class Coefficients:\n    \"\"\"\"\n    coefficients for sublinear estimator were computed running the sublinear\n    paper's authors' code\n    \"\"\"\n    SLE = {1: np.array([0.60355337]),\n           2: np.array([1.52705001, -0.34841729]),\n           3: np.array([2.90254224, -1.87216745, 0.]),\n           4: np.array([4.63445685, -5.19936195, 0., 1.50391676]),\n           5: np.array([6.92948049, -14.12216211, 9.4475009, 0., -1.21093546]),\n           6: np.array([9.54431082, -28.09414643, 31.84703652, -11.18763791, -1.14175281, 0.]),\n           7: np.array([12.54505041, -49.64891525, 79.78828031, -46.72250909, 0., 0., 5.02973646]),\n           8: np.array([16.03550163, -84.286182, 196.86078756, -215.36747071, 92.63961263, 0., 0., -4.86280869]),\n           9: np.array([19.86409184, -130.76801006, 390.95349861, -570.09210416, 354.77764899, 0., -73.84234865, 0., 10.09148767]),\n           10: np.array([2.41117752e+01, -1.94946061e+02, 7.34214614e+02, -1.42851995e+03, 1.41567410e+03, \\\n                         -5.81738134e+02, 0., 0., 3.11664751e+01, 1.05018365e+00]),\n           11: np.array([28.75280839, -279.22576729, 1280.46325445, -3104.47148101, 3990.6092248, -2300.29413333, \\\n                         0., 427.35289033, 0., 0., -42.17587475]),\n           12: np.array([33.85141912, -391.4229382, 2184.97827882, -6716.28280208, 11879.75233977, -11739.97267239, \\\n                         5384.94542245, 0., -674.23291712, 0., 0., 39.37456439])}",
  "def def_train_opt(p):\n    \"\"\"\n    Return the default optimizer.\n    \"\"\"\n    return torch.optim.Adam(p, 1e-1, amsgrad=False)",
  "def revcumsum(U):\n    \"\"\"\n    Reverse cumulative sum for faster performance.\n    \"\"\"\n    return U.flip(dims=[0]).cumsum(dim=0).flip(dims=[0])",
  "def triudr(X, r):\n\n    Zr = torch.zeros_like(X, requires_grad=False)\n    U = X * r\n    Zr[:-1] = X[:-1] * revcumsum(U)[1:]\n\n    return Zr",
  "def triudl(X, l):\n\n    Zl = torch.zeros_like(X, requires_grad=False)\n    U = X * l\n    Zl[1:] = X[1:] * (U.cumsum(dim=0)[:-1])\n\n    return Zl",
  "class ramp(torch.autograd.Function):\n    \"\"\"\n    Ensures input is between 0 and 1\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input_data):\n        ctx.save_for_backward(input_data)\n        return input_data.clamp(min=0, max=1)\n\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_data, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input_data < 0] = 1e-2\n        grad_input[input_data > 1] = -1e-2\n        return grad_input",
  "class safesqrt(torch.autograd.Function):\n    \"\"\"\n    Square root without dividing by 0.\n    \"\"\"\n    @staticmethod\n    def forward(ctx, input_data):\n        o = input_data.sqrt()\n        ctx.save_for_backward(input_data, o)\n        return o\n\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        _, o = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input *= 0.5 / (o + constants.EPSILON)\n        return grad_input",
  "class LearnabilityMB(nn.Module):\n    \"\"\"\n    Calculates the learnability of a set of features.\n    mini-batch version w/ \"left\" and \"right\" multiplies\n    \"\"\"\n\n\n    def __init__(self, Nminibatch, D, coeff, groups=None, binary=False,\n                 device=constants.Device.CPU):\n        super(LearnabilityMB, self).__init__()\n\n        a = coeff / scipy.special.binom(Nminibatch, np.arange(coeff.size) + 2)\n        self.order = a.size\n        # pylint: disable=E1102\n        self.a = torch.tensor(a, dtype=torch.get_default_dtype(), requires_grad=False)\n        self.binary = binary\n\n        self.a = self.a.to(device)\n\n\n    def ret_val(self, z):\n        \"\"\"\n        Get the return value based on z.\n        \"\"\"\n\n        if not self.binary:\n            return 1 - z\n\n        else:\n            return 0.5 * (1 - safesqrt.apply(ramp.apply(z)))\n\n\n    def forward(self, s, X, y):\n\n        l = y.clone()\n        r = y.clone()\n        z = 0\n\n        for i in range(self.order):\n            if i % 2 == 0:\n                Z = triudr(X, r)\n                r = torch.mm(Z, s)\n            else:\n                Z = triudl(X, l)\n                l = torch.mm(Z, s)\n            if self.a[i] != 0:\n                # same the computation if a[i] is 0\n                p = torch.mm(l.t(), r)\n                z += self.a[i] * p\n        return self.ret_val(z)",
  "class Solver(nn.Module):\n    \"\"\"\n    Class that performs the main optimization.\n    Keeps track of the current x and iterates through data to learn x given the penalty and order.\n    \"\"\"\n\n    def __init__(self,\n                 PreparedData,\n                 order,\n                 Nminibatch=None,\n                 groups=None,\n                 soft_groups=None,\n                 x0=None,\n                 C=1,\n                 ftransform=torch.sigmoid,\n                 get_train_opt=def_train_opt,\n                 accum_steps=1,\n                 rng=np.random.RandomState(0),\n                 max_norm_clip=1.,\n                 shuffle=True,\n                 device=constants.Device.CPU,\n                 verbose=1):\n        \"\"\"\n\n        Parameters\n        ----------\n        PreparedData : Dataset of PrepareData class\n        order : int\n            What order of interactions to include. Higher orders\n            may be more accurate but increase the run time. 12 is the maximum allowed order.\n        Nminibatch : int\n            Number of rows in a mini batch\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        soft_groups : array-like\n            optional, shape = [n_features]\n            Groups of columns come from the same source\n            Used to encourage sparsity of number of sources selected\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        x0 : torch.tensor\n            Optional, initialization of x.\n        C : float\n            Penalty parameter.\n        get_train_opt : function\n            Function that returns a pytorch optimizer, Adam is the default\n        accum_steps : int\n            Number of steps\n        rng : random state\n        max_norm_clip : float\n            Maximum allowable size of the gradient\n        shuffle : bool\n            Whether or not to shuffle data within the dataloader\n        order : int\n            What order of interactions to include. Higher orders\n            may be more accurate but increase the run time. 12 is the maximum allowed order.\n        penalty : int\n            Constant that multiplies the regularization term.\n        ftransform : function\n            Function to transform the x. sigmoid is the default.\n        device : str\n            'cpu' to run on CPU and 'cuda' to run on GPU. Runs much faster on GPU\n        verbose : int\n            Controls the verbosity when fitting. Set to 0 for no printing\n            1 or higher for printing every verbose number of gradient steps.\n        \"\"\"\n        super(Solver, self).__init__()\n\n        self.Ntrain, self.D = PreparedData.N, PreparedData.n_features\n        if groups is not None:\n            # pylint: disable=E1102\n            groups = torch.tensor(groups, dtype=torch.long)\n            self.groups = groups\n        else:\n            self.groups = None\n        if soft_groups is not None:\n            # pylint: disable=E1102\n            soft_groups = torch.tensor(soft_groups, dtype=torch.long)\n            self.soft_D = torch.unique(soft_groups).size()[0]\n        else:\n            self.soft_D = None\n        self.soft_groups = soft_groups\n\n        if Nminibatch is None:\n            Nminibatch = self.Ntrain\n        else:\n            if Nminibatch > self.Ntrain:\n                print('Minibatch larger than sample size.'\n                      + (' Reducing from %d to %d.'\n                         % (Nminibatch, self.Ntrain)))\n                Nminibatch = self.Ntrain\n        if Nminibatch > PreparedData.max_rows:\n            print('Minibatch larger than mem-allowed.'\n                  + (' Reducing from %d to %d.' % (Nminibatch,\n                                                   PreparedData.max_rows)))\n            Nminibatch = int(np.min([Nminibatch, PreparedData.max_rows]))\n        self.Nminibatch = Nminibatch\n        self.accum_steps = accum_steps\n\n        if x0 is None:\n            x0 = torch.zeros(self.D, 1, dtype=torch.get_default_dtype())\n        self.ftransform = ftransform\n        self.x = nn.Parameter(x0)\n        self.max_norm = max_norm_clip\n\n        self.device = device\n        self.verbose = verbose\n\n        self.multiclass = PreparedData.classification and PreparedData.n_classes and PreparedData.n_classes > 2\n        if self.multiclass:\n            self.n_classes = PreparedData.n_classes\n        else:\n            self.n_classes = None\n        # whether to treat all classes equally\n        self.balanced = PreparedData.balanced\n        self.ordinal = PreparedData.ordinal\n\n        if (hasattr(PreparedData, 'mappings')\n                or PreparedData.storage_level == 'disk'):\n            num_workers = PreparedData.num_workers\n        elif PreparedData.storage_level == constants.StorageLevel.DENSE:\n            num_workers = 0\n        else:\n            num_workers = 0\n\n        if constants.Device.CUDA in device:\n            pin_memory = False\n        else:\n            pin_memory = False\n\n        if num_workers == 0:\n            timeout = 0\n        else:\n            timeout = 60\n\n        self.ds_train = ChunkDataLoader(\n            PreparedData,\n            batch_size=self.Nminibatch,\n            shuffle=shuffle,\n            drop_last=True,\n            num_workers=num_workers,\n            pin_memory=pin_memory,\n            timeout=timeout)\n        self.f_train = LearnabilityMB(self.Nminibatch, self.D,\n                                      constants.Coefficients.SLE[order],\n                                      self.groups,\n                                      binary=PreparedData.classification,\n                                      device=self.device)\n        self.opt_train = get_train_opt(torch.nn.ParameterList([self.x]))\n        self.it = 0\n        self.iters_per_epoch = int(np.ceil(len(self.ds_train.dataset)\n                                           / self.ds_train.batch_size))\n        self.f_train = self.f_train.to(device)\n        # pylint: disable=E1102\n        self.w = torch.tensor(\n            C / (C + 1),\n            dtype=torch.get_default_dtype(), requires_grad=False)\n        self.w = self.w.to(device)\n\n\n    def penalty(self, s):\n        \"\"\"\n        Calculate L1 Penalty.\n        \"\"\"\n        to_return = torch.sum(s) / self.D\n        if self.soft_groups is not None:\n            # if soft_groups, there is an additional penalty for using more\n            # groups\n            s_grouped = torch.zeros(self.soft_D, 1,\n                                    dtype=torch.get_default_dtype(),\n                                    device=self.device)\n            for group in torch.unique(self.soft_groups):\n                # groups should be indexed 0 to n_group - 1\n                # TODO: consider other functions here\n                s_grouped[group] = s[self.soft_groups == group].max()\n            # each component of the penalty contributes .5\n            # TODO: could make this a user given parameter\n            to_return = (to_return + torch.sum(s_grouped) / self.soft_D) * .5\n        return to_return\n\n\n    def forward_and_backward(self, s, xsub, ysub, retain_graph=False):\n        \"\"\"\n        Completes the forward operation and computes gradients for learnability and penalty.\n        \"\"\"\n        f_train = self.f_train(s, xsub, ysub)\n        pen = self.penalty(s).unsqueeze(0).unsqueeze(0)\n        # pylint: disable=E1102\n        grad_outputs = torch.tensor([[1]], dtype=torch.get_default_dtype(),\n                                    device=self.device)\n        g1, = torch.autograd.grad([f_train], [self.x], grad_outputs,\n                                  retain_graph=True)\n        # pylint: disable=E1102\n        grad_outputs = torch.tensor([[1]], dtype=torch.get_default_dtype(),\n                                    device=self.device)\n        g2, = torch.autograd.grad([pen], [self.x], grad_outputs,\n                                  retain_graph=retain_graph)\n        return f_train, pen, g1, g2\n\n\n    def combine_gradient(self, g1, g2):\n        \"\"\"\n        Combine gradients from learnability and penalty\n\n        Parameters\n        ----------\n        g1 : array-like\n            gradient from learnability\n        g2 : array-like\n            gradient from penalty\n        \"\"\"\n        to_return = ((1 - self.w) * g1 + self.w * g2) / self.accum_steps\n        if self.groups is not None:\n            # each column will get a gradient\n            # but we can only up or down groups, so the gradient for the group\n            # should be the average of the gradients of the columns\n            to_return_grouped = torch.zeros_like(self.x)\n            for group in torch.unique(self.groups):\n                to_return_grouped[self.groups ==\n                                  group] = to_return[self.groups == group].mean()\n            to_return = to_return_grouped\n        return to_return\n\n\n    def combine_loss(self, f_train, pen):\n        \"\"\"\n        Combine the learnability and L1 penalty.\n        \"\"\"\n        return ((1 - self.w) * f_train.detach() + self.w * pen.detach()) \\\n            / self.accum_steps\n\n\n    def transform_y_into_binary(self, ysub, target_class):\n        \"\"\"\n        Transforms multiclass classification problems into a binary classification problem.\n        \"\"\"\n        with torch.no_grad():\n            ysub_binary = torch.zeros_like(ysub)\n            if self.ordinal:\n                # turn ordinal problems into n-1 classifications of is this\n                # example less than rank k\n                if target_class == 0:\n                    return None\n\n                ysub_binary[ysub >= target_class] = 1\n                ysub_binary[ysub < target_class] = -1\n            else:\n                # turn multiclass problems into n binary classifications\n                ysub_binary[ysub == target_class] = 1\n                ysub_binary[ysub != target_class] = -1\n        return ysub_binary\n\n\n    def _get_scaling_value(self, ysub, target_class):\n        \"\"\"\n        Returns the weight given to a class for multiclass classification.\n        \"\"\"\n        if self.balanced:\n            if self.ordinal:\n                return 1 / (torch.unique(ysub).size()[0] - 1)\n\n            return 1 / torch.unique(ysub).size()[0]\n        else:\n            if self.ordinal:\n                this_class_proportion = torch.mean(ysub >= target_class)\n                normalizing_constant = 0\n                for i in range(1, self.n_classes):\n                    normalizing_constant += torch.mean(ysub >= i)\n                return this_class_proportion / normalizing_constant\n            else:\n                return torch.mean(ysub == target_class)\n\n\n    def _skip_y_forward(self, y):\n        \"\"\"\n        Returns boolean of whether to skip the currrent y if there is nothing to be learned from it.\n        \"\"\"\n        if y is None:\n            return True\n        elif torch.unique(y).size()[0] < 2:\n            return True\n        else:\n            return False\n\n\n    def train(self, f_callback=None, f_stop=None):\n        \"\"\"\n        Trains the estimator to determine which features to include.\n\n        Parameters\n        ----------\n        f_callback : function\n            Function that performs a callback\n        f_stop: function\n            Function that tells you when to stop\n        \"\"\"\n\n        t = time.time()\n        h = torch.zeros([1, 1], dtype=torch.get_default_dtype())\n        h = h.to(self.device)\n        # h_complete is so when we divide by the number of classes\n        # we only do that for that minibatch if accumulating\n        h_complete = h.clone()\n        flag_stop = False\n        dataloader_iterator = iter(self.ds_train)\n        self.x.grad = torch.zeros_like(self.x)\n        while not flag_stop:\n            try:\n                xsub, ysub = next(dataloader_iterator)\n            except StopIteration:\n                dataloader_iterator = iter(self.ds_train)\n                xsub, ysub = next(dataloader_iterator)\n            try:\n                s = self.ftransform(self.x)\n                s = s.to(self.device)\n                if self.multiclass:\n                    # accumulate gradients over each class, classes range from\n                    # 0 to n_classes - 1\n                    #num_classes_batch = torch.unique(ysub).size()[0]\n                    for target_class in range(self.n_classes):\n                        ysub_binary = self.transform_y_into_binary(\n                            ysub, target_class)\n                        if self._skip_y_forward(ysub_binary):\n                            continue\n                        # should should skip if target class is not included\n                        # but that changes what we divide by\n                        scaling_value = self._get_scaling_value(\n                            ysub, target_class)\n                        f_train, pen, g1, g2 = self.forward_and_backward(\n                            s, xsub, ysub_binary, retain_graph=True)\n                        self.x.grad += self.combine_gradient(\n                            g1, g2) * scaling_value\n                        h += self.combine_loss(f_train,\n                                               pen) * scaling_value\n                else:\n                    if not self._skip_y_forward(ysub):\n                        f_train, pen, g1, g2 = self.forward_and_backward(\n                            s, xsub, ysub)\n                        self.x.grad += self.combine_gradient(g1, g2)\n                        h += self.combine_loss(f_train, pen)\n                    else:\n                        continue\n                h_complete += h\n                self.it += 1\n                if torch.isnan(h):\n                    raise constants.NanError(\n                        'Loss is nan, something may be misconfigured')\n                if self.it % self.accum_steps == 0:\n                    torch.nn.utils.clip_grad_norm_(\n                        torch.nn.ParameterList([self.x]),\n                        max_norm=self.max_norm)\n                    self.opt_train.step()\n\n                    t = time.time() - t\n                    if f_stop is not None:\n                        flag_stop = f_stop(self, h, self.it, t)\n\n                    if f_callback is not None:\n                        f_callback(self, h, self.it, t)\n                    elif self.verbose and (self.it // self.accum_steps) % self.verbose == 0:\n                        epoch = int(self.it / self.iters_per_epoch)\n                        print(\n                            '[Minibatch: %6d/ Epoch: %3d/ t: %3.3f s] Loss: %0.3f' %\n                            (self.it, epoch, t, h_complete / self.accum_steps))\n\n                    if flag_stop:\n                        break\n\n                    self.opt_train.zero_grad()\n                    h = 0\n                    h_complete = 0\n                    t = time.time()\n            except KeyboardInterrupt:\n                flag_stop = True\n                break",
  "def forward(ctx, input_data):\n        ctx.save_for_backward(input_data)\n        return input_data.clamp(min=0, max=1)",
  "def backward(ctx, grad_output):\n        input_data, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input_data < 0] = 1e-2\n        grad_input[input_data > 1] = -1e-2\n        return grad_input",
  "def forward(ctx, input_data):\n        o = input_data.sqrt()\n        ctx.save_for_backward(input_data, o)\n        return o",
  "def backward(ctx, grad_output):\n        _, o = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input *= 0.5 / (o + constants.EPSILON)\n        return grad_input",
  "def __init__(self, Nminibatch, D, coeff, groups=None, binary=False,\n                 device=constants.Device.CPU):\n        super(LearnabilityMB, self).__init__()\n\n        a = coeff / scipy.special.binom(Nminibatch, np.arange(coeff.size) + 2)\n        self.order = a.size\n        # pylint: disable=E1102\n        self.a = torch.tensor(a, dtype=torch.get_default_dtype(), requires_grad=False)\n        self.binary = binary\n\n        self.a = self.a.to(device)",
  "def ret_val(self, z):\n        \"\"\"\n        Get the return value based on z.\n        \"\"\"\n\n        if not self.binary:\n            return 1 - z\n\n        else:\n            return 0.5 * (1 - safesqrt.apply(ramp.apply(z)))",
  "def forward(self, s, X, y):\n\n        l = y.clone()\n        r = y.clone()\n        z = 0\n\n        for i in range(self.order):\n            if i % 2 == 0:\n                Z = triudr(X, r)\n                r = torch.mm(Z, s)\n            else:\n                Z = triudl(X, l)\n                l = torch.mm(Z, s)\n            if self.a[i] != 0:\n                # same the computation if a[i] is 0\n                p = torch.mm(l.t(), r)\n                z += self.a[i] * p\n        return self.ret_val(z)",
  "def __init__(self,\n                 PreparedData,\n                 order,\n                 Nminibatch=None,\n                 groups=None,\n                 soft_groups=None,\n                 x0=None,\n                 C=1,\n                 ftransform=torch.sigmoid,\n                 get_train_opt=def_train_opt,\n                 accum_steps=1,\n                 rng=np.random.RandomState(0),\n                 max_norm_clip=1.,\n                 shuffle=True,\n                 device=constants.Device.CPU,\n                 verbose=1):\n        \"\"\"\n\n        Parameters\n        ----------\n        PreparedData : Dataset of PrepareData class\n        order : int\n            What order of interactions to include. Higher orders\n            may be more accurate but increase the run time. 12 is the maximum allowed order.\n        Nminibatch : int\n            Number of rows in a mini batch\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        soft_groups : array-like\n            optional, shape = [n_features]\n            Groups of columns come from the same source\n            Used to encourage sparsity of number of sources selected\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        x0 : torch.tensor\n            Optional, initialization of x.\n        C : float\n            Penalty parameter.\n        get_train_opt : function\n            Function that returns a pytorch optimizer, Adam is the default\n        accum_steps : int\n            Number of steps\n        rng : random state\n        max_norm_clip : float\n            Maximum allowable size of the gradient\n        shuffle : bool\n            Whether or not to shuffle data within the dataloader\n        order : int\n            What order of interactions to include. Higher orders\n            may be more accurate but increase the run time. 12 is the maximum allowed order.\n        penalty : int\n            Constant that multiplies the regularization term.\n        ftransform : function\n            Function to transform the x. sigmoid is the default.\n        device : str\n            'cpu' to run on CPU and 'cuda' to run on GPU. Runs much faster on GPU\n        verbose : int\n            Controls the verbosity when fitting. Set to 0 for no printing\n            1 or higher for printing every verbose number of gradient steps.\n        \"\"\"\n        super(Solver, self).__init__()\n\n        self.Ntrain, self.D = PreparedData.N, PreparedData.n_features\n        if groups is not None:\n            # pylint: disable=E1102\n            groups = torch.tensor(groups, dtype=torch.long)\n            self.groups = groups\n        else:\n            self.groups = None\n        if soft_groups is not None:\n            # pylint: disable=E1102\n            soft_groups = torch.tensor(soft_groups, dtype=torch.long)\n            self.soft_D = torch.unique(soft_groups).size()[0]\n        else:\n            self.soft_D = None\n        self.soft_groups = soft_groups\n\n        if Nminibatch is None:\n            Nminibatch = self.Ntrain\n        else:\n            if Nminibatch > self.Ntrain:\n                print('Minibatch larger than sample size.'\n                      + (' Reducing from %d to %d.'\n                         % (Nminibatch, self.Ntrain)))\n                Nminibatch = self.Ntrain\n        if Nminibatch > PreparedData.max_rows:\n            print('Minibatch larger than mem-allowed.'\n                  + (' Reducing from %d to %d.' % (Nminibatch,\n                                                   PreparedData.max_rows)))\n            Nminibatch = int(np.min([Nminibatch, PreparedData.max_rows]))\n        self.Nminibatch = Nminibatch\n        self.accum_steps = accum_steps\n\n        if x0 is None:\n            x0 = torch.zeros(self.D, 1, dtype=torch.get_default_dtype())\n        self.ftransform = ftransform\n        self.x = nn.Parameter(x0)\n        self.max_norm = max_norm_clip\n\n        self.device = device\n        self.verbose = verbose\n\n        self.multiclass = PreparedData.classification and PreparedData.n_classes and PreparedData.n_classes > 2\n        if self.multiclass:\n            self.n_classes = PreparedData.n_classes\n        else:\n            self.n_classes = None\n        # whether to treat all classes equally\n        self.balanced = PreparedData.balanced\n        self.ordinal = PreparedData.ordinal\n\n        if (hasattr(PreparedData, 'mappings')\n                or PreparedData.storage_level == 'disk'):\n            num_workers = PreparedData.num_workers\n        elif PreparedData.storage_level == constants.StorageLevel.DENSE:\n            num_workers = 0\n        else:\n            num_workers = 0\n\n        if constants.Device.CUDA in device:\n            pin_memory = False\n        else:\n            pin_memory = False\n\n        if num_workers == 0:\n            timeout = 0\n        else:\n            timeout = 60\n\n        self.ds_train = ChunkDataLoader(\n            PreparedData,\n            batch_size=self.Nminibatch,\n            shuffle=shuffle,\n            drop_last=True,\n            num_workers=num_workers,\n            pin_memory=pin_memory,\n            timeout=timeout)\n        self.f_train = LearnabilityMB(self.Nminibatch, self.D,\n                                      constants.Coefficients.SLE[order],\n                                      self.groups,\n                                      binary=PreparedData.classification,\n                                      device=self.device)\n        self.opt_train = get_train_opt(torch.nn.ParameterList([self.x]))\n        self.it = 0\n        self.iters_per_epoch = int(np.ceil(len(self.ds_train.dataset)\n                                           / self.ds_train.batch_size))\n        self.f_train = self.f_train.to(device)\n        # pylint: disable=E1102\n        self.w = torch.tensor(\n            C / (C + 1),\n            dtype=torch.get_default_dtype(), requires_grad=False)\n        self.w = self.w.to(device)",
  "def penalty(self, s):\n        \"\"\"\n        Calculate L1 Penalty.\n        \"\"\"\n        to_return = torch.sum(s) / self.D\n        if self.soft_groups is not None:\n            # if soft_groups, there is an additional penalty for using more\n            # groups\n            s_grouped = torch.zeros(self.soft_D, 1,\n                                    dtype=torch.get_default_dtype(),\n                                    device=self.device)\n            for group in torch.unique(self.soft_groups):\n                # groups should be indexed 0 to n_group - 1\n                # TODO: consider other functions here\n                s_grouped[group] = s[self.soft_groups == group].max()\n            # each component of the penalty contributes .5\n            # TODO: could make this a user given parameter\n            to_return = (to_return + torch.sum(s_grouped) / self.soft_D) * .5\n        return to_return",
  "def forward_and_backward(self, s, xsub, ysub, retain_graph=False):\n        \"\"\"\n        Completes the forward operation and computes gradients for learnability and penalty.\n        \"\"\"\n        f_train = self.f_train(s, xsub, ysub)\n        pen = self.penalty(s).unsqueeze(0).unsqueeze(0)\n        # pylint: disable=E1102\n        grad_outputs = torch.tensor([[1]], dtype=torch.get_default_dtype(),\n                                    device=self.device)\n        g1, = torch.autograd.grad([f_train], [self.x], grad_outputs,\n                                  retain_graph=True)\n        # pylint: disable=E1102\n        grad_outputs = torch.tensor([[1]], dtype=torch.get_default_dtype(),\n                                    device=self.device)\n        g2, = torch.autograd.grad([pen], [self.x], grad_outputs,\n                                  retain_graph=retain_graph)\n        return f_train, pen, g1, g2",
  "def combine_gradient(self, g1, g2):\n        \"\"\"\n        Combine gradients from learnability and penalty\n\n        Parameters\n        ----------\n        g1 : array-like\n            gradient from learnability\n        g2 : array-like\n            gradient from penalty\n        \"\"\"\n        to_return = ((1 - self.w) * g1 + self.w * g2) / self.accum_steps\n        if self.groups is not None:\n            # each column will get a gradient\n            # but we can only up or down groups, so the gradient for the group\n            # should be the average of the gradients of the columns\n            to_return_grouped = torch.zeros_like(self.x)\n            for group in torch.unique(self.groups):\n                to_return_grouped[self.groups ==\n                                  group] = to_return[self.groups == group].mean()\n            to_return = to_return_grouped\n        return to_return",
  "def combine_loss(self, f_train, pen):\n        \"\"\"\n        Combine the learnability and L1 penalty.\n        \"\"\"\n        return ((1 - self.w) * f_train.detach() + self.w * pen.detach()) \\\n            / self.accum_steps",
  "def transform_y_into_binary(self, ysub, target_class):\n        \"\"\"\n        Transforms multiclass classification problems into a binary classification problem.\n        \"\"\"\n        with torch.no_grad():\n            ysub_binary = torch.zeros_like(ysub)\n            if self.ordinal:\n                # turn ordinal problems into n-1 classifications of is this\n                # example less than rank k\n                if target_class == 0:\n                    return None\n\n                ysub_binary[ysub >= target_class] = 1\n                ysub_binary[ysub < target_class] = -1\n            else:\n                # turn multiclass problems into n binary classifications\n                ysub_binary[ysub == target_class] = 1\n                ysub_binary[ysub != target_class] = -1\n        return ysub_binary",
  "def _get_scaling_value(self, ysub, target_class):\n        \"\"\"\n        Returns the weight given to a class for multiclass classification.\n        \"\"\"\n        if self.balanced:\n            if self.ordinal:\n                return 1 / (torch.unique(ysub).size()[0] - 1)\n\n            return 1 / torch.unique(ysub).size()[0]\n        else:\n            if self.ordinal:\n                this_class_proportion = torch.mean(ysub >= target_class)\n                normalizing_constant = 0\n                for i in range(1, self.n_classes):\n                    normalizing_constant += torch.mean(ysub >= i)\n                return this_class_proportion / normalizing_constant\n            else:\n                return torch.mean(ysub == target_class)",
  "def _skip_y_forward(self, y):\n        \"\"\"\n        Returns boolean of whether to skip the currrent y if there is nothing to be learned from it.\n        \"\"\"\n        if y is None:\n            return True\n        elif torch.unique(y).size()[0] < 2:\n            return True\n        else:\n            return False",
  "def train(self, f_callback=None, f_stop=None):\n        \"\"\"\n        Trains the estimator to determine which features to include.\n\n        Parameters\n        ----------\n        f_callback : function\n            Function that performs a callback\n        f_stop: function\n            Function that tells you when to stop\n        \"\"\"\n\n        t = time.time()\n        h = torch.zeros([1, 1], dtype=torch.get_default_dtype())\n        h = h.to(self.device)\n        # h_complete is so when we divide by the number of classes\n        # we only do that for that minibatch if accumulating\n        h_complete = h.clone()\n        flag_stop = False\n        dataloader_iterator = iter(self.ds_train)\n        self.x.grad = torch.zeros_like(self.x)\n        while not flag_stop:\n            try:\n                xsub, ysub = next(dataloader_iterator)\n            except StopIteration:\n                dataloader_iterator = iter(self.ds_train)\n                xsub, ysub = next(dataloader_iterator)\n            try:\n                s = self.ftransform(self.x)\n                s = s.to(self.device)\n                if self.multiclass:\n                    # accumulate gradients over each class, classes range from\n                    # 0 to n_classes - 1\n                    #num_classes_batch = torch.unique(ysub).size()[0]\n                    for target_class in range(self.n_classes):\n                        ysub_binary = self.transform_y_into_binary(\n                            ysub, target_class)\n                        if self._skip_y_forward(ysub_binary):\n                            continue\n                        # should should skip if target class is not included\n                        # but that changes what we divide by\n                        scaling_value = self._get_scaling_value(\n                            ysub, target_class)\n                        f_train, pen, g1, g2 = self.forward_and_backward(\n                            s, xsub, ysub_binary, retain_graph=True)\n                        self.x.grad += self.combine_gradient(\n                            g1, g2) * scaling_value\n                        h += self.combine_loss(f_train,\n                                               pen) * scaling_value\n                else:\n                    if not self._skip_y_forward(ysub):\n                        f_train, pen, g1, g2 = self.forward_and_backward(\n                            s, xsub, ysub)\n                        self.x.grad += self.combine_gradient(g1, g2)\n                        h += self.combine_loss(f_train, pen)\n                    else:\n                        continue\n                h_complete += h\n                self.it += 1\n                if torch.isnan(h):\n                    raise constants.NanError(\n                        'Loss is nan, something may be misconfigured')\n                if self.it % self.accum_steps == 0:\n                    torch.nn.utils.clip_grad_norm_(\n                        torch.nn.ParameterList([self.x]),\n                        max_norm=self.max_norm)\n                    self.opt_train.step()\n\n                    t = time.time() - t\n                    if f_stop is not None:\n                        flag_stop = f_stop(self, h, self.it, t)\n\n                    if f_callback is not None:\n                        f_callback(self, h, self.it, t)\n                    elif self.verbose and (self.it // self.accum_steps) % self.verbose == 0:\n                        epoch = int(self.it / self.iters_per_epoch)\n                        print(\n                            '[Minibatch: %6d/ Epoch: %3d/ t: %3.3f s] Loss: %0.3f' %\n                            (self.it, epoch, t, h_complete / self.accum_steps))\n\n                    if flag_stop:\n                        break\n\n                    self.opt_train.zero_grad()\n                    h = 0\n                    h_complete = 0\n                    t = time.time()\n            except KeyboardInterrupt:\n                flag_stop = True\n                break",
  "def get_optim_f_stop(maxiter, maxtime, dftol_stop, freltol_stop,\n                     minibatch=True):\n    \"\"\"\n    Check stopping conditions.\n    \"\"\"\n\n    discount_factor = 1. / 3\n\n    total_t = [0.]\n    df_store = [np.nan]\n    it_store = [0]\n    relchange_store = [np.nan]\n    f_ma = EMA(discount_factor=discount_factor)\n    df_ma = EMA(discount_factor=discount_factor)\n\n    def f_stop(f0, v0, it, t):\n\n        flag_stop = False\n\n        total_t[-1] += t\n        g = f0.x.grad.clone().cpu().detach()\n        df = g.abs().max().numpy().squeeze()\n        v = v0.clone().cpu().detach()\n        f = v.numpy().squeeze()\n\n        if it >= maxiter:\n            flag_stop = True\n\n        elif total_t[-1] >= maxtime:\n            flag_stop = True\n\n        f_ma.update(f)\n        df_ma.update(df)\n        rel_change = f_ma.relchange()\n\n        if ((not minibatch) and (df < dftol_stop)) \\\n           or (minibatch and (df_ma() < dftol_stop)):\n            flag_stop = True\n\n        if rel_change < freltol_stop:\n            flag_stop = True\n\n        if not minibatch:\n            df_store[-1] = df\n        else:\n            df_store[-1] = df_ma()\n        relchange_store[-1] = rel_change\n        it_store[-1] = it\n\n        return flag_stop\n\n    return f_stop, {'t': total_t, 'it': it_store, 'df': df_store,\n                    'relchange': relchange_store}",
  "def get_init(data_train, init_type='on', rng=np.random.RandomState(0), prev_score=None):\n    \"\"\"\n    Initialize the 'x' variable with different settings\n    \"\"\"\n\n    D = data_train.n_features\n    value_off = constants.Initialization.VALUE_DICT[\n        constants.Initialization.OFF]\n    value_on = constants.Initialization.VALUE_DICT[\n        constants.Initialization.ON]\n\n    if prev_score is not None:\n        x0 = prev_score\n    elif not isinstance(init_type, str):\n        x0 = value_off * np.ones(D)\n        x0[init_type] = value_on\n    elif init_type.startswith(constants.Initialization.RANDOM):\n        d = int(init_type.replace(constants.Initialization.RANDOM, ''))\n        x0 = value_off * np.ones(D)\n        x0[rng.permutation(D)[:d]] = value_on\n    elif init_type == constants.Initialization.SKLEARN:\n        B = data_train.return_raw\n        X, y = data_train.get_dense_data()\n        data_train.set_return_raw(B)\n        ix = train_sk_dense(init_type, X, y, data_train.classification)\n        x0 = value_off * np.ones(D)\n        x0[ix] = value_on\n    elif init_type in constants.Initialization.VALUE_DICT:\n        x0 = constants.Initialization.VALUE_DICT[init_type] * np.ones(D)\n    else:\n        raise NotImplementedError(\n            'init_type {0} not supported yet'.format(init_type))\n    # pylint: disable=E1102\n    return torch.tensor(x0.reshape((-1, 1)),\n                        dtype=torch.get_default_dtype())",
  "def get_checkpoint(S, stop_conds, rng=None, get_state=True):\n    \"\"\"\n    Save the necessary information into a dictionary\n    \"\"\"\n\n    m = {}\n    m['ninitfeats'] = S.ninitfeats\n    m['x0'] = S.x0\n    x = S.x.clone().cpu().detach()\n    m['feats'] = np.where(x.numpy() >= 0)[0]\n    m.update({k: v[0] for k, v in stop_conds.items()})\n    if get_state:\n        m.update({constants.Checkpoint.MODEL: S.state_dict(),\n                  constants.Checkpoint.OPT: S.opt_train.state_dict(),\n                  constants.Checkpoint.RNG: torch.get_rng_state(),\n                  })\n    if rng:\n        m.update({'rng_state': rng.get_state()})\n\n    return m",
  "def _train(data_train, Nminibatch, order, C, rng, lr_train, debug, maxiter,\n           maxtime, init, dftol_stop, freltol_stop, dn_log, accum_steps,\n           path_save, shuffle, device=constants.Device.CPU,\n           verbose=1,\n           prev_checkpoint=None,\n           groups=None,\n           soft_groups=None):\n    \"\"\"\n    Main training loop.\n    \"\"\"\n\n    t_init = time.time()\n\n    x0 = get_init(data_train, init, rng)\n    if isinstance(init, str) and init == constants.Initialization.ZERO:\n        ninitfeats = -1\n    else:\n        ninitfeats = np.where(x0.detach().numpy() > 0)[0].size\n\n    S = Solver(data_train, order,\n               Nminibatch=Nminibatch, x0=x0, C=C,\n               ftransform=lambda x: torch.sigmoid(2 * x),\n               get_train_opt=lambda p: torch.optim.Adam(p, lr_train),\n               rng=rng,\n               accum_steps=accum_steps,\n               shuffle=shuffle,\n               groups=groups,\n               soft_groups=soft_groups,\n               device=device,\n               verbose=verbose)\n    S = S.to(device)\n\n    S.ninitfeats = ninitfeats\n    S.x0 = x0\n\n    if prev_checkpoint:\n        S.load_state_dict(prev_checkpoint[constants.Checkpoint.MODEL])\n        S.opt_train.load_state_dict(prev_checkpoint[constants.Checkpoint.OPT])\n        torch.set_rng_state(prev_checkpoint[constants.Checkpoint.RNG])\n\n    minibatch = S.Ntrain != S.Nminibatch\n\n    f_stop, stop_conds = get_optim_f_stop(maxiter, maxtime, dftol_stop,\n                                          freltol_stop, minibatch=minibatch)\n\n    if debug:\n        pass\n    else:\n        f_callback = None\n    stop_conds['t'][-1] = time.time() - t_init\n\n    S.train(f_stop=f_stop, f_callback=f_callback)\n\n    return get_checkpoint(S, stop_conds, rng), S",
  "def train_sk_dense(ty, X, y, classification):\n    if classification:\n        if ty.startswith('skf'):\n            d = int(ty.replace('skf', ''))\n            f_sk = f_classif\n        elif ty.startswith('skmi'):\n            d = int(ty.replace('skmi', ''))\n            f_sk = mutual_info_classif\n    else:\n        if ty.startswith('skf'):\n            d = int(ty.replace('skf', ''))\n            f_sk = f_regression\n        elif ty.startswith('skmi'):\n            d = int(ty.replace('skmi', ''))\n            f_sk = mutual_info_regression\n    t = time.time()\n    clf = SelectKBest(f_sk, k=d)\n    clf.fit_transform(X, y.squeeze())\n    ix = np.argsort(-clf.scores_)\n    ix = ix[np.where(np.invert(np.isnan(clf.scores_[ix])))[0]][:d]\n    t = time.time() - t\n    return {'feats': ix, 't': t}",
  "def f_stop(f0, v0, it, t):\n\n        flag_stop = False\n\n        total_t[-1] += t\n        g = f0.x.grad.clone().cpu().detach()\n        df = g.abs().max().numpy().squeeze()\n        v = v0.clone().cpu().detach()\n        f = v.numpy().squeeze()\n\n        if it >= maxiter:\n            flag_stop = True\n\n        elif total_t[-1] >= maxtime:\n            flag_stop = True\n\n        f_ma.update(f)\n        df_ma.update(df)\n        rel_change = f_ma.relchange()\n\n        if ((not minibatch) and (df < dftol_stop)) \\\n           or (minibatch and (df_ma() < dftol_stop)):\n            flag_stop = True\n\n        if rel_change < freltol_stop:\n            flag_stop = True\n\n        if not minibatch:\n            df_store[-1] = df\n        else:\n            df_store[-1] = df_ma()\n        relchange_store[-1] = rel_change\n        it_store[-1] = it\n\n        return flag_stop",
  "class SMACClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('config_dedup'): bool\n        }).validate(kwargs)",
  "class SMACTuner(Tuner):\n    \"\"\"\n    This is a wrapper of [SMAC](https://github.com/automl/SMAC3) following NNI tuner interface.\n    It only supports ``SMAC`` mode, and does not support the multiple instances of SMAC3 (i.e.,\n    the same configuration is run multiple times).\n    \"\"\"\n    def __init__(self, optimize_mode=\"maximize\", config_dedup=False):\n        \"\"\"\n        Parameters\n        ----------\n        optimize_mode : str\n            Optimize mode, 'maximize' or 'minimize', by default 'maximize'\n        config_dedup : bool\n            If True, the tuner will not generate a configuration that has been already generated.\n            If False, a configuration may be generated twice, but it is rare for relatively large search space.\n        \"\"\"\n        self.logger = logger\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.total_data = {}\n        self.optimizer = None\n        self.smbo_solver = None\n        self.first_one = True\n        self.update_ss_done = False\n        self.loguniform_key = set()\n        self.categorical_dict = {}\n        self.cs = None\n        self.dedup = config_dedup\n\n    def _main_cli(self):\n        \"\"\"\n        Main function of SMAC for CLI interface. Some initializations of the wrapped SMAC are done\n        in this function.\n\n        Returns\n        -------\n        obj\n            The object of the SMAC optimizer\n        \"\"\"\n        self.logger.info(\"SMAC call: %s\", \" \".join(sys.argv))\n\n        cmd_reader = CMDReader()\n        args, _ = cmd_reader.read_cmd()\n\n        root_logger = logging.getLogger()\n        root_logger.setLevel(args.verbose_level)\n        logger_handler = logging.StreamHandler(stream=sys.stdout)\n        if root_logger.level >= logging.INFO:\n            formatter = logging.Formatter(\"%(levelname)s:\\t%(message)s\")\n        else:\n            formatter = logging.Formatter(\n                \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\",\n                \"%Y-%m-%d %H:%M:%S\")\n        logger_handler.setFormatter(formatter)\n        root_logger.addHandler(logger_handler)\n        # remove default handler\n        root_logger.removeHandler(root_logger.handlers[0])\n\n        # Create defaults\n        rh = None\n        initial_configs = None\n        stats = None\n        incumbent = None\n\n        # Create scenario-object\n        scen = Scenario(args.scenario_file, [])\n        self.cs = scen.cs\n\n        if args.mode == \"SMAC\":\n            optimizer = SMAC(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                stats=stats,\n                restore_incumbent=incumbent,\n                run_id=args.seed)\n        elif args.mode == \"ROAR\":\n            optimizer = ROAR(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                run_id=args.seed)\n        elif args.mode == \"EPILS\":\n            optimizer = EPILS(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                run_id=args.seed)\n        else:\n            optimizer = None\n\n        return optimizer\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Convert search_space to the format that ``SMAC3`` could recognize, thus, not all the search space types\n        are supported. In this function, we also do the initialization of `SMAC3`, i.e., calling ``self._main_cli``.\n\n        NOTE: updating search space during experiment running is not supported.\n\n        Parameters\n        ----------\n        search_space : dict\n            The format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        \"\"\"\n        self.logger.info('update search space in SMAC.')\n        if not self.update_ss_done:\n            self.categorical_dict = generate_scenario(search_space)\n            if self.categorical_dict is None:\n                raise RuntimeError('categorical dict is not correctly returned after parsing search space.')\n            # TODO: this is ugly, we put all the initialization work in this method, because initialization relies\n            #         on search space, also because update_search_space is called at the beginning.\n            self.optimizer = self._main_cli()\n            self.smbo_solver = self.optimizer.solver\n            self.loguniform_key = {key for key in search_space.keys() if search_space[key]['_type'] == 'loguniform'}\n            self.update_ss_done = True\n        else:\n            self.logger.warning('update search space is not supported.')\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Receive a trial's final performance result reported through :func:``nni.report_final_result`` by the trial.\n        GridSearchTuner does not need trial's results.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n\n        Raises\n        ------\n        RuntimeError\n            Received parameter id not in ``self.total_data``\n        \"\"\"\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        if self.first_one:\n            self.smbo_solver.nni_smac_receive_first_run(self.total_data[parameter_id], reward)\n            self.first_one = False\n        else:\n            self.smbo_solver.nni_smac_receive_runs(self.total_data[parameter_id], reward)\n\n    def param_postprocess(self, challenger_dict):\n        \"\"\"\n        Postprocessing for a set of hyperparameters includes:\n            1. Convert the values of type ``loguniform`` back to their initial range.\n            2. Convert ``categorical``: categorical values in search space are changed to list of numbers before,\n               those original values will be changed back in this function.\n\n        Parameters\n        ----------\n        challenger_dict : dict\n            challenger dict\n\n        Returns\n        -------\n        dict\n            dict which stores copy of challengers\n        \"\"\"\n        converted_dict = {}\n        for key, value in challenger_dict.items():\n            # convert to loguniform\n            if key in self.loguniform_key:\n                converted_dict[key] = np.exp(challenger_dict[key])\n            # convert categorical back to original value\n            elif key in self.categorical_dict:\n                idx = challenger_dict[key]\n                converted_dict[key] = self.categorical_dict[key][idx]\n            else:\n                converted_dict[key] = value\n        return converted_dict\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate one instance of hyperparameters (i.e., one configuration).\n        Get one from SMAC3's ``challengers``.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters. This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n        \"\"\"\n        if self.first_one:\n            init_challenger = self.smbo_solver.nni_smac_start()\n            self.total_data[parameter_id] = init_challenger\n            return self.param_postprocess(init_challenger.get_dictionary())\n        else:\n            challengers = self.smbo_solver.nni_smac_request_challengers()\n            challengers_empty = True\n            for challenger in challengers:\n                challengers_empty = False\n                if self.dedup:\n                    match = [v for k, v in self.total_data.items() \\\n                             if v.get_dictionary() == challenger.get_dictionary()]\n                    if match:\n                        continue\n                self.total_data[parameter_id] = challenger\n                return self.param_postprocess(challenger.get_dictionary())\n            assert challengers_empty is False, 'The case that challengers is empty is not handled.'\n            self.logger.info('In generate_parameters: No more new parameters.')\n            raise nni.NoMoreTrialError('No more new parameters.')\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Generate mutiple instances of hyperparameters. If it is a first request,\n        retrieve the instances from initial challengers. While if it is not, request\n        new challengers and retrieve instances from the requested challengers.\n\n        Parameters\n        ----------\n        parameter_id_list: list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        list\n            a list of newly generated configurations\n        \"\"\"\n        if self.first_one:\n            params = []\n            for one_id in parameter_id_list:\n                init_challenger = self.smbo_solver.nni_smac_start()\n                self.total_data[one_id] = init_challenger\n                params.append(self.param_postprocess(init_challenger.get_dictionary()))\n        else:\n            challengers = self.smbo_solver.nni_smac_request_challengers()\n            cnt = 0\n            params = []\n            for challenger in challengers:\n                if cnt >= len(parameter_id_list):\n                    break\n                if self.dedup:\n                    match = [v for k, v in self.total_data.items() \\\n                             if v.get_dictionary() == challenger.get_dictionary()]\n                    if match:\n                        continue\n                self.total_data[parameter_id_list[cnt]] = challenger\n                params.append(self.param_postprocess(challenger.get_dictionary()))\n                cnt += 1\n            if self.dedup and not params:\n                self.logger.info('In generate_multiple_parameters: No more new parameters.')\n        return params\n\n    def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning.\n\n        Parameters\n        ----------\n        data : list of dict\n            Each of which has at least two keys, ``parameter`` and ``value``.\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            self.logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            # simply validate data format\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                self.logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            _value = extract_scalar_reward(_value)\n            # convert the keys in loguniform and categorical types\n            valid_entry = True\n            for key, value in _params.items():\n                if key in self.loguniform_key:\n                    _params[key] = np.log(value)\n                elif key in self.categorical_dict:\n                    if value in self.categorical_dict[key]:\n                        _params[key] = self.categorical_dict[key].index(value)\n                    else:\n                        self.logger.info(\"The value %s of key %s is not in search space.\", str(value), key)\n                        valid_entry = False\n                        break\n            if not valid_entry:\n                continue\n            # start import this data entry\n            _completed_num += 1\n            config = Configuration(self.cs, values=_params)\n            if self.optimize_mode is OptimizeMode.Maximize:\n                _value = -_value\n            if self.first_one:\n                self.smbo_solver.nni_smac_receive_first_run(config, _value)\n                self.first_one = False\n            else:\n                self.smbo_solver.nni_smac_receive_runs(config, _value)\n        self.logger.info(\"Successfully import data to smac tuner, total data: %d, imported data: %d.\", len(data), _completed_num)",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('config_dedup'): bool\n        }).validate(kwargs)",
  "def __init__(self, optimize_mode=\"maximize\", config_dedup=False):\n        \"\"\"\n        Parameters\n        ----------\n        optimize_mode : str\n            Optimize mode, 'maximize' or 'minimize', by default 'maximize'\n        config_dedup : bool\n            If True, the tuner will not generate a configuration that has been already generated.\n            If False, a configuration may be generated twice, but it is rare for relatively large search space.\n        \"\"\"\n        self.logger = logger\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.total_data = {}\n        self.optimizer = None\n        self.smbo_solver = None\n        self.first_one = True\n        self.update_ss_done = False\n        self.loguniform_key = set()\n        self.categorical_dict = {}\n        self.cs = None\n        self.dedup = config_dedup",
  "def _main_cli(self):\n        \"\"\"\n        Main function of SMAC for CLI interface. Some initializations of the wrapped SMAC are done\n        in this function.\n\n        Returns\n        -------\n        obj\n            The object of the SMAC optimizer\n        \"\"\"\n        self.logger.info(\"SMAC call: %s\", \" \".join(sys.argv))\n\n        cmd_reader = CMDReader()\n        args, _ = cmd_reader.read_cmd()\n\n        root_logger = logging.getLogger()\n        root_logger.setLevel(args.verbose_level)\n        logger_handler = logging.StreamHandler(stream=sys.stdout)\n        if root_logger.level >= logging.INFO:\n            formatter = logging.Formatter(\"%(levelname)s:\\t%(message)s\")\n        else:\n            formatter = logging.Formatter(\n                \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\",\n                \"%Y-%m-%d %H:%M:%S\")\n        logger_handler.setFormatter(formatter)\n        root_logger.addHandler(logger_handler)\n        # remove default handler\n        root_logger.removeHandler(root_logger.handlers[0])\n\n        # Create defaults\n        rh = None\n        initial_configs = None\n        stats = None\n        incumbent = None\n\n        # Create scenario-object\n        scen = Scenario(args.scenario_file, [])\n        self.cs = scen.cs\n\n        if args.mode == \"SMAC\":\n            optimizer = SMAC(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                stats=stats,\n                restore_incumbent=incumbent,\n                run_id=args.seed)\n        elif args.mode == \"ROAR\":\n            optimizer = ROAR(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                run_id=args.seed)\n        elif args.mode == \"EPILS\":\n            optimizer = EPILS(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                run_id=args.seed)\n        else:\n            optimizer = None\n\n        return optimizer",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Convert search_space to the format that ``SMAC3`` could recognize, thus, not all the search space types\n        are supported. In this function, we also do the initialization of `SMAC3`, i.e., calling ``self._main_cli``.\n\n        NOTE: updating search space during experiment running is not supported.\n\n        Parameters\n        ----------\n        search_space : dict\n            The format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        \"\"\"\n        self.logger.info('update search space in SMAC.')\n        if not self.update_ss_done:\n            self.categorical_dict = generate_scenario(search_space)\n            if self.categorical_dict is None:\n                raise RuntimeError('categorical dict is not correctly returned after parsing search space.')\n            # TODO: this is ugly, we put all the initialization work in this method, because initialization relies\n            #         on search space, also because update_search_space is called at the beginning.\n            self.optimizer = self._main_cli()\n            self.smbo_solver = self.optimizer.solver\n            self.loguniform_key = {key for key in search_space.keys() if search_space[key]['_type'] == 'loguniform'}\n            self.update_ss_done = True\n        else:\n            self.logger.warning('update search space is not supported.')",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Receive a trial's final performance result reported through :func:``nni.report_final_result`` by the trial.\n        GridSearchTuner does not need trial's results.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n\n        Raises\n        ------\n        RuntimeError\n            Received parameter id not in ``self.total_data``\n        \"\"\"\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        if self.first_one:\n            self.smbo_solver.nni_smac_receive_first_run(self.total_data[parameter_id], reward)\n            self.first_one = False\n        else:\n            self.smbo_solver.nni_smac_receive_runs(self.total_data[parameter_id], reward)",
  "def param_postprocess(self, challenger_dict):\n        \"\"\"\n        Postprocessing for a set of hyperparameters includes:\n            1. Convert the values of type ``loguniform`` back to their initial range.\n            2. Convert ``categorical``: categorical values in search space are changed to list of numbers before,\n               those original values will be changed back in this function.\n\n        Parameters\n        ----------\n        challenger_dict : dict\n            challenger dict\n\n        Returns\n        -------\n        dict\n            dict which stores copy of challengers\n        \"\"\"\n        converted_dict = {}\n        for key, value in challenger_dict.items():\n            # convert to loguniform\n            if key in self.loguniform_key:\n                converted_dict[key] = np.exp(challenger_dict[key])\n            # convert categorical back to original value\n            elif key in self.categorical_dict:\n                idx = challenger_dict[key]\n                converted_dict[key] = self.categorical_dict[key][idx]\n            else:\n                converted_dict[key] = value\n        return converted_dict",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate one instance of hyperparameters (i.e., one configuration).\n        Get one from SMAC3's ``challengers``.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters. This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n        \"\"\"\n        if self.first_one:\n            init_challenger = self.smbo_solver.nni_smac_start()\n            self.total_data[parameter_id] = init_challenger\n            return self.param_postprocess(init_challenger.get_dictionary())\n        else:\n            challengers = self.smbo_solver.nni_smac_request_challengers()\n            challengers_empty = True\n            for challenger in challengers:\n                challengers_empty = False\n                if self.dedup:\n                    match = [v for k, v in self.total_data.items() \\\n                             if v.get_dictionary() == challenger.get_dictionary()]\n                    if match:\n                        continue\n                self.total_data[parameter_id] = challenger\n                return self.param_postprocess(challenger.get_dictionary())\n            assert challengers_empty is False, 'The case that challengers is empty is not handled.'\n            self.logger.info('In generate_parameters: No more new parameters.')\n            raise nni.NoMoreTrialError('No more new parameters.')",
  "def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Generate mutiple instances of hyperparameters. If it is a first request,\n        retrieve the instances from initial challengers. While if it is not, request\n        new challengers and retrieve instances from the requested challengers.\n\n        Parameters\n        ----------\n        parameter_id_list: list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        list\n            a list of newly generated configurations\n        \"\"\"\n        if self.first_one:\n            params = []\n            for one_id in parameter_id_list:\n                init_challenger = self.smbo_solver.nni_smac_start()\n                self.total_data[one_id] = init_challenger\n                params.append(self.param_postprocess(init_challenger.get_dictionary()))\n        else:\n            challengers = self.smbo_solver.nni_smac_request_challengers()\n            cnt = 0\n            params = []\n            for challenger in challengers:\n                if cnt >= len(parameter_id_list):\n                    break\n                if self.dedup:\n                    match = [v for k, v in self.total_data.items() \\\n                             if v.get_dictionary() == challenger.get_dictionary()]\n                    if match:\n                        continue\n                self.total_data[parameter_id_list[cnt]] = challenger\n                params.append(self.param_postprocess(challenger.get_dictionary()))\n                cnt += 1\n            if self.dedup and not params:\n                self.logger.info('In generate_multiple_parameters: No more new parameters.')\n        return params",
  "def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning.\n\n        Parameters\n        ----------\n        data : list of dict\n            Each of which has at least two keys, ``parameter`` and ``value``.\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            self.logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            # simply validate data format\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                self.logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            _value = extract_scalar_reward(_value)\n            # convert the keys in loguniform and categorical types\n            valid_entry = True\n            for key, value in _params.items():\n                if key in self.loguniform_key:\n                    _params[key] = np.log(value)\n                elif key in self.categorical_dict:\n                    if value in self.categorical_dict[key]:\n                        _params[key] = self.categorical_dict[key].index(value)\n                    else:\n                        self.logger.info(\"The value %s of key %s is not in search space.\", str(value), key)\n                        valid_entry = False\n                        break\n            if not valid_entry:\n                continue\n            # start import this data entry\n            _completed_num += 1\n            config = Configuration(self.cs, values=_params)\n            if self.optimize_mode is OptimizeMode.Maximize:\n                _value = -_value\n            if self.first_one:\n                self.smbo_solver.nni_smac_receive_first_run(config, _value)\n                self.first_one = False\n            else:\n                self.smbo_solver.nni_smac_receive_runs(config, _value)\n        self.logger.info(\"Successfully import data to smac tuner, total data: %d, imported data: %d.\", len(data), _completed_num)",
  "def get_json_content(file_path):\n    \"\"\"\n    Load json file content\n\n    Parameters\n    ----------\n    file_path:\n        path to the file\n\n    Raises\n    ------\n    TypeError\n        Error with the file path\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            return json.load(file)\n    except TypeError as err:\n        print('Error: ', err)\n        return None",
  "def generate_pcs(nni_search_space_content):\n    \"\"\"\n    Generate the Parameter Configuration Space (PCS) which defines the\n    legal ranges of the parameters to be optimized and their default values.\n    Generally, the format is:\n    # parameter_name categorical {value_1, ..., value_N} [default value]\n    # parameter_name ordinal {value_1, ..., value_N} [default value]\n    # parameter_name integer [min_value, max_value] [default value]\n    # parameter_name integer [min_value, max_value] [default value] log\n    # parameter_name real [min_value, max_value] [default value]\n    # parameter_name real [min_value, max_value] [default value] log\n    Reference: https://automl.github.io/SMAC3/stable/options.html\n\n    Parameters\n    ----------\n    nni_search_space_content: search_space\n        The search space in this experiment in nni\n\n    Returns\n    -------\n    Parameter Configuration Space (PCS)\n        the legal ranges of the parameters to be optimized and their default values\n\n    Raises\n    ------\n    RuntimeError\n        unsupported type or value error or incorrect search space\n    \"\"\"\n    categorical_dict = {}\n    search_space = nni_search_space_content\n\n    def dump_categorical(fd, key, categories):\n        choice_len = len(categories)\n        if key in categorical_dict:\n            raise RuntimeError(\n                '%s has already existed, please make sure search space has no duplicate key.' % key)\n        categorical_dict[key] = search_space[key]['_value']\n        fd.write('%s categorical {%s} [0]\\n' % (key, ','.join(map(str, range(choice_len)))))\n\n    with open('param_config_space.pcs', 'w') as pcs_fd:\n        if isinstance(search_space, dict):\n            for key in search_space.keys():\n                if isinstance(search_space[key], dict):\n                    try:\n                        if search_space[key]['_type'] == 'choice':\n                            dump_categorical(pcs_fd, key, search_space[key]['_value'])\n                        elif search_space[key]['_type'] == 'randint':\n                            lower, upper = search_space[key]['_value']\n                            if lower + 1 == upper:\n                                dump_categorical(pcs_fd, key, [lower])\n                            else:\n                                pcs_fd.write('%s integer [%d, %d] [%d]\\n' % (key, lower, upper - 1, lower))\n                        elif search_space[key]['_type'] == 'uniform':\n                            low, high = search_space[key]['_value']\n                            if low == high:\n                                dump_categorical(pcs_fd, key, [low])\n                            else:\n                                pcs_fd.write('%s real [%s, %s] [%s]\\n' % (key, low, high, low))\n                        elif search_space[key]['_type'] == 'loguniform':\n                            # use np.round here to ensure that the rounded default value is in the range,\n                            # which will be rounded in configure_space package\n                            low, high = list(np.round(np.log(search_space[key]['_value']), 10))\n                            if low == high:\n                                dump_categorical(pcs_fd, key, [search_space[key]['_value'][0]])\n                            else:\n                                pcs_fd.write('%s real [%s, %s] [%s]\\n' % (key, low, high, low))\n                        elif search_space[key]['_type'] == 'quniform':\n                            low, high, q = search_space[key]['_value'][0:3]\n                            vals = np.clip(np.arange(np.round(low / q), np.round(high / q) + 1) * q, low, high).tolist()\n                            pcs_fd.write('%s ordinal {%s} [%s]\\n' % (\n                                key,\n                                json.dumps(vals)[1:-1],\n                                json.dumps(vals[0])))\n                        else:\n                            raise RuntimeError('unsupported _type %s' % search_space[key]['_type'])\n                    except:\n                        raise RuntimeError('_type or _value error.')\n        else:\n            raise RuntimeError('incorrect search space.')\n        return categorical_dict\n    return None",
  "def generate_scenario(ss_content):\n    \"\"\"\n    Generate the scenario. The scenario-object (smac.scenario.scenario.Scenario) is used to configure SMAC and\n    can be constructed either by providing an actual scenario-object, or by specifing the options in a scenario file.\n    Reference: https://automl.github.io/SMAC3/stable/options.html\n    The format of the scenario file is one option per line:\n    OPTION1 = VALUE1\n    OPTION2 = VALUE2\n    ...\n    Parameters\n    ----------\n    abort_on_first_run_crash: bool\n        If true, SMAC will abort if the first run of the target algorithm crashes. Default: True,\n        because trials reported to nni tuner would always in success state\n    algo: function\n        Specifies the target algorithm call that SMAC will optimize. Interpreted as a bash-command.\n        Not required by tuner, but required by nni's training service for running trials\n    always_race_default:\n        Race new incumbents always against default configuration\n    cost_for_crash:\n        Defines the cost-value for crashed runs on scenarios with quality as run-obj. Default: 2147483647.0.\n        Trials reported to nni tuner would always in success state\n    cutoff_time:\n        Maximum runtime, after which the target algorithm is cancelled. `Required if *run_obj* is runtime`\n    deterministic: bool\n        If true, the optimization process will be repeatable.\n    execdir:\n        Specifies the path to the execution-directory. Default: .\n        Trials are executed by nni's training service\n    feature_file:\n        Specifies the file with the instance-features.\n        No features specified or feature file is not supported\n    initial_incumbent:\n        DEFAULT is the default from the PCS. Default: DEFAULT. Must be from: [\u2018DEFAULT\u2019, \u2018RANDOM\u2019].\n    input_psmac_dirs:\n        For parallel SMAC, multiple output-directories are used.\n        Parallelism is supported by nni\n    instance_file:\n        Specifies the file with the training-instances. Not supported\n    intensification_percentage:\n        The fraction of time to be used on intensification (versus choice of next Configurations). Default: 0.5.\n        Not supported, trials are controlled by nni's training service and kill be assessor\n    maxR: int\n        Maximum number of calls per configuration. Default: 2000.\n    memory_limit:\n        Maximum available memory the target algorithm can occupy before being cancelled.\n    minR: int\n        Minimum number of calls per configuration. Default: 1.\n    output_dir:\n        Specifies the output-directory for all emerging files, such as logging and results.\n        Default: smac3-output_2018-01-22_15:05:56_807070.\n    overall_obj:\n    \tPARX, where X is an integer defining the penalty imposed on timeouts (i.e. runtimes that exceed the cutoff-time).\n        Timeout is not supported\n    paramfile:\n        Specifies the path to the PCS-file.\n    run_obj:\n        Defines what metric to optimize. When optimizing runtime, cutoff_time is required as well.\n        Must be from: [\u2018runtime\u2019, \u2018quality\u2019].\n    runcount_limit: int\n        Maximum number of algorithm-calls during optimization. Default: inf.\n        Use default because this is controlled by nni\n    shared_model:\n        Whether to run SMAC in parallel mode. Parallelism is supported by nni\n    test_instance_file:\n        Specifies the file with the test-instances. Instance is not supported\n    tuner-timeout:\n        Maximum amount of CPU-time used for optimization. Not supported\n    wallclock_limit: int\n        Maximum amount of wallclock-time used for optimization. Default: inf.\n        Use default because this is controlled by nni\n\n    Returns\n    -------\n    Scenario:\n        The scenario-object (smac.scenario.scenario.Scenario) is used to configure SMAC and can be constructed\n        either by providing an actual scenario-object, or by specifing the options in a scenario file\n    \"\"\"\n    with open('scenario.txt', 'w') as sce_fd:\n        sce_fd.write('deterministic = 0\\n')\n        # sce_fd.write('output_dir = \\n')\n        sce_fd.write('paramfile = param_config_space.pcs\\n')\n        sce_fd.write('run_obj = quality\\n')\n\n    return generate_pcs(ss_content)",
  "def dump_categorical(fd, key, categories):\n        choice_len = len(categories)\n        if key in categorical_dict:\n            raise RuntimeError(\n                '%s has already existed, please make sure search space has no duplicate key.' % key)\n        categorical_dict[key] = search_space[key]['_value']\n        fd.write('%s categorical {%s} [0]\\n' % (key, ','.join(map(str, range(choice_len)))))",
  "def perturbation(hyperparameter_type, value, resample_probablity, uv, ub, lv, lb, random_state):\n    \"\"\"\n    Perturbation for hyperparameters\n\n    Parameters\n    ----------\n    hyperparameter_type : str\n        type of hyperparameter\n    value : list\n        parameters for sampling hyperparameter\n    resample_probability : float\n        probability for resampling\n    uv : float/int\n        upper value after perturbation\n    ub : float/int\n        upper bound\n    lv : float/int\n        lower value after perturbation\n    lb : float/int\n        lower bound\n    random_state : RandomState\n        random state\n    \"\"\"\n    if random.random() < resample_probablity:\n        if hyperparameter_type == \"choice\":\n            return value.index(nni.parameter_expressions.choice(value, random_state))\n        else:\n            return getattr(nni.parameter_expressions, hyperparameter_type)(*(value + [random_state]))\n    else:\n        if random.random() > 0.5:\n            return min(uv, ub)\n        else:\n            return max(lv, lb)",
  "def exploit_and_explore(bot_trial_info, top_trial_info, factor, resample_probability, epoch, search_space):\n    \"\"\"\n    Replace checkpoint of bot_trial with top, and perturb hyperparameters\n\n    Parameters\n    ----------\n    bot_trial_info : TrialInfo\n        bottom model whose parameters should be replaced\n    top_trial_info : TrialInfo\n        better model\n    factor : float\n        factor for perturbation\n    resample_probability : float\n        probability for resampling\n    epoch : int\n        step of PBTTuner\n    search_space : dict\n        search_space to keep perturbed hyperparameters in range\n    \"\"\"\n    bot_checkpoint_dir = bot_trial_info.checkpoint_dir\n    top_hyper_parameters = top_trial_info.hyper_parameters\n    hyper_parameters = copy.deepcopy(top_hyper_parameters)\n    random_state = np.random.RandomState()\n    hyper_parameters['load_checkpoint_dir'] = hyper_parameters['save_checkpoint_dir']\n    hyper_parameters['save_checkpoint_dir'] = os.path.join(bot_checkpoint_dir, str(epoch))\n    for key in hyper_parameters.keys():\n        hyper_parameter = hyper_parameters[key]\n        if key == 'load_checkpoint_dir' or key == 'save_checkpoint_dir':\n            continue\n        elif search_space[key][\"_type\"] == \"choice\":\n            choices = search_space[key][\"_value\"]\n            ub, uv = len(choices) - 1, choices.index(hyper_parameter) + 1\n            lb, lv = 0, choices.index(hyper_parameter) - 1\n        elif search_space[key][\"_type\"] == \"randint\":\n            lb, ub = search_space[key][\"_value\"][:2]\n            ub -= 1\n            uv = hyper_parameter + 1\n            lv = hyper_parameter - 1\n        elif search_space[key][\"_type\"] == \"uniform\":\n            lb, ub = search_space[key][\"_value\"][:2]\n            perturb = (ub - lb) * factor\n            uv = hyper_parameter + perturb\n            lv = hyper_parameter - perturb\n        elif search_space[key][\"_type\"] == \"quniform\":\n            lb, ub, q = search_space[key][\"_value\"][:3]\n            multi = round(hyper_parameter / q)\n            uv = (multi + 1) * q\n            lv = (multi - 1) * q\n        elif search_space[key][\"_type\"] == \"loguniform\":\n            lb, ub = search_space[key][\"_value\"][:2]\n            perturb = (np.log(ub) - np.log(lb)) * factor\n            uv = np.exp(min(np.log(hyper_parameter) + perturb, np.log(ub)))\n            lv = np.exp(max(np.log(hyper_parameter) - perturb, np.log(lb)))\n        elif search_space[key][\"_type\"] == \"qloguniform\":\n            lb, ub, q = search_space[key][\"_value\"][:3]\n            multi = round(hyper_parameter / q)\n            uv = (multi + 1) * q\n            lv = (multi - 1) * q\n        elif search_space[key][\"_type\"] == \"normal\":\n            sigma = search_space[key][\"_value\"][1]\n            perturb = sigma * factor\n            uv = ub = hyper_parameter + perturb\n            lv = lb = hyper_parameter - perturb\n        elif search_space[key][\"_type\"] == \"qnormal\":\n            q = search_space[key][\"_value\"][2]\n            uv = ub = hyper_parameter + q\n            lv = lb = hyper_parameter - q\n        elif search_space[key][\"_type\"] == \"lognormal\":\n            sigma = search_space[key][\"_value\"][1]\n            perturb = sigma * factor\n            uv = ub = np.exp(np.log(hyper_parameter) + perturb)\n            lv = lb = np.exp(np.log(hyper_parameter) - perturb)\n        elif search_space[key][\"_type\"] == \"qlognormal\":\n            q = search_space[key][\"_value\"][2]\n            uv = ub = hyper_parameter + q\n            lv, lb = hyper_parameter - q, 1E-10\n        else:\n            logger.warning(\"Illegal type to perturb: %s\", search_space[key][\"_type\"])\n            continue\n\n        if search_space[key][\"_type\"] == \"choice\":\n            idx = perturbation(search_space[key][\"_type\"], search_space[key][\"_value\"],\n                               resample_probability, uv, ub, lv, lb, random_state)\n            hyper_parameters[key] = choices[idx]\n        else:\n            hyper_parameters[key] = perturbation(search_space[key][\"_type\"], search_space[key][\"_value\"],\n                                                 resample_probability, uv, ub, lv, lb, random_state)\n    bot_trial_info.hyper_parameters = hyper_parameters\n    bot_trial_info.clean_id()",
  "class TrialInfo:\n    \"\"\"\n    Information of each trial, refresh for each epoch\n\n    \"\"\"\n\n    def __init__(self, checkpoint_dir=None, hyper_parameters=None, parameter_id=None, score=None):\n        self.checkpoint_dir = checkpoint_dir\n        self.hyper_parameters = hyper_parameters\n        self.parameter_id = parameter_id\n        self.score = score\n\n    def clean_id(self):\n        self.parameter_id = None",
  "class PBTClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('all_checkpoint_dir'): str,\n            Optional('population_size'): self.range('population_size', int, 0, 99999),\n            Optional('factors'): float,\n            Optional('fraction'): float,\n        }).validate(kwargs)",
  "class PBTTuner(Tuner):\n    def __init__(self, optimize_mode=\"maximize\", all_checkpoint_dir=None, population_size=10, factor=0.2,\n                 resample_probability=0.25, fraction=0.2):\n        \"\"\"\n        Initialization\n\n        Parameters\n        ----------\n        optimize_mode : str\n            maximize or minimize\n        all_checkpoint_dir : str\n            directory to store training model checkpoint\n        population_size : int\n            number of trials for each epoch\n        factor : float\n            factor for perturbation\n        resample_probability : float\n            probability for resampling\n        fraction : float\n            fraction for selecting bottom and top trials\n        \"\"\"\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        if all_checkpoint_dir is None:\n            all_checkpoint_dir = os.getenv('NNI_CHECKPOINT_DIRECTORY')\n            logger.info(\"Checkpoint dir is set to %s by default.\", all_checkpoint_dir)\n        self.all_checkpoint_dir = all_checkpoint_dir\n        self.population_size = population_size\n        self.factor = factor\n        self.resample_probability = resample_probability\n        self.fraction = fraction\n        # defined in trial code\n        #self.perturbation_interval = perturbation_interval\n\n        self.population = None\n        self.pos = -1\n        self.param_ids = []\n        self.running = {}\n        self.finished = []\n        self.credit = 0\n        self.finished_trials = 0\n        self.epoch = 0\n\n        self.searchspace_json = None\n        self.space = None\n\n        self.send_trial_callback = None\n\n        logger.info('PBT tuner initialization')\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Get search space\n\n        Parameters\n        ----------\n        search_space : dict\n            Search space\n        \"\"\"\n        logger.info('Update search space %s', search_space)\n        self.searchspace_json = search_space\n        self.space = json2space(self.searchspace_json)\n\n        self.random_state = np.random.RandomState()\n        self.population = []\n        is_rand = dict()\n\n        for item in self.space:\n            is_rand[item] = True\n\n        for i in range(self.population_size):\n            hyper_parameters = json2parameter(\n                self.searchspace_json, is_rand, self.random_state)\n            hyper_parameters = split_index(hyper_parameters)\n            checkpoint_dir = os.path.join(self.all_checkpoint_dir, str(i))\n            hyper_parameters['load_checkpoint_dir'] = os.path.join(checkpoint_dir, str(self.epoch))\n            hyper_parameters['save_checkpoint_dir'] = os.path.join(checkpoint_dir, str(self.epoch))\n            self.population.append(TrialInfo(checkpoint_dir=checkpoint_dir, hyper_parameters=hyper_parameters))\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Used for send_trial_callback.\n\n        Returns\n        -------\n        list\n            A list of newly generated configurations\n        \"\"\"\n        result = []\n        self.send_trial_callback = kwargs['st_callback']\n        for parameter_id in parameter_id_list:\n            had_exception = False\n            try:\n                logger.debug(\"generating param for %s\", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                had_exception = True\n            if not had_exception:\n                result.append(res)\n        return result\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate parameters, if no trial configration for now, self.credit plus 1 to send the config later\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters.\n            This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n\n        \"\"\"\n        if self.pos == self.population_size - 1:\n            logger.debug('Credit added by one in parameters request')\n            self.credit += 1\n            self.param_ids.append(parameter_id)\n            raise nni.NoMoreTrialError('No more parameters now.')\n        self.pos += 1\n        trial_info = self.population[self.pos]\n        trial_info.parameter_id = parameter_id\n        self.running[parameter_id] = trial_info\n        logger.info('Generate parameter : %s', trial_info.hyper_parameters)\n        return trial_info.hyper_parameters\n\n    def _proceed_next_epoch(self):\n        \"\"\"\n        \"\"\"\n        logger.info('Proceeding to next epoch')\n        self.epoch += 1\n        self.population = []\n        self.pos = -1\n        self.running = {}\n        #exploit and explore\n        reverse = True if self.optimize_mode == OptimizeMode.Maximize else False\n        self.finished = sorted(self.finished, key=lambda x: x.score, reverse=reverse)\n        cutoff = int(np.ceil(self.fraction * len(self.finished)))\n        tops = self.finished[:cutoff]\n        bottoms = self.finished[self.finished_trials - cutoff:]\n        for bottom in bottoms:\n            top = np.random.choice(tops)\n            exploit_and_explore(bottom, top, self.factor, self.resample_probability, self.epoch, self.searchspace_json)\n        for trial in self.finished:\n            if trial not in bottoms:\n                trial.clean_id()\n                trial.hyper_parameters['load_checkpoint_dir'] = trial.hyper_parameters['save_checkpoint_dir']\n                trial.hyper_parameters['save_checkpoint_dir'] = os.path.join(trial.checkpoint_dir, str(self.epoch))\n        self.finished_trials = 0\n        for _ in range(self.population_size):\n            trial_info = self.finished.pop()\n            self.population.append(trial_info)\n        while self.credit > 0 and self.pos + 1 < len(self.population):\n            self.credit -= 1\n            self.pos += 1\n            parameter_id = self.param_ids.pop()\n            trial_info = self.population[self.pos]\n            trial_info.parameter_id = parameter_id\n            self.running[parameter_id] = trial_info\n            self.send_trial_callback(parameter_id, trial_info.hyper_parameters)\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Receive trial's result. if the number of finished trials equals ``self.population_size``, start the next epoch to\n        train the model.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        \"\"\"\n        logger.info('Get one trial result, id = %d, value = %s', parameter_id, value)\n        value = extract_scalar_reward(value)\n        trial_info = self.running.pop(parameter_id, None)\n        trial_info.score = value\n        self.finished.append(trial_info)\n        self.finished_trials += 1\n        if self.finished_trials == self.population_size:\n            self._proceed_next_epoch()\n\n    def trial_end(self, parameter_id, success, **kwargs):\n        \"\"\"\n        Deal with trial failure\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        \"\"\"\n        if success:\n            return\n        if self.optimize_mode == OptimizeMode.Minimize:\n            value = float('inf')\n        else:\n            value = float('-inf')\n        trial_info = self.running.pop(parameter_id, None)\n        trial_info.score = value\n        self.finished.append(trial_info)\n        self.finished_trials += 1\n        if self.finished_trials == self.population_size:\n            self._proceed_next_epoch()\n\n    def import_data(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data : json obj\n            imported data records\n\n        Returns\n        -------\n        int\n            the start epoch number after data imported, only used for unittest\n        \"\"\"\n        if self.running:\n            logger.warning(\"Do not support importing data in the middle of experiment\")\n            return\n        # the following is for experiment resume\n        _completed_num = 0\n        epoch_data_dict = {}\n        for trial_info in data:\n            logger.info(\"Process data record %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            # simply validate data format\n            _params = trial_info[\"parameter\"]\n            _value = trial_info['value']\n            # assign fake value for failed trials\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                _value = float('inf') if self.optimize_mode == OptimizeMode.Minimize else float('-inf')\n            _value = extract_scalar_reward(_value)\n            if 'save_checkpoint_dir' not in _params:\n                logger.warning(\"Invalid data record: save_checkpoint_dir is missing, abandon data import.\")\n                return\n            epoch_num = int(os.path.basename(_params['save_checkpoint_dir']))\n            if epoch_num not in epoch_data_dict:\n                epoch_data_dict[epoch_num] = []\n            epoch_data_dict[epoch_num].append((_params, _value))\n        if not epoch_data_dict:\n            logger.warning(\"No valid epochs, abandon data import.\")\n            return\n        # figure out start epoch for resume\n        max_epoch_num = max(epoch_data_dict, key=int)\n        if len(epoch_data_dict[max_epoch_num]) < self.population_size:\n            max_epoch_num -= 1\n        # If there is no a single complete round, no data to import, start from scratch\n        if max_epoch_num < 0:\n            logger.warning(\"No completed epoch, abandon data import.\")\n            return\n        assert len(epoch_data_dict[max_epoch_num]) == self.population_size\n        # check existence of trial save checkpoint dir\n        for params, _ in epoch_data_dict[max_epoch_num]:\n            if not os.path.isdir(params['save_checkpoint_dir']):\n                logger.warning(\"save_checkpoint_dir %s does not exist, data will not be resumed\", params['save_checkpoint_dir'])\n                return\n        # resume data\n        self.epoch = max_epoch_num\n        self.finished_trials = self.population_size\n        for params, value in epoch_data_dict[max_epoch_num]:\n            checkpoint_dir = os.path.dirname(params['save_checkpoint_dir'])\n            self.finished.append(TrialInfo(checkpoint_dir=checkpoint_dir, hyper_parameters=params, score=value))\n        self._proceed_next_epoch()\n        logger.info(\"Successfully import data to PBT tuner, total data: %d, imported data: %d.\", len(data), self.population_size)\n        logger.info(\"Start from epoch %d ...\", self.epoch)\n        return self.epoch",
  "def __init__(self, checkpoint_dir=None, hyper_parameters=None, parameter_id=None, score=None):\n        self.checkpoint_dir = checkpoint_dir\n        self.hyper_parameters = hyper_parameters\n        self.parameter_id = parameter_id\n        self.score = score",
  "def clean_id(self):\n        self.parameter_id = None",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('all_checkpoint_dir'): str,\n            Optional('population_size'): self.range('population_size', int, 0, 99999),\n            Optional('factors'): float,\n            Optional('fraction'): float,\n        }).validate(kwargs)",
  "def __init__(self, optimize_mode=\"maximize\", all_checkpoint_dir=None, population_size=10, factor=0.2,\n                 resample_probability=0.25, fraction=0.2):\n        \"\"\"\n        Initialization\n\n        Parameters\n        ----------\n        optimize_mode : str\n            maximize or minimize\n        all_checkpoint_dir : str\n            directory to store training model checkpoint\n        population_size : int\n            number of trials for each epoch\n        factor : float\n            factor for perturbation\n        resample_probability : float\n            probability for resampling\n        fraction : float\n            fraction for selecting bottom and top trials\n        \"\"\"\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        if all_checkpoint_dir is None:\n            all_checkpoint_dir = os.getenv('NNI_CHECKPOINT_DIRECTORY')\n            logger.info(\"Checkpoint dir is set to %s by default.\", all_checkpoint_dir)\n        self.all_checkpoint_dir = all_checkpoint_dir\n        self.population_size = population_size\n        self.factor = factor\n        self.resample_probability = resample_probability\n        self.fraction = fraction\n        # defined in trial code\n        #self.perturbation_interval = perturbation_interval\n\n        self.population = None\n        self.pos = -1\n        self.param_ids = []\n        self.running = {}\n        self.finished = []\n        self.credit = 0\n        self.finished_trials = 0\n        self.epoch = 0\n\n        self.searchspace_json = None\n        self.space = None\n\n        self.send_trial_callback = None\n\n        logger.info('PBT tuner initialization')",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Get search space\n\n        Parameters\n        ----------\n        search_space : dict\n            Search space\n        \"\"\"\n        logger.info('Update search space %s', search_space)\n        self.searchspace_json = search_space\n        self.space = json2space(self.searchspace_json)\n\n        self.random_state = np.random.RandomState()\n        self.population = []\n        is_rand = dict()\n\n        for item in self.space:\n            is_rand[item] = True\n\n        for i in range(self.population_size):\n            hyper_parameters = json2parameter(\n                self.searchspace_json, is_rand, self.random_state)\n            hyper_parameters = split_index(hyper_parameters)\n            checkpoint_dir = os.path.join(self.all_checkpoint_dir, str(i))\n            hyper_parameters['load_checkpoint_dir'] = os.path.join(checkpoint_dir, str(self.epoch))\n            hyper_parameters['save_checkpoint_dir'] = os.path.join(checkpoint_dir, str(self.epoch))\n            self.population.append(TrialInfo(checkpoint_dir=checkpoint_dir, hyper_parameters=hyper_parameters))",
  "def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Used for send_trial_callback.\n\n        Returns\n        -------\n        list\n            A list of newly generated configurations\n        \"\"\"\n        result = []\n        self.send_trial_callback = kwargs['st_callback']\n        for parameter_id in parameter_id_list:\n            had_exception = False\n            try:\n                logger.debug(\"generating param for %s\", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                had_exception = True\n            if not had_exception:\n                result.append(res)\n        return result",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate parameters, if no trial configration for now, self.credit plus 1 to send the config later\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters.\n            This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n\n        \"\"\"\n        if self.pos == self.population_size - 1:\n            logger.debug('Credit added by one in parameters request')\n            self.credit += 1\n            self.param_ids.append(parameter_id)\n            raise nni.NoMoreTrialError('No more parameters now.')\n        self.pos += 1\n        trial_info = self.population[self.pos]\n        trial_info.parameter_id = parameter_id\n        self.running[parameter_id] = trial_info\n        logger.info('Generate parameter : %s', trial_info.hyper_parameters)\n        return trial_info.hyper_parameters",
  "def _proceed_next_epoch(self):\n        \"\"\"\n        \"\"\"\n        logger.info('Proceeding to next epoch')\n        self.epoch += 1\n        self.population = []\n        self.pos = -1\n        self.running = {}\n        #exploit and explore\n        reverse = True if self.optimize_mode == OptimizeMode.Maximize else False\n        self.finished = sorted(self.finished, key=lambda x: x.score, reverse=reverse)\n        cutoff = int(np.ceil(self.fraction * len(self.finished)))\n        tops = self.finished[:cutoff]\n        bottoms = self.finished[self.finished_trials - cutoff:]\n        for bottom in bottoms:\n            top = np.random.choice(tops)\n            exploit_and_explore(bottom, top, self.factor, self.resample_probability, self.epoch, self.searchspace_json)\n        for trial in self.finished:\n            if trial not in bottoms:\n                trial.clean_id()\n                trial.hyper_parameters['load_checkpoint_dir'] = trial.hyper_parameters['save_checkpoint_dir']\n                trial.hyper_parameters['save_checkpoint_dir'] = os.path.join(trial.checkpoint_dir, str(self.epoch))\n        self.finished_trials = 0\n        for _ in range(self.population_size):\n            trial_info = self.finished.pop()\n            self.population.append(trial_info)\n        while self.credit > 0 and self.pos + 1 < len(self.population):\n            self.credit -= 1\n            self.pos += 1\n            parameter_id = self.param_ids.pop()\n            trial_info = self.population[self.pos]\n            trial_info.parameter_id = parameter_id\n            self.running[parameter_id] = trial_info\n            self.send_trial_callback(parameter_id, trial_info.hyper_parameters)",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Receive trial's result. if the number of finished trials equals ``self.population_size``, start the next epoch to\n        train the model.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        \"\"\"\n        logger.info('Get one trial result, id = %d, value = %s', parameter_id, value)\n        value = extract_scalar_reward(value)\n        trial_info = self.running.pop(parameter_id, None)\n        trial_info.score = value\n        self.finished.append(trial_info)\n        self.finished_trials += 1\n        if self.finished_trials == self.population_size:\n            self._proceed_next_epoch()",
  "def trial_end(self, parameter_id, success, **kwargs):\n        \"\"\"\n        Deal with trial failure\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        \"\"\"\n        if success:\n            return\n        if self.optimize_mode == OptimizeMode.Minimize:\n            value = float('inf')\n        else:\n            value = float('-inf')\n        trial_info = self.running.pop(parameter_id, None)\n        trial_info.score = value\n        self.finished.append(trial_info)\n        self.finished_trials += 1\n        if self.finished_trials == self.population_size:\n            self._proceed_next_epoch()",
  "def import_data(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data : json obj\n            imported data records\n\n        Returns\n        -------\n        int\n            the start epoch number after data imported, only used for unittest\n        \"\"\"\n        if self.running:\n            logger.warning(\"Do not support importing data in the middle of experiment\")\n            return\n        # the following is for experiment resume\n        _completed_num = 0\n        epoch_data_dict = {}\n        for trial_info in data:\n            logger.info(\"Process data record %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            # simply validate data format\n            _params = trial_info[\"parameter\"]\n            _value = trial_info['value']\n            # assign fake value for failed trials\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                _value = float('inf') if self.optimize_mode == OptimizeMode.Minimize else float('-inf')\n            _value = extract_scalar_reward(_value)\n            if 'save_checkpoint_dir' not in _params:\n                logger.warning(\"Invalid data record: save_checkpoint_dir is missing, abandon data import.\")\n                return\n            epoch_num = int(os.path.basename(_params['save_checkpoint_dir']))\n            if epoch_num not in epoch_data_dict:\n                epoch_data_dict[epoch_num] = []\n            epoch_data_dict[epoch_num].append((_params, _value))\n        if not epoch_data_dict:\n            logger.warning(\"No valid epochs, abandon data import.\")\n            return\n        # figure out start epoch for resume\n        max_epoch_num = max(epoch_data_dict, key=int)\n        if len(epoch_data_dict[max_epoch_num]) < self.population_size:\n            max_epoch_num -= 1\n        # If there is no a single complete round, no data to import, start from scratch\n        if max_epoch_num < 0:\n            logger.warning(\"No completed epoch, abandon data import.\")\n            return\n        assert len(epoch_data_dict[max_epoch_num]) == self.population_size\n        # check existence of trial save checkpoint dir\n        for params, _ in epoch_data_dict[max_epoch_num]:\n            if not os.path.isdir(params['save_checkpoint_dir']):\n                logger.warning(\"save_checkpoint_dir %s does not exist, data will not be resumed\", params['save_checkpoint_dir'])\n                return\n        # resume data\n        self.epoch = max_epoch_num\n        self.finished_trials = self.population_size\n        for params, value in epoch_data_dict[max_epoch_num]:\n            checkpoint_dir = os.path.dirname(params['save_checkpoint_dir'])\n            self.finished.append(TrialInfo(checkpoint_dir=checkpoint_dir, hyper_parameters=params, score=value))\n        self._proceed_next_epoch()\n        logger.info(\"Successfully import data to PBT tuner, total data: %d, imported data: %d.\", len(data), self.population_size)\n        logger.info(\"Start from epoch %d ...\", self.epoch)\n        return self.epoch",
  "def match_val_type(vals, vals_bounds, vals_types):\n    '''\n    Update values in the array, to match their corresponding type\n    '''\n    vals_new = []\n\n    for i, _ in enumerate(vals_types):\n        if vals_types[i] == \"discrete_int\":\n            # Find the closest integer in the array, vals_bounds\n            # pylint: disable=cell-var-from-loop\n            vals_new.append(min(vals_bounds[i], key=lambda x: abs(x - vals[i])))\n        elif vals_types[i] == \"range_int\":\n            # Round down to the nearest integer\n            vals_new.append(math.floor(vals[i]))\n        elif vals_types[i] == \"range_continuous\":\n            # Don't do any processing for continous numbers\n            vals_new.append(vals[i])\n        else:\n            return None\n\n    return vals_new",
  "def rand(x_bounds, x_types):\n    '''\n    Random generate variable value within their bounds\n    '''\n    outputs = []\n\n    for i, _ in enumerate(x_bounds):\n        if x_types[i] == \"discrete_int\":\n            temp = x_bounds[i][random.randint(0, len(x_bounds[i]) - 1)]\n            outputs.append(temp)\n        elif x_types[i] == \"range_int\":\n            temp = random.randint(x_bounds[i][0], x_bounds[i][1] - 1)\n            outputs.append(temp)\n        elif x_types[i] == \"range_continuous\":\n            temp = random.uniform(x_bounds[i][0], x_bounds[i][1])\n            outputs.append(temp)\n        else:\n            return None\n\n    return outputs",
  "def check_feasibility(x_bounds, lowerbound, upperbound):\n    '''\n    This can have false positives.\n    For examples, parameters can only be 0 or 5, and the summation constraint is between 6 and 7.\n    '''\n    # x_bounds should be sorted, so even for \"discrete_int\" type,\n    # the smallest and the largest number should the first and the last element\n    x_bounds_lowerbound = sum([x_bound[0] for x_bound in x_bounds])\n    x_bounds_upperbound = sum([x_bound[-1] for x_bound in x_bounds])\n\n    # return ((x_bounds_lowerbound <= lowerbound) and (x_bounds_upperbound >= lowerbound)) or \\\n    #        ((x_bounds_lowerbound <= upperbound) and (x_bounds_upperbound >= upperbound))\n    return (x_bounds_lowerbound <= lowerbound <= x_bounds_upperbound) or \\\n           (x_bounds_lowerbound <= upperbound <= x_bounds_upperbound)",
  "def rand(x_bounds, x_types, lowerbound, upperbound, max_retries=100):\n    '''\n    Key idea is that we try to move towards upperbound, by randomly choose one\n    value for each parameter. However, for the last parameter,\n    we need to make sure that its value can help us get above lowerbound\n    '''\n    outputs = None\n\n    if check_feasibility(x_bounds, lowerbound, upperbound) is True:\n        # Order parameters by their range size. We want the smallest range first,\n        # because the corresponding parameter has less numbers to choose from\n        x_idx_sorted = []\n        for i, _ in enumerate(x_bounds):\n            if x_types[i] == \"discrete_int\":\n                x_idx_sorted.append([i, len(x_bounds[i])])\n            elif (x_types[i] == \"range_int\") or (x_types[i] == \"range_continuous\"):\n                x_idx_sorted.append(\n                    [i, math.floor(x_bounds[i][1] - x_bounds[i][0])])\n        x_idx_sorted = sorted(x_idx_sorted, key=itemgetter(1))\n\n        for _ in range(max_retries):\n            budget_allocated = 0\n            outputs = [None] * len(x_bounds)\n\n            for i, _ in enumerate(x_idx_sorted):\n                x_idx = x_idx_sorted[i][0]\n                # The amount of unallocated space that we have\n                budget_max = upperbound - budget_allocated\n                # NOT the Last x that we need to assign a random number\n                if i < (len(x_idx_sorted) - 1):\n                    if x_bounds[x_idx][0] <= budget_max:\n                        if x_types[x_idx] == \"discrete_int\":\n                            # Note the valid integer\n                            temp = []\n                            for j in x_bounds[x_idx]:\n                                if j <= budget_max:\n                                    temp.append(j)\n                            # Randomly pick a number from the integer array\n                            if temp:\n                                outputs[x_idx] = temp[random.randint(\n                                    0, len(temp) - 1)]\n\n                        elif (x_types[x_idx] == \"range_int\") or \\\n                                (x_types[x_idx] == \"range_continuous\"):\n                            outputs[x_idx] = random.randint(\n                                x_bounds[x_idx][0], min(x_bounds[x_idx][-1], budget_max))\n\n                else:\n                    # The last x that we need to assign a random number\n                    randint_lowerbound = lowerbound - budget_allocated\n                    randint_lowerbound = 0 if randint_lowerbound < 0 else randint_lowerbound\n\n                    # This check:\n                    # is our smallest possible value going to overflow the available budget space,\n                    # and is our largest possible value going to underflow the\n                    # lower bound\n                    if (x_bounds[x_idx][0] <= budget_max) and \\\n                            (x_bounds[x_idx][-1] >= randint_lowerbound):\n                        if x_types[x_idx] == \"discrete_int\":\n                            temp = []\n                            for j in x_bounds[x_idx]:\n                                # if (j <= budget_max) and (j >=\n                                # randint_lowerbound):\n                                if randint_lowerbound <= j <= budget_max:\n                                    temp.append(j)\n                            if temp:\n                                outputs[x_idx] = temp[random.randint(\n                                    0, len(temp) - 1)]\n                        elif (x_types[x_idx] == \"range_int\") or \\\n                                (x_types[x_idx] == \"range_continuous\"):\n                            outputs[x_idx] = random.randint(\n                                randint_lowerbound, min(\n                                    x_bounds[x_idx][1], budget_max))\n                if outputs[x_idx] is None:\n                    break\n                budget_allocated += outputs[x_idx]\n            if None not in outputs:\n                break\n    return outputs",
  "class MetisClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('no_resampling'): bool,\n            Optional('no_candidates'): bool,\n            Optional('selection_num_starting_points'): int,\n            Optional('cold_start_num'): int,\n        }).validate(kwargs)",
  "class MetisTuner(Tuner):\n    \"\"\"\n    Metis Tuner\n\n    More algorithm information you could reference here:\n    https://www.microsoft.com/en-us/research/publication/metis-robustly-tuning-tail-latencies-cloud-systems/\n\n    Attributes\n    ----------\n        optimize_mode : str\n            optimize_mode is a string that including two mode \"maximize\" and \"minimize\"\n\n        no_resampling : bool\n            True or False.\n            Should Metis consider re-sampling as part of the search strategy?\n            If you are confident that the training dataset is noise-free,\n            then you do not need re-sampling.\n\n        no_candidates : bool\n            True or False.\n            Should Metis suggest parameters for the next benchmark?\n            If you do not plan to do more benchmarks,\n            Metis can skip this step.\n\n        selection_num_starting_points : int\n            How many times Metis should try to find the global optimal in the search space?\n            The higher the number, the longer it takes to output the solution.\n\n        cold_start_num : int\n            Metis need some trial result to get cold start.\n            when the number of trial result is less than\n            cold_start_num, Metis will randomly sample hyper-parameter for trial.\n\n        exploration_probability: float\n            The probability of Metis to select parameter from exploration instead of exploitation.\n    \"\"\"\n\n    def __init__(\n            self,\n            optimize_mode=\"maximize\",\n            no_resampling=True,\n            no_candidates=False,\n            selection_num_starting_points=600,\n            cold_start_num=10,\n            exploration_probability=0.9):\n        \"\"\"\n        Parameters\n        ----------\n        optimize_mode : str\n            optimize_mode is a string that including two mode \"maximize\" and \"minimize\"\n\n        no_resampling : bool\n            True or False.\n            Should Metis consider re-sampling as part of the search strategy?\n            If you are confident that the training dataset is noise-free,\n            then you do not need re-sampling.\n\n        no_candidates : bool\n            True or False.\n            Should Metis suggest parameters for the next benchmark?\n            If you do not plan to do more benchmarks,\n            Metis can skip this step.\n\n        selection_num_starting_points : int\n            How many times Metis should try to find the global optimal in the search space?\n            The higher the number, the longer it takes to output the solution.\n\n        cold_start_num : int\n            Metis need some trial result to get cold start.\n            when the number of trial result is less than\n            cold_start_num, Metis will randomly sample hyper-parameter for trial.\n\n        exploration_probability : float\n            The probability of Metis to select parameter from exploration instead of exploitation.\n\n        x_bounds : list\n            The constration of parameters.\n\n        x_types : list\n            The type of parameters.\n        \"\"\"\n\n        self.samples_x = []\n        self.samples_y = []\n        self.samples_y_aggregation = []\n        self.total_data = []\n        self.space = None\n        self.no_resampling = no_resampling\n        self.no_candidates = no_candidates\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.key_order = []\n        self.cold_start_num = cold_start_num\n        self.selection_num_starting_points = selection_num_starting_points\n        self.exploration_probability = exploration_probability\n        self.minimize_constraints_fun = None\n        self.minimize_starting_points = None\n        self.supplement_data_num = 0\n        self.x_bounds = []\n        self.x_types = []\n\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Update the self.x_bounds and self.x_types by the search_space.json\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self.x_bounds = [[] for i in range(len(search_space))]\n        self.x_types = [NONE_TYPE for i in range(len(search_space))]\n\n        for key in search_space:\n            self.key_order.append(key)\n\n        key_type = {}\n        if isinstance(search_space, dict):\n            for key in search_space:\n                key_type = search_space[key]['_type']\n                key_range = search_space[key]['_value']\n                idx = self.key_order.index(key)\n                if key_type == 'quniform':\n                    if key_range[2] == 1 and key_range[0].is_integer(\n                    ) and key_range[1].is_integer():\n                        self.x_bounds[idx] = [key_range[0], key_range[1] + 1]\n                        self.x_types[idx] = 'range_int'\n                    else:\n                        low, high, q = key_range\n                        bounds = np.clip(\n                            np.arange(\n                                np.round(\n                                    low / q),\n                                np.round(\n                                    high / q) + 1) * q,\n                            low,\n                            high)\n                        self.x_bounds[idx] = bounds\n                        self.x_types[idx] = 'discrete_int'\n                elif key_type == 'randint':\n                    self.x_bounds[idx] = [key_range[0], key_range[1]]\n                    self.x_types[idx] = 'range_int'\n                elif key_type == 'uniform':\n                    self.x_bounds[idx] = [key_range[0], key_range[1]]\n                    self.x_types[idx] = 'range_continuous'\n                elif key_type == 'choice':\n                    self.x_bounds[idx] = key_range\n\n                    for key_value in key_range:\n                        if not isinstance(key_value, (int, float)):\n                            raise RuntimeError(\n                                \"Metis Tuner only support numerical choice.\")\n\n                    self.x_types[idx] = 'discrete_int'\n                else:\n                    logger.info(\n                        \"Metis Tuner doesn't support this kind of variable: %s\",\n                        str(key_type))\n                    raise RuntimeError(\n                        \"Metis Tuner doesn't support this kind of variable: %s\" %\n                        str(key_type))\n        else:\n            logger.info(\"The format of search space is not a dict.\")\n            raise RuntimeError(\"The format of search space is not a dict.\")\n\n        self.minimize_starting_points = _rand_init(\n            self.x_bounds, self.x_types, self.selection_num_starting_points)\n\n\n    def _pack_output(self, init_parameter):\n        \"\"\"\n        Pack the output\n\n        Parameters\n        ----------\n        init_parameter : dict\n\n        Returns\n        -------\n        output : dict\n        \"\"\"\n        output = {}\n        for i, param in enumerate(init_parameter):\n            output[self.key_order[i]] = param\n\n        return output\n\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate next parameter for trial\n\n        If the number of trial result is lower than cold start number,\n        metis will first random generate some parameters.\n        Otherwise, metis will choose the parameters by\n        the Gussian Process Model and the Gussian Mixture Model.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        result : dict\n        \"\"\"\n        if len(self.samples_x) < self.cold_start_num:\n            init_parameter = _rand_init(self.x_bounds, self.x_types, 1)[0]\n            results = self._pack_output(init_parameter)\n        else:\n            self.minimize_starting_points = _rand_init(\n                self.x_bounds, self.x_types, self.selection_num_starting_points)\n            results = self._selection(\n                self.samples_x,\n                self.samples_y_aggregation,\n                self.samples_y,\n                self.x_bounds,\n                self.x_types,\n                threshold_samplessize_resampling=(\n                    None if self.no_resampling is True else 50),\n                no_candidates=self.no_candidates,\n                minimize_starting_points=self.minimize_starting_points,\n                minimize_constraints_fun=self.minimize_constraints_fun)\n\n        logger.info(\"Generate paramageters: \\n%s\", str(results))\n        return results\n\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Tuner receive result from trial.\n\n        Parameters\n        ----------\n        parameter_id : int\n            The id of parameters, generated by nni manager.\n        parameters : dict\n            A group of parameters that trial has tried.\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n        \"\"\"\n        value = extract_scalar_reward(value)\n        if self.optimize_mode == OptimizeMode.Maximize:\n            value = -value\n\n        logger.info(\"Received trial result.\")\n        logger.info(\"value is : %s\", str(value))\n        logger.info(\"parameter is : %s\", str(parameters))\n\n        # parse parameter to sample_x\n        sample_x = [0 for i in range(len(self.key_order))]\n        for key in parameters:\n            idx = self.key_order.index(key)\n            sample_x[idx] = parameters[key]\n\n        # parse value to sample_y\n        temp_y = []\n        if sample_x in self.samples_x:\n            idx = self.samples_x.index(sample_x)\n            temp_y = self.samples_y[idx]\n            temp_y.append(value)\n            self.samples_y[idx] = temp_y\n\n            # calculate y aggregation\n            median = get_median(temp_y)\n            self.samples_y_aggregation[idx] = [median]\n        else:\n            self.samples_x.append(sample_x)\n            self.samples_y.append([value])\n\n            # calculate y aggregation\n            self.samples_y_aggregation.append([value])\n\n\n    def _selection(\n            self,\n            samples_x,\n            samples_y_aggregation,\n            samples_y,\n            x_bounds,\n            x_types,\n            max_resampling_per_x=3,\n            threshold_samplessize_exploitation=12,\n            threshold_samplessize_resampling=50,\n            no_candidates=False,\n            minimize_starting_points=None,\n            minimize_constraints_fun=None):\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n\n        next_candidate = None\n        candidates = []\n        samples_size_all = sum([len(i) for i in samples_y])\n        samples_size_unique = len(samples_y)\n\n        # ===== STEP 1: Compute the current optimum =====\n        gp_model = gp_create_model.create_model(\n            samples_x, samples_y_aggregation)\n        lm_current = gp_selection.selection(\n            \"lm\",\n            samples_y_aggregation,\n            x_bounds,\n            x_types,\n            gp_model['model'],\n            minimize_starting_points,\n            minimize_constraints_fun=minimize_constraints_fun)\n        if not lm_current:\n            return None\n        logger.info({\n            'hyperparameter': lm_current['hyperparameter'],\n            'expected_mu': lm_current['expected_mu'],\n            'expected_sigma': lm_current['expected_sigma'],\n            'reason': \"exploitation_gp\"\n        })\n\n        if no_candidates is False:\n            # ===== STEP 2: Get recommended configurations for exploration ====\n            results_exploration = gp_selection.selection(\n                \"lc\",\n                samples_y_aggregation,\n                x_bounds,\n                x_types,\n                gp_model['model'],\n                minimize_starting_points,\n                minimize_constraints_fun=minimize_constraints_fun)\n\n            if results_exploration is not None:\n                if _num_past_samples(results_exploration['hyperparameter'], samples_x, samples_y) == 0:\n                    temp_candidate = {\n                        'hyperparameter': results_exploration['hyperparameter'],\n                        'expected_mu': results_exploration['expected_mu'],\n                        'expected_sigma': results_exploration['expected_sigma'],\n                        'reason': \"exploration\"\n                    }\n                    candidates.append(temp_candidate)\n\n                    logger.info(\"DEBUG: 1 exploration candidate selected\\n\")\n                    logger.info(temp_candidate)\n            else:\n                logger.info(\"DEBUG: No suitable exploration candidates were\")\n\n            # ===== STEP 3: Get recommended configurations for exploitation ===\n            if samples_size_all >= threshold_samplessize_exploitation:\n                logger.info(\"Getting candidates for exploitation...\\n\")\n                try:\n                    gmm = gmm_create_model.create_model(\n                        samples_x, samples_y_aggregation)\n\n                    if (\"discrete_int\" in x_types) or (\"range_int\" in x_types):\n                        results_exploitation = gmm_selection.selection(\n                            x_bounds,\n                            x_types,\n                            gmm['clusteringmodel_good'],\n                            gmm['clusteringmodel_bad'],\n                            minimize_starting_points,\n                            minimize_constraints_fun=minimize_constraints_fun)\n                    else:\n                        # If all parameters are of \"range_continuous\",\n                        # let's use GMM to generate random starting points\n                        results_exploitation = gmm_selection.selection_r(\n                            x_bounds,\n                            x_types,\n                            gmm['clusteringmodel_good'],\n                            gmm['clusteringmodel_bad'],\n                            num_starting_points=self.selection_num_starting_points,\n                            minimize_constraints_fun=minimize_constraints_fun)\n\n                    if results_exploitation is not None:\n                        if _num_past_samples(results_exploitation['hyperparameter'], samples_x, samples_y) == 0:\n                            temp_expected_mu, temp_expected_sigma = \\\n                                    gp_prediction.predict(results_exploitation['hyperparameter'], gp_model['model'])\n                            temp_candidate = {\n                                'hyperparameter': results_exploitation['hyperparameter'],\n                                'expected_mu': temp_expected_mu,\n                                'expected_sigma': temp_expected_sigma,\n                                'reason': \"exploitation_gmm\"\n                            }\n                            candidates.append(temp_candidate)\n\n                            logger.info(\n                                \"DEBUG: 1 exploitation_gmm candidate selected\\n\")\n                            logger.info(temp_candidate)\n                    else:\n                        logger.info(\n                            \"DEBUG: No suitable exploitation_gmm candidates were found\\n\")\n\n                except ValueError as exception:\n                    # The exception: ValueError: Fitting the mixture model failed\n                    # because some components have ill-defined empirical covariance\n                    # (for instance caused by singleton or collapsed samples).\n                    # Try to decrease the number of components, or increase\n                    # reg_covar.\n                    logger.info(\n                        \"DEBUG: No suitable exploitation_gmm \\\n                        candidates were found due to exception.\")\n                    logger.info(exception)\n\n            # ===== STEP 4: Get a list of outliers =====\n            if (threshold_samplessize_resampling is not None) and \\\n                    (samples_size_unique >= threshold_samplessize_resampling):\n                logger.info(\"Getting candidates for re-sampling...\\n\")\n                results_outliers = gp_outlier_detection.outlierDetection_threaded(\n                    samples_x, samples_y_aggregation)\n\n                if results_outliers is not None:\n                    for results_outlier in results_outliers:  # pylint: disable=not-an-iterable\n                        if _num_past_samples(samples_x[results_outlier['samples_idx']], samples_x, samples_y) < max_resampling_per_x:\n                            temp_candidate = {'hyperparameter': samples_x[results_outlier['samples_idx']],\\\n                                               'expected_mu': results_outlier['expected_mu'],\\\n                                               'expected_sigma': results_outlier['expected_sigma'],\\\n                                               'reason': \"resampling\"}\n                            candidates.append(temp_candidate)\n                    logger.info(\"DEBUG: %d re-sampling candidates selected\\n\")\n                    logger.info(temp_candidate)\n                else:\n                    logger.info(\n                        \"DEBUG: No suitable resampling candidates were found\\n\")\n\n            if candidates:\n                # ===== STEP 5: Compute the information gain of each candidate\n                logger.info(\n                    \"Evaluating information gain of %d candidates...\\n\")\n                next_improvement = 0\n\n                threads_inputs = [[\n                    candidate, samples_x, samples_y, x_bounds, x_types,\n                    minimize_constraints_fun, minimize_starting_points\n                ] for candidate in candidates]\n                threads_pool = ThreadPool(4)\n                # Evaluate what would happen if we actually sample each\n                # candidate\n                threads_results = threads_pool.map(\n                    _calculate_lowest_mu_threaded, threads_inputs)\n                threads_pool.close()\n                threads_pool.join()\n\n                for threads_result in threads_results:\n                    if threads_result['expected_lowest_mu'] < lm_current['expected_mu']:\n                        # Information gain\n                        temp_improvement = threads_result['expected_lowest_mu'] - \\\n                            lm_current['expected_mu']\n\n                        if next_improvement > temp_improvement:\n                            next_improvement = temp_improvement\n                            next_candidate = threads_result['candidate']\n            else:\n                # ===== STEP 6: If we have no candidates, randomly pick one ===\n                logger.info(\n                    \"DEBUG: No candidates from exploration, exploitation,\\\n                                 and resampling. We will random a candidate for next_candidate\\n\"\n                )\n\n                next_candidate = _rand_with_constraints(\n                    x_bounds,\n                    x_types) if minimize_starting_points is None else minimize_starting_points[0]\n                next_candidate = lib_data.match_val_type(\n                    next_candidate, x_bounds, x_types)\n                expected_mu, expected_sigma = gp_prediction.predict(\n                    next_candidate, gp_model['model'])\n                next_candidate = {\n                    'hyperparameter': next_candidate,\n                    'reason': \"random\",\n                    'expected_mu': expected_mu,\n                    'expected_sigma': expected_sigma}\n\n        # STEP 7: If current optimal hyperparameter occurs in the history\n        # or exploration probability is less than the threshold, take next\n        # config as exploration step\n        outputs = self._pack_output(lm_current['hyperparameter'])\n        ap = random.uniform(0, 1)\n        if outputs in self.total_data or ap <= self.exploration_probability:\n            if next_candidate is not None:\n                outputs = self._pack_output(next_candidate['hyperparameter'])\n            else:\n                random_parameter = _rand_init(x_bounds, x_types, 1)[0]\n                outputs = self._pack_output(random_parameter)\n        self.total_data.append(outputs)\n        return outputs\n\n    def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        data : a list of dict\n               each of which has at least two keys: 'parameter' and 'value'.\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = '_'.join(\n                [\"ImportData\", str(self.supplement_data_num)])\n            self.total_data.append(_params)\n            self.receive_trial_result(\n                parameter_id=_parameter_id,\n                parameters=_params,\n                value=_value)\n        logger.info(\"Successfully import data to metis tuner.\")",
  "def _rand_with_constraints(x_bounds, x_types):\n    outputs = None\n    x_bounds_withconstraints = [x_bounds[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_types_withconstraints = [x_types[i] for i in CONSTRAINT_PARAMS_IDX]\n\n    x_val_withconstraints = lib_constraint_summation.rand(\n        x_bounds_withconstraints,\n        x_types_withconstraints,\n        CONSTRAINT_LOWERBOUND,\n        CONSTRAINT_UPPERBOUND)\n    if not x_val_withconstraints:\n        outputs = [None] * len(x_bounds)\n\n        for i, _ in enumerate(CONSTRAINT_PARAMS_IDX):\n            outputs[CONSTRAINT_PARAMS_IDX[i]] = x_val_withconstraints[i]\n\n        for i, output in enumerate(outputs):\n            if not output:\n                outputs[i] = random.randint(x_bounds[i][0], x_bounds[i][1])\n    return outputs",
  "def _calculate_lowest_mu_threaded(inputs):\n    [candidate, samples_x, samples_y, x_bounds, x_types,\n     minimize_constraints_fun, minimize_starting_points] = inputs\n\n    outputs = {\"candidate\": candidate, \"expected_lowest_mu\": None}\n\n    for expected_mu in [\n            candidate['expected_mu'] +\n            1.96 *\n            candidate['expected_sigma'],\n            candidate['expected_mu'] -\n            1.96 *\n            candidate['expected_sigma']]:\n        temp_samples_x = copy.deepcopy(samples_x)\n        temp_samples_y = copy.deepcopy(samples_y)\n\n        try:\n            idx = temp_samples_x.index(candidate['hyperparameter'])\n            # This handles the case of re-sampling a potential outlier\n            temp_samples_y[idx].append(expected_mu)\n        except ValueError:\n            temp_samples_x.append(candidate['hyperparameter'])\n            temp_samples_y.append([expected_mu])\n\n        # Aggregates multiple observation of the sample sampling points\n        temp_y_aggregation = [statistics.median(\n            temp_sample_y) for temp_sample_y in temp_samples_y]\n        temp_gp = gp_create_model.create_model(\n            temp_samples_x, temp_y_aggregation)\n        temp_results = gp_selection.selection(\n            \"lm\",\n            temp_y_aggregation,\n            x_bounds,\n            x_types,\n            temp_gp['model'],\n            minimize_starting_points,\n            minimize_constraints_fun=minimize_constraints_fun)\n\n        if outputs[\"expected_lowest_mu\"] is None \\\n            or outputs[\"expected_lowest_mu\"] > temp_results['expected_mu']:\n            outputs[\"expected_lowest_mu\"] = temp_results['expected_mu']\n\n    return outputs",
  "def _num_past_samples(x, samples_x, samples_y):\n    try:\n        idx = samples_x.index(x)\n        return len(samples_y[idx])\n    except ValueError:\n        logger.info(\"x not in sample_x\")\n        return 0",
  "def _rand_init(x_bounds, x_types, selection_num_starting_points):\n    '''\n    Random sample some init seed within bounds.\n    '''\n    return [lib_data.rand(x_bounds, x_types) for i\n            in range(0, selection_num_starting_points)]",
  "def get_median(temp_list):\n    \"\"\"\n    Return median\n    \"\"\"\n    num = len(temp_list)\n    temp_list.sort()\n    print(temp_list)\n    if num % 2 == 0:\n        median = (temp_list[int(num / 2)] + temp_list[int(num / 2) - 1]) / 2\n    else:\n        median = temp_list[int(num / 2)]\n    return median",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('no_resampling'): bool,\n            Optional('no_candidates'): bool,\n            Optional('selection_num_starting_points'): int,\n            Optional('cold_start_num'): int,\n        }).validate(kwargs)",
  "def __init__(\n            self,\n            optimize_mode=\"maximize\",\n            no_resampling=True,\n            no_candidates=False,\n            selection_num_starting_points=600,\n            cold_start_num=10,\n            exploration_probability=0.9):\n        \"\"\"\n        Parameters\n        ----------\n        optimize_mode : str\n            optimize_mode is a string that including two mode \"maximize\" and \"minimize\"\n\n        no_resampling : bool\n            True or False.\n            Should Metis consider re-sampling as part of the search strategy?\n            If you are confident that the training dataset is noise-free,\n            then you do not need re-sampling.\n\n        no_candidates : bool\n            True or False.\n            Should Metis suggest parameters for the next benchmark?\n            If you do not plan to do more benchmarks,\n            Metis can skip this step.\n\n        selection_num_starting_points : int\n            How many times Metis should try to find the global optimal in the search space?\n            The higher the number, the longer it takes to output the solution.\n\n        cold_start_num : int\n            Metis need some trial result to get cold start.\n            when the number of trial result is less than\n            cold_start_num, Metis will randomly sample hyper-parameter for trial.\n\n        exploration_probability : float\n            The probability of Metis to select parameter from exploration instead of exploitation.\n\n        x_bounds : list\n            The constration of parameters.\n\n        x_types : list\n            The type of parameters.\n        \"\"\"\n\n        self.samples_x = []\n        self.samples_y = []\n        self.samples_y_aggregation = []\n        self.total_data = []\n        self.space = None\n        self.no_resampling = no_resampling\n        self.no_candidates = no_candidates\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.key_order = []\n        self.cold_start_num = cold_start_num\n        self.selection_num_starting_points = selection_num_starting_points\n        self.exploration_probability = exploration_probability\n        self.minimize_constraints_fun = None\n        self.minimize_starting_points = None\n        self.supplement_data_num = 0\n        self.x_bounds = []\n        self.x_types = []",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Update the self.x_bounds and self.x_types by the search_space.json\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self.x_bounds = [[] for i in range(len(search_space))]\n        self.x_types = [NONE_TYPE for i in range(len(search_space))]\n\n        for key in search_space:\n            self.key_order.append(key)\n\n        key_type = {}\n        if isinstance(search_space, dict):\n            for key in search_space:\n                key_type = search_space[key]['_type']\n                key_range = search_space[key]['_value']\n                idx = self.key_order.index(key)\n                if key_type == 'quniform':\n                    if key_range[2] == 1 and key_range[0].is_integer(\n                    ) and key_range[1].is_integer():\n                        self.x_bounds[idx] = [key_range[0], key_range[1] + 1]\n                        self.x_types[idx] = 'range_int'\n                    else:\n                        low, high, q = key_range\n                        bounds = np.clip(\n                            np.arange(\n                                np.round(\n                                    low / q),\n                                np.round(\n                                    high / q) + 1) * q,\n                            low,\n                            high)\n                        self.x_bounds[idx] = bounds\n                        self.x_types[idx] = 'discrete_int'\n                elif key_type == 'randint':\n                    self.x_bounds[idx] = [key_range[0], key_range[1]]\n                    self.x_types[idx] = 'range_int'\n                elif key_type == 'uniform':\n                    self.x_bounds[idx] = [key_range[0], key_range[1]]\n                    self.x_types[idx] = 'range_continuous'\n                elif key_type == 'choice':\n                    self.x_bounds[idx] = key_range\n\n                    for key_value in key_range:\n                        if not isinstance(key_value, (int, float)):\n                            raise RuntimeError(\n                                \"Metis Tuner only support numerical choice.\")\n\n                    self.x_types[idx] = 'discrete_int'\n                else:\n                    logger.info(\n                        \"Metis Tuner doesn't support this kind of variable: %s\",\n                        str(key_type))\n                    raise RuntimeError(\n                        \"Metis Tuner doesn't support this kind of variable: %s\" %\n                        str(key_type))\n        else:\n            logger.info(\"The format of search space is not a dict.\")\n            raise RuntimeError(\"The format of search space is not a dict.\")\n\n        self.minimize_starting_points = _rand_init(\n            self.x_bounds, self.x_types, self.selection_num_starting_points)",
  "def _pack_output(self, init_parameter):\n        \"\"\"\n        Pack the output\n\n        Parameters\n        ----------\n        init_parameter : dict\n\n        Returns\n        -------\n        output : dict\n        \"\"\"\n        output = {}\n        for i, param in enumerate(init_parameter):\n            output[self.key_order[i]] = param\n\n        return output",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate next parameter for trial\n\n        If the number of trial result is lower than cold start number,\n        metis will first random generate some parameters.\n        Otherwise, metis will choose the parameters by\n        the Gussian Process Model and the Gussian Mixture Model.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        result : dict\n        \"\"\"\n        if len(self.samples_x) < self.cold_start_num:\n            init_parameter = _rand_init(self.x_bounds, self.x_types, 1)[0]\n            results = self._pack_output(init_parameter)\n        else:\n            self.minimize_starting_points = _rand_init(\n                self.x_bounds, self.x_types, self.selection_num_starting_points)\n            results = self._selection(\n                self.samples_x,\n                self.samples_y_aggregation,\n                self.samples_y,\n                self.x_bounds,\n                self.x_types,\n                threshold_samplessize_resampling=(\n                    None if self.no_resampling is True else 50),\n                no_candidates=self.no_candidates,\n                minimize_starting_points=self.minimize_starting_points,\n                minimize_constraints_fun=self.minimize_constraints_fun)\n\n        logger.info(\"Generate paramageters: \\n%s\", str(results))\n        return results",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Tuner receive result from trial.\n\n        Parameters\n        ----------\n        parameter_id : int\n            The id of parameters, generated by nni manager.\n        parameters : dict\n            A group of parameters that trial has tried.\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n        \"\"\"\n        value = extract_scalar_reward(value)\n        if self.optimize_mode == OptimizeMode.Maximize:\n            value = -value\n\n        logger.info(\"Received trial result.\")\n        logger.info(\"value is : %s\", str(value))\n        logger.info(\"parameter is : %s\", str(parameters))\n\n        # parse parameter to sample_x\n        sample_x = [0 for i in range(len(self.key_order))]\n        for key in parameters:\n            idx = self.key_order.index(key)\n            sample_x[idx] = parameters[key]\n\n        # parse value to sample_y\n        temp_y = []\n        if sample_x in self.samples_x:\n            idx = self.samples_x.index(sample_x)\n            temp_y = self.samples_y[idx]\n            temp_y.append(value)\n            self.samples_y[idx] = temp_y\n\n            # calculate y aggregation\n            median = get_median(temp_y)\n            self.samples_y_aggregation[idx] = [median]\n        else:\n            self.samples_x.append(sample_x)\n            self.samples_y.append([value])\n\n            # calculate y aggregation\n            self.samples_y_aggregation.append([value])",
  "def _selection(\n            self,\n            samples_x,\n            samples_y_aggregation,\n            samples_y,\n            x_bounds,\n            x_types,\n            max_resampling_per_x=3,\n            threshold_samplessize_exploitation=12,\n            threshold_samplessize_resampling=50,\n            no_candidates=False,\n            minimize_starting_points=None,\n            minimize_constraints_fun=None):\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n\n        next_candidate = None\n        candidates = []\n        samples_size_all = sum([len(i) for i in samples_y])\n        samples_size_unique = len(samples_y)\n\n        # ===== STEP 1: Compute the current optimum =====\n        gp_model = gp_create_model.create_model(\n            samples_x, samples_y_aggregation)\n        lm_current = gp_selection.selection(\n            \"lm\",\n            samples_y_aggregation,\n            x_bounds,\n            x_types,\n            gp_model['model'],\n            minimize_starting_points,\n            minimize_constraints_fun=minimize_constraints_fun)\n        if not lm_current:\n            return None\n        logger.info({\n            'hyperparameter': lm_current['hyperparameter'],\n            'expected_mu': lm_current['expected_mu'],\n            'expected_sigma': lm_current['expected_sigma'],\n            'reason': \"exploitation_gp\"\n        })\n\n        if no_candidates is False:\n            # ===== STEP 2: Get recommended configurations for exploration ====\n            results_exploration = gp_selection.selection(\n                \"lc\",\n                samples_y_aggregation,\n                x_bounds,\n                x_types,\n                gp_model['model'],\n                minimize_starting_points,\n                minimize_constraints_fun=minimize_constraints_fun)\n\n            if results_exploration is not None:\n                if _num_past_samples(results_exploration['hyperparameter'], samples_x, samples_y) == 0:\n                    temp_candidate = {\n                        'hyperparameter': results_exploration['hyperparameter'],\n                        'expected_mu': results_exploration['expected_mu'],\n                        'expected_sigma': results_exploration['expected_sigma'],\n                        'reason': \"exploration\"\n                    }\n                    candidates.append(temp_candidate)\n\n                    logger.info(\"DEBUG: 1 exploration candidate selected\\n\")\n                    logger.info(temp_candidate)\n            else:\n                logger.info(\"DEBUG: No suitable exploration candidates were\")\n\n            # ===== STEP 3: Get recommended configurations for exploitation ===\n            if samples_size_all >= threshold_samplessize_exploitation:\n                logger.info(\"Getting candidates for exploitation...\\n\")\n                try:\n                    gmm = gmm_create_model.create_model(\n                        samples_x, samples_y_aggregation)\n\n                    if (\"discrete_int\" in x_types) or (\"range_int\" in x_types):\n                        results_exploitation = gmm_selection.selection(\n                            x_bounds,\n                            x_types,\n                            gmm['clusteringmodel_good'],\n                            gmm['clusteringmodel_bad'],\n                            minimize_starting_points,\n                            minimize_constraints_fun=minimize_constraints_fun)\n                    else:\n                        # If all parameters are of \"range_continuous\",\n                        # let's use GMM to generate random starting points\n                        results_exploitation = gmm_selection.selection_r(\n                            x_bounds,\n                            x_types,\n                            gmm['clusteringmodel_good'],\n                            gmm['clusteringmodel_bad'],\n                            num_starting_points=self.selection_num_starting_points,\n                            minimize_constraints_fun=minimize_constraints_fun)\n\n                    if results_exploitation is not None:\n                        if _num_past_samples(results_exploitation['hyperparameter'], samples_x, samples_y) == 0:\n                            temp_expected_mu, temp_expected_sigma = \\\n                                    gp_prediction.predict(results_exploitation['hyperparameter'], gp_model['model'])\n                            temp_candidate = {\n                                'hyperparameter': results_exploitation['hyperparameter'],\n                                'expected_mu': temp_expected_mu,\n                                'expected_sigma': temp_expected_sigma,\n                                'reason': \"exploitation_gmm\"\n                            }\n                            candidates.append(temp_candidate)\n\n                            logger.info(\n                                \"DEBUG: 1 exploitation_gmm candidate selected\\n\")\n                            logger.info(temp_candidate)\n                    else:\n                        logger.info(\n                            \"DEBUG: No suitable exploitation_gmm candidates were found\\n\")\n\n                except ValueError as exception:\n                    # The exception: ValueError: Fitting the mixture model failed\n                    # because some components have ill-defined empirical covariance\n                    # (for instance caused by singleton or collapsed samples).\n                    # Try to decrease the number of components, or increase\n                    # reg_covar.\n                    logger.info(\n                        \"DEBUG: No suitable exploitation_gmm \\\n                        candidates were found due to exception.\")\n                    logger.info(exception)\n\n            # ===== STEP 4: Get a list of outliers =====\n            if (threshold_samplessize_resampling is not None) and \\\n                    (samples_size_unique >= threshold_samplessize_resampling):\n                logger.info(\"Getting candidates for re-sampling...\\n\")\n                results_outliers = gp_outlier_detection.outlierDetection_threaded(\n                    samples_x, samples_y_aggregation)\n\n                if results_outliers is not None:\n                    for results_outlier in results_outliers:  # pylint: disable=not-an-iterable\n                        if _num_past_samples(samples_x[results_outlier['samples_idx']], samples_x, samples_y) < max_resampling_per_x:\n                            temp_candidate = {'hyperparameter': samples_x[results_outlier['samples_idx']],\\\n                                               'expected_mu': results_outlier['expected_mu'],\\\n                                               'expected_sigma': results_outlier['expected_sigma'],\\\n                                               'reason': \"resampling\"}\n                            candidates.append(temp_candidate)\n                    logger.info(\"DEBUG: %d re-sampling candidates selected\\n\")\n                    logger.info(temp_candidate)\n                else:\n                    logger.info(\n                        \"DEBUG: No suitable resampling candidates were found\\n\")\n\n            if candidates:\n                # ===== STEP 5: Compute the information gain of each candidate\n                logger.info(\n                    \"Evaluating information gain of %d candidates...\\n\")\n                next_improvement = 0\n\n                threads_inputs = [[\n                    candidate, samples_x, samples_y, x_bounds, x_types,\n                    minimize_constraints_fun, minimize_starting_points\n                ] for candidate in candidates]\n                threads_pool = ThreadPool(4)\n                # Evaluate what would happen if we actually sample each\n                # candidate\n                threads_results = threads_pool.map(\n                    _calculate_lowest_mu_threaded, threads_inputs)\n                threads_pool.close()\n                threads_pool.join()\n\n                for threads_result in threads_results:\n                    if threads_result['expected_lowest_mu'] < lm_current['expected_mu']:\n                        # Information gain\n                        temp_improvement = threads_result['expected_lowest_mu'] - \\\n                            lm_current['expected_mu']\n\n                        if next_improvement > temp_improvement:\n                            next_improvement = temp_improvement\n                            next_candidate = threads_result['candidate']\n            else:\n                # ===== STEP 6: If we have no candidates, randomly pick one ===\n                logger.info(\n                    \"DEBUG: No candidates from exploration, exploitation,\\\n                                 and resampling. We will random a candidate for next_candidate\\n\"\n                )\n\n                next_candidate = _rand_with_constraints(\n                    x_bounds,\n                    x_types) if minimize_starting_points is None else minimize_starting_points[0]\n                next_candidate = lib_data.match_val_type(\n                    next_candidate, x_bounds, x_types)\n                expected_mu, expected_sigma = gp_prediction.predict(\n                    next_candidate, gp_model['model'])\n                next_candidate = {\n                    'hyperparameter': next_candidate,\n                    'reason': \"random\",\n                    'expected_mu': expected_mu,\n                    'expected_sigma': expected_sigma}\n\n        # STEP 7: If current optimal hyperparameter occurs in the history\n        # or exploration probability is less than the threshold, take next\n        # config as exploration step\n        outputs = self._pack_output(lm_current['hyperparameter'])\n        ap = random.uniform(0, 1)\n        if outputs in self.total_data or ap <= self.exploration_probability:\n            if next_candidate is not None:\n                outputs = self._pack_output(next_candidate['hyperparameter'])\n            else:\n                random_parameter = _rand_init(x_bounds, x_types, 1)[0]\n                outputs = self._pack_output(random_parameter)\n        self.total_data.append(outputs)\n        return outputs",
  "def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        data : a list of dict\n               each of which has at least two keys: 'parameter' and 'value'.\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = '_'.join(\n                [\"ImportData\", str(self.supplement_data_num)])\n            self.total_data.append(_params)\n            self.receive_trial_result(\n                parameter_id=_parameter_id,\n                parameters=_params,\n                value=_value)\n        logger.info(\"Successfully import data to metis tuner.\")",
  "def next_hyperparameter_expected_improvement(fun_prediction,\n                                             fun_prediction_args,\n                                             x_bounds, x_types,\n                                             samples_y_aggregation,\n                                             minimize_starting_points,\n                                             minimize_constraints_fun=None):\n    \"\"\"\n    \"Expected Improvement\" acquisition function\n    \"\"\"\n    best_x = None\n    best_acquisition_value = None\n    x_bounds_minmax = [[i[0], i[-1]] for i in x_bounds]\n    x_bounds_minmax = numpy.array(x_bounds_minmax)\n\n    for starting_point in numpy.array(minimize_starting_points):\n        res = minimize(fun=_expected_improvement,\n                       x0=starting_point.reshape(1, -1),\n                       bounds=x_bounds_minmax,\n                       method=\"L-BFGS-B\",\n                       args=(fun_prediction,\n                             fun_prediction_args,\n                             x_bounds,\n                             x_types,\n                             samples_y_aggregation,\n                             minimize_constraints_fun))\n\n        if (best_acquisition_value is None) or \\\n                (res.fun < best_acquisition_value):\n            res.x = numpy.ndarray.tolist(res.x)\n            res.x = lib_data.match_val_type(res.x, x_bounds, x_types)\n            if (minimize_constraints_fun is None) or \\\n                    (minimize_constraints_fun(res.x) is True):\n                best_acquisition_value = res.fun\n                best_x = res.x\n\n    outputs = None\n    if best_x is not None:\n        mu, sigma = fun_prediction(best_x, *fun_prediction_args)\n        outputs = {'hyperparameter': best_x, 'expected_mu': mu,\n                   'expected_sigma': sigma, 'acquisition_func': \"ei\"}\n\n    return outputs",
  "def _expected_improvement(x, fun_prediction, fun_prediction_args,\n                          x_bounds, x_types, samples_y_aggregation,\n                          minimize_constraints_fun):\n    # This is only for step-wise optimization\n    x = lib_data.match_val_type(x, x_bounds, x_types)\n\n    expected_improvement = sys.maxsize\n    if (minimize_constraints_fun is None) or (\n            minimize_constraints_fun(x) is True):\n        mu, sigma = fun_prediction(x, *fun_prediction_args)\n\n        loss_optimum = min(samples_y_aggregation)\n        scaling_factor = -1\n\n        # In case sigma equals zero\n        with numpy.errstate(divide=\"ignore\"):\n            Z = scaling_factor * (mu - loss_optimum) / sigma\n            expected_improvement = scaling_factor * (mu - loss_optimum) * \\\n                norm.cdf(Z) + sigma * norm.pdf(Z)\n            expected_improvement = 0.0 if sigma == 0.0 else expected_improvement\n\n        # We want expected_improvement to be as large as possible\n        # (i.e., as small as possible for minimize(...))\n        expected_improvement = -1 * expected_improvement\n    return expected_improvement",
  "def next_hyperparameter_lowest_confidence(fun_prediction,\n                                          fun_prediction_args,\n                                          x_bounds, x_types,\n                                          minimize_starting_points,\n                                          minimize_constraints_fun=None):\n    \"\"\"\n    \"Lowest Confidence\" acquisition function\n    \"\"\"\n    best_x = None\n    best_acquisition_value = None\n    x_bounds_minmax = [[i[0], i[-1]] for i in x_bounds]\n    x_bounds_minmax = numpy.array(x_bounds_minmax)\n\n    for starting_point in numpy.array(minimize_starting_points):\n        res = minimize(fun=_lowest_confidence,\n                       x0=starting_point.reshape(1, -1),\n                       bounds=x_bounds_minmax,\n                       method=\"L-BFGS-B\",\n                       args=(fun_prediction,\n                             fun_prediction_args,\n                             x_bounds,\n                             x_types,\n                             minimize_constraints_fun))\n\n        if (best_acquisition_value) is None or (\n                res.fun < best_acquisition_value):\n            res.x = numpy.ndarray.tolist(res.x)\n            res.x = lib_data.match_val_type(res.x, x_bounds, x_types)\n            if (minimize_constraints_fun is None) or (\n                    minimize_constraints_fun(res.x) is True):\n                best_acquisition_value = res.fun\n                best_x = res.x\n\n    outputs = None\n    if best_x is not None:\n        mu, sigma = fun_prediction(best_x, *fun_prediction_args)\n        outputs = {'hyperparameter': best_x, 'expected_mu': mu,\n                   'expected_sigma': sigma, 'acquisition_func': \"lc\"}\n    return outputs",
  "def _lowest_confidence(x, fun_prediction, fun_prediction_args,\n                       x_bounds, x_types, minimize_constraints_fun):\n    # This is only for step-wise optimization\n    x = lib_data.match_val_type(x, x_bounds, x_types)\n\n    ci = sys.maxsize\n    if (minimize_constraints_fun is None) or (\n            minimize_constraints_fun(x) is True):\n        mu, sigma = fun_prediction(x, *fun_prediction_args)\n        ci = (sigma * 1.96 * 2) / mu\n        # We want ci to be as large as possible\n        # (i.e., as small as possible for minimize(...),\n        # because this would mean lowest confidence\n        ci = -1 * ci\n\n    return ci",
  "def next_hyperparameter_lowest_mu(fun_prediction,\n                                  fun_prediction_args,\n                                  x_bounds, x_types,\n                                  minimize_starting_points,\n                                  minimize_constraints_fun=None):\n    \"\"\"\n    \"Lowest Mu\" acquisition function\n    \"\"\"\n    best_x = None\n    best_acquisition_value = None\n    x_bounds_minmax = [[i[0], i[-1]] for i in x_bounds]\n    x_bounds_minmax = numpy.array(x_bounds_minmax)\n\n    for starting_point in numpy.array(minimize_starting_points):\n        res = minimize(fun=_lowest_mu,\n                       x0=starting_point.reshape(1, -1),\n                       bounds=x_bounds_minmax,\n                       method=\"L-BFGS-B\",\n                       args=(fun_prediction, fun_prediction_args,\n                             x_bounds, x_types, minimize_constraints_fun))\n\n        if (best_acquisition_value is None) or (\n                res.fun < best_acquisition_value):\n            res.x = numpy.ndarray.tolist(res.x)\n            res.x = lib_data.match_val_type(res.x, x_bounds, x_types)\n            if (minimize_constraints_fun is None) or (\n                    minimize_constraints_fun(res.x) is True):\n                best_acquisition_value = res.fun\n                best_x = res.x\n\n    outputs = None\n    if best_x is not None:\n        mu, sigma = fun_prediction(best_x, *fun_prediction_args)\n        outputs = {'hyperparameter': best_x, 'expected_mu': mu,\n                   'expected_sigma': sigma, 'acquisition_func': \"lm\"}\n    return outputs",
  "def _lowest_mu(x, fun_prediction, fun_prediction_args,\n               x_bounds, x_types, minimize_constraints_fun):\n    \"\"\"\n    Calculate the lowest mu\n    \"\"\"\n    # This is only for step-wise optimization\n    x = lib_data.match_val_type(x, x_bounds, x_types)\n\n    mu = sys.maxsize\n    if (minimize_constraints_fun is None) or (\n            minimize_constraints_fun(x) is True):\n        mu, _ = fun_prediction(x, *fun_prediction_args)\n    return mu",
  "def create_model(samples_x, samples_y_aggregation, percentage_goodbatch=0.34):\n    '''\n    Create the Gaussian Mixture Model\n    '''\n    samples = [samples_x[i] + [samples_y_aggregation[i]]\n               for i in range(0, len(samples_x))]\n\n    # Sorts so that we can get the top samples\n    samples = sorted(samples, key=itemgetter(-1))\n    samples_goodbatch_size = int(len(samples) * percentage_goodbatch)\n    samples_goodbatch = samples[0:samples_goodbatch_size]\n    samples_badbatch = samples[samples_goodbatch_size:]\n\n    samples_x_goodbatch = [sample_goodbatch[0:-1]\n                           for sample_goodbatch in samples_goodbatch]\n    #samples_y_goodbatch = [sample_goodbatch[-1] for sample_goodbatch in samples_goodbatch]\n    samples_x_badbatch = [sample_badbatch[0:-1]\n                          for sample_badbatch in samples_badbatch]\n\n    # === Trains GMM clustering models === #\n    #sys.stderr.write(\"[%s] Train GMM's GMM model\\n\" % (os.path.basename(__file__)))\n    bgmm_goodbatch = mm.BayesianGaussianMixture(\n        n_components=max(1, samples_goodbatch_size - 1))\n    bad_n_components = max(1, len(samples_x) - samples_goodbatch_size - 1)\n    bgmm_badbatch = mm.BayesianGaussianMixture(n_components=bad_n_components)\n    bgmm_goodbatch.fit(samples_x_goodbatch)\n    bgmm_badbatch.fit(samples_x_badbatch)\n\n    model = {}\n    model['clusteringmodel_good'] = bgmm_goodbatch\n    model['clusteringmodel_bad'] = bgmm_badbatch\n    return model",
  "def _ratio_scores(parameters_value, clusteringmodel_gmm_good,\n                  clusteringmodel_gmm_bad):\n    '''\n    The ratio is smaller the better\n    '''\n    ratio = clusteringmodel_gmm_good.score(\n        [parameters_value]) / clusteringmodel_gmm_bad.score([parameters_value])\n    sigma = 0\n    return ratio, sigma",
  "def selection_r(x_bounds,\n                x_types,\n                clusteringmodel_gmm_good,\n                clusteringmodel_gmm_bad,\n                num_starting_points=100,\n                minimize_constraints_fun=None):\n    '''\n    Select using different types.\n    '''\n    minimize_starting_points = clusteringmodel_gmm_good.sample(n_samples=num_starting_points)\n\n    outputs = selection(x_bounds, x_types,\n                        clusteringmodel_gmm_good,\n                        clusteringmodel_gmm_bad,\n                        minimize_starting_points[0],\n                        minimize_constraints_fun)\n\n    return outputs",
  "def selection(x_bounds,\n              x_types,\n              clusteringmodel_gmm_good,\n              clusteringmodel_gmm_bad,\n              minimize_starting_points,\n              minimize_constraints_fun=None):\n    '''\n    Select the lowest mu value\n    '''\n    results = lib_acquisition_function.next_hyperparameter_lowest_mu(\n        _ratio_scores, [clusteringmodel_gmm_good, clusteringmodel_gmm_bad],\n        x_bounds, x_types, minimize_starting_points,\n        minimize_constraints_fun=minimize_constraints_fun)\n\n    return results",
  "def _rand_with_constraints(x_bounds, x_types):\n    '''\n    Random generate the variable with constraints\n    '''\n    outputs = None\n    x_bounds_withconstraints = [x_bounds[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_types_withconstraints = [x_types[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_val_withconstraints = lib_constraint_summation.rand(x_bounds_withconstraints,\n                                                          x_types_withconstraints,\n                                                          CONSTRAINT_LOWERBOUND,\n                                                          CONSTRAINT_UPPERBOUND)\n    if x_val_withconstraints is not None:\n        outputs = [None] * len(x_bounds)\n        for i, _ in enumerate(CONSTRAINT_PARAMS_IDX):\n            outputs[CONSTRAINT_PARAMS_IDX[i]] = x_val_withconstraints[i]\n        for i, _ in enumerate(outputs):\n            if outputs[i] is None:\n                outputs[i] = random.randint(x_bounds[i][0], x_bounds[i][1])\n    return outputs",
  "def _minimize_constraints_fun_summation(x):\n    '''\n    Minimize constraints fun summation\n    '''\n    summation = sum([x[i] for i in CONSTRAINT_PARAMS_IDX])\n    return CONSTRAINT_UPPERBOUND >= summation >= CONSTRAINT_LOWERBOUND",
  "def create_model(samples_x, samples_y_aggregation,\n                 n_restarts_optimizer=250, is_white_kernel=False):\n    '''\n    Trains GP regression model\n    '''\n    kernel = gp.kernels.ConstantKernel(constant_value=1,\n                                       constant_value_bounds=(1e-12, 1e12)) * \\\n                                                gp.kernels.Matern(nu=1.5)\n    if is_white_kernel is True:\n        kernel += gp.kernels.WhiteKernel(noise_level=1, noise_level_bounds=(1e-12, 1e12))\n    regressor = gp.GaussianProcessRegressor(kernel=kernel,\n                                            n_restarts_optimizer=n_restarts_optimizer,\n                                            normalize_y=True,\n                                            alpha=1e-10)\n    regressor.fit(numpy.array(samples_x), numpy.array(samples_y_aggregation))\n\n    model = {}\n    model['model'] = regressor\n    model['kernel_prior'] = str(kernel)\n    model['kernel_posterior'] = str(regressor.kernel_)\n    model['model_loglikelihood'] = regressor.log_marginal_likelihood(regressor.kernel_.theta)\n\n    return model",
  "def predict(parameters_value, regressor_gp):\n    '''\n    Predict by Gaussian Process Model\n    '''\n    parameters_value = numpy.array(parameters_value).reshape(-1, len(parameters_value))\n    mu, sigma = regressor_gp.predict(parameters_value, return_std=True)\n\n    return mu[0], sigma[0]",
  "def selection_r(acquisition_function,\n                samples_y_aggregation,\n                x_bounds,\n                x_types,\n                regressor_gp,\n                num_starting_points=100,\n                minimize_constraints_fun=None):\n    '''\n    Selecte R value\n    '''\n    minimize_starting_points = [lib_data.rand(x_bounds, x_types) \\\n                                    for i in range(0, num_starting_points)]\n    outputs = selection(acquisition_function, samples_y_aggregation,\n                        x_bounds, x_types, regressor_gp,\n                        minimize_starting_points,\n                        minimize_constraints_fun=minimize_constraints_fun)\n\n    return outputs",
  "def selection(acquisition_function,\n              samples_y_aggregation,\n              x_bounds, x_types,\n              regressor_gp,\n              minimize_starting_points,\n              minimize_constraints_fun=None):\n    '''\n    selection\n    '''\n    outputs = None\n\n    sys.stderr.write(\"[%s] Exercise \\\"%s\\\" acquisition function\\n\" \\\n                        % (os.path.basename(__file__), acquisition_function))\n\n    if acquisition_function == \"ei\":\n        outputs = lib_acquisition_function.next_hyperparameter_expected_improvement(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types, \\\n                        samples_y_aggregation, minimize_starting_points, \\\n                        minimize_constraints_fun=minimize_constraints_fun)\n    elif acquisition_function == \"lc\":\n        outputs = lib_acquisition_function.next_hyperparameter_lowest_confidence(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types,\\\n                        minimize_starting_points, minimize_constraints_fun=minimize_constraints_fun)\n    elif acquisition_function == \"lm\":\n        outputs = lib_acquisition_function.next_hyperparameter_lowest_mu(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types,\\\n                        minimize_starting_points, minimize_constraints_fun=minimize_constraints_fun)\n    return outputs",
  "def _rand_with_constraints(x_bounds, x_types):\n    '''\n    Random generate with constraints\n    '''\n    outputs = None\n\n    x_bounds_withconstraints = [x_bounds[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_types_withconstraints = [x_types[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_val_withconstraints = lib_constraint_summation.rand(x_bounds_withconstraints,\n                                                          x_types_withconstraints,\n                                                          CONSTRAINT_LOWERBOUND,\n                                                          CONSTRAINT_UPPERBOUND)\n    if x_val_withconstraints is not None:\n        outputs = [None] * len(x_bounds)\n\n        for i, _ in enumerate(CONSTRAINT_PARAMS_IDX):\n            outputs[CONSTRAINT_PARAMS_IDX[i]] = x_val_withconstraints[i]\n\n        for i, _ in enumerate(outputs):\n            if outputs[i] is None:\n                outputs[i] = random.randint(x_bounds[i][0], x_bounds[i][1])\n    return outputs",
  "def _minimize_constraints_fun_summation(x):\n    '''\n    Minimize the constraints fun summation\n    '''\n    summation = sum([x[i] for i in CONSTRAINT_PARAMS_IDX])\n    return CONSTRAINT_UPPERBOUND >= summation >= CONSTRAINT_LOWERBOUND",
  "def _outlierDetection_threaded(inputs):\n    \"\"\"\n    Detect the outlier\n    \"\"\"\n    [samples_idx, samples_x, samples_y_aggregation] = inputs\n    sys.stderr.write(\"[%s] DEBUG: Evaluating %dth of %d samples\\n\"\n                     % (os.path.basename(__file__), samples_idx + 1, len(samples_x)))\n    outlier = None\n\n    # Create a diagnostic regression model which removes the sample that we\n    # want to evaluate\n    diagnostic_regressor_gp = gp_create_model.create_model(\n        samples_x[0:samples_idx] + samples_x[samples_idx + 1:],\n        samples_y_aggregation[0:samples_idx] + samples_y_aggregation[samples_idx + 1:])\n    mu, sigma = gp_prediction.predict(\n        samples_x[samples_idx], diagnostic_regressor_gp['model'])\n\n    # 2.33 is the z-score for 98% confidence level\n    if abs(samples_y_aggregation[samples_idx] - mu) > (2.33 * sigma):\n        outlier = {\"samples_idx\": samples_idx,\n                   \"expected_mu\": mu,\n                   \"expected_sigma\": sigma,\n                   \"difference\": abs(samples_y_aggregation[samples_idx] - mu) - (2.33 * sigma)}\n    return outlier",
  "def outlierDetection_threaded(samples_x, samples_y_aggregation):\n    \"\"\"\n    Use Multi-thread to detect the outlier\n    \"\"\"\n    outliers = []\n\n    threads_inputs = [[samples_idx, samples_x, samples_y_aggregation]\n                      for samples_idx in range(0, len(samples_x))]\n    threads_pool = ThreadPool(min(4, len(threads_inputs)))\n    threads_results = threads_pool.map(\n        _outlierDetection_threaded, threads_inputs)\n    threads_pool.close()\n    threads_pool.join()\n\n    for threads_result in threads_results:\n        if threads_result is not None:\n            outliers.append(threads_result)\n        else:\n            print(\"Error: threads_result is None.\")\n\n    outliers = outliers if outliers else None\n    return outliers",
  "def outlierDetection(samples_x, samples_y_aggregation):\n    outliers = []\n    for samples_idx, _ in enumerate(samples_x):\n        #sys.stderr.write(\"[%s] DEBUG: Evaluating %d of %d samples\\n\"\n        #  \\ % (os.path.basename(__file__), samples_idx + 1, len(samples_x)))\n        diagnostic_regressor_gp = gp_create_model.create_model(\\\n                                        samples_x[0:samples_idx] + samples_x[samples_idx + 1:],\\\n                                        samples_y_aggregation[0:samples_idx] + samples_y_aggregation[samples_idx + 1:])\n        mu, sigma = gp_prediction.predict(samples_x[samples_idx],\n                                          diagnostic_regressor_gp['model'])\n        # 2.33 is the z-score for 98% confidence level\n        if abs(samples_y_aggregation[samples_idx] - mu) > (2.33 * sigma):\n            outliers.append({\"samples_idx\": samples_idx,\n                             \"expected_mu\": mu,\n                             \"expected_sigma\": sigma,\n                             \"difference\": \\\n                                abs(samples_y_aggregation[samples_idx] - mu) - (2.33 * sigma)})\n\n    outliers = outliers if outliers else None\n    return outliers",
  "def create_parameter_id():\n    \"\"\"Create an id\n\n    Returns\n    -------\n    int\n        parameter id\n    \"\"\"\n    global _next_parameter_id\n    _next_parameter_id += 1\n    return _next_parameter_id - 1",
  "def create_bracket_parameter_id(brackets_id, brackets_curr_decay, increased_id=-1):\n    \"\"\"Create a full id for a specific bracket's hyperparameter configuration\n\n    Parameters\n    ----------\n    brackets_id: int\n        brackets id\n    brackets_curr_decay:\n        brackets curr decay\n    increased_id: int\n        increased id\n\n    Returns\n    -------\n    int\n        params id\n    \"\"\"\n    if increased_id == -1:\n        increased_id = str(create_parameter_id())\n    params_id = '_'.join([str(brackets_id),\n                          str(brackets_curr_decay),\n                          increased_id])\n    return params_id",
  "def json2parameter(ss_spec, random_state):\n    \"\"\"Randomly generate values for hyperparameters from hyperparameter space i.e., x.\n\n    Parameters\n    ----------\n    ss_spec:\n        hyperparameter space\n    random_state:\n        random operator to generate random values\n\n    Returns\n    -------\n    Parameter:\n        Parameters in this experiment\n    \"\"\"\n    if isinstance(ss_spec, dict):\n        if NodeType.TYPE in ss_spec.keys():\n            _type = ss_spec[NodeType.TYPE]\n            _value = ss_spec[NodeType.VALUE]\n            if _type == 'choice':\n                _index = random_state.randint(len(_value))\n                chosen_params = json2parameter(ss_spec[NodeType.VALUE][_index], random_state)\n            else:\n                chosen_params = getattr(parameter_expressions, _type)(*(_value + [random_state]))\n        else:\n            chosen_params = dict()\n            for key in ss_spec.keys():\n                chosen_params[key] = json2parameter(ss_spec[key], random_state)\n    elif isinstance(ss_spec, list):\n        chosen_params = list()\n        for _, subspec in enumerate(ss_spec):\n            chosen_params.append(json2parameter(subspec, random_state))\n    else:\n        chosen_params = copy.deepcopy(ss_spec)\n    return chosen_params",
  "class Bracket():\n    \"\"\"A bracket in Hyperband, all the information of a bracket is managed by an instance of this class\n\n    Parameters\n    ----------\n    s: int\n        The current SH iteration index.\n    s_max: int\n        total number of SH iterations\n    eta: float\n        In each iteration, a complete run of sequential halving is executed. In it,\n\t\tafter evaluating each configuration on the same subset size, only a fraction of\n\t\t1/eta of them 'advances' to the next round.\n    R:\n        the budget associated with each stage\n    optimize_mode: str\n        optimize mode, 'maximize' or 'minimize'\n    \"\"\"\n\n    def __init__(self, s, s_max, eta, R, optimize_mode):\n        self.bracket_id = s\n        self.s_max = s_max\n        self.eta = eta\n        self.n = math.ceil((s_max + 1) * (eta ** s) / (s + 1) - _epsilon)\n        self.r = R / eta ** s\n        self.i = 0\n        self.hyper_configs = []  # [ {id: params}, {}, ... ]\n        self.configs_perf = []  # [ {id: [seq, acc]}, {}, ... ]\n        self.num_configs_to_run = []  # [ n, n, n, ... ]\n        self.num_finished_configs = []  # [ n, n, n, ... ]\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.no_more_trial = False\n\n    def is_completed(self):\n        \"\"\"check whether this bracket has sent out all the hyperparameter configurations\"\"\"\n        return self.no_more_trial\n\n    def get_n_r(self):\n        \"\"\"return the values of n and r for the next round\"\"\"\n        return math.floor(self.n / self.eta ** self.i + _epsilon), math.floor(self.r * self.eta ** self.i + _epsilon)\n\n    def increase_i(self):\n        \"\"\"i means the ith round. Increase i by 1\"\"\"\n        self.i += 1\n        if self.i > self.bracket_id:\n            self.no_more_trial = True\n\n    def set_config_perf(self, i, parameter_id, seq, value):\n        \"\"\"update trial's latest result with its sequence number, e.g., epoch number or batch number\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        parameter_id: int\n            the id of the trial/parameter\n        seq: int\n            sequence number, e.g., epoch number or batch number\n        value: int\n            latest result with sequence number seq\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if parameter_id in self.configs_perf[i]:\n            if self.configs_perf[i][parameter_id][0] < seq:\n                self.configs_perf[i][parameter_id] = [seq, value]\n        else:\n            self.configs_perf[i][parameter_id] = [seq, value]\n\n    def inform_trial_end(self, i):\n        \"\"\"If the trial is finished and the corresponding round (i.e., i) has all its trials finished,\n        it will choose the top k trials for the next round (i.e., i+1)\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        \"\"\"\n        global _KEY\n        self.num_finished_configs[i] += 1\n        _logger.debug('bracket id: %d, round: %d %d, finished: %d, all: %d', self.bracket_id, self.i, i,\n                      self.num_finished_configs[i], self.num_configs_to_run[i])\n        if self.num_finished_configs[i] >= self.num_configs_to_run[i] \\\n                and self.no_more_trial is False:\n            # choose candidate configs from finished configs to run in the next round\n            assert self.i == i + 1\n            this_round_perf = self.configs_perf[i]\n            if self.optimize_mode is OptimizeMode.Maximize:\n                sorted_perf = sorted(this_round_perf.items(), key=lambda kv: kv[1][1], reverse=True)  # reverse\n            else:\n                sorted_perf = sorted(this_round_perf.items(), key=lambda kv: kv[1][1])\n            _logger.debug('bracket %s next round %s, sorted hyper configs: %s', self.bracket_id, self.i, sorted_perf)\n            next_n, next_r = self.get_n_r()\n            _logger.debug('bracket %s next round %s, next_n=%d, next_r=%d', self.bracket_id, self.i, next_n, next_r)\n            hyper_configs = dict()\n            for k in range(next_n):\n                params_id = sorted_perf[k][0]\n                params = self.hyper_configs[i][params_id]\n                params[_KEY] = next_r  # modify r\n                # generate new id\n                increased_id = params_id.split('_')[-1]\n                new_id = create_bracket_parameter_id(self.bracket_id, self.i, increased_id)\n                hyper_configs[new_id] = params\n            self._record_hyper_configs(hyper_configs)\n            return [[key, value] for key, value in hyper_configs.items()]\n        return None\n\n    def get_hyperparameter_configurations(self, num, r, searchspace_json, random_state):\n        \"\"\"Randomly generate num hyperparameter configurations from search space\n\n        Parameters\n        ----------\n        num: int\n            the number of hyperparameter configurations\n\n        Returns\n        -------\n        list\n            a list of hyperparameter configurations. Format: [[key1, value1], [key2, value2], ...]\n        \"\"\"\n        global _KEY\n        assert self.i == 0\n        hyperparameter_configs = dict()\n        for _ in range(num):\n            params_id = create_bracket_parameter_id(self.bracket_id, self.i)\n            params = json2parameter(searchspace_json, random_state)\n            params[_KEY] = r\n            hyperparameter_configs[params_id] = params\n        self._record_hyper_configs(hyperparameter_configs)\n        return [[key, value] for key, value in hyperparameter_configs.items()]\n\n    def _record_hyper_configs(self, hyper_configs):\n        \"\"\"after generating one round of hyperconfigs, this function records the generated hyperconfigs,\n        creates a dict to record the performance when those hyperconifgs are running, set the number of finished configs\n        in this round to be 0, and increase the round number.\n\n        Parameters\n        ----------\n        hyper_configs: list\n            the generated hyperconfigs\n        \"\"\"\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()",
  "class HyperbandClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('R'): int,\n            Optional('eta'): int\n        }).validate(kwargs)",
  "class Hyperband(MsgDispatcherBase):\n    \"\"\"Hyperband inherit from MsgDispatcherBase rather than Tuner, because it integrates both tuner's functions and assessor's functions.\n    This is an implementation that could fully leverage available resources, i.e., high parallelism.\n    A single execution of Hyperband takes a finite budget of (s_max + 1)B.\n\n    Parameters\n    ----------\n    R: int\n        the maximum amount of resource that can be allocated to a single configuration\n    eta: int\n        the variable that controls the proportion of configurations discarded in each round of SuccessiveHalving\n    optimize_mode: str\n        optimize mode, 'maximize' or 'minimize'\n    \"\"\"\n\n    def __init__(self, R=60, eta=3, optimize_mode='maximize'):\n        \"\"\"B = (s_max + 1)R\"\"\"\n        super(Hyperband, self).__init__()\n        self.R = R\n        self.eta = eta\n        self.brackets = dict()  # dict of Bracket\n        self.generated_hyper_configs = []  # all the configs waiting for run\n        self.completed_hyper_configs = []  # all the completed configs\n        self.s_max = math.floor(math.log(self.R, self.eta) + _epsilon)\n        self.curr_s = self.s_max\n\n        self.searchspace_json = None\n        self.random_state = None\n        self.optimize_mode = OptimizeMode(optimize_mode)\n\n        # This is for the case that nnimanager requests trial config, but tuner cannot provide immediately.\n        # In this case, tuner increases self.credit to issue a trial config sometime later.\n        self.credit = 0\n\n        # record the latest parameter_id of the trial job trial_job_id.\n        # if there is no running parameter_id, self.job_id_para_id_map[trial_job_id] == None\n        # new trial job is added to this dict and finished trial job is removed from it.\n        self.job_id_para_id_map = dict()\n\n    def handle_initialize(self, data):\n        \"\"\"callback for initializing the advisor\n        Parameters\n        ----------\n        data: dict\n            search space\n        \"\"\"\n        self.handle_update_search_space(data)\n        send(CommandType.Initialized, '')\n\n    def handle_request_trial_jobs(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data: int\n            number of trial jobs\n        \"\"\"\n        for _ in range(data):\n            ret = self._get_one_trial_job()\n            send(CommandType.NewTrialJob, json_tricks.dumps(ret))\n\n    def _get_one_trial_job(self):\n        \"\"\"get one trial job, i.e., one hyperparameter configuration.\"\"\"\n        if not self.generated_hyper_configs:\n            if self.curr_s < 0:\n                self.curr_s = self.s_max\n            _logger.debug('create a new bracket, self.curr_s=%d', self.curr_s)\n            self.brackets[self.curr_s] = Bracket(self.curr_s, self.s_max, self.eta, self.R, self.optimize_mode)\n            next_n, next_r = self.brackets[self.curr_s].get_n_r()\n            _logger.debug('new bracket, next_n=%d, next_r=%d', next_n, next_r)\n            assert self.searchspace_json is not None and self.random_state is not None\n            generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(next_n, next_r,\n                                                                                                   self.searchspace_json,\n                                                                                                   self.random_state)\n            self.generated_hyper_configs = generated_hyper_configs.copy()\n            self.curr_s -= 1\n\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop(0)\n        ret = {\n            'parameter_id': params[0],\n            'parameter_source': 'algorithm',\n            'parameters': params[1]\n        }\n        return ret\n\n    def handle_update_search_space(self, data):\n        \"\"\"data: JSON object, which is search space\n        \"\"\"\n        self.searchspace_json = data\n        self.random_state = np.random.RandomState()\n\n    def _handle_trial_end(self, parameter_id):\n        \"\"\"\n        Parameters\n        ----------\n        parameter_id: parameter id of the finished config\n        \"\"\"\n        bracket_id, i, _ = parameter_id.split('_')\n        hyper_configs = self.brackets[int(bracket_id)].inform_trial_end(int(i))\n        if hyper_configs is not None:\n            _logger.debug('bracket %s next round %s, hyper_configs: %s', bracket_id, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs\n\n    def handle_trial_end(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data: dict()\n            it has three keys: trial_job_id, event, hyper_params\n            trial_job_id: the id generated by training service\n            event: the job's state\n            hyper_params: the hyperparameters (a string) generated and returned by tuner\n        \"\"\"\n        hyper_params = json_tricks.loads(data['hyper_params'])\n        self._handle_trial_end(hyper_params['parameter_id'])\n        if data['trial_job_id'] in self.job_id_para_id_map:\n            del self.job_id_para_id_map[data['trial_job_id']]\n\n    def handle_report_metric_data(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data:\n            it is an object which has keys 'parameter_id', 'value', 'trial_job_id', 'type', 'sequence'.\n\n        Raises\n        ------\n        ValueError\n            Data type not supported\n        \"\"\"\n        if 'value' in data:\n            data['value'] = json_tricks.loads(data['value'])\n        if data['type'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data['trial_job_id'] is not None\n            assert data['parameter_index'] is not None\n            assert data['trial_job_id'] in self.job_id_para_id_map\n            self._handle_trial_end(self.job_id_para_id_map[data['trial_job_id']])\n            ret = self._get_one_trial_job()\n            if data['trial_job_id'] is not None:\n                ret['trial_job_id'] = data['trial_job_id']\n            if data['parameter_index'] is not None:\n                ret['parameter_index'] = data['parameter_index']\n            self.job_id_para_id_map[data['trial_job_id']] = ret['parameter_id']\n            send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        else:\n            value = extract_scalar_reward(data['value'])\n            bracket_id, i, _ = data['parameter_id'].split('_')\n            bracket_id = int(bracket_id)\n\n            # add <trial_job_id, parameter_id> to self.job_id_para_id_map here,\n            # because when the first parameter_id is created, trial_job_id is not known yet.\n            if data['trial_job_id'] in self.job_id_para_id_map:\n                assert self.job_id_para_id_map[data['trial_job_id']] == data['parameter_id']\n            else:\n                self.job_id_para_id_map[data['trial_job_id']] = data['parameter_id']\n\n            if data['type'] == MetricType.FINAL:\n                # sys.maxsize indicates this value is from FINAL metric data, because data['sequence'] from FINAL metric\n                # and PERIODICAL metric are independent, thus, not comparable.\n                self.brackets[bracket_id].set_config_perf(int(i), data['parameter_id'], sys.maxsize, value)\n                self.completed_hyper_configs.append(data)\n            elif data['type'] == MetricType.PERIODICAL:\n                self.brackets[bracket_id].set_config_perf(int(i), data['parameter_id'], data['sequence'], value)\n            else:\n                raise ValueError('Data type not supported: {}'.format(data['type']))\n\n    def handle_add_customized_trial(self, data):\n        pass\n\n    def handle_import_data(self, data):\n        pass",
  "def __init__(self, s, s_max, eta, R, optimize_mode):\n        self.bracket_id = s\n        self.s_max = s_max\n        self.eta = eta\n        self.n = math.ceil((s_max + 1) * (eta ** s) / (s + 1) - _epsilon)\n        self.r = R / eta ** s\n        self.i = 0\n        self.hyper_configs = []  # [ {id: params}, {}, ... ]\n        self.configs_perf = []  # [ {id: [seq, acc]}, {}, ... ]\n        self.num_configs_to_run = []  # [ n, n, n, ... ]\n        self.num_finished_configs = []  # [ n, n, n, ... ]\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.no_more_trial = False",
  "def is_completed(self):\n        \"\"\"check whether this bracket has sent out all the hyperparameter configurations\"\"\"\n        return self.no_more_trial",
  "def get_n_r(self):\n        \"\"\"return the values of n and r for the next round\"\"\"\n        return math.floor(self.n / self.eta ** self.i + _epsilon), math.floor(self.r * self.eta ** self.i + _epsilon)",
  "def increase_i(self):\n        \"\"\"i means the ith round. Increase i by 1\"\"\"\n        self.i += 1\n        if self.i > self.bracket_id:\n            self.no_more_trial = True",
  "def set_config_perf(self, i, parameter_id, seq, value):\n        \"\"\"update trial's latest result with its sequence number, e.g., epoch number or batch number\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        parameter_id: int\n            the id of the trial/parameter\n        seq: int\n            sequence number, e.g., epoch number or batch number\n        value: int\n            latest result with sequence number seq\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if parameter_id in self.configs_perf[i]:\n            if self.configs_perf[i][parameter_id][0] < seq:\n                self.configs_perf[i][parameter_id] = [seq, value]\n        else:\n            self.configs_perf[i][parameter_id] = [seq, value]",
  "def inform_trial_end(self, i):\n        \"\"\"If the trial is finished and the corresponding round (i.e., i) has all its trials finished,\n        it will choose the top k trials for the next round (i.e., i+1)\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        \"\"\"\n        global _KEY\n        self.num_finished_configs[i] += 1\n        _logger.debug('bracket id: %d, round: %d %d, finished: %d, all: %d', self.bracket_id, self.i, i,\n                      self.num_finished_configs[i], self.num_configs_to_run[i])\n        if self.num_finished_configs[i] >= self.num_configs_to_run[i] \\\n                and self.no_more_trial is False:\n            # choose candidate configs from finished configs to run in the next round\n            assert self.i == i + 1\n            this_round_perf = self.configs_perf[i]\n            if self.optimize_mode is OptimizeMode.Maximize:\n                sorted_perf = sorted(this_round_perf.items(), key=lambda kv: kv[1][1], reverse=True)  # reverse\n            else:\n                sorted_perf = sorted(this_round_perf.items(), key=lambda kv: kv[1][1])\n            _logger.debug('bracket %s next round %s, sorted hyper configs: %s', self.bracket_id, self.i, sorted_perf)\n            next_n, next_r = self.get_n_r()\n            _logger.debug('bracket %s next round %s, next_n=%d, next_r=%d', self.bracket_id, self.i, next_n, next_r)\n            hyper_configs = dict()\n            for k in range(next_n):\n                params_id = sorted_perf[k][0]\n                params = self.hyper_configs[i][params_id]\n                params[_KEY] = next_r  # modify r\n                # generate new id\n                increased_id = params_id.split('_')[-1]\n                new_id = create_bracket_parameter_id(self.bracket_id, self.i, increased_id)\n                hyper_configs[new_id] = params\n            self._record_hyper_configs(hyper_configs)\n            return [[key, value] for key, value in hyper_configs.items()]\n        return None",
  "def get_hyperparameter_configurations(self, num, r, searchspace_json, random_state):\n        \"\"\"Randomly generate num hyperparameter configurations from search space\n\n        Parameters\n        ----------\n        num: int\n            the number of hyperparameter configurations\n\n        Returns\n        -------\n        list\n            a list of hyperparameter configurations. Format: [[key1, value1], [key2, value2], ...]\n        \"\"\"\n        global _KEY\n        assert self.i == 0\n        hyperparameter_configs = dict()\n        for _ in range(num):\n            params_id = create_bracket_parameter_id(self.bracket_id, self.i)\n            params = json2parameter(searchspace_json, random_state)\n            params[_KEY] = r\n            hyperparameter_configs[params_id] = params\n        self._record_hyper_configs(hyperparameter_configs)\n        return [[key, value] for key, value in hyperparameter_configs.items()]",
  "def _record_hyper_configs(self, hyper_configs):\n        \"\"\"after generating one round of hyperconfigs, this function records the generated hyperconfigs,\n        creates a dict to record the performance when those hyperconifgs are running, set the number of finished configs\n        in this round to be 0, and increase the round number.\n\n        Parameters\n        ----------\n        hyper_configs: list\n            the generated hyperconfigs\n        \"\"\"\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('R'): int,\n            Optional('eta'): int\n        }).validate(kwargs)",
  "def __init__(self, R=60, eta=3, optimize_mode='maximize'):\n        \"\"\"B = (s_max + 1)R\"\"\"\n        super(Hyperband, self).__init__()\n        self.R = R\n        self.eta = eta\n        self.brackets = dict()  # dict of Bracket\n        self.generated_hyper_configs = []  # all the configs waiting for run\n        self.completed_hyper_configs = []  # all the completed configs\n        self.s_max = math.floor(math.log(self.R, self.eta) + _epsilon)\n        self.curr_s = self.s_max\n\n        self.searchspace_json = None\n        self.random_state = None\n        self.optimize_mode = OptimizeMode(optimize_mode)\n\n        # This is for the case that nnimanager requests trial config, but tuner cannot provide immediately.\n        # In this case, tuner increases self.credit to issue a trial config sometime later.\n        self.credit = 0\n\n        # record the latest parameter_id of the trial job trial_job_id.\n        # if there is no running parameter_id, self.job_id_para_id_map[trial_job_id] == None\n        # new trial job is added to this dict and finished trial job is removed from it.\n        self.job_id_para_id_map = dict()",
  "def handle_initialize(self, data):\n        \"\"\"callback for initializing the advisor\n        Parameters\n        ----------\n        data: dict\n            search space\n        \"\"\"\n        self.handle_update_search_space(data)\n        send(CommandType.Initialized, '')",
  "def handle_request_trial_jobs(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data: int\n            number of trial jobs\n        \"\"\"\n        for _ in range(data):\n            ret = self._get_one_trial_job()\n            send(CommandType.NewTrialJob, json_tricks.dumps(ret))",
  "def _get_one_trial_job(self):\n        \"\"\"get one trial job, i.e., one hyperparameter configuration.\"\"\"\n        if not self.generated_hyper_configs:\n            if self.curr_s < 0:\n                self.curr_s = self.s_max\n            _logger.debug('create a new bracket, self.curr_s=%d', self.curr_s)\n            self.brackets[self.curr_s] = Bracket(self.curr_s, self.s_max, self.eta, self.R, self.optimize_mode)\n            next_n, next_r = self.brackets[self.curr_s].get_n_r()\n            _logger.debug('new bracket, next_n=%d, next_r=%d', next_n, next_r)\n            assert self.searchspace_json is not None and self.random_state is not None\n            generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(next_n, next_r,\n                                                                                                   self.searchspace_json,\n                                                                                                   self.random_state)\n            self.generated_hyper_configs = generated_hyper_configs.copy()\n            self.curr_s -= 1\n\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop(0)\n        ret = {\n            'parameter_id': params[0],\n            'parameter_source': 'algorithm',\n            'parameters': params[1]\n        }\n        return ret",
  "def handle_update_search_space(self, data):\n        \"\"\"data: JSON object, which is search space\n        \"\"\"\n        self.searchspace_json = data\n        self.random_state = np.random.RandomState()",
  "def _handle_trial_end(self, parameter_id):\n        \"\"\"\n        Parameters\n        ----------\n        parameter_id: parameter id of the finished config\n        \"\"\"\n        bracket_id, i, _ = parameter_id.split('_')\n        hyper_configs = self.brackets[int(bracket_id)].inform_trial_end(int(i))\n        if hyper_configs is not None:\n            _logger.debug('bracket %s next round %s, hyper_configs: %s', bracket_id, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs",
  "def handle_trial_end(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data: dict()\n            it has three keys: trial_job_id, event, hyper_params\n            trial_job_id: the id generated by training service\n            event: the job's state\n            hyper_params: the hyperparameters (a string) generated and returned by tuner\n        \"\"\"\n        hyper_params = json_tricks.loads(data['hyper_params'])\n        self._handle_trial_end(hyper_params['parameter_id'])\n        if data['trial_job_id'] in self.job_id_para_id_map:\n            del self.job_id_para_id_map[data['trial_job_id']]",
  "def handle_report_metric_data(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data:\n            it is an object which has keys 'parameter_id', 'value', 'trial_job_id', 'type', 'sequence'.\n\n        Raises\n        ------\n        ValueError\n            Data type not supported\n        \"\"\"\n        if 'value' in data:\n            data['value'] = json_tricks.loads(data['value'])\n        if data['type'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data['trial_job_id'] is not None\n            assert data['parameter_index'] is not None\n            assert data['trial_job_id'] in self.job_id_para_id_map\n            self._handle_trial_end(self.job_id_para_id_map[data['trial_job_id']])\n            ret = self._get_one_trial_job()\n            if data['trial_job_id'] is not None:\n                ret['trial_job_id'] = data['trial_job_id']\n            if data['parameter_index'] is not None:\n                ret['parameter_index'] = data['parameter_index']\n            self.job_id_para_id_map[data['trial_job_id']] = ret['parameter_id']\n            send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        else:\n            value = extract_scalar_reward(data['value'])\n            bracket_id, i, _ = data['parameter_id'].split('_')\n            bracket_id = int(bracket_id)\n\n            # add <trial_job_id, parameter_id> to self.job_id_para_id_map here,\n            # because when the first parameter_id is created, trial_job_id is not known yet.\n            if data['trial_job_id'] in self.job_id_para_id_map:\n                assert self.job_id_para_id_map[data['trial_job_id']] == data['parameter_id']\n            else:\n                self.job_id_para_id_map[data['trial_job_id']] = data['parameter_id']\n\n            if data['type'] == MetricType.FINAL:\n                # sys.maxsize indicates this value is from FINAL metric data, because data['sequence'] from FINAL metric\n                # and PERIODICAL metric are independent, thus, not comparable.\n                self.brackets[bracket_id].set_config_perf(int(i), data['parameter_id'], sys.maxsize, value)\n                self.completed_hyper_configs.append(data)\n            elif data['type'] == MetricType.PERIODICAL:\n                self.brackets[bracket_id].set_config_perf(int(i), data['parameter_id'], data['sequence'], value)\n            else:\n                raise ValueError('Data type not supported: {}'.format(data['type']))",
  "def handle_add_customized_trial(self, data):\n        pass",
  "def handle_import_data(self, data):\n        pass",
  "class Pd:\n    \"\"\"\n    A particular probability distribution\n    \"\"\"\n    def flatparam(self):\n        raise NotImplementedError\n    def mode(self):\n        raise NotImplementedError\n    def neglogp(self, x):\n        # Usually it's easier to define the negative logprob\n        raise NotImplementedError\n    def kl(self, other):\n        raise NotImplementedError\n    def entropy(self):\n        raise NotImplementedError\n    def sample(self):\n        raise NotImplementedError\n    def logp(self, x):\n        return - self.neglogp(x)\n    def get_shape(self):\n        return self.flatparam().shape\n    @property\n    def shape(self):\n        return self.get_shape()\n    def __getitem__(self, idx):\n        return self.__class__(self.flatparam()[idx])",
  "class PdType:\n    \"\"\"\n    Parametrized family of probability distributions\n    \"\"\"\n    def pdclass(self):\n        raise NotImplementedError\n    def pdfromflat(self, flat, mask, nsteps, size, is_act_model):\n        return self.pdclass()(flat, mask, nsteps, size, is_act_model)\n    def pdfromlatent(self, latent_vector, init_scale, init_bias):\n        raise NotImplementedError\n    def param_shape(self):\n        raise NotImplementedError\n    def sample_shape(self):\n        raise NotImplementedError\n    def sample_dtype(self):\n        raise NotImplementedError\n\n    def param_placeholder(self, prepend_shape, name=None):\n        return tf.placeholder(dtype=tf.float32, shape=prepend_shape+self.param_shape(), name=name)\n    def sample_placeholder(self, prepend_shape, name=None):\n        return tf.placeholder(dtype=self.sample_dtype(), shape=prepend_shape+self.sample_shape(), name=name)",
  "class CategoricalPd(Pd):\n    \"\"\"\n    Categorical probability distribution\n    \"\"\"\n    def __init__(self, logits, mask_npinf, nsteps, size, is_act_model):\n        self.logits = logits\n        self.mask_npinf = mask_npinf\n        self.nsteps = nsteps\n        self.size = size\n        self.is_act_model = is_act_model\n    def flatparam(self):\n        return self.logits\n    def mode(self):\n        return tf.argmax(self.logits, axis=-1)\n\n    @property\n    def mean(self):\n        return tf.nn.softmax(self.logits)\n    def neglogp(self, x):\n        \"\"\"\n        return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n        Note: we can't use sparse_softmax_cross_entropy_with_logits because\n              the implementation does not allow second-order derivatives...\n        \"\"\"\n        if x.dtype in {tf.uint8, tf.int32, tf.int64}:\n            # one-hot encoding\n            x_shape_list = x.shape.as_list()\n            logits_shape_list = self.logits.get_shape().as_list()[:-1]\n            for xs, ls in zip(x_shape_list, logits_shape_list):\n                if xs is not None and ls is not None:\n                    assert xs == ls, 'shape mismatch: {} in x vs {} in logits'.format(xs, ls)\n\n            x = tf.one_hot(x, self.logits.get_shape().as_list()[-1])\n        else:\n            # already encoded\n            assert x.shape.as_list() == self.logits.shape.as_list()\n\n        return tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=self.logits,\n            labels=x)\n\n    def kl(self, other):\n        \"\"\"kl\"\"\"\n        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)\n        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keepdims=True)\n        ea0 = tf.exp(a0)\n        ea1 = tf.exp(a1)\n        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n        z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)\n\n    def entropy(self):\n        \"\"\"compute entropy\"\"\"\n        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)\n        ea0 = tf.exp(a0)\n        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n\n    def sample(self):\n        \"\"\"sample from logits\"\"\"\n        if not self.is_act_model:\n            re_res = tf.reshape(self.logits, [-1, self.nsteps, self.size])\n            masked_res = tf.math.add(re_res, self.mask_npinf)\n            re_masked_res = tf.reshape(masked_res, [-1, self.size])\n\n            u = tf.random_uniform(tf.shape(re_masked_res), dtype=self.logits.dtype)\n            return tf.argmax(re_masked_res - tf.log(-1*tf.log(u)), axis=-1)\n        else:\n            u = tf.random_uniform(tf.shape(self.logits), dtype=self.logits.dtype)\n            return tf.argmax(self.logits - tf.log(-1*tf.log(u)), axis=-1)\n\n    @classmethod\n    def fromflat(cls, flat):\n        return cls(flat)",
  "class CategoricalPdType(PdType):\n    \"\"\"\n    To create CategoricalPd\n    \"\"\"\n    def __init__(self, ncat, nsteps, np_mask, is_act_model):\n        self.ncat = ncat\n        self.nsteps = nsteps\n        self.np_mask = np_mask\n        self.is_act_model = is_act_model\n    def pdclass(self):\n        return CategoricalPd\n\n    def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):\n        \"\"\"add fc and create CategoricalPd\"\"\"\n        pdparam, mask, mask_npinf = _matching_fc(latent_vector, 'pi', self.ncat, self.nsteps,\n                                                 init_scale=init_scale, init_bias=init_bias,\n                                                 np_mask=self.np_mask, is_act_model=self.is_act_model)\n        return self.pdfromflat(pdparam, mask_npinf, self.nsteps, self.ncat, self.is_act_model), pdparam, mask, mask_npinf\n\n    def param_shape(self):\n        return [self.ncat]\n    def sample_shape(self):\n        return []\n    def sample_dtype(self):\n        return tf.int32",
  "def _matching_fc(tensor, name, size, nsteps, init_scale, init_bias, np_mask, is_act_model):\n    \"\"\"\n    Add fc op, and add mask op when not in action mode\n    \"\"\"\n    if tensor.shape[-1] == size:\n        assert False\n        return tensor\n    else:\n        mask = tf.get_variable(\"act_mask\", dtype=tf.float32, initializer=np_mask[0], trainable=False)\n        mask_npinf = tf.get_variable(\"act_mask_npinf\", dtype=tf.float32, initializer=np_mask[1], trainable=False)\n        res = fc(tensor, name, size, init_scale=init_scale, init_bias=init_bias)\n        if not is_act_model:\n            re_res = tf.reshape(res, [-1, nsteps, size])\n            masked_res = tf.math.multiply(re_res, mask)\n            re_masked_res = tf.reshape(masked_res, [-1, size])\n            return re_masked_res, mask, mask_npinf\n        else:\n            return res, mask, mask_npinf",
  "def flatparam(self):\n        raise NotImplementedError",
  "def mode(self):\n        raise NotImplementedError",
  "def neglogp(self, x):\n        # Usually it's easier to define the negative logprob\n        raise NotImplementedError",
  "def kl(self, other):\n        raise NotImplementedError",
  "def entropy(self):\n        raise NotImplementedError",
  "def sample(self):\n        raise NotImplementedError",
  "def logp(self, x):\n        return - self.neglogp(x)",
  "def get_shape(self):\n        return self.flatparam().shape",
  "def shape(self):\n        return self.get_shape()",
  "def __getitem__(self, idx):\n        return self.__class__(self.flatparam()[idx])",
  "def pdclass(self):\n        raise NotImplementedError",
  "def pdfromflat(self, flat, mask, nsteps, size, is_act_model):\n        return self.pdclass()(flat, mask, nsteps, size, is_act_model)",
  "def pdfromlatent(self, latent_vector, init_scale, init_bias):\n        raise NotImplementedError",
  "def param_shape(self):\n        raise NotImplementedError",
  "def sample_shape(self):\n        raise NotImplementedError",
  "def sample_dtype(self):\n        raise NotImplementedError",
  "def param_placeholder(self, prepend_shape, name=None):\n        return tf.placeholder(dtype=tf.float32, shape=prepend_shape+self.param_shape(), name=name)",
  "def sample_placeholder(self, prepend_shape, name=None):\n        return tf.placeholder(dtype=self.sample_dtype(), shape=prepend_shape+self.sample_shape(), name=name)",
  "def __init__(self, logits, mask_npinf, nsteps, size, is_act_model):\n        self.logits = logits\n        self.mask_npinf = mask_npinf\n        self.nsteps = nsteps\n        self.size = size\n        self.is_act_model = is_act_model",
  "def flatparam(self):\n        return self.logits",
  "def mode(self):\n        return tf.argmax(self.logits, axis=-1)",
  "def mean(self):\n        return tf.nn.softmax(self.logits)",
  "def neglogp(self, x):\n        \"\"\"\n        return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n        Note: we can't use sparse_softmax_cross_entropy_with_logits because\n              the implementation does not allow second-order derivatives...\n        \"\"\"\n        if x.dtype in {tf.uint8, tf.int32, tf.int64}:\n            # one-hot encoding\n            x_shape_list = x.shape.as_list()\n            logits_shape_list = self.logits.get_shape().as_list()[:-1]\n            for xs, ls in zip(x_shape_list, logits_shape_list):\n                if xs is not None and ls is not None:\n                    assert xs == ls, 'shape mismatch: {} in x vs {} in logits'.format(xs, ls)\n\n            x = tf.one_hot(x, self.logits.get_shape().as_list()[-1])\n        else:\n            # already encoded\n            assert x.shape.as_list() == self.logits.shape.as_list()\n\n        return tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=self.logits,\n            labels=x)",
  "def kl(self, other):\n        \"\"\"kl\"\"\"\n        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)\n        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keepdims=True)\n        ea0 = tf.exp(a0)\n        ea1 = tf.exp(a1)\n        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n        z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)",
  "def entropy(self):\n        \"\"\"compute entropy\"\"\"\n        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)\n        ea0 = tf.exp(a0)\n        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)",
  "def sample(self):\n        \"\"\"sample from logits\"\"\"\n        if not self.is_act_model:\n            re_res = tf.reshape(self.logits, [-1, self.nsteps, self.size])\n            masked_res = tf.math.add(re_res, self.mask_npinf)\n            re_masked_res = tf.reshape(masked_res, [-1, self.size])\n\n            u = tf.random_uniform(tf.shape(re_masked_res), dtype=self.logits.dtype)\n            return tf.argmax(re_masked_res - tf.log(-1*tf.log(u)), axis=-1)\n        else:\n            u = tf.random_uniform(tf.shape(self.logits), dtype=self.logits.dtype)\n            return tf.argmax(self.logits - tf.log(-1*tf.log(u)), axis=-1)",
  "def fromflat(cls, flat):\n        return cls(flat)",
  "def __init__(self, ncat, nsteps, np_mask, is_act_model):\n        self.ncat = ncat\n        self.nsteps = nsteps\n        self.np_mask = np_mask\n        self.is_act_model = is_act_model",
  "def pdclass(self):\n        return CategoricalPd",
  "def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):\n        \"\"\"add fc and create CategoricalPd\"\"\"\n        pdparam, mask, mask_npinf = _matching_fc(latent_vector, 'pi', self.ncat, self.nsteps,\n                                                 init_scale=init_scale, init_bias=init_bias,\n                                                 np_mask=self.np_mask, is_act_model=self.is_act_model)\n        return self.pdfromflat(pdparam, mask_npinf, self.nsteps, self.ncat, self.is_act_model), pdparam, mask, mask_npinf",
  "def param_shape(self):\n        return [self.ncat]",
  "def sample_shape(self):\n        return []",
  "def sample_dtype(self):\n        return tf.int32",
  "def set_global_seeds(i):\n    \"\"\"set global seeds\"\"\"\n    rank = 0\n    myseed = i  + 1000 * rank if i is not None else None\n    tf.set_random_seed(myseed)\n    np.random.seed(myseed)\n    random.seed(myseed)",
  "def batch_to_seq(h, nbatch, nsteps, flat=False):\n    \"\"\"convert from batch to sequence\"\"\"\n    if flat:\n        h = tf.reshape(h, [nbatch, nsteps])\n    else:\n        h = tf.reshape(h, [nbatch, nsteps, -1])\n    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]",
  "def seq_to_batch(h, flat=False):\n    \"\"\"convert from sequence to batch\"\"\"\n    shape = h[0].get_shape().as_list()\n    if not flat:\n        assert len(shape) > 1\n        nh = h[0].get_shape()[-1].value\n        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])\n    else:\n        return tf.reshape(tf.stack(values=h, axis=1), [-1])",
  "def lstm(xs, ms, s, scope, nh, init_scale=1.0):\n    \"\"\"lstm cell\"\"\"\n    _, nin = [v.value for v in xs[0].get_shape()] # the first is nbatch\n    with tf.variable_scope(scope):\n        wx = tf.get_variable(\"wx\", [nin, nh*4], initializer=ortho_init(init_scale))\n        wh = tf.get_variable(\"wh\", [nh, nh*4], initializer=ortho_init(init_scale))\n        b = tf.get_variable(\"b\", [nh*4], initializer=tf.constant_initializer(0.0))\n\n    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)\n    for idx, (x, m) in enumerate(zip(xs, ms)):\n        c = c*(1-m)\n        h = h*(1-m)\n        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)\n        i = tf.nn.sigmoid(i)\n        f = tf.nn.sigmoid(f)\n        o = tf.nn.sigmoid(o)\n        u = tf.tanh(u)\n        c = f*c + i*u\n        h = o*tf.tanh(c)\n        xs[idx] = h\n    s = tf.concat(axis=1, values=[c, h])\n    return xs, s",
  "def lstm_model(nlstm=128, layer_norm=False):\n    \"\"\"\n    Builds LSTM (Long-Short Term Memory) network to be used in a policy.\n    Note that the resulting function returns not only the output of the LSTM\n    (i.e. hidden state of lstm for each step in the sequence), but also a dictionary\n    with auxiliary tensors to be set as policy attributes.\n\n    Specifically,\n        S is a placeholder to feed current state (LSTM state has to be managed outside policy)\n        M is a placeholder for the mask (used to mask out observations after the end of the episode, but can be used for other purposes too)\n        initial_state is a numpy array containing initial lstm state (usually zeros)\n        state is the output LSTM state (to be fed into S at the next call)\n\n\n    An example of usage of lstm-based policy can be found here: common/tests/test_doc_examples.py/test_lstm_example\n\n    Parameters\n    ----------\n    nlstm : int\n        LSTM hidden state size\n    layer_norm : bool\n        if True, layer-normalized version of LSTM is used\n\n    Returns\n    -------\n    function that builds LSTM with a given input tensor / placeholder\n    \"\"\"\n\n    def network_fn(X, nenv=1, obs_size=-1):\n        with tf.variable_scope(\"emb\", reuse=tf.AUTO_REUSE):\n            w_emb = tf.get_variable(\"w_emb\", [obs_size+1, 32])\n            X = tf.nn.embedding_lookup(w_emb, X)\n\n        nbatch = X.shape[0]\n        nsteps = nbatch // nenv\n\n        h = tf.layers.flatten(X)\n\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, 2*nlstm]) #states\n\n        xs = batch_to_seq(h, nenv, nsteps)\n        ms = batch_to_seq(M, nenv, nsteps)\n\n        assert not layer_norm\n        h5, snew = lstm(xs, ms, S, scope='lstm', nh=nlstm)\n\n        h = seq_to_batch(h5)\n        initial_state = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {'S':S, 'M':M, 'state':snew, 'initial_state':initial_state}\n\n    return network_fn",
  "def ortho_init(scale=1.0):\n    \"\"\"init approach\"\"\"\n    def _ortho_init(shape, dtype, partition_info=None):\n        #lasagne ortho init for tf\n        shape = tuple(shape)\n        if len(shape) == 2:\n            flat_shape = shape\n        elif len(shape) == 4: # assumes NHWC\n            flat_shape = (np.prod(shape[:-1]), shape[-1])\n        else:\n            raise NotImplementedError\n        a = np.random.normal(0.0, 1.0, flat_shape)\n        u, _, v = np.linalg.svd(a, full_matrices=False)\n        q = u if u.shape == flat_shape else v # pick the one with the correct shape\n        q = q.reshape(shape)\n        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)\n    return _ortho_init",
  "def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):\n    \"\"\"fully connected op\"\"\"\n    with tf.variable_scope(scope):\n        nin = x.get_shape()[1].value\n        w = tf.get_variable(\"w\", [nin, nh], initializer=ortho_init(init_scale))\n        b = tf.get_variable(\"b\", [nh], initializer=tf.constant_initializer(init_bias))\n        return tf.matmul(x, w)+b",
  "def _check_shape(placeholder_shape, data_shape):\n    \"\"\"\n    check if two shapes are compatible (i.e. differ only by dimensions of size 1, or by the batch dimension)\n    \"\"\"\n\n    return True",
  "def adjust_shape(placeholder, data):\n    \"\"\"\n    adjust shape of the data to the shape of the placeholder if possible.\n    If shape is incompatible, AssertionError is thrown\n\n    Parameters\n    ----------\n    placeholder\n        tensorflow input placeholder\n    data\n        input data to be (potentially) reshaped to be fed into placeholder\n\n    Returns\n    -------\n    reshaped data\n    \"\"\"\n    if not isinstance(data, np.ndarray) and not isinstance(data, list):\n        return data\n    if isinstance(data, list):\n        data = np.array(data)\n\n    placeholder_shape = [x or -1 for x in placeholder.shape.as_list()]\n\n    assert _check_shape(placeholder_shape, data.shape), \\\n        'Shape of data {} is not compatible with shape of the placeholder {}'.format(data.shape, placeholder_shape)\n\n    return np.reshape(data, placeholder_shape)",
  "def get_session(config=None):\n    \"\"\"Get default session or create one with a given config\"\"\"\n    sess = tf.get_default_session()\n    if sess is None:\n        sess = make_session(config=config, make_default=True)\n    return sess",
  "def make_session(config=None, num_cpu=None, make_default=False, graph=None):\n    \"\"\"Returns a session that will use <num_cpu> CPU's only\"\"\"\n    if num_cpu is None:\n        num_cpu = int(os.getenv('RCALL_NUM_CPU', multiprocessing.cpu_count()))\n    if config is None:\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            inter_op_parallelism_threads=num_cpu,\n            intra_op_parallelism_threads=num_cpu)\n        config.gpu_options.allow_growth = True\n\n    if make_default:\n        return tf.InteractiveSession(config=config, graph=graph)\n    else:\n        return tf.Session(config=config, graph=graph)",
  "def initialize():\n    \"\"\"Initialize all the uninitialized variables in the global scope.\"\"\"\n    new_variables = set(tf.global_variables()) - ALREADY_INITIALIZED\n    get_session().run(tf.variables_initializer(new_variables))\n\n    ALREADY_INITIALIZED.update(new_variables)",
  "def observation_placeholder(ob_space, batch_size=None, name='Ob'):\n    \"\"\"\n    Create placeholder to feed observations into of the size appropriate to the observation space\n\n    Parameters\n    ----------\n    ob_space : gym.Space\n        observation space\n    batch_size : int\n        size of the batch to be fed into input. Can be left None in most cases.\n    name : str\n        name of the placeholder\n\n    Returns\n    -------\n    tensorflow placeholder tensor\n    \"\"\"\n\n    assert isinstance(ob_space, (Discrete, Box, MultiDiscrete)), \\\n        'Can only deal with Discrete and Box observation spaces for now'\n\n    dtype = ob_space.dtype\n    if dtype == np.int8:\n        dtype = np.uint8\n\n    return tf.placeholder(shape=(batch_size,) + ob_space.shape, dtype=dtype, name=name)",
  "def explained_variance(ypred, y):\n    \"\"\"\n    Computes fraction of variance that ypred explains about y.\n    Returns 1 - Var[y-ypred] / Var[y]\n\n    interpretation:\n        ev=0  =>  might as well have predicted zero\n        ev=1  =>  perfect prediction\n        ev<0  =>  worse than just predicting zero\n\n    \"\"\"\n    assert y.ndim == 1 and ypred.ndim == 1\n    vary = np.var(y)\n    return np.nan if vary == 0 else 1 - np.var(y-ypred)/vary",
  "def network_fn(X, nenv=1, obs_size=-1):\n        with tf.variable_scope(\"emb\", reuse=tf.AUTO_REUSE):\n            w_emb = tf.get_variable(\"w_emb\", [obs_size+1, 32])\n            X = tf.nn.embedding_lookup(w_emb, X)\n\n        nbatch = X.shape[0]\n        nsteps = nbatch // nenv\n\n        h = tf.layers.flatten(X)\n\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, 2*nlstm]) #states\n\n        xs = batch_to_seq(h, nenv, nsteps)\n        ms = batch_to_seq(M, nenv, nsteps)\n\n        assert not layer_norm\n        h5, snew = lstm(xs, ms, S, scope='lstm', nh=nlstm)\n\n        h = seq_to_batch(h5)\n        initial_state = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {'S':S, 'M':M, 'state':snew, 'initial_state':initial_state}",
  "def _ortho_init(shape, dtype, partition_info=None):\n        #lasagne ortho init for tf\n        shape = tuple(shape)\n        if len(shape) == 2:\n            flat_shape = shape\n        elif len(shape) == 4: # assumes NHWC\n            flat_shape = (np.prod(shape[:-1]), shape[-1])\n        else:\n            raise NotImplementedError\n        a = np.random.normal(0.0, 1.0, flat_shape)\n        u, _, v = np.linalg.svd(a, full_matrices=False)\n        q = u if u.shape == flat_shape else v # pick the one with the correct shape\n        q = q.reshape(shape)\n        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)",
  "def _constfn(val):\n    \"\"\"\n    Wrap as function\n    \"\"\"\n    def f(_):\n        return val\n    return f",
  "class ModelConfig:\n    \"\"\"\n    Configurations of the PPO model\n    \"\"\"\n    def __init__(self):\n        self.observation_space = None\n        self.action_space = None\n        self.num_envs = 0\n        self.nsteps = 0\n\n        self.ent_coef = 0.0\n        self.lr = 3e-4\n        self.vf_coef = 0.5\n        self.max_grad_norm = 0.5\n        self.gamma = 0.99\n        self.lam = 0.95\n        self.cliprange = 0.2\n        self.embedding_size = None  # the embedding is for each action\n\n        self.noptepochs = 4         # number of training epochs per update\n        self.total_timesteps = 5000 # number of timesteps (i.e. number of actions taken in the environment)\n        self.nminibatches = 4",
  "class TrialsInfo:\n    \"\"\"\n    Informations of each trial from one model inference\n    \"\"\"\n    def __init__(self, obs, actions, values, neglogpacs, dones, last_value, inf_batch_size):\n        self.iter = 0\n        self.obs = obs\n        self.actions = actions\n        self.values = values\n        self.neglogpacs = neglogpacs\n        self.dones = dones\n        self.last_value = last_value\n\n        self.rewards = None\n        self.returns = None\n\n        self.inf_batch_size = inf_batch_size\n        #self.states = None\n\n    def get_next(self):\n        \"\"\"\n        Get actions of the next trial\n        \"\"\"\n        if self.iter >= self.inf_batch_size:\n            return None, None\n        actions = []\n        for step in self.actions:\n            actions.append(step[self.iter])\n        self.iter += 1\n        return self.iter - 1, actions\n\n    def update_rewards(self, rewards, returns):\n        \"\"\"\n        After the trial is finished, reward and return of this trial is updated\n        \"\"\"\n        self.rewards = rewards\n        self.returns = returns\n\n    def convert_shape(self):\n        \"\"\"\n        Convert shape\n        \"\"\"\n        def sf01(arr):\n            \"\"\"\n            swap and then flatten axes 0 and 1\n            \"\"\"\n            s = arr.shape\n            return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n        self.obs = sf01(self.obs)\n        self.returns = sf01(self.returns)\n        self.dones = sf01(self.dones)\n        self.actions = sf01(self.actions)\n        self.values = sf01(self.values)\n        self.neglogpacs = sf01(self.neglogpacs)",
  "class PPOModel:\n    \"\"\"\n    PPO Model\n    \"\"\"\n    def __init__(self, model_config, mask):\n        self.model_config = model_config\n        self.states = None    # initial state of lstm in policy/value network\n        self.nupdates = None  # the number of func train is invoked, used to tune lr and cliprange\n        self.cur_update = 1   # record the current update\n        self.np_mask = mask   # record the mask of each action within one trial\n\n        set_global_seeds(None)\n        assert isinstance(self.model_config.lr, float)\n        self.lr = _constfn(self.model_config.lr)\n        assert isinstance(self.model_config.cliprange, float)\n        self.cliprange = _constfn(self.model_config.cliprange)\n\n        # build lstm policy network, value share the same network\n        policy = build_lstm_policy(model_config)\n\n        # Get the nb of env\n        nenvs = model_config.num_envs\n\n        # Calculate the batch_size\n        self.nbatch = nbatch = nenvs * model_config.nsteps # num of record per update\n        nbatch_train = nbatch // model_config.nminibatches # get batch size\n        # self.nupdates is used to tune lr and cliprange\n        self.nupdates = self.model_config.total_timesteps // self.nbatch\n\n        # Instantiate the model object (that creates act_model and train_model)\n        self.model = Model(policy=policy, nbatch_act=nenvs, nbatch_train=nbatch_train,\n                           nsteps=model_config.nsteps, ent_coef=model_config.ent_coef, vf_coef=model_config.vf_coef,\n                           max_grad_norm=model_config.max_grad_norm, np_mask=self.np_mask)\n\n        self.states = self.model.initial_state\n\n        logger.info('=== finished PPOModel initialization')\n\n    def inference(self, num):\n        \"\"\"\n        Generate actions along with related info from policy network.\n        observation is the action of the last step.\n\n        Parameters\n        ----------\n        num: int\n            The number of trials to generate\n\n        Returns\n        -------\n        mb_obs : list\n            Observation of the ``num`` configurations\n        mb_actions : list\n            Actions of the ``num`` configurations\n        mb_values : list\n            Values from the value function of the ``num`` configurations\n        mb_neglogpacs : list\n            ``neglogp`` of the ``num`` configurations\n        mb_dones : list\n            To show whether the play is done, always ``True``\n        last_values : tensorflow tensor\n            The last values of the ``num`` configurations, got with session run\n        \"\"\"\n        # Here, we init the lists that will contain the mb of experiences\n        mb_obs, mb_actions, mb_values, mb_dones, mb_neglogpacs = [], [], [], [], []\n        # initial observation\n        # use the (n+1)th embedding to represent the first step action\n        first_step_ob = self.model_config.action_space.n\n        obs = [first_step_ob for _ in range(num)]\n        dones = [True for _ in range(num)]\n        states = self.states\n        # For n in range number of steps\n        for cur_step in range(self.model_config.nsteps):\n            # Given observations, get action value and neglopacs\n            # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init\n            actions, values, states, neglogpacs = self.model.step(cur_step, obs, S=states, M=dones)\n            mb_obs.append(obs.copy())\n            mb_actions.append(actions)\n            mb_values.append(values)\n            mb_neglogpacs.append(neglogpacs)\n            mb_dones.append(dones)\n\n            # Take actions in env and look the results\n            # Infos contains a ton of useful informations\n            obs[:] = actions\n            if cur_step == self.model_config.nsteps - 1:\n                dones = [True for _ in range(num)]\n            else:\n                dones = [False for _ in range(num)]\n\n        #batch of steps to batch of rollouts\n        np_obs = np.asarray(obs)\n        mb_obs = np.asarray(mb_obs, dtype=np_obs.dtype)\n        mb_actions = np.asarray(mb_actions)\n        mb_values = np.asarray(mb_values, dtype=np.float32)\n        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n        last_values = self.model.value(np_obs, S=states, M=dones)\n\n        return mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values\n\n    def compute_rewards(self, trials_info, trials_result):\n        \"\"\"\n        Compute the rewards of the trials in trials_info based on trials_result,\n        and update the rewards in trials_info\n\n        Parameters\n        ----------\n        trials_info : TrialsInfo\n            Info of the generated trials\n        trials_result : list\n            Final results (e.g., acc) of the generated trials\n        \"\"\"\n        mb_rewards = np.asarray([trials_result for _ in trials_info.actions], dtype=np.float32)\n        # discount/bootstrap off value fn\n        mb_returns = np.zeros_like(mb_rewards)\n        mb_advs = np.zeros_like(mb_rewards)\n        lastgaelam = 0\n        last_dones = np.asarray([True for _ in trials_result], dtype=np.bool) # ugly\n        for t in reversed(range(self.model_config.nsteps)):\n            if t == self.model_config.nsteps - 1:\n                nextnonterminal = 1.0 - last_dones\n                nextvalues = trials_info.last_value\n            else:\n                nextnonterminal = 1.0 - trials_info.dones[t+1]\n                nextvalues = trials_info.values[t+1]\n            delta = mb_rewards[t] + self.model_config.gamma * nextvalues * nextnonterminal - trials_info.values[t]\n            lastgaelam = delta + self.model_config.gamma * self.model_config.lam * nextnonterminal * lastgaelam\n            mb_advs[t] = lastgaelam # pylint: disable=unsupported-assignment-operation\n        mb_returns = mb_advs + trials_info.values\n\n        trials_info.update_rewards(mb_rewards, mb_returns)\n        trials_info.convert_shape()\n\n    def train(self, trials_info, nenvs):\n        \"\"\"\n        Train the policy/value network using trials_info\n\n        Parameters\n        ----------\n        trials_info : TrialsInfo\n            Complete info of the generated trials from the previous inference\n        nenvs : int\n            The batch size of the (previous) inference\n        \"\"\"\n        # keep frac decay for future optimization\n        if self.cur_update <= self.nupdates:\n            frac = 1.0 - (self.cur_update - 1.0) / self.nupdates\n        else:\n            logger.warning('current update (self.cur_update) %d has exceeded total updates (self.nupdates) %d',\n                           self.cur_update, self.nupdates)\n            frac = 1.0 - (self.nupdates - 1.0) / self.nupdates\n        lrnow = self.lr(frac)\n        cliprangenow = self.cliprange(frac)\n        self.cur_update += 1\n\n        states = self.states\n\n        assert states is not None # recurrent version\n        assert nenvs % self.model_config.nminibatches == 0\n        envsperbatch = nenvs // self.model_config.nminibatches\n        envinds = np.arange(nenvs)\n        flatinds = np.arange(nenvs * self.model_config.nsteps).reshape(nenvs, self.model_config.nsteps)\n        for _ in range(self.model_config.noptepochs):\n            np.random.shuffle(envinds)\n            for start in range(0, nenvs, envsperbatch):\n                end = start + envsperbatch\n                mbenvinds = envinds[start:end]\n                mbflatinds = flatinds[mbenvinds].ravel()\n                slices = (arr[mbflatinds] for arr in (trials_info.obs, trials_info.returns, trials_info.dones,\n                                                      trials_info.actions, trials_info.values, trials_info.neglogpacs))\n                mbstates = states[mbenvinds]\n                self.model.train(lrnow, cliprangenow, *slices, mbstates)",
  "class PPOClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('trials_per_update'): self.range('trials_per_update', int, 0, 99999),\n            Optional('epochs_per_update'): self.range('epochs_per_update', int, 0, 99999),\n            Optional('minibatch_size'): self.range('minibatch_size', int, 0, 99999),\n            Optional('ent_coef'): float,\n            Optional('lr'): float,\n            Optional('vf_coef'): float,\n            Optional('max_grad_norm'): float,\n            Optional('gamma'):  float,\n            Optional('lam'):  float,\n            Optional('cliprange'): float,\n        }).validate(kwargs)",
  "class PPOTuner(Tuner):\n    \"\"\"\n    PPOTuner, the implementation inherits the main logic of the implementation\n    [ppo2 from openai](https://github.com/openai/baselines/tree/master/baselines/ppo2), and is adapted for NAS scenario.\n    It uses ``lstm`` for its policy network and value network, policy and value share the same network.\n    \"\"\"\n\n    def __init__(self, optimize_mode, trials_per_update=20, epochs_per_update=4, minibatch_size=4,\n                 ent_coef=0.0, lr=3e-4, vf_coef=0.5, max_grad_norm=0.5, gamma=0.99, lam=0.95, cliprange=0.2):\n        \"\"\"\n        Initialization, PPO model is not initialized here as search space is not received yet.\n\n        Parameters\n        ----------\n        optimize_mode : str\n            maximize or minimize\n        trials_per_update : int\n            Number of trials to have for each model update\n        epochs_per_update : int\n            Number of epochs to run for each model update\n        minibatch_size : int\n            Minibatch size (number of trials) for the update\n        ent_coef : float\n            Policy entropy coefficient in the optimization objective\n        lr : float\n            Learning rate of the model (lstm network), constant\n        vf_coef : float\n            Value function loss coefficient in the optimization objective\n        max_grad_norm : float\n            Gradient norm clipping coefficient\n        gamma : float\n            Discounting factor\n        lam : float\n            Advantage estimation discounting factor (lambda in the paper)\n        cliprange : float\n            Cliprange in the PPO algorithm, constant\n        \"\"\"\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.model_config = ModelConfig()\n        self.model = None\n        self.search_space = None\n        self.running_trials = {}                  # key: parameter_id, value: actions/states/etc.\n        self.inf_batch_size = trials_per_update   # number of trials to generate in one inference\n        self.first_inf = True                     # indicate whether it is the first time to inference new trials\n        self.trials_result = [None for _ in range(self.inf_batch_size)] # results of finished trials\n\n        self.credit = 0 # record the unsatisfied trial requests\n        self.param_ids = []\n        self.finished_trials = 0\n        self.chosen_arch_template = {}\n\n        self.actions_spaces = None\n        self.actions_to_config = None\n        self.full_act_space = None\n        self.trials_info = None\n\n        self.all_trials = {} # used to dedup the same trial, key: config, value: final result\n\n        self.model_config.num_envs = self.inf_batch_size\n        self.model_config.noptepochs = epochs_per_update\n        self.model_config.nminibatches = minibatch_size\n\n        self.send_trial_callback = None\n        logger.info('Finished PPOTuner initialization')\n\n    def _process_nas_space(self, search_space):\n        actions_spaces = []\n        actions_to_config = []\n        for key, val in search_space.items():\n            if val['_type'] == 'layer_choice':\n                actions_to_config.append((key, 'layer_choice'))\n                actions_spaces.append(val['_value'])\n                self.chosen_arch_template[key] = None\n            elif val['_type'] == 'input_choice':\n                candidates = val['_value']['candidates']\n                n_chosen = val['_value']['n_chosen']\n                if n_chosen not in [0, 1, [0, 1]]:\n                    raise ValueError('Optional_input_size can only be 0, 1, or [0, 1], but the pecified one is %s'\n                                     % (n_chosen))\n                if isinstance(n_chosen, list):\n                    actions_to_config.append((key, 'input_choice'))\n                    # FIXME: risk, candidates might also have None\n                    actions_spaces.append(['None', *candidates])\n                    self.chosen_arch_template[key] = None\n                elif n_chosen == 1:\n                    actions_to_config.append((key, 'input_choice'))\n                    actions_spaces.append(candidates)\n                    self.chosen_arch_template[key] = None\n                elif n_chosen == 0:\n                    self.chosen_arch_template[key] = []\n            else:\n                raise ValueError('Unsupported search space type: %s' % (val['_type']))\n\n        # calculate observation space\n        dedup = {}\n        for step in actions_spaces:\n            for action in step:\n                dedup[action] = 1\n        full_act_space = [act for act, _ in dedup.items()]\n        assert len(full_act_space) == len(dedup)\n        observation_space = len(full_act_space)\n        nsteps = len(actions_spaces)\n\n        return actions_spaces, actions_to_config, full_act_space, observation_space, nsteps\n\n    def _generate_action_mask(self):\n        \"\"\"\n        Different step could have different action space. to deal with this case, we merge all the\n        possible actions into one action space, and use mask to indicate available actions for each step\n        \"\"\"\n        two_masks = []\n\n        mask = []\n        for acts in self.actions_spaces:\n            one_mask = [0 for _ in range(len(self.full_act_space))]\n            for act in acts:\n                idx = self.full_act_space.index(act)\n                one_mask[idx] = 1\n            mask.append(one_mask)\n        two_masks.append(mask)\n\n        mask = []\n        for acts in self.actions_spaces:\n            one_mask = [-np.inf for _ in range(len(self.full_act_space))]\n            for act in acts:\n                idx = self.full_act_space.index(act)\n                one_mask[idx] = 0\n            mask.append(one_mask)\n        two_masks.append(mask)\n\n        return np.asarray(two_masks, dtype=np.float32)\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Get search space, currently the space only includes that for NAS\n\n        Parameters\n        ----------\n        search_space : dict\n            Search space for NAS\n            the format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        \"\"\"\n        logger.info('update search space %s', search_space)\n        assert self.search_space is None\n        self.search_space = search_space\n\n        assert self.model_config.observation_space is None\n        assert self.model_config.action_space is None\n\n        self.actions_spaces, self.actions_to_config, self.full_act_space, obs_space, nsteps = self._process_nas_space(search_space)\n\n        self.model_config.observation_space = spaces.Discrete(obs_space)\n        self.model_config.action_space = spaces.Discrete(obs_space)\n        self.model_config.nsteps = nsteps\n\n        # generate mask in numpy\n        mask = self._generate_action_mask()\n\n        assert self.model is None\n        self.model = PPOModel(self.model_config, mask)\n\n    def _actions_to_config(self, actions):\n        \"\"\"\n        Given actions, to generate the corresponding trial configuration\n        \"\"\"\n        chosen_arch = copy.deepcopy(self.chosen_arch_template)\n        for cnt, act in enumerate(actions):\n            act_name = self.full_act_space[act]\n            (_key, _type) = self.actions_to_config[cnt]\n            if _type == 'input_choice':\n                if act_name == 'None':\n                    chosen_arch[_key] = {'_value': [], '_idx': []}\n                else:\n                    candidates = self.search_space[_key]['_value']['candidates']\n                    idx = candidates.index(act_name)\n                    chosen_arch[_key] = {'_value': [act_name], '_idx': [idx]}\n            elif _type == 'layer_choice':\n                idx = self.search_space[_key]['_value'].index(act_name)\n                chosen_arch[_key] = {'_value': act_name, '_idx': idx}\n            else:\n                raise ValueError('unrecognized key: {0}'.format(_type))\n        return chosen_arch\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        list\n            A list of newly generated configurations\n        \"\"\"\n        result = []\n        self.send_trial_callback = kwargs['st_callback']\n        for parameter_id in parameter_id_list:\n            had_exception = False\n            try:\n                logger.debug(\"generating param for %s\", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                had_exception = True\n            if not had_exception:\n                result.append(res)\n        return result\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate parameters, if no trial configration for now, self.credit plus 1 to send the config later\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters.\n            This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n\n        \"\"\"\n        if self.first_inf:\n            self.trials_result = [None for _ in range(self.inf_batch_size)]\n            mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values = self.model.inference(self.inf_batch_size)\n            self.trials_info = TrialsInfo(mb_obs, mb_actions, mb_values, mb_neglogpacs,\n                                          mb_dones, last_values, self.inf_batch_size)\n            self.first_inf = False\n\n        trial_info_idx, actions = self.trials_info.get_next()\n        if trial_info_idx is None:\n            logger.debug('Credit added by one in parameters request')\n            self.credit += 1\n            self.param_ids.append(parameter_id)\n            raise nni.NoMoreTrialError('no more parameters now.')\n\n        self.running_trials[parameter_id] = trial_info_idx\n        new_config = self._actions_to_config(actions)\n        return new_config\n\n    def _next_round_inference(self):\n        \"\"\"\n        Run a inference to generate next batch of configurations\n        \"\"\"\n        logger.debug('Start next round inference...')\n        self.finished_trials = 0\n        self.model.compute_rewards(self.trials_info, self.trials_result)\n        self.model.train(self.trials_info, self.inf_batch_size)\n        self.running_trials = {}\n        # generate new trials\n        self.trials_result = [None for _ in range(self.inf_batch_size)]\n        mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values = self.model.inference(self.inf_batch_size)\n        self.trials_info = TrialsInfo(mb_obs, mb_actions,\n                                      mb_values, mb_neglogpacs,\n                                      mb_dones, last_values,\n                                      self.inf_batch_size)\n        logger.debug('Next round inference complete.')\n        # check credit and submit new trials\n        for _ in range(self.credit):\n            trial_info_idx, actions = self.trials_info.get_next()\n            if trial_info_idx is None:\n                logger.warning('No enough trial config, trials_per_update is suggested to be larger than trialConcurrency')\n                break\n            assert self.param_ids\n            param_id = self.param_ids.pop()\n            self.running_trials[param_id] = trial_info_idx\n            new_config = self._actions_to_config(actions)\n            self.send_trial_callback(param_id, new_config)\n            self.credit -= 1\n            logger.debug('Send new trial (%d, %s) for reducing credit', param_id, new_config)\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Receive trial's result. if the number of finished trials equals self.inf_batch_size, start the next update to\n        train the model.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        \"\"\"\n        trial_info_idx = self.running_trials.pop(parameter_id, None)\n        assert trial_info_idx is not None\n\n        value = extract_scalar_reward(value)\n        if self.optimize_mode == OptimizeMode.Minimize:\n            value = -value\n\n        self.trials_result[trial_info_idx] = value\n        self.finished_trials += 1\n\n        logger.debug('receive_trial_result, parameter_id %d, trial_info_idx %d, finished_trials %d, inf_batch_size %d',\n                     parameter_id, trial_info_idx, self.finished_trials, self.inf_batch_size)\n        if self.finished_trials == self.inf_batch_size:\n            logger.debug('Start next round inference in receive_trial_result')\n            self._next_round_inference()\n\n    def trial_end(self, parameter_id, success, **kwargs):\n        \"\"\"\n        To deal with trial failure. If a trial fails, it is popped out from ``self.running_trials``,\n        and the final result of this trial is assigned with the average of the finished trials.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Not used\n        \"\"\"\n        if not success:\n            if parameter_id not in self.running_trials:\n                logger.warning('The trial is failed, but self.running_trial does not have this trial')\n                return\n            trial_info_idx = self.running_trials.pop(parameter_id, None)\n            assert trial_info_idx is not None\n            # use mean of finished trials as the result of this failed trial\n            values = [val for val in self.trials_result if val is not None]\n            logger.warning('In trial_end, values: %s', values)\n            self.trials_result[trial_info_idx] = (sum(values) / len(values)) if values else 0\n            self.finished_trials += 1\n            if self.finished_trials == self.inf_batch_size:\n                logger.debug('Start next round inference in trial_end')\n                self._next_round_inference()\n\n    def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning, not supported yet.\n\n        Parameters\n        ----------\n        data : list\n            A list of dictionarys, each of which has at least two keys, ``parameter`` and ``value``\n        \"\"\"\n        logger.warning('PPOTuner cannot leverage imported data.')",
  "def f(_):\n        return val",
  "def __init__(self):\n        self.observation_space = None\n        self.action_space = None\n        self.num_envs = 0\n        self.nsteps = 0\n\n        self.ent_coef = 0.0\n        self.lr = 3e-4\n        self.vf_coef = 0.5\n        self.max_grad_norm = 0.5\n        self.gamma = 0.99\n        self.lam = 0.95\n        self.cliprange = 0.2\n        self.embedding_size = None  # the embedding is for each action\n\n        self.noptepochs = 4         # number of training epochs per update\n        self.total_timesteps = 5000 # number of timesteps (i.e. number of actions taken in the environment)\n        self.nminibatches = 4",
  "def __init__(self, obs, actions, values, neglogpacs, dones, last_value, inf_batch_size):\n        self.iter = 0\n        self.obs = obs\n        self.actions = actions\n        self.values = values\n        self.neglogpacs = neglogpacs\n        self.dones = dones\n        self.last_value = last_value\n\n        self.rewards = None\n        self.returns = None\n\n        self.inf_batch_size = inf_batch_size",
  "def get_next(self):\n        \"\"\"\n        Get actions of the next trial\n        \"\"\"\n        if self.iter >= self.inf_batch_size:\n            return None, None\n        actions = []\n        for step in self.actions:\n            actions.append(step[self.iter])\n        self.iter += 1\n        return self.iter - 1, actions",
  "def update_rewards(self, rewards, returns):\n        \"\"\"\n        After the trial is finished, reward and return of this trial is updated\n        \"\"\"\n        self.rewards = rewards\n        self.returns = returns",
  "def convert_shape(self):\n        \"\"\"\n        Convert shape\n        \"\"\"\n        def sf01(arr):\n            \"\"\"\n            swap and then flatten axes 0 and 1\n            \"\"\"\n            s = arr.shape\n            return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n        self.obs = sf01(self.obs)\n        self.returns = sf01(self.returns)\n        self.dones = sf01(self.dones)\n        self.actions = sf01(self.actions)\n        self.values = sf01(self.values)\n        self.neglogpacs = sf01(self.neglogpacs)",
  "def __init__(self, model_config, mask):\n        self.model_config = model_config\n        self.states = None    # initial state of lstm in policy/value network\n        self.nupdates = None  # the number of func train is invoked, used to tune lr and cliprange\n        self.cur_update = 1   # record the current update\n        self.np_mask = mask   # record the mask of each action within one trial\n\n        set_global_seeds(None)\n        assert isinstance(self.model_config.lr, float)\n        self.lr = _constfn(self.model_config.lr)\n        assert isinstance(self.model_config.cliprange, float)\n        self.cliprange = _constfn(self.model_config.cliprange)\n\n        # build lstm policy network, value share the same network\n        policy = build_lstm_policy(model_config)\n\n        # Get the nb of env\n        nenvs = model_config.num_envs\n\n        # Calculate the batch_size\n        self.nbatch = nbatch = nenvs * model_config.nsteps # num of record per update\n        nbatch_train = nbatch // model_config.nminibatches # get batch size\n        # self.nupdates is used to tune lr and cliprange\n        self.nupdates = self.model_config.total_timesteps // self.nbatch\n\n        # Instantiate the model object (that creates act_model and train_model)\n        self.model = Model(policy=policy, nbatch_act=nenvs, nbatch_train=nbatch_train,\n                           nsteps=model_config.nsteps, ent_coef=model_config.ent_coef, vf_coef=model_config.vf_coef,\n                           max_grad_norm=model_config.max_grad_norm, np_mask=self.np_mask)\n\n        self.states = self.model.initial_state\n\n        logger.info('=== finished PPOModel initialization')",
  "def inference(self, num):\n        \"\"\"\n        Generate actions along with related info from policy network.\n        observation is the action of the last step.\n\n        Parameters\n        ----------\n        num: int\n            The number of trials to generate\n\n        Returns\n        -------\n        mb_obs : list\n            Observation of the ``num`` configurations\n        mb_actions : list\n            Actions of the ``num`` configurations\n        mb_values : list\n            Values from the value function of the ``num`` configurations\n        mb_neglogpacs : list\n            ``neglogp`` of the ``num`` configurations\n        mb_dones : list\n            To show whether the play is done, always ``True``\n        last_values : tensorflow tensor\n            The last values of the ``num`` configurations, got with session run\n        \"\"\"\n        # Here, we init the lists that will contain the mb of experiences\n        mb_obs, mb_actions, mb_values, mb_dones, mb_neglogpacs = [], [], [], [], []\n        # initial observation\n        # use the (n+1)th embedding to represent the first step action\n        first_step_ob = self.model_config.action_space.n\n        obs = [first_step_ob for _ in range(num)]\n        dones = [True for _ in range(num)]\n        states = self.states\n        # For n in range number of steps\n        for cur_step in range(self.model_config.nsteps):\n            # Given observations, get action value and neglopacs\n            # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init\n            actions, values, states, neglogpacs = self.model.step(cur_step, obs, S=states, M=dones)\n            mb_obs.append(obs.copy())\n            mb_actions.append(actions)\n            mb_values.append(values)\n            mb_neglogpacs.append(neglogpacs)\n            mb_dones.append(dones)\n\n            # Take actions in env and look the results\n            # Infos contains a ton of useful informations\n            obs[:] = actions\n            if cur_step == self.model_config.nsteps - 1:\n                dones = [True for _ in range(num)]\n            else:\n                dones = [False for _ in range(num)]\n\n        #batch of steps to batch of rollouts\n        np_obs = np.asarray(obs)\n        mb_obs = np.asarray(mb_obs, dtype=np_obs.dtype)\n        mb_actions = np.asarray(mb_actions)\n        mb_values = np.asarray(mb_values, dtype=np.float32)\n        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n        last_values = self.model.value(np_obs, S=states, M=dones)\n\n        return mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values",
  "def compute_rewards(self, trials_info, trials_result):\n        \"\"\"\n        Compute the rewards of the trials in trials_info based on trials_result,\n        and update the rewards in trials_info\n\n        Parameters\n        ----------\n        trials_info : TrialsInfo\n            Info of the generated trials\n        trials_result : list\n            Final results (e.g., acc) of the generated trials\n        \"\"\"\n        mb_rewards = np.asarray([trials_result for _ in trials_info.actions], dtype=np.float32)\n        # discount/bootstrap off value fn\n        mb_returns = np.zeros_like(mb_rewards)\n        mb_advs = np.zeros_like(mb_rewards)\n        lastgaelam = 0\n        last_dones = np.asarray([True for _ in trials_result], dtype=np.bool) # ugly\n        for t in reversed(range(self.model_config.nsteps)):\n            if t == self.model_config.nsteps - 1:\n                nextnonterminal = 1.0 - last_dones\n                nextvalues = trials_info.last_value\n            else:\n                nextnonterminal = 1.0 - trials_info.dones[t+1]\n                nextvalues = trials_info.values[t+1]\n            delta = mb_rewards[t] + self.model_config.gamma * nextvalues * nextnonterminal - trials_info.values[t]\n            lastgaelam = delta + self.model_config.gamma * self.model_config.lam * nextnonterminal * lastgaelam\n            mb_advs[t] = lastgaelam # pylint: disable=unsupported-assignment-operation\n        mb_returns = mb_advs + trials_info.values\n\n        trials_info.update_rewards(mb_rewards, mb_returns)\n        trials_info.convert_shape()",
  "def train(self, trials_info, nenvs):\n        \"\"\"\n        Train the policy/value network using trials_info\n\n        Parameters\n        ----------\n        trials_info : TrialsInfo\n            Complete info of the generated trials from the previous inference\n        nenvs : int\n            The batch size of the (previous) inference\n        \"\"\"\n        # keep frac decay for future optimization\n        if self.cur_update <= self.nupdates:\n            frac = 1.0 - (self.cur_update - 1.0) / self.nupdates\n        else:\n            logger.warning('current update (self.cur_update) %d has exceeded total updates (self.nupdates) %d',\n                           self.cur_update, self.nupdates)\n            frac = 1.0 - (self.nupdates - 1.0) / self.nupdates\n        lrnow = self.lr(frac)\n        cliprangenow = self.cliprange(frac)\n        self.cur_update += 1\n\n        states = self.states\n\n        assert states is not None # recurrent version\n        assert nenvs % self.model_config.nminibatches == 0\n        envsperbatch = nenvs // self.model_config.nminibatches\n        envinds = np.arange(nenvs)\n        flatinds = np.arange(nenvs * self.model_config.nsteps).reshape(nenvs, self.model_config.nsteps)\n        for _ in range(self.model_config.noptepochs):\n            np.random.shuffle(envinds)\n            for start in range(0, nenvs, envsperbatch):\n                end = start + envsperbatch\n                mbenvinds = envinds[start:end]\n                mbflatinds = flatinds[mbenvinds].ravel()\n                slices = (arr[mbflatinds] for arr in (trials_info.obs, trials_info.returns, trials_info.dones,\n                                                      trials_info.actions, trials_info.values, trials_info.neglogpacs))\n                mbstates = states[mbenvinds]\n                self.model.train(lrnow, cliprangenow, *slices, mbstates)",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('trials_per_update'): self.range('trials_per_update', int, 0, 99999),\n            Optional('epochs_per_update'): self.range('epochs_per_update', int, 0, 99999),\n            Optional('minibatch_size'): self.range('minibatch_size', int, 0, 99999),\n            Optional('ent_coef'): float,\n            Optional('lr'): float,\n            Optional('vf_coef'): float,\n            Optional('max_grad_norm'): float,\n            Optional('gamma'):  float,\n            Optional('lam'):  float,\n            Optional('cliprange'): float,\n        }).validate(kwargs)",
  "def __init__(self, optimize_mode, trials_per_update=20, epochs_per_update=4, minibatch_size=4,\n                 ent_coef=0.0, lr=3e-4, vf_coef=0.5, max_grad_norm=0.5, gamma=0.99, lam=0.95, cliprange=0.2):\n        \"\"\"\n        Initialization, PPO model is not initialized here as search space is not received yet.\n\n        Parameters\n        ----------\n        optimize_mode : str\n            maximize or minimize\n        trials_per_update : int\n            Number of trials to have for each model update\n        epochs_per_update : int\n            Number of epochs to run for each model update\n        minibatch_size : int\n            Minibatch size (number of trials) for the update\n        ent_coef : float\n            Policy entropy coefficient in the optimization objective\n        lr : float\n            Learning rate of the model (lstm network), constant\n        vf_coef : float\n            Value function loss coefficient in the optimization objective\n        max_grad_norm : float\n            Gradient norm clipping coefficient\n        gamma : float\n            Discounting factor\n        lam : float\n            Advantage estimation discounting factor (lambda in the paper)\n        cliprange : float\n            Cliprange in the PPO algorithm, constant\n        \"\"\"\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.model_config = ModelConfig()\n        self.model = None\n        self.search_space = None\n        self.running_trials = {}                  # key: parameter_id, value: actions/states/etc.\n        self.inf_batch_size = trials_per_update   # number of trials to generate in one inference\n        self.first_inf = True                     # indicate whether it is the first time to inference new trials\n        self.trials_result = [None for _ in range(self.inf_batch_size)] # results of finished trials\n\n        self.credit = 0 # record the unsatisfied trial requests\n        self.param_ids = []\n        self.finished_trials = 0\n        self.chosen_arch_template = {}\n\n        self.actions_spaces = None\n        self.actions_to_config = None\n        self.full_act_space = None\n        self.trials_info = None\n\n        self.all_trials = {} # used to dedup the same trial, key: config, value: final result\n\n        self.model_config.num_envs = self.inf_batch_size\n        self.model_config.noptepochs = epochs_per_update\n        self.model_config.nminibatches = minibatch_size\n\n        self.send_trial_callback = None\n        logger.info('Finished PPOTuner initialization')",
  "def _process_nas_space(self, search_space):\n        actions_spaces = []\n        actions_to_config = []\n        for key, val in search_space.items():\n            if val['_type'] == 'layer_choice':\n                actions_to_config.append((key, 'layer_choice'))\n                actions_spaces.append(val['_value'])\n                self.chosen_arch_template[key] = None\n            elif val['_type'] == 'input_choice':\n                candidates = val['_value']['candidates']\n                n_chosen = val['_value']['n_chosen']\n                if n_chosen not in [0, 1, [0, 1]]:\n                    raise ValueError('Optional_input_size can only be 0, 1, or [0, 1], but the pecified one is %s'\n                                     % (n_chosen))\n                if isinstance(n_chosen, list):\n                    actions_to_config.append((key, 'input_choice'))\n                    # FIXME: risk, candidates might also have None\n                    actions_spaces.append(['None', *candidates])\n                    self.chosen_arch_template[key] = None\n                elif n_chosen == 1:\n                    actions_to_config.append((key, 'input_choice'))\n                    actions_spaces.append(candidates)\n                    self.chosen_arch_template[key] = None\n                elif n_chosen == 0:\n                    self.chosen_arch_template[key] = []\n            else:\n                raise ValueError('Unsupported search space type: %s' % (val['_type']))\n\n        # calculate observation space\n        dedup = {}\n        for step in actions_spaces:\n            for action in step:\n                dedup[action] = 1\n        full_act_space = [act for act, _ in dedup.items()]\n        assert len(full_act_space) == len(dedup)\n        observation_space = len(full_act_space)\n        nsteps = len(actions_spaces)\n\n        return actions_spaces, actions_to_config, full_act_space, observation_space, nsteps",
  "def _generate_action_mask(self):\n        \"\"\"\n        Different step could have different action space. to deal with this case, we merge all the\n        possible actions into one action space, and use mask to indicate available actions for each step\n        \"\"\"\n        two_masks = []\n\n        mask = []\n        for acts in self.actions_spaces:\n            one_mask = [0 for _ in range(len(self.full_act_space))]\n            for act in acts:\n                idx = self.full_act_space.index(act)\n                one_mask[idx] = 1\n            mask.append(one_mask)\n        two_masks.append(mask)\n\n        mask = []\n        for acts in self.actions_spaces:\n            one_mask = [-np.inf for _ in range(len(self.full_act_space))]\n            for act in acts:\n                idx = self.full_act_space.index(act)\n                one_mask[idx] = 0\n            mask.append(one_mask)\n        two_masks.append(mask)\n\n        return np.asarray(two_masks, dtype=np.float32)",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Get search space, currently the space only includes that for NAS\n\n        Parameters\n        ----------\n        search_space : dict\n            Search space for NAS\n            the format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        \"\"\"\n        logger.info('update search space %s', search_space)\n        assert self.search_space is None\n        self.search_space = search_space\n\n        assert self.model_config.observation_space is None\n        assert self.model_config.action_space is None\n\n        self.actions_spaces, self.actions_to_config, self.full_act_space, obs_space, nsteps = self._process_nas_space(search_space)\n\n        self.model_config.observation_space = spaces.Discrete(obs_space)\n        self.model_config.action_space = spaces.Discrete(obs_space)\n        self.model_config.nsteps = nsteps\n\n        # generate mask in numpy\n        mask = self._generate_action_mask()\n\n        assert self.model is None\n        self.model = PPOModel(self.model_config, mask)",
  "def _actions_to_config(self, actions):\n        \"\"\"\n        Given actions, to generate the corresponding trial configuration\n        \"\"\"\n        chosen_arch = copy.deepcopy(self.chosen_arch_template)\n        for cnt, act in enumerate(actions):\n            act_name = self.full_act_space[act]\n            (_key, _type) = self.actions_to_config[cnt]\n            if _type == 'input_choice':\n                if act_name == 'None':\n                    chosen_arch[_key] = {'_value': [], '_idx': []}\n                else:\n                    candidates = self.search_space[_key]['_value']['candidates']\n                    idx = candidates.index(act_name)\n                    chosen_arch[_key] = {'_value': [act_name], '_idx': [idx]}\n            elif _type == 'layer_choice':\n                idx = self.search_space[_key]['_value'].index(act_name)\n                chosen_arch[_key] = {'_value': act_name, '_idx': idx}\n            else:\n                raise ValueError('unrecognized key: {0}'.format(_type))\n        return chosen_arch",
  "def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        list\n            A list of newly generated configurations\n        \"\"\"\n        result = []\n        self.send_trial_callback = kwargs['st_callback']\n        for parameter_id in parameter_id_list:\n            had_exception = False\n            try:\n                logger.debug(\"generating param for %s\", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                had_exception = True\n            if not had_exception:\n                result.append(res)\n        return result",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate parameters, if no trial configration for now, self.credit plus 1 to send the config later\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters.\n            This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n\n        \"\"\"\n        if self.first_inf:\n            self.trials_result = [None for _ in range(self.inf_batch_size)]\n            mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values = self.model.inference(self.inf_batch_size)\n            self.trials_info = TrialsInfo(mb_obs, mb_actions, mb_values, mb_neglogpacs,\n                                          mb_dones, last_values, self.inf_batch_size)\n            self.first_inf = False\n\n        trial_info_idx, actions = self.trials_info.get_next()\n        if trial_info_idx is None:\n            logger.debug('Credit added by one in parameters request')\n            self.credit += 1\n            self.param_ids.append(parameter_id)\n            raise nni.NoMoreTrialError('no more parameters now.')\n\n        self.running_trials[parameter_id] = trial_info_idx\n        new_config = self._actions_to_config(actions)\n        return new_config",
  "def _next_round_inference(self):\n        \"\"\"\n        Run a inference to generate next batch of configurations\n        \"\"\"\n        logger.debug('Start next round inference...')\n        self.finished_trials = 0\n        self.model.compute_rewards(self.trials_info, self.trials_result)\n        self.model.train(self.trials_info, self.inf_batch_size)\n        self.running_trials = {}\n        # generate new trials\n        self.trials_result = [None for _ in range(self.inf_batch_size)]\n        mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values = self.model.inference(self.inf_batch_size)\n        self.trials_info = TrialsInfo(mb_obs, mb_actions,\n                                      mb_values, mb_neglogpacs,\n                                      mb_dones, last_values,\n                                      self.inf_batch_size)\n        logger.debug('Next round inference complete.')\n        # check credit and submit new trials\n        for _ in range(self.credit):\n            trial_info_idx, actions = self.trials_info.get_next()\n            if trial_info_idx is None:\n                logger.warning('No enough trial config, trials_per_update is suggested to be larger than trialConcurrency')\n                break\n            assert self.param_ids\n            param_id = self.param_ids.pop()\n            self.running_trials[param_id] = trial_info_idx\n            new_config = self._actions_to_config(actions)\n            self.send_trial_callback(param_id, new_config)\n            self.credit -= 1\n            logger.debug('Send new trial (%d, %s) for reducing credit', param_id, new_config)",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Receive trial's result. if the number of finished trials equals self.inf_batch_size, start the next update to\n        train the model.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        \"\"\"\n        trial_info_idx = self.running_trials.pop(parameter_id, None)\n        assert trial_info_idx is not None\n\n        value = extract_scalar_reward(value)\n        if self.optimize_mode == OptimizeMode.Minimize:\n            value = -value\n\n        self.trials_result[trial_info_idx] = value\n        self.finished_trials += 1\n\n        logger.debug('receive_trial_result, parameter_id %d, trial_info_idx %d, finished_trials %d, inf_batch_size %d',\n                     parameter_id, trial_info_idx, self.finished_trials, self.inf_batch_size)\n        if self.finished_trials == self.inf_batch_size:\n            logger.debug('Start next round inference in receive_trial_result')\n            self._next_round_inference()",
  "def trial_end(self, parameter_id, success, **kwargs):\n        \"\"\"\n        To deal with trial failure. If a trial fails, it is popped out from ``self.running_trials``,\n        and the final result of this trial is assigned with the average of the finished trials.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Not used\n        \"\"\"\n        if not success:\n            if parameter_id not in self.running_trials:\n                logger.warning('The trial is failed, but self.running_trial does not have this trial')\n                return\n            trial_info_idx = self.running_trials.pop(parameter_id, None)\n            assert trial_info_idx is not None\n            # use mean of finished trials as the result of this failed trial\n            values = [val for val in self.trials_result if val is not None]\n            logger.warning('In trial_end, values: %s', values)\n            self.trials_result[trial_info_idx] = (sum(values) / len(values)) if values else 0\n            self.finished_trials += 1\n            if self.finished_trials == self.inf_batch_size:\n                logger.debug('Start next round inference in trial_end')\n                self._next_round_inference()",
  "def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning, not supported yet.\n\n        Parameters\n        ----------\n        data : list\n            A list of dictionarys, each of which has at least two keys, ``parameter`` and ``value``\n        \"\"\"\n        logger.warning('PPOTuner cannot leverage imported data.')",
  "def sf01(arr):\n            \"\"\"\n            swap and then flatten axes 0 and 1\n            \"\"\"\n            s = arr.shape\n            return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])",
  "class Model:\n    \"\"\"\n    We use this object to :\n        __init__:\n            - Creates the step_model\n            - Creates the train_model\n\n        train():\n            - Make the training part (feedforward and retropropagation of gradients)\n\n        save/load():\n            - Save load the model\n    \"\"\"\n    def __init__(self, *, policy, nbatch_act, nbatch_train,\n                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):\n        self.sess = sess = get_session()\n\n        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):\n            # CREATE OUR TWO MODELS\n            # act_model that is used for sampling\n            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)\n\n            # Train model for training\n            if microbatch_size is None:\n                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)\n            else:\n                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)\n\n        # CREATE THE PLACEHOLDERS\n        self.A = A = train_model.pdtype.sample_placeholder([None])\n        self.ADV = ADV = tf.placeholder(tf.float32, [None])\n        self.R = R = tf.placeholder(tf.float32, [None])\n        # Keep track of old actor\n        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])\n        # Keep track of old critic\n        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])\n        self.LR = LR = tf.placeholder(tf.float32, [])\n        # Cliprange\n        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])\n\n        neglogpac = train_model.pd.neglogp(A)\n\n        # Calculate the entropy\n        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n        entropy = tf.reduce_mean(train_model.pd.entropy())\n\n        # CALCULATE THE LOSS\n        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n\n        # Clip the value to reduce variability during Critic training\n        # Get the predicted value\n        vpred = train_model.vf\n        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)\n        # Unclipped value\n        vf_losses1 = tf.square(vpred - R)\n        # Clipped value\n        vf_losses2 = tf.square(vpredclipped - R)\n\n        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n\n        # Calculate ratio (pi current policy / pi old policy)\n        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)\n\n        # Defining Loss = - J is equivalent to max J\n        pg_losses = -ADV * ratio\n\n        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)\n\n        # Final PG loss\n        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))\n        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))\n\n        # Total loss\n        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n\n        # UPDATE THE PARAMETERS USING LOSS\n        # 1. Get the model parameters\n        params = tf.trainable_variables('ppo2_model')\n        # 2. Build our trainer\n        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)\n        # 3. Calculate the gradients\n        grads_and_var = self.trainer.compute_gradients(loss, params)\n        grads, var = zip(*grads_and_var)\n\n        if max_grad_norm is not None:\n            # Clip the gradients (normalize)\n            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n        grads_and_var = list(zip(grads, var))\n        # zip aggregate each gradient with parameters associated\n        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\n\n        self.grads = grads\n        self.var = var\n        self._train_op = self.trainer.apply_gradients(grads_and_var)\n        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']\n        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]\n\n\n        self.train_model = train_model\n        self.act_model = act_model\n        self.step = act_model.step\n        self.value = act_model.value\n        self.initial_state = act_model.initial_state\n\n        initialize()\n\n    def train(self, lr, cliprange, obs, returns, masks, actions, values, neglogpacs, states=None):\n        \"\"\"\n        Train the model.\n        Here we calculate advantage A(s,a) = R + yV(s') - V(s)\n\n        Returns\n        -------\n        obj\n            = R + yV(s')\n        \"\"\"\n        advs = returns - values\n\n        # Normalize the advantages\n        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n\n        td_map = {\n            self.train_model.X : obs,\n            self.A : actions,\n            self.ADV : advs,\n            self.R : returns,\n            self.LR : lr,\n            self.CLIPRANGE : cliprange,\n            self.OLDNEGLOGPAC : neglogpacs,\n            self.OLDVPRED : values\n        }\n        if states is not None:\n            td_map[self.train_model.S] = states\n            td_map[self.train_model.M] = masks\n\n        return self.sess.run(\n            self.stats_list + [self._train_op],\n            td_map\n        )[:-1]",
  "def __init__(self, *, policy, nbatch_act, nbatch_train,\n                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):\n        self.sess = sess = get_session()\n\n        with tf.variable_scope('ppo2_model', reuse=tf.AUTO_REUSE):\n            # CREATE OUR TWO MODELS\n            # act_model that is used for sampling\n            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)\n\n            # Train model for training\n            if microbatch_size is None:\n                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)\n            else:\n                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)\n\n        # CREATE THE PLACEHOLDERS\n        self.A = A = train_model.pdtype.sample_placeholder([None])\n        self.ADV = ADV = tf.placeholder(tf.float32, [None])\n        self.R = R = tf.placeholder(tf.float32, [None])\n        # Keep track of old actor\n        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])\n        # Keep track of old critic\n        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])\n        self.LR = LR = tf.placeholder(tf.float32, [])\n        # Cliprange\n        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])\n\n        neglogpac = train_model.pd.neglogp(A)\n\n        # Calculate the entropy\n        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n        entropy = tf.reduce_mean(train_model.pd.entropy())\n\n        # CALCULATE THE LOSS\n        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n\n        # Clip the value to reduce variability during Critic training\n        # Get the predicted value\n        vpred = train_model.vf\n        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)\n        # Unclipped value\n        vf_losses1 = tf.square(vpred - R)\n        # Clipped value\n        vf_losses2 = tf.square(vpredclipped - R)\n\n        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n\n        # Calculate ratio (pi current policy / pi old policy)\n        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)\n\n        # Defining Loss = - J is equivalent to max J\n        pg_losses = -ADV * ratio\n\n        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)\n\n        # Final PG loss\n        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))\n        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))\n\n        # Total loss\n        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n\n        # UPDATE THE PARAMETERS USING LOSS\n        # 1. Get the model parameters\n        params = tf.trainable_variables('ppo2_model')\n        # 2. Build our trainer\n        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)\n        # 3. Calculate the gradients\n        grads_and_var = self.trainer.compute_gradients(loss, params)\n        grads, var = zip(*grads_and_var)\n\n        if max_grad_norm is not None:\n            # Clip the gradients (normalize)\n            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n        grads_and_var = list(zip(grads, var))\n        # zip aggregate each gradient with parameters associated\n        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\n\n        self.grads = grads\n        self.var = var\n        self._train_op = self.trainer.apply_gradients(grads_and_var)\n        self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']\n        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]\n\n\n        self.train_model = train_model\n        self.act_model = act_model\n        self.step = act_model.step\n        self.value = act_model.value\n        self.initial_state = act_model.initial_state\n\n        initialize()",
  "def train(self, lr, cliprange, obs, returns, masks, actions, values, neglogpacs, states=None):\n        \"\"\"\n        Train the model.\n        Here we calculate advantage A(s,a) = R + yV(s') - V(s)\n\n        Returns\n        -------\n        obj\n            = R + yV(s')\n        \"\"\"\n        advs = returns - values\n\n        # Normalize the advantages\n        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n\n        td_map = {\n            self.train_model.X : obs,\n            self.A : actions,\n            self.ADV : advs,\n            self.R : returns,\n            self.LR : lr,\n            self.CLIPRANGE : cliprange,\n            self.OLDNEGLOGPAC : neglogpacs,\n            self.OLDVPRED : values\n        }\n        if states is not None:\n            td_map[self.train_model.S] = states\n            td_map[self.train_model.M] = masks\n\n        return self.sess.run(\n            self.stats_list + [self._train_op],\n            td_map\n        )[:-1]",
  "class PolicyWithValue:\n    \"\"\"\n    Encapsulates fields and methods for RL policy and value function estimation with shared parameters\n    \"\"\"\n\n    def __init__(self, env, observations, latent, estimate_q=False, vf_latent=None, sess=None, np_mask=None, is_act_model=False, **tensors):\n        \"\"\"\n        Parameters\n        ----------\n        env : obj\n            RL environment\n        observations : tensorflow placeholder\n            Tensorflow placeholder in which the observations will be fed\n        latent : tensor\n            Latent state from which policy distribution parameters should be inferred\n        vf_latent : tensor\n            Latent state from which value function should be inferred (if None, then latent is used)\n        sess : tensorflow session\n            Tensorflow session to run calculations in (if None, default session is used)\n        **tensors\n            Tensorflow tensors for additional attributes such as state or mask\n        \"\"\"\n\n        self.X = observations\n        self.state = tf.constant([])\n        self.initial_state = None\n        self.__dict__.update(tensors)\n\n        vf_latent = vf_latent if vf_latent is not None else latent\n\n        vf_latent = tf.layers.flatten(vf_latent)\n        latent = tf.layers.flatten(latent)\n\n        # Based on the action space, will select what probability distribution type\n        self.np_mask = np_mask\n        self.pdtype = CategoricalPdType(env.action_space.n, env.nsteps, np_mask, is_act_model)\n\n        self.act_latent = latent\n        self.nh = env.action_space.n\n\n        self.pd, self.pi, self.mask, self.mask_npinf = self.pdtype.pdfromlatent(latent, init_scale=0.01)\n\n        # Take an action\n        self.action = self.pd.sample()\n\n        # Calculate the neg log of our probability\n        self.neglogp = self.pd.neglogp(self.action)\n        self.sess = sess or tf.get_default_session()\n\n        assert estimate_q is False\n        self.vf = fc(vf_latent, 'vf', 1)\n        self.vf = self.vf[:, 0]\n\n        if is_act_model:\n            self._build_model_for_step()\n\n    def _evaluate(self, variables, observation, **extra_feed):\n        sess = self.sess\n        feed_dict = {self.X: adjust_shape(self.X, observation)}\n        for inpt_name, data in extra_feed.items():\n            if inpt_name in self.__dict__.keys():\n                inpt = self.__dict__[inpt_name]\n                if isinstance(inpt, tf.Tensor) and inpt._op.type == 'Placeholder':\n                    feed_dict[inpt] = adjust_shape(inpt, data)\n\n        return sess.run(variables, feed_dict)\n\n    def _build_model_for_step(self):\n        # multiply with weight and apply mask on self.act_latent to generate\n        self.act_step = step = tf.placeholder(shape=(), dtype=tf.int64, name='act_step')\n        with tf.variable_scope('pi', reuse=tf.AUTO_REUSE):\n            from .util import ortho_init\n            nin = self.act_latent.get_shape()[1].value\n            w = tf.get_variable(\"w\", [nin, self.nh], initializer=ortho_init(0.01))\n            b = tf.get_variable(\"b\", [self.nh], initializer=tf.constant_initializer(0.0))\n            logits = tf.matmul(self.act_latent, w)+b\n            piece = tf.slice(self.mask, [step, 0], [1, self.nh])\n            re_piece = tf.reshape(piece, [-1])\n            masked_logits = tf.math.multiply(logits, re_piece)\n\n            npinf_piece = tf.slice(self.mask_npinf, [step, 0], [1, self.nh])\n            re_npinf_piece = tf.reshape(npinf_piece, [-1])\n\n        def sample(logits, mask_npinf):\n            new_logits = tf.math.add(logits, mask_npinf)\n            u = tf.random_uniform(tf.shape(new_logits), dtype=logits.dtype)\n            return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)\n\n        def neglogp(logits, x):\n            # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n            # Note: we can't use sparse_softmax_cross_entropy_with_logits because\n            #       the implementation does not allow second-order derivatives...\n            if x.dtype in {tf.uint8, tf.int32, tf.int64}:\n                # one-hot encoding\n                x_shape_list = x.shape.as_list()\n                logits_shape_list = logits.get_shape().as_list()[:-1]\n                for xs, ls in zip(x_shape_list, logits_shape_list):\n                    if xs is not None and ls is not None:\n                        assert xs == ls, 'shape mismatch: {} in x vs {} in logits'.format(xs, ls)\n\n                x = tf.one_hot(x, logits.get_shape().as_list()[-1])\n            else:\n                # already encoded\n                assert x.shape.as_list() == logits.shape.as_list()\n\n            return tf.nn.softmax_cross_entropy_with_logits_v2(\n                logits=logits,\n                labels=x)\n\n        self.act_action = sample(masked_logits, re_npinf_piece)\n        self.act_neglogp = neglogp(masked_logits, self.act_action)\n\n\n    def step(self, step, observation, **extra_feed):\n        \"\"\"\n        Compute next action(s) given the observation(s)\n\n        Parameters\n        ----------\n        observation : np array\n            Observation data (either single or a batch)\n        **extra_feed\n            Additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns\n        -------\n        (action, value estimate, next state, negative log likelihood of the action under current policy parameters) tuple\n        \"\"\"\n        extra_feed['act_step'] = step\n        a, v, state, neglogp = self._evaluate([self.act_action, self.vf, self.state, self.act_neglogp], observation, **extra_feed)\n        if state.size == 0:\n            state = None\n        return a, v, state, neglogp\n\n    def value(self, ob, *args, **kwargs):\n        \"\"\"\n        Compute value estimate(s) given the observation(s)\n\n        Parameters\n        ----------\n        observation : np array\n            Observation data (either single or a batch)\n        **extra_feed\n            Additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns\n        -------\n        Value estimate\n        \"\"\"\n        return self._evaluate(self.vf, ob, *args, **kwargs)",
  "def build_lstm_policy(model_config, value_network=None, estimate_q=False, **policy_kwargs):\n    \"\"\"\n    Build lstm policy and value network, they share the same lstm network.\n    the parameters all use their default values.\n\n    Parameter\n    ---------\n    model_config : obj\n        Configurations of the model\n    value_network : obj\n        The network for value function\n    estimate_q : bool\n        Whether to estimate ``q``\n    **policy_kwargs\n        The kwargs for policy network, i.e., lstm model\n\n    Returns\n    -------\n    func\n        The policy network\n    \"\"\"\n    policy_network = lstm_model(**policy_kwargs)\n\n    def policy_fn(nbatch=None, nsteps=None, sess=None, observ_placeholder=None, np_mask=None, is_act_model=False):\n        ob_space = model_config.observation_space\n\n        X = observ_placeholder if observ_placeholder is not None else observation_placeholder(ob_space, batch_size=nbatch)\n\n        extra_tensors = {}\n\n        # encode_observation is not necessary anymore as we use embedding_lookup\n        encoded_x = X\n\n        with tf.variable_scope('pi', reuse=tf.AUTO_REUSE):\n            policy_latent = policy_network(encoded_x, 1, model_config.observation_space.n)\n            if isinstance(policy_latent, tuple):\n                policy_latent, recurrent_tensors = policy_latent\n\n                if recurrent_tensors is not None:\n                    # recurrent architecture, need a few more steps\n                    nenv = nbatch // nsteps\n                    assert nenv > 0, 'Bad input for recurrent policy: batch size {} smaller than nsteps {}'.format(nbatch, nsteps)\n                    policy_latent, recurrent_tensors = policy_network(encoded_x, nenv, model_config.observation_space.n)\n                    extra_tensors.update(recurrent_tensors)\n\n        _v_net = value_network\n\n        assert _v_net is None or _v_net == 'shared'\n        vf_latent = policy_latent\n\n        policy = PolicyWithValue(\n            env=model_config,\n            observations=X,\n            latent=policy_latent,\n            vf_latent=vf_latent,\n            sess=sess,\n            estimate_q=estimate_q,\n            np_mask=np_mask,\n            is_act_model=is_act_model,\n            **extra_tensors\n        )\n        return policy\n\n    return policy_fn",
  "def __init__(self, env, observations, latent, estimate_q=False, vf_latent=None, sess=None, np_mask=None, is_act_model=False, **tensors):\n        \"\"\"\n        Parameters\n        ----------\n        env : obj\n            RL environment\n        observations : tensorflow placeholder\n            Tensorflow placeholder in which the observations will be fed\n        latent : tensor\n            Latent state from which policy distribution parameters should be inferred\n        vf_latent : tensor\n            Latent state from which value function should be inferred (if None, then latent is used)\n        sess : tensorflow session\n            Tensorflow session to run calculations in (if None, default session is used)\n        **tensors\n            Tensorflow tensors for additional attributes such as state or mask\n        \"\"\"\n\n        self.X = observations\n        self.state = tf.constant([])\n        self.initial_state = None\n        self.__dict__.update(tensors)\n\n        vf_latent = vf_latent if vf_latent is not None else latent\n\n        vf_latent = tf.layers.flatten(vf_latent)\n        latent = tf.layers.flatten(latent)\n\n        # Based on the action space, will select what probability distribution type\n        self.np_mask = np_mask\n        self.pdtype = CategoricalPdType(env.action_space.n, env.nsteps, np_mask, is_act_model)\n\n        self.act_latent = latent\n        self.nh = env.action_space.n\n\n        self.pd, self.pi, self.mask, self.mask_npinf = self.pdtype.pdfromlatent(latent, init_scale=0.01)\n\n        # Take an action\n        self.action = self.pd.sample()\n\n        # Calculate the neg log of our probability\n        self.neglogp = self.pd.neglogp(self.action)\n        self.sess = sess or tf.get_default_session()\n\n        assert estimate_q is False\n        self.vf = fc(vf_latent, 'vf', 1)\n        self.vf = self.vf[:, 0]\n\n        if is_act_model:\n            self._build_model_for_step()",
  "def _evaluate(self, variables, observation, **extra_feed):\n        sess = self.sess\n        feed_dict = {self.X: adjust_shape(self.X, observation)}\n        for inpt_name, data in extra_feed.items():\n            if inpt_name in self.__dict__.keys():\n                inpt = self.__dict__[inpt_name]\n                if isinstance(inpt, tf.Tensor) and inpt._op.type == 'Placeholder':\n                    feed_dict[inpt] = adjust_shape(inpt, data)\n\n        return sess.run(variables, feed_dict)",
  "def _build_model_for_step(self):\n        # multiply with weight and apply mask on self.act_latent to generate\n        self.act_step = step = tf.placeholder(shape=(), dtype=tf.int64, name='act_step')\n        with tf.variable_scope('pi', reuse=tf.AUTO_REUSE):\n            from .util import ortho_init\n            nin = self.act_latent.get_shape()[1].value\n            w = tf.get_variable(\"w\", [nin, self.nh], initializer=ortho_init(0.01))\n            b = tf.get_variable(\"b\", [self.nh], initializer=tf.constant_initializer(0.0))\n            logits = tf.matmul(self.act_latent, w)+b\n            piece = tf.slice(self.mask, [step, 0], [1, self.nh])\n            re_piece = tf.reshape(piece, [-1])\n            masked_logits = tf.math.multiply(logits, re_piece)\n\n            npinf_piece = tf.slice(self.mask_npinf, [step, 0], [1, self.nh])\n            re_npinf_piece = tf.reshape(npinf_piece, [-1])\n\n        def sample(logits, mask_npinf):\n            new_logits = tf.math.add(logits, mask_npinf)\n            u = tf.random_uniform(tf.shape(new_logits), dtype=logits.dtype)\n            return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)\n\n        def neglogp(logits, x):\n            # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n            # Note: we can't use sparse_softmax_cross_entropy_with_logits because\n            #       the implementation does not allow second-order derivatives...\n            if x.dtype in {tf.uint8, tf.int32, tf.int64}:\n                # one-hot encoding\n                x_shape_list = x.shape.as_list()\n                logits_shape_list = logits.get_shape().as_list()[:-1]\n                for xs, ls in zip(x_shape_list, logits_shape_list):\n                    if xs is not None and ls is not None:\n                        assert xs == ls, 'shape mismatch: {} in x vs {} in logits'.format(xs, ls)\n\n                x = tf.one_hot(x, logits.get_shape().as_list()[-1])\n            else:\n                # already encoded\n                assert x.shape.as_list() == logits.shape.as_list()\n\n            return tf.nn.softmax_cross_entropy_with_logits_v2(\n                logits=logits,\n                labels=x)\n\n        self.act_action = sample(masked_logits, re_npinf_piece)\n        self.act_neglogp = neglogp(masked_logits, self.act_action)",
  "def step(self, step, observation, **extra_feed):\n        \"\"\"\n        Compute next action(s) given the observation(s)\n\n        Parameters\n        ----------\n        observation : np array\n            Observation data (either single or a batch)\n        **extra_feed\n            Additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns\n        -------\n        (action, value estimate, next state, negative log likelihood of the action under current policy parameters) tuple\n        \"\"\"\n        extra_feed['act_step'] = step\n        a, v, state, neglogp = self._evaluate([self.act_action, self.vf, self.state, self.act_neglogp], observation, **extra_feed)\n        if state.size == 0:\n            state = None\n        return a, v, state, neglogp",
  "def value(self, ob, *args, **kwargs):\n        \"\"\"\n        Compute value estimate(s) given the observation(s)\n\n        Parameters\n        ----------\n        observation : np array\n            Observation data (either single or a batch)\n        **extra_feed\n            Additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns\n        -------\n        Value estimate\n        \"\"\"\n        return self._evaluate(self.vf, ob, *args, **kwargs)",
  "def policy_fn(nbatch=None, nsteps=None, sess=None, observ_placeholder=None, np_mask=None, is_act_model=False):\n        ob_space = model_config.observation_space\n\n        X = observ_placeholder if observ_placeholder is not None else observation_placeholder(ob_space, batch_size=nbatch)\n\n        extra_tensors = {}\n\n        # encode_observation is not necessary anymore as we use embedding_lookup\n        encoded_x = X\n\n        with tf.variable_scope('pi', reuse=tf.AUTO_REUSE):\n            policy_latent = policy_network(encoded_x, 1, model_config.observation_space.n)\n            if isinstance(policy_latent, tuple):\n                policy_latent, recurrent_tensors = policy_latent\n\n                if recurrent_tensors is not None:\n                    # recurrent architecture, need a few more steps\n                    nenv = nbatch // nsteps\n                    assert nenv > 0, 'Bad input for recurrent policy: batch size {} smaller than nsteps {}'.format(nbatch, nsteps)\n                    policy_latent, recurrent_tensors = policy_network(encoded_x, nenv, model_config.observation_space.n)\n                    extra_tensors.update(recurrent_tensors)\n\n        _v_net = value_network\n\n        assert _v_net is None or _v_net == 'shared'\n        vf_latent = policy_latent\n\n        policy = PolicyWithValue(\n            env=model_config,\n            observations=X,\n            latent=policy_latent,\n            vf_latent=vf_latent,\n            sess=sess,\n            estimate_q=estimate_q,\n            np_mask=np_mask,\n            is_act_model=is_act_model,\n            **extra_tensors\n        )\n        return policy",
  "def sample(logits, mask_npinf):\n            new_logits = tf.math.add(logits, mask_npinf)\n            u = tf.random_uniform(tf.shape(new_logits), dtype=logits.dtype)\n            return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)",
  "def neglogp(logits, x):\n            # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n            # Note: we can't use sparse_softmax_cross_entropy_with_logits because\n            #       the implementation does not allow second-order derivatives...\n            if x.dtype in {tf.uint8, tf.int32, tf.int64}:\n                # one-hot encoding\n                x_shape_list = x.shape.as_list()\n                logits_shape_list = logits.get_shape().as_list()[:-1]\n                for xs, ls in zip(x_shape_list, logits_shape_list):\n                    if xs is not None and ls is not None:\n                        assert xs == ls, 'shape mismatch: {} in x vs {} in logits'.format(xs, ls)\n\n                x = tf.one_hot(x, logits.get_shape().as_list()[-1])\n            else:\n                # already encoded\n                assert x.shape.as_list() == logits.shape.as_list()\n\n            return tf.nn.softmax_cross_entropy_with_logits_v2(\n                logits=logits,\n                labels=x)",
  "class BatchTuner(Tuner):\n    \"\"\"\n    BatchTuner is tuner will running all the configure that user want to run batchly.\n\n    Examples\n    --------\n    The search space only be accepted like:\n\n        ::\n\n            {'combine_params':\n                { '_type': 'choice',\n                            '_value': '[{...}, {...}, {...}]',\n                }\n            }\n\n    \"\"\"\n\n    def __init__(self):\n        self._count = -1\n        self._values = []\n\n    def is_valid(self, search_space):\n        \"\"\"\n        Check the search space is valid: only contains 'choice' type\n\n        Parameters\n        ----------\n        search_space : dict\n\n        Returns\n        -------\n        None or list\n            If valid, return candidate values; else return None.\n        \"\"\"\n        if not len(search_space) == 1:\n            raise RuntimeError('BatchTuner only supprt one combined-paramreters key.')\n\n        for param in search_space:\n            param_type = search_space[param][TYPE]\n            if not param_type == CHOICE:\n                raise RuntimeError('BatchTuner only supprt \\\n                                    one combined-paramreters type is choice.')\n\n            if isinstance(search_space[param][VALUE], list):\n                return search_space[param][VALUE]\n\n            raise RuntimeError('The combined-paramreters \\\n                                value in BatchTuner is not a list.')\n        return None\n\n    def update_search_space(self, search_space):\n        \"\"\"Update the search space\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self._values = self.is_valid(search_space)\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"Returns a dict of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        dict\n            A candidate parameter group.\n        \"\"\"\n        self._count += 1\n        if self._count > len(self._values) - 1:\n            raise nni.NoMoreTrialError('no more parameters now.')\n        return self._values[self._count]\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        pass\n\n    def import_data(self, data):\n        \"\"\"Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        if not self._values:\n            LOGGER.info(\"Search space has not been initialized, skip this data import\")\n            return\n\n        self._values = self._values[(self._count+1):]\n        self._count = -1\n\n        _completed_num = 0\n        for trial_info in data:\n            LOGGER .info(\"Importing data, current processing \\\n                            progress %s / %s\", _completed_num, len(data))\n            # simply validate data format\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                LOGGER.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            _completed_num += 1\n            if _params in self._values:\n                self._values.remove(_params)\n        LOGGER .info(\"Successfully import data to batch tuner, \\\n                        total data: %d, imported data: %d.\", len(data), _completed_num)",
  "def __init__(self):\n        self._count = -1\n        self._values = []",
  "def is_valid(self, search_space):\n        \"\"\"\n        Check the search space is valid: only contains 'choice' type\n\n        Parameters\n        ----------\n        search_space : dict\n\n        Returns\n        -------\n        None or list\n            If valid, return candidate values; else return None.\n        \"\"\"\n        if not len(search_space) == 1:\n            raise RuntimeError('BatchTuner only supprt one combined-paramreters key.')\n\n        for param in search_space:\n            param_type = search_space[param][TYPE]\n            if not param_type == CHOICE:\n                raise RuntimeError('BatchTuner only supprt \\\n                                    one combined-paramreters type is choice.')\n\n            if isinstance(search_space[param][VALUE], list):\n                return search_space[param][VALUE]\n\n            raise RuntimeError('The combined-paramreters \\\n                                value in BatchTuner is not a list.')\n        return None",
  "def update_search_space(self, search_space):\n        \"\"\"Update the search space\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self._values = self.is_valid(search_space)",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"Returns a dict of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        dict\n            A candidate parameter group.\n        \"\"\"\n        self._count += 1\n        if self._count > len(self._values) - 1:\n            raise nni.NoMoreTrialError('no more parameters now.')\n        return self._values[self._count]",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        pass",
  "def import_data(self, data):\n        \"\"\"Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        if not self._values:\n            LOGGER.info(\"Search space has not been initialized, skip this data import\")\n            return\n\n        self._values = self._values[(self._count+1):]\n        self._count = -1\n\n        _completed_num = 0\n        for trial_info in data:\n            LOGGER .info(\"Importing data, current processing \\\n                            progress %s / %s\", _completed_num, len(data))\n            # simply validate data format\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                LOGGER.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            _completed_num += 1\n            if _params in self._values:\n                self._values.remove(_params)\n        LOGGER .info(\"Successfully import data to batch tuner, \\\n                        total data: %d, imported data: %d.\", len(data), _completed_num)",
  "class Constant:\n    '''Constant for the Tuner.\n    '''\n    MAX_LAYERS = 100\n    N_NEIGHBOURS = 8\n    MAX_MODEL_SIZE = 1 << 24\n    KERNEL_LAMBDA = 1.0\n    BETA = 2.576\n    MLP_MODEL_LEN = 3\n    MLP_MODEL_WIDTH = 5\n    MODEL_LEN = 3\n    MODEL_WIDTH = 64\n    POOLING_KERNEL_SIZE = 2\n    DENSE_DROPOUT_RATE = 0.5\n    CONV_DROPOUT_RATE = 0.25\n    MLP_DROPOUT_RATE = 0.25\n    CONV_BLOCK_DISTANCE = 2\n    BATCH_SIZE = 128\n    T_MIN = 0.0001",
  "class NetworkGenerator:\n    \"\"\"The base class for generating a network.\n    It can be used to generate a CNN or Multi-Layer Perceptron.\n    Attributes:\n        n_output_node: Number of output nodes in the network.\n        input_shape: A tuple to represent the input shape.\n    \"\"\"\n\n    def __init__(self, n_output_node, input_shape):\n        self.n_output_node = n_output_node\n        self.input_shape = input_shape\n\n    @abstractmethod\n    def generate(self, model_len, model_width):\n        pass",
  "class CnnGenerator(NetworkGenerator):\n    \"\"\"A class to generate CNN.\n    Attributes:\n          n_dim: `len(self.input_shape) - 1`\n          conv: A class that represents `(n_dim-1)` dimensional convolution.\n          dropout: A class that represents `(n_dim-1)` dimensional dropout.\n          global_avg_pooling: A class that represents `(n_dim-1)` dimensional Global Average Pooling.\n          pooling: A class that represents `(n_dim-1)` dimensional pooling.\n          batch_norm: A class that represents `(n_dim-1)` dimensional batch normalization.\n    \"\"\"\n\n    def __init__(self, n_output_node, input_shape):\n        super(CnnGenerator, self).__init__(n_output_node, input_shape)\n        self.n_dim = len(self.input_shape) - 1\n        if len(self.input_shape) > 4:\n            raise ValueError(\"The input dimension is too high.\")\n        if len(self.input_shape) < 2:\n            raise ValueError(\"The input dimension is too low.\")\n        self.conv = get_conv_class(self.n_dim)\n        self.dropout = get_dropout_class(self.n_dim)\n        self.global_avg_pooling = get_global_avg_pooling_class(self.n_dim)\n        self.pooling = get_pooling_class(self.n_dim)\n        self.batch_norm = get_batch_norm_class(self.n_dim)\n\n    def generate(self, model_len=None, model_width=None):\n        \"\"\"Generates a CNN.\n        Args:\n            model_len: An integer. Number of convolutional layers.\n            model_width: An integer. Number of filters for the convolutional layers.\n        Returns:\n            An instance of the class Graph. Represents the neural architecture graph of the generated model.\n        \"\"\"\n\n        if model_len is None:\n            model_len = Constant.MODEL_LEN\n        if model_width is None:\n            model_width = Constant.MODEL_WIDTH\n        pooling_len = int(model_len / 4)\n        graph = Graph(self.input_shape, False)\n        temp_input_channel = self.input_shape[-1]\n        output_node_id = 0\n        stride = 1\n        for i in range(model_len):\n            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n            output_node_id = graph.add_layer(\n                self.batch_norm(\n                    graph.node_list[output_node_id].shape[-1]), output_node_id\n            )\n            output_node_id = graph.add_layer(\n                self.conv(\n                    temp_input_channel,\n                    model_width,\n                    kernel_size=3,\n                    stride=stride),\n                output_node_id,\n            )\n            temp_input_channel = model_width\n            if pooling_len == 0 or (\n                    (i + 1) % pooling_len == 0 and i != model_len - 1):\n                output_node_id = graph.add_layer(\n                    self.pooling(), output_node_id)\n\n        output_node_id = graph.add_layer(\n            self.global_avg_pooling(), output_node_id)\n        output_node_id = graph.add_layer(\n            self.dropout(Constant.CONV_DROPOUT_RATE), output_node_id\n        )\n        output_node_id = graph.add_layer(\n            StubDense(graph.node_list[output_node_id].shape[0], model_width),\n            output_node_id,\n        )\n        output_node_id = graph.add_layer(StubReLU(), output_node_id)\n        graph.add_layer(\n            StubDense(\n                model_width,\n                self.n_output_node),\n            output_node_id)\n        return graph",
  "class MlpGenerator(NetworkGenerator):\n    \"\"\"A class to generate Multi-Layer Perceptron.\n    \"\"\"\n\n    def __init__(self, n_output_node, input_shape):\n        \"\"\"Initialize the instance.\n        Args:\n            n_output_node: An integer. Number of output nodes in the network.\n            input_shape: A tuple. Input shape of the network. If it is 1D, ensure the value is appended by a comma\n                in the tuple.\n        \"\"\"\n        super(MlpGenerator, self).__init__(n_output_node, input_shape)\n        if len(self.input_shape) > 1:\n            raise ValueError(\"The input dimension is too high.\")\n\n    def generate(self, model_len=None, model_width=None):\n        \"\"\"Generates a Multi-Layer Perceptron.\n        Args:\n            model_len: An integer. Number of hidden layers.\n            model_width: An integer or a list of integers of length `model_len`. If it is a list, it represents the\n                number of nodes in each hidden layer. If it is an integer, all hidden layers have nodes equal to this\n                value.\n        Returns:\n            An instance of the class Graph. Represents the neural architecture graph of the generated model.\n        \"\"\"\n        if model_len is None:\n            model_len = Constant.MODEL_LEN\n        if model_width is None:\n            model_width = Constant.MODEL_WIDTH\n        if isinstance(model_width, list) and not len(model_width) == model_len:\n            raise ValueError(\n                \"The length of 'model_width' does not match 'model_len'\")\n        elif isinstance(model_width, int):\n            model_width = [model_width] * model_len\n\n        graph = Graph(self.input_shape, False)\n        output_node_id = 0\n        n_nodes_prev_layer = self.input_shape[0]\n        for width in model_width:\n            output_node_id = graph.add_layer(\n                StubDense(n_nodes_prev_layer, width), output_node_id\n            )\n            output_node_id = graph.add_layer(\n                StubDropout1d(Constant.MLP_DROPOUT_RATE), output_node_id\n            )\n            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n            n_nodes_prev_layer = width\n\n        graph.add_layer(\n            StubDense(\n                n_nodes_prev_layer,\n                self.n_output_node),\n            output_node_id)\n        return graph",
  "def __init__(self, n_output_node, input_shape):\n        self.n_output_node = n_output_node\n        self.input_shape = input_shape",
  "def generate(self, model_len, model_width):\n        pass",
  "def __init__(self, n_output_node, input_shape):\n        super(CnnGenerator, self).__init__(n_output_node, input_shape)\n        self.n_dim = len(self.input_shape) - 1\n        if len(self.input_shape) > 4:\n            raise ValueError(\"The input dimension is too high.\")\n        if len(self.input_shape) < 2:\n            raise ValueError(\"The input dimension is too low.\")\n        self.conv = get_conv_class(self.n_dim)\n        self.dropout = get_dropout_class(self.n_dim)\n        self.global_avg_pooling = get_global_avg_pooling_class(self.n_dim)\n        self.pooling = get_pooling_class(self.n_dim)\n        self.batch_norm = get_batch_norm_class(self.n_dim)",
  "def generate(self, model_len=None, model_width=None):\n        \"\"\"Generates a CNN.\n        Args:\n            model_len: An integer. Number of convolutional layers.\n            model_width: An integer. Number of filters for the convolutional layers.\n        Returns:\n            An instance of the class Graph. Represents the neural architecture graph of the generated model.\n        \"\"\"\n\n        if model_len is None:\n            model_len = Constant.MODEL_LEN\n        if model_width is None:\n            model_width = Constant.MODEL_WIDTH\n        pooling_len = int(model_len / 4)\n        graph = Graph(self.input_shape, False)\n        temp_input_channel = self.input_shape[-1]\n        output_node_id = 0\n        stride = 1\n        for i in range(model_len):\n            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n            output_node_id = graph.add_layer(\n                self.batch_norm(\n                    graph.node_list[output_node_id].shape[-1]), output_node_id\n            )\n            output_node_id = graph.add_layer(\n                self.conv(\n                    temp_input_channel,\n                    model_width,\n                    kernel_size=3,\n                    stride=stride),\n                output_node_id,\n            )\n            temp_input_channel = model_width\n            if pooling_len == 0 or (\n                    (i + 1) % pooling_len == 0 and i != model_len - 1):\n                output_node_id = graph.add_layer(\n                    self.pooling(), output_node_id)\n\n        output_node_id = graph.add_layer(\n            self.global_avg_pooling(), output_node_id)\n        output_node_id = graph.add_layer(\n            self.dropout(Constant.CONV_DROPOUT_RATE), output_node_id\n        )\n        output_node_id = graph.add_layer(\n            StubDense(graph.node_list[output_node_id].shape[0], model_width),\n            output_node_id,\n        )\n        output_node_id = graph.add_layer(StubReLU(), output_node_id)\n        graph.add_layer(\n            StubDense(\n                model_width,\n                self.n_output_node),\n            output_node_id)\n        return graph",
  "def __init__(self, n_output_node, input_shape):\n        \"\"\"Initialize the instance.\n        Args:\n            n_output_node: An integer. Number of output nodes in the network.\n            input_shape: A tuple. Input shape of the network. If it is 1D, ensure the value is appended by a comma\n                in the tuple.\n        \"\"\"\n        super(MlpGenerator, self).__init__(n_output_node, input_shape)\n        if len(self.input_shape) > 1:\n            raise ValueError(\"The input dimension is too high.\")",
  "def generate(self, model_len=None, model_width=None):\n        \"\"\"Generates a Multi-Layer Perceptron.\n        Args:\n            model_len: An integer. Number of hidden layers.\n            model_width: An integer or a list of integers of length `model_len`. If it is a list, it represents the\n                number of nodes in each hidden layer. If it is an integer, all hidden layers have nodes equal to this\n                value.\n        Returns:\n            An instance of the class Graph. Represents the neural architecture graph of the generated model.\n        \"\"\"\n        if model_len is None:\n            model_len = Constant.MODEL_LEN\n        if model_width is None:\n            model_width = Constant.MODEL_WIDTH\n        if isinstance(model_width, list) and not len(model_width) == model_len:\n            raise ValueError(\n                \"The length of 'model_width' does not match 'model_len'\")\n        elif isinstance(model_width, int):\n            model_width = [model_width] * model_len\n\n        graph = Graph(self.input_shape, False)\n        output_node_id = 0\n        n_nodes_prev_layer = self.input_shape[0]\n        for width in model_width:\n            output_node_id = graph.add_layer(\n                StubDense(n_nodes_prev_layer, width), output_node_id\n            )\n            output_node_id = graph.add_layer(\n                StubDropout1d(Constant.MLP_DROPOUT_RATE), output_node_id\n            )\n            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n            n_nodes_prev_layer = width\n\n        graph.add_layer(\n            StubDense(\n                n_nodes_prev_layer,\n                self.n_output_node),\n            output_node_id)\n        return graph",
  "class NetworkDescriptor:\n    \"\"\"A class describing the neural architecture for neural network kernel.\n    It only record the width of convolutional and dense layers, and the skip-connection types and positions.\n    \"\"\"\n\n    CONCAT_CONNECT = \"concat\"\n    ADD_CONNECT = \"add\"\n\n    def __init__(self):\n        self.skip_connections = []\n        self.layers = []\n\n    @property\n    def n_layers(self):\n        return len(self.layers)\n\n    def add_skip_connection(self, u, v, connection_type):\n        \"\"\" Add a skip-connection to the descriptor.\n        Args:\n            u: Number of convolutional layers before the starting point.\n            v: Number of convolutional layers before the ending point.\n            connection_type: Must be either CONCAT_CONNECT or ADD_CONNECT.\n        \"\"\"\n        if connection_type not in [self.CONCAT_CONNECT, self.ADD_CONNECT]:\n            raise ValueError(\n                \"connection_type should be NetworkDescriptor.CONCAT_CONNECT \"\n                \"or NetworkDescriptor.ADD_CONNECT.\"\n            )\n        self.skip_connections.append((u, v, connection_type))\n\n    def to_json(self):\n        ''' NetworkDescriptor to json representation\n        '''\n\n        skip_list = []\n        for u, v, connection_type in self.skip_connections:\n            skip_list.append({\"from\": u, \"to\": v, \"type\": connection_type})\n        return {\"node_list\": self.layers, \"skip_list\": skip_list}\n\n    def add_layer(self, layer):\n        ''' add one layer\n        '''\n\n        self.layers.append(layer)",
  "class Node:\n    \"\"\"A class for intermediate output tensor (node) in the Graph.\n    Attributes:\n        shape: A tuple describing the shape of the tensor.\n    \"\"\"\n\n    def __init__(self, shape):\n        self.shape = shape",
  "class Graph:\n    \"\"\"A class representing the neural architecture graph of a model.\n    Graph extracts the neural architecture graph from a model.\n    Each node in the graph is a intermediate tensor between layers.\n    Each layer is an edge in the graph.\n    Notably, multiple edges may refer to the same layer.\n    (e.g. Add layer is adding two tensor into one tensor. So it is related to two edges.)\n    Attributes:\n        weighted: A boolean of whether the weights and biases in the neural network\n            should be included in the graph.\n        input_shape: A tuple of integers, which does not include the batch axis.\n        node_list: A list of integers. The indices of the list are the identifiers.\n        layer_list: A list of stub layers. The indices of the list are the identifiers.\n        node_to_id: A dict instance mapping from node integers to their identifiers.\n        layer_to_id: A dict instance mapping from stub layers to their identifiers.\n        layer_id_to_input_node_ids: A dict instance mapping from layer identifiers\n            to their input nodes identifiers.\n        layer_id_to_output_node_ids: A dict instance mapping from layer identifiers\n            to their output nodes identifiers.\n        adj_list: A two dimensional list. The adjacency list of the graph. The first dimension is\n            identified by tensor identifiers. In each edge list, the elements are two-element tuples\n            of (tensor identifier, layer identifier).\n        reverse_adj_list: A reverse adjacent list in the same format as adj_list.\n        operation_history: A list saving all the network morphism operations.\n        vis: A dictionary of temporary storage for whether an local operation has been done\n            during the network morphism.\n    \"\"\"\n\n    def __init__(self, input_shape, weighted=True):\n        \"\"\"Initializer for Graph.\n        \"\"\"\n        self.input_shape = input_shape\n        self.weighted = weighted\n        self.node_list = []\n        self.layer_list = []\n        # node id start with 0\n        self.node_to_id = {}\n        self.layer_to_id = {}\n        self.layer_id_to_input_node_ids = {}\n        self.layer_id_to_output_node_ids = {}\n        self.adj_list = {}\n        self.reverse_adj_list = {}\n        self.operation_history = []\n        self.n_dim = len(input_shape) - 1\n        self.conv = get_conv_class(self.n_dim)\n        self.batch_norm = get_batch_norm_class(self.n_dim)\n\n        self.vis = None\n        self._add_node(Node(input_shape))\n\n    def add_layer(self, layer, input_node_id):\n        \"\"\"Add a layer to the Graph.\n        Args:\n            layer: An instance of the subclasses of StubLayer in layers.py.\n            input_node_id: An integer. The ID of the input node of the layer.\n        Returns:\n            output_node_id: An integer. The ID of the output node of the layer.\n        \"\"\"\n        if isinstance(input_node_id, Iterable):\n            layer.input = list(map(lambda x: self.node_list[x], input_node_id))\n            output_node_id = self._add_node(Node(layer.output_shape))\n            for node_id in input_node_id:\n                self._add_edge(layer, node_id, output_node_id)\n\n        else:\n            layer.input = self.node_list[input_node_id]\n            output_node_id = self._add_node(Node(layer.output_shape))\n            self._add_edge(layer, input_node_id, output_node_id)\n\n        layer.output = self.node_list[output_node_id]\n        return output_node_id\n\n    def clear_operation_history(self):\n        self.operation_history = []\n\n    @property\n    def n_nodes(self):\n        \"\"\"Return the number of nodes in the model.\"\"\"\n        return len(self.node_list)\n\n    @property\n    def n_layers(self):\n        \"\"\"Return the number of layers in the model.\"\"\"\n        return len(self.layer_list)\n\n    def _add_node(self, node):\n        \"\"\"Add a new node to node_list and give the node an ID.\n        Args:\n            node: An instance of Node.\n        Returns:\n            node_id: An integer.\n        \"\"\"\n        node_id = len(self.node_list)\n        self.node_to_id[node] = node_id\n        self.node_list.append(node)\n        self.adj_list[node_id] = []\n        self.reverse_adj_list[node_id] = []\n        return node_id\n\n    def _add_edge(self, layer, input_id, output_id):\n        \"\"\"Add a new layer to the graph. The nodes should be created in advance.\"\"\"\n\n        if layer in self.layer_to_id:\n            layer_id = self.layer_to_id[layer]\n            if input_id not in self.layer_id_to_input_node_ids[layer_id]:\n                self.layer_id_to_input_node_ids[layer_id].append(input_id)\n            if output_id not in self.layer_id_to_output_node_ids[layer_id]:\n                self.layer_id_to_output_node_ids[layer_id].append(output_id)\n        else:\n            layer_id = len(self.layer_list)\n            self.layer_list.append(layer)\n            self.layer_to_id[layer] = layer_id\n            self.layer_id_to_input_node_ids[layer_id] = [input_id]\n            self.layer_id_to_output_node_ids[layer_id] = [output_id]\n\n        self.adj_list[input_id].append((output_id, layer_id))\n        self.reverse_adj_list[output_id].append((input_id, layer_id))\n\n    def _redirect_edge(self, u_id, v_id, new_v_id):\n        \"\"\"Redirect the layer to a new node.\n        Change the edge originally from `u_id` to `v_id` into an edge from `u_id` to `new_v_id`\n        while keeping all other property of the edge the same.\n        \"\"\"\n        layer_id = None\n        for index, edge_tuple in enumerate(self.adj_list[u_id]):\n            if edge_tuple[0] == v_id:\n                layer_id = edge_tuple[1]\n                self.adj_list[u_id][index] = (new_v_id, layer_id)\n                self.layer_list[layer_id].output = self.node_list[new_v_id]\n                break\n\n        for index, edge_tuple in enumerate(self.reverse_adj_list[v_id]):\n            if edge_tuple[0] == u_id:\n                layer_id = edge_tuple[1]\n                self.reverse_adj_list[v_id].remove(edge_tuple)\n                break\n        self.reverse_adj_list[new_v_id].append((u_id, layer_id))\n        for index, value in enumerate(\n                self.layer_id_to_output_node_ids[layer_id]):\n            if value == v_id:\n                self.layer_id_to_output_node_ids[layer_id][index] = new_v_id\n                break\n\n    def _replace_layer(self, layer_id, new_layer):\n        \"\"\"Replace the layer with a new layer.\"\"\"\n        old_layer = self.layer_list[layer_id]\n        new_layer.input = old_layer.input\n        new_layer.output = old_layer.output\n        new_layer.output.shape = new_layer.output_shape\n        self.layer_list[layer_id] = new_layer\n        self.layer_to_id[new_layer] = layer_id\n        self.layer_to_id.pop(old_layer)\n\n    @property\n    def topological_order(self):\n        \"\"\"Return the topological order of the node IDs from the input node to the output node.\"\"\"\n        q = Queue()\n        in_degree = {}\n        for i in range(self.n_nodes):\n            in_degree[i] = 0\n        for u in range(self.n_nodes):\n            for v, _ in self.adj_list[u]:\n                in_degree[v] += 1\n        for i in range(self.n_nodes):\n            if in_degree[i] == 0:\n                q.put(i)\n\n        order_list = []\n        while not q.empty():\n            u = q.get()\n            order_list.append(u)\n            for v, _ in self.adj_list[u]:\n                in_degree[v] -= 1\n                if in_degree[v] == 0:\n                    q.put(v)\n        return order_list\n\n    def _get_pooling_layers(self, start_node_id, end_node_id):\n        \"\"\"Given two node IDs, return all the pooling layers between them.\"\"\"\n        layer_list = []\n        node_list = [start_node_id]\n        assert self._depth_first_search(end_node_id, layer_list, node_list)\n        ret = []\n        for layer_id in layer_list:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Pooling\"):\n                ret.append(layer)\n            elif is_layer(layer, \"Conv\") and layer.stride != 1:\n                ret.append(layer)\n        return ret\n\n    def _depth_first_search(self, target_id, layer_id_list, node_list):\n        \"\"\"Search for all the layers and nodes down the path.\n        A recursive function to search all the layers and nodes between the node in the node_list\n            and the node with target_id.\"\"\"\n        assert len(node_list) <= self.n_nodes\n        u = node_list[-1]\n        if u == target_id:\n            return True\n\n        for v, layer_id in self.adj_list[u]:\n            layer_id_list.append(layer_id)\n            node_list.append(v)\n            if self._depth_first_search(target_id, layer_id_list, node_list):\n                return True\n            layer_id_list.pop()\n            node_list.pop()\n\n        return False\n\n    def _search(self, u, start_dim, total_dim, n_add):\n        \"\"\"Search the graph for all the layers to be widened caused by an operation.\n        It is an recursive function with duplication check to avoid deadlock.\n        It searches from a starting node u until the corresponding layers has been widened.\n        Args:\n            u: The starting node ID.\n            start_dim: The position to insert the additional dimensions.\n            total_dim: The total number of dimensions the layer has before widening.\n            n_add: The number of dimensions to add.\n        \"\"\"\n        if (u, start_dim, total_dim, n_add) in self.vis:\n            return\n        self.vis[(u, start_dim, total_dim, n_add)] = True\n        for v, layer_id in self.adj_list[u]:\n            layer = self.layer_list[layer_id]\n\n            if is_layer(layer, \"Conv\"):\n                new_layer = wider_next_conv(\n                    layer, start_dim, total_dim, n_add, self.weighted\n                )\n                self._replace_layer(layer_id, new_layer)\n\n            elif is_layer(layer, \"Dense\"):\n                new_layer = wider_next_dense(\n                    layer, start_dim, total_dim, n_add, self.weighted\n                )\n                self._replace_layer(layer_id, new_layer)\n\n            elif is_layer(layer, \"BatchNormalization\"):\n                new_layer = wider_bn(\n                    layer, start_dim, total_dim, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n                self._search(v, start_dim, total_dim, n_add)\n\n            elif is_layer(layer, \"Concatenate\"):\n                if self.layer_id_to_input_node_ids[layer_id][1] == u:\n                    # u is on the right of the concat\n                    # next_start_dim += next_total_dim - total_dim\n                    left_dim = self._upper_layer_width(\n                        self.layer_id_to_input_node_ids[layer_id][0]\n                    )\n                    next_start_dim = start_dim + left_dim\n                    next_total_dim = total_dim + left_dim\n                else:\n                    next_start_dim = start_dim\n                    next_total_dim = total_dim + self._upper_layer_width(\n                        self.layer_id_to_input_node_ids[layer_id][1]\n                    )\n                self._search(v, next_start_dim, next_total_dim, n_add)\n\n            else:\n                self._search(v, start_dim, total_dim, n_add)\n\n        for v, layer_id in self.reverse_adj_list[u]:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Conv\"):\n                new_layer = wider_pre_conv(layer, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n            elif is_layer(layer, \"Dense\"):\n                new_layer = wider_pre_dense(layer, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n            elif is_layer(layer, \"Concatenate\"):\n                continue\n            else:\n                self._search(v, start_dim, total_dim, n_add)\n\n    def _upper_layer_width(self, u):\n        for v, layer_id in self.reverse_adj_list[u]:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Conv\") or is_layer(layer, \"Dense\"):\n                return layer_width(layer)\n            elif is_layer(layer, \"Concatenate\"):\n                a = self.layer_id_to_input_node_ids[layer_id][0]\n                b = self.layer_id_to_input_node_ids[layer_id][1]\n                return self._upper_layer_width(a) + self._upper_layer_width(b)\n            else:\n                return self._upper_layer_width(v)\n        return self.node_list[0].shape[-1]\n\n    def to_deeper_model(self, target_id, new_layer):\n        \"\"\"Insert a relu-conv-bn block after the target block.\n        Args:\n            target_id: A convolutional layer ID. The new block should be inserted after the block.\n            new_layer: An instance of StubLayer subclasses.\n        \"\"\"\n        self.operation_history.append(\n            (\"to_deeper_model\", target_id, new_layer))\n        input_id = self.layer_id_to_input_node_ids[target_id][0]\n        output_id = self.layer_id_to_output_node_ids[target_id][0]\n        if self.weighted:\n            if is_layer(new_layer, \"Dense\"):\n                init_dense_weight(new_layer)\n            elif is_layer(new_layer, \"Conv\"):\n                init_conv_weight(new_layer)\n            elif is_layer(new_layer, \"BatchNormalization\"):\n                init_bn_weight(new_layer)\n\n        self._insert_new_layers([new_layer], input_id, output_id)\n\n    def to_wider_model(self, pre_layer_id, n_add):\n        \"\"\"Widen the last dimension of the output of the pre_layer.\n        Args:\n            pre_layer_id: The ID of a convolutional layer or dense layer.\n            n_add: The number of dimensions to add.\n        \"\"\"\n        self.operation_history.append((\"to_wider_model\", pre_layer_id, n_add))\n        pre_layer = self.layer_list[pre_layer_id]\n        output_id = self.layer_id_to_output_node_ids[pre_layer_id][0]\n        dim = layer_width(pre_layer)\n        self.vis = {}\n        self._search(output_id, dim, dim, n_add)\n        # Update the tensor shapes.\n        for u in self.topological_order:\n            for v, layer_id in self.adj_list[u]:\n                self.node_list[v].shape = self.layer_list[layer_id].output_shape\n\n    def _insert_new_layers(self, new_layers, start_node_id, end_node_id):\n        \"\"\"Insert the new_layers after the node with start_node_id.\"\"\"\n        new_node_id = self._add_node(deepcopy(self.node_list[end_node_id]))\n        temp_output_id = new_node_id\n        for layer in new_layers[:-1]:\n            temp_output_id = self.add_layer(layer, temp_output_id)\n\n        self._add_edge(new_layers[-1], temp_output_id, end_node_id)\n        new_layers[-1].input = self.node_list[temp_output_id]\n        new_layers[-1].output = self.node_list[end_node_id]\n        self._redirect_edge(start_node_id, end_node_id, new_node_id)\n\n    def _block_end_node(self, layer_id, block_size):\n        ret = self.layer_id_to_output_node_ids[layer_id][0]\n        for _ in range(block_size - 2):\n            ret = self.adj_list[ret][0][0]\n        return ret\n\n    def _dense_block_end_node(self, layer_id):\n        return self.layer_id_to_input_node_ids[layer_id][0]\n\n    def _conv_block_end_node(self, layer_id):\n        \"\"\"Get the input node ID of the last layer in the block by layer ID.\n            Return the input node ID of the last layer in the convolutional block.\n        Args:\n            layer_id: the convolutional layer ID.\n        \"\"\"\n        return self._block_end_node(layer_id, Constant.CONV_BLOCK_DISTANCE)\n\n    def to_add_skip_model(self, start_id, end_id):\n        \"\"\"Add a weighted add skip-connection from after start node to end node.\n        Args:\n            start_id: The convolutional layer ID, after which to start the skip-connection.\n            end_id: The convolutional layer ID, after which to end the skip-connection.\n        \"\"\"\n        self.operation_history.append((\"to_add_skip_model\", start_id, end_id))\n        filters_end = self.layer_list[end_id].output.shape[-1]\n        filters_start = self.layer_list[start_id].output.shape[-1]\n        start_node_id = self.layer_id_to_output_node_ids[start_id][0]\n\n        pre_end_node_id = self.layer_id_to_input_node_ids[end_id][0]\n        end_node_id = self.layer_id_to_output_node_ids[end_id][0]\n\n        skip_output_id = self._insert_pooling_layer_chain(\n            start_node_id, end_node_id)\n\n        # Add the conv layer\n        new_conv_layer = get_conv_class(\n            self.n_dim)(\n                filters_start,\n                filters_end,\n                1)\n        skip_output_id = self.add_layer(new_conv_layer, skip_output_id)\n\n        # Add the add layer.\n        add_input_node_id = self._add_node(\n            deepcopy(self.node_list[end_node_id]))\n        add_layer = StubAdd()\n\n        self._redirect_edge(pre_end_node_id, end_node_id, add_input_node_id)\n        self._add_edge(add_layer, add_input_node_id, end_node_id)\n        self._add_edge(add_layer, skip_output_id, end_node_id)\n        add_layer.input = [\n            self.node_list[add_input_node_id],\n            self.node_list[skip_output_id],\n        ]\n        add_layer.output = self.node_list[end_node_id]\n        self.node_list[end_node_id].shape = add_layer.output_shape\n\n        # Set weights to the additional conv layer.\n        if self.weighted:\n            filter_shape = (1,) * self.n_dim\n            weights = np.zeros((filters_end, filters_start) + filter_shape)\n            bias = np.zeros(filters_end)\n            new_conv_layer.set_weights(\n                (add_noise(weights, np.array([0, 1])), add_noise(\n                    bias, np.array([0, 1])))\n            )\n\n    def to_concat_skip_model(self, start_id, end_id):\n        \"\"\"Add a weighted add concatenate connection from after start node to end node.\n        Args:\n            start_id: The convolutional layer ID, after which to start the skip-connection.\n            end_id: The convolutional layer ID, after which to end the skip-connection.\n        \"\"\"\n        self.operation_history.append(\n            (\"to_concat_skip_model\", start_id, end_id))\n        filters_end = self.layer_list[end_id].output.shape[-1]\n        filters_start = self.layer_list[start_id].output.shape[-1]\n        start_node_id = self.layer_id_to_output_node_ids[start_id][0]\n\n        pre_end_node_id = self.layer_id_to_input_node_ids[end_id][0]\n        end_node_id = self.layer_id_to_output_node_ids[end_id][0]\n\n        skip_output_id = self._insert_pooling_layer_chain(\n            start_node_id, end_node_id)\n\n        concat_input_node_id = self._add_node(\n            deepcopy(self.node_list[end_node_id]))\n        self._redirect_edge(pre_end_node_id, end_node_id, concat_input_node_id)\n\n        concat_layer = StubConcatenate()\n        concat_layer.input = [\n            self.node_list[concat_input_node_id],\n            self.node_list[skip_output_id],\n        ]\n        concat_output_node_id = self._add_node(Node(concat_layer.output_shape))\n        self._add_edge(\n            concat_layer,\n            concat_input_node_id,\n            concat_output_node_id)\n        self._add_edge(concat_layer, skip_output_id, concat_output_node_id)\n        concat_layer.output = self.node_list[concat_output_node_id]\n        self.node_list[concat_output_node_id].shape = concat_layer.output_shape\n\n        # Add the concatenate layer.\n        new_conv_layer = get_conv_class(self.n_dim)(\n            filters_start + filters_end, filters_end, 1\n        )\n        self._add_edge(new_conv_layer, concat_output_node_id, end_node_id)\n        new_conv_layer.input = self.node_list[concat_output_node_id]\n        new_conv_layer.output = self.node_list[end_node_id]\n        self.node_list[end_node_id].shape = new_conv_layer.output_shape\n\n        if self.weighted:\n            filter_shape = (1,) * self.n_dim\n            weights = np.zeros((filters_end, filters_end) + filter_shape)\n            for i in range(filters_end):\n                filter_weight = np.zeros((filters_end,) + filter_shape)\n                center_index = (i,) + (0,) * self.n_dim\n                filter_weight[center_index] = 1\n                weights[i, ...] = filter_weight\n            weights = np.concatenate(\n                (weights, np.zeros((filters_end, filters_start) + filter_shape)), axis=1\n            )\n            bias = np.zeros(filters_end)\n            new_conv_layer.set_weights(\n                (add_noise(weights, np.array([0, 1])), add_noise(\n                    bias, np.array([0, 1])))\n            )\n\n    def _insert_pooling_layer_chain(self, start_node_id, end_node_id):\n        skip_output_id = start_node_id\n        for layer in self._get_pooling_layers(start_node_id, end_node_id):\n            new_layer = deepcopy(layer)\n            if is_layer(new_layer, \"Conv\"):\n                filters = self.node_list[start_node_id].shape[-1]\n                new_layer = get_conv_class(self.n_dim)(\n                    filters, filters, 1, layer.stride)\n                if self.weighted:\n                    init_conv_weight(new_layer)\n            else:\n                new_layer = deepcopy(layer)\n            skip_output_id = self.add_layer(new_layer, skip_output_id)\n        skip_output_id = self.add_layer(StubReLU(), skip_output_id)\n        return skip_output_id\n\n    def extract_descriptor(self):\n        \"\"\"Extract the the description of the Graph as an instance of NetworkDescriptor.\"\"\"\n        main_chain = self.get_main_chain()\n        index_in_main_chain = {}\n        for index, u in enumerate(main_chain):\n            index_in_main_chain[u] = index\n\n        ret = NetworkDescriptor()\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v not in index_in_main_chain:\n                    continue\n                layer = self.layer_list[layer_id]\n                copied_layer = copy(layer)\n                copied_layer.weights = None\n                ret.add_layer(deepcopy(copied_layer))\n\n        for u in index_in_main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v not in index_in_main_chain:\n                    temp_u = u\n                    temp_v = v\n                    temp_layer_id = layer_id\n                    skip_type = None\n                    while not (\n                            temp_v in index_in_main_chain and temp_u in index_in_main_chain):\n                        if is_layer(\n                                self.layer_list[temp_layer_id], \"Concatenate\"):\n                            skip_type = NetworkDescriptor.CONCAT_CONNECT\n                        if is_layer(self.layer_list[temp_layer_id], \"Add\"):\n                            skip_type = NetworkDescriptor.ADD_CONNECT\n                        temp_u = temp_v\n                        temp_v, temp_layer_id = self.adj_list[temp_v][0]\n                    ret.add_skip_connection(\n                        index_in_main_chain[u], index_in_main_chain[temp_u], skip_type\n                    )\n\n                elif index_in_main_chain[v] - index_in_main_chain[u] != 1:\n                    skip_type = None\n                    if is_layer(self.layer_list[layer_id], \"Concatenate\"):\n                        skip_type = NetworkDescriptor.CONCAT_CONNECT\n                    if is_layer(self.layer_list[layer_id], \"Add\"):\n                        skip_type = NetworkDescriptor.ADD_CONNECT\n                    ret.add_skip_connection(\n                        index_in_main_chain[u], index_in_main_chain[v], skip_type\n                    )\n\n        return ret\n\n    def clear_weights(self):\n        ''' clear weights of the graph\n        '''\n        self.weighted = False\n        for layer in self.layer_list:\n            layer.weights = None\n\n    def produce_torch_model(self):\n        \"\"\"Build a new Torch model based on the current graph.\"\"\"\n        return TorchModel(self)\n\n    def produce_keras_model(self):\n        \"\"\"Build a new keras model based on the current graph.\"\"\"\n        return KerasModel(self).model\n\n    def produce_onnx_model(self):\n        \"\"\"Build a new ONNX model based on the current graph.\"\"\"\n        return ONNXModel(self)\n\n    def parsing_onnx_model(self, onnx_model):\n        '''to do in the future to use the onnx model\n        '''\n        return ONNXModel(onnx_model)\n\n    def produce_json_model(self):\n        \"\"\"Build a new Json model based on the current graph.\"\"\"\n        return JSONModel(self).data\n\n    @classmethod\n    def parsing_json_model(cls, json_model):\n        '''build a graph from json\n        '''\n        return json_to_graph(json_model)\n\n    def _layer_ids_in_order(self, layer_ids):\n        node_id_to_order_index = {}\n        for index, node_id in enumerate(self.topological_order):\n            node_id_to_order_index[node_id] = index\n        return sorted(\n            layer_ids,\n            key=lambda layer_id: node_id_to_order_index[\n                self.layer_id_to_output_node_ids[layer_id][0]\n            ],\n        )\n\n    def _layer_ids_by_type(self, type_str):\n        return list(\n            filter(\n                lambda layer_id: is_layer(self.layer_list[layer_id], type_str),\n                range(self.n_layers),\n            )\n        )\n\n    def get_main_chain_layers(self):\n        \"\"\"Return a list of layer IDs in the main chain.\"\"\"\n        main_chain = self.get_main_chain()\n        ret = []\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v in main_chain and u in main_chain:\n                    ret.append(layer_id)\n        return ret\n\n    def _conv_layer_ids_in_order(self):\n        return list(\n            filter(\n                lambda layer_id: is_layer(self.layer_list[layer_id], \"Conv\"),\n                self.get_main_chain_layers(),\n            )\n        )\n\n    def _dense_layer_ids_in_order(self):\n        return self._layer_ids_in_order(self._layer_ids_by_type(\"Dense\"))\n\n    def deep_layer_ids(self):\n        ret = []\n        for layer_id in self.get_main_chain_layers():\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"GlobalAveragePooling\"):\n                break\n            if is_layer(layer, \"Add\") or is_layer(layer, \"Concatenate\"):\n                continue\n            ret.append(layer_id)\n        return ret\n\n    def wide_layer_ids(self):\n        return (\n            self._conv_layer_ids_in_order(\n            )[:-1] + self._dense_layer_ids_in_order()[:-1]\n        )\n\n    def skip_connection_layer_ids(self):\n        return self.deep_layer_ids()[:-1]\n\n    def size(self):\n        return sum(list(map(lambda x: x.size(), self.layer_list)))\n\n    def get_main_chain(self):\n        \"\"\"Returns the main chain node ID list.\"\"\"\n        pre_node = {}\n        distance = {}\n        for i in range(self.n_nodes):\n            distance[i] = 0\n            pre_node[i] = i\n        for i in range(self.n_nodes - 1):\n            for u in range(self.n_nodes):\n                for v, _ in self.adj_list[u]:\n                    if distance[u] + 1 > distance[v]:\n                        distance[v] = distance[u] + 1\n                        pre_node[v] = u\n        temp_id = 0\n        for i in range(self.n_nodes):\n            if distance[i] > distance[temp_id]:\n                temp_id = i\n        ret = []\n        for i in range(self.n_nodes + 5):\n            ret.append(temp_id)\n            if pre_node[temp_id] == temp_id:\n                break\n            temp_id = pre_node[temp_id]\n        assert temp_id == pre_node[temp_id]\n        ret.reverse()\n        return ret",
  "class TorchModel(torch.nn.Module):\n    \"\"\"A neural network class using pytorch constructed from an instance of Graph.\"\"\"\n\n    def __init__(self, graph):\n        super(TorchModel, self).__init__()\n        self.graph = graph\n        self.layers = []\n        for layer in graph.layer_list:\n            self.layers.append(layer.to_real_layer())\n        if graph.weighted:\n            for index, layer in enumerate(self.layers):\n                set_stub_weight_to_torch(self.graph.layer_list[index], layer)\n        for index, layer in enumerate(self.layers):\n            self.add_module(str(index), layer)\n\n    def forward(self, input_tensor):\n        topo_node_list = self.graph.topological_order\n        output_id = topo_node_list[-1]\n        input_id = topo_node_list[0]\n\n        node_list = deepcopy(self.graph.node_list)\n        node_list[input_id] = input_tensor\n\n        for v in topo_node_list:\n            for u, layer_id in self.graph.reverse_adj_list[v]:\n                layer = self.graph.layer_list[layer_id]\n                torch_layer = self.layers[layer_id]\n\n                if isinstance(layer, (StubAdd, StubConcatenate)):\n                    edge_input_tensor = list(\n                        map(\n                            lambda x: node_list[x],\n                            self.graph.layer_id_to_input_node_ids[layer_id],\n                        )\n                    )\n                else:\n                    edge_input_tensor = node_list[u]\n\n                temp_tensor = torch_layer(edge_input_tensor)\n                node_list[v] = temp_tensor\n        return node_list[output_id]\n\n    def set_weight_to_graph(self):\n        self.graph.weighted = True\n        for index, layer in enumerate(self.layers):\n            set_torch_weight_to_stub(layer, self.graph.layer_list[index])",
  "class KerasModel:\n    def __init__(self, graph):\n        import keras\n\n        self.graph = graph\n        self.layers = []\n        for layer in graph.layer_list:\n            self.layers.append(to_real_keras_layer(layer))\n\n        # Construct the keras graph.\n        # Input\n        topo_node_list = self.graph.topological_order\n        output_id = topo_node_list[-1]\n        input_id = topo_node_list[0]\n        input_tensor = keras.layers.Input(\n            shape=graph.node_list[input_id].shape)\n\n        node_list = deepcopy(self.graph.node_list)\n        node_list[input_id] = input_tensor\n\n        # Output\n        for v in topo_node_list:\n            for u, layer_id in self.graph.reverse_adj_list[v]:\n                layer = self.graph.layer_list[layer_id]\n                keras_layer = self.layers[layer_id]\n\n                if isinstance(layer, (StubAdd, StubConcatenate)):\n                    edge_input_tensor = list(\n                        map(\n                            lambda x: node_list[x],\n                            self.graph.layer_id_to_input_node_ids[layer_id],\n                        )\n                    )\n                else:\n                    edge_input_tensor = node_list[u]\n\n                temp_tensor = keras_layer(edge_input_tensor)\n                node_list[v] = temp_tensor\n\n        output_tensor = node_list[output_id]\n        output_tensor = keras.layers.Activation(\"softmax\", name=\"activation_add\")(\n            output_tensor\n        )\n        self.model = keras.models.Model(\n            inputs=input_tensor, outputs=output_tensor)\n\n        if graph.weighted:\n            for index, layer in enumerate(self.layers):\n                set_stub_weight_to_keras(self.graph.layer_list[index], layer)\n\n    def set_weight_to_graph(self):\n        self.graph.weighted = True\n        for index, layer in enumerate(self.layers):\n            set_keras_weight_to_stub(layer, self.graph.layer_list[index])",
  "class ONNXModel:\n    # to do in the future using onnx ir\n    def __init__(self, graph):\n        pass",
  "class JSONModel:\n    def __init__(self, graph):\n        data = dict()\n        node_list = list()\n        layer_list = list()\n        operation_history = list()\n\n        data[\"input_shape\"] = graph.input_shape\n        vis = graph.vis\n        data[\"vis\"] = list(vis.keys()) if vis is not None else None\n        data[\"weighted\"] = graph.weighted\n\n        for item in graph.operation_history:\n            if item[0] == \"to_deeper_model\":\n                operation_history.append(\n                    [\n                        item[0],\n                        item[1],\n                        layer_description_extractor(item[2], graph.node_to_id),\n                    ]\n                )\n            else:\n                operation_history.append(item)\n        data[\"operation_history\"] = operation_history\n        data[\"layer_id_to_input_node_ids\"] = graph.layer_id_to_input_node_ids\n        data[\"layer_id_to_output_node_ids\"] = graph.layer_id_to_output_node_ids\n        data[\"adj_list\"] = graph.adj_list\n        data[\"reverse_adj_list\"] = graph.reverse_adj_list\n\n        for node in graph.node_list:\n            node_id = graph.node_to_id[node]\n            node_information = node.shape\n            node_list.append((node_id, node_information))\n\n        for layer_id, item in enumerate(graph.layer_list):\n            layer = graph.layer_list[layer_id]\n            layer_information = layer_description_extractor(\n                layer, graph.node_to_id)\n            layer_list.append((layer_id, layer_information))\n\n        data[\"node_list\"] = node_list\n        data[\"layer_list\"] = layer_list\n\n        self.data = data",
  "def graph_to_onnx(graph, onnx_model_path):\n    import onnx\n    # to do in the future using onnx ir\n    onnx_out = graph.produce_onnx_model()\n    onnx.save(onnx_out, onnx_model_path)\n    return onnx_out",
  "def onnx_to_graph(onnx_model, input_shape):\n    # to do in the future using onnx ir\n    graph = Graph(input_shape, False)\n    graph.parsing_onnx_model(onnx_model)\n    return graph",
  "def graph_to_json(graph, json_model_path):\n    json_out = graph.produce_json_model()\n    with open(json_model_path, \"w\") as fout:\n        json.dump(json_out, fout)\n    json_out = json.dumps(json_out)\n    return json_out",
  "def json_to_graph(json_model: str):\n    json_model = json.loads(json_model)\n    # restore graph data from json data\n    input_shape = tuple(json_model[\"input_shape\"])\n    node_list = list()\n    node_to_id = dict()\n    id_to_node = dict()\n    layer_list = list()\n    layer_to_id = dict()\n    operation_history = list()\n    graph = Graph(input_shape, False)\n\n    graph.input_shape = input_shape\n    vis = json_model[\"vis\"]\n    graph.vis = {\n        tuple(item): True for item in vis} if vis is not None else None\n    graph.weighted = json_model[\"weighted\"]\n    layer_id_to_input_node_ids = json_model[\"layer_id_to_input_node_ids\"]\n    graph.layer_id_to_input_node_ids = {\n        int(k): v for k, v in layer_id_to_input_node_ids.items()\n    }\n    layer_id_to_output_node_ids = json_model[\"layer_id_to_output_node_ids\"]\n    graph.layer_id_to_output_node_ids = {\n        int(k): v for k, v in layer_id_to_output_node_ids.items()\n    }\n    adj_list = {}\n    for k, v in json_model[\"adj_list\"].items():\n        adj_list[int(k)] = [tuple(i) for i in v]\n    graph.adj_list = adj_list\n    reverse_adj_list = {}\n    for k, v in json_model[\"reverse_adj_list\"].items():\n        reverse_adj_list[int(k)] = [tuple(i) for i in v]\n    graph.reverse_adj_list = reverse_adj_list\n\n    for item in json_model[\"node_list\"]:\n        new_node = Node(tuple(item[1]))\n        node_id = item[0]\n        node_list.append(new_node)\n        node_to_id[new_node] = node_id\n        id_to_node[node_id] = new_node\n\n    for item in json_model[\"operation_history\"]:\n        if item[0] == \"to_deeper_model\":\n            operation_history.append(\n                (item[0], item[1], layer_description_builder(item[2], id_to_node))\n            )\n        else:\n            operation_history.append(item)\n    graph.operation_history = operation_history\n\n    for item in json_model[\"layer_list\"]:\n        new_layer = layer_description_builder(item[1], id_to_node)\n        layer_id = int(item[0])\n        layer_list.append(new_layer)\n        layer_to_id[new_layer] = layer_id\n\n    graph.node_list = node_list\n    graph.node_to_id = node_to_id\n    graph.layer_list = layer_list\n    graph.layer_to_id = layer_to_id\n\n    return graph",
  "def __init__(self):\n        self.skip_connections = []\n        self.layers = []",
  "def n_layers(self):\n        return len(self.layers)",
  "def add_skip_connection(self, u, v, connection_type):\n        \"\"\" Add a skip-connection to the descriptor.\n        Args:\n            u: Number of convolutional layers before the starting point.\n            v: Number of convolutional layers before the ending point.\n            connection_type: Must be either CONCAT_CONNECT or ADD_CONNECT.\n        \"\"\"\n        if connection_type not in [self.CONCAT_CONNECT, self.ADD_CONNECT]:\n            raise ValueError(\n                \"connection_type should be NetworkDescriptor.CONCAT_CONNECT \"\n                \"or NetworkDescriptor.ADD_CONNECT.\"\n            )\n        self.skip_connections.append((u, v, connection_type))",
  "def to_json(self):\n        ''' NetworkDescriptor to json representation\n        '''\n\n        skip_list = []\n        for u, v, connection_type in self.skip_connections:\n            skip_list.append({\"from\": u, \"to\": v, \"type\": connection_type})\n        return {\"node_list\": self.layers, \"skip_list\": skip_list}",
  "def add_layer(self, layer):\n        ''' add one layer\n        '''\n\n        self.layers.append(layer)",
  "def __init__(self, shape):\n        self.shape = shape",
  "def __init__(self, input_shape, weighted=True):\n        \"\"\"Initializer for Graph.\n        \"\"\"\n        self.input_shape = input_shape\n        self.weighted = weighted\n        self.node_list = []\n        self.layer_list = []\n        # node id start with 0\n        self.node_to_id = {}\n        self.layer_to_id = {}\n        self.layer_id_to_input_node_ids = {}\n        self.layer_id_to_output_node_ids = {}\n        self.adj_list = {}\n        self.reverse_adj_list = {}\n        self.operation_history = []\n        self.n_dim = len(input_shape) - 1\n        self.conv = get_conv_class(self.n_dim)\n        self.batch_norm = get_batch_norm_class(self.n_dim)\n\n        self.vis = None\n        self._add_node(Node(input_shape))",
  "def add_layer(self, layer, input_node_id):\n        \"\"\"Add a layer to the Graph.\n        Args:\n            layer: An instance of the subclasses of StubLayer in layers.py.\n            input_node_id: An integer. The ID of the input node of the layer.\n        Returns:\n            output_node_id: An integer. The ID of the output node of the layer.\n        \"\"\"\n        if isinstance(input_node_id, Iterable):\n            layer.input = list(map(lambda x: self.node_list[x], input_node_id))\n            output_node_id = self._add_node(Node(layer.output_shape))\n            for node_id in input_node_id:\n                self._add_edge(layer, node_id, output_node_id)\n\n        else:\n            layer.input = self.node_list[input_node_id]\n            output_node_id = self._add_node(Node(layer.output_shape))\n            self._add_edge(layer, input_node_id, output_node_id)\n\n        layer.output = self.node_list[output_node_id]\n        return output_node_id",
  "def clear_operation_history(self):\n        self.operation_history = []",
  "def n_nodes(self):\n        \"\"\"Return the number of nodes in the model.\"\"\"\n        return len(self.node_list)",
  "def n_layers(self):\n        \"\"\"Return the number of layers in the model.\"\"\"\n        return len(self.layer_list)",
  "def _add_node(self, node):\n        \"\"\"Add a new node to node_list and give the node an ID.\n        Args:\n            node: An instance of Node.\n        Returns:\n            node_id: An integer.\n        \"\"\"\n        node_id = len(self.node_list)\n        self.node_to_id[node] = node_id\n        self.node_list.append(node)\n        self.adj_list[node_id] = []\n        self.reverse_adj_list[node_id] = []\n        return node_id",
  "def _add_edge(self, layer, input_id, output_id):\n        \"\"\"Add a new layer to the graph. The nodes should be created in advance.\"\"\"\n\n        if layer in self.layer_to_id:\n            layer_id = self.layer_to_id[layer]\n            if input_id not in self.layer_id_to_input_node_ids[layer_id]:\n                self.layer_id_to_input_node_ids[layer_id].append(input_id)\n            if output_id not in self.layer_id_to_output_node_ids[layer_id]:\n                self.layer_id_to_output_node_ids[layer_id].append(output_id)\n        else:\n            layer_id = len(self.layer_list)\n            self.layer_list.append(layer)\n            self.layer_to_id[layer] = layer_id\n            self.layer_id_to_input_node_ids[layer_id] = [input_id]\n            self.layer_id_to_output_node_ids[layer_id] = [output_id]\n\n        self.adj_list[input_id].append((output_id, layer_id))\n        self.reverse_adj_list[output_id].append((input_id, layer_id))",
  "def _redirect_edge(self, u_id, v_id, new_v_id):\n        \"\"\"Redirect the layer to a new node.\n        Change the edge originally from `u_id` to `v_id` into an edge from `u_id` to `new_v_id`\n        while keeping all other property of the edge the same.\n        \"\"\"\n        layer_id = None\n        for index, edge_tuple in enumerate(self.adj_list[u_id]):\n            if edge_tuple[0] == v_id:\n                layer_id = edge_tuple[1]\n                self.adj_list[u_id][index] = (new_v_id, layer_id)\n                self.layer_list[layer_id].output = self.node_list[new_v_id]\n                break\n\n        for index, edge_tuple in enumerate(self.reverse_adj_list[v_id]):\n            if edge_tuple[0] == u_id:\n                layer_id = edge_tuple[1]\n                self.reverse_adj_list[v_id].remove(edge_tuple)\n                break\n        self.reverse_adj_list[new_v_id].append((u_id, layer_id))\n        for index, value in enumerate(\n                self.layer_id_to_output_node_ids[layer_id]):\n            if value == v_id:\n                self.layer_id_to_output_node_ids[layer_id][index] = new_v_id\n                break",
  "def _replace_layer(self, layer_id, new_layer):\n        \"\"\"Replace the layer with a new layer.\"\"\"\n        old_layer = self.layer_list[layer_id]\n        new_layer.input = old_layer.input\n        new_layer.output = old_layer.output\n        new_layer.output.shape = new_layer.output_shape\n        self.layer_list[layer_id] = new_layer\n        self.layer_to_id[new_layer] = layer_id\n        self.layer_to_id.pop(old_layer)",
  "def topological_order(self):\n        \"\"\"Return the topological order of the node IDs from the input node to the output node.\"\"\"\n        q = Queue()\n        in_degree = {}\n        for i in range(self.n_nodes):\n            in_degree[i] = 0\n        for u in range(self.n_nodes):\n            for v, _ in self.adj_list[u]:\n                in_degree[v] += 1\n        for i in range(self.n_nodes):\n            if in_degree[i] == 0:\n                q.put(i)\n\n        order_list = []\n        while not q.empty():\n            u = q.get()\n            order_list.append(u)\n            for v, _ in self.adj_list[u]:\n                in_degree[v] -= 1\n                if in_degree[v] == 0:\n                    q.put(v)\n        return order_list",
  "def _get_pooling_layers(self, start_node_id, end_node_id):\n        \"\"\"Given two node IDs, return all the pooling layers between them.\"\"\"\n        layer_list = []\n        node_list = [start_node_id]\n        assert self._depth_first_search(end_node_id, layer_list, node_list)\n        ret = []\n        for layer_id in layer_list:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Pooling\"):\n                ret.append(layer)\n            elif is_layer(layer, \"Conv\") and layer.stride != 1:\n                ret.append(layer)\n        return ret",
  "def _depth_first_search(self, target_id, layer_id_list, node_list):\n        \"\"\"Search for all the layers and nodes down the path.\n        A recursive function to search all the layers and nodes between the node in the node_list\n            and the node with target_id.\"\"\"\n        assert len(node_list) <= self.n_nodes\n        u = node_list[-1]\n        if u == target_id:\n            return True\n\n        for v, layer_id in self.adj_list[u]:\n            layer_id_list.append(layer_id)\n            node_list.append(v)\n            if self._depth_first_search(target_id, layer_id_list, node_list):\n                return True\n            layer_id_list.pop()\n            node_list.pop()\n\n        return False",
  "def _search(self, u, start_dim, total_dim, n_add):\n        \"\"\"Search the graph for all the layers to be widened caused by an operation.\n        It is an recursive function with duplication check to avoid deadlock.\n        It searches from a starting node u until the corresponding layers has been widened.\n        Args:\n            u: The starting node ID.\n            start_dim: The position to insert the additional dimensions.\n            total_dim: The total number of dimensions the layer has before widening.\n            n_add: The number of dimensions to add.\n        \"\"\"\n        if (u, start_dim, total_dim, n_add) in self.vis:\n            return\n        self.vis[(u, start_dim, total_dim, n_add)] = True\n        for v, layer_id in self.adj_list[u]:\n            layer = self.layer_list[layer_id]\n\n            if is_layer(layer, \"Conv\"):\n                new_layer = wider_next_conv(\n                    layer, start_dim, total_dim, n_add, self.weighted\n                )\n                self._replace_layer(layer_id, new_layer)\n\n            elif is_layer(layer, \"Dense\"):\n                new_layer = wider_next_dense(\n                    layer, start_dim, total_dim, n_add, self.weighted\n                )\n                self._replace_layer(layer_id, new_layer)\n\n            elif is_layer(layer, \"BatchNormalization\"):\n                new_layer = wider_bn(\n                    layer, start_dim, total_dim, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n                self._search(v, start_dim, total_dim, n_add)\n\n            elif is_layer(layer, \"Concatenate\"):\n                if self.layer_id_to_input_node_ids[layer_id][1] == u:\n                    # u is on the right of the concat\n                    # next_start_dim += next_total_dim - total_dim\n                    left_dim = self._upper_layer_width(\n                        self.layer_id_to_input_node_ids[layer_id][0]\n                    )\n                    next_start_dim = start_dim + left_dim\n                    next_total_dim = total_dim + left_dim\n                else:\n                    next_start_dim = start_dim\n                    next_total_dim = total_dim + self._upper_layer_width(\n                        self.layer_id_to_input_node_ids[layer_id][1]\n                    )\n                self._search(v, next_start_dim, next_total_dim, n_add)\n\n            else:\n                self._search(v, start_dim, total_dim, n_add)\n\n        for v, layer_id in self.reverse_adj_list[u]:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Conv\"):\n                new_layer = wider_pre_conv(layer, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n            elif is_layer(layer, \"Dense\"):\n                new_layer = wider_pre_dense(layer, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n            elif is_layer(layer, \"Concatenate\"):\n                continue\n            else:\n                self._search(v, start_dim, total_dim, n_add)",
  "def _upper_layer_width(self, u):\n        for v, layer_id in self.reverse_adj_list[u]:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Conv\") or is_layer(layer, \"Dense\"):\n                return layer_width(layer)\n            elif is_layer(layer, \"Concatenate\"):\n                a = self.layer_id_to_input_node_ids[layer_id][0]\n                b = self.layer_id_to_input_node_ids[layer_id][1]\n                return self._upper_layer_width(a) + self._upper_layer_width(b)\n            else:\n                return self._upper_layer_width(v)\n        return self.node_list[0].shape[-1]",
  "def to_deeper_model(self, target_id, new_layer):\n        \"\"\"Insert a relu-conv-bn block after the target block.\n        Args:\n            target_id: A convolutional layer ID. The new block should be inserted after the block.\n            new_layer: An instance of StubLayer subclasses.\n        \"\"\"\n        self.operation_history.append(\n            (\"to_deeper_model\", target_id, new_layer))\n        input_id = self.layer_id_to_input_node_ids[target_id][0]\n        output_id = self.layer_id_to_output_node_ids[target_id][0]\n        if self.weighted:\n            if is_layer(new_layer, \"Dense\"):\n                init_dense_weight(new_layer)\n            elif is_layer(new_layer, \"Conv\"):\n                init_conv_weight(new_layer)\n            elif is_layer(new_layer, \"BatchNormalization\"):\n                init_bn_weight(new_layer)\n\n        self._insert_new_layers([new_layer], input_id, output_id)",
  "def to_wider_model(self, pre_layer_id, n_add):\n        \"\"\"Widen the last dimension of the output of the pre_layer.\n        Args:\n            pre_layer_id: The ID of a convolutional layer or dense layer.\n            n_add: The number of dimensions to add.\n        \"\"\"\n        self.operation_history.append((\"to_wider_model\", pre_layer_id, n_add))\n        pre_layer = self.layer_list[pre_layer_id]\n        output_id = self.layer_id_to_output_node_ids[pre_layer_id][0]\n        dim = layer_width(pre_layer)\n        self.vis = {}\n        self._search(output_id, dim, dim, n_add)\n        # Update the tensor shapes.\n        for u in self.topological_order:\n            for v, layer_id in self.adj_list[u]:\n                self.node_list[v].shape = self.layer_list[layer_id].output_shape",
  "def _insert_new_layers(self, new_layers, start_node_id, end_node_id):\n        \"\"\"Insert the new_layers after the node with start_node_id.\"\"\"\n        new_node_id = self._add_node(deepcopy(self.node_list[end_node_id]))\n        temp_output_id = new_node_id\n        for layer in new_layers[:-1]:\n            temp_output_id = self.add_layer(layer, temp_output_id)\n\n        self._add_edge(new_layers[-1], temp_output_id, end_node_id)\n        new_layers[-1].input = self.node_list[temp_output_id]\n        new_layers[-1].output = self.node_list[end_node_id]\n        self._redirect_edge(start_node_id, end_node_id, new_node_id)",
  "def _block_end_node(self, layer_id, block_size):\n        ret = self.layer_id_to_output_node_ids[layer_id][0]\n        for _ in range(block_size - 2):\n            ret = self.adj_list[ret][0][0]\n        return ret",
  "def _dense_block_end_node(self, layer_id):\n        return self.layer_id_to_input_node_ids[layer_id][0]",
  "def _conv_block_end_node(self, layer_id):\n        \"\"\"Get the input node ID of the last layer in the block by layer ID.\n            Return the input node ID of the last layer in the convolutional block.\n        Args:\n            layer_id: the convolutional layer ID.\n        \"\"\"\n        return self._block_end_node(layer_id, Constant.CONV_BLOCK_DISTANCE)",
  "def to_add_skip_model(self, start_id, end_id):\n        \"\"\"Add a weighted add skip-connection from after start node to end node.\n        Args:\n            start_id: The convolutional layer ID, after which to start the skip-connection.\n            end_id: The convolutional layer ID, after which to end the skip-connection.\n        \"\"\"\n        self.operation_history.append((\"to_add_skip_model\", start_id, end_id))\n        filters_end = self.layer_list[end_id].output.shape[-1]\n        filters_start = self.layer_list[start_id].output.shape[-1]\n        start_node_id = self.layer_id_to_output_node_ids[start_id][0]\n\n        pre_end_node_id = self.layer_id_to_input_node_ids[end_id][0]\n        end_node_id = self.layer_id_to_output_node_ids[end_id][0]\n\n        skip_output_id = self._insert_pooling_layer_chain(\n            start_node_id, end_node_id)\n\n        # Add the conv layer\n        new_conv_layer = get_conv_class(\n            self.n_dim)(\n                filters_start,\n                filters_end,\n                1)\n        skip_output_id = self.add_layer(new_conv_layer, skip_output_id)\n\n        # Add the add layer.\n        add_input_node_id = self._add_node(\n            deepcopy(self.node_list[end_node_id]))\n        add_layer = StubAdd()\n\n        self._redirect_edge(pre_end_node_id, end_node_id, add_input_node_id)\n        self._add_edge(add_layer, add_input_node_id, end_node_id)\n        self._add_edge(add_layer, skip_output_id, end_node_id)\n        add_layer.input = [\n            self.node_list[add_input_node_id],\n            self.node_list[skip_output_id],\n        ]\n        add_layer.output = self.node_list[end_node_id]\n        self.node_list[end_node_id].shape = add_layer.output_shape\n\n        # Set weights to the additional conv layer.\n        if self.weighted:\n            filter_shape = (1,) * self.n_dim\n            weights = np.zeros((filters_end, filters_start) + filter_shape)\n            bias = np.zeros(filters_end)\n            new_conv_layer.set_weights(\n                (add_noise(weights, np.array([0, 1])), add_noise(\n                    bias, np.array([0, 1])))\n            )",
  "def to_concat_skip_model(self, start_id, end_id):\n        \"\"\"Add a weighted add concatenate connection from after start node to end node.\n        Args:\n            start_id: The convolutional layer ID, after which to start the skip-connection.\n            end_id: The convolutional layer ID, after which to end the skip-connection.\n        \"\"\"\n        self.operation_history.append(\n            (\"to_concat_skip_model\", start_id, end_id))\n        filters_end = self.layer_list[end_id].output.shape[-1]\n        filters_start = self.layer_list[start_id].output.shape[-1]\n        start_node_id = self.layer_id_to_output_node_ids[start_id][0]\n\n        pre_end_node_id = self.layer_id_to_input_node_ids[end_id][0]\n        end_node_id = self.layer_id_to_output_node_ids[end_id][0]\n\n        skip_output_id = self._insert_pooling_layer_chain(\n            start_node_id, end_node_id)\n\n        concat_input_node_id = self._add_node(\n            deepcopy(self.node_list[end_node_id]))\n        self._redirect_edge(pre_end_node_id, end_node_id, concat_input_node_id)\n\n        concat_layer = StubConcatenate()\n        concat_layer.input = [\n            self.node_list[concat_input_node_id],\n            self.node_list[skip_output_id],\n        ]\n        concat_output_node_id = self._add_node(Node(concat_layer.output_shape))\n        self._add_edge(\n            concat_layer,\n            concat_input_node_id,\n            concat_output_node_id)\n        self._add_edge(concat_layer, skip_output_id, concat_output_node_id)\n        concat_layer.output = self.node_list[concat_output_node_id]\n        self.node_list[concat_output_node_id].shape = concat_layer.output_shape\n\n        # Add the concatenate layer.\n        new_conv_layer = get_conv_class(self.n_dim)(\n            filters_start + filters_end, filters_end, 1\n        )\n        self._add_edge(new_conv_layer, concat_output_node_id, end_node_id)\n        new_conv_layer.input = self.node_list[concat_output_node_id]\n        new_conv_layer.output = self.node_list[end_node_id]\n        self.node_list[end_node_id].shape = new_conv_layer.output_shape\n\n        if self.weighted:\n            filter_shape = (1,) * self.n_dim\n            weights = np.zeros((filters_end, filters_end) + filter_shape)\n            for i in range(filters_end):\n                filter_weight = np.zeros((filters_end,) + filter_shape)\n                center_index = (i,) + (0,) * self.n_dim\n                filter_weight[center_index] = 1\n                weights[i, ...] = filter_weight\n            weights = np.concatenate(\n                (weights, np.zeros((filters_end, filters_start) + filter_shape)), axis=1\n            )\n            bias = np.zeros(filters_end)\n            new_conv_layer.set_weights(\n                (add_noise(weights, np.array([0, 1])), add_noise(\n                    bias, np.array([0, 1])))\n            )",
  "def _insert_pooling_layer_chain(self, start_node_id, end_node_id):\n        skip_output_id = start_node_id\n        for layer in self._get_pooling_layers(start_node_id, end_node_id):\n            new_layer = deepcopy(layer)\n            if is_layer(new_layer, \"Conv\"):\n                filters = self.node_list[start_node_id].shape[-1]\n                new_layer = get_conv_class(self.n_dim)(\n                    filters, filters, 1, layer.stride)\n                if self.weighted:\n                    init_conv_weight(new_layer)\n            else:\n                new_layer = deepcopy(layer)\n            skip_output_id = self.add_layer(new_layer, skip_output_id)\n        skip_output_id = self.add_layer(StubReLU(), skip_output_id)\n        return skip_output_id",
  "def extract_descriptor(self):\n        \"\"\"Extract the the description of the Graph as an instance of NetworkDescriptor.\"\"\"\n        main_chain = self.get_main_chain()\n        index_in_main_chain = {}\n        for index, u in enumerate(main_chain):\n            index_in_main_chain[u] = index\n\n        ret = NetworkDescriptor()\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v not in index_in_main_chain:\n                    continue\n                layer = self.layer_list[layer_id]\n                copied_layer = copy(layer)\n                copied_layer.weights = None\n                ret.add_layer(deepcopy(copied_layer))\n\n        for u in index_in_main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v not in index_in_main_chain:\n                    temp_u = u\n                    temp_v = v\n                    temp_layer_id = layer_id\n                    skip_type = None\n                    while not (\n                            temp_v in index_in_main_chain and temp_u in index_in_main_chain):\n                        if is_layer(\n                                self.layer_list[temp_layer_id], \"Concatenate\"):\n                            skip_type = NetworkDescriptor.CONCAT_CONNECT\n                        if is_layer(self.layer_list[temp_layer_id], \"Add\"):\n                            skip_type = NetworkDescriptor.ADD_CONNECT\n                        temp_u = temp_v\n                        temp_v, temp_layer_id = self.adj_list[temp_v][0]\n                    ret.add_skip_connection(\n                        index_in_main_chain[u], index_in_main_chain[temp_u], skip_type\n                    )\n\n                elif index_in_main_chain[v] - index_in_main_chain[u] != 1:\n                    skip_type = None\n                    if is_layer(self.layer_list[layer_id], \"Concatenate\"):\n                        skip_type = NetworkDescriptor.CONCAT_CONNECT\n                    if is_layer(self.layer_list[layer_id], \"Add\"):\n                        skip_type = NetworkDescriptor.ADD_CONNECT\n                    ret.add_skip_connection(\n                        index_in_main_chain[u], index_in_main_chain[v], skip_type\n                    )\n\n        return ret",
  "def clear_weights(self):\n        ''' clear weights of the graph\n        '''\n        self.weighted = False\n        for layer in self.layer_list:\n            layer.weights = None",
  "def produce_torch_model(self):\n        \"\"\"Build a new Torch model based on the current graph.\"\"\"\n        return TorchModel(self)",
  "def produce_keras_model(self):\n        \"\"\"Build a new keras model based on the current graph.\"\"\"\n        return KerasModel(self).model",
  "def produce_onnx_model(self):\n        \"\"\"Build a new ONNX model based on the current graph.\"\"\"\n        return ONNXModel(self)",
  "def parsing_onnx_model(self, onnx_model):\n        '''to do in the future to use the onnx model\n        '''\n        return ONNXModel(onnx_model)",
  "def produce_json_model(self):\n        \"\"\"Build a new Json model based on the current graph.\"\"\"\n        return JSONModel(self).data",
  "def parsing_json_model(cls, json_model):\n        '''build a graph from json\n        '''\n        return json_to_graph(json_model)",
  "def _layer_ids_in_order(self, layer_ids):\n        node_id_to_order_index = {}\n        for index, node_id in enumerate(self.topological_order):\n            node_id_to_order_index[node_id] = index\n        return sorted(\n            layer_ids,\n            key=lambda layer_id: node_id_to_order_index[\n                self.layer_id_to_output_node_ids[layer_id][0]\n            ],\n        )",
  "def _layer_ids_by_type(self, type_str):\n        return list(\n            filter(\n                lambda layer_id: is_layer(self.layer_list[layer_id], type_str),\n                range(self.n_layers),\n            )\n        )",
  "def get_main_chain_layers(self):\n        \"\"\"Return a list of layer IDs in the main chain.\"\"\"\n        main_chain = self.get_main_chain()\n        ret = []\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v in main_chain and u in main_chain:\n                    ret.append(layer_id)\n        return ret",
  "def _conv_layer_ids_in_order(self):\n        return list(\n            filter(\n                lambda layer_id: is_layer(self.layer_list[layer_id], \"Conv\"),\n                self.get_main_chain_layers(),\n            )\n        )",
  "def _dense_layer_ids_in_order(self):\n        return self._layer_ids_in_order(self._layer_ids_by_type(\"Dense\"))",
  "def deep_layer_ids(self):\n        ret = []\n        for layer_id in self.get_main_chain_layers():\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"GlobalAveragePooling\"):\n                break\n            if is_layer(layer, \"Add\") or is_layer(layer, \"Concatenate\"):\n                continue\n            ret.append(layer_id)\n        return ret",
  "def wide_layer_ids(self):\n        return (\n            self._conv_layer_ids_in_order(\n            )[:-1] + self._dense_layer_ids_in_order()[:-1]\n        )",
  "def skip_connection_layer_ids(self):\n        return self.deep_layer_ids()[:-1]",
  "def size(self):\n        return sum(list(map(lambda x: x.size(), self.layer_list)))",
  "def get_main_chain(self):\n        \"\"\"Returns the main chain node ID list.\"\"\"\n        pre_node = {}\n        distance = {}\n        for i in range(self.n_nodes):\n            distance[i] = 0\n            pre_node[i] = i\n        for i in range(self.n_nodes - 1):\n            for u in range(self.n_nodes):\n                for v, _ in self.adj_list[u]:\n                    if distance[u] + 1 > distance[v]:\n                        distance[v] = distance[u] + 1\n                        pre_node[v] = u\n        temp_id = 0\n        for i in range(self.n_nodes):\n            if distance[i] > distance[temp_id]:\n                temp_id = i\n        ret = []\n        for i in range(self.n_nodes + 5):\n            ret.append(temp_id)\n            if pre_node[temp_id] == temp_id:\n                break\n            temp_id = pre_node[temp_id]\n        assert temp_id == pre_node[temp_id]\n        ret.reverse()\n        return ret",
  "def __init__(self, graph):\n        super(TorchModel, self).__init__()\n        self.graph = graph\n        self.layers = []\n        for layer in graph.layer_list:\n            self.layers.append(layer.to_real_layer())\n        if graph.weighted:\n            for index, layer in enumerate(self.layers):\n                set_stub_weight_to_torch(self.graph.layer_list[index], layer)\n        for index, layer in enumerate(self.layers):\n            self.add_module(str(index), layer)",
  "def forward(self, input_tensor):\n        topo_node_list = self.graph.topological_order\n        output_id = topo_node_list[-1]\n        input_id = topo_node_list[0]\n\n        node_list = deepcopy(self.graph.node_list)\n        node_list[input_id] = input_tensor\n\n        for v in topo_node_list:\n            for u, layer_id in self.graph.reverse_adj_list[v]:\n                layer = self.graph.layer_list[layer_id]\n                torch_layer = self.layers[layer_id]\n\n                if isinstance(layer, (StubAdd, StubConcatenate)):\n                    edge_input_tensor = list(\n                        map(\n                            lambda x: node_list[x],\n                            self.graph.layer_id_to_input_node_ids[layer_id],\n                        )\n                    )\n                else:\n                    edge_input_tensor = node_list[u]\n\n                temp_tensor = torch_layer(edge_input_tensor)\n                node_list[v] = temp_tensor\n        return node_list[output_id]",
  "def set_weight_to_graph(self):\n        self.graph.weighted = True\n        for index, layer in enumerate(self.layers):\n            set_torch_weight_to_stub(layer, self.graph.layer_list[index])",
  "def __init__(self, graph):\n        import keras\n\n        self.graph = graph\n        self.layers = []\n        for layer in graph.layer_list:\n            self.layers.append(to_real_keras_layer(layer))\n\n        # Construct the keras graph.\n        # Input\n        topo_node_list = self.graph.topological_order\n        output_id = topo_node_list[-1]\n        input_id = topo_node_list[0]\n        input_tensor = keras.layers.Input(\n            shape=graph.node_list[input_id].shape)\n\n        node_list = deepcopy(self.graph.node_list)\n        node_list[input_id] = input_tensor\n\n        # Output\n        for v in topo_node_list:\n            for u, layer_id in self.graph.reverse_adj_list[v]:\n                layer = self.graph.layer_list[layer_id]\n                keras_layer = self.layers[layer_id]\n\n                if isinstance(layer, (StubAdd, StubConcatenate)):\n                    edge_input_tensor = list(\n                        map(\n                            lambda x: node_list[x],\n                            self.graph.layer_id_to_input_node_ids[layer_id],\n                        )\n                    )\n                else:\n                    edge_input_tensor = node_list[u]\n\n                temp_tensor = keras_layer(edge_input_tensor)\n                node_list[v] = temp_tensor\n\n        output_tensor = node_list[output_id]\n        output_tensor = keras.layers.Activation(\"softmax\", name=\"activation_add\")(\n            output_tensor\n        )\n        self.model = keras.models.Model(\n            inputs=input_tensor, outputs=output_tensor)\n\n        if graph.weighted:\n            for index, layer in enumerate(self.layers):\n                set_stub_weight_to_keras(self.graph.layer_list[index], layer)",
  "def set_weight_to_graph(self):\n        self.graph.weighted = True\n        for index, layer in enumerate(self.layers):\n            set_keras_weight_to_stub(layer, self.graph.layer_list[index])",
  "def __init__(self, graph):\n        pass",
  "def __init__(self, graph):\n        data = dict()\n        node_list = list()\n        layer_list = list()\n        operation_history = list()\n\n        data[\"input_shape\"] = graph.input_shape\n        vis = graph.vis\n        data[\"vis\"] = list(vis.keys()) if vis is not None else None\n        data[\"weighted\"] = graph.weighted\n\n        for item in graph.operation_history:\n            if item[0] == \"to_deeper_model\":\n                operation_history.append(\n                    [\n                        item[0],\n                        item[1],\n                        layer_description_extractor(item[2], graph.node_to_id),\n                    ]\n                )\n            else:\n                operation_history.append(item)\n        data[\"operation_history\"] = operation_history\n        data[\"layer_id_to_input_node_ids\"] = graph.layer_id_to_input_node_ids\n        data[\"layer_id_to_output_node_ids\"] = graph.layer_id_to_output_node_ids\n        data[\"adj_list\"] = graph.adj_list\n        data[\"reverse_adj_list\"] = graph.reverse_adj_list\n\n        for node in graph.node_list:\n            node_id = graph.node_to_id[node]\n            node_information = node.shape\n            node_list.append((node_id, node_information))\n\n        for layer_id, item in enumerate(graph.layer_list):\n            layer = graph.layer_list[layer_id]\n            layer_information = layer_description_extractor(\n                layer, graph.node_to_id)\n            layer_list.append((layer_id, layer_information))\n\n        data[\"node_list\"] = node_list\n        data[\"layer_list\"] = layer_list\n\n        self.data = data",
  "def layer_distance(a, b):\n    \"\"\"The distance between two layers.\"\"\"\n    # pylint: disable=unidiomatic-typecheck\n    if not isinstance(a, type(b)):\n        return 1.0\n    if is_layer(a, \"Conv\"):\n        att_diff = [\n            (a.filters, b.filters),\n            (a.kernel_size, b.kernel_size),\n            (a.stride, b.stride),\n        ]\n        return attribute_difference(att_diff)\n    if is_layer(a, \"Pooling\"):\n        att_diff = [\n            (a.padding, b.padding),\n            (a.kernel_size, b.kernel_size),\n            (a.stride, b.stride),\n        ]\n        return attribute_difference(att_diff)\n    return 0.0",
  "def attribute_difference(att_diff):\n    ''' The attribute distance.\n    '''\n\n    ret = 0\n    for a_value, b_value in att_diff:\n        if max(a_value, b_value) == 0:\n            ret += 0\n        else:\n            ret += abs(a_value - b_value) * 1.0 / max(a_value, b_value)\n    return ret * 1.0 / len(att_diff)",
  "def layers_distance(list_a, list_b):\n    \"\"\"The distance between the layers of two neural networks.\"\"\"\n    len_a = len(list_a)\n    len_b = len(list_b)\n    f = np.zeros((len_a + 1, len_b + 1))\n    f[-1][-1] = 0\n    for i in range(-1, len_a):\n        f[i][-1] = i + 1\n    for j in range(-1, len_b):\n        f[-1][j] = j + 1\n    for i in range(len_a):\n        for j in range(len_b):\n            f[i][j] = min(\n                f[i][j - 1] + 1,\n                f[i - 1][j] + 1,\n                f[i - 1][j - 1] + layer_distance(list_a[i], list_b[j]),\n            )\n    return f[len_a - 1][len_b - 1]",
  "def skip_connection_distance(a, b):\n    \"\"\"The distance between two skip-connections.\"\"\"\n    if a[2] != b[2]:\n        return 1.0\n    len_a = abs(a[1] - a[0])\n    len_b = abs(b[1] - b[0])\n    return (abs(a[0] - b[0]) + abs(len_a - len_b)) / \\\n        (max(a[0], b[0]) + max(len_a, len_b))",
  "def skip_connections_distance(list_a, list_b):\n    \"\"\"The distance between the skip-connections of two neural networks.\"\"\"\n    distance_matrix = np.zeros((len(list_a), len(list_b)))\n    for i, a in enumerate(list_a):\n        for j, b in enumerate(list_b):\n            distance_matrix[i][j] = skip_connection_distance(a, b)\n    return distance_matrix[linear_sum_assignment(distance_matrix)].sum() + abs(\n        len(list_a) - len(list_b)\n    )",
  "def edit_distance(x, y):\n    \"\"\"The distance between two neural networks.\n    Args:\n        x: An instance of NetworkDescriptor.\n        y: An instance of NetworkDescriptor\n    Returns:\n        The edit-distance between x and y.\n    \"\"\"\n\n    ret = layers_distance(x.layers, y.layers)\n    ret += Constant.KERNEL_LAMBDA * skip_connections_distance(\n        x.skip_connections, y.skip_connections\n    )\n    return ret",
  "class IncrementalGaussianProcess:\n    \"\"\"Gaussian process regressor.\n    Attributes:\n        alpha: A hyperparameter.\n    \"\"\"\n\n    def __init__(self):\n        self.alpha = 1e-10\n        self._distance_matrix = None\n        self._x = None\n        self._y = None\n        self._first_fitted = False\n        self._l_matrix = None\n        self._alpha_vector = None\n\n    @property\n    def kernel_matrix(self):\n        ''' Kernel matric.\n        '''\n        return self._distance_matrix\n\n    def fit(self, train_x, train_y):\n        \"\"\" Fit the regressor with more data.\n        Args:\n            train_x: A list of NetworkDescriptor.\n            train_y: A list of metric values.\n        \"\"\"\n        if self.first_fitted:\n            self.incremental_fit(train_x, train_y)\n        else:\n            self.first_fit(train_x, train_y)\n\n    def incremental_fit(self, train_x, train_y):\n        \"\"\" Incrementally fit the regressor. \"\"\"\n        if not self._first_fitted:\n            raise ValueError(\n                \"The first_fit function needs to be called first.\")\n\n        train_x, train_y = np.array(train_x), np.array(train_y)\n\n        # Incrementally compute K\n        up_right_k = edit_distance_matrix(self._x, train_x)\n        down_left_k = np.transpose(up_right_k)\n        down_right_k = edit_distance_matrix(train_x)\n        up_k = np.concatenate((self._distance_matrix, up_right_k), axis=1)\n        down_k = np.concatenate((down_left_k, down_right_k), axis=1)\n        temp_distance_matrix = np.concatenate((up_k, down_k), axis=0)\n        k_matrix = bourgain_embedding_matrix(temp_distance_matrix)\n        diagonal = np.diag_indices_from(k_matrix)\n        diagonal = (diagonal[0][-len(train_x):], diagonal[1][-len(train_x):])\n        k_matrix[diagonal] += self.alpha\n\n        try:\n            self._l_matrix = cholesky(k_matrix, lower=True)  # Line 2\n        except LinAlgError:\n            return self\n\n        self._x = np.concatenate((self._x, train_x), axis=0)\n        self._y = np.concatenate((self._y, train_y), axis=0)\n        self._distance_matrix = temp_distance_matrix\n\n        self._alpha_vector = cho_solve(\n            (self._l_matrix, True), self._y)  # Line 3\n\n        return self\n\n    @property\n    def first_fitted(self):\n        ''' if it is firsr fitted\n        '''\n        return self._first_fitted\n\n    def first_fit(self, train_x, train_y):\n        \"\"\" Fit the regressor for the first time. \"\"\"\n        train_x, train_y = np.array(train_x), np.array(train_y)\n\n        self._x = np.copy(train_x)\n        self._y = np.copy(train_y)\n\n        self._distance_matrix = edit_distance_matrix(self._x)\n        k_matrix = bourgain_embedding_matrix(self._distance_matrix)\n        k_matrix[np.diag_indices_from(k_matrix)] += self.alpha\n\n        self._l_matrix = cholesky(k_matrix, lower=True)  # Line 2\n\n        self._alpha_vector = cho_solve(\n            (self._l_matrix, True), self._y)  # Line 3\n\n        self._first_fitted = True\n        return self\n\n    def predict(self, train_x):\n        \"\"\"Predict the result.\n        Args:\n            train_x: A list of NetworkDescriptor.\n        Returns:\n            y_mean: The predicted mean.\n            y_std: The predicted standard deviation.\n        \"\"\"\n        k_trans = np.exp(-np.power(edit_distance_matrix(train_x, self._x), 2))\n        y_mean = k_trans.dot(self._alpha_vector)  # Line 4 (y_mean = f_star)\n\n        # compute inverse K_inv of K based on its Cholesky\n        # decomposition L and its inverse L_inv\n        l_inv = solve_triangular(\n            self._l_matrix.T, np.eye(\n                self._l_matrix.shape[0]))\n        k_inv = l_inv.dot(l_inv.T)\n        # Compute variance of predictive distribution\n        y_var = np.ones(len(train_x), dtype=np.float)\n        y_var -= np.einsum(\"ij,ij->i\", np.dot(k_trans, k_inv), k_trans)\n\n        # Check if any of the variances is negative because of\n        # numerical issues. If yes: set the variance to 0.\n        y_var_negative = y_var < 0\n        if np.any(y_var_negative):\n            y_var[y_var_negative] = 0.0\n        return y_mean, np.sqrt(y_var)",
  "def edit_distance_matrix(train_x, train_y=None):\n    \"\"\"Calculate the edit distance.\n    Args:\n        train_x: A list of neural architectures.\n        train_y: A list of neural architectures.\n    Returns:\n        An edit-distance matrix.\n    \"\"\"\n    if train_y is None:\n        ret = np.zeros((train_x.shape[0], train_x.shape[0]))\n        for x_index, x in enumerate(train_x):\n            for y_index, y in enumerate(train_x):\n                if x_index == y_index:\n                    ret[x_index][y_index] = 0\n                elif x_index < y_index:\n                    ret[x_index][y_index] = edit_distance(x, y)\n                else:\n                    ret[x_index][y_index] = ret[y_index][x_index]\n        return ret\n    ret = np.zeros((train_x.shape[0], train_y.shape[0]))\n    for x_index, x in enumerate(train_x):\n        for y_index, y in enumerate(train_y):\n            ret[x_index][y_index] = edit_distance(x, y)\n    return ret",
  "def vector_distance(a, b):\n    \"\"\"The Euclidean distance between two vectors.\"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    return np.linalg.norm(a - b)",
  "def bourgain_embedding_matrix(distance_matrix):\n    \"\"\"Use Bourgain algorithm to embed the neural architectures based on their edit-distance.\n    Args:\n        distance_matrix: A matrix of edit-distances.\n    Returns:\n        A matrix of distances after embedding.\n    \"\"\"\n    distance_matrix = np.array(distance_matrix)\n    n = len(distance_matrix)\n    if n == 1:\n        return distance_matrix\n    np.random.seed(123)\n    distort_elements = []\n    r = range(n)\n    k = int(math.ceil(math.log(n) / math.log(2) - 1))\n    t = int(math.ceil(math.log(n)))\n    counter = 0\n    for i in range(0, k + 1):\n        for t in range(t):\n            s = np.random.choice(r, 2 ** i)\n            for j in r:\n                d = min([distance_matrix[j][s] for s in s])\n                counter += len(s)\n                if i == 0 and t == 0:\n                    distort_elements.append([d])\n                else:\n                    distort_elements[j].append(d)\n    return rbf_kernel(distort_elements, distort_elements)",
  "class BayesianOptimizer:\n    \"\"\" A Bayesian optimizer for neural architectures.\n    Attributes:\n        searcher: The Searcher which is calling the Bayesian optimizer.\n        t_min: The minimum temperature for simulated annealing.\n        metric: An instance of the Metric subclasses.\n        gpr: A GaussianProcessRegressor for bayesian optimization.\n        beta: The beta in acquisition function. (refer to our paper)\n        search_tree: The network morphism search tree.\n    \"\"\"\n\n    def __init__(self, searcher, t_min, optimizemode, beta=None):\n        self.searcher = searcher\n        self.t_min = t_min\n        self.optimizemode = optimizemode\n        self.gpr = IncrementalGaussianProcess()\n        self.beta = beta if beta is not None else Constant.BETA\n        self.search_tree = SearchTree()\n\n    def fit(self, x_queue, y_queue):\n        \"\"\" Fit the optimizer with new architectures and performances.\n        Args:\n            x_queue: A list of NetworkDescriptor.\n            y_queue: A list of metric values.\n        \"\"\"\n        self.gpr.fit(x_queue, y_queue)\n\n    def generate(self, descriptors):\n        \"\"\"Generate new architecture.\n        Args:\n            descriptors: All the searched neural architectures.\n        Returns:\n            graph: An instance of Graph. A morphed neural network with weights.\n            father_id: The father node ID in the search tree.\n        \"\"\"\n        model_ids = self.search_tree.adj_list.keys()\n\n        target_graph = None\n        father_id = None\n        descriptors = deepcopy(descriptors)\n        elem_class = Elem\n        if self.optimizemode is OptimizeMode.Maximize:\n            elem_class = ReverseElem\n\n        # Initialize the priority queue.\n        pq = PriorityQueue()\n        temp_list = []\n        for model_id in model_ids:\n            metric_value = self.searcher.get_metric_value_by_id(model_id)\n            temp_list.append((metric_value, model_id))\n        temp_list = sorted(temp_list)\n        for metric_value, model_id in temp_list:\n            graph = self.searcher.load_model_by_id(model_id)\n            graph.clear_operation_history()\n            graph.clear_weights()\n            pq.put(elem_class(metric_value, model_id, graph))\n\n        t = 1.0\n        t_min = self.t_min\n        alpha = 0.9\n        opt_acq = self._get_init_opt_acq_value()\n        while not pq.empty() and t > t_min:\n            elem = pq.get()\n            if self.optimizemode is OptimizeMode.Maximize:\n                temp_exp = min((elem.metric_value - opt_acq) / t, 1.0)\n            else:\n                temp_exp = min((opt_acq - elem.metric_value) / t, 1.0)\n            ap = math.exp(temp_exp)\n            if ap >= random.uniform(0, 1):\n                for temp_graph in transform(elem.graph):\n                    if contain(descriptors, temp_graph.extract_descriptor()):\n                        continue\n\n                    temp_acq_value = self.acq(temp_graph)\n                    pq.put(\n                        elem_class(\n                            temp_acq_value,\n                            elem.father_id,\n                            temp_graph))\n                    descriptors.append(temp_graph.extract_descriptor())\n                    if self._accept_new_acq_value(opt_acq, temp_acq_value):\n                        opt_acq = temp_acq_value\n                        father_id = elem.father_id\n                        target_graph = deepcopy(temp_graph)\n            t *= alpha\n\n        # Did not found a not duplicated architecture\n        if father_id is None:\n            return None, None\n        nm_graph = self.searcher.load_model_by_id(father_id)\n        for args in target_graph.operation_history:\n            getattr(nm_graph, args[0])(*list(args[1:]))\n        return nm_graph, father_id\n\n    def acq(self, graph):\n        ''' estimate the value of generated graph\n        '''\n        mean, std = self.gpr.predict(np.array([graph.extract_descriptor()]))\n        if self.optimizemode is OptimizeMode.Maximize:\n            return mean + self.beta * std\n        return mean - self.beta * std\n\n    def _get_init_opt_acq_value(self):\n        if self.optimizemode is OptimizeMode.Maximize:\n            return -np.inf\n        return np.inf\n\n    def _accept_new_acq_value(self, opt_acq, temp_acq_value):\n        if temp_acq_value > opt_acq and self.optimizemode is OptimizeMode.Maximize:\n            return True\n        if temp_acq_value < opt_acq and not self.optimizemode is OptimizeMode.Maximize:\n            return True\n        return False\n\n    def add_child(self, father_id, model_id):\n        ''' add child to the search tree\n        Arguments:\n            father_id {int} -- father id\n            model_id {int} -- model id\n        '''\n\n        self.search_tree.add_child(father_id, model_id)",
  "class Elem:\n    \"\"\"Elements to be sorted according to metric value.\"\"\"\n\n    def __init__(self, metric_value, father_id, graph):\n        self.father_id = father_id\n        self.graph = graph\n        self.metric_value = metric_value\n\n    def __eq__(self, other):\n        return self.metric_value == other.metric_value\n\n    def __lt__(self, other):\n        return self.metric_value < other.metric_value",
  "class ReverseElem(Elem):\n    \"\"\"Elements to be reversely sorted according to metric value.\"\"\"\n\n    def __lt__(self, other):\n        return self.metric_value > other.metric_value",
  "def contain(descriptors, target_descriptor):\n    \"\"\"Check if the target descriptor is in the descriptors.\"\"\"\n    for descriptor in descriptors:\n        if edit_distance(descriptor, target_descriptor) < 1e-5:\n            return True\n    return False",
  "class SearchTree:\n    \"\"\"The network morphism search tree.\"\"\"\n\n    def __init__(self):\n        self.root = None\n        self.adj_list = {}\n\n    def add_child(self, u, v):\n        ''' add child to search tree itself.\n        Arguments:\n            u {int} -- father id\n            v {int} --  child id\n        '''\n\n        if u == -1:\n            self.root = v\n            self.adj_list[v] = []\n            return\n        if v not in self.adj_list[u]:\n            self.adj_list[u].append(v)\n        if v not in self.adj_list:\n            self.adj_list[v] = []\n\n    def get_dict(self, u=None):\n        \"\"\" A recursive function to return the content of the tree in a dict.\"\"\"\n        if u is None:\n            return self.get_dict(self.root)\n        children = []\n        for v in self.adj_list[u]:\n            children.append(self.get_dict(v))\n        ret = {\"name\": u, \"children\": children}\n        return ret",
  "def __init__(self):\n        self.alpha = 1e-10\n        self._distance_matrix = None\n        self._x = None\n        self._y = None\n        self._first_fitted = False\n        self._l_matrix = None\n        self._alpha_vector = None",
  "def kernel_matrix(self):\n        ''' Kernel matric.\n        '''\n        return self._distance_matrix",
  "def fit(self, train_x, train_y):\n        \"\"\" Fit the regressor with more data.\n        Args:\n            train_x: A list of NetworkDescriptor.\n            train_y: A list of metric values.\n        \"\"\"\n        if self.first_fitted:\n            self.incremental_fit(train_x, train_y)\n        else:\n            self.first_fit(train_x, train_y)",
  "def incremental_fit(self, train_x, train_y):\n        \"\"\" Incrementally fit the regressor. \"\"\"\n        if not self._first_fitted:\n            raise ValueError(\n                \"The first_fit function needs to be called first.\")\n\n        train_x, train_y = np.array(train_x), np.array(train_y)\n\n        # Incrementally compute K\n        up_right_k = edit_distance_matrix(self._x, train_x)\n        down_left_k = np.transpose(up_right_k)\n        down_right_k = edit_distance_matrix(train_x)\n        up_k = np.concatenate((self._distance_matrix, up_right_k), axis=1)\n        down_k = np.concatenate((down_left_k, down_right_k), axis=1)\n        temp_distance_matrix = np.concatenate((up_k, down_k), axis=0)\n        k_matrix = bourgain_embedding_matrix(temp_distance_matrix)\n        diagonal = np.diag_indices_from(k_matrix)\n        diagonal = (diagonal[0][-len(train_x):], diagonal[1][-len(train_x):])\n        k_matrix[diagonal] += self.alpha\n\n        try:\n            self._l_matrix = cholesky(k_matrix, lower=True)  # Line 2\n        except LinAlgError:\n            return self\n\n        self._x = np.concatenate((self._x, train_x), axis=0)\n        self._y = np.concatenate((self._y, train_y), axis=0)\n        self._distance_matrix = temp_distance_matrix\n\n        self._alpha_vector = cho_solve(\n            (self._l_matrix, True), self._y)  # Line 3\n\n        return self",
  "def first_fitted(self):\n        ''' if it is firsr fitted\n        '''\n        return self._first_fitted",
  "def first_fit(self, train_x, train_y):\n        \"\"\" Fit the regressor for the first time. \"\"\"\n        train_x, train_y = np.array(train_x), np.array(train_y)\n\n        self._x = np.copy(train_x)\n        self._y = np.copy(train_y)\n\n        self._distance_matrix = edit_distance_matrix(self._x)\n        k_matrix = bourgain_embedding_matrix(self._distance_matrix)\n        k_matrix[np.diag_indices_from(k_matrix)] += self.alpha\n\n        self._l_matrix = cholesky(k_matrix, lower=True)  # Line 2\n\n        self._alpha_vector = cho_solve(\n            (self._l_matrix, True), self._y)  # Line 3\n\n        self._first_fitted = True\n        return self",
  "def predict(self, train_x):\n        \"\"\"Predict the result.\n        Args:\n            train_x: A list of NetworkDescriptor.\n        Returns:\n            y_mean: The predicted mean.\n            y_std: The predicted standard deviation.\n        \"\"\"\n        k_trans = np.exp(-np.power(edit_distance_matrix(train_x, self._x), 2))\n        y_mean = k_trans.dot(self._alpha_vector)  # Line 4 (y_mean = f_star)\n\n        # compute inverse K_inv of K based on its Cholesky\n        # decomposition L and its inverse L_inv\n        l_inv = solve_triangular(\n            self._l_matrix.T, np.eye(\n                self._l_matrix.shape[0]))\n        k_inv = l_inv.dot(l_inv.T)\n        # Compute variance of predictive distribution\n        y_var = np.ones(len(train_x), dtype=np.float)\n        y_var -= np.einsum(\"ij,ij->i\", np.dot(k_trans, k_inv), k_trans)\n\n        # Check if any of the variances is negative because of\n        # numerical issues. If yes: set the variance to 0.\n        y_var_negative = y_var < 0\n        if np.any(y_var_negative):\n            y_var[y_var_negative] = 0.0\n        return y_mean, np.sqrt(y_var)",
  "def __init__(self, searcher, t_min, optimizemode, beta=None):\n        self.searcher = searcher\n        self.t_min = t_min\n        self.optimizemode = optimizemode\n        self.gpr = IncrementalGaussianProcess()\n        self.beta = beta if beta is not None else Constant.BETA\n        self.search_tree = SearchTree()",
  "def fit(self, x_queue, y_queue):\n        \"\"\" Fit the optimizer with new architectures and performances.\n        Args:\n            x_queue: A list of NetworkDescriptor.\n            y_queue: A list of metric values.\n        \"\"\"\n        self.gpr.fit(x_queue, y_queue)",
  "def generate(self, descriptors):\n        \"\"\"Generate new architecture.\n        Args:\n            descriptors: All the searched neural architectures.\n        Returns:\n            graph: An instance of Graph. A morphed neural network with weights.\n            father_id: The father node ID in the search tree.\n        \"\"\"\n        model_ids = self.search_tree.adj_list.keys()\n\n        target_graph = None\n        father_id = None\n        descriptors = deepcopy(descriptors)\n        elem_class = Elem\n        if self.optimizemode is OptimizeMode.Maximize:\n            elem_class = ReverseElem\n\n        # Initialize the priority queue.\n        pq = PriorityQueue()\n        temp_list = []\n        for model_id in model_ids:\n            metric_value = self.searcher.get_metric_value_by_id(model_id)\n            temp_list.append((metric_value, model_id))\n        temp_list = sorted(temp_list)\n        for metric_value, model_id in temp_list:\n            graph = self.searcher.load_model_by_id(model_id)\n            graph.clear_operation_history()\n            graph.clear_weights()\n            pq.put(elem_class(metric_value, model_id, graph))\n\n        t = 1.0\n        t_min = self.t_min\n        alpha = 0.9\n        opt_acq = self._get_init_opt_acq_value()\n        while not pq.empty() and t > t_min:\n            elem = pq.get()\n            if self.optimizemode is OptimizeMode.Maximize:\n                temp_exp = min((elem.metric_value - opt_acq) / t, 1.0)\n            else:\n                temp_exp = min((opt_acq - elem.metric_value) / t, 1.0)\n            ap = math.exp(temp_exp)\n            if ap >= random.uniform(0, 1):\n                for temp_graph in transform(elem.graph):\n                    if contain(descriptors, temp_graph.extract_descriptor()):\n                        continue\n\n                    temp_acq_value = self.acq(temp_graph)\n                    pq.put(\n                        elem_class(\n                            temp_acq_value,\n                            elem.father_id,\n                            temp_graph))\n                    descriptors.append(temp_graph.extract_descriptor())\n                    if self._accept_new_acq_value(opt_acq, temp_acq_value):\n                        opt_acq = temp_acq_value\n                        father_id = elem.father_id\n                        target_graph = deepcopy(temp_graph)\n            t *= alpha\n\n        # Did not found a not duplicated architecture\n        if father_id is None:\n            return None, None\n        nm_graph = self.searcher.load_model_by_id(father_id)\n        for args in target_graph.operation_history:\n            getattr(nm_graph, args[0])(*list(args[1:]))\n        return nm_graph, father_id",
  "def acq(self, graph):\n        ''' estimate the value of generated graph\n        '''\n        mean, std = self.gpr.predict(np.array([graph.extract_descriptor()]))\n        if self.optimizemode is OptimizeMode.Maximize:\n            return mean + self.beta * std\n        return mean - self.beta * std",
  "def _get_init_opt_acq_value(self):\n        if self.optimizemode is OptimizeMode.Maximize:\n            return -np.inf\n        return np.inf",
  "def _accept_new_acq_value(self, opt_acq, temp_acq_value):\n        if temp_acq_value > opt_acq and self.optimizemode is OptimizeMode.Maximize:\n            return True\n        if temp_acq_value < opt_acq and not self.optimizemode is OptimizeMode.Maximize:\n            return True\n        return False",
  "def add_child(self, father_id, model_id):\n        ''' add child to the search tree\n        Arguments:\n            father_id {int} -- father id\n            model_id {int} -- model id\n        '''\n\n        self.search_tree.add_child(father_id, model_id)",
  "def __init__(self, metric_value, father_id, graph):\n        self.father_id = father_id\n        self.graph = graph\n        self.metric_value = metric_value",
  "def __eq__(self, other):\n        return self.metric_value == other.metric_value",
  "def __lt__(self, other):\n        return self.metric_value < other.metric_value",
  "def __lt__(self, other):\n        return self.metric_value > other.metric_value",
  "def __init__(self):\n        self.root = None\n        self.adj_list = {}",
  "def add_child(self, u, v):\n        ''' add child to search tree itself.\n        Arguments:\n            u {int} -- father id\n            v {int} --  child id\n        '''\n\n        if u == -1:\n            self.root = v\n            self.adj_list[v] = []\n            return\n        if v not in self.adj_list[u]:\n            self.adj_list[u].append(v)\n        if v not in self.adj_list:\n            self.adj_list[v] = []",
  "def get_dict(self, u=None):\n        \"\"\" A recursive function to return the content of the tree in a dict.\"\"\"\n        if u is None:\n            return self.get_dict(self.root)\n        children = []\n        for v in self.adj_list[u]:\n            children.append(self.get_dict(v))\n        ret = {\"name\": u, \"children\": children}\n        return ret",
  "def to_wider_graph(graph):\n    ''' wider graph\n    '''\n    weighted_layer_ids = graph.wide_layer_ids()\n    weighted_layer_ids = list(\n        filter(\n            lambda x: graph.layer_list[x].output.shape[-1], weighted_layer_ids)\n    )\n    wider_layers = sample(weighted_layer_ids, 1)\n\n    for layer_id in wider_layers:\n        layer = graph.layer_list[layer_id]\n        if is_layer(layer, \"Conv\"):\n            n_add = layer.filters\n        else:\n            n_add = layer.units\n\n        graph.to_wider_model(layer_id, n_add)\n    return graph",
  "def to_skip_connection_graph(graph):\n    ''' skip connection graph\n    '''\n    # The last conv layer cannot be widen since wider operator cannot be done\n    # over the two sides of flatten.\n    weighted_layer_ids = graph.skip_connection_layer_ids()\n    valid_connection = []\n    for skip_type in sorted(\n            [NetworkDescriptor.ADD_CONNECT, NetworkDescriptor.CONCAT_CONNECT]):\n        for index_a in range(len(weighted_layer_ids)):\n            for index_b in range(len(weighted_layer_ids))[index_a + 1:]:\n                valid_connection.append((index_a, index_b, skip_type))\n\n    if not valid_connection:\n        return graph\n    for index_a, index_b, skip_type in sample(valid_connection, 1):\n        a_id = weighted_layer_ids[index_a]\n        b_id = weighted_layer_ids[index_b]\n        if skip_type == NetworkDescriptor.ADD_CONNECT:\n            graph.to_add_skip_model(a_id, b_id)\n        else:\n            graph.to_concat_skip_model(a_id, b_id)\n    return graph",
  "def create_new_layer(layer, n_dim):\n    ''' create  new layer for the graph\n    '''\n\n    input_shape = layer.output.shape\n    dense_deeper_classes = [StubDense, get_dropout_class(n_dim), StubReLU]\n    conv_deeper_classes = [\n        get_conv_class(n_dim),\n        get_batch_norm_class(n_dim),\n        StubReLU]\n    if is_layer(layer, \"ReLU\"):\n        conv_deeper_classes = [\n            get_conv_class(n_dim),\n            get_batch_norm_class(n_dim)]\n        dense_deeper_classes = [StubDense, get_dropout_class(n_dim)]\n    elif is_layer(layer, \"Dropout\"):\n        dense_deeper_classes = [StubDense, StubReLU]\n    elif is_layer(layer, \"BatchNormalization\"):\n        conv_deeper_classes = [get_conv_class(n_dim), StubReLU]\n\n    layer_class = None\n    if len(input_shape) == 1:\n        # It is in the dense layer part.\n        layer_class = sample(dense_deeper_classes, 1)[0]\n    else:\n        # It is in the conv layer part.\n        layer_class = sample(conv_deeper_classes, 1)[0]\n\n    if layer_class == StubDense:\n        new_layer = StubDense(input_shape[0], input_shape[0])\n\n    elif layer_class == get_dropout_class(n_dim):\n        new_layer = layer_class(Constant.DENSE_DROPOUT_RATE)\n\n    elif layer_class == get_conv_class(n_dim):\n        new_layer = layer_class(\n            input_shape[-1], input_shape[-1], sample((1, 3, 5), 1)[0], stride=1\n        )\n\n    elif layer_class == get_batch_norm_class(n_dim):\n        new_layer = layer_class(input_shape[-1])\n\n    elif layer_class == get_pooling_class(n_dim):\n        new_layer = layer_class(sample((1, 3, 5), 1)[0])\n\n    else:\n        new_layer = layer_class()\n\n    return new_layer",
  "def to_deeper_graph(graph):\n    ''' deeper graph\n    '''\n\n    weighted_layer_ids = graph.deep_layer_ids()\n    if len(weighted_layer_ids) >= Constant.MAX_LAYERS:\n        return None\n\n    deeper_layer_ids = sample(weighted_layer_ids, 1)\n\n    for layer_id in deeper_layer_ids:\n        layer = graph.layer_list[layer_id]\n        new_layer = create_new_layer(layer, graph.n_dim)\n        graph.to_deeper_model(layer_id, new_layer)\n    return graph",
  "def legal_graph(graph):\n    '''judge if a graph is legal or not.\n    '''\n\n    descriptor = graph.extract_descriptor()\n    skips = descriptor.skip_connections\n    if len(skips) != len(set(skips)):\n        return False\n    return True",
  "def transform(graph):\n    '''core transform function for graph.\n    '''\n\n    graphs = []\n    for _ in range(Constant.N_NEIGHBOURS * 2):\n        random_num = randrange(3)\n        temp_graph = None\n        if random_num == 0:\n            temp_graph = to_deeper_graph(deepcopy(graph))\n        elif random_num == 1:\n            temp_graph = to_wider_graph(deepcopy(graph))\n        elif random_num == 2:\n            temp_graph = to_skip_connection_graph(deepcopy(graph))\n\n        if temp_graph is not None and temp_graph.size() <= Constant.MAX_MODEL_SIZE:\n            graphs.append(temp_graph)\n\n        if len(graphs) >= Constant.N_NEIGHBOURS:\n            break\n\n    return graphs",
  "def deeper_conv_block(conv_layer, kernel_size, weighted=True):\n    '''deeper conv layer.\n    '''\n    n_dim = get_n_dim(conv_layer)\n    filter_shape = (kernel_size,) * 2\n    n_filters = conv_layer.filters\n    weight = np.zeros((n_filters, n_filters) + filter_shape)\n    center = tuple(map(lambda x: int((x - 1) / 2), filter_shape))\n    for i in range(n_filters):\n        filter_weight = np.zeros((n_filters,) + filter_shape)\n        index = (i,) + center\n        filter_weight[index] = 1\n        weight[i, ...] = filter_weight\n    bias = np.zeros(n_filters)\n    new_conv_layer = get_conv_class(n_dim)(\n        conv_layer.filters, n_filters, kernel_size=kernel_size\n    )\n    bn = get_batch_norm_class(n_dim)(n_filters)\n\n    if weighted:\n        new_conv_layer.set_weights(\n            (add_noise(weight, np.array([0, 1])),\n             add_noise(bias, np.array([0, 1])))\n        )\n        new_weights = [\n            add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n        ]\n        bn.set_weights(new_weights)\n\n    return [StubReLU(), new_conv_layer, bn]",
  "def dense_to_deeper_block(dense_layer, weighted=True):\n    '''deeper dense layer.\n    '''\n    units = dense_layer.units\n    weight = np.eye(units)\n    bias = np.zeros(units)\n    new_dense_layer = StubDense(units, units)\n    if weighted:\n        new_dense_layer.set_weights(\n            (add_noise(weight, np.array([0, 1])),\n             add_noise(bias, np.array([0, 1])))\n        )\n    return [StubReLU(), new_dense_layer]",
  "def wider_pre_dense(layer, n_add, weighted=True):\n    '''wider previous dense layer.\n    '''\n    if not weighted:\n        return StubDense(layer.input_units, layer.units + n_add)\n\n    n_units2 = layer.units\n\n    teacher_w, teacher_b = layer.get_weights()\n    rand = np.random.randint(n_units2, size=n_add)\n    student_w = teacher_w.copy()\n    student_b = teacher_b.copy()\n\n    # target layer update (i)\n    for i in range(n_add):\n        teacher_index = rand[i]\n        new_weight = teacher_w[teacher_index, :]\n        new_weight = new_weight[np.newaxis, :]\n        student_w = np.concatenate(\n            (student_w, add_noise(new_weight, student_w)), axis=0)\n        student_b = np.append(\n            student_b, add_noise(\n                teacher_b[teacher_index], student_b))\n\n    new_pre_layer = StubDense(layer.input_units, n_units2 + n_add)\n    new_pre_layer.set_weights((student_w, student_b))\n\n    return new_pre_layer",
  "def wider_pre_conv(layer, n_add_filters, weighted=True):\n    '''wider previous conv layer.\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_conv_class(n_dim)(\n            layer.input_channel,\n            layer.filters + n_add_filters,\n            kernel_size=layer.kernel_size,\n        )\n\n    n_pre_filters = layer.filters\n    rand = np.random.randint(n_pre_filters, size=n_add_filters)\n    teacher_w, teacher_b = layer.get_weights()\n\n    student_w = teacher_w.copy()\n    student_b = teacher_b.copy()\n    # target layer update (i)\n    for i, _ in enumerate(rand):\n        teacher_index = rand[i]\n        new_weight = teacher_w[teacher_index, ...]\n        new_weight = new_weight[np.newaxis, ...]\n        student_w = np.concatenate((student_w, new_weight), axis=0)\n        student_b = np.append(student_b, teacher_b[teacher_index])\n    new_pre_layer = get_conv_class(n_dim)(\n        layer.input_channel, n_pre_filters + n_add_filters, layer.kernel_size\n    )\n    new_pre_layer.set_weights(\n        (add_noise(student_w, teacher_w), add_noise(student_b, teacher_b))\n    )\n    return new_pre_layer",
  "def wider_next_conv(layer, start_dim, total_dim, n_add, weighted=True):\n    '''wider next conv layer.\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_conv_class(n_dim)(layer.input_channel + n_add,\n                                     layer.filters,\n                                     kernel_size=layer.kernel_size,\n                                     stride=layer.stride)\n    n_filters = layer.filters\n    teacher_w, teacher_b = layer.get_weights()\n\n    new_weight_shape = list(teacher_w.shape)\n    new_weight_shape[1] = n_add\n    new_weight = np.zeros(tuple(new_weight_shape))\n\n    student_w = np.concatenate((teacher_w[:, :start_dim, ...].copy(),\n                                add_noise(new_weight, teacher_w),\n                                teacher_w[:, start_dim:total_dim, ...].copy()), axis=1)\n    new_layer = get_conv_class(n_dim)(layer.input_channel + n_add,\n                                      n_filters,\n                                      kernel_size=layer.kernel_size,\n                                      stride=layer.stride)\n    new_layer.set_weights((student_w, teacher_b))\n    return new_layer",
  "def wider_bn(layer, start_dim, total_dim, n_add, weighted=True):\n    '''wider batch norm layer.\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_batch_norm_class(n_dim)(layer.num_features + n_add)\n\n    weights = layer.get_weights()\n\n    new_weights = [\n        add_noise(np.ones(n_add, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_add, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_add, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.ones(n_add, dtype=np.float32), np.array([0, 1])),\n    ]\n\n    student_w = tuple()\n    for weight, new_weight in zip(weights, new_weights):\n        temp_w = weight.copy()\n        temp_w = np.concatenate(\n            (temp_w[:start_dim], new_weight, temp_w[start_dim:total_dim])\n        )\n        student_w += (temp_w,)\n    new_layer = get_batch_norm_class(n_dim)(layer.num_features + n_add)\n    new_layer.set_weights(student_w)\n    return new_layer",
  "def wider_next_dense(layer, start_dim, total_dim, n_add, weighted=True):\n    '''wider next dense layer.\n    '''\n    if not weighted:\n        return StubDense(layer.input_units + n_add, layer.units)\n    teacher_w, teacher_b = layer.get_weights()\n    student_w = teacher_w.copy()\n    n_units_each_channel = int(teacher_w.shape[1] / total_dim)\n\n    new_weight = np.zeros((teacher_w.shape[0], n_add * n_units_each_channel))\n    student_w = np.concatenate(\n        (\n            student_w[:, : start_dim * n_units_each_channel],\n            add_noise(new_weight, student_w),\n            student_w[\n                :, start_dim * n_units_each_channel: total_dim * n_units_each_channel\n            ],\n        ),\n        axis=1,\n    )\n\n    new_layer = StubDense(layer.input_units + n_add, layer.units)\n    new_layer.set_weights((student_w, teacher_b))\n    return new_layer",
  "def add_noise(weights, other_weights):\n    '''add noise to the layer.\n    '''\n    w_range = np.ptp(other_weights.flatten())\n    noise_range = NOISE_RATIO * w_range\n    noise = np.random.uniform(-noise_range / 2.0,\n                              noise_range / 2.0, weights.shape)\n    return np.add(noise, weights)",
  "def init_dense_weight(layer):\n    '''initilize dense layer weight.\n    '''\n    units = layer.units\n    weight = np.eye(units)\n    bias = np.zeros(units)\n    layer.set_weights(\n        (add_noise(weight, np.array([0, 1])),\n         add_noise(bias, np.array([0, 1])))\n    )",
  "def init_conv_weight(layer):\n    '''initilize conv layer weight.\n    '''\n    n_filters = layer.filters\n    filter_shape = (layer.kernel_size,) * get_n_dim(layer)\n    weight = np.zeros((n_filters, n_filters) + filter_shape)\n\n    center = tuple(map(lambda x: int((x - 1) / 2), filter_shape))\n    for i in range(n_filters):\n        filter_weight = np.zeros((n_filters,) + filter_shape)\n        index = (i,) + center\n        filter_weight[index] = 1\n        weight[i, ...] = filter_weight\n    bias = np.zeros(n_filters)\n\n    layer.set_weights(\n        (add_noise(weight, np.array([0, 1])),\n         add_noise(bias, np.array([0, 1])))\n    )",
  "def init_bn_weight(layer):\n    '''initilize batch norm layer weight.\n    '''\n    n_filters = layer.num_features\n    new_weights = [\n        add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n    ]\n    layer.set_weights(new_weights)",
  "class NetworkMorphismClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('task'): self.choices('task', 'cv', 'nlp', 'common'),\n            Optional('input_width'): int,\n            Optional('input_channel'): int,\n            Optional('n_output_node'): int\n        }).validate(kwargs)",
  "class NetworkMorphismTuner(Tuner):\n    \"\"\"\n    NetworkMorphismTuner is a tuner which using network morphism techniques.\n\n    Attributes\n    ----------\n    n_classes : int\n        The class number or output node number (default: ``10``)\n    input_shape : tuple\n        A tuple including: (input_width, input_width, input_channel)\n    t_min : float\n        The minimum temperature for simulated annealing. (default: ``Constant.T_MIN``)\n    beta : float\n        The beta in acquisition function. (default: ``Constant.BETA``)\n    algorithm_name : str\n        algorithm name used in the network morphism (default: ``\"Bayesian\"``)\n    optimize_mode : str\n        optimize mode \"minimize\" or \"maximize\" (default: ``\"minimize\"``)\n    verbose : bool\n        verbose to print the log (default: ``True``)\n    bo : BayesianOptimizer\n        The optimizer used in networkmorphsim tuner.\n    max_model_size : int\n        max model size to the graph (default: ``Constant.MAX_MODEL_SIZE``)\n    default_model_len : int\n        default model length (default: ``Constant.MODEL_LEN``)\n    default_model_width : int\n        default model width (default: ``Constant.MODEL_WIDTH``)\n    search_space : dict\n    \"\"\"\n\n    def __init__(\n            self,\n            task=\"cv\",\n            input_width=32,\n            input_channel=3,\n            n_output_node=10,\n            algorithm_name=\"Bayesian\",\n            optimize_mode=\"maximize\",\n            path=\"model_path\",\n            verbose=True,\n            beta=Constant.BETA,\n            t_min=Constant.T_MIN,\n            max_model_size=Constant.MAX_MODEL_SIZE,\n            default_model_len=Constant.MODEL_LEN,\n            default_model_width=Constant.MODEL_WIDTH,\n    ):\n        \"\"\"\n        initilizer of the NetworkMorphismTuner.\n        \"\"\"\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n        self.path = os.path.join(os.getcwd(), path)\n        if task == \"cv\":\n            self.generators = [CnnGenerator]\n        elif task == \"common\":\n            self.generators = [MlpGenerator]\n        else:\n            raise NotImplementedError(\n                '{} task not supported in List [\"cv\",\"common\"]')\n\n        self.n_classes = n_output_node\n        self.input_shape = (input_width, input_width, input_channel)\n\n        self.t_min = t_min\n        self.beta = beta\n        self.algorithm_name = algorithm_name\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.json = None\n        self.total_data = {}\n        self.verbose = verbose\n        self.model_count = 0\n\n        self.bo = BayesianOptimizer(\n            self, self.t_min, self.optimize_mode, self.beta)\n        self.training_queue = []\n        self.descriptors = []\n        self.history = []\n\n        self.max_model_size = max_model_size\n        self.default_model_len = default_model_len\n        self.default_model_width = default_model_width\n\n        self.search_space = dict()\n\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Update search space definition in tuner by search_space in neural architecture.\n        \"\"\"\n        self.search_space = search_space\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Returns a set of trial neural architecture, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n        \"\"\"\n        if not self.history:\n            self.init_search()\n\n        new_father_id = None\n        generated_graph = None\n        if not self.training_queue:\n            new_father_id, generated_graph = self.generate()\n            new_model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append(\n                (generated_graph, new_father_id, new_model_id))\n            self.descriptors.append(generated_graph.extract_descriptor())\n\n        graph, father_id, model_id = self.training_queue.pop(0)\n\n        # from graph to json\n        json_model_path = os.path.join(self.path, str(model_id) + \".json\")\n        json_out = graph_to_json(graph, json_model_path)\n        self.total_data[parameter_id] = (json_out, father_id, model_id)\n\n        return json_out\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Record an observation of the objective function.\n\n        Parameters\n        ----------\n        parameter_id : int\n            the id of a group of paramters that generated by nni manager.\n        parameters : dict\n            A group of parameters.\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n        \"\"\"\n        reward = extract_scalar_reward(value)\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError(\"Received parameter_id not in total_data.\")\n\n        (_, father_id, model_id) = self.total_data[parameter_id]\n\n        graph = self.bo.searcher.load_model_by_id(model_id)\n\n        # to use the value and graph\n        self.add_model(reward, model_id)\n        self.update(father_id, graph, reward, model_id)\n\n\n    def init_search(self):\n        \"\"\"\n        Call the generators to generate the initial architectures for the search.\n        \"\"\"\n        if self.verbose:\n            logger.info(\"Initializing search.\")\n        for generator in self.generators:\n            graph = generator(self.n_classes, self.input_shape).generate(\n                self.default_model_len, self.default_model_width\n            )\n            model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append((graph, -1, model_id))\n            self.descriptors.append(graph.extract_descriptor())\n\n        if self.verbose:\n            logger.info(\"Initialization finished.\")\n\n\n    def generate(self):\n        \"\"\"\n        Generate the next neural architecture.\n\n        Returns\n        -------\n        other_info : any object\n            Anything to be saved in the training queue together with the architecture.\n        generated_graph : Graph\n            An instance of Graph.\n        \"\"\"\n        generated_graph, new_father_id = self.bo.generate(self.descriptors)\n        if new_father_id is None:\n            new_father_id = 0\n            generated_graph = self.generators[0](\n                self.n_classes, self.input_shape\n            ).generate(self.default_model_len, self.default_model_width)\n\n        return new_father_id, generated_graph\n\n    def update(self, other_info, graph, metric_value, model_id):\n        \"\"\"\n        Update the controller with evaluation result of a neural architecture.\n\n        Parameters\n        ----------\n        other_info: any object\n            In our case it is the father ID in the search tree.\n        graph: Graph\n            An instance of Graph. The trained neural architecture.\n        metric_value: float\n            The final evaluated metric value.\n        model_id: int\n        \"\"\"\n        father_id = other_info\n        self.bo.fit([graph.extract_descriptor()], [metric_value])\n        self.bo.add_child(father_id, model_id)\n\n    def add_model(self, metric_value, model_id):\n        \"\"\"\n        Add model to the history, x_queue and y_queue\n\n        Parameters\n        ----------\n        metric_value : float\n        graph : dict\n        model_id : int\n\n        Returns\n        -------\n        model : dict\n        \"\"\"\n        if self.verbose:\n            logger.info(\"Saving model.\")\n\n        # Update best_model text file\n        ret = {\"model_id\": model_id, \"metric_value\": metric_value}\n        self.history.append(ret)\n        if model_id == self.get_best_model_id():\n            file = open(os.path.join(self.path, \"best_model.txt\"), \"w\")\n            file.write(\"best model: \" + str(model_id))\n            file.close()\n        return ret\n\n\n    def get_best_model_id(self):\n        \"\"\"\n        Get the best model_id from history using the metric value\n        \"\"\"\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            return max(self.history, key=lambda x: x[\"metric_value\"])[\n                \"model_id\"]\n        return min(self.history, key=lambda x: x[\"metric_value\"])[\"model_id\"]\n\n\n    def load_model_by_id(self, model_id):\n        \"\"\"\n        Get the model by model_id\n\n        Parameters\n        ----------\n        model_id : int\n            model index\n\n        Returns\n        -------\n        load_model : Graph\n            the model graph representation\n        \"\"\"\n\n        with open(os.path.join(self.path, str(model_id) + \".json\")) as fin:\n            json_str = fin.read().replace(\"\\n\", \"\")\n\n        load_model = json_to_graph(json_str)\n        return load_model\n\n    def load_best_model(self):\n        \"\"\"\n        Get the best model by model id\n\n        Returns\n        -------\n        load_model : Graph\n            the model graph representation\n        \"\"\"\n        return self.load_model_by_id(self.get_best_model_id())\n\n    def get_metric_value_by_id(self, model_id):\n        \"\"\"\n        Get the model metric valud by its model_id\n\n        Parameters\n        ----------\n        model_id : int\n            model index\n\n        Returns\n        -------\n        float\n             the model metric\n        \"\"\"\n        for item in self.history:\n            if item[\"model_id\"] == model_id:\n                return item[\"metric_value\"]\n        return None\n\n    def import_data(self, data):\n        pass",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('task'): self.choices('task', 'cv', 'nlp', 'common'),\n            Optional('input_width'): int,\n            Optional('input_channel'): int,\n            Optional('n_output_node'): int\n        }).validate(kwargs)",
  "def __init__(\n            self,\n            task=\"cv\",\n            input_width=32,\n            input_channel=3,\n            n_output_node=10,\n            algorithm_name=\"Bayesian\",\n            optimize_mode=\"maximize\",\n            path=\"model_path\",\n            verbose=True,\n            beta=Constant.BETA,\n            t_min=Constant.T_MIN,\n            max_model_size=Constant.MAX_MODEL_SIZE,\n            default_model_len=Constant.MODEL_LEN,\n            default_model_width=Constant.MODEL_WIDTH,\n    ):\n        \"\"\"\n        initilizer of the NetworkMorphismTuner.\n        \"\"\"\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n        self.path = os.path.join(os.getcwd(), path)\n        if task == \"cv\":\n            self.generators = [CnnGenerator]\n        elif task == \"common\":\n            self.generators = [MlpGenerator]\n        else:\n            raise NotImplementedError(\n                '{} task not supported in List [\"cv\",\"common\"]')\n\n        self.n_classes = n_output_node\n        self.input_shape = (input_width, input_width, input_channel)\n\n        self.t_min = t_min\n        self.beta = beta\n        self.algorithm_name = algorithm_name\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.json = None\n        self.total_data = {}\n        self.verbose = verbose\n        self.model_count = 0\n\n        self.bo = BayesianOptimizer(\n            self, self.t_min, self.optimize_mode, self.beta)\n        self.training_queue = []\n        self.descriptors = []\n        self.history = []\n\n        self.max_model_size = max_model_size\n        self.default_model_len = default_model_len\n        self.default_model_width = default_model_width\n\n        self.search_space = dict()",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Update search space definition in tuner by search_space in neural architecture.\n        \"\"\"\n        self.search_space = search_space",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Returns a set of trial neural architecture, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n        \"\"\"\n        if not self.history:\n            self.init_search()\n\n        new_father_id = None\n        generated_graph = None\n        if not self.training_queue:\n            new_father_id, generated_graph = self.generate()\n            new_model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append(\n                (generated_graph, new_father_id, new_model_id))\n            self.descriptors.append(generated_graph.extract_descriptor())\n\n        graph, father_id, model_id = self.training_queue.pop(0)\n\n        # from graph to json\n        json_model_path = os.path.join(self.path, str(model_id) + \".json\")\n        json_out = graph_to_json(graph, json_model_path)\n        self.total_data[parameter_id] = (json_out, father_id, model_id)\n\n        return json_out",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Record an observation of the objective function.\n\n        Parameters\n        ----------\n        parameter_id : int\n            the id of a group of paramters that generated by nni manager.\n        parameters : dict\n            A group of parameters.\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n        \"\"\"\n        reward = extract_scalar_reward(value)\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError(\"Received parameter_id not in total_data.\")\n\n        (_, father_id, model_id) = self.total_data[parameter_id]\n\n        graph = self.bo.searcher.load_model_by_id(model_id)\n\n        # to use the value and graph\n        self.add_model(reward, model_id)\n        self.update(father_id, graph, reward, model_id)",
  "def init_search(self):\n        \"\"\"\n        Call the generators to generate the initial architectures for the search.\n        \"\"\"\n        if self.verbose:\n            logger.info(\"Initializing search.\")\n        for generator in self.generators:\n            graph = generator(self.n_classes, self.input_shape).generate(\n                self.default_model_len, self.default_model_width\n            )\n            model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append((graph, -1, model_id))\n            self.descriptors.append(graph.extract_descriptor())\n\n        if self.verbose:\n            logger.info(\"Initialization finished.\")",
  "def generate(self):\n        \"\"\"\n        Generate the next neural architecture.\n\n        Returns\n        -------\n        other_info : any object\n            Anything to be saved in the training queue together with the architecture.\n        generated_graph : Graph\n            An instance of Graph.\n        \"\"\"\n        generated_graph, new_father_id = self.bo.generate(self.descriptors)\n        if new_father_id is None:\n            new_father_id = 0\n            generated_graph = self.generators[0](\n                self.n_classes, self.input_shape\n            ).generate(self.default_model_len, self.default_model_width)\n\n        return new_father_id, generated_graph",
  "def update(self, other_info, graph, metric_value, model_id):\n        \"\"\"\n        Update the controller with evaluation result of a neural architecture.\n\n        Parameters\n        ----------\n        other_info: any object\n            In our case it is the father ID in the search tree.\n        graph: Graph\n            An instance of Graph. The trained neural architecture.\n        metric_value: float\n            The final evaluated metric value.\n        model_id: int\n        \"\"\"\n        father_id = other_info\n        self.bo.fit([graph.extract_descriptor()], [metric_value])\n        self.bo.add_child(father_id, model_id)",
  "def add_model(self, metric_value, model_id):\n        \"\"\"\n        Add model to the history, x_queue and y_queue\n\n        Parameters\n        ----------\n        metric_value : float\n        graph : dict\n        model_id : int\n\n        Returns\n        -------\n        model : dict\n        \"\"\"\n        if self.verbose:\n            logger.info(\"Saving model.\")\n\n        # Update best_model text file\n        ret = {\"model_id\": model_id, \"metric_value\": metric_value}\n        self.history.append(ret)\n        if model_id == self.get_best_model_id():\n            file = open(os.path.join(self.path, \"best_model.txt\"), \"w\")\n            file.write(\"best model: \" + str(model_id))\n            file.close()\n        return ret",
  "def get_best_model_id(self):\n        \"\"\"\n        Get the best model_id from history using the metric value\n        \"\"\"\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            return max(self.history, key=lambda x: x[\"metric_value\"])[\n                \"model_id\"]\n        return min(self.history, key=lambda x: x[\"metric_value\"])[\"model_id\"]",
  "def load_model_by_id(self, model_id):\n        \"\"\"\n        Get the model by model_id\n\n        Parameters\n        ----------\n        model_id : int\n            model index\n\n        Returns\n        -------\n        load_model : Graph\n            the model graph representation\n        \"\"\"\n\n        with open(os.path.join(self.path, str(model_id) + \".json\")) as fin:\n            json_str = fin.read().replace(\"\\n\", \"\")\n\n        load_model = json_to_graph(json_str)\n        return load_model",
  "def load_best_model(self):\n        \"\"\"\n        Get the best model by model id\n\n        Returns\n        -------\n        load_model : Graph\n            the model graph representation\n        \"\"\"\n        return self.load_model_by_id(self.get_best_model_id())",
  "def get_metric_value_by_id(self, model_id):\n        \"\"\"\n        Get the model metric valud by its model_id\n\n        Parameters\n        ----------\n        model_id : int\n            model index\n\n        Returns\n        -------\n        float\n             the model metric\n        \"\"\"\n        for item in self.history:\n            if item[\"model_id\"] == model_id:\n                return item[\"metric_value\"]\n        return None",
  "def import_data(self, data):\n        pass",
  "class AvgPool(nn.Module):\n    \"\"\"\n    AvgPool Module.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def forward(self, input_tensor):\n        pass",
  "class GlobalAvgPool1d(AvgPool):\n    \"\"\"\n    GlobalAvgPool1d Module.\n    \"\"\"\n\n    def forward(self, input_tensor):\n        return functional.avg_pool1d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )",
  "class GlobalAvgPool2d(AvgPool):\n    \"\"\"\n    GlobalAvgPool2d Module.\n    \"\"\"\n\n    def forward(self, input_tensor):\n        return functional.avg_pool2d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )",
  "class GlobalAvgPool3d(AvgPool):\n    \"\"\"\n    GlobalAvgPool3d Module.\n    \"\"\"\n\n    def forward(self, input_tensor):\n        return functional.avg_pool3d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )",
  "class StubLayer:\n    \"\"\"\n    StubLayer Module. Base Module.\n    \"\"\"\n\n    def __init__(self, input_node=None, output_node=None):\n        self.input = input_node\n        self.output = output_node\n        self.weights = None\n\n    def build(self, shape):\n        \"\"\"\n        build shape.\n        \"\"\"\n\n    def set_weights(self, weights):\n        \"\"\"\n        set weights.\n        \"\"\"\n        self.weights = weights\n\n    def import_weights(self, torch_layer):\n        \"\"\"\n        import weights.\n        \"\"\"\n\n    def import_weights_keras(self, keras_layer):\n        \"\"\"\n        import weights from keras layer.\n        \"\"\"\n\n    def export_weights(self, torch_layer):\n        \"\"\"\n        export weights.\n        \"\"\"\n\n    def export_weights_keras(self, keras_layer):\n        \"\"\"\n        export weights to keras layer.\n        \"\"\"\n\n    def get_weights(self):\n        \"\"\"\n        get weights.\n        \"\"\"\n        return self.weights\n\n    def size(self):\n        \"\"\"\n        size().\n        \"\"\"\n        return 0\n\n    @property\n    def output_shape(self):\n        \"\"\"\n        output shape.\n        \"\"\"\n        return self.input.shape\n\n    def to_real_layer(self):\n        \"\"\"\n        to real layer.\n        \"\"\"\n\n    def __str__(self):\n        \"\"\"\n        str() function to print.\n        \"\"\"\n        return type(self).__name__[4:]",
  "class StubWeightBiasLayer(StubLayer):\n    \"\"\"\n    StubWeightBiasLayer Module to set the bias.\n    \"\"\"\n\n    def import_weights(self, torch_layer):\n        self.set_weights(\n            (torch_layer.weight.data.cpu().numpy(),\n             torch_layer.bias.data.cpu().numpy())\n        )\n\n    def import_weights_keras(self, keras_layer):\n        self.set_weights(keras_layer.get_weights())\n\n    def export_weights(self, torch_layer):\n        torch_layer.weight.data = torch.Tensor(self.weights[0])\n        torch_layer.bias.data = torch.Tensor(self.weights[1])\n\n    def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights(self.weights)",
  "class StubBatchNormalization(StubWeightBiasLayer):\n    \"\"\"\n    StubBatchNormalization Module. Batch Norm.\n    \"\"\"\n\n    def __init__(self, num_features, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.num_features = num_features\n\n    def import_weights(self, torch_layer):\n        self.set_weights(\n            (\n                torch_layer.weight.data.cpu().numpy(),\n                torch_layer.bias.data.cpu().numpy(),\n                torch_layer.running_mean.cpu().numpy(),\n                torch_layer.running_var.cpu().numpy(),\n            )\n        )\n\n    def export_weights(self, torch_layer):\n        torch_layer.weight.data = torch.Tensor(self.weights[0])\n        torch_layer.bias.data = torch.Tensor(self.weights[1])\n        torch_layer.running_mean = torch.Tensor(self.weights[2])\n        torch_layer.running_var = torch.Tensor(self.weights[3])\n\n    def size(self):\n        return self.num_features * 4\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass",
  "class StubBatchNormalization1d(StubBatchNormalization):\n    \"\"\"\n    StubBatchNormalization1d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.BatchNorm1d(self.num_features)",
  "class StubBatchNormalization2d(StubBatchNormalization):\n    \"\"\"\n    StubBatchNormalization2d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.BatchNorm2d(self.num_features)",
  "class StubBatchNormalization3d(StubBatchNormalization):\n    \"\"\"\n    StubBatchNormalization3d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.BatchNorm3d(self.num_features)",
  "class StubDense(StubWeightBiasLayer):\n    \"\"\"\n    StubDense Module. Linear.\n    \"\"\"\n\n    def __init__(self, input_units, units, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.input_units = input_units\n        self.units = units\n\n    @property\n    def output_shape(self):\n        return (self.units,)\n\n    def import_weights_keras(self, keras_layer):\n        self.set_weights(\n            (keras_layer.get_weights()[0].T,\n             keras_layer.get_weights()[1]))\n\n    def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights((self.weights[0].T, self.weights[1]))\n\n    def size(self):\n        return self.input_units * self.units + self.units\n\n    def to_real_layer(self):\n        return torch.nn.Linear(self.input_units, self.units)",
  "class StubConv(StubWeightBiasLayer):\n    \"\"\"\n    StubConv Module. Conv.\n    \"\"\"\n\n    def __init__(self, input_channel, filters, kernel_size,\n                 stride=1, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.input_channel = input_channel\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = int(self.kernel_size / 2)\n\n    @property\n    def output_shape(self):\n        ret = list(self.input.shape[:-1])\n        for index, dim in enumerate(ret):\n            ret[index] = (\n                int((dim + 2 * self.padding - self.kernel_size) / self.stride) + 1\n            )\n        ret = ret + [self.filters]\n        return tuple(ret)\n\n    def import_weights_keras(self, keras_layer):\n        self.set_weights(\n            (keras_layer.get_weights()[0].T,\n             keras_layer.get_weights()[1]))\n\n    def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights((self.weights[0].T, self.weights[1]))\n\n    def size(self):\n        return (self.input_channel * self.kernel_size *\n                self.kernel_size + 1) * self.filters\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass\n\n    def __str__(self):\n        return (\n            super().__str__()\n            + \"(\"\n            + \", \".join(\n                str(item)\n                for item in [\n                    self.input_channel,\n                    self.filters,\n                    self.kernel_size,\n                    self.stride,\n                ]\n            )\n            + \")\"\n        )",
  "class StubConv1d(StubConv):\n    \"\"\"\n    StubConv1d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.Conv1d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )",
  "class StubConv2d(StubConv):\n    \"\"\"\n    StubConv2d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.Conv2d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )",
  "class StubConv3d(StubConv):\n    \"\"\"\n    StubConv3d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.Conv3d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )",
  "class StubAggregateLayer(StubLayer):\n    \"\"\"\n    StubAggregateLayer Module.\n    \"\"\"\n\n    def __init__(self, input_nodes=None, output_node=None):\n        if input_nodes is None:\n            input_nodes = []\n        super().__init__(input_nodes, output_node)",
  "class StubConcatenate(StubAggregateLayer):\n    \"\"\"StubConcatenate Module.\n    \"\"\"\n    @property\n    def output_shape(self):\n        ret = 0\n        for current_input in self.input:\n            ret += current_input.shape[-1]\n        ret = self.input[0].shape[:-1] + (ret,)\n        return ret\n\n    def to_real_layer(self):\n        return TorchConcatenate()",
  "class StubAdd(StubAggregateLayer):\n    \"\"\"\n    StubAdd Module.\n    \"\"\"\n    @property\n    def output_shape(self):\n        return self.input[0].shape\n\n    def to_real_layer(self):\n        return TorchAdd()",
  "class StubFlatten(StubLayer):\n    \"\"\"\n    StubFlatten Module.\n    \"\"\"\n    @property\n    def output_shape(self):\n        ret = 1\n        for dim in self.input.shape:\n            ret *= dim\n        return (ret,)\n\n    def to_real_layer(self):\n        return TorchFlatten()",
  "class StubReLU(StubLayer):\n    \"\"\"\n    StubReLU Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.ReLU()",
  "class StubSoftmax(StubLayer):\n    \"\"\"\n    StubSoftmax Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.LogSoftmax(dim=1)",
  "class StubDropout(StubLayer):\n    \"\"\"\n    StubDropout Module.\n    \"\"\"\n\n    def __init__(self, rate, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.rate = rate\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass",
  "class StubDropout1d(StubDropout):\n    \"\"\"\n    StubDropout1d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.Dropout(self.rate)",
  "class StubDropout2d(StubDropout):\n    \"\"\"\n    StubDropout2d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.Dropout2d(self.rate)",
  "class StubDropout3d(StubDropout):\n    \"\"\"\n    StubDropout3d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.Dropout3d(self.rate)",
  "class StubInput(StubLayer):\n    \"\"\"\n    StubInput Module.\n    \"\"\"\n\n    def __init__(self, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)",
  "class StubPooling(StubLayer):\n    \"\"\"\n    StubPooling Module.\n    \"\"\"\n\n    def __init__(self,\n                 kernel_size=None,\n                 stride=None,\n                 padding=0,\n                 input_node=None,\n                 output_node=None):\n        super().__init__(input_node, output_node)\n        self.kernel_size = (\n            kernel_size if kernel_size is not None else Constant.POOLING_KERNEL_SIZE\n        )\n        self.stride = stride if stride is not None else self.kernel_size\n        self.padding = padding\n\n    @property\n    def output_shape(self):\n        ret = tuple()\n        for dim in self.input.shape[:-1]:\n            ret = ret + (max(int(dim / self.kernel_size), 1),)\n        ret = ret + (self.input.shape[-1],)\n        return ret\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass",
  "class StubPooling1d(StubPooling):\n    \"\"\"\n    StubPooling1d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.MaxPool1d(self.kernel_size, stride=self.stride)",
  "class StubPooling2d(StubPooling):\n    \"\"\"\n    StubPooling2d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.MaxPool2d(self.kernel_size, stride=self.stride)",
  "class StubPooling3d(StubPooling):\n    \"\"\"\n    StubPooling3d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return torch.nn.MaxPool3d(self.kernel_size, stride=self.stride)",
  "class StubGlobalPooling(StubLayer):\n    \"\"\"\n    StubGlobalPooling Module.\n    \"\"\"\n\n    def __init__(self, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n\n    @property\n    def output_shape(self):\n        return (self.input.shape[-1],)\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass",
  "class StubGlobalPooling1d(StubGlobalPooling):\n    \"\"\"\n    StubGlobalPooling1d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return GlobalAvgPool1d()",
  "class StubGlobalPooling2d(StubGlobalPooling):\n    \"\"\"\n    StubGlobalPooling2d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return GlobalAvgPool2d()",
  "class StubGlobalPooling3d(StubGlobalPooling):\n    \"\"\"\n    StubGlobalPooling3d Module.\n    \"\"\"\n\n    def to_real_layer(self):\n        return GlobalAvgPool3d()",
  "class TorchConcatenate(nn.Module):\n    \"\"\"\n    TorchConcatenate Module.\n    \"\"\"\n\n    def forward(self, input_list):\n        return torch.cat(input_list, dim=1)",
  "class TorchAdd(nn.Module):\n    \"\"\"\n    TorchAdd Module.\n    \"\"\"\n\n    def forward(self, input_list):\n        return input_list[0] + input_list[1]",
  "class TorchFlatten(nn.Module):\n    \"\"\"\n    TorchFlatten Module.\n    \"\"\"\n\n    def forward(self, input_tensor):\n        return input_tensor.view(input_tensor.size(0), -1)",
  "def keras_dropout(layer, rate):\n    \"\"\"\n    Keras dropout layer.\n    \"\"\"\n\n    from keras import layers\n\n    input_dim = len(layer.input.shape)\n    if input_dim == 2:\n        return layers.SpatialDropout1D(rate)\n    elif input_dim == 3:\n        return layers.SpatialDropout2D(rate)\n    elif input_dim == 4:\n        return layers.SpatialDropout3D(rate)\n    else:\n        return layers.Dropout(rate)",
  "def to_real_keras_layer(layer):\n    \"\"\"\n    Real keras layer.\n    \"\"\"\n    from keras import layers\n\n    if is_layer(layer, \"Dense\"):\n        return layers.Dense(layer.units, input_shape=(layer.input_units,))\n    if is_layer(layer, \"Conv\"):\n        return layers.Conv2D(\n            layer.filters,\n            layer.kernel_size,\n            input_shape=layer.input.shape,\n            padding=\"same\",\n        )  # padding\n    if is_layer(layer, \"Pooling\"):\n        return layers.MaxPool2D(2)\n    if is_layer(layer, \"BatchNormalization\"):\n        return layers.BatchNormalization(input_shape=layer.input.shape)\n    if is_layer(layer, \"Concatenate\"):\n        return layers.Concatenate()\n    if is_layer(layer, \"Add\"):\n        return layers.Add()\n    if is_layer(layer, \"Dropout\"):\n        return keras_dropout(layer, layer.rate)\n    if is_layer(layer, \"ReLU\"):\n        return layers.Activation(\"relu\")\n    if is_layer(layer, \"Softmax\"):\n        return layers.Activation(\"softmax\")\n    if is_layer(layer, \"Flatten\"):\n        return layers.Flatten()\n    if is_layer(layer, \"GlobalAveragePooling\"):\n        return layers.GlobalAveragePooling2D()\n    return None",
  "def is_layer(layer, layer_type):\n    \"\"\"\n    Judge the layer type.\n\n    Returns\n    -------\n    bool\n        boolean -- True or False\n    \"\"\"\n\n    if layer_type == \"Input\":\n        return isinstance(layer, StubInput)\n    elif layer_type == \"Conv\":\n        return isinstance(layer, StubConv)\n    elif layer_type == \"Dense\":\n        return isinstance(layer, (StubDense,))\n    elif layer_type == \"BatchNormalization\":\n        return isinstance(layer, (StubBatchNormalization,))\n    elif layer_type == \"Concatenate\":\n        return isinstance(layer, (StubConcatenate,))\n    elif layer_type == \"Add\":\n        return isinstance(layer, (StubAdd,))\n    elif layer_type == \"Pooling\":\n        return isinstance(layer, StubPooling)\n    elif layer_type == \"Dropout\":\n        return isinstance(layer, (StubDropout,))\n    elif layer_type == \"Softmax\":\n        return isinstance(layer, (StubSoftmax,))\n    elif layer_type == \"ReLU\":\n        return isinstance(layer, (StubReLU,))\n    elif layer_type == \"Flatten\":\n        return isinstance(layer, (StubFlatten,))\n    elif layer_type == \"GlobalAveragePooling\":\n        return isinstance(layer, StubGlobalPooling)\n    return None",
  "def layer_description_extractor(layer, node_to_id):\n    \"\"\"\n    Get layer description.\n    \"\"\"\n\n    layer_input = layer.input\n    layer_output = layer.output\n    if layer_input is not None:\n        if isinstance(layer_input, Iterable):\n            layer_input = list(map(lambda x: node_to_id[x], layer_input))\n        else:\n            layer_input = node_to_id[layer_input]\n\n    if layer_output is not None:\n        layer_output = node_to_id[layer_output]\n\n    if isinstance(layer, StubConv):\n        return (\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.input_channel,\n            layer.filters,\n            layer.kernel_size,\n            layer.stride,\n            layer.padding,\n        )\n    elif isinstance(layer, (StubDense,)):\n        return [\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.input_units,\n            layer.units,\n        ]\n    elif isinstance(layer, (StubBatchNormalization,)):\n        return (type(layer).__name__, layer_input,\n                layer_output, layer.num_features)\n    elif isinstance(layer, (StubDropout,)):\n        return (type(layer).__name__, layer_input, layer_output, layer.rate)\n    elif isinstance(layer, StubPooling):\n        return (\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.kernel_size,\n            layer.stride,\n            layer.padding,\n        )\n    else:\n        return (type(layer).__name__, layer_input, layer_output)",
  "def layer_description_builder(layer_information, id_to_node):\n    \"\"\"build layer from description.\n    \"\"\"\n    layer_type = layer_information[0]\n\n    layer_input_ids = layer_information[1]\n    if isinstance(layer_input_ids, Iterable):\n        layer_input = list(map(lambda x: id_to_node[x], layer_input_ids))\n    else:\n        layer_input = id_to_node[layer_input_ids]\n    layer_output = id_to_node[layer_information[2]]\n    if layer_type.startswith(\"StubConv\"):\n        input_channel = layer_information[3]\n        filters = layer_information[4]\n        kernel_size = layer_information[5]\n        stride = layer_information[6]\n        return globals()[layer_type](\n            input_channel, filters, kernel_size, stride, layer_input, layer_output\n        )\n    elif layer_type.startswith(\"StubDense\"):\n        input_units = layer_information[3]\n        units = layer_information[4]\n        return globals()[layer_type](input_units, units, layer_input, layer_output)\n    elif layer_type.startswith(\"StubBatchNormalization\"):\n        num_features = layer_information[3]\n        return globals()[layer_type](num_features, layer_input, layer_output)\n    elif layer_type.startswith(\"StubDropout\"):\n        rate = layer_information[3]\n        return globals()[layer_type](rate, layer_input, layer_output)\n    elif layer_type.startswith(\"StubPooling\"):\n        kernel_size = layer_information[3]\n        stride = layer_information[4]\n        padding = layer_information[5]\n        return globals()[layer_type](kernel_size, stride, padding, layer_input, layer_output)\n    else:\n        return globals()[layer_type](layer_input, layer_output)",
  "def layer_width(layer):\n    \"\"\"\n    Get layer width.\n    \"\"\"\n\n    if is_layer(layer, \"Dense\"):\n        return layer.units\n    if is_layer(layer, \"Conv\"):\n        return layer.filters\n    raise TypeError(\"The layer should be either Dense or Conv layer.\")",
  "def set_torch_weight_to_stub(torch_layer, stub_layer):\n    stub_layer.import_weights(torch_layer)",
  "def set_keras_weight_to_stub(keras_layer, stub_layer):\n    stub_layer.import_weights_keras(keras_layer)",
  "def set_stub_weight_to_torch(stub_layer, torch_layer):\n    stub_layer.export_weights(torch_layer)",
  "def set_stub_weight_to_keras(stub_layer, keras_layer):\n    stub_layer.export_weights_keras(keras_layer)",
  "def get_conv_class(n_dim):\n    conv_class_list = [StubConv1d, StubConv2d, StubConv3d]\n    return conv_class_list[n_dim - 1]",
  "def get_dropout_class(n_dim):\n    dropout_class_list = [StubDropout1d, StubDropout2d, StubDropout3d]\n    return dropout_class_list[n_dim - 1]",
  "def get_global_avg_pooling_class(n_dim):\n    global_avg_pooling_class_list = [\n        StubGlobalPooling1d,\n        StubGlobalPooling2d,\n        StubGlobalPooling3d,\n    ]\n    return global_avg_pooling_class_list[n_dim - 1]",
  "def get_pooling_class(n_dim):\n    pooling_class_list = [StubPooling1d, StubPooling2d, StubPooling3d]\n    return pooling_class_list[n_dim - 1]",
  "def get_batch_norm_class(n_dim):\n    batch_norm_class_list = [\n        StubBatchNormalization1d,\n        StubBatchNormalization2d,\n        StubBatchNormalization3d,\n    ]\n    return batch_norm_class_list[n_dim - 1]",
  "def get_n_dim(layer):\n    if isinstance(layer, (\n            StubConv1d,\n            StubDropout1d,\n            StubGlobalPooling1d,\n            StubPooling1d,\n            StubBatchNormalization1d,\n    )):\n        return 1\n    if isinstance(layer, (\n            StubConv2d,\n            StubDropout2d,\n            StubGlobalPooling2d,\n            StubPooling2d,\n            StubBatchNormalization2d,\n    )):\n        return 2\n    if isinstance(layer, (\n            StubConv3d,\n            StubDropout3d,\n            StubGlobalPooling3d,\n            StubPooling3d,\n            StubBatchNormalization3d,\n    )):\n        return 3\n    return -1",
  "def __init__(self):\n        super().__init__()",
  "def forward(self, input_tensor):\n        pass",
  "def forward(self, input_tensor):\n        return functional.avg_pool1d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )",
  "def forward(self, input_tensor):\n        return functional.avg_pool2d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )",
  "def forward(self, input_tensor):\n        return functional.avg_pool3d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )",
  "def __init__(self, input_node=None, output_node=None):\n        self.input = input_node\n        self.output = output_node\n        self.weights = None",
  "def build(self, shape):\n        \"\"\"\n        build shape.\n        \"\"\"",
  "def set_weights(self, weights):\n        \"\"\"\n        set weights.\n        \"\"\"\n        self.weights = weights",
  "def import_weights(self, torch_layer):\n        \"\"\"\n        import weights.\n        \"\"\"",
  "def import_weights_keras(self, keras_layer):\n        \"\"\"\n        import weights from keras layer.\n        \"\"\"",
  "def export_weights(self, torch_layer):\n        \"\"\"\n        export weights.\n        \"\"\"",
  "def export_weights_keras(self, keras_layer):\n        \"\"\"\n        export weights to keras layer.\n        \"\"\"",
  "def get_weights(self):\n        \"\"\"\n        get weights.\n        \"\"\"\n        return self.weights",
  "def size(self):\n        \"\"\"\n        size().\n        \"\"\"\n        return 0",
  "def output_shape(self):\n        \"\"\"\n        output shape.\n        \"\"\"\n        return self.input.shape",
  "def to_real_layer(self):\n        \"\"\"\n        to real layer.\n        \"\"\"",
  "def __str__(self):\n        \"\"\"\n        str() function to print.\n        \"\"\"\n        return type(self).__name__[4:]",
  "def import_weights(self, torch_layer):\n        self.set_weights(\n            (torch_layer.weight.data.cpu().numpy(),\n             torch_layer.bias.data.cpu().numpy())\n        )",
  "def import_weights_keras(self, keras_layer):\n        self.set_weights(keras_layer.get_weights())",
  "def export_weights(self, torch_layer):\n        torch_layer.weight.data = torch.Tensor(self.weights[0])\n        torch_layer.bias.data = torch.Tensor(self.weights[1])",
  "def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights(self.weights)",
  "def __init__(self, num_features, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.num_features = num_features",
  "def import_weights(self, torch_layer):\n        self.set_weights(\n            (\n                torch_layer.weight.data.cpu().numpy(),\n                torch_layer.bias.data.cpu().numpy(),\n                torch_layer.running_mean.cpu().numpy(),\n                torch_layer.running_var.cpu().numpy(),\n            )\n        )",
  "def export_weights(self, torch_layer):\n        torch_layer.weight.data = torch.Tensor(self.weights[0])\n        torch_layer.bias.data = torch.Tensor(self.weights[1])\n        torch_layer.running_mean = torch.Tensor(self.weights[2])\n        torch_layer.running_var = torch.Tensor(self.weights[3])",
  "def size(self):\n        return self.num_features * 4",
  "def to_real_layer(self):\n        pass",
  "def to_real_layer(self):\n        return torch.nn.BatchNorm1d(self.num_features)",
  "def to_real_layer(self):\n        return torch.nn.BatchNorm2d(self.num_features)",
  "def to_real_layer(self):\n        return torch.nn.BatchNorm3d(self.num_features)",
  "def __init__(self, input_units, units, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.input_units = input_units\n        self.units = units",
  "def output_shape(self):\n        return (self.units,)",
  "def import_weights_keras(self, keras_layer):\n        self.set_weights(\n            (keras_layer.get_weights()[0].T,\n             keras_layer.get_weights()[1]))",
  "def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights((self.weights[0].T, self.weights[1]))",
  "def size(self):\n        return self.input_units * self.units + self.units",
  "def to_real_layer(self):\n        return torch.nn.Linear(self.input_units, self.units)",
  "def __init__(self, input_channel, filters, kernel_size,\n                 stride=1, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.input_channel = input_channel\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = int(self.kernel_size / 2)",
  "def output_shape(self):\n        ret = list(self.input.shape[:-1])\n        for index, dim in enumerate(ret):\n            ret[index] = (\n                int((dim + 2 * self.padding - self.kernel_size) / self.stride) + 1\n            )\n        ret = ret + [self.filters]\n        return tuple(ret)",
  "def import_weights_keras(self, keras_layer):\n        self.set_weights(\n            (keras_layer.get_weights()[0].T,\n             keras_layer.get_weights()[1]))",
  "def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights((self.weights[0].T, self.weights[1]))",
  "def size(self):\n        return (self.input_channel * self.kernel_size *\n                self.kernel_size + 1) * self.filters",
  "def to_real_layer(self):\n        pass",
  "def __str__(self):\n        return (\n            super().__str__()\n            + \"(\"\n            + \", \".join(\n                str(item)\n                for item in [\n                    self.input_channel,\n                    self.filters,\n                    self.kernel_size,\n                    self.stride,\n                ]\n            )\n            + \")\"\n        )",
  "def to_real_layer(self):\n        return torch.nn.Conv1d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )",
  "def to_real_layer(self):\n        return torch.nn.Conv2d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )",
  "def to_real_layer(self):\n        return torch.nn.Conv3d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )",
  "def __init__(self, input_nodes=None, output_node=None):\n        if input_nodes is None:\n            input_nodes = []\n        super().__init__(input_nodes, output_node)",
  "def output_shape(self):\n        ret = 0\n        for current_input in self.input:\n            ret += current_input.shape[-1]\n        ret = self.input[0].shape[:-1] + (ret,)\n        return ret",
  "def to_real_layer(self):\n        return TorchConcatenate()",
  "def output_shape(self):\n        return self.input[0].shape",
  "def to_real_layer(self):\n        return TorchAdd()",
  "def output_shape(self):\n        ret = 1\n        for dim in self.input.shape:\n            ret *= dim\n        return (ret,)",
  "def to_real_layer(self):\n        return TorchFlatten()",
  "def to_real_layer(self):\n        return torch.nn.ReLU()",
  "def to_real_layer(self):\n        return torch.nn.LogSoftmax(dim=1)",
  "def __init__(self, rate, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.rate = rate",
  "def to_real_layer(self):\n        pass",
  "def to_real_layer(self):\n        return torch.nn.Dropout(self.rate)",
  "def to_real_layer(self):\n        return torch.nn.Dropout2d(self.rate)",
  "def to_real_layer(self):\n        return torch.nn.Dropout3d(self.rate)",
  "def __init__(self, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)",
  "def __init__(self,\n                 kernel_size=None,\n                 stride=None,\n                 padding=0,\n                 input_node=None,\n                 output_node=None):\n        super().__init__(input_node, output_node)\n        self.kernel_size = (\n            kernel_size if kernel_size is not None else Constant.POOLING_KERNEL_SIZE\n        )\n        self.stride = stride if stride is not None else self.kernel_size\n        self.padding = padding",
  "def output_shape(self):\n        ret = tuple()\n        for dim in self.input.shape[:-1]:\n            ret = ret + (max(int(dim / self.kernel_size), 1),)\n        ret = ret + (self.input.shape[-1],)\n        return ret",
  "def to_real_layer(self):\n        pass",
  "def to_real_layer(self):\n        return torch.nn.MaxPool1d(self.kernel_size, stride=self.stride)",
  "def to_real_layer(self):\n        return torch.nn.MaxPool2d(self.kernel_size, stride=self.stride)",
  "def to_real_layer(self):\n        return torch.nn.MaxPool3d(self.kernel_size, stride=self.stride)",
  "def __init__(self, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)",
  "def output_shape(self):\n        return (self.input.shape[-1],)",
  "def to_real_layer(self):\n        pass",
  "def to_real_layer(self):\n        return GlobalAvgPool1d()",
  "def to_real_layer(self):\n        return GlobalAvgPool2d()",
  "def to_real_layer(self):\n        return GlobalAvgPool3d()",
  "def forward(self, input_list):\n        return torch.cat(input_list, dim=1)",
  "def forward(self, input_list):\n        return input_list[0] + input_list[1]",
  "def forward(self, input_tensor):\n        return input_tensor.view(input_tensor.size(0), -1)",
  "def json2space(in_x, name=NodeType.ROOT):\n    \"\"\"\n    Change json to search space in hyperopt.\n\n    Parameters\n    ----------\n    in_x : dict/list/str/int/float\n        The part of json.\n    name : str\n        name could be NodeType.ROOT, NodeType.TYPE, NodeType.VALUE or NodeType.INDEX, NodeType.NAME.\n    \"\"\"\n    out_y = copy.deepcopy(in_x)\n    if isinstance(in_x, dict):\n        if NodeType.TYPE in in_x.keys():\n            _type = in_x[NodeType.TYPE]\n            name = name + '-' + _type\n            _value = json2space(in_x[NodeType.VALUE], name=name)\n            if _type == 'choice':\n                out_y = hp.hp.choice(name, _value)\n            elif _type == 'randint':\n                out_y = hp.hp.randint(name, _value[1] - _value[0])\n            else:\n                if _type in ['loguniform', 'qloguniform']:\n                    _value[:2] = np.log(_value[:2])\n                out_y = getattr(hp.hp, _type)(name, *_value)\n        else:\n            out_y = dict()\n            for key in in_x.keys():\n                out_y[key] = json2space(in_x[key], name + '[%s]' % str(key))\n    elif isinstance(in_x, list):\n        out_y = list()\n        for i, x_i in enumerate(in_x):\n            if isinstance(x_i, dict):\n                if NodeType.NAME not in x_i.keys():\n                    raise RuntimeError(\n                        '\\'_name\\' key is not found in this nested search space.'\n                    )\n            out_y.append(json2space(x_i, name + '[%d]' % i))\n    return out_y",
  "def json2parameter(in_x, parameter, name=NodeType.ROOT):\n    \"\"\"\n    Change json to parameters.\n    \"\"\"\n    out_y = copy.deepcopy(in_x)\n    if isinstance(in_x, dict):\n        if NodeType.TYPE in in_x.keys():\n            _type = in_x[NodeType.TYPE]\n            name = name + '-' + _type\n            if _type == 'choice':\n                _index = parameter[name]\n                out_y = {\n                    NodeType.INDEX:\n                    _index,\n                    NodeType.VALUE:\n                    json2parameter(in_x[NodeType.VALUE][_index],\n                                   parameter,\n                                   name=name + '[%d]' % _index)\n                }\n            else:\n                if _type in ['quniform', 'qloguniform']:\n                    out_y = np.clip(parameter[name], in_x[NodeType.VALUE][0], in_x[NodeType.VALUE][1])\n                elif _type == 'randint':\n                    out_y = parameter[name] + in_x[NodeType.VALUE][0]\n                else:\n                    out_y = parameter[name]\n        else:\n            out_y = dict()\n            for key in in_x.keys():\n                out_y[key] = json2parameter(in_x[key], parameter,\n                                            name + '[%s]' % str(key))\n    elif isinstance(in_x, list):\n        out_y = list()\n        for i, x_i in enumerate(in_x):\n            if isinstance(x_i, dict):\n                if NodeType.NAME not in x_i.keys():\n                    raise RuntimeError(\n                        '\\'_name\\' key is not found in this nested search space.'\n                    )\n            out_y.append(json2parameter(x_i, parameter, name + '[%d]' % i))\n    return out_y",
  "def json2vals(in_x, vals, out_y, name=NodeType.ROOT):\n    if isinstance(in_x, dict):\n        if NodeType.TYPE in in_x.keys():\n            _type = in_x[NodeType.TYPE]\n            name = name + '-' + _type\n\n            try:\n                out_y[name] = vals[NodeType.INDEX]\n            # TODO - catch exact Exception\n            except Exception:\n                out_y[name] = vals\n\n            if _type == 'choice':\n                _index = vals[NodeType.INDEX]\n                json2vals(in_x[NodeType.VALUE][_index],\n                          vals[NodeType.VALUE],\n                          out_y,\n                          name=name + '[%d]' % _index)\n            if _type == 'randint':\n                out_y[name] -= in_x[NodeType.VALUE][0]\n        else:\n            for key in in_x.keys():\n                json2vals(in_x[key], vals[key], out_y,\n                          name + '[%s]' % str(key))\n    elif isinstance(in_x, list):\n        for i, temp in enumerate(in_x):\n            # nested json\n            if isinstance(temp, dict):\n                if NodeType.NAME not in temp.keys():\n                    raise RuntimeError(\n                        '\\'_name\\' key is not found in this nested search space.'\n                    )\n                else:\n                    json2vals(temp, vals[i], out_y, name + '[%d]' % i)\n            else:\n                json2vals(temp, vals[i], out_y, name + '[%d]' % i)",
  "def _add_index(in_x, parameter):\n    \"\"\"\n    change parameters in NNI format to parameters in hyperopt format(This function also support nested dict.).\n    For example, receive parameters like:\n        {'dropout_rate': 0.8, 'conv_size': 3, 'hidden_size': 512}\n    Will change to format in hyperopt, like:\n        {'dropout_rate': 0.8, 'conv_size': {'_index': 1, '_value': 3}, 'hidden_size': {'_index': 1, '_value': 512}}\n    \"\"\"\n    if NodeType.TYPE not in in_x: # if at the top level\n        out_y = dict()\n        for key, value in parameter.items():\n            out_y[key] = _add_index(in_x[key], value)\n        return out_y\n    elif isinstance(in_x, dict):\n        value_type = in_x[NodeType.TYPE]\n        value_format = in_x[NodeType.VALUE]\n        if value_type == \"choice\":\n            choice_name = parameter[0] if isinstance(parameter,\n                                                     list) else parameter\n            for pos, item in enumerate(\n                    value_format):  # here value_format is a list\n                if isinstance(\n                        item,\n                        list):  # this format is [\"choice_key\", format_dict]\n                    choice_key = item[0]\n                    choice_value_format = item[1]\n                    if choice_key == choice_name:\n                        return {\n                            NodeType.INDEX: pos,\n                            NodeType.VALUE: [\n                                choice_name,\n                                _add_index(choice_value_format, parameter[1])\n                            ]\n                        }\n                elif choice_name == item:\n                    return {NodeType.INDEX: pos, NodeType.VALUE: item}\n        else:\n            return parameter\n    return None",
  "class HyperoptClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('parallel_optimize'): bool,\n            Optional('constant_liar_type'): self.choices('constant_liar_type', 'min', 'max', 'mean')\n        }).validate(kwargs)",
  "class HyperoptTuner(Tuner):\n    \"\"\"\n    HyperoptTuner is a tuner which using hyperopt algorithm.\n    \"\"\"\n\n    def __init__(self, algorithm_name, optimize_mode='minimize',\n                 parallel_optimize=False, constant_liar_type='min'):\n        \"\"\"\n        Parameters\n        ----------\n        algorithm_name : str\n            algorithm_name includes \"tpe\", \"random_search\" and anneal\".\n        optimize_mode : str\n        parallel_optimize : bool\n            More detail could reference: docs/en_US/Tuner/HyperoptTuner.md\n        constant_liar_type : str\n            constant_liar_type including \"min\", \"max\" and \"mean\"\n            More detail could reference: docs/en_US/Tuner/HyperoptTuner.md\n        \"\"\"\n        self.algorithm_name = algorithm_name\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.json = None\n        self.total_data = {}\n        self.rval = None\n        self.supplement_data_num = 0\n\n        self.parallel = parallel_optimize\n        if self.parallel:\n            self.CL_rval = None\n            self.constant_liar_type = constant_liar_type\n            self.running_data = []\n            self.optimal_y = None\n\n    def _choose_tuner(self, algorithm_name):\n        \"\"\"\n        Parameters\n        ----------\n        algorithm_name : str\n            algorithm_name includes \"tpe\", \"random_search\" and anneal\"\n        \"\"\"\n        if algorithm_name == 'tpe':\n            return hp.tpe.suggest\n        if algorithm_name == 'random_search':\n            return hp.rand.suggest\n        if algorithm_name == 'anneal':\n            return hp.anneal.suggest\n        raise RuntimeError('Not support tuner algorithm in hyperopt.')\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Update search space definition in tuner by search_space in parameters.\n\n        Will called when first setup experiemnt or update search space in WebUI.\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self.json = search_space\n\n        search_space_instance = json2space(self.json)\n        rstate = np.random.RandomState()\n        trials = hp.Trials()\n        domain = hp.Domain(None,\n                           search_space_instance,\n                           pass_expr_memo_ctrl=None)\n        algorithm = self._choose_tuner(self.algorithm_name)\n        self.rval = hp.FMinIter(algorithm,\n                                domain,\n                                trials,\n                                max_evals=-1,\n                                rstate=rstate,\n                                verbose=0)\n        self.rval.catch_eval_exceptions = False\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Returns a set of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        params : dict\n        \"\"\"\n        total_params = self.get_suggestion(random_search=False)\n        # avoid generating same parameter with concurrent trials because hyperopt doesn't support parallel mode\n        if total_params in self.total_data.values():\n            # but it can cause duplicate parameter rarely\n            total_params = self.get_suggestion(random_search=True)\n        self.total_data[parameter_id] = total_params\n\n        if self.parallel:\n            self.running_data.append(parameter_id)\n\n        params = split_index(total_params)\n        return params\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Record an observation of the objective function\n\n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n            value is final metrics of the trial.\n        \"\"\"\n        reward = extract_scalar_reward(value)\n        # restore the paramsters contains '_index'\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        params = self.total_data[parameter_id]\n\n        # code for parallel\n        if self.parallel:\n            constant_liar = kwargs.get('constant_liar', False)\n\n            if constant_liar:\n                rval = self.CL_rval\n            else:\n                rval = self.rval\n                # ignore duplicated reported final result (due to aware of intermedate result)\n                if parameter_id not in self.running_data:\n                    logger.info(\"Received duplicated final result with parameter id: %s\", parameter_id)\n                    return\n                self.running_data.remove(parameter_id)\n\n                # update the reward of optimal_y\n                if self.optimal_y is None:\n                    if self.constant_liar_type == 'mean':\n                        self.optimal_y = [reward, 1]\n                    else:\n                        self.optimal_y = reward\n                else:\n                    if self.constant_liar_type == 'mean':\n                        _sum = self.optimal_y[0] + reward\n                        _number = self.optimal_y[1] + 1\n                        self.optimal_y = [_sum, _number]\n                    elif self.constant_liar_type == 'min':\n                        self.optimal_y = min(self.optimal_y, reward)\n                    elif self.constant_liar_type == 'max':\n                        self.optimal_y = max(self.optimal_y, reward)\n                logger.debug(\"Update optimal_y with reward, optimal_y = %s\", self.optimal_y)\n        else:\n            rval = self.rval\n\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        domain = rval.domain\n        trials = rval.trials\n\n        new_id = len(trials)\n\n        rval_specs = [None]\n        rval_results = [domain.new_result()]\n        rval_miscs = [dict(tid=new_id, cmd=domain.cmd, workdir=domain.workdir)]\n\n        vals = params\n        idxs = dict()\n\n        out_y = dict()\n        json2vals(self.json, vals, out_y)\n        vals = out_y\n        for key in domain.params:\n            if key in [NodeType.VALUE, NodeType.INDEX]:\n                continue\n            if key not in vals or vals[key] is None or vals[key] == []:\n                idxs[key] = vals[key] = []\n            else:\n                idxs[key] = [new_id]\n                vals[key] = [vals[key]]\n\n        self.miscs_update_idxs_vals(rval_miscs,\n                                    idxs,\n                                    vals,\n                                    idxs_map={new_id: new_id},\n                                    assert_all_vals_used=False)\n\n        trial = trials.new_trial_docs([new_id], rval_specs, rval_results,\n                                      rval_miscs)[0]\n        trial['result'] = {'loss': reward, 'status': 'ok'}\n        trial['state'] = hp.JOB_STATE_DONE\n        trials.insert_trial_docs([trial])\n        trials.refresh()\n\n    def miscs_update_idxs_vals(self,\n                               miscs,\n                               idxs,\n                               vals,\n                               assert_all_vals_used=True,\n                               idxs_map=None):\n        \"\"\"\n        Unpack the idxs-vals format into the list of dictionaries that is\n        `misc`.\n\n        Parameters\n        ----------\n        idxs_map : dict\n            idxs_map is a dictionary of id->id mappings so that the misc['idxs'] can\n        contain different numbers than the idxs argument.\n        \"\"\"\n        if idxs_map is None:\n            idxs_map = {}\n\n        assert set(idxs.keys()) == set(vals.keys())\n\n        misc_by_id = {m['tid']: m for m in miscs}\n        for m in miscs:\n            m['idxs'] = {key: [] for key in idxs}\n            m['vals'] = {key: [] for key in idxs}\n\n        for key in idxs:\n            assert len(idxs[key]) == len(vals[key])\n            for tid, val in zip(idxs[key], vals[key]):\n                tid = idxs_map.get(tid, tid)\n                if assert_all_vals_used or tid in misc_by_id:\n                    misc_by_id[tid]['idxs'][key] = [tid]\n                    misc_by_id[tid]['vals'][key] = [val]\n\n    def get_suggestion(self, random_search=False):\n        \"\"\"\n        get suggestion from hyperopt\n\n        Parameters\n        ----------\n        random_search : bool\n            flag to indicate random search or not (default: {False})\n\n        Returns\n        ----------\n        total_params : dict\n            parameter suggestion\n        \"\"\"\n        if self.parallel and len(self.total_data) > 20 and self.running_data and self.optimal_y is not None:\n            self.CL_rval = copy.deepcopy(self.rval)\n            if self.constant_liar_type == 'mean':\n                _constant_liar_y = self.optimal_y[0] / self.optimal_y[1]\n            else:\n                _constant_liar_y = self.optimal_y\n            for _parameter_id in self.running_data:\n                self.receive_trial_result(parameter_id=_parameter_id, parameters=None, value=_constant_liar_y, constant_liar=True)\n            rval = self.CL_rval\n\n            random_state = np.random.randint(2**31 - 1)\n        else:\n            rval = self.rval\n            random_state = rval.rstate.randint(2**31 - 1)\n\n        trials = rval.trials\n        algorithm = rval.algo\n        new_ids = rval.trials.new_trial_ids(1)\n        rval.trials.refresh()\n\n        if random_search:\n            new_trials = hp.rand.suggest(new_ids, rval.domain, trials,\n                                         random_state)\n        else:\n            new_trials = algorithm(new_ids, rval.domain, trials, random_state)\n        rval.trials.refresh()\n        vals = new_trials[0]['misc']['vals']\n        parameter = dict()\n        for key in vals:\n            try:\n                parameter[key] = vals[key][0].item()\n            except (KeyError, IndexError):\n                parameter[key] = None\n\n        # remove '_index' from json2parameter and save params-id\n        total_params = json2parameter(self.json, parameter)\n        return total_params\n\n    def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            if self.algorithm_name == 'random_search':\n                return\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = '_'.join(\n                [\"ImportData\", str(self.supplement_data_num)])\n            self.total_data[_parameter_id] = _add_index(in_x=self.json,\n                                                        parameter=_params)\n            self.receive_trial_result(parameter_id=_parameter_id,\n                                      parameters=_params,\n                                      value=_value)\n        logger.info(\"Successfully import data to TPE/Anneal tuner.\")",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('parallel_optimize'): bool,\n            Optional('constant_liar_type'): self.choices('constant_liar_type', 'min', 'max', 'mean')\n        }).validate(kwargs)",
  "def __init__(self, algorithm_name, optimize_mode='minimize',\n                 parallel_optimize=False, constant_liar_type='min'):\n        \"\"\"\n        Parameters\n        ----------\n        algorithm_name : str\n            algorithm_name includes \"tpe\", \"random_search\" and anneal\".\n        optimize_mode : str\n        parallel_optimize : bool\n            More detail could reference: docs/en_US/Tuner/HyperoptTuner.md\n        constant_liar_type : str\n            constant_liar_type including \"min\", \"max\" and \"mean\"\n            More detail could reference: docs/en_US/Tuner/HyperoptTuner.md\n        \"\"\"\n        self.algorithm_name = algorithm_name\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.json = None\n        self.total_data = {}\n        self.rval = None\n        self.supplement_data_num = 0\n\n        self.parallel = parallel_optimize\n        if self.parallel:\n            self.CL_rval = None\n            self.constant_liar_type = constant_liar_type\n            self.running_data = []\n            self.optimal_y = None",
  "def _choose_tuner(self, algorithm_name):\n        \"\"\"\n        Parameters\n        ----------\n        algorithm_name : str\n            algorithm_name includes \"tpe\", \"random_search\" and anneal\"\n        \"\"\"\n        if algorithm_name == 'tpe':\n            return hp.tpe.suggest\n        if algorithm_name == 'random_search':\n            return hp.rand.suggest\n        if algorithm_name == 'anneal':\n            return hp.anneal.suggest\n        raise RuntimeError('Not support tuner algorithm in hyperopt.')",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Update search space definition in tuner by search_space in parameters.\n\n        Will called when first setup experiemnt or update search space in WebUI.\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self.json = search_space\n\n        search_space_instance = json2space(self.json)\n        rstate = np.random.RandomState()\n        trials = hp.Trials()\n        domain = hp.Domain(None,\n                           search_space_instance,\n                           pass_expr_memo_ctrl=None)\n        algorithm = self._choose_tuner(self.algorithm_name)\n        self.rval = hp.FMinIter(algorithm,\n                                domain,\n                                trials,\n                                max_evals=-1,\n                                rstate=rstate,\n                                verbose=0)\n        self.rval.catch_eval_exceptions = False",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Returns a set of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        params : dict\n        \"\"\"\n        total_params = self.get_suggestion(random_search=False)\n        # avoid generating same parameter with concurrent trials because hyperopt doesn't support parallel mode\n        if total_params in self.total_data.values():\n            # but it can cause duplicate parameter rarely\n            total_params = self.get_suggestion(random_search=True)\n        self.total_data[parameter_id] = total_params\n\n        if self.parallel:\n            self.running_data.append(parameter_id)\n\n        params = split_index(total_params)\n        return params",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Record an observation of the objective function\n\n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n            value is final metrics of the trial.\n        \"\"\"\n        reward = extract_scalar_reward(value)\n        # restore the paramsters contains '_index'\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        params = self.total_data[parameter_id]\n\n        # code for parallel\n        if self.parallel:\n            constant_liar = kwargs.get('constant_liar', False)\n\n            if constant_liar:\n                rval = self.CL_rval\n            else:\n                rval = self.rval\n                # ignore duplicated reported final result (due to aware of intermedate result)\n                if parameter_id not in self.running_data:\n                    logger.info(\"Received duplicated final result with parameter id: %s\", parameter_id)\n                    return\n                self.running_data.remove(parameter_id)\n\n                # update the reward of optimal_y\n                if self.optimal_y is None:\n                    if self.constant_liar_type == 'mean':\n                        self.optimal_y = [reward, 1]\n                    else:\n                        self.optimal_y = reward\n                else:\n                    if self.constant_liar_type == 'mean':\n                        _sum = self.optimal_y[0] + reward\n                        _number = self.optimal_y[1] + 1\n                        self.optimal_y = [_sum, _number]\n                    elif self.constant_liar_type == 'min':\n                        self.optimal_y = min(self.optimal_y, reward)\n                    elif self.constant_liar_type == 'max':\n                        self.optimal_y = max(self.optimal_y, reward)\n                logger.debug(\"Update optimal_y with reward, optimal_y = %s\", self.optimal_y)\n        else:\n            rval = self.rval\n\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        domain = rval.domain\n        trials = rval.trials\n\n        new_id = len(trials)\n\n        rval_specs = [None]\n        rval_results = [domain.new_result()]\n        rval_miscs = [dict(tid=new_id, cmd=domain.cmd, workdir=domain.workdir)]\n\n        vals = params\n        idxs = dict()\n\n        out_y = dict()\n        json2vals(self.json, vals, out_y)\n        vals = out_y\n        for key in domain.params:\n            if key in [NodeType.VALUE, NodeType.INDEX]:\n                continue\n            if key not in vals or vals[key] is None or vals[key] == []:\n                idxs[key] = vals[key] = []\n            else:\n                idxs[key] = [new_id]\n                vals[key] = [vals[key]]\n\n        self.miscs_update_idxs_vals(rval_miscs,\n                                    idxs,\n                                    vals,\n                                    idxs_map={new_id: new_id},\n                                    assert_all_vals_used=False)\n\n        trial = trials.new_trial_docs([new_id], rval_specs, rval_results,\n                                      rval_miscs)[0]\n        trial['result'] = {'loss': reward, 'status': 'ok'}\n        trial['state'] = hp.JOB_STATE_DONE\n        trials.insert_trial_docs([trial])\n        trials.refresh()",
  "def miscs_update_idxs_vals(self,\n                               miscs,\n                               idxs,\n                               vals,\n                               assert_all_vals_used=True,\n                               idxs_map=None):\n        \"\"\"\n        Unpack the idxs-vals format into the list of dictionaries that is\n        `misc`.\n\n        Parameters\n        ----------\n        idxs_map : dict\n            idxs_map is a dictionary of id->id mappings so that the misc['idxs'] can\n        contain different numbers than the idxs argument.\n        \"\"\"\n        if idxs_map is None:\n            idxs_map = {}\n\n        assert set(idxs.keys()) == set(vals.keys())\n\n        misc_by_id = {m['tid']: m for m in miscs}\n        for m in miscs:\n            m['idxs'] = {key: [] for key in idxs}\n            m['vals'] = {key: [] for key in idxs}\n\n        for key in idxs:\n            assert len(idxs[key]) == len(vals[key])\n            for tid, val in zip(idxs[key], vals[key]):\n                tid = idxs_map.get(tid, tid)\n                if assert_all_vals_used or tid in misc_by_id:\n                    misc_by_id[tid]['idxs'][key] = [tid]\n                    misc_by_id[tid]['vals'][key] = [val]",
  "def get_suggestion(self, random_search=False):\n        \"\"\"\n        get suggestion from hyperopt\n\n        Parameters\n        ----------\n        random_search : bool\n            flag to indicate random search or not (default: {False})\n\n        Returns\n        ----------\n        total_params : dict\n            parameter suggestion\n        \"\"\"\n        if self.parallel and len(self.total_data) > 20 and self.running_data and self.optimal_y is not None:\n            self.CL_rval = copy.deepcopy(self.rval)\n            if self.constant_liar_type == 'mean':\n                _constant_liar_y = self.optimal_y[0] / self.optimal_y[1]\n            else:\n                _constant_liar_y = self.optimal_y\n            for _parameter_id in self.running_data:\n                self.receive_trial_result(parameter_id=_parameter_id, parameters=None, value=_constant_liar_y, constant_liar=True)\n            rval = self.CL_rval\n\n            random_state = np.random.randint(2**31 - 1)\n        else:\n            rval = self.rval\n            random_state = rval.rstate.randint(2**31 - 1)\n\n        trials = rval.trials\n        algorithm = rval.algo\n        new_ids = rval.trials.new_trial_ids(1)\n        rval.trials.refresh()\n\n        if random_search:\n            new_trials = hp.rand.suggest(new_ids, rval.domain, trials,\n                                         random_state)\n        else:\n            new_trials = algorithm(new_ids, rval.domain, trials, random_state)\n        rval.trials.refresh()\n        vals = new_trials[0]['misc']['vals']\n        parameter = dict()\n        for key in vals:\n            try:\n                parameter[key] = vals[key][0].item()\n            except (KeyError, IndexError):\n                parameter[key] = None\n\n        # remove '_index' from json2parameter and save params-id\n        total_params = json2parameter(self.json, parameter)\n        return total_params",
  "def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            if self.algorithm_name == 'random_search':\n                return\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = '_'.join(\n                [\"ImportData\", str(self.supplement_data_num)])\n            self.total_data[_parameter_id] = _add_index(in_x=self.json,\n                                                        parameter=_params)\n            self.receive_trial_result(parameter_id=_parameter_id,\n                                      parameters=_params,\n                                      value=_value)\n        logger.info(\"Successfully import data to TPE/Anneal tuner.\")",
  "def get_next_parameter():\n    _logger.warning('Requesting parameter without NNI framework, returning empty dict')\n    return {\n        'parameter_id': None,\n        'parameters': {}\n    }",
  "def get_experiment_id():\n    return 'STANDALONE'",
  "def get_trial_id():\n    return 'STANDALONE'",
  "def get_sequence_id():\n    return 0",
  "def send_metric(string):\n    metric = json_tricks.loads(string)\n    if metric['type'] == 'FINAL':\n        _logger.info('Final result: %s', metric['value'])\n    elif metric['type'] == 'PERIODICAL':\n        _logger.info('Intermediate result: %s  (Index %s)', metric['value'], metric['sequence'])\n    else:\n        _logger.error('Unexpected metric: %s', string)",
  "def request_next_parameter():\n    metric = to_json({\n        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,\n        'type': 'REQUEST_PARAMETER',\n        'sequence': 0,\n        'parameter_index': _param_index\n    })\n    send_metric(metric)",
  "def get_next_parameter():\n    global _param_index\n    params_file_name = ''\n    if _multiphase in ('true', 'True'):\n        params_file_name = ('parameter_{}.cfg'.format(_param_index), 'parameter.cfg')[_param_index == 0]\n    else:\n        if _param_index > 0:\n            return None\n        elif _param_index == 0:\n            params_file_name = 'parameter.cfg'\n        else:\n            raise AssertionError('_param_index value ({}) should >=0'.format(_param_index))\n\n    params_filepath = os.path.join(_sysdir, params_file_name)\n    if not os.path.isfile(params_filepath):\n        request_next_parameter()\n    while not (os.path.isfile(params_filepath) and os.path.getsize(params_filepath) > 0):\n        time.sleep(3)\n    params_file = open(params_filepath, 'r')\n    params = json.load(params_file)\n    _param_index += 1\n    return params",
  "def send_metric(string):\n    if _nni_platform != 'local':\n        assert len(string) < 1000000, 'Metric too long'\n        print(\"NNISDK_MEb'%s'\" % (string), flush=True)\n    else:\n        data = (string + '\\n').encode('utf8')\n        assert len(data) < 1000000, 'Metric too long'\n        _metric_file.write(b'ME%06d%b' % (len(data), data))\n        _metric_file.flush()\n        if sys.platform == \"win32\":\n            file = open(_metric_file.name)\n            file.close()\n        else:\n            subprocess.run(['touch', _metric_file.name], check=True)",
  "def get_experiment_id():\n    return trial_env_vars.NNI_EXP_ID",
  "def get_trial_id():\n    return trial_env_vars.NNI_TRIAL_JOB_ID",
  "def get_sequence_id():\n    return int(trial_env_vars.NNI_TRIAL_SEQ_ID)",
  "def get_next_parameter():\n    return _params",
  "def get_experiment_id():\n    return 'fakeidex'",
  "def get_trial_id():\n    return 'fakeidtr'",
  "def get_sequence_id():\n    return 0",
  "def send_metric(string):\n    global _last_metric\n    _last_metric = string",
  "def init_params(params):\n    global _params\n    _params = copy.deepcopy(params)",
  "def get_last_metric():\n    metrics = json_tricks.loads(_last_metric)\n    metrics['value'] = json_tricks.loads(metrics['value'])\n\n    return metrics",
  "class CurvefittingClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            'epoch_num': self.range('epoch_num', int, 0, 9999),\n            Optional('start_step'): self.range('start_step', int, 0, 9999),\n            Optional('threshold'): self.range('threshold', float, 0, 9999),\n            Optional('gap'): self.range('gap', int, 1, 9999),\n        }).validate(kwargs)",
  "class CurvefittingAssessor(Assessor):\n    \"\"\"CurvefittingAssessor uses learning curve fitting algorithm to predict the learning curve performance in the future.\n    It stops a pending trial X at step S if the trial's forecast result at target step is convergence and lower than the\n    best performance in the history.\n\n    Parameters\n    ----------\n    epoch_num : int\n        The total number of epoch\n    start_step : int\n        only after receiving start_step number of reported intermediate results\n    threshold : float\n        The threshold that we decide to early stop the worse performance curve.\n    \"\"\"\n\n    def __init__(self, epoch_num=20, start_step=6, threshold=0.95, gap=1):\n        if start_step <= 0:\n            logger.warning('It\\'s recommended to set start_step to a positive number')\n        # Record the target position we predict\n        self.target_pos = epoch_num\n        # Start forecasting when historical data reaches start step\n        self.start_step = start_step\n        # Record the compared threshold\n        self.threshold = threshold\n        # Record the number of gap\n        self.gap = gap\n        # Record the number of intermediate result in the lastest judgment\n        self.last_judgment_num = dict()\n        # Record the best performance\n        self.set_best_performance = False\n        self.completed_best_performance = None\n        self.trial_history = []\n        logger.info('Successfully initials the curvefitting assessor')\n\n    def trial_end(self, trial_job_id, success):\n        \"\"\"update the best performance of completed trial job\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        success : bool\n            True if succssfully finish the experiment, False otherwise\n        \"\"\"\n        if success:\n            if self.set_best_performance:\n                self.completed_best_performance = max(self.completed_best_performance, self.trial_history[-1])\n            else:\n                self.set_best_performance = True\n                self.completed_best_performance = self.trial_history[-1]\n            logger.info('Updated complted best performance, trial job id: %s', trial_job_id)\n        else:\n            logger.info('No need to update, trial job id: %s', trial_job_id)\n\n    def assess_trial(self, trial_job_id, trial_history):\n        \"\"\"assess whether a trial should be early stop by curve fitting algorithm\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n\n        Returns\n        -------\n        bool\n            AssessResult.Good or AssessResult.Bad\n\n        Raises\n        ------\n        Exception\n            unrecognize exception in curvefitting_assessor\n        \"\"\"\n        scalar_trial_history = extract_scalar_history(trial_history)\n        self.trial_history = scalar_trial_history\n        if not self.set_best_performance:\n            return AssessResult.Good\n        curr_step = len(scalar_trial_history)\n        if curr_step < self.start_step:\n            return AssessResult.Good\n\n        if trial_job_id in self.last_judgment_num.keys() and curr_step - self.last_judgment_num[trial_job_id] < self.gap:\n            return AssessResult.Good\n        self.last_judgment_num[trial_job_id] = curr_step\n\n        try:\n            start_time = datetime.datetime.now()\n            # Predict the final result\n            curvemodel = CurveModel(self.target_pos)\n            predict_y = curvemodel.predict(scalar_trial_history)\n            log_message = \"Prediction done. Trial job id = {}, Predict value = {}\".format(trial_job_id, predict_y)\n            if predict_y is None:\n                logger.info('%s, wait for more information to predict precisely', log_message)\n                return AssessResult.Good\n            else:\n                logger.info(log_message)\n            standard_performance = self.completed_best_performance * self.threshold\n\n            end_time = datetime.datetime.now()\n            if (end_time - start_time).seconds > 60:\n                logger.warning(\n                    'Curve Fitting Assessor Runtime Exceeds 60s, Trial Id = %s Trial History = %s',\n                    trial_job_id, self.trial_history\n                )\n\n            if predict_y > standard_performance:\n                return AssessResult.Good\n            return AssessResult.Bad\n\n        except Exception as exception:\n            logger.exception('unrecognize exception in curvefitting_assessor %s', exception)",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            'epoch_num': self.range('epoch_num', int, 0, 9999),\n            Optional('start_step'): self.range('start_step', int, 0, 9999),\n            Optional('threshold'): self.range('threshold', float, 0, 9999),\n            Optional('gap'): self.range('gap', int, 1, 9999),\n        }).validate(kwargs)",
  "def __init__(self, epoch_num=20, start_step=6, threshold=0.95, gap=1):\n        if start_step <= 0:\n            logger.warning('It\\'s recommended to set start_step to a positive number')\n        # Record the target position we predict\n        self.target_pos = epoch_num\n        # Start forecasting when historical data reaches start step\n        self.start_step = start_step\n        # Record the compared threshold\n        self.threshold = threshold\n        # Record the number of gap\n        self.gap = gap\n        # Record the number of intermediate result in the lastest judgment\n        self.last_judgment_num = dict()\n        # Record the best performance\n        self.set_best_performance = False\n        self.completed_best_performance = None\n        self.trial_history = []\n        logger.info('Successfully initials the curvefitting assessor')",
  "def trial_end(self, trial_job_id, success):\n        \"\"\"update the best performance of completed trial job\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        success : bool\n            True if succssfully finish the experiment, False otherwise\n        \"\"\"\n        if success:\n            if self.set_best_performance:\n                self.completed_best_performance = max(self.completed_best_performance, self.trial_history[-1])\n            else:\n                self.set_best_performance = True\n                self.completed_best_performance = self.trial_history[-1]\n            logger.info('Updated complted best performance, trial job id: %s', trial_job_id)\n        else:\n            logger.info('No need to update, trial job id: %s', trial_job_id)",
  "def assess_trial(self, trial_job_id, trial_history):\n        \"\"\"assess whether a trial should be early stop by curve fitting algorithm\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n\n        Returns\n        -------\n        bool\n            AssessResult.Good or AssessResult.Bad\n\n        Raises\n        ------\n        Exception\n            unrecognize exception in curvefitting_assessor\n        \"\"\"\n        scalar_trial_history = extract_scalar_history(trial_history)\n        self.trial_history = scalar_trial_history\n        if not self.set_best_performance:\n            return AssessResult.Good\n        curr_step = len(scalar_trial_history)\n        if curr_step < self.start_step:\n            return AssessResult.Good\n\n        if trial_job_id in self.last_judgment_num.keys() and curr_step - self.last_judgment_num[trial_job_id] < self.gap:\n            return AssessResult.Good\n        self.last_judgment_num[trial_job_id] = curr_step\n\n        try:\n            start_time = datetime.datetime.now()\n            # Predict the final result\n            curvemodel = CurveModel(self.target_pos)\n            predict_y = curvemodel.predict(scalar_trial_history)\n            log_message = \"Prediction done. Trial job id = {}, Predict value = {}\".format(trial_job_id, predict_y)\n            if predict_y is None:\n                logger.info('%s, wait for more information to predict precisely', log_message)\n                return AssessResult.Good\n            else:\n                logger.info(log_message)\n            standard_performance = self.completed_best_performance * self.threshold\n\n            end_time = datetime.datetime.now()\n            if (end_time - start_time).seconds > 60:\n                logger.warning(\n                    'Curve Fitting Assessor Runtime Exceeds 60s, Trial Id = %s Trial History = %s',\n                    trial_job_id, self.trial_history\n                )\n\n            if predict_y > standard_performance:\n                return AssessResult.Good\n            return AssessResult.Bad\n\n        except Exception as exception:\n            logger.exception('unrecognize exception in curvefitting_assessor %s', exception)",
  "def vap(x, a, b, c):\n    \"\"\"Vapor pressure model\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    b : float\n    c : float\n\n    Returns\n    -------\n    float\n        np.exp(a+b/x+c*np.log(x))\n    \"\"\"\n    return np.exp(a+b/x+c*np.log(x))",
  "def pow3(x, c, a, alpha):\n    \"\"\"pow3\n\n    Parameters\n    ----------\n    x : int\n    c : float\n    a : float\n    alpha : float\n\n    Returns\n    -------\n    float\n        c - a * x**(-alpha)\n    \"\"\"\n    return c - a * x**(-alpha)",
  "def linear(x, a, b):\n    \"\"\"linear\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    b : float\n\n    Returns\n    -------\n    float\n        a*x + b\n    \"\"\"\n    return a*x + b",
  "def logx_linear(x, a, b):\n    \"\"\"logx linear\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    b : float\n\n    Returns\n    -------\n    float\n        a * np.log(x) + b\n    \"\"\"\n    x = np.log(x)\n    return a*x + b",
  "def dr_hill_zero_background(x, theta, eta, kappa):\n    \"\"\"dr hill zero background\n\n    Parameters\n    ----------\n    x : int\n    theta : float\n    eta : float\n    kappa : float\n\n    Returns\n    -------\n    float\n        (theta* x**eta) / (kappa**eta + x**eta)\n    \"\"\"\n    return (theta * x**eta) / (kappa**eta + x**eta)",
  "def log_power(x, a, b, c):\n    \"\"\"\"logistic power\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    b : float\n    c : float\n\n    Returns\n    -------\n    float\n        a/(1.+(x/np.exp(b))**c)\n    \"\"\"\n    return a/(1.+(x/np.exp(b))**c)",
  "def pow4(x, alpha, a, b, c):\n    \"\"\"pow4\n\n    Parameters\n    ----------\n    x : int\n    alpha : float\n    a : float\n    b : float\n    c : float\n\n    Returns\n    -------\n    float\n        c - (a*x+b)**-alpha\n    \"\"\"\n    return c - (a*x+b)**-alpha",
  "def mmf(x, alpha, beta, kappa, delta):\n    \"\"\"Morgan-Mercer-Flodin\n    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n\n    Parameters\n    ----------\n    x : int\n    alpha : float\n    beta : float\n    kappa : float\n    delta : float\n\n    Returns\n    -------\n    float\n        alpha - (alpha - beta) / (1. + (kappa * x)**delta)\n    \"\"\"\n    return alpha - (alpha - beta) / (1. + (kappa * x)**delta)",
  "def exp4(x, c, a, b, alpha):\n    \"\"\"exp4\n\n    Parameters\n    ----------\n    x : int\n    c : float\n    a : float\n    b : float\n    alpha : float\n\n    Returns\n    -------\n    float\n        c - np.exp(-a*(x**alpha)+b)\n    \"\"\"\n    return c - np.exp(-a*(x**alpha)+b)",
  "def ilog2(x, c, a):\n    \"\"\"ilog2\n\n    Parameters\n    ----------\n    x : int\n    c : float\n    a : float\n\n    Returns\n    -------\n    float\n        c - a / np.log(x)\n    \"\"\"\n    return c - a / np.log(x)",
  "def weibull(x, alpha, beta, kappa, delta):\n    \"\"\"Weibull model\n    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n\n    Parameters\n    ----------\n    x : int\n    alpha : float\n    beta : float\n    kappa : float\n    delta : float\n\n    Returns\n    -------\n    float\n        alpha - (alpha - beta) * np.exp(-(kappa * x)**delta)\n    \"\"\"\n    return alpha - (alpha - beta) * np.exp(-(kappa * x)**delta)",
  "def janoschek(x, a, beta, k, delta):\n    \"\"\"http://www.pisces-conservation.com/growthhelp/janoschek.htm\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    beta : float\n    k : float\n    delta : float\n\n    Returns\n    -------\n    float\n        a - (a - beta) * np.exp(-k*x**delta)\n    \"\"\"\n    return a - (a - beta) * np.exp(-k*x**delta)",
  "class CurveModel:\n    \"\"\"Build a Curve Model to predict the performance\n\n    Algorithm: https://github.com/Microsoft/nni/blob/master/src/sdk/pynni/nni/curvefitting_assessor/README.md\n\n    Parameters\n    ----------\n    target_pos : int\n        The point we need to predict\n    \"\"\"\n    def __init__(self, target_pos):\n        self.target_pos = target_pos\n        self.trial_history = []\n        self.point_num = 0\n        self.effective_model = []\n        self.effective_model_num = 0\n        self.weight_samples = []\n\n    def fit_theta(self):\n        \"\"\"use least squares to fit all default curves parameter seperately\n\n        Returns\n        -------\n        None\n        \"\"\"\n        x = range(1, self.point_num + 1)\n        y = self.trial_history\n        for i in range(NUM_OF_FUNCTIONS):\n            model = curve_combination_models[i]\n            try:\n                # The maximum number of iterations to fit is 100*(N+1), where N is the number of elements in `x0`.\n                if model_para_num[model] == 2:\n                    a, b = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                elif model_para_num[model] == 3:\n                    a, b, c = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                elif model_para_num[model] == 4:\n                    a, b, c, d = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                    model_para[model][3] = d\n            except (RuntimeError, FloatingPointError, OverflowError, ZeroDivisionError):\n                # Ignore exceptions caused by numerical calculations\n                pass\n            except Exception as exception:\n                logger.critical(\"Exceptions in fit_theta: %s\", exception)\n\n    def filter_curve(self):\n        \"\"\"filter the poor performing curve\n\n        Returns\n        -------\n        None\n        \"\"\"\n        avg = np.sum(self.trial_history) / self.point_num\n        standard = avg * avg * self.point_num\n        predict_data = []\n        tmp_model = []\n        for i in range(NUM_OF_FUNCTIONS):\n            var = 0\n            model = curve_combination_models[i]\n            for j in range(1, self.point_num + 1):\n                y = self.predict_y(model, j)\n                var += (y - self.trial_history[j - 1]) * (y - self.trial_history[j - 1])\n            if var < standard:\n                predict_data.append(y)\n                tmp_model.append(curve_combination_models[i])\n        median = np.median(predict_data)\n        std = np.std(predict_data)\n        for model in tmp_model:\n            y = self.predict_y(model, self.target_pos)\n            epsilon = self.point_num / 10 * std\n            if y < median + epsilon and y > median - epsilon:\n                self.effective_model.append(model)\n        self.effective_model_num = len(self.effective_model)\n        logger.info('List of effective model: %s', self.effective_model)\n\n    def predict_y(self, model, pos):\n        \"\"\"return the predict y of 'model' when epoch = pos\n\n        Parameters\n        ----------\n        model : string\n            name of the curve function model\n        pos : int\n            the epoch number of the position you want to predict\n\n        Returns\n        -------\n        int\n            The expected matrix at pos\n        \"\"\"\n        if model_para_num[model] == 2:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1])\n        elif model_para_num[model] == 3:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1], model_para[model][2])\n        elif model_para_num[model] == 4:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1], model_para[model][2], model_para[model][3])\n        return y\n\n    def f_comb(self, pos, sample):\n        \"\"\"return the value of the f_comb when epoch = pos\n\n        Parameters\n        ----------\n        pos : int\n            the epoch number of the position you want to predict\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        int\n            The expected matrix at pos with all the active function's prediction\n        \"\"\"\n        ret = 0\n        for i in range(self.effective_model_num):\n            model = self.effective_model[i]\n            y = self.predict_y(model, pos)\n            ret += sample[i] * y\n        return ret\n\n    def normalize_weight(self, samples):\n        \"\"\"normalize weight\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        list\n            samples after normalize weight\n        \"\"\"\n        for i in range(NUM_OF_INSTANCE):\n            total = 0\n            for j in range(self.effective_model_num):\n                total += samples[i][j]\n            for j in range(self.effective_model_num):\n                samples[i][j] /= total\n        return samples\n\n    def sigma_sq(self, sample):\n        \"\"\"returns the value of sigma square, given the weight's sample\n\n        Parameters\n        ----------\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            the value of sigma square, given the weight's sample\n        \"\"\"\n        ret = 0\n        for i in range(1, self.point_num + 1):\n            temp = self.trial_history[i - 1] - self.f_comb(i, sample)\n            ret += temp * temp\n        return 1.0 * ret / self.point_num\n\n    def normal_distribution(self, pos, sample):\n        \"\"\"returns the value of normal distribution, given the weight's sample and target position\n\n        Parameters\n        ----------\n        pos : int\n            the epoch number of the position you want to predict\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            the value of normal distribution\n        \"\"\"\n        curr_sigma_sq = self.sigma_sq(sample)\n        delta = self.trial_history[pos - 1] - self.f_comb(pos, sample)\n        return np.exp(np.square(delta) / (-2.0 * curr_sigma_sq)) / np.sqrt(2 * np.pi * np.sqrt(curr_sigma_sq))\n\n    def likelihood(self, samples):\n        \"\"\"likelihood\n\n        Parameters\n        ----------\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            likelihood\n        \"\"\"\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            for j in range(1, self.point_num + 1):\n                ret[i] *= self.normal_distribution(j, samples[i])\n        return ret\n\n    def prior(self, samples):\n        \"\"\"priori distribution\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        float\n            priori distribution\n        \"\"\"\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            for j in range(self.effective_model_num):\n                if not samples[i][j] > 0:\n                    ret[i] = 0\n            if self.f_comb(1, samples[i]) >= self.f_comb(self.target_pos, samples[i]):\n                ret[i] = 0\n        return ret\n\n    def target_distribution(self, samples):\n        \"\"\"posterior probability\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        float\n            posterior probability\n        \"\"\"\n        curr_likelihood = self.likelihood(samples)\n        curr_prior = self.prior(samples)\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            ret[i] = curr_likelihood[i] * curr_prior[i]\n        return ret\n\n    def mcmc_sampling(self):\n        \"\"\"Adjust the weight of each function using mcmc sampling.\n        The initial value of each weight is evenly distribute.\n        Brief introduction:\n        (1)Definition of sample:\n            Sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n        (2)Definition of samples:\n            Samples is a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n        (3)Definition of model:\n            Model is the function we chose right now. Such as: 'wap', 'weibull'.\n        (4)Definition of pos:\n            Pos is the position we want to predict, corresponds to the value of epoch.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        init_weight = np.ones((self.effective_model_num), dtype=np.float) / self.effective_model_num\n        self.weight_samples = np.broadcast_to(init_weight, (NUM_OF_INSTANCE, self.effective_model_num))\n        for _ in range(NUM_OF_SIMULATION_TIME):\n            # sample new value from Q(i, j)\n            new_values = np.random.randn(NUM_OF_INSTANCE, self.effective_model_num) * STEP_SIZE + self.weight_samples\n            new_values = self.normalize_weight(new_values)\n            # compute alpha(i, j) = min{1, P(j)Q(j, i)/P(i)Q(i, j)}\n            alpha = np.minimum(1, self.target_distribution(new_values) / self.target_distribution(self.weight_samples))\n            # sample u\n            u = np.random.rand(NUM_OF_INSTANCE)\n            # new value\n            change_value_flag = (u < alpha).astype(np.int)\n            for j in range(NUM_OF_INSTANCE):\n                new_values[j] = self.weight_samples[j] * (1 - change_value_flag[j]) + new_values[j] * change_value_flag[j]\n            self.weight_samples = new_values\n\n    def predict(self, trial_history):\n        \"\"\"predict the value of target position\n\n        Parameters\n        ----------\n        trial_history : list\n            The history performance matrix of each trial.\n\n        Returns\n        -------\n        float\n            expected final result performance of this hyperparameter config\n        \"\"\"\n        self.trial_history = trial_history\n        self.point_num = len(trial_history)\n        self.fit_theta()\n        self.filter_curve()\n        if self.effective_model_num < LEAST_FITTED_FUNCTION:\n            # different curve's predictions are too scattered, requires more information\n            return None\n        self.mcmc_sampling()\n        ret = 0\n        for i in range(NUM_OF_INSTANCE):\n            ret += self.f_comb(self.target_pos, self.weight_samples[i])\n        return ret / NUM_OF_INSTANCE",
  "def __init__(self, target_pos):\n        self.target_pos = target_pos\n        self.trial_history = []\n        self.point_num = 0\n        self.effective_model = []\n        self.effective_model_num = 0\n        self.weight_samples = []",
  "def fit_theta(self):\n        \"\"\"use least squares to fit all default curves parameter seperately\n\n        Returns\n        -------\n        None\n        \"\"\"\n        x = range(1, self.point_num + 1)\n        y = self.trial_history\n        for i in range(NUM_OF_FUNCTIONS):\n            model = curve_combination_models[i]\n            try:\n                # The maximum number of iterations to fit is 100*(N+1), where N is the number of elements in `x0`.\n                if model_para_num[model] == 2:\n                    a, b = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                elif model_para_num[model] == 3:\n                    a, b, c = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                elif model_para_num[model] == 4:\n                    a, b, c, d = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                    model_para[model][3] = d\n            except (RuntimeError, FloatingPointError, OverflowError, ZeroDivisionError):\n                # Ignore exceptions caused by numerical calculations\n                pass\n            except Exception as exception:\n                logger.critical(\"Exceptions in fit_theta: %s\", exception)",
  "def filter_curve(self):\n        \"\"\"filter the poor performing curve\n\n        Returns\n        -------\n        None\n        \"\"\"\n        avg = np.sum(self.trial_history) / self.point_num\n        standard = avg * avg * self.point_num\n        predict_data = []\n        tmp_model = []\n        for i in range(NUM_OF_FUNCTIONS):\n            var = 0\n            model = curve_combination_models[i]\n            for j in range(1, self.point_num + 1):\n                y = self.predict_y(model, j)\n                var += (y - self.trial_history[j - 1]) * (y - self.trial_history[j - 1])\n            if var < standard:\n                predict_data.append(y)\n                tmp_model.append(curve_combination_models[i])\n        median = np.median(predict_data)\n        std = np.std(predict_data)\n        for model in tmp_model:\n            y = self.predict_y(model, self.target_pos)\n            epsilon = self.point_num / 10 * std\n            if y < median + epsilon and y > median - epsilon:\n                self.effective_model.append(model)\n        self.effective_model_num = len(self.effective_model)\n        logger.info('List of effective model: %s', self.effective_model)",
  "def predict_y(self, model, pos):\n        \"\"\"return the predict y of 'model' when epoch = pos\n\n        Parameters\n        ----------\n        model : string\n            name of the curve function model\n        pos : int\n            the epoch number of the position you want to predict\n\n        Returns\n        -------\n        int\n            The expected matrix at pos\n        \"\"\"\n        if model_para_num[model] == 2:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1])\n        elif model_para_num[model] == 3:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1], model_para[model][2])\n        elif model_para_num[model] == 4:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1], model_para[model][2], model_para[model][3])\n        return y",
  "def f_comb(self, pos, sample):\n        \"\"\"return the value of the f_comb when epoch = pos\n\n        Parameters\n        ----------\n        pos : int\n            the epoch number of the position you want to predict\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        int\n            The expected matrix at pos with all the active function's prediction\n        \"\"\"\n        ret = 0\n        for i in range(self.effective_model_num):\n            model = self.effective_model[i]\n            y = self.predict_y(model, pos)\n            ret += sample[i] * y\n        return ret",
  "def normalize_weight(self, samples):\n        \"\"\"normalize weight\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        list\n            samples after normalize weight\n        \"\"\"\n        for i in range(NUM_OF_INSTANCE):\n            total = 0\n            for j in range(self.effective_model_num):\n                total += samples[i][j]\n            for j in range(self.effective_model_num):\n                samples[i][j] /= total\n        return samples",
  "def sigma_sq(self, sample):\n        \"\"\"returns the value of sigma square, given the weight's sample\n\n        Parameters\n        ----------\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            the value of sigma square, given the weight's sample\n        \"\"\"\n        ret = 0\n        for i in range(1, self.point_num + 1):\n            temp = self.trial_history[i - 1] - self.f_comb(i, sample)\n            ret += temp * temp\n        return 1.0 * ret / self.point_num",
  "def normal_distribution(self, pos, sample):\n        \"\"\"returns the value of normal distribution, given the weight's sample and target position\n\n        Parameters\n        ----------\n        pos : int\n            the epoch number of the position you want to predict\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            the value of normal distribution\n        \"\"\"\n        curr_sigma_sq = self.sigma_sq(sample)\n        delta = self.trial_history[pos - 1] - self.f_comb(pos, sample)\n        return np.exp(np.square(delta) / (-2.0 * curr_sigma_sq)) / np.sqrt(2 * np.pi * np.sqrt(curr_sigma_sq))",
  "def likelihood(self, samples):\n        \"\"\"likelihood\n\n        Parameters\n        ----------\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            likelihood\n        \"\"\"\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            for j in range(1, self.point_num + 1):\n                ret[i] *= self.normal_distribution(j, samples[i])\n        return ret",
  "def prior(self, samples):\n        \"\"\"priori distribution\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        float\n            priori distribution\n        \"\"\"\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            for j in range(self.effective_model_num):\n                if not samples[i][j] > 0:\n                    ret[i] = 0\n            if self.f_comb(1, samples[i]) >= self.f_comb(self.target_pos, samples[i]):\n                ret[i] = 0\n        return ret",
  "def target_distribution(self, samples):\n        \"\"\"posterior probability\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        float\n            posterior probability\n        \"\"\"\n        curr_likelihood = self.likelihood(samples)\n        curr_prior = self.prior(samples)\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            ret[i] = curr_likelihood[i] * curr_prior[i]\n        return ret",
  "def mcmc_sampling(self):\n        \"\"\"Adjust the weight of each function using mcmc sampling.\n        The initial value of each weight is evenly distribute.\n        Brief introduction:\n        (1)Definition of sample:\n            Sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n        (2)Definition of samples:\n            Samples is a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n        (3)Definition of model:\n            Model is the function we chose right now. Such as: 'wap', 'weibull'.\n        (4)Definition of pos:\n            Pos is the position we want to predict, corresponds to the value of epoch.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        init_weight = np.ones((self.effective_model_num), dtype=np.float) / self.effective_model_num\n        self.weight_samples = np.broadcast_to(init_weight, (NUM_OF_INSTANCE, self.effective_model_num))\n        for _ in range(NUM_OF_SIMULATION_TIME):\n            # sample new value from Q(i, j)\n            new_values = np.random.randn(NUM_OF_INSTANCE, self.effective_model_num) * STEP_SIZE + self.weight_samples\n            new_values = self.normalize_weight(new_values)\n            # compute alpha(i, j) = min{1, P(j)Q(j, i)/P(i)Q(i, j)}\n            alpha = np.minimum(1, self.target_distribution(new_values) / self.target_distribution(self.weight_samples))\n            # sample u\n            u = np.random.rand(NUM_OF_INSTANCE)\n            # new value\n            change_value_flag = (u < alpha).astype(np.int)\n            for j in range(NUM_OF_INSTANCE):\n                new_values[j] = self.weight_samples[j] * (1 - change_value_flag[j]) + new_values[j] * change_value_flag[j]\n            self.weight_samples = new_values",
  "def predict(self, trial_history):\n        \"\"\"predict the value of target position\n\n        Parameters\n        ----------\n        trial_history : list\n            The history performance matrix of each trial.\n\n        Returns\n        -------\n        float\n            expected final result performance of this hyperparameter config\n        \"\"\"\n        self.trial_history = trial_history\n        self.point_num = len(trial_history)\n        self.fit_theta()\n        self.filter_curve()\n        if self.effective_model_num < LEAST_FITTED_FUNCTION:\n            # different curve's predictions are too scattered, requires more information\n            return None\n        self.mcmc_sampling()\n        ret = 0\n        for i in range(NUM_OF_INSTANCE):\n            ret += self.f_comb(self.target_pos, self.weight_samples[i])\n        return ret / NUM_OF_INSTANCE",
  "class LayerInfo:\n    \"\"\"\n    This structure contains all infomation needed to compress a TensorFlow ``Layer``.\n\n\n    Attributes\n    ----------\n    layer : tf.keras.layers.Layer\n        The layer.\n    name : str\n        The layer's name. Note that it's local to sub-model and may differ from its attribute name.\n    type : str\n        Name of the layer's class.\n    path : list of str or tuple of (str, int)\n        The layer object's and its parents' attribute name / list index.\n        For example, if the path is `[('cells', 2), 'conv']`, then the layer can be accessed as `model.cells[2].conv`.\n    config : JSON object\n        Selected configuration for this layer. The format is detailed in tutorial.\n\n    Parameters\n    ----------\n    layer : tf.keras.layers.Layer\n        See attributes section.\n    path : list of str or tuple of (str, int)\n        See attributes section.\n    \"\"\"\n\n    def __init__(self, layer, path=None):\n        self.layer = layer\n        self.name = layer.name\n        self.type = type(layer).__name__\n        self.path = path\n        self.config = None",
  "class Compressor:\n    \"\"\"\n    Common base class for all compressors.\n\n    This class is designed for other base classes.\n    Algorithms should inherit ``Pruner`` or ``Quantizer`` instead.\n\n\n    Attributes\n    ----------\n    bound_model : tf.keras.Model\n        Compressed user model.\n    wrappers : list of tf.keras.Model\n        A wrapper is an instrumented TF ``Layer``, in ``Model`` format.\n        The list is ordered by preorder traversal.\n\n    Parameters\n    ----------\n    LayerWrapperClass : a class derive from Model\n        The class used to instrument layers.\n    model : tf.keras.Model\n        The user model to be compressed.\n    config_list : list of JSON object\n        User configuration. The format is detailed in tutorial.\n    \"\"\"\n\n    def __init__(self, LayerWrapperClass, model, config_list):\n        assert isinstance(model, tf.keras.Model)\n        if isinstance(model, tf.keras.Sequential):\n            raise ValueError('NNI model compression does not support `Sequential` model for now')\n        self.validate_config(model, config_list)\n\n        self.bound_model = model\n        self.wrappers = []\n\n        for layer_info in _detect_layers_to_compress(model, config_list):\n            self.wrappers.append(LayerWrapperClass(layer_info, self))\n        if not self.wrappers:\n            _logger.warning('Nothing is configured to compress, please check your model and config list')\n\n        _instrument_model(model, self.wrappers)\n\n    def set_wrappers_attribute(self, name, value):\n        \"\"\"\n        Call ``setattr`` on all wrappers.\n        \"\"\"\n        for wrapper in self.wrappers:\n            setattr(wrapper, name, value)",
  "class Pruner(Compressor):\n    \"\"\"\n    Base class for pruning algorithms.\n\n    End users should use ``compress`` and callback APIs (WIP) to prune their models.\n\n    The underlying model is instrumented upon initialization of pruner object.\n    So if you want to pre-train the model, train it before creating pruner object.\n\n    The compressed model can only execute in eager mode.\n\n    Algorithm developers should override ``calc_masks`` method to specify pruning strategy.\n\n    Parameters\n    ----------\n    model : tf.keras.Model\n        The user model to prune.\n    config_list : list of JSON object\n        User configuration. The format is detailed in tutorial.\n    \"\"\"\n    def __init__(self, model, config_list):\n        super().__init__(PrunerLayerWrapper, model, config_list)\n        #self.callback = PrunerCallback(self)\n\n    def compress(self):\n        \"\"\"\n        Apply compression on a pre-trained model.\n\n        If you want to prune the model during training, use callback API (WIP) instead.\n\n        Returns\n        -------\n        tf.keras.Model\n            The compressed model, for convenience. This is exactly the same object to constructor argument.\n        \"\"\"\n        self._update_mask()\n        return self.bound_model\n\n    def calc_masks(self, wrapper, **kwargs):\n        \"\"\"\n        Abstract method to be overridden by algorithm. End users should ignore it.\n\n        If the callback is set up, this method will be invoked at end of each training minibatch.\n        If not, it will only be called when end user invokes ``compress``.\n\n        Parameters\n        ----------\n        wrapper : PrunerLayerWrapper\n            The instrumented layer.\n        **kwargs\n            Reserved for forward compatibility.\n\n        Returns\n        -------\n        dict of (str, tf.Tensor), or None\n            The key is weight ``Variable``'s name. The value is a mask ``Tensor`` of weight's shape and dtype.\n            If a weight's key does not appear in the return value, that weight will not be pruned.\n            Returning ``None`` means the mask is not changed since last time.\n            Weight names are globally unique, e.g. `model/conv_1/kernel:0`.\n        \"\"\"\n        # TODO: maybe it should be able to calc on weight-granularity, beside from layer-granularity\n        raise NotImplementedError(\"Pruners must overload calc_masks()\")\n\n    def _update_mask(self):\n        for wrapper_idx, wrapper in enumerate(self.wrappers):\n            masks = self.calc_masks(wrapper, wrapper_idx=wrapper_idx)\n            if masks is not None:\n                wrapper.masks = masks",
  "class PrunerLayerWrapper(tf.keras.Model):\n    \"\"\"\n    Instrumented TF layer.\n\n    Wrappers will be passed to pruner's ``calc_masks`` API,\n    and the pruning algorithm should use wrapper's attributes to calculate masks.\n\n    Once instrumented, underlying layer's weights will get **modified** by masks before forward pass.\n\n    Attributes\n    ----------\n    layer_info : LayerInfo\n        All static information of the original layer.\n    layer : tf.keras.layers.Layer\n        The original layer.\n    config : JSON object\n        Selected configuration. The format is detailed in tutorial.\n    pruner : Pruner\n        Bound pruner object.\n    masks : dict of (str, tf.Tensor)\n        Current masks. The key is weight's name and the value is mask tensor.\n        On initialization, `masks` is an empty dict, which means no weight is pruned.\n        Afterwards, `masks` is the last return value of ``Pruner.calc_masks``.\n        See ``Pruner.calc_masks`` for details.\n    \"\"\"\n    def __init__(self, layer_info, pruner):\n        super().__init__()\n        self.layer_info = layer_info\n        self.layer = layer_info.layer\n        self.config = layer_info.config\n        self.pruner = pruner\n        self.masks = {}\n        _logger.info('Layer detected to compress: %s', self.layer.name)\n\n    def call(self, *inputs):\n        new_weights = []\n        for weight in self.layer.weights:\n            mask = self.masks.get(weight.name)\n            if mask is not None:\n                new_weights.append(tf.math.multiply(weight, mask))\n            else:\n                new_weights.append(weight)\n        if new_weights and not hasattr(new_weights[0], 'numpy'):\n            raise RuntimeError('NNI: Compressed model can only run in eager mode')\n        self.layer.set_weights([weight.numpy() for weight in new_weights])\n        return self.layer(*inputs)",
  "def _detect_layers_to_compress(model, config_list):\n    # Returns list of LayerInfo.\n    located_layers = _locate_layers(model)\n    ret = []\n    for layer in model.layers:\n        config = _select_config(LayerInfo(layer), config_list)\n        if config is not None:\n            if id(layer) not in located_layers:\n                _logger.error('Failed to locate layer %s in model. The layer will not be compressed. '\n                              'This is a bug in NNI, feel free to fire an issue.', layer.name)\n                continue\n            layer_info = located_layers[id(layer)]\n            layer_info.config = config\n            ret.append(layer_info)\n    return ret",
  "def _locate_layers(model, cur_path=[]):\n    # Find out how to access layers from model object.\n    # Returns dict of (layer's object ID, LayerInfo).\n    # This function is required because TF framework does not track layer's attribute name,\n    # and to my knowledge `Layer.name` is only useful for read-only access.\n    # `cur_path`s format is documented in `LayerInfo.path`.\n    # TODO: it can only find layers in `Model` and `list` for now.\n    assert isinstance(model, tf.keras.Model)\n    if isinstance(model, tf.keras.Sequential):\n        _logger.warning('`Sequential` model is not supported yet, ignored.')\n    ret = {}\n    for key, value in model.__dict__.items():\n        if isinstance(value, tf.keras.Model):\n            ret.update(_locate_layers(value, cur_path + [key]))\n        elif isinstance(value, tf.keras.layers.Layer):\n            ret[id(value)] = LayerInfo(value, cur_path + [key])\n        elif isinstance(value, list):\n            for i, item in enumerate(value):\n                if isinstance(item, tf.keras.Model):\n                    ret.update(_locate_layers(item, cur_path + [(key, i)]))\n                elif isinstance(item, tf.keras.layers.Layer):\n                    ret[id(item)] = LayerInfo(item, cur_path + [(key, i)])\n    return ret",
  "def _select_config(layer_info, config_list):\n    # Find the last matching config block for given layer.\n    # Returns None if the layer should not be compressed.\n    ret = None\n    for config in config_list:\n        if 'op_types' in config:\n            match = layer_info.type in config['op_types']\n            match_default = 'default' in config['op_types'] and layer_info.type in default_layers.weighted_modules\n            if not match and not match_default:\n                continue\n        if 'op_names' in config and layer_info.name not in config['op_names']:\n            continue\n        ret = config\n    if ret is None or 'exclude' in ret:\n        return None\n    return ret",
  "def _instrument_model(model, wrappers):\n    # Replace layers to wrappers\n    for wrapper in reversed(wrappers):\n        cur = model\n        for key in wrapper.layer_info.path[:-1]:\n            if isinstance(key, str):\n                cur = getattr(cur, key)\n            else:\n                name, index = key\n                cur = getattr(cur, name)[index]\n        key = wrapper.layer_info.path[-1]\n        if isinstance(key, str):\n            setattr(cur, key, wrapper)\n        else:\n            name, index = key\n            getattr(cur, name)[index] = wrapper",
  "def __init__(self, layer, path=None):\n        self.layer = layer\n        self.name = layer.name\n        self.type = type(layer).__name__\n        self.path = path\n        self.config = None",
  "def __init__(self, LayerWrapperClass, model, config_list):\n        assert isinstance(model, tf.keras.Model)\n        if isinstance(model, tf.keras.Sequential):\n            raise ValueError('NNI model compression does not support `Sequential` model for now')\n        self.validate_config(model, config_list)\n\n        self.bound_model = model\n        self.wrappers = []\n\n        for layer_info in _detect_layers_to_compress(model, config_list):\n            self.wrappers.append(LayerWrapperClass(layer_info, self))\n        if not self.wrappers:\n            _logger.warning('Nothing is configured to compress, please check your model and config list')\n\n        _instrument_model(model, self.wrappers)",
  "def set_wrappers_attribute(self, name, value):\n        \"\"\"\n        Call ``setattr`` on all wrappers.\n        \"\"\"\n        for wrapper in self.wrappers:\n            setattr(wrapper, name, value)",
  "def __init__(self, model, config_list):\n        super().__init__(PrunerLayerWrapper, model, config_list)",
  "def compress(self):\n        \"\"\"\n        Apply compression on a pre-trained model.\n\n        If you want to prune the model during training, use callback API (WIP) instead.\n\n        Returns\n        -------\n        tf.keras.Model\n            The compressed model, for convenience. This is exactly the same object to constructor argument.\n        \"\"\"\n        self._update_mask()\n        return self.bound_model",
  "def calc_masks(self, wrapper, **kwargs):\n        \"\"\"\n        Abstract method to be overridden by algorithm. End users should ignore it.\n\n        If the callback is set up, this method will be invoked at end of each training minibatch.\n        If not, it will only be called when end user invokes ``compress``.\n\n        Parameters\n        ----------\n        wrapper : PrunerLayerWrapper\n            The instrumented layer.\n        **kwargs\n            Reserved for forward compatibility.\n\n        Returns\n        -------\n        dict of (str, tf.Tensor), or None\n            The key is weight ``Variable``'s name. The value is a mask ``Tensor`` of weight's shape and dtype.\n            If a weight's key does not appear in the return value, that weight will not be pruned.\n            Returning ``None`` means the mask is not changed since last time.\n            Weight names are globally unique, e.g. `model/conv_1/kernel:0`.\n        \"\"\"\n        # TODO: maybe it should be able to calc on weight-granularity, beside from layer-granularity\n        raise NotImplementedError(\"Pruners must overload calc_masks()\")",
  "def _update_mask(self):\n        for wrapper_idx, wrapper in enumerate(self.wrappers):\n            masks = self.calc_masks(wrapper, wrapper_idx=wrapper_idx)\n            if masks is not None:\n                wrapper.masks = masks",
  "def __init__(self, layer_info, pruner):\n        super().__init__()\n        self.layer_info = layer_info\n        self.layer = layer_info.layer\n        self.config = layer_info.config\n        self.pruner = pruner\n        self.masks = {}\n        _logger.info('Layer detected to compress: %s', self.layer.name)",
  "def call(self, *inputs):\n        new_weights = []\n        for weight in self.layer.weights:\n            mask = self.masks.get(weight.name)\n            if mask is not None:\n                new_weights.append(tf.math.multiply(weight, mask))\n            else:\n                new_weights.append(weight)\n        if new_weights and not hasattr(new_weights[0], 'numpy'):\n            raise RuntimeError('NNI: Compressed model can only run in eager mode')\n        self.layer.set_weights([weight.numpy() for weight in new_weights])\n        return self.layer(*inputs)",
  "class OneshotPruner(Pruner):\n    def __init__(self, model, config_list, pruning_algorithm='level', **algo_kwargs):\n        super().__init__(model, config_list)\n        self.set_wrappers_attribute('calculated', False)\n        self.masker = MASKER_DICT[pruning_algorithm](model, self, **algo_kwargs)\n\n    def validate_config(self, model, config_list):\n        pass  # TODO\n\n    def calc_masks(self, wrapper, wrapper_idx=None):\n        if wrapper.calculated:\n            return None\n        sparsity = wrapper.config['sparsity']\n        masks = self.masker.calc_masks(sparsity, wrapper, wrapper_idx)\n        if masks is not None:\n            wrapper.calculated = True\n        return masks",
  "class LevelPruner(OneshotPruner):\n    def __init__(self, model, config_list):\n        super().__init__(model, config_list, pruning_algorithm='level')",
  "class WeightMasker:\n    def __init__(self, model, pruner, **kwargs):\n        self.model = model\n        self.pruner = pruner\n\n    def calc_masks(self, sparsity, wrapper, wrapper_idx=None):\n        raise NotImplementedError()",
  "class LevelPrunerMasker(WeightMasker):\n    def calc_masks(self, sparsity, wrapper, wrapper_idx=None):\n        masks = {}\n        for weight_variable in wrapper.layer.weights:\n            if 'bias' in weight_variable.name:\n                continue\n\n            num_prune = int(tf.size(weight_variable).numpy() * sparsity)\n            if num_prune == 0:\n                continue\n\n            weight = weight_variable.read_value()\n            if wrapper.masks.get(weight_variable.name) is not None:\n                weight = tf.math.multiply(weight, wrapper.masks[weight_variable.name])\n\n            w_abs = tf.math.abs(weight)\n            k = tf.size(weight) - num_prune\n            topk = tf.math.top_k(tf.reshape(w_abs, [-1]), k)[0]\n            if tf.size(topk) == 0:\n                mask = tf.zeros_like(weight)\n            else:\n                mask = tf.math.greater_equal(w_abs, topk[-1])\n            masks[weight_variable.name] = tf.cast(mask, weight.dtype)\n        return masks",
  "def __init__(self, model, config_list, pruning_algorithm='level', **algo_kwargs):\n        super().__init__(model, config_list)\n        self.set_wrappers_attribute('calculated', False)\n        self.masker = MASKER_DICT[pruning_algorithm](model, self, **algo_kwargs)",
  "def validate_config(self, model, config_list):\n        pass",
  "def calc_masks(self, wrapper, wrapper_idx=None):\n        if wrapper.calculated:\n            return None\n        sparsity = wrapper.config['sparsity']\n        masks = self.masker.calc_masks(sparsity, wrapper, wrapper_idx)\n        if masks is not None:\n            wrapper.calculated = True\n        return masks",
  "def __init__(self, model, config_list):\n        super().__init__(model, config_list, pruning_algorithm='level')",
  "def __init__(self, model, pruner, **kwargs):\n        self.model = model\n        self.pruner = pruner",
  "def calc_masks(self, sparsity, wrapper, wrapper_idx=None):\n        raise NotImplementedError()",
  "def calc_masks(self, sparsity, wrapper, wrapper_idx=None):\n        masks = {}\n        for weight_variable in wrapper.layer.weights:\n            if 'bias' in weight_variable.name:\n                continue\n\n            num_prune = int(tf.size(weight_variable).numpy() * sparsity)\n            if num_prune == 0:\n                continue\n\n            weight = weight_variable.read_value()\n            if wrapper.masks.get(weight_variable.name) is not None:\n                weight = tf.math.multiply(weight, wrapper.masks[weight_variable.name])\n\n            w_abs = tf.math.abs(weight)\n            k = tf.size(weight) - num_prune\n            topk = tf.math.top_k(tf.reshape(w_abs, [-1]), k)[0]\n            if tf.size(topk) == 0:\n                mask = tf.zeros_like(weight)\n            else:\n                mask = tf.math.greater_equal(w_abs, topk[-1])\n            masks[weight_variable.name] = tf.cast(mask, weight.dtype)\n        return masks",
  "class LayerInfo:\n    def __init__(self, name, module):\n        self.module = module\n        self.name = name\n        self.type = type(module).__name__",
  "def _setattr(model, name, module):\n    name_list = name.split(\".\")\n    for name in name_list[:-1]:\n        model = getattr(model, name)\n    setattr(model, name_list[-1], module)",
  "class Compressor:\n    \"\"\"\n    Abstract base PyTorch compressor\n    \"\"\"\n\n    def __init__(self, model, config_list, optimizer=None):\n        \"\"\"\n        Record necessary info in class members\n\n        Parameters\n        ----------\n        model : pytorch model\n            the model user wants to compress\n        config_list : list\n            the configurations that users specify for compression\n        optimizer: pytorch optimizer\n            optimizer used to train the model\n        \"\"\"\n        assert isinstance(model, torch.nn.Module)\n        self.validate_config(model, config_list)\n\n        self.bound_model = model\n        self.config_list = config_list\n        self.optimizer = optimizer\n\n        self.modules_to_compress = None\n        self.modules_wrapper = []\n        self.is_wrapped = False\n\n        self._fwd_hook_handles = {}\n        self._fwd_hook_id = 0\n\n        self.reset()\n\n        if not self.modules_wrapper:\n            _logger.warning('Nothing is configured to compress, please check your model and config_list')\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        subclass can optionally implement this method to check if config_list if valid\n        \"\"\"\n        pass\n\n    def reset(self, checkpoint=None):\n        \"\"\"\n        reset model state dict and model wrapper\n        \"\"\"\n        self._unwrap_model()\n        if checkpoint is not None:\n            self.bound_model.load_state_dict(checkpoint)\n\n        self.modules_to_compress = None\n        self.modules_wrapper = []\n\n        for layer, config in self._detect_modules_to_compress():\n            wrapper = self._wrap_modules(layer, config)\n            self.modules_wrapper.append(wrapper)\n\n        self._wrap_model()\n\n    def _detect_modules_to_compress(self):\n        \"\"\"\n        detect all modules should be compressed, and save the result in `self.modules_to_compress`.\n        The model will be instrumented and user should never edit it after calling this method.\n        \"\"\"\n        if self.modules_to_compress is None:\n            self.modules_to_compress = []\n            for name, module in self.bound_model.named_modules():\n                if module == self.bound_model:\n                    continue\n                layer = LayerInfo(name, module)\n                config = self.select_config(layer)\n                if config is not None:\n                    self.modules_to_compress.append((layer, config))\n        return self.modules_to_compress\n\n    def _wrap_model(self):\n        \"\"\"\n        wrap all modules that needed to be compressed\n\n        \"\"\"\n        for wrapper in reversed(self.get_modules_wrapper()):\n            _setattr(self.bound_model, wrapper.name, wrapper)\n        self.is_wrapped = True\n\n    def _unwrap_model(self):\n        \"\"\"\n        unwrap all modules that needed to be compressed\n\n        \"\"\"\n        for wrapper in self.get_modules_wrapper():\n            _setattr(self.bound_model, wrapper.name, wrapper.module)\n        self.is_wrapped = False\n\n    def compress(self):\n        \"\"\"\n        Compress the model with algorithm implemented by subclass.\n\n        The model will be instrumented and user should never edit it after calling this method.\n        `self.modules_to_compress` records all the to-be-compressed layers\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        return self.bound_model\n\n    def set_wrappers_attribute(self, name, value):\n        \"\"\"\n        To register attributes used in wrapped module's forward method.\n        If the type of the value is Torch.tensor, then this value is registered as a buffer in wrapper,\n        which will be saved by model.state_dict. Otherwise, this value is just a regular variable in wrapper.\n\n        Parameters\n        ----------\n        name : str\n            name of the variable\n        value: any\n            value of the variable\n        \"\"\"\n        for wrapper in self.get_modules_wrapper():\n            if isinstance(value, torch.Tensor):\n                wrapper.register_buffer(name, value.clone())\n            else:\n                setattr(wrapper, name, value)\n\n    def get_modules_to_compress(self):\n        \"\"\"\n        To obtain all the to-be-compressed modules.\n\n        Returns\n        -------\n        list\n            a list of the layers, each of which is a tuple (`layer`, `config`),\n            `layer` is `LayerInfo`, `config` is a `dict`\n        \"\"\"\n        return self.modules_to_compress\n\n    def get_modules_wrapper(self):\n        \"\"\"\n        To obtain all the wrapped modules.\n\n        Returns\n        -------\n        list\n            a list of the wrapped modules\n        \"\"\"\n        return self.modules_wrapper\n\n    def select_config(self, layer):\n        \"\"\"\n        Find the configuration for `layer` by parsing `self.config_list`\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            one layer\n\n        Returns\n        -------\n        config or None\n            the retrieved configuration for this layer, if None, this layer should\n            not be compressed\n        \"\"\"\n        ret = None\n        for config in self.config_list:\n            config = config.copy()\n            # expand config if key `default` is in config['op_types']\n            if 'op_types' in config and 'default' in config['op_types']:\n                expanded_op_types = []\n                for op_type in config['op_types']:\n                    if op_type == 'default':\n                        expanded_op_types.extend(default_layers.weighted_modules)\n                    else:\n                        expanded_op_types.append(op_type)\n                config['op_types'] = expanded_op_types\n\n            # check if condition is satisified\n            if 'op_types' in config and layer.type not in config['op_types']:\n                continue\n            if 'op_names' in config and layer.name not in config['op_names']:\n                continue\n\n            ret = config\n        if ret is None or 'exclude' in ret:\n            return None\n        return ret\n\n    def update_epoch(self, epoch):\n        \"\"\"\n        If user want to update model every epoch, user can override this method.\n        This method should be called at the beginning of each epoch\n\n        Parameters\n        ----------\n        epoch : num\n            the current epoch number\n        \"\"\"\n        pass\n\n    def _wrap_modules(self, layer, config):\n        \"\"\"\n        This method is implemented in the subclasses, i.e., `Pruner` and `Quantizer`\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the compression operation\n        config : dict\n            the configuration for compressing this layer\n        \"\"\"\n        raise NotImplementedError()\n\n\n    def add_activation_collector(self, collector):\n        self._fwd_hook_id += 1\n        self._fwd_hook_handles[self._fwd_hook_id] = []\n        for wrapper in self.get_modules_wrapper():\n            handle = wrapper.register_forward_hook(collector)\n            self._fwd_hook_handles[self._fwd_hook_id].append(handle)\n        return self._fwd_hook_id\n\n    def remove_activation_collector(self, fwd_hook_id):\n        if fwd_hook_id not in self._fwd_hook_handles:\n            raise ValueError(\"%s is not a valid collector id\" % str(fwd_hook_id))\n        for handle in self._fwd_hook_handles[fwd_hook_id]:\n            handle.remove()\n        del self._fwd_hook_handles[fwd_hook_id]\n\n    def patch_optimizer(self, *tasks):\n        def patch_step(old_step):\n            def new_step(_, *args, **kwargs):\n                # call origin optimizer step method\n                output = old_step(*args, **kwargs)\n                # calculate mask\n                for task in tasks:\n                    task()\n                return output\n            return new_step\n        if self.optimizer is not None:\n            self.optimizer.step = types.MethodType(patch_step(self.optimizer.step), self.optimizer)",
  "class PrunerModuleWrapper(torch.nn.Module):\n    def __init__(self, module, module_name, module_type, config, pruner):\n        \"\"\"\n        Wrap an module to enable data parallel, forward method customization and buffer registeration.\n\n        Parameters\n        ----------\n        module : pytorch module\n            the module user wants to compress\n        config : dict\n            the configurations that users specify for compression\n        module_name : str\n            the name of the module to compress, wrapper module shares same name\n        module_type : str\n            the type of the module to compress\n        pruner \uff1a Pruner\n            the pruner used to calculate mask\n        \"\"\"\n        super().__init__()\n        # origin layer information\n        self.module = module\n        self.name = module_name\n        self.type = module_type\n        # config and pruner\n        self.config = config\n        self.pruner = pruner\n\n        # register buffer for mask\n        self.register_buffer(\"weight_mask\", torch.ones(self.module.weight.shape))\n        if hasattr(self.module, 'bias') and self.module.bias is not None:\n            self.register_buffer(\"bias_mask\", torch.ones(self.module.bias.shape))\n        else:\n            self.register_buffer(\"bias_mask\", None)\n\n    def forward(self, *inputs):\n        # apply mask to weight, bias\n        self.module.weight.data = self.module.weight.data.mul_(self.weight_mask)\n        if hasattr(self.module, 'bias') and self.module.bias is not None:\n            self.module.bias.data = self.module.bias.data.mul_(self.bias_mask)\n        return self.module(*inputs)",
  "class Pruner(Compressor):\n    \"\"\"\n    Prune to an exact pruning level specification\n\n    Attributes\n    ----------\n    mask_dict : dict\n        Dictionary for saving masks, `key` should be layer name and\n        `value` should be a tensor which has the same shape with layer's weight\n\n    \"\"\"\n\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        if optimizer is not None:\n            self.patch_optimizer(self.update_mask)\n\n    def compress(self):\n        self.update_mask()\n        return self.bound_model\n\n    def update_mask(self):\n        for wrapper_idx, wrapper in enumerate(self.get_modules_wrapper()):\n            masks = self.calc_mask(wrapper, wrapper_idx=wrapper_idx)\n            if masks is not None:\n                for k in masks:\n                    assert hasattr(wrapper, k), \"there is no attribute '%s' in wrapper\" % k\n                    setattr(wrapper, k, masks[k])\n\n    def calc_mask(self, wrapper, **kwargs):\n        \"\"\"\n        Pruners should overload this method to provide mask for weight tensors.\n        The mask must have the same shape and type comparing to the weight.\n        It will be applied with `mul()` operation on the weight.\n        This method is effectively hooked to `forward()` method of the model.\n\n        Parameters\n        ----------\n        wrapper : Module\n            calculate mask for `wrapper.module`'s weight\n        \"\"\"\n        raise NotImplementedError(\"Pruners must overload calc_mask()\")\n\n    def _wrap_modules(self, layer, config):\n        \"\"\"\n        Create a wrapper module to replace the original one.\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the mask\n        config : dict\n            the configuration for generating the mask\n        \"\"\"\n        _logger.debug(\"Module detected to compress : %s.\", layer.name)\n        wrapper = PrunerModuleWrapper(layer.module, layer.name, layer.type, config, self)\n        assert hasattr(layer.module, 'weight'), \"module %s does not have 'weight' attribute\" % layer.name\n        # move newly registered buffers to the same device of weight\n        wrapper.to(layer.module.weight.device)\n        return wrapper\n\n    def export_model(self, model_path, mask_path=None, onnx_path=None, input_shape=None, device=None):\n        \"\"\"\n        Export pruned model weights, masks and onnx model(optional)\n\n        Parameters\n        ----------\n        model_path : str\n            path to save pruned model state_dict\n        mask_path : str\n            (optional) path to save mask dict\n        onnx_path : str\n            (optional) path to save onnx model\n        input_shape : list or tuple\n            input shape to onnx model\n        device : torch.device\n            device of the model, used to place the dummy input tensor for exporting onnx file.\n            the tensor is placed on cpu if ```device``` is None\n        \"\"\"\n        assert model_path is not None, 'model_path must be specified'\n        mask_dict = {}\n        self._unwrap_model() # used for generating correct state_dict name without wrapper state\n\n        for wrapper in self.get_modules_wrapper():\n            weight_mask = wrapper.weight_mask\n            bias_mask = wrapper.bias_mask\n            if weight_mask is not None:\n                mask_sum = weight_mask.sum().item()\n                mask_num = weight_mask.numel()\n                _logger.debug('Layer: %s  Sparsity: %.4f', wrapper.name, 1 - mask_sum / mask_num)\n                wrapper.module.weight.data = wrapper.module.weight.data.mul(weight_mask)\n            if bias_mask is not None:\n                wrapper.module.bias.data = wrapper.module.bias.data.mul(bias_mask)\n            # save mask to dict\n            mask_dict[wrapper.name] = {\"weight\": weight_mask, \"bias\": bias_mask}\n\n        torch.save(self.bound_model.state_dict(), model_path)\n        _logger.info('Model state_dict saved to %s', model_path)\n        if mask_path is not None:\n            torch.save(mask_dict, mask_path)\n            _logger.info('Mask dict saved to %s', mask_path)\n        if onnx_path is not None:\n            assert input_shape is not None, 'input_shape must be specified to export onnx model'\n            # input info needed\n            if device is None:\n                device = torch.device('cpu')\n            input_data = torch.Tensor(*input_shape)\n            torch.onnx.export(self.bound_model, input_data.to(device), onnx_path)\n            _logger.info('Model in onnx with input shape %s saved to %s', input_data.shape, onnx_path)\n\n        self._wrap_model()\n\n    def load_model_state_dict(self, model_state):\n        \"\"\"\n        Load the state dict saved from unwrapped model.\n\n        Parameters:\n        -----------\n        model_state : dict\n            state dict saved from unwrapped model\n        \"\"\"\n        if self.is_wrapped:\n            self._unwrap_model()\n            self.bound_model.load_state_dict(model_state)\n            self._wrap_model()\n        else:\n            self.bound_model.load_state_dict(model_state)",
  "class QuantizerModuleWrapper(torch.nn.Module):\n    def __init__(self, module, module_name, module_type, config, quantizer):\n        \"\"\"\n        Wrap an module to enable data parallel, forward method customization and buffer registeration.\n\n        Parameters\n        ----------\n        module : pytorch module\n            the module user wants to compress\n        config : dict\n            the configurations that users specify for compression\n        module_name : str\n            the name of the module to compress, wrapper module shares same name\n        module_type : str\n            the type of the module to compress\n        quantizer \uff1aquantizer\n            the quantizer used to calculate mask\n        \"\"\"\n        super().__init__()\n        # origin layer information\n        self.module = module\n        self.name = module_name\n        self.type = module_type\n        # config and pruner\n        self.config = config\n        self.quantizer = quantizer\n\n        # register buffer and parameter\n        # old_weight is used to store origin weight and weight is used to store quantized weight\n        # the reason why weight is buffer instead of parameter is because in pytorch parameter is used as leaf\n        # if weight is leaf , then old_weight can not be updated.\n        if 'weight' in config['quant_types']:\n            if not _check_weight(self.module):\n                _logger.warning('Module %s does not have parameter \"weight\"', self.name)\n            else:\n                self.module.register_parameter('old_weight', torch.nn.Parameter(self.module.weight))\n                delattr(self.module, 'weight')\n                self.module.register_buffer('weight', self.module.old_weight)\n\n    def forward(self, *inputs):\n        if 'input' in self.config['quant_types']:\n            inputs = self.quantizer.quant_grad.apply(\n                inputs,\n                QuantType.QUANT_INPUT,\n                self)\n\n        if 'weight' in self.config['quant_types'] and _check_weight(self.module):\n            new_weight = self.quantizer.quant_grad.apply(\n                self.module.old_weight,\n                QuantType.QUANT_WEIGHT,\n                self)\n            self.module.weight = new_weight\n            result = self.module(*inputs)\n        else:\n            result = self.module(*inputs)\n\n        if 'output' in self.config['quant_types']:\n            result = self.quantizer.quant_grad.apply(\n                result,\n                QuantType.QUANT_OUTPUT,\n                self)\n        return result",
  "class Quantizer(Compressor):\n    \"\"\"\n    Base quantizer for pytorch quantizer\n    \"\"\"\n\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.quant_grad = QuantGrad\n        if self.optimizer is not None:\n            self.patch_optimizer(self.step_with_optimizer)\n            for wrapper in self.get_modules_wrapper():\n                if 'weight' in wrapper.config['quant_types']:\n                    # old_weight is registered to keep track of weight before quantization\n                    # and it is trainable, therefore, it should be added to optimizer.\n                    self.optimizer.add_param_group({\"params\": wrapper.module.old_weight})\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        \"\"\"\n        quantize should overload this method to quantize weight.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        weight : Tensor\n            weight that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        \"\"\"\n        raise NotImplementedError('Quantizer must overload quantize_weight()')\n\n    def quantize_output(self, output, wrapper, **kwargs):\n        \"\"\"\n        quantize should overload this method to quantize output.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        output : Tensor\n            output that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        \"\"\"\n        raise NotImplementedError('Quantizer must overload quantize_output()')\n\n    def quantize_input(self, *inputs, wrapper, **kwargs):\n        \"\"\"\n        quantize should overload this method to quantize input.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        inputs : Tensor\n            inputs that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        \"\"\"\n        raise NotImplementedError('Quantizer must overload quantize_input()')\n\n\n    def _wrap_modules(self, layer, config):\n        \"\"\"\n        Create a wrapper forward function to replace the original one.\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the mask\n        config : dict\n            the configuration for quantization\n        \"\"\"\n        assert 'quant_types' in config, 'must provide quant_types in config'\n        assert isinstance(config['quant_types'], list), 'quant_types must be list type'\n        assert 'quant_bits' in config, 'must provide quant_bits in config'\n        assert isinstance(config['quant_bits'], int) or isinstance(config['quant_bits'], dict), 'quant_bits must be dict type or int type'\n\n        if isinstance(config['quant_bits'], dict):\n            for quant_type in config['quant_types']:\n                assert quant_type in config['quant_bits'], 'bits length for %s must be specified in quant_bits dict' % quant_type\n\n        return QuantizerModuleWrapper(layer.module, layer.name, layer.type, config, self)\n\n    def step_with_optimizer(self):\n        pass",
  "class QuantType:\n    \"\"\"\n    Enum class for quantization type.\n    \"\"\"\n    QUANT_INPUT = 0\n    QUANT_WEIGHT = 1\n    QUANT_OUTPUT = 2",
  "class QuantGrad(torch.autograd.Function):\n    \"\"\"\n    Base class for overriding backward function of quantization operation.\n    \"\"\"\n    @staticmethod\n    def quant_backward(tensor, grad_output, quant_type):\n        \"\"\"\n        This method should be overrided by subclass to provide customized backward function,\n        default implementation is Straight-Through Estimator\n        Parameters\n        ----------\n        tensor : Tensor\n            input of quantization operation\n        grad_output : Tensor\n            gradient of the output of quantization operation\n        quant_type : QuantType\n            the type of quantization, it can be `QuantType.QUANT_INPUT`, `QuantType.QUANT_WEIGHT`, `QuantType.QUANT_OUTPUT`,\n            you can define different behavior for different types.\n        Returns\n        -------\n        tensor\n            gradient of the input of quantization operation\n        \"\"\"\n        return grad_output\n\n    @staticmethod\n    def forward(ctx, tensor, quant_type, wrapper, **kwargs):\n        ctx.save_for_backward(tensor, torch.Tensor([quant_type]))\n        if quant_type == QuantType.QUANT_INPUT:\n            return wrapper.quantizer.quantize_input(tensor, wrapper, **kwargs)\n        elif quant_type == QuantType.QUANT_WEIGHT:\n            return wrapper.quantizer.quantize_weight(tensor, wrapper, **kwargs)\n        elif quant_type == QuantType.QUANT_OUTPUT:\n            return wrapper.quantizer.quantize_output(tensor, wrapper, **kwargs)\n        else:\n            raise ValueError(\"unrecognized QuantType.\")\n\n    @classmethod\n    def backward(cls, ctx, grad_output):\n        tensor, quant_type = ctx.saved_variables\n        output = cls.quant_backward(tensor, grad_output, quant_type)\n        return output, None, None, None",
  "def _check_weight(module):\n    try:\n        return isinstance(module.weight.data, torch.Tensor)\n    except AttributeError:\n        return False",
  "def __init__(self, name, module):\n        self.module = module\n        self.name = name\n        self.type = type(module).__name__",
  "def __init__(self, model, config_list, optimizer=None):\n        \"\"\"\n        Record necessary info in class members\n\n        Parameters\n        ----------\n        model : pytorch model\n            the model user wants to compress\n        config_list : list\n            the configurations that users specify for compression\n        optimizer: pytorch optimizer\n            optimizer used to train the model\n        \"\"\"\n        assert isinstance(model, torch.nn.Module)\n        self.validate_config(model, config_list)\n\n        self.bound_model = model\n        self.config_list = config_list\n        self.optimizer = optimizer\n\n        self.modules_to_compress = None\n        self.modules_wrapper = []\n        self.is_wrapped = False\n\n        self._fwd_hook_handles = {}\n        self._fwd_hook_id = 0\n\n        self.reset()\n\n        if not self.modules_wrapper:\n            _logger.warning('Nothing is configured to compress, please check your model and config_list')",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        subclass can optionally implement this method to check if config_list if valid\n        \"\"\"\n        pass",
  "def reset(self, checkpoint=None):\n        \"\"\"\n        reset model state dict and model wrapper\n        \"\"\"\n        self._unwrap_model()\n        if checkpoint is not None:\n            self.bound_model.load_state_dict(checkpoint)\n\n        self.modules_to_compress = None\n        self.modules_wrapper = []\n\n        for layer, config in self._detect_modules_to_compress():\n            wrapper = self._wrap_modules(layer, config)\n            self.modules_wrapper.append(wrapper)\n\n        self._wrap_model()",
  "def _detect_modules_to_compress(self):\n        \"\"\"\n        detect all modules should be compressed, and save the result in `self.modules_to_compress`.\n        The model will be instrumented and user should never edit it after calling this method.\n        \"\"\"\n        if self.modules_to_compress is None:\n            self.modules_to_compress = []\n            for name, module in self.bound_model.named_modules():\n                if module == self.bound_model:\n                    continue\n                layer = LayerInfo(name, module)\n                config = self.select_config(layer)\n                if config is not None:\n                    self.modules_to_compress.append((layer, config))\n        return self.modules_to_compress",
  "def _wrap_model(self):\n        \"\"\"\n        wrap all modules that needed to be compressed\n\n        \"\"\"\n        for wrapper in reversed(self.get_modules_wrapper()):\n            _setattr(self.bound_model, wrapper.name, wrapper)\n        self.is_wrapped = True",
  "def _unwrap_model(self):\n        \"\"\"\n        unwrap all modules that needed to be compressed\n\n        \"\"\"\n        for wrapper in self.get_modules_wrapper():\n            _setattr(self.bound_model, wrapper.name, wrapper.module)\n        self.is_wrapped = False",
  "def compress(self):\n        \"\"\"\n        Compress the model with algorithm implemented by subclass.\n\n        The model will be instrumented and user should never edit it after calling this method.\n        `self.modules_to_compress` records all the to-be-compressed layers\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        return self.bound_model",
  "def set_wrappers_attribute(self, name, value):\n        \"\"\"\n        To register attributes used in wrapped module's forward method.\n        If the type of the value is Torch.tensor, then this value is registered as a buffer in wrapper,\n        which will be saved by model.state_dict. Otherwise, this value is just a regular variable in wrapper.\n\n        Parameters\n        ----------\n        name : str\n            name of the variable\n        value: any\n            value of the variable\n        \"\"\"\n        for wrapper in self.get_modules_wrapper():\n            if isinstance(value, torch.Tensor):\n                wrapper.register_buffer(name, value.clone())\n            else:\n                setattr(wrapper, name, value)",
  "def get_modules_to_compress(self):\n        \"\"\"\n        To obtain all the to-be-compressed modules.\n\n        Returns\n        -------\n        list\n            a list of the layers, each of which is a tuple (`layer`, `config`),\n            `layer` is `LayerInfo`, `config` is a `dict`\n        \"\"\"\n        return self.modules_to_compress",
  "def get_modules_wrapper(self):\n        \"\"\"\n        To obtain all the wrapped modules.\n\n        Returns\n        -------\n        list\n            a list of the wrapped modules\n        \"\"\"\n        return self.modules_wrapper",
  "def select_config(self, layer):\n        \"\"\"\n        Find the configuration for `layer` by parsing `self.config_list`\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            one layer\n\n        Returns\n        -------\n        config or None\n            the retrieved configuration for this layer, if None, this layer should\n            not be compressed\n        \"\"\"\n        ret = None\n        for config in self.config_list:\n            config = config.copy()\n            # expand config if key `default` is in config['op_types']\n            if 'op_types' in config and 'default' in config['op_types']:\n                expanded_op_types = []\n                for op_type in config['op_types']:\n                    if op_type == 'default':\n                        expanded_op_types.extend(default_layers.weighted_modules)\n                    else:\n                        expanded_op_types.append(op_type)\n                config['op_types'] = expanded_op_types\n\n            # check if condition is satisified\n            if 'op_types' in config and layer.type not in config['op_types']:\n                continue\n            if 'op_names' in config and layer.name not in config['op_names']:\n                continue\n\n            ret = config\n        if ret is None or 'exclude' in ret:\n            return None\n        return ret",
  "def update_epoch(self, epoch):\n        \"\"\"\n        If user want to update model every epoch, user can override this method.\n        This method should be called at the beginning of each epoch\n\n        Parameters\n        ----------\n        epoch : num\n            the current epoch number\n        \"\"\"\n        pass",
  "def _wrap_modules(self, layer, config):\n        \"\"\"\n        This method is implemented in the subclasses, i.e., `Pruner` and `Quantizer`\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the compression operation\n        config : dict\n            the configuration for compressing this layer\n        \"\"\"\n        raise NotImplementedError()",
  "def add_activation_collector(self, collector):\n        self._fwd_hook_id += 1\n        self._fwd_hook_handles[self._fwd_hook_id] = []\n        for wrapper in self.get_modules_wrapper():\n            handle = wrapper.register_forward_hook(collector)\n            self._fwd_hook_handles[self._fwd_hook_id].append(handle)\n        return self._fwd_hook_id",
  "def remove_activation_collector(self, fwd_hook_id):\n        if fwd_hook_id not in self._fwd_hook_handles:\n            raise ValueError(\"%s is not a valid collector id\" % str(fwd_hook_id))\n        for handle in self._fwd_hook_handles[fwd_hook_id]:\n            handle.remove()\n        del self._fwd_hook_handles[fwd_hook_id]",
  "def patch_optimizer(self, *tasks):\n        def patch_step(old_step):\n            def new_step(_, *args, **kwargs):\n                # call origin optimizer step method\n                output = old_step(*args, **kwargs)\n                # calculate mask\n                for task in tasks:\n                    task()\n                return output\n            return new_step\n        if self.optimizer is not None:\n            self.optimizer.step = types.MethodType(patch_step(self.optimizer.step), self.optimizer)",
  "def __init__(self, module, module_name, module_type, config, pruner):\n        \"\"\"\n        Wrap an module to enable data parallel, forward method customization and buffer registeration.\n\n        Parameters\n        ----------\n        module : pytorch module\n            the module user wants to compress\n        config : dict\n            the configurations that users specify for compression\n        module_name : str\n            the name of the module to compress, wrapper module shares same name\n        module_type : str\n            the type of the module to compress\n        pruner \uff1a Pruner\n            the pruner used to calculate mask\n        \"\"\"\n        super().__init__()\n        # origin layer information\n        self.module = module\n        self.name = module_name\n        self.type = module_type\n        # config and pruner\n        self.config = config\n        self.pruner = pruner\n\n        # register buffer for mask\n        self.register_buffer(\"weight_mask\", torch.ones(self.module.weight.shape))\n        if hasattr(self.module, 'bias') and self.module.bias is not None:\n            self.register_buffer(\"bias_mask\", torch.ones(self.module.bias.shape))\n        else:\n            self.register_buffer(\"bias_mask\", None)",
  "def forward(self, *inputs):\n        # apply mask to weight, bias\n        self.module.weight.data = self.module.weight.data.mul_(self.weight_mask)\n        if hasattr(self.module, 'bias') and self.module.bias is not None:\n            self.module.bias.data = self.module.bias.data.mul_(self.bias_mask)\n        return self.module(*inputs)",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        if optimizer is not None:\n            self.patch_optimizer(self.update_mask)",
  "def compress(self):\n        self.update_mask()\n        return self.bound_model",
  "def update_mask(self):\n        for wrapper_idx, wrapper in enumerate(self.get_modules_wrapper()):\n            masks = self.calc_mask(wrapper, wrapper_idx=wrapper_idx)\n            if masks is not None:\n                for k in masks:\n                    assert hasattr(wrapper, k), \"there is no attribute '%s' in wrapper\" % k\n                    setattr(wrapper, k, masks[k])",
  "def calc_mask(self, wrapper, **kwargs):\n        \"\"\"\n        Pruners should overload this method to provide mask for weight tensors.\n        The mask must have the same shape and type comparing to the weight.\n        It will be applied with `mul()` operation on the weight.\n        This method is effectively hooked to `forward()` method of the model.\n\n        Parameters\n        ----------\n        wrapper : Module\n            calculate mask for `wrapper.module`'s weight\n        \"\"\"\n        raise NotImplementedError(\"Pruners must overload calc_mask()\")",
  "def _wrap_modules(self, layer, config):\n        \"\"\"\n        Create a wrapper module to replace the original one.\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the mask\n        config : dict\n            the configuration for generating the mask\n        \"\"\"\n        _logger.debug(\"Module detected to compress : %s.\", layer.name)\n        wrapper = PrunerModuleWrapper(layer.module, layer.name, layer.type, config, self)\n        assert hasattr(layer.module, 'weight'), \"module %s does not have 'weight' attribute\" % layer.name\n        # move newly registered buffers to the same device of weight\n        wrapper.to(layer.module.weight.device)\n        return wrapper",
  "def export_model(self, model_path, mask_path=None, onnx_path=None, input_shape=None, device=None):\n        \"\"\"\n        Export pruned model weights, masks and onnx model(optional)\n\n        Parameters\n        ----------\n        model_path : str\n            path to save pruned model state_dict\n        mask_path : str\n            (optional) path to save mask dict\n        onnx_path : str\n            (optional) path to save onnx model\n        input_shape : list or tuple\n            input shape to onnx model\n        device : torch.device\n            device of the model, used to place the dummy input tensor for exporting onnx file.\n            the tensor is placed on cpu if ```device``` is None\n        \"\"\"\n        assert model_path is not None, 'model_path must be specified'\n        mask_dict = {}\n        self._unwrap_model() # used for generating correct state_dict name without wrapper state\n\n        for wrapper in self.get_modules_wrapper():\n            weight_mask = wrapper.weight_mask\n            bias_mask = wrapper.bias_mask\n            if weight_mask is not None:\n                mask_sum = weight_mask.sum().item()\n                mask_num = weight_mask.numel()\n                _logger.debug('Layer: %s  Sparsity: %.4f', wrapper.name, 1 - mask_sum / mask_num)\n                wrapper.module.weight.data = wrapper.module.weight.data.mul(weight_mask)\n            if bias_mask is not None:\n                wrapper.module.bias.data = wrapper.module.bias.data.mul(bias_mask)\n            # save mask to dict\n            mask_dict[wrapper.name] = {\"weight\": weight_mask, \"bias\": bias_mask}\n\n        torch.save(self.bound_model.state_dict(), model_path)\n        _logger.info('Model state_dict saved to %s', model_path)\n        if mask_path is not None:\n            torch.save(mask_dict, mask_path)\n            _logger.info('Mask dict saved to %s', mask_path)\n        if onnx_path is not None:\n            assert input_shape is not None, 'input_shape must be specified to export onnx model'\n            # input info needed\n            if device is None:\n                device = torch.device('cpu')\n            input_data = torch.Tensor(*input_shape)\n            torch.onnx.export(self.bound_model, input_data.to(device), onnx_path)\n            _logger.info('Model in onnx with input shape %s saved to %s', input_data.shape, onnx_path)\n\n        self._wrap_model()",
  "def load_model_state_dict(self, model_state):\n        \"\"\"\n        Load the state dict saved from unwrapped model.\n\n        Parameters:\n        -----------\n        model_state : dict\n            state dict saved from unwrapped model\n        \"\"\"\n        if self.is_wrapped:\n            self._unwrap_model()\n            self.bound_model.load_state_dict(model_state)\n            self._wrap_model()\n        else:\n            self.bound_model.load_state_dict(model_state)",
  "def __init__(self, module, module_name, module_type, config, quantizer):\n        \"\"\"\n        Wrap an module to enable data parallel, forward method customization and buffer registeration.\n\n        Parameters\n        ----------\n        module : pytorch module\n            the module user wants to compress\n        config : dict\n            the configurations that users specify for compression\n        module_name : str\n            the name of the module to compress, wrapper module shares same name\n        module_type : str\n            the type of the module to compress\n        quantizer \uff1aquantizer\n            the quantizer used to calculate mask\n        \"\"\"\n        super().__init__()\n        # origin layer information\n        self.module = module\n        self.name = module_name\n        self.type = module_type\n        # config and pruner\n        self.config = config\n        self.quantizer = quantizer\n\n        # register buffer and parameter\n        # old_weight is used to store origin weight and weight is used to store quantized weight\n        # the reason why weight is buffer instead of parameter is because in pytorch parameter is used as leaf\n        # if weight is leaf , then old_weight can not be updated.\n        if 'weight' in config['quant_types']:\n            if not _check_weight(self.module):\n                _logger.warning('Module %s does not have parameter \"weight\"', self.name)\n            else:\n                self.module.register_parameter('old_weight', torch.nn.Parameter(self.module.weight))\n                delattr(self.module, 'weight')\n                self.module.register_buffer('weight', self.module.old_weight)",
  "def forward(self, *inputs):\n        if 'input' in self.config['quant_types']:\n            inputs = self.quantizer.quant_grad.apply(\n                inputs,\n                QuantType.QUANT_INPUT,\n                self)\n\n        if 'weight' in self.config['quant_types'] and _check_weight(self.module):\n            new_weight = self.quantizer.quant_grad.apply(\n                self.module.old_weight,\n                QuantType.QUANT_WEIGHT,\n                self)\n            self.module.weight = new_weight\n            result = self.module(*inputs)\n        else:\n            result = self.module(*inputs)\n\n        if 'output' in self.config['quant_types']:\n            result = self.quantizer.quant_grad.apply(\n                result,\n                QuantType.QUANT_OUTPUT,\n                self)\n        return result",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.quant_grad = QuantGrad\n        if self.optimizer is not None:\n            self.patch_optimizer(self.step_with_optimizer)\n            for wrapper in self.get_modules_wrapper():\n                if 'weight' in wrapper.config['quant_types']:\n                    # old_weight is registered to keep track of weight before quantization\n                    # and it is trainable, therefore, it should be added to optimizer.\n                    self.optimizer.add_param_group({\"params\": wrapper.module.old_weight})",
  "def quantize_weight(self, weight, wrapper, **kwargs):\n        \"\"\"\n        quantize should overload this method to quantize weight.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        weight : Tensor\n            weight that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        \"\"\"\n        raise NotImplementedError('Quantizer must overload quantize_weight()')",
  "def quantize_output(self, output, wrapper, **kwargs):\n        \"\"\"\n        quantize should overload this method to quantize output.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        output : Tensor\n            output that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        \"\"\"\n        raise NotImplementedError('Quantizer must overload quantize_output()')",
  "def quantize_input(self, *inputs, wrapper, **kwargs):\n        \"\"\"\n        quantize should overload this method to quantize input.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        inputs : Tensor\n            inputs that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        \"\"\"\n        raise NotImplementedError('Quantizer must overload quantize_input()')",
  "def _wrap_modules(self, layer, config):\n        \"\"\"\n        Create a wrapper forward function to replace the original one.\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the mask\n        config : dict\n            the configuration for quantization\n        \"\"\"\n        assert 'quant_types' in config, 'must provide quant_types in config'\n        assert isinstance(config['quant_types'], list), 'quant_types must be list type'\n        assert 'quant_bits' in config, 'must provide quant_bits in config'\n        assert isinstance(config['quant_bits'], int) or isinstance(config['quant_bits'], dict), 'quant_bits must be dict type or int type'\n\n        if isinstance(config['quant_bits'], dict):\n            for quant_type in config['quant_types']:\n                assert quant_type in config['quant_bits'], 'bits length for %s must be specified in quant_bits dict' % quant_type\n\n        return QuantizerModuleWrapper(layer.module, layer.name, layer.type, config, self)",
  "def step_with_optimizer(self):\n        pass",
  "def quant_backward(tensor, grad_output, quant_type):\n        \"\"\"\n        This method should be overrided by subclass to provide customized backward function,\n        default implementation is Straight-Through Estimator\n        Parameters\n        ----------\n        tensor : Tensor\n            input of quantization operation\n        grad_output : Tensor\n            gradient of the output of quantization operation\n        quant_type : QuantType\n            the type of quantization, it can be `QuantType.QUANT_INPUT`, `QuantType.QUANT_WEIGHT`, `QuantType.QUANT_OUTPUT`,\n            you can define different behavior for different types.\n        Returns\n        -------\n        tensor\n            gradient of the input of quantization operation\n        \"\"\"\n        return grad_output",
  "def forward(ctx, tensor, quant_type, wrapper, **kwargs):\n        ctx.save_for_backward(tensor, torch.Tensor([quant_type]))\n        if quant_type == QuantType.QUANT_INPUT:\n            return wrapper.quantizer.quantize_input(tensor, wrapper, **kwargs)\n        elif quant_type == QuantType.QUANT_WEIGHT:\n            return wrapper.quantizer.quantize_weight(tensor, wrapper, **kwargs)\n        elif quant_type == QuantType.QUANT_OUTPUT:\n            return wrapper.quantizer.quantize_output(tensor, wrapper, **kwargs)\n        else:\n            raise ValueError(\"unrecognized QuantType.\")",
  "def backward(cls, ctx, grad_output):\n        tensor, quant_type = ctx.saved_variables\n        output = cls.quant_backward(tensor, grad_output, quant_type)\n        return output, None, None, None",
  "def patch_step(old_step):\n            def new_step(_, *args, **kwargs):\n                # call origin optimizer step method\n                output = old_step(*args, **kwargs)\n                # calculate mask\n                for task in tasks:\n                    task()\n                return output\n            return new_step",
  "def new_step(_, *args, **kwargs):\n                # call origin optimizer step method\n                output = old_step(*args, **kwargs)\n                # calculate mask\n                for task in tasks:\n                    task()\n                return output",
  "class ADMMPruner(OneshotPruner):\n    \"\"\"\n    A Pytorch implementation of ADMM Pruner algorithm.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned.\n    config_list : list\n        List on pruning configs.\n    trainer : function\n        Function used for the first subproblem.\n        Users should write this function as a normal function to train the Pytorch model\n        and include `model, optimizer, criterion, epoch, callback` as function arguments.\n        Here `callback` acts as an L2 regulizer as presented in the formula (7) of the original paper.\n        The logic of `callback` is implemented inside the Pruner,\n        users are just required to insert `callback()` between `loss.backward()` and `optimizer.step()`.\n        Example::\n\n            def trainer(model, criterion, optimizer, epoch, callback):\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n                train_loader = ...\n                model.train()\n                for batch_idx, (data, target) in enumerate(train_loader):\n                    data, target = data.to(device), target.to(device)\n                    optimizer.zero_grad()\n                    output = model(data)\n                    loss = criterion(output, target)\n                    loss.backward()\n                    # callback should be inserted between loss.backward() and optimizer.step()\n                    if callback:\n                        callback()\n                    optimizer.step()\n    num_iterations : int\n        Total number of iterations.\n    training_epochs : int\n        Training epochs of the first subproblem.\n    row : float\n        Penalty parameters for ADMM training.\n    base_algo : str\n        Base pruning algorithm. `level`, `l1` or `l2`, by default `l1`. Given the sparsity distribution among the ops,\n        the assigned `base_algo` is used to decide which filters/channels/weights to prune.\n\n    \"\"\"\n\n    def __init__(self, model, config_list, trainer, num_iterations=30, training_epochs=5, row=1e-4, base_algo='l1'):\n        self._base_algo = base_algo\n\n        super().__init__(model, config_list)\n\n        self._trainer = trainer\n        self._num_iterations = num_iterations\n        self._training_epochs = training_epochs\n        self._row = row\n\n        self.set_wrappers_attribute(\"if_calculated\", False)\n        self.masker = MASKER_DICT[self._base_algo](self.bound_model, self)\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self._base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self._base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)\n\n    def _projection(self, weight, sparsity):\n        '''\n        Return the Euclidean projection of the weight matrix according to the pruning mode.\n\n        Parameters\n        ----------\n        weight : tensor\n            original matrix\n        sparsity : float\n            the ratio of parameters which need to be set to zero\n\n        Returns\n        -------\n        tensor\n            the projected matrix\n        '''\n        w_abs = weight.abs()\n        if self._base_algo == 'level':\n            k = int(weight.numel() * sparsity)\n            if k == 0:\n                mask_weight = torch.ones(weight.shape).type_as(weight)\n            else:\n                threshold = torch.topk(w_abs.view(-1), k, largest=False)[0].max()\n                mask_weight = torch.gt(w_abs, threshold).type_as(weight)\n        elif self._base_algo in ['l1', 'l2']:\n            filters = weight.size(0)\n            num_prune = int(filters * sparsity)\n            if filters < 2 or num_prune < 1:\n                mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n            else:\n                w_abs_structured = w_abs.view(filters, -1).sum(dim=1)\n                threshold = torch.topk(w_abs_structured.view(-1), num_prune, largest=False)[0].max()\n                mask_weight = torch.gt(w_abs_structured, threshold)[:, None, None, None].expand_as(weight).type_as(weight)\n\n        return weight.data.mul(mask_weight)\n\n    def compress(self):\n        \"\"\"\n        Compress the model with ADMM.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        _logger.info('Starting ADMM Compression...')\n\n        # initiaze Z, U\n        # Z_i^0 = W_i^0\n        # U_i^0 = 0\n        Z = []\n        U = []\n        for wrapper in self.get_modules_wrapper():\n            z = wrapper.module.weight.data\n            Z.append(z)\n            U.append(torch.zeros_like(z))\n\n        optimizer = torch.optim.Adam(\n            self.bound_model.parameters(), lr=1e-3, weight_decay=5e-5)\n\n        # Loss = cross_entropy +  l2 regulization + \\Sum_{i=1}^N \\row_i ||W_i - Z_i^k + U_i^k||^2\n        criterion = torch.nn.CrossEntropyLoss()\n\n        # callback function to do additonal optimization, refer to the deriatives of Formula (7)\n        def callback():\n            for i, wrapper in enumerate(self.get_modules_wrapper()):\n                wrapper.module.weight.data -= self._row * \\\n                    (wrapper.module.weight.data - Z[i] + U[i])\n\n        # optimization iteration\n        for k in range(self._num_iterations):\n            _logger.info('ADMM iteration : %d', k)\n\n            # step 1: optimize W with AdamOptimizer\n            for epoch in range(self._training_epochs):\n                self._trainer(self.bound_model, optimizer=optimizer,\n                              criterion=criterion, epoch=epoch, callback=callback)\n\n            # step 2: update Z, U\n            # Z_i^{k+1} = projection(W_i^{k+1} + U_i^k)\n            # U_i^{k+1} = U^k + W_i^{k+1} - Z_i^{k+1}\n            for i, wrapper in enumerate(self.get_modules_wrapper()):\n                z = wrapper.module.weight.data + U[i]\n                Z[i] = self._projection(z, wrapper.config['sparsity'])\n                U[i] = U[i] + wrapper.module.weight.data - Z[i]\n\n        # apply prune\n        self.update_mask()\n\n        _logger.info('Compression finished.')\n\n        return self.bound_model",
  "def __init__(self, model, config_list, trainer, num_iterations=30, training_epochs=5, row=1e-4, base_algo='l1'):\n        self._base_algo = base_algo\n\n        super().__init__(model, config_list)\n\n        self._trainer = trainer\n        self._num_iterations = num_iterations\n        self._training_epochs = training_epochs\n        self._row = row\n\n        self.set_wrappers_attribute(\"if_calculated\", False)\n        self.masker = MASKER_DICT[self._base_algo](self.bound_model, self)",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self._base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self._base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)",
  "def _projection(self, weight, sparsity):\n        '''\n        Return the Euclidean projection of the weight matrix according to the pruning mode.\n\n        Parameters\n        ----------\n        weight : tensor\n            original matrix\n        sparsity : float\n            the ratio of parameters which need to be set to zero\n\n        Returns\n        -------\n        tensor\n            the projected matrix\n        '''\n        w_abs = weight.abs()\n        if self._base_algo == 'level':\n            k = int(weight.numel() * sparsity)\n            if k == 0:\n                mask_weight = torch.ones(weight.shape).type_as(weight)\n            else:\n                threshold = torch.topk(w_abs.view(-1), k, largest=False)[0].max()\n                mask_weight = torch.gt(w_abs, threshold).type_as(weight)\n        elif self._base_algo in ['l1', 'l2']:\n            filters = weight.size(0)\n            num_prune = int(filters * sparsity)\n            if filters < 2 or num_prune < 1:\n                mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n            else:\n                w_abs_structured = w_abs.view(filters, -1).sum(dim=1)\n                threshold = torch.topk(w_abs_structured.view(-1), num_prune, largest=False)[0].max()\n                mask_weight = torch.gt(w_abs_structured, threshold)[:, None, None, None].expand_as(weight).type_as(weight)\n\n        return weight.data.mul(mask_weight)",
  "def compress(self):\n        \"\"\"\n        Compress the model with ADMM.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        _logger.info('Starting ADMM Compression...')\n\n        # initiaze Z, U\n        # Z_i^0 = W_i^0\n        # U_i^0 = 0\n        Z = []\n        U = []\n        for wrapper in self.get_modules_wrapper():\n            z = wrapper.module.weight.data\n            Z.append(z)\n            U.append(torch.zeros_like(z))\n\n        optimizer = torch.optim.Adam(\n            self.bound_model.parameters(), lr=1e-3, weight_decay=5e-5)\n\n        # Loss = cross_entropy +  l2 regulization + \\Sum_{i=1}^N \\row_i ||W_i - Z_i^k + U_i^k||^2\n        criterion = torch.nn.CrossEntropyLoss()\n\n        # callback function to do additonal optimization, refer to the deriatives of Formula (7)\n        def callback():\n            for i, wrapper in enumerate(self.get_modules_wrapper()):\n                wrapper.module.weight.data -= self._row * \\\n                    (wrapper.module.weight.data - Z[i] + U[i])\n\n        # optimization iteration\n        for k in range(self._num_iterations):\n            _logger.info('ADMM iteration : %d', k)\n\n            # step 1: optimize W with AdamOptimizer\n            for epoch in range(self._training_epochs):\n                self._trainer(self.bound_model, optimizer=optimizer,\n                              criterion=criterion, epoch=epoch, callback=callback)\n\n            # step 2: update Z, U\n            # Z_i^{k+1} = projection(W_i^{k+1} + U_i^k)\n            # U_i^{k+1} = U^k + W_i^{k+1} - Z_i^{k+1}\n            for i, wrapper in enumerate(self.get_modules_wrapper()):\n                z = wrapper.module.weight.data + U[i]\n                Z[i] = self._projection(z, wrapper.config['sparsity'])\n                U[i] = U[i] + wrapper.module.weight.data - Z[i]\n\n        # apply prune\n        self.update_mask()\n\n        _logger.info('Compression finished.')\n\n        return self.bound_model",
  "def callback():\n            for i, wrapper in enumerate(self.get_modules_wrapper()):\n                wrapper.module.weight.data -= self._row * \\\n                    (wrapper.module.weight.data - Z[i] + U[i])",
  "class AutoCompressPruner(Pruner):\n    \"\"\"\n    A Pytorch implementation of AutoCompress pruning algorithm.\n\n    Parameters\n    ----------\n    model : pytorch model\n        The model to be pruned.\n    config_list : list\n        Supported keys:\n            - sparsity : The target overall sparsity.\n            - op_types : The operation type to prune.\n    trainer : function\n        Function used for the first subproblem of ADMM Pruner.\n        Users should write this function as a normal function to train the Pytorch model\n        and include `model, optimizer, criterion, epoch, callback` as function arguments.\n        Here `callback` acts as an L2 regulizer as presented in the formula (7) of the original paper.\n        The logic of `callback` is implemented inside the Pruner,\n        users are just required to insert `callback()` between `loss.backward()` and `optimizer.step()`.\n        Example::\n\n            def trainer(model, criterion, optimizer, epoch, callback):\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n                train_loader = ...\n                model.train()\n                for batch_idx, (data, target) in enumerate(train_loader):\n                    data, target = data.to(device), target.to(device)\n                    optimizer.zero_grad()\n                    output = model(data)\n                    loss = criterion(output, target)\n                    loss.backward()\n                    # callback should be inserted between loss.backward() and optimizer.step()\n                    if callback:\n                        callback()\n                    optimizer.step()\n    evaluator : function\n        function to evaluate the pruned model.\n        This function should include `model` as the only parameter, and returns a scalar value.\n        Example::\n\n            def evaluator(model):\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n                val_loader = ...\n                model.eval()\n                correct = 0\n                with torch.no_grad():\n                    for data, target in val_loader:\n                        data, target = data.to(device), target.to(device)\n                        output = model(data)\n                        # get the index of the max log-probability\n                        pred = output.argmax(dim=1, keepdim=True)\n                        correct += pred.eq(target.view_as(pred)).sum().item()\n                accuracy = correct / len(val_loader.dataset)\n                return accuracy\n    dummy_input : pytorch tensor\n        The dummy input for ```jit.trace```, users should put it on right device before pass in.\n    num_iterations : int\n        Number of overall iterations.\n    optimize_mode : str\n        optimize mode, `maximize` or `minimize`, by default `maximize`.\n    base_algo : str\n        Base pruning algorithm. `level`, `l1` or `l2`, by default `l1`. Given the sparsity distribution among the ops,\n        the assigned `base_algo` is used to decide which filters/channels/weights to prune.\n    start_temperature : float\n        Start temperature of the simulated annealing process.\n    stop_temperature : float\n        Stop temperature of the simulated annealing process.\n    cool_down_rate : float\n        Cool down rate of the temperature.\n    perturbation_magnitude : float\n        Initial perturbation magnitude to the sparsities. The magnitude decreases with current temperature.\n    admm_num_iterations : int\n        Number of iterations of ADMM Pruner.\n    admm_training_epochs : int\n        Training epochs of the first optimization subproblem of ADMMPruner.\n    row : float\n        Penalty parameters for ADMM training.\n    experiment_data_dir : string\n        PATH to store temporary experiment data.\n    \"\"\"\n\n    def __init__(self, model, config_list, trainer, evaluator, dummy_input,\n                 num_iterations=3, optimize_mode='maximize', base_algo='l1',\n                 # SimulatedAnnealing related\n                 start_temperature=100, stop_temperature=20, cool_down_rate=0.9, perturbation_magnitude=0.35,\n                 # ADMM related\n                 admm_num_iterations=30, admm_training_epochs=5, row=1e-4,\n                 experiment_data_dir='./'):\n        # original model\n        self._model_to_prune = model\n        self._base_algo = base_algo\n\n        self._trainer = trainer\n        self._evaluator = evaluator\n        self._dummy_input = dummy_input\n        self._num_iterations = num_iterations\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # hyper parameters for SA algorithm\n        self._start_temperature = start_temperature\n        self._stop_temperature = stop_temperature\n        self._cool_down_rate = cool_down_rate\n        self._perturbation_magnitude = perturbation_magnitude\n\n        # hyper parameters for ADMM algorithm\n        self._admm_num_iterations = admm_num_iterations\n        self._admm_training_epochs = admm_training_epochs\n        self._row = row\n\n        # overall pruning rate\n        self._sparsity = config_list[0]['sparsity']\n\n        self._experiment_data_dir = experiment_data_dir\n        if not os.path.exists(self._experiment_data_dir):\n            os.makedirs(self._experiment_data_dir)\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self._base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self._base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)\n\n    def calc_mask(self, wrapper, **kwargs):\n        return None\n\n    def compress(self):\n        \"\"\"\n        Compress the model with AutoCompress.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        _logger.info('Starting AutoCompress pruning...')\n\n        sparsity_each_round = 1 - pow(1-self._sparsity, 1/self._num_iterations)\n\n        for i in range(self._num_iterations):\n            _logger.info('Pruning iteration: %d', i)\n            _logger.info('Target sparsity this round: %s',\n                         1-pow(1-sparsity_each_round, i+1))\n\n            # SimulatedAnnealingPruner\n            _logger.info(\n                'Generating sparsities with SimulatedAnnealingPruner...')\n            SApruner = SimulatedAnnealingPruner(\n                model=copy.deepcopy(self._model_to_prune),\n                config_list=[\n                    {\"sparsity\": sparsity_each_round, \"op_types\": ['Conv2d']}],\n                evaluator=self._evaluator,\n                optimize_mode=self._optimize_mode,\n                base_algo=self._base_algo,\n                start_temperature=self._start_temperature,\n                stop_temperature=self._stop_temperature,\n                cool_down_rate=self._cool_down_rate,\n                perturbation_magnitude=self._perturbation_magnitude,\n                experiment_data_dir=self._experiment_data_dir)\n            config_list = SApruner.compress(return_config_list=True)\n            _logger.info(\"Generated config_list : %s\", config_list)\n\n            # ADMMPruner\n            _logger.info('Performing structured pruning with ADMMPruner...')\n            ADMMpruner = ADMMPruner(\n                model=copy.deepcopy(self._model_to_prune),\n                config_list=config_list,\n                trainer=self._trainer,\n                num_iterations=self._admm_num_iterations,\n                training_epochs=self._admm_training_epochs,\n                row=self._row,\n                base_algo=self._base_algo)\n            ADMMpruner.compress()\n\n            ADMMpruner.export_model(os.path.join(self._experiment_data_dir, 'model_admm_masked.pth'), os.path.join(\n                self._experiment_data_dir, 'mask.pth'))\n\n            # use speed up to prune the model before next iteration, because SimulatedAnnealingPruner & ADMMPruner don't take masked models\n            self._model_to_prune.load_state_dict(torch.load(os.path.join(\n                self._experiment_data_dir, 'model_admm_masked.pth')))\n\n            masks_file = os.path.join(self._experiment_data_dir, 'mask.pth')\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n            _logger.info('Speeding up models...')\n            m_speedup = ModelSpeedup(self._model_to_prune, self._dummy_input, masks_file, device)\n            m_speedup.speedup_model()\n\n            evaluation_result = self._evaluator(self._model_to_prune)\n            _logger.info('Evaluation result of the pruned model in iteration %d: %s', i, evaluation_result)\n\n        _logger.info('----------Compression finished--------------')\n\n        os.remove(os.path.join(self._experiment_data_dir, 'model_admm_masked.pth'))\n        os.remove(os.path.join(self._experiment_data_dir, 'mask.pth'))\n\n        return self._model_to_prune\n\n    def export_model(self, model_path, mask_path=None, onnx_path=None, input_shape=None, device=None):\n        _logger.info(\"AutoCompressPruner export directly the pruned model without mask\")\n\n        torch.save(self._model_to_prune.state_dict(), model_path)\n        _logger.info('Model state_dict saved to %s', model_path)\n\n        if onnx_path is not None:\n            assert input_shape is not None, 'input_shape must be specified to export onnx model'\n            # input info needed\n            if device is None:\n                device = torch.device('cpu')\n            input_data = torch.Tensor(*input_shape)\n            torch.onnx.export(self._model_to_prune, input_data.to(device), onnx_path)\n            _logger.info('Model in onnx with input shape %s saved to %s', input_data.shape, onnx_path)",
  "def __init__(self, model, config_list, trainer, evaluator, dummy_input,\n                 num_iterations=3, optimize_mode='maximize', base_algo='l1',\n                 # SimulatedAnnealing related\n                 start_temperature=100, stop_temperature=20, cool_down_rate=0.9, perturbation_magnitude=0.35,\n                 # ADMM related\n                 admm_num_iterations=30, admm_training_epochs=5, row=1e-4,\n                 experiment_data_dir='./'):\n        # original model\n        self._model_to_prune = model\n        self._base_algo = base_algo\n\n        self._trainer = trainer\n        self._evaluator = evaluator\n        self._dummy_input = dummy_input\n        self._num_iterations = num_iterations\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # hyper parameters for SA algorithm\n        self._start_temperature = start_temperature\n        self._stop_temperature = stop_temperature\n        self._cool_down_rate = cool_down_rate\n        self._perturbation_magnitude = perturbation_magnitude\n\n        # hyper parameters for ADMM algorithm\n        self._admm_num_iterations = admm_num_iterations\n        self._admm_training_epochs = admm_training_epochs\n        self._row = row\n\n        # overall pruning rate\n        self._sparsity = config_list[0]['sparsity']\n\n        self._experiment_data_dir = experiment_data_dir\n        if not os.path.exists(self._experiment_data_dir):\n            os.makedirs(self._experiment_data_dir)",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self._base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self._base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)",
  "def calc_mask(self, wrapper, **kwargs):\n        return None",
  "def compress(self):\n        \"\"\"\n        Compress the model with AutoCompress.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        _logger.info('Starting AutoCompress pruning...')\n\n        sparsity_each_round = 1 - pow(1-self._sparsity, 1/self._num_iterations)\n\n        for i in range(self._num_iterations):\n            _logger.info('Pruning iteration: %d', i)\n            _logger.info('Target sparsity this round: %s',\n                         1-pow(1-sparsity_each_round, i+1))\n\n            # SimulatedAnnealingPruner\n            _logger.info(\n                'Generating sparsities with SimulatedAnnealingPruner...')\n            SApruner = SimulatedAnnealingPruner(\n                model=copy.deepcopy(self._model_to_prune),\n                config_list=[\n                    {\"sparsity\": sparsity_each_round, \"op_types\": ['Conv2d']}],\n                evaluator=self._evaluator,\n                optimize_mode=self._optimize_mode,\n                base_algo=self._base_algo,\n                start_temperature=self._start_temperature,\n                stop_temperature=self._stop_temperature,\n                cool_down_rate=self._cool_down_rate,\n                perturbation_magnitude=self._perturbation_magnitude,\n                experiment_data_dir=self._experiment_data_dir)\n            config_list = SApruner.compress(return_config_list=True)\n            _logger.info(\"Generated config_list : %s\", config_list)\n\n            # ADMMPruner\n            _logger.info('Performing structured pruning with ADMMPruner...')\n            ADMMpruner = ADMMPruner(\n                model=copy.deepcopy(self._model_to_prune),\n                config_list=config_list,\n                trainer=self._trainer,\n                num_iterations=self._admm_num_iterations,\n                training_epochs=self._admm_training_epochs,\n                row=self._row,\n                base_algo=self._base_algo)\n            ADMMpruner.compress()\n\n            ADMMpruner.export_model(os.path.join(self._experiment_data_dir, 'model_admm_masked.pth'), os.path.join(\n                self._experiment_data_dir, 'mask.pth'))\n\n            # use speed up to prune the model before next iteration, because SimulatedAnnealingPruner & ADMMPruner don't take masked models\n            self._model_to_prune.load_state_dict(torch.load(os.path.join(\n                self._experiment_data_dir, 'model_admm_masked.pth')))\n\n            masks_file = os.path.join(self._experiment_data_dir, 'mask.pth')\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n            _logger.info('Speeding up models...')\n            m_speedup = ModelSpeedup(self._model_to_prune, self._dummy_input, masks_file, device)\n            m_speedup.speedup_model()\n\n            evaluation_result = self._evaluator(self._model_to_prune)\n            _logger.info('Evaluation result of the pruned model in iteration %d: %s', i, evaluation_result)\n\n        _logger.info('----------Compression finished--------------')\n\n        os.remove(os.path.join(self._experiment_data_dir, 'model_admm_masked.pth'))\n        os.remove(os.path.join(self._experiment_data_dir, 'mask.pth'))\n\n        return self._model_to_prune",
  "def export_model(self, model_path, mask_path=None, onnx_path=None, input_shape=None, device=None):\n        _logger.info(\"AutoCompressPruner export directly the pruned model without mask\")\n\n        torch.save(self._model_to_prune.state_dict(), model_path)\n        _logger.info('Model state_dict saved to %s', model_path)\n\n        if onnx_path is not None:\n            assert input_shape is not None, 'input_shape must be specified to export onnx model'\n            # input info needed\n            if device is None:\n                device = torch.device('cpu')\n            input_data = torch.Tensor(*input_shape)\n            torch.onnx.export(self._model_to_prune, input_data.to(device), onnx_path)\n            _logger.info('Model in onnx with input shape %s saved to %s', input_data.shape, onnx_path)",
  "class OneshotPruner(Pruner):\n    \"\"\"\n    Prune model to an exact pruning level for one time.\n    \"\"\"\n\n    def __init__(self, model, config_list, pruning_algorithm='level', optimizer=None, **algo_kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        pruning_algorithm: str\n            algorithms being used to prune model\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        algo_kwargs: dict\n            Additional parameters passed to pruning algorithm masker class\n        \"\"\"\n\n        super().__init__(model, config_list, optimizer)\n        self.set_wrappers_attribute(\"if_calculated\", False)\n        self.masker = MASKER_DICT[pruning_algorithm](model, self, **algo_kwargs)\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def calc_mask(self, wrapper, wrapper_idx=None):\n        \"\"\"\n        Calculate the mask of given layer\n        Parameters\n        ----------\n        wrapper : Module\n            the module to instrument the compression operation\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n        if wrapper.if_calculated:\n            return None\n\n        sparsity = wrapper.config['sparsity']\n        if not wrapper.if_calculated:\n            masks = self.masker.calc_mask(sparsity=sparsity, wrapper=wrapper, wrapper_idx=wrapper_idx)\n\n            # masker.calc_mask returns None means calc_mask is not calculated sucessfully, can try later\n            if masks is not None:\n                wrapper.if_calculated = True\n            return masks\n        else:\n            return None",
  "class LevelPruner(OneshotPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : This is to specify the sparsity operations to be compressed to.\n            - op_types : Operation types to prune.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='level', optimizer=optimizer)",
  "class SlimPruner(OneshotPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : This is to specify the sparsity operations to be compressed to.\n            - op_types : Only BatchNorm2d is supported in Slim Pruner.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='slim', optimizer=optimizer)\n\n    def validate_config(self, model, config_list):\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            'op_types': ['BatchNorm2d'],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n        if len(config_list) > 1:\n            logger.warning('Slim pruner only supports 1 configuration')",
  "class _StructuredFilterPruner(OneshotPruner):\n    def __init__(self, model, config_list, pruning_algorithm, optimizer=None, **algo_kwargs):\n        super().__init__(model, config_list, pruning_algorithm=pruning_algorithm, optimizer=optimizer, **algo_kwargs)\n\n    def validate_config(self, model, config_list):\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            'op_types': ['Conv2d'],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)",
  "class L1FilterPruner(_StructuredFilterPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : This is to specify the sparsity operations to be compressed to.\n            - op_types : Only Conv2d is supported in L1FilterPruner.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='l1', optimizer=optimizer)",
  "class L2FilterPruner(_StructuredFilterPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : This is to specify the sparsity operations to be compressed to.\n            - op_types : Only Conv2d is supported in L2FilterPruner.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='l2', optimizer=optimizer)",
  "class FPGMPruner(_StructuredFilterPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : This is to specify the sparsity operations to be compressed to.\n            - op_types : Only Conv2d is supported in FPGM Pruner.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='fpgm', optimizer=optimizer)",
  "class TaylorFOWeightFilterPruner(_StructuredFilterPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : How much percentage of convolutional filters are to be pruned.\n            - op_types : Currently only Conv2d is supported in TaylorFOWeightFilterPruner.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None, statistics_batch_num=1):\n        super().__init__(model, config_list, pruning_algorithm='taylorfo', optimizer=optimizer, statistics_batch_num=statistics_batch_num)",
  "class ActivationAPoZRankFilterPruner(_StructuredFilterPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : How much percentage of convolutional filters are to be pruned.\n            - op_types : Only Conv2d is supported in ActivationAPoZRankFilterPruner.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None, activation='relu', statistics_batch_num=1):\n        super().__init__(model, config_list, pruning_algorithm='apoz', optimizer=optimizer, \\\n            activation=activation, statistics_batch_num=statistics_batch_num)",
  "class ActivationMeanRankFilterPruner(_StructuredFilterPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : How much percentage of convolutional filters are to be pruned.\n            - op_types : Only Conv2d is supported in ActivationMeanRankFilterPruner.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None, activation='relu', statistics_batch_num=1):\n        super().__init__(model, config_list, pruning_algorithm='mean_activation', optimizer=optimizer, \\\n            activation=activation, statistics_batch_num=statistics_batch_num)",
  "def __init__(self, model, config_list, pruning_algorithm='level', optimizer=None, **algo_kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        pruning_algorithm: str\n            algorithms being used to prune model\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        algo_kwargs: dict\n            Additional parameters passed to pruning algorithm masker class\n        \"\"\"\n\n        super().__init__(model, config_list, optimizer)\n        self.set_wrappers_attribute(\"if_calculated\", False)\n        self.masker = MASKER_DICT[pruning_algorithm](model, self, **algo_kwargs)",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)",
  "def calc_mask(self, wrapper, wrapper_idx=None):\n        \"\"\"\n        Calculate the mask of given layer\n        Parameters\n        ----------\n        wrapper : Module\n            the module to instrument the compression operation\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n        if wrapper.if_calculated:\n            return None\n\n        sparsity = wrapper.config['sparsity']\n        if not wrapper.if_calculated:\n            masks = self.masker.calc_mask(sparsity=sparsity, wrapper=wrapper, wrapper_idx=wrapper_idx)\n\n            # masker.calc_mask returns None means calc_mask is not calculated sucessfully, can try later\n            if masks is not None:\n                wrapper.if_calculated = True\n            return masks\n        else:\n            return None",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='level', optimizer=optimizer)",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='slim', optimizer=optimizer)",
  "def validate_config(self, model, config_list):\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            'op_types': ['BatchNorm2d'],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n        if len(config_list) > 1:\n            logger.warning('Slim pruner only supports 1 configuration')",
  "def __init__(self, model, config_list, pruning_algorithm, optimizer=None, **algo_kwargs):\n        super().__init__(model, config_list, pruning_algorithm=pruning_algorithm, optimizer=optimizer, **algo_kwargs)",
  "def validate_config(self, model, config_list):\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            'op_types': ['Conv2d'],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='l1', optimizer=optimizer)",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='l2', optimizer=optimizer)",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='fpgm', optimizer=optimizer)",
  "def __init__(self, model, config_list, optimizer=None, statistics_batch_num=1):\n        super().__init__(model, config_list, pruning_algorithm='taylorfo', optimizer=optimizer, statistics_batch_num=statistics_batch_num)",
  "def __init__(self, model, config_list, optimizer=None, activation='relu', statistics_batch_num=1):\n        super().__init__(model, config_list, pruning_algorithm='apoz', optimizer=optimizer, \\\n            activation=activation, statistics_batch_num=statistics_batch_num)",
  "def __init__(self, model, config_list, optimizer=None, activation='relu', statistics_batch_num=1):\n        super().__init__(model, config_list, pruning_algorithm='mean_activation', optimizer=optimizer, \\\n            activation=activation, statistics_batch_num=statistics_batch_num)",
  "class SensitivityPruner(Pruner):\n    \"\"\"\n    This function prune the model based on the sensitivity\n    for each layer.\n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        model to be compressed\n    evaluator: function\n        validation function for the model. This function should return the accuracy\n        of the validation dataset. The input parameters of evaluator can be specified\n        in the parameter `eval_args` and 'eval_kwargs' of the compress function if needed.\n        Example:\n        >>> def evaluator(model):\n        >>>     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        >>>     val_loader = ...\n        >>>     model.eval()\n        >>>     correct = 0\n        >>>     with torch.no_grad():\n        >>>         for data, target in val_loader:\n        >>>             data, target = data.to(device), target.to(device)\n        >>>             output = model(data)\n        >>>             # get the index of the max log-probability\n        >>>             pred = output.argmax(dim=1, keepdim=True)\n        >>>             correct += pred.eq(target.view_as(pred)).sum().item()\n        >>>     accuracy = correct / len(val_loader.dataset)\n        >>>     return accuracy\n    finetuner: function\n        finetune function for the model. This parameter is not essential, if is not None,\n        the sensitivity pruner will finetune the model after pruning in each iteration.\n        The input parameters of finetuner can be specified in the parameter of compress\n        called `finetune_args` and `finetune_kwargs` if needed.\n        Example:\n        >>> def finetuner(model, epoch=3):\n        >>>     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        >>>     train_loader = ...\n        >>>     criterion = torch.nn.CrossEntropyLoss()\n        >>>     optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n        >>>     model.train()\n        >>>     for _ in range(epoch):\n        >>>         for _, (data, target) in enumerate(train_loader):\n        >>>             data, target = data.to(device), target.to(device)\n        >>>             optimizer.zero_grad()\n        >>>             output = model(data)\n        >>>             loss = criterion(output, target)\n        >>>             loss.backward()\n        >>>             optimizer.step()\n    base_algo: str\n        base pruning algorithm. `level`, `l1` or `l2`, by default `l1`.\n    sparsity_proportion_calc: function\n        This function generate the sparsity proportion between the conv layers according to the\n        sensitivity analysis results. We provide a default function to quantify the sparsity\n        proportion according to the sensitivity analysis results. Users can also customize\n        this function according to their needs. The input of this function is a dict,\n        for example : {'conv1' : {0.1: 0.9, 0.2 : 0.8}, 'conv2' : {0.1: 0.9, 0.2 : 0.8}},\n        in which, 'conv1' and is the name of the conv layer, and 0.1:0.9 means when the\n        sparsity of conv1 is 0.1 (10%), the model's val accuracy equals to 0.9.\n    sparsity_per_iter: float\n        The sparsity of the model that the pruner try to prune in each iteration.\n    acc_drop_threshold : float\n        The hyperparameter used to quantifiy the sensitivity for each layer.\n    checkpoint_dir: str\n        The dir path to save the checkpoints during the pruning.\n    \"\"\"\n\n    def __init__(self, model, config_list, evaluator,\n                 finetuner=None, base_algo='l1', sparsity_proportion_calc=None,\n                 sparsity_per_iter=0.1, acc_drop_threshold=0.05, checkpoint_dir=None):\n\n        self.base_algo = base_algo\n        self.model = model\n        super(SensitivityPruner, self).__init__(model, config_list)\n        # unwrap the model\n        self._unwrap_model()\n        _logger.debug(str(self.model))\n        self.evaluator = evaluator\n        self.finetuner = finetuner\n        self.analyzer = SensitivityAnalysis(\n            self.model, self.evaluator, prune_type=base_algo, \\\n            early_stop_mode='dropped', early_stop_value=acc_drop_threshold)\n        # Get the original accuracy of the pretrained model\n        self.ori_acc = None\n        # Copy the original weights before pruning\n        self.ori_state_dict = copy.deepcopy(self.model.state_dict())\n        self.sensitivities = {}\n        # Save the weight count for each layer\n        self.weight_count = {}\n        self.weight_sum = 0\n        # Map the layer name to the layer module\n        self.named_module = {}\n\n        self.Pruner = PRUNER_DICT[self.base_algo]\n        # Count the total weight count of the model\n        for name, submodule in self.model.named_modules():\n            self.named_module[name] = submodule\n            if name in self.analyzer.target_layer:\n                # Currently, only count the weights in the conv layers\n                # else the fully connected layer (which contains\n                # the most weights) may make the pruner prune the\n                # model too hard\n                # if hasattr(submodule, 'weight'): # Count all the weights of the model\n                self.weight_count[name] = submodule.weight.data.numel()\n                self.weight_sum += self.weight_count[name]\n        # function to generate the sparsity proportion between the conv layers\n        if sparsity_proportion_calc is None:\n            self.sparsity_proportion_calc = self._max_prune_ratio\n        else:\n            self.sparsity_proportion_calc = sparsity_proportion_calc\n        # The ratio of remained weights is 1.0 at the begining\n        self.remained_ratio = 1.0\n        self.sparsity_per_iter = sparsity_per_iter\n        self.acc_drop_threshold = acc_drop_threshold\n        self.checkpoint_dir = checkpoint_dir\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self.base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self.base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)\n\n    def load_sensitivity(self, filepath):\n        \"\"\"\n        load the sensitivity results exported by the sensitivity analyzer\n        \"\"\"\n        assert os.path.exists(filepath)\n        with open(filepath, 'r') as csvf:\n            csv_r = csv.reader(csvf)\n            header = next(csv_r)\n            sparsities = [float(x) for x in header[1:]]\n            sensitivities = {}\n            for row in csv_r:\n                layername = row[0]\n                accuracies = [float(x) for x in row[1:]]\n                sensitivities[layername] = {}\n                for i, accuracy in enumerate(accuracies):\n                    sensitivities[layername][sparsities[i]] = accuracy\n            return sensitivities\n\n    def _max_prune_ratio(self, ori_acc, threshold, sensitivities):\n        \"\"\"\n        Find the maximum prune ratio for a single layer whose accuracy\n        drop is lower than the threshold.\n\n        Parameters\n        ----------\n        ori_acc: float\n            Original accuracy\n        threshold: float\n            Accuracy drop threshold\n        sensitivities: dict\n            The dict object that stores the sensitivity results for each layer.\n            For example: {'conv1' : {0.1: 0.9, 0.2 : 0.8}}\n        Returns\n        -------\n        max_ratios: dict\n            return the maximum prune ratio for each layer. For example:\n            {'conv1':0.1, 'conv2':0.2}\n        \"\"\"\n        max_ratio = {}\n        for layer in sensitivities:\n            prune_ratios = sorted(sensitivities[layer].keys())\n            last_ratio = 0\n            for ratio in prune_ratios:\n                last_ratio = ratio\n                cur_acc = sensitivities[layer][ratio]\n                if cur_acc + threshold < ori_acc:\n                    break\n            max_ratio[layer] = last_ratio\n        return max_ratio\n\n    def normalize(self, ratios, target_pruned):\n        \"\"\"\n        Normalize the prune ratio of each layer according to the\n        total already pruned ratio and the final target total pruning\n        ratio\n\n        Parameters\n        ----------\n            ratios:\n                Dict object that save the prune ratio for each layer\n            target_pruned:\n                The amount of the weights expected to be pruned in this\n                iteration\n\n        Returns\n        -------\n            new_ratios:\n                return the normalized prune ratios for each layer.\n\n        \"\"\"\n        w_sum = 0\n        _Max = 0\n        for layername, ratio in ratios.items():\n            wcount = self.weight_count[layername]\n            w_sum += ratio * wcount * \\\n                (1-self.analyzer.already_pruned[layername])\n        target_count = self.weight_sum * target_pruned\n        for layername in ratios:\n            ratios[layername] = ratios[layername] * target_count / w_sum\n            _Max = max(_Max, ratios[layername])\n        # Cannot Prune too much in a single iteration\n        # If a layer's prune ratio is larger than the\n        # MAX_PRUNE_RATIO_PER_ITER we rescal all prune\n        # ratios under this threshold\n        if _Max > MAX_PRUNE_RATIO_PER_ITER:\n\n            for layername in ratios:\n                ratios[layername] = ratios[layername] * \\\n                    MAX_PRUNE_RATIO_PER_ITER / _Max\n        return ratios\n\n    def create_cfg(self, ratios):\n        \"\"\"\n        Generate the cfg_list for the pruner according to the prune ratios.\n\n        Parameters\n        ---------\n            ratios:\n                For example: {'conv1' : 0.2}\n\n        Returns\n        -------\n            cfg_list:\n                For example: [{'sparsity':0.2, 'op_names':['conv1'], 'op_types':['Conv2d']}]\n        \"\"\"\n        cfg_list = []\n        for layername in ratios:\n            prune_ratio = ratios[layername]\n            remain = 1 - self.analyzer.already_pruned[layername]\n            sparsity = remain * prune_ratio + \\\n                self.analyzer.already_pruned[layername]\n            if sparsity > 0:\n                # Pruner does not allow the prune ratio to be zero\n                cfg = {'sparsity': sparsity, 'op_names': [\n                    layername], 'op_types': ['Conv2d']}\n                cfg_list.append(cfg)\n        return cfg_list\n\n    def current_sparsity(self):\n        \"\"\"\n        The sparsity of the weight.\n        \"\"\"\n        pruned_weight = 0\n        for layer_name in self.analyzer.already_pruned:\n            w_count = self.weight_count[layer_name]\n            prune_ratio = self.analyzer.already_pruned[layer_name]\n            pruned_weight += w_count * prune_ratio\n        return pruned_weight / self.weight_sum\n\n    def compress(self, eval_args=None, eval_kwargs=None,\n                 finetune_args=None, finetune_kwargs=None, resume_sensitivity=None):\n        \"\"\"\n        This function iteratively prune the model according to the results of\n        the sensitivity analysis.\n\n        Parameters\n        ----------\n        eval_args: list\n        eval_kwargs: list& dict\n            Parameters for the val_funtion, the val_function will be called like\n            evaluator(*eval_args, **eval_kwargs)\n        finetune_args: list\n        finetune_kwargs: dict\n            Parameters for the finetuner function if needed.\n        resume_sensitivity:\n            resume the sensitivity results from this file.\n        \"\"\"\n        # pylint suggest not use the empty list and dict\n        # as the default input parameter\n        if not eval_args:\n            eval_args = []\n        if not eval_kwargs:\n            eval_kwargs = {}\n        if not finetune_args:\n            finetune_args = []\n        if not finetune_kwargs:\n            finetune_kwargs = {}\n        if self.ori_acc is None:\n            self.ori_acc = self.evaluator(*eval_args, **eval_kwargs)\n        assert isinstance(self.ori_acc, float) or isinstance(self.ori_acc, int)\n        if not resume_sensitivity:\n            self.sensitivities = self.analyzer.analysis(\n                val_args=eval_args, val_kwargs=eval_kwargs)\n        else:\n            self.sensitivities = self.load_sensitivity(resume_sensitivity)\n            self.analyzer.sensitivities = self.sensitivities\n        # the final target sparsity of the model\n        target_ratio = 1 - self.config_list[0]['sparsity']\n        cur_ratio = self.remained_ratio\n        ori_acc = self.ori_acc\n        iteration_count = 0\n        if self.checkpoint_dir is not None:\n            os.makedirs(self.checkpoint_dir, exist_ok=True)\n        modules_wrapper_final = None\n        while cur_ratio > target_ratio:\n            iteration_count += 1\n            # Each round have three steps:\n            # 1) Get the current sensitivity for each layer(the sensitivity\n            # of each layer may change during the pruning)\n            # 2) Prune each layer according the sensitivies\n            # 3) finetune the model\n            _logger.info('Current base accuracy %f', ori_acc)\n            _logger.info('Remained %f weights', cur_ratio)\n            # determine the sparsity proportion between different\n            # layers according to the sensitivity result\n            proportion = self.sparsity_proportion_calc(\n                ori_acc, self.acc_drop_threshold, self.sensitivities)\n\n            new_pruneratio = self.normalize(proportion, self.sparsity_per_iter)\n            cfg_list = self.create_cfg(new_pruneratio)\n            if not cfg_list:\n                _logger.error('The threshold is too small, please set a larger threshold')\n                return self.model\n            _logger.debug('Pruner Config: %s', str(cfg_list))\n            cfg_str = ['%s:%.3f'%(cfg['op_names'][0], cfg['sparsity']) for cfg in cfg_list]\n            _logger.info('Current Sparsities: %s', ','.join(cfg_str))\n\n            pruner = self.Pruner(self.model, cfg_list)\n            pruner.compress()\n            pruned_acc = self.evaluator(*eval_args, **eval_kwargs)\n            _logger.info('Accuracy after pruning: %f', pruned_acc)\n            finetune_acc = pruned_acc\n            if self.finetuner is not None:\n                # if the finetune function is None, then skip the finetune\n                self.finetuner(*finetune_args, **finetune_kwargs)\n                finetune_acc = self.evaluator(*eval_args, **eval_kwargs)\n            _logger.info('Accuracy after finetune: %f', finetune_acc)\n            ori_acc = finetune_acc\n            # unwrap the pruner\n            pruner._unwrap_model()\n            # update the already prune ratio of each layer befor the new\n            # sensitivity analysis\n            for layer_cfg in cfg_list:\n                name = layer_cfg['op_names'][0]\n                sparsity = layer_cfg['sparsity']\n                self.analyzer.already_pruned[name] = sparsity\n            # update the cur_ratio\n            cur_ratio = 1 - self.current_sparsity()\n            modules_wrapper_final = pruner.get_modules_wrapper()\n            del pruner\n            _logger.info('Currently remained weights: %f', cur_ratio)\n\n            if self.checkpoint_dir is not None:\n                checkpoint_name = 'Iter_%d_finetune_acc_%.5f_sparsity_%.4f' % (\n                    iteration_count, finetune_acc, cur_ratio)\n                checkpoint_path = os.path.join(\n                    self.checkpoint_dir, '%s.pth' % checkpoint_name)\n                cfg_path = os.path.join(\n                    self.checkpoint_dir, '%s_pruner.json' % checkpoint_name)\n                sensitivity_path = os.path.join(\n                    self.checkpoint_dir, '%s_sensitivity.csv' % checkpoint_name)\n                torch.save(self.model.state_dict(), checkpoint_path)\n                with open(cfg_path, 'w') as jf:\n                    json.dump(cfg_list, jf)\n                self.analyzer.export(sensitivity_path)\n\n            if cur_ratio > target_ratio:\n                # If this is the last prune iteration, skip the time-consuming\n                # sensitivity analysis\n\n                self.analyzer.load_state_dict(self.model.state_dict())\n                self.sensitivities = self.analyzer.analysis(\n                    val_args=eval_args, val_kwargs=eval_kwargs)\n\n        _logger.info('After Pruning: %.2f weights remains', cur_ratio)\n        self.modules_wrapper = modules_wrapper_final\n\n        self._wrap_model()\n        return self.model\n\n    def calc_mask(self, wrapper, **kwargs):\n        return None",
  "def __init__(self, model, config_list, evaluator,\n                 finetuner=None, base_algo='l1', sparsity_proportion_calc=None,\n                 sparsity_per_iter=0.1, acc_drop_threshold=0.05, checkpoint_dir=None):\n\n        self.base_algo = base_algo\n        self.model = model\n        super(SensitivityPruner, self).__init__(model, config_list)\n        # unwrap the model\n        self._unwrap_model()\n        _logger.debug(str(self.model))\n        self.evaluator = evaluator\n        self.finetuner = finetuner\n        self.analyzer = SensitivityAnalysis(\n            self.model, self.evaluator, prune_type=base_algo, \\\n            early_stop_mode='dropped', early_stop_value=acc_drop_threshold)\n        # Get the original accuracy of the pretrained model\n        self.ori_acc = None\n        # Copy the original weights before pruning\n        self.ori_state_dict = copy.deepcopy(self.model.state_dict())\n        self.sensitivities = {}\n        # Save the weight count for each layer\n        self.weight_count = {}\n        self.weight_sum = 0\n        # Map the layer name to the layer module\n        self.named_module = {}\n\n        self.Pruner = PRUNER_DICT[self.base_algo]\n        # Count the total weight count of the model\n        for name, submodule in self.model.named_modules():\n            self.named_module[name] = submodule\n            if name in self.analyzer.target_layer:\n                # Currently, only count the weights in the conv layers\n                # else the fully connected layer (which contains\n                # the most weights) may make the pruner prune the\n                # model too hard\n                # if hasattr(submodule, 'weight'): # Count all the weights of the model\n                self.weight_count[name] = submodule.weight.data.numel()\n                self.weight_sum += self.weight_count[name]\n        # function to generate the sparsity proportion between the conv layers\n        if sparsity_proportion_calc is None:\n            self.sparsity_proportion_calc = self._max_prune_ratio\n        else:\n            self.sparsity_proportion_calc = sparsity_proportion_calc\n        # The ratio of remained weights is 1.0 at the begining\n        self.remained_ratio = 1.0\n        self.sparsity_per_iter = sparsity_per_iter\n        self.acc_drop_threshold = acc_drop_threshold\n        self.checkpoint_dir = checkpoint_dir",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self.base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self.base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)",
  "def load_sensitivity(self, filepath):\n        \"\"\"\n        load the sensitivity results exported by the sensitivity analyzer\n        \"\"\"\n        assert os.path.exists(filepath)\n        with open(filepath, 'r') as csvf:\n            csv_r = csv.reader(csvf)\n            header = next(csv_r)\n            sparsities = [float(x) for x in header[1:]]\n            sensitivities = {}\n            for row in csv_r:\n                layername = row[0]\n                accuracies = [float(x) for x in row[1:]]\n                sensitivities[layername] = {}\n                for i, accuracy in enumerate(accuracies):\n                    sensitivities[layername][sparsities[i]] = accuracy\n            return sensitivities",
  "def _max_prune_ratio(self, ori_acc, threshold, sensitivities):\n        \"\"\"\n        Find the maximum prune ratio for a single layer whose accuracy\n        drop is lower than the threshold.\n\n        Parameters\n        ----------\n        ori_acc: float\n            Original accuracy\n        threshold: float\n            Accuracy drop threshold\n        sensitivities: dict\n            The dict object that stores the sensitivity results for each layer.\n            For example: {'conv1' : {0.1: 0.9, 0.2 : 0.8}}\n        Returns\n        -------\n        max_ratios: dict\n            return the maximum prune ratio for each layer. For example:\n            {'conv1':0.1, 'conv2':0.2}\n        \"\"\"\n        max_ratio = {}\n        for layer in sensitivities:\n            prune_ratios = sorted(sensitivities[layer].keys())\n            last_ratio = 0\n            for ratio in prune_ratios:\n                last_ratio = ratio\n                cur_acc = sensitivities[layer][ratio]\n                if cur_acc + threshold < ori_acc:\n                    break\n            max_ratio[layer] = last_ratio\n        return max_ratio",
  "def normalize(self, ratios, target_pruned):\n        \"\"\"\n        Normalize the prune ratio of each layer according to the\n        total already pruned ratio and the final target total pruning\n        ratio\n\n        Parameters\n        ----------\n            ratios:\n                Dict object that save the prune ratio for each layer\n            target_pruned:\n                The amount of the weights expected to be pruned in this\n                iteration\n\n        Returns\n        -------\n            new_ratios:\n                return the normalized prune ratios for each layer.\n\n        \"\"\"\n        w_sum = 0\n        _Max = 0\n        for layername, ratio in ratios.items():\n            wcount = self.weight_count[layername]\n            w_sum += ratio * wcount * \\\n                (1-self.analyzer.already_pruned[layername])\n        target_count = self.weight_sum * target_pruned\n        for layername in ratios:\n            ratios[layername] = ratios[layername] * target_count / w_sum\n            _Max = max(_Max, ratios[layername])\n        # Cannot Prune too much in a single iteration\n        # If a layer's prune ratio is larger than the\n        # MAX_PRUNE_RATIO_PER_ITER we rescal all prune\n        # ratios under this threshold\n        if _Max > MAX_PRUNE_RATIO_PER_ITER:\n\n            for layername in ratios:\n                ratios[layername] = ratios[layername] * \\\n                    MAX_PRUNE_RATIO_PER_ITER / _Max\n        return ratios",
  "def create_cfg(self, ratios):\n        \"\"\"\n        Generate the cfg_list for the pruner according to the prune ratios.\n\n        Parameters\n        ---------\n            ratios:\n                For example: {'conv1' : 0.2}\n\n        Returns\n        -------\n            cfg_list:\n                For example: [{'sparsity':0.2, 'op_names':['conv1'], 'op_types':['Conv2d']}]\n        \"\"\"\n        cfg_list = []\n        for layername in ratios:\n            prune_ratio = ratios[layername]\n            remain = 1 - self.analyzer.already_pruned[layername]\n            sparsity = remain * prune_ratio + \\\n                self.analyzer.already_pruned[layername]\n            if sparsity > 0:\n                # Pruner does not allow the prune ratio to be zero\n                cfg = {'sparsity': sparsity, 'op_names': [\n                    layername], 'op_types': ['Conv2d']}\n                cfg_list.append(cfg)\n        return cfg_list",
  "def current_sparsity(self):\n        \"\"\"\n        The sparsity of the weight.\n        \"\"\"\n        pruned_weight = 0\n        for layer_name in self.analyzer.already_pruned:\n            w_count = self.weight_count[layer_name]\n            prune_ratio = self.analyzer.already_pruned[layer_name]\n            pruned_weight += w_count * prune_ratio\n        return pruned_weight / self.weight_sum",
  "def compress(self, eval_args=None, eval_kwargs=None,\n                 finetune_args=None, finetune_kwargs=None, resume_sensitivity=None):\n        \"\"\"\n        This function iteratively prune the model according to the results of\n        the sensitivity analysis.\n\n        Parameters\n        ----------\n        eval_args: list\n        eval_kwargs: list& dict\n            Parameters for the val_funtion, the val_function will be called like\n            evaluator(*eval_args, **eval_kwargs)\n        finetune_args: list\n        finetune_kwargs: dict\n            Parameters for the finetuner function if needed.\n        resume_sensitivity:\n            resume the sensitivity results from this file.\n        \"\"\"\n        # pylint suggest not use the empty list and dict\n        # as the default input parameter\n        if not eval_args:\n            eval_args = []\n        if not eval_kwargs:\n            eval_kwargs = {}\n        if not finetune_args:\n            finetune_args = []\n        if not finetune_kwargs:\n            finetune_kwargs = {}\n        if self.ori_acc is None:\n            self.ori_acc = self.evaluator(*eval_args, **eval_kwargs)\n        assert isinstance(self.ori_acc, float) or isinstance(self.ori_acc, int)\n        if not resume_sensitivity:\n            self.sensitivities = self.analyzer.analysis(\n                val_args=eval_args, val_kwargs=eval_kwargs)\n        else:\n            self.sensitivities = self.load_sensitivity(resume_sensitivity)\n            self.analyzer.sensitivities = self.sensitivities\n        # the final target sparsity of the model\n        target_ratio = 1 - self.config_list[0]['sparsity']\n        cur_ratio = self.remained_ratio\n        ori_acc = self.ori_acc\n        iteration_count = 0\n        if self.checkpoint_dir is not None:\n            os.makedirs(self.checkpoint_dir, exist_ok=True)\n        modules_wrapper_final = None\n        while cur_ratio > target_ratio:\n            iteration_count += 1\n            # Each round have three steps:\n            # 1) Get the current sensitivity for each layer(the sensitivity\n            # of each layer may change during the pruning)\n            # 2) Prune each layer according the sensitivies\n            # 3) finetune the model\n            _logger.info('Current base accuracy %f', ori_acc)\n            _logger.info('Remained %f weights', cur_ratio)\n            # determine the sparsity proportion between different\n            # layers according to the sensitivity result\n            proportion = self.sparsity_proportion_calc(\n                ori_acc, self.acc_drop_threshold, self.sensitivities)\n\n            new_pruneratio = self.normalize(proportion, self.sparsity_per_iter)\n            cfg_list = self.create_cfg(new_pruneratio)\n            if not cfg_list:\n                _logger.error('The threshold is too small, please set a larger threshold')\n                return self.model\n            _logger.debug('Pruner Config: %s', str(cfg_list))\n            cfg_str = ['%s:%.3f'%(cfg['op_names'][0], cfg['sparsity']) for cfg in cfg_list]\n            _logger.info('Current Sparsities: %s', ','.join(cfg_str))\n\n            pruner = self.Pruner(self.model, cfg_list)\n            pruner.compress()\n            pruned_acc = self.evaluator(*eval_args, **eval_kwargs)\n            _logger.info('Accuracy after pruning: %f', pruned_acc)\n            finetune_acc = pruned_acc\n            if self.finetuner is not None:\n                # if the finetune function is None, then skip the finetune\n                self.finetuner(*finetune_args, **finetune_kwargs)\n                finetune_acc = self.evaluator(*eval_args, **eval_kwargs)\n            _logger.info('Accuracy after finetune: %f', finetune_acc)\n            ori_acc = finetune_acc\n            # unwrap the pruner\n            pruner._unwrap_model()\n            # update the already prune ratio of each layer befor the new\n            # sensitivity analysis\n            for layer_cfg in cfg_list:\n                name = layer_cfg['op_names'][0]\n                sparsity = layer_cfg['sparsity']\n                self.analyzer.already_pruned[name] = sparsity\n            # update the cur_ratio\n            cur_ratio = 1 - self.current_sparsity()\n            modules_wrapper_final = pruner.get_modules_wrapper()\n            del pruner\n            _logger.info('Currently remained weights: %f', cur_ratio)\n\n            if self.checkpoint_dir is not None:\n                checkpoint_name = 'Iter_%d_finetune_acc_%.5f_sparsity_%.4f' % (\n                    iteration_count, finetune_acc, cur_ratio)\n                checkpoint_path = os.path.join(\n                    self.checkpoint_dir, '%s.pth' % checkpoint_name)\n                cfg_path = os.path.join(\n                    self.checkpoint_dir, '%s_pruner.json' % checkpoint_name)\n                sensitivity_path = os.path.join(\n                    self.checkpoint_dir, '%s_sensitivity.csv' % checkpoint_name)\n                torch.save(self.model.state_dict(), checkpoint_path)\n                with open(cfg_path, 'w') as jf:\n                    json.dump(cfg_list, jf)\n                self.analyzer.export(sensitivity_path)\n\n            if cur_ratio > target_ratio:\n                # If this is the last prune iteration, skip the time-consuming\n                # sensitivity analysis\n\n                self.analyzer.load_state_dict(self.model.state_dict())\n                self.sensitivities = self.analyzer.analysis(\n                    val_args=eval_args, val_kwargs=eval_kwargs)\n\n        _logger.info('After Pruning: %.2f weights remains', cur_ratio)\n        self.modules_wrapper = modules_wrapper_final\n\n        self._wrap_model()\n        return self.model",
  "def calc_mask(self, wrapper, **kwargs):\n        return None",
  "class LotteryTicketPruner(Pruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : pytorch model\n        The model to be pruned\n    config_list : list\n        Supported keys:\n            - prune_iterations : The number of rounds for the iterative pruning.\n            - sparsity : The final sparsity when the compression is done.\n    optimizer : pytorch optimizer\n        The optimizer for the model\n    lr_scheduler : pytorch lr scheduler\n        The lr scheduler for the model if used\n    reset_weights : bool\n        Whether reset weights and optimizer at the beginning of each round.\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None, lr_scheduler=None, reset_weights=True):\n        # save init weights and optimizer\n        self.reset_weights = reset_weights\n        if self.reset_weights:\n            self._model = model\n            self._optimizer = optimizer\n            self._model_state = copy.deepcopy(model.state_dict())\n            self._optimizer_state = copy.deepcopy(optimizer.state_dict())\n            self._lr_scheduler = lr_scheduler\n            if lr_scheduler is not None:\n                self._scheduler_state = copy.deepcopy(lr_scheduler.state_dict())\n\n        super().__init__(model, config_list, optimizer)\n        self.curr_prune_iteration = None\n        self.prune_iterations = config_list[0]['prune_iterations']\n        self.masker = LevelPrunerMasker(model, self)\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            Supported keys:\n                - prune_iterations : The number of rounds for the iterative pruning.\n                - sparsity : The final sparsity when the compression is done.\n        \"\"\"\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            'prune_iterations': And(int, lambda n: n > 0),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n        assert len(set([x['prune_iterations'] for x in config_list])) == 1, 'The values of prune_iterations must be equal in your config'\n\n    def _calc_sparsity(self, sparsity):\n        keep_ratio_once = (1 - sparsity) ** (1 / self.prune_iterations)\n        curr_keep_ratio = keep_ratio_once ** self.curr_prune_iteration\n        return max(1 - curr_keep_ratio, 0)\n\n    def _calc_mask(self, wrapper, sparsity):\n        weight = wrapper.module.weight.data\n        if self.curr_prune_iteration == 0:\n            mask = {'weight_mask': torch.ones(weight.shape).type_as(weight)}\n        else:\n            curr_sparsity = self._calc_sparsity(sparsity)\n            mask = self.masker.calc_mask(sparsity=curr_sparsity, wrapper=wrapper)\n        return mask\n\n    def calc_mask(self, wrapper, **kwargs):\n        \"\"\"\n        Generate mask for the given ``weight``.\n\n        Parameters\n        ----------\n        wrapper : Module\n            The layer to be pruned\n\n        Returns\n        -------\n        tensor\n            The mask for this weight, it is ```None``` because this pruner\n            calculates and assigns masks in ```prune_iteration_start```,\n            no need to do anything in this function.\n        \"\"\"\n        return None\n\n    def get_prune_iterations(self):\n        \"\"\"\n        Return the range for iterations.\n        In the first prune iteration, masks are all one, thus, add one more iteration\n\n        Returns\n        -------\n        list\n            A list for pruning iterations\n        \"\"\"\n        return range(self.prune_iterations + 1)\n\n    def prune_iteration_start(self):\n        \"\"\"\n        Control the pruning procedure on updated epoch number.\n        Should be called at the beginning of the epoch.\n        \"\"\"\n        if self.curr_prune_iteration is None:\n            self.curr_prune_iteration = 0\n        else:\n            self.curr_prune_iteration += 1\n        assert self.curr_prune_iteration < self.prune_iterations + 1, 'Exceed the configured prune_iterations'\n\n        modules_wrapper = self.get_modules_wrapper()\n        modules_to_compress = self.get_modules_to_compress()\n        for layer, config in modules_to_compress:\n            module_wrapper = None\n            for wrapper in modules_wrapper:\n                if wrapper.name == layer.name:\n                    module_wrapper = wrapper\n                    break\n            assert module_wrapper is not None\n\n            sparsity = config.get('sparsity')\n            mask = self._calc_mask(module_wrapper, sparsity)\n            # TODO: directly use weight_mask is not good\n            module_wrapper.weight_mask = mask['weight_mask']\n            # there is no mask for bias\n\n        # reinit weights back to original after new masks are generated\n        if self.reset_weights:\n            # should use this member function to reset model weights\n            self.load_model_state_dict(self._model_state)\n            self._optimizer.load_state_dict(self._optimizer_state)\n            if self._lr_scheduler is not None:\n                self._lr_scheduler.load_state_dict(self._scheduler_state)",
  "def __init__(self, model, config_list, optimizer=None, lr_scheduler=None, reset_weights=True):\n        # save init weights and optimizer\n        self.reset_weights = reset_weights\n        if self.reset_weights:\n            self._model = model\n            self._optimizer = optimizer\n            self._model_state = copy.deepcopy(model.state_dict())\n            self._optimizer_state = copy.deepcopy(optimizer.state_dict())\n            self._lr_scheduler = lr_scheduler\n            if lr_scheduler is not None:\n                self._scheduler_state = copy.deepcopy(lr_scheduler.state_dict())\n\n        super().__init__(model, config_list, optimizer)\n        self.curr_prune_iteration = None\n        self.prune_iterations = config_list[0]['prune_iterations']\n        self.masker = LevelPrunerMasker(model, self)",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            Supported keys:\n                - prune_iterations : The number of rounds for the iterative pruning.\n                - sparsity : The final sparsity when the compression is done.\n        \"\"\"\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            'prune_iterations': And(int, lambda n: n > 0),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n        assert len(set([x['prune_iterations'] for x in config_list])) == 1, 'The values of prune_iterations must be equal in your config'",
  "def _calc_sparsity(self, sparsity):\n        keep_ratio_once = (1 - sparsity) ** (1 / self.prune_iterations)\n        curr_keep_ratio = keep_ratio_once ** self.curr_prune_iteration\n        return max(1 - curr_keep_ratio, 0)",
  "def _calc_mask(self, wrapper, sparsity):\n        weight = wrapper.module.weight.data\n        if self.curr_prune_iteration == 0:\n            mask = {'weight_mask': torch.ones(weight.shape).type_as(weight)}\n        else:\n            curr_sparsity = self._calc_sparsity(sparsity)\n            mask = self.masker.calc_mask(sparsity=curr_sparsity, wrapper=wrapper)\n        return mask",
  "def calc_mask(self, wrapper, **kwargs):\n        \"\"\"\n        Generate mask for the given ``weight``.\n\n        Parameters\n        ----------\n        wrapper : Module\n            The layer to be pruned\n\n        Returns\n        -------\n        tensor\n            The mask for this weight, it is ```None``` because this pruner\n            calculates and assigns masks in ```prune_iteration_start```,\n            no need to do anything in this function.\n        \"\"\"\n        return None",
  "def get_prune_iterations(self):\n        \"\"\"\n        Return the range for iterations.\n        In the first prune iteration, masks are all one, thus, add one more iteration\n\n        Returns\n        -------\n        list\n            A list for pruning iterations\n        \"\"\"\n        return range(self.prune_iterations + 1)",
  "def prune_iteration_start(self):\n        \"\"\"\n        Control the pruning procedure on updated epoch number.\n        Should be called at the beginning of the epoch.\n        \"\"\"\n        if self.curr_prune_iteration is None:\n            self.curr_prune_iteration = 0\n        else:\n            self.curr_prune_iteration += 1\n        assert self.curr_prune_iteration < self.prune_iterations + 1, 'Exceed the configured prune_iterations'\n\n        modules_wrapper = self.get_modules_wrapper()\n        modules_to_compress = self.get_modules_to_compress()\n        for layer, config in modules_to_compress:\n            module_wrapper = None\n            for wrapper in modules_wrapper:\n                if wrapper.name == layer.name:\n                    module_wrapper = wrapper\n                    break\n            assert module_wrapper is not None\n\n            sparsity = config.get('sparsity')\n            mask = self._calc_mask(module_wrapper, sparsity)\n            # TODO: directly use weight_mask is not good\n            module_wrapper.weight_mask = mask['weight_mask']\n            # there is no mask for bias\n\n        # reinit weights back to original after new masks are generated\n        if self.reset_weights:\n            # should use this member function to reset model weights\n            self.load_model_state_dict(self._model_state)\n            self._optimizer.load_state_dict(self._optimizer_state)\n            if self._lr_scheduler is not None:\n                self._lr_scheduler.load_state_dict(self._scheduler_state)",
  "class LevelPrunerMasker(WeightMasker):\n    \"\"\"\n    Prune to an exact pruning level specification\n    \"\"\"\n\n    def calc_mask(self, sparsity, wrapper, wrapper_idx=None):\n        weight = wrapper.module.weight.data.clone()\n        if wrapper.weight_mask is not None:\n            # apply base mask for iterative pruning\n            weight = weight * wrapper.weight_mask\n\n        w_abs = weight.abs()\n        k = int(weight.numel() * sparsity)\n        if k == 0:\n            return {'weight_mask': torch.ones(weight.shape).type_as(weight)}\n        threshold = torch.topk(w_abs.view(-1), k, largest=False)[0].max()\n        mask_weight = torch.gt(w_abs, threshold).type_as(weight)\n        mask = {'weight_mask': mask_weight}\n        return mask",
  "def calc_mask(self, sparsity, wrapper, wrapper_idx=None):\n        weight = wrapper.module.weight.data.clone()\n        if wrapper.weight_mask is not None:\n            # apply base mask for iterative pruning\n            weight = weight * wrapper.weight_mask\n\n        w_abs = weight.abs()\n        k = int(weight.numel() * sparsity)\n        if k == 0:\n            return {'weight_mask': torch.ones(weight.shape).type_as(weight)}\n        threshold = torch.topk(w_abs.view(-1), k, largest=False)[0].max()\n        mask_weight = torch.gt(w_abs, threshold).type_as(weight)\n        mask = {'weight_mask': mask_weight}\n        return mask",
  "class NetAdaptPruner(Pruner):\n    \"\"\"\n    A Pytorch implementation of NetAdapt compression algorithm.\n\n    Parameters\n    ----------\n    model : pytorch model\n        The model to be pruned.\n    config_list : list\n        Supported keys:\n            - sparsity : The target overall sparsity.\n            - op_types : The operation type to prune.\n    short_term_fine_tuner : function\n        function to short-term fine tune the masked model.\n        This function should include `model` as the only parameter,\n        and fine tune the model for a short term after each pruning iteration.\n        Example::\n\n            def short_term_fine_tuner(model, epoch=3):\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n                train_loader = ...\n                criterion = torch.nn.CrossEntropyLoss()\n                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n                model.train()\n                for _ in range(epoch):\n                    for batch_idx, (data, target) in enumerate(train_loader):\n                        data, target = data.to(device), target.to(device)\n                        optimizer.zero_grad()\n                        output = model(data)\n                        loss = criterion(output, target)\n                        loss.backward()\n                        optimizer.step()\n    evaluator : function\n        function to evaluate the masked model.\n        This function should include `model` as the only parameter, and returns a scalar value.\n        Example::\n\n            def evaluator(model):\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n                val_loader = ...\n                model.eval()\n                correct = 0\n                with torch.no_grad():\n                    for data, target in val_loader:\n                        data, target = data.to(device), target.to(device)\n                        output = model(data)\n                        # get the index of the max log-probability\n                        pred = output.argmax(dim=1, keepdim=True)\n                        correct += pred.eq(target.view_as(pred)).sum().item()\n                accuracy = correct / len(val_loader.dataset)\n                return accuracy\n    optimize_mode : str\n        optimize mode, `maximize` or `minimize`, by default `maximize`.\n    base_algo : str\n        Base pruning algorithm. `level`, `l1` or `l2`, by default `l1`. Given the sparsity distribution among the ops,\n        the assigned `base_algo` is used to decide which filters/channels/weights to prune.\n    sparsity_per_iteration : float\n        sparsity to prune in each iteration.\n    experiment_data_dir : str\n        PATH to save experiment data,\n        including the config_list generated for the base pruning algorithm and the performance of the pruned model.\n    \"\"\"\n\n    def __init__(self, model, config_list, short_term_fine_tuner, evaluator,\n                 optimize_mode='maximize', base_algo='l1', sparsity_per_iteration=0.05, experiment_data_dir='./'):\n        # models used for iterative pruning and evaluation\n        self._model_to_prune = copy.deepcopy(model)\n        self._base_algo = base_algo\n\n        super().__init__(model, config_list)\n\n        self._short_term_fine_tuner = short_term_fine_tuner\n        self._evaluator = evaluator\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # hyper parameters for NetAdapt algorithm\n        self._sparsity_per_iteration = sparsity_per_iteration\n\n        # overall pruning rate\n        self._sparsity = config_list[0]['sparsity']\n\n        # config_list\n        self._config_list_generated = []\n\n        self._experiment_data_dir = experiment_data_dir\n        if not os.path.exists(self._experiment_data_dir):\n            os.makedirs(self._experiment_data_dir)\n\n        self._tmp_model_path = os.path.join(self._experiment_data_dir, 'tmp_model.pth')\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self._base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self._base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)\n\n    def calc_mask(self, wrapper, **kwargs):\n        return None\n\n    def _update_config_list(self, config_list, op_name, sparsity):\n        '''\n        update sparsity of op_name in config_list\n        '''\n        config_list_updated = copy.deepcopy(config_list)\n\n        for idx, item in enumerate(config_list):\n            if op_name in item['op_names']:\n                config_list_updated[idx]['sparsity'] = sparsity\n                return config_list_updated\n\n        # if op_name is not in self._config_list_generated, create a new json item\n        if self._base_algo in ['l1', 'l2']:\n            config_list_updated.append(\n                {'sparsity': sparsity, 'op_types': ['Conv2d'], 'op_names': [op_name]})\n        elif self._base_algo == 'level':\n            config_list_updated.append(\n                {'sparsity': sparsity, 'op_names': [op_name]})\n\n        return config_list_updated\n\n    def _get_op_num_weights_remained(self, op_name, module):\n        '''\n        Get the number of weights remained after channel pruning with current sparsity\n\n        Returns\n        -------\n        int\n            remained number of weights of the op\n        '''\n\n        # if op is wrapped by the pruner\n        for wrapper in self.get_modules_wrapper():\n            if wrapper.name == op_name:\n                return wrapper.weight_mask.sum().item()\n\n        # if op is not wrapped by the pruner\n        return module.weight.data.numel()\n\n    def _get_op_sparsity(self, op_name):\n        for config in self._config_list_generated:\n            if 'op_names' in config and op_name in config['op_names']:\n                return config['sparsity']\n        return 0\n\n    def _calc_num_related_weights(self, op_name):\n        '''\n        Calculate total number weights of the op and the next op, applicable only for models without dependencies among ops\n\n        Parameters\n        ----------\n        op_name : str\n\n        Returns\n        -------\n        int\n            total number of all the realted (current and the next) op weights\n        '''\n        num_weights = 0\n        flag_found = False\n        previous_name = None\n        previous_module = None\n\n        for name, module in self._model_to_prune.named_modules():\n            if not flag_found and name != op_name and type(module).__name__ in ['Conv2d', 'Linear']:\n                previous_name = name\n                previous_module = module\n            if not flag_found and name == op_name:\n                _logger.debug(\"original module found: %s\", name)\n                num_weights = module.weight.data.numel()\n\n                # consider related pruning in this op caused by previous op's pruning\n                if previous_module:\n                    sparsity_previous_op = self._get_op_sparsity(previous_name)\n                    if sparsity_previous_op:\n                        _logger.debug(\n                            \"decrease op's weights by %s due to previous op %s's pruning...\", sparsity_previous_op, previous_name)\n                        num_weights *= (1-sparsity_previous_op)\n\n                flag_found = True\n                continue\n            if flag_found and type(module).__name__ in ['Conv2d', 'Linear']:\n                _logger.debug(\"related module found: %s\", name)\n                # channel/filter pruning crossing is considered here, so only the num_weights after channel pruning is valuable\n                num_weights += self._get_op_num_weights_remained(name, module)\n                break\n\n        _logger.debug(\"num related weights of op %s : %d\", op_name, num_weights)\n\n        return num_weights\n\n    def compress(self):\n        \"\"\"\n        Compress the model.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        _logger.info('Starting NetAdapt Compression...')\n\n        pruning_iteration = 0\n        current_sparsity = 0\n        delta_num_weights_per_iteration = \\\n            int(get_total_num_weights(self._model_to_prune, ['Conv2d', 'Linear']) * self._sparsity_per_iteration)\n\n        # stop condition\n        while current_sparsity < self._sparsity:\n            _logger.info('Pruning iteration: %d', pruning_iteration)\n\n            # calculate target sparsity of this iteration\n            target_sparsity = current_sparsity + self._sparsity_per_iteration\n\n            # variable to store the info of the best layer found in this iteration\n            best_op = {}\n\n            for wrapper in self.get_modules_wrapper():\n                _logger.debug(\"op name : %s\", wrapper.name)\n                _logger.debug(\"op weights : %d\", wrapper.weight_mask.numel())\n                _logger.debug(\"op left weights : %d\", wrapper.weight_mask.sum().item())\n\n                current_op_sparsity = 1 - wrapper.weight_mask.sum().item() / wrapper.weight_mask.numel()\n                _logger.debug(\"current op sparsity : %s\", current_op_sparsity)\n\n                # sparsity that this layer needs to prune to satisfy the requirement\n                target_op_sparsity = current_op_sparsity + delta_num_weights_per_iteration / self._calc_num_related_weights(wrapper.name)\n\n                if target_op_sparsity >= 1:\n                    _logger.info('Layer %s has no enough weights (remained) to prune', wrapper.name)\n                    continue\n\n                config_list = self._update_config_list(self._config_list_generated, wrapper.name, target_op_sparsity)\n                _logger.debug(\"config_list used : %s\", config_list)\n\n                pruner = PRUNER_DICT[self._base_algo](copy.deepcopy(self._model_to_prune), config_list)\n                model_masked = pruner.compress()\n\n                # Short-term fine tune the pruned model\n                self._short_term_fine_tuner(model_masked)\n\n                performance = self._evaluator(model_masked)\n                _logger.info(\"Layer : %s, evaluation result after short-term fine tuning : %s\", wrapper.name, performance)\n\n                if not best_op \\\n                    or (self._optimize_mode is OptimizeMode.Maximize and performance > best_op['performance']) \\\n                    or (self._optimize_mode is OptimizeMode.Minimize and performance < best_op['performance']):\n                    _logger.debug(\"updating best layer to %s...\", wrapper.name)\n                    # find weight mask of this layer\n                    for w in pruner.get_modules_wrapper():\n                        if w.name == wrapper.name:\n                            masks = {'weight_mask': w.weight_mask,\n                                     'bias_mask': w.bias_mask}\n                            break\n                    best_op = {\n                        'op_name': wrapper.name,\n                        'sparsity': target_op_sparsity,\n                        'performance': performance,\n                        'masks': masks\n                    }\n\n                    # save model weights\n                    pruner.export_model(self._tmp_model_path)\n\n            if not best_op:\n                # decrease pruning step\n                self._sparsity_per_iteration *= 0.5\n                _logger.info(\"No more layers to prune, decrease pruning step to %s\", self._sparsity_per_iteration)\n                continue\n\n            # Pick the best layer to prune, update iterative information\n            # update config_list\n            self._config_list_generated = self._update_config_list(\n                self._config_list_generated, best_op['op_name'], best_op['sparsity'])\n\n            # update weights parameters\n            self._model_to_prune.load_state_dict(torch.load(self._tmp_model_path))\n\n            # update mask of the chosen op\n            for wrapper in self.get_modules_wrapper():\n                if wrapper.name == best_op['op_name']:\n                    for k in best_op['masks']:\n                        setattr(wrapper, k, best_op['masks'][k])\n                    break\n\n            current_sparsity = target_sparsity\n            _logger.info('Pruning iteration %d finished, current sparsity: %s', pruning_iteration, current_sparsity)\n            _logger.info('Layer %s seleted with sparsity %s, performance after pruning & short term fine-tuning : %s',\n                         best_op['op_name'], best_op['sparsity'], best_op['performance'])\n            pruning_iteration += 1\n\n            self._final_performance = best_op['performance']\n\n        # load weights parameters\n        self.load_model_state_dict(torch.load(self._tmp_model_path))\n        os.remove(self._tmp_model_path)\n\n        _logger.info('----------Compression finished--------------')\n        _logger.info('config_list generated: %s', self._config_list_generated)\n        _logger.info(\"Performance after pruning: %s\", self._final_performance)\n        _logger.info(\"Masked sparsity: %.6f\", current_sparsity)\n\n        # save best config found and best performance\n        with open(os.path.join(self._experiment_data_dir, 'search_result.json'), 'w') as jsonfile:\n            json.dump({\n                'performance': self._final_performance,\n                'config_list': json.dumps(self._config_list_generated)\n            }, jsonfile)\n\n        _logger.info('search history and result saved to foler : %s', self._experiment_data_dir)\n\n        return self.bound_model",
  "def __init__(self, model, config_list, short_term_fine_tuner, evaluator,\n                 optimize_mode='maximize', base_algo='l1', sparsity_per_iteration=0.05, experiment_data_dir='./'):\n        # models used for iterative pruning and evaluation\n        self._model_to_prune = copy.deepcopy(model)\n        self._base_algo = base_algo\n\n        super().__init__(model, config_list)\n\n        self._short_term_fine_tuner = short_term_fine_tuner\n        self._evaluator = evaluator\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # hyper parameters for NetAdapt algorithm\n        self._sparsity_per_iteration = sparsity_per_iteration\n\n        # overall pruning rate\n        self._sparsity = config_list[0]['sparsity']\n\n        # config_list\n        self._config_list_generated = []\n\n        self._experiment_data_dir = experiment_data_dir\n        if not os.path.exists(self._experiment_data_dir):\n            os.makedirs(self._experiment_data_dir)\n\n        self._tmp_model_path = os.path.join(self._experiment_data_dir, 'tmp_model.pth')",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self._base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self._base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)",
  "def calc_mask(self, wrapper, **kwargs):\n        return None",
  "def _update_config_list(self, config_list, op_name, sparsity):\n        '''\n        update sparsity of op_name in config_list\n        '''\n        config_list_updated = copy.deepcopy(config_list)\n\n        for idx, item in enumerate(config_list):\n            if op_name in item['op_names']:\n                config_list_updated[idx]['sparsity'] = sparsity\n                return config_list_updated\n\n        # if op_name is not in self._config_list_generated, create a new json item\n        if self._base_algo in ['l1', 'l2']:\n            config_list_updated.append(\n                {'sparsity': sparsity, 'op_types': ['Conv2d'], 'op_names': [op_name]})\n        elif self._base_algo == 'level':\n            config_list_updated.append(\n                {'sparsity': sparsity, 'op_names': [op_name]})\n\n        return config_list_updated",
  "def _get_op_num_weights_remained(self, op_name, module):\n        '''\n        Get the number of weights remained after channel pruning with current sparsity\n\n        Returns\n        -------\n        int\n            remained number of weights of the op\n        '''\n\n        # if op is wrapped by the pruner\n        for wrapper in self.get_modules_wrapper():\n            if wrapper.name == op_name:\n                return wrapper.weight_mask.sum().item()\n\n        # if op is not wrapped by the pruner\n        return module.weight.data.numel()",
  "def _get_op_sparsity(self, op_name):\n        for config in self._config_list_generated:\n            if 'op_names' in config and op_name in config['op_names']:\n                return config['sparsity']\n        return 0",
  "def _calc_num_related_weights(self, op_name):\n        '''\n        Calculate total number weights of the op and the next op, applicable only for models without dependencies among ops\n\n        Parameters\n        ----------\n        op_name : str\n\n        Returns\n        -------\n        int\n            total number of all the realted (current and the next) op weights\n        '''\n        num_weights = 0\n        flag_found = False\n        previous_name = None\n        previous_module = None\n\n        for name, module in self._model_to_prune.named_modules():\n            if not flag_found and name != op_name and type(module).__name__ in ['Conv2d', 'Linear']:\n                previous_name = name\n                previous_module = module\n            if not flag_found and name == op_name:\n                _logger.debug(\"original module found: %s\", name)\n                num_weights = module.weight.data.numel()\n\n                # consider related pruning in this op caused by previous op's pruning\n                if previous_module:\n                    sparsity_previous_op = self._get_op_sparsity(previous_name)\n                    if sparsity_previous_op:\n                        _logger.debug(\n                            \"decrease op's weights by %s due to previous op %s's pruning...\", sparsity_previous_op, previous_name)\n                        num_weights *= (1-sparsity_previous_op)\n\n                flag_found = True\n                continue\n            if flag_found and type(module).__name__ in ['Conv2d', 'Linear']:\n                _logger.debug(\"related module found: %s\", name)\n                # channel/filter pruning crossing is considered here, so only the num_weights after channel pruning is valuable\n                num_weights += self._get_op_num_weights_remained(name, module)\n                break\n\n        _logger.debug(\"num related weights of op %s : %d\", op_name, num_weights)\n\n        return num_weights",
  "def compress(self):\n        \"\"\"\n        Compress the model.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        _logger.info('Starting NetAdapt Compression...')\n\n        pruning_iteration = 0\n        current_sparsity = 0\n        delta_num_weights_per_iteration = \\\n            int(get_total_num_weights(self._model_to_prune, ['Conv2d', 'Linear']) * self._sparsity_per_iteration)\n\n        # stop condition\n        while current_sparsity < self._sparsity:\n            _logger.info('Pruning iteration: %d', pruning_iteration)\n\n            # calculate target sparsity of this iteration\n            target_sparsity = current_sparsity + self._sparsity_per_iteration\n\n            # variable to store the info of the best layer found in this iteration\n            best_op = {}\n\n            for wrapper in self.get_modules_wrapper():\n                _logger.debug(\"op name : %s\", wrapper.name)\n                _logger.debug(\"op weights : %d\", wrapper.weight_mask.numel())\n                _logger.debug(\"op left weights : %d\", wrapper.weight_mask.sum().item())\n\n                current_op_sparsity = 1 - wrapper.weight_mask.sum().item() / wrapper.weight_mask.numel()\n                _logger.debug(\"current op sparsity : %s\", current_op_sparsity)\n\n                # sparsity that this layer needs to prune to satisfy the requirement\n                target_op_sparsity = current_op_sparsity + delta_num_weights_per_iteration / self._calc_num_related_weights(wrapper.name)\n\n                if target_op_sparsity >= 1:\n                    _logger.info('Layer %s has no enough weights (remained) to prune', wrapper.name)\n                    continue\n\n                config_list = self._update_config_list(self._config_list_generated, wrapper.name, target_op_sparsity)\n                _logger.debug(\"config_list used : %s\", config_list)\n\n                pruner = PRUNER_DICT[self._base_algo](copy.deepcopy(self._model_to_prune), config_list)\n                model_masked = pruner.compress()\n\n                # Short-term fine tune the pruned model\n                self._short_term_fine_tuner(model_masked)\n\n                performance = self._evaluator(model_masked)\n                _logger.info(\"Layer : %s, evaluation result after short-term fine tuning : %s\", wrapper.name, performance)\n\n                if not best_op \\\n                    or (self._optimize_mode is OptimizeMode.Maximize and performance > best_op['performance']) \\\n                    or (self._optimize_mode is OptimizeMode.Minimize and performance < best_op['performance']):\n                    _logger.debug(\"updating best layer to %s...\", wrapper.name)\n                    # find weight mask of this layer\n                    for w in pruner.get_modules_wrapper():\n                        if w.name == wrapper.name:\n                            masks = {'weight_mask': w.weight_mask,\n                                     'bias_mask': w.bias_mask}\n                            break\n                    best_op = {\n                        'op_name': wrapper.name,\n                        'sparsity': target_op_sparsity,\n                        'performance': performance,\n                        'masks': masks\n                    }\n\n                    # save model weights\n                    pruner.export_model(self._tmp_model_path)\n\n            if not best_op:\n                # decrease pruning step\n                self._sparsity_per_iteration *= 0.5\n                _logger.info(\"No more layers to prune, decrease pruning step to %s\", self._sparsity_per_iteration)\n                continue\n\n            # Pick the best layer to prune, update iterative information\n            # update config_list\n            self._config_list_generated = self._update_config_list(\n                self._config_list_generated, best_op['op_name'], best_op['sparsity'])\n\n            # update weights parameters\n            self._model_to_prune.load_state_dict(torch.load(self._tmp_model_path))\n\n            # update mask of the chosen op\n            for wrapper in self.get_modules_wrapper():\n                if wrapper.name == best_op['op_name']:\n                    for k in best_op['masks']:\n                        setattr(wrapper, k, best_op['masks'][k])\n                    break\n\n            current_sparsity = target_sparsity\n            _logger.info('Pruning iteration %d finished, current sparsity: %s', pruning_iteration, current_sparsity)\n            _logger.info('Layer %s seleted with sparsity %s, performance after pruning & short term fine-tuning : %s',\n                         best_op['op_name'], best_op['sparsity'], best_op['performance'])\n            pruning_iteration += 1\n\n            self._final_performance = best_op['performance']\n\n        # load weights parameters\n        self.load_model_state_dict(torch.load(self._tmp_model_path))\n        os.remove(self._tmp_model_path)\n\n        _logger.info('----------Compression finished--------------')\n        _logger.info('config_list generated: %s', self._config_list_generated)\n        _logger.info(\"Performance after pruning: %s\", self._final_performance)\n        _logger.info(\"Masked sparsity: %.6f\", current_sparsity)\n\n        # save best config found and best performance\n        with open(os.path.join(self._experiment_data_dir, 'search_result.json'), 'w') as jsonfile:\n            json.dump({\n                'performance': self._final_performance,\n                'config_list': json.dumps(self._config_list_generated)\n            }, jsonfile)\n\n        _logger.info('search history and result saved to foler : %s', self._experiment_data_dir)\n\n        return self.bound_model",
  "class SimulatedAnnealingPruner(Pruner):\n    \"\"\"\n    A Pytorch implementation of Simulated Annealing compression algorithm.\n\n    Parameters\n    ----------\n    model : pytorch model\n        The model to be pruned.\n    config_list : list\n        Supported keys:\n            - sparsity : The target overall sparsity.\n            - op_types : The operation type to prune.\n    evaluator : function\n        Function to evaluate the pruned model.\n        This function should include `model` as the only parameter, and returns a scalar value.\n        Example::\n\n            def evaluator(model):\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n                val_loader = ...\n                model.eval()\n                correct = 0\n                with torch.no_grad():\n                    for data, target in val_loader:\n                        data, target = data.to(device), target.to(device)\n                        output = model(data)\n                        # get the index of the max log-probability\n                        pred = output.argmax(dim=1, keepdim=True)\n                        correct += pred.eq(target.view_as(pred)).sum().item()\n                accuracy = correct / len(val_loader.dataset)\n                return accuracy\n    optimize_mode : str\n        Optimize mode, `maximize` or `minimize`, by default `maximize`.\n    base_algo : str\n        Base pruning algorithm. `level`, `l1` or `l2`, by default `l1`. Given the sparsity distribution among the ops,\n        the assigned `base_algo` is used to decide which filters/channels/weights to prune.\n    start_temperature : float\n        Start temperature of the simulated annealing process.\n    stop_temperature : float\n        Stop temperature of the simulated annealing process.\n    cool_down_rate : float\n        Cool down rate of the temperature.\n    perturbation_magnitude : float\n        Initial perturbation magnitude to the sparsities. The magnitude decreases with current temperature.\n    experiment_data_dir : string\n        PATH to save experiment data,\n        including the config_list generated for the base pruning algorithm, the performance of the pruned model and the pruning history.\n\n    \"\"\"\n\n    def __init__(self, model, config_list, evaluator, optimize_mode='maximize', base_algo='l1',\n                 start_temperature=100, stop_temperature=20, cool_down_rate=0.9, perturbation_magnitude=0.35, experiment_data_dir='./'):\n        # original model\n        self._model_to_prune = copy.deepcopy(model)\n        self._base_algo = base_algo\n\n        super().__init__(model, config_list)\n\n        self._evaluator = evaluator\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # hyper parameters for SA algorithm\n        self._start_temperature = start_temperature\n        self._current_temperature = start_temperature\n        self._stop_temperature = stop_temperature\n        self._cool_down_rate = cool_down_rate\n        self._perturbation_magnitude = perturbation_magnitude\n\n        # overall pruning rate\n        self._sparsity = config_list[0]['sparsity']\n        # pruning rates of the layers\n        self._sparsities = None\n\n        # init current performance & best performance\n        self._current_performance = -np.inf\n        self._best_performance = -np.inf\n        self._best_config_list = []\n\n        self._search_history = []\n\n        self._experiment_data_dir = experiment_data_dir\n        if not os.path.exists(self._experiment_data_dir):\n            os.makedirs(self._experiment_data_dir)\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self._base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self._base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)\n\n    def _sparsities_2_config_list(self, sparsities):\n        '''\n        convert sparsities vector into config_list for LevelPruner or L1FilterPruner\n\n        Parameters\n        ----------\n        sparsities : list\n            list of sparsities\n\n        Returns\n        -------\n        list of dict\n            config_list for LevelPruner or L1FilterPruner\n        '''\n        config_list = []\n\n        sparsities = sorted(sparsities)\n        self.modules_wrapper = sorted(\n            self.modules_wrapper, key=lambda wrapper: wrapper.module.weight.data.numel())\n\n        # a layer with more weights will have no less pruning rate\n        for idx, wrapper in enumerate(self.get_modules_wrapper()):\n            # L1Filter Pruner requires to specify op_types\n            if self._base_algo in ['l1', 'l2']:\n                config_list.append(\n                    {'sparsity': sparsities[idx], 'op_types': ['Conv2d'], 'op_names': [wrapper.name]})\n            elif self._base_algo == 'level':\n                config_list.append(\n                    {'sparsity': sparsities[idx], 'op_names': [wrapper.name]})\n\n        config_list = [val for val in config_list if not math.isclose(val['sparsity'], 0, abs_tol=1e-6)]\n\n        return config_list\n\n    def _rescale_sparsities(self, sparsities, target_sparsity):\n        '''\n        Rescale the sparsities list to satisfy the target overall sparsity\n\n        Parameters\n        ----------\n        sparsities : list\n\n        target_sparsity : float\n            the target overall sparsity\n\n        Returns\n        -------\n        list\n            the rescaled sparsities\n        '''\n        num_weights = []\n        for wrapper in self.get_modules_wrapper():\n            num_weights.append(wrapper.module.weight.data.numel())\n\n        num_weights = sorted(num_weights)\n        sparsities = sorted(sparsities)\n\n        total_weights = 0\n        total_weights_pruned = 0\n\n        # calculate the scale\n        for idx, num_weight in enumerate(num_weights):\n            total_weights += num_weight\n            total_weights_pruned += int(num_weight*sparsities[idx])\n        if total_weights_pruned == 0:\n            return None\n        scale = target_sparsity / (total_weights_pruned/total_weights)\n\n        # rescale the sparsities\n        sparsities = np.asarray(sparsities)*scale\n\n        return sparsities\n\n    def _init_sparsities(self):\n        '''\n        Generate a sorted sparsities vector\n        '''\n        # repeatedly generate a distribution until satisfies the overall sparsity requirement\n        _logger.info('Gererating sparsities...')\n        while True:\n            sparsities = sorted(np.random.uniform(\n                0, 1, len(self.get_modules_wrapper())))\n\n            sparsities = self._rescale_sparsities(\n                sparsities, target_sparsity=self._sparsity)\n\n            if sparsities is not None and sparsities[0] >= 0 and sparsities[-1] < 1:\n                _logger.info('Initial sparsities generated : %s', sparsities)\n                self._sparsities = sparsities\n                break\n\n    def _generate_perturbations(self):\n        '''\n        Generate perturbation to the current sparsities distribution.\n\n        Returns:\n        --------\n        list\n            perturbated sparsities\n        '''\n        _logger.info(\"Gererating perturbations to the current sparsities...\")\n\n        # decrease magnitude with current temperature\n        magnitude = self._current_temperature / \\\n            self._start_temperature * self._perturbation_magnitude\n        _logger.info('current perturation magnitude:%s', magnitude)\n\n        while True:\n            perturbation = np.random.uniform(-magnitude, magnitude, len(self.get_modules_wrapper()))\n            sparsities = np.clip(0, self._sparsities + perturbation, None)\n            _logger.debug(\"sparsities before rescalling:%s\", sparsities)\n\n            sparsities = self._rescale_sparsities(sparsities, target_sparsity=self._sparsity)\n            _logger.debug(\"sparsities after rescalling:%s\", sparsities)\n\n            if sparsities is not None and sparsities[0] >= 0 and sparsities[-1] < 1:\n                _logger.info(\"Sparsities perturbated:%s\", sparsities)\n                return sparsities\n\n    def calc_mask(self, wrapper, **kwargs):\n        return None\n\n    def compress(self, return_config_list=False):\n        \"\"\"\n        Compress the model with Simulated Annealing.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        _logger.info('Starting Simulated Annealing Compression...')\n\n        # initiaze a randomized action\n        pruning_iteration = 0\n        self._init_sparsities()\n\n        # stop condition\n        self._current_temperature = self._start_temperature\n        while self._current_temperature > self._stop_temperature:\n            _logger.info('Pruning iteration: %d', pruning_iteration)\n            _logger.info('Current temperature: %d, Stop temperature: %d',\n                         self._current_temperature, self._stop_temperature)\n            while True:\n                # generate perturbation\n                sparsities_perturbated = self._generate_perturbations()\n                config_list = self._sparsities_2_config_list(\n                    sparsities_perturbated)\n                _logger.info(\n                    \"config_list for Pruner generated: %s\", config_list)\n\n                # fast evaluation\n                pruner = PRUNER_DICT[self._base_algo](copy.deepcopy(self._model_to_prune), config_list)\n                model_masked = pruner.compress()\n                evaluation_result = self._evaluator(model_masked)\n\n                self._search_history.append(\n                    {'sparsity': self._sparsity, 'performance': evaluation_result, 'config_list': config_list})\n\n                if self._optimize_mode is OptimizeMode.Minimize:\n                    evaluation_result *= -1\n\n                # if better evaluation result, then accept the perturbation\n                if evaluation_result > self._current_performance:\n                    self._current_performance = evaluation_result\n                    self._sparsities = sparsities_perturbated\n\n                    # save best performance and best params\n                    if evaluation_result > self._best_performance:\n                        _logger.info('updating best model...')\n                        self._best_performance = evaluation_result\n                        self._best_config_list = config_list\n\n                        # save the overall best masked model\n                        self.bound_model = model_masked\n                        # the ops with sparsity 0 are not included in this modules_wrapper\n                        modules_wrapper_final = pruner.get_modules_wrapper()\n                    break\n                # if not, accept with probability e^(-deltaE/current_temperature)\n                else:\n                    delta_E = np.abs(evaluation_result -\n                                     self._current_performance)\n                    probability = math.exp(-1 * delta_E /\n                                           self._current_temperature)\n                    if np.random.uniform(0, 1) < probability:\n                        self._current_performance = evaluation_result\n                        self._sparsities = sparsities_perturbated\n                        break\n\n            # cool down\n            self._current_temperature *= self._cool_down_rate\n            pruning_iteration += 1\n\n        _logger.info('----------Compression finished--------------')\n        _logger.info('Best performance: %s', self._best_performance)\n        _logger.info('config_list found : %s',\n                     self._best_config_list)\n\n        # save search history\n        with open(os.path.join(self._experiment_data_dir, 'search_history.csv'), 'w') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=['sparsity', 'performance', 'config_list'])\n            writer.writeheader()\n            for item in self._search_history:\n                writer.writerow({'sparsity': item['sparsity'], 'performance': item['performance'], 'config_list': json.dumps(\n                    item['config_list'])})\n\n        # save best config found and best performance\n        if self._optimize_mode is OptimizeMode.Minimize:\n            self._best_performance *= -1\n        with open(os.path.join(self._experiment_data_dir, 'search_result.json'), 'w+') as jsonfile:\n            json.dump({\n                'performance': self._best_performance,\n                'config_list': json.dumps(self._best_config_list)\n            }, jsonfile)\n\n        _logger.info('search history and result saved to foler : %s',\n                     self._experiment_data_dir)\n\n        if return_config_list:\n            return self._best_config_list\n\n        # This should be done only at the final stage,\n        # because the modules_wrapper with all the ops are used during the annealing process\n        self.modules_wrapper = modules_wrapper_final\n\n        return self.bound_model",
  "def __init__(self, model, config_list, evaluator, optimize_mode='maximize', base_algo='l1',\n                 start_temperature=100, stop_temperature=20, cool_down_rate=0.9, perturbation_magnitude=0.35, experiment_data_dir='./'):\n        # original model\n        self._model_to_prune = copy.deepcopy(model)\n        self._base_algo = base_algo\n\n        super().__init__(model, config_list)\n\n        self._evaluator = evaluator\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # hyper parameters for SA algorithm\n        self._start_temperature = start_temperature\n        self._current_temperature = start_temperature\n        self._stop_temperature = stop_temperature\n        self._cool_down_rate = cool_down_rate\n        self._perturbation_magnitude = perturbation_magnitude\n\n        # overall pruning rate\n        self._sparsity = config_list[0]['sparsity']\n        # pruning rates of the layers\n        self._sparsities = None\n\n        # init current performance & best performance\n        self._current_performance = -np.inf\n        self._best_performance = -np.inf\n        self._best_config_list = []\n\n        self._search_history = []\n\n        self._experiment_data_dir = experiment_data_dir\n        if not os.path.exists(self._experiment_data_dir):\n            os.makedirs(self._experiment_data_dir)",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n\n        if self._base_algo == 'level':\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                Optional('op_types'): [str],\n                Optional('op_names'): [str],\n            }], model, _logger)\n        elif self._base_algo in ['l1', 'l2']:\n            schema = CompressorSchema([{\n                'sparsity': And(float, lambda n: 0 < n < 1),\n                'op_types': ['Conv2d'],\n                Optional('op_names'): [str]\n            }], model, _logger)\n\n        schema.validate(config_list)",
  "def _sparsities_2_config_list(self, sparsities):\n        '''\n        convert sparsities vector into config_list for LevelPruner or L1FilterPruner\n\n        Parameters\n        ----------\n        sparsities : list\n            list of sparsities\n\n        Returns\n        -------\n        list of dict\n            config_list for LevelPruner or L1FilterPruner\n        '''\n        config_list = []\n\n        sparsities = sorted(sparsities)\n        self.modules_wrapper = sorted(\n            self.modules_wrapper, key=lambda wrapper: wrapper.module.weight.data.numel())\n\n        # a layer with more weights will have no less pruning rate\n        for idx, wrapper in enumerate(self.get_modules_wrapper()):\n            # L1Filter Pruner requires to specify op_types\n            if self._base_algo in ['l1', 'l2']:\n                config_list.append(\n                    {'sparsity': sparsities[idx], 'op_types': ['Conv2d'], 'op_names': [wrapper.name]})\n            elif self._base_algo == 'level':\n                config_list.append(\n                    {'sparsity': sparsities[idx], 'op_names': [wrapper.name]})\n\n        config_list = [val for val in config_list if not math.isclose(val['sparsity'], 0, abs_tol=1e-6)]\n\n        return config_list",
  "def _rescale_sparsities(self, sparsities, target_sparsity):\n        '''\n        Rescale the sparsities list to satisfy the target overall sparsity\n\n        Parameters\n        ----------\n        sparsities : list\n\n        target_sparsity : float\n            the target overall sparsity\n\n        Returns\n        -------\n        list\n            the rescaled sparsities\n        '''\n        num_weights = []\n        for wrapper in self.get_modules_wrapper():\n            num_weights.append(wrapper.module.weight.data.numel())\n\n        num_weights = sorted(num_weights)\n        sparsities = sorted(sparsities)\n\n        total_weights = 0\n        total_weights_pruned = 0\n\n        # calculate the scale\n        for idx, num_weight in enumerate(num_weights):\n            total_weights += num_weight\n            total_weights_pruned += int(num_weight*sparsities[idx])\n        if total_weights_pruned == 0:\n            return None\n        scale = target_sparsity / (total_weights_pruned/total_weights)\n\n        # rescale the sparsities\n        sparsities = np.asarray(sparsities)*scale\n\n        return sparsities",
  "def _init_sparsities(self):\n        '''\n        Generate a sorted sparsities vector\n        '''\n        # repeatedly generate a distribution until satisfies the overall sparsity requirement\n        _logger.info('Gererating sparsities...')\n        while True:\n            sparsities = sorted(np.random.uniform(\n                0, 1, len(self.get_modules_wrapper())))\n\n            sparsities = self._rescale_sparsities(\n                sparsities, target_sparsity=self._sparsity)\n\n            if sparsities is not None and sparsities[0] >= 0 and sparsities[-1] < 1:\n                _logger.info('Initial sparsities generated : %s', sparsities)\n                self._sparsities = sparsities\n                break",
  "def _generate_perturbations(self):\n        '''\n        Generate perturbation to the current sparsities distribution.\n\n        Returns:\n        --------\n        list\n            perturbated sparsities\n        '''\n        _logger.info(\"Gererating perturbations to the current sparsities...\")\n\n        # decrease magnitude with current temperature\n        magnitude = self._current_temperature / \\\n            self._start_temperature * self._perturbation_magnitude\n        _logger.info('current perturation magnitude:%s', magnitude)\n\n        while True:\n            perturbation = np.random.uniform(-magnitude, magnitude, len(self.get_modules_wrapper()))\n            sparsities = np.clip(0, self._sparsities + perturbation, None)\n            _logger.debug(\"sparsities before rescalling:%s\", sparsities)\n\n            sparsities = self._rescale_sparsities(sparsities, target_sparsity=self._sparsity)\n            _logger.debug(\"sparsities after rescalling:%s\", sparsities)\n\n            if sparsities is not None and sparsities[0] >= 0 and sparsities[-1] < 1:\n                _logger.info(\"Sparsities perturbated:%s\", sparsities)\n                return sparsities",
  "def calc_mask(self, wrapper, **kwargs):\n        return None",
  "def compress(self, return_config_list=False):\n        \"\"\"\n        Compress the model with Simulated Annealing.\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        \"\"\"\n        _logger.info('Starting Simulated Annealing Compression...')\n\n        # initiaze a randomized action\n        pruning_iteration = 0\n        self._init_sparsities()\n\n        # stop condition\n        self._current_temperature = self._start_temperature\n        while self._current_temperature > self._stop_temperature:\n            _logger.info('Pruning iteration: %d', pruning_iteration)\n            _logger.info('Current temperature: %d, Stop temperature: %d',\n                         self._current_temperature, self._stop_temperature)\n            while True:\n                # generate perturbation\n                sparsities_perturbated = self._generate_perturbations()\n                config_list = self._sparsities_2_config_list(\n                    sparsities_perturbated)\n                _logger.info(\n                    \"config_list for Pruner generated: %s\", config_list)\n\n                # fast evaluation\n                pruner = PRUNER_DICT[self._base_algo](copy.deepcopy(self._model_to_prune), config_list)\n                model_masked = pruner.compress()\n                evaluation_result = self._evaluator(model_masked)\n\n                self._search_history.append(\n                    {'sparsity': self._sparsity, 'performance': evaluation_result, 'config_list': config_list})\n\n                if self._optimize_mode is OptimizeMode.Minimize:\n                    evaluation_result *= -1\n\n                # if better evaluation result, then accept the perturbation\n                if evaluation_result > self._current_performance:\n                    self._current_performance = evaluation_result\n                    self._sparsities = sparsities_perturbated\n\n                    # save best performance and best params\n                    if evaluation_result > self._best_performance:\n                        _logger.info('updating best model...')\n                        self._best_performance = evaluation_result\n                        self._best_config_list = config_list\n\n                        # save the overall best masked model\n                        self.bound_model = model_masked\n                        # the ops with sparsity 0 are not included in this modules_wrapper\n                        modules_wrapper_final = pruner.get_modules_wrapper()\n                    break\n                # if not, accept with probability e^(-deltaE/current_temperature)\n                else:\n                    delta_E = np.abs(evaluation_result -\n                                     self._current_performance)\n                    probability = math.exp(-1 * delta_E /\n                                           self._current_temperature)\n                    if np.random.uniform(0, 1) < probability:\n                        self._current_performance = evaluation_result\n                        self._sparsities = sparsities_perturbated\n                        break\n\n            # cool down\n            self._current_temperature *= self._cool_down_rate\n            pruning_iteration += 1\n\n        _logger.info('----------Compression finished--------------')\n        _logger.info('Best performance: %s', self._best_performance)\n        _logger.info('config_list found : %s',\n                     self._best_config_list)\n\n        # save search history\n        with open(os.path.join(self._experiment_data_dir, 'search_history.csv'), 'w') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=['sparsity', 'performance', 'config_list'])\n            writer.writeheader()\n            for item in self._search_history:\n                writer.writerow({'sparsity': item['sparsity'], 'performance': item['performance'], 'config_list': json.dumps(\n                    item['config_list'])})\n\n        # save best config found and best performance\n        if self._optimize_mode is OptimizeMode.Minimize:\n            self._best_performance *= -1\n        with open(os.path.join(self._experiment_data_dir, 'search_result.json'), 'w+') as jsonfile:\n            json.dump({\n                'performance': self._best_performance,\n                'config_list': json.dumps(self._best_config_list)\n            }, jsonfile)\n\n        _logger.info('search history and result saved to foler : %s',\n                     self._experiment_data_dir)\n\n        if return_config_list:\n            return self._best_config_list\n\n        # This should be done only at the final stage,\n        # because the modules_wrapper with all the ops are used during the annealing process\n        self.modules_wrapper = modules_wrapper_final\n\n        return self.bound_model",
  "class AGPPruner(Pruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned.\n    config_list : listlist\n        Supported keys:\n            - initial_sparsity: This is to specify the sparsity when compressor starts to compress.\n            - final_sparsity: This is to specify the sparsity when compressor finishes to compress.\n            - start_epoch: This is to specify the epoch number when compressor starts to compress, default start from epoch 0.\n            - end_epoch: This is to specify the epoch number when compressor finishes to compress.\n            - frequency: This is to specify every *frequency* number epochs compressor compress once, default frequency=1.\n    optimizer: torch.optim.Optimizer\n        Optimizer used to train model.\n    pruning_algorithm: str\n        Algorithms being used to prune model,\n        choose from `['level', 'slim', 'l1', 'l2', 'fpgm', 'taylorfo', 'apoz', 'mean_activation']`, by default `level`\n    \"\"\"\n\n    def __init__(self, model, config_list, optimizer, pruning_algorithm='level'):\n        super().__init__(model, config_list, optimizer)\n        assert isinstance(optimizer, torch.optim.Optimizer), \"AGP pruner is an iterative pruner, please pass optimizer of the model to it\"\n        self.masker = MASKER_DICT[pruning_algorithm](model, self)\n\n        self.now_epoch = 0\n        self.set_wrappers_attribute(\"if_calculated\", False)\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n        schema = CompressorSchema([{\n            'initial_sparsity': And(float, lambda n: 0 <= n <= 1),\n            'final_sparsity': And(float, lambda n: 0 <= n <= 1),\n            'start_epoch': And(int, lambda n: n >= 0),\n            'end_epoch': And(int, lambda n: n >= 0),\n            'frequency': And(int, lambda n: n > 0),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def calc_mask(self, wrapper, wrapper_idx=None):\n        \"\"\"\n        Calculate the mask of given layer.\n        Scale factors with the smallest absolute value in the BN layer are masked.\n        Parameters\n        ----------\n        wrapper : Module\n            the layer to instrument the compression operation\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict | None\n            Dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n\n        config = wrapper.config\n\n        start_epoch = config.get('start_epoch', 0)\n        freq = config.get('frequency', 1)\n\n        if wrapper.if_calculated:\n            return None\n        if not (self.now_epoch >= start_epoch and (self.now_epoch - start_epoch) % freq == 0):\n            return None\n\n        target_sparsity = self.compute_target_sparsity(config)\n        new_mask = self.masker.calc_mask(sparsity=target_sparsity, wrapper=wrapper, wrapper_idx=wrapper_idx)\n        if new_mask is not None:\n            wrapper.if_calculated = True\n\n        return new_mask\n\n    def compute_target_sparsity(self, config):\n        \"\"\"\n        Calculate the sparsity for pruning\n        Parameters\n        ----------\n        config : dict\n            Layer's pruning config\n        Returns\n        -------\n        float\n            Target sparsity to be pruned\n        \"\"\"\n\n        end_epoch = config.get('end_epoch', 1)\n        start_epoch = config.get('start_epoch', 0)\n        freq = config.get('frequency', 1)\n        final_sparsity = config.get('final_sparsity', 0)\n        initial_sparsity = config.get('initial_sparsity', 0)\n        if end_epoch <= start_epoch or initial_sparsity >= final_sparsity:\n            logger.warning('your end epoch <= start epoch or initial_sparsity >= final_sparsity')\n            return final_sparsity\n\n        if end_epoch <= self.now_epoch:\n            return final_sparsity\n\n        span = ((end_epoch - start_epoch - 1) // freq) * freq\n        assert span > 0\n        target_sparsity = (final_sparsity +\n                           (initial_sparsity - final_sparsity) *\n                           (1.0 - ((self.now_epoch - start_epoch) / span)) ** 3)\n        return target_sparsity\n\n    def update_epoch(self, epoch):\n        \"\"\"\n        Update epoch\n        Parameters\n        ----------\n        epoch : int\n            current training epoch\n        \"\"\"\n\n        if epoch > 0:\n            self.now_epoch = epoch\n            for wrapper in self.get_modules_wrapper():\n                wrapper.if_calculated = False",
  "def __init__(self, model, config_list, optimizer, pruning_algorithm='level'):\n        super().__init__(model, config_list, optimizer)\n        assert isinstance(optimizer, torch.optim.Optimizer), \"AGP pruner is an iterative pruner, please pass optimizer of the model to it\"\n        self.masker = MASKER_DICT[pruning_algorithm](model, self)\n\n        self.now_epoch = 0\n        self.set_wrappers_attribute(\"if_calculated\", False)",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        \"\"\"\n        schema = CompressorSchema([{\n            'initial_sparsity': And(float, lambda n: 0 <= n <= 1),\n            'final_sparsity': And(float, lambda n: 0 <= n <= 1),\n            'start_epoch': And(int, lambda n: n >= 0),\n            'end_epoch': And(int, lambda n: n >= 0),\n            'frequency': And(int, lambda n: n > 0),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)",
  "def calc_mask(self, wrapper, wrapper_idx=None):\n        \"\"\"\n        Calculate the mask of given layer.\n        Scale factors with the smallest absolute value in the BN layer are masked.\n        Parameters\n        ----------\n        wrapper : Module\n            the layer to instrument the compression operation\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict | None\n            Dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n\n        config = wrapper.config\n\n        start_epoch = config.get('start_epoch', 0)\n        freq = config.get('frequency', 1)\n\n        if wrapper.if_calculated:\n            return None\n        if not (self.now_epoch >= start_epoch and (self.now_epoch - start_epoch) % freq == 0):\n            return None\n\n        target_sparsity = self.compute_target_sparsity(config)\n        new_mask = self.masker.calc_mask(sparsity=target_sparsity, wrapper=wrapper, wrapper_idx=wrapper_idx)\n        if new_mask is not None:\n            wrapper.if_calculated = True\n\n        return new_mask",
  "def compute_target_sparsity(self, config):\n        \"\"\"\n        Calculate the sparsity for pruning\n        Parameters\n        ----------\n        config : dict\n            Layer's pruning config\n        Returns\n        -------\n        float\n            Target sparsity to be pruned\n        \"\"\"\n\n        end_epoch = config.get('end_epoch', 1)\n        start_epoch = config.get('start_epoch', 0)\n        freq = config.get('frequency', 1)\n        final_sparsity = config.get('final_sparsity', 0)\n        initial_sparsity = config.get('initial_sparsity', 0)\n        if end_epoch <= start_epoch or initial_sparsity >= final_sparsity:\n            logger.warning('your end epoch <= start epoch or initial_sparsity >= final_sparsity')\n            return final_sparsity\n\n        if end_epoch <= self.now_epoch:\n            return final_sparsity\n\n        span = ((end_epoch - start_epoch - 1) // freq) * freq\n        assert span > 0\n        target_sparsity = (final_sparsity +\n                           (initial_sparsity - final_sparsity) *\n                           (1.0 - ((self.now_epoch - start_epoch) / span)) ** 3)\n        return target_sparsity",
  "def update_epoch(self, epoch):\n        \"\"\"\n        Update epoch\n        Parameters\n        ----------\n        epoch : int\n            current training epoch\n        \"\"\"\n\n        if epoch > 0:\n            self.now_epoch = epoch\n            for wrapper in self.get_modules_wrapper():\n                wrapper.if_calculated = False",
  "class StructuredWeightMasker(WeightMasker):\n    \"\"\"\n    A structured pruning masker base class that prunes convolutional layer filters.\n\n    Parameters\n    ----------\n    model: nn.Module\n        model to be pruned\n    pruner: Pruner\n        A Pruner instance used to prune the model\n    preserve_round: int\n        after pruning, preserve filters/channels round to `preserve_round`, for example:\n        for a Conv2d layer, output channel is 32, sparsity is 0.2, if preserve_round is\n        1 (no preserve round), then there will be int(32 * 0.2) = 6 filters pruned, and\n        32 - 6 = 26 filters are preserved. If preserve_round is 4, preserved filters will\n        be round up to 28 (which can be divided by 4) and only 4 filters are pruned.\n\n    \"\"\"\n    def __init__(self, model, pruner, preserve_round=1):\n        self.model = model\n        self.pruner = pruner\n        self.preserve_round = preserve_round\n\n    def calc_mask(self, sparsity, wrapper, wrapper_idx=None):\n        \"\"\"\n        Calculate the mask of given layer.\n        Parameters\n        ----------\n        sparsity: float\n            pruning ratio,  preserved weight ratio is `1 - sparsity`\n        wrapper: PrunerModuleWrapper\n            layer wrapper of this layer\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n        msg = 'module type {} is not supported!'.format(wrapper.type)\n        assert wrapper.type == 'Conv2d', msg\n        weight = wrapper.module.weight.data\n        bias = None\n        if hasattr(wrapper.module, 'bias') and wrapper.module.bias is not None:\n            bias = wrapper.module.bias.data\n\n        if wrapper.weight_mask is None:\n            mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n        else:\n            mask_weight = wrapper.weight_mask.clone()\n        if bias is not None:\n            if wrapper.bias_mask is None:\n                mask_bias = torch.ones(bias.size()).type_as(bias).detach()\n            else:\n                mask_bias = wrapper.bias_mask.clone()\n        else:\n            mask_bias = None\n        mask = {'weight_mask': mask_weight, 'bias_mask': mask_bias}\n\n        num_total = weight.size(0)\n        num_prune = int(num_total * sparsity)\n        if self.preserve_round > 1:\n            num_preserve = num_total - num_prune\n            num_preserve = int(math.ceil(num_preserve * 1. / self.preserve_round) * self.preserve_round)\n            if num_preserve > num_total:\n                num_preserve = int(math.floor(num_total * 1. / self.preserve_round) * self.preserve_round)\n            num_prune = num_total - num_preserve\n\n        if num_total < 2 or num_prune < 1:\n            return mask\n        # weight*mask_weight: apply base mask for iterative pruning\n        return self.get_mask(mask, weight*mask_weight, num_prune, wrapper, wrapper_idx)\n\n    def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        \"\"\"\n        Calculate the mask of given layer.\n        Parameters\n        ----------\n        base_mask: dict\n            The basic mask with the same shape of weight, all item in the basic mask is 1.\n        weight: tensor\n            the module weight to be pruned\n        num_prune: int\n            Num of filters to prune\n        wrapper: PrunerModuleWrapper\n            layer wrapper of this layer\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        \"\"\"\n        raise NotImplementedError('{} get_mask is not implemented'.format(self.__class__.__name__))",
  "class L1FilterPrunerMasker(StructuredWeightMasker):\n    \"\"\"\n    A structured pruning algorithm that prunes the filters of smallest magnitude\n    weights sum in the convolution layers to achieve a preset level of network sparsity.\n    Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet and Hans Peter Graf,\n    \"PRUNING FILTERS FOR EFFICIENT CONVNETS\", 2017 ICLR\n    https://arxiv.org/abs/1608.08710\n    \"\"\"\n\n    def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        filters = weight.shape[0]\n        w_abs = weight.abs()\n        w_abs_structured = w_abs.view(filters, -1).sum(dim=1)\n        threshold = torch.topk(w_abs_structured.view(-1), num_prune, largest=False)[0].max()\n        mask_weight = torch.gt(w_abs_structured, threshold)[:, None, None, None].expand_as(weight).type_as(weight)\n        mask_bias = torch.gt(w_abs_structured, threshold).type_as(weight).detach() if base_mask['bias_mask'] is not None else None\n\n        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
  "class L2FilterPrunerMasker(StructuredWeightMasker):\n    \"\"\"\n    A structured pruning algorithm that prunes the filters with the\n    smallest L2 norm of the weights.\n    \"\"\"\n    def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        filters = weight.shape[0]\n        w = weight.view(filters, -1)\n        w_l2_norm = torch.sqrt((w ** 2).sum(dim=1))\n        threshold = torch.topk(w_l2_norm.view(-1), num_prune, largest=False)[0].max()\n        mask_weight = torch.gt(w_l2_norm, threshold)[:, None, None, None].expand_as(weight).type_as(weight)\n        mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight).detach() if base_mask['bias_mask'] is not None else None\n\n        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
  "class FPGMPrunerMasker(StructuredWeightMasker):\n    \"\"\"\n    A filter pruner via geometric median.\n    \"Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration\",\n    https://arxiv.org/pdf/1811.00250.pdf\n    \"\"\"\n    def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        min_gm_idx = self._get_min_gm_kernel_idx(weight, num_prune)\n        for idx in min_gm_idx:\n            base_mask['weight_mask'][idx] = 0.\n            if base_mask['bias_mask'] is not None:\n                base_mask['bias_mask'][idx] = 0.\n        return base_mask\n\n    def _get_min_gm_kernel_idx(self, weight, n):\n        assert len(weight.size()) in [3, 4]\n\n        dist_list = []\n        for out_i in range(weight.size(0)):\n            dist_sum = self._get_distance_sum(weight, out_i)\n            dist_list.append((dist_sum, out_i))\n        min_gm_kernels = sorted(dist_list, key=lambda x: x[0])[:n]\n        return [x[1] for x in min_gm_kernels]\n\n    def _get_distance_sum(self, weight, out_idx):\n        \"\"\"\n        Calculate the total distance between a specified filter (by out_idex and in_idx) and\n        all other filters.\n        Parameters\n        ----------\n        weight: Tensor\n            convolutional filter weight\n        out_idx: int\n            output channel index of specified filter, this method calculates the total distance\n            between this specified filter and all other filters.\n        Returns\n        -------\n        float32\n            The total distance\n        \"\"\"\n        logger.debug('weight size: %s', weight.size())\n        assert len(weight.size()) in [3, 4], 'unsupported weight shape'\n\n        w = weight.view(weight.size(0), -1)\n        anchor_w = w[out_idx].unsqueeze(0).expand(w.size(0), w.size(1))\n        x = w - anchor_w\n        x = (x * x).sum(-1)\n        x = torch.sqrt(x)\n        return x.sum()",
  "class TaylorFOWeightFilterPrunerMasker(StructuredWeightMasker):\n    \"\"\"\n    A structured pruning algorithm that prunes the filters with the smallest\n    importance approximations based on the first order taylor expansion on the weight.\n    Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan,\n    \"Importance Estimation for Neural Network Pruning\", CVPR 2019.\n    http://jankautz.com/publications/Importance4NNPruning_CVPR19.pdf\n    \"\"\"\n    def __init__(self, model, pruner, statistics_batch_num=1):\n        super().__init__(model, pruner)\n        self.pruner.statistics_batch_num = statistics_batch_num\n        self.pruner.set_wrappers_attribute(\"contribution\", None)\n        self.pruner.iterations = 0\n        self.pruner.patch_optimizer(self.calc_contributions)\n\n    def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        if self.pruner.iterations < self.pruner.statistics_batch_num:\n            return None\n\n        if wrapper.contribution is None:\n            return None\n\n        prune_indices = torch.argsort(wrapper.contribution)[:num_prune]\n        for idx in prune_indices:\n            base_mask['weight_mask'][idx] = 0.\n            if base_mask['bias_mask'] is not None:\n                base_mask['bias_mask'][idx] = 0.\n        return base_mask\n\n    def calc_contributions(self):\n        \"\"\"\n        Calculate the estimated importance of filters as a sum of individual contribution\n        based on the first order taylor expansion.\n        \"\"\"\n        if self.pruner.iterations >= self.pruner.statistics_batch_num:\n            return\n        for wrapper in self.pruner.get_modules_wrapper():\n            filters = wrapper.module.weight.size(0)\n            contribution = (wrapper.module.weight*wrapper.module.weight.grad).data.pow(2).view(filters, -1).sum(dim=1)\n            if wrapper.contribution is None:\n                wrapper.contribution = contribution\n            else:\n                wrapper.contribution += contribution\n\n        self.pruner.iterations += 1",
  "class ActivationFilterPrunerMasker(StructuredWeightMasker):\n    def __init__(self, model, pruner, statistics_batch_num=1, activation='relu'):\n        super().__init__(model, pruner)\n        self.statistics_batch_num = statistics_batch_num\n        self.pruner.hook_id = self._add_activation_collector(self.pruner)\n\n        assert activation in ['relu', 'relu6']\n        if activation == 'relu':\n            self.pruner.activation = torch.nn.functional.relu\n        elif activation == 'relu6':\n            self.pruner.activation = torch.nn.functional.relu6\n        else:\n            self.pruner.activation = None\n\n    def _add_activation_collector(self, pruner):\n        def collector(collected_activation):\n            def hook(module_, input_, output):\n                collected_activation.append(pruner.activation(output.detach().cpu()))\n            return hook\n        pruner.collected_activation = {}\n        pruner._fwd_hook_id += 1\n        pruner._fwd_hook_handles[pruner._fwd_hook_id] = []\n\n        for wrapper_idx, wrapper in enumerate(pruner.get_modules_wrapper()):\n            pruner.collected_activation[wrapper_idx] = []\n            handle = wrapper.register_forward_hook(collector(pruner.collected_activation[wrapper_idx]))\n\n            pruner._fwd_hook_handles[pruner._fwd_hook_id].append(handle)\n        return pruner._fwd_hook_id",
  "class ActivationAPoZRankFilterPrunerMasker(ActivationFilterPrunerMasker):\n    \"\"\"\n    A structured pruning algorithm that prunes the filters with the\n    smallest APoZ(average percentage of zeros) of output activations.\n    Hengyuan Hu, Rui Peng, Yu-Wing Tai and Chi-Keung Tang,\n    \"Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures\", ICLR 2016.\n    https://arxiv.org/abs/1607.03250\n    \"\"\"\n    def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        assert wrapper_idx is not None\n        activations = self.pruner.collected_activation[wrapper_idx]\n        if len(activations) < self.statistics_batch_num:\n            return None\n        apoz = self._calc_apoz(activations)\n        prune_indices = torch.argsort(apoz, descending=True)[:num_prune]\n        for idx in prune_indices:\n            base_mask['weight_mask'][idx] = 0.\n            if base_mask['bias_mask'] is not None:\n                base_mask['bias_mask'][idx] = 0.\n\n        if len(activations) >= self.statistics_batch_num and self.pruner.hook_id in self.pruner._fwd_hook_handles:\n            self.pruner.remove_activation_collector(self.pruner.hook_id)\n\n        return base_mask\n\n    def _calc_apoz(self, activations):\n        \"\"\"\n        Calculate APoZ(average percentage of zeros) of activations.\n\n        Parameters\n        ----------\n        activations : list\n            Layer's output activations\n\n        Returns\n        -------\n        torch.Tensor\n            Filter's APoZ(average percentage of zeros) of the activations\n        \"\"\"\n        activations = torch.cat(activations, 0)\n        _eq_zero = torch.eq(activations, torch.zeros_like(activations))\n        _apoz = torch.sum(_eq_zero, dim=(0, 2, 3)) / torch.numel(_eq_zero[:, 0, :, :])\n        return _apoz",
  "class ActivationMeanRankFilterPrunerMasker(ActivationFilterPrunerMasker):\n    \"\"\"\n    A structured pruning algorithm that prunes the filters with the\n    smallest mean value of output activations.\n    Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila and Jan Kautz,\n    \"Pruning Convolutional Neural Networks for Resource Efficient Inference\", ICLR 2017.\n    https://arxiv.org/abs/1611.06440\n    \"\"\"\n    def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        assert wrapper_idx is not None\n        activations = self.pruner.collected_activation[wrapper_idx]\n        if len(activations) < self.statistics_batch_num:\n            return None\n        mean_activation = self._cal_mean_activation(activations)\n        prune_indices = torch.argsort(mean_activation)[:num_prune]\n        for idx in prune_indices:\n            base_mask['weight_mask'][idx] = 0.\n            if base_mask['bias_mask'] is not None:\n                base_mask['bias_mask'][idx] = 0.\n\n        if len(activations) >= self.statistics_batch_num and self.pruner.hook_id in self.pruner._fwd_hook_handles:\n            self.pruner.remove_activation_collector(self.pruner.hook_id)\n\n        return base_mask\n\n    def _cal_mean_activation(self, activations):\n        \"\"\"\n        Calculate mean value of activations.\n\n        Parameters\n        ----------\n        activations : list\n            Layer's output activations\n\n        Returns\n        -------\n        torch.Tensor\n            Filter's mean value of the output activations\n        \"\"\"\n        activations = torch.cat(activations, 0)\n        mean_activation = torch.mean(activations, dim=(0, 2, 3))\n        return mean_activation",
  "class SlimPrunerMasker(WeightMasker):\n    \"\"\"\n    A structured pruning algorithm that prunes channels by pruning the weights of BN layers.\n    Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan and Changshui Zhang\n    \"Learning Efficient Convolutional Networks through Network Slimming\", 2017 ICCV\n    https://arxiv.org/pdf/1708.06519.pdf\n    \"\"\"\n\n    def __init__(self, model, pruner, **kwargs):\n        super().__init__(model, pruner)\n        weight_list = []\n        for (layer, _) in pruner.get_modules_to_compress():\n            weight_list.append(layer.module.weight.data.abs().clone())\n        all_bn_weights = torch.cat(weight_list)\n        k = int(all_bn_weights.shape[0] * pruner.config_list[0]['sparsity'])\n        self.global_threshold = torch.topk(all_bn_weights.view(-1), k, largest=False)[0].max()\n\n    def calc_mask(self, sparsity, wrapper, wrapper_idx=None):\n        assert wrapper.type == 'BatchNorm2d', 'SlimPruner only supports 2d batch normalization layer pruning'\n        weight = wrapper.module.weight.data.clone()\n        if wrapper.weight_mask is not None:\n            # apply base mask for iterative pruning\n            weight = weight * wrapper.weight_mask\n\n        base_mask = torch.ones(weight.size()).type_as(weight).detach()\n        mask = {'weight_mask': base_mask.detach(), 'bias_mask': base_mask.clone().detach()}\n        filters = weight.size(0)\n        num_prune = int(filters * sparsity)\n        if filters >= 2 and num_prune >= 1:\n            w_abs = weight.abs()\n            mask_weight = torch.gt(w_abs, self.global_threshold).type_as(weight)\n            mask_bias = mask_weight.clone()\n            mask = {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias.detach()}\n        return mask",
  "def least_square_sklearn(X, Y):\n    from sklearn.linear_model import LinearRegression\n    reg = LinearRegression(fit_intercept=False)\n    reg.fit(X, Y)\n    return reg.coef_",
  "class AMCWeightMasker(WeightMasker):\n    \"\"\"\n    Weight maskser class for AMC pruner. Currently, AMCPruner only supports pruning kernel\n    size 1x1 pointwise Conv2d layer. Before using this class to prune kernels, AMCPruner\n    collected input and output feature maps for each layer, the features maps are flattened\n    and save into wrapper.input_feat and wrapper.output_feat.\n\n    Parameters\n    ----------\n    model: nn.Module\n        model to be pruned\n    pruner: Pruner\n        A Pruner instance used to prune the model\n    preserve_round: int\n        after pruning, preserve filters/channels round to `preserve_round`, for example:\n        for a Conv2d layer, output channel is 32, sparsity is 0.2, if preserve_round is\n        1 (no preserve round), then there will be int(32 * 0.2) = 6 filters pruned, and\n        32 - 6 = 26 filters are preserved. If preserve_round is 4, preserved filters will\n        be round up to 28 (which can be divided by 4) and only 4 filters are pruned.\n    \"\"\"\n    def __init__(self, model, pruner, preserve_round=1):\n        self.model = model\n        self.pruner = pruner\n        self.preserve_round = preserve_round\n\n    def calc_mask(self, sparsity, wrapper, wrapper_idx=None, preserve_idx=None):\n        \"\"\"\n        Calculate the mask of given layer.\n        Parameters\n        ----------\n        sparsity: float\n            pruning ratio,  preserved weight ratio is `1 - sparsity`\n        wrapper: PrunerModuleWrapper\n            layer wrapper of this layer\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n        msg = 'module type {} is not supported!'.format(wrapper.type)\n        assert wrapper.type in ['Conv2d', 'Linear'], msg\n        weight = wrapper.module.weight.data\n        bias = None\n        if hasattr(wrapper.module, 'bias') and wrapper.module.bias is not None:\n            bias = wrapper.module.bias.data\n\n        if wrapper.weight_mask is None:\n            mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n        else:\n            mask_weight = wrapper.weight_mask.clone()\n        if bias is not None:\n            if wrapper.bias_mask is None:\n                mask_bias = torch.ones(bias.size()).type_as(bias).detach()\n            else:\n                mask_bias = wrapper.bias_mask.clone()\n        else:\n            mask_bias = None\n        mask = {'weight_mask': mask_weight, 'bias_mask': mask_bias}\n\n        num_total = weight.size(1)\n        num_prune = int(num_total * sparsity)\n        if self.preserve_round > 1:\n            num_preserve = num_total - num_prune\n            num_preserve = int(math.ceil(num_preserve * 1. / self.preserve_round) * self.preserve_round)\n            if num_preserve > num_total:\n                num_preserve = num_total\n            num_prune = num_total - num_preserve\n\n        if (num_total < 2 or num_prune < 1) and preserve_idx is None:\n            return mask\n\n        return self.get_mask(mask, weight, num_preserve, wrapper, wrapper_idx, preserve_idx)\n\n    def get_mask(self, base_mask, weight, num_preserve, wrapper, wrapper_idx, preserve_idx):\n        w = weight.data.cpu().numpy()\n        if wrapper.type == 'Linear':\n            w = w[:, :, None, None]\n\n        if preserve_idx is None:\n            importance = np.abs(w).sum((0, 2, 3))\n            sorted_idx = np.argsort(-importance)  # sum magnitude along C_in, sort descend\n            d_prime = num_preserve\n            preserve_idx = sorted_idx[:d_prime]  # to preserve index\n        else:\n            d_prime = len(preserve_idx)\n\n        assert len(preserve_idx) == d_prime\n        mask = np.zeros(w.shape[1], bool)\n        mask[preserve_idx] = True\n\n        # reconstruct, X, Y <= [N, C]\n        X, Y = wrapper.input_feat, wrapper.output_feat\n        masked_X = X[:, mask]\n        if w.shape[2] == 1:  # 1x1 conv or fc\n            rec_weight = least_square_sklearn(X=masked_X, Y=Y)\n            rec_weight = rec_weight.reshape(-1, 1, 1, d_prime)  # (C_out, K_h, K_w, C_in')\n            rec_weight = np.transpose(rec_weight, (0, 3, 1, 2))  # (C_out, C_in', K_h, K_w)\n        else:\n            raise NotImplementedError('Current code only supports 1x1 conv now!')\n        rec_weight_pad = np.zeros_like(w)\n        # pylint: disable=all\n        rec_weight_pad[:, mask, :, :] = rec_weight\n        rec_weight = rec_weight_pad\n\n        if wrapper.type == 'Linear':\n            rec_weight = rec_weight.squeeze()\n            assert len(rec_weight.shape) == 2\n\n        # now assign\n        wrapper.module.weight.data = torch.from_numpy(rec_weight).to(weight.device)\n\n        mask_weight = torch.zeros_like(weight)\n        if wrapper.type == 'Linear':\n            mask_weight[:, preserve_idx] = 1.\n            if base_mask['bias_mask'] is not None and wrapper.module.bias is not None:\n                mask_bias = torch.ones_like(wrapper.module.bias)\n        else:\n            mask_weight[:, preserve_idx, :, :] = 1.\n            mask_bias = None\n\n        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
  "def __init__(self, model, pruner, preserve_round=1):\n        self.model = model\n        self.pruner = pruner\n        self.preserve_round = preserve_round",
  "def calc_mask(self, sparsity, wrapper, wrapper_idx=None):\n        \"\"\"\n        Calculate the mask of given layer.\n        Parameters\n        ----------\n        sparsity: float\n            pruning ratio,  preserved weight ratio is `1 - sparsity`\n        wrapper: PrunerModuleWrapper\n            layer wrapper of this layer\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n        msg = 'module type {} is not supported!'.format(wrapper.type)\n        assert wrapper.type == 'Conv2d', msg\n        weight = wrapper.module.weight.data\n        bias = None\n        if hasattr(wrapper.module, 'bias') and wrapper.module.bias is not None:\n            bias = wrapper.module.bias.data\n\n        if wrapper.weight_mask is None:\n            mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n        else:\n            mask_weight = wrapper.weight_mask.clone()\n        if bias is not None:\n            if wrapper.bias_mask is None:\n                mask_bias = torch.ones(bias.size()).type_as(bias).detach()\n            else:\n                mask_bias = wrapper.bias_mask.clone()\n        else:\n            mask_bias = None\n        mask = {'weight_mask': mask_weight, 'bias_mask': mask_bias}\n\n        num_total = weight.size(0)\n        num_prune = int(num_total * sparsity)\n        if self.preserve_round > 1:\n            num_preserve = num_total - num_prune\n            num_preserve = int(math.ceil(num_preserve * 1. / self.preserve_round) * self.preserve_round)\n            if num_preserve > num_total:\n                num_preserve = int(math.floor(num_total * 1. / self.preserve_round) * self.preserve_round)\n            num_prune = num_total - num_preserve\n\n        if num_total < 2 or num_prune < 1:\n            return mask\n        # weight*mask_weight: apply base mask for iterative pruning\n        return self.get_mask(mask, weight*mask_weight, num_prune, wrapper, wrapper_idx)",
  "def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        \"\"\"\n        Calculate the mask of given layer.\n        Parameters\n        ----------\n        base_mask: dict\n            The basic mask with the same shape of weight, all item in the basic mask is 1.\n        weight: tensor\n            the module weight to be pruned\n        num_prune: int\n            Num of filters to prune\n        wrapper: PrunerModuleWrapper\n            layer wrapper of this layer\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        \"\"\"\n        raise NotImplementedError('{} get_mask is not implemented'.format(self.__class__.__name__))",
  "def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        filters = weight.shape[0]\n        w_abs = weight.abs()\n        w_abs_structured = w_abs.view(filters, -1).sum(dim=1)\n        threshold = torch.topk(w_abs_structured.view(-1), num_prune, largest=False)[0].max()\n        mask_weight = torch.gt(w_abs_structured, threshold)[:, None, None, None].expand_as(weight).type_as(weight)\n        mask_bias = torch.gt(w_abs_structured, threshold).type_as(weight).detach() if base_mask['bias_mask'] is not None else None\n\n        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
  "def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        filters = weight.shape[0]\n        w = weight.view(filters, -1)\n        w_l2_norm = torch.sqrt((w ** 2).sum(dim=1))\n        threshold = torch.topk(w_l2_norm.view(-1), num_prune, largest=False)[0].max()\n        mask_weight = torch.gt(w_l2_norm, threshold)[:, None, None, None].expand_as(weight).type_as(weight)\n        mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight).detach() if base_mask['bias_mask'] is not None else None\n\n        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
  "def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        min_gm_idx = self._get_min_gm_kernel_idx(weight, num_prune)\n        for idx in min_gm_idx:\n            base_mask['weight_mask'][idx] = 0.\n            if base_mask['bias_mask'] is not None:\n                base_mask['bias_mask'][idx] = 0.\n        return base_mask",
  "def _get_min_gm_kernel_idx(self, weight, n):\n        assert len(weight.size()) in [3, 4]\n\n        dist_list = []\n        for out_i in range(weight.size(0)):\n            dist_sum = self._get_distance_sum(weight, out_i)\n            dist_list.append((dist_sum, out_i))\n        min_gm_kernels = sorted(dist_list, key=lambda x: x[0])[:n]\n        return [x[1] for x in min_gm_kernels]",
  "def _get_distance_sum(self, weight, out_idx):\n        \"\"\"\n        Calculate the total distance between a specified filter (by out_idex and in_idx) and\n        all other filters.\n        Parameters\n        ----------\n        weight: Tensor\n            convolutional filter weight\n        out_idx: int\n            output channel index of specified filter, this method calculates the total distance\n            between this specified filter and all other filters.\n        Returns\n        -------\n        float32\n            The total distance\n        \"\"\"\n        logger.debug('weight size: %s', weight.size())\n        assert len(weight.size()) in [3, 4], 'unsupported weight shape'\n\n        w = weight.view(weight.size(0), -1)\n        anchor_w = w[out_idx].unsqueeze(0).expand(w.size(0), w.size(1))\n        x = w - anchor_w\n        x = (x * x).sum(-1)\n        x = torch.sqrt(x)\n        return x.sum()",
  "def __init__(self, model, pruner, statistics_batch_num=1):\n        super().__init__(model, pruner)\n        self.pruner.statistics_batch_num = statistics_batch_num\n        self.pruner.set_wrappers_attribute(\"contribution\", None)\n        self.pruner.iterations = 0\n        self.pruner.patch_optimizer(self.calc_contributions)",
  "def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        if self.pruner.iterations < self.pruner.statistics_batch_num:\n            return None\n\n        if wrapper.contribution is None:\n            return None\n\n        prune_indices = torch.argsort(wrapper.contribution)[:num_prune]\n        for idx in prune_indices:\n            base_mask['weight_mask'][idx] = 0.\n            if base_mask['bias_mask'] is not None:\n                base_mask['bias_mask'][idx] = 0.\n        return base_mask",
  "def calc_contributions(self):\n        \"\"\"\n        Calculate the estimated importance of filters as a sum of individual contribution\n        based on the first order taylor expansion.\n        \"\"\"\n        if self.pruner.iterations >= self.pruner.statistics_batch_num:\n            return\n        for wrapper in self.pruner.get_modules_wrapper():\n            filters = wrapper.module.weight.size(0)\n            contribution = (wrapper.module.weight*wrapper.module.weight.grad).data.pow(2).view(filters, -1).sum(dim=1)\n            if wrapper.contribution is None:\n                wrapper.contribution = contribution\n            else:\n                wrapper.contribution += contribution\n\n        self.pruner.iterations += 1",
  "def __init__(self, model, pruner, statistics_batch_num=1, activation='relu'):\n        super().__init__(model, pruner)\n        self.statistics_batch_num = statistics_batch_num\n        self.pruner.hook_id = self._add_activation_collector(self.pruner)\n\n        assert activation in ['relu', 'relu6']\n        if activation == 'relu':\n            self.pruner.activation = torch.nn.functional.relu\n        elif activation == 'relu6':\n            self.pruner.activation = torch.nn.functional.relu6\n        else:\n            self.pruner.activation = None",
  "def _add_activation_collector(self, pruner):\n        def collector(collected_activation):\n            def hook(module_, input_, output):\n                collected_activation.append(pruner.activation(output.detach().cpu()))\n            return hook\n        pruner.collected_activation = {}\n        pruner._fwd_hook_id += 1\n        pruner._fwd_hook_handles[pruner._fwd_hook_id] = []\n\n        for wrapper_idx, wrapper in enumerate(pruner.get_modules_wrapper()):\n            pruner.collected_activation[wrapper_idx] = []\n            handle = wrapper.register_forward_hook(collector(pruner.collected_activation[wrapper_idx]))\n\n            pruner._fwd_hook_handles[pruner._fwd_hook_id].append(handle)\n        return pruner._fwd_hook_id",
  "def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        assert wrapper_idx is not None\n        activations = self.pruner.collected_activation[wrapper_idx]\n        if len(activations) < self.statistics_batch_num:\n            return None\n        apoz = self._calc_apoz(activations)\n        prune_indices = torch.argsort(apoz, descending=True)[:num_prune]\n        for idx in prune_indices:\n            base_mask['weight_mask'][idx] = 0.\n            if base_mask['bias_mask'] is not None:\n                base_mask['bias_mask'][idx] = 0.\n\n        if len(activations) >= self.statistics_batch_num and self.pruner.hook_id in self.pruner._fwd_hook_handles:\n            self.pruner.remove_activation_collector(self.pruner.hook_id)\n\n        return base_mask",
  "def _calc_apoz(self, activations):\n        \"\"\"\n        Calculate APoZ(average percentage of zeros) of activations.\n\n        Parameters\n        ----------\n        activations : list\n            Layer's output activations\n\n        Returns\n        -------\n        torch.Tensor\n            Filter's APoZ(average percentage of zeros) of the activations\n        \"\"\"\n        activations = torch.cat(activations, 0)\n        _eq_zero = torch.eq(activations, torch.zeros_like(activations))\n        _apoz = torch.sum(_eq_zero, dim=(0, 2, 3)) / torch.numel(_eq_zero[:, 0, :, :])\n        return _apoz",
  "def get_mask(self, base_mask, weight, num_prune, wrapper, wrapper_idx):\n        assert wrapper_idx is not None\n        activations = self.pruner.collected_activation[wrapper_idx]\n        if len(activations) < self.statistics_batch_num:\n            return None\n        mean_activation = self._cal_mean_activation(activations)\n        prune_indices = torch.argsort(mean_activation)[:num_prune]\n        for idx in prune_indices:\n            base_mask['weight_mask'][idx] = 0.\n            if base_mask['bias_mask'] is not None:\n                base_mask['bias_mask'][idx] = 0.\n\n        if len(activations) >= self.statistics_batch_num and self.pruner.hook_id in self.pruner._fwd_hook_handles:\n            self.pruner.remove_activation_collector(self.pruner.hook_id)\n\n        return base_mask",
  "def _cal_mean_activation(self, activations):\n        \"\"\"\n        Calculate mean value of activations.\n\n        Parameters\n        ----------\n        activations : list\n            Layer's output activations\n\n        Returns\n        -------\n        torch.Tensor\n            Filter's mean value of the output activations\n        \"\"\"\n        activations = torch.cat(activations, 0)\n        mean_activation = torch.mean(activations, dim=(0, 2, 3))\n        return mean_activation",
  "def __init__(self, model, pruner, **kwargs):\n        super().__init__(model, pruner)\n        weight_list = []\n        for (layer, _) in pruner.get_modules_to_compress():\n            weight_list.append(layer.module.weight.data.abs().clone())\n        all_bn_weights = torch.cat(weight_list)\n        k = int(all_bn_weights.shape[0] * pruner.config_list[0]['sparsity'])\n        self.global_threshold = torch.topk(all_bn_weights.view(-1), k, largest=False)[0].max()",
  "def calc_mask(self, sparsity, wrapper, wrapper_idx=None):\n        assert wrapper.type == 'BatchNorm2d', 'SlimPruner only supports 2d batch normalization layer pruning'\n        weight = wrapper.module.weight.data.clone()\n        if wrapper.weight_mask is not None:\n            # apply base mask for iterative pruning\n            weight = weight * wrapper.weight_mask\n\n        base_mask = torch.ones(weight.size()).type_as(weight).detach()\n        mask = {'weight_mask': base_mask.detach(), 'bias_mask': base_mask.clone().detach()}\n        filters = weight.size(0)\n        num_prune = int(filters * sparsity)\n        if filters >= 2 and num_prune >= 1:\n            w_abs = weight.abs()\n            mask_weight = torch.gt(w_abs, self.global_threshold).type_as(weight)\n            mask_bias = mask_weight.clone()\n            mask = {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias.detach()}\n        return mask",
  "def __init__(self, model, pruner, preserve_round=1):\n        self.model = model\n        self.pruner = pruner\n        self.preserve_round = preserve_round",
  "def calc_mask(self, sparsity, wrapper, wrapper_idx=None, preserve_idx=None):\n        \"\"\"\n        Calculate the mask of given layer.\n        Parameters\n        ----------\n        sparsity: float\n            pruning ratio,  preserved weight ratio is `1 - sparsity`\n        wrapper: PrunerModuleWrapper\n            layer wrapper of this layer\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n        msg = 'module type {} is not supported!'.format(wrapper.type)\n        assert wrapper.type in ['Conv2d', 'Linear'], msg\n        weight = wrapper.module.weight.data\n        bias = None\n        if hasattr(wrapper.module, 'bias') and wrapper.module.bias is not None:\n            bias = wrapper.module.bias.data\n\n        if wrapper.weight_mask is None:\n            mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n        else:\n            mask_weight = wrapper.weight_mask.clone()\n        if bias is not None:\n            if wrapper.bias_mask is None:\n                mask_bias = torch.ones(bias.size()).type_as(bias).detach()\n            else:\n                mask_bias = wrapper.bias_mask.clone()\n        else:\n            mask_bias = None\n        mask = {'weight_mask': mask_weight, 'bias_mask': mask_bias}\n\n        num_total = weight.size(1)\n        num_prune = int(num_total * sparsity)\n        if self.preserve_round > 1:\n            num_preserve = num_total - num_prune\n            num_preserve = int(math.ceil(num_preserve * 1. / self.preserve_round) * self.preserve_round)\n            if num_preserve > num_total:\n                num_preserve = num_total\n            num_prune = num_total - num_preserve\n\n        if (num_total < 2 or num_prune < 1) and preserve_idx is None:\n            return mask\n\n        return self.get_mask(mask, weight, num_preserve, wrapper, wrapper_idx, preserve_idx)",
  "def get_mask(self, base_mask, weight, num_preserve, wrapper, wrapper_idx, preserve_idx):\n        w = weight.data.cpu().numpy()\n        if wrapper.type == 'Linear':\n            w = w[:, :, None, None]\n\n        if preserve_idx is None:\n            importance = np.abs(w).sum((0, 2, 3))\n            sorted_idx = np.argsort(-importance)  # sum magnitude along C_in, sort descend\n            d_prime = num_preserve\n            preserve_idx = sorted_idx[:d_prime]  # to preserve index\n        else:\n            d_prime = len(preserve_idx)\n\n        assert len(preserve_idx) == d_prime\n        mask = np.zeros(w.shape[1], bool)\n        mask[preserve_idx] = True\n\n        # reconstruct, X, Y <= [N, C]\n        X, Y = wrapper.input_feat, wrapper.output_feat\n        masked_X = X[:, mask]\n        if w.shape[2] == 1:  # 1x1 conv or fc\n            rec_weight = least_square_sklearn(X=masked_X, Y=Y)\n            rec_weight = rec_weight.reshape(-1, 1, 1, d_prime)  # (C_out, K_h, K_w, C_in')\n            rec_weight = np.transpose(rec_weight, (0, 3, 1, 2))  # (C_out, C_in', K_h, K_w)\n        else:\n            raise NotImplementedError('Current code only supports 1x1 conv now!')\n        rec_weight_pad = np.zeros_like(w)\n        # pylint: disable=all\n        rec_weight_pad[:, mask, :, :] = rec_weight\n        rec_weight = rec_weight_pad\n\n        if wrapper.type == 'Linear':\n            rec_weight = rec_weight.squeeze()\n            assert len(rec_weight.shape) == 2\n\n        # now assign\n        wrapper.module.weight.data = torch.from_numpy(rec_weight).to(weight.device)\n\n        mask_weight = torch.zeros_like(weight)\n        if wrapper.type == 'Linear':\n            mask_weight[:, preserve_idx] = 1.\n            if base_mask['bias_mask'] is not None and wrapper.module.bias is not None:\n                mask_bias = torch.ones_like(wrapper.module.bias)\n        else:\n            mask_weight[:, preserve_idx, :, :] = 1.\n            mask_bias = None\n\n        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
  "def collector(collected_activation):\n            def hook(module_, input_, output):\n                collected_activation.append(pruner.activation(output.detach().cpu()))\n            return hook",
  "def hook(module_, input_, output):\n                collected_activation.append(pruner.activation(output.detach().cpu()))",
  "class WeightMasker(object):\n    def __init__(self, model, pruner, **kwargs):\n        self.model = model\n        self.pruner = pruner\n\n    def calc_mask(self, sparsity, wrapper, wrapper_idx=None):\n        \"\"\"\n        Calculate the mask of given layer.\n        Parameters\n        ----------\n        sparsity: float\n            pruning ratio,  preserved weight ratio is `1 - sparsity`\n        wrapper: PrunerModuleWrapper\n            layer wrapper of this layer\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n\n        raise NotImplementedError('{} calc_mask is not implemented'.format(self.__class__.__name__))",
  "def __init__(self, model, pruner, **kwargs):\n        self.model = model\n        self.pruner = pruner",
  "def calc_mask(self, sparsity, wrapper, wrapper_idx=None):\n        \"\"\"\n        Calculate the mask of given layer.\n        Parameters\n        ----------\n        sparsity: float\n            pruning ratio,  preserved weight ratio is `1 - sparsity`\n        wrapper: PrunerModuleWrapper\n            layer wrapper of this layer\n        wrapper_idx: int\n            index of this wrapper in pruner's all wrappers\n        Returns\n        -------\n        dict\n            dictionary for storing masks, keys of the dict:\n            'weight_mask':  weight mask tensor\n            'bias_mask': bias mask tensor (optional)\n        \"\"\"\n\n        raise NotImplementedError('{} calc_mask is not implemented'.format(self.__class__.__name__))",
  "def apply_compression_results(model, masks_file, map_location=None):\n    \"\"\"\n    Apply the masks from ```masks_file``` to the model\n    Note: this API is for inference, because it simply multiplies weights with\n    corresponding masks when this API is called.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to be compressed\n    masks_file : str\n        The path of the mask file\n    map_location : str\n        the device on which masks are placed, same to map_location in ```torch.load```\n    \"\"\"\n    masks = torch.load(masks_file, map_location)\n    for name, module in model.named_modules():\n        if name in masks:\n            module.weight.data = module.weight.data.mul_(masks[name]['weight'])\n            if hasattr(module, 'bias') and module.bias is not None and 'bias' in masks[name]:\n                module.bias.data = module.bias.data.mul_(masks[name]['bias'])",
  "class AMCPruner(Pruner):\n    \"\"\"\n    A pytorch implementation of AMC: AutoML for Model Compression and Acceleration on Mobile Devices.\n    (https://arxiv.org/pdf/1802.03494.pdf)\n\n    Parameters:\n        model: nn.Module\n            The model to be pruned.\n        config_list: list\n            Configuration list to configure layer pruning.\n            Supported keys:\n            - op_types: operation type to be pruned\n            - op_names: operation name to be pruned\n        evaluator: function\n            function to evaluate the pruned model.\n            The prototype of the function:\n            >>> def evaluator(val_loader, model):\n            >>>     ...\n            >>>     return acc\n        val_loader: torch.utils.data.DataLoader\n            Data loader of validation dataset.\n        suffix: str\n            suffix to help you remember what experiment you ran. Default: None.\n        job: str\n            train_export: search best pruned model and export after search.\n            export_only: export a searched model, searched_model_path and export_path must be specified.\n        searched_model_path: str\n            when job == export_only, use searched_model_path to specify the path of the searched model.\n        export_path: str\n            path for exporting models\n\n        # parameters for pruning environment\n        model_type: str\n            model type to prune, currently 'mobilenet' and 'mobilenetv2' are supported. Default: mobilenet\n        flops_ratio: float\n            preserve flops ratio. Default: 0.5\n        lbound: float\n            minimum weight preserve ratio for each layer. Default: 0.2\n        rbound: float\n            maximum weight preserve ratio for each layer. Default: 1.0\n        reward: function\n            reward function type:\n            - acc_reward: accuracy * 0.01\n            - acc_flops_reward: - (100 - accuracy) * 0.01 * np.log(flops)\n            Default: acc_reward\n        # parameters for channel pruning\n        n_calibration_batches: int\n            number of batches to extract layer information. Default: 60\n        n_points_per_layer: int\n            number of feature points per layer. Default: 10\n        channel_round: int\n            round channel to multiple of channel_round. Default: 8\n\n        # parameters for ddpg agent\n        hidden1: int\n            hidden num of first fully connect layer. Default: 300\n        hidden2: int\n            hidden num of second fully connect layer. Default: 300\n        lr_c: float\n            learning rate for critic. Default: 1e-3\n        lr_a: float\n            learning rate for actor. Default: 1e-4\n        warmup: int\n            number of episodes without training but only filling the replay memory. During warmup episodes,\n            random actions ares used for pruning. Default: 100\n        discount: float\n            next Q value discount for deep Q value target. Default: 0.99\n        bsize: int\n            minibatch size for training DDPG agent. Default: 64\n        rmsize: int\n            memory size for each layer. Default: 100\n        window_length: int\n            replay buffer window length. Default: 1\n        tau: float\n            moving average for target network being used by soft_update. Default: 0.99\n        # noise\n        init_delta: float\n            initial variance of truncated normal distribution\n        delta_decay: float\n            delta decay during exploration\n\n        # parameters for training ddpg agent\n        max_episode_length: int\n            maximum episode length\n        output_dir: str\n            output directory to save log files and model files. Default: ./logs\n        debug: boolean\n            debug mode\n        train_episode: int\n            train iters each timestep. Default: 800\n        epsilon: int\n            linear decay of exploration policy. Default: 50000\n        seed: int\n            random seed to set for reproduce experiment. Default: None\n    \"\"\"\n\n    def __init__(\n            self,\n            model,\n            config_list,\n            evaluator,\n            val_loader,\n            suffix=None,\n            job='train_export',\n            export_path=None,\n            searched_model_path=None,\n            model_type='mobilenet',\n            dataset='cifar10',\n            flops_ratio=0.5,\n            lbound=0.2,\n            rbound=1.,\n            reward='acc_reward',\n            n_calibration_batches=60,\n            n_points_per_layer=10,\n            channel_round=8,\n            hidden1=300,\n            hidden2=300,\n            lr_c=1e-3,\n            lr_a=1e-4,\n            warmup=100,\n            discount=1.,\n            bsize=64,\n            rmsize=100,\n            window_length=1,\n            tau=0.01,\n            init_delta=0.5,\n            delta_decay=0.99,\n            max_episode_length=1e9,\n            output_dir='./logs',\n            debug=False,\n            train_episode=800,\n            epsilon=50000,\n            seed=None):\n\n        self.job = job\n        self.searched_model_path = searched_model_path\n        self.export_path = export_path\n\n        if seed is not None:\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed(seed)\n\n        checkpoint = deepcopy(model.state_dict())\n\n        super().__init__(model, config_list, optimizer=None)\n\n        # build folder and logs\n        base_folder_name = '{}_{}_r{}_search'.format(model_type, dataset, flops_ratio)\n        if suffix is not None:\n            base_folder_name = base_folder_name + '_' + suffix\n        self.output_dir = get_output_folder(output_dir, base_folder_name)\n\n        if self.export_path is None:\n            self.export_path = os.path.join(self.output_dir, '{}_r{}_exported.pth'.format(model_type, flops_ratio))\n\n        self.env_args = Namespace(\n            model_type=model_type,\n            preserve_ratio=flops_ratio,\n            lbound=lbound,\n            rbound=rbound,\n            reward=reward,\n            n_calibration_batches=n_calibration_batches,\n            n_points_per_layer=n_points_per_layer,\n            channel_round=channel_round,\n            output=self.output_dir\n        )\n\n        self.env = ChannelPruningEnv(\n            self, evaluator, val_loader, checkpoint, args=self.env_args)\n\n        if self.job == 'train_export':\n            print('=> Saving logs to {}'.format(self.output_dir))\n            self.tfwriter = SummaryWriter(log_dir=self.output_dir)\n            self.text_writer = open(os.path.join(self.output_dir, 'log.txt'), 'w')\n            print('=> Output path: {}...'.format(self.output_dir))\n\n            nb_states = self.env.layer_embedding.shape[1]\n            nb_actions = 1  # just 1 action here\n\n            rmsize = rmsize * len(self.env.prunable_idx)  # for each layer\n            print('** Actual replay buffer size: {}'.format(rmsize))\n\n            self.ddpg_args = Namespace(\n                hidden1=hidden1,\n                hidden2=hidden2,\n                lr_c=lr_c,\n                lr_a=lr_a,\n                warmup=warmup,\n                discount=discount,\n                bsize=bsize,\n                rmsize=rmsize,\n                window_length=window_length,\n                tau=tau,\n                init_delta=init_delta,\n                delta_decay=delta_decay,\n                max_episode_length=max_episode_length,\n                debug=debug,\n                train_episode=train_episode,\n                epsilon=epsilon\n            )\n            self.agent = DDPG(nb_states, nb_actions, self.ddpg_args)\n\n\n    def compress(self):\n        if self.job == 'train_export':\n            self.train(self.ddpg_args.train_episode, self.agent, self.env, self.output_dir)\n        self.export_pruned_model()\n\n    def train(self, num_episode, agent, env, output_dir):\n        agent.is_training = True\n        step = episode = episode_steps = 0\n        episode_reward = 0.\n        observation = None\n        T = []  # trajectory\n        while episode < num_episode:  # counting based on episode\n            # reset if it is the start of episode\n            if observation is None:\n                observation = deepcopy(env.reset())\n                agent.reset(observation)\n\n            # agent pick action ...\n            if episode <= self.ddpg_args.warmup:\n                action = agent.random_action()\n                # action = sample_from_truncated_normal_distribution(lower=0., upper=1., mu=env.preserve_ratio, sigma=0.5)\n            else:\n                action = agent.select_action(observation, episode=episode)\n\n            # env response with next_observation, reward, terminate_info\n            observation2, reward, done, info = env.step(action)\n\n            T.append([reward, deepcopy(observation), deepcopy(observation2), action, done])\n\n            # fix-length, never reach here\n            # if max_episode_length and episode_steps >= max_episode_length - 1:\n            #     done = True\n\n            # [optional] save intermideate model\n            if num_episode / 3 <= 1 or episode % int(num_episode / 3) == 0:\n                agent.save_model(output_dir)\n\n            # update\n            step += 1\n            episode_steps += 1\n            episode_reward += reward\n            observation = deepcopy(observation2)\n\n            if done:  # end of episode\n                print(\n                    '#{}: episode_reward:{:.4f} acc: {:.4f}, ratio: {:.4f}'.format(\n                        episode, episode_reward,\n                        info['accuracy'],\n                        info['compress_ratio']\n                    )\n                )\n                self.text_writer.write(\n                    '#{}: episode_reward:{:.4f} acc: {:.4f}, ratio: {:.4f}\\n'.format(\n                        episode, episode_reward,\n                        info['accuracy'],\n                        info['compress_ratio']\n                    )\n                )\n                final_reward = T[-1][0]\n                # print('final_reward: {}'.format(final_reward))\n                # agent observe and update policy\n                for _, s_t, s_t1, a_t, done in T:\n                    agent.observe(final_reward, s_t, s_t1, a_t, done)\n                    if episode > self.ddpg_args.warmup:\n                        agent.update_policy()\n\n                #agent.memory.append(\n                #    observation,\n                #    agent.select_action(observation, episode=episode),\n                #    0., False\n                #)\n\n                # reset\n                observation = None\n                episode_steps = 0\n                episode_reward = 0.\n                episode += 1\n                T = []\n\n                self.tfwriter.add_scalar('reward/last', final_reward, episode)\n                self.tfwriter.add_scalar('reward/best', env.best_reward, episode)\n                self.tfwriter.add_scalar('info/accuracy', info['accuracy'], episode)\n                self.tfwriter.add_scalar('info/compress_ratio', info['compress_ratio'], episode)\n                self.tfwriter.add_text('info/best_policy', str(env.best_strategy), episode)\n                # record the preserve rate for each layer\n                for i, preserve_rate in enumerate(env.strategy):\n                    self.tfwriter.add_scalar('preserve_rate/{}'.format(i), preserve_rate, episode)\n\n                self.text_writer.write('best reward: {}\\n'.format(env.best_reward))\n                self.text_writer.write('best policy: {}\\n'.format(env.best_strategy))\n        self.text_writer.close()\n\n    def export_pruned_model(self):\n        if self.searched_model_path is None:\n            wrapper_model_ckpt = os.path.join(self.output_dir, 'best_wrapped_model.pth')\n        else:\n            wrapper_model_ckpt = self.searched_model_path\n        self.env.reset()\n        self.bound_model.load_state_dict(torch.load(wrapper_model_ckpt))\n\n        print('validate searched model:', self.env._validate(self.env._val_loader, self.env.model))\n        self.env.export_model()\n        self._unwrap_model()\n        print('validate exported model:', self.env._validate(self.env._val_loader, self.env.model))\n\n        torch.save(self.bound_model, self.export_path)\n        print('exported model saved to: {}'.format(self.export_path))",
  "def __init__(\n            self,\n            model,\n            config_list,\n            evaluator,\n            val_loader,\n            suffix=None,\n            job='train_export',\n            export_path=None,\n            searched_model_path=None,\n            model_type='mobilenet',\n            dataset='cifar10',\n            flops_ratio=0.5,\n            lbound=0.2,\n            rbound=1.,\n            reward='acc_reward',\n            n_calibration_batches=60,\n            n_points_per_layer=10,\n            channel_round=8,\n            hidden1=300,\n            hidden2=300,\n            lr_c=1e-3,\n            lr_a=1e-4,\n            warmup=100,\n            discount=1.,\n            bsize=64,\n            rmsize=100,\n            window_length=1,\n            tau=0.01,\n            init_delta=0.5,\n            delta_decay=0.99,\n            max_episode_length=1e9,\n            output_dir='./logs',\n            debug=False,\n            train_episode=800,\n            epsilon=50000,\n            seed=None):\n\n        self.job = job\n        self.searched_model_path = searched_model_path\n        self.export_path = export_path\n\n        if seed is not None:\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed(seed)\n\n        checkpoint = deepcopy(model.state_dict())\n\n        super().__init__(model, config_list, optimizer=None)\n\n        # build folder and logs\n        base_folder_name = '{}_{}_r{}_search'.format(model_type, dataset, flops_ratio)\n        if suffix is not None:\n            base_folder_name = base_folder_name + '_' + suffix\n        self.output_dir = get_output_folder(output_dir, base_folder_name)\n\n        if self.export_path is None:\n            self.export_path = os.path.join(self.output_dir, '{}_r{}_exported.pth'.format(model_type, flops_ratio))\n\n        self.env_args = Namespace(\n            model_type=model_type,\n            preserve_ratio=flops_ratio,\n            lbound=lbound,\n            rbound=rbound,\n            reward=reward,\n            n_calibration_batches=n_calibration_batches,\n            n_points_per_layer=n_points_per_layer,\n            channel_round=channel_round,\n            output=self.output_dir\n        )\n\n        self.env = ChannelPruningEnv(\n            self, evaluator, val_loader, checkpoint, args=self.env_args)\n\n        if self.job == 'train_export':\n            print('=> Saving logs to {}'.format(self.output_dir))\n            self.tfwriter = SummaryWriter(log_dir=self.output_dir)\n            self.text_writer = open(os.path.join(self.output_dir, 'log.txt'), 'w')\n            print('=> Output path: {}...'.format(self.output_dir))\n\n            nb_states = self.env.layer_embedding.shape[1]\n            nb_actions = 1  # just 1 action here\n\n            rmsize = rmsize * len(self.env.prunable_idx)  # for each layer\n            print('** Actual replay buffer size: {}'.format(rmsize))\n\n            self.ddpg_args = Namespace(\n                hidden1=hidden1,\n                hidden2=hidden2,\n                lr_c=lr_c,\n                lr_a=lr_a,\n                warmup=warmup,\n                discount=discount,\n                bsize=bsize,\n                rmsize=rmsize,\n                window_length=window_length,\n                tau=tau,\n                init_delta=init_delta,\n                delta_decay=delta_decay,\n                max_episode_length=max_episode_length,\n                debug=debug,\n                train_episode=train_episode,\n                epsilon=epsilon\n            )\n            self.agent = DDPG(nb_states, nb_actions, self.ddpg_args)",
  "def compress(self):\n        if self.job == 'train_export':\n            self.train(self.ddpg_args.train_episode, self.agent, self.env, self.output_dir)\n        self.export_pruned_model()",
  "def train(self, num_episode, agent, env, output_dir):\n        agent.is_training = True\n        step = episode = episode_steps = 0\n        episode_reward = 0.\n        observation = None\n        T = []  # trajectory\n        while episode < num_episode:  # counting based on episode\n            # reset if it is the start of episode\n            if observation is None:\n                observation = deepcopy(env.reset())\n                agent.reset(observation)\n\n            # agent pick action ...\n            if episode <= self.ddpg_args.warmup:\n                action = agent.random_action()\n                # action = sample_from_truncated_normal_distribution(lower=0., upper=1., mu=env.preserve_ratio, sigma=0.5)\n            else:\n                action = agent.select_action(observation, episode=episode)\n\n            # env response with next_observation, reward, terminate_info\n            observation2, reward, done, info = env.step(action)\n\n            T.append([reward, deepcopy(observation), deepcopy(observation2), action, done])\n\n            # fix-length, never reach here\n            # if max_episode_length and episode_steps >= max_episode_length - 1:\n            #     done = True\n\n            # [optional] save intermideate model\n            if num_episode / 3 <= 1 or episode % int(num_episode / 3) == 0:\n                agent.save_model(output_dir)\n\n            # update\n            step += 1\n            episode_steps += 1\n            episode_reward += reward\n            observation = deepcopy(observation2)\n\n            if done:  # end of episode\n                print(\n                    '#{}: episode_reward:{:.4f} acc: {:.4f}, ratio: {:.4f}'.format(\n                        episode, episode_reward,\n                        info['accuracy'],\n                        info['compress_ratio']\n                    )\n                )\n                self.text_writer.write(\n                    '#{}: episode_reward:{:.4f} acc: {:.4f}, ratio: {:.4f}\\n'.format(\n                        episode, episode_reward,\n                        info['accuracy'],\n                        info['compress_ratio']\n                    )\n                )\n                final_reward = T[-1][0]\n                # print('final_reward: {}'.format(final_reward))\n                # agent observe and update policy\n                for _, s_t, s_t1, a_t, done in T:\n                    agent.observe(final_reward, s_t, s_t1, a_t, done)\n                    if episode > self.ddpg_args.warmup:\n                        agent.update_policy()\n\n                #agent.memory.append(\n                #    observation,\n                #    agent.select_action(observation, episode=episode),\n                #    0., False\n                #)\n\n                # reset\n                observation = None\n                episode_steps = 0\n                episode_reward = 0.\n                episode += 1\n                T = []\n\n                self.tfwriter.add_scalar('reward/last', final_reward, episode)\n                self.tfwriter.add_scalar('reward/best', env.best_reward, episode)\n                self.tfwriter.add_scalar('info/accuracy', info['accuracy'], episode)\n                self.tfwriter.add_scalar('info/compress_ratio', info['compress_ratio'], episode)\n                self.tfwriter.add_text('info/best_policy', str(env.best_strategy), episode)\n                # record the preserve rate for each layer\n                for i, preserve_rate in enumerate(env.strategy):\n                    self.tfwriter.add_scalar('preserve_rate/{}'.format(i), preserve_rate, episode)\n\n                self.text_writer.write('best reward: {}\\n'.format(env.best_reward))\n                self.text_writer.write('best policy: {}\\n'.format(env.best_strategy))\n        self.text_writer.close()",
  "def export_pruned_model(self):\n        if self.searched_model_path is None:\n            wrapper_model_ckpt = os.path.join(self.output_dir, 'best_wrapped_model.pth')\n        else:\n            wrapper_model_ckpt = self.searched_model_path\n        self.env.reset()\n        self.bound_model.load_state_dict(torch.load(wrapper_model_ckpt))\n\n        print('validate searched model:', self.env._validate(self.env._val_loader, self.env.model))\n        self.env.export_model()\n        self._unwrap_model()\n        print('validate exported model:', self.env._validate(self.env._val_loader, self.env.model))\n\n        torch.save(self.bound_model, self.export_path)\n        print('exported model saved to: {}'.format(self.export_path))",
  "def acc_reward(net, acc, flops):\n    return acc * 0.01",
  "def acc_flops_reward(net, acc, flops):\n    error = (100 - acc) * 0.01\n    return -error * np.log(flops)",
  "class ChannelPruningEnv:\n    \"\"\"\n    Env for channel pruning search.\n    This class is used to prune model using specified pruner. It prunes one layer when\n    step() is called. When the last layer is pruned, it evaluate the pruned model using\n    evaluator, and use the returned value of evaluator as reward of the episode.\n\n    Usage:\n        env = ChannelPruningEnv(pruner, evaluator, val_loader, checkpoint, env_args)\n        episode = 0\n        T = []\n        while episode < num_episode:\n            action = agent.select_action(observation)\n            observation2, reward, done, info = env.step(action)\n            T.append([reward, deepcopy(observation), deepcopy(observation2), action, done])\n\n            if done: # end of episode, last layer pruned\n                episode += 1\n                # train agent with episode data\n                for _, s_t, s_t1, a_t, done in T:\n                    agent.observe(final_reward, s_t, s_t1, a_t, done)\n                    agent.update_policy()\n                T = []\n\n    Attributes:\n        prunable_idx: layer indices for pruable layers, the index values are the index\n            of list(self.model.modules()). Pruable layers are pointwise Conv2d layers and Linear\n            layers.\n        buffer_idx: layer indices for buffer layers which refers the depthwise layers.\n            Each depthwise layer is always followd by a pointwise layer for both mobilenet and\n            mobilenetv2. The depthwise layer's filters are pruned when its next pointwise layer's\n            corresponding input channels are pruned.\n        shared_idx: layer indices for layers which share input.\n            For example: [[1,4], [8, 10, 15]] means layer 1 and 4 share same input, and layer\n            8, 10 and 15 share another input.\n        layer_embedding: embeddings for each prunable layers, the embedding is used as\n            observation for DDPG agent.\n        layer_info_dict: flops and number of parameters of each layer.\n        min_strategy_dict: key is layer index, value is a tuple, the first value is the minimum\n            action of input channel, the second value is the minimum action value of output channel.\n        strategy_dict: key is layer index, value is a tuple, the first value is the action of input\n            channel, the second value is the action of output channel.\n\n    Parameters:\n        pruner: Pruner\n            NNI Pruner instance used to prune model.\n        evaluator: function\n            function to evaluate the pruned model.\n            The prototype of the function:\n                >>> def evaluator(val_loader, model):\n                >>>     ...\n                >>>     return acc\n        val_loader: torch.utils.data.DataLoader\n            Data loader of validation dataset.\n        checkpoint: dict\n            checkpoint of the model to be pruned. It is used to reset model at beginning of each\n            episode.\n        args:\n            A Namespace object containing following arguments:\n            model_type: str\n                model type to prune, currently 'mobilenet' and 'mobilenetv2' are supported.\n            flops_ratio: float\n                preserve flops ratio.\n            lbound: float\n                minimum weight preserve ratio for each layer.\n            rbound: float\n                maximum weight preserve ratio for each layer.\n            reward: function\n                reward function type\n\n            # parameters for channel pruning\n            n_calibration_batches: int\n                number of batches to extract layer information.\n            n_points_per_layer: int\n                number of feature points per layer.\n            channel_round: int\n                round channel to multiple of channel_round.\n\n    \"\"\"\n    def __init__(self, pruner, evaluator, val_loader, checkpoint, args):\n        self.pruner = pruner\n        self.model = pruner.bound_model\n        self.checkpoint = checkpoint\n        self.batch_size = val_loader.batch_size\n        self.preserve_ratio = args.preserve_ratio\n        self.channel_prune_masker = AMCWeightMasker(self.model, self.pruner, args.channel_round)\n\n        # options from args\n        self.args = args\n        self.lbound = args.lbound\n        self.rbound = args.rbound\n\n        self.n_calibration_batches = args.n_calibration_batches\n        self.n_points_per_layer = args.n_points_per_layer\n        self.channel_round = args.channel_round\n\n        # sanity check\n        assert self.preserve_ratio > self.lbound, 'Error! You can not achieve preserve_ratio smaller than lbound!'\n\n        # prepare data\n        self._val_loader = val_loader\n        self._validate = evaluator\n\n        # build indexs\n        self._build_index()\n        self.n_prunable_layer = len(self.prunable_idx)\n\n        # extract information for preparing\n        self._extract_layer_information()\n\n        # build embedding (static part)\n        self._build_state_embedding()\n\n        # build reward\n        self.reset()  # restore weight\n        self.org_acc = self._validate(self._val_loader, self.model)\n        print('=> original acc: {:.3f}%'.format(self.org_acc))\n        self.org_model_size = sum(self.wsize_list)\n        print('=> original weight size: {:.4f} M param'.format(self.org_model_size * 1. / 1e6))\n        self.org_flops = sum(self.flops_list)\n        print('=> FLOPs:')\n        print([self.layer_info_dict[idx]['flops']/1e6 for idx in sorted(self.layer_info_dict.keys())])\n        print('=> original FLOPs: {:.4f} M'.format(self.org_flops * 1. / 1e6))\n\n        self.expected_preserve_computation = self.preserve_ratio * self.org_flops\n\n        self.reward = eval(args.reward)\n\n        self.best_reward = -math.inf\n        self.best_strategy = None\n        self.best_d_prime_list = None\n        self.best_masks = None\n\n        self.org_w_size = sum(self.wsize_list)\n\n    def step(self, action):\n        # Pseudo prune and get the corresponding statistics. The real pruning happens till the end of all pseudo pruning\n        if self.visited[self.cur_ind]:\n            action = self.strategy_dict[self.prunable_idx[self.cur_ind]][0]\n            preserve_idx = self.index_buffer[self.cur_ind]\n        else:\n            action = self._action_wall(action)  # percentage to preserve\n            preserve_idx = None\n        # prune and update action\n        action, d_prime, preserve_idx = self.prune_kernel(self.prunable_idx[self.cur_ind], action, preserve_idx)\n        if not self.visited[self.cur_ind]:\n            for group in self.shared_idx:\n                if self.cur_ind in group:  # set the shared ones\n                    for g_idx in group:\n                        self.strategy_dict[self.prunable_idx[g_idx]][0] = action\n                        self.strategy_dict[self.prunable_idx[g_idx - 1]][1] = action\n                        self.visited[g_idx] = True\n                        self.index_buffer[g_idx] = preserve_idx.copy()\n\n        self.strategy.append(action)  # save action to strategy\n        self.d_prime_list.append(d_prime)\n\n        self.strategy_dict[self.prunable_idx[self.cur_ind]][0] = action\n        if self.cur_ind > 0:\n            self.strategy_dict[self.prunable_idx[self.cur_ind - 1]][1] = action\n\n        # all the actions are made\n        if self._is_final_layer():\n            assert len(self.strategy) == len(self.prunable_idx)\n            current_flops = self._cur_flops()\n            acc_t1 = time.time()\n            acc = self._validate(self._val_loader, self.model)\n            acc_t2 = time.time()\n            self.val_time = acc_t2 - acc_t1\n            compress_ratio = current_flops * 1. / self.org_flops\n            info_set = {'compress_ratio': compress_ratio, 'accuracy': acc, 'strategy': self.strategy.copy()}\n            reward = self.reward(self, acc, current_flops)\n\n            if reward > self.best_reward:\n                self.best_reward = reward\n                self.best_strategy = self.strategy.copy()\n                self.best_d_prime_list = self.d_prime_list.copy()\n                torch.save(self.model.state_dict(), os.path.join(self.args.output, 'best_wrapped_model.pth'))\n                prGreen('New best reward: {:.4f}, acc: {:.4f}, compress: {:.4f}'.format(self.best_reward, acc, compress_ratio))\n                prGreen('New best policy: {}'.format(self.best_strategy))\n                prGreen('New best d primes: {}'.format(self.best_d_prime_list))\n            obs = self.layer_embedding[self.cur_ind, :].copy()  # actually the same as the last state\n            done = True\n            return obs, reward, done, info_set\n\n        info_set = None\n        reward = 0\n        done = False\n        self.visited[self.cur_ind] = True  # set to visited\n        self.cur_ind += 1  # the index of next layer\n        # build next state (in-place modify)\n        self.layer_embedding[self.cur_ind][-3] = self._cur_reduced() * 1. / self.org_flops  # reduced\n        self.layer_embedding[self.cur_ind][-2] = sum(self.flops_list[self.cur_ind + 1:]) * 1. / self.org_flops  # rest\n        self.layer_embedding[self.cur_ind][-1] = self.strategy[-1]  # last action\n        obs = self.layer_embedding[self.cur_ind, :].copy()\n\n        return obs, reward, done, info_set\n\n    def reset(self):\n        # restore env by loading the checkpoint\n        self.pruner.reset(self.checkpoint)\n        self.cur_ind = 0\n        self.strategy = []  # pruning strategy\n        self.d_prime_list = []\n        self.strategy_dict = copy.deepcopy(self.min_strategy_dict)\n        # reset layer embeddings\n        self.layer_embedding[:, -1] = 1.\n        self.layer_embedding[:, -2] = 0.\n        self.layer_embedding[:, -3] = 0.\n        obs = self.layer_embedding[0].copy()\n        obs[-2] = sum(self.wsize_list[1:]) * 1. / sum(self.wsize_list)\n        self.extract_time = 0\n        self.fit_time = 0\n        self.val_time = 0\n        # for share index\n        self.visited = [False] * len(self.prunable_idx)\n        self.index_buffer = {}\n        return obs\n\n    def set_export_path(self, path):\n        self.export_path = path\n\n    def prune_kernel(self, op_idx, preserve_ratio, preserve_idx=None):\n        m_list = list(self.model.modules())\n        op = m_list[op_idx]\n        assert (0. < preserve_ratio <= 1.)\n        assert type(op) == PrunerModuleWrapper\n        if preserve_ratio == 1:  # do not prune\n            if (preserve_idx is None) or (len(preserve_idx) == op.module.weight.size(1)):\n                return 1., op.module.weight.size(1), None  # should be a full index\n        op.input_feat = self.layer_info_dict[op_idx]['input_feat']\n        op.output_feat = self.layer_info_dict[op_idx]['output_feat']\n\n        masks = self.channel_prune_masker.calc_mask(sparsity=1-preserve_ratio, wrapper=op, preserve_idx=preserve_idx)\n        m = masks['weight_mask'].cpu().data\n        if type(op.module) == nn.Conv2d:\n            d_prime = (m.sum((0, 2, 3)) > 0).sum().item()\n            preserve_idx = np.nonzero((m.sum((0, 2, 3)) > 0).numpy())[0]\n        else:\n            assert type(op.module) == nn.Linear\n            d_prime = (m.sum(1) > 0).sum().item()\n            preserve_idx = np.nonzero((m.sum(1) > 0).numpy())[0]\n\n        op.weight_mask = masks['weight_mask']\n        if hasattr(op.module, 'bias') and op.module.bias is not None and 'bias_mask' in masks:\n            op.bias_mask = masks['bias_mask']\n\n        action = (m == 1).sum().item() / m.numel()\n        return action, d_prime, preserve_idx\n\n    def export_model(self):\n        while True:\n            self.export_layer(self.prunable_idx[self.cur_ind])\n            if self._is_final_layer():\n                break\n            self.cur_ind += 1\n\n    #TODO replace this speedup implementation with nni.compression.torch.ModelSpeedup\n    def export_layer(self, op_idx):\n        m_list = list(self.model.modules())\n        op = m_list[op_idx]\n        assert type(op) == PrunerModuleWrapper\n        w = op.module.weight.cpu().data\n        m = op.weight_mask.cpu().data\n        if type(op.module) == nn.Linear:\n            w = w.unsqueeze(-1).unsqueeze(-1)\n            m = m.unsqueeze(-1).unsqueeze(-1)\n\n        d_prime = (m.sum((0, 2, 3)) > 0).sum().item()\n        preserve_idx = np.nonzero((m.sum((0, 2, 3)) > 0).numpy())[0]\n        assert d_prime <= w.size(1)\n\n        if d_prime == w.size(1):\n            return\n\n        mask = np.zeros(w.size(1), bool)\n        mask[preserve_idx] = True\n        rec_weight = torch.zeros((w.size(0), d_prime, w.size(2), w.size(3)))\n        rec_weight = w[:, preserve_idx, :, :]\n        if type(op.module) == nn.Linear:\n            rec_weight = rec_weight.squeeze()\n        # no need to provide bias mask for channel pruning\n        rec_mask = torch.ones_like(rec_weight)\n\n        # assign new weight and mask\n        device = op.module.weight.device\n        op.module.weight.data = rec_weight.to(device)\n        op.weight_mask = rec_mask.to(device)\n        if type(op.module) == nn.Conv2d:\n            op.module.in_channels = d_prime\n        else:\n            # Linear\n            op.module.in_features = d_prime\n\n        # export prev layers\n        prev_idx = self.prunable_idx[self.prunable_idx.index(op_idx) - 1]\n        for idx in range(prev_idx, op_idx):\n            m = m_list[idx]\n            if type(m) == nn.Conv2d:  # depthwise\n                m.weight.data = m.weight.data[mask, :, :, :]\n                if m.groups == m.in_channels:\n                    m.groups = int(np.sum(mask))\n                m.out_channels = d_prime\n            elif type(m) == nn.BatchNorm2d:\n                m.weight.data = m.weight.data[mask]\n                m.bias.data = m.bias.data[mask]\n                m.running_mean.data = m.running_mean.data[mask]\n                m.running_var.data = m.running_var.data[mask]\n                m.num_features = d_prime\n\n    def _is_final_layer(self):\n        return self.cur_ind == len(self.prunable_idx) - 1\n\n    def _action_wall(self, action):\n        \"\"\"\n        Limit the action generated by DDPG for this layer by two constraints:\n        1. The total flops must meet the flops reduce target.\n           For example: the original flops of entire model is 1000, target flops ratio is 0.5, target flops\n           is 1000*0.5 = 500. The reduced flops of other layers is 400, so the remaining flops quota is 500-400=100,\n           if the total original flops of this layer is 250, then the maximum ratio is 100/250 = 0.4. So the\n           action of this layer can not be greater than 0.4.\n        2. The action must be greater than lbound which is stored in self.strategy_dict.\n        \"\"\"\n        assert len(self.strategy) == self.cur_ind\n\n        action = float(action)\n        action = np.clip(action, 0, 1)\n\n        other_comp = 0\n        this_comp = 0\n        for i, idx in enumerate(self.prunable_idx):\n            flop = self.layer_info_dict[idx]['flops']\n            buffer_flop = self._get_buffer_flops(idx)\n\n            if i == self.cur_ind - 1:  # TODO: add other member in the set\n                this_comp += flop * self.strategy_dict[idx][0]\n                # add buffer (but not influenced by ratio)\n                other_comp += buffer_flop * self.strategy_dict[idx][0]\n            elif i == self.cur_ind:\n                this_comp += flop * self.strategy_dict[idx][1]\n                # also add buffer here (influenced by ratio)\n                this_comp += buffer_flop\n            else:\n                other_comp += flop * self.strategy_dict[idx][0] * self.strategy_dict[idx][1]\n                # add buffer\n                other_comp += buffer_flop * self.strategy_dict[idx][0]  # only consider input reduction\n\n        self.expected_min_preserve = other_comp + this_comp * action\n        max_preserve_ratio = (self.expected_preserve_computation - other_comp) * 1. / this_comp\n\n        action = np.minimum(action, max_preserve_ratio)\n        action = np.maximum(action, self.strategy_dict[self.prunable_idx[self.cur_ind]][0])  # impossible (should be)\n\n        return action\n\n    def _get_buffer_flops(self, idx):\n        buffer_idx = self.buffer_dict[idx]\n        buffer_flop = sum([self.layer_info_dict[_]['flops'] for _ in buffer_idx])\n        return buffer_flop\n\n    def _cur_flops(self):\n        flops = 0\n        for idx in self.prunable_idx:\n            c, n = self.strategy_dict[idx]  # input, output pruning ratio\n            flops += self.layer_info_dict[idx]['flops'] * c * n\n            # add buffer computation\n            flops += self._get_buffer_flops(idx) * c  # only related to input channel reduction\n        return flops\n\n    def _cur_reduced(self):\n        # return the reduced weight\n        reduced = self.org_flops - self._cur_flops()\n        return reduced\n\n    def _build_index(self):\n        \"\"\"\n        Build following information/data for later pruning:\n        self.prunable_idx: layer indices for pruable layers, the index values are the index\n            of list(self.model.modules()). Pruable layers are pointwise Conv2d layers and Linear\n            layers.\n        self.prunable_ops: prunable modules\n        self.buffer_idx: layer indices for buffer layers which refers the depthwise layers.\n            Each depthwise layer is always followd by a pointwise layer for both mobilenet and\n            mobilenetv2. The depthwise layer's filters are pruned when its next pointwise layer's\n            corresponding input channels are pruned.\n        self.shared_idx: layer indices for layers which share input.\n            For example: [[1,4], [8, 10, 15]] means layer 1 and 4 share same input, and layer\n            8, 10 and 15 share another input.\n        self.org_channels: number of input channels for each layer\n        self.min_strategy_dict: key is layer index, value is a tuple, the first value is the minimum\n            action of input channel, the second value is the minimum action value of output channel.\n        self.strategy_dict: same as self.min_strategy_dict, but it will be updated later.\n        \"\"\"\n        self.prunable_idx = []\n        self.prunable_ops = []\n        self.layer_type_dict = {}\n        self.strategy_dict = {}\n        self.buffer_dict = {}\n        this_buffer_list = []\n        self.org_channels = []\n        # build index and the min strategy dict\n        for i, m in enumerate(self.model.modules()):\n            if isinstance(m, PrunerModuleWrapper):\n                m = m.module\n                if type(m) == nn.Conv2d and m.groups == m.in_channels:  # depth-wise conv, buffer\n                    this_buffer_list.append(i)\n                else:  # really prunable\n                    self.prunable_idx.append(i)\n                    self.prunable_ops.append(m)\n                    self.layer_type_dict[i] = type(m)\n                    self.buffer_dict[i] = this_buffer_list\n                    this_buffer_list = []  # empty\n                    self.org_channels.append(m.in_channels if type(m) == nn.Conv2d else m.in_features)\n\n                    self.strategy_dict[i] = [self.lbound, self.lbound]\n\n        self.strategy_dict[self.prunable_idx[0]][0] = 1  # modify the input\n        self.strategy_dict[self.prunable_idx[-1]][1] = 1  # modify the output\n\n        self.shared_idx = []\n        if self.args.model_type == 'mobilenetv2':  # TODO: to be tested! Share index for residual connection\n            connected_idx = [4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]  # to be partitioned\n            last_ch = -1\n            share_group = None\n            for c_idx in connected_idx:\n                if self.prunable_ops[c_idx].in_channels != last_ch:  # new group\n                    last_ch = self.prunable_ops[c_idx].in_channels\n                    if share_group is not None:\n                        self.shared_idx.append(share_group)\n                    share_group = [c_idx]\n                else:  # same group\n                    share_group.append(c_idx)\n            self.shared_idx.append(share_group)\n            print('=> Conv layers to share channels: {}'.format(self.shared_idx))\n\n        self.min_strategy_dict = copy.deepcopy(self.strategy_dict)\n\n        self.buffer_idx = []\n        for _, v in self.buffer_dict.items():\n            self.buffer_idx += v\n\n        print('=> Prunable layer idx: {}'.format(self.prunable_idx))\n        print('=> Buffer layer idx: {}'.format(self.buffer_idx))\n        print('=> Shared idx: {}'.format(self.shared_idx))\n        print('=> Initial min strategy dict: {}'.format(self.min_strategy_dict))\n\n        # added for supporting residual connections during pruning\n        self.visited = [False] * len(self.prunable_idx)\n        self.index_buffer = {}\n\n    def _extract_layer_information(self):\n        m_list = list(self.model.modules())\n\n        self.data_saver = []\n        self.layer_info_dict = dict()\n        self.wsize_list = []\n        self.flops_list = []\n\n        from .lib.utils import measure_layer_for_pruning\n\n        # extend the forward fn to record layer info\n        def new_forward(m):\n            def lambda_forward(x):\n                m.input_feat = x.clone()\n                #TODO replace this flops counter with nni.compression.torch.utils.counter.count_flops_params\n                measure_layer_for_pruning(m, x)\n                y = m.old_forward(x)\n                m.output_feat = y.clone()\n                return y\n\n            return lambda_forward\n\n        device = None\n        for idx in self.prunable_idx + self.buffer_idx:  # get all\n            m = m_list[idx]\n            m.old_forward = m.forward\n            m.forward = new_forward(m)\n            if device is None and type(m) == PrunerModuleWrapper:\n                device = m.module.weight.device\n\n        # now let the image flow\n        print('=> Extracting information...')\n        with torch.no_grad():\n            for i_b, (inputs, target) in enumerate(self._val_loader):  # use image from train set\n                if i_b == self.n_calibration_batches:\n                    break\n                self.data_saver.append((inputs.clone(), target.clone()))\n                input_var = torch.autograd.Variable(inputs).to(device)\n\n                # inference and collect stats\n                _ = self.model(input_var)\n\n                if i_b == 0:  # first batch\n                    for idx in self.prunable_idx + self.buffer_idx:\n                        self.layer_info_dict[idx] = dict()\n                        self.layer_info_dict[idx]['params'] = m_list[idx].params\n                        self.layer_info_dict[idx]['flops'] = m_list[idx].flops\n                        self.wsize_list.append(m_list[idx].params)\n                        self.flops_list.append(m_list[idx].flops)\n                    print('flops:', self.flops_list)\n                for idx in self.prunable_idx:\n                    f_in_np = m_list[idx].input_feat.data.cpu().numpy()\n                    f_out_np = m_list[idx].output_feat.data.cpu().numpy()\n                    if len(f_in_np.shape) == 4:  # conv\n                        if self.prunable_idx.index(idx) == 0:  # first conv\n                            f_in2save, f_out2save = None, None\n                        elif m_list[idx].module.weight.size(3) > 1:  # normal conv\n                            f_in2save, f_out2save = f_in_np, f_out_np\n                        else:  # 1x1 conv\n                            # assert f_out_np.shape[2] == f_in_np.shape[2]  # now support k=3\n                            randx = np.random.randint(0, f_out_np.shape[2] - 0, self.n_points_per_layer)\n                            randy = np.random.randint(0, f_out_np.shape[3] - 0, self.n_points_per_layer)\n                            # input: [N, C, H, W]\n                            self.layer_info_dict[idx][(i_b, 'randx')] = randx.copy()\n                            self.layer_info_dict[idx][(i_b, 'randy')] = randy.copy()\n\n                            f_in2save = f_in_np[:, :, randx, randy].copy().transpose(0, 2, 1)\\\n                                .reshape(self.batch_size * self.n_points_per_layer, -1)\n\n                            f_out2save = f_out_np[:, :, randx, randy].copy().transpose(0, 2, 1) \\\n                                .reshape(self.batch_size * self.n_points_per_layer, -1)\n                    else:\n                        assert len(f_in_np.shape) == 2\n                        f_in2save = f_in_np.copy()\n                        f_out2save = f_out_np.copy()\n                    if 'input_feat' not in self.layer_info_dict[idx]:\n                        self.layer_info_dict[idx]['input_feat'] = f_in2save\n                        self.layer_info_dict[idx]['output_feat'] = f_out2save\n                    else:\n                        self.layer_info_dict[idx]['input_feat'] = np.vstack(\n                            (self.layer_info_dict[idx]['input_feat'], f_in2save))\n                        self.layer_info_dict[idx]['output_feat'] = np.vstack(\n                            (self.layer_info_dict[idx]['output_feat'], f_out2save))\n\n    def _build_state_embedding(self):\n        # build the static part of the state embedding\n        print('Building state embedding...')\n        layer_embedding = []\n        module_list = list(self.model.modules())\n        for i, ind in enumerate(self.prunable_idx):\n            m = module_list[ind].module\n            this_state = []\n            if type(m) == nn.Conv2d:\n                this_state.append(i)  # index\n                this_state.append(0)  # layer type, 0 for conv\n                this_state.append(m.in_channels)  # in channels\n                this_state.append(m.out_channels)  # out channels\n                this_state.append(m.stride[0])  # stride\n                this_state.append(m.kernel_size[0])  # kernel size\n                this_state.append(np.prod(m.weight.size()))  # weight size\n            elif type(m) == nn.Linear:\n                this_state.append(i)  # index\n                this_state.append(1)  # layer type, 1 for fc\n                this_state.append(m.in_features)  # in channels\n                this_state.append(m.out_features)  # out channels\n                this_state.append(0)  # stride\n                this_state.append(1)  # kernel size\n                this_state.append(np.prod(m.weight.size()))  # weight size\n\n            # this 3 features need to be changed later\n            this_state.append(0.)  # reduced\n            this_state.append(0.)  # rest\n            this_state.append(1.)  # a_{t-1}\n            layer_embedding.append(np.array(this_state))\n\n        # normalize the state\n        layer_embedding = np.array(layer_embedding, 'float')\n        print('=> shape of embedding (n_layer * n_dim): {}'.format(layer_embedding.shape))\n        assert len(layer_embedding.shape) == 2, layer_embedding.shape\n        for i in range(layer_embedding.shape[1]):\n            fmin = min(layer_embedding[:, i])\n            fmax = max(layer_embedding[:, i])\n            if fmax - fmin > 0:\n                layer_embedding[:, i] = (layer_embedding[:, i] - fmin) / (fmax - fmin)\n\n        self.layer_embedding = layer_embedding",
  "def __init__(self, pruner, evaluator, val_loader, checkpoint, args):\n        self.pruner = pruner\n        self.model = pruner.bound_model\n        self.checkpoint = checkpoint\n        self.batch_size = val_loader.batch_size\n        self.preserve_ratio = args.preserve_ratio\n        self.channel_prune_masker = AMCWeightMasker(self.model, self.pruner, args.channel_round)\n\n        # options from args\n        self.args = args\n        self.lbound = args.lbound\n        self.rbound = args.rbound\n\n        self.n_calibration_batches = args.n_calibration_batches\n        self.n_points_per_layer = args.n_points_per_layer\n        self.channel_round = args.channel_round\n\n        # sanity check\n        assert self.preserve_ratio > self.lbound, 'Error! You can not achieve preserve_ratio smaller than lbound!'\n\n        # prepare data\n        self._val_loader = val_loader\n        self._validate = evaluator\n\n        # build indexs\n        self._build_index()\n        self.n_prunable_layer = len(self.prunable_idx)\n\n        # extract information for preparing\n        self._extract_layer_information()\n\n        # build embedding (static part)\n        self._build_state_embedding()\n\n        # build reward\n        self.reset()  # restore weight\n        self.org_acc = self._validate(self._val_loader, self.model)\n        print('=> original acc: {:.3f}%'.format(self.org_acc))\n        self.org_model_size = sum(self.wsize_list)\n        print('=> original weight size: {:.4f} M param'.format(self.org_model_size * 1. / 1e6))\n        self.org_flops = sum(self.flops_list)\n        print('=> FLOPs:')\n        print([self.layer_info_dict[idx]['flops']/1e6 for idx in sorted(self.layer_info_dict.keys())])\n        print('=> original FLOPs: {:.4f} M'.format(self.org_flops * 1. / 1e6))\n\n        self.expected_preserve_computation = self.preserve_ratio * self.org_flops\n\n        self.reward = eval(args.reward)\n\n        self.best_reward = -math.inf\n        self.best_strategy = None\n        self.best_d_prime_list = None\n        self.best_masks = None\n\n        self.org_w_size = sum(self.wsize_list)",
  "def step(self, action):\n        # Pseudo prune and get the corresponding statistics. The real pruning happens till the end of all pseudo pruning\n        if self.visited[self.cur_ind]:\n            action = self.strategy_dict[self.prunable_idx[self.cur_ind]][0]\n            preserve_idx = self.index_buffer[self.cur_ind]\n        else:\n            action = self._action_wall(action)  # percentage to preserve\n            preserve_idx = None\n        # prune and update action\n        action, d_prime, preserve_idx = self.prune_kernel(self.prunable_idx[self.cur_ind], action, preserve_idx)\n        if not self.visited[self.cur_ind]:\n            for group in self.shared_idx:\n                if self.cur_ind in group:  # set the shared ones\n                    for g_idx in group:\n                        self.strategy_dict[self.prunable_idx[g_idx]][0] = action\n                        self.strategy_dict[self.prunable_idx[g_idx - 1]][1] = action\n                        self.visited[g_idx] = True\n                        self.index_buffer[g_idx] = preserve_idx.copy()\n\n        self.strategy.append(action)  # save action to strategy\n        self.d_prime_list.append(d_prime)\n\n        self.strategy_dict[self.prunable_idx[self.cur_ind]][0] = action\n        if self.cur_ind > 0:\n            self.strategy_dict[self.prunable_idx[self.cur_ind - 1]][1] = action\n\n        # all the actions are made\n        if self._is_final_layer():\n            assert len(self.strategy) == len(self.prunable_idx)\n            current_flops = self._cur_flops()\n            acc_t1 = time.time()\n            acc = self._validate(self._val_loader, self.model)\n            acc_t2 = time.time()\n            self.val_time = acc_t2 - acc_t1\n            compress_ratio = current_flops * 1. / self.org_flops\n            info_set = {'compress_ratio': compress_ratio, 'accuracy': acc, 'strategy': self.strategy.copy()}\n            reward = self.reward(self, acc, current_flops)\n\n            if reward > self.best_reward:\n                self.best_reward = reward\n                self.best_strategy = self.strategy.copy()\n                self.best_d_prime_list = self.d_prime_list.copy()\n                torch.save(self.model.state_dict(), os.path.join(self.args.output, 'best_wrapped_model.pth'))\n                prGreen('New best reward: {:.4f}, acc: {:.4f}, compress: {:.4f}'.format(self.best_reward, acc, compress_ratio))\n                prGreen('New best policy: {}'.format(self.best_strategy))\n                prGreen('New best d primes: {}'.format(self.best_d_prime_list))\n            obs = self.layer_embedding[self.cur_ind, :].copy()  # actually the same as the last state\n            done = True\n            return obs, reward, done, info_set\n\n        info_set = None\n        reward = 0\n        done = False\n        self.visited[self.cur_ind] = True  # set to visited\n        self.cur_ind += 1  # the index of next layer\n        # build next state (in-place modify)\n        self.layer_embedding[self.cur_ind][-3] = self._cur_reduced() * 1. / self.org_flops  # reduced\n        self.layer_embedding[self.cur_ind][-2] = sum(self.flops_list[self.cur_ind + 1:]) * 1. / self.org_flops  # rest\n        self.layer_embedding[self.cur_ind][-1] = self.strategy[-1]  # last action\n        obs = self.layer_embedding[self.cur_ind, :].copy()\n\n        return obs, reward, done, info_set",
  "def reset(self):\n        # restore env by loading the checkpoint\n        self.pruner.reset(self.checkpoint)\n        self.cur_ind = 0\n        self.strategy = []  # pruning strategy\n        self.d_prime_list = []\n        self.strategy_dict = copy.deepcopy(self.min_strategy_dict)\n        # reset layer embeddings\n        self.layer_embedding[:, -1] = 1.\n        self.layer_embedding[:, -2] = 0.\n        self.layer_embedding[:, -3] = 0.\n        obs = self.layer_embedding[0].copy()\n        obs[-2] = sum(self.wsize_list[1:]) * 1. / sum(self.wsize_list)\n        self.extract_time = 0\n        self.fit_time = 0\n        self.val_time = 0\n        # for share index\n        self.visited = [False] * len(self.prunable_idx)\n        self.index_buffer = {}\n        return obs",
  "def set_export_path(self, path):\n        self.export_path = path",
  "def prune_kernel(self, op_idx, preserve_ratio, preserve_idx=None):\n        m_list = list(self.model.modules())\n        op = m_list[op_idx]\n        assert (0. < preserve_ratio <= 1.)\n        assert type(op) == PrunerModuleWrapper\n        if preserve_ratio == 1:  # do not prune\n            if (preserve_idx is None) or (len(preserve_idx) == op.module.weight.size(1)):\n                return 1., op.module.weight.size(1), None  # should be a full index\n        op.input_feat = self.layer_info_dict[op_idx]['input_feat']\n        op.output_feat = self.layer_info_dict[op_idx]['output_feat']\n\n        masks = self.channel_prune_masker.calc_mask(sparsity=1-preserve_ratio, wrapper=op, preserve_idx=preserve_idx)\n        m = masks['weight_mask'].cpu().data\n        if type(op.module) == nn.Conv2d:\n            d_prime = (m.sum((0, 2, 3)) > 0).sum().item()\n            preserve_idx = np.nonzero((m.sum((0, 2, 3)) > 0).numpy())[0]\n        else:\n            assert type(op.module) == nn.Linear\n            d_prime = (m.sum(1) > 0).sum().item()\n            preserve_idx = np.nonzero((m.sum(1) > 0).numpy())[0]\n\n        op.weight_mask = masks['weight_mask']\n        if hasattr(op.module, 'bias') and op.module.bias is not None and 'bias_mask' in masks:\n            op.bias_mask = masks['bias_mask']\n\n        action = (m == 1).sum().item() / m.numel()\n        return action, d_prime, preserve_idx",
  "def export_model(self):\n        while True:\n            self.export_layer(self.prunable_idx[self.cur_ind])\n            if self._is_final_layer():\n                break\n            self.cur_ind += 1",
  "def export_layer(self, op_idx):\n        m_list = list(self.model.modules())\n        op = m_list[op_idx]\n        assert type(op) == PrunerModuleWrapper\n        w = op.module.weight.cpu().data\n        m = op.weight_mask.cpu().data\n        if type(op.module) == nn.Linear:\n            w = w.unsqueeze(-1).unsqueeze(-1)\n            m = m.unsqueeze(-1).unsqueeze(-1)\n\n        d_prime = (m.sum((0, 2, 3)) > 0).sum().item()\n        preserve_idx = np.nonzero((m.sum((0, 2, 3)) > 0).numpy())[0]\n        assert d_prime <= w.size(1)\n\n        if d_prime == w.size(1):\n            return\n\n        mask = np.zeros(w.size(1), bool)\n        mask[preserve_idx] = True\n        rec_weight = torch.zeros((w.size(0), d_prime, w.size(2), w.size(3)))\n        rec_weight = w[:, preserve_idx, :, :]\n        if type(op.module) == nn.Linear:\n            rec_weight = rec_weight.squeeze()\n        # no need to provide bias mask for channel pruning\n        rec_mask = torch.ones_like(rec_weight)\n\n        # assign new weight and mask\n        device = op.module.weight.device\n        op.module.weight.data = rec_weight.to(device)\n        op.weight_mask = rec_mask.to(device)\n        if type(op.module) == nn.Conv2d:\n            op.module.in_channels = d_prime\n        else:\n            # Linear\n            op.module.in_features = d_prime\n\n        # export prev layers\n        prev_idx = self.prunable_idx[self.prunable_idx.index(op_idx) - 1]\n        for idx in range(prev_idx, op_idx):\n            m = m_list[idx]\n            if type(m) == nn.Conv2d:  # depthwise\n                m.weight.data = m.weight.data[mask, :, :, :]\n                if m.groups == m.in_channels:\n                    m.groups = int(np.sum(mask))\n                m.out_channels = d_prime\n            elif type(m) == nn.BatchNorm2d:\n                m.weight.data = m.weight.data[mask]\n                m.bias.data = m.bias.data[mask]\n                m.running_mean.data = m.running_mean.data[mask]\n                m.running_var.data = m.running_var.data[mask]\n                m.num_features = d_prime",
  "def _is_final_layer(self):\n        return self.cur_ind == len(self.prunable_idx) - 1",
  "def _action_wall(self, action):\n        \"\"\"\n        Limit the action generated by DDPG for this layer by two constraints:\n        1. The total flops must meet the flops reduce target.\n           For example: the original flops of entire model is 1000, target flops ratio is 0.5, target flops\n           is 1000*0.5 = 500. The reduced flops of other layers is 400, so the remaining flops quota is 500-400=100,\n           if the total original flops of this layer is 250, then the maximum ratio is 100/250 = 0.4. So the\n           action of this layer can not be greater than 0.4.\n        2. The action must be greater than lbound which is stored in self.strategy_dict.\n        \"\"\"\n        assert len(self.strategy) == self.cur_ind\n\n        action = float(action)\n        action = np.clip(action, 0, 1)\n\n        other_comp = 0\n        this_comp = 0\n        for i, idx in enumerate(self.prunable_idx):\n            flop = self.layer_info_dict[idx]['flops']\n            buffer_flop = self._get_buffer_flops(idx)\n\n            if i == self.cur_ind - 1:  # TODO: add other member in the set\n                this_comp += flop * self.strategy_dict[idx][0]\n                # add buffer (but not influenced by ratio)\n                other_comp += buffer_flop * self.strategy_dict[idx][0]\n            elif i == self.cur_ind:\n                this_comp += flop * self.strategy_dict[idx][1]\n                # also add buffer here (influenced by ratio)\n                this_comp += buffer_flop\n            else:\n                other_comp += flop * self.strategy_dict[idx][0] * self.strategy_dict[idx][1]\n                # add buffer\n                other_comp += buffer_flop * self.strategy_dict[idx][0]  # only consider input reduction\n\n        self.expected_min_preserve = other_comp + this_comp * action\n        max_preserve_ratio = (self.expected_preserve_computation - other_comp) * 1. / this_comp\n\n        action = np.minimum(action, max_preserve_ratio)\n        action = np.maximum(action, self.strategy_dict[self.prunable_idx[self.cur_ind]][0])  # impossible (should be)\n\n        return action",
  "def _get_buffer_flops(self, idx):\n        buffer_idx = self.buffer_dict[idx]\n        buffer_flop = sum([self.layer_info_dict[_]['flops'] for _ in buffer_idx])\n        return buffer_flop",
  "def _cur_flops(self):\n        flops = 0\n        for idx in self.prunable_idx:\n            c, n = self.strategy_dict[idx]  # input, output pruning ratio\n            flops += self.layer_info_dict[idx]['flops'] * c * n\n            # add buffer computation\n            flops += self._get_buffer_flops(idx) * c  # only related to input channel reduction\n        return flops",
  "def _cur_reduced(self):\n        # return the reduced weight\n        reduced = self.org_flops - self._cur_flops()\n        return reduced",
  "def _build_index(self):\n        \"\"\"\n        Build following information/data for later pruning:\n        self.prunable_idx: layer indices for pruable layers, the index values are the index\n            of list(self.model.modules()). Pruable layers are pointwise Conv2d layers and Linear\n            layers.\n        self.prunable_ops: prunable modules\n        self.buffer_idx: layer indices for buffer layers which refers the depthwise layers.\n            Each depthwise layer is always followd by a pointwise layer for both mobilenet and\n            mobilenetv2. The depthwise layer's filters are pruned when its next pointwise layer's\n            corresponding input channels are pruned.\n        self.shared_idx: layer indices for layers which share input.\n            For example: [[1,4], [8, 10, 15]] means layer 1 and 4 share same input, and layer\n            8, 10 and 15 share another input.\n        self.org_channels: number of input channels for each layer\n        self.min_strategy_dict: key is layer index, value is a tuple, the first value is the minimum\n            action of input channel, the second value is the minimum action value of output channel.\n        self.strategy_dict: same as self.min_strategy_dict, but it will be updated later.\n        \"\"\"\n        self.prunable_idx = []\n        self.prunable_ops = []\n        self.layer_type_dict = {}\n        self.strategy_dict = {}\n        self.buffer_dict = {}\n        this_buffer_list = []\n        self.org_channels = []\n        # build index and the min strategy dict\n        for i, m in enumerate(self.model.modules()):\n            if isinstance(m, PrunerModuleWrapper):\n                m = m.module\n                if type(m) == nn.Conv2d and m.groups == m.in_channels:  # depth-wise conv, buffer\n                    this_buffer_list.append(i)\n                else:  # really prunable\n                    self.prunable_idx.append(i)\n                    self.prunable_ops.append(m)\n                    self.layer_type_dict[i] = type(m)\n                    self.buffer_dict[i] = this_buffer_list\n                    this_buffer_list = []  # empty\n                    self.org_channels.append(m.in_channels if type(m) == nn.Conv2d else m.in_features)\n\n                    self.strategy_dict[i] = [self.lbound, self.lbound]\n\n        self.strategy_dict[self.prunable_idx[0]][0] = 1  # modify the input\n        self.strategy_dict[self.prunable_idx[-1]][1] = 1  # modify the output\n\n        self.shared_idx = []\n        if self.args.model_type == 'mobilenetv2':  # TODO: to be tested! Share index for residual connection\n            connected_idx = [4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]  # to be partitioned\n            last_ch = -1\n            share_group = None\n            for c_idx in connected_idx:\n                if self.prunable_ops[c_idx].in_channels != last_ch:  # new group\n                    last_ch = self.prunable_ops[c_idx].in_channels\n                    if share_group is not None:\n                        self.shared_idx.append(share_group)\n                    share_group = [c_idx]\n                else:  # same group\n                    share_group.append(c_idx)\n            self.shared_idx.append(share_group)\n            print('=> Conv layers to share channels: {}'.format(self.shared_idx))\n\n        self.min_strategy_dict = copy.deepcopy(self.strategy_dict)\n\n        self.buffer_idx = []\n        for _, v in self.buffer_dict.items():\n            self.buffer_idx += v\n\n        print('=> Prunable layer idx: {}'.format(self.prunable_idx))\n        print('=> Buffer layer idx: {}'.format(self.buffer_idx))\n        print('=> Shared idx: {}'.format(self.shared_idx))\n        print('=> Initial min strategy dict: {}'.format(self.min_strategy_dict))\n\n        # added for supporting residual connections during pruning\n        self.visited = [False] * len(self.prunable_idx)\n        self.index_buffer = {}",
  "def _extract_layer_information(self):\n        m_list = list(self.model.modules())\n\n        self.data_saver = []\n        self.layer_info_dict = dict()\n        self.wsize_list = []\n        self.flops_list = []\n\n        from .lib.utils import measure_layer_for_pruning\n\n        # extend the forward fn to record layer info\n        def new_forward(m):\n            def lambda_forward(x):\n                m.input_feat = x.clone()\n                #TODO replace this flops counter with nni.compression.torch.utils.counter.count_flops_params\n                measure_layer_for_pruning(m, x)\n                y = m.old_forward(x)\n                m.output_feat = y.clone()\n                return y\n\n            return lambda_forward\n\n        device = None\n        for idx in self.prunable_idx + self.buffer_idx:  # get all\n            m = m_list[idx]\n            m.old_forward = m.forward\n            m.forward = new_forward(m)\n            if device is None and type(m) == PrunerModuleWrapper:\n                device = m.module.weight.device\n\n        # now let the image flow\n        print('=> Extracting information...')\n        with torch.no_grad():\n            for i_b, (inputs, target) in enumerate(self._val_loader):  # use image from train set\n                if i_b == self.n_calibration_batches:\n                    break\n                self.data_saver.append((inputs.clone(), target.clone()))\n                input_var = torch.autograd.Variable(inputs).to(device)\n\n                # inference and collect stats\n                _ = self.model(input_var)\n\n                if i_b == 0:  # first batch\n                    for idx in self.prunable_idx + self.buffer_idx:\n                        self.layer_info_dict[idx] = dict()\n                        self.layer_info_dict[idx]['params'] = m_list[idx].params\n                        self.layer_info_dict[idx]['flops'] = m_list[idx].flops\n                        self.wsize_list.append(m_list[idx].params)\n                        self.flops_list.append(m_list[idx].flops)\n                    print('flops:', self.flops_list)\n                for idx in self.prunable_idx:\n                    f_in_np = m_list[idx].input_feat.data.cpu().numpy()\n                    f_out_np = m_list[idx].output_feat.data.cpu().numpy()\n                    if len(f_in_np.shape) == 4:  # conv\n                        if self.prunable_idx.index(idx) == 0:  # first conv\n                            f_in2save, f_out2save = None, None\n                        elif m_list[idx].module.weight.size(3) > 1:  # normal conv\n                            f_in2save, f_out2save = f_in_np, f_out_np\n                        else:  # 1x1 conv\n                            # assert f_out_np.shape[2] == f_in_np.shape[2]  # now support k=3\n                            randx = np.random.randint(0, f_out_np.shape[2] - 0, self.n_points_per_layer)\n                            randy = np.random.randint(0, f_out_np.shape[3] - 0, self.n_points_per_layer)\n                            # input: [N, C, H, W]\n                            self.layer_info_dict[idx][(i_b, 'randx')] = randx.copy()\n                            self.layer_info_dict[idx][(i_b, 'randy')] = randy.copy()\n\n                            f_in2save = f_in_np[:, :, randx, randy].copy().transpose(0, 2, 1)\\\n                                .reshape(self.batch_size * self.n_points_per_layer, -1)\n\n                            f_out2save = f_out_np[:, :, randx, randy].copy().transpose(0, 2, 1) \\\n                                .reshape(self.batch_size * self.n_points_per_layer, -1)\n                    else:\n                        assert len(f_in_np.shape) == 2\n                        f_in2save = f_in_np.copy()\n                        f_out2save = f_out_np.copy()\n                    if 'input_feat' not in self.layer_info_dict[idx]:\n                        self.layer_info_dict[idx]['input_feat'] = f_in2save\n                        self.layer_info_dict[idx]['output_feat'] = f_out2save\n                    else:\n                        self.layer_info_dict[idx]['input_feat'] = np.vstack(\n                            (self.layer_info_dict[idx]['input_feat'], f_in2save))\n                        self.layer_info_dict[idx]['output_feat'] = np.vstack(\n                            (self.layer_info_dict[idx]['output_feat'], f_out2save))",
  "def _build_state_embedding(self):\n        # build the static part of the state embedding\n        print('Building state embedding...')\n        layer_embedding = []\n        module_list = list(self.model.modules())\n        for i, ind in enumerate(self.prunable_idx):\n            m = module_list[ind].module\n            this_state = []\n            if type(m) == nn.Conv2d:\n                this_state.append(i)  # index\n                this_state.append(0)  # layer type, 0 for conv\n                this_state.append(m.in_channels)  # in channels\n                this_state.append(m.out_channels)  # out channels\n                this_state.append(m.stride[0])  # stride\n                this_state.append(m.kernel_size[0])  # kernel size\n                this_state.append(np.prod(m.weight.size()))  # weight size\n            elif type(m) == nn.Linear:\n                this_state.append(i)  # index\n                this_state.append(1)  # layer type, 1 for fc\n                this_state.append(m.in_features)  # in channels\n                this_state.append(m.out_features)  # out channels\n                this_state.append(0)  # stride\n                this_state.append(1)  # kernel size\n                this_state.append(np.prod(m.weight.size()))  # weight size\n\n            # this 3 features need to be changed later\n            this_state.append(0.)  # reduced\n            this_state.append(0.)  # rest\n            this_state.append(1.)  # a_{t-1}\n            layer_embedding.append(np.array(this_state))\n\n        # normalize the state\n        layer_embedding = np.array(layer_embedding, 'float')\n        print('=> shape of embedding (n_layer * n_dim): {}'.format(layer_embedding.shape))\n        assert len(layer_embedding.shape) == 2, layer_embedding.shape\n        for i in range(layer_embedding.shape[1]):\n            fmin = min(layer_embedding[:, i])\n            fmax = max(layer_embedding[:, i])\n            if fmax - fmin > 0:\n                layer_embedding[:, i] = (layer_embedding[:, i] - fmin) / (fmax - fmin)\n\n        self.layer_embedding = layer_embedding",
  "def new_forward(m):\n            def lambda_forward(x):\n                m.input_feat = x.clone()\n                #TODO replace this flops counter with nni.compression.torch.utils.counter.count_flops_params\n                measure_layer_for_pruning(m, x)\n                y = m.old_forward(x)\n                m.output_feat = y.clone()\n                return y\n\n            return lambda_forward",
  "def lambda_forward(x):\n                m.input_feat = x.clone()\n                #TODO replace this flops counter with nni.compression.torch.utils.counter.count_flops_params\n                measure_layer_for_pruning(m, x)\n                y = m.old_forward(x)\n                m.output_feat = y.clone()\n                return y",
  "class Actor(nn.Module):\n    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(nb_states, hidden1)\n        self.fc2 = nn.Linear(hidden1, hidden2)\n        self.fc3 = nn.Linear(hidden2, nb_actions)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        out = self.sigmoid(out)\n        return out",
  "class Critic(nn.Module):\n    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300):\n        super(Critic, self).__init__()\n        self.fc11 = nn.Linear(nb_states, hidden1)\n        self.fc12 = nn.Linear(nb_actions, hidden1)\n        self.fc2 = nn.Linear(hidden1, hidden2)\n        self.fc3 = nn.Linear(hidden2, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, xs):\n        x, a = xs\n        out = self.fc11(x) + self.fc12(a)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out",
  "class DDPG(object):\n    def __init__(self, nb_states, nb_actions, args):\n\n        self.nb_states = nb_states\n        self.nb_actions = nb_actions\n\n        # Create Actor and Critic Network\n        net_cfg = {\n            'hidden1': args.hidden1,\n            'hidden2': args.hidden2,\n            # 'init_w': args.init_w\n        }\n        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n        self.actor_optim = Adam(self.actor.parameters(), lr=args.lr_a)\n\n        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n        self.critic_optim = Adam(self.critic.parameters(), lr=args.lr_c)\n\n        self.hard_update(self.actor_target, self.actor)  # Make sure target is with the same weight\n        self.hard_update(self.critic_target, self.critic)\n\n        # Create replay buffer\n        self.memory = SequentialMemory(limit=args.rmsize, window_length=args.window_length)\n        # self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args.ou_theta, mu=args.ou_mu,\n        #                                                sigma=args.ou_sigma)\n\n        # Hyper-parameters\n        self.batch_size = args.bsize\n        self.tau = args.tau\n        self.discount = args.discount\n        self.depsilon = 1.0 / args.epsilon\n        self.lbound = 0.  # args.lbound\n        self.rbound = 1.  # args.rbound\n\n        # noise\n        self.init_delta = args.init_delta\n        self.delta_decay = args.delta_decay\n        self.warmup = args.warmup\n\n        #\n        self.epsilon = 1.0\n        # self.s_t = None  # Most recent state\n        # self.a_t = None  # Most recent action\n        self.is_training = True\n\n        #\n        if USE_CUDA: self.cuda()\n\n        # moving average baseline\n        self.moving_average = None\n        self.moving_alpha = 0.5  # based on batch, so small\n\n    def update_policy(self):\n        # Sample batch\n        state_batch, action_batch, reward_batch, \\\n        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n\n        # normalize the reward\n        batch_mean_reward = np.mean(reward_batch)\n        if self.moving_average is None:\n            self.moving_average = batch_mean_reward\n        else:\n            self.moving_average += self.moving_alpha * (batch_mean_reward - self.moving_average)\n        reward_batch -= self.moving_average\n        # if reward_batch.std() > 0:\n        #     reward_batch /= reward_batch.std()\n\n        # Prepare for the target q batch\n        with torch.no_grad():\n            next_q_values = self.critic_target([\n                to_tensor(next_state_batch),\n                self.actor_target(to_tensor(next_state_batch)),\n            ])\n\n        target_q_batch = to_tensor(reward_batch) + \\\n                         self.discount * to_tensor(terminal_batch.astype(np.float)) * next_q_values\n\n        # Critic update\n        self.critic.zero_grad()\n\n        q_batch = self.critic([to_tensor(state_batch), to_tensor(action_batch)])\n\n        value_loss = criterion(q_batch, target_q_batch)\n        value_loss.backward()\n        self.critic_optim.step()\n\n        # Actor update\n        self.actor.zero_grad()\n\n        policy_loss = -self.critic([ # pylint: disable=all\n            to_tensor(state_batch),\n            self.actor(to_tensor(state_batch))\n        ])\n\n        policy_loss = policy_loss.mean()\n        policy_loss.backward()\n        self.actor_optim.step()\n\n        # Target update\n        self.soft_update(self.actor_target, self.actor)\n        self.soft_update(self.critic_target, self.critic)\n\n    def eval(self):\n        self.actor.eval()\n        self.actor_target.eval()\n        self.critic.eval()\n        self.critic_target.eval()\n\n    def cuda(self):\n        self.actor.cuda()\n        self.actor_target.cuda()\n        self.critic.cuda()\n        self.critic_target.cuda()\n\n    def observe(self, r_t, s_t, s_t1, a_t, done):\n        if self.is_training:\n            self.memory.append(s_t, a_t, r_t, done)  # save to memory\n            # self.s_t = s_t1\n\n    def random_action(self):\n        action = np.random.uniform(self.lbound, self.rbound, self.nb_actions)\n        # self.a_t = action\n        return action\n\n    def select_action(self, s_t, episode):\n        # assert episode >= self.warmup, 'Episode: {} warmup: {}'.format(episode, self.warmup)\n        action = to_numpy(self.actor(to_tensor(np.array(s_t).reshape(1, -1)))).squeeze(0)\n        delta = self.init_delta * (self.delta_decay ** (episode - self.warmup))\n        # action += self.is_training * max(self.epsilon, 0) * self.random_process.sample()\n        action = self.sample_from_truncated_normal_distribution(lower=self.lbound, upper=self.rbound, mu=action, sigma=delta)\n        action = np.clip(action, self.lbound, self.rbound)\n\n        # self.a_t = action\n        return action\n\n    def reset(self, obs):\n        pass\n        # self.s_t = obs\n        # self.random_process.reset_states()\n\n    def load_weights(self, output):\n        if output is None: return\n\n        self.actor.load_state_dict(\n            torch.load('{}/actor.pkl'.format(output))\n        )\n\n        self.critic.load_state_dict(\n            torch.load('{}/critic.pkl'.format(output))\n        )\n\n    def save_model(self, output):\n        torch.save(\n            self.actor.state_dict(),\n            '{}/actor.pkl'.format(output)\n        )\n        torch.save(\n            self.critic.state_dict(),\n            '{}/critic.pkl'.format(output)\n        )\n\n    def soft_update(self, target, source):\n        for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(\n                target_param.data * (1.0 - self.tau) + param.data * self.tau\n            )\n\n    def hard_update(self, target, source):\n        for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(param.data)\n\n    def sample_from_truncated_normal_distribution(self, lower, upper, mu, sigma, size=1):\n        from scipy import stats\n        return stats.truncnorm.rvs((lower-mu)/sigma, (upper-mu)/sigma, loc=mu, scale=sigma, size=size)",
  "def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(nb_states, hidden1)\n        self.fc2 = nn.Linear(hidden1, hidden2)\n        self.fc3 = nn.Linear(hidden2, nb_actions)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()",
  "def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        out = self.sigmoid(out)\n        return out",
  "def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300):\n        super(Critic, self).__init__()\n        self.fc11 = nn.Linear(nb_states, hidden1)\n        self.fc12 = nn.Linear(nb_actions, hidden1)\n        self.fc2 = nn.Linear(hidden1, hidden2)\n        self.fc3 = nn.Linear(hidden2, 1)\n        self.relu = nn.ReLU()",
  "def forward(self, xs):\n        x, a = xs\n        out = self.fc11(x) + self.fc12(a)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out",
  "def __init__(self, nb_states, nb_actions, args):\n\n        self.nb_states = nb_states\n        self.nb_actions = nb_actions\n\n        # Create Actor and Critic Network\n        net_cfg = {\n            'hidden1': args.hidden1,\n            'hidden2': args.hidden2,\n            # 'init_w': args.init_w\n        }\n        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n        self.actor_optim = Adam(self.actor.parameters(), lr=args.lr_a)\n\n        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n        self.critic_optim = Adam(self.critic.parameters(), lr=args.lr_c)\n\n        self.hard_update(self.actor_target, self.actor)  # Make sure target is with the same weight\n        self.hard_update(self.critic_target, self.critic)\n\n        # Create replay buffer\n        self.memory = SequentialMemory(limit=args.rmsize, window_length=args.window_length)\n        # self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=args.ou_theta, mu=args.ou_mu,\n        #                                                sigma=args.ou_sigma)\n\n        # Hyper-parameters\n        self.batch_size = args.bsize\n        self.tau = args.tau\n        self.discount = args.discount\n        self.depsilon = 1.0 / args.epsilon\n        self.lbound = 0.  # args.lbound\n        self.rbound = 1.  # args.rbound\n\n        # noise\n        self.init_delta = args.init_delta\n        self.delta_decay = args.delta_decay\n        self.warmup = args.warmup\n\n        #\n        self.epsilon = 1.0\n        # self.s_t = None  # Most recent state\n        # self.a_t = None  # Most recent action\n        self.is_training = True\n\n        #\n        if USE_CUDA: self.cuda()\n\n        # moving average baseline\n        self.moving_average = None\n        self.moving_alpha = 0.5",
  "def update_policy(self):\n        # Sample batch\n        state_batch, action_batch, reward_batch, \\\n        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n\n        # normalize the reward\n        batch_mean_reward = np.mean(reward_batch)\n        if self.moving_average is None:\n            self.moving_average = batch_mean_reward\n        else:\n            self.moving_average += self.moving_alpha * (batch_mean_reward - self.moving_average)\n        reward_batch -= self.moving_average\n        # if reward_batch.std() > 0:\n        #     reward_batch /= reward_batch.std()\n\n        # Prepare for the target q batch\n        with torch.no_grad():\n            next_q_values = self.critic_target([\n                to_tensor(next_state_batch),\n                self.actor_target(to_tensor(next_state_batch)),\n            ])\n\n        target_q_batch = to_tensor(reward_batch) + \\\n                         self.discount * to_tensor(terminal_batch.astype(np.float)) * next_q_values\n\n        # Critic update\n        self.critic.zero_grad()\n\n        q_batch = self.critic([to_tensor(state_batch), to_tensor(action_batch)])\n\n        value_loss = criterion(q_batch, target_q_batch)\n        value_loss.backward()\n        self.critic_optim.step()\n\n        # Actor update\n        self.actor.zero_grad()\n\n        policy_loss = -self.critic([ # pylint: disable=all\n            to_tensor(state_batch),\n            self.actor(to_tensor(state_batch))\n        ])\n\n        policy_loss = policy_loss.mean()\n        policy_loss.backward()\n        self.actor_optim.step()\n\n        # Target update\n        self.soft_update(self.actor_target, self.actor)\n        self.soft_update(self.critic_target, self.critic)",
  "def eval(self):\n        self.actor.eval()\n        self.actor_target.eval()\n        self.critic.eval()\n        self.critic_target.eval()",
  "def cuda(self):\n        self.actor.cuda()\n        self.actor_target.cuda()\n        self.critic.cuda()\n        self.critic_target.cuda()",
  "def observe(self, r_t, s_t, s_t1, a_t, done):\n        if self.is_training:\n            self.memory.append(s_t, a_t, r_t, done)",
  "def random_action(self):\n        action = np.random.uniform(self.lbound, self.rbound, self.nb_actions)\n        # self.a_t = action\n        return action",
  "def select_action(self, s_t, episode):\n        # assert episode >= self.warmup, 'Episode: {} warmup: {}'.format(episode, self.warmup)\n        action = to_numpy(self.actor(to_tensor(np.array(s_t).reshape(1, -1)))).squeeze(0)\n        delta = self.init_delta * (self.delta_decay ** (episode - self.warmup))\n        # action += self.is_training * max(self.epsilon, 0) * self.random_process.sample()\n        action = self.sample_from_truncated_normal_distribution(lower=self.lbound, upper=self.rbound, mu=action, sigma=delta)\n        action = np.clip(action, self.lbound, self.rbound)\n\n        # self.a_t = action\n        return action",
  "def reset(self, obs):\n        pass",
  "def load_weights(self, output):\n        if output is None: return\n\n        self.actor.load_state_dict(\n            torch.load('{}/actor.pkl'.format(output))\n        )\n\n        self.critic.load_state_dict(\n            torch.load('{}/critic.pkl'.format(output))\n        )",
  "def save_model(self, output):\n        torch.save(\n            self.actor.state_dict(),\n            '{}/actor.pkl'.format(output)\n        )\n        torch.save(\n            self.critic.state_dict(),\n            '{}/critic.pkl'.format(output)\n        )",
  "def soft_update(self, target, source):\n        for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(\n                target_param.data * (1.0 - self.tau) + param.data * self.tau\n            )",
  "def hard_update(self, target, source):\n        for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(param.data)",
  "def sample_from_truncated_normal_distribution(self, lower, upper, mu, sigma, size=1):\n        from scipy import stats\n        return stats.truncnorm.rvs((lower-mu)/sigma, (upper-mu)/sigma, loc=mu, scale=sigma, size=size)",
  "class TextLogger(object):\n    \"\"\"Write log immediately to the disk\"\"\"\n    def __init__(self, filepath):\n        self.f = open(filepath, 'w')\n        self.fid = self.f.fileno()\n        self.filepath = filepath\n\n    def close(self):\n        self.f.close()\n\n    def write(self, content):\n        self.f.write(content)\n        self.f.flush()\n        os.fsync(self.fid)\n\n    def write_buf(self, content):\n        self.f.write(content)\n\n    def print_and_write(self, content):\n        print(content)\n        self.write(content+'\\n')",
  "def to_numpy(var):\n    use_cuda = torch.cuda.is_available()\n    return var.cpu().data.numpy() if use_cuda else var.data.numpy()",
  "def to_tensor(ndarray, requires_grad=False):  # return a float tensor by default\n    tensor = torch.from_numpy(ndarray).float()  # by default does not require grad\n    if requires_grad:\n        tensor.requires_grad_()\n    return tensor.cuda() if torch.cuda.is_available() else tensor",
  "def measure_layer_for_pruning(wrapper, x):\n    def get_layer_type(layer):\n        layer_str = str(layer)\n        return layer_str[:layer_str.find('(')].strip()\n\n    def get_layer_param(model):\n        import operator\n        import functools\n\n        return sum([functools.reduce(operator.mul, i.size(), 1) for i in model.parameters()])\n\n    multi_add = 1\n    layer = wrapper.module\n    type_name = get_layer_type(layer)\n\n    # ops_conv\n    if type_name in ['Conv2d']:\n        out_h = int((x.size()[2] + 2 * layer.padding[0] - layer.kernel_size[0]) /\n                    layer.stride[0] + 1)\n        out_w = int((x.size()[3] + 2 * layer.padding[1] - layer.kernel_size[1]) /\n                    layer.stride[1] + 1)\n        wrapper.flops = layer.in_channels * layer.out_channels * layer.kernel_size[0] *  \\\n                    layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n        wrapper.params = get_layer_param(layer)\n    # ops_linear\n    elif type_name in ['Linear']:\n        weight_ops = layer.weight.numel() * multi_add\n        bias_ops = layer.bias.numel()\n        wrapper.flops = weight_ops + bias_ops\n        wrapper.params = get_layer_param(layer)\n    return",
  "def least_square_sklearn(X, Y):\n    from sklearn.linear_model import LinearRegression\n    reg = LinearRegression(fit_intercept=False)\n    reg.fit(X, Y)\n    return reg.coef_",
  "def get_output_folder(parent_dir, env_name):\n    \"\"\"Return save folder.\n    Assumes folders in the parent_dir have suffix -run{run\n    number}. Finds the highest run number and sets the output folder\n    to that number + 1. This is just convenient so that if you run the\n    same script multiple times tensorboard can plot all of the results\n    on the same plots with different names.\n    Parameters\n    ----------\n    parent_dir: str\n      Path of the directory containing all experiment runs.\n    Returns\n    -------\n    parent_dir/run_dir\n      Path to this run's save directory.\n    \"\"\"\n    os.makedirs(parent_dir, exist_ok=True)\n    experiment_id = 0\n    for folder_name in os.listdir(parent_dir):\n        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n            continue\n        try:\n            folder_name = int(folder_name.split('-run')[-1])\n            if folder_name > experiment_id:\n                experiment_id = folder_name\n        except:\n            pass\n    experiment_id += 1\n\n    parent_dir = os.path.join(parent_dir, env_name)\n    parent_dir = parent_dir + '-run{}'.format(experiment_id)\n    os.makedirs(parent_dir, exist_ok=True)\n    return parent_dir",
  "def prRed(prt): print(\"\\033[91m {}\\033[00m\" .format(prt))",
  "def prGreen(prt): print(\"\\033[92m {}\\033[00m\" .format(prt))",
  "def prYellow(prt): print(\"\\033[93m {}\\033[00m\" .format(prt))",
  "def prLightPurple(prt): print(\"\\033[94m {}\\033[00m\" .format(prt))",
  "def prPurple(prt): print(\"\\033[95m {}\\033[00m\" .format(prt))",
  "def prCyan(prt): print(\"\\033[96m {}\\033[00m\" .format(prt))",
  "def prLightGray(prt): print(\"\\033[97m {}\\033[00m\" .format(prt))",
  "def prBlack(prt): print(\"\\033[98m {}\\033[00m\" .format(prt))",
  "def __init__(self, filepath):\n        self.f = open(filepath, 'w')\n        self.fid = self.f.fileno()\n        self.filepath = filepath",
  "def close(self):\n        self.f.close()",
  "def write(self, content):\n        self.f.write(content)\n        self.f.flush()\n        os.fsync(self.fid)",
  "def write_buf(self, content):\n        self.f.write(content)",
  "def print_and_write(self, content):\n        print(content)\n        self.write(content+'\\n')",
  "def get_layer_type(layer):\n        layer_str = str(layer)\n        return layer_str[:layer_str.find('(')].strip()",
  "def get_layer_param(model):\n        import operator\n        import functools\n\n        return sum([functools.reduce(operator.mul, i.size(), 1) for i in model.parameters()])",
  "def sample_batch_indexes(low, high, size):\n    if high - low >= size:\n        # We have enough data. Draw without replacement, that is each index is unique in the\n        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n        r = range(low, high)\n        batch_idxs = random.sample(r, size)\n    else:\n        # Not enough data. Help ourselves with sampling from the range, but the same index\n        # can occur multiple times. This is not good and should be avoided by picking a\n        # large enough warm-up phase.\n        warnings.warn(\n            'Not enough entries to sample without replacement. '\n            'Consider increasing your warm-up phase to avoid oversampling!')\n        batch_idxs = np.random.random_integers(low, high - 1, size=size)\n    assert len(batch_idxs) == size\n    return batch_idxs",
  "class RingBuffer(object):\n    def __init__(self, maxlen):\n        self.maxlen = maxlen\n        self.start = 0\n        self.length = 0\n        self.data = [None for _ in range(maxlen)]\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        if idx < 0 or idx >= self.length:\n            raise KeyError()\n        return self.data[(self.start + idx) % self.maxlen]\n\n    def append(self, v):\n        if self.length < self.maxlen:\n            # We have space, simply increase the length.\n            self.length += 1\n        elif self.length == self.maxlen:\n            # No space, \"remove\" the first item.\n            self.start = (self.start + 1) % self.maxlen\n        else:\n            # This should never happen.\n            raise RuntimeError()\n        self.data[(self.start + self.length - 1) % self.maxlen] = v",
  "def zeroed_observation(observation):\n    if hasattr(observation, 'shape'):\n        return np.zeros(observation.shape)\n    elif hasattr(observation, '__iter__'):\n        out = []\n        for x in observation:\n            out.append(zeroed_observation(x))\n        return out\n    else:\n        return 0.",
  "class Memory(object):\n    def __init__(self, window_length, ignore_episode_boundaries=False):\n        self.window_length = window_length\n        self.ignore_episode_boundaries = ignore_episode_boundaries\n\n        self.recent_observations = deque(maxlen=window_length)\n        self.recent_terminals = deque(maxlen=window_length)\n\n    def sample(self, batch_size, batch_idxs=None):\n        raise NotImplementedError()\n\n    def append(self, observation, action, reward, terminal, training=True):\n        self.recent_observations.append(observation)\n        self.recent_terminals.append(terminal)\n\n    def get_recent_state(self, current_observation):\n        # This code is slightly complicated by the fact that subsequent observations might be\n        # from different episodes. We ensure that an experience never spans multiple episodes.\n        # This is probably not that important in practice but it seems cleaner.\n        state = [current_observation]\n        idx = len(self.recent_observations) - 1\n        for offset in range(0, self.window_length - 1):\n            current_idx = idx - offset\n            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                # The previously handled observation was terminal, don't add the current one.\n                # Otherwise we would leak into a different episode.\n                break\n            state.insert(0, self.recent_observations[current_idx])\n        while len(state) < self.window_length:\n            state.insert(0, zeroed_observation(state[0]))\n        return state\n\n    def get_config(self):\n        config = {\n            'window_length': self.window_length,\n            'ignore_episode_boundaries': self.ignore_episode_boundaries,\n        }\n        return config",
  "class SequentialMemory(Memory):\n    def __init__(self, limit, **kwargs):\n        super(SequentialMemory, self).__init__(**kwargs)\n\n        self.limit = limit\n\n        # Do not use deque to implement the memory. This data structure may seem convenient but\n        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n        self.actions = RingBuffer(limit)\n        self.rewards = RingBuffer(limit)\n        self.terminals = RingBuffer(limit)\n        self.observations = RingBuffer(limit)\n\n    def sample(self, batch_size, batch_idxs=None):\n        if batch_idxs is None:\n            # Draw random indexes such that we have at least a single entry before each\n            # index.\n            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)\n        batch_idxs = np.array(batch_idxs) + 1\n        assert np.min(batch_idxs) >= 1\n        assert np.max(batch_idxs) < self.nb_entries\n        assert len(batch_idxs) == batch_size\n\n        # Create experiences\n        experiences = []\n        for idx in batch_idxs:\n            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n            while terminal0:\n                # Skip this transition because the environment was reset here. Select a new, random\n                # transition and use this instead. This may cause the batch to contain the same\n                # transition twice.\n                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n            assert 1 <= idx < self.nb_entries\n\n            # This code is slightly complicated by the fact that subsequent observations might be\n            # from different episodes. We ensure that an experience never spans multiple episodes.\n            # This is probably not that important in practice but it seems cleaner.\n            state0 = [self.observations[idx - 1]]\n            for offset in range(0, self.window_length - 1):\n                current_idx = idx - 2 - offset\n                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                    # The previously handled observation was terminal, don't add the current one.\n                    # Otherwise we would leak into a different episode.\n                    break\n                state0.insert(0, self.observations[current_idx])\n            while len(state0) < self.window_length:\n                state0.insert(0, zeroed_observation(state0[0]))\n            action = self.actions[idx - 1]\n            reward = self.rewards[idx - 1]\n            terminal1 = self.terminals[idx - 1]\n\n            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n            # to the right. Again, we need to be careful to not include an observation from the next\n            # episode if the last state is terminal.\n            state1 = [np.copy(x) for x in state0[1:]]\n            state1.append(self.observations[idx])\n\n            assert len(state0) == self.window_length\n            assert len(state1) == len(state0)\n            experiences.append(Experience(state0=state0, action=action, reward=reward,\n                                          state1=state1, terminal1=terminal1))\n        assert len(experiences) == batch_size\n        return experiences\n\n    def sample_and_split(self, batch_size, batch_idxs=None):\n        experiences = self.sample(batch_size, batch_idxs)\n\n        state0_batch = []\n        reward_batch = []\n        action_batch = []\n        terminal1_batch = []\n        state1_batch = []\n        for e in experiences:\n            state0_batch.append(e.state0)\n            state1_batch.append(e.state1)\n            reward_batch.append(e.reward)\n            action_batch.append(e.action)\n            terminal1_batch.append(0. if e.terminal1 else 1.)\n\n        # Prepare and validate parameters.\n        state0_batch = np.array(state0_batch, 'double').reshape(batch_size, -1)\n        state1_batch = np.array(state1_batch, 'double').reshape(batch_size, -1)\n        terminal1_batch = np.array(terminal1_batch, 'double').reshape(batch_size, -1)\n        reward_batch = np.array(reward_batch, 'double').reshape(batch_size, -1)\n        action_batch = np.array(action_batch, 'double').reshape(batch_size, -1)\n\n        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch\n\n    def append(self, observation, action, reward, terminal, training=True):\n        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n\n        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n        # and weather the next state is `terminal` or not.\n        if training:\n            self.observations.append(observation)\n            self.actions.append(action)\n            self.rewards.append(reward)\n            self.terminals.append(terminal)\n\n    @property\n    def nb_entries(self):\n        return len(self.observations)\n\n    def get_config(self):\n        config = super(SequentialMemory, self).get_config()\n        config['limit'] = self.limit\n        return config",
  "def __init__(self, maxlen):\n        self.maxlen = maxlen\n        self.start = 0\n        self.length = 0\n        self.data = [None for _ in range(maxlen)]",
  "def __len__(self):\n        return self.length",
  "def __getitem__(self, idx):\n        if idx < 0 or idx >= self.length:\n            raise KeyError()\n        return self.data[(self.start + idx) % self.maxlen]",
  "def append(self, v):\n        if self.length < self.maxlen:\n            # We have space, simply increase the length.\n            self.length += 1\n        elif self.length == self.maxlen:\n            # No space, \"remove\" the first item.\n            self.start = (self.start + 1) % self.maxlen\n        else:\n            # This should never happen.\n            raise RuntimeError()\n        self.data[(self.start + self.length - 1) % self.maxlen] = v",
  "def __init__(self, window_length, ignore_episode_boundaries=False):\n        self.window_length = window_length\n        self.ignore_episode_boundaries = ignore_episode_boundaries\n\n        self.recent_observations = deque(maxlen=window_length)\n        self.recent_terminals = deque(maxlen=window_length)",
  "def sample(self, batch_size, batch_idxs=None):\n        raise NotImplementedError()",
  "def append(self, observation, action, reward, terminal, training=True):\n        self.recent_observations.append(observation)\n        self.recent_terminals.append(terminal)",
  "def get_recent_state(self, current_observation):\n        # This code is slightly complicated by the fact that subsequent observations might be\n        # from different episodes. We ensure that an experience never spans multiple episodes.\n        # This is probably not that important in practice but it seems cleaner.\n        state = [current_observation]\n        idx = len(self.recent_observations) - 1\n        for offset in range(0, self.window_length - 1):\n            current_idx = idx - offset\n            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                # The previously handled observation was terminal, don't add the current one.\n                # Otherwise we would leak into a different episode.\n                break\n            state.insert(0, self.recent_observations[current_idx])\n        while len(state) < self.window_length:\n            state.insert(0, zeroed_observation(state[0]))\n        return state",
  "def get_config(self):\n        config = {\n            'window_length': self.window_length,\n            'ignore_episode_boundaries': self.ignore_episode_boundaries,\n        }\n        return config",
  "def __init__(self, limit, **kwargs):\n        super(SequentialMemory, self).__init__(**kwargs)\n\n        self.limit = limit\n\n        # Do not use deque to implement the memory. This data structure may seem convenient but\n        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n        self.actions = RingBuffer(limit)\n        self.rewards = RingBuffer(limit)\n        self.terminals = RingBuffer(limit)\n        self.observations = RingBuffer(limit)",
  "def sample(self, batch_size, batch_idxs=None):\n        if batch_idxs is None:\n            # Draw random indexes such that we have at least a single entry before each\n            # index.\n            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)\n        batch_idxs = np.array(batch_idxs) + 1\n        assert np.min(batch_idxs) >= 1\n        assert np.max(batch_idxs) < self.nb_entries\n        assert len(batch_idxs) == batch_size\n\n        # Create experiences\n        experiences = []\n        for idx in batch_idxs:\n            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n            while terminal0:\n                # Skip this transition because the environment was reset here. Select a new, random\n                # transition and use this instead. This may cause the batch to contain the same\n                # transition twice.\n                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n            assert 1 <= idx < self.nb_entries\n\n            # This code is slightly complicated by the fact that subsequent observations might be\n            # from different episodes. We ensure that an experience never spans multiple episodes.\n            # This is probably not that important in practice but it seems cleaner.\n            state0 = [self.observations[idx - 1]]\n            for offset in range(0, self.window_length - 1):\n                current_idx = idx - 2 - offset\n                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                    # The previously handled observation was terminal, don't add the current one.\n                    # Otherwise we would leak into a different episode.\n                    break\n                state0.insert(0, self.observations[current_idx])\n            while len(state0) < self.window_length:\n                state0.insert(0, zeroed_observation(state0[0]))\n            action = self.actions[idx - 1]\n            reward = self.rewards[idx - 1]\n            terminal1 = self.terminals[idx - 1]\n\n            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n            # to the right. Again, we need to be careful to not include an observation from the next\n            # episode if the last state is terminal.\n            state1 = [np.copy(x) for x in state0[1:]]\n            state1.append(self.observations[idx])\n\n            assert len(state0) == self.window_length\n            assert len(state1) == len(state0)\n            experiences.append(Experience(state0=state0, action=action, reward=reward,\n                                          state1=state1, terminal1=terminal1))\n        assert len(experiences) == batch_size\n        return experiences",
  "def sample_and_split(self, batch_size, batch_idxs=None):\n        experiences = self.sample(batch_size, batch_idxs)\n\n        state0_batch = []\n        reward_batch = []\n        action_batch = []\n        terminal1_batch = []\n        state1_batch = []\n        for e in experiences:\n            state0_batch.append(e.state0)\n            state1_batch.append(e.state1)\n            reward_batch.append(e.reward)\n            action_batch.append(e.action)\n            terminal1_batch.append(0. if e.terminal1 else 1.)\n\n        # Prepare and validate parameters.\n        state0_batch = np.array(state0_batch, 'double').reshape(batch_size, -1)\n        state1_batch = np.array(state1_batch, 'double').reshape(batch_size, -1)\n        terminal1_batch = np.array(terminal1_batch, 'double').reshape(batch_size, -1)\n        reward_batch = np.array(reward_batch, 'double').reshape(batch_size, -1)\n        action_batch = np.array(action_batch, 'double').reshape(batch_size, -1)\n\n        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch",
  "def append(self, observation, action, reward, terminal, training=True):\n        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n\n        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n        # and weather the next state is `terminal` or not.\n        if training:\n            self.observations.append(observation)\n            self.actions.append(action)\n            self.rewards.append(reward)\n            self.terminals.append(terminal)",
  "def nb_entries(self):\n        return len(self.observations)",
  "def get_config(self):\n        config = super(SequentialMemory, self).get_config()\n        config['limit'] = self.limit\n        return config",
  "def get_num_gen(gen):\n    return sum(1 for _ in gen)",
  "def is_leaf(model):\n    return get_num_gen(model.children()) == 0",
  "def get_layer_info(layer):\n    layer_str = str(layer)\n    type_name = layer_str[:layer_str.find('(')].strip()\n    return type_name",
  "def get_layer_param(model):\n    import operator\n    import functools\n\n    return sum([functools.reduce(operator.mul, i.size(), 1) for i in model.parameters()])",
  "def measure_layer(layer, x):\n    global count_ops, count_params\n    delta_ops = 0\n    delta_params = 0\n    multi_add = 1\n    type_name = get_layer_info(layer)\n\n    # ops_conv\n    if type_name in ['Conv2d']:\n        out_h = int((x.size()[2] + 2 * layer.padding[0] - layer.kernel_size[0]) /\n                    layer.stride[0] + 1)\n        out_w = int((x.size()[3] + 2 * layer.padding[1] - layer.kernel_size[1]) /\n                    layer.stride[1] + 1)\n        delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] *  \\\n                layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n        delta_params = get_layer_param(layer)\n\n    # ops_nonlinearity\n    elif type_name in ['ReLU']:\n        delta_ops = x.numel() / x.size(0)\n        delta_params = get_layer_param(layer)\n\n    # ops_pooling\n    elif type_name in ['AvgPool2d']:\n        in_w = x.size()[2]\n        kernel_ops = layer.kernel_size * layer.kernel_size\n        out_w = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n        out_h = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n        delta_ops = x.size()[1] * out_w * out_h * kernel_ops\n        delta_params = get_layer_param(layer)\n\n    elif type_name in ['AdaptiveAvgPool2d']:\n        delta_ops = x.size()[1] * x.size()[2] * x.size()[3]\n        delta_params = get_layer_param(layer)\n\n    # ops_linear\n    elif type_name in ['Linear']:\n        weight_ops = layer.weight.numel() * multi_add\n        bias_ops = layer.bias.numel()\n        delta_ops = weight_ops + bias_ops\n        delta_params = get_layer_param(layer)\n\n    # ops_nothing\n    elif type_name in ['BatchNorm2d', 'Dropout2d', 'DropChannel', 'Dropout']:\n        delta_params = get_layer_param(layer)\n\n    # unknown layer type\n    else:\n        delta_params = get_layer_param(layer)\n\n    count_ops += delta_ops\n    count_params += delta_params\n\n    return",
  "def measure_model(model, H, W):\n    global count_ops, count_params\n    count_ops = 0\n    count_params = 0\n    data = torch.zeros(2, 3, H, W).cuda()\n\n    def should_measure(x):\n        return is_leaf(x)\n\n    def modify_forward(model):\n        for child in model.children():\n            if should_measure(child):\n                def new_forward(m):\n                    def lambda_forward(x):\n                        measure_layer(m, x)\n                        return m.old_forward(x)\n                    return lambda_forward\n                child.old_forward = child.forward\n                child.forward = new_forward(child)\n            else:\n                modify_forward(child)\n\n    def restore_forward(model):\n        for child in model.children():\n            # leaf node\n            if is_leaf(child) and hasattr(child, 'old_forward'):\n                child.forward = child.old_forward\n                child.old_forward = None\n            else:\n                restore_forward(child)\n\n    modify_forward(model)\n    model.forward(data)\n    restore_forward(model)\n\n    return count_ops, count_params",
  "def should_measure(x):\n        return is_leaf(x)",
  "def modify_forward(model):\n        for child in model.children():\n            if should_measure(child):\n                def new_forward(m):\n                    def lambda_forward(x):\n                        measure_layer(m, x)\n                        return m.old_forward(x)\n                    return lambda_forward\n                child.old_forward = child.forward\n                child.forward = new_forward(child)\n            else:\n                modify_forward(child)",
  "def restore_forward(model):\n        for child in model.children():\n            # leaf node\n            if is_leaf(child) and hasattr(child, 'old_forward'):\n                child.forward = child.old_forward\n                child.old_forward = None\n            else:\n                restore_forward(child)",
  "def new_forward(m):\n                    def lambda_forward(x):\n                        measure_layer(m, x)\n                        return m.old_forward(x)\n                    return lambda_forward",
  "def lambda_forward(x):\n                        measure_layer(m, x)\n                        return m.old_forward(x)",
  "class SensitivityAnalysis:\n    def __init__(self, model, val_func, sparsities=None, prune_type='l1', early_stop_mode=None, early_stop_value=None):\n        \"\"\"\n        Perform sensitivity analysis for this model.\n        Parameters\n        ----------\n        model : torch.nn.Module\n            the model to perform sensitivity analysis\n        val_func : function\n            validation function for the model. Due to\n            different models may need different dataset/criterion\n            , therefore the user need to cover this part by themselves.\n            In the val_func, the model should be tested on the validation dateset,\n            and the validation accuracy/loss should be returned as the output of val_func.\n            There are no restrictions on the input parameters of the val_function.\n            User can use the val_args, val_kwargs parameters in analysis\n            to pass all the parameters that val_func needed.\n        sparsities : list\n            The sparsity list provided by users. This parameter is set when the user\n            only wants to test some specific sparsities. In the sparsity list, each element\n            is a sparsity value which means how much weight the pruner should prune. Take\n            [0.25, 0.5, 0.75] for an example, the SensitivityAnalysis will prune 25% 50% 75%\n            weights gradually for each layer.\n        prune_type : str\n            The pruner type used to prune the conv layers, default is 'l1',\n            and 'l2', 'fine-grained' is also supported.\n        early_stop_mode : str\n            If this flag is set, the sensitivity analysis\n            for a conv layer will early stop when the validation metric(\n            for example, accurracy/loss) has alreay meet the threshold. We\n            support four different early stop modes: minimize, maximize, dropped,\n            raised. The default value is None, which means the analysis won't stop\n            until all given sparsities are tested. This option should be used with\n            early_stop_value together.\n\n            minimize: The analysis stops when the validation metric return by the val_func\n            lower than early_stop_value.\n            maximize: The analysis stops when the validation metric return by the val_func\n            larger than early_stop_value.\n            dropped: The analysis stops when the validation metric has dropped by early_stop_value.\n            raised: The analysis stops when the validation metric has raised by early_stop_value.\n        early_stop_value : float\n            This value is used as the threshold for different earlystop modes.\n            This value is effective only when the early_stop_mode is set.\n\n        \"\"\"\n        self.model = model\n        self.val_func = val_func\n        self.target_layer = OrderedDict()\n        self.ori_state_dict = copy.deepcopy(self.model.state_dict())\n        self.target_layer = {}\n        self.sensitivities = {}\n        if sparsities is not None:\n            self.sparsities = sorted(sparsities)\n        else:\n            self.sparsities = np.arange(0.1, 1.0, 0.1)\n        self.sparsities = [np.round(x, 2) for x in self.sparsities]\n        self.Pruner = PRUNER_DICT[prune_type]\n        self.early_stop_mode = early_stop_mode\n        self.early_stop_value = early_stop_value\n        self.ori_metric = None  # original validation metric for the model\n        # already_pruned is for the iterative sensitivity analysis\n        # For example, sensitivity_pruner iteratively prune the target\n        # model according to the sensitivity. After each round of\n        # pruning, the sensitivity_pruner will test the new sensitivity\n        # for each layer\n        self.already_pruned = {}\n        self.model_parse()\n\n    @property\n    def layers_count(self):\n        return len(self.target_layer)\n\n    def model_parse(self):\n        for name, submodel in self.model.named_modules():\n            for op_type in SUPPORTED_OP_TYPE:\n                if isinstance(submodel, op_type):\n                    self.target_layer[name] = submodel\n                    self.already_pruned[name] = 0\n\n    def _need_to_stop(self, ori_metric, cur_metric):\n        \"\"\"\n        Judge if meet the stop conditon(early_stop, min_threshold,\n        max_threshold).\n        Parameters\n        ----------\n        ori_metric : float\n            original validation metric\n        cur_metric : float\n            current validation metric\n\n        Returns\n        -------\n        stop : bool\n            if stop the sensitivity analysis\n        \"\"\"\n        if self.early_stop_mode is None:\n            # early stop mode is not enable\n            return False\n        assert self.early_stop_value is not None\n        if self.early_stop_mode == 'minimize':\n            if cur_metric < self.early_stop_value:\n                return True\n        elif self.early_stop_mode == 'maximize':\n            if cur_metric > self.early_stop_value:\n                return True\n        elif self.early_stop_mode == 'dropped':\n            if cur_metric < ori_metric - self.early_stop_value:\n                return True\n        elif self.early_stop_mode == 'raised':\n            if cur_metric > ori_metric + self.early_stop_value:\n                return True\n        return False\n\n    def analysis(self, val_args=None, val_kwargs=None, specified_layers=None):\n        \"\"\"\n        This function analyze the sensitivity to pruning for\n        each conv layer in the target model.\n        If start and end are not set, we analyze all the conv\n        layers by default. Users can specify several layers to\n        analyze or parallelize the analysis process easily through\n        the start and end parameter.\n\n        Parameters\n        ----------\n        val_args : list\n            args for the val_function\n        val_kwargs : dict\n            kwargs for the val_funtion\n        specified_layers : list\n            list of layer names to analyze sensitivity.\n            If this variable is set, then only analyze\n            the conv layers that specified in the list.\n            User can also use this option to parallelize\n            the sensitivity analysis easily.\n        Returns\n        -------\n        sensitivities : dict\n            dict object that stores the trajectory of the\n            accuracy/loss when the prune ratio changes\n        \"\"\"\n        if val_args is None:\n            val_args = []\n        if val_kwargs is None:\n            val_kwargs = {}\n        # Get the original validation metric(accuracy/loss) before pruning\n        # Get the accuracy baseline before starting the analysis.\n        self.ori_metric = self.val_func(*val_args, **val_kwargs)\n        namelist = list(self.target_layer.keys())\n        if specified_layers is not None:\n            # only analyze several specified conv layers\n            namelist = list(filter(lambda x: x in specified_layers, namelist))\n        for name in namelist:\n            self.sensitivities[name] = {}\n            for sparsity in self.sparsities:\n                # here the sparsity is the relative sparsity of the\n                # the remained weights\n                # Calculate the actual prune ratio based on the already pruned ratio\n                real_sparsity = (\n                    1.0 - self.already_pruned[name]) * sparsity + self.already_pruned[name]\n                # TODO In current L1/L2 Filter Pruner, the 'op_types' is still necessary\n                # I think the L1/L2 Pruner should specify the op_types automaticlly\n                # according to the op_names\n                cfg = [{'sparsity': real_sparsity, 'op_names': [\n                    name], 'op_types': ['Conv2d']}]\n                pruner = self.Pruner(self.model, cfg)\n                pruner.compress()\n                val_metric = self.val_func(*val_args, **val_kwargs)\n                logger.info('Layer: %s Sparsity: %.2f Validation Metric: %.4f',\n                            name, real_sparsity, val_metric)\n\n                self.sensitivities[name][sparsity] = val_metric\n                pruner._unwrap_model()\n                del pruner\n                # check if the current metric meet the stop condition\n                if self._need_to_stop(self.ori_metric, val_metric):\n                    break\n\n            # reset the weights pruned by the pruner, because the\n            # input sparsities is sorted, so we donnot need to reset\n            # weight of the layer when the sparsity changes, instead,\n            # we only need reset the weight when the pruning layer changes.\n            self.model.load_state_dict(self.ori_state_dict)\n\n        return self.sensitivities\n\n    def export(self, filepath):\n        \"\"\"\n        Export the results of the sensitivity analysis\n        to a csv file. The firstline of the csv file describe the content\n        structure. The first line is constructed by 'layername' and sparsity\n        list. Each line below records the validation metric returned by val_func\n        when this layer is under different sparsities. Note that, due to the early_stop\n        option, some layers may not have the metrics under all sparsities.\n\n        layername, 0.25, 0.5, 0.75\n        conv1, 0.6, 0.55\n        conv2, 0.61, 0.57, 0.56\n\n        Parameters\n        ----------\n        filepath : str\n            Path of the output file\n        \"\"\"\n        str_sparsities = [str(x) for x in self.sparsities]\n        header = ['layername'] + str_sparsities\n        with open(filepath, 'w') as csvf:\n            csv_w = csv.writer(csvf)\n            csv_w.writerow(header)\n            for layername in self.sensitivities:\n                row = []\n                row.append(layername)\n                for sparsity in sorted(self.sensitivities[layername].keys()):\n                    row.append(self.sensitivities[layername][sparsity])\n                csv_w.writerow(row)\n\n    def update_already_pruned(self, layername, ratio):\n        \"\"\"\n        Set the already pruned ratio for the target layer.\n        \"\"\"\n        self.already_pruned[layername] = ratio\n\n    def load_state_dict(self, state_dict):\n        \"\"\"\n        Update the weight of the model\n        \"\"\"\n        self.ori_state_dict = copy.deepcopy(state_dict)\n        self.model.load_state_dict(self.ori_state_dict)",
  "def __init__(self, model, val_func, sparsities=None, prune_type='l1', early_stop_mode=None, early_stop_value=None):\n        \"\"\"\n        Perform sensitivity analysis for this model.\n        Parameters\n        ----------\n        model : torch.nn.Module\n            the model to perform sensitivity analysis\n        val_func : function\n            validation function for the model. Due to\n            different models may need different dataset/criterion\n            , therefore the user need to cover this part by themselves.\n            In the val_func, the model should be tested on the validation dateset,\n            and the validation accuracy/loss should be returned as the output of val_func.\n            There are no restrictions on the input parameters of the val_function.\n            User can use the val_args, val_kwargs parameters in analysis\n            to pass all the parameters that val_func needed.\n        sparsities : list\n            The sparsity list provided by users. This parameter is set when the user\n            only wants to test some specific sparsities. In the sparsity list, each element\n            is a sparsity value which means how much weight the pruner should prune. Take\n            [0.25, 0.5, 0.75] for an example, the SensitivityAnalysis will prune 25% 50% 75%\n            weights gradually for each layer.\n        prune_type : str\n            The pruner type used to prune the conv layers, default is 'l1',\n            and 'l2', 'fine-grained' is also supported.\n        early_stop_mode : str\n            If this flag is set, the sensitivity analysis\n            for a conv layer will early stop when the validation metric(\n            for example, accurracy/loss) has alreay meet the threshold. We\n            support four different early stop modes: minimize, maximize, dropped,\n            raised. The default value is None, which means the analysis won't stop\n            until all given sparsities are tested. This option should be used with\n            early_stop_value together.\n\n            minimize: The analysis stops when the validation metric return by the val_func\n            lower than early_stop_value.\n            maximize: The analysis stops when the validation metric return by the val_func\n            larger than early_stop_value.\n            dropped: The analysis stops when the validation metric has dropped by early_stop_value.\n            raised: The analysis stops when the validation metric has raised by early_stop_value.\n        early_stop_value : float\n            This value is used as the threshold for different earlystop modes.\n            This value is effective only when the early_stop_mode is set.\n\n        \"\"\"\n        self.model = model\n        self.val_func = val_func\n        self.target_layer = OrderedDict()\n        self.ori_state_dict = copy.deepcopy(self.model.state_dict())\n        self.target_layer = {}\n        self.sensitivities = {}\n        if sparsities is not None:\n            self.sparsities = sorted(sparsities)\n        else:\n            self.sparsities = np.arange(0.1, 1.0, 0.1)\n        self.sparsities = [np.round(x, 2) for x in self.sparsities]\n        self.Pruner = PRUNER_DICT[prune_type]\n        self.early_stop_mode = early_stop_mode\n        self.early_stop_value = early_stop_value\n        self.ori_metric = None  # original validation metric for the model\n        # already_pruned is for the iterative sensitivity analysis\n        # For example, sensitivity_pruner iteratively prune the target\n        # model according to the sensitivity. After each round of\n        # pruning, the sensitivity_pruner will test the new sensitivity\n        # for each layer\n        self.already_pruned = {}\n        self.model_parse()",
  "def layers_count(self):\n        return len(self.target_layer)",
  "def model_parse(self):\n        for name, submodel in self.model.named_modules():\n            for op_type in SUPPORTED_OP_TYPE:\n                if isinstance(submodel, op_type):\n                    self.target_layer[name] = submodel\n                    self.already_pruned[name] = 0",
  "def _need_to_stop(self, ori_metric, cur_metric):\n        \"\"\"\n        Judge if meet the stop conditon(early_stop, min_threshold,\n        max_threshold).\n        Parameters\n        ----------\n        ori_metric : float\n            original validation metric\n        cur_metric : float\n            current validation metric\n\n        Returns\n        -------\n        stop : bool\n            if stop the sensitivity analysis\n        \"\"\"\n        if self.early_stop_mode is None:\n            # early stop mode is not enable\n            return False\n        assert self.early_stop_value is not None\n        if self.early_stop_mode == 'minimize':\n            if cur_metric < self.early_stop_value:\n                return True\n        elif self.early_stop_mode == 'maximize':\n            if cur_metric > self.early_stop_value:\n                return True\n        elif self.early_stop_mode == 'dropped':\n            if cur_metric < ori_metric - self.early_stop_value:\n                return True\n        elif self.early_stop_mode == 'raised':\n            if cur_metric > ori_metric + self.early_stop_value:\n                return True\n        return False",
  "def analysis(self, val_args=None, val_kwargs=None, specified_layers=None):\n        \"\"\"\n        This function analyze the sensitivity to pruning for\n        each conv layer in the target model.\n        If start and end are not set, we analyze all the conv\n        layers by default. Users can specify several layers to\n        analyze or parallelize the analysis process easily through\n        the start and end parameter.\n\n        Parameters\n        ----------\n        val_args : list\n            args for the val_function\n        val_kwargs : dict\n            kwargs for the val_funtion\n        specified_layers : list\n            list of layer names to analyze sensitivity.\n            If this variable is set, then only analyze\n            the conv layers that specified in the list.\n            User can also use this option to parallelize\n            the sensitivity analysis easily.\n        Returns\n        -------\n        sensitivities : dict\n            dict object that stores the trajectory of the\n            accuracy/loss when the prune ratio changes\n        \"\"\"\n        if val_args is None:\n            val_args = []\n        if val_kwargs is None:\n            val_kwargs = {}\n        # Get the original validation metric(accuracy/loss) before pruning\n        # Get the accuracy baseline before starting the analysis.\n        self.ori_metric = self.val_func(*val_args, **val_kwargs)\n        namelist = list(self.target_layer.keys())\n        if specified_layers is not None:\n            # only analyze several specified conv layers\n            namelist = list(filter(lambda x: x in specified_layers, namelist))\n        for name in namelist:\n            self.sensitivities[name] = {}\n            for sparsity in self.sparsities:\n                # here the sparsity is the relative sparsity of the\n                # the remained weights\n                # Calculate the actual prune ratio based on the already pruned ratio\n                real_sparsity = (\n                    1.0 - self.already_pruned[name]) * sparsity + self.already_pruned[name]\n                # TODO In current L1/L2 Filter Pruner, the 'op_types' is still necessary\n                # I think the L1/L2 Pruner should specify the op_types automaticlly\n                # according to the op_names\n                cfg = [{'sparsity': real_sparsity, 'op_names': [\n                    name], 'op_types': ['Conv2d']}]\n                pruner = self.Pruner(self.model, cfg)\n                pruner.compress()\n                val_metric = self.val_func(*val_args, **val_kwargs)\n                logger.info('Layer: %s Sparsity: %.2f Validation Metric: %.4f',\n                            name, real_sparsity, val_metric)\n\n                self.sensitivities[name][sparsity] = val_metric\n                pruner._unwrap_model()\n                del pruner\n                # check if the current metric meet the stop condition\n                if self._need_to_stop(self.ori_metric, val_metric):\n                    break\n\n            # reset the weights pruned by the pruner, because the\n            # input sparsities is sorted, so we donnot need to reset\n            # weight of the layer when the sparsity changes, instead,\n            # we only need reset the weight when the pruning layer changes.\n            self.model.load_state_dict(self.ori_state_dict)\n\n        return self.sensitivities",
  "def export(self, filepath):\n        \"\"\"\n        Export the results of the sensitivity analysis\n        to a csv file. The firstline of the csv file describe the content\n        structure. The first line is constructed by 'layername' and sparsity\n        list. Each line below records the validation metric returned by val_func\n        when this layer is under different sparsities. Note that, due to the early_stop\n        option, some layers may not have the metrics under all sparsities.\n\n        layername, 0.25, 0.5, 0.75\n        conv1, 0.6, 0.55\n        conv2, 0.61, 0.57, 0.56\n\n        Parameters\n        ----------\n        filepath : str\n            Path of the output file\n        \"\"\"\n        str_sparsities = [str(x) for x in self.sparsities]\n        header = ['layername'] + str_sparsities\n        with open(filepath, 'w') as csvf:\n            csv_w = csv.writer(csvf)\n            csv_w.writerow(header)\n            for layername in self.sensitivities:\n                row = []\n                row.append(layername)\n                for sparsity in sorted(self.sensitivities[layername].keys()):\n                    row.append(self.sensitivities[layername][sparsity])\n                csv_w.writerow(row)",
  "def update_already_pruned(self, layername, ratio):\n        \"\"\"\n        Set the already pruned ratio for the target layer.\n        \"\"\"\n        self.already_pruned[layername] = ratio",
  "def load_state_dict(self, state_dict):\n        \"\"\"\n        Update the weight of the model\n        \"\"\"\n        self.ori_state_dict = copy.deepcopy(state_dict)\n        self.model.load_state_dict(self.ori_state_dict)",
  "def validate_op_names(model, op_names, logger):\n    found_names = set(map(lambda x: x[0], model.named_modules()))\n\n    not_found_op_names = list(set(op_names) - found_names)\n    if not_found_op_names:\n        logger.warning('op_names %s not found in model', not_found_op_names)\n\n    return True",
  "def validate_op_types(model, op_types, logger):\n    found_types = set(['default']) | set(map(lambda x: type(x[1]).__name__, model.named_modules()))\n\n    not_found_op_types = list(set(op_types) - found_types)\n    if not_found_op_types:\n        logger.warning('op_types %s not found in model', not_found_op_types)\n\n    return True",
  "def validate_op_types_op_names(data):\n    if not ('op_types' in data or 'op_names' in data):\n        raise SchemaError('Either op_types or op_names must be specified.')\n    return True",
  "class CompressorSchema:\n    def __init__(self, data_schema, model, logger):\n        assert isinstance(data_schema, list) and len(data_schema) <= 1\n        self.data_schema = data_schema\n        self.compressor_schema = Schema(self._modify_schema(data_schema, model, logger))\n\n    def _modify_schema(self, data_schema, model, logger):\n        if not data_schema:\n            return data_schema\n\n        for k in data_schema[0]:\n            old_schema = data_schema[0][k]\n            if k == 'op_types' or (isinstance(k, Schema) and k._schema == 'op_types'):\n                new_schema = And(old_schema, lambda n: validate_op_types(model, n, logger))\n                data_schema[0][k] = new_schema\n            if k == 'op_names' or (isinstance(k, Schema) and k._schema == 'op_names'):\n                new_schema = And(old_schema, lambda n: validate_op_names(model, n, logger))\n                data_schema[0][k] = new_schema\n\n        data_schema[0] = And(data_schema[0], lambda d: validate_op_types_op_names(d))\n\n        return data_schema\n\n    def validate(self, data):\n        self.compressor_schema.validate(data)",
  "def __init__(self, data_schema, model, logger):\n        assert isinstance(data_schema, list) and len(data_schema) <= 1\n        self.data_schema = data_schema\n        self.compressor_schema = Schema(self._modify_schema(data_schema, model, logger))",
  "def _modify_schema(self, data_schema, model, logger):\n        if not data_schema:\n            return data_schema\n\n        for k in data_schema[0]:\n            old_schema = data_schema[0][k]\n            if k == 'op_types' or (isinstance(k, Schema) and k._schema == 'op_types'):\n                new_schema = And(old_schema, lambda n: validate_op_types(model, n, logger))\n                data_schema[0][k] = new_schema\n            if k == 'op_names' or (isinstance(k, Schema) and k._schema == 'op_names'):\n                new_schema = And(old_schema, lambda n: validate_op_names(model, n, logger))\n                data_schema[0][k] = new_schema\n\n        data_schema[0] = And(data_schema[0], lambda d: validate_op_types_op_names(d))\n\n        return data_schema",
  "def validate(self, data):\n        self.compressor_schema.validate(data)",
  "def get_total_num_weights(model, op_types=['default']):\n        '''\n        calculate the total number of weights\n\n        Returns\n        -------\n        int\n            total weights of all the op considered\n        '''\n        num_weights = 0\n        for _, module in model.named_modules():\n            if module == model:\n                continue\n            if 'default' in op_types or type(module).__name__ in op_types:\n                num_weights += module.weight.data.numel()\n        return num_weights",
  "class Dependency:\n    def __init__(self, model=None, dummy_input=None, traced_model=None):\n        \"\"\"\n        Build the graph for the model.\n        \"\"\"\n        from nni._graph_utils import TorchModuleGraph\n\n        # check if the input is legal\n        if traced_model is None:\n            # user should provide model & dummy_input to trace\n            # the model or a already traced model\n            assert model is not None and dummy_input is not None\n        self.graph = TorchModuleGraph(model, dummy_input, traced_model)\n        self.dependency = dict()\n        self.build_dependency()\n\n    def build_dependency(self):\n        raise NotImplementedError\n\n    def export(self, filepath):\n        raise NotImplementedError",
  "class ChannelDependency(Dependency):\n    def __init__(self, model=None, dummy_input=None, traced_model=None):\n        \"\"\"\n        This model analyze the channel dependencis between the conv\n        layers in a model.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            The model to be analyzed.\n        data : torch.Tensor\n            The example input data to trace the network architecture.\n        traced_model : torch._C.Graph\n            if we alreay has the traced graph of the target model, we donnot\n            need to trace the model again.\n        \"\"\"\n        super(ChannelDependency, self).__init__(model, dummy_input, traced_model)\n\n    def _get_parent_layers(self, node):\n        \"\"\"\n        Find the nearest father conv layers for the target node.\n\n        Parameters\n        ---------\n        node : torch._C.Node\n            target node.\n\n        Returns\n        -------\n        parent_layers: list\n            nearest father conv/linear layers for the target worknode.\n        \"\"\"\n        parent_layers = []\n        queue = []\n        queue.append(node)\n        while queue:\n            curnode = queue.pop(0)\n            if curnode.op_type == 'Conv2d' or curnode.op_type == 'Linear':\n                # find the first met conv\n                parent_layers.append(curnode.name)\n                continue\n            parents = self.graph.find_predecessors(curnode.unique_name)\n            parents = [self.graph.name_to_node[name] for name in parents]\n            for parent in parents:\n                queue.append(parent)\n        return parent_layers\n\n    def build_dependency(self):\n        \"\"\"\n        Build the channel dependency for the conv layers\n        in the model.\n        \"\"\"\n        # unpack the tuple/list manually before analyze the\n        # channel dependency\n        self.graph.unpack_manually()\n        for node in self.graph.nodes_py.nodes_op:\n            parent_layers = []\n            # find the node that contains aten::add\n            # or aten::cat operations\n            if node.op_type in ADD_TYPES:\n                parent_layers = self._get_parent_layers(node)\n            elif node.op_type == CAT_TYPE:\n                # To determine if this cat operation will introduce channel\n                # dependency, we need the specific input parameters of the cat\n                # opertion. To get the input parameters of the cat opertion, we\n                # need to traverse all the cpp_nodes included by this NodePyGroup,\n                # because, TorchModuleGraph merges the important nodes and the adjacent\n                # unimportant nodes (nodes started with prim::attr, for example) into a\n                # NodepyGroup.\n                cat_dim = None\n                for cnode in node.node_cpps:\n                    if cnode.kind() == CAT_TYPE:\n                        cat_dim = list(cnode.inputs())[1].toIValue()\n                        break\n                if cat_dim != 1:\n                    parent_layers = self._get_parent_layers(node)\n            dependency_set = set(parent_layers)\n            # merge the dependencies\n            for parent in parent_layers:\n                if parent in self.dependency:\n                    dependency_set.update(self.dependency[parent])\n            # save the dependencies\n            for _node in dependency_set:\n                self.dependency[_node] = dependency_set\n\n\n    def export(self, filepath):\n        \"\"\"\n        export the channel dependencies as a csv file.\n        The layers at the same line have output channel\n        dependencies with each other. For example,\n        layer1.1.conv2, conv1, and layer1.0.conv2 have\n        output channel dependencies with each other, which\n        means the output channel(filters) numbers of these\n        three layers should be same with each other, otherwise\n        the model may has shape conflict.\n\n        Output example:\n        Dependency Set,Convolutional Layers\n        Set 1,layer1.1.conv2,layer1.0.conv2,conv1\n        Set 2,layer1.0.conv1\n        Set 3,layer1.1.conv1\n        \"\"\"\n        header = ['Dependency Set', 'Layers']\n        setid = 0\n        visited = set()\n        with open(filepath, 'w') as csvf:\n            csv_w = csv.writer(csvf, delimiter=',')\n            csv_w.writerow(header)\n            for node in self.graph.nodes_py.nodes_op:\n                if node.op_type != 'Conv2d' or node in visited:\n                    continue\n                setid += 1\n                row = ['Set %d' % setid]\n                if node.name not in self.dependency:\n                    visited.add(node)\n                    row.append(node.name)\n                else:\n                    for other in self.dependency[node.name]:\n                        visited.add(self.graph.name_to_node[other])\n                        row.append(other)\n                csv_w.writerow(row)\n\n    @property\n    def dependency_sets(self):\n        \"\"\"\n        Get the list of the dependency set.\n\n        Returns\n        -------\n        dependency_sets : list\n            list of the dependency sets. For example,\n            [set(['conv1', 'conv2']), set(['conv3', 'conv4'])]\n\n        \"\"\"\n        d_sets = []\n        visited = set()\n        for node in self.graph.nodes_py.nodes_op:\n            if node.op_type != 'Conv2d' or node in visited:\n                continue\n            tmp_set = set()\n            if node.name not in self.dependency:\n                visited.add(node)\n                tmp_set.add(node.name)\n            else:\n                for other in self.dependency[node.name]:\n                    visited.add(self.graph.name_to_node[other])\n                    tmp_set.add(other)\n            d_sets.append(tmp_set)\n        return d_sets",
  "class CatPaddingDependency(ChannelDependency):\n    def __init__(self, model=None, dummy_input=None, traced_model=None):\n        super(CatPaddingDependency, self).__init__(model, dummy_input, traced_model)\n\n    def build_dependency(self):\n        \"\"\"\n        Build the cat padding dependencies.\n        If the output features of several layers are stitched together\n        by cat operation, then these layers have cat padding dependencies.\n        This is because when inferring the cat mask, we need all the input\n        masks for the cat operation. At this time we need to know the source\n        of all input vectors of a cat operation.\n        \"\"\"\n        for node in self.graph.nodes_py.nodes_op:\n            parent_layers = []\n            if node.op_type == CAT_TYPE:\n                parent_layers = self._get_parent_layers(node)\n                dependency_set = set(parent_layers)\n                # merge the dependencies\n                for parent in parent_layers:\n                    if parent in self.dependency:\n                        dependency_set.update(self.dependency[parent])\n                # save the dependencies\n                for _node in dependency_set:\n                    self.dependency[_node] = dependency_set\n\n    @property\n    def dependency_sets(self):\n        d_sets = []\n        visited = set()\n        for nodename in self.dependency:\n            if nodename in visited:\n                continue\n            d_sets.append(self.dependency[nodename])\n        return d_sets\n\n    def export(self, filepath):\n        \"\"\"\n        Export the dependencies into a file.\n        In the output file, each line contains a set of layers\n        whose output features are stitched together by the cat\n        operation.\n\n        output example:\n        Dependency Set, Layers\n        set1, Conv1, Conv2\n        set2, Conv3, Conv4\n        \"\"\"\n        header = ['Dependency Set', 'Layers']\n        setid = 0\n        with open(filepath, 'w') as csvf:\n            csv_w = csv.writer(csvf, delimiter=',')\n            csv_w.writerow(header)\n            for layers in self.dependency_sets:\n                setid += 1\n                row = ['Set %d' % setid]\n                row.extend(list(layers))\n                csv_w.writerow(row)",
  "class GroupDependency(Dependency):\n    def __init__(self, model=None, dummy_input=None, traced_model=None):\n        \"\"\"\n        This model analyze the group dependencis between the conv\n        layers in a model.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            The model to be analyzed.\n        data : torch.Tensor\n            The example input data to trace the network architecture.\n        traced_model : torch._C.Graph\n            if we alreay has the traced graph of the target model, we donnot\n            need to trace the model again.\n        \"\"\"\n        super(GroupDependency, self).__init__(model, dummy_input, traced_model)\n\n    def _get_parent_convs(self, node):\n        \"\"\"\n        Find the nearest father conv layers for the target node.\n\n        Parameters\n        ---------\n        node : torch._C.Node\n            target node.\n\n        Returns\n        -------\n        parent_layers : list\n            nearest father conv layers for the target node. Due to the group\n            dependency only exists between the conv layers, so we only find\n            the parent conv layers.\n        \"\"\"\n        parent_layers = []\n        # the input node is a Conv node\n        predeessors = self.graph.find_predecessors(node.unique_name)\n        predeessors = [self.graph.name_to_node[x] for x in predeessors]\n        queue = predeessors\n        while queue:\n            curnode = queue.pop(0)\n            if curnode.op_type == 'Conv2d':\n                # find the first met conv\n                parent_layers.append(curnode.name)\n                continue\n            parents = self.graph.find_predecessors(curnode.unique_name)\n            parents = [self.graph.name_to_node[name] for name in parents]\n            for parent in parents:\n                queue.append(parent)\n        return parent_layers\n\n    def _get_conv_groups(self, node_group):\n        \"\"\"\n        Get the number of groups for a convolutional layer.\n\n        Parameters\n        ----------\n        node_group : NodePyGroup\n            target node.\n\n        Returns\n        -------\n        group : int\n            the number of the groups of the target conv layer.\n        \"\"\"\n        cpp_conv = list(filter(lambda x: x.kind() == CONV_TYPE, node_group.node_cpps))\n        assert len(cpp_conv) == 1\n        cpp_conv = cpp_conv[0]\n        inputs = list(cpp_conv.inputs())\n        # get the number of the group from the input parameters\n        group = inputs[8].toIValue()\n        return group\n\n    def build_dependency(self):\n        \"\"\"\n        Build the channel dependency for the conv layers\n        in the model. This function return the group number\n        of each conv layers. Note that, here, the group count\n        of conv layers may be larger than their originl groups.\n        This is because that the input channel will also be grouped\n        for the group conv layers. To make this clear, assume we\n        have two group conv layers: conv1(group=2), conv2(group=4).\n        conv2 takes the output features of conv1 as input.\n        Then we have to the filters of conv1 can still be\n        divided into 4 groups after filter pruning, because\n        the input channels of conv2 shoule be divided into\n        4 groups.\n\n        Returns\n        -------\n        self.dependency : dict\n            key: the name of conv layers, value: the minimum value that the number of\n            filters should be divisible to.\n        \"\"\"\n        for node in self.graph.nodes_py.nodes_op:\n            if node.op_type == 'Conv2d':\n                group = self._get_conv_groups(node)\n                if node.name in self.dependency:\n                    # the conv layer whose group is larger than 1 will require that\n                    # it's number of output channel to be divisible by the number of group.\n                    self.dependency[node.name] = max(self.dependency[node.name], group)\n                else:\n                    self.dependency[node.name] = group\n                if group > 1:\n                    # for the conv layer whose group is larger than 1, it will require the number\n                    # of output channels of their parent conv layer to be divisible by group.\n                    parent_convs = self._get_parent_convs(node)\n                    for parent in parent_convs:\n                        if parent in self.dependency:\n                            self.dependency[parent] = max(self.dependency[parent], group)\n                        else:\n                            self.dependency[parent] = group\n        return self.dependency\n\n    def export(self, filepath):\n        \"\"\"\n        export the group dependency to a csv file.\n        Each line describes a convolution layer, the\n        first part of each line is the Pytorch module\n        name of the conv layer. The second part of each\n        line is the group count of the filters in this layer.\n        Note that, the group count may be larger than this\n        layers original group number.\n\n        output example:\n        Conv layer, Groups\n        Conv1, 1\n        Conv2, 2\n        Conv3, 4\n        \"\"\"\n        header = ['Conv Layer Name', 'Group']\n        with open(filepath, 'w') as csvf:\n            csv_w = csv.writer(csvf, delimiter=',')\n            csv_w.writerow(header)\n            for name in self.dependency:\n                group = self.dependency[name]\n                csv_w.writerow([name, group])",
  "def __init__(self, model=None, dummy_input=None, traced_model=None):\n        \"\"\"\n        Build the graph for the model.\n        \"\"\"\n        from nni._graph_utils import TorchModuleGraph\n\n        # check if the input is legal\n        if traced_model is None:\n            # user should provide model & dummy_input to trace\n            # the model or a already traced model\n            assert model is not None and dummy_input is not None\n        self.graph = TorchModuleGraph(model, dummy_input, traced_model)\n        self.dependency = dict()\n        self.build_dependency()",
  "def build_dependency(self):\n        raise NotImplementedError",
  "def export(self, filepath):\n        raise NotImplementedError",
  "def __init__(self, model=None, dummy_input=None, traced_model=None):\n        \"\"\"\n        This model analyze the channel dependencis between the conv\n        layers in a model.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            The model to be analyzed.\n        data : torch.Tensor\n            The example input data to trace the network architecture.\n        traced_model : torch._C.Graph\n            if we alreay has the traced graph of the target model, we donnot\n            need to trace the model again.\n        \"\"\"\n        super(ChannelDependency, self).__init__(model, dummy_input, traced_model)",
  "def _get_parent_layers(self, node):\n        \"\"\"\n        Find the nearest father conv layers for the target node.\n\n        Parameters\n        ---------\n        node : torch._C.Node\n            target node.\n\n        Returns\n        -------\n        parent_layers: list\n            nearest father conv/linear layers for the target worknode.\n        \"\"\"\n        parent_layers = []\n        queue = []\n        queue.append(node)\n        while queue:\n            curnode = queue.pop(0)\n            if curnode.op_type == 'Conv2d' or curnode.op_type == 'Linear':\n                # find the first met conv\n                parent_layers.append(curnode.name)\n                continue\n            parents = self.graph.find_predecessors(curnode.unique_name)\n            parents = [self.graph.name_to_node[name] for name in parents]\n            for parent in parents:\n                queue.append(parent)\n        return parent_layers",
  "def build_dependency(self):\n        \"\"\"\n        Build the channel dependency for the conv layers\n        in the model.\n        \"\"\"\n        # unpack the tuple/list manually before analyze the\n        # channel dependency\n        self.graph.unpack_manually()\n        for node in self.graph.nodes_py.nodes_op:\n            parent_layers = []\n            # find the node that contains aten::add\n            # or aten::cat operations\n            if node.op_type in ADD_TYPES:\n                parent_layers = self._get_parent_layers(node)\n            elif node.op_type == CAT_TYPE:\n                # To determine if this cat operation will introduce channel\n                # dependency, we need the specific input parameters of the cat\n                # opertion. To get the input parameters of the cat opertion, we\n                # need to traverse all the cpp_nodes included by this NodePyGroup,\n                # because, TorchModuleGraph merges the important nodes and the adjacent\n                # unimportant nodes (nodes started with prim::attr, for example) into a\n                # NodepyGroup.\n                cat_dim = None\n                for cnode in node.node_cpps:\n                    if cnode.kind() == CAT_TYPE:\n                        cat_dim = list(cnode.inputs())[1].toIValue()\n                        break\n                if cat_dim != 1:\n                    parent_layers = self._get_parent_layers(node)\n            dependency_set = set(parent_layers)\n            # merge the dependencies\n            for parent in parent_layers:\n                if parent in self.dependency:\n                    dependency_set.update(self.dependency[parent])\n            # save the dependencies\n            for _node in dependency_set:\n                self.dependency[_node] = dependency_set",
  "def export(self, filepath):\n        \"\"\"\n        export the channel dependencies as a csv file.\n        The layers at the same line have output channel\n        dependencies with each other. For example,\n        layer1.1.conv2, conv1, and layer1.0.conv2 have\n        output channel dependencies with each other, which\n        means the output channel(filters) numbers of these\n        three layers should be same with each other, otherwise\n        the model may has shape conflict.\n\n        Output example:\n        Dependency Set,Convolutional Layers\n        Set 1,layer1.1.conv2,layer1.0.conv2,conv1\n        Set 2,layer1.0.conv1\n        Set 3,layer1.1.conv1\n        \"\"\"\n        header = ['Dependency Set', 'Layers']\n        setid = 0\n        visited = set()\n        with open(filepath, 'w') as csvf:\n            csv_w = csv.writer(csvf, delimiter=',')\n            csv_w.writerow(header)\n            for node in self.graph.nodes_py.nodes_op:\n                if node.op_type != 'Conv2d' or node in visited:\n                    continue\n                setid += 1\n                row = ['Set %d' % setid]\n                if node.name not in self.dependency:\n                    visited.add(node)\n                    row.append(node.name)\n                else:\n                    for other in self.dependency[node.name]:\n                        visited.add(self.graph.name_to_node[other])\n                        row.append(other)\n                csv_w.writerow(row)",
  "def dependency_sets(self):\n        \"\"\"\n        Get the list of the dependency set.\n\n        Returns\n        -------\n        dependency_sets : list\n            list of the dependency sets. For example,\n            [set(['conv1', 'conv2']), set(['conv3', 'conv4'])]\n\n        \"\"\"\n        d_sets = []\n        visited = set()\n        for node in self.graph.nodes_py.nodes_op:\n            if node.op_type != 'Conv2d' or node in visited:\n                continue\n            tmp_set = set()\n            if node.name not in self.dependency:\n                visited.add(node)\n                tmp_set.add(node.name)\n            else:\n                for other in self.dependency[node.name]:\n                    visited.add(self.graph.name_to_node[other])\n                    tmp_set.add(other)\n            d_sets.append(tmp_set)\n        return d_sets",
  "def __init__(self, model=None, dummy_input=None, traced_model=None):\n        super(CatPaddingDependency, self).__init__(model, dummy_input, traced_model)",
  "def build_dependency(self):\n        \"\"\"\n        Build the cat padding dependencies.\n        If the output features of several layers are stitched together\n        by cat operation, then these layers have cat padding dependencies.\n        This is because when inferring the cat mask, we need all the input\n        masks for the cat operation. At this time we need to know the source\n        of all input vectors of a cat operation.\n        \"\"\"\n        for node in self.graph.nodes_py.nodes_op:\n            parent_layers = []\n            if node.op_type == CAT_TYPE:\n                parent_layers = self._get_parent_layers(node)\n                dependency_set = set(parent_layers)\n                # merge the dependencies\n                for parent in parent_layers:\n                    if parent in self.dependency:\n                        dependency_set.update(self.dependency[parent])\n                # save the dependencies\n                for _node in dependency_set:\n                    self.dependency[_node] = dependency_set",
  "def dependency_sets(self):\n        d_sets = []\n        visited = set()\n        for nodename in self.dependency:\n            if nodename in visited:\n                continue\n            d_sets.append(self.dependency[nodename])\n        return d_sets",
  "def export(self, filepath):\n        \"\"\"\n        Export the dependencies into a file.\n        In the output file, each line contains a set of layers\n        whose output features are stitched together by the cat\n        operation.\n\n        output example:\n        Dependency Set, Layers\n        set1, Conv1, Conv2\n        set2, Conv3, Conv4\n        \"\"\"\n        header = ['Dependency Set', 'Layers']\n        setid = 0\n        with open(filepath, 'w') as csvf:\n            csv_w = csv.writer(csvf, delimiter=',')\n            csv_w.writerow(header)\n            for layers in self.dependency_sets:\n                setid += 1\n                row = ['Set %d' % setid]\n                row.extend(list(layers))\n                csv_w.writerow(row)",
  "def __init__(self, model=None, dummy_input=None, traced_model=None):\n        \"\"\"\n        This model analyze the group dependencis between the conv\n        layers in a model.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            The model to be analyzed.\n        data : torch.Tensor\n            The example input data to trace the network architecture.\n        traced_model : torch._C.Graph\n            if we alreay has the traced graph of the target model, we donnot\n            need to trace the model again.\n        \"\"\"\n        super(GroupDependency, self).__init__(model, dummy_input, traced_model)",
  "def _get_parent_convs(self, node):\n        \"\"\"\n        Find the nearest father conv layers for the target node.\n\n        Parameters\n        ---------\n        node : torch._C.Node\n            target node.\n\n        Returns\n        -------\n        parent_layers : list\n            nearest father conv layers for the target node. Due to the group\n            dependency only exists between the conv layers, so we only find\n            the parent conv layers.\n        \"\"\"\n        parent_layers = []\n        # the input node is a Conv node\n        predeessors = self.graph.find_predecessors(node.unique_name)\n        predeessors = [self.graph.name_to_node[x] for x in predeessors]\n        queue = predeessors\n        while queue:\n            curnode = queue.pop(0)\n            if curnode.op_type == 'Conv2d':\n                # find the first met conv\n                parent_layers.append(curnode.name)\n                continue\n            parents = self.graph.find_predecessors(curnode.unique_name)\n            parents = [self.graph.name_to_node[name] for name in parents]\n            for parent in parents:\n                queue.append(parent)\n        return parent_layers",
  "def _get_conv_groups(self, node_group):\n        \"\"\"\n        Get the number of groups for a convolutional layer.\n\n        Parameters\n        ----------\n        node_group : NodePyGroup\n            target node.\n\n        Returns\n        -------\n        group : int\n            the number of the groups of the target conv layer.\n        \"\"\"\n        cpp_conv = list(filter(lambda x: x.kind() == CONV_TYPE, node_group.node_cpps))\n        assert len(cpp_conv) == 1\n        cpp_conv = cpp_conv[0]\n        inputs = list(cpp_conv.inputs())\n        # get the number of the group from the input parameters\n        group = inputs[8].toIValue()\n        return group",
  "def build_dependency(self):\n        \"\"\"\n        Build the channel dependency for the conv layers\n        in the model. This function return the group number\n        of each conv layers. Note that, here, the group count\n        of conv layers may be larger than their originl groups.\n        This is because that the input channel will also be grouped\n        for the group conv layers. To make this clear, assume we\n        have two group conv layers: conv1(group=2), conv2(group=4).\n        conv2 takes the output features of conv1 as input.\n        Then we have to the filters of conv1 can still be\n        divided into 4 groups after filter pruning, because\n        the input channels of conv2 shoule be divided into\n        4 groups.\n\n        Returns\n        -------\n        self.dependency : dict\n            key: the name of conv layers, value: the minimum value that the number of\n            filters should be divisible to.\n        \"\"\"\n        for node in self.graph.nodes_py.nodes_op:\n            if node.op_type == 'Conv2d':\n                group = self._get_conv_groups(node)\n                if node.name in self.dependency:\n                    # the conv layer whose group is larger than 1 will require that\n                    # it's number of output channel to be divisible by the number of group.\n                    self.dependency[node.name] = max(self.dependency[node.name], group)\n                else:\n                    self.dependency[node.name] = group\n                if group > 1:\n                    # for the conv layer whose group is larger than 1, it will require the number\n                    # of output channels of their parent conv layer to be divisible by group.\n                    parent_convs = self._get_parent_convs(node)\n                    for parent in parent_convs:\n                        if parent in self.dependency:\n                            self.dependency[parent] = max(self.dependency[parent], group)\n                        else:\n                            self.dependency[parent] = group\n        return self.dependency",
  "def export(self, filepath):\n        \"\"\"\n        export the group dependency to a csv file.\n        Each line describes a convolution layer, the\n        first part of each line is the Pytorch module\n        name of the conv layer. The second part of each\n        line is the group count of the filters in this layer.\n        Note that, the group count may be larger than this\n        layers original group number.\n\n        output example:\n        Conv layer, Groups\n        Conv1, 1\n        Conv2, 2\n        Conv3, 4\n        \"\"\"\n        header = ['Conv Layer Name', 'Group']\n        with open(filepath, 'w') as csvf:\n            csv_w = csv.writer(csvf, delimiter=',')\n            csv_w.writerow(header)\n            for name in self.dependency:\n                group = self.dependency[name]\n                csv_w.writerow([name, group])",
  "def count_flops_params(model: nn.Module, input_size, verbose=True):\n    \"\"\"\n    Count FLOPs and Params of the given model.\n    This function would identify the mask on the module\n    and take the pruned shape into consideration.\n    Note that, for sturctured pruning, we only identify\n    the remained filters according to its mask, which\n    not taking the pruned input channels into consideration,\n    so the calculated FLOPs will be larger than real number.\n\n    Parameters\n    ---------\n    model : nn.Module\n        target model.\n    input_size: list, tuple\n        the input shape of data\n\n\n    Returns\n    -------\n    flops: float\n        total flops of the model\n    params:\n        total params of the model\n    \"\"\"\n\n    assert input_size is not None\n\n    device = next(model.parameters()).device\n    inputs = torch.randn(input_size).to(device)\n\n    hook_module_list = []\n    prev_m = None\n    for m in model.modules():\n        weight_mask = None\n        m_type = type(m)\n        if m_type in custom_ops:\n            if isinstance(prev_m, PrunerModuleWrapper):\n                weight_mask = prev_m.weight_mask\n\n            m.register_buffer('weight_mask', weight_mask)\n            hook_module_list.append(m)\n        prev_m = m\n\n    flops, params = profile(model, inputs=(inputs, ), custom_ops=custom_ops, verbose=verbose)\n\n\n    for m in hook_module_list:\n        m._buffers.pop(\"weight_mask\")\n    # Remove registerd buffer on the model, and fixed following issue:\n    # https://github.com/Lyken17/pytorch-OpCounter/issues/96\n    for m in model.modules():\n        if 'total_ops' in m._buffers:\n            m._buffers.pop(\"total_ops\")\n        if 'total_params' in m._buffers:\n            m._buffers.pop(\"total_params\")\n\n    return flops, params",
  "def count_convNd_mask(m, x, y):\n    \"\"\"\n    The forward hook to count FLOPs and Parameters of convolution operation.\n\n    Parameters\n    ----------\n    m : torch.nn.Module\n        convolution module to calculate the FLOPs and Parameters\n    x : torch.Tensor\n        input data\n    y : torch.Tensor\n        output data\n    \"\"\"\n    output_channel = y.size()[1]\n    output_size = torch.zeros(y.size()[2:]).numel()\n    kernel_size = torch.zeros(m.weight.size()[2:]).numel()\n\n    bias_flops = 1 if m.bias is not None else 0\n\n    if m.weight_mask is not None:\n        output_channel = m.weight_mask.sum() // (m.in_channels * kernel_size)\n\n    total_ops = output_channel * output_size * (m.in_channels // m.groups * kernel_size + bias_flops)\n\n    m.total_ops += torch.DoubleTensor([int(total_ops)])",
  "def count_linear_mask(m, x, y):\n    \"\"\"\n    The forward hook to count FLOPs and Parameters of linear transformation.\n\n    Parameters\n    ----------\n    m : torch.nn.Module\n        linear to calculate the FLOPs and Parameters\n    x : torch.Tensor\n        input data\n    y : torch.Tensor\n        output data\n    \"\"\"\n    output_channel = y.size()[1]\n    output_size = torch.zeros(y.size()[2:]).numel()\n\n    bias_flops = 1 if m.bias is not None else 0\n\n    if m.weight_mask is not None:\n        output_channel = m.weight_mask.sum() // m.in_features\n\n    total_ops = output_channel * output_size * (m.in_features + bias_flops)\n\n    m.total_ops += torch.DoubleTensor([int(total_ops)])",
  "def fix_mask_conflict(masks, model=None, dummy_input=None, traced=None):\n    \"\"\"\n    MaskConflict fix the mask conflict for the channel dependencies\n    and group dependency.\n\n    Parameters\n    ----------\n    masks : dict/str\n        A dict object that stores the masks or the path of the mask file\n    model : torch.nn.Module\n        model to fix the mask conflict\n    dummy_input : torch.Tensor\n        input example to trace the model\n    traced : torch._C.torch.jit.TopLevelTracedModule\n        the traced model of the target model, is this parameter is not None,\n        we donnot use the model and dummpy_input to get the trace graph.\n    \"\"\"\n    if isinstance(masks, str):\n        # if the input is the path of the mask_file\n        assert os.path.exists(masks)\n        masks = torch.load(masks)\n    # if the user uses the model and dummy_input to trace the model, we\n    # should get the traced model handly, so that, we only trace the\n    # model once, GroupMaskConflict and ChannelMaskConflict will reuse\n    # this traced model.\n    if traced is None:\n        assert model is not None and dummy_input is not None\n        with torch.onnx.set_training(model, False):\n            # We need to trace the model in this way, else it will have problems\n            traced = torch.jit.trace(model, dummy_input)\n\n    fix_group_mask = GroupMaskConflict(masks, model, dummy_input, traced)\n    masks = fix_group_mask.fix_mask()\n    fix_channel_mask = ChannelMaskConflict(masks, model, dummy_input, traced)\n    masks = fix_channel_mask.fix_mask()\n    padding_cat_mask = CatMaskPadding(masks, model, dummy_input, traced)\n    masks = padding_cat_mask.fix_mask()\n    return masks",
  "class MaskFix:\n    def __init__(self, masks, model=None, dummy_input=None, traced=None):\n        # check if the parameters are valid\n        parameter_valid = False\n        if traced is not None:\n            parameter_valid = True\n        elif (model is not None) and (dummy_input is not None):\n            parameter_valid = True\n        if not parameter_valid:\n            raise Exception('The input parameters is invalid!')\n        self.model = model\n        self.dummy_input = dummy_input\n        self.traced = traced\n        self.masks = masks\n\n    def fix_mask(self):\n        raise NotImplementedError\n\n    def export(self, path):\n        \"\"\"\n        Export the masks after fixing the conflict to file.\n        \"\"\"\n        torch.save(self.masks, path)",
  "class CatMaskPadding(MaskFix):\n    def __init__(self, masks, model, dummy_input=None, traced=None):\n        \"\"\"\n        CatMaskPadding find the layers whose output tensor is passed\n        to the same cat operation. The cat operation concatnates the\n        masks of the input tensors as the output mask, so when some\n        of the input layers of the cat operation are not pruned, we still\n        need to pass the masks of these non-pruned layers(the mask are\n        all ones) to the cat operation to ensure the shape of the output\n        mask is right.\n\n        Parameters\n        ----------\n        masks : dict\n            a dict object that stores the masks\n        model : torch.nn.Module\n            model to fix the mask conflict\n        dummy_input : torch.Tensor\n            input example to trace the model\n        traced : torch._C.torch.jit.TopLevelTracedModule\n            the traced model of the target model, is this parameter is not None,\n            we donnot use the model and dummpy_input to get the trace graph.\n        \"\"\"\n        super(CatMaskPadding, self).__init__(masks, model, dummy_input, traced)\n\n    def fix_mask(self):\n        cat_padding_depen = CatPaddingDependency(self.model, self.dummy_input, self.traced)\n        name_to_module = {}\n        for name, module in self.model.named_modules():\n            name_to_module[name] = module\n        depen = cat_padding_depen.dependency_sets\n        for layers in depen:\n            device = None\n            count = 0\n            for layer in layers:\n                if layer in self.masks:\n                    count += 1\n                    if device is None:\n                        device = self.masks[layer]['weight'].device\n            if count == 0:\n                # no layer is pruned\n                continue\n            elif count == len(layers):\n                # all the layers have been pruned\n                continue\n            # pad the mask for the non-pruned layers\n            for layer in layers:\n                if layer in self.masks:\n                    continue\n                module = name_to_module[layer]\n                w_shape = module.weight.data.size()\n                w_mask = torch.ones(w_shape).to(device)\n                b_mask = None\n                if hasattr(module, 'bias') and module.bias is not None:\n                    # module.bias may be None\n                    b_shape = module.bias.data.size()\n                    b_mask = torch.ones(b_shape).to(device)\n                self.masks[layer] = {'weight':w_mask, 'bias':b_mask}\n        return self.masks",
  "class GroupMaskConflict(MaskFix):\n    def __init__(self, masks, model=None, dummy_input=None, traced=None):\n        \"\"\"\n        GroupMaskConflict fix the mask conflict between the layers that\n        has group dependecy with each other.\n\n        Parameters\n        ----------\n        masks : dict\n            a dict object that stores the masks\n        model : torch.nn.Module\n            model to fix the mask conflict\n        dummy_input : torch.Tensor\n            input example to trace the model\n        traced : torch._C.torch.jit.TopLevelTracedModule\n            the traced model of the target model, is this parameter is not None,\n            we donnot use the model and dummpy_input to get the trace graph.\n        \"\"\"\n        super(GroupMaskConflict, self).__init__(masks, model, dummy_input, traced)\n\n\n    def fix_mask(self):\n        \"\"\"\n        Fix the mask conflict before the mask inference for the layers that\n        has group dependencies. This function should be called before the\n        mask inference of the 'speedup' module.\n        \"\"\"\n        group_depen = GroupDependency(self.model, self.dummy_input, self.traced)\n        depens = group_depen.dependency\n        _logger.info(depens)\n        for layername in depens:\n            group = depens[layername]\n            if layername not in self.masks:\n                # this layer not pruned\n                continue\n            w_mask = self.masks[layername]['weight']\n            shape = w_mask.size()\n            count = np.prod(shape[1:])\n            all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n            all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n            if len(all_ones) + len(all_zeros) < w_mask.size(0):\n                # In fine-grained pruning, skip this layer\n                _logger.info('Layers %s using fine-grained pruning', layername)\n                continue\n            assert shape[0] % group == 0\n            # Find the number of masked filter for each group (mini_masked).\n            # Because we have to keep the pruned filter can still\n            # be divided into the same number of groups, so we only can\n            # prune mini_masked filters for each group.\n            step = shape[0] / group\n            group_masked = []\n            for i in range(group):\n                _start = step * i\n                _end = step * (i+1)\n                _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n                group_masked.append(_tmp_list)\n            mini_masked = min([len(x) for x in group_masked])\n            for gm in group_masked:\n                for i in range(mini_masked, len(gm)):\n                    # To keep the output channel number still being divisible to\n                    # groups, we set the masks of following filters to be zero.\n                    pos = gm[i]\n                    self.masks[layername]['weight'][pos] = torch.ones(shape[1:])\n                    if hasattr(self.masks[layername], 'bias'):\n                        self.masks[layername]['bias'][pos] = 1\n        return self.masks",
  "class ChannelMaskConflict(MaskFix):\n    def __init__(self, masks, model=None, dummy_input=None, traced=None):\n        \"\"\"\n        ChannelMaskConflict fix the mask conflict between the layers that\n        has channel dependecy with each other.\n\n        Parameters\n        ----------\n        masks : dict\n            a dict object that stores the masks\n        model : torch.nn.Module\n            model to fix the mask conflict\n        dummy_input : torch.Tensor\n            input example to trace the model\n        graph : torch._C.torch.jit.TopLevelTracedModule\n            the traced graph of the target model, is this parameter is not None,\n            we donnot use the model and dummpy_input to get the trace graph.\n        \"\"\"\n        super(ChannelMaskConflict, self).__init__(masks, model, dummy_input, traced)\n\n    def fix_mask(self):\n        \"\"\"\n        Fix the mask conflict before the mask inference for the layers that\n        has shape dependencies. This function should be called before the\n        mask inference of the 'speedup' module.\n        \"\"\"\n        channel_depen = ChannelDependency(self.model, self.dummy_input, self.traced)\n        depen_sets = channel_depen.dependency_sets\n        for dset in depen_sets:\n            if len(dset) == 1:\n                # This layer has no channel dependency with other layers\n                continue\n            channel_remain = set()\n            fine_grained = False\n            out_channels = None\n            # A flag that represents if all the layers in\n            # the dependency set are pruned\n            all_pruned = True\n            for name in dset:\n                if name not in self.masks:\n                    # this layer is not pruned\n                    all_pruned = False\n                    continue\n                w_mask = self.masks[name]['weight']\n                if out_channels is None:\n                    out_channels = w_mask.size(0)\n                shape = w_mask.size()\n                count = np.prod(shape[1:])\n                all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n                all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n                if len(all_ones) + len(all_zeros) < w_mask.size(0):\n                    # In fine-grained pruning, there is no need to check\n                    # the shape conflict\n                    _logger.info('Layers %s using fine-grained pruning', ','.join(dset))\n                    fine_grained = True\n                    break\n                channel_remain.update(all_ones)\n                _logger.debug('Layer: %s ', name)\n                _logger.debug('Original pruned filters: %s', str(all_zeros))\n            # Update the masks for the layers in the dependency set\n            if fine_grained or out_channels is None:\n                # if use the fine-grained pruner or all the layers in\n                # this dependency set are not pruned\n                continue\n            if not all_pruned:\n                # if some layer are not pruned at all\n                # then all the layers in this dependency set\n                # cannot be pruned due to the shape dependency.\n                channel_remain.update(range(out_channels))\n            ori_channels = 0\n            for name in dset:\n                if name not in self.masks:\n                    # this layer is not pruned at all\n                    # in this case, all_pruned is False\n                    # and the other layers in the same dset\n                    # will not be pruned either.\n                    continue\n                mask = self.masks[name]\n                w_shape = mask['weight'].size()\n                ori_channels = w_shape[0]\n                for i in channel_remain:\n                    mask['weight'][i] = torch.ones(w_shape[1:])\n                    if hasattr(mask, 'bias'):\n                        mask['bias'][i] = 1\n            _logger.info(','.join(dset))\n            _logger.info('Pruned Filters after fixing conflict:')\n            pruned_filters = set(list(range(ori_channels)))-channel_remain\n            _logger.info(str(sorted(pruned_filters)))\n        return self.masks",
  "def __init__(self, masks, model=None, dummy_input=None, traced=None):\n        # check if the parameters are valid\n        parameter_valid = False\n        if traced is not None:\n            parameter_valid = True\n        elif (model is not None) and (dummy_input is not None):\n            parameter_valid = True\n        if not parameter_valid:\n            raise Exception('The input parameters is invalid!')\n        self.model = model\n        self.dummy_input = dummy_input\n        self.traced = traced\n        self.masks = masks",
  "def fix_mask(self):\n        raise NotImplementedError",
  "def export(self, path):\n        \"\"\"\n        Export the masks after fixing the conflict to file.\n        \"\"\"\n        torch.save(self.masks, path)",
  "def __init__(self, masks, model, dummy_input=None, traced=None):\n        \"\"\"\n        CatMaskPadding find the layers whose output tensor is passed\n        to the same cat operation. The cat operation concatnates the\n        masks of the input tensors as the output mask, so when some\n        of the input layers of the cat operation are not pruned, we still\n        need to pass the masks of these non-pruned layers(the mask are\n        all ones) to the cat operation to ensure the shape of the output\n        mask is right.\n\n        Parameters\n        ----------\n        masks : dict\n            a dict object that stores the masks\n        model : torch.nn.Module\n            model to fix the mask conflict\n        dummy_input : torch.Tensor\n            input example to trace the model\n        traced : torch._C.torch.jit.TopLevelTracedModule\n            the traced model of the target model, is this parameter is not None,\n            we donnot use the model and dummpy_input to get the trace graph.\n        \"\"\"\n        super(CatMaskPadding, self).__init__(masks, model, dummy_input, traced)",
  "def fix_mask(self):\n        cat_padding_depen = CatPaddingDependency(self.model, self.dummy_input, self.traced)\n        name_to_module = {}\n        for name, module in self.model.named_modules():\n            name_to_module[name] = module\n        depen = cat_padding_depen.dependency_sets\n        for layers in depen:\n            device = None\n            count = 0\n            for layer in layers:\n                if layer in self.masks:\n                    count += 1\n                    if device is None:\n                        device = self.masks[layer]['weight'].device\n            if count == 0:\n                # no layer is pruned\n                continue\n            elif count == len(layers):\n                # all the layers have been pruned\n                continue\n            # pad the mask for the non-pruned layers\n            for layer in layers:\n                if layer in self.masks:\n                    continue\n                module = name_to_module[layer]\n                w_shape = module.weight.data.size()\n                w_mask = torch.ones(w_shape).to(device)\n                b_mask = None\n                if hasattr(module, 'bias') and module.bias is not None:\n                    # module.bias may be None\n                    b_shape = module.bias.data.size()\n                    b_mask = torch.ones(b_shape).to(device)\n                self.masks[layer] = {'weight':w_mask, 'bias':b_mask}\n        return self.masks",
  "def __init__(self, masks, model=None, dummy_input=None, traced=None):\n        \"\"\"\n        GroupMaskConflict fix the mask conflict between the layers that\n        has group dependecy with each other.\n\n        Parameters\n        ----------\n        masks : dict\n            a dict object that stores the masks\n        model : torch.nn.Module\n            model to fix the mask conflict\n        dummy_input : torch.Tensor\n            input example to trace the model\n        traced : torch._C.torch.jit.TopLevelTracedModule\n            the traced model of the target model, is this parameter is not None,\n            we donnot use the model and dummpy_input to get the trace graph.\n        \"\"\"\n        super(GroupMaskConflict, self).__init__(masks, model, dummy_input, traced)",
  "def fix_mask(self):\n        \"\"\"\n        Fix the mask conflict before the mask inference for the layers that\n        has group dependencies. This function should be called before the\n        mask inference of the 'speedup' module.\n        \"\"\"\n        group_depen = GroupDependency(self.model, self.dummy_input, self.traced)\n        depens = group_depen.dependency\n        _logger.info(depens)\n        for layername in depens:\n            group = depens[layername]\n            if layername not in self.masks:\n                # this layer not pruned\n                continue\n            w_mask = self.masks[layername]['weight']\n            shape = w_mask.size()\n            count = np.prod(shape[1:])\n            all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n            all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n            if len(all_ones) + len(all_zeros) < w_mask.size(0):\n                # In fine-grained pruning, skip this layer\n                _logger.info('Layers %s using fine-grained pruning', layername)\n                continue\n            assert shape[0] % group == 0\n            # Find the number of masked filter for each group (mini_masked).\n            # Because we have to keep the pruned filter can still\n            # be divided into the same number of groups, so we only can\n            # prune mini_masked filters for each group.\n            step = shape[0] / group\n            group_masked = []\n            for i in range(group):\n                _start = step * i\n                _end = step * (i+1)\n                _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n                group_masked.append(_tmp_list)\n            mini_masked = min([len(x) for x in group_masked])\n            for gm in group_masked:\n                for i in range(mini_masked, len(gm)):\n                    # To keep the output channel number still being divisible to\n                    # groups, we set the masks of following filters to be zero.\n                    pos = gm[i]\n                    self.masks[layername]['weight'][pos] = torch.ones(shape[1:])\n                    if hasattr(self.masks[layername], 'bias'):\n                        self.masks[layername]['bias'][pos] = 1\n        return self.masks",
  "def __init__(self, masks, model=None, dummy_input=None, traced=None):\n        \"\"\"\n        ChannelMaskConflict fix the mask conflict between the layers that\n        has channel dependecy with each other.\n\n        Parameters\n        ----------\n        masks : dict\n            a dict object that stores the masks\n        model : torch.nn.Module\n            model to fix the mask conflict\n        dummy_input : torch.Tensor\n            input example to trace the model\n        graph : torch._C.torch.jit.TopLevelTracedModule\n            the traced graph of the target model, is this parameter is not None,\n            we donnot use the model and dummpy_input to get the trace graph.\n        \"\"\"\n        super(ChannelMaskConflict, self).__init__(masks, model, dummy_input, traced)",
  "def fix_mask(self):\n        \"\"\"\n        Fix the mask conflict before the mask inference for the layers that\n        has shape dependencies. This function should be called before the\n        mask inference of the 'speedup' module.\n        \"\"\"\n        channel_depen = ChannelDependency(self.model, self.dummy_input, self.traced)\n        depen_sets = channel_depen.dependency_sets\n        for dset in depen_sets:\n            if len(dset) == 1:\n                # This layer has no channel dependency with other layers\n                continue\n            channel_remain = set()\n            fine_grained = False\n            out_channels = None\n            # A flag that represents if all the layers in\n            # the dependency set are pruned\n            all_pruned = True\n            for name in dset:\n                if name not in self.masks:\n                    # this layer is not pruned\n                    all_pruned = False\n                    continue\n                w_mask = self.masks[name]['weight']\n                if out_channels is None:\n                    out_channels = w_mask.size(0)\n                shape = w_mask.size()\n                count = np.prod(shape[1:])\n                all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n                all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n                if len(all_ones) + len(all_zeros) < w_mask.size(0):\n                    # In fine-grained pruning, there is no need to check\n                    # the shape conflict\n                    _logger.info('Layers %s using fine-grained pruning', ','.join(dset))\n                    fine_grained = True\n                    break\n                channel_remain.update(all_ones)\n                _logger.debug('Layer: %s ', name)\n                _logger.debug('Original pruned filters: %s', str(all_zeros))\n            # Update the masks for the layers in the dependency set\n            if fine_grained or out_channels is None:\n                # if use the fine-grained pruner or all the layers in\n                # this dependency set are not pruned\n                continue\n            if not all_pruned:\n                # if some layer are not pruned at all\n                # then all the layers in this dependency set\n                # cannot be pruned due to the shape dependency.\n                channel_remain.update(range(out_channels))\n            ori_channels = 0\n            for name in dset:\n                if name not in self.masks:\n                    # this layer is not pruned at all\n                    # in this case, all_pruned is False\n                    # and the other layers in the same dset\n                    # will not be pruned either.\n                    continue\n                mask = self.masks[name]\n                w_shape = mask['weight'].size()\n                ori_channels = w_shape[0]\n                for i in channel_remain:\n                    mask['weight'][i] = torch.ones(w_shape[1:])\n                    if hasattr(mask, 'bias'):\n                        mask['bias'][i] = 1\n            _logger.info(','.join(dset))\n            _logger.info('Pruned Filters after fixing conflict:')\n            pruned_filters = set(list(range(ori_channels)))-channel_remain\n            _logger.info(str(sorted(pruned_filters)))\n        return self.masks",
  "class NaiveQuantizer(Quantizer):\n    \"\"\"quantize weight to 8 bits\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.layer_scale = {}\n\n    def validate_config(self, model, config_list):\n        schema = CompressorSchema([{\n            Optional('quant_types'): ['weight'],\n            Optional('quant_bits'): Or(8, {'weight': 8}),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        new_scale = weight.abs().max() / 127\n        scale = max(self.layer_scale.get(wrapper.name, 0), new_scale)\n        self.layer_scale[wrapper.name] = scale\n        orig_type = weight.type()  # TODO: user layer\n        return weight.div(scale).type(torch.int8).type(orig_type).mul(scale)",
  "def update_ema(biased_ema, value, decay, step):\n    \"\"\"\n    calculate biased stat and unbiased stat in each step using exponential moving average method\n\n    Parameters\n    ----------\n    biased_ema : float\n        previous stat value\n    value : float\n        current stat value\n    decay : float\n        the weight of previous stat value, larger means smoother curve\n    step : int\n        current step\n\n    Returns\n    -------\n    float, float\n    \"\"\"\n    biased_ema = biased_ema * decay + (1 - decay) * value\n    unbiased_ema = biased_ema / (1 - decay ** step)  # Bias correction\n    return biased_ema, unbiased_ema",
  "def update_quantization_param(bits, rmin, rmax):\n    \"\"\"\n    calculate the `zero_point` and `scale`.\n\n    Parameters\n    ----------\n    bits : int\n        quantization bits length\n    rmin : float\n        min value of real value\n    rmax : float\n        max value of real value\n\n    Returns\n    -------\n    float, float\n    \"\"\"\n    # extend the [min, max] interval to ensure that it contains 0.\n    # Otherwise, we would not meet the requirement that 0 be an exactly\n    # representable value.\n    rmin = min(rmin, 0)\n    rmax = max(rmax, 0)\n\n    # the min and max quantized values, as floating-point values\n    qmin = 0\n    qmax = (1 << bits) - 1\n    # First determine the scale.\n    scale = (rmax - rmin) / (qmax - qmin)\n\n    # Zero-point computation.\n    initial_zero_point = qmin - rmin / scale\n\n    # Now we need to nudge the zero point to be an integer\n    nudged_zero_point = 0\n    if initial_zero_point < qmin:\n        nudged_zero_point = qmin\n    elif initial_zero_point > qmax:\n        nudged_zero_point = qmax\n    else:\n        nudged_zero_point = torch.round(initial_zero_point)\n\n    return scale, nudged_zero_point",
  "def get_bits_length(config, quant_type):\n    if isinstance(config[\"quant_bits\"], int):\n        return config[\"quant_bits\"]\n    else:\n        return config[\"quant_bits\"].get(quant_type)",
  "class QAT_Quantizer(Quantizer):\n    \"\"\"Quantizer defined in:\n    Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\n    http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        \"\"\"\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to quantize\n        config_list : list of dict\n            list of configurations for quantization\n            supported keys for dict:\n                - quant_types : list of string\n                    type of quantization you want to apply, currently support 'weight', 'input', 'output'\n                - quant_bits : int or dict of {str : int}\n                    bits length of quantization, key is the quantization type, value is the length, eg. {'weight', 8},\n                    when the type is int, all quantization types share same bits length\n                - quant_start_step : int\n                    disable quantization until model are run by certain number of steps, this allows the network to enter a more stable\n                    state where activation quantization ranges do not exclude a signi\ufb01cant fraction of values, default value is 0\n                - op_types : list of string\n                    types of nn.module you want to apply quantization, eg. 'Conv2d'\n        \"\"\"\n        super().__init__(model, config_list, optimizer)\n        self.steps = 1\n        modules_to_compress = self.get_modules_to_compress()\n        for layer, config in modules_to_compress:\n            layer.module.register_buffer(\"zero_point\", None)\n            layer.module.register_buffer(\"scale\", None)\n            if \"output\" in config.get(\"quant_types\", []):\n                layer.module.register_buffer('ema_decay', torch.Tensor([0.99]))\n                layer.module.register_buffer('tracked_min_biased', torch.zeros(1))\n                layer.module.register_buffer('tracked_min', torch.zeros(1))\n                layer.module.register_buffer('tracked_max_biased', torch.zeros(1))\n                layer.module.register_buffer('tracked_max', torch.zeros(1))\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        \"\"\"\n        schema = CompressorSchema([{\n            Optional('quant_types'): Schema([lambda x: x in ['weight', 'output']]),\n            Optional('quant_bits'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional('weight'): And(int, lambda n: 0 < n < 32),\n                Optional('output'): And(int, lambda n: 0 < n < 32),\n            })),\n            Optional('quant_start_step'): And(int, lambda n: n >= 0),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def _quantize(self, bits, op, real_val):\n        \"\"\"\n        quantize real value.\n\n        Parameters\n        ----------\n        bits : int\n            quantization bits length\n        op : torch.nn.Module\n            target module\n        real_val : float\n            real value to be quantized\n\n        Returns\n        -------\n        float\n        \"\"\"\n        transformed_val = op.zero_point + real_val / op.scale\n        qmin = 0\n        qmax = (1 << bits) - 1\n        clamped_val = torch.clamp(transformed_val, qmin, qmax)\n        quantized_val = torch.round(clamped_val)\n        return quantized_val\n\n    def _dequantize(self, op, quantized_val):\n        \"\"\"\n        dequantize quantized value.\n        Because we simulate quantization in training process, all the computations still happen as float point computations, which means we\n        first quantize tensors then dequantize them. For more details, please refer to the paper.\n\n        Parameters\n        ----------\n        op : torch.nn.Module\n            target module\n        quantized_val : float\n            quantized_val value to be dequantized\n\n        Returns\n        -------\n        float\n        \"\"\"\n        real_val = op.scale * (quantized_val - op.zero_point)\n        return real_val\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        config = wrapper.config\n        module = wrapper.module\n        weight_bits = get_bits_length(config, 'weight')\n        quant_start_step = config.get('quant_start_step', 0)\n        assert weight_bits >= 1, \"quant bits length should be at least 1\"\n\n        if quant_start_step > self.steps:\n            return weight\n        rmin, rmax = torch.min(weight), torch.max(weight)\n        module.scale, module.zero_point = update_quantization_param(weight_bits, rmin, rmax)\n        out = self._quantize(weight_bits, module, weight)\n        out = self._dequantize(module, out)\n        return out\n\n    def quantize_output(self, output, wrapper, **kwargs):\n        config = wrapper.config\n        module = wrapper.module\n        output_bits = get_bits_length(config, 'output')\n        quant_start_step = config.get('quant_start_step', 0)\n        assert output_bits >= 1, \"quant bits length should be at least 1\"\n\n        if quant_start_step > self.steps:\n            return output\n\n        current_min, current_max = torch.min(output), torch.max(output)\n        module.tracked_min_biased, module.tracked_min = update_ema(module.tracked_min_biased, current_min, module.ema_decay, self.steps)\n        module.tracked_max_biased, module.tracked_max = update_ema(module.tracked_max_biased, current_max, module.ema_decay, self.steps)\n        module.scale, module.zero_point = update_quantization_param(output_bits, module.tracked_min, module.tracked_max)\n        out = self._quantize(output_bits, module, output)\n        out = self._dequantize(module, out)\n        return out\n\n    def fold_bn(self, config, **kwargs):\n        # TODO simulate folded weight\n        pass\n\n    def step_with_optimizer(self):\n        \"\"\"\n        override `compressor` `step` method, quantization only happens after certain number of steps\n        \"\"\"\n        self.steps += 1",
  "class DoReFaQuantizer(Quantizer):\n    \"\"\"Quantizer using the DoReFa scheme, as defined in:\n    Zhou et al., DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\n    (https://arxiv.org/abs/1606.06160)\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        \"\"\"\n        schema = CompressorSchema([{\n            Optional('quant_types'): Schema([lambda x: x in ['weight']]),\n            Optional('quant_bits'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional('weight'): And(int, lambda n: 0 < n < 32)\n            })),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        weight_bits = get_bits_length(wrapper.config, 'weight')\n        out = weight.tanh()\n        out = out / (2 * out.abs().max()) + 0.5\n        out = self.quantize(out, weight_bits)\n        out = 2 * out -1\n        return out\n\n    def quantize(self, input_ri, q_bits):\n        scale = pow(2, q_bits)-1\n        output = torch.round(input_ri*scale)/scale\n        return output",
  "class ClipGrad(QuantGrad):\n    @staticmethod\n    def quant_backward(tensor, grad_output, quant_type):\n        if quant_type == QuantType.QUANT_OUTPUT:\n            grad_output[torch.abs(tensor) > 1] = 0\n        return grad_output",
  "class BNNQuantizer(Quantizer):\n    \"\"\"Binarized Neural Networks, as defined in:\n    Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\n    (https://arxiv.org/abs/1602.02830)\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.quant_grad = ClipGrad\n\n    def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        \"\"\"\n        schema = CompressorSchema([{\n            Optional('quant_types'): Schema([lambda x: x in ['weight', 'output']]),\n            Optional('quant_bits'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional('weight'): And(int, lambda n: 0 < n < 32),\n                Optional('output'): And(int, lambda n: 0 < n < 32),\n            })),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        out = torch.sign(weight)\n        # remove zeros\n        out[out == 0] = 1\n        return out\n\n    def quantize_output(self, output, wrapper, **kwargs):\n        out = torch.sign(output)\n        # remove zeros\n        out[out == 0] = 1\n        return out",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.layer_scale = {}",
  "def validate_config(self, model, config_list):\n        schema = CompressorSchema([{\n            Optional('quant_types'): ['weight'],\n            Optional('quant_bits'): Or(8, {'weight': 8}),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)",
  "def quantize_weight(self, weight, wrapper, **kwargs):\n        new_scale = weight.abs().max() / 127\n        scale = max(self.layer_scale.get(wrapper.name, 0), new_scale)\n        self.layer_scale[wrapper.name] = scale\n        orig_type = weight.type()  # TODO: user layer\n        return weight.div(scale).type(torch.int8).type(orig_type).mul(scale)",
  "def __init__(self, model, config_list, optimizer=None):\n        \"\"\"\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to quantize\n        config_list : list of dict\n            list of configurations for quantization\n            supported keys for dict:\n                - quant_types : list of string\n                    type of quantization you want to apply, currently support 'weight', 'input', 'output'\n                - quant_bits : int or dict of {str : int}\n                    bits length of quantization, key is the quantization type, value is the length, eg. {'weight', 8},\n                    when the type is int, all quantization types share same bits length\n                - quant_start_step : int\n                    disable quantization until model are run by certain number of steps, this allows the network to enter a more stable\n                    state where activation quantization ranges do not exclude a signi\ufb01cant fraction of values, default value is 0\n                - op_types : list of string\n                    types of nn.module you want to apply quantization, eg. 'Conv2d'\n        \"\"\"\n        super().__init__(model, config_list, optimizer)\n        self.steps = 1\n        modules_to_compress = self.get_modules_to_compress()\n        for layer, config in modules_to_compress:\n            layer.module.register_buffer(\"zero_point\", None)\n            layer.module.register_buffer(\"scale\", None)\n            if \"output\" in config.get(\"quant_types\", []):\n                layer.module.register_buffer('ema_decay', torch.Tensor([0.99]))\n                layer.module.register_buffer('tracked_min_biased', torch.zeros(1))\n                layer.module.register_buffer('tracked_min', torch.zeros(1))\n                layer.module.register_buffer('tracked_max_biased', torch.zeros(1))\n                layer.module.register_buffer('tracked_max', torch.zeros(1))",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        \"\"\"\n        schema = CompressorSchema([{\n            Optional('quant_types'): Schema([lambda x: x in ['weight', 'output']]),\n            Optional('quant_bits'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional('weight'): And(int, lambda n: 0 < n < 32),\n                Optional('output'): And(int, lambda n: 0 < n < 32),\n            })),\n            Optional('quant_start_step'): And(int, lambda n: n >= 0),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)",
  "def _quantize(self, bits, op, real_val):\n        \"\"\"\n        quantize real value.\n\n        Parameters\n        ----------\n        bits : int\n            quantization bits length\n        op : torch.nn.Module\n            target module\n        real_val : float\n            real value to be quantized\n\n        Returns\n        -------\n        float\n        \"\"\"\n        transformed_val = op.zero_point + real_val / op.scale\n        qmin = 0\n        qmax = (1 << bits) - 1\n        clamped_val = torch.clamp(transformed_val, qmin, qmax)\n        quantized_val = torch.round(clamped_val)\n        return quantized_val",
  "def _dequantize(self, op, quantized_val):\n        \"\"\"\n        dequantize quantized value.\n        Because we simulate quantization in training process, all the computations still happen as float point computations, which means we\n        first quantize tensors then dequantize them. For more details, please refer to the paper.\n\n        Parameters\n        ----------\n        op : torch.nn.Module\n            target module\n        quantized_val : float\n            quantized_val value to be dequantized\n\n        Returns\n        -------\n        float\n        \"\"\"\n        real_val = op.scale * (quantized_val - op.zero_point)\n        return real_val",
  "def quantize_weight(self, weight, wrapper, **kwargs):\n        config = wrapper.config\n        module = wrapper.module\n        weight_bits = get_bits_length(config, 'weight')\n        quant_start_step = config.get('quant_start_step', 0)\n        assert weight_bits >= 1, \"quant bits length should be at least 1\"\n\n        if quant_start_step > self.steps:\n            return weight\n        rmin, rmax = torch.min(weight), torch.max(weight)\n        module.scale, module.zero_point = update_quantization_param(weight_bits, rmin, rmax)\n        out = self._quantize(weight_bits, module, weight)\n        out = self._dequantize(module, out)\n        return out",
  "def quantize_output(self, output, wrapper, **kwargs):\n        config = wrapper.config\n        module = wrapper.module\n        output_bits = get_bits_length(config, 'output')\n        quant_start_step = config.get('quant_start_step', 0)\n        assert output_bits >= 1, \"quant bits length should be at least 1\"\n\n        if quant_start_step > self.steps:\n            return output\n\n        current_min, current_max = torch.min(output), torch.max(output)\n        module.tracked_min_biased, module.tracked_min = update_ema(module.tracked_min_biased, current_min, module.ema_decay, self.steps)\n        module.tracked_max_biased, module.tracked_max = update_ema(module.tracked_max_biased, current_max, module.ema_decay, self.steps)\n        module.scale, module.zero_point = update_quantization_param(output_bits, module.tracked_min, module.tracked_max)\n        out = self._quantize(output_bits, module, output)\n        out = self._dequantize(module, out)\n        return out",
  "def fold_bn(self, config, **kwargs):\n        # TODO simulate folded weight\n        pass",
  "def step_with_optimizer(self):\n        \"\"\"\n        override `compressor` `step` method, quantization only happens after certain number of steps\n        \"\"\"\n        self.steps += 1",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        \"\"\"\n        schema = CompressorSchema([{\n            Optional('quant_types'): Schema([lambda x: x in ['weight']]),\n            Optional('quant_bits'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional('weight'): And(int, lambda n: 0 < n < 32)\n            })),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)",
  "def quantize_weight(self, weight, wrapper, **kwargs):\n        weight_bits = get_bits_length(wrapper.config, 'weight')\n        out = weight.tanh()\n        out = out / (2 * out.abs().max()) + 0.5\n        out = self.quantize(out, weight_bits)\n        out = 2 * out -1\n        return out",
  "def quantize(self, input_ri, q_bits):\n        scale = pow(2, q_bits)-1\n        output = torch.round(input_ri*scale)/scale\n        return output",
  "def quant_backward(tensor, grad_output, quant_type):\n        if quant_type == QuantType.QUANT_OUTPUT:\n            grad_output[torch.abs(tensor) > 1] = 0\n        return grad_output",
  "def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.quant_grad = ClipGrad",
  "def validate_config(self, model, config_list):\n        \"\"\"\n        Parameters\n        ----------\n        model : torch.nn.Module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        \"\"\"\n        schema = CompressorSchema([{\n            Optional('quant_types'): Schema([lambda x: x in ['weight', 'output']]),\n            Optional('quant_bits'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional('weight'): And(int, lambda n: 0 < n < 32),\n                Optional('output'): And(int, lambda n: 0 < n < 32),\n            })),\n            Optional('op_types'): [str],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)",
  "def quantize_weight(self, weight, wrapper, **kwargs):\n        out = torch.sign(weight)\n        # remove zeros\n        out[out == 0] = 1\n        return out",
  "def quantize_output(self, output, wrapper, **kwargs):\n        out = torch.sign(output)\n        # remove zeros\n        out[out == 0] = 1\n        return out",
  "def get_module_by_name(model, module_name):\n    \"\"\"\n    Get a module specified by its module name\n\n    Parameters\n    ----------\n    model : pytorch model\n        the pytorch model from which to get its module\n    module_name : str\n        the name of the required module\n\n    Returns\n    -------\n    module, module\n        the parent module of the required module, the required module\n    \"\"\"\n    name_list = module_name.split(\".\")\n    for name in name_list[:-1]:\n        model = getattr(model, name)\n    leaf_module = getattr(model, name_list[-1])\n    return model, leaf_module",
  "class ModelSpeedup:\n    \"\"\"\n    This class is to speedup the model with provided weight mask\n    \"\"\"\n\n    def __init__(self, model, dummy_input, masks_file, map_location=None):\n        \"\"\"\n        Parameters\n        ----------\n        model : pytorch model\n            The model user wants to speed up\n        dummy_input : pytorch tensor\n            The dummy input for ```jit.trace```, users should put it on right device before pass in\n        masks_file : str\n            The path of user provided mask file\n        map_location : str\n            the device on which masks are placed, same to map_location in ```torch.load```\n        \"\"\"\n        from nni._graph_utils import build_module_graph\n\n        self.bound_model = model\n        self.masks = torch.load(masks_file, map_location)\n        self.inferred_masks = dict() # key: module_name, value: ModuleMasks\n        self.dummy_input = dummy_input\n        self.torch_graph = build_module_graph(model, dummy_input)\n\n    def infer_module_mask(self, module_name, last_module, mask=None, in_shape=None, out_shape=None):\n        \"\"\"\n        Infer input shape / output shape based on the module's weight mask / input shape / output shape.\n\n        For a module:\n            Infer its input and output shape from its weight mask\n            Infer its output shape from its input shape\n            Infer its input shape from its output shape\n\n        If its input shape is changed, continue infering its predecessors\n        If its output shape is changed, continue infering its successors\n\n        Parameters\n        ----------\n        module_name : str\n            The name of the node\n        last_module : str\n            The name of last visited node\n        mask : tensor of mask or ModuleMasks\n            Mask of the weights in this node (i.e., module)\n        in_shape : ModuleMasks\n            Input shape of this node\n        out_shape : ModuleMasks\n            Output shape of this node\n        \"\"\"\n        input_cmask = output_cmask = None\n        if module_name in self.inferred_masks:\n            module_masks = self.inferred_masks[module_name]\n        else:\n            module_masks = ModuleMasks(module_name)\n            self.inferred_masks[module_name] = module_masks\n\n        m_type = self.torch_graph.name_to_node[module_name].op_type\n        _logger.debug(\"infer mask of module %s with op_type %s\", module_name, m_type)\n        if mask is not None:\n            _logger.debug(\"mask is not None\")\n            if not m_type in infer_from_mask:\n                raise RuntimeError(\n                    \"Has not supported infering input/output shape from mask for module/function: `{}`, {}\"\n                    .format(m_type, module_name))\n            input_cmask, output_cmask = infer_from_mask[m_type](module_masks, mask)\n        if in_shape is not None:\n            _logger.debug(\"in_shape is not None\")\n            if not m_type in infer_from_inshape:\n                raise RuntimeError(\n                    \"Has not supported infering output shape from input shape for module/function: `{}`, {}\"\n                    .format(m_type, module_name))\n            if m_type in ['aten::view', 'aten::flatten', 'aten::mean', 'aten::reshape']:\n                output_cmask = infer_from_inshape[m_type](module_masks,\n                                                          in_shape,\n                                                          self.torch_graph.name_to_node[module_name].auxiliary)\n            elif m_type in ['aten::cat']:\n                # To calculate the mask for concat operation, the output shape\n                # , cat dimension, and the order of the input parameters.\n                output_cmask = infer_from_inshape[m_type](module_masks,\n                                                          in_shape,\n                                                          self.torch_graph.name_to_node[module_name].auxiliary,\n                                                          last_module)\n            else:\n                output_cmask = infer_from_inshape[m_type](module_masks, in_shape)\n        if out_shape is not None:\n            _logger.debug(\"out_shape is not None\")\n            if not m_type in infer_from_outshape:\n                raise RuntimeError(\n                    \"Has not supported infering input shape from output shape for module/function: `{}`, {}\"\n                    .format(m_type, module_name))\n            input_cmask = infer_from_outshape[m_type](module_masks, out_shape)\n\n        if input_cmask:\n            predecessors = self.torch_graph.find_predecessors(module_name)\n            for _module_name in predecessors:\n                self.infer_module_mask(_module_name, module_name, out_shape=input_cmask)\n        if output_cmask:\n            successors = self.torch_graph.find_successors(module_name)\n            for _module_name in successors:\n                self.infer_module_mask(_module_name, module_name, in_shape=output_cmask)\n\n    def infer_modules_masks(self):\n        \"\"\"\n        Do shape inference of involved modules, including the shape of weights, inputs, output\n        \"\"\"\n        for module_name, mask in self.masks.items():\n            _logger.debug('Start mask inference from %s', module_name)\n            if module_name not in self.torch_graph.name_to_node:\n                # this module is not traced in the torch_graph,\n                # jit.trace only correctly records functions and\n                # modules which are not data dependent (e.g., do\n                # not have conditionals on data in tensors)\n                # so, if a node is not traced, we just skip it.\n                _logger.warning('%s has mask, but not found in the traced graph, just skip it.', module_name)\n                continue\n            self.infer_module_mask(module_name, None, mask=mask)\n\n    def replace_compressed_modules(self):\n        \"\"\"\n        Replace all the modules that have changed (weights/inputs/output) shape.\n        The new module is created using the same arguments of the to-be-replaced module,\n        and correctly inherits its weights.\n\n        NOTE: ```func``` type cannot be replaced as it is not a module, thus, one limitation\n        is that ```func``` should be not required to be replaced.\n        \"\"\"\n        for module_name in self.inferred_masks:\n            g_node = self.torch_graph.name_to_node[module_name]\n            _logger.debug(\"replace %s, in %s type, with op_type %s\",\n                          module_name, g_node.type, g_node.op_type)\n            if g_node.type == 'module':\n                super_module, leaf_module = get_module_by_name(self.bound_model, g_node.name)\n                m_type = g_node.op_type\n                if not m_type in replace_module:\n                    raise RuntimeError(\"Has not supported replacing the module: `{}`\".format(m_type))\n                _logger.info(\"replace module (name: %s, op_type: %s)\", g_node.name, m_type)\n                compressed_module = replace_module[m_type](leaf_module, self.inferred_masks[module_name])\n                setattr(super_module, g_node.name.split('.')[-1], compressed_module)\n            elif g_node.type == 'func':\n                _logger.info(\"Warning: cannot replace (name: %s, op_type: %s) which is func type\",\n                             module_name, g_node.op_type)\n            else:\n                raise RuntimeError(\"Unsupported node type: {}\".format(g_node.type))\n\n\n    def speedup_model(self):\n        \"\"\"\n        There are basically two steps:\n        first, do mask/shape inference,\n        second, replace modules\n        \"\"\"\n        training = self.bound_model.training\n        _logger.info(\"start to speed up the model\")\n        _logger.info(\"fix the mask conflict of the interdependent layers\")\n        fix_mask_conflict(self.masks, self.bound_model, self.dummy_input)\n        _logger.info(\"infer module masks...\")\n        self.infer_modules_masks()\n        _logger.info(\"replace compressed modules...\")\n        self.replace_compressed_modules()\n        self.bound_model.train(training)\n        _logger.info(\"speedup done\")",
  "def __init__(self, model, dummy_input, masks_file, map_location=None):\n        \"\"\"\n        Parameters\n        ----------\n        model : pytorch model\n            The model user wants to speed up\n        dummy_input : pytorch tensor\n            The dummy input for ```jit.trace```, users should put it on right device before pass in\n        masks_file : str\n            The path of user provided mask file\n        map_location : str\n            the device on which masks are placed, same to map_location in ```torch.load```\n        \"\"\"\n        from nni._graph_utils import build_module_graph\n\n        self.bound_model = model\n        self.masks = torch.load(masks_file, map_location)\n        self.inferred_masks = dict() # key: module_name, value: ModuleMasks\n        self.dummy_input = dummy_input\n        self.torch_graph = build_module_graph(model, dummy_input)",
  "def infer_module_mask(self, module_name, last_module, mask=None, in_shape=None, out_shape=None):\n        \"\"\"\n        Infer input shape / output shape based on the module's weight mask / input shape / output shape.\n\n        For a module:\n            Infer its input and output shape from its weight mask\n            Infer its output shape from its input shape\n            Infer its input shape from its output shape\n\n        If its input shape is changed, continue infering its predecessors\n        If its output shape is changed, continue infering its successors\n\n        Parameters\n        ----------\n        module_name : str\n            The name of the node\n        last_module : str\n            The name of last visited node\n        mask : tensor of mask or ModuleMasks\n            Mask of the weights in this node (i.e., module)\n        in_shape : ModuleMasks\n            Input shape of this node\n        out_shape : ModuleMasks\n            Output shape of this node\n        \"\"\"\n        input_cmask = output_cmask = None\n        if module_name in self.inferred_masks:\n            module_masks = self.inferred_masks[module_name]\n        else:\n            module_masks = ModuleMasks(module_name)\n            self.inferred_masks[module_name] = module_masks\n\n        m_type = self.torch_graph.name_to_node[module_name].op_type\n        _logger.debug(\"infer mask of module %s with op_type %s\", module_name, m_type)\n        if mask is not None:\n            _logger.debug(\"mask is not None\")\n            if not m_type in infer_from_mask:\n                raise RuntimeError(\n                    \"Has not supported infering input/output shape from mask for module/function: `{}`, {}\"\n                    .format(m_type, module_name))\n            input_cmask, output_cmask = infer_from_mask[m_type](module_masks, mask)\n        if in_shape is not None:\n            _logger.debug(\"in_shape is not None\")\n            if not m_type in infer_from_inshape:\n                raise RuntimeError(\n                    \"Has not supported infering output shape from input shape for module/function: `{}`, {}\"\n                    .format(m_type, module_name))\n            if m_type in ['aten::view', 'aten::flatten', 'aten::mean', 'aten::reshape']:\n                output_cmask = infer_from_inshape[m_type](module_masks,\n                                                          in_shape,\n                                                          self.torch_graph.name_to_node[module_name].auxiliary)\n            elif m_type in ['aten::cat']:\n                # To calculate the mask for concat operation, the output shape\n                # , cat dimension, and the order of the input parameters.\n                output_cmask = infer_from_inshape[m_type](module_masks,\n                                                          in_shape,\n                                                          self.torch_graph.name_to_node[module_name].auxiliary,\n                                                          last_module)\n            else:\n                output_cmask = infer_from_inshape[m_type](module_masks, in_shape)\n        if out_shape is not None:\n            _logger.debug(\"out_shape is not None\")\n            if not m_type in infer_from_outshape:\n                raise RuntimeError(\n                    \"Has not supported infering input shape from output shape for module/function: `{}`, {}\"\n                    .format(m_type, module_name))\n            input_cmask = infer_from_outshape[m_type](module_masks, out_shape)\n\n        if input_cmask:\n            predecessors = self.torch_graph.find_predecessors(module_name)\n            for _module_name in predecessors:\n                self.infer_module_mask(_module_name, module_name, out_shape=input_cmask)\n        if output_cmask:\n            successors = self.torch_graph.find_successors(module_name)\n            for _module_name in successors:\n                self.infer_module_mask(_module_name, module_name, in_shape=output_cmask)",
  "def infer_modules_masks(self):\n        \"\"\"\n        Do shape inference of involved modules, including the shape of weights, inputs, output\n        \"\"\"\n        for module_name, mask in self.masks.items():\n            _logger.debug('Start mask inference from %s', module_name)\n            if module_name not in self.torch_graph.name_to_node:\n                # this module is not traced in the torch_graph,\n                # jit.trace only correctly records functions and\n                # modules which are not data dependent (e.g., do\n                # not have conditionals on data in tensors)\n                # so, if a node is not traced, we just skip it.\n                _logger.warning('%s has mask, but not found in the traced graph, just skip it.', module_name)\n                continue\n            self.infer_module_mask(module_name, None, mask=mask)",
  "def replace_compressed_modules(self):\n        \"\"\"\n        Replace all the modules that have changed (weights/inputs/output) shape.\n        The new module is created using the same arguments of the to-be-replaced module,\n        and correctly inherits its weights.\n\n        NOTE: ```func``` type cannot be replaced as it is not a module, thus, one limitation\n        is that ```func``` should be not required to be replaced.\n        \"\"\"\n        for module_name in self.inferred_masks:\n            g_node = self.torch_graph.name_to_node[module_name]\n            _logger.debug(\"replace %s, in %s type, with op_type %s\",\n                          module_name, g_node.type, g_node.op_type)\n            if g_node.type == 'module':\n                super_module, leaf_module = get_module_by_name(self.bound_model, g_node.name)\n                m_type = g_node.op_type\n                if not m_type in replace_module:\n                    raise RuntimeError(\"Has not supported replacing the module: `{}`\".format(m_type))\n                _logger.info(\"replace module (name: %s, op_type: %s)\", g_node.name, m_type)\n                compressed_module = replace_module[m_type](leaf_module, self.inferred_masks[module_name])\n                setattr(super_module, g_node.name.split('.')[-1], compressed_module)\n            elif g_node.type == 'func':\n                _logger.info(\"Warning: cannot replace (name: %s, op_type: %s) which is func type\",\n                             module_name, g_node.op_type)\n            else:\n                raise RuntimeError(\"Unsupported node type: {}\".format(g_node.type))",
  "def speedup_model(self):\n        \"\"\"\n        There are basically two steps:\n        first, do mask/shape inference,\n        second, replace modules\n        \"\"\"\n        training = self.bound_model.training\n        _logger.info(\"start to speed up the model\")\n        _logger.info(\"fix the mask conflict of the interdependent layers\")\n        fix_mask_conflict(self.masks, self.bound_model, self.dummy_input)\n        _logger.info(\"infer module masks...\")\n        self.infer_modules_masks()\n        _logger.info(\"replace compressed modules...\")\n        self.replace_compressed_modules()\n        self.bound_model.train(training)\n        _logger.info(\"speedup done\")",
  "def no_replace(module, mask):\n    \"\"\"\n    No need to replace\n    \"\"\"\n    _logger.debug(\"no need to replace\")\n    return module",
  "def replace_linear(linear, mask):\n    \"\"\"\n    Parameters\n    ----------\n    linear : torch.nn.Linear\n        The linear module to be replace\n    mask : ModuleMasks\n        The masks of this module\n\n    Returns\n    -------\n    torch.nn.Linear\n        The new linear module\n    \"\"\"\n    assert isinstance(mask, ModuleMasks)\n    assert mask.input_mask is not None\n    assert mask.output_mask is None\n    assert not mask.param_masks\n    index = mask.input_mask.mask_index[-1]\n    in_features = index.size()[0]\n    _logger.debug(\"replace linear with new in_features: %d\", in_features)\n    new_linear = torch.nn.Linear(in_features=in_features,\n                                 out_features=linear.out_features,\n                                 bias=linear.bias is not None)\n    new_linear.to(linear.weight.device)\n    new_linear.weight.data = torch.index_select(linear.weight.data, -1, index.to(linear.weight.device))\n    if linear.bias is not None:\n        new_linear.bias.data.copy_(linear.bias.data)\n    return new_linear",
  "def replace_batchnorm2d(norm, mask):\n    \"\"\"\n    Parameters\n    ----------\n    norm : torch.nn.BatchNorm2d\n        The batchnorm module to be replace\n    mask : ModuleMasks\n        The masks of this module\n\n    Returns\n    -------\n    torch.nn.BatchNorm2d\n        The new batchnorm module\n    \"\"\"\n    assert isinstance(mask, ModuleMasks)\n    assert 'weight' in mask.param_masks and 'bias' in mask.param_masks\n    index = mask.param_masks['weight'].mask_index[0]\n    num_features = index.size()[0]\n    _logger.debug(\"replace batchnorm2d with num_features: %d\", num_features)\n    new_norm = torch.nn.BatchNorm2d(num_features=num_features,\n                                    eps=norm.eps,\n                                    momentum=norm.momentum,\n                                    affine=norm.affine,\n                                    track_running_stats=norm.track_running_stats)\n    # assign weights\n    new_norm.weight.data = torch.index_select(norm.weight.data, 0, index)\n    new_norm.bias.data = torch.index_select(norm.bias.data, 0, index)\n    if norm.track_running_stats:\n        new_norm.running_mean.data = torch.index_select(norm.running_mean.data, 0, index)\n        new_norm.running_var.data = torch.index_select(norm.running_var.data, 0, index)\n    return new_norm",
  "def replace_conv2d(conv, mask):\n    \"\"\"\n    Parameters\n    ----------\n    conv : torch.nn.Conv2d\n        The conv2d module to be replaced\n    mask : ModuleMasks\n        The masks of this module\n\n    Returns\n    -------\n    torch.nn.Conv2d\n        The new conv2d module\n    \"\"\"\n    assert isinstance(mask, ModuleMasks)\n    if mask.input_mask is None:\n        in_channels = conv.in_channels\n    else:\n        in_channels_index = mask.input_mask.mask_index[1]\n        in_channels = in_channels_index.size()[0]\n    if mask.output_mask is None:\n        out_channels = conv.out_channels\n    else:\n        out_channels_index = mask.output_mask.mask_index[1]\n        out_channels = out_channels_index.size()[0]\n\n    _logger.debug(\"replace conv2d with in_channels: %d, out_channels: %d\", in_channels, out_channels)\n    new_conv = torch.nn.Conv2d(in_channels=in_channels,\n                               out_channels=out_channels,\n                               kernel_size=conv.kernel_size,\n                               stride=conv.stride,\n                               padding=conv.padding,\n                               dilation=conv.dilation,\n                               groups=conv.groups,\n                               bias=conv.bias is not None,\n                               padding_mode=conv.padding_mode)\n\n    new_conv.to(conv.weight.device)\n    tmp_weight_data = tmp_bias_data = None\n\n    if mask.output_mask is not None:\n        tmp_weight_data = torch.index_select(conv.weight.data, 0, out_channels_index)\n        if conv.bias is not None:\n            tmp_bias_data = torch.index_select(conv.bias.data, 0, out_channels_index)\n    else:\n        tmp_weight_data = conv.weight.data\n    # For the convolutional layers that have more than one group\n    # we need to copy the weight group by group, because the input\n    # channal is also divided into serveral groups and each group\n    # filter may have different input channel indexes.\n    input_step = int(conv.in_channels / conv.groups)\n    in_channels_group = int(in_channels / conv.groups)\n    filter_step = int(out_channels / conv.groups)\n    if mask.input_mask is not None:\n        for groupid in range(conv.groups):\n            start = groupid * input_step\n            end = (groupid + 1) * input_step\n            current_input_index = list(filter(lambda x: start <= x and x < end, in_channels_index.tolist()))\n            # shift the global index into the group index\n            current_input_index = [x-start for x in current_input_index]\n            # if the groups is larger than 1, the input channels of each\n            # group should be pruned evenly.\n            assert len(current_input_index) == in_channels_group, \\\n                'Input channels of each group are not pruned evenly'\n            current_input_index = torch.tensor(current_input_index).to(tmp_weight_data.device) # pylint: disable=not-callable\n            f_start = groupid * filter_step\n            f_end = (groupid + 1) * filter_step\n            new_conv.weight.data[f_start:f_end] = torch.index_select(tmp_weight_data[f_start:f_end], 1, current_input_index)\n    else:\n        new_conv.weight.data.copy_(tmp_weight_data)\n\n    if conv.bias is not None:\n        new_conv.bias.data.copy_(conv.bias.data if tmp_bias_data is None else tmp_bias_data)\n\n    return new_conv",
  "class CoarseMask:\n    \"\"\"\n    Coarse grained mask for a given tensor, here tensor could be weights,\n    input tensor, or output tensor\n    \"\"\"\n\n    def __init__(self, num_dim):\n        \"\"\"\n        Parameters\n        ----------\n        num_dim : int\n            The number of dimensions of the tensor that will be masked\n        \"\"\"\n        self.mask_index = [None for _ in range(num_dim)]\n\n    def add_index_mask(self, dim, index):\n        \"\"\"\n        Add mask for the specified dimension\n\n        Parameters\n        ----------\n        dim : int\n            The dimension to add mask\n        index : tensor\n            The mask for this dimension, its a 1 dimension tensor which specifies\n            the index of the elements that are not pruned\n        \"\"\"\n        self.mask_index[dim] = index\n\n    @staticmethod\n    def merge_index(index_a, index_b):\n        \"\"\"\n        Parameters\n        ----------\n        index_a : tensor\n            One index (1-dimension) tensor\n        index_b : tensor\n            The other index (1-dimension) tensor\n\n        Returns\n        -------\n        tensor\n            The merged index (1-dimension) tensor\n            Note that: the output tensor will be moved\n            to the same device as index_a.\n        \"\"\"\n        device = index_a.device\n        s = set()\n        for num in index_a.tolist():\n            # we need to transfer the tensor to list here\n            # first, directly traversing the tensor by for\n            # loop will return the list of tensor(x) object,\n            # even the value are the same, but they are different\n            # tensor objects, so the set will contains multiple\n            # tensor objects that has the same value. For example\n            # for num in torch.ones(2):\n            #   s.add(num)\n            # s will be {tensor(1), tensor(1)}\n            s.add(num)\n        for num in index_b.tolist():\n            s.add(num)\n        # move the output tensor to the same device with index_a\n        return torch.tensor(sorted(s)).to(device)  # pylint: disable=not-callable\n\n    def merge(self, cmask):\n        \"\"\"\n        Merge another CoarseMask\n\n        Parameters\n        ----------\n        cmask : CoarseMask\n            Another CoarseMask to merge\n\n        Returns\n        -------\n        list\n            The member variable ```mask_index```\n        \"\"\"\n        assert isinstance(cmask, CoarseMask)\n        assert len(self.mask_index) == len(cmask.mask_index), \\\n            \"Only masks with the same number of dimensions can be merged\"\n        for i, index in enumerate(self.mask_index):\n            if index is None:\n                self.mask_index[i] = cmask.mask_index[i]\n            elif cmask.mask_index[i] is not None:\n                self.mask_index[i] = CoarseMask.merge_index(self.mask_index[i],\n                                                            cmask.mask_index[i])\n        return self.mask_index\n\n    def __repr__(self):\n        return 'mask_index: {}'.format(self.mask_index)\n\n    def eq_on_dim(self, other, dim):\n        assert isinstance(other, CoarseMask)\n        if self.mask_index[dim] is None and other.mask_index[dim] is None:\n            return True\n        elif isinstance(self.mask_index[dim], torch.Tensor) \\\n                and isinstance(other.mask_index[dim], torch.Tensor):\n            return torch.equal(self.mask_index[dim], other.mask_index[dim])\n        else:\n            return False\n\n    def __eq__(self, other):\n        assert isinstance(other, CoarseMask)\n        if len(self.mask_index) != len(other.mask_index):\n            return False\n        for i in range(len(self.mask_index)):\n            if not self.eq_on_dim(other, i):\n                return False\n        return True\n\n    def __lt__(self, other):\n        \"\"\"\n        Judge if the mask is a subset of another CoarseMask.\n        \"\"\"\n        assert isinstance(other, CoarseMask)\n        for dim, _ in enumerate(self.mask_index):\n            # if self has more dimensions\n            if dim >= len(other.mask_index):\n                return False\n            if self.mask_index[dim] is None:\n                # if no mask on this dimension, then we have less\n                # masks then the other CoraseMask.\n                continue\n            elif other.mask_index[dim] is None:\n                return False\n            else:\n                s1 = set(self.mask_index[dim].tolist())\n                s2 = set(other.mask_index[dim].tolist())\n                if not s1 < s2:\n                    return False\n        return True\n\n    def __le__(self, other):\n        \"\"\"\n        Return if self's mask is less or equal to other's mask.\n        \"\"\"\n        assert isinstance(other, CoarseMask)\n        if self.__lt__(other) or self.__eq__(other):\n            return True\n        return False\n\n    def __ne__(self, other):\n        return not self.__eq__(other)",
  "class ModuleMasks:\n    \"\"\"\n    The masks of a module, including the masks for weights, inputs, output\n    \"\"\"\n\n    def __init__(self, module_name):\n        \"\"\"\n        Parameters\n        ----------\n        module_name : str\n            The name of the module or function\n        \"\"\"\n        self.module_name = module_name\n        self.param_masks = dict()\n        self.input_mask = None\n        self.output_mask = None\n\n    def set_param_masks(self, name, mask):\n        \"\"\"\n        Parameters\n        ----------\n        name : str\n            The name of the weight\n        mask : CoarseMask\n            The mask for this weight\n        \"\"\"\n        self.param_masks[name] = mask\n\n    def set_input_mask(self, mask):\n        \"\"\"\n        Parameters\n        ----------\n        mask : CoarseMask\n            The mask for input\n        \"\"\"\n        self.input_mask = mask\n\n    def set_output_mask(self, mask):\n        \"\"\"\n        Parameters\n        ----------\n        mask : CoarseMask\n            The mask for output\n        \"\"\"\n        self.output_mask = mask\n\n    def __repr__(self):\n        return 'input_mask: {}, output_mask: {}, param_masks: {}'.format(\n            self.input_mask, self.output_mask, self.param_masks\n        )",
  "def dropout_inshape(module_masks, mask):\n    if module_masks.input_mask is None:\n        module_masks.set_input_mask(mask)\n        module_masks.set_output_mask(mask)\n        return module_masks.output_mask\n    # if alreay visited\n    assert module_masks.input_mask <= mask\n    # It should be the same, we pass the masks by the reference(not the value),\n    # so they acutually are two references of the same object(mask,\n    # module_masks.input_mask). So we should continue pass the mask\n    # to the following nodes even module_masks.input_mask == mask.\n    # if pass the mask by copy.deepcopy(), then we can stop when\n    # module_masks.input_mask == mask.\n    # if module_masks.input_mask == mask:\n    #     return None\n    module_masks.set_input_mask(mask)\n    module_masks.set_output_mask(mask)\n    return module_masks.output_mask",
  "def cat_inshape(module_masks, mask, cat_info, last_visited):\n    \"\"\"\n    Inference the output mask of the cat operation from the\n    input mask.\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the Conv2d\n    mask : CoarseMask\n        The mask of its input tensor\n    cat_info: dict\n        Dict object that records the necessary information\n        of cat operation, such as the order of the input\n        tensors.\n    last_visited: str\n        The unique_name of the last visited node group.\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n\n    \"\"\"\n    assert isinstance(mask, CoarseMask)\n    out_shape = cat_info['out_shape']\n    cat_dim = cat_info['cat_dim']\n    in_order = cat_info['in_order']\n    in_shape = cat_info['in_shape']\n    if module_masks.output_mask is None:\n        # First visit to this cat node\n        # initialize the mask based on\n        # the number of the output channel.\n        output_mask = CoarseMask(num_dim=len(out_shape))\n        for dim, _ in enumerate(out_shape):\n            if dim == cat_dim:\n                if mask.mask_index[dim] is None:\n                    continue\n                device = mask.mask_index[dim].device\n                # calculate the offset of the mask\n                pos = in_order.index(last_visited)\n                offsets = [in_shape[i][cat_dim]\n                           for i, _ in enumerate(in_shape)]\n                offset = 0\n                for i in range(pos):\n                    offset += offsets[i]\n                _tmp_mask = (mask.mask_index[dim] + offset).to(device)\n                output_mask.mask_index[dim] = _tmp_mask\n            else:\n                # directly copy the mask\n                if mask.mask_index[dim] is not None:\n                    output_mask.mask_index[dim] = mask.mask_index[dim].data.clone(\n                    )\n        module_masks.set_output_mask(output_mask)\n\n        return module_masks.output_mask\n    # If this cat node is already visited, we need\n    # validating if the mask is legel, for cat operation,\n    # the mask on the 'cat_dim' dimension should be stitched\n    # together. In the other dimensions, the mask should be\n    # the same, else the mask is not legal.\n    for dim, _ in enumerate(out_shape):\n        if dim == cat_dim:\n            if mask.mask_index[dim] is None:\n                continue\n            pos = in_order.index(last_visited)\n            offsets = [in_shape[i][cat_dim] for i, _ in enumerate(in_shape)]\n            offset = 0\n            for i in range(pos):\n                offset += offsets[i]\n            device = mask.mask_index[dim].device\n            new_mask = mask.mask_index[dim] + offset\n            module_masks.output_mask.mask_index[dim] = CoarseMask.merge_index(\n                module_masks.output_mask.mask_index[dim], new_mask).to(device)\n        else:\n            assert module_masks.output_mask.eq_on_dim(mask, dim)\n\n    return module_masks.output_mask",
  "def add_inshape(module_masks, mask):\n    \"\"\"\n    Inference the output mask of the add operation from the\n    input mask.\n    \"\"\"\n    assert isinstance(mask, CoarseMask)\n    if module_masks.input_mask is None:\n        module_masks.set_input_mask(mask)\n        module_masks.set_output_mask(mask)\n        # module_masks.input_mask = mask\n        return mask\n    # If alreay visited, validate if have the conflict\n    # if the mask is different with previous input_mask\n    # then there is a mask confilct.\n    if mask != module_masks.input_mask:\n        raise Exception('Mask conflict happenes!')\n    return None",
  "def batchnorm2d_inshape(module_masks, mask):\n    \"\"\"\n    We assume only the second dimension has coarse grained mask\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the batchnorm2d\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    \"\"\"\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n    module_masks.set_input_mask(mask)\n    module_masks.set_output_mask(mask)\n    weight_cmask = CoarseMask(num_dim=1)\n    weight_cmask.add_index_mask(dim=0, index=mask.mask_index[1])\n    module_masks.set_param_masks('weight', weight_cmask)\n    module_masks.set_param_masks('bias', weight_cmask)\n    return mask",
  "def linear_inshape(module_masks, mask):\n    \"\"\"\n    Coarse grained input mask does not change the shape of weights and output tensor\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the linear\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor, ```None``` means shape of output tensor is not changed\n    \"\"\"\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[0] is None\n    if module_masks.input_mask is not None:\n        assert module_masks.input_mask <= mask\n    module_masks.set_input_mask(mask)\n    return None",
  "def view_inshape(module_masks, mask, shape):\n    \"\"\"\n    This is a limited support\n\n    TODO: consider replace tensor.view with nn.Flatten, because tensor.view is not\n    included in module, thus, cannot be replaced by our framework.\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the ```view``` op\n    mask : CoarseMask\n        The mask of its input tensor\n    shape : dict\n        Original shape of its input and output tensors\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    \"\"\"\n    # NOTE: the case constrained by the following four asserts\n    assert shape['in_shape'][0] == shape['out_shape'][0]\n    assert len(shape['in_shape']) == 4\n    assert len(shape['out_shape']) == 2\n    assert shape['out_shape'][1] == shape['in_shape'][1] * \\\n        shape['in_shape'][2]*shape['in_shape'][3]\n\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n    # due to the cat operation, the same node may be\n    # accessed more than once\n    if module_masks.input_mask is not None:\n        assert module_masks.input_mask <= mask\n    module_masks.set_input_mask(mask)\n    output_cmask = CoarseMask(num_dim=2)\n    index = []\n    step_size = shape['in_shape'][2] * shape['in_shape'][3]\n    for loc in mask.mask_index[1]:\n        index.extend([loc * step_size + i for i in range(step_size)])\n    output_cmask.add_index_mask(dim=1, index=torch.tensor(index))  # pylint: disable=not-callable\n    module_masks.set_output_mask(output_cmask)\n    return output_cmask",
  "def size_inshape(module_masks, mask):\n    \"\"\"\n    No need to do anything for this ```size``` op\n    \"\"\"\n    return None",
  "def mean_inshape(module_masks, mask, shape):\n    \"\"\"\n    Similar to view operation, currently mask inference only supports\n    the mean operation on the 3rd and 4th dimensions.\n    \"\"\"\n    assert shape['in_shape'][0] == shape['out_shape'][0]\n    assert shape['out_shape'][1] == shape['in_shape'][1]\n    assert len(shape['in_shape']) == 4\n    assert len(shape['out_shape']) == 2\n\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n    module_masks.set_input_mask(mask)\n\n    output_cmask = CoarseMask(num_dim=2)\n    output_cmask.add_index_mask(dim=1, index=mask.mask_index[1])\n    module_masks.set_output_mask(output_cmask)\n    return output_cmask",
  "def maxpool2d_inshape(module_masks, mask):\n    \"\"\"\n    Assume only the second dimension is masked\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the maxpool2d\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    \"\"\"\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n    if module_masks.input_mask is not None:\n        assert module_masks.input_mask <= mask\n    # assert module_masks.input_mask is None\n    module_masks.set_input_mask(mask)\n    module_masks.set_output_mask(mask)\n    return mask",
  "def relu_inshape(module_masks, mask):\n    \"\"\"\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the relu\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    \"\"\"\n    assert isinstance(mask, CoarseMask)\n    if module_masks.input_mask is not None:\n        # check if has a mask conflict\n        assert module_masks.input_mask <= mask\n    # assert module_masks.input_mask is None, \"A relu op can only be processed once\"\n    module_masks.set_input_mask(mask)\n    module_masks.set_output_mask(mask)\n    return mask",
  "def batchnorm2d_mask(module_masks, mask):\n    \"\"\"\n    Infer input and output shape from weight mask\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the batchnorm2d\n    mask : dict\n        The mask of its weights, from the user provided mask file\n\n    Returns\n    -------\n    CoarseMask, CoarseMask\n        The mask of its input tensor, the mask of its output tensor\n    \"\"\"\n    assert 'weight' in mask and 'bias' in mask\n    sum_mask = mask['weight'] + mask['bias']\n    nonzero_index = torch.nonzero(sum_mask, as_tuple=True)[0]\n    # infer shape of parameters\n    param_cmask = CoarseMask(num_dim=1)\n    param_cmask.add_index_mask(dim=0, index=nonzero_index)\n    module_masks.set_param_masks('weight', param_cmask)\n    module_masks.set_param_masks('bias', param_cmask)\n    # infer shape of input tensor\n    input_cmask = CoarseMask(num_dim=4)\n    input_cmask.add_index_mask(dim=1,\n                               index=torch.nonzero(mask['weight'], as_tuple=True)[0])\n    module_masks.set_input_mask(input_cmask)\n    # infer shape of output tensor\n    output_cmask = CoarseMask(num_dim=4)\n    output_cmask.add_index_mask(dim=1, index=nonzero_index)\n    module_masks.set_output_mask(output_cmask)\n    return input_cmask, output_cmask",
  "def conv2d_mask(module_masks, mask):\n    \"\"\"\n    Infer input and output shape from weight mask\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the conv2d\n    mask : dict\n        The mask of its weights, from the user provided mask file\n\n    Returns\n    -------\n    CoarseMask, CoarseMask\n        The mask of its input tensor, the mask of its output tensor\n    \"\"\"\n    def convert_to_coarse_mask(mask):\n        \"\"\"\n        Parameters\n        ----------\n        mask : dict\n            Weight mask from user provided mask file\n\n        Returns\n        -------\n        LongTensor, CoarseMask, CoarseMask\n            Index of the masked dimension, weight mask, bias mask\n        \"\"\"\n        assert 'weight' in mask\n        assert isinstance(mask['weight'], torch.Tensor)\n        weight_mask = mask['weight']\n        shape = weight_mask.size()\n        ones = torch.ones(shape[1:]).to(weight_mask.device)\n        zeros = torch.zeros(shape[1:]).to(weight_mask.device)\n        index = []\n        for i in range(shape[0]):\n            if torch.all(torch.eq(weight_mask[i], ones)):\n                index.append(i)\n            elif torch.all(torch.eq(weight_mask[i], zeros)):\n                continue\n            else:\n                index = None\n                break\n        if index is None:\n            return None, None, None\n        else:\n            index = torch.LongTensor(index).to(weight_mask.device)\n            weight_cmask = CoarseMask(num_dim=4)\n            weight_cmask.add_index_mask(dim=0, index=index)\n            bias_cmask = None\n            if 'bias' in mask and mask['bias'] is not None:\n                bias_index = torch.nonzero(mask['bias'], as_tuple=True)[0]\n                assert torch.all(torch.eq(index, bias_index)), \\\n                    \"bias mask should be consistent with weight mask\"\n                bias_cmask = CoarseMask(num_dim=1)\n                bias_cmask.add_index_mask(dim=0, index=bias_index)\n            return index, weight_cmask, bias_cmask\n    index, weight_cmask, bias_cmask = convert_to_coarse_mask(mask)\n    if index is None:\n        # TODO: fine grained mask speedup\n        return None, None\n    # deal with coarse grain mask\n    if 'weight' in module_masks.param_masks:\n        module_masks.param_masks['weight'].merge(weight_cmask)\n        module_masks.param_masks['bias'].merge(bias_cmask)\n    else:\n        module_masks.set_param_masks('weight', weight_cmask)\n        module_masks.set_param_masks('bias', bias_cmask)\n    output_cmask = CoarseMask(num_dim=4)\n    output_cmask.add_index_mask(dim=1, index=index)\n    if module_masks.output_mask is None:\n        module_masks.set_output_mask(output_cmask)\n    else:\n        module_masks.output_mask.merge(output_cmask)\n    return None, module_masks.output_mask",
  "def conv2d_inshape(module_masks, mask):\n    \"\"\"\n    Shape change of input tensor does not affect the shape of its output tensor\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the conv2d\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    \"\"\"\n    assert isinstance(mask, CoarseMask)\n    if module_masks.input_mask is None:\n        module_masks.set_input_mask(mask)\n    else:\n        # the same conv layer may be accessed more\n        # than once, such as a concat operation.\n        assert module_masks.input_mask <= mask\n        module_masks.input_mask.merge(mask)\n    return None",
  "def conv2d_outshape(module_masks, mask):\n    \"\"\"\n    Assume only the second dimension is masked\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the conv2d\n    mask : CoarseMask\n        The mask of its output tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its input tensor\n    \"\"\"\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n\n    if module_masks.output_mask is not None:\n        assert isinstance(module_masks.output_mask, CoarseMask)\n        # set shape of output\n        mask = module_masks.output_mask.merge(mask)\n    else:\n        module_masks.output_mask = mask\n    # infer shape of parameters\n    weight_cmask = CoarseMask(num_dim=4)\n    weight_cmask.add_index_mask(dim=0, index=mask.mask_index[1])\n    bias_cmask = CoarseMask(num_dim=1)\n    bias_cmask.add_index_mask(dim=0, index=mask.mask_index[1])\n    module_masks.set_param_masks('weight', weight_cmask)\n    module_masks.set_param_masks('bias', bias_cmask)\n    # input shape is not changed\n    return None",
  "def __init__(self, num_dim):\n        \"\"\"\n        Parameters\n        ----------\n        num_dim : int\n            The number of dimensions of the tensor that will be masked\n        \"\"\"\n        self.mask_index = [None for _ in range(num_dim)]",
  "def add_index_mask(self, dim, index):\n        \"\"\"\n        Add mask for the specified dimension\n\n        Parameters\n        ----------\n        dim : int\n            The dimension to add mask\n        index : tensor\n            The mask for this dimension, its a 1 dimension tensor which specifies\n            the index of the elements that are not pruned\n        \"\"\"\n        self.mask_index[dim] = index",
  "def merge_index(index_a, index_b):\n        \"\"\"\n        Parameters\n        ----------\n        index_a : tensor\n            One index (1-dimension) tensor\n        index_b : tensor\n            The other index (1-dimension) tensor\n\n        Returns\n        -------\n        tensor\n            The merged index (1-dimension) tensor\n            Note that: the output tensor will be moved\n            to the same device as index_a.\n        \"\"\"\n        device = index_a.device\n        s = set()\n        for num in index_a.tolist():\n            # we need to transfer the tensor to list here\n            # first, directly traversing the tensor by for\n            # loop will return the list of tensor(x) object,\n            # even the value are the same, but they are different\n            # tensor objects, so the set will contains multiple\n            # tensor objects that has the same value. For example\n            # for num in torch.ones(2):\n            #   s.add(num)\n            # s will be {tensor(1), tensor(1)}\n            s.add(num)\n        for num in index_b.tolist():\n            s.add(num)\n        # move the output tensor to the same device with index_a\n        return torch.tensor(sorted(s)).to(device)",
  "def merge(self, cmask):\n        \"\"\"\n        Merge another CoarseMask\n\n        Parameters\n        ----------\n        cmask : CoarseMask\n            Another CoarseMask to merge\n\n        Returns\n        -------\n        list\n            The member variable ```mask_index```\n        \"\"\"\n        assert isinstance(cmask, CoarseMask)\n        assert len(self.mask_index) == len(cmask.mask_index), \\\n            \"Only masks with the same number of dimensions can be merged\"\n        for i, index in enumerate(self.mask_index):\n            if index is None:\n                self.mask_index[i] = cmask.mask_index[i]\n            elif cmask.mask_index[i] is not None:\n                self.mask_index[i] = CoarseMask.merge_index(self.mask_index[i],\n                                                            cmask.mask_index[i])\n        return self.mask_index",
  "def __repr__(self):\n        return 'mask_index: {}'.format(self.mask_index)",
  "def eq_on_dim(self, other, dim):\n        assert isinstance(other, CoarseMask)\n        if self.mask_index[dim] is None and other.mask_index[dim] is None:\n            return True\n        elif isinstance(self.mask_index[dim], torch.Tensor) \\\n                and isinstance(other.mask_index[dim], torch.Tensor):\n            return torch.equal(self.mask_index[dim], other.mask_index[dim])\n        else:\n            return False",
  "def __eq__(self, other):\n        assert isinstance(other, CoarseMask)\n        if len(self.mask_index) != len(other.mask_index):\n            return False\n        for i in range(len(self.mask_index)):\n            if not self.eq_on_dim(other, i):\n                return False\n        return True",
  "def __lt__(self, other):\n        \"\"\"\n        Judge if the mask is a subset of another CoarseMask.\n        \"\"\"\n        assert isinstance(other, CoarseMask)\n        for dim, _ in enumerate(self.mask_index):\n            # if self has more dimensions\n            if dim >= len(other.mask_index):\n                return False\n            if self.mask_index[dim] is None:\n                # if no mask on this dimension, then we have less\n                # masks then the other CoraseMask.\n                continue\n            elif other.mask_index[dim] is None:\n                return False\n            else:\n                s1 = set(self.mask_index[dim].tolist())\n                s2 = set(other.mask_index[dim].tolist())\n                if not s1 < s2:\n                    return False\n        return True",
  "def __le__(self, other):\n        \"\"\"\n        Return if self's mask is less or equal to other's mask.\n        \"\"\"\n        assert isinstance(other, CoarseMask)\n        if self.__lt__(other) or self.__eq__(other):\n            return True\n        return False",
  "def __ne__(self, other):\n        return not self.__eq__(other)",
  "def __init__(self, module_name):\n        \"\"\"\n        Parameters\n        ----------\n        module_name : str\n            The name of the module or function\n        \"\"\"\n        self.module_name = module_name\n        self.param_masks = dict()\n        self.input_mask = None\n        self.output_mask = None",
  "def set_param_masks(self, name, mask):\n        \"\"\"\n        Parameters\n        ----------\n        name : str\n            The name of the weight\n        mask : CoarseMask\n            The mask for this weight\n        \"\"\"\n        self.param_masks[name] = mask",
  "def set_input_mask(self, mask):\n        \"\"\"\n        Parameters\n        ----------\n        mask : CoarseMask\n            The mask for input\n        \"\"\"\n        self.input_mask = mask",
  "def set_output_mask(self, mask):\n        \"\"\"\n        Parameters\n        ----------\n        mask : CoarseMask\n            The mask for output\n        \"\"\"\n        self.output_mask = mask",
  "def __repr__(self):\n        return 'input_mask: {}, output_mask: {}, param_masks: {}'.format(\n            self.input_mask, self.output_mask, self.param_masks\n        )",
  "def convert_to_coarse_mask(mask):\n        \"\"\"\n        Parameters\n        ----------\n        mask : dict\n            Weight mask from user provided mask file\n\n        Returns\n        -------\n        LongTensor, CoarseMask, CoarseMask\n            Index of the masked dimension, weight mask, bias mask\n        \"\"\"\n        assert 'weight' in mask\n        assert isinstance(mask['weight'], torch.Tensor)\n        weight_mask = mask['weight']\n        shape = weight_mask.size()\n        ones = torch.ones(shape[1:]).to(weight_mask.device)\n        zeros = torch.zeros(shape[1:]).to(weight_mask.device)\n        index = []\n        for i in range(shape[0]):\n            if torch.all(torch.eq(weight_mask[i], ones)):\n                index.append(i)\n            elif torch.all(torch.eq(weight_mask[i], zeros)):\n                continue\n            else:\n                index = None\n                break\n        if index is None:\n            return None, None, None\n        else:\n            index = torch.LongTensor(index).to(weight_mask.device)\n            weight_cmask = CoarseMask(num_dim=4)\n            weight_cmask.add_index_mask(dim=0, index=index)\n            bias_cmask = None\n            if 'bias' in mask and mask['bias'] is not None:\n                bias_index = torch.nonzero(mask['bias'], as_tuple=True)[0]\n                assert torch.all(torch.eq(index, bias_index)), \\\n                    \"bias mask should be consistent with weight mask\"\n                bias_cmask = CoarseMask(num_dim=1)\n                bias_cmask.add_index_mask(dim=0, index=bias_index)\n            return index, weight_cmask, bias_cmask",
  "class GridSearchTuner(Tuner):\n    \"\"\"\n    GridSearchTuner will search all the possible configures that the user define in the searchSpace.\n    The only acceptable types of search space are ``choice``, ``quniform``, ``randint``\n\n    Type ``choice`` will select one of the options. Note that it can also be nested.\n\n    Type ``quniform`` will receive three values [``low``, ``high``, ``q``],\n    where [``low``, ``high``] specifies a range and ``q`` specifies the interval.\n    It will be sampled in a way that the first sampled value is ``low``,\n    and each of the following values is 'interval' larger than the value in front of it.\n\n    Type ``randint`` gives all possible intergers in range[``low``, ``high``). Note that ``high`` is not included.\n    \"\"\"\n\n    def __init__(self):\n        self.count = -1\n        self.expanded_search_space = []\n        self.supplement_data = dict()\n\n    def _json2parameter(self, ss_spec):\n        \"\"\"\n        Generate all possible configs for hyperparameters from hyperparameter space.\n\n        Parameters\n        ----------\n        ss_spec : dict or list\n            Hyperparameter space or the ``_value`` of a hyperparameter\n\n        Returns\n        -------\n        list or dict\n            All the candidate choices of hyperparameters. for a hyperparameter, chosen_params\n            is a list. for multiple hyperparameters (e.g., search space), chosen_params is a dict.\n        \"\"\"\n        if isinstance(ss_spec, dict):\n            if '_type' in ss_spec.keys():\n                _type = ss_spec['_type']\n                _value = ss_spec['_value']\n                chosen_params = list()\n                if _type == 'choice':\n                    for value in _value:\n                        choice = self._json2parameter(value)\n                        if isinstance(choice, list):\n                            chosen_params.extend(choice)\n                        else:\n                            chosen_params.append(choice)\n                elif _type == 'quniform':\n                    chosen_params = self._parse_quniform(_value)\n                elif _type == 'randint':\n                    chosen_params = self._parse_randint(_value)\n                else:\n                    raise RuntimeError(\"Not supported type: %s\" % _type)\n            else:\n                chosen_params = dict()\n                for key in ss_spec.keys():\n                    chosen_params[key] = self._json2parameter(ss_spec[key])\n                return self._expand_parameters(chosen_params)\n        elif isinstance(ss_spec, list):\n            chosen_params = list()\n            for subspec in ss_spec[1:]:\n                choice = self._json2parameter(subspec)\n                if isinstance(choice, list):\n                    chosen_params.extend(choice)\n                else:\n                    chosen_params.append(choice)\n            chosen_params = list(map(lambda v: {ss_spec[0]: v}, chosen_params))\n        else:\n            chosen_params = copy.deepcopy(ss_spec)\n        return chosen_params\n\n    def _parse_quniform(self, param_value):\n        \"\"\"\n        Parse type of quniform parameter and return a list\n        \"\"\"\n        low, high, q = param_value[0], param_value[1], param_value[2]\n        return np.clip(np.arange(np.round(low/q), np.round(high/q)+1) * q, low, high)\n\n    def _parse_randint(self, param_value):\n        \"\"\"\n        Parse type of randint parameter and return a list\n        \"\"\"\n        if param_value[0] >= param_value[1]:\n            raise ValueError(\"Randint should contain at least 1 candidate, but [%s, %s) contains none.\",\n                             param_value[0], param_value[1])\n        return np.arange(param_value[0], param_value[1]).tolist()\n\n    def _expand_parameters(self, para):\n        \"\"\"\n        Enumerate all possible combinations of all parameters\n\n        Parameters\n        ----------\n        para : dict\n            {key1: [v11, v12, ...], key2: [v21, v22, ...], ...}\n\n        Returns\n        -------\n        dict\n            {{key1: v11, key2: v21, ...}, {key1: v11, key2: v22, ...}, ...}\n        \"\"\"\n        if len(para) == 1:\n            for key, values in para.items():\n                return list(map(lambda v: {key: v}, values))\n\n        key = list(para)[0]\n        values = para.pop(key)\n        rest_para = self._expand_parameters(para)\n        ret_para = list()\n        for val in values:\n            for config in rest_para:\n                config[key] = val\n                ret_para.append(copy.deepcopy(config))\n        return ret_para\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Check if the search space is valid and expand it: support only ``choice``, ``quniform``, ``randint``.\n\n        Parameters\n        ----------\n        search_space : dict\n            The format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        \"\"\"\n        self.expanded_search_space = self._json2parameter(search_space)\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate parameters for one trial.\n\n        Parameters\n        ----------\n        parameter_id : int\n            The id for the generated hyperparameter\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One configuration from the expanded search space.\n\n        Raises\n        ------\n        NoMoreTrialError\n            If all the configurations has been sent, raise :class:`~nni.NoMoreTrialError`.\n        \"\"\"\n        self.count += 1\n        while self.count <= len(self.expanded_search_space) - 1:\n            _params_tuple = convert_dict2tuple(self.expanded_search_space[self.count])\n            if _params_tuple in self.supplement_data:\n                self.count += 1\n            else:\n                return self.expanded_search_space[self.count]\n        raise nni.NoMoreTrialError('no more parameters now.')\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Receive a trial's final performance result reported through :func:`~nni.report_final_result` by the trial.\n        GridSearchTuner does not need trial's results.\n        \"\"\"\n        pass\n\n    def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        list\n            A list of dictionarys, each of which has at least two keys, ``parameter`` and ``value``\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            _params_tuple = convert_dict2tuple(_params)\n            self.supplement_data[_params_tuple] = True\n        logger.info(\"Successfully import data to grid search tuner.\")",
  "def __init__(self):\n        self.count = -1\n        self.expanded_search_space = []\n        self.supplement_data = dict()",
  "def _json2parameter(self, ss_spec):\n        \"\"\"\n        Generate all possible configs for hyperparameters from hyperparameter space.\n\n        Parameters\n        ----------\n        ss_spec : dict or list\n            Hyperparameter space or the ``_value`` of a hyperparameter\n\n        Returns\n        -------\n        list or dict\n            All the candidate choices of hyperparameters. for a hyperparameter, chosen_params\n            is a list. for multiple hyperparameters (e.g., search space), chosen_params is a dict.\n        \"\"\"\n        if isinstance(ss_spec, dict):\n            if '_type' in ss_spec.keys():\n                _type = ss_spec['_type']\n                _value = ss_spec['_value']\n                chosen_params = list()\n                if _type == 'choice':\n                    for value in _value:\n                        choice = self._json2parameter(value)\n                        if isinstance(choice, list):\n                            chosen_params.extend(choice)\n                        else:\n                            chosen_params.append(choice)\n                elif _type == 'quniform':\n                    chosen_params = self._parse_quniform(_value)\n                elif _type == 'randint':\n                    chosen_params = self._parse_randint(_value)\n                else:\n                    raise RuntimeError(\"Not supported type: %s\" % _type)\n            else:\n                chosen_params = dict()\n                for key in ss_spec.keys():\n                    chosen_params[key] = self._json2parameter(ss_spec[key])\n                return self._expand_parameters(chosen_params)\n        elif isinstance(ss_spec, list):\n            chosen_params = list()\n            for subspec in ss_spec[1:]:\n                choice = self._json2parameter(subspec)\n                if isinstance(choice, list):\n                    chosen_params.extend(choice)\n                else:\n                    chosen_params.append(choice)\n            chosen_params = list(map(lambda v: {ss_spec[0]: v}, chosen_params))\n        else:\n            chosen_params = copy.deepcopy(ss_spec)\n        return chosen_params",
  "def _parse_quniform(self, param_value):\n        \"\"\"\n        Parse type of quniform parameter and return a list\n        \"\"\"\n        low, high, q = param_value[0], param_value[1], param_value[2]\n        return np.clip(np.arange(np.round(low/q), np.round(high/q)+1) * q, low, high)",
  "def _parse_randint(self, param_value):\n        \"\"\"\n        Parse type of randint parameter and return a list\n        \"\"\"\n        if param_value[0] >= param_value[1]:\n            raise ValueError(\"Randint should contain at least 1 candidate, but [%s, %s) contains none.\",\n                             param_value[0], param_value[1])\n        return np.arange(param_value[0], param_value[1]).tolist()",
  "def _expand_parameters(self, para):\n        \"\"\"\n        Enumerate all possible combinations of all parameters\n\n        Parameters\n        ----------\n        para : dict\n            {key1: [v11, v12, ...], key2: [v21, v22, ...], ...}\n\n        Returns\n        -------\n        dict\n            {{key1: v11, key2: v21, ...}, {key1: v11, key2: v22, ...}, ...}\n        \"\"\"\n        if len(para) == 1:\n            for key, values in para.items():\n                return list(map(lambda v: {key: v}, values))\n\n        key = list(para)[0]\n        values = para.pop(key)\n        rest_para = self._expand_parameters(para)\n        ret_para = list()\n        for val in values:\n            for config in rest_para:\n                config[key] = val\n                ret_para.append(copy.deepcopy(config))\n        return ret_para",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Check if the search space is valid and expand it: support only ``choice``, ``quniform``, ``randint``.\n\n        Parameters\n        ----------\n        search_space : dict\n            The format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        \"\"\"\n        self.expanded_search_space = self._json2parameter(search_space)",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        Generate parameters for one trial.\n\n        Parameters\n        ----------\n        parameter_id : int\n            The id for the generated hyperparameter\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One configuration from the expanded search space.\n\n        Raises\n        ------\n        NoMoreTrialError\n            If all the configurations has been sent, raise :class:`~nni.NoMoreTrialError`.\n        \"\"\"\n        self.count += 1\n        while self.count <= len(self.expanded_search_space) - 1:\n            _params_tuple = convert_dict2tuple(self.expanded_search_space[self.count])\n            if _params_tuple in self.supplement_data:\n                self.count += 1\n            else:\n                return self.expanded_search_space[self.count]\n        raise nni.NoMoreTrialError('no more parameters now.')",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Receive a trial's final performance result reported through :func:`~nni.report_final_result` by the trial.\n        GridSearchTuner does not need trial's results.\n        \"\"\"\n        pass",
  "def import_data(self, data):\n        \"\"\"\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        list\n            A list of dictionarys, each of which has at least two keys, ``parameter`` and ``value``\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            _params_tuple = convert_dict2tuple(_params)\n            self.supplement_data[_params_tuple] = True\n        logger.info(\"Successfully import data to grid search tuner.\")",
  "class Individual:\n    \"\"\"\n    Indicidual class to store the indv info.\n\n    Attributes\n    ----------\n    config : str\n        Search space.\n    info : str\n        The str to save information of individual.\n    result : float\n        The final metric of a individual.\n    store_dir : str\n    save_dir : str\n    \"\"\"\n\n    def __init__(self, config=None, info=None, result=None, save_dir=None):\n        \"\"\"\n        Parameters\n        ----------\n        config : str\n            A config to represent a group of parameters.\n        info : str\n        result : float\n        save_dir : str\n        \"\"\"\n        self.config = config\n        self.result = result\n        self.info = info\n        self.restore_dir = None\n        self.save_dir = save_dir\n\n    def __str__(self):\n        return \"info: \" + str(self.info) + \\\n            \", config :\" + str(self.config) + \", result: \" + str(self.result)\n\n    def mutation(self, config=None, info=None, save_dir=None):\n        \"\"\"\n        Mutation by reset state information.\n\n        Parameters\n        ----------\n        config : str\n        info : str\n        save_dir : str\n        \"\"\"\n        self.result = None\n        self.config = config\n        self.restore_dir = self.save_dir\n        self.save_dir = save_dir\n        self.info = info",
  "class EvolutionClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('population_size'): self.range('population_size', int, 0, 99999),\n        }).validate(kwargs)",
  "class EvolutionTuner(Tuner):\n    \"\"\"\n    EvolutionTuner is tuner using navie evolution algorithm.\n    \"\"\"\n\n    def __init__(self, optimize_mode=\"maximize\", population_size=32):\n        \"\"\"\n        Parameters\n        ----------\n        optimize_mode : str, default 'maximize'\n        population_size : int\n            initial population size. The larger population size,\n        the better evolution performance.\n        \"\"\"\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.population_size = population_size\n\n        self.trial_result = []\n        self.searchspace_json = None\n        self.total_data = {}\n        self.random_state = None\n        self.population = None\n        self.space = None\n\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Update search space.\n\n        Search_space contains the information that user pre-defined.\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self.searchspace_json = search_space\n        self.space = json2space(self.searchspace_json)\n\n        self.random_state = np.random.RandomState()\n        self.population = []\n        is_rand = dict()\n\n        for item in self.space:\n            is_rand[item] = True\n\n        for _ in range(self.population_size):\n            config = json2parameter(\n                self.searchspace_json, is_rand, self.random_state)\n            self.population.append(Individual(config=config))\n\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        This function will returns a dict of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        dict\n            A group of candaidte parameters that evolution tuner generated.\n        \"\"\"\n        if not self.population:\n            raise RuntimeError('The population is empty')\n\n        pos = -1\n\n        for i in range(len(self.population)):\n            if self.population[i].result is None:\n                pos = i\n                break\n\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            total_config = indiv.config\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n\n            # mutation\n            space = json2space(self.searchspace_json,\n                               self.population[0].config)\n            is_rand = dict()\n            mutation_pos = space[random.randint(0, len(space)-1)]\n\n            for i in range(len(self.space)):\n                is_rand[self.space[i]] = (self.space[i] == mutation_pos)\n            config = json2parameter(\n                self.searchspace_json, is_rand, self.random_state, self.population[0].config)\n            self.population.pop(1)\n            # remove \"_index\" from config and save params-id\n\n            total_config = config\n\n        self.total_data[parameter_id] = total_config\n        config = split_index(total_config)\n\n        return config\n\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Record the result from a trial\n\n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n            value is final metrics of the trial.\n        \"\"\"\n        reward = extract_scalar_reward(value)\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        # restore the paramsters contains \"_index\"\n        params = self.total_data[parameter_id]\n\n        if self.optimize_mode == OptimizeMode.Minimize:\n            reward = -reward\n\n        indiv = Individual(config=params, result=reward)\n        self.population.append(indiv)\n\n    def import_data(self, data):\n        pass",
  "def __init__(self, config=None, info=None, result=None, save_dir=None):\n        \"\"\"\n        Parameters\n        ----------\n        config : str\n            A config to represent a group of parameters.\n        info : str\n        result : float\n        save_dir : str\n        \"\"\"\n        self.config = config\n        self.result = result\n        self.info = info\n        self.restore_dir = None\n        self.save_dir = save_dir",
  "def __str__(self):\n        return \"info: \" + str(self.info) + \\\n            \", config :\" + str(self.config) + \", result: \" + str(self.result)",
  "def mutation(self, config=None, info=None, save_dir=None):\n        \"\"\"\n        Mutation by reset state information.\n\n        Parameters\n        ----------\n        config : str\n        info : str\n        save_dir : str\n        \"\"\"\n        self.result = None\n        self.config = config\n        self.restore_dir = self.save_dir\n        self.save_dir = save_dir\n        self.info = info",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('population_size'): self.range('population_size', int, 0, 99999),\n        }).validate(kwargs)",
  "def __init__(self, optimize_mode=\"maximize\", population_size=32):\n        \"\"\"\n        Parameters\n        ----------\n        optimize_mode : str, default 'maximize'\n        population_size : int\n            initial population size. The larger population size,\n        the better evolution performance.\n        \"\"\"\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.population_size = population_size\n\n        self.trial_result = []\n        self.searchspace_json = None\n        self.total_data = {}\n        self.random_state = None\n        self.population = None\n        self.space = None",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Update search space.\n\n        Search_space contains the information that user pre-defined.\n\n        Parameters\n        ----------\n        search_space : dict\n        \"\"\"\n        self.searchspace_json = search_space\n        self.space = json2space(self.searchspace_json)\n\n        self.random_state = np.random.RandomState()\n        self.population = []\n        is_rand = dict()\n\n        for item in self.space:\n            is_rand[item] = True\n\n        for _ in range(self.population_size):\n            config = json2parameter(\n                self.searchspace_json, is_rand, self.random_state)\n            self.population.append(Individual(config=config))",
  "def generate_parameters(self, parameter_id, **kwargs):\n        \"\"\"\n        This function will returns a dict of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        dict\n            A group of candaidte parameters that evolution tuner generated.\n        \"\"\"\n        if not self.population:\n            raise RuntimeError('The population is empty')\n\n        pos = -1\n\n        for i in range(len(self.population)):\n            if self.population[i].result is None:\n                pos = i\n                break\n\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            total_config = indiv.config\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n\n            # mutation\n            space = json2space(self.searchspace_json,\n                               self.population[0].config)\n            is_rand = dict()\n            mutation_pos = space[random.randint(0, len(space)-1)]\n\n            for i in range(len(self.space)):\n                is_rand[self.space[i]] = (self.space[i] == mutation_pos)\n            config = json2parameter(\n                self.searchspace_json, is_rand, self.random_state, self.population[0].config)\n            self.population.pop(1)\n            # remove \"_index\" from config and save params-id\n\n            total_config = config\n\n        self.total_data[parameter_id] = total_config\n        config = split_index(total_config)\n\n        return config",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Record the result from a trial\n\n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n            value is final metrics of the trial.\n        \"\"\"\n        reward = extract_scalar_reward(value)\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        # restore the paramsters contains \"_index\"\n        params = self.total_data[parameter_id]\n\n        if self.optimize_mode == OptimizeMode.Minimize:\n            reward = -reward\n\n        indiv = Individual(config=params, result=reward)\n        self.population.append(indiv)",
  "def import_data(self, data):\n        pass",
  "class MedianstopClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('start_step'): self.range('start_step', int, 0, 9999),\n        }).validate(kwargs)",
  "class MedianstopAssessor(Assessor):\n    \"\"\"MedianstopAssessor is The median stopping rule stops a pending trial X at step S\n    if the trial\u2019s best objective value by step S is strictly worse than the median value\n    of the running averages of all completed trials\u2019 objectives reported up to step S\n\n    Parameters\n    ----------\n    optimize_mode : str\n        optimize mode, 'maximize' or 'minimize'\n    start_step : int\n        only after receiving start_step number of reported intermediate results\n    \"\"\"\n    def __init__(self, optimize_mode='maximize', start_step=0):\n        self._start_step = start_step\n        self._running_history = dict()\n        self._completed_avg_history = dict()\n        if optimize_mode == 'maximize':\n            self._high_better = True\n        elif optimize_mode == 'minimize':\n            self._high_better = False\n        else:\n            self._high_better = True\n            logger.warning('unrecognized optimize_mode %s', optimize_mode)\n\n    def _update_data(self, trial_job_id, trial_history):\n        \"\"\"update data\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n        \"\"\"\n        if trial_job_id not in self._running_history:\n            self._running_history[trial_job_id] = []\n        self._running_history[trial_job_id].extend(trial_history[len(self._running_history[trial_job_id]):])\n\n    def trial_end(self, trial_job_id, success):\n        \"\"\"trial_end\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        success : bool\n            True if succssfully finish the experiment, False otherwise\n        \"\"\"\n        if trial_job_id in self._running_history:\n            if success:\n                cnt = 0\n                history_sum = 0\n                self._completed_avg_history[trial_job_id] = []\n                for each in self._running_history[trial_job_id]:\n                    cnt += 1\n                    history_sum += each\n                    self._completed_avg_history[trial_job_id].append(history_sum / cnt)\n            self._running_history.pop(trial_job_id)\n        else:\n            logger.warning('trial_end: trial_job_id does not exist in running_history')\n\n    def assess_trial(self, trial_job_id, trial_history):\n        \"\"\"assess_trial\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n\n        Returns\n        -------\n        bool\n            AssessResult.Good or AssessResult.Bad\n\n        Raises\n        ------\n        Exception\n            unrecognize exception in medianstop_assessor\n        \"\"\"\n        curr_step = len(trial_history)\n        if curr_step < self._start_step:\n            return AssessResult.Good\n\n        scalar_trial_history = extract_scalar_history(trial_history)\n        self._update_data(trial_job_id, scalar_trial_history)\n        if self._high_better:\n            best_history = max(scalar_trial_history)\n        else:\n            best_history = min(scalar_trial_history)\n\n        avg_array = []\n        for id_ in self._completed_avg_history:\n            if len(self._completed_avg_history[id_]) >= curr_step:\n                avg_array.append(self._completed_avg_history[id_][curr_step - 1])\n        if avg_array:\n            avg_array.sort()\n            if self._high_better:\n                median = avg_array[(len(avg_array)-1) // 2]\n                return AssessResult.Bad if best_history < median else AssessResult.Good\n            else:\n                median = avg_array[len(avg_array) // 2]\n                return AssessResult.Bad if best_history > median else AssessResult.Good\n        else:\n            return AssessResult.Good",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            Optional('optimize_mode'): self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('start_step'): self.range('start_step', int, 0, 9999),\n        }).validate(kwargs)",
  "def __init__(self, optimize_mode='maximize', start_step=0):\n        self._start_step = start_step\n        self._running_history = dict()\n        self._completed_avg_history = dict()\n        if optimize_mode == 'maximize':\n            self._high_better = True\n        elif optimize_mode == 'minimize':\n            self._high_better = False\n        else:\n            self._high_better = True\n            logger.warning('unrecognized optimize_mode %s', optimize_mode)",
  "def _update_data(self, trial_job_id, trial_history):\n        \"\"\"update data\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n        \"\"\"\n        if trial_job_id not in self._running_history:\n            self._running_history[trial_job_id] = []\n        self._running_history[trial_job_id].extend(trial_history[len(self._running_history[trial_job_id]):])",
  "def trial_end(self, trial_job_id, success):\n        \"\"\"trial_end\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        success : bool\n            True if succssfully finish the experiment, False otherwise\n        \"\"\"\n        if trial_job_id in self._running_history:\n            if success:\n                cnt = 0\n                history_sum = 0\n                self._completed_avg_history[trial_job_id] = []\n                for each in self._running_history[trial_job_id]:\n                    cnt += 1\n                    history_sum += each\n                    self._completed_avg_history[trial_job_id].append(history_sum / cnt)\n            self._running_history.pop(trial_job_id)\n        else:\n            logger.warning('trial_end: trial_job_id does not exist in running_history')",
  "def assess_trial(self, trial_job_id, trial_history):\n        \"\"\"assess_trial\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n\n        Returns\n        -------\n        bool\n            AssessResult.Good or AssessResult.Bad\n\n        Raises\n        ------\n        Exception\n            unrecognize exception in medianstop_assessor\n        \"\"\"\n        curr_step = len(trial_history)\n        if curr_step < self._start_step:\n            return AssessResult.Good\n\n        scalar_trial_history = extract_scalar_history(trial_history)\n        self._update_data(trial_job_id, scalar_trial_history)\n        if self._high_better:\n            best_history = max(scalar_trial_history)\n        else:\n            best_history = min(scalar_trial_history)\n\n        avg_array = []\n        for id_ in self._completed_avg_history:\n            if len(self._completed_avg_history[id_]) >= curr_step:\n                avg_array.append(self._completed_avg_history[id_][curr_step - 1])\n        if avg_array:\n            avg_array.sort()\n            if self._high_better:\n                median = avg_array[(len(avg_array)-1) // 2]\n                return AssessResult.Bad if best_history < median else AssessResult.Good\n            else:\n                median = avg_array[len(avg_array) // 2]\n                return AssessResult.Bad if best_history > median else AssessResult.Good\n        else:\n            return AssessResult.Good",
  "def test():\n    '''\n    tests.\n    '''\n    parser = argparse.ArgumentParser(description='parse command line parameters.')\n    parser.add_argument('--start_from', type=int, default=10, dest='start_step',\n                        help='Assessing each trial from the step start_step.')\n    parser.add_argument('--optimize_mode', type=str, default='maximize',\n                        help='Select optimize mode for Tuner: minimize or maximize.')\n    FLAGS, _ = parser.parse_known_args()\n\n    lcs = [[1,1,1,1,1,1,1,1,1,1],\n           [2,2,2,2,2,2,2,2,2,2],\n           [3,3,3,3,3,3,3,3,3,3],\n           [4,4,4,4,4,4,4,4,4,4]]\n    #lcs = [[1,1,1,1,1,1,1,1,1,1],\n    #       [1,1,1,1,1,1,1,1,1,1],\n    #       [1,1,1,1,1,1,1,1,1,1]]\n\n    assessor = MedianstopAssessor(FLAGS.optimize_mode, FLAGS.start_step)\n    for i in range(len(lcs)):\n        #lc = []\n        to_complete = True\n        for k in range(len(lcs[0])):\n            #d = random.randint(i*100+0, i*100+100)\n            #lc.append(d)\n            ret = assessor.assess_trial(i, lcs[i][:k+1])\n            print('result: %d', ret)\n            if ret == AssessResult.Bad:\n                assessor.trial_end(i, False)\n                to_complete = False\n                break\n        if to_complete:\n            assessor.trial_end(i, True)",
  "def global_mutable_counting():\n    global _counter\n    _counter += 1\n    return _counter",
  "class AverageMeter:\n    def __init__(self, name):\n        self.name = name\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val):\n        self.val = val\n        self.sum += val\n        self.count += 1\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        return '{name} {val:4f} ({avg:4f})'.format(**self.__dict__)\n\n    def summary(self):\n        return '{name}: {avg:4f}'.format(**self.__dict__)",
  "class AverageMeterGroup:\n    def __init__(self):\n        self.meters = {}\n\n    def update(self, data):\n        for k, v in data.items():\n            if k not in self.meters:\n                self.meters[k] = AverageMeter(k)\n            self.meters[k].update(v)\n\n    def __str__(self):\n        return '  '.join(str(v) for v in self.meters.values())\n\n    def summary(self):\n        return '  '.join(v.summary() for v in self.meters.values())",
  "class StructuredMutableTreeNode:\n    def __init__(self, mutable):\n        self.mutable = mutable\n        self.children = []\n\n    def add_child(self, mutable):\n        self.children.append(StructuredMutableTreeNode(mutable))\n        return self.children[-1]\n\n    def type(self):\n        return type(self.mutable)\n\n    def __iter__(self):\n        return self.traverse()\n\n    def traverse(self, order=\"pre\", deduplicate=True, memo=None):\n        if memo is None:\n            memo = set()\n        assert order in [\"pre\", \"post\"]\n        if order == \"pre\":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable\n        for child in self.children:\n            for m in child.traverse(order=order, deduplicate=deduplicate, memo=memo):\n                yield m\n        if order == \"post\":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable",
  "def fill_zero_grads(grads, weights):\n    ret = []\n    for grad, weight in zip(grads, weights):\n        if grad is not None:\n            ret.append(grad)\n        else:\n            ret.append(tf.zeros_like(weight))\n    return ret",
  "def __init__(self, name):\n        self.name = name\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0",
  "def update(self, val):\n        self.val = val\n        self.sum += val\n        self.count += 1\n        self.avg = self.sum / self.count",
  "def __str__(self):\n        return '{name} {val:4f} ({avg:4f})'.format(**self.__dict__)",
  "def summary(self):\n        return '{name}: {avg:4f}'.format(**self.__dict__)",
  "def __init__(self):\n        self.meters = {}",
  "def update(self, data):\n        for k, v in data.items():\n            if k not in self.meters:\n                self.meters[k] = AverageMeter(k)\n            self.meters[k].update(v)",
  "def __str__(self):\n        return '  '.join(str(v) for v in self.meters.values())",
  "def summary(self):\n        return '  '.join(v.summary() for v in self.meters.values())",
  "def __init__(self, mutable):\n        self.mutable = mutable\n        self.children = []",
  "def add_child(self, mutable):\n        self.children.append(StructuredMutableTreeNode(mutable))\n        return self.children[-1]",
  "def type(self):\n        return type(self.mutable)",
  "def __iter__(self):\n        return self.traverse()",
  "def traverse(self, order=\"pre\", deduplicate=True, memo=None):\n        if memo is None:\n            memo = set()\n        assert order in [\"pre\", \"post\"]\n        if order == \"pre\":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable\n        for child in self.children:\n            for m in child.traverse(order=order, deduplicate=deduplicate, memo=memo):\n                yield m\n        if order == \"post\":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable",
  "class BaseMutator(Model):\n    def __init__(self, model):\n        super().__init__()\n        self.__dict__['model'] = model\n        self._structured_mutables = self._parse_search_space(self.model)\n\n    def _parse_search_space(self, module, root=None, prefix='', memo=None, nested_detection=None):\n        if memo is None:\n            memo = set()\n        if root is None:\n            root = StructuredMutableTreeNode(None)\n        if module not in memo:\n            memo.add(module)\n            if isinstance(module, Mutable):\n                if nested_detection is not None:\n                    raise RuntimeError('Cannot have nested search space. Error at {} in {}'\n                                       .format(module, nested_detection))\n                module.name = prefix\n                module.set_mutator(self)\n                root = root.add_child(module)\n                if not isinstance(module, MutableScope):\n                    nested_detection = module\n                if isinstance(module, InputChoice):\n                    for k in module.choose_from:\n                        if k != InputChoice.NO_KEY and k not in [m.key for m in memo if isinstance(m, Mutable)]:\n                            raise RuntimeError('\"{}\" required by \"{}\" not found in keys that appeared before, and is not NO_KEY.'\n                                               .format(k, module.key))\n            for submodule in module.layers:\n                if not isinstance(submodule, Model):\n                    continue\n                submodule_prefix = prefix + ('.' if prefix else '') + submodule.name\n                self._parse_search_space(submodule, root, submodule_prefix, memo=memo, nested_detection=nested_detection)\n        return root\n\n    @property\n    def mutables(self):\n        return self._structured_mutables\n\n    def undedup_mutables(self):\n        return self._structured_mutables.traverse(deduplicate=False)\n\n    def call(self, *inputs):\n        raise RuntimeError('Call is undefined for mutators.')\n\n    def __setattr__(self, name, value):\n        if name == 'model':\n            raise AttributeError(\"Attribute `model` can be set at most once, and you shouldn't use `self.model = model` to \"\n                                 \"include your network, as it will include all parameters in model into the mutator.\")\n        return super().__setattr__(name, value)\n\n    def enter_mutable_scope(self, mutable_scope):\n        pass\n\n    def exit_mutable_scope(self, mutable_scope):\n        pass\n\n    def on_forward_layer_choice(self, mutable, *inputs):\n        raise NotImplementedError\n\n    def on_forward_input_choice(self, mutable, tensor_list):\n        raise NotImplementedError\n\n    def export(self):\n        raise NotImplementedError",
  "def __init__(self, model):\n        super().__init__()\n        self.__dict__['model'] = model\n        self._structured_mutables = self._parse_search_space(self.model)",
  "def _parse_search_space(self, module, root=None, prefix='', memo=None, nested_detection=None):\n        if memo is None:\n            memo = set()\n        if root is None:\n            root = StructuredMutableTreeNode(None)\n        if module not in memo:\n            memo.add(module)\n            if isinstance(module, Mutable):\n                if nested_detection is not None:\n                    raise RuntimeError('Cannot have nested search space. Error at {} in {}'\n                                       .format(module, nested_detection))\n                module.name = prefix\n                module.set_mutator(self)\n                root = root.add_child(module)\n                if not isinstance(module, MutableScope):\n                    nested_detection = module\n                if isinstance(module, InputChoice):\n                    for k in module.choose_from:\n                        if k != InputChoice.NO_KEY and k not in [m.key for m in memo if isinstance(m, Mutable)]:\n                            raise RuntimeError('\"{}\" required by \"{}\" not found in keys that appeared before, and is not NO_KEY.'\n                                               .format(k, module.key))\n            for submodule in module.layers:\n                if not isinstance(submodule, Model):\n                    continue\n                submodule_prefix = prefix + ('.' if prefix else '') + submodule.name\n                self._parse_search_space(submodule, root, submodule_prefix, memo=memo, nested_detection=nested_detection)\n        return root",
  "def mutables(self):\n        return self._structured_mutables",
  "def undedup_mutables(self):\n        return self._structured_mutables.traverse(deduplicate=False)",
  "def call(self, *inputs):\n        raise RuntimeError('Call is undefined for mutators.')",
  "def __setattr__(self, name, value):\n        if name == 'model':\n            raise AttributeError(\"Attribute `model` can be set at most once, and you shouldn't use `self.model = model` to \"\n                                 \"include your network, as it will include all parameters in model into the mutator.\")\n        return super().__setattr__(name, value)",
  "def enter_mutable_scope(self, mutable_scope):\n        pass",
  "def exit_mutable_scope(self, mutable_scope):\n        pass",
  "def on_forward_layer_choice(self, mutable, *inputs):\n        raise NotImplementedError",
  "def on_forward_input_choice(self, mutable, tensor_list):\n        raise NotImplementedError",
  "def export(self):\n        raise NotImplementedError",
  "class Mutator(BaseMutator):\n    def __init__(self, model):\n        super().__init__(model)\n        self._cache = {}\n\n    def sample_search(self):\n        raise NotImplementedError('Method `sample_search` must be overridden')\n\n    def sample_final(self):\n        raise NotImplementedError('Method `sample_final` must be overriden for exporting')\n\n    def reset(self):\n        self._cache = self.sample_search()\n\n    def export(self):\n        return self.sample_final()\n\n    # TODO: status\n    # TODO: graph\n\n    def on_forward_layer_choice(self, mutable, *inputs):\n        mask = self._get_decision(mutable)\n        assert len(mask) == len(mutable), \\\n                'Invalid mask, expected {} to be of length {}.'.format(mask, len(mutable))\n        out = self._select_with_mask(lambda choice: choice(*inputs), mutable.choices, mask)\n        return self._tensor_reduction(mutable.reduction, out), mask\n\n    def on_forward_input_choice(self, mutable, tensor_list):\n        mask = self._get_decision(mutable)\n        assert len(mask) == mutable.n_candidates, \\\n                'Invalid mask, expected {} to be of length {}.'.format(mask, mutable.n_candidates)\n        out = self._select_with_mask(lambda tensor: tensor, tensor_list, mask)\n        return self._tensor_reduction(mutable.reduction, out), mask\n\n    def _select_with_mask(self, map_fn, candidates, mask):\n        if mask.dtype.is_bool:\n            out = [map_fn(cand) for cand, m in zip(candidates, mask) if m]\n        elif mask.dtype.is_floating:\n            out = [map_fn(cand) * m for cand, m in zip(candidates, mask) if m]\n        else:\n            raise ValueError('Unrecognized mask, dtype is {}'.format(mask.dtype.name))\n        return out\n\n    def _tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == 'none':\n            return tensor_list\n        if not tensor_list:\n            return None\n        if len(tensor_list) == 1:\n            return tensor_list[0]\n        if reduction_type == 'sum':\n            return sum(tensor_list)\n        if reduction_type == 'mean':\n            return sum(tensor_list) / len(tensor_list)\n        if reduction_type == 'concat':\n            return tf.concat(tensor_list, axis=0)\n        raise ValueError('Unrecognized reduction policy: \"{}'.format(reduction_type))\n\n    def _get_decision(self, mutable):\n        if mutable.key not in self._cache:\n            raise ValueError('\"{}\" not found in decision cache.'.format(mutable.key))\n        result = self._cache[mutable.key]\n        _logger.debug('Decision %s: %s', mutable.key, result)\n        return result",
  "def __init__(self, model):\n        super().__init__(model)\n        self._cache = {}",
  "def sample_search(self):\n        raise NotImplementedError('Method `sample_search` must be overridden')",
  "def sample_final(self):\n        raise NotImplementedError('Method `sample_final` must be overriden for exporting')",
  "def reset(self):\n        self._cache = self.sample_search()",
  "def export(self):\n        return self.sample_final()",
  "def on_forward_layer_choice(self, mutable, *inputs):\n        mask = self._get_decision(mutable)\n        assert len(mask) == len(mutable), \\\n                'Invalid mask, expected {} to be of length {}.'.format(mask, len(mutable))\n        out = self._select_with_mask(lambda choice: choice(*inputs), mutable.choices, mask)\n        return self._tensor_reduction(mutable.reduction, out), mask",
  "def on_forward_input_choice(self, mutable, tensor_list):\n        mask = self._get_decision(mutable)\n        assert len(mask) == mutable.n_candidates, \\\n                'Invalid mask, expected {} to be of length {}.'.format(mask, mutable.n_candidates)\n        out = self._select_with_mask(lambda tensor: tensor, tensor_list, mask)\n        return self._tensor_reduction(mutable.reduction, out), mask",
  "def _select_with_mask(self, map_fn, candidates, mask):\n        if mask.dtype.is_bool:\n            out = [map_fn(cand) for cand, m in zip(candidates, mask) if m]\n        elif mask.dtype.is_floating:\n            out = [map_fn(cand) * m for cand, m in zip(candidates, mask) if m]\n        else:\n            raise ValueError('Unrecognized mask, dtype is {}'.format(mask.dtype.name))\n        return out",
  "def _tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == 'none':\n            return tensor_list\n        if not tensor_list:\n            return None\n        if len(tensor_list) == 1:\n            return tensor_list[0]\n        if reduction_type == 'sum':\n            return sum(tensor_list)\n        if reduction_type == 'mean':\n            return sum(tensor_list) / len(tensor_list)\n        if reduction_type == 'concat':\n            return tf.concat(tensor_list, axis=0)\n        raise ValueError('Unrecognized reduction policy: \"{}'.format(reduction_type))",
  "def _get_decision(self, mutable):\n        if mutable.key not in self._cache:\n            raise ValueError('\"{}\" not found in decision cache.'.format(mutable.key))\n        result = self._cache[mutable.key]\n        _logger.debug('Decision %s: %s', mutable.key, result)\n        return result",
  "class Mutable(Model):\n    def __init__(self, key=None):\n        super().__init__()\n        if key is None:\n            self._key = '{}_{}'.format(type(self).__name__, global_mutable_counting())\n        elif isinstance(key, str):\n            self._key = key\n        else:\n            self._key = str(key)\n            _logger.warning('Key \"%s\" is not string, converted to string.', key)\n        self.init_hook = None\n        self.forward_hook = None\n\n    def __deepcopy__(self, memodict=None):\n        raise NotImplementedError(\"Deep copy doesn't work for mutables.\")\n\n    def set_mutator(self, mutator):\n        if hasattr(self, 'mutator'):\n            raise RuntimeError('`set_mutator is called more than once. '\n                               'Did you parse the search space multiple times? '\n                               'Or did you apply multiple fixed architectures?')\n        self.mutator = mutator\n\n    def call(self, *inputs):\n        raise NotImplementedError('Method `call` of Mutable must be overridden')\n\n    def build(self, input_shape):\n        self._check_built()\n\n    @property\n    def key(self):\n        return self._key\n\n    @property\n    def name(self):\n        return self._name if hasattr(self, '_name') else self._key\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    def _check_built(self):\n        if not hasattr(self, 'mutator'):\n            raise ValueError(\n                \"Mutator not set for {}. You might have forgotten to initialize and apply your mutator. \"\n                \"Or did you initialize a mutable on the fly in forward pass? Move to `__init__` \"\n                \"so that trainer can locate all your mutables. See NNI docs for more details.\".format(self))\n\n    def __repr__(self):\n        return '{} ({})'.format(self.name, self.key)",
  "class MutableScope(Mutable):\n    def __call__(self, *args, **kwargs):\n        try:\n            self.mutator.enter_mutable_scope(self)\n            return super().__call__(*args, **kwargs)\n        finally:\n            self.mutator.exit_mutable_scope(self)",
  "class LayerChoice(Mutable):\n    def __init__(self, op_candidates, reduction='sum', return_mask=False, key=None):\n        super().__init__(key=key)\n        self.names = []\n        if isinstance(op_candidates, OrderedDict):\n            for name in op_candidates:\n                assert name not in [\"length\", \"reduction\", \"return_mask\", \"_key\", \"key\", \"names\"], \\\n                    \"Please don't use a reserved name '{}' for your module.\".format(name)\n                self.names.append(name)\n        elif isinstance(op_candidates, list):\n            for i, _ in enumerate(op_candidates):\n                self.names.append(str(i))\n        else:\n            raise TypeError(\"Unsupported op_candidates type: {}\".format(type(op_candidates)))\n\n        self.length = len(op_candidates)\n        self.choices = op_candidates\n        self.reduction = reduction\n        self.return_mask = return_mask\n\n    def call(self, *inputs):\n        out, mask = self.mutator.on_forward_layer_choice(self, *inputs)\n        if self.return_mask:\n            return out, mask\n        return out\n\n    def build(self, input_shape):\n        self._check_built()\n        for op in self.choices:\n            op.build(input_shape)\n\n    def __len__(self):\n        return len(self.choices)",
  "class InputChoice(Mutable):\n    NO_KEY = ''\n\n    def __init__(self, n_candidates=None, choose_from=None, n_chosen=None, reduction='sum', return_mask=False, key=None):\n        super().__init__(key=key)\n        assert n_candidates is not None or choose_from is not None, \\\n                'At least one of `n_candidates` and `choose_from` must be not None.'\n        if choose_from is not None and n_candidates is None:\n            n_candidates = len(choose_from)\n        elif choose_from is None and n_candidates is not None:\n            choose_from = [self.NO_KEY] * n_candidates\n        assert n_candidates == len(choose_from), 'Number of candidates must be equal to the length of `choose_from`.'\n        assert n_candidates > 0, 'Number of candidates must be greater than 0.'\n        assert n_chosen is None or 0 <= n_chosen <= n_candidates, \\\n                'Expected selected number must be None or no more than number of candidates.'\n\n        self.n_candidates = n_candidates\n        self.choose_from = choose_from.copy()\n        self.n_chosen = n_chosen\n        self.reduction = reduction\n        self.return_mask = return_mask\n\n    def call(self, optional_inputs):\n        optional_input_list = optional_inputs\n        if isinstance(optional_inputs, dict):\n            optional_input_list = [optional_inputs[tag] for tag in self.choose_from]\n        assert isinstance(optional_input_list, list), \\\n                'Optional input list must be a list, not a {}.'.format(type(optional_input_list))\n        assert len(optional_inputs) == self.n_candidates, \\\n                'Length of the input list must be equal to number of candidates.'\n        out, mask = self.mutator.on_forward_input_choice(self, optional_input_list)\n        if self.return_mask:\n            return out, mask\n        return out",
  "def __init__(self, key=None):\n        super().__init__()\n        if key is None:\n            self._key = '{}_{}'.format(type(self).__name__, global_mutable_counting())\n        elif isinstance(key, str):\n            self._key = key\n        else:\n            self._key = str(key)\n            _logger.warning('Key \"%s\" is not string, converted to string.', key)\n        self.init_hook = None\n        self.forward_hook = None",
  "def __deepcopy__(self, memodict=None):\n        raise NotImplementedError(\"Deep copy doesn't work for mutables.\")",
  "def set_mutator(self, mutator):\n        if hasattr(self, 'mutator'):\n            raise RuntimeError('`set_mutator is called more than once. '\n                               'Did you parse the search space multiple times? '\n                               'Or did you apply multiple fixed architectures?')\n        self.mutator = mutator",
  "def call(self, *inputs):\n        raise NotImplementedError('Method `call` of Mutable must be overridden')",
  "def build(self, input_shape):\n        self._check_built()",
  "def key(self):\n        return self._key",
  "def name(self):\n        return self._name if hasattr(self, '_name') else self._key",
  "def name(self, name):\n        self._name = name",
  "def _check_built(self):\n        if not hasattr(self, 'mutator'):\n            raise ValueError(\n                \"Mutator not set for {}. You might have forgotten to initialize and apply your mutator. \"\n                \"Or did you initialize a mutable on the fly in forward pass? Move to `__init__` \"\n                \"so that trainer can locate all your mutables. See NNI docs for more details.\".format(self))",
  "def __repr__(self):\n        return '{} ({})'.format(self.name, self.key)",
  "def __call__(self, *args, **kwargs):\n        try:\n            self.mutator.enter_mutable_scope(self)\n            return super().__call__(*args, **kwargs)\n        finally:\n            self.mutator.exit_mutable_scope(self)",
  "def __init__(self, op_candidates, reduction='sum', return_mask=False, key=None):\n        super().__init__(key=key)\n        self.names = []\n        if isinstance(op_candidates, OrderedDict):\n            for name in op_candidates:\n                assert name not in [\"length\", \"reduction\", \"return_mask\", \"_key\", \"key\", \"names\"], \\\n                    \"Please don't use a reserved name '{}' for your module.\".format(name)\n                self.names.append(name)\n        elif isinstance(op_candidates, list):\n            for i, _ in enumerate(op_candidates):\n                self.names.append(str(i))\n        else:\n            raise TypeError(\"Unsupported op_candidates type: {}\".format(type(op_candidates)))\n\n        self.length = len(op_candidates)\n        self.choices = op_candidates\n        self.reduction = reduction\n        self.return_mask = return_mask",
  "def call(self, *inputs):\n        out, mask = self.mutator.on_forward_layer_choice(self, *inputs)\n        if self.return_mask:\n            return out, mask\n        return out",
  "def build(self, input_shape):\n        self._check_built()\n        for op in self.choices:\n            op.build(input_shape)",
  "def __len__(self):\n        return len(self.choices)",
  "def __init__(self, n_candidates=None, choose_from=None, n_chosen=None, reduction='sum', return_mask=False, key=None):\n        super().__init__(key=key)\n        assert n_candidates is not None or choose_from is not None, \\\n                'At least one of `n_candidates` and `choose_from` must be not None.'\n        if choose_from is not None and n_candidates is None:\n            n_candidates = len(choose_from)\n        elif choose_from is None and n_candidates is not None:\n            choose_from = [self.NO_KEY] * n_candidates\n        assert n_candidates == len(choose_from), 'Number of candidates must be equal to the length of `choose_from`.'\n        assert n_candidates > 0, 'Number of candidates must be greater than 0.'\n        assert n_chosen is None or 0 <= n_chosen <= n_candidates, \\\n                'Expected selected number must be None or no more than number of candidates.'\n\n        self.n_candidates = n_candidates\n        self.choose_from = choose_from.copy()\n        self.n_chosen = n_chosen\n        self.reduction = reduction\n        self.return_mask = return_mask",
  "def call(self, optional_inputs):\n        optional_input_list = optional_inputs\n        if isinstance(optional_inputs, dict):\n            optional_input_list = [optional_inputs[tag] for tag in self.choose_from]\n        assert isinstance(optional_input_list, list), \\\n                'Optional input list must be a list, not a {}.'.format(type(optional_input_list))\n        assert len(optional_inputs) == self.n_candidates, \\\n                'Length of the input list must be equal to number of candidates.'\n        out, mask = self.mutator.on_forward_input_choice(self, optional_input_list)\n        if self.return_mask:\n            return out, mask\n        return out",
  "class EnasMutator(Mutator):\n    def __init__(self, model,\n                 lstm_size=64,\n                 lstm_num_layers=1,\n                 tanh_constant=1.5,\n                 cell_exit_extra_step=False,\n                 skip_target=0.4,\n                 temperature=None,\n                 branch_bias=0.25,\n                 entropy_reduction='sum'):\n        super().__init__(model)\n        self.tanh_constant = tanh_constant\n        self.temperature = temperature\n        self.cell_exit_extra_step = cell_exit_extra_step\n\n        cells = [LSTMCell(units=lstm_size, use_bias=False) for _ in range(lstm_num_layers)]\n        self.lstm = RNN(cells, stateful=True)\n        self.g_emb = tf.random.normal((1, 1, lstm_size)) * 0.1\n        self.skip_targets = tf.constant([1.0 - skip_target, skip_target])\n\n        self.max_layer_choice = 0\n        self.bias_dict = {}\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                if self.max_layer_choice == 0:\n                    self.max_layer_choice = len(mutable)\n                assert self.max_layer_choice == len(mutable), \\\n                        \"ENAS mutator requires all layer choice have the same number of candidates.\"\n                if 'reduce' in mutable.key:\n                    bias = []\n                    for choice in mutable.choices:\n                        if 'conv' in str(type(choice)).lower():\n                            bias.append(branch_bias)\n                        else:\n                            bias.append(-branch_bias)\n                    self.bias_dict[mutable.key] = tf.constant(bias)\n\n        # exposed for trainer\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0\n\n        # internal nn layers\n        self.embedding = Embedding(self.max_layer_choice + 1, lstm_size)\n        self.soft = Dense(self.max_layer_choice, use_bias=False)\n        self.attn_anchor = Dense(lstm_size, use_bias=False)\n        self.attn_query = Dense(lstm_size, use_bias=False)\n        self.v_attn = Dense(1, use_bias=False)\n        assert entropy_reduction in ['sum', 'mean'], 'Entropy reduction must be one of sum and mean.'\n        self.entropy_reduction = tf.reduce_sum if entropy_reduction == 'sum' else tf.reduce_mean\n        self.cross_entropy_loss = SparseCategoricalCrossentropy(from_logits=True, reduction=Reduction.NONE)\n\n        self._first_sample = True\n\n    def sample_search(self):\n        self._initialize()\n        self._sample(self.mutables)\n        self._first_sample = False\n        return self._choices\n\n    def sample_final(self):\n        return self.sample_search()\n\n    def _sample(self, tree):\n        mutable = tree.mutable\n        if isinstance(mutable, LayerChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_layer_choice(mutable)\n        elif isinstance(mutable, InputChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_input_choice(mutable)\n        for child in tree.children:\n            self._sample(child)\n        if self.cell_exit_extra_step and isinstance(mutable, MutableScope) and mutable.key not in self._anchors_hid:\n            self._anchors_hid[mutable.key] = self.lstm(self._inputs, 1)\n\n    def _initialize(self):\n        self._choices = {}\n        self._anchors_hid = {}\n        self._inputs = self.g_emb\n        # seems the `input_shape` parameter of RNN does not work\n        # workaround it by omitting `reset_states` for first run\n        if not self._first_sample:\n            self.lstm.reset_states()\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0\n\n    def _sample_layer_choice(self, mutable):\n        logit = self.soft(self.lstm(self._inputs))\n        if self.temperature is not None:\n            logit /= self.temperature\n        if self.tanh_constant is not None:\n            logit = self.tanh_constant * tf.tanh(logit)\n        if mutable.key in self.bias_dict:\n            logit += self.bias_dict[mutable.key]\n        softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n        branch_id = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [1])\n        log_prob = self.cross_entropy_loss(branch_id, logit)\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = log_prob * tf.math.exp(-log_prob)\n        self.sample_entropy += self.entropy_reduction(entropy)\n        self._inputs = tf.reshape(self.embedding(branch_id), [1, 1, -1])\n        mask = tf.one_hot(branch_id, self.max_layer_choice)\n        return tf.cast(tf.reshape(mask, [-1]), tf.bool)\n\n    def _sample_input_choice(self, mutable):\n        query, anchors = [], []\n        for label in mutable.choose_from:\n            if label not in self._anchors_hid:\n                self._anchors_hid[label] = self.lstm(self._inputs)\n            query.append(self.attn_anchor(self._anchors_hid[label]))\n            anchors.append(self._anchors_hid[label])\n        query = tf.concat(query, axis=0)\n        query = tf.tanh(query + self.attn_query(anchors[-1]))\n        query = self.v_attn(query)\n\n        if self.temperature is not None:\n            query /= self.temperature\n        if self.tanh_constant is not None:\n            query = self.tanh_constant * tf.tanh(query)\n\n        if mutable.n_chosen is None:\n            logit = tf.concat([-query, query], axis=1)\n            softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n            skip = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [-1])\n            skip_prob = tf.math.sigmoid(logit)\n            kl = tf.reduce_sum(skip_prob * tf.math.log(skip_prob / self.skip_targets))\n            self.sample_skip_penalty += kl\n            log_prob = self.cross_entropy_loss(skip, logit)\n\n            skip = tf.cast(skip, tf.float32)\n            inputs = tf.tensordot(skip, tf.concat(anchors, 0), 1) / (1. + tf.reduce_sum(skip))\n            self._inputs = tf.reshape(inputs, [1, 1, -1])\n\n        else:\n            assert mutable.n_chosen == 1, \"Input choice must select exactly one or any in ENAS.\"\n            logit = tf.reshape(query, [1, -1])\n            softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n            index = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [-1])\n            skip = tf.reshape(tf.one_hot(index, mutable.n_candidates), [-1])\n            # when the size is 1, tf does not accept tensor here, complaining the shape is wrong\n            # but using a numpy array seems fine\n            log_prob = self.cross_entropy_loss(logit, query.numpy())\n            self._inputs = tf.reshape(anchors[index.numpy()[0]], [1, 1, -1])\n\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = log_prob * tf.exp(-log_prob)\n        self.sample_entropy += self.entropy_reduction(entropy)\n        assert len(skip) == mutable.n_candidates, (skip, mutable.n_candidates, mutable.n_chosen)\n        return tf.cast(skip, tf.bool)",
  "def __init__(self, model,\n                 lstm_size=64,\n                 lstm_num_layers=1,\n                 tanh_constant=1.5,\n                 cell_exit_extra_step=False,\n                 skip_target=0.4,\n                 temperature=None,\n                 branch_bias=0.25,\n                 entropy_reduction='sum'):\n        super().__init__(model)\n        self.tanh_constant = tanh_constant\n        self.temperature = temperature\n        self.cell_exit_extra_step = cell_exit_extra_step\n\n        cells = [LSTMCell(units=lstm_size, use_bias=False) for _ in range(lstm_num_layers)]\n        self.lstm = RNN(cells, stateful=True)\n        self.g_emb = tf.random.normal((1, 1, lstm_size)) * 0.1\n        self.skip_targets = tf.constant([1.0 - skip_target, skip_target])\n\n        self.max_layer_choice = 0\n        self.bias_dict = {}\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                if self.max_layer_choice == 0:\n                    self.max_layer_choice = len(mutable)\n                assert self.max_layer_choice == len(mutable), \\\n                        \"ENAS mutator requires all layer choice have the same number of candidates.\"\n                if 'reduce' in mutable.key:\n                    bias = []\n                    for choice in mutable.choices:\n                        if 'conv' in str(type(choice)).lower():\n                            bias.append(branch_bias)\n                        else:\n                            bias.append(-branch_bias)\n                    self.bias_dict[mutable.key] = tf.constant(bias)\n\n        # exposed for trainer\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0\n\n        # internal nn layers\n        self.embedding = Embedding(self.max_layer_choice + 1, lstm_size)\n        self.soft = Dense(self.max_layer_choice, use_bias=False)\n        self.attn_anchor = Dense(lstm_size, use_bias=False)\n        self.attn_query = Dense(lstm_size, use_bias=False)\n        self.v_attn = Dense(1, use_bias=False)\n        assert entropy_reduction in ['sum', 'mean'], 'Entropy reduction must be one of sum and mean.'\n        self.entropy_reduction = tf.reduce_sum if entropy_reduction == 'sum' else tf.reduce_mean\n        self.cross_entropy_loss = SparseCategoricalCrossentropy(from_logits=True, reduction=Reduction.NONE)\n\n        self._first_sample = True",
  "def sample_search(self):\n        self._initialize()\n        self._sample(self.mutables)\n        self._first_sample = False\n        return self._choices",
  "def sample_final(self):\n        return self.sample_search()",
  "def _sample(self, tree):\n        mutable = tree.mutable\n        if isinstance(mutable, LayerChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_layer_choice(mutable)\n        elif isinstance(mutable, InputChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_input_choice(mutable)\n        for child in tree.children:\n            self._sample(child)\n        if self.cell_exit_extra_step and isinstance(mutable, MutableScope) and mutable.key not in self._anchors_hid:\n            self._anchors_hid[mutable.key] = self.lstm(self._inputs, 1)",
  "def _initialize(self):\n        self._choices = {}\n        self._anchors_hid = {}\n        self._inputs = self.g_emb\n        # seems the `input_shape` parameter of RNN does not work\n        # workaround it by omitting `reset_states` for first run\n        if not self._first_sample:\n            self.lstm.reset_states()\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0",
  "def _sample_layer_choice(self, mutable):\n        logit = self.soft(self.lstm(self._inputs))\n        if self.temperature is not None:\n            logit /= self.temperature\n        if self.tanh_constant is not None:\n            logit = self.tanh_constant * tf.tanh(logit)\n        if mutable.key in self.bias_dict:\n            logit += self.bias_dict[mutable.key]\n        softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n        branch_id = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [1])\n        log_prob = self.cross_entropy_loss(branch_id, logit)\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = log_prob * tf.math.exp(-log_prob)\n        self.sample_entropy += self.entropy_reduction(entropy)\n        self._inputs = tf.reshape(self.embedding(branch_id), [1, 1, -1])\n        mask = tf.one_hot(branch_id, self.max_layer_choice)\n        return tf.cast(tf.reshape(mask, [-1]), tf.bool)",
  "def _sample_input_choice(self, mutable):\n        query, anchors = [], []\n        for label in mutable.choose_from:\n            if label not in self._anchors_hid:\n                self._anchors_hid[label] = self.lstm(self._inputs)\n            query.append(self.attn_anchor(self._anchors_hid[label]))\n            anchors.append(self._anchors_hid[label])\n        query = tf.concat(query, axis=0)\n        query = tf.tanh(query + self.attn_query(anchors[-1]))\n        query = self.v_attn(query)\n\n        if self.temperature is not None:\n            query /= self.temperature\n        if self.tanh_constant is not None:\n            query = self.tanh_constant * tf.tanh(query)\n\n        if mutable.n_chosen is None:\n            logit = tf.concat([-query, query], axis=1)\n            softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n            skip = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [-1])\n            skip_prob = tf.math.sigmoid(logit)\n            kl = tf.reduce_sum(skip_prob * tf.math.log(skip_prob / self.skip_targets))\n            self.sample_skip_penalty += kl\n            log_prob = self.cross_entropy_loss(skip, logit)\n\n            skip = tf.cast(skip, tf.float32)\n            inputs = tf.tensordot(skip, tf.concat(anchors, 0), 1) / (1. + tf.reduce_sum(skip))\n            self._inputs = tf.reshape(inputs, [1, 1, -1])\n\n        else:\n            assert mutable.n_chosen == 1, \"Input choice must select exactly one or any in ENAS.\"\n            logit = tf.reshape(query, [1, -1])\n            softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n            index = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [-1])\n            skip = tf.reshape(tf.one_hot(index, mutable.n_candidates), [-1])\n            # when the size is 1, tf does not accept tensor here, complaining the shape is wrong\n            # but using a numpy array seems fine\n            log_prob = self.cross_entropy_loss(logit, query.numpy())\n            self._inputs = tf.reshape(anchors[index.numpy()[0]], [1, 1, -1])\n\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = log_prob * tf.exp(-log_prob)\n        self.sample_entropy += self.entropy_reduction(entropy)\n        assert len(skip) == mutable.n_candidates, (skip, mutable.n_candidates, mutable.n_chosen)\n        return tf.cast(skip, tf.bool)",
  "class EnasTrainer:\n    def __init__(self, model, loss, metrics, reward_function, optimizer, batch_size, num_epochs,\n                 dataset_train, dataset_valid):\n        self.model = model\n        self.loss = loss\n        self.metrics = metrics\n        self.reward_function = reward_function\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n\n        x, y = dataset_train\n        split = int(len(x) * 0.9)\n        self.train_set = tf.data.Dataset.from_tensor_slices((x[:split], y[:split]))\n        self.valid_set = tf.data.Dataset.from_tensor_slices((x[split:], y[split:]))\n        self.test_set = tf.data.Dataset.from_tensor_slices(dataset_valid)\n\n        self.mutator = EnasMutator(model)\n        self.mutator_optim = Adam(learning_rate=mutator_lr)\n\n        self.baseline = 0.\n\n\n    def train(self, validate=True):\n        for epoch in range(self.num_epochs):\n            logger.info(\"Epoch %d Training\", epoch + 1)\n            self.train_one_epoch(epoch)\n            logger.info(\"Epoch %d Validating\", epoch + 1)\n            self.validate_one_epoch(epoch)\n\n    def validate(self):\n        self.validate_one_epoch(-1)\n\n\n    def train_one_epoch(self, epoch):\n        train_loader, valid_loader = self._create_train_loader()\n\n        # Sample model and train\n        meters = AverageMeterGroup()\n\n        for step in range(1, child_steps + 1):\n            x, y = next(train_loader)\n            self.mutator.reset()\n\n            with tf.GradientTape() as tape:\n                logits = self.model(x, training=True)\n                if isinstance(logits, tuple):\n                    logits, aux_logits = logits\n                    aux_loss = self.loss(aux_logits, y)\n                else:\n                    aux_loss = 0.\n                metrics = self.metrics(y, logits)\n                loss = self.loss(y, logits) + aux_weight * aux_loss\n\n            grads = tape.gradient(loss, self.model.trainable_weights)\n            grads = fill_zero_grads(grads, self.model.trainable_weights)\n            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n\n            metrics['loss'] = tf.reduce_mean(loss).numpy()\n            meters.update(metrics)\n\n            if log_frequency and step % log_frequency == 0:\n                logger.info(\"Model Epoch [%d/%d] Step [%d/%d]  %s\", epoch + 1,\n                            self.num_epochs, step, child_steps, meters)\n\n        # Train sampler (mutator)\n        meters = AverageMeterGroup()\n        for mutator_step in range(1, mutator_steps + 1):\n            grads_list = []\n            for step in range(1, mutator_steps_aggregate + 1):\n                with tf.GradientTape() as tape:\n                    x, y = next(valid_loader)\n                    self.mutator.reset()\n\n                    logits = self.model(x, training=False)\n                    metrics = self.metrics(y, logits)\n                    reward = self.reward_function(y, logits) + entropy_weight * self.mutator.sample_entropy\n                    self.baseline = self.baseline * baseline_decay + reward * (1 - baseline_decay)\n                    loss = self.mutator.sample_log_prob * (reward - self.baseline)\n                    loss += skip_weight * self.mutator.sample_skip_penalty\n\n                    meters.update({\n                        'reward': reward,\n                        'loss': tf.reduce_mean(loss).numpy(),\n                        'ent': self.mutator.sample_entropy.numpy(),\n                        'log_prob': self.mutator.sample_log_prob.numpy(),\n                        'baseline': self.baseline,\n                        'skip': self.mutator.sample_skip_penalty,\n                    })\n\n                    cur_step = step + (mutator_step - 1) * mutator_steps_aggregate\n                    if log_frequency and cur_step % log_frequency == 0:\n                        logger.info(\"RL Epoch [%d/%d] Step [%d/%d] [%d/%d]  %s\", epoch + 1, self.num_epochs,\n                                    mutator_step, mutator_steps, step, mutator_steps_aggregate,\n                                    meters)\n\n                grads = tape.gradient(loss, self.mutator.trainable_weights)\n                grads = fill_zero_grads(grads, self.mutator.trainable_weights)\n                grads_list.append(grads)\n            total_grads = [tf.math.add_n(weight_grads) for weight_grads in zip(*grads_list)]\n            total_grads, _ = tf.clip_by_global_norm(total_grads, 5.0)\n            self.mutator_optim.apply_gradients(zip(total_grads, self.mutator.trainable_weights))\n\n    def validate_one_epoch(self, epoch):\n        test_loader = self._create_validate_loader()\n\n        for arc_id in range(test_arc_per_epoch):\n            meters = AverageMeterGroup()\n            for x, y in test_loader:\n                self.mutator.reset()\n                logits = self.model(x, training=False)\n                if isinstance(logits, tuple):\n                    logits, _ = logits\n                metrics = self.metrics(y, logits)\n                loss = self.loss(y, logits)\n                metrics['loss'] = tf.reduce_mean(loss).numpy()\n                meters.update(metrics)\n\n            logger.info(\"Test Epoch [%d/%d] Arc [%d/%d] Summary  %s\",\n                        epoch + 1, self.num_epochs, arc_id + 1, test_arc_per_epoch,\n                        meters.summary())\n\n\n    def _create_train_loader(self):\n        train_set = self.train_set.shuffle(1000000).repeat().batch(self.batch_size)\n        test_set = self.valid_set.shuffle(1000000).repeat().batch(self.batch_size)\n        return iter(train_set), iter(test_set)\n\n    def _create_validate_loader(self):\n        return iter(self.test_set.shuffle(1000000).batch(self.batch_size))",
  "def __init__(self, model, loss, metrics, reward_function, optimizer, batch_size, num_epochs,\n                 dataset_train, dataset_valid):\n        self.model = model\n        self.loss = loss\n        self.metrics = metrics\n        self.reward_function = reward_function\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n\n        x, y = dataset_train\n        split = int(len(x) * 0.9)\n        self.train_set = tf.data.Dataset.from_tensor_slices((x[:split], y[:split]))\n        self.valid_set = tf.data.Dataset.from_tensor_slices((x[split:], y[split:]))\n        self.test_set = tf.data.Dataset.from_tensor_slices(dataset_valid)\n\n        self.mutator = EnasMutator(model)\n        self.mutator_optim = Adam(learning_rate=mutator_lr)\n\n        self.baseline = 0.",
  "def train(self, validate=True):\n        for epoch in range(self.num_epochs):\n            logger.info(\"Epoch %d Training\", epoch + 1)\n            self.train_one_epoch(epoch)\n            logger.info(\"Epoch %d Validating\", epoch + 1)\n            self.validate_one_epoch(epoch)",
  "def validate(self):\n        self.validate_one_epoch(-1)",
  "def train_one_epoch(self, epoch):\n        train_loader, valid_loader = self._create_train_loader()\n\n        # Sample model and train\n        meters = AverageMeterGroup()\n\n        for step in range(1, child_steps + 1):\n            x, y = next(train_loader)\n            self.mutator.reset()\n\n            with tf.GradientTape() as tape:\n                logits = self.model(x, training=True)\n                if isinstance(logits, tuple):\n                    logits, aux_logits = logits\n                    aux_loss = self.loss(aux_logits, y)\n                else:\n                    aux_loss = 0.\n                metrics = self.metrics(y, logits)\n                loss = self.loss(y, logits) + aux_weight * aux_loss\n\n            grads = tape.gradient(loss, self.model.trainable_weights)\n            grads = fill_zero_grads(grads, self.model.trainable_weights)\n            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n\n            metrics['loss'] = tf.reduce_mean(loss).numpy()\n            meters.update(metrics)\n\n            if log_frequency and step % log_frequency == 0:\n                logger.info(\"Model Epoch [%d/%d] Step [%d/%d]  %s\", epoch + 1,\n                            self.num_epochs, step, child_steps, meters)\n\n        # Train sampler (mutator)\n        meters = AverageMeterGroup()\n        for mutator_step in range(1, mutator_steps + 1):\n            grads_list = []\n            for step in range(1, mutator_steps_aggregate + 1):\n                with tf.GradientTape() as tape:\n                    x, y = next(valid_loader)\n                    self.mutator.reset()\n\n                    logits = self.model(x, training=False)\n                    metrics = self.metrics(y, logits)\n                    reward = self.reward_function(y, logits) + entropy_weight * self.mutator.sample_entropy\n                    self.baseline = self.baseline * baseline_decay + reward * (1 - baseline_decay)\n                    loss = self.mutator.sample_log_prob * (reward - self.baseline)\n                    loss += skip_weight * self.mutator.sample_skip_penalty\n\n                    meters.update({\n                        'reward': reward,\n                        'loss': tf.reduce_mean(loss).numpy(),\n                        'ent': self.mutator.sample_entropy.numpy(),\n                        'log_prob': self.mutator.sample_log_prob.numpy(),\n                        'baseline': self.baseline,\n                        'skip': self.mutator.sample_skip_penalty,\n                    })\n\n                    cur_step = step + (mutator_step - 1) * mutator_steps_aggregate\n                    if log_frequency and cur_step % log_frequency == 0:\n                        logger.info(\"RL Epoch [%d/%d] Step [%d/%d] [%d/%d]  %s\", epoch + 1, self.num_epochs,\n                                    mutator_step, mutator_steps, step, mutator_steps_aggregate,\n                                    meters)\n\n                grads = tape.gradient(loss, self.mutator.trainable_weights)\n                grads = fill_zero_grads(grads, self.mutator.trainable_weights)\n                grads_list.append(grads)\n            total_grads = [tf.math.add_n(weight_grads) for weight_grads in zip(*grads_list)]\n            total_grads, _ = tf.clip_by_global_norm(total_grads, 5.0)\n            self.mutator_optim.apply_gradients(zip(total_grads, self.mutator.trainable_weights))",
  "def validate_one_epoch(self, epoch):\n        test_loader = self._create_validate_loader()\n\n        for arc_id in range(test_arc_per_epoch):\n            meters = AverageMeterGroup()\n            for x, y in test_loader:\n                self.mutator.reset()\n                logits = self.model(x, training=False)\n                if isinstance(logits, tuple):\n                    logits, _ = logits\n                metrics = self.metrics(y, logits)\n                loss = self.loss(y, logits)\n                metrics['loss'] = tf.reduce_mean(loss).numpy()\n                meters.update(metrics)\n\n            logger.info(\"Test Epoch [%d/%d] Arc [%d/%d] Summary  %s\",\n                        epoch + 1, self.num_epochs, arc_id + 1, test_arc_per_epoch,\n                        meters.summary())",
  "def _create_train_loader(self):\n        train_set = self.train_set.shuffle(1000000).repeat().batch(self.batch_size)\n        test_set = self.valid_set.shuffle(1000000).repeat().batch(self.batch_size)\n        return iter(train_set), iter(test_set)",
  "def _create_validate_loader(self):\n        return iter(self.test_set.shuffle(1000000).batch(self.batch_size))",
  "def get_and_apply_next_architecture(model):\n    \"\"\"\n    Wrapper of :class:`~nni.nas.tensorflow.classic_nas.mutator.ClassicMutator` to make it more meaningful,\n    similar to ``get_next_parameter`` for HPO.\n    Tt will generate search space based on ``model``.\n    If env ``NNI_GEN_SEARCH_SPACE`` exists, this is in dry run mode for\n    generating search space for the experiment.\n    If not, there are still two mode, one is nni experiment mode where users\n    use ``nnictl`` to start an experiment. The other is standalone mode\n    where users directly run the trial command, this mode chooses the first\n    one(s) for each LayerChoice and InputChoice.\n    Parameters\n    ----------\n    model : nn.Module\n        User's model with search space (e.g., LayerChoice, InputChoice) embedded in it.\n    \"\"\"\n    ClassicMutator(model)",
  "class ClassicMutator(Mutator):\n    \"\"\"\n    This mutator is to apply the architecture chosen from tuner.\n    It implements the forward function of LayerChoice and InputChoice,\n    to only activate the chosen ones.\n    Parameters\n    ----------\n    model : nn.Module\n        User's model with search space (e.g., LayerChoice, InputChoice) embedded in it.\n    \"\"\"\n\n    def __init__(self, model):\n        super(ClassicMutator, self).__init__(model)\n        self._chosen_arch = {}\n        self._search_space = self._generate_search_space()\n        if NNI_GEN_SEARCH_SPACE in os.environ:\n            # dry run for only generating search space\n            self._dump_search_space(os.environ[NNI_GEN_SEARCH_SPACE])\n            sys.exit(0)\n\n        if trial_env_vars.NNI_PLATFORM is None:\n            logger.warning(\"This is in standalone mode, the chosen are the first one(s).\")\n            self._chosen_arch = self._standalone_generate_chosen()\n        else:\n            # get chosen arch from tuner\n            self._chosen_arch = nni.get_next_parameter()\n            if self._chosen_arch is None:\n                if trial_env_vars.NNI_PLATFORM == \"unittest\":\n                    # happens if NNI_PLATFORM is intentionally set, e.g., in UT\n                    logger.warning(\"`NNI_PLATFORM` is set but `param` is None. Falling back to standalone mode.\")\n                    self._chosen_arch = self._standalone_generate_chosen()\n                else:\n                    raise RuntimeError(\"Chosen architecture is None. This may be a platform error.\")\n        self.reset()\n\n    def _sample_layer_choice(self, mutable, idx, value, search_space_item):\n        \"\"\"\n        Convert layer choice to tensor representation.\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        \"\"\"\n        # doesn't support multihot for layer choice yet\n        assert 0 <= idx < len(mutable) and search_space_item[idx] == value, \\\n            \"Index '{}' in search space '{}' is not '{}'\".format(idx, search_space_item, value)\n        mask = tf.one_hot(idx, len(mutable))\n        return tf.cast(tf.reshape(mask, [-1]), tf.bool)\n\n    def _sample_input_choice(self, mutable, idx, value, search_space_item):\n        \"\"\"\n        Convert input choice to tensor representation.\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        \"\"\"\n        candidate_repr = search_space_item[\"candidates\"]\n        multihot_list = [False] * mutable.n_candidates\n        for i, v in zip(idx, value):\n            assert 0 <= i < mutable.n_candidates and candidate_repr[i] == v, \\\n                \"Index '{}' in search space '{}' is not '{}'\".format(i, candidate_repr, v)\n            assert not multihot_list[i], \"'{}' is selected twice in '{}', which is not allowed.\".format(i, idx)\n            multihot_list[i] = True\n        return tf.cast(multihot_list, tf.bool)  # pylint: disable=not-callable\n\n    def sample_search(self):\n        \"\"\"\n        See :meth:`sample_final`.\n        \"\"\"\n        return self.sample_final()\n\n    def sample_final(self):\n        \"\"\"\n        Convert the chosen arch and apply it on model.\n        \"\"\"\n        assert set(self._chosen_arch.keys()) == set(self._search_space.keys()), \\\n            \"Unmatched keys, expected keys '{}' from search space, found '{}'.\".format(self._search_space.keys(),\n                                                                                       self._chosen_arch.keys())\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, (LayerChoice, InputChoice)):\n                assert mutable.key in self._chosen_arch, \\\n                    \"Expected '{}' in chosen arch, but not found.\".format(mutable.key)\n                data = self._chosen_arch[mutable.key]\n                assert isinstance(data, dict) and \"_value\" in data and \"_idx\" in data, \\\n                    \"'{}' is not a valid choice.\".format(data)\n            if isinstance(mutable, LayerChoice):\n                result[mutable.key] = self._sample_layer_choice(mutable, data[\"_idx\"], data[\"_value\"],\n                                                                self._search_space[mutable.key][\"_value\"])\n            elif isinstance(mutable, InputChoice):\n                result[mutable.key] = self._sample_input_choice(mutable, data[\"_idx\"], data[\"_value\"],\n                                                                self._search_space[mutable.key][\"_value\"])\n            elif isinstance(mutable, MutableScope):\n                logger.info(\"Mutable scope '%s' is skipped during parsing choices.\", mutable.key)\n            else:\n                raise TypeError(\"Unsupported mutable type: '%s'.\" % type(mutable))\n        return result\n\n    def _standalone_generate_chosen(self):\n        \"\"\"\n        Generate the chosen architecture for standalone mode,\n        i.e., choose the first one(s) for LayerChoice and InputChoice.\n        ::\n            { key_name: {\"_value\": \"conv1\",\n                         \"_idx\": 0} }\n            { key_name: {\"_value\": [\"in1\"],\n                         \"_idx\": [0]} }\n        Returns\n        -------\n        dict\n            the chosen architecture\n        \"\"\"\n        chosen_arch = {}\n        for key, val in self._search_space.items():\n            if val[\"_type\"] == LAYER_CHOICE:\n                choices = val[\"_value\"]\n                chosen_arch[key] = {\"_value\": choices[0], \"_idx\": 0}\n            elif val[\"_type\"] == INPUT_CHOICE:\n                choices = val[\"_value\"][\"candidates\"]\n                n_chosen = val[\"_value\"][\"n_chosen\"]\n                if n_chosen is None:\n                    n_chosen = len(choices)\n                chosen_arch[key] = {\"_value\": choices[:n_chosen], \"_idx\": list(range(n_chosen))}\n            else:\n                raise ValueError(\"Unknown key '%s' and value '%s'.\" % (key, val))\n        return chosen_arch\n\n    def _generate_search_space(self):\n        \"\"\"\n        Generate search space from mutables.\n        Here is the search space format:\n        ::\n            { key_name: {\"_type\": \"layer_choice\",\n                         \"_value\": [\"conv1\", \"conv2\"]} }\n            { key_name: {\"_type\": \"input_choice\",\n                         \"_value\": {\"candidates\": [\"in1\", \"in2\"],\n                                    \"n_chosen\": 1}} }\n        Returns\n        -------\n        dict\n            the generated search space\n        \"\"\"\n        search_space = {}\n        for mutable in self.mutables:\n            # for now we only generate flattened search space\n            if isinstance(mutable, LayerChoice):\n                key = mutable.key\n                val = mutable.names\n                search_space[key] = {\"_type\": LAYER_CHOICE, \"_value\": val}\n            elif isinstance(mutable, InputChoice):\n                key = mutable.key\n                search_space[key] = {\"_type\": INPUT_CHOICE,\n                                     \"_value\": {\"candidates\": mutable.choose_from,\n                                                \"n_chosen\": mutable.n_chosen}}\n            elif isinstance(mutable, MutableScope):\n                logger.info(\"Mutable scope '%s' is skipped during generating search space.\", mutable.key)\n            else:\n                raise TypeError(\"Unsupported mutable type: '%s'.\" % type(mutable))\n        return search_space\n\n    def _dump_search_space(self, file_path):\n        with open(file_path, \"w\") as ss_file:\n            json.dump(self._search_space, ss_file, sort_keys=True, indent=2)",
  "def __init__(self, model):\n        super(ClassicMutator, self).__init__(model)\n        self._chosen_arch = {}\n        self._search_space = self._generate_search_space()\n        if NNI_GEN_SEARCH_SPACE in os.environ:\n            # dry run for only generating search space\n            self._dump_search_space(os.environ[NNI_GEN_SEARCH_SPACE])\n            sys.exit(0)\n\n        if trial_env_vars.NNI_PLATFORM is None:\n            logger.warning(\"This is in standalone mode, the chosen are the first one(s).\")\n            self._chosen_arch = self._standalone_generate_chosen()\n        else:\n            # get chosen arch from tuner\n            self._chosen_arch = nni.get_next_parameter()\n            if self._chosen_arch is None:\n                if trial_env_vars.NNI_PLATFORM == \"unittest\":\n                    # happens if NNI_PLATFORM is intentionally set, e.g., in UT\n                    logger.warning(\"`NNI_PLATFORM` is set but `param` is None. Falling back to standalone mode.\")\n                    self._chosen_arch = self._standalone_generate_chosen()\n                else:\n                    raise RuntimeError(\"Chosen architecture is None. This may be a platform error.\")\n        self.reset()",
  "def _sample_layer_choice(self, mutable, idx, value, search_space_item):\n        \"\"\"\n        Convert layer choice to tensor representation.\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        \"\"\"\n        # doesn't support multihot for layer choice yet\n        assert 0 <= idx < len(mutable) and search_space_item[idx] == value, \\\n            \"Index '{}' in search space '{}' is not '{}'\".format(idx, search_space_item, value)\n        mask = tf.one_hot(idx, len(mutable))\n        return tf.cast(tf.reshape(mask, [-1]), tf.bool)",
  "def _sample_input_choice(self, mutable, idx, value, search_space_item):\n        \"\"\"\n        Convert input choice to tensor representation.\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        \"\"\"\n        candidate_repr = search_space_item[\"candidates\"]\n        multihot_list = [False] * mutable.n_candidates\n        for i, v in zip(idx, value):\n            assert 0 <= i < mutable.n_candidates and candidate_repr[i] == v, \\\n                \"Index '{}' in search space '{}' is not '{}'\".format(i, candidate_repr, v)\n            assert not multihot_list[i], \"'{}' is selected twice in '{}', which is not allowed.\".format(i, idx)\n            multihot_list[i] = True\n        return tf.cast(multihot_list, tf.bool)",
  "def sample_search(self):\n        \"\"\"\n        See :meth:`sample_final`.\n        \"\"\"\n        return self.sample_final()",
  "def sample_final(self):\n        \"\"\"\n        Convert the chosen arch and apply it on model.\n        \"\"\"\n        assert set(self._chosen_arch.keys()) == set(self._search_space.keys()), \\\n            \"Unmatched keys, expected keys '{}' from search space, found '{}'.\".format(self._search_space.keys(),\n                                                                                       self._chosen_arch.keys())\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, (LayerChoice, InputChoice)):\n                assert mutable.key in self._chosen_arch, \\\n                    \"Expected '{}' in chosen arch, but not found.\".format(mutable.key)\n                data = self._chosen_arch[mutable.key]\n                assert isinstance(data, dict) and \"_value\" in data and \"_idx\" in data, \\\n                    \"'{}' is not a valid choice.\".format(data)\n            if isinstance(mutable, LayerChoice):\n                result[mutable.key] = self._sample_layer_choice(mutable, data[\"_idx\"], data[\"_value\"],\n                                                                self._search_space[mutable.key][\"_value\"])\n            elif isinstance(mutable, InputChoice):\n                result[mutable.key] = self._sample_input_choice(mutable, data[\"_idx\"], data[\"_value\"],\n                                                                self._search_space[mutable.key][\"_value\"])\n            elif isinstance(mutable, MutableScope):\n                logger.info(\"Mutable scope '%s' is skipped during parsing choices.\", mutable.key)\n            else:\n                raise TypeError(\"Unsupported mutable type: '%s'.\" % type(mutable))\n        return result",
  "def _standalone_generate_chosen(self):\n        \"\"\"\n        Generate the chosen architecture for standalone mode,\n        i.e., choose the first one(s) for LayerChoice and InputChoice.\n        ::\n            { key_name: {\"_value\": \"conv1\",\n                         \"_idx\": 0} }\n            { key_name: {\"_value\": [\"in1\"],\n                         \"_idx\": [0]} }\n        Returns\n        -------\n        dict\n            the chosen architecture\n        \"\"\"\n        chosen_arch = {}\n        for key, val in self._search_space.items():\n            if val[\"_type\"] == LAYER_CHOICE:\n                choices = val[\"_value\"]\n                chosen_arch[key] = {\"_value\": choices[0], \"_idx\": 0}\n            elif val[\"_type\"] == INPUT_CHOICE:\n                choices = val[\"_value\"][\"candidates\"]\n                n_chosen = val[\"_value\"][\"n_chosen\"]\n                if n_chosen is None:\n                    n_chosen = len(choices)\n                chosen_arch[key] = {\"_value\": choices[:n_chosen], \"_idx\": list(range(n_chosen))}\n            else:\n                raise ValueError(\"Unknown key '%s' and value '%s'.\" % (key, val))\n        return chosen_arch",
  "def _generate_search_space(self):\n        \"\"\"\n        Generate search space from mutables.\n        Here is the search space format:\n        ::\n            { key_name: {\"_type\": \"layer_choice\",\n                         \"_value\": [\"conv1\", \"conv2\"]} }\n            { key_name: {\"_type\": \"input_choice\",\n                         \"_value\": {\"candidates\": [\"in1\", \"in2\"],\n                                    \"n_chosen\": 1}} }\n        Returns\n        -------\n        dict\n            the generated search space\n        \"\"\"\n        search_space = {}\n        for mutable in self.mutables:\n            # for now we only generate flattened search space\n            if isinstance(mutable, LayerChoice):\n                key = mutable.key\n                val = mutable.names\n                search_space[key] = {\"_type\": LAYER_CHOICE, \"_value\": val}\n            elif isinstance(mutable, InputChoice):\n                key = mutable.key\n                search_space[key] = {\"_type\": INPUT_CHOICE,\n                                     \"_value\": {\"candidates\": mutable.choose_from,\n                                                \"n_chosen\": mutable.n_chosen}}\n            elif isinstance(mutable, MutableScope):\n                logger.info(\"Mutable scope '%s' is skipped during generating search space.\", mutable.key)\n            else:\n                raise TypeError(\"Unsupported mutable type: '%s'.\" % type(mutable))\n        return search_space",
  "def _dump_search_space(self, file_path):\n        with open(file_path, \"w\") as ss_file:\n            json.dump(self._search_space, ss_file, sort_keys=True, indent=2)",
  "def query_nb101_trial_stats(arch, num_epochs, isomorphism=True, reduction=None, include_intermediates=False):\n    \"\"\"\n    Query trial stats of NAS-Bench-101 given conditions.\n\n    Parameters\n    ----------\n    arch : dict or None\n        If a dict, it is in the format that is described in\n        :class:`nni.nas.benchmark.nasbench101.Nb101TrialConfig`. Only trial stats\n        matched will be returned. If none, all architectures in the database will be matched.\n    num_epochs : int or None\n        If int, matching results will be returned. Otherwise a wildcard.\n    isomorphism : boolean\n        Whether to match essentially-same architecture, i.e., architecture with the\n        same graph-invariant hash value.\n    reduction : str or None\n        If 'none' or None, all trial stats will be returned directly.\n        If 'mean', fields in trial stats will be averaged given the same trial config.\n    include_intermediates : boolean\n        If true, intermediate results will be returned.\n\n    Returns\n    -------\n    generator of dict\n        A generator of :class:`nni.nas.benchmark.nasbench101.Nb101TrialStats` objects,\n        where each of them has been converted into a dict.\n    \"\"\"\n    fields = []\n    if reduction == 'none':\n        reduction = None\n    if reduction == 'mean':\n        for field_name in Nb101TrialStats._meta.sorted_field_names:\n            if field_name not in ['id', 'config']:\n                fields.append(fn.AVG(getattr(Nb101TrialStats, field_name)).alias(field_name))\n    elif reduction is None:\n        fields.append(Nb101TrialStats)\n    else:\n        raise ValueError('Unsupported reduction: \\'%s\\'' % reduction)\n    query = Nb101TrialStats.select(*fields, Nb101TrialConfig).join(Nb101TrialConfig)\n    conditions = []\n    if arch is not None:\n        if isomorphism:\n            num_vertices = infer_num_vertices(arch)\n            conditions.append(Nb101TrialConfig.hash == hash_module(arch, num_vertices))\n        else:\n            conditions.append(Nb101TrialConfig.arch == arch)\n    if num_epochs is not None:\n        conditions.append(Nb101TrialConfig.num_epochs == num_epochs)\n    if conditions:\n        query = query.where(functools.reduce(lambda a, b: a & b, conditions))\n    if reduction is not None:\n        query = query.group_by(Nb101TrialStats.config)\n    for trial in query:\n        if include_intermediates:\n            data = model_to_dict(trial)\n            # exclude 'trial' from intermediates as it is already available in data\n            data['intermediates'] = [\n                {k: v for k, v in model_to_dict(t).items() if k != 'trial'} for t in trial.intermediates\n            ]\n            yield data\n        else:\n            yield model_to_dict(trial)",
  "class Nb101TrialConfig(Model):\n    \"\"\"\n    Trial config for NAS-Bench-101.\n\n    Attributes\n    ----------\n    arch : dict\n        A dict with keys ``op1``, ``op2``, ... and ``input1``, ``input2``, ... Vertices are\n        enumerate from 0. Since node 0 is input node, it is skipped in this dict. Each ``op``\n        is one of :const:`nni.nas.benchmark.nasbench101.CONV3X3_BN_RELU`,\n        :const:`nni.nas.benchmark.nasbench101.CONV1X1_BN_RELU`, and :const:`nni.nas.benchmark.nasbench101.MAXPOOL3X3`.\n        Each ``input`` is a list of previous nodes. For example ``input5`` can be ``[0, 1, 3]``.\n    num_vertices : int\n        Number of vertices (nodes) in one cell. Should be less than or equal to 7 in default setup.\n    hash : str\n        Graph-invariant MD5 string for this architecture.\n    num_epochs : int\n        Number of epochs planned for this trial. Should be one of 4, 12, 36, 108 in default setup.\n    \"\"\"\n\n    arch = JSONField(json_dumps=json_dumps, index=True)\n    num_vertices = IntegerField(index=True)\n    hash = CharField(max_length=64, index=True)\n    num_epochs = IntegerField(index=True)\n\n    class Meta:\n        database = db",
  "class Nb101TrialStats(Model):\n    \"\"\"\n    Computation statistics for NAS-Bench-101. Each corresponds to one trial.\n    Each config has multiple trials with different random seeds, but unfortunately seed for each trial is unavailable.\n    NAS-Bench-101 trains and evaluates on CIFAR-10 by default. The original training set is divided into\n    40k training images and 10k validation images, and the original validation set is used for test only.\n\n    Attributes\n    ----------\n    config : Nb101TrialConfig\n        Setup for this trial data.\n    train_acc : float\n        Final accuracy on training data, ranging from 0 to 100.\n    valid_acc : float\n        Final accuracy on validation data, ranging from 0 to 100.\n    test_acc : float\n        Final accuracy on test data, ranging from 0 to 100.\n    parameters : float\n        Number of trainable parameters in million.\n    training_time : float\n        Duration of training in seconds.\n    \"\"\"\n    config = ForeignKeyField(Nb101TrialConfig, backref='trial_stats', index=True)\n    train_acc = FloatField()\n    valid_acc = FloatField()\n    test_acc = FloatField()\n    parameters = FloatField()\n    training_time = FloatField()\n\n    class Meta:\n        database = db",
  "class Nb101IntermediateStats(Model):\n    \"\"\"\n    Intermediate statistics for NAS-Bench-101.\n\n    Attributes\n    ----------\n    trial : Nb101TrialStats\n        The exact trial where the intermediate result is produced.\n    current_epoch : int\n        Elapsed epochs when evaluation is done.\n    train_acc : float\n        Intermediate accuracy on training data, ranging from 0 to 100.\n    valid_acc : float\n        Intermediate accuracy on validation data, ranging from 0 to 100.\n    test_acc : float\n        Intermediate accuracy on test data, ranging from 0 to 100.\n    training_time : float\n        Time elapsed in seconds.\n    \"\"\"\n\n    trial = ForeignKeyField(Nb101TrialStats, backref='intermediates', index=True)\n    current_epoch = IntegerField(index=True)\n    train_acc = FloatField()\n    valid_acc = FloatField()\n    test_acc = FloatField()\n    training_time = FloatField()\n\n    class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "def _labeling_from_architecture(architecture, vertices):\n    return [INPUT] + [architecture['op{}'.format(i)] for i in range(1, vertices - 1)] + [OUTPUT]",
  "def _adjancency_matrix_from_architecture(architecture, vertices):\n    matrix = np.zeros((vertices, vertices), dtype=np.bool)\n    for i in range(1, vertices):\n        for k in architecture['input{}'.format(i)]:\n            matrix[k, i] = 1\n    return matrix",
  "def nasbench_format_to_architecture_repr(adjacency_matrix, labeling):\n    \"\"\"\n    Computes a graph-invariance MD5 hash of the matrix and label pair.\n    Imported from NAS-Bench-101 repo.\n\n    Parameters\n    ----------\n    adjacency_matrix : np.ndarray\n        A 2D array of shape NxN, where N is the number of vertices.\n        ``matrix[u][v]`` is 1 if there is a direct edge from `u` to `v`,\n        otherwise it will be 0.\n    labeling : list of str\n        A list of str that starts with input and ends with output. The intermediate\n        nodes are chosen from candidate operators.\n\n    Returns\n    -------\n    tuple and int and dict\n        Converted number of vertices and architecture.\n    \"\"\"\n    num_vertices = adjacency_matrix.shape[0]\n    assert len(labeling) == num_vertices\n    architecture = {}\n    for i in range(1, num_vertices - 1):\n        architecture['op{}'.format(i)] = labeling[i]\n        assert labeling[i] not in [INPUT, OUTPUT]\n    for i in range(1, num_vertices):\n        architecture['input{}'.format(i)] = [k for k in range(i) if adjacency_matrix[k, i]]\n    return num_vertices, architecture",
  "def infer_num_vertices(architecture):\n    \"\"\"\n    Infer number of vertices from an architecture dict.\n\n    Parameters\n    ----------\n    architecture : dict\n        Architecture in NNI format.\n\n    Returns\n    -------\n    int\n        Number of vertices.\n    \"\"\"\n    op_keys = set([k for k in architecture.keys() if k.startswith('op')])\n    intermediate_vertices = len(op_keys)\n    assert op_keys == {'op{}'.format(i) for i in range(1, intermediate_vertices + 1)}\n    return intermediate_vertices + 2",
  "def hash_module(architecture, vertices):\n    \"\"\"\n    Computes a graph-invariance MD5 hash of the matrix and label pair.\n    This snippet is modified from code in NAS-Bench-101 repo.\n\n    Parameters\n    ----------\n    matrix : np.ndarray\n        Square upper-triangular adjacency matrix.\n    labeling : list of int\n        Labels of length equal to both dimensions of matrix.\n\n    Returns\n    -------\n    str\n        MD5 hash of the matrix and labeling.\n    \"\"\"\n    labeling = _labeling_from_architecture(architecture, vertices)\n    labeling = [LABEL2ID[t] for t in labeling]\n    matrix = _adjancency_matrix_from_architecture(architecture, vertices)\n    in_edges = np.sum(matrix, axis=0).tolist()\n    out_edges = np.sum(matrix, axis=1).tolist()\n\n    assert len(in_edges) == len(out_edges) == len(labeling)\n    hashes = list(zip(out_edges, in_edges, labeling))\n    hashes = [hashlib.md5(str(h).encode('utf-8')).hexdigest() for h in hashes]\n    # Computing this up to the diameter is probably sufficient but since the\n    # operation is fast, it is okay to repeat more times.\n    for _ in range(vertices):\n        new_hashes = []\n        for v in range(vertices):\n            in_neighbors = [hashes[w] for w in range(vertices) if matrix[w, v]]\n            out_neighbors = [hashes[w] for w in range(vertices) if matrix[v, w]]\n            new_hashes.append(hashlib.md5(\n                (''.join(sorted(in_neighbors)) + '|' +\n                 ''.join(sorted(out_neighbors)) + '|' +\n                 hashes[v]).encode('utf-8')).hexdigest())\n        hashes = new_hashes\n    fingerprint = hashlib.md5(str(sorted(hashes)).encode('utf-8')).hexdigest()\n\n    return fingerprint",
  "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('input_file',\n                        help='Path to the file to be converted, e.g., nasbench_full.tfrecord')\n    args = parser.parse_args()\n    nasbench = api.NASBench(args.input_file)\n    with db:\n        db.create_tables([Nb101TrialConfig, Nb101TrialStats, Nb101IntermediateStats])\n        for hashval in tqdm(nasbench.hash_iterator(), desc='Dumping data into database'):\n            metadata, metrics = nasbench.get_metrics_from_hash(hashval)\n            num_vertices, architecture = nasbench_format_to_architecture_repr(\n                metadata['module_adjacency'], metadata['module_operations'])\n            assert hashval == hash_module(architecture, num_vertices)\n            for epochs in [4, 12, 36, 108]:\n                trial_config = Nb101TrialConfig.create(\n                    arch=architecture,\n                    num_vertices=num_vertices,\n                    hash=hashval,\n                    num_epochs=epochs\n                )\n\n                for seed in range(3):\n                    cur = metrics[epochs][seed]\n                    trial = Nb101TrialStats.create(\n                        config=trial_config,\n                        train_acc=cur['final_train_accuracy'] * 100,\n                        valid_acc=cur['final_validation_accuracy'] * 100,\n                        test_acc=cur['final_test_accuracy'] * 100,\n                        parameters=metadata['trainable_parameters'] / 1e6,\n                        training_time=cur['final_training_time'] * 60\n                    )\n                    for t in ['halfway', 'final']:\n                        Nb101IntermediateStats.create(\n                            trial=trial,\n                            current_epoch=epochs // 2 if t == 'halfway' else epochs,\n                            training_time=cur[t + '_training_time'],\n                            train_acc=cur[t + '_train_accuracy'] * 100,\n                            valid_acc=cur[t + '_validation_accuracy'] * 100,\n                            test_acc=cur[t + '_test_accuracy'] * 100\n                        )",
  "def query_nb201_trial_stats(arch, num_epochs, dataset, reduction=None, include_intermediates=False):\n    \"\"\"\n    Query trial stats of NAS-Bench-201 given conditions.\n\n    Parameters\n    ----------\n    arch : dict or None\n        If a dict, it is in the format that is described in\n        :class:`nni.nas.benchmark.nasbench201.Nb201TrialConfig`. Only trial stats\n        matched will be returned. If none, all architectures in the database will be matched.\n    num_epochs : int or None\n        If int, matching results will be returned. Otherwise a wildcard.\n    dataset : str or None\n        If specified, can be one of the dataset available in :class:`nni.nas.benchmark.nasbench201.Nb201TrialConfig`.\n        Otherwise a wildcard.\n    reduction : str or None\n        If 'none' or None, all trial stats will be returned directly.\n        If 'mean', fields in trial stats will be averaged given the same trial config.\n    include_intermediates : boolean\n        If true, intermediate results will be returned.\n\n    Returns\n    -------\n    generator of dict\n        A generator of :class:`nni.nas.benchmark.nasbench201.Nb201TrialStats` objects,\n        where each of them has been converted into a dict.\n    \"\"\"\n    fields = []\n    if reduction == 'none':\n        reduction = None\n    if reduction == 'mean':\n        for field_name in Nb201TrialStats._meta.sorted_field_names:\n            if field_name not in ['id', 'config', 'seed']:\n                fields.append(fn.AVG(getattr(Nb201TrialStats, field_name)).alias(field_name))\n    elif reduction is None:\n        fields.append(Nb201TrialStats)\n    else:\n        raise ValueError('Unsupported reduction: \\'%s\\'' % reduction)\n    query = Nb201TrialStats.select(*fields, Nb201TrialConfig).join(Nb201TrialConfig)\n    conditions = []\n    if arch is not None:\n        conditions.append(Nb201TrialConfig.arch == arch)\n    if num_epochs is not None:\n        conditions.append(Nb201TrialConfig.num_epochs == num_epochs)\n    if dataset is not None:\n        conditions.append(Nb201TrialConfig.dataset == dataset)\n    if conditions:\n        query = query.where(functools.reduce(lambda a, b: a & b, conditions))\n    if reduction is not None:\n        query = query.group_by(Nb201TrialStats.config)\n    for trial in query:\n        if include_intermediates:\n            data = model_to_dict(trial)\n            # exclude 'trial' from intermediates as it is already available in data\n            data['intermediates'] = [\n                {k: v for k, v in model_to_dict(t).items() if k != 'trial'} for t in trial.intermediates\n            ]\n            yield data\n        else:\n            yield model_to_dict(trial)",
  "class Nb201TrialConfig(Model):\n    \"\"\"\n    Trial config for NAS-Bench-201.\n\n    Attributes\n    ----------\n    arch : dict\n        A dict with keys ``0_1``, ``0_2``, ``0_3``, ``1_2``, ``1_3``, ``2_3``, each of which\n        is an operator chosen from :const:`nni.nas.benchmark.nasbench201.NONE`,\n        :const:`nni.nas.benchmark.nasbench201.SKIP_CONNECT`,\n        :const:`nni.nas.benchmark.nasbench201.CONV_1X1`,\n        :const:`nni.nas.benchmark.nasbench201.CONV_3X3` and :const:`nni.nas.benchmark.nasbench201.AVG_POOL_3X3`.\n    num_epochs : int\n        Number of epochs planned for this trial. Should be one of 12 and 200.\n    num_channels: int\n        Number of channels for initial convolution. 16 by default.\n    num_cells: int\n        Number of cells per stage. 5 by default.\n    dataset: str\n        Dataset used for training and evaluation. NAS-Bench-201 provides the following 4 options:\n        ``cifar10-valid`` (training data is splited into 25k for training and 25k for validation,\n        validation data is used for test), ``cifar10`` (training data is used in training, validation\n        data is splited into 25k for validation and 25k for testing), ``cifar100`` (same protocol as ``cifar10``),\n        and ``imagenet16-120`` (a subset of 120 classes in ImageNet, downscaled to 16x16, using training data\n        for training, 6k images from validation set for validation and the other 6k for testing).\n    \"\"\"\n\n    arch = JSONField(json_dumps=json_dumps, index=True)\n    num_epochs = IntegerField(index=True)\n    num_channels = IntegerField()\n    num_cells = IntegerField()\n    dataset = CharField(max_length=20, index=True, choices=[\n        'cifar10-valid',  # 25k+25k+10k\n        'cifar10',  # 50k+5k+5k\n        'cifar100',  # 50k+5k+5k\n        'imagenet16-120',\n    ])\n\n    class Meta:\n        database = db",
  "class Nb201TrialStats(Model):\n    \"\"\"\n    Computation statistics for NAS-Bench-201. Each corresponds to one trial.\n\n    Attributes\n    ----------\n    config : Nb201TrialConfig\n        Setup for this trial data.\n    seed : int\n        Random seed selected, for reproduction.\n    train_acc : float\n        Final accuracy on training data, ranging from 0 to 100.\n    valid_acc : float\n        Final accuracy on validation data, ranging from 0 to 100.\n    test_acc : float\n        Final accuracy on test data, ranging from 0 to 100.\n    ori_test_acc : float\n        Test accuracy on original validation set (10k for CIFAR and 12k for Imagenet16-120),\n        ranging from 0 to 100.\n    train_loss : float or None\n        Final cross entropy loss on training data. Note that loss could be NaN, in which case\n        this attributed will be None.\n    valid_loss : float or None\n        Final cross entropy loss on validation data.\n    test_loss : float or None\n        Final cross entropy loss on test data.\n    ori_test_loss : float or None\n        Final cross entropy loss on original validation set.\n    parameters : float\n        Number of trainable parameters in million.\n    latency : float\n        Latency in seconds.\n    flops : float\n        FLOPs in million.\n    training_time : float\n        Duration of training in seconds.\n    valid_evaluation_time : float\n        Time elapsed to evaluate on validation set.\n    test_evaluation_time : float\n        Time elapsed to evaluate on test set.\n    ori_test_evaluation_time : float\n        Time elapsed to evaluate on original test set.\n    \"\"\"\n    config = ForeignKeyField(Nb201TrialConfig, backref='trial_stats', index=True)\n    seed = IntegerField()\n    train_acc = FloatField()\n    valid_acc = FloatField()\n    test_acc = FloatField()\n    ori_test_acc = FloatField()  # test accuracy of the original test set\n    train_loss = FloatField(null=True)  # possibly nan\n    valid_loss = FloatField(null=True)\n    test_loss = FloatField(null=True)\n    ori_test_loss = FloatField(null=True)\n    parameters = FloatField()  # parameters in million\n    latency = FloatField()  # latency in milliseconds\n    flops = FloatField()  # flops in million\n    training_time = FloatField()\n    valid_evaluation_time = FloatField()\n    test_evaluation_time = FloatField()\n    ori_test_evaluation_time = FloatField()\n\n    class Meta:\n        database = db",
  "class Nb201IntermediateStats(Model):\n    \"\"\"\n    Intermediate statistics for NAS-Bench-201.\n\n    Attributes\n    ----------\n    trial : Nb201TrialStats\n        Corresponding trial.\n    current_epoch : int\n        Elapsed epochs.\n    train_acc : float\n        Current accuracy on training data, ranging from 0 to 100.\n    valid_acc : float\n        Current accuracy on validation data, ranging from 0 to 100.\n    test_acc : float\n        Current accuracy on test data, ranging from 0 to 100.\n    ori_test_acc : float\n        Test accuracy on original validation set (10k for CIFAR and 12k for Imagenet16-120),\n        ranging from 0 to 100.\n    train_loss : float or None\n        Current cross entropy loss on training data.\n    valid_loss : float or None\n        Current cross entropy loss on validation data.\n    test_loss : float or None\n        Current cross entropy loss on test data.\n    ori_test_loss : float or None\n        Current cross entropy loss on original validation set.\n    \"\"\"\n\n    trial = ForeignKeyField(Nb201TrialStats, backref='intermediates', index=True)\n    current_epoch = IntegerField(index=True)\n    train_acc = FloatField(null=True)\n    valid_acc = FloatField(null=True)\n    test_acc = FloatField(null=True)\n    ori_test_acc = FloatField(null=True)\n    train_loss = FloatField(null=True)\n    valid_loss = FloatField(null=True)\n    test_loss = FloatField(null=True)\n    ori_test_loss = FloatField(null=True)\n\n    class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "def parse_arch_str(arch_str):\n    mp = {\n        'none': NONE,\n        'skip_connect': SKIP_CONNECT,\n        'nor_conv_1x1': CONV_1X1,\n        'nor_conv_3x3': CONV_3X3,\n        'avg_pool_3x3': AVG_POOL_3X3\n    }\n    m = re.match(r'\\|(.*)~0\\|\\+\\|(.*)~0\\|(.*)~1\\|\\+\\|(.*)~0\\|(.*)~1\\|(.*)~2\\|', arch_str)\n    return {\n        '0_1': mp[m.group(1)],\n        '0_2': mp[m.group(2)],\n        '1_2': mp[m.group(3)],\n        '0_3': mp[m.group(4)],\n        '1_3': mp[m.group(5)],\n        '2_3': mp[m.group(6)]\n    }",
  "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('input_file',\n                        help='Path to the file to be converted, e.g., NAS-Bench-201-v1_1-096897.pth.')\n    args = parser.parse_args()\n    dataset_split = {\n        'cifar10-valid': ['train', 'x-valid', 'ori-test', 'ori-test'],\n        'cifar10': ['train', 'ori-test', 'ori-test', 'ori-test'],\n        'cifar100': ['train', 'x-valid', 'x-test', 'ori-test'],\n        'imagenet16-120': ['train', 'x-valid', 'x-test', 'ori-test'],\n    }\n\n    with db:\n        db.create_tables([Nb201TrialConfig, Nb201TrialStats, Nb201IntermediateStats])\n        print('Loading NAS-Bench-201 pickle...')\n        nb201_data = torch.load(args.input_file)\n        print('Dumping architectures...')\n        for arch_str in nb201_data['meta_archs']:\n            arch_json = parse_arch_str(arch_str)\n            for epochs in [12, 200]:\n                for dataset in Nb201TrialConfig.dataset.choices:\n                    Nb201TrialConfig.create(arch=arch_json, num_epochs=epochs, dataset=dataset,\n                                            num_channels=16, num_cells=5)\n        for arch_info in tqdm.tqdm(nb201_data['arch2infos'].values(),\n                                   desc='Processing architecture statistics'):\n            for epochs_verb, d in arch_info.items():\n                if epochs_verb == 'less':\n                    epochs = 12\n                else:\n                    epochs = 200\n                arch_json = parse_arch_str(d['arch_str'])\n                for (dataset, seed), r in d['all_results'].items():\n                    sp = dataset_split[dataset.lower()]\n                    data_parsed = {\n                        'train_acc': r['train_acc1es'][epochs - 1],\n                        'valid_acc': r['eval_acc1es']['{}@{}'.format(sp[1], epochs - 1)],\n                        'test_acc': r['eval_acc1es']['{}@{}'.format(sp[2], epochs - 1)],\n                        'ori_test_acc': r['eval_acc1es']['{}@{}'.format(sp[3], epochs - 1)],\n                        'train_loss': r['train_losses'][epochs - 1],\n                        'valid_loss': r['eval_losses']['{}@{}'.format(sp[1], epochs - 1)],\n                        'test_loss': r['eval_losses']['{}@{}'.format(sp[2], epochs - 1)],\n                        'ori_test_loss': r['eval_losses']['{}@{}'.format(sp[3], epochs - 1)],\n                        'parameters': r['params'],\n                        'flops': r['flop'],\n                        'latency': r['latency'][0],\n                        'training_time': r['train_times'][epochs - 1] * epochs,\n                        'valid_evaluation_time': r['eval_times']['{}@{}'.format(sp[1], epochs - 1)],\n                        'test_evaluation_time': r['eval_times']['{}@{}'.format(sp[2], epochs - 1)],\n                        'ori_test_evaluation_time': r['eval_times']['{}@{}'.format(sp[3], epochs - 1)],\n                    }\n                    config = Nb201TrialConfig.get(\n                        (Nb201TrialConfig.num_epochs == epochs) &\n                        (Nb201TrialConfig.arch == arch_json) &\n                        (Nb201TrialConfig.dataset == dataset.lower())\n                    )\n                    trial_stats = Nb201TrialStats.create(config=config, seed=seed, **data_parsed)\n                    intermediate_stats = []\n                    for epoch in range(epochs):\n                        data_parsed = {\n                            'train_acc': r['train_acc1es'].get(epoch),\n                            'valid_acc': r['eval_acc1es'].get('{}@{}'.format(sp[1], epoch)),\n                            'test_acc': r['eval_acc1es'].get('{}@{}'.format(sp[2], epoch)),\n                            'ori_test_acc': r['eval_acc1es'].get('{}@{}'.format(sp[3], epoch)),\n                            'train_loss': r['train_losses'].get(epoch),\n                            'valid_loss': r['eval_losses'].get('{}@{}'.format(sp[1], epoch)),\n                            'test_loss': r['eval_losses'].get('{}@{}'.format(sp[2], epoch)),\n                            'ori_test_loss': r['eval_losses'].get('{}@{}'.format(sp[3], epoch)),\n                        }\n                        if all([v is None for v in data_parsed.values()]):\n                            continue\n                        data_parsed.update(current_epoch=epoch + 1, trial=trial_stats)\n                        intermediate_stats.append(data_parsed)\n                    Nb201IntermediateStats.insert_many(intermediate_stats).execute(db)",
  "def query_nds_trial_stats(model_family, proposer, generator, model_spec, cell_spec, dataset,\n                          num_epochs=None, reduction=None, include_intermediates=False):\n    \"\"\"\n    Query trial stats of NDS given conditions.\n\n    Parameters\n    ----------\n    model_family : str or None\n        If str, can be one of the model families available in :class:`nni.nas.benchmark.nds.NdsTrialConfig`.\n        Otherwise a wildcard.\n    proposer : str or None\n        If str, can be one of the proposers available in :class:`nni.nas.benchmark.nds.NdsTrialConfig`. Otherwise a wildcard.\n    generator : str or None\n        If str, can be one of the generators available in :class:`nni.nas.benchmark.nds.NdsTrialConfig`. Otherwise a wildcard.\n    model_spec : dict or None\n        If specified, can be one of the model spec available in :class:`nni.nas.benchmark.nds.NdsTrialConfig`.\n        Otherwise a wildcard.\n    cell_spec : dict or None\n        If specified, can be one of the cell spec available in :class:`nni.nas.benchmark.nds.NdsTrialConfig`.\n        Otherwise a wildcard.\n    dataset : str or None\n        If str, can be one of the datasets available in :class:`nni.nas.benchmark.nds.NdsTrialConfig`. Otherwise a wildcard.\n    num_epochs : float or None\n        If int, matching results will be returned. Otherwise a wildcard.\n    reduction : str or None\n        If 'none' or None, all trial stats will be returned directly.\n        If 'mean', fields in trial stats will be averaged given the same trial config.\n    include_intermediates : boolean\n        If true, intermediate results will be returned.\n\n    Returns\n    -------\n    generator of dict\n        A generator of :class:`nni.nas.benchmark.nds.NdsTrialStats` objects,\n        where each of them has been converted into a dict.\n    \"\"\"\n    fields = []\n    if reduction == 'none':\n        reduction = None\n    if reduction == 'mean':\n        for field_name in NdsTrialStats._meta.sorted_field_names:\n            if field_name not in ['id', 'config', 'seed']:\n                fields.append(fn.AVG(getattr(NdsTrialStats, field_name)).alias(field_name))\n    elif reduction is None:\n        fields.append(NdsTrialStats)\n    else:\n        raise ValueError('Unsupported reduction: \\'%s\\'' % reduction)\n    query = NdsTrialStats.select(*fields, NdsTrialConfig).join(NdsTrialConfig)\n    conditions = []\n    for field_name in ['model_family', 'proposer', 'generator', 'model_spec', 'cell_spec',\n                       'dataset', 'num_epochs']:\n        if locals()[field_name] is not None:\n            conditions.append(getattr(NdsTrialConfig, field_name) == locals()[field_name])\n    if conditions:\n        query = query.where(functools.reduce(lambda a, b: a & b, conditions))\n    if reduction is not None:\n        query = query.group_by(NdsTrialStats.config)\n    for trial in query:\n        if include_intermediates:\n            data = model_to_dict(trial)\n            # exclude 'trial' from intermediates as it is already available in data\n            data['intermediates'] = [\n                {k: v for k, v in model_to_dict(t).items() if k != 'trial'} for t in trial.intermediates\n            ]\n            yield data\n        else:\n            yield model_to_dict(trial)",
  "class NdsTrialConfig(Model):\n    \"\"\"\n    Trial config for NDS.\n\n    Attributes\n    ----------\n    model_family : str\n        Could be ``nas_cell``, ``residual_bottleneck``, ``residual_basic`` or ``vanilla``.\n    model_spec : dict\n        If ``model_family`` is ``nas_cell``, it contains ``num_nodes_normal``, ``num_nodes_reduce``, ``depth``,\n        ``width``, ``aux`` and ``drop_prob``. If ``model_family`` is ``residual_bottleneck``, it contains ``bot_muls``,\n        ``ds`` (depths), ``num_gs`` (number of groups) and ``ss`` (strides). If ``model_family`` is ``residual_basic`` or\n        ``vanilla``, it contains ``ds``, ``ss`` and ``ws``.\n    cell_spec : dict\n        If ``model_family`` is not ``nas_cell`` it will be an empty dict. Otherwise, it specifies\n        ``<normal/reduce>_<i>_<op/input>_<x/y>``, where i ranges from 0 to ``num_nodes_<normal/reduce> - 1``.\n        If it is an ``op``, the value is chosen from the constants specified previously like :const:`nni.nas.benchmark.nds.CONV_1X1`.\n        If it is i's ``input``, the value range from 0 to ``i + 1``, as ``nas_cell`` uses previous two nodes as inputs, and\n        node 0 is actually the second node. Refer to NASNet paper for details. Finally, another two key-value pairs\n        ``normal_concat`` and ``reduce_concat`` specify which nodes are eventually concatenated into output.\n    dataset : str\n        Dataset used. Could be ``cifar10`` or ``imagenet``.\n    generator : str\n        Can be one of ``random`` which generates configurations at random, while keeping learning rate and weight decay fixed,\n        ``fix_w_d`` which further keeps ``width`` and ``depth`` fixed, only applicable for ``nas_cell``. ``tune_lr_wd`` which\n        further tunes learning rate and weight decay.\n    proposer : str\n        Paper who has proposed the distribution for random sampling. Available proposers include ``nasnet``, ``darts``, ``enas``,\n        ``pnas``, ``amoeba``, ``vanilla``, ``resnext-a``, ``resnext-b``, ``resnet``, ``resnet-b`` (ResNet with bottleneck).\n        See NDS paper for details.\n    base_lr : float\n        Initial learning rate.\n    weight_decay : float\n        L2 weight decay applied on weights.\n    num_epochs : int\n        Number of epochs scheduled, during which learning rate will decay to 0 following cosine annealing.\n    \"\"\"\n\n    model_family = CharField(max_length=20, index=True, choices=[\n        'nas_cell',\n        'residual_bottleneck',\n        'residual_basic',\n        'vanilla',\n    ])\n    model_spec = JSONField(json_dumps=json_dumps, index=True)\n    cell_spec = JSONField(json_dumps=json_dumps, index=True, null=True)\n    dataset = CharField(max_length=15, index=True, choices=['cifar10', 'imagenet'])\n    generator = CharField(max_length=15, index=True, choices=[\n        'random',\n        'fix_w_d',\n        'tune_lr_wd',\n    ])\n    proposer = CharField(max_length=15, index=True)\n    base_lr = FloatField()\n    weight_decay = FloatField()\n    num_epochs = IntegerField()\n\n    class Meta:\n        database = db",
  "class NdsTrialStats(Model):\n    \"\"\"\n    Computation statistics for NDS. Each corresponds to one trial.\n\n    Attributes\n    ----------\n    config : NdsTrialConfig\n        Corresponding config for trial.\n    seed : int\n        Random seed selected, for reproduction.\n    final_train_acc : float\n        Final accuracy on training data, ranging from 0 to 100.\n    final_train_loss : float or None\n        Final cross entropy loss on training data. Could be NaN (None).\n    final_test_acc : float\n        Final accuracy on test data, ranging from 0 to 100.\n    best_train_acc : float\n        Best accuracy on training data, ranging from 0 to 100.\n    best_train_loss : float or None\n        Best cross entropy loss on training data. Could be NaN (None).\n    best_test_acc : float\n        Best accuracy on test data, ranging from 0 to 100.\n    parameters : float\n        Number of trainable parameters in million.\n    flops : float\n        FLOPs in million.\n    iter_time : float\n        Seconds elapsed for each iteration.\n    \"\"\"\n    config = ForeignKeyField(NdsTrialConfig, backref='trial_stats', index=True)\n    seed = IntegerField()\n    final_train_acc = FloatField()\n    final_train_loss = FloatField(null=True)\n    final_test_acc = FloatField()\n    best_train_acc = FloatField()\n    best_train_loss = FloatField(null=True)\n    best_test_acc = FloatField()\n    parameters = FloatField()\n    flops = FloatField()\n    iter_time = FloatField()\n\n    class Meta:\n        database = db",
  "class NdsIntermediateStats(Model):\n    \"\"\"\n    Intermediate statistics for NDS.\n\n    Attributes\n    ----------\n    trial : NdsTrialStats\n        Corresponding trial.\n    current_epoch : int\n        Elapsed epochs.\n    train_loss : float or None\n        Current cross entropy loss on training data. Can be NaN (None).\n    train_acc : float\n        Current accuracy on training data, ranging from 0 to 100.\n    test_acc : float\n        Current accuracy on test data, ranging from 0 to 100.\n    \"\"\"\n\n    trial = ForeignKeyField(NdsTrialStats, backref='intermediates', index=True)\n    current_epoch = IntegerField(index=True)\n    train_loss = FloatField(null=True)\n    train_acc = FloatField()\n    test_acc = FloatField()\n\n    class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "class Meta:\n        database = db",
  "def inject_item(db, item, proposer, dataset, generator):\n    if 'genotype' in item['net']:\n        model_family = 'nas_cell'\n        num_nodes_normal = len(item['net']['genotype']['normal']) // 2\n        num_nodes_reduce = len(item['net']['genotype']['reduce']) // 2\n        model_spec = {\n            'num_nodes_normal': num_nodes_normal,\n            'num_nodes_reduce': num_nodes_reduce,\n            'depth': item['net']['depth'],\n            'width': item['net']['width'],\n            'aux': item['net']['aux'],\n            'drop_prob': item['net']['drop_prob'],\n        }\n        cell_spec = {}\n        for cell_type in ['normal', 'reduce']:\n            for i in range(num_nodes_normal):\n                for j, label in enumerate(['x', 'y']):\n                    cell_spec['{}_{}_op_{}'.format(cell_type, i, label)] = \\\n                        item['net']['genotype'][cell_type][i * 2 + j][0]\n                    cell_spec['{}_{}_input_{}'.format(cell_type, i, label)] = \\\n                        item['net']['genotype'][cell_type][i * 2 + j][1]\n            cell_spec['{}_concat'.format(cell_type)] = item['net']['genotype']['{}_concat'.format(cell_type)]\n    else:\n        if item['net']['block_type'].startswith('res_bottleneck'):\n            model_family = 'residual_bottleneck'\n        elif item['net']['block_type'].startswith('res_basic'):\n            model_family = 'residual_basic'\n        elif item['net']['block_type'].startswith('double_plain'):\n            model_family = 'vanilla'\n        else:\n            raise ValueError('Unrecognized block type')\n        model_spec = {k: v for k, v in item['net'].items() if v and k != 'block_type'}\n        cell_spec = {}\n    trial_config, _ = NdsTrialConfig.get_or_create(\n        model_family=model_family,\n        model_spec=model_spec,\n        cell_spec=cell_spec,\n        proposer=proposer,\n        base_lr=item['optim']['base_lr'],\n        weight_decay=item['optim']['wd'],\n        num_epochs=item['optim']['max_ep'],\n        dataset=dataset,\n        generator=generator\n    )\n    assert len(item['train_ep_top1']) == len(item['test_ep_top1']) == trial_config.num_epochs\n    trial = NdsTrialStats.create(\n        config=trial_config,\n        seed=item['rng_seed'],\n        final_train_acc=100 - item['train_ep_top1'][-1],\n        final_train_loss=item['train_ep_loss'][-1],\n        final_test_acc=100 - item['test_ep_top1'][-1],\n        best_train_acc=100 - min(item['train_ep_top1']),\n        best_train_loss=np.nanmin(item['train_ep_loss']).item(),\n        best_test_acc=100 - min(item['test_ep_top1']),\n        parameters=item['params'] / 1e6,\n        flops=item['flops'] / 1e6,\n        iter_time=item['iter_time']\n    )\n    intermediate_stats = []\n    for i in range(trial_config.num_epochs):\n        intermediate_stats.append({\n            'trial': trial,\n            'current_epoch': i + 1,\n            'train_loss': item['train_ep_loss'][i],\n            'train_acc': 100 - item['train_ep_top1'][i],\n            'test_acc': 100 - item['test_ep_top1'][i]\n        })\n    NdsIntermediateStats.insert_many(intermediate_stats).execute(db)",
  "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('input_dir', help='Path to extracted NDS data dir.')\n    args = parser.parse_args()\n\n    sweep_list = [\n        'Amoeba.json',\n        'Amoeba_in.json',\n        'DARTS.json',\n        'DARTS_fix-w-d.json',\n        'DARTS_in.json',\n        'DARTS_lr-wd.json',\n        'DARTS_lr-wd_in.json',\n        'ENAS.json',\n        'ENAS_fix-w-d.json',\n        'ENAS_in.json',\n        'NASNet.json',\n        'NASNet_in.json',\n        'PNAS.json',\n        'PNAS_fix-w-d.json',\n        'PNAS_in.json',\n        'ResNeXt-A.json',\n        'ResNeXt-A_in.json',\n        'ResNeXt-B.json',\n        'ResNeXt-B_in.json',\n        'ResNet-B.json',\n        'ResNet.json',\n        'ResNet_lr-wd.json',\n        'ResNet_lr-wd_in.json',\n        'ResNet_reruns.json',\n        'ResNet_rng1.json',\n        'ResNet_rng2.json',\n        'ResNet_rng3.json',\n        'Vanilla.json',\n        'Vanilla_lr-wd.json',\n        'Vanilla_lr-wd_in.json',\n        'Vanilla_reruns.json',\n        'Vanilla_rng1.json',\n        'Vanilla_rng2.json',\n        'Vanilla_rng3.json'\n    ]\n\n    with db:\n        db.create_tables([NdsTrialConfig, NdsTrialStats, NdsIntermediateStats])\n        for json_idx, json_file in enumerate(sweep_list, start=1):\n            if 'fix-w-d' in json_file:\n                generator = 'fix_w_d'\n            elif 'lr-wd' in json_file:\n                generator = 'tune_lr_wd'\n            else:\n                generator = 'random'\n            if '_in' in json_file:\n                dataset = 'imagenet'\n            else:\n                dataset = 'cifar10'\n            proposer = json_file.split(\".\")[0].split(\"_\")[0].lower()\n            with open(os.path.join(args.input_dir, json_file), 'r') as f:\n                data = json.load(f)\n            if 'top' in data and 'mid' in data:\n                for t in tqdm.tqdm(data['top'],\n                                   desc='[{}/{}] Processing {} (top)'.format(json_idx, len(sweep_list), json_file)):\n                    inject_item(db, t, proposer, dataset, generator)\n                for t in tqdm.tqdm(data['mid'],\n                                   desc='[{}/{}] Processing {} (mid)'.format(json_idx, len(sweep_list), json_file)):\n                    inject_item(db, t, proposer, dataset, generator)\n            else:\n                for job in tqdm.tqdm(data,\n                                     desc='[{}/{}] Processing {}'.format(json_idx, len(sweep_list), json_file)):\n                    inject_item(db, job, proposer, dataset, generator)",
  "def global_mutable_counting():\n    \"\"\"\n    A program level counter starting from 1.\n    \"\"\"\n    global _counter\n    _counter += 1\n    return _counter",
  "def _reset_global_mutable_counting():\n    \"\"\"\n    Reset the global mutable counting to count from 1. Useful when defining multiple models with default keys.\n    \"\"\"\n    global _counter\n    _counter = 0",
  "def to_device(obj, device):\n    \"\"\"\n    Move a tensor, tuple, list, or dict onto device.\n    \"\"\"\n    if torch.is_tensor(obj):\n        return obj.to(device)\n    if isinstance(obj, tuple):\n        return tuple(to_device(t, device) for t in obj)\n    if isinstance(obj, list):\n        return [to_device(t, device) for t in obj]\n    if isinstance(obj, dict):\n        return {k: to_device(v, device) for k, v in obj.items()}\n    if isinstance(obj, (int, float, str)):\n        return obj\n    raise ValueError(\"'%s' has unsupported type '%s'\" % (obj, type(obj)))",
  "def to_list(arr):\n    if torch.is_tensor(arr):\n        return arr.cpu().numpy().tolist()\n    if isinstance(arr, np.ndarray):\n        return arr.tolist()\n    if isinstance(arr, (list, tuple)):\n        return list(arr)\n    return arr",
  "class AverageMeterGroup:\n    \"\"\"\n    Average meter group for multiple average meters.\n    \"\"\"\n\n    def __init__(self):\n        self.meters = OrderedDict()\n\n    def update(self, data):\n        \"\"\"\n        Update the meter group with a dict of metrics.\n        Non-exist average meters will be automatically created.\n        \"\"\"\n        for k, v in data.items():\n            if k not in self.meters:\n                self.meters[k] = AverageMeter(k, \":4f\")\n            self.meters[k].update(v)\n\n    def __getattr__(self, item):\n        return self.meters[item]\n\n    def __getitem__(self, item):\n        return self.meters[item]\n\n    def __str__(self):\n        return \"  \".join(str(v) for v in self.meters.values())\n\n    def summary(self):\n        \"\"\"\n        Return a summary string of group data.\n        \"\"\"\n        return \"  \".join(v.summary() for v in self.meters.values())",
  "class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value.\n\n    Parameters\n    ----------\n    name : str\n        Name to display.\n    fmt : str\n        Format string to print the values.\n    \"\"\"\n\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Reset the meter.\n        \"\"\"\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        \"\"\"\n        Update with value and weight.\n\n        Parameters\n        ----------\n        val : float or int\n            The new value to be accounted in.\n        n : int\n            The weight of the new value.\n        \"\"\"\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n    def summary(self):\n        fmtstr = '{name}: {avg' + self.fmt + '}'\n        return fmtstr.format(**self.__dict__)",
  "class StructuredMutableTreeNode:\n    \"\"\"\n    A structured representation of a search space.\n    A search space comes with a root (with `None` stored in its `mutable`), and a bunch of children in its `children`.\n    This tree can be seen as a \"flattened\" version of the module tree. Since nested mutable entity is not supported yet,\n    the following must be true: each subtree corresponds to a ``MutableScope`` and each leaf corresponds to a\n    ``Mutable`` (other than ``MutableScope``).\n\n    Parameters\n    ----------\n    mutable : nni.nas.pytorch.mutables.Mutable\n        The mutable that current node is linked with.\n    \"\"\"\n\n    def __init__(self, mutable):\n        self.mutable = mutable\n        self.children = []\n\n    def add_child(self, mutable):\n        \"\"\"\n        Add a tree node to the children list of current node.\n        \"\"\"\n        self.children.append(StructuredMutableTreeNode(mutable))\n        return self.children[-1]\n\n    def type(self):\n        \"\"\"\n        Return the ``type`` of mutable content.\n        \"\"\"\n        return type(self.mutable)\n\n    def __iter__(self):\n        return self.traverse()\n\n    def traverse(self, order=\"pre\", deduplicate=True, memo=None):\n        \"\"\"\n        Return a generator that generates a list of mutables in this tree.\n\n        Parameters\n        ----------\n        order : str\n            pre or post. If pre, current mutable is yield before children. Otherwise after.\n        deduplicate : bool\n            If true, mutables with the same key will not appear after the first appearance.\n        memo : dict\n            An auxiliary dict that memorize keys seen before, so that deduplication is possible.\n\n        Returns\n        -------\n        generator of Mutable\n        \"\"\"\n        if memo is None:\n            memo = set()\n        assert order in [\"pre\", \"post\"]\n        if order == \"pre\":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable\n        for child in self.children:\n            for m in child.traverse(order=order, deduplicate=deduplicate, memo=memo):\n                yield m\n        if order == \"post\":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable",
  "def __init__(self):\n        self.meters = OrderedDict()",
  "def update(self, data):\n        \"\"\"\n        Update the meter group with a dict of metrics.\n        Non-exist average meters will be automatically created.\n        \"\"\"\n        for k, v in data.items():\n            if k not in self.meters:\n                self.meters[k] = AverageMeter(k, \":4f\")\n            self.meters[k].update(v)",
  "def __getattr__(self, item):\n        return self.meters[item]",
  "def __getitem__(self, item):\n        return self.meters[item]",
  "def __str__(self):\n        return \"  \".join(str(v) for v in self.meters.values())",
  "def summary(self):\n        \"\"\"\n        Return a summary string of group data.\n        \"\"\"\n        return \"  \".join(v.summary() for v in self.meters.values())",
  "def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()",
  "def reset(self):\n        \"\"\"\n        Reset the meter.\n        \"\"\"\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0",
  "def update(self, val, n=1):\n        \"\"\"\n        Update with value and weight.\n\n        Parameters\n        ----------\n        val : float or int\n            The new value to be accounted in.\n        n : int\n            The weight of the new value.\n        \"\"\"\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count",
  "def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)",
  "def summary(self):\n        fmtstr = '{name}: {avg' + self.fmt + '}'\n        return fmtstr.format(**self.__dict__)",
  "def __init__(self, mutable):\n        self.mutable = mutable\n        self.children = []",
  "def add_child(self, mutable):\n        \"\"\"\n        Add a tree node to the children list of current node.\n        \"\"\"\n        self.children.append(StructuredMutableTreeNode(mutable))\n        return self.children[-1]",
  "def type(self):\n        \"\"\"\n        Return the ``type`` of mutable content.\n        \"\"\"\n        return type(self.mutable)",
  "def __iter__(self):\n        return self.traverse()",
  "def traverse(self, order=\"pre\", deduplicate=True, memo=None):\n        \"\"\"\n        Return a generator that generates a list of mutables in this tree.\n\n        Parameters\n        ----------\n        order : str\n            pre or post. If pre, current mutable is yield before children. Otherwise after.\n        deduplicate : bool\n            If true, mutables with the same key will not appear after the first appearance.\n        memo : dict\n            An auxiliary dict that memorize keys seen before, so that deduplication is possible.\n\n        Returns\n        -------\n        generator of Mutable\n        \"\"\"\n        if memo is None:\n            memo = set()\n        assert order in [\"pre\", \"post\"]\n        if order == \"pre\":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable\n        for child in self.children:\n            for m in child.traverse(order=order, deduplicate=deduplicate, memo=memo):\n                yield m\n        if order == \"post\":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable",
  "class BaseMutator(nn.Module):\n    \"\"\"\n    A mutator is responsible for mutating a graph by obtaining the search space from the network and implementing\n    callbacks that are called in ``forward`` in mutables.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model to apply mutator on.\n    \"\"\"\n\n    def __init__(self, model):\n        super().__init__()\n        self.__dict__[\"model\"] = model\n        self._structured_mutables = self._parse_search_space(self.model)\n\n    def _parse_search_space(self, module, root=None, prefix=\"\", memo=None, nested_detection=None):\n        if memo is None:\n            memo = set()\n        if root is None:\n            root = StructuredMutableTreeNode(None)\n        if module not in memo:\n            memo.add(module)\n            if isinstance(module, Mutable):\n                if nested_detection is not None:\n                    raise RuntimeError(\"Cannot have nested search space. Error at {} in {}\"\n                                       .format(module, nested_detection))\n                module.name = prefix\n                module.set_mutator(self)\n                root = root.add_child(module)\n                if not isinstance(module, MutableScope):\n                    nested_detection = module\n                if isinstance(module, InputChoice):\n                    for k in module.choose_from:\n                        if k != InputChoice.NO_KEY and k not in [m.key for m in memo if isinstance(m, Mutable)]:\n                            raise RuntimeError(\"'{}' required by '{}' not found in keys that appeared before, and is not NO_KEY.\"\n                                               .format(k, module.key))\n            for name, submodule in module._modules.items():\n                if submodule is None:\n                    continue\n                submodule_prefix = prefix + (\".\" if prefix else \"\") + name\n                self._parse_search_space(submodule, root, submodule_prefix, memo=memo,\n                                         nested_detection=nested_detection)\n        return root\n\n    @property\n    def mutables(self):\n        \"\"\"\n        A generator of all modules inheriting :class:`~nni.nas.pytorch.mutables.Mutable`.\n        Modules are yielded in the order that they are defined in ``__init__``.\n        For mutables with their keys appearing multiple times, only the first one will appear.\n        \"\"\"\n        return self._structured_mutables\n\n    @property\n    def undedup_mutables(self):\n        return self._structured_mutables.traverse(deduplicate=False)\n\n    def forward(self, *inputs):\n        \"\"\"\n        Warnings\n        --------\n        Don't call forward of a mutator.\n        \"\"\"\n        raise RuntimeError(\"Forward is undefined for mutators.\")\n\n    def __setattr__(self, name, value):\n        if name == \"model\":\n            raise AttributeError(\"Attribute `model` can be set at most once, and you shouldn't use `self.model = model` to \"\n                                 \"include you network, as it will include all parameters in model into the mutator.\")\n        return super().__setattr__(name, value)\n\n    def enter_mutable_scope(self, mutable_scope):\n        \"\"\"\n        Callback when forward of a MutableScope is entered.\n\n        Parameters\n        ----------\n        mutable_scope : MutableScope\n            The mutable scope that is entered.\n        \"\"\"\n        pass\n\n    def exit_mutable_scope(self, mutable_scope):\n        \"\"\"\n        Callback when forward of a MutableScope is exited.\n\n        Parameters\n        ----------\n        mutable_scope : MutableScope\n            The mutable scope that is exited.\n        \"\"\"\n        pass\n\n    def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        \"\"\"\n        Callbacks of forward in LayerChoice.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            Module whose forward is called.\n        args : list of torch.Tensor\n            The arguments of its forward function.\n        kwargs : dict\n            The keyword arguments of its forward function.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output tensor and mask.\n        \"\"\"\n        raise NotImplementedError\n\n    def on_forward_input_choice(self, mutable, tensor_list):\n        \"\"\"\n        Callbacks of forward in InputChoice.\n\n        Parameters\n        ----------\n        mutable : InputChoice\n            Mutable that is called.\n        tensor_list : list of torch.Tensor\n            The arguments mutable is called with.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output tensor and mask.\n        \"\"\"\n        raise NotImplementedError\n\n    def export(self):\n        \"\"\"\n        Export the data of all decisions. This should output the decisions of all the mutables, so that the whole\n        network can be fully determined with these decisions for further training from scratch.\n\n        Returns\n        -------\n        dict\n            Mappings from mutable keys to decisions.\n        \"\"\"\n        raise NotImplementedError",
  "def __init__(self, model):\n        super().__init__()\n        self.__dict__[\"model\"] = model\n        self._structured_mutables = self._parse_search_space(self.model)",
  "def _parse_search_space(self, module, root=None, prefix=\"\", memo=None, nested_detection=None):\n        if memo is None:\n            memo = set()\n        if root is None:\n            root = StructuredMutableTreeNode(None)\n        if module not in memo:\n            memo.add(module)\n            if isinstance(module, Mutable):\n                if nested_detection is not None:\n                    raise RuntimeError(\"Cannot have nested search space. Error at {} in {}\"\n                                       .format(module, nested_detection))\n                module.name = prefix\n                module.set_mutator(self)\n                root = root.add_child(module)\n                if not isinstance(module, MutableScope):\n                    nested_detection = module\n                if isinstance(module, InputChoice):\n                    for k in module.choose_from:\n                        if k != InputChoice.NO_KEY and k not in [m.key for m in memo if isinstance(m, Mutable)]:\n                            raise RuntimeError(\"'{}' required by '{}' not found in keys that appeared before, and is not NO_KEY.\"\n                                               .format(k, module.key))\n            for name, submodule in module._modules.items():\n                if submodule is None:\n                    continue\n                submodule_prefix = prefix + (\".\" if prefix else \"\") + name\n                self._parse_search_space(submodule, root, submodule_prefix, memo=memo,\n                                         nested_detection=nested_detection)\n        return root",
  "def mutables(self):\n        \"\"\"\n        A generator of all modules inheriting :class:`~nni.nas.pytorch.mutables.Mutable`.\n        Modules are yielded in the order that they are defined in ``__init__``.\n        For mutables with their keys appearing multiple times, only the first one will appear.\n        \"\"\"\n        return self._structured_mutables",
  "def undedup_mutables(self):\n        return self._structured_mutables.traverse(deduplicate=False)",
  "def forward(self, *inputs):\n        \"\"\"\n        Warnings\n        --------\n        Don't call forward of a mutator.\n        \"\"\"\n        raise RuntimeError(\"Forward is undefined for mutators.\")",
  "def __setattr__(self, name, value):\n        if name == \"model\":\n            raise AttributeError(\"Attribute `model` can be set at most once, and you shouldn't use `self.model = model` to \"\n                                 \"include you network, as it will include all parameters in model into the mutator.\")\n        return super().__setattr__(name, value)",
  "def enter_mutable_scope(self, mutable_scope):\n        \"\"\"\n        Callback when forward of a MutableScope is entered.\n\n        Parameters\n        ----------\n        mutable_scope : MutableScope\n            The mutable scope that is entered.\n        \"\"\"\n        pass",
  "def exit_mutable_scope(self, mutable_scope):\n        \"\"\"\n        Callback when forward of a MutableScope is exited.\n\n        Parameters\n        ----------\n        mutable_scope : MutableScope\n            The mutable scope that is exited.\n        \"\"\"\n        pass",
  "def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        \"\"\"\n        Callbacks of forward in LayerChoice.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            Module whose forward is called.\n        args : list of torch.Tensor\n            The arguments of its forward function.\n        kwargs : dict\n            The keyword arguments of its forward function.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output tensor and mask.\n        \"\"\"\n        raise NotImplementedError",
  "def on_forward_input_choice(self, mutable, tensor_list):\n        \"\"\"\n        Callbacks of forward in InputChoice.\n\n        Parameters\n        ----------\n        mutable : InputChoice\n            Mutable that is called.\n        tensor_list : list of torch.Tensor\n            The arguments mutable is called with.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output tensor and mask.\n        \"\"\"\n        raise NotImplementedError",
  "def export(self):\n        \"\"\"\n        Export the data of all decisions. This should output the decisions of all the mutables, so that the whole\n        network can be fully determined with these decisions for further training from scratch.\n\n        Returns\n        -------\n        dict\n            Mappings from mutable keys to decisions.\n        \"\"\"\n        raise NotImplementedError",
  "class Mutator(BaseMutator):\n\n    def __init__(self, model):\n        super().__init__(model)\n        self._cache = dict()\n        self._connect_all = False\n\n    def sample_search(self):\n        \"\"\"\n        Override to implement this method to iterate over mutables and make decisions.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        \"\"\"\n        raise NotImplementedError\n\n    def sample_final(self):\n        \"\"\"\n        Override to implement this method to iterate over mutables and make decisions that is final\n        for export and retraining.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Reset the mutator by call the `sample_search` to resample (for search). Stores the result in a local\n        variable so that `on_forward_layer_choice` and `on_forward_input_choice` can use the decision directly.\n        \"\"\"\n        self._cache = self.sample_search()\n\n    def export(self):\n        \"\"\"\n        Resample (for final) and return results.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        \"\"\"\n        sampled = self.sample_final()\n        result = dict()\n        for mutable in self.mutables:\n            if not isinstance(mutable, (LayerChoice, InputChoice)):\n                # not supported as built-in\n                continue\n            result[mutable.key] = self._convert_mutable_decision_to_human_readable(mutable, sampled.pop(mutable.key))\n        if sampled:\n            raise ValueError(\"Unexpected keys returned from 'sample_final()': %s\", list(sampled.keys()))\n        return result\n\n    def status(self):\n        \"\"\"\n        Return current selection status of mutator.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions. All weights (boolean type and float type)\n            are converted into real number values. Numpy arrays and tensors are converted into list.\n        \"\"\"\n        data = dict()\n        for k, v in self._cache.items():\n            if torch.is_tensor(v):\n                v = v.detach().cpu().numpy()\n            if isinstance(v, np.ndarray):\n                v = v.astype(np.float32).tolist()\n            data[k] = v\n        return data\n\n    def graph(self, inputs):\n        \"\"\"\n        Return model supernet graph.\n\n        Parameters\n        ----------\n        inputs: tuple of tensor\n            Inputs that will be feeded into the network.\n\n        Returns\n        -------\n        dict\n            Containing ``node``, in Tensorboard GraphDef format.\n            Additional key ``mutable`` is a map from key to list of modules.\n        \"\"\"\n        if not torch.__version__.startswith(\"1.4\"):\n            logger.warning(\"Graph is only tested with PyTorch 1.4. Other versions might not work.\")\n        from nni._graph_utils import build_graph\n        from google.protobuf import json_format\n        # protobuf should be installed as long as tensorboard is installed\n        try:\n            self._connect_all = True\n            graph_def, _ = build_graph(self.model, inputs, verbose=False)\n            result = json_format.MessageToDict(graph_def)\n        finally:\n            self._connect_all = False\n\n        # `mutable` is to map the keys to a list of corresponding modules.\n        # A key can be linked to multiple modules, use `dedup=False` to find them all.\n        result[\"mutable\"] = defaultdict(list)\n        for mutable in self.mutables.traverse(deduplicate=False):\n            # A module will be represent in the format of\n            # [{\"type\": \"Net\", \"name\": \"\"}, {\"type\": \"Cell\", \"name\": \"cell1\"}, {\"type\": \"Conv2d\": \"name\": \"conv\"}]\n            # which will be concatenated into Net/Cell[cell1]/Conv2d[conv] in frontend.\n            # This format is aligned with the scope name jit gives.\n            modules = mutable.name.split(\".\")\n            path = [\n                {\"type\": self.model.__class__.__name__, \"name\": \"\"}\n            ]\n            m = self.model\n            for module in modules:\n                m = getattr(m, module)\n                path.append({\n                    \"type\": m.__class__.__name__,\n                    \"name\": module\n                })\n            result[\"mutable\"][mutable.key].append(path)\n        return result\n\n    def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        \"\"\"\n        On default, this method retrieves the decision obtained previously, and select certain operations.\n        Only operations with non-zero weight will be executed. The results will be added to a list.\n        Then it will reduce the list of all tensor outputs with the policy specified in `mutable.reduction`.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            Layer choice module.\n        args : list of torch.Tensor\n            Inputs\n        kwargs : dict\n            Inputs\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output and mask.\n        \"\"\"\n        if self._connect_all:\n            return self._all_connect_tensor_reduction(mutable.reduction,\n                                                      [op(*args, **kwargs) for op in mutable]), \\\n                torch.ones(len(mutable)).bool()\n\n        def _map_fn(op, args, kwargs):\n            return op(*args, **kwargs)\n\n        mask = self._get_decision(mutable)\n        assert len(mask) == len(mutable), \\\n            \"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable))\n        out, mask = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable], mask)\n        return self._tensor_reduction(mutable.reduction, out), mask\n\n    def on_forward_input_choice(self, mutable, tensor_list):\n        \"\"\"\n        On default, this method retrieves the decision obtained previously, and select certain tensors.\n        Then it will reduce the list of all tensor outputs with the policy specified in `mutable.reduction`.\n\n        Parameters\n        ----------\n        mutable : InputChoice\n            Input choice module.\n        tensor_list : list of torch.Tensor\n            Tensor list to apply the decision on.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output and mask.\n        \"\"\"\n        if self._connect_all:\n            return self._all_connect_tensor_reduction(mutable.reduction, tensor_list), \\\n                torch.ones(mutable.n_candidates).bool()\n        mask = self._get_decision(mutable)\n        assert len(mask) == mutable.n_candidates, \\\n            \"Invalid mask, expected {} to be of length {}.\".format(mask, mutable.n_candidates)\n        out, mask = self._select_with_mask(lambda x: x, [(t,) for t in tensor_list], mask)\n        return self._tensor_reduction(mutable.reduction, out), mask\n\n    def _select_with_mask(self, map_fn, candidates, mask):\n        \"\"\"\n        Select masked tensors and return a list of tensors.\n\n        Parameters\n        ----------\n        map_fn : function\n            Convert candidates to target candidates. Can be simply identity.\n        candidates : list of torch.Tensor\n            Tensor list to apply the decision on.\n        mask : list-like object\n            Can be a list, an numpy array or a tensor (recommended). Needs to\n            have the same length as ``candidates``.\n\n        Returns\n        -------\n        tuple of list of torch.Tensor and torch.Tensor\n            Output and mask.\n        \"\"\"\n        if (isinstance(mask, list) and len(mask) >= 1 and isinstance(mask[0], bool)) or \\\n                (isinstance(mask, np.ndarray) and mask.dtype == np.bool) or \\\n                \"BoolTensor\" in mask.type():\n            out = [map_fn(*cand) for cand, m in zip(candidates, mask) if m]\n        elif (isinstance(mask, list) and len(mask) >= 1 and isinstance(mask[0], (float, int))) or \\\n                (isinstance(mask, np.ndarray) and mask.dtype in (np.float32, np.float64, np.int32, np.int64)) or \\\n                \"FloatTensor\" in mask.type():\n            out = [map_fn(*cand) * m for cand, m in zip(candidates, mask) if m]\n        else:\n            raise ValueError(\"Unrecognized mask '%s'\" % mask)\n        if not torch.is_tensor(mask):\n            mask = torch.tensor(mask)  # pylint: disable=not-callable\n        return out, mask\n\n    def _tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == \"none\":\n            return tensor_list\n        if not tensor_list:\n            return None  # empty. return None for now\n        if len(tensor_list) == 1:\n            return tensor_list[0]\n        if reduction_type == \"sum\":\n            return sum(tensor_list)\n        if reduction_type == \"mean\":\n            return sum(tensor_list) / len(tensor_list)\n        if reduction_type == \"concat\":\n            return torch.cat(tensor_list, dim=1)\n        raise ValueError(\"Unrecognized reduction policy: \\\"{}\\\"\".format(reduction_type))\n\n    def _all_connect_tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == \"none\":\n            return tensor_list\n        if reduction_type == \"concat\":\n            return torch.cat(tensor_list, dim=1)\n        return torch.stack(tensor_list).sum(0)\n\n    def _get_decision(self, mutable):\n        \"\"\"\n        By default, this method checks whether `mutable.key` is already in the decision cache,\n        and returns the result without double-check.\n\n        Parameters\n        ----------\n        mutable : Mutable\n\n        Returns\n        -------\n        object\n        \"\"\"\n        if mutable.key not in self._cache:\n            raise ValueError(\"\\\"{}\\\" not found in decision cache.\".format(mutable.key))\n        result = self._cache[mutable.key]\n        logger.debug(\"Decision %s: %s\", mutable.key, result)\n        return result\n\n    def _convert_mutable_decision_to_human_readable(self, mutable, sampled):\n        # Assert the existence of mutable.key in returned architecture.\n        # Also check if there is anything extra.\n        multihot_list = to_list(sampled)\n        converted = None\n        # If it's a boolean array, we can do optimization.\n        if all([t == 0 or t == 1 for t in multihot_list]):\n            if isinstance(mutable, LayerChoice):\n                assert len(multihot_list) == len(mutable), \\\n                    \"Results returned from 'sample_final()' (%s: %s) either too short or too long.\" \\\n                        % (mutable.key, multihot_list)\n                # check if all modules have different names and they indeed have names\n                if len(set(mutable.names)) == len(mutable) and not all(d.isdigit() for d in mutable.names):\n                    converted = [name for i, name in enumerate(mutable.names) if multihot_list[i]]\n                else:\n                    converted = [i for i in range(len(multihot_list)) if multihot_list[i]]\n            if isinstance(mutable, InputChoice):\n                assert len(multihot_list) == mutable.n_candidates, \\\n                    \"Results returned from 'sample_final()' (%s: %s) either too short or too long.\" \\\n                        % (mutable.key, multihot_list)\n                # check if all input candidates have different names\n                if len(set(mutable.choose_from)) == mutable.n_candidates:\n                    converted = [name for i, name in enumerate(mutable.choose_from) if multihot_list[i]]\n                else:\n                    converted = [i for i in range(len(multihot_list)) if multihot_list[i]]\n        if converted is not None:\n            # if only one element, then remove the bracket\n            if len(converted) == 1:\n                converted = converted[0]\n        else:\n            # do nothing\n            converted = multihot_list\n        return converted",
  "def __init__(self, model):\n        super().__init__(model)\n        self._cache = dict()\n        self._connect_all = False",
  "def sample_search(self):\n        \"\"\"\n        Override to implement this method to iterate over mutables and make decisions.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        \"\"\"\n        raise NotImplementedError",
  "def sample_final(self):\n        \"\"\"\n        Override to implement this method to iterate over mutables and make decisions that is final\n        for export and retraining.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        \"\"\"\n        raise NotImplementedError",
  "def reset(self):\n        \"\"\"\n        Reset the mutator by call the `sample_search` to resample (for search). Stores the result in a local\n        variable so that `on_forward_layer_choice` and `on_forward_input_choice` can use the decision directly.\n        \"\"\"\n        self._cache = self.sample_search()",
  "def export(self):\n        \"\"\"\n        Resample (for final) and return results.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        \"\"\"\n        sampled = self.sample_final()\n        result = dict()\n        for mutable in self.mutables:\n            if not isinstance(mutable, (LayerChoice, InputChoice)):\n                # not supported as built-in\n                continue\n            result[mutable.key] = self._convert_mutable_decision_to_human_readable(mutable, sampled.pop(mutable.key))\n        if sampled:\n            raise ValueError(\"Unexpected keys returned from 'sample_final()': %s\", list(sampled.keys()))\n        return result",
  "def status(self):\n        \"\"\"\n        Return current selection status of mutator.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions. All weights (boolean type and float type)\n            are converted into real number values. Numpy arrays and tensors are converted into list.\n        \"\"\"\n        data = dict()\n        for k, v in self._cache.items():\n            if torch.is_tensor(v):\n                v = v.detach().cpu().numpy()\n            if isinstance(v, np.ndarray):\n                v = v.astype(np.float32).tolist()\n            data[k] = v\n        return data",
  "def graph(self, inputs):\n        \"\"\"\n        Return model supernet graph.\n\n        Parameters\n        ----------\n        inputs: tuple of tensor\n            Inputs that will be feeded into the network.\n\n        Returns\n        -------\n        dict\n            Containing ``node``, in Tensorboard GraphDef format.\n            Additional key ``mutable`` is a map from key to list of modules.\n        \"\"\"\n        if not torch.__version__.startswith(\"1.4\"):\n            logger.warning(\"Graph is only tested with PyTorch 1.4. Other versions might not work.\")\n        from nni._graph_utils import build_graph\n        from google.protobuf import json_format\n        # protobuf should be installed as long as tensorboard is installed\n        try:\n            self._connect_all = True\n            graph_def, _ = build_graph(self.model, inputs, verbose=False)\n            result = json_format.MessageToDict(graph_def)\n        finally:\n            self._connect_all = False\n\n        # `mutable` is to map the keys to a list of corresponding modules.\n        # A key can be linked to multiple modules, use `dedup=False` to find them all.\n        result[\"mutable\"] = defaultdict(list)\n        for mutable in self.mutables.traverse(deduplicate=False):\n            # A module will be represent in the format of\n            # [{\"type\": \"Net\", \"name\": \"\"}, {\"type\": \"Cell\", \"name\": \"cell1\"}, {\"type\": \"Conv2d\": \"name\": \"conv\"}]\n            # which will be concatenated into Net/Cell[cell1]/Conv2d[conv] in frontend.\n            # This format is aligned with the scope name jit gives.\n            modules = mutable.name.split(\".\")\n            path = [\n                {\"type\": self.model.__class__.__name__, \"name\": \"\"}\n            ]\n            m = self.model\n            for module in modules:\n                m = getattr(m, module)\n                path.append({\n                    \"type\": m.__class__.__name__,\n                    \"name\": module\n                })\n            result[\"mutable\"][mutable.key].append(path)\n        return result",
  "def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        \"\"\"\n        On default, this method retrieves the decision obtained previously, and select certain operations.\n        Only operations with non-zero weight will be executed. The results will be added to a list.\n        Then it will reduce the list of all tensor outputs with the policy specified in `mutable.reduction`.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            Layer choice module.\n        args : list of torch.Tensor\n            Inputs\n        kwargs : dict\n            Inputs\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output and mask.\n        \"\"\"\n        if self._connect_all:\n            return self._all_connect_tensor_reduction(mutable.reduction,\n                                                      [op(*args, **kwargs) for op in mutable]), \\\n                torch.ones(len(mutable)).bool()\n\n        def _map_fn(op, args, kwargs):\n            return op(*args, **kwargs)\n\n        mask = self._get_decision(mutable)\n        assert len(mask) == len(mutable), \\\n            \"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable))\n        out, mask = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable], mask)\n        return self._tensor_reduction(mutable.reduction, out), mask",
  "def on_forward_input_choice(self, mutable, tensor_list):\n        \"\"\"\n        On default, this method retrieves the decision obtained previously, and select certain tensors.\n        Then it will reduce the list of all tensor outputs with the policy specified in `mutable.reduction`.\n\n        Parameters\n        ----------\n        mutable : InputChoice\n            Input choice module.\n        tensor_list : list of torch.Tensor\n            Tensor list to apply the decision on.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output and mask.\n        \"\"\"\n        if self._connect_all:\n            return self._all_connect_tensor_reduction(mutable.reduction, tensor_list), \\\n                torch.ones(mutable.n_candidates).bool()\n        mask = self._get_decision(mutable)\n        assert len(mask) == mutable.n_candidates, \\\n            \"Invalid mask, expected {} to be of length {}.\".format(mask, mutable.n_candidates)\n        out, mask = self._select_with_mask(lambda x: x, [(t,) for t in tensor_list], mask)\n        return self._tensor_reduction(mutable.reduction, out), mask",
  "def _select_with_mask(self, map_fn, candidates, mask):\n        \"\"\"\n        Select masked tensors and return a list of tensors.\n\n        Parameters\n        ----------\n        map_fn : function\n            Convert candidates to target candidates. Can be simply identity.\n        candidates : list of torch.Tensor\n            Tensor list to apply the decision on.\n        mask : list-like object\n            Can be a list, an numpy array or a tensor (recommended). Needs to\n            have the same length as ``candidates``.\n\n        Returns\n        -------\n        tuple of list of torch.Tensor and torch.Tensor\n            Output and mask.\n        \"\"\"\n        if (isinstance(mask, list) and len(mask) >= 1 and isinstance(mask[0], bool)) or \\\n                (isinstance(mask, np.ndarray) and mask.dtype == np.bool) or \\\n                \"BoolTensor\" in mask.type():\n            out = [map_fn(*cand) for cand, m in zip(candidates, mask) if m]\n        elif (isinstance(mask, list) and len(mask) >= 1 and isinstance(mask[0], (float, int))) or \\\n                (isinstance(mask, np.ndarray) and mask.dtype in (np.float32, np.float64, np.int32, np.int64)) or \\\n                \"FloatTensor\" in mask.type():\n            out = [map_fn(*cand) * m for cand, m in zip(candidates, mask) if m]\n        else:\n            raise ValueError(\"Unrecognized mask '%s'\" % mask)\n        if not torch.is_tensor(mask):\n            mask = torch.tensor(mask)  # pylint: disable=not-callable\n        return out, mask",
  "def _tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == \"none\":\n            return tensor_list\n        if not tensor_list:\n            return None  # empty. return None for now\n        if len(tensor_list) == 1:\n            return tensor_list[0]\n        if reduction_type == \"sum\":\n            return sum(tensor_list)\n        if reduction_type == \"mean\":\n            return sum(tensor_list) / len(tensor_list)\n        if reduction_type == \"concat\":\n            return torch.cat(tensor_list, dim=1)\n        raise ValueError(\"Unrecognized reduction policy: \\\"{}\\\"\".format(reduction_type))",
  "def _all_connect_tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == \"none\":\n            return tensor_list\n        if reduction_type == \"concat\":\n            return torch.cat(tensor_list, dim=1)\n        return torch.stack(tensor_list).sum(0)",
  "def _get_decision(self, mutable):\n        \"\"\"\n        By default, this method checks whether `mutable.key` is already in the decision cache,\n        and returns the result without double-check.\n\n        Parameters\n        ----------\n        mutable : Mutable\n\n        Returns\n        -------\n        object\n        \"\"\"\n        if mutable.key not in self._cache:\n            raise ValueError(\"\\\"{}\\\" not found in decision cache.\".format(mutable.key))\n        result = self._cache[mutable.key]\n        logger.debug(\"Decision %s: %s\", mutable.key, result)\n        return result",
  "def _convert_mutable_decision_to_human_readable(self, mutable, sampled):\n        # Assert the existence of mutable.key in returned architecture.\n        # Also check if there is anything extra.\n        multihot_list = to_list(sampled)\n        converted = None\n        # If it's a boolean array, we can do optimization.\n        if all([t == 0 or t == 1 for t in multihot_list]):\n            if isinstance(mutable, LayerChoice):\n                assert len(multihot_list) == len(mutable), \\\n                    \"Results returned from 'sample_final()' (%s: %s) either too short or too long.\" \\\n                        % (mutable.key, multihot_list)\n                # check if all modules have different names and they indeed have names\n                if len(set(mutable.names)) == len(mutable) and not all(d.isdigit() for d in mutable.names):\n                    converted = [name for i, name in enumerate(mutable.names) if multihot_list[i]]\n                else:\n                    converted = [i for i in range(len(multihot_list)) if multihot_list[i]]\n            if isinstance(mutable, InputChoice):\n                assert len(multihot_list) == mutable.n_candidates, \\\n                    \"Results returned from 'sample_final()' (%s: %s) either too short or too long.\" \\\n                        % (mutable.key, multihot_list)\n                # check if all input candidates have different names\n                if len(set(mutable.choose_from)) == mutable.n_candidates:\n                    converted = [name for i, name in enumerate(mutable.choose_from) if multihot_list[i]]\n                else:\n                    converted = [i for i in range(len(multihot_list)) if multihot_list[i]]\n        if converted is not None:\n            # if only one element, then remove the bracket\n            if len(converted) == 1:\n                converted = converted[0]\n        else:\n            # do nothing\n            converted = multihot_list\n        return converted",
  "def _map_fn(op, args, kwargs):\n            return op(*args, **kwargs)",
  "class TorchTensorEncoder(json.JSONEncoder):\n    def default(self, o):  # pylint: disable=method-hidden\n        if isinstance(o, torch.Tensor):\n            olist = o.tolist()\n            if \"bool\" not in o.type().lower() and all(map(lambda d: d == 0 or d == 1, olist)):\n                _logger.warning(\"Every element in %s is either 0 or 1. \"\n                                \"You might consider convert it into bool.\", olist)\n            return olist\n        return super().default(o)",
  "class Trainer(BaseTrainer):\n    \"\"\"\n    A trainer with some helper functions implemented. To implement a new trainer,\n    users need to implement :meth:`train_one_epoch`, :meth:`validate_one_epoch` and :meth:`checkpoint`.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Model with mutables.\n    mutator : BaseMutator\n        A mutator object that has been initialized with the model.\n    loss : callable\n        Called with logits and targets. Returns a loss tensor.\n        See `PyTorch loss functions`_ for examples.\n    metrics : callable\n        Called with logits and targets. Returns a dict that maps metrics keys to metrics data. For example,\n\n        .. code-block:: python\n\n            def metrics_fn(output, target):\n                return {\"acc1\": accuracy(output, target, topk=1), \"acc5\": accuracy(output, target, topk=5)}\n\n    optimizer : Optimizer\n        Optimizer that optimizes the model.\n    num_epochs : int\n        Number of epochs of training.\n    dataset_train : torch.utils.data.Dataset\n        Dataset of training. If not otherwise specified, ``dataset_train`` and ``dataset_valid`` should be standard\n        PyTorch Dataset. See `torch.utils.data`_ for examples.\n    dataset_valid : torch.utils.data.Dataset\n        Dataset of validation/testing.\n    batch_size : int\n        Batch size.\n    workers : int\n        Number of workers used in data preprocessing.\n    device : torch.device\n        Device object. Either ``torch.device(\"cuda\")`` or ``torch.device(\"cpu\")``. When ``None``, trainer will\n        automatic detects GPU and selects GPU first.\n    log_frequency : int\n        Number of mini-batches to log metrics.\n    callbacks : list of Callback\n        Callbacks to plug into the trainer. See Callbacks.\n\n\n    .. _`PyTorch loss functions`: https://pytorch.org/docs/stable/nn.html#loss-functions\n    .. _`torch.utils.data`: https://pytorch.org/docs/stable/data.html\n    \"\"\"\n    def __init__(self, model, mutator, loss, metrics, optimizer, num_epochs,\n                 dataset_train, dataset_valid, batch_size, workers, device, log_frequency, callbacks):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n        self.model = model\n        self.mutator = mutator\n        self.loss = loss\n\n        self.metrics = metrics\n        self.optimizer = optimizer\n\n        self.model.to(self.device)\n        self.mutator.to(self.device)\n        self.loss.to(self.device)\n\n        self.num_epochs = num_epochs\n        self.dataset_train = dataset_train\n        self.dataset_valid = dataset_valid\n        self.batch_size = batch_size\n        self.workers = workers\n        self.log_frequency = log_frequency\n        self.log_dir = os.path.join(\"logs\", str(time.time()))\n        os.makedirs(self.log_dir, exist_ok=True)\n        self.status_writer = open(os.path.join(self.log_dir, \"log\"), \"w\")\n        self.callbacks = callbacks if callbacks is not None else []\n        for callback in self.callbacks:\n            callback.build(self.model, self.mutator, self)\n\n    @abstractmethod\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Train one epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number starting from 0.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_one_epoch(self, epoch):\n        \"\"\"\n        Validate one epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number starting from 0.\n        \"\"\"\n        pass\n\n    def train(self, validate=True):\n        \"\"\"\n        Train ``num_epochs``.\n        Trigger callbacks at the start and the end of each epoch.\n\n        Parameters\n        ----------\n        validate : bool\n            If ``true``, will do validation every epoch.\n        \"\"\"\n        for epoch in range(self.num_epochs):\n            for callback in self.callbacks:\n                callback.on_epoch_begin(epoch)\n\n            # training\n            _logger.info(\"Epoch %d Training\", epoch + 1)\n            self.train_one_epoch(epoch)\n\n            if validate:\n                # validation\n                _logger.info(\"Epoch %d Validating\", epoch + 1)\n                self.validate_one_epoch(epoch)\n\n            for callback in self.callbacks:\n                callback.on_epoch_end(epoch)\n\n    def validate(self):\n        \"\"\"\n        Do one validation.\n        \"\"\"\n        self.validate_one_epoch(-1)\n\n    def export(self, file):\n        \"\"\"\n        Call ``mutator.export()`` and dump the architecture to ``file``.\n\n        Parameters\n        ----------\n        file : str\n            A file path. Expected to be a JSON.\n        \"\"\"\n        mutator_export = self.mutator.export()\n        with open(file, \"w\") as f:\n            json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n\n    def checkpoint(self):\n        \"\"\"\n        Return trainer checkpoint.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented yet\")\n\n    def enable_visualization(self):\n        \"\"\"\n        Enable visualization. Write graph and training log to folder ``logs/<timestamp>``.\n        \"\"\"\n        sample = None\n        for x, _ in self.train_loader:\n            sample = x.to(self.device)[:2]\n            break\n        if sample is None:\n            _logger.warning(\"Sample is %s.\", sample)\n        _logger.info(\"Creating graph json, writing to %s. Visualization enabled.\", self.log_dir)\n        with open(os.path.join(self.log_dir, \"graph.json\"), \"w\") as f:\n            json.dump(self.mutator.graph(sample), f)\n        self.visualization_enabled = True\n\n    def _write_graph_status(self):\n        if hasattr(self, \"visualization_enabled\") and self.visualization_enabled:\n            print(json.dumps(self.mutator.status()), file=self.status_writer, flush=True)",
  "def default(self, o):  # pylint: disable=method-hidden\n        if isinstance(o, torch.Tensor):\n            olist = o.tolist()\n            if \"bool\" not in o.type().lower() and all(map(lambda d: d == 0 or d == 1, olist)):\n                _logger.warning(\"Every element in %s is either 0 or 1. \"\n                                \"You might consider convert it into bool.\", olist)\n            return olist\n        return super().default(o)",
  "def __init__(self, model, mutator, loss, metrics, optimizer, num_epochs,\n                 dataset_train, dataset_valid, batch_size, workers, device, log_frequency, callbacks):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n        self.model = model\n        self.mutator = mutator\n        self.loss = loss\n\n        self.metrics = metrics\n        self.optimizer = optimizer\n\n        self.model.to(self.device)\n        self.mutator.to(self.device)\n        self.loss.to(self.device)\n\n        self.num_epochs = num_epochs\n        self.dataset_train = dataset_train\n        self.dataset_valid = dataset_valid\n        self.batch_size = batch_size\n        self.workers = workers\n        self.log_frequency = log_frequency\n        self.log_dir = os.path.join(\"logs\", str(time.time()))\n        os.makedirs(self.log_dir, exist_ok=True)\n        self.status_writer = open(os.path.join(self.log_dir, \"log\"), \"w\")\n        self.callbacks = callbacks if callbacks is not None else []\n        for callback in self.callbacks:\n            callback.build(self.model, self.mutator, self)",
  "def train_one_epoch(self, epoch):\n        \"\"\"\n        Train one epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number starting from 0.\n        \"\"\"\n        pass",
  "def validate_one_epoch(self, epoch):\n        \"\"\"\n        Validate one epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number starting from 0.\n        \"\"\"\n        pass",
  "def train(self, validate=True):\n        \"\"\"\n        Train ``num_epochs``.\n        Trigger callbacks at the start and the end of each epoch.\n\n        Parameters\n        ----------\n        validate : bool\n            If ``true``, will do validation every epoch.\n        \"\"\"\n        for epoch in range(self.num_epochs):\n            for callback in self.callbacks:\n                callback.on_epoch_begin(epoch)\n\n            # training\n            _logger.info(\"Epoch %d Training\", epoch + 1)\n            self.train_one_epoch(epoch)\n\n            if validate:\n                # validation\n                _logger.info(\"Epoch %d Validating\", epoch + 1)\n                self.validate_one_epoch(epoch)\n\n            for callback in self.callbacks:\n                callback.on_epoch_end(epoch)",
  "def validate(self):\n        \"\"\"\n        Do one validation.\n        \"\"\"\n        self.validate_one_epoch(-1)",
  "def export(self, file):\n        \"\"\"\n        Call ``mutator.export()`` and dump the architecture to ``file``.\n\n        Parameters\n        ----------\n        file : str\n            A file path. Expected to be a JSON.\n        \"\"\"\n        mutator_export = self.mutator.export()\n        with open(file, \"w\") as f:\n            json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)",
  "def checkpoint(self):\n        \"\"\"\n        Return trainer checkpoint.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented yet\")",
  "def enable_visualization(self):\n        \"\"\"\n        Enable visualization. Write graph and training log to folder ``logs/<timestamp>``.\n        \"\"\"\n        sample = None\n        for x, _ in self.train_loader:\n            sample = x.to(self.device)[:2]\n            break\n        if sample is None:\n            _logger.warning(\"Sample is %s.\", sample)\n        _logger.info(\"Creating graph json, writing to %s. Visualization enabled.\", self.log_dir)\n        with open(os.path.join(self.log_dir, \"graph.json\"), \"w\") as f:\n            json.dump(self.mutator.graph(sample), f)\n        self.visualization_enabled = True",
  "def _write_graph_status(self):\n        if hasattr(self, \"visualization_enabled\") and self.visualization_enabled:\n            print(json.dumps(self.mutator.status()), file=self.status_writer, flush=True)",
  "class BaseTrainer(ABC):\n\n    @abstractmethod\n    def train(self):\n        \"\"\"\n        Override the method to train.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def validate(self):\n        \"\"\"\n        Override the method to validate.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def export(self, file):\n        \"\"\"\n        Override the method to export to file.\n\n        Parameters\n        ----------\n        file : str\n            File path to export to.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def checkpoint(self):\n        \"\"\"\n        Override to dump a checkpoint.\n        \"\"\"\n        raise NotImplementedError",
  "def train(self):\n        \"\"\"\n        Override the method to train.\n        \"\"\"\n        raise NotImplementedError",
  "def validate(self):\n        \"\"\"\n        Override the method to validate.\n        \"\"\"\n        raise NotImplementedError",
  "def export(self, file):\n        \"\"\"\n        Override the method to export to file.\n\n        Parameters\n        ----------\n        file : str\n            File path to export to.\n        \"\"\"\n        raise NotImplementedError",
  "def checkpoint(self):\n        \"\"\"\n        Override to dump a checkpoint.\n        \"\"\"\n        raise NotImplementedError",
  "class Callback:\n    \"\"\"\n    Callback provides an easy way to react to events like begin/end of epochs.\n    \"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.mutator = None\n        self.trainer = None\n\n    def build(self, model, mutator, trainer):\n        \"\"\"\n        Callback needs to be built with model, mutator, trainer, to get updates from them.\n\n        Parameters\n        ----------\n        model : nn.Module\n            Model to be trained.\n        mutator : nn.Module\n            Mutator that mutates the model.\n        trainer : BaseTrainer\n            Trainer that is to call the callback.\n        \"\"\"\n        self.model = model\n        self.mutator = mutator\n        self.trainer = trainer\n\n    def on_epoch_begin(self, epoch):\n        \"\"\"\n        Implement this to do something at the begin of epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number, starting from 0.\n        \"\"\"\n        pass\n\n    def on_epoch_end(self, epoch):\n        \"\"\"\n        Implement this to do something at the end of epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number, starting from 0.\n        \"\"\"\n        pass\n\n    def on_batch_begin(self, epoch):\n        pass\n\n    def on_batch_end(self, epoch):\n        pass",
  "class LRSchedulerCallback(Callback):\n    \"\"\"\n    Calls scheduler on every epoch ends.\n\n    Parameters\n    ----------\n    scheduler : LRScheduler\n        Scheduler to be called.\n    \"\"\"\n    def __init__(self, scheduler, mode=\"epoch\"):\n        super().__init__()\n        assert mode == \"epoch\"\n        self.scheduler = scheduler\n        self.mode = mode\n\n    def on_epoch_end(self, epoch):\n        \"\"\"\n        Call ``self.scheduler.step()`` on epoch end.\n        \"\"\"\n        self.scheduler.step()",
  "class ArchitectureCheckpoint(Callback):\n    \"\"\"\n    Calls ``trainer.export()`` on every epoch ends.\n\n    Parameters\n    ----------\n    checkpoint_dir : str\n        Location to save checkpoints.\n    \"\"\"\n    def __init__(self, checkpoint_dir):\n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n    def on_epoch_end(self, epoch):\n        \"\"\"\n        Dump to ``/checkpoint_dir/epoch_{number}.json`` on epoch end.\n        \"\"\"\n        dest_path = os.path.join(self.checkpoint_dir, \"epoch_{}.json\".format(epoch))\n        _logger.info(\"Saving architecture to %s\", dest_path)\n        self.trainer.export(dest_path)",
  "class ModelCheckpoint(Callback):\n    \"\"\"\n    Calls ``trainer.export()`` on every epoch ends.\n\n    Parameters\n    ----------\n    checkpoint_dir : str\n        Location to save checkpoints.\n    \"\"\"\n    def __init__(self, checkpoint_dir):\n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n    def on_epoch_end(self, epoch):\n        \"\"\"\n        Dump to ``/checkpoint_dir/epoch_{number}.pth.tar`` on every epoch end.\n        ``DataParallel`` object will have their inside modules exported.\n        \"\"\"\n        if isinstance(self.model, nn.DataParallel):\n            state_dict = self.model.module.state_dict()\n        else:\n            state_dict = self.model.state_dict()\n        dest_path = os.path.join(self.checkpoint_dir, \"epoch_{}.pth.tar\".format(epoch))\n        _logger.info(\"Saving model to %s\", dest_path)\n        torch.save(state_dict, dest_path)",
  "def __init__(self):\n        self.model = None\n        self.mutator = None\n        self.trainer = None",
  "def build(self, model, mutator, trainer):\n        \"\"\"\n        Callback needs to be built with model, mutator, trainer, to get updates from them.\n\n        Parameters\n        ----------\n        model : nn.Module\n            Model to be trained.\n        mutator : nn.Module\n            Mutator that mutates the model.\n        trainer : BaseTrainer\n            Trainer that is to call the callback.\n        \"\"\"\n        self.model = model\n        self.mutator = mutator\n        self.trainer = trainer",
  "def on_epoch_begin(self, epoch):\n        \"\"\"\n        Implement this to do something at the begin of epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number, starting from 0.\n        \"\"\"\n        pass",
  "def on_epoch_end(self, epoch):\n        \"\"\"\n        Implement this to do something at the end of epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number, starting from 0.\n        \"\"\"\n        pass",
  "def on_batch_begin(self, epoch):\n        pass",
  "def on_batch_end(self, epoch):\n        pass",
  "def __init__(self, scheduler, mode=\"epoch\"):\n        super().__init__()\n        assert mode == \"epoch\"\n        self.scheduler = scheduler\n        self.mode = mode",
  "def on_epoch_end(self, epoch):\n        \"\"\"\n        Call ``self.scheduler.step()`` on epoch end.\n        \"\"\"\n        self.scheduler.step()",
  "def __init__(self, checkpoint_dir):\n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(self.checkpoint_dir, exist_ok=True)",
  "def on_epoch_end(self, epoch):\n        \"\"\"\n        Dump to ``/checkpoint_dir/epoch_{number}.json`` on epoch end.\n        \"\"\"\n        dest_path = os.path.join(self.checkpoint_dir, \"epoch_{}.json\".format(epoch))\n        _logger.info(\"Saving architecture to %s\", dest_path)\n        self.trainer.export(dest_path)",
  "def __init__(self, checkpoint_dir):\n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(self.checkpoint_dir, exist_ok=True)",
  "def on_epoch_end(self, epoch):\n        \"\"\"\n        Dump to ``/checkpoint_dir/epoch_{number}.pth.tar`` on every epoch end.\n        ``DataParallel`` object will have their inside modules exported.\n        \"\"\"\n        if isinstance(self.model, nn.DataParallel):\n            state_dict = self.model.module.state_dict()\n        else:\n            state_dict = self.model.state_dict()\n        dest_path = os.path.join(self.checkpoint_dir, \"epoch_{}.pth.tar\".format(epoch))\n        _logger.info(\"Saving model to %s\", dest_path)\n        torch.save(state_dict, dest_path)",
  "class FixedArchitecture(Mutator):\n    \"\"\"\n    Fixed architecture mutator that always selects a certain graph.\n\n    Parameters\n    ----------\n    model : nn.Module\n        A mutable network.\n    fixed_arc : dict\n        Preloaded architecture object.\n    strict : bool\n        Force everything that appears in ``fixed_arc`` to be used at least once.\n    \"\"\"\n\n    def __init__(self, model, fixed_arc, strict=True):\n        super().__init__(model)\n        self._fixed_arc = fixed_arc\n\n        mutable_keys = set([mutable.key for mutable in self.mutables if not isinstance(mutable, MutableScope)])\n        fixed_arc_keys = set(self._fixed_arc.keys())\n        if fixed_arc_keys - mutable_keys:\n            raise RuntimeError(\"Unexpected keys found in fixed architecture: {}.\".format(fixed_arc_keys - mutable_keys))\n        if mutable_keys - fixed_arc_keys:\n            raise RuntimeError(\"Missing keys in fixed architecture: {}.\".format(mutable_keys - fixed_arc_keys))\n        self._fixed_arc = self._from_human_readable_architecture(self._fixed_arc)\n\n    def _from_human_readable_architecture(self, human_arc):\n        # convert from an exported architecture\n        result_arc = {k: to_list(v) for k, v in human_arc.items()}  # there could be tensors, numpy arrays, etc.\n        # First, convert non-list to list, because there could be {\"op1\": 0} or {\"op1\": \"conv\"},\n        # which means {\"op1\": [0, ]} ir {\"op1\": [\"conv\", ]}\n        result_arc = {k: v if isinstance(v, list) else [v] for k, v in result_arc.items()}\n        # Second, infer which ones are multi-hot arrays and which ones are in human-readable format.\n        # This is non-trivial, since if an array in [0, 1], we cannot know for sure it means [false, true] or [true, true].\n        # Here, we assume an multihot array has to be a boolean array or a float array and matches the length.\n        for mutable in self.mutables:\n            if mutable.key not in result_arc:\n                continue  # skip silently\n            choice_arr = result_arc[mutable.key]\n            if all(isinstance(v, bool) for v in choice_arr) or all(isinstance(v, float) for v in choice_arr):\n                if (isinstance(mutable, LayerChoice) and len(mutable) == len(choice_arr)) or \\\n                        (isinstance(mutable, InputChoice) and mutable.n_candidates == len(choice_arr)):\n                    # multihot, do nothing\n                    continue\n            if isinstance(mutable, LayerChoice):\n                choice_arr = [mutable.names.index(val) if isinstance(val, str) else val for val in choice_arr]\n                choice_arr = [i in choice_arr for i in range(len(mutable))]\n            elif isinstance(mutable, InputChoice):\n                choice_arr = [mutable.choose_from.index(val) if isinstance(val, str) else val for val in choice_arr]\n                choice_arr = [i in choice_arr for i in range(mutable.n_candidates)]\n            result_arc[mutable.key] = choice_arr\n        return result_arc\n\n    def sample_search(self):\n        \"\"\"\n        Always returns the fixed architecture.\n        \"\"\"\n        return self._fixed_arc\n\n    def sample_final(self):\n        \"\"\"\n        Always returns the fixed architecture.\n        \"\"\"\n        return self._fixed_arc\n\n    def replace_layer_choice(self, module=None, prefix=\"\"):\n        \"\"\"\n        Replace layer choices with selected candidates. It's done with best effort.\n        In case of weighted choices or multiple choices. if some of the choices on weighted with zero, delete them.\n        If single choice, replace the module with a normal module.\n\n        Parameters\n        ----------\n        module : nn.Module\n            Module to be processed.\n        prefix : str\n            Module name under global namespace.\n        \"\"\"\n        if module is None:\n            module = self.model\n        for name, mutable in module.named_children():\n            global_name = (prefix + \".\" if prefix else \"\") + name\n            if isinstance(mutable, LayerChoice):\n                chosen = self._fixed_arc[mutable.key]\n                if sum(chosen) == 1 and max(chosen) == 1 and not mutable.return_mask:\n                    # sum is one, max is one, there has to be an only one\n                    # this is compatible with both integer arrays, boolean arrays and float arrays\n                    _logger.info(\"Replacing %s with candidate number %d.\", global_name, chosen.index(1))\n                    setattr(module, name, mutable[chosen.index(1)])\n                else:\n                    if mutable.return_mask:\n                        _logger.info(\"`return_mask` flag of %s is true. As it relies on the behavior of LayerChoice, \" \\\n                                     \"LayerChoice will not be replaced.\")\n                    # remove unused parameters\n                    for ch, n in zip(chosen, mutable.names):\n                        if ch == 0 and not isinstance(ch, float):\n                            setattr(mutable, n, None)\n            else:\n                self.replace_layer_choice(mutable, global_name)",
  "def apply_fixed_architecture(model, fixed_arc):\n    \"\"\"\n    Load architecture from `fixed_arc` and apply to model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model with mutables.\n    fixed_arc : str or dict\n        Path to the JSON that stores the architecture, or dict that stores the exported architecture.\n\n    Returns\n    -------\n    FixedArchitecture\n        Mutator that is responsible for fixes the graph.\n    \"\"\"\n\n    if isinstance(fixed_arc, str):\n        with open(fixed_arc) as f:\n            fixed_arc = json.load(f)\n    architecture = FixedArchitecture(model, fixed_arc)\n    architecture.reset()\n\n    # for the convenience of parameters counting\n    architecture.replace_layer_choice()\n    return architecture",
  "def __init__(self, model, fixed_arc, strict=True):\n        super().__init__(model)\n        self._fixed_arc = fixed_arc\n\n        mutable_keys = set([mutable.key for mutable in self.mutables if not isinstance(mutable, MutableScope)])\n        fixed_arc_keys = set(self._fixed_arc.keys())\n        if fixed_arc_keys - mutable_keys:\n            raise RuntimeError(\"Unexpected keys found in fixed architecture: {}.\".format(fixed_arc_keys - mutable_keys))\n        if mutable_keys - fixed_arc_keys:\n            raise RuntimeError(\"Missing keys in fixed architecture: {}.\".format(mutable_keys - fixed_arc_keys))\n        self._fixed_arc = self._from_human_readable_architecture(self._fixed_arc)",
  "def _from_human_readable_architecture(self, human_arc):\n        # convert from an exported architecture\n        result_arc = {k: to_list(v) for k, v in human_arc.items()}  # there could be tensors, numpy arrays, etc.\n        # First, convert non-list to list, because there could be {\"op1\": 0} or {\"op1\": \"conv\"},\n        # which means {\"op1\": [0, ]} ir {\"op1\": [\"conv\", ]}\n        result_arc = {k: v if isinstance(v, list) else [v] for k, v in result_arc.items()}\n        # Second, infer which ones are multi-hot arrays and which ones are in human-readable format.\n        # This is non-trivial, since if an array in [0, 1], we cannot know for sure it means [false, true] or [true, true].\n        # Here, we assume an multihot array has to be a boolean array or a float array and matches the length.\n        for mutable in self.mutables:\n            if mutable.key not in result_arc:\n                continue  # skip silently\n            choice_arr = result_arc[mutable.key]\n            if all(isinstance(v, bool) for v in choice_arr) or all(isinstance(v, float) for v in choice_arr):\n                if (isinstance(mutable, LayerChoice) and len(mutable) == len(choice_arr)) or \\\n                        (isinstance(mutable, InputChoice) and mutable.n_candidates == len(choice_arr)):\n                    # multihot, do nothing\n                    continue\n            if isinstance(mutable, LayerChoice):\n                choice_arr = [mutable.names.index(val) if isinstance(val, str) else val for val in choice_arr]\n                choice_arr = [i in choice_arr for i in range(len(mutable))]\n            elif isinstance(mutable, InputChoice):\n                choice_arr = [mutable.choose_from.index(val) if isinstance(val, str) else val for val in choice_arr]\n                choice_arr = [i in choice_arr for i in range(mutable.n_candidates)]\n            result_arc[mutable.key] = choice_arr\n        return result_arc",
  "def sample_search(self):\n        \"\"\"\n        Always returns the fixed architecture.\n        \"\"\"\n        return self._fixed_arc",
  "def sample_final(self):\n        \"\"\"\n        Always returns the fixed architecture.\n        \"\"\"\n        return self._fixed_arc",
  "def replace_layer_choice(self, module=None, prefix=\"\"):\n        \"\"\"\n        Replace layer choices with selected candidates. It's done with best effort.\n        In case of weighted choices or multiple choices. if some of the choices on weighted with zero, delete them.\n        If single choice, replace the module with a normal module.\n\n        Parameters\n        ----------\n        module : nn.Module\n            Module to be processed.\n        prefix : str\n            Module name under global namespace.\n        \"\"\"\n        if module is None:\n            module = self.model\n        for name, mutable in module.named_children():\n            global_name = (prefix + \".\" if prefix else \"\") + name\n            if isinstance(mutable, LayerChoice):\n                chosen = self._fixed_arc[mutable.key]\n                if sum(chosen) == 1 and max(chosen) == 1 and not mutable.return_mask:\n                    # sum is one, max is one, there has to be an only one\n                    # this is compatible with both integer arrays, boolean arrays and float arrays\n                    _logger.info(\"Replacing %s with candidate number %d.\", global_name, chosen.index(1))\n                    setattr(module, name, mutable[chosen.index(1)])\n                else:\n                    if mutable.return_mask:\n                        _logger.info(\"`return_mask` flag of %s is true. As it relies on the behavior of LayerChoice, \" \\\n                                     \"LayerChoice will not be replaced.\")\n                    # remove unused parameters\n                    for ch, n in zip(chosen, mutable.names):\n                        if ch == 0 and not isinstance(ch, float):\n                            setattr(mutable, n, None)\n            else:\n                self.replace_layer_choice(mutable, global_name)",
  "class Mutable(nn.Module):\n    \"\"\"\n    Mutable is designed to function as a normal layer, with all necessary operators' weights.\n    States and weights of architectures should be included in mutator, instead of the layer itself.\n\n    Mutable has a key, which marks the identity of the mutable. This key can be used by users to share\n    decisions among different mutables. In mutator's implementation, mutators should use the key to\n    distinguish different mutables. Mutables that share the same key should be \"similar\" to each other.\n\n    Currently the default scope for keys is global. By default, the keys uses a global counter from 1 to\n    produce unique ids.\n\n    Parameters\n    ----------\n    key : str\n        The key of mutable.\n\n    Notes\n    -----\n    The counter is program level, but mutables are model level. In case multiple models are defined, and\n    you want to have `counter` starting from 1 in the second model, it's recommended to assign keys manually\n    instead of using automatic keys.\n    \"\"\"\n\n    def __init__(self, key=None):\n        super().__init__()\n        if key is not None:\n            if not isinstance(key, str):\n                key = str(key)\n                logger.warning(\"Warning: key \\\"%s\\\" is not string, converted to string.\", key)\n            self._key = key\n        else:\n            self._key = self.__class__.__name__ + str(global_mutable_counting())\n        self.init_hook = self.forward_hook = None\n\n    def __deepcopy__(self, memodict=None):\n        raise NotImplementedError(\"Deep copy doesn't work for mutables.\")\n\n    def __call__(self, *args, **kwargs):\n        self._check_built()\n        return super().__call__(*args, **kwargs)\n\n    def set_mutator(self, mutator):\n        if \"mutator\" in self.__dict__:\n            raise RuntimeError(\"`set_mutator` is called more than once. Did you parse the search space multiple times? \"\n                               \"Or did you apply multiple fixed architectures?\")\n        self.__dict__[\"mutator\"] = mutator\n\n    @property\n    def key(self):\n        \"\"\"\n        Read-only property of key.\n        \"\"\"\n        return self._key\n\n    @property\n    def name(self):\n        \"\"\"\n        After the search space is parsed, it will be the module name of the mutable.\n        \"\"\"\n        return self._name if hasattr(self, \"_name\") else \"_key\"\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    def _check_built(self):\n        if not hasattr(self, \"mutator\"):\n            raise ValueError(\n                \"Mutator not set for {}. You might have forgotten to initialize and apply your mutator. \"\n                \"Or did you initialize a mutable on the fly in forward pass? Move to `__init__` \"\n                \"so that trainer can locate all your mutables. See NNI docs for more details.\".format(self))",
  "class MutableScope(Mutable):\n    \"\"\"\n    Mutable scope marks a subgraph/submodule to help mutators make better decisions.\n\n    If not annotated with mutable scope, search space will be flattened as a list. However, some mutators might\n    need to leverage the concept of a \"cell\". So if a module is defined as a mutable scope, everything in it will\n    look like \"sub-search-space\" in the scope. Scopes can be nested.\n\n    There are two ways mutators can use mutable scope. One is to traverse the search space as a tree during initialization\n    and reset. The other is to implement `enter_mutable_scope` and `exit_mutable_scope`. They are called before and after\n    the forward method of the class inheriting mutable scope.\n\n    Mutable scopes are also mutables that are listed in the mutator.mutables (search space), but they are not supposed\n    to appear in the dict of choices.\n\n    Parameters\n    ----------\n    key : str\n        Key of mutable scope.\n    \"\"\"\n    def __init__(self, key):\n        super().__init__(key=key)\n\n    def __call__(self, *args, **kwargs):\n        try:\n            self._check_built()\n            self.mutator.enter_mutable_scope(self)\n            return super().__call__(*args, **kwargs)\n        finally:\n            self.mutator.exit_mutable_scope(self)",
  "class LayerChoice(Mutable):\n    \"\"\"\n    Layer choice selects one of the ``op_candidates``, then apply it on inputs and return results.\n    In rare cases, it can also select zero or many.\n\n    Layer choice does not allow itself to be nested.\n\n    Parameters\n    ----------\n    op_candidates : list of nn.Module or OrderedDict\n        A module list to be selected from.\n    reduction : str\n        ``mean``, ``concat``, ``sum`` or ``none``. Policy if multiples are selected.\n        If ``none``, a list is returned. ``mean`` returns the average. ``sum`` returns the sum.\n        ``concat`` concatenate the list at dimension 1.\n    return_mask : bool\n        If ``return_mask``, return output tensor and a mask. Otherwise return tensor only.\n    key : str\n        Key of the input choice.\n\n    Attributes\n    ----------\n    length : int\n        Deprecated. Number of ops to choose from. ``len(layer_choice)`` is recommended.\n    names : list of str\n        Names of candidates.\n    choices : list of Module\n        Deprecated. A list of all candidate modules in the layer choice module.\n        ``list(layer_choice)`` is recommended, which will serve the same purpose.\n\n    Notes\n    -----\n    ``op_candidates`` can be a list of modules or a ordered dict of named modules, for example,\n\n    .. code-block:: python\n\n        self.op_choice = LayerChoice(OrderedDict([\n            (\"conv3x3\", nn.Conv2d(3, 16, 128)),\n            (\"conv5x5\", nn.Conv2d(5, 16, 128)),\n            (\"conv7x7\", nn.Conv2d(7, 16, 128))\n        ]))\n\n    Elements in layer choice can be modified or deleted. Use ``del self.op_choice[\"conv5x5\"]`` or\n    ``self.op_choice[1] = nn.Conv3d(...)``. Adding more choices is not supported yet.\n    \"\"\"\n\n    def __init__(self, op_candidates, reduction=\"sum\", return_mask=False, key=None):\n        super().__init__(key=key)\n        self.names = []\n        if isinstance(op_candidates, OrderedDict):\n            for name, module in op_candidates.items():\n                assert name not in [\"length\", \"reduction\", \"return_mask\", \"_key\", \"key\", \"names\"], \\\n                    \"Please don't use a reserved name '{}' for your module.\".format(name)\n                self.add_module(name, module)\n                self.names.append(name)\n        elif isinstance(op_candidates, list):\n            for i, module in enumerate(op_candidates):\n                self.add_module(str(i), module)\n                self.names.append(str(i))\n        else:\n            raise TypeError(\"Unsupported op_candidates type: {}\".format(type(op_candidates)))\n        self.reduction = reduction\n        self.return_mask = return_mask\n\n    def __getitem__(self, idx):\n        if isinstance(idx, str):\n            return self._modules[idx]\n        return list(self)[idx]\n\n    def __setitem__(self, idx, module):\n        key = idx if isinstance(idx, str) else self.names[idx]\n        return setattr(self, key, module)\n\n    def __delitem__(self, idx):\n        if isinstance(idx, slice):\n            for key in self.names[idx]:\n                delattr(self, key)\n        else:\n            if isinstance(idx, str):\n                key, idx = idx, self.names.index(idx)\n            else:\n                key = self.names[idx]\n            delattr(self, key)\n        del self.names[idx]\n\n    @property\n    def length(self):\n        warnings.warn(\"layer_choice.length is deprecated. Use `len(layer_choice)` instead.\", DeprecationWarning)\n        return len(self)\n\n    def __len__(self):\n        return len(self.names)\n\n    def __iter__(self):\n        return map(lambda name: self._modules[name], self.names)\n\n    @property\n    def choices(self):\n        warnings.warn(\"layer_choice.choices is deprecated. Use `list(layer_choice)` instead.\", DeprecationWarning)\n        return list(self)\n\n    def forward(self, *args, **kwargs):\n        \"\"\"\n        Returns\n        -------\n        tuple of tensors\n            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.\n        \"\"\"\n        out, mask = self.mutator.on_forward_layer_choice(self, *args, **kwargs)\n        if self.return_mask:\n            return out, mask\n        return out",
  "class InputChoice(Mutable):\n    \"\"\"\n    Input choice selects ``n_chosen`` inputs from ``choose_from`` (contains ``n_candidates`` keys). For beginners,\n    use ``n_candidates`` instead of ``choose_from`` is a safe option. To get the most power out of it, you might want to\n    know about ``choose_from``.\n\n    The keys in ``choose_from`` can be keys that appear in past mutables, or ``NO_KEY`` if there are no suitable ones.\n    The keys are designed to be the keys of the sources. To help mutators make better decisions,\n    mutators might be interested in how the tensors to choose from come into place. For example, the tensor is the\n    output of some operator, some node, some cell, or some module. If this operator happens to be a mutable (e.g.,\n    ``LayerChoice`` or ``InputChoice``), it has a key naturally that can be used as a source key. If it's a\n    module/submodule, it needs to be annotated with a key: that's where a :class:`MutableScope` is needed.\n\n    In the example below, ``input_choice`` is a 4-choose-any. The first 3 is semantically output of cell1, output of cell2,\n    output of cell3 with respectively. Notice that an extra max pooling is followed by cell1, indicating x1 is not\n    \"actually\" the direct output of cell1.\n\n    .. code-block:: python\n\n        class Cell(MutableScope):\n            pass\n\n        class Net(nn.Module):\n            def __init__(self):\n                self.cell1 = Cell(\"cell1\")\n                self.cell2 = Cell(\"cell2\")\n                self.op = LayerChoice([conv3x3(), conv5x5()], key=\"op\")\n                self.input_choice = InputChoice(choose_from=[\"cell1\", \"cell2\", \"op\", InputChoice.NO_KEY])\n\n            def forward(self, x):\n                x1 = max_pooling(self.cell1(x))\n                x2 = self.cell2(x)\n                x3 = self.op(x)\n                x4 = torch.zeros_like(x)\n                return self.input_choice([x1, x2, x3, x4])\n\n    Parameters\n    ----------\n    n_candidates : int\n        Number of inputs to choose from.\n    choose_from : list of str\n        List of source keys to choose from. At least of one of ``choose_from`` and ``n_candidates`` must be fulfilled.\n        If ``n_candidates`` has a value but ``choose_from`` is None, it will be automatically treated as ``n_candidates``\n        number of empty string.\n    n_chosen : int\n        Recommended inputs to choose. If None, mutator is instructed to select any.\n    reduction : str\n        ``mean``, ``concat``, ``sum`` or ``none``. See :class:`LayerChoice`.\n    return_mask : bool\n        If ``return_mask``, return output tensor and a mask. Otherwise return tensor only.\n    key : str\n        Key of the input choice.\n    \"\"\"\n\n    NO_KEY = \"\"\n\n    def __init__(self, n_candidates=None, choose_from=None, n_chosen=None,\n                 reduction=\"sum\", return_mask=False, key=None):\n        super().__init__(key=key)\n        # precondition check\n        assert n_candidates is not None or choose_from is not None, \"At least one of `n_candidates` and `choose_from`\" \\\n                                                                    \"must be not None.\"\n        if choose_from is not None and n_candidates is None:\n            n_candidates = len(choose_from)\n        elif choose_from is None and n_candidates is not None:\n            choose_from = [self.NO_KEY] * n_candidates\n        assert n_candidates == len(choose_from), \"Number of candidates must be equal to the length of `choose_from`.\"\n        assert n_candidates > 0, \"Number of candidates must be greater than 0.\"\n        assert n_chosen is None or 0 <= n_chosen <= n_candidates, \"Expected selected number must be None or no more \" \\\n                                                                  \"than number of candidates.\"\n\n        self.n_candidates = n_candidates\n        self.choose_from = choose_from.copy()\n        self.n_chosen = n_chosen\n        self.reduction = reduction\n        self.return_mask = return_mask\n\n    def forward(self, optional_inputs):\n        \"\"\"\n        Forward method of LayerChoice.\n\n        Parameters\n        ----------\n        optional_inputs : list or dict\n            Recommended to be a dict. As a dict, inputs will be converted to a list that follows the order of\n            ``choose_from`` in initialization. As a list, inputs must follow the semantic order that is the same as\n            ``choose_from``.\n\n        Returns\n        -------\n        tuple of tensors\n            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.\n        \"\"\"\n        optional_input_list = optional_inputs\n        if isinstance(optional_inputs, dict):\n            optional_input_list = [optional_inputs[tag] for tag in self.choose_from]\n        assert isinstance(optional_input_list, list), \\\n            \"Optional input list must be a list, not a {}.\".format(type(optional_input_list))\n        assert len(optional_inputs) == self.n_candidates, \\\n            \"Length of the input list must be equal to number of candidates.\"\n        out, mask = self.mutator.on_forward_input_choice(self, optional_input_list)\n        if self.return_mask:\n            return out, mask\n        return out",
  "def __init__(self, key=None):\n        super().__init__()\n        if key is not None:\n            if not isinstance(key, str):\n                key = str(key)\n                logger.warning(\"Warning: key \\\"%s\\\" is not string, converted to string.\", key)\n            self._key = key\n        else:\n            self._key = self.__class__.__name__ + str(global_mutable_counting())\n        self.init_hook = self.forward_hook = None",
  "def __deepcopy__(self, memodict=None):\n        raise NotImplementedError(\"Deep copy doesn't work for mutables.\")",
  "def __call__(self, *args, **kwargs):\n        self._check_built()\n        return super().__call__(*args, **kwargs)",
  "def set_mutator(self, mutator):\n        if \"mutator\" in self.__dict__:\n            raise RuntimeError(\"`set_mutator` is called more than once. Did you parse the search space multiple times? \"\n                               \"Or did you apply multiple fixed architectures?\")\n        self.__dict__[\"mutator\"] = mutator",
  "def key(self):\n        \"\"\"\n        Read-only property of key.\n        \"\"\"\n        return self._key",
  "def name(self):\n        \"\"\"\n        After the search space is parsed, it will be the module name of the mutable.\n        \"\"\"\n        return self._name if hasattr(self, \"_name\") else \"_key\"",
  "def name(self, name):\n        self._name = name",
  "def _check_built(self):\n        if not hasattr(self, \"mutator\"):\n            raise ValueError(\n                \"Mutator not set for {}. You might have forgotten to initialize and apply your mutator. \"\n                \"Or did you initialize a mutable on the fly in forward pass? Move to `__init__` \"\n                \"so that trainer can locate all your mutables. See NNI docs for more details.\".format(self))",
  "def __init__(self, key):\n        super().__init__(key=key)",
  "def __call__(self, *args, **kwargs):\n        try:\n            self._check_built()\n            self.mutator.enter_mutable_scope(self)\n            return super().__call__(*args, **kwargs)\n        finally:\n            self.mutator.exit_mutable_scope(self)",
  "def __init__(self, op_candidates, reduction=\"sum\", return_mask=False, key=None):\n        super().__init__(key=key)\n        self.names = []\n        if isinstance(op_candidates, OrderedDict):\n            for name, module in op_candidates.items():\n                assert name not in [\"length\", \"reduction\", \"return_mask\", \"_key\", \"key\", \"names\"], \\\n                    \"Please don't use a reserved name '{}' for your module.\".format(name)\n                self.add_module(name, module)\n                self.names.append(name)\n        elif isinstance(op_candidates, list):\n            for i, module in enumerate(op_candidates):\n                self.add_module(str(i), module)\n                self.names.append(str(i))\n        else:\n            raise TypeError(\"Unsupported op_candidates type: {}\".format(type(op_candidates)))\n        self.reduction = reduction\n        self.return_mask = return_mask",
  "def __getitem__(self, idx):\n        if isinstance(idx, str):\n            return self._modules[idx]\n        return list(self)[idx]",
  "def __setitem__(self, idx, module):\n        key = idx if isinstance(idx, str) else self.names[idx]\n        return setattr(self, key, module)",
  "def __delitem__(self, idx):\n        if isinstance(idx, slice):\n            for key in self.names[idx]:\n                delattr(self, key)\n        else:\n            if isinstance(idx, str):\n                key, idx = idx, self.names.index(idx)\n            else:\n                key = self.names[idx]\n            delattr(self, key)\n        del self.names[idx]",
  "def length(self):\n        warnings.warn(\"layer_choice.length is deprecated. Use `len(layer_choice)` instead.\", DeprecationWarning)\n        return len(self)",
  "def __len__(self):\n        return len(self.names)",
  "def __iter__(self):\n        return map(lambda name: self._modules[name], self.names)",
  "def choices(self):\n        warnings.warn(\"layer_choice.choices is deprecated. Use `list(layer_choice)` instead.\", DeprecationWarning)\n        return list(self)",
  "def forward(self, *args, **kwargs):\n        \"\"\"\n        Returns\n        -------\n        tuple of tensors\n            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.\n        \"\"\"\n        out, mask = self.mutator.on_forward_layer_choice(self, *args, **kwargs)\n        if self.return_mask:\n            return out, mask\n        return out",
  "def __init__(self, n_candidates=None, choose_from=None, n_chosen=None,\n                 reduction=\"sum\", return_mask=False, key=None):\n        super().__init__(key=key)\n        # precondition check\n        assert n_candidates is not None or choose_from is not None, \"At least one of `n_candidates` and `choose_from`\" \\\n                                                                    \"must be not None.\"\n        if choose_from is not None and n_candidates is None:\n            n_candidates = len(choose_from)\n        elif choose_from is None and n_candidates is not None:\n            choose_from = [self.NO_KEY] * n_candidates\n        assert n_candidates == len(choose_from), \"Number of candidates must be equal to the length of `choose_from`.\"\n        assert n_candidates > 0, \"Number of candidates must be greater than 0.\"\n        assert n_chosen is None or 0 <= n_chosen <= n_candidates, \"Expected selected number must be None or no more \" \\\n                                                                  \"than number of candidates.\"\n\n        self.n_candidates = n_candidates\n        self.choose_from = choose_from.copy()\n        self.n_chosen = n_chosen\n        self.reduction = reduction\n        self.return_mask = return_mask",
  "def forward(self, optional_inputs):\n        \"\"\"\n        Forward method of LayerChoice.\n\n        Parameters\n        ----------\n        optional_inputs : list or dict\n            Recommended to be a dict. As a dict, inputs will be converted to a list that follows the order of\n            ``choose_from`` in initialization. As a list, inputs must follow the semantic order that is the same as\n            ``choose_from``.\n\n        Returns\n        -------\n        tuple of tensors\n            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.\n        \"\"\"\n        optional_input_list = optional_inputs\n        if isinstance(optional_inputs, dict):\n            optional_input_list = [optional_inputs[tag] for tag in self.choose_from]\n        assert isinstance(optional_input_list, list), \\\n            \"Optional input list must be a list, not a {}.\".format(type(optional_input_list))\n        assert len(optional_inputs) == self.n_candidates, \\\n            \"Length of the input list must be equal to number of candidates.\"\n        out, mask = self.mutator.on_forward_input_choice(self, optional_input_list)\n        if self.return_mask:\n            return out, mask\n        return out",
  "class StackedLSTMCell(nn.Module):\n    def __init__(self, layers, size, bias):\n        super().__init__()\n        self.lstm_num_layers = layers\n        self.lstm_modules = nn.ModuleList([nn.LSTMCell(size, size, bias=bias)\n                                           for _ in range(self.lstm_num_layers)])\n\n    def forward(self, inputs, hidden):\n        prev_c, prev_h = hidden\n        next_c, next_h = [], []\n        for i, m in enumerate(self.lstm_modules):\n            curr_c, curr_h = m(inputs, (prev_c[i], prev_h[i]))\n            next_c.append(curr_c)\n            next_h.append(curr_h)\n            # current implementation only supports batch size equals 1,\n            # but the algorithm does not necessarily have this limitation\n            inputs = curr_h[-1].view(1, -1)\n        return next_c, next_h",
  "class EnasMutator(Mutator):\n    \"\"\"\n    A mutator that mutates the graph with RL.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model.\n    lstm_size : int\n        Controller LSTM hidden units.\n    lstm_num_layers : int\n        Number of layers for stacked LSTM.\n    tanh_constant : float\n        Logits will be equal to ``tanh_constant * tanh(logits)``. Don't use ``tanh`` if this value is ``None``.\n    cell_exit_extra_step : bool\n        If true, RL controller will perform an extra step at the exit of each MutableScope, dump the hidden state\n        and mark it as the hidden state of this MutableScope. This is to align with the original implementation of paper.\n    skip_target : float\n        Target probability that skipconnect will appear.\n    temperature : float\n        Temperature constant that divides the logits.\n    branch_bias : float\n        Manual bias applied to make some operations more likely to be chosen.\n        Currently this is implemented with a hardcoded match rule that aligns with original repo.\n        If a mutable has a ``reduce`` in its key, all its op choices\n        that contains `conv` in their typename will receive a bias of ``+self.branch_bias`` initially; while others\n        receive a bias of ``-self.branch_bias``.\n    entropy_reduction : str\n        Can be one of ``sum`` and ``mean``. How the entropy of multi-input-choice is reduced.\n    \"\"\"\n\n    def __init__(self, model, lstm_size=64, lstm_num_layers=1, tanh_constant=1.5, cell_exit_extra_step=False,\n                 skip_target=0.4, temperature=None, branch_bias=0.25, entropy_reduction=\"sum\"):\n        super().__init__(model)\n        self.lstm_size = lstm_size\n        self.lstm_num_layers = lstm_num_layers\n        self.tanh_constant = tanh_constant\n        self.temperature = temperature\n        self.cell_exit_extra_step = cell_exit_extra_step\n        self.skip_target = skip_target\n        self.branch_bias = branch_bias\n\n        self.lstm = StackedLSTMCell(self.lstm_num_layers, self.lstm_size, False)\n        self.attn_anchor = nn.Linear(self.lstm_size, self.lstm_size, bias=False)\n        self.attn_query = nn.Linear(self.lstm_size, self.lstm_size, bias=False)\n        self.v_attn = nn.Linear(self.lstm_size, 1, bias=False)\n        self.g_emb = nn.Parameter(torch.randn(1, self.lstm_size) * 0.1)\n        self.skip_targets = nn.Parameter(torch.tensor([1.0 - self.skip_target, self.skip_target]), requires_grad=False)  # pylint: disable=not-callable\n        assert entropy_reduction in [\"sum\", \"mean\"], \"Entropy reduction must be one of sum and mean.\"\n        self.entropy_reduction = torch.sum if entropy_reduction == \"sum\" else torch.mean\n        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction=\"none\")\n        self.bias_dict = nn.ParameterDict()\n\n        self.max_layer_choice = 0\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                if self.max_layer_choice == 0:\n                    self.max_layer_choice = len(mutable)\n                assert self.max_layer_choice == len(mutable), \\\n                    \"ENAS mutator requires all layer choice have the same number of candidates.\"\n                # We are judging by keys and module types to add biases to layer choices. Needs refactor.\n                if \"reduce\" in mutable.key:\n                    def is_conv(choice):\n                        return \"conv\" in str(type(choice)).lower()\n                    bias = torch.tensor([self.branch_bias if is_conv(choice) else -self.branch_bias  # pylint: disable=not-callable\n                                         for choice in mutable])\n                    self.bias_dict[mutable.key] = nn.Parameter(bias, requires_grad=False)\n\n        self.embedding = nn.Embedding(self.max_layer_choice + 1, self.lstm_size)\n        self.soft = nn.Linear(self.lstm_size, self.max_layer_choice, bias=False)\n\n    def sample_search(self):\n        self._initialize()\n        self._sample(self.mutables)\n        return self._choices\n\n    def sample_final(self):\n        return self.sample_search()\n\n    def _sample(self, tree):\n        mutable = tree.mutable\n        if isinstance(mutable, LayerChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_layer_choice(mutable)\n        elif isinstance(mutable, InputChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_input_choice(mutable)\n        for child in tree.children:\n            self._sample(child)\n        if isinstance(mutable, MutableScope) and mutable.key not in self._anchors_hid:\n            if self.cell_exit_extra_step:\n                self._lstm_next_step()\n            self._mark_anchor(mutable.key)\n\n    def _initialize(self):\n        self._choices = dict()\n        self._anchors_hid = dict()\n        self._inputs = self.g_emb.data\n        self._c = [torch.zeros((1, self.lstm_size),\n                               dtype=self._inputs.dtype,\n                               device=self._inputs.device) for _ in range(self.lstm_num_layers)]\n        self._h = [torch.zeros((1, self.lstm_size),\n                               dtype=self._inputs.dtype,\n                               device=self._inputs.device) for _ in range(self.lstm_num_layers)]\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0\n\n    def _lstm_next_step(self):\n        self._c, self._h = self.lstm(self._inputs, (self._c, self._h))\n\n    def _mark_anchor(self, key):\n        self._anchors_hid[key] = self._h[-1]\n\n    def _sample_layer_choice(self, mutable):\n        self._lstm_next_step()\n        logit = self.soft(self._h[-1])\n        if self.temperature is not None:\n            logit /= self.temperature\n        if self.tanh_constant is not None:\n            logit = self.tanh_constant * torch.tanh(logit)\n        if mutable.key in self.bias_dict:\n            logit += self.bias_dict[mutable.key]\n        branch_id = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n        log_prob = self.cross_entropy_loss(logit, branch_id)\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = (log_prob * torch.exp(-log_prob)).detach()  # pylint: disable=invalid-unary-operand-type\n        self.sample_entropy += self.entropy_reduction(entropy)\n        self._inputs = self.embedding(branch_id)\n        return F.one_hot(branch_id, num_classes=self.max_layer_choice).bool().view(-1)\n\n    def _sample_input_choice(self, mutable):\n        query, anchors = [], []\n        for label in mutable.choose_from:\n            if label not in self._anchors_hid:\n                self._lstm_next_step()\n                self._mark_anchor(label)  # empty loop, fill not found\n            query.append(self.attn_anchor(self._anchors_hid[label]))\n            anchors.append(self._anchors_hid[label])\n        query = torch.cat(query, 0)\n        query = torch.tanh(query + self.attn_query(self._h[-1]))\n        query = self.v_attn(query)\n        if self.temperature is not None:\n            query /= self.temperature\n        if self.tanh_constant is not None:\n            query = self.tanh_constant * torch.tanh(query)\n\n        if mutable.n_chosen is None:\n            logit = torch.cat([-query, query], 1)  # pylint: disable=invalid-unary-operand-type\n\n            skip = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n            skip_prob = torch.sigmoid(logit)\n            kl = torch.sum(skip_prob * torch.log(skip_prob / self.skip_targets))\n            self.sample_skip_penalty += kl\n            log_prob = self.cross_entropy_loss(logit, skip)\n            self._inputs = (torch.matmul(skip.float(), torch.cat(anchors, 0)) / (1. + torch.sum(skip))).unsqueeze(0)\n        else:\n            assert mutable.n_chosen == 1, \"Input choice must select exactly one or any in ENAS.\"\n            logit = query.view(1, -1)\n            index = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n            skip = F.one_hot(index, num_classes=mutable.n_candidates).view(-1)\n            log_prob = self.cross_entropy_loss(logit, index)\n            self._inputs = anchors[index.item()]\n\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = (log_prob * torch.exp(-log_prob)).detach()  # pylint: disable=invalid-unary-operand-type\n        self.sample_entropy += self.entropy_reduction(entropy)\n        return skip.bool()",
  "def __init__(self, layers, size, bias):\n        super().__init__()\n        self.lstm_num_layers = layers\n        self.lstm_modules = nn.ModuleList([nn.LSTMCell(size, size, bias=bias)\n                                           for _ in range(self.lstm_num_layers)])",
  "def forward(self, inputs, hidden):\n        prev_c, prev_h = hidden\n        next_c, next_h = [], []\n        for i, m in enumerate(self.lstm_modules):\n            curr_c, curr_h = m(inputs, (prev_c[i], prev_h[i]))\n            next_c.append(curr_c)\n            next_h.append(curr_h)\n            # current implementation only supports batch size equals 1,\n            # but the algorithm does not necessarily have this limitation\n            inputs = curr_h[-1].view(1, -1)\n        return next_c, next_h",
  "def __init__(self, model, lstm_size=64, lstm_num_layers=1, tanh_constant=1.5, cell_exit_extra_step=False,\n                 skip_target=0.4, temperature=None, branch_bias=0.25, entropy_reduction=\"sum\"):\n        super().__init__(model)\n        self.lstm_size = lstm_size\n        self.lstm_num_layers = lstm_num_layers\n        self.tanh_constant = tanh_constant\n        self.temperature = temperature\n        self.cell_exit_extra_step = cell_exit_extra_step\n        self.skip_target = skip_target\n        self.branch_bias = branch_bias\n\n        self.lstm = StackedLSTMCell(self.lstm_num_layers, self.lstm_size, False)\n        self.attn_anchor = nn.Linear(self.lstm_size, self.lstm_size, bias=False)\n        self.attn_query = nn.Linear(self.lstm_size, self.lstm_size, bias=False)\n        self.v_attn = nn.Linear(self.lstm_size, 1, bias=False)\n        self.g_emb = nn.Parameter(torch.randn(1, self.lstm_size) * 0.1)\n        self.skip_targets = nn.Parameter(torch.tensor([1.0 - self.skip_target, self.skip_target]), requires_grad=False)  # pylint: disable=not-callable\n        assert entropy_reduction in [\"sum\", \"mean\"], \"Entropy reduction must be one of sum and mean.\"\n        self.entropy_reduction = torch.sum if entropy_reduction == \"sum\" else torch.mean\n        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction=\"none\")\n        self.bias_dict = nn.ParameterDict()\n\n        self.max_layer_choice = 0\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                if self.max_layer_choice == 0:\n                    self.max_layer_choice = len(mutable)\n                assert self.max_layer_choice == len(mutable), \\\n                    \"ENAS mutator requires all layer choice have the same number of candidates.\"\n                # We are judging by keys and module types to add biases to layer choices. Needs refactor.\n                if \"reduce\" in mutable.key:\n                    def is_conv(choice):\n                        return \"conv\" in str(type(choice)).lower()\n                    bias = torch.tensor([self.branch_bias if is_conv(choice) else -self.branch_bias  # pylint: disable=not-callable\n                                         for choice in mutable])\n                    self.bias_dict[mutable.key] = nn.Parameter(bias, requires_grad=False)\n\n        self.embedding = nn.Embedding(self.max_layer_choice + 1, self.lstm_size)\n        self.soft = nn.Linear(self.lstm_size, self.max_layer_choice, bias=False)",
  "def sample_search(self):\n        self._initialize()\n        self._sample(self.mutables)\n        return self._choices",
  "def sample_final(self):\n        return self.sample_search()",
  "def _sample(self, tree):\n        mutable = tree.mutable\n        if isinstance(mutable, LayerChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_layer_choice(mutable)\n        elif isinstance(mutable, InputChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_input_choice(mutable)\n        for child in tree.children:\n            self._sample(child)\n        if isinstance(mutable, MutableScope) and mutable.key not in self._anchors_hid:\n            if self.cell_exit_extra_step:\n                self._lstm_next_step()\n            self._mark_anchor(mutable.key)",
  "def _initialize(self):\n        self._choices = dict()\n        self._anchors_hid = dict()\n        self._inputs = self.g_emb.data\n        self._c = [torch.zeros((1, self.lstm_size),\n                               dtype=self._inputs.dtype,\n                               device=self._inputs.device) for _ in range(self.lstm_num_layers)]\n        self._h = [torch.zeros((1, self.lstm_size),\n                               dtype=self._inputs.dtype,\n                               device=self._inputs.device) for _ in range(self.lstm_num_layers)]\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0",
  "def _lstm_next_step(self):\n        self._c, self._h = self.lstm(self._inputs, (self._c, self._h))",
  "def _mark_anchor(self, key):\n        self._anchors_hid[key] = self._h[-1]",
  "def _sample_layer_choice(self, mutable):\n        self._lstm_next_step()\n        logit = self.soft(self._h[-1])\n        if self.temperature is not None:\n            logit /= self.temperature\n        if self.tanh_constant is not None:\n            logit = self.tanh_constant * torch.tanh(logit)\n        if mutable.key in self.bias_dict:\n            logit += self.bias_dict[mutable.key]\n        branch_id = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n        log_prob = self.cross_entropy_loss(logit, branch_id)\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = (log_prob * torch.exp(-log_prob)).detach()  # pylint: disable=invalid-unary-operand-type\n        self.sample_entropy += self.entropy_reduction(entropy)\n        self._inputs = self.embedding(branch_id)\n        return F.one_hot(branch_id, num_classes=self.max_layer_choice).bool().view(-1)",
  "def _sample_input_choice(self, mutable):\n        query, anchors = [], []\n        for label in mutable.choose_from:\n            if label not in self._anchors_hid:\n                self._lstm_next_step()\n                self._mark_anchor(label)  # empty loop, fill not found\n            query.append(self.attn_anchor(self._anchors_hid[label]))\n            anchors.append(self._anchors_hid[label])\n        query = torch.cat(query, 0)\n        query = torch.tanh(query + self.attn_query(self._h[-1]))\n        query = self.v_attn(query)\n        if self.temperature is not None:\n            query /= self.temperature\n        if self.tanh_constant is not None:\n            query = self.tanh_constant * torch.tanh(query)\n\n        if mutable.n_chosen is None:\n            logit = torch.cat([-query, query], 1)  # pylint: disable=invalid-unary-operand-type\n\n            skip = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n            skip_prob = torch.sigmoid(logit)\n            kl = torch.sum(skip_prob * torch.log(skip_prob / self.skip_targets))\n            self.sample_skip_penalty += kl\n            log_prob = self.cross_entropy_loss(logit, skip)\n            self._inputs = (torch.matmul(skip.float(), torch.cat(anchors, 0)) / (1. + torch.sum(skip))).unsqueeze(0)\n        else:\n            assert mutable.n_chosen == 1, \"Input choice must select exactly one or any in ENAS.\"\n            logit = query.view(1, -1)\n            index = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n            skip = F.one_hot(index, num_classes=mutable.n_candidates).view(-1)\n            log_prob = self.cross_entropy_loss(logit, index)\n            self._inputs = anchors[index.item()]\n\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = (log_prob * torch.exp(-log_prob)).detach()  # pylint: disable=invalid-unary-operand-type\n        self.sample_entropy += self.entropy_reduction(entropy)\n        return skip.bool()",
  "def is_conv(choice):\n                        return \"conv\" in str(type(choice)).lower()",
  "class EnasTrainer(Trainer):\n    \"\"\"\n    ENAS trainer.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model to be trained.\n    loss : callable\n        Receives logits and ground truth label, return a loss tensor.\n    metrics : callable\n        Receives logits and ground truth label, return a dict of metrics.\n    reward_function : callable\n        Receives logits and ground truth label, return a tensor, which will be feeded to RL controller as reward.\n    optimizer : Optimizer\n        The optimizer used for optimizing the model.\n    num_epochs : int\n        Number of epochs planned for training.\n    dataset_train : Dataset\n        Dataset for training. Will be split for training weights and architecture weights.\n    dataset_valid : Dataset\n        Dataset for testing.\n    mutator : EnasMutator\n        Use when customizing your own mutator or a mutator with customized parameters.\n    batch_size : int\n        Batch size.\n    workers : int\n        Workers for data loading.\n    device : torch.device\n        ``torch.device(\"cpu\")`` or ``torch.device(\"cuda\")``.\n    log_frequency : int\n        Step count per logging.\n    callbacks : list of Callback\n        list of callbacks to trigger at events.\n    entropy_weight : float\n        Weight of sample entropy loss.\n    skip_weight : float\n        Weight of skip penalty loss.\n    baseline_decay : float\n        Decay factor of baseline. New baseline will be equal to ``baseline_decay * baseline_old + reward * (1 - baseline_decay)``.\n    child_steps : int\n        How many mini-batches for model training per epoch.\n    mutator_lr : float\n        Learning rate for RL controller.\n    mutator_steps_aggregate : int\n        Number of steps that will be aggregated into one mini-batch for RL controller.\n    mutator_steps : int\n        Number of mini-batches for each epoch of RL controller learning.\n    aux_weight : float\n        Weight of auxiliary head loss. ``aux_weight * aux_loss`` will be added to total loss.\n    test_arc_per_epoch : int\n        How many architectures are chosen for direct test after each epoch.\n    \"\"\"\n    def __init__(self, model, loss, metrics, reward_function,\n                 optimizer, num_epochs, dataset_train, dataset_valid,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None, callbacks=None,\n                 entropy_weight=0.0001, skip_weight=0.8, baseline_decay=0.999, child_steps=500,\n                 mutator_lr=0.00035, mutator_steps_aggregate=20, mutator_steps=50, aux_weight=0.4,\n                 test_arc_per_epoch=1):\n        super().__init__(model, mutator if mutator is not None else EnasMutator(model),\n                         loss, metrics, optimizer, num_epochs, dataset_train, dataset_valid,\n                         batch_size, workers, device, log_frequency, callbacks)\n        self.reward_function = reward_function\n        self.mutator_optim = optim.Adam(self.mutator.parameters(), lr=mutator_lr)\n        self.batch_size = batch_size\n        self.workers = workers\n\n        self.entropy_weight = entropy_weight\n        self.skip_weight = skip_weight\n        self.baseline_decay = baseline_decay\n        self.baseline = 0.\n        self.mutator_steps_aggregate = mutator_steps_aggregate\n        self.mutator_steps = mutator_steps\n        self.child_steps = child_steps\n        self.aux_weight = aux_weight\n        self.test_arc_per_epoch = test_arc_per_epoch\n\n        self.init_dataloader()\n\n    def init_dataloader(self):\n        n_train = len(self.dataset_train)\n        split = n_train // 10\n        indices = list(range(n_train))\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:-split])\n        valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[-split:])\n        self.train_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=self.batch_size,\n                                                        sampler=train_sampler,\n                                                        num_workers=self.workers)\n        self.valid_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=self.batch_size,\n                                                        sampler=valid_sampler,\n                                                        num_workers=self.workers)\n        self.test_loader = torch.utils.data.DataLoader(self.dataset_valid,\n                                                       batch_size=self.batch_size,\n                                                       num_workers=self.workers)\n        self.train_loader = cycle(self.train_loader)\n        self.valid_loader = cycle(self.valid_loader)\n\n    def train_one_epoch(self, epoch):\n        # Sample model and train\n        self.model.train()\n        self.mutator.eval()\n        meters = AverageMeterGroup()\n        for step in range(1, self.child_steps + 1):\n            x, y = next(self.train_loader)\n            x, y = to_device(x, self.device), to_device(y, self.device)\n            self.optimizer.zero_grad()\n\n            with torch.no_grad():\n                self.mutator.reset()\n            self._write_graph_status()\n            logits = self.model(x)\n\n            if isinstance(logits, tuple):\n                logits, aux_logits = logits\n                aux_loss = self.loss(aux_logits, y)\n            else:\n                aux_loss = 0.\n            metrics = self.metrics(logits, y)\n            loss = self.loss(logits, y)\n            loss = loss + self.aux_weight * aux_loss\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.model.parameters(), 5.)\n            self.optimizer.step()\n            metrics[\"loss\"] = loss.item()\n            meters.update(metrics)\n\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(\"Model Epoch [%d/%d] Step [%d/%d]  %s\", epoch + 1,\n                            self.num_epochs, step, self.child_steps, meters)\n\n        # Train sampler (mutator)\n        self.model.eval()\n        self.mutator.train()\n        meters = AverageMeterGroup()\n        for mutator_step in range(1, self.mutator_steps + 1):\n            self.mutator_optim.zero_grad()\n            for step in range(1, self.mutator_steps_aggregate + 1):\n                x, y = next(self.valid_loader)\n                x, y = to_device(x, self.device), to_device(y, self.device)\n\n                self.mutator.reset()\n                with torch.no_grad():\n                    logits = self.model(x)\n                self._write_graph_status()\n                metrics = self.metrics(logits, y)\n                reward = self.reward_function(logits, y)\n                if self.entropy_weight:\n                    reward += self.entropy_weight * self.mutator.sample_entropy.item()\n                self.baseline = self.baseline * self.baseline_decay + reward * (1 - self.baseline_decay)\n                loss = self.mutator.sample_log_prob * (reward - self.baseline)\n                if self.skip_weight:\n                    loss += self.skip_weight * self.mutator.sample_skip_penalty\n                metrics[\"reward\"] = reward\n                metrics[\"loss\"] = loss.item()\n                metrics[\"ent\"] = self.mutator.sample_entropy.item()\n                metrics[\"log_prob\"] = self.mutator.sample_log_prob.item()\n                metrics[\"baseline\"] = self.baseline\n                metrics[\"skip\"] = self.mutator.sample_skip_penalty\n\n                loss /= self.mutator_steps_aggregate\n                loss.backward()\n                meters.update(metrics)\n\n                cur_step = step + (mutator_step - 1) * self.mutator_steps_aggregate\n                if self.log_frequency is not None and cur_step % self.log_frequency == 0:\n                    logger.info(\"RL Epoch [%d/%d] Step [%d/%d] [%d/%d]  %s\", epoch + 1, self.num_epochs,\n                                mutator_step, self.mutator_steps, step, self.mutator_steps_aggregate,\n                                meters)\n\n            nn.utils.clip_grad_norm_(self.mutator.parameters(), 5.)\n            self.mutator_optim.step()\n\n    def validate_one_epoch(self, epoch):\n        with torch.no_grad():\n            for arc_id in range(self.test_arc_per_epoch):\n                meters = AverageMeterGroup()\n                for x, y in self.test_loader:\n                    x, y = to_device(x, self.device), to_device(y, self.device)\n                    self.mutator.reset()\n                    logits = self.model(x)\n                    if isinstance(logits, tuple):\n                        logits, _ = logits\n                    metrics = self.metrics(logits, y)\n                    loss = self.loss(logits, y)\n                    metrics[\"loss\"] = loss.item()\n                    meters.update(metrics)\n\n                logger.info(\"Test Epoch [%d/%d] Arc [%d/%d] Summary  %s\",\n                            epoch + 1, self.num_epochs, arc_id + 1, self.test_arc_per_epoch,\n                            meters.summary())",
  "def __init__(self, model, loss, metrics, reward_function,\n                 optimizer, num_epochs, dataset_train, dataset_valid,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None, callbacks=None,\n                 entropy_weight=0.0001, skip_weight=0.8, baseline_decay=0.999, child_steps=500,\n                 mutator_lr=0.00035, mutator_steps_aggregate=20, mutator_steps=50, aux_weight=0.4,\n                 test_arc_per_epoch=1):\n        super().__init__(model, mutator if mutator is not None else EnasMutator(model),\n                         loss, metrics, optimizer, num_epochs, dataset_train, dataset_valid,\n                         batch_size, workers, device, log_frequency, callbacks)\n        self.reward_function = reward_function\n        self.mutator_optim = optim.Adam(self.mutator.parameters(), lr=mutator_lr)\n        self.batch_size = batch_size\n        self.workers = workers\n\n        self.entropy_weight = entropy_weight\n        self.skip_weight = skip_weight\n        self.baseline_decay = baseline_decay\n        self.baseline = 0.\n        self.mutator_steps_aggregate = mutator_steps_aggregate\n        self.mutator_steps = mutator_steps\n        self.child_steps = child_steps\n        self.aux_weight = aux_weight\n        self.test_arc_per_epoch = test_arc_per_epoch\n\n        self.init_dataloader()",
  "def init_dataloader(self):\n        n_train = len(self.dataset_train)\n        split = n_train // 10\n        indices = list(range(n_train))\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:-split])\n        valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[-split:])\n        self.train_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=self.batch_size,\n                                                        sampler=train_sampler,\n                                                        num_workers=self.workers)\n        self.valid_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=self.batch_size,\n                                                        sampler=valid_sampler,\n                                                        num_workers=self.workers)\n        self.test_loader = torch.utils.data.DataLoader(self.dataset_valid,\n                                                       batch_size=self.batch_size,\n                                                       num_workers=self.workers)\n        self.train_loader = cycle(self.train_loader)\n        self.valid_loader = cycle(self.valid_loader)",
  "def train_one_epoch(self, epoch):\n        # Sample model and train\n        self.model.train()\n        self.mutator.eval()\n        meters = AverageMeterGroup()\n        for step in range(1, self.child_steps + 1):\n            x, y = next(self.train_loader)\n            x, y = to_device(x, self.device), to_device(y, self.device)\n            self.optimizer.zero_grad()\n\n            with torch.no_grad():\n                self.mutator.reset()\n            self._write_graph_status()\n            logits = self.model(x)\n\n            if isinstance(logits, tuple):\n                logits, aux_logits = logits\n                aux_loss = self.loss(aux_logits, y)\n            else:\n                aux_loss = 0.\n            metrics = self.metrics(logits, y)\n            loss = self.loss(logits, y)\n            loss = loss + self.aux_weight * aux_loss\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.model.parameters(), 5.)\n            self.optimizer.step()\n            metrics[\"loss\"] = loss.item()\n            meters.update(metrics)\n\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(\"Model Epoch [%d/%d] Step [%d/%d]  %s\", epoch + 1,\n                            self.num_epochs, step, self.child_steps, meters)\n\n        # Train sampler (mutator)\n        self.model.eval()\n        self.mutator.train()\n        meters = AverageMeterGroup()\n        for mutator_step in range(1, self.mutator_steps + 1):\n            self.mutator_optim.zero_grad()\n            for step in range(1, self.mutator_steps_aggregate + 1):\n                x, y = next(self.valid_loader)\n                x, y = to_device(x, self.device), to_device(y, self.device)\n\n                self.mutator.reset()\n                with torch.no_grad():\n                    logits = self.model(x)\n                self._write_graph_status()\n                metrics = self.metrics(logits, y)\n                reward = self.reward_function(logits, y)\n                if self.entropy_weight:\n                    reward += self.entropy_weight * self.mutator.sample_entropy.item()\n                self.baseline = self.baseline * self.baseline_decay + reward * (1 - self.baseline_decay)\n                loss = self.mutator.sample_log_prob * (reward - self.baseline)\n                if self.skip_weight:\n                    loss += self.skip_weight * self.mutator.sample_skip_penalty\n                metrics[\"reward\"] = reward\n                metrics[\"loss\"] = loss.item()\n                metrics[\"ent\"] = self.mutator.sample_entropy.item()\n                metrics[\"log_prob\"] = self.mutator.sample_log_prob.item()\n                metrics[\"baseline\"] = self.baseline\n                metrics[\"skip\"] = self.mutator.sample_skip_penalty\n\n                loss /= self.mutator_steps_aggregate\n                loss.backward()\n                meters.update(metrics)\n\n                cur_step = step + (mutator_step - 1) * self.mutator_steps_aggregate\n                if self.log_frequency is not None and cur_step % self.log_frequency == 0:\n                    logger.info(\"RL Epoch [%d/%d] Step [%d/%d] [%d/%d]  %s\", epoch + 1, self.num_epochs,\n                                mutator_step, self.mutator_steps, step, self.mutator_steps_aggregate,\n                                meters)\n\n            nn.utils.clip_grad_norm_(self.mutator.parameters(), 5.)\n            self.mutator_optim.step()",
  "def validate_one_epoch(self, epoch):\n        with torch.no_grad():\n            for arc_id in range(self.test_arc_per_epoch):\n                meters = AverageMeterGroup()\n                for x, y in self.test_loader:\n                    x, y = to_device(x, self.device), to_device(y, self.device)\n                    self.mutator.reset()\n                    logits = self.model(x)\n                    if isinstance(logits, tuple):\n                        logits, _ = logits\n                    metrics = self.metrics(logits, y)\n                    loss = self.loss(logits, y)\n                    metrics[\"loss\"] = loss.item()\n                    meters.update(metrics)\n\n                logger.info(\"Test Epoch [%d/%d] Arc [%d/%d] Summary  %s\",\n                            epoch + 1, self.num_epochs, arc_id + 1, self.test_arc_per_epoch,\n                            meters.summary())",
  "def detach_variable(inputs):\n    \"\"\"\n    Detach variables\n\n    Parameters\n    ----------\n    inputs : pytorch tensors\n        pytorch tensors\n    \"\"\"\n    if isinstance(inputs, tuple):\n        return tuple([detach_variable(x) for x in inputs])\n    else:\n        x = inputs.detach()\n        x.requires_grad = inputs.requires_grad\n        return x",
  "def cross_entropy_with_label_smoothing(pred, target, label_smoothing=0.1):\n    \"\"\"\n    Parameters\n    ----------\n    pred : pytorch tensor\n        predicted value\n    target : pytorch tensor\n        label\n    label_smoothing : float\n        the degree of label smoothing\n\n    Returns\n    -------\n    pytorch tensor\n        cross entropy\n    \"\"\"\n    logsoftmax = nn.LogSoftmax()\n    n_classes = pred.size(1)\n    # convert to one-hot\n    target = torch.unsqueeze(target, 1)\n    soft_target = torch.zeros_like(pred)\n    soft_target.scatter_(1, target, 1)\n    # label smoothing\n    soft_target = soft_target * (1 - label_smoothing) + label_smoothing / n_classes\n    return torch.mean(torch.sum(- soft_target * logsoftmax(pred), 1))",
  "def accuracy(output, target, topk=(1,)):\n    \"\"\"\n    Computes the precision@k for the specified values of k\n\n    Parameters\n    ----------\n    output : pytorch tensor\n        output, e.g., predicted value\n    target : pytorch tensor\n        label\n    topk : tuple\n        specify top1 and top5\n\n    Returns\n    -------\n    list\n        accuracy of top1 and top5\n    \"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res",
  "class ArchGradientFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, binary_gates, run_func, backward_func):\n        ctx.run_func = run_func\n        ctx.backward_func = backward_func\n\n        detached_x = detach_variable(x)\n        with torch.enable_grad():\n            output = run_func(detached_x)\n        ctx.save_for_backward(detached_x, output)\n        return output.data\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        detached_x, output = ctx.saved_tensors\n\n        grad_x = torch.autograd.grad(output, detached_x, grad_output, only_inputs=True)\n        # compute gradients w.r.t. binary_gates\n        binary_grads = ctx.backward_func(detached_x.data, output.data, grad_output.data)\n\n        return grad_x[0], binary_grads, None, None",
  "class MixedOp(nn.Module):\n    \"\"\"\n    This class is to instantiate and manage info of one LayerChoice.\n    It includes architecture weights, binary weights, and member functions\n    operating the weights.\n\n    forward_mode:\n        forward/backward mode for LayerChoice: None, two, full, and full_v2.\n        For training architecture weights, we use full_v2 by default, and for training\n        model weights, we use None.\n    \"\"\"\n    forward_mode = None\n    def __init__(self, mutable):\n        \"\"\"\n        Parameters\n        ----------\n        mutable : LayerChoice\n            A LayerChoice in user model\n        \"\"\"\n        super(MixedOp, self).__init__()\n        self.ap_path_alpha = nn.Parameter(torch.Tensor(len(mutable)))\n        self.ap_path_wb = nn.Parameter(torch.Tensor(len(mutable)))\n        self.ap_path_alpha.requires_grad = False\n        self.ap_path_wb.requires_grad = False\n        self.active_index = [0]\n        self.inactive_index = None\n        self.log_prob = None\n        self.current_prob_over_ops = None\n        self.n_choices = len(mutable)\n\n    def get_ap_path_alpha(self):\n        return self.ap_path_alpha\n\n    def to_requires_grad(self):\n        self.ap_path_alpha.requires_grad = True\n        self.ap_path_wb.requires_grad = True\n\n    def to_disable_grad(self):\n        self.ap_path_alpha.requires_grad = False\n        self.ap_path_wb.requires_grad = False\n\n    def forward(self, mutable, x):\n        \"\"\"\n        Define forward of LayerChoice. For 'full_v2', backward is also defined.\n        The 'two' mode is explained in section 3.2.1 in the paper.\n        The 'full_v2' mode is explained in Appendix D in the paper.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            this layer's mutable\n        x : tensor\n            inputs of this layer, only support one input\n\n        Returns\n        -------\n        output: tensor\n            output of this layer\n        \"\"\"\n        if MixedOp.forward_mode == 'full' or MixedOp.forward_mode == 'two':\n            output = 0\n            for _i in self.active_index:\n                oi = self.candidate_ops[_i](x)\n                output = output + self.ap_path_wb[_i] * oi\n            for _i in self.inactive_index:\n                oi = self.candidate_ops[_i](x)\n                output = output + self.ap_path_wb[_i] * oi.detach()\n        elif MixedOp.forward_mode == 'full_v2':\n            def run_function(key, candidate_ops, active_id):\n                def forward(_x):\n                    return candidate_ops[active_id](_x)\n                return forward\n\n            def backward_function(key, candidate_ops, active_id, binary_gates):\n                def backward(_x, _output, grad_output):\n                    binary_grads = torch.zeros_like(binary_gates.data)\n                    with torch.no_grad():\n                        for k in range(len(candidate_ops)):\n                            if k != active_id:\n                                out_k = candidate_ops[k](_x.data)\n                            else:\n                                out_k = _output.data\n                            grad_k = torch.sum(out_k * grad_output)\n                            binary_grads[k] = grad_k\n                    return binary_grads\n                return backward\n            output = ArchGradientFunction.apply(\n                x, self.ap_path_wb, run_function(mutable.key, list(mutable), self.active_index[0]),\n                backward_function(mutable.key, list(mutable), self.active_index[0], self.ap_path_wb))\n        else:\n            output = self.active_op(mutable)(x)\n        return output\n\n    @property\n    def probs_over_ops(self):\n        \"\"\"\n        Apply softmax on alpha to generate probability distribution\n\n        Returns\n        -------\n        pytorch tensor\n            probability distribution\n        \"\"\"\n        probs = F.softmax(self.ap_path_alpha, dim=0)  # softmax to probability\n        return probs\n\n    @property\n    def chosen_index(self):\n        \"\"\"\n        choose the op with max prob\n\n        Returns\n        -------\n        int\n            index of the chosen one\n        numpy.float32\n            prob of the chosen one\n        \"\"\"\n        probs = self.probs_over_ops.data.cpu().numpy()\n        index = int(np.argmax(probs))\n        return index, probs[index]\n\n    def active_op(self, mutable):\n        \"\"\"\n        assume only one path is active\n\n        Returns\n        -------\n        PyTorch module\n            the chosen operation\n        \"\"\"\n        return mutable[self.active_index[0]]\n\n    @property\n    def active_op_index(self):\n        \"\"\"\n        return active op's index, the active op is sampled\n\n        Returns\n        -------\n        int\n            index of the active op\n        \"\"\"\n        return self.active_index[0]\n\n    def set_chosen_op_active(self):\n        \"\"\"\n        set chosen index, active and inactive indexes\n        \"\"\"\n        chosen_idx, _ = self.chosen_index\n        self.active_index = [chosen_idx]\n        self.inactive_index = [_i for _i in range(0, chosen_idx)] + \\\n                              [_i for _i in range(chosen_idx + 1, self.n_choices)]\n\n    def binarize(self, mutable):\n        \"\"\"\n        Sample based on alpha, and set binary weights accordingly.\n        ap_path_wb is set in this function, which is called binarize.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            this layer's mutable\n        \"\"\"\n        self.log_prob = None\n        # reset binary gates\n        self.ap_path_wb.data.zero_()\n        probs = self.probs_over_ops\n        if MixedOp.forward_mode == 'two':\n            # sample two ops according to probs\n            sample_op = torch.multinomial(probs.data, 2, replacement=False)\n            probs_slice = F.softmax(torch.stack([\n                self.ap_path_alpha[idx] for idx in sample_op\n            ]), dim=0)\n            self.current_prob_over_ops = torch.zeros_like(probs)\n            for i, idx in enumerate(sample_op):\n                self.current_prob_over_ops[idx] = probs_slice[i]\n            # choose one to be active and the other to be inactive according to probs_slice\n            c = torch.multinomial(probs_slice.data, 1)[0] # 0 or 1\n            active_op = sample_op[c].item()\n            inactive_op = sample_op[1-c].item()\n            self.active_index = [active_op]\n            self.inactive_index = [inactive_op]\n            # set binary gate\n            self.ap_path_wb.data[active_op] = 1.0\n        else:\n            sample = torch.multinomial(probs, 1)[0].item()\n            self.active_index = [sample]\n            self.inactive_index = [_i for _i in range(0, sample)] + \\\n                                [_i for _i in range(sample + 1, len(mutable))]\n            self.log_prob = torch.log(probs[sample])\n            self.current_prob_over_ops = probs\n            self.ap_path_wb.data[sample] = 1.0\n        # avoid over-regularization\n        for choice in mutable:\n            for _, param in choice.named_parameters():\n                param.grad = None\n\n    @staticmethod\n    def delta_ij(i, j):\n        if i == j:\n            return 1\n        else:\n            return 0\n\n    def set_arch_param_grad(self, mutable):\n        \"\"\"\n        Calculate alpha gradient for this LayerChoice.\n        It is calculated using gradient of binary gate, probs of ops.\n        \"\"\"\n        binary_grads = self.ap_path_wb.grad.data\n        if self.active_op(mutable).is_zero_layer():\n            self.ap_path_alpha.grad = None\n            return\n        if self.ap_path_alpha.grad is None:\n            self.ap_path_alpha.grad = torch.zeros_like(self.ap_path_alpha.data)\n        if MixedOp.forward_mode == 'two':\n            involved_idx = self.active_index + self.inactive_index\n            probs_slice = F.softmax(torch.stack([\n                self.ap_path_alpha[idx] for idx in involved_idx\n            ]), dim=0).data\n            for i in range(2):\n                for j in range(2):\n                    origin_i = involved_idx[i]\n                    origin_j = involved_idx[j]\n                    self.ap_path_alpha.grad.data[origin_i] += \\\n                        binary_grads[origin_j] * probs_slice[j] * (MixedOp.delta_ij(i, j) - probs_slice[i])\n            for _i, idx in enumerate(self.active_index):\n                self.active_index[_i] = (idx, self.ap_path_alpha.data[idx].item())\n            for _i, idx in enumerate(self.inactive_index):\n                self.inactive_index[_i] = (idx, self.ap_path_alpha.data[idx].item())\n        else:\n            probs = self.probs_over_ops.data\n            for i in range(self.n_choices):\n                for j in range(self.n_choices):\n                    self.ap_path_alpha.grad.data[i] += binary_grads[j] * probs[j] * (MixedOp.delta_ij(i, j) - probs[i])\n        return\n\n    def rescale_updated_arch_param(self):\n        \"\"\"\n        rescale architecture weights for the 'two' mode.\n        \"\"\"\n        if not isinstance(self.active_index[0], tuple):\n            assert self.active_op.is_zero_layer()\n            return\n        involved_idx = [idx for idx, _ in (self.active_index + self.inactive_index)]\n        old_alphas = [alpha for _, alpha in (self.active_index + self.inactive_index)]\n        new_alphas = [self.ap_path_alpha.data[idx] for idx in involved_idx]\n\n        offset = math.log(\n            sum([math.exp(alpha) for alpha in new_alphas]) / sum([math.exp(alpha) for alpha in old_alphas])\n        )\n\n        for idx in involved_idx:\n            self.ap_path_alpha.data[idx] -= offset",
  "class ProxylessNasMutator(BaseMutator):\n    \"\"\"\n    This mutator initializes and operates all the LayerChoices of the input model.\n    It is for the corresponding trainer to control the training process of LayerChoices,\n    coordinating with whole training process.\n    \"\"\"\n    def __init__(self, model):\n        \"\"\"\n        Init a MixedOp instance for each mutable i.e., LayerChoice.\n        And register the instantiated MixedOp in corresponding LayerChoice.\n        If does not register it in LayerChoice, DataParallel does not work then,\n        because architecture weights are not included in the DataParallel model.\n        When MixedOPs are registered, we use ```requires_grad``` to control\n        whether calculate gradients of architecture weights.\n\n        Parameters\n        ----------\n        model : pytorch model\n            The model that users want to tune, it includes search space defined with nni nas apis\n        \"\"\"\n        super(ProxylessNasMutator, self).__init__(model)\n        self._unused_modules = None\n        self.mutable_list = []\n        for mutable in self.undedup_mutables:\n            self.mutable_list.append(mutable)\n            mutable.registered_module = MixedOp(mutable)\n\n    def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        \"\"\"\n        Callback of layer choice forward. This function defines the forward\n        logic of the input mutable. So mutable is only interface, its real\n        implementation is defined in mutator.\n\n        Parameters\n        ----------\n        mutable: LayerChoice\n            forward logic of this input mutable\n        args: list of torch.Tensor\n            inputs of this mutable\n        kwargs: dict\n            inputs of this mutable\n\n        Returns\n        -------\n        torch.Tensor\n            output of this mutable, i.e., LayerChoice\n        int\n            index of the chosen op\n        \"\"\"\n        # FIXME: return mask, to be consistent with other algorithms\n        idx = mutable.registered_module.active_op_index\n        return mutable.registered_module(mutable, *args, **kwargs), idx\n\n    def reset_binary_gates(self):\n        \"\"\"\n        For each LayerChoice, binarize binary weights\n        based on alpha to only activate one op.\n        It traverses all the mutables in the model to do this.\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.binarize(mutable)\n\n    def set_chosen_op_active(self):\n        \"\"\"\n        For each LayerChoice, set the op with highest alpha as the chosen op.\n        Usually used for validation.\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.set_chosen_op_active()\n\n    def num_arch_params(self):\n        \"\"\"\n        The number of mutables, i.e., LayerChoice\n\n        Returns\n        -------\n        int\n            the number of LayerChoice in user model\n        \"\"\"\n        return len(self.mutable_list)\n\n    def set_arch_param_grad(self):\n        \"\"\"\n        For each LayerChoice, calculate gradients for architecture weights, i.e., alpha\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.set_arch_param_grad(mutable)\n\n    def get_architecture_parameters(self):\n        \"\"\"\n        Get all the architecture parameters.\n\n        yield\n        -----\n        PyTorch Parameter\n            Return ap_path_alpha of the traversed mutable\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            yield mutable.registered_module.get_ap_path_alpha()\n\n    def change_forward_mode(self, mode):\n        \"\"\"\n        Update forward mode of MixedOps, as training architecture weights and\n        model weights use different forward modes.\n        \"\"\"\n        MixedOp.forward_mode = mode\n\n    def get_forward_mode(self):\n        \"\"\"\n        Get forward mode of MixedOp\n\n        Returns\n        -------\n        string\n            the current forward mode of MixedOp\n        \"\"\"\n        return MixedOp.forward_mode\n\n    def rescale_updated_arch_param(self):\n        \"\"\"\n        Rescale architecture weights in 'two' mode.\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.rescale_updated_arch_param()\n\n    def unused_modules_off(self):\n        \"\"\"\n        Remove unused modules for each mutables.\n        The removed modules are kept in ```self._unused_modules``` for resume later.\n        \"\"\"\n        self._unused_modules = []\n        for mutable in self.undedup_mutables:\n            mixed_op = mutable.registered_module\n            unused = {}\n            if self.get_forward_mode() in ['full', 'two', 'full_v2']:\n                involved_index = mixed_op.active_index + mixed_op.inactive_index\n            else:\n                involved_index = mixed_op.active_index\n            for i in range(mixed_op.n_choices):\n                if i not in involved_index:\n                    unused[i] = mutable[i]\n                    mutable[i] = None\n            self._unused_modules.append(unused)\n\n    def unused_modules_back(self):\n        \"\"\"\n        Resume the removed modules back.\n        \"\"\"\n        if self._unused_modules is None:\n            return\n        for m, unused in zip(self.mutable_list, self._unused_modules):\n            for i in unused:\n                m[i] = unused[i]\n        self._unused_modules = None\n\n    def arch_requires_grad(self):\n        \"\"\"\n        Make architecture weights require gradient\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.to_requires_grad()\n\n    def arch_disable_grad(self):\n        \"\"\"\n        Disable gradient of architecture weights, i.e., does not\n        calcuate gradient for them.\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.to_disable_grad()\n\n    def sample_final(self):\n        \"\"\"\n        Generate the final chosen architecture.\n\n        Returns\n        -------\n        dict\n            the choice of each mutable, i.e., LayerChoice\n        \"\"\"\n        result = dict()\n        for mutable in self.undedup_mutables:\n            assert isinstance(mutable, LayerChoice)\n            index, _ = mutable.registered_module.chosen_index\n            # pylint: disable=not-callable\n            result[mutable.key] = F.one_hot(torch.tensor(index), num_classes=len(mutable)).view(-1).bool()\n        return result",
  "def forward(ctx, x, binary_gates, run_func, backward_func):\n        ctx.run_func = run_func\n        ctx.backward_func = backward_func\n\n        detached_x = detach_variable(x)\n        with torch.enable_grad():\n            output = run_func(detached_x)\n        ctx.save_for_backward(detached_x, output)\n        return output.data",
  "def backward(ctx, grad_output):\n        detached_x, output = ctx.saved_tensors\n\n        grad_x = torch.autograd.grad(output, detached_x, grad_output, only_inputs=True)\n        # compute gradients w.r.t. binary_gates\n        binary_grads = ctx.backward_func(detached_x.data, output.data, grad_output.data)\n\n        return grad_x[0], binary_grads, None, None",
  "def __init__(self, mutable):\n        \"\"\"\n        Parameters\n        ----------\n        mutable : LayerChoice\n            A LayerChoice in user model\n        \"\"\"\n        super(MixedOp, self).__init__()\n        self.ap_path_alpha = nn.Parameter(torch.Tensor(len(mutable)))\n        self.ap_path_wb = nn.Parameter(torch.Tensor(len(mutable)))\n        self.ap_path_alpha.requires_grad = False\n        self.ap_path_wb.requires_grad = False\n        self.active_index = [0]\n        self.inactive_index = None\n        self.log_prob = None\n        self.current_prob_over_ops = None\n        self.n_choices = len(mutable)",
  "def get_ap_path_alpha(self):\n        return self.ap_path_alpha",
  "def to_requires_grad(self):\n        self.ap_path_alpha.requires_grad = True\n        self.ap_path_wb.requires_grad = True",
  "def to_disable_grad(self):\n        self.ap_path_alpha.requires_grad = False\n        self.ap_path_wb.requires_grad = False",
  "def forward(self, mutable, x):\n        \"\"\"\n        Define forward of LayerChoice. For 'full_v2', backward is also defined.\n        The 'two' mode is explained in section 3.2.1 in the paper.\n        The 'full_v2' mode is explained in Appendix D in the paper.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            this layer's mutable\n        x : tensor\n            inputs of this layer, only support one input\n\n        Returns\n        -------\n        output: tensor\n            output of this layer\n        \"\"\"\n        if MixedOp.forward_mode == 'full' or MixedOp.forward_mode == 'two':\n            output = 0\n            for _i in self.active_index:\n                oi = self.candidate_ops[_i](x)\n                output = output + self.ap_path_wb[_i] * oi\n            for _i in self.inactive_index:\n                oi = self.candidate_ops[_i](x)\n                output = output + self.ap_path_wb[_i] * oi.detach()\n        elif MixedOp.forward_mode == 'full_v2':\n            def run_function(key, candidate_ops, active_id):\n                def forward(_x):\n                    return candidate_ops[active_id](_x)\n                return forward\n\n            def backward_function(key, candidate_ops, active_id, binary_gates):\n                def backward(_x, _output, grad_output):\n                    binary_grads = torch.zeros_like(binary_gates.data)\n                    with torch.no_grad():\n                        for k in range(len(candidate_ops)):\n                            if k != active_id:\n                                out_k = candidate_ops[k](_x.data)\n                            else:\n                                out_k = _output.data\n                            grad_k = torch.sum(out_k * grad_output)\n                            binary_grads[k] = grad_k\n                    return binary_grads\n                return backward\n            output = ArchGradientFunction.apply(\n                x, self.ap_path_wb, run_function(mutable.key, list(mutable), self.active_index[0]),\n                backward_function(mutable.key, list(mutable), self.active_index[0], self.ap_path_wb))\n        else:\n            output = self.active_op(mutable)(x)\n        return output",
  "def probs_over_ops(self):\n        \"\"\"\n        Apply softmax on alpha to generate probability distribution\n\n        Returns\n        -------\n        pytorch tensor\n            probability distribution\n        \"\"\"\n        probs = F.softmax(self.ap_path_alpha, dim=0)  # softmax to probability\n        return probs",
  "def chosen_index(self):\n        \"\"\"\n        choose the op with max prob\n\n        Returns\n        -------\n        int\n            index of the chosen one\n        numpy.float32\n            prob of the chosen one\n        \"\"\"\n        probs = self.probs_over_ops.data.cpu().numpy()\n        index = int(np.argmax(probs))\n        return index, probs[index]",
  "def active_op(self, mutable):\n        \"\"\"\n        assume only one path is active\n\n        Returns\n        -------\n        PyTorch module\n            the chosen operation\n        \"\"\"\n        return mutable[self.active_index[0]]",
  "def active_op_index(self):\n        \"\"\"\n        return active op's index, the active op is sampled\n\n        Returns\n        -------\n        int\n            index of the active op\n        \"\"\"\n        return self.active_index[0]",
  "def set_chosen_op_active(self):\n        \"\"\"\n        set chosen index, active and inactive indexes\n        \"\"\"\n        chosen_idx, _ = self.chosen_index\n        self.active_index = [chosen_idx]\n        self.inactive_index = [_i for _i in range(0, chosen_idx)] + \\\n                              [_i for _i in range(chosen_idx + 1, self.n_choices)]",
  "def binarize(self, mutable):\n        \"\"\"\n        Sample based on alpha, and set binary weights accordingly.\n        ap_path_wb is set in this function, which is called binarize.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            this layer's mutable\n        \"\"\"\n        self.log_prob = None\n        # reset binary gates\n        self.ap_path_wb.data.zero_()\n        probs = self.probs_over_ops\n        if MixedOp.forward_mode == 'two':\n            # sample two ops according to probs\n            sample_op = torch.multinomial(probs.data, 2, replacement=False)\n            probs_slice = F.softmax(torch.stack([\n                self.ap_path_alpha[idx] for idx in sample_op\n            ]), dim=0)\n            self.current_prob_over_ops = torch.zeros_like(probs)\n            for i, idx in enumerate(sample_op):\n                self.current_prob_over_ops[idx] = probs_slice[i]\n            # choose one to be active and the other to be inactive according to probs_slice\n            c = torch.multinomial(probs_slice.data, 1)[0] # 0 or 1\n            active_op = sample_op[c].item()\n            inactive_op = sample_op[1-c].item()\n            self.active_index = [active_op]\n            self.inactive_index = [inactive_op]\n            # set binary gate\n            self.ap_path_wb.data[active_op] = 1.0\n        else:\n            sample = torch.multinomial(probs, 1)[0].item()\n            self.active_index = [sample]\n            self.inactive_index = [_i for _i in range(0, sample)] + \\\n                                [_i for _i in range(sample + 1, len(mutable))]\n            self.log_prob = torch.log(probs[sample])\n            self.current_prob_over_ops = probs\n            self.ap_path_wb.data[sample] = 1.0\n        # avoid over-regularization\n        for choice in mutable:\n            for _, param in choice.named_parameters():\n                param.grad = None",
  "def delta_ij(i, j):\n        if i == j:\n            return 1\n        else:\n            return 0",
  "def set_arch_param_grad(self, mutable):\n        \"\"\"\n        Calculate alpha gradient for this LayerChoice.\n        It is calculated using gradient of binary gate, probs of ops.\n        \"\"\"\n        binary_grads = self.ap_path_wb.grad.data\n        if self.active_op(mutable).is_zero_layer():\n            self.ap_path_alpha.grad = None\n            return\n        if self.ap_path_alpha.grad is None:\n            self.ap_path_alpha.grad = torch.zeros_like(self.ap_path_alpha.data)\n        if MixedOp.forward_mode == 'two':\n            involved_idx = self.active_index + self.inactive_index\n            probs_slice = F.softmax(torch.stack([\n                self.ap_path_alpha[idx] for idx in involved_idx\n            ]), dim=0).data\n            for i in range(2):\n                for j in range(2):\n                    origin_i = involved_idx[i]\n                    origin_j = involved_idx[j]\n                    self.ap_path_alpha.grad.data[origin_i] += \\\n                        binary_grads[origin_j] * probs_slice[j] * (MixedOp.delta_ij(i, j) - probs_slice[i])\n            for _i, idx in enumerate(self.active_index):\n                self.active_index[_i] = (idx, self.ap_path_alpha.data[idx].item())\n            for _i, idx in enumerate(self.inactive_index):\n                self.inactive_index[_i] = (idx, self.ap_path_alpha.data[idx].item())\n        else:\n            probs = self.probs_over_ops.data\n            for i in range(self.n_choices):\n                for j in range(self.n_choices):\n                    self.ap_path_alpha.grad.data[i] += binary_grads[j] * probs[j] * (MixedOp.delta_ij(i, j) - probs[i])\n        return",
  "def rescale_updated_arch_param(self):\n        \"\"\"\n        rescale architecture weights for the 'two' mode.\n        \"\"\"\n        if not isinstance(self.active_index[0], tuple):\n            assert self.active_op.is_zero_layer()\n            return\n        involved_idx = [idx for idx, _ in (self.active_index + self.inactive_index)]\n        old_alphas = [alpha for _, alpha in (self.active_index + self.inactive_index)]\n        new_alphas = [self.ap_path_alpha.data[idx] for idx in involved_idx]\n\n        offset = math.log(\n            sum([math.exp(alpha) for alpha in new_alphas]) / sum([math.exp(alpha) for alpha in old_alphas])\n        )\n\n        for idx in involved_idx:\n            self.ap_path_alpha.data[idx] -= offset",
  "def __init__(self, model):\n        \"\"\"\n        Init a MixedOp instance for each mutable i.e., LayerChoice.\n        And register the instantiated MixedOp in corresponding LayerChoice.\n        If does not register it in LayerChoice, DataParallel does not work then,\n        because architecture weights are not included in the DataParallel model.\n        When MixedOPs are registered, we use ```requires_grad``` to control\n        whether calculate gradients of architecture weights.\n\n        Parameters\n        ----------\n        model : pytorch model\n            The model that users want to tune, it includes search space defined with nni nas apis\n        \"\"\"\n        super(ProxylessNasMutator, self).__init__(model)\n        self._unused_modules = None\n        self.mutable_list = []\n        for mutable in self.undedup_mutables:\n            self.mutable_list.append(mutable)\n            mutable.registered_module = MixedOp(mutable)",
  "def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        \"\"\"\n        Callback of layer choice forward. This function defines the forward\n        logic of the input mutable. So mutable is only interface, its real\n        implementation is defined in mutator.\n\n        Parameters\n        ----------\n        mutable: LayerChoice\n            forward logic of this input mutable\n        args: list of torch.Tensor\n            inputs of this mutable\n        kwargs: dict\n            inputs of this mutable\n\n        Returns\n        -------\n        torch.Tensor\n            output of this mutable, i.e., LayerChoice\n        int\n            index of the chosen op\n        \"\"\"\n        # FIXME: return mask, to be consistent with other algorithms\n        idx = mutable.registered_module.active_op_index\n        return mutable.registered_module(mutable, *args, **kwargs), idx",
  "def reset_binary_gates(self):\n        \"\"\"\n        For each LayerChoice, binarize binary weights\n        based on alpha to only activate one op.\n        It traverses all the mutables in the model to do this.\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.binarize(mutable)",
  "def set_chosen_op_active(self):\n        \"\"\"\n        For each LayerChoice, set the op with highest alpha as the chosen op.\n        Usually used for validation.\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.set_chosen_op_active()",
  "def num_arch_params(self):\n        \"\"\"\n        The number of mutables, i.e., LayerChoice\n\n        Returns\n        -------\n        int\n            the number of LayerChoice in user model\n        \"\"\"\n        return len(self.mutable_list)",
  "def set_arch_param_grad(self):\n        \"\"\"\n        For each LayerChoice, calculate gradients for architecture weights, i.e., alpha\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.set_arch_param_grad(mutable)",
  "def get_architecture_parameters(self):\n        \"\"\"\n        Get all the architecture parameters.\n\n        yield\n        -----\n        PyTorch Parameter\n            Return ap_path_alpha of the traversed mutable\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            yield mutable.registered_module.get_ap_path_alpha()",
  "def change_forward_mode(self, mode):\n        \"\"\"\n        Update forward mode of MixedOps, as training architecture weights and\n        model weights use different forward modes.\n        \"\"\"\n        MixedOp.forward_mode = mode",
  "def get_forward_mode(self):\n        \"\"\"\n        Get forward mode of MixedOp\n\n        Returns\n        -------\n        string\n            the current forward mode of MixedOp\n        \"\"\"\n        return MixedOp.forward_mode",
  "def rescale_updated_arch_param(self):\n        \"\"\"\n        Rescale architecture weights in 'two' mode.\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.rescale_updated_arch_param()",
  "def unused_modules_off(self):\n        \"\"\"\n        Remove unused modules for each mutables.\n        The removed modules are kept in ```self._unused_modules``` for resume later.\n        \"\"\"\n        self._unused_modules = []\n        for mutable in self.undedup_mutables:\n            mixed_op = mutable.registered_module\n            unused = {}\n            if self.get_forward_mode() in ['full', 'two', 'full_v2']:\n                involved_index = mixed_op.active_index + mixed_op.inactive_index\n            else:\n                involved_index = mixed_op.active_index\n            for i in range(mixed_op.n_choices):\n                if i not in involved_index:\n                    unused[i] = mutable[i]\n                    mutable[i] = None\n            self._unused_modules.append(unused)",
  "def unused_modules_back(self):\n        \"\"\"\n        Resume the removed modules back.\n        \"\"\"\n        if self._unused_modules is None:\n            return\n        for m, unused in zip(self.mutable_list, self._unused_modules):\n            for i in unused:\n                m[i] = unused[i]\n        self._unused_modules = None",
  "def arch_requires_grad(self):\n        \"\"\"\n        Make architecture weights require gradient\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.to_requires_grad()",
  "def arch_disable_grad(self):\n        \"\"\"\n        Disable gradient of architecture weights, i.e., does not\n        calcuate gradient for them.\n        \"\"\"\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.to_disable_grad()",
  "def sample_final(self):\n        \"\"\"\n        Generate the final chosen architecture.\n\n        Returns\n        -------\n        dict\n            the choice of each mutable, i.e., LayerChoice\n        \"\"\"\n        result = dict()\n        for mutable in self.undedup_mutables:\n            assert isinstance(mutable, LayerChoice)\n            index, _ = mutable.registered_module.chosen_index\n            # pylint: disable=not-callable\n            result[mutable.key] = F.one_hot(torch.tensor(index), num_classes=len(mutable)).view(-1).bool()\n        return result",
  "def run_function(key, candidate_ops, active_id):\n                def forward(_x):\n                    return candidate_ops[active_id](_x)\n                return forward",
  "def backward_function(key, candidate_ops, active_id, binary_gates):\n                def backward(_x, _output, grad_output):\n                    binary_grads = torch.zeros_like(binary_gates.data)\n                    with torch.no_grad():\n                        for k in range(len(candidate_ops)):\n                            if k != active_id:\n                                out_k = candidate_ops[k](_x.data)\n                            else:\n                                out_k = _output.data\n                            grad_k = torch.sum(out_k * grad_output)\n                            binary_grads[k] = grad_k\n                    return binary_grads\n                return backward",
  "def forward(_x):\n                    return candidate_ops[active_id](_x)",
  "def backward(_x, _output, grad_output):\n                    binary_grads = torch.zeros_like(binary_gates.data)\n                    with torch.no_grad():\n                        for k in range(len(candidate_ops)):\n                            if k != active_id:\n                                out_k = candidate_ops[k](_x.data)\n                            else:\n                                out_k = _output.data\n                            grad_k = torch.sum(out_k * grad_output)\n                            binary_grads[k] = grad_k\n                    return binary_grads",
  "class ProxylessNasTrainer(BaseTrainer):\n    def __init__(self, model, model_optim, device,\n                 train_loader, valid_loader, label_smoothing=0.1,\n                 n_epochs=120, init_lr=0.025, binary_mode='full_v2',\n                 arch_init_type='normal', arch_init_ratio=1e-3,\n                 arch_optim_lr=1e-3, arch_weight_decay=0,\n                 grad_update_arch_param_every=5, grad_update_steps=1,\n                 warmup=True, warmup_epochs=25,\n                 arch_valid_frequency=1,\n                 load_ckpt=False, ckpt_path=None, arch_path=None):\n        \"\"\"\n        Parameters\n        ----------\n        model : pytorch model\n            the user model, which has mutables\n        model_optim : pytorch optimizer\n            the user defined optimizer\n        device : pytorch device\n            the devices to train/search the model\n        train_loader : pytorch data loader\n            data loader for the training set\n        valid_loader : pytorch data loader\n            data loader for the validation set\n        label_smoothing : float\n            for label smoothing\n        n_epochs : int\n            number of epochs to train/search\n        init_lr : float\n            init learning rate for training the model\n        binary_mode : str\n            the forward/backward mode for the binary weights in mutator\n        arch_init_type : str\n            the way to init architecture parameters\n        arch_init_ratio : float\n            the ratio to init architecture parameters\n        arch_optim_lr : float\n            learning rate of the architecture parameters optimizer\n        arch_weight_decay : float\n            weight decay of the architecture parameters optimizer\n        grad_update_arch_param_every : int\n            update architecture weights every this number of minibatches\n        grad_update_steps : int\n            during each update of architecture weights, the number of steps to train\n        warmup : bool\n            whether to do warmup\n        warmup_epochs : int\n            the number of epochs to do during warmup\n        arch_valid_frequency : int\n            frequency of printing validation result\n        load_ckpt : bool\n            whether load checkpoint\n        ckpt_path : str\n            checkpoint path, if load_ckpt is True, ckpt_path cannot be None\n        arch_path : str\n            the path to store chosen architecture\n        \"\"\"\n        self.model = model\n        self.model_optim = model_optim\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n        self.device = device\n        self.n_epochs = n_epochs\n        self.init_lr = init_lr\n        self.warmup = warmup\n        self.warmup_epochs = warmup_epochs\n        self.arch_valid_frequency = arch_valid_frequency\n        self.label_smoothing = label_smoothing\n\n        self.train_batch_size = train_loader.batch_sampler.batch_size\n        self.valid_batch_size = valid_loader.batch_sampler.batch_size\n        # update architecture parameters every this number of minibatches\n        self.grad_update_arch_param_every = grad_update_arch_param_every\n        # the number of steps per architecture parameter update\n        self.grad_update_steps = grad_update_steps\n        self.binary_mode = binary_mode\n\n        self.load_ckpt = load_ckpt\n        self.ckpt_path = ckpt_path\n        self.arch_path = arch_path\n\n        # init mutator\n        self.mutator = ProxylessNasMutator(model)\n\n        # DataParallel should be put behind the init of mutator\n        self.model = torch.nn.DataParallel(self.model)\n        self.model.to(self.device)\n\n        # iter of valid dataset for training architecture weights\n        self._valid_iter = None\n        # init architecture weights\n        self._init_arch_params(arch_init_type, arch_init_ratio)\n        # build architecture optimizer\n        self.arch_optimizer = torch.optim.Adam(self.mutator.get_architecture_parameters(),\n                                               arch_optim_lr,\n                                               weight_decay=arch_weight_decay,\n                                               betas=(0, 0.999),\n                                               eps=1e-8)\n\n        self.criterion = nn.CrossEntropyLoss()\n        self.warmup_curr_epoch = 0\n        self.train_curr_epoch = 0\n\n    def _init_arch_params(self, init_type='normal', init_ratio=1e-3):\n        \"\"\"\n        Initialize architecture weights\n        \"\"\"\n        for param in self.mutator.get_architecture_parameters():\n            if init_type == 'normal':\n                param.data.normal_(0, init_ratio)\n            elif init_type == 'uniform':\n                param.data.uniform_(-init_ratio, init_ratio)\n            else:\n                raise NotImplementedError\n\n    def _validate(self):\n        \"\"\"\n        Do validation. During validation, LayerChoices use the chosen active op.\n\n        Returns\n        -------\n        float, float, float\n            average loss, average top1 accuracy, average top5 accuracy\n        \"\"\"\n        self.valid_loader.batch_sampler.batch_size = self.valid_batch_size\n        self.valid_loader.batch_sampler.drop_last = False\n\n        self.mutator.set_chosen_op_active()\n        # remove unused modules to save memory\n        self.mutator.unused_modules_off()\n        # test on validation set under train mode\n        self.model.train()\n        batch_time = AverageMeter('batch_time')\n        losses = AverageMeter('losses')\n        top1 = AverageMeter('top1')\n        top5 = AverageMeter('top5')\n        end = time.time()\n        with torch.no_grad():\n            for i, (images, labels) in enumerate(self.valid_loader):\n                images, labels = images.to(self.device), labels.to(self.device)\n                output = self.model(images)\n                loss = self.criterion(output, labels)\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % 10 == 0 or i + 1 == len(self.valid_loader):\n                    test_log = 'Valid' + ': [{0}/{1}]\\t'\\\n                                        'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\\\n                                        'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\\\n                                        'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})'.\\\n                        format(i, len(self.valid_loader) - 1, batch_time=batch_time, loss=losses, top1=top1)\n                    # return top5:\n                    test_log += '\\tTop-5 acc {top5.val:.3f} ({top5.avg:.3f})'.format(top5=top5)\n                    logger.info(test_log)\n        self.mutator.unused_modules_back()\n        return losses.avg, top1.avg, top5.avg\n\n    def _warm_up(self):\n        \"\"\"\n        Warm up the model, during warm up, architecture weights are not trained.\n        \"\"\"\n        lr_max = 0.05\n        data_loader = self.train_loader\n        nBatch = len(data_loader)\n        T_total = self.warmup_epochs * nBatch # total num of batches\n\n        for epoch in range(self.warmup_curr_epoch, self.warmup_epochs):\n            logger.info('\\n--------Warmup epoch: %d--------\\n', epoch + 1)\n            batch_time = AverageMeter('batch_time')\n            data_time = AverageMeter('data_time')\n            losses = AverageMeter('losses')\n            top1 = AverageMeter('top1')\n            top5 = AverageMeter('top5')\n            # switch to train mode\n            self.model.train()\n\n            end = time.time()\n            logger.info('warm_up epoch: %d', epoch)\n            for i, (images, labels) in enumerate(data_loader):\n                data_time.update(time.time() - end)\n                # lr\n                T_cur = epoch * nBatch + i\n                warmup_lr = 0.5 * lr_max * (1 + math.cos(math.pi * T_cur / T_total))\n                for param_group in self.model_optim.param_groups:\n                    param_group['lr'] = warmup_lr\n                images, labels = images.to(self.device), labels.to(self.device)\n                # compute output\n                self.mutator.reset_binary_gates() # random sample binary gates\n                self.mutator.unused_modules_off() # remove unused module for speedup\n                output = self.model(images)\n                if self.label_smoothing > 0:\n                    loss = cross_entropy_with_label_smoothing(output, labels, self.label_smoothing)\n                else:\n                    loss = self.criterion(output, labels)\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                # compute gradient and do SGD step\n                self.model.zero_grad()\n                loss.backward()\n                self.model_optim.step()\n                # unused modules back\n                self.mutator.unused_modules_back()\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % 10 == 0 or i + 1 == nBatch:\n                    batch_log = 'Warmup Train [{0}][{1}/{2}]\\t' \\\n                                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n                                'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t' \\\n                                'Loss {losses.val:.4f} ({losses.avg:.4f})\\t' \\\n                                'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n                                'Top-5 acc {top5.val:.3f} ({top5.avg:.3f})\\tlr {lr:.5f}'. \\\n                        format(epoch + 1, i, nBatch - 1, batch_time=batch_time, data_time=data_time,\n                               losses=losses, top1=top1, top5=top5, lr=warmup_lr)\n                    logger.info(batch_log)\n            val_loss, val_top1, val_top5 = self._validate()\n            val_log = 'Warmup Valid [{0}/{1}]\\tloss {2:.3f}\\ttop-1 acc {3:.3f}\\ttop-5 acc {4:.3f}\\t' \\\n                      'Train top-1 {top1.avg:.3f}\\ttop-5 {top5.avg:.3f}M'. \\\n                format(epoch + 1, self.warmup_epochs, val_loss, val_top1, val_top5, top1=top1, top5=top5)\n            logger.info(val_log)\n            self.save_checkpoint()\n            self.warmup_curr_epoch += 1\n\n    def _get_update_schedule(self, nBatch):\n        \"\"\"\n        Generate schedule for training architecture weights. Key means after which minibatch\n        to update architecture weights, value means how many steps for the update.\n\n        Parameters\n        ----------\n        nBatch : int\n            the total number of minibatches in one epoch\n\n        Returns\n        -------\n        dict\n            the schedule for updating architecture weights\n        \"\"\"\n        schedule = {}\n        for i in range(nBatch):\n            if (i + 1) % self.grad_update_arch_param_every == 0:\n                schedule[i] = self.grad_update_steps\n        return schedule\n\n    def _calc_learning_rate(self, epoch, batch=0, nBatch=None):\n        \"\"\"\n        Update learning rate.\n        \"\"\"\n        T_total = self.n_epochs * nBatch\n        T_cur = epoch * nBatch + batch\n        lr = 0.5 * self.init_lr * (1 + math.cos(math.pi * T_cur / T_total))\n        return lr\n\n    def _adjust_learning_rate(self, optimizer, epoch, batch=0, nBatch=None):\n        \"\"\"\n        Adjust learning of a given optimizer and return the new learning rate\n\n        Parameters\n        ----------\n        optimizer : pytorch optimizer\n            the used optimizer\n        epoch : int\n            the current epoch number\n        batch : int\n            the current minibatch\n        nBatch : int\n            the total number of minibatches in one epoch\n\n        Returns\n        -------\n        float\n            the adjusted learning rate\n        \"\"\"\n        new_lr = self._calc_learning_rate(epoch, batch, nBatch)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = new_lr\n        return new_lr\n\n    def _train(self):\n        \"\"\"\n        Train the model, it trains model weights and architecute weights.\n        Architecture weights are trained according to the schedule.\n        Before updating architecture weights, ```requires_grad``` is enabled.\n        Then, it is disabled after the updating, in order not to update\n        architecture weights when training model weights.\n        \"\"\"\n        nBatch = len(self.train_loader)\n        arch_param_num = self.mutator.num_arch_params()\n        binary_gates_num = self.mutator.num_arch_params()\n        logger.info('#arch_params: %d\\t#binary_gates: %d', arch_param_num, binary_gates_num)\n\n        update_schedule = self._get_update_schedule(nBatch)\n\n        for epoch in range(self.train_curr_epoch, self.n_epochs):\n            logger.info('\\n--------Train epoch: %d--------\\n', epoch + 1)\n            batch_time = AverageMeter('batch_time')\n            data_time = AverageMeter('data_time')\n            losses = AverageMeter('losses')\n            top1 = AverageMeter('top1')\n            top5 = AverageMeter('top5')\n            # switch to train mode\n            self.model.train()\n\n            end = time.time()\n            for i, (images, labels) in enumerate(self.train_loader):\n                data_time.update(time.time() - end)\n                lr = self._adjust_learning_rate(self.model_optim, epoch, batch=i, nBatch=nBatch)\n                # train weight parameters\n                images, labels = images.to(self.device), labels.to(self.device)\n                self.mutator.reset_binary_gates()\n                self.mutator.unused_modules_off()\n                output = self.model(images)\n                if self.label_smoothing > 0:\n                    loss = cross_entropy_with_label_smoothing(output, labels, self.label_smoothing)\n                else:\n                    loss = self.criterion(output, labels)\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                self.model.zero_grad()\n                loss.backward()\n                self.model_optim.step()\n                self.mutator.unused_modules_back()\n                if epoch > 0:\n                    for _ in range(update_schedule.get(i, 0)):\n                        start_time = time.time()\n                        # GradientArchSearchConfig\n                        self.mutator.arch_requires_grad()\n                        arch_loss, exp_value = self._gradient_step()\n                        self.mutator.arch_disable_grad()\n                        used_time = time.time() - start_time\n                        log_str = 'Architecture [%d-%d]\\t Time %.4f\\t Loss %.4f\\t null %s' % \\\n                                    (epoch + 1, i, used_time, arch_loss, exp_value)\n                        logger.info(log_str)\n                batch_time.update(time.time() - end)\n                end = time.time()\n                # training log\n                if i % 10 == 0 or i + 1 == nBatch:\n                    batch_log = 'Train [{0}][{1}/{2}]\\t' \\\n                                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n                                'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t' \\\n                                'Loss {losses.val:.4f} ({losses.avg:.4f})\\t' \\\n                                'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n                                'Top-5 acc {top5.val:.3f} ({top5.avg:.3f})\\tlr {lr:.5f}'. \\\n                        format(epoch + 1, i, nBatch - 1, batch_time=batch_time, data_time=data_time,\n                               losses=losses, top1=top1, top5=top5, lr=lr)\n                    logger.info(batch_log)\n            # validate\n            if (epoch + 1) % self.arch_valid_frequency == 0:\n                val_loss, val_top1, val_top5 = self._validate()\n                val_log = 'Valid [{0}]\\tloss {1:.3f}\\ttop-1 acc {2:.3f} \\ttop-5 acc {3:.3f}\\t' \\\n                          'Train top-1 {top1.avg:.3f}\\ttop-5 {top5.avg:.3f}'. \\\n                    format(epoch + 1, val_loss, val_top1, val_top5, top1=top1, top5=top5)\n                logger.info(val_log)\n            self.save_checkpoint()\n            self.train_curr_epoch += 1\n\n    def _valid_next_batch(self):\n        \"\"\"\n        Get next one minibatch from validation set\n\n        Returns\n        -------\n        (tensor, tensor)\n            the tuple of images and labels\n        \"\"\"\n        if self._valid_iter is None:\n            self._valid_iter = iter(self.valid_loader)\n        try:\n            data = next(self._valid_iter)\n        except StopIteration:\n            self._valid_iter = iter(self.valid_loader)\n            data = next(self._valid_iter)\n        return data\n\n    def _gradient_step(self):\n        \"\"\"\n        This gradient step is for updating architecture weights.\n        Mutator is intensively used in this function to operate on\n        architecture weights.\n\n        Returns\n        -------\n        float, None\n            loss of the model, None\n        \"\"\"\n        # use the same batch size as train batch size for architecture weights\n        self.valid_loader.batch_sampler.batch_size = self.train_batch_size\n        self.valid_loader.batch_sampler.drop_last = True\n        self.model.train()\n        self.mutator.change_forward_mode(self.binary_mode)\n        time1 = time.time()  # time\n        # sample a batch of data from validation set\n        images, labels = self._valid_next_batch()\n        images, labels = images.to(self.device), labels.to(self.device)\n        time2 = time.time()  # time\n        self.mutator.reset_binary_gates()\n        self.mutator.unused_modules_off()\n        output = self.model(images)\n        time3 = time.time()\n        ce_loss = self.criterion(output, labels)\n        expected_value = None\n        loss = ce_loss\n        self.model.zero_grad()\n        loss.backward()\n        self.mutator.set_arch_param_grad()\n        self.arch_optimizer.step()\n        if self.mutator.get_forward_mode() == 'two':\n            self.mutator.rescale_updated_arch_param()\n        self.mutator.unused_modules_back()\n        self.mutator.change_forward_mode(None)\n        time4 = time.time()\n        logger.info('(%.4f, %.4f, %.4f)', time2 - time1, time3 - time2, time4 - time3)\n        return loss.data.item(), expected_value.item() if expected_value is not None else None\n\n    def save_checkpoint(self):\n        \"\"\"\n        Save checkpoint of the whole model. Saving model weights and architecture weights in\n        ```ckpt_path```, and saving currently chosen architecture in ```arch_path```.\n        \"\"\"\n        if self.ckpt_path:\n            state = {\n                'warmup_curr_epoch': self.warmup_curr_epoch,\n                'train_curr_epoch': self.train_curr_epoch,\n                'model': self.model.state_dict(),\n                'optim': self.model_optim.state_dict(),\n                'arch_optim': self.arch_optimizer.state_dict()\n            }\n            torch.save(state, self.ckpt_path)\n        if self.arch_path:\n            self.export(self.arch_path)\n\n    def load_checkpoint(self):\n        \"\"\"\n        Load the checkpoint from ```ckpt_path```.\n        \"\"\"\n        assert self.ckpt_path is not None, \"If load_ckpt is not None, ckpt_path should not be None\"\n        ckpt = torch.load(self.ckpt_path)\n        self.warmup_curr_epoch = ckpt['warmup_curr_epoch']\n        self.train_curr_epoch = ckpt['train_curr_epoch']\n        self.model.load_state_dict(ckpt['model'])\n        self.model_optim.load_state_dict(ckpt['optim'])\n        self.arch_optimizer.load_state_dict(ckpt['arch_optim'])\n\n    def train(self):\n        \"\"\"\n        Train the whole model.\n        \"\"\"\n        if self.load_ckpt:\n            self.load_checkpoint()\n        if self.warmup:\n            self._warm_up()\n        self._train()\n\n    def export(self, file_name):\n        \"\"\"\n        Export the chosen architecture into a file\n\n        Parameters\n        ----------\n        file_name : str\n            the file that stores exported chosen architecture\n        \"\"\"\n        exported_arch = self.mutator.sample_final()\n        with open(file_name, 'w') as f:\n            json.dump(exported_arch, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n\n    def validate(self):\n        raise NotImplementedError\n\n    def checkpoint(self):\n        raise NotImplementedError",
  "def __init__(self, model, model_optim, device,\n                 train_loader, valid_loader, label_smoothing=0.1,\n                 n_epochs=120, init_lr=0.025, binary_mode='full_v2',\n                 arch_init_type='normal', arch_init_ratio=1e-3,\n                 arch_optim_lr=1e-3, arch_weight_decay=0,\n                 grad_update_arch_param_every=5, grad_update_steps=1,\n                 warmup=True, warmup_epochs=25,\n                 arch_valid_frequency=1,\n                 load_ckpt=False, ckpt_path=None, arch_path=None):\n        \"\"\"\n        Parameters\n        ----------\n        model : pytorch model\n            the user model, which has mutables\n        model_optim : pytorch optimizer\n            the user defined optimizer\n        device : pytorch device\n            the devices to train/search the model\n        train_loader : pytorch data loader\n            data loader for the training set\n        valid_loader : pytorch data loader\n            data loader for the validation set\n        label_smoothing : float\n            for label smoothing\n        n_epochs : int\n            number of epochs to train/search\n        init_lr : float\n            init learning rate for training the model\n        binary_mode : str\n            the forward/backward mode for the binary weights in mutator\n        arch_init_type : str\n            the way to init architecture parameters\n        arch_init_ratio : float\n            the ratio to init architecture parameters\n        arch_optim_lr : float\n            learning rate of the architecture parameters optimizer\n        arch_weight_decay : float\n            weight decay of the architecture parameters optimizer\n        grad_update_arch_param_every : int\n            update architecture weights every this number of minibatches\n        grad_update_steps : int\n            during each update of architecture weights, the number of steps to train\n        warmup : bool\n            whether to do warmup\n        warmup_epochs : int\n            the number of epochs to do during warmup\n        arch_valid_frequency : int\n            frequency of printing validation result\n        load_ckpt : bool\n            whether load checkpoint\n        ckpt_path : str\n            checkpoint path, if load_ckpt is True, ckpt_path cannot be None\n        arch_path : str\n            the path to store chosen architecture\n        \"\"\"\n        self.model = model\n        self.model_optim = model_optim\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n        self.device = device\n        self.n_epochs = n_epochs\n        self.init_lr = init_lr\n        self.warmup = warmup\n        self.warmup_epochs = warmup_epochs\n        self.arch_valid_frequency = arch_valid_frequency\n        self.label_smoothing = label_smoothing\n\n        self.train_batch_size = train_loader.batch_sampler.batch_size\n        self.valid_batch_size = valid_loader.batch_sampler.batch_size\n        # update architecture parameters every this number of minibatches\n        self.grad_update_arch_param_every = grad_update_arch_param_every\n        # the number of steps per architecture parameter update\n        self.grad_update_steps = grad_update_steps\n        self.binary_mode = binary_mode\n\n        self.load_ckpt = load_ckpt\n        self.ckpt_path = ckpt_path\n        self.arch_path = arch_path\n\n        # init mutator\n        self.mutator = ProxylessNasMutator(model)\n\n        # DataParallel should be put behind the init of mutator\n        self.model = torch.nn.DataParallel(self.model)\n        self.model.to(self.device)\n\n        # iter of valid dataset for training architecture weights\n        self._valid_iter = None\n        # init architecture weights\n        self._init_arch_params(arch_init_type, arch_init_ratio)\n        # build architecture optimizer\n        self.arch_optimizer = torch.optim.Adam(self.mutator.get_architecture_parameters(),\n                                               arch_optim_lr,\n                                               weight_decay=arch_weight_decay,\n                                               betas=(0, 0.999),\n                                               eps=1e-8)\n\n        self.criterion = nn.CrossEntropyLoss()\n        self.warmup_curr_epoch = 0\n        self.train_curr_epoch = 0",
  "def _init_arch_params(self, init_type='normal', init_ratio=1e-3):\n        \"\"\"\n        Initialize architecture weights\n        \"\"\"\n        for param in self.mutator.get_architecture_parameters():\n            if init_type == 'normal':\n                param.data.normal_(0, init_ratio)\n            elif init_type == 'uniform':\n                param.data.uniform_(-init_ratio, init_ratio)\n            else:\n                raise NotImplementedError",
  "def _validate(self):\n        \"\"\"\n        Do validation. During validation, LayerChoices use the chosen active op.\n\n        Returns\n        -------\n        float, float, float\n            average loss, average top1 accuracy, average top5 accuracy\n        \"\"\"\n        self.valid_loader.batch_sampler.batch_size = self.valid_batch_size\n        self.valid_loader.batch_sampler.drop_last = False\n\n        self.mutator.set_chosen_op_active()\n        # remove unused modules to save memory\n        self.mutator.unused_modules_off()\n        # test on validation set under train mode\n        self.model.train()\n        batch_time = AverageMeter('batch_time')\n        losses = AverageMeter('losses')\n        top1 = AverageMeter('top1')\n        top5 = AverageMeter('top5')\n        end = time.time()\n        with torch.no_grad():\n            for i, (images, labels) in enumerate(self.valid_loader):\n                images, labels = images.to(self.device), labels.to(self.device)\n                output = self.model(images)\n                loss = self.criterion(output, labels)\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % 10 == 0 or i + 1 == len(self.valid_loader):\n                    test_log = 'Valid' + ': [{0}/{1}]\\t'\\\n                                        'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\\\n                                        'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\\\n                                        'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})'.\\\n                        format(i, len(self.valid_loader) - 1, batch_time=batch_time, loss=losses, top1=top1)\n                    # return top5:\n                    test_log += '\\tTop-5 acc {top5.val:.3f} ({top5.avg:.3f})'.format(top5=top5)\n                    logger.info(test_log)\n        self.mutator.unused_modules_back()\n        return losses.avg, top1.avg, top5.avg",
  "def _warm_up(self):\n        \"\"\"\n        Warm up the model, during warm up, architecture weights are not trained.\n        \"\"\"\n        lr_max = 0.05\n        data_loader = self.train_loader\n        nBatch = len(data_loader)\n        T_total = self.warmup_epochs * nBatch # total num of batches\n\n        for epoch in range(self.warmup_curr_epoch, self.warmup_epochs):\n            logger.info('\\n--------Warmup epoch: %d--------\\n', epoch + 1)\n            batch_time = AverageMeter('batch_time')\n            data_time = AverageMeter('data_time')\n            losses = AverageMeter('losses')\n            top1 = AverageMeter('top1')\n            top5 = AverageMeter('top5')\n            # switch to train mode\n            self.model.train()\n\n            end = time.time()\n            logger.info('warm_up epoch: %d', epoch)\n            for i, (images, labels) in enumerate(data_loader):\n                data_time.update(time.time() - end)\n                # lr\n                T_cur = epoch * nBatch + i\n                warmup_lr = 0.5 * lr_max * (1 + math.cos(math.pi * T_cur / T_total))\n                for param_group in self.model_optim.param_groups:\n                    param_group['lr'] = warmup_lr\n                images, labels = images.to(self.device), labels.to(self.device)\n                # compute output\n                self.mutator.reset_binary_gates() # random sample binary gates\n                self.mutator.unused_modules_off() # remove unused module for speedup\n                output = self.model(images)\n                if self.label_smoothing > 0:\n                    loss = cross_entropy_with_label_smoothing(output, labels, self.label_smoothing)\n                else:\n                    loss = self.criterion(output, labels)\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                # compute gradient and do SGD step\n                self.model.zero_grad()\n                loss.backward()\n                self.model_optim.step()\n                # unused modules back\n                self.mutator.unused_modules_back()\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % 10 == 0 or i + 1 == nBatch:\n                    batch_log = 'Warmup Train [{0}][{1}/{2}]\\t' \\\n                                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n                                'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t' \\\n                                'Loss {losses.val:.4f} ({losses.avg:.4f})\\t' \\\n                                'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n                                'Top-5 acc {top5.val:.3f} ({top5.avg:.3f})\\tlr {lr:.5f}'. \\\n                        format(epoch + 1, i, nBatch - 1, batch_time=batch_time, data_time=data_time,\n                               losses=losses, top1=top1, top5=top5, lr=warmup_lr)\n                    logger.info(batch_log)\n            val_loss, val_top1, val_top5 = self._validate()\n            val_log = 'Warmup Valid [{0}/{1}]\\tloss {2:.3f}\\ttop-1 acc {3:.3f}\\ttop-5 acc {4:.3f}\\t' \\\n                      'Train top-1 {top1.avg:.3f}\\ttop-5 {top5.avg:.3f}M'. \\\n                format(epoch + 1, self.warmup_epochs, val_loss, val_top1, val_top5, top1=top1, top5=top5)\n            logger.info(val_log)\n            self.save_checkpoint()\n            self.warmup_curr_epoch += 1",
  "def _get_update_schedule(self, nBatch):\n        \"\"\"\n        Generate schedule for training architecture weights. Key means after which minibatch\n        to update architecture weights, value means how many steps for the update.\n\n        Parameters\n        ----------\n        nBatch : int\n            the total number of minibatches in one epoch\n\n        Returns\n        -------\n        dict\n            the schedule for updating architecture weights\n        \"\"\"\n        schedule = {}\n        for i in range(nBatch):\n            if (i + 1) % self.grad_update_arch_param_every == 0:\n                schedule[i] = self.grad_update_steps\n        return schedule",
  "def _calc_learning_rate(self, epoch, batch=0, nBatch=None):\n        \"\"\"\n        Update learning rate.\n        \"\"\"\n        T_total = self.n_epochs * nBatch\n        T_cur = epoch * nBatch + batch\n        lr = 0.5 * self.init_lr * (1 + math.cos(math.pi * T_cur / T_total))\n        return lr",
  "def _adjust_learning_rate(self, optimizer, epoch, batch=0, nBatch=None):\n        \"\"\"\n        Adjust learning of a given optimizer and return the new learning rate\n\n        Parameters\n        ----------\n        optimizer : pytorch optimizer\n            the used optimizer\n        epoch : int\n            the current epoch number\n        batch : int\n            the current minibatch\n        nBatch : int\n            the total number of minibatches in one epoch\n\n        Returns\n        -------\n        float\n            the adjusted learning rate\n        \"\"\"\n        new_lr = self._calc_learning_rate(epoch, batch, nBatch)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = new_lr\n        return new_lr",
  "def _train(self):\n        \"\"\"\n        Train the model, it trains model weights and architecute weights.\n        Architecture weights are trained according to the schedule.\n        Before updating architecture weights, ```requires_grad``` is enabled.\n        Then, it is disabled after the updating, in order not to update\n        architecture weights when training model weights.\n        \"\"\"\n        nBatch = len(self.train_loader)\n        arch_param_num = self.mutator.num_arch_params()\n        binary_gates_num = self.mutator.num_arch_params()\n        logger.info('#arch_params: %d\\t#binary_gates: %d', arch_param_num, binary_gates_num)\n\n        update_schedule = self._get_update_schedule(nBatch)\n\n        for epoch in range(self.train_curr_epoch, self.n_epochs):\n            logger.info('\\n--------Train epoch: %d--------\\n', epoch + 1)\n            batch_time = AverageMeter('batch_time')\n            data_time = AverageMeter('data_time')\n            losses = AverageMeter('losses')\n            top1 = AverageMeter('top1')\n            top5 = AverageMeter('top5')\n            # switch to train mode\n            self.model.train()\n\n            end = time.time()\n            for i, (images, labels) in enumerate(self.train_loader):\n                data_time.update(time.time() - end)\n                lr = self._adjust_learning_rate(self.model_optim, epoch, batch=i, nBatch=nBatch)\n                # train weight parameters\n                images, labels = images.to(self.device), labels.to(self.device)\n                self.mutator.reset_binary_gates()\n                self.mutator.unused_modules_off()\n                output = self.model(images)\n                if self.label_smoothing > 0:\n                    loss = cross_entropy_with_label_smoothing(output, labels, self.label_smoothing)\n                else:\n                    loss = self.criterion(output, labels)\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                self.model.zero_grad()\n                loss.backward()\n                self.model_optim.step()\n                self.mutator.unused_modules_back()\n                if epoch > 0:\n                    for _ in range(update_schedule.get(i, 0)):\n                        start_time = time.time()\n                        # GradientArchSearchConfig\n                        self.mutator.arch_requires_grad()\n                        arch_loss, exp_value = self._gradient_step()\n                        self.mutator.arch_disable_grad()\n                        used_time = time.time() - start_time\n                        log_str = 'Architecture [%d-%d]\\t Time %.4f\\t Loss %.4f\\t null %s' % \\\n                                    (epoch + 1, i, used_time, arch_loss, exp_value)\n                        logger.info(log_str)\n                batch_time.update(time.time() - end)\n                end = time.time()\n                # training log\n                if i % 10 == 0 or i + 1 == nBatch:\n                    batch_log = 'Train [{0}][{1}/{2}]\\t' \\\n                                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n                                'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t' \\\n                                'Loss {losses.val:.4f} ({losses.avg:.4f})\\t' \\\n                                'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n                                'Top-5 acc {top5.val:.3f} ({top5.avg:.3f})\\tlr {lr:.5f}'. \\\n                        format(epoch + 1, i, nBatch - 1, batch_time=batch_time, data_time=data_time,\n                               losses=losses, top1=top1, top5=top5, lr=lr)\n                    logger.info(batch_log)\n            # validate\n            if (epoch + 1) % self.arch_valid_frequency == 0:\n                val_loss, val_top1, val_top5 = self._validate()\n                val_log = 'Valid [{0}]\\tloss {1:.3f}\\ttop-1 acc {2:.3f} \\ttop-5 acc {3:.3f}\\t' \\\n                          'Train top-1 {top1.avg:.3f}\\ttop-5 {top5.avg:.3f}'. \\\n                    format(epoch + 1, val_loss, val_top1, val_top5, top1=top1, top5=top5)\n                logger.info(val_log)\n            self.save_checkpoint()\n            self.train_curr_epoch += 1",
  "def _valid_next_batch(self):\n        \"\"\"\n        Get next one minibatch from validation set\n\n        Returns\n        -------\n        (tensor, tensor)\n            the tuple of images and labels\n        \"\"\"\n        if self._valid_iter is None:\n            self._valid_iter = iter(self.valid_loader)\n        try:\n            data = next(self._valid_iter)\n        except StopIteration:\n            self._valid_iter = iter(self.valid_loader)\n            data = next(self._valid_iter)\n        return data",
  "def _gradient_step(self):\n        \"\"\"\n        This gradient step is for updating architecture weights.\n        Mutator is intensively used in this function to operate on\n        architecture weights.\n\n        Returns\n        -------\n        float, None\n            loss of the model, None\n        \"\"\"\n        # use the same batch size as train batch size for architecture weights\n        self.valid_loader.batch_sampler.batch_size = self.train_batch_size\n        self.valid_loader.batch_sampler.drop_last = True\n        self.model.train()\n        self.mutator.change_forward_mode(self.binary_mode)\n        time1 = time.time()  # time\n        # sample a batch of data from validation set\n        images, labels = self._valid_next_batch()\n        images, labels = images.to(self.device), labels.to(self.device)\n        time2 = time.time()  # time\n        self.mutator.reset_binary_gates()\n        self.mutator.unused_modules_off()\n        output = self.model(images)\n        time3 = time.time()\n        ce_loss = self.criterion(output, labels)\n        expected_value = None\n        loss = ce_loss\n        self.model.zero_grad()\n        loss.backward()\n        self.mutator.set_arch_param_grad()\n        self.arch_optimizer.step()\n        if self.mutator.get_forward_mode() == 'two':\n            self.mutator.rescale_updated_arch_param()\n        self.mutator.unused_modules_back()\n        self.mutator.change_forward_mode(None)\n        time4 = time.time()\n        logger.info('(%.4f, %.4f, %.4f)', time2 - time1, time3 - time2, time4 - time3)\n        return loss.data.item(), expected_value.item() if expected_value is not None else None",
  "def save_checkpoint(self):\n        \"\"\"\n        Save checkpoint of the whole model. Saving model weights and architecture weights in\n        ```ckpt_path```, and saving currently chosen architecture in ```arch_path```.\n        \"\"\"\n        if self.ckpt_path:\n            state = {\n                'warmup_curr_epoch': self.warmup_curr_epoch,\n                'train_curr_epoch': self.train_curr_epoch,\n                'model': self.model.state_dict(),\n                'optim': self.model_optim.state_dict(),\n                'arch_optim': self.arch_optimizer.state_dict()\n            }\n            torch.save(state, self.ckpt_path)\n        if self.arch_path:\n            self.export(self.arch_path)",
  "def load_checkpoint(self):\n        \"\"\"\n        Load the checkpoint from ```ckpt_path```.\n        \"\"\"\n        assert self.ckpt_path is not None, \"If load_ckpt is not None, ckpt_path should not be None\"\n        ckpt = torch.load(self.ckpt_path)\n        self.warmup_curr_epoch = ckpt['warmup_curr_epoch']\n        self.train_curr_epoch = ckpt['train_curr_epoch']\n        self.model.load_state_dict(ckpt['model'])\n        self.model_optim.load_state_dict(ckpt['optim'])\n        self.arch_optimizer.load_state_dict(ckpt['arch_optim'])",
  "def train(self):\n        \"\"\"\n        Train the whole model.\n        \"\"\"\n        if self.load_ckpt:\n            self.load_checkpoint()\n        if self.warmup:\n            self._warm_up()\n        self._train()",
  "def export(self, file_name):\n        \"\"\"\n        Export the chosen architecture into a file\n\n        Parameters\n        ----------\n        file_name : str\n            the file that stores exported chosen architecture\n        \"\"\"\n        exported_arch = self.mutator.sample_final()\n        with open(file_name, 'w') as f:\n            json.dump(exported_arch, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)",
  "def validate(self):\n        raise NotImplementedError",
  "def checkpoint(self):\n        raise NotImplementedError",
  "class PdartsMutator(DartsMutator):\n    \"\"\"\n    It works with PdartsTrainer to calculate ops weights,\n    and drop weights in different PDARTS epochs.\n    \"\"\"\n\n    def __init__(self, model, pdarts_epoch_index, pdarts_num_to_drop, switches={}):\n        self.pdarts_epoch_index = pdarts_epoch_index\n        self.pdarts_num_to_drop = pdarts_num_to_drop\n        if switches is None:\n            self.switches = {}\n        else:\n            self.switches = switches\n\n        super(PdartsMutator, self).__init__(model)\n\n        # this loop go through mutables with different keys,\n        # it's mainly to update length of choices.\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n\n                switches = self.switches.get(mutable.key, [True for j in range(len(mutable))])\n                choices = self.choices[mutable.key]\n\n                operations_count = np.sum(switches)\n                # +1 and -1 are caused by zero operation in darts network\n                # the zero operation is not in choices list in network, but its weight are in,\n                # so it needs one more weights and switch for zero.\n                self.choices[mutable.key] = nn.Parameter(1.0E-3 * torch.randn(operations_count + 1))\n                self.switches[mutable.key] = switches\n\n        # update LayerChoice instances in model,\n        # it's physically remove dropped choices operations.\n        for module in self.model.modules():\n            if isinstance(module, LayerChoice):\n                switches = self.switches.get(module.key)\n                choices = self.choices[module.key]\n                if len(module) > len(choices):\n                    # from last to first, so that it won't effect previous indexes after removed one.\n                    for index in range(len(switches)-1, -1, -1):\n                        if switches[index] == False:\n                            del module[index]\n                assert len(module) <= len(choices), \"Failed to remove dropped choices.\"\n\n    def export(self):\n        # Cannot rely on super().export() because P-DARTS has deleted some of the choices and has misaligned length.\n        results = super().sample_final()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                # As some operations are dropped physically,\n                # so it needs to fill back false to track dropped operations.\n                trained_result = results[mutable.key]\n                trained_index = 0\n                switches = self.switches[mutable.key]\n                result = torch.Tensor(switches).bool()\n                for index in range(len(result)):\n                    if result[index]:\n                        result[index] = trained_result[trained_index]\n                        trained_index += 1\n                results[mutable.key] = result\n        return results\n\n    def drop_paths(self):\n        \"\"\"\n        This method is called when a PDARTS epoch is finished.\n        It prepares switches for next epoch.\n        candidate operations with False switch will be doppped in next epoch.\n        \"\"\"\n        all_switches = copy.deepcopy(self.switches)\n        for key in all_switches:\n            switches = all_switches[key]\n            idxs = []\n            for j in range(len(switches)):\n                if switches[j]:\n                    idxs.append(j)\n            sorted_weights = self.choices[key].data.cpu().numpy()[:-1]\n            drop = np.argsort(sorted_weights)[:self.pdarts_num_to_drop[self.pdarts_epoch_index]]\n            for idx in drop:\n                switches[idxs[idx]] = False\n        return all_switches",
  "def __init__(self, model, pdarts_epoch_index, pdarts_num_to_drop, switches={}):\n        self.pdarts_epoch_index = pdarts_epoch_index\n        self.pdarts_num_to_drop = pdarts_num_to_drop\n        if switches is None:\n            self.switches = {}\n        else:\n            self.switches = switches\n\n        super(PdartsMutator, self).__init__(model)\n\n        # this loop go through mutables with different keys,\n        # it's mainly to update length of choices.\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n\n                switches = self.switches.get(mutable.key, [True for j in range(len(mutable))])\n                choices = self.choices[mutable.key]\n\n                operations_count = np.sum(switches)\n                # +1 and -1 are caused by zero operation in darts network\n                # the zero operation is not in choices list in network, but its weight are in,\n                # so it needs one more weights and switch for zero.\n                self.choices[mutable.key] = nn.Parameter(1.0E-3 * torch.randn(operations_count + 1))\n                self.switches[mutable.key] = switches\n\n        # update LayerChoice instances in model,\n        # it's physically remove dropped choices operations.\n        for module in self.model.modules():\n            if isinstance(module, LayerChoice):\n                switches = self.switches.get(module.key)\n                choices = self.choices[module.key]\n                if len(module) > len(choices):\n                    # from last to first, so that it won't effect previous indexes after removed one.\n                    for index in range(len(switches)-1, -1, -1):\n                        if switches[index] == False:\n                            del module[index]\n                assert len(module) <= len(choices), \"Failed to remove dropped choices.\"",
  "def export(self):\n        # Cannot rely on super().export() because P-DARTS has deleted some of the choices and has misaligned length.\n        results = super().sample_final()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                # As some operations are dropped physically,\n                # so it needs to fill back false to track dropped operations.\n                trained_result = results[mutable.key]\n                trained_index = 0\n                switches = self.switches[mutable.key]\n                result = torch.Tensor(switches).bool()\n                for index in range(len(result)):\n                    if result[index]:\n                        result[index] = trained_result[trained_index]\n                        trained_index += 1\n                results[mutable.key] = result\n        return results",
  "def drop_paths(self):\n        \"\"\"\n        This method is called when a PDARTS epoch is finished.\n        It prepares switches for next epoch.\n        candidate operations with False switch will be doppped in next epoch.\n        \"\"\"\n        all_switches = copy.deepcopy(self.switches)\n        for key in all_switches:\n            switches = all_switches[key]\n            idxs = []\n            for j in range(len(switches)):\n                if switches[j]:\n                    idxs.append(j)\n            sorted_weights = self.choices[key].data.cpu().numpy()[:-1]\n            drop = np.argsort(sorted_weights)[:self.pdarts_num_to_drop[self.pdarts_epoch_index]]\n            for idx in drop:\n                switches[idxs[idx]] = False\n        return all_switches",
  "class PdartsTrainer(BaseTrainer):\n    \"\"\"\n    This trainer implements the PDARTS algorithm.\n    PDARTS bases on DARTS algorithm, and provides a network growth approach to find deeper and better network.\n    This class relies on pdarts_num_layers and pdarts_num_to_drop parameters to control how network grows.\n    pdarts_num_layers means how many layers more than first epoch.\n    pdarts_num_to_drop means how many candidate operations should be dropped in each epoch.\n        So that the grew network can in similar size.\n    \"\"\"\n\n    def __init__(self, model_creator, init_layers, metrics,\n                 num_epochs, dataset_train, dataset_valid,\n                 pdarts_num_layers=[0, 6, 12], pdarts_num_to_drop=[3, 2, 1],\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None, callbacks=None, unrolled=False):\n        super(PdartsTrainer, self).__init__()\n        self.model_creator = model_creator\n        self.init_layers = init_layers\n        self.pdarts_num_layers = pdarts_num_layers\n        self.pdarts_num_to_drop = pdarts_num_to_drop\n        self.pdarts_epoch = len(pdarts_num_to_drop)\n        self.darts_parameters = {\n            \"metrics\": metrics,\n            \"num_epochs\": num_epochs,\n            \"dataset_train\": dataset_train,\n            \"dataset_valid\": dataset_valid,\n            \"batch_size\": batch_size,\n            \"workers\": workers,\n            \"device\": device,\n            \"log_frequency\": log_frequency,\n            \"unrolled\": unrolled\n        }\n        self.callbacks = callbacks if callbacks is not None else []\n\n    def train(self):\n\n        switches = None\n        for epoch in range(self.pdarts_epoch):\n\n            layers = self.init_layers+self.pdarts_num_layers[epoch]\n            model, criterion, optim, lr_scheduler = self.model_creator(layers)\n            self.mutator = PdartsMutator(model, epoch, self.pdarts_num_to_drop, switches)\n\n            for callback in self.callbacks:\n                callback.build(model, self.mutator, self)\n                callback.on_epoch_begin(epoch)\n\n            darts_callbacks = []\n            if lr_scheduler is not None:\n                darts_callbacks.append(LRSchedulerCallback(lr_scheduler))\n\n            self.trainer = DartsTrainer(model, mutator=self.mutator, loss=criterion, optimizer=optim,\n                                        callbacks=darts_callbacks, **self.darts_parameters)\n            logger.info(\"start pdarts training epoch %s...\", epoch)\n\n            self.trainer.train()\n\n            switches = self.mutator.drop_paths()\n\n            for callback in self.callbacks:\n                callback.on_epoch_end(epoch)\n\n    def validate(self):\n        self.trainer.validate()\n\n    def export(self, file):\n        mutator_export = self.mutator.export()\n        with open(file, \"w\") as f:\n            json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n\n    def checkpoint(self):\n        raise NotImplementedError(\"Not implemented yet\")",
  "def __init__(self, model_creator, init_layers, metrics,\n                 num_epochs, dataset_train, dataset_valid,\n                 pdarts_num_layers=[0, 6, 12], pdarts_num_to_drop=[3, 2, 1],\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None, callbacks=None, unrolled=False):\n        super(PdartsTrainer, self).__init__()\n        self.model_creator = model_creator\n        self.init_layers = init_layers\n        self.pdarts_num_layers = pdarts_num_layers\n        self.pdarts_num_to_drop = pdarts_num_to_drop\n        self.pdarts_epoch = len(pdarts_num_to_drop)\n        self.darts_parameters = {\n            \"metrics\": metrics,\n            \"num_epochs\": num_epochs,\n            \"dataset_train\": dataset_train,\n            \"dataset_valid\": dataset_valid,\n            \"batch_size\": batch_size,\n            \"workers\": workers,\n            \"device\": device,\n            \"log_frequency\": log_frequency,\n            \"unrolled\": unrolled\n        }\n        self.callbacks = callbacks if callbacks is not None else []",
  "def train(self):\n\n        switches = None\n        for epoch in range(self.pdarts_epoch):\n\n            layers = self.init_layers+self.pdarts_num_layers[epoch]\n            model, criterion, optim, lr_scheduler = self.model_creator(layers)\n            self.mutator = PdartsMutator(model, epoch, self.pdarts_num_to_drop, switches)\n\n            for callback in self.callbacks:\n                callback.build(model, self.mutator, self)\n                callback.on_epoch_begin(epoch)\n\n            darts_callbacks = []\n            if lr_scheduler is not None:\n                darts_callbacks.append(LRSchedulerCallback(lr_scheduler))\n\n            self.trainer = DartsTrainer(model, mutator=self.mutator, loss=criterion, optimizer=optim,\n                                        callbacks=darts_callbacks, **self.darts_parameters)\n            logger.info(\"start pdarts training epoch %s...\", epoch)\n\n            self.trainer.train()\n\n            switches = self.mutator.drop_paths()\n\n            for callback in self.callbacks:\n                callback.on_epoch_end(epoch)",
  "def validate(self):\n        self.trainer.validate()",
  "def export(self, file):\n        mutator_export = self.mutator.export()\n        with open(file, \"w\") as f:\n            json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)",
  "def checkpoint(self):\n        raise NotImplementedError(\"Not implemented yet\")",
  "class DartsMutator(Mutator):\n    \"\"\"\n    Connects the model in a DARTS (differentiable) way.\n\n    An extra connection is automatically inserted for each LayerChoice, when this connection is selected, there is no\n    op on this LayerChoice (namely a ``ZeroOp``), in which case, every element in the exported choice list is ``false``\n    (not chosen).\n\n    All input choice will be fully connected in the search phase. On exporting, the input choice will choose inputs based\n    on keys in ``choose_from``. If the keys were to be keys of LayerChoices, the top logit of the corresponding LayerChoice\n    will join the competition of input choice to compete against other logits. Otherwise, the logit will be assumed 0.\n\n    It's possible to cut branches by setting parameter ``choices`` in a particular position to ``-inf``. After softmax, the\n    value would be 0. Framework will ignore 0 values and not connect. Note that the gradient on the ``-inf`` location will\n    be 0. Since manipulations with ``-inf`` will be ``nan``, you need to handle the gradient update phase carefully.\n\n    Attributes\n    ----------\n    choices: ParameterDict\n        dict that maps keys of LayerChoices to weighted-connection float tensors.\n    \"\"\"\n    def __init__(self, model):\n        super().__init__(model)\n        self.choices = nn.ParameterDict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                self.choices[mutable.key] = nn.Parameter(1.0E-3 * torch.randn(mutable.length + 1))\n\n    def device(self):\n        for v in self.choices.values():\n            return v.device\n\n    def sample_search(self):\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                result[mutable.key] = F.softmax(self.choices[mutable.key], dim=-1)[:-1]\n            elif isinstance(mutable, InputChoice):\n                result[mutable.key] = torch.ones(mutable.n_candidates, dtype=torch.bool, device=self.device())\n        return result\n\n    def sample_final(self):\n        result = dict()\n        edges_max = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                max_val, index = torch.max(F.softmax(self.choices[mutable.key], dim=-1)[:-1], 0)\n                edges_max[mutable.key] = max_val\n                result[mutable.key] = F.one_hot(index, num_classes=len(mutable)).view(-1).bool()\n        for mutable in self.mutables:\n            if isinstance(mutable, InputChoice):\n                if mutable.n_chosen is not None:\n                    weights = []\n                    for src_key in mutable.choose_from:\n                        if src_key not in edges_max:\n                            _logger.warning(\"InputChoice.NO_KEY in '%s' is weighted 0 when selecting inputs.\", mutable.key)\n                        weights.append(edges_max.get(src_key, 0.))\n                    weights = torch.tensor(weights)  # pylint: disable=not-callable\n                    _, topk_edge_indices = torch.topk(weights, mutable.n_chosen)\n                    selected_multihot = []\n                    for i, src_key in enumerate(mutable.choose_from):\n                        if i not in topk_edge_indices and src_key in result:\n                            # If an edge is never selected, there is no need to calculate any op on this edge.\n                            # This is to eliminate redundant calculation.\n                            result[src_key] = torch.zeros_like(result[src_key])\n                        selected_multihot.append(i in topk_edge_indices)\n                    result[mutable.key] = torch.tensor(selected_multihot, dtype=torch.bool, device=self.device())  # pylint: disable=not-callable\n                else:\n                    result[mutable.key] = torch.ones(mutable.n_candidates, dtype=torch.bool, device=self.device())  # pylint: disable=not-callable\n        return result",
  "def __init__(self, model):\n        super().__init__(model)\n        self.choices = nn.ParameterDict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                self.choices[mutable.key] = nn.Parameter(1.0E-3 * torch.randn(mutable.length + 1))",
  "def device(self):\n        for v in self.choices.values():\n            return v.device",
  "def sample_search(self):\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                result[mutable.key] = F.softmax(self.choices[mutable.key], dim=-1)[:-1]\n            elif isinstance(mutable, InputChoice):\n                result[mutable.key] = torch.ones(mutable.n_candidates, dtype=torch.bool, device=self.device())\n        return result",
  "def sample_final(self):\n        result = dict()\n        edges_max = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                max_val, index = torch.max(F.softmax(self.choices[mutable.key], dim=-1)[:-1], 0)\n                edges_max[mutable.key] = max_val\n                result[mutable.key] = F.one_hot(index, num_classes=len(mutable)).view(-1).bool()\n        for mutable in self.mutables:\n            if isinstance(mutable, InputChoice):\n                if mutable.n_chosen is not None:\n                    weights = []\n                    for src_key in mutable.choose_from:\n                        if src_key not in edges_max:\n                            _logger.warning(\"InputChoice.NO_KEY in '%s' is weighted 0 when selecting inputs.\", mutable.key)\n                        weights.append(edges_max.get(src_key, 0.))\n                    weights = torch.tensor(weights)  # pylint: disable=not-callable\n                    _, topk_edge_indices = torch.topk(weights, mutable.n_chosen)\n                    selected_multihot = []\n                    for i, src_key in enumerate(mutable.choose_from):\n                        if i not in topk_edge_indices and src_key in result:\n                            # If an edge is never selected, there is no need to calculate any op on this edge.\n                            # This is to eliminate redundant calculation.\n                            result[src_key] = torch.zeros_like(result[src_key])\n                        selected_multihot.append(i in topk_edge_indices)\n                    result[mutable.key] = torch.tensor(selected_multihot, dtype=torch.bool, device=self.device())  # pylint: disable=not-callable\n                else:\n                    result[mutable.key] = torch.ones(mutable.n_candidates, dtype=torch.bool, device=self.device())  # pylint: disable=not-callable\n        return result",
  "class DartsTrainer(Trainer):\n    \"\"\"\n    DARTS trainer.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model to be trained.\n    loss : callable\n        Receives logits and ground truth label, return a loss tensor.\n    metrics : callable\n        Receives logits and ground truth label, return a dict of metrics.\n    optimizer : Optimizer\n        The optimizer used for optimizing the model.\n    num_epochs : int\n        Number of epochs planned for training.\n    dataset_train : Dataset\n        Dataset for training. Will be split for training weights and architecture weights.\n    dataset_valid : Dataset\n        Dataset for testing.\n    mutator : DartsMutator\n        Use in case of customizing your own DartsMutator. By default will instantiate a DartsMutator.\n    batch_size : int\n        Batch size.\n    workers : int\n        Workers for data loading.\n    device : torch.device\n        ``torch.device(\"cpu\")`` or ``torch.device(\"cuda\")``.\n    log_frequency : int\n        Step count per logging.\n    callbacks : list of Callback\n        list of callbacks to trigger at events.\n    arc_learning_rate : float\n        Learning rate of architecture parameters.\n    unrolled : float\n        ``True`` if using second order optimization, else first order optimization.\n    \"\"\"\n    def __init__(self, model, loss, metrics,\n                 optimizer, num_epochs, dataset_train, dataset_valid,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None,\n                 callbacks=None, arc_learning_rate=3.0E-4, unrolled=False):\n        super().__init__(model, mutator if mutator is not None else DartsMutator(model),\n                         loss, metrics, optimizer, num_epochs, dataset_train, dataset_valid,\n                         batch_size, workers, device, log_frequency, callbacks)\n\n        self.ctrl_optim = torch.optim.Adam(self.mutator.parameters(), arc_learning_rate, betas=(0.5, 0.999),\n                                           weight_decay=1.0E-3)\n        self.unrolled = unrolled\n\n        n_train = len(self.dataset_train)\n        split = n_train // 2\n        indices = list(range(n_train))\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split])\n        valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[split:])\n        self.train_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=batch_size,\n                                                        sampler=train_sampler,\n                                                        num_workers=workers)\n        self.valid_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=batch_size,\n                                                        sampler=valid_sampler,\n                                                        num_workers=workers)\n        self.test_loader = torch.utils.data.DataLoader(self.dataset_valid,\n                                                       batch_size=batch_size,\n                                                       num_workers=workers)\n\n    def train_one_epoch(self, epoch):\n        self.model.train()\n        self.mutator.train()\n        meters = AverageMeterGroup()\n        for step, ((trn_X, trn_y), (val_X, val_y)) in enumerate(zip(self.train_loader, self.valid_loader)):\n            trn_X, trn_y = trn_X.to(self.device), trn_y.to(self.device)\n            val_X, val_y = val_X.to(self.device), val_y.to(self.device)\n\n            # phase 1. architecture step\n            self.ctrl_optim.zero_grad()\n            if self.unrolled:\n                self._unrolled_backward(trn_X, trn_y, val_X, val_y)\n            else:\n                self._backward(val_X, val_y)\n            self.ctrl_optim.step()\n\n            # phase 2: child network step\n            self.optimizer.zero_grad()\n            logits, loss = self._logits_and_loss(trn_X, trn_y)\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.model.parameters(), 5.)  # gradient clipping\n            self.optimizer.step()\n\n            metrics = self.metrics(logits, trn_y)\n            metrics[\"loss\"] = loss.item()\n            meters.update(metrics)\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(\"Epoch [%s/%s] Step [%s/%s]  %s\", epoch + 1,\n                            self.num_epochs, step + 1, len(self.train_loader), meters)\n\n    def validate_one_epoch(self, epoch):\n        self.model.eval()\n        self.mutator.eval()\n        meters = AverageMeterGroup()\n        with torch.no_grad():\n            self.mutator.reset()\n            for step, (X, y) in enumerate(self.test_loader):\n                X, y = X.to(self.device), y.to(self.device)\n                logits = self.model(X)\n                metrics = self.metrics(logits, y)\n                meters.update(metrics)\n                if self.log_frequency is not None and step % self.log_frequency == 0:\n                    logger.info(\"Epoch [%s/%s] Step [%s/%s]  %s\", epoch + 1,\n                                self.num_epochs, step + 1, len(self.test_loader), meters)\n\n    def _logits_and_loss(self, X, y):\n        self.mutator.reset()\n        logits = self.model(X)\n        loss = self.loss(logits, y)\n        self._write_graph_status()\n        return logits, loss\n\n    def _backward(self, val_X, val_y):\n        \"\"\"\n        Simple backward with gradient descent\n        \"\"\"\n        _, loss = self._logits_and_loss(val_X, val_y)\n        loss.backward()\n\n    def _unrolled_backward(self, trn_X, trn_y, val_X, val_y):\n        \"\"\"\n        Compute unrolled loss and backward its gradients\n        \"\"\"\n        backup_params = copy.deepcopy(tuple(self.model.parameters()))\n\n        # do virtual step on training data\n        lr = self.optimizer.param_groups[0][\"lr\"]\n        momentum = self.optimizer.param_groups[0][\"momentum\"]\n        weight_decay = self.optimizer.param_groups[0][\"weight_decay\"]\n        self._compute_virtual_model(trn_X, trn_y, lr, momentum, weight_decay)\n\n        # calculate unrolled loss on validation data\n        # keep gradients for model here for compute hessian\n        _, loss = self._logits_and_loss(val_X, val_y)\n        w_model, w_ctrl = tuple(self.model.parameters()), tuple(self.mutator.parameters())\n        w_grads = torch.autograd.grad(loss, w_model + w_ctrl)\n        d_model, d_ctrl = w_grads[:len(w_model)], w_grads[len(w_model):]\n\n        # compute hessian and final gradients\n        hessian = self._compute_hessian(backup_params, d_model, trn_X, trn_y)\n        with torch.no_grad():\n            for param, d, h in zip(w_ctrl, d_ctrl, hessian):\n                # gradient = dalpha - lr * hessian\n                param.grad = d - lr * h\n\n        # restore weights\n        self._restore_weights(backup_params)\n\n    def _compute_virtual_model(self, X, y, lr, momentum, weight_decay):\n        \"\"\"\n        Compute unrolled weights w`\n        \"\"\"\n        # don't need zero_grad, using autograd to calculate gradients\n        _, loss = self._logits_and_loss(X, y)\n        gradients = torch.autograd.grad(loss, self.model.parameters())\n        with torch.no_grad():\n            for w, g in zip(self.model.parameters(), gradients):\n                m = self.optimizer.state[w].get(\"momentum_buffer\", 0.)\n                w = w - lr * (momentum * m + g + weight_decay * w)\n\n    def _restore_weights(self, backup_params):\n        with torch.no_grad():\n            for param, backup in zip(self.model.parameters(), backup_params):\n                param.copy_(backup)\n\n    def _compute_hessian(self, backup_params, dw, trn_X, trn_y):\n        \"\"\"\n            dw = dw` { L_val(w`, alpha) }\n            w+ = w + eps * dw\n            w- = w - eps * dw\n            hessian = (dalpha { L_trn(w+, alpha) } - dalpha { L_trn(w-, alpha) }) / (2*eps)\n            eps = 0.01 / ||dw||\n        \"\"\"\n        self._restore_weights(backup_params)\n        norm = torch.cat([w.view(-1) for w in dw]).norm()\n        eps = 0.01 / norm\n        if norm < 1E-8:\n            logger.warning(\"In computing hessian, norm is smaller than 1E-8, cause eps to be %.6f.\", norm.item())\n\n        dalphas = []\n        for e in [eps, -2. * eps]:\n            # w+ = w + eps*dw`, w- = w - eps*dw`\n            with torch.no_grad():\n                for p, d in zip(self.model.parameters(), dw):\n                    p += e * d\n\n            _, loss = self._logits_and_loss(trn_X, trn_y)\n            dalphas.append(torch.autograd.grad(loss, self.mutator.parameters()))\n\n        dalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }\n        hessian = [(p - n) / 2. * eps for p, n in zip(dalpha_pos, dalpha_neg)]\n        return hessian",
  "def __init__(self, model, loss, metrics,\n                 optimizer, num_epochs, dataset_train, dataset_valid,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None,\n                 callbacks=None, arc_learning_rate=3.0E-4, unrolled=False):\n        super().__init__(model, mutator if mutator is not None else DartsMutator(model),\n                         loss, metrics, optimizer, num_epochs, dataset_train, dataset_valid,\n                         batch_size, workers, device, log_frequency, callbacks)\n\n        self.ctrl_optim = torch.optim.Adam(self.mutator.parameters(), arc_learning_rate, betas=(0.5, 0.999),\n                                           weight_decay=1.0E-3)\n        self.unrolled = unrolled\n\n        n_train = len(self.dataset_train)\n        split = n_train // 2\n        indices = list(range(n_train))\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split])\n        valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[split:])\n        self.train_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=batch_size,\n                                                        sampler=train_sampler,\n                                                        num_workers=workers)\n        self.valid_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=batch_size,\n                                                        sampler=valid_sampler,\n                                                        num_workers=workers)\n        self.test_loader = torch.utils.data.DataLoader(self.dataset_valid,\n                                                       batch_size=batch_size,\n                                                       num_workers=workers)",
  "def train_one_epoch(self, epoch):\n        self.model.train()\n        self.mutator.train()\n        meters = AverageMeterGroup()\n        for step, ((trn_X, trn_y), (val_X, val_y)) in enumerate(zip(self.train_loader, self.valid_loader)):\n            trn_X, trn_y = trn_X.to(self.device), trn_y.to(self.device)\n            val_X, val_y = val_X.to(self.device), val_y.to(self.device)\n\n            # phase 1. architecture step\n            self.ctrl_optim.zero_grad()\n            if self.unrolled:\n                self._unrolled_backward(trn_X, trn_y, val_X, val_y)\n            else:\n                self._backward(val_X, val_y)\n            self.ctrl_optim.step()\n\n            # phase 2: child network step\n            self.optimizer.zero_grad()\n            logits, loss = self._logits_and_loss(trn_X, trn_y)\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.model.parameters(), 5.)  # gradient clipping\n            self.optimizer.step()\n\n            metrics = self.metrics(logits, trn_y)\n            metrics[\"loss\"] = loss.item()\n            meters.update(metrics)\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(\"Epoch [%s/%s] Step [%s/%s]  %s\", epoch + 1,\n                            self.num_epochs, step + 1, len(self.train_loader), meters)",
  "def validate_one_epoch(self, epoch):\n        self.model.eval()\n        self.mutator.eval()\n        meters = AverageMeterGroup()\n        with torch.no_grad():\n            self.mutator.reset()\n            for step, (X, y) in enumerate(self.test_loader):\n                X, y = X.to(self.device), y.to(self.device)\n                logits = self.model(X)\n                metrics = self.metrics(logits, y)\n                meters.update(metrics)\n                if self.log_frequency is not None and step % self.log_frequency == 0:\n                    logger.info(\"Epoch [%s/%s] Step [%s/%s]  %s\", epoch + 1,\n                                self.num_epochs, step + 1, len(self.test_loader), meters)",
  "def _logits_and_loss(self, X, y):\n        self.mutator.reset()\n        logits = self.model(X)\n        loss = self.loss(logits, y)\n        self._write_graph_status()\n        return logits, loss",
  "def _backward(self, val_X, val_y):\n        \"\"\"\n        Simple backward with gradient descent\n        \"\"\"\n        _, loss = self._logits_and_loss(val_X, val_y)\n        loss.backward()",
  "def _unrolled_backward(self, trn_X, trn_y, val_X, val_y):\n        \"\"\"\n        Compute unrolled loss and backward its gradients\n        \"\"\"\n        backup_params = copy.deepcopy(tuple(self.model.parameters()))\n\n        # do virtual step on training data\n        lr = self.optimizer.param_groups[0][\"lr\"]\n        momentum = self.optimizer.param_groups[0][\"momentum\"]\n        weight_decay = self.optimizer.param_groups[0][\"weight_decay\"]\n        self._compute_virtual_model(trn_X, trn_y, lr, momentum, weight_decay)\n\n        # calculate unrolled loss on validation data\n        # keep gradients for model here for compute hessian\n        _, loss = self._logits_and_loss(val_X, val_y)\n        w_model, w_ctrl = tuple(self.model.parameters()), tuple(self.mutator.parameters())\n        w_grads = torch.autograd.grad(loss, w_model + w_ctrl)\n        d_model, d_ctrl = w_grads[:len(w_model)], w_grads[len(w_model):]\n\n        # compute hessian and final gradients\n        hessian = self._compute_hessian(backup_params, d_model, trn_X, trn_y)\n        with torch.no_grad():\n            for param, d, h in zip(w_ctrl, d_ctrl, hessian):\n                # gradient = dalpha - lr * hessian\n                param.grad = d - lr * h\n\n        # restore weights\n        self._restore_weights(backup_params)",
  "def _compute_virtual_model(self, X, y, lr, momentum, weight_decay):\n        \"\"\"\n        Compute unrolled weights w`\n        \"\"\"\n        # don't need zero_grad, using autograd to calculate gradients\n        _, loss = self._logits_and_loss(X, y)\n        gradients = torch.autograd.grad(loss, self.model.parameters())\n        with torch.no_grad():\n            for w, g in zip(self.model.parameters(), gradients):\n                m = self.optimizer.state[w].get(\"momentum_buffer\", 0.)\n                w = w - lr * (momentum * m + g + weight_decay * w)",
  "def _restore_weights(self, backup_params):\n        with torch.no_grad():\n            for param, backup in zip(self.model.parameters(), backup_params):\n                param.copy_(backup)",
  "def _compute_hessian(self, backup_params, dw, trn_X, trn_y):\n        \"\"\"\n            dw = dw` { L_val(w`, alpha) }\n            w+ = w + eps * dw\n            w- = w - eps * dw\n            hessian = (dalpha { L_trn(w+, alpha) } - dalpha { L_trn(w-, alpha) }) / (2*eps)\n            eps = 0.01 / ||dw||\n        \"\"\"\n        self._restore_weights(backup_params)\n        norm = torch.cat([w.view(-1) for w in dw]).norm()\n        eps = 0.01 / norm\n        if norm < 1E-8:\n            logger.warning(\"In computing hessian, norm is smaller than 1E-8, cause eps to be %.6f.\", norm.item())\n\n        dalphas = []\n        for e in [eps, -2. * eps]:\n            # w+ = w + eps*dw`, w- = w - eps*dw`\n            with torch.no_grad():\n                for p, d in zip(self.model.parameters(), dw):\n                    p += e * d\n\n            _, loss = self._logits_and_loss(trn_X, trn_y)\n            dalphas.append(torch.autograd.grad(loss, self.mutator.parameters()))\n\n        dalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }\n        hessian = [(p - n) / 2. * eps for p, n in zip(dalpha_pos, dalpha_neg)]\n        return hessian",
  "def get_and_apply_next_architecture(model):\n    \"\"\"\n    Wrapper of :class:`~nni.nas.pytorch.classic_nas.mutator.ClassicMutator` to make it more meaningful,\n    similar to ``get_next_parameter`` for HPO.\n\n    Tt will generate search space based on ``model``.\n    If env ``NNI_GEN_SEARCH_SPACE`` exists, this is in dry run mode for\n    generating search space for the experiment.\n    If not, there are still two mode, one is nni experiment mode where users\n    use ``nnictl`` to start an experiment. The other is standalone mode\n    where users directly run the trial command, this mode chooses the first\n    one(s) for each LayerChoice and InputChoice.\n\n    Parameters\n    ----------\n    model : nn.Module\n        User's model with search space (e.g., LayerChoice, InputChoice) embedded in it.\n    \"\"\"\n    ClassicMutator(model)",
  "class ClassicMutator(Mutator):\n    \"\"\"\n    This mutator is to apply the architecture chosen from tuner.\n    It implements the forward function of LayerChoice and InputChoice,\n    to only activate the chosen ones.\n\n    Parameters\n    ----------\n    model : nn.Module\n        User's model with search space (e.g., LayerChoice, InputChoice) embedded in it.\n    \"\"\"\n\n    def __init__(self, model):\n        super(ClassicMutator, self).__init__(model)\n        self._chosen_arch = {}\n        self._search_space = self._generate_search_space()\n        if NNI_GEN_SEARCH_SPACE in os.environ:\n            # dry run for only generating search space\n            self._dump_search_space(os.environ[NNI_GEN_SEARCH_SPACE])\n            sys.exit(0)\n\n        if trial_env_vars.NNI_PLATFORM is None:\n            logger.warning(\"This is in standalone mode, the chosen are the first one(s).\")\n            self._chosen_arch = self._standalone_generate_chosen()\n        else:\n            # get chosen arch from tuner\n            self._chosen_arch = nni.get_next_parameter()\n            if self._chosen_arch is None:\n                if trial_env_vars.NNI_PLATFORM == \"unittest\":\n                    # happens if NNI_PLATFORM is intentionally set, e.g., in UT\n                    logger.warning(\"`NNI_PLATFORM` is set but `param` is None. Falling back to standalone mode.\")\n                    self._chosen_arch = self._standalone_generate_chosen()\n                else:\n                    raise RuntimeError(\"Chosen architecture is None. This may be a platform error.\")\n        self.reset()\n\n    def _sample_layer_choice(self, mutable, idx, value, search_space_item):\n        \"\"\"\n        Convert layer choice to tensor representation.\n\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        \"\"\"\n        # doesn't support multihot for layer choice yet\n        onehot_list = [False] * len(mutable)\n        assert 0 <= idx < len(mutable) and search_space_item[idx] == value, \\\n            \"Index '{}' in search space '{}' is not '{}'\".format(idx, search_space_item, value)\n        onehot_list[idx] = True\n        return torch.tensor(onehot_list, dtype=torch.bool)  # pylint: disable=not-callable\n\n    def _sample_input_choice(self, mutable, idx, value, search_space_item):\n        \"\"\"\n        Convert input choice to tensor representation.\n\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        \"\"\"\n        candidate_repr = search_space_item[\"candidates\"]\n        multihot_list = [False] * mutable.n_candidates\n        for i, v in zip(idx, value):\n            assert 0 <= i < mutable.n_candidates and candidate_repr[i] == v, \\\n                \"Index '{}' in search space '{}' is not '{}'\".format(i, candidate_repr, v)\n            assert not multihot_list[i], \"'{}' is selected twice in '{}', which is not allowed.\".format(i, idx)\n            multihot_list[i] = True\n        return torch.tensor(multihot_list, dtype=torch.bool)  # pylint: disable=not-callable\n\n    def sample_search(self):\n        \"\"\"\n        See :meth:`sample_final`.\n        \"\"\"\n        return self.sample_final()\n\n    def sample_final(self):\n        \"\"\"\n        Convert the chosen arch and apply it on model.\n        \"\"\"\n        assert set(self._chosen_arch.keys()) == set(self._search_space.keys()), \\\n            \"Unmatched keys, expected keys '{}' from search space, found '{}'.\".format(self._search_space.keys(),\n                                                                                       self._chosen_arch.keys())\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, (LayerChoice, InputChoice)):\n                assert mutable.key in self._chosen_arch, \\\n                    \"Expected '{}' in chosen arch, but not found.\".format(mutable.key)\n                data = self._chosen_arch[mutable.key]\n                assert isinstance(data, dict) and \"_value\" in data and \"_idx\" in data, \\\n                    \"'{}' is not a valid choice.\".format(data)\n            if isinstance(mutable, LayerChoice):\n                result[mutable.key] = self._sample_layer_choice(mutable, data[\"_idx\"], data[\"_value\"],\n                                                                self._search_space[mutable.key][\"_value\"])\n            elif isinstance(mutable, InputChoice):\n                result[mutable.key] = self._sample_input_choice(mutable, data[\"_idx\"], data[\"_value\"],\n                                                                self._search_space[mutable.key][\"_value\"])\n            elif isinstance(mutable, MutableScope):\n                logger.info(\"Mutable scope '%s' is skipped during parsing choices.\", mutable.key)\n            else:\n                raise TypeError(\"Unsupported mutable type: '%s'.\" % type(mutable))\n        return result\n\n    def _standalone_generate_chosen(self):\n        \"\"\"\n        Generate the chosen architecture for standalone mode,\n        i.e., choose the first one(s) for LayerChoice and InputChoice.\n        ::\n            { key_name: {\"_value\": \"conv1\",\n                         \"_idx\": 0} }\n            { key_name: {\"_value\": [\"in1\"],\n                         \"_idx\": [0]} }\n        Returns\n        -------\n        dict\n            the chosen architecture\n        \"\"\"\n        chosen_arch = {}\n        for key, val in self._search_space.items():\n            if val[\"_type\"] == LAYER_CHOICE:\n                choices = val[\"_value\"]\n                chosen_arch[key] = {\"_value\": choices[0], \"_idx\": 0}\n            elif val[\"_type\"] == INPUT_CHOICE:\n                choices = val[\"_value\"][\"candidates\"]\n                n_chosen = val[\"_value\"][\"n_chosen\"]\n                if n_chosen is None:\n                    n_chosen = len(choices)\n                chosen_arch[key] = {\"_value\": choices[:n_chosen], \"_idx\": list(range(n_chosen))}\n            else:\n                raise ValueError(\"Unknown key '%s' and value '%s'.\" % (key, val))\n        return chosen_arch\n\n    def _generate_search_space(self):\n        \"\"\"\n        Generate search space from mutables.\n        Here is the search space format:\n        ::\n            { key_name: {\"_type\": \"layer_choice\",\n                         \"_value\": [\"conv1\", \"conv2\"]} }\n            { key_name: {\"_type\": \"input_choice\",\n                         \"_value\": {\"candidates\": [\"in1\", \"in2\"],\n                                    \"n_chosen\": 1}} }\n        Returns\n        -------\n        dict\n            the generated search space\n        \"\"\"\n        search_space = {}\n        for mutable in self.mutables:\n            # for now we only generate flattened search space\n            if isinstance(mutable, LayerChoice):\n                key = mutable.key\n                val = mutable.names\n                search_space[key] = {\"_type\": LAYER_CHOICE, \"_value\": val}\n            elif isinstance(mutable, InputChoice):\n                key = mutable.key\n                search_space[key] = {\"_type\": INPUT_CHOICE,\n                                     \"_value\": {\"candidates\": mutable.choose_from,\n                                                \"n_chosen\": mutable.n_chosen}}\n            elif isinstance(mutable, MutableScope):\n                logger.info(\"Mutable scope '%s' is skipped during generating search space.\", mutable.key)\n            else:\n                raise TypeError(\"Unsupported mutable type: '%s'.\" % type(mutable))\n        return search_space\n\n    def _dump_search_space(self, file_path):\n        with open(file_path, \"w\") as ss_file:\n            json.dump(self._search_space, ss_file, sort_keys=True, indent=2)",
  "def __init__(self, model):\n        super(ClassicMutator, self).__init__(model)\n        self._chosen_arch = {}\n        self._search_space = self._generate_search_space()\n        if NNI_GEN_SEARCH_SPACE in os.environ:\n            # dry run for only generating search space\n            self._dump_search_space(os.environ[NNI_GEN_SEARCH_SPACE])\n            sys.exit(0)\n\n        if trial_env_vars.NNI_PLATFORM is None:\n            logger.warning(\"This is in standalone mode, the chosen are the first one(s).\")\n            self._chosen_arch = self._standalone_generate_chosen()\n        else:\n            # get chosen arch from tuner\n            self._chosen_arch = nni.get_next_parameter()\n            if self._chosen_arch is None:\n                if trial_env_vars.NNI_PLATFORM == \"unittest\":\n                    # happens if NNI_PLATFORM is intentionally set, e.g., in UT\n                    logger.warning(\"`NNI_PLATFORM` is set but `param` is None. Falling back to standalone mode.\")\n                    self._chosen_arch = self._standalone_generate_chosen()\n                else:\n                    raise RuntimeError(\"Chosen architecture is None. This may be a platform error.\")\n        self.reset()",
  "def _sample_layer_choice(self, mutable, idx, value, search_space_item):\n        \"\"\"\n        Convert layer choice to tensor representation.\n\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        \"\"\"\n        # doesn't support multihot for layer choice yet\n        onehot_list = [False] * len(mutable)\n        assert 0 <= idx < len(mutable) and search_space_item[idx] == value, \\\n            \"Index '{}' in search space '{}' is not '{}'\".format(idx, search_space_item, value)\n        onehot_list[idx] = True\n        return torch.tensor(onehot_list, dtype=torch.bool)",
  "def _sample_input_choice(self, mutable, idx, value, search_space_item):\n        \"\"\"\n        Convert input choice to tensor representation.\n\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        \"\"\"\n        candidate_repr = search_space_item[\"candidates\"]\n        multihot_list = [False] * mutable.n_candidates\n        for i, v in zip(idx, value):\n            assert 0 <= i < mutable.n_candidates and candidate_repr[i] == v, \\\n                \"Index '{}' in search space '{}' is not '{}'\".format(i, candidate_repr, v)\n            assert not multihot_list[i], \"'{}' is selected twice in '{}', which is not allowed.\".format(i, idx)\n            multihot_list[i] = True\n        return torch.tensor(multihot_list, dtype=torch.bool)",
  "def sample_search(self):\n        \"\"\"\n        See :meth:`sample_final`.\n        \"\"\"\n        return self.sample_final()",
  "def sample_final(self):\n        \"\"\"\n        Convert the chosen arch and apply it on model.\n        \"\"\"\n        assert set(self._chosen_arch.keys()) == set(self._search_space.keys()), \\\n            \"Unmatched keys, expected keys '{}' from search space, found '{}'.\".format(self._search_space.keys(),\n                                                                                       self._chosen_arch.keys())\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, (LayerChoice, InputChoice)):\n                assert mutable.key in self._chosen_arch, \\\n                    \"Expected '{}' in chosen arch, but not found.\".format(mutable.key)\n                data = self._chosen_arch[mutable.key]\n                assert isinstance(data, dict) and \"_value\" in data and \"_idx\" in data, \\\n                    \"'{}' is not a valid choice.\".format(data)\n            if isinstance(mutable, LayerChoice):\n                result[mutable.key] = self._sample_layer_choice(mutable, data[\"_idx\"], data[\"_value\"],\n                                                                self._search_space[mutable.key][\"_value\"])\n            elif isinstance(mutable, InputChoice):\n                result[mutable.key] = self._sample_input_choice(mutable, data[\"_idx\"], data[\"_value\"],\n                                                                self._search_space[mutable.key][\"_value\"])\n            elif isinstance(mutable, MutableScope):\n                logger.info(\"Mutable scope '%s' is skipped during parsing choices.\", mutable.key)\n            else:\n                raise TypeError(\"Unsupported mutable type: '%s'.\" % type(mutable))\n        return result",
  "def _standalone_generate_chosen(self):\n        \"\"\"\n        Generate the chosen architecture for standalone mode,\n        i.e., choose the first one(s) for LayerChoice and InputChoice.\n        ::\n            { key_name: {\"_value\": \"conv1\",\n                         \"_idx\": 0} }\n            { key_name: {\"_value\": [\"in1\"],\n                         \"_idx\": [0]} }\n        Returns\n        -------\n        dict\n            the chosen architecture\n        \"\"\"\n        chosen_arch = {}\n        for key, val in self._search_space.items():\n            if val[\"_type\"] == LAYER_CHOICE:\n                choices = val[\"_value\"]\n                chosen_arch[key] = {\"_value\": choices[0], \"_idx\": 0}\n            elif val[\"_type\"] == INPUT_CHOICE:\n                choices = val[\"_value\"][\"candidates\"]\n                n_chosen = val[\"_value\"][\"n_chosen\"]\n                if n_chosen is None:\n                    n_chosen = len(choices)\n                chosen_arch[key] = {\"_value\": choices[:n_chosen], \"_idx\": list(range(n_chosen))}\n            else:\n                raise ValueError(\"Unknown key '%s' and value '%s'.\" % (key, val))\n        return chosen_arch",
  "def _generate_search_space(self):\n        \"\"\"\n        Generate search space from mutables.\n        Here is the search space format:\n        ::\n            { key_name: {\"_type\": \"layer_choice\",\n                         \"_value\": [\"conv1\", \"conv2\"]} }\n            { key_name: {\"_type\": \"input_choice\",\n                         \"_value\": {\"candidates\": [\"in1\", \"in2\"],\n                                    \"n_chosen\": 1}} }\n        Returns\n        -------\n        dict\n            the generated search space\n        \"\"\"\n        search_space = {}\n        for mutable in self.mutables:\n            # for now we only generate flattened search space\n            if isinstance(mutable, LayerChoice):\n                key = mutable.key\n                val = mutable.names\n                search_space[key] = {\"_type\": LAYER_CHOICE, \"_value\": val}\n            elif isinstance(mutable, InputChoice):\n                key = mutable.key\n                search_space[key] = {\"_type\": INPUT_CHOICE,\n                                     \"_value\": {\"candidates\": mutable.choose_from,\n                                                \"n_chosen\": mutable.n_chosen}}\n            elif isinstance(mutable, MutableScope):\n                logger.info(\"Mutable scope '%s' is skipped during generating search space.\", mutable.key)\n            else:\n                raise TypeError(\"Unsupported mutable type: '%s'.\" % type(mutable))\n        return search_space",
  "def _dump_search_space(self, file_path):\n        with open(file_path, \"w\") as ss_file:\n            json.dump(self._search_space, ss_file, sort_keys=True, indent=2)",
  "class DropPath(nn.Module):\n    def __init__(self, p=0.):\n        \"\"\"\n        Drop path with probability.\n\n        Parameters\n        ----------\n        p : float\n            Probability of an path to be zeroed.\n        \"\"\"\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        if self.training and self.p > 0.:\n            keep_prob = 1. - self.p\n            # per data point mask\n            mask = torch.zeros((x.size(0), 1, 1, 1), device=x.device).bernoulli_(keep_prob)\n            return x / keep_prob * mask\n\n        return x",
  "class PoolBN(nn.Module):\n    \"\"\"\n    AvgPool or MaxPool with BN. ``pool_type`` must be ``max`` or ``avg``.\n\n    Parameters\n    ---\n    pool_type: str\n        choose operation\n    C: int\n        number of channels\n    kernal_size: int\n\t    size of the convolving kernel\n    stride: int\n\t    stride of the convolution\n    padding: int\n\t    zero-padding added to both sides of the input\n    affine: bool\n        is using affine in BatchNorm\n    \"\"\"\n\n    def __init__(self, pool_type, C, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        if pool_type.lower() == 'max':\n            self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n        elif pool_type.lower() == 'avg':\n            self.pool = nn.AvgPool2d(kernel_size, stride, padding, count_include_pad=False)\n        else:\n            raise ValueError()\n\n        self.bn = nn.BatchNorm2d(C, affine=affine)\n\n    def forward(self, x):\n        out = self.pool(x)\n        out = self.bn(out)\n        return out",
  "class StdConv(nn.Sequential):\n    \"\"\"\n    Standard conv: ReLU - Conv - BN\n\n    Parameters\n    ---\n    C_in: int\n        the number of input channels\n    C_out: int\n        the number of output channels\n    kernel_size: int\n        size of the convolution kernel\n    padding:\n        zero-padding added to both sides of the input\n    affine: bool\n        is using affine in BatchNorm\n    \"\"\"\n\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential\n        for idx, ops in enumerate((nn.ReLU(), nn.Conv2d(C_in, C_out, kernel_size, stride, padding, bias=False),\n                                   nn.BatchNorm2d(C_out, affine=affine))):\n            self.add_module(str(idx), ops)",
  "class FacConv(nn.Module):\n    \"\"\"\n    Factorized conv: ReLU - Conv(Kx1) - Conv(1xK) - BN\n    \"\"\"\n\n    def __init__(self, C_in, C_out, kernel_length, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_in, (kernel_length, 1), stride, padding, bias=False),\n            nn.Conv2d(C_in, C_out, (1, kernel_length), stride, padding, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)",
  "class DilConv(nn.Module):\n    \"\"\"\n    (Dilated) depthwise separable conv.\n    ReLU - (Dilated) depthwise separable - Pointwise - BN.\n    If dilation == 2, 3x3 conv => 5x5 receptive field, 5x5 conv => 9x9 receptive field.\n\n    Parameters\n    ---\n    C_in: int\n        the number of input channels\n    C_out: int\n        the number of output channels\n    kernal_size:\n        size of the convolving kernel\n    padding:\n        zero-padding added to both sides of the input\n    dilation: int\n        spacing between kernel elements.\n    affine: bool\n        is using affine in BatchNorm\n    \"\"\"\n\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_in, kernel_size, stride, padding, dilation=dilation, groups=C_in,\n                      bias=False),\n            nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)",
  "class SepConv(nn.Module):\n    \"\"\"\n    Depthwise separable conv.\n    DilConv(dilation=1) * 2.\n\n    Parameters\n    ---\n    C_in: int\n        the number of input channels\n    C_out: int\n        the number of output channels\n    kernal_size:\n        size of the convolving kernel\n    padding:\n        zero-padding added to both sides of the input\n    dilation: int\n        spacing between kernel elements.\n    affine: bool\n        is using affine in BatchNorm\n    \"\"\"\n\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            DilConv(C_in, C_in, kernel_size, stride, padding, dilation=1, affine=affine),\n            DilConv(C_in, C_out, kernel_size, 1, padding, dilation=1, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)",
  "class FactorizedReduce(nn.Module):\n    \"\"\"\n    Reduce feature map size by factorized pointwise (stride=2).\n    \"\"\"\n\n    def __init__(self, C_in, C_out, affine=True):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n\n    def forward(self, x):\n        x = self.relu(x)\n        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)\n        out = self.bn(out)\n        return out",
  "def __init__(self, p=0.):\n        \"\"\"\n        Drop path with probability.\n\n        Parameters\n        ----------\n        p : float\n            Probability of an path to be zeroed.\n        \"\"\"\n        super().__init__()\n        self.p = p",
  "def forward(self, x):\n        if self.training and self.p > 0.:\n            keep_prob = 1. - self.p\n            # per data point mask\n            mask = torch.zeros((x.size(0), 1, 1, 1), device=x.device).bernoulli_(keep_prob)\n            return x / keep_prob * mask\n\n        return x",
  "def __init__(self, pool_type, C, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        if pool_type.lower() == 'max':\n            self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n        elif pool_type.lower() == 'avg':\n            self.pool = nn.AvgPool2d(kernel_size, stride, padding, count_include_pad=False)\n        else:\n            raise ValueError()\n\n        self.bn = nn.BatchNorm2d(C, affine=affine)",
  "def forward(self, x):\n        out = self.pool(x)\n        out = self.bn(out)\n        return out",
  "def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential\n        for idx, ops in enumerate((nn.ReLU(), nn.Conv2d(C_in, C_out, kernel_size, stride, padding, bias=False),\n                                   nn.BatchNorm2d(C_out, affine=affine))):\n            self.add_module(str(idx), ops)",
  "def __init__(self, C_in, C_out, kernel_length, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_in, (kernel_length, 1), stride, padding, bias=False),\n            nn.Conv2d(C_in, C_out, (1, kernel_length), stride, padding, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )",
  "def forward(self, x):\n        return self.net(x)",
  "def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_in, kernel_size, stride, padding, dilation=dilation, groups=C_in,\n                      bias=False),\n            nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )",
  "def forward(self, x):\n        return self.net(x)",
  "def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            DilConv(C_in, C_in, kernel_size, stride, padding, dilation=1, affine=affine),\n            DilConv(C_in, C_out, kernel_size, 1, padding, dilation=1, affine=affine)\n        )",
  "def forward(self, x):\n        return self.net(x)",
  "def __init__(self, C_in, C_out, affine=True):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)",
  "def forward(self, x):\n        x = self.relu(x)\n        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)\n        out = self.bn(out)\n        return out",
  "class Cell(nn.Module):\n    def __init__(self, cell_name, prev_labels, channels):\n        super().__init__()\n        self.input_choice = mutables.InputChoice(choose_from=prev_labels, n_chosen=1, return_mask=True,\n                                                 key=cell_name + \"_input\")\n        self.op_choice = mutables.LayerChoice([\n            SepConvBN(channels, channels, 3, 1),\n            SepConvBN(channels, channels, 5, 2),\n            Pool(\"avg\", 3, 1, 1),\n            Pool(\"max\", 3, 1, 1),\n            nn.Identity()\n        ], key=cell_name + \"_op\")\n\n    def forward(self, prev_layers):\n        chosen_input, chosen_mask = self.input_choice(prev_layers)\n        cell_out = self.op_choice(chosen_input)\n        return cell_out, chosen_mask",
  "class Node(mutables.MutableScope):\n    def __init__(self, node_name, prev_node_names, channels):\n        super().__init__(node_name)\n        self.cell_x = Cell(node_name + \"_x\", prev_node_names, channels)\n        self.cell_y = Cell(node_name + \"_y\", prev_node_names, channels)\n\n    def forward(self, prev_layers):\n        out_x, mask_x = self.cell_x(prev_layers)\n        out_y, mask_y = self.cell_y(prev_layers)\n        return out_x + out_y, mask_x | mask_y",
  "class Calibration(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.process = None\n        if in_channels != out_channels:\n            self.process = StdConv(in_channels, out_channels)\n\n    def forward(self, x):\n        if self.process is None:\n            return x\n        return self.process(x)",
  "class ENASMicroLayer(nn.Module):\n    \"\"\"\n    Builtin EnasMicroLayer. Micro search designs only one building block whose architecture is repeated\n    throughout the final architecture. A cell has ``num_nodes`` nodes and searches the topology and\n    operations among them in RL way. The first two nodes in a layer stand for the outputs from previous\n    previous layer and previous layer respectively. For the following nodes, the controller chooses\n    two previous nodes and applies two operations respectively for each node. Nodes that are not served\n    as input for any other node are viewed as the output of the layer. If there are multiple output nodes,\n    the model will calculate the average of these nodes as the layer output. Every node's output has ``out_channels``\n    channels so the result of the layer has the same number of channels as each node.\n\n    Parameters\n    ---\n    num_nodes: int\n        the number of nodes contained in this layer\n    in_channles_pp: int\n        the number of previous previous layer's output channels\n    in_channels_p: int\n        the number of previous layer's output channels\n    out_channels: int\n        output channels of this layer\n    reduction: bool\n        is reduction operation empolyed before this layer\n    \"\"\"\n    def __init__(self, num_nodes, in_channels_pp, in_channels_p, out_channels, reduction):\n        super().__init__()\n        self.reduction = reduction\n        if self.reduction:\n            self.reduce0 = FactorizedReduce(in_channels_pp, out_channels, affine=False)\n            self.reduce1 = FactorizedReduce(in_channels_p, out_channels, affine=False)\n            in_channels_pp = in_channels_p = out_channels\n        self.preproc0 = Calibration(in_channels_pp, out_channels)\n        self.preproc1 = Calibration(in_channels_p, out_channels)\n\n        self.num_nodes = num_nodes\n        name_prefix = \"reduce\" if reduction else \"normal\"\n        self.nodes = nn.ModuleList()\n        node_labels = [mutables.InputChoice.NO_KEY, mutables.InputChoice.NO_KEY]\n        for i in range(num_nodes):\n            node_labels.append(\"{}_node_{}\".format(name_prefix, i))\n            self.nodes.append(Node(node_labels[-1], node_labels[:-1], out_channels))\n        self.final_conv_w = nn.Parameter(torch.zeros(out_channels, self.num_nodes + 2, out_channels, 1, 1),\n                                         requires_grad=True)\n        self.bn = nn.BatchNorm2d(out_channels, affine=False)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_normal_(self.final_conv_w)\n\n    def forward(self, pprev, prev):\n        \"\"\"\n        Parameters\n        ---\n        pprev: torch.Tensor\n            the output of the previous previous layer\n        prev: torch.Tensor\n            the output of the previous layer\n        \"\"\"\n        if self.reduction:\n            pprev, prev = self.reduce0(pprev), self.reduce1(prev)\n        pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)\n\n        prev_nodes_out = [pprev_, prev_]\n        nodes_used_mask = torch.zeros(self.num_nodes + 2, dtype=torch.bool, device=prev.device)\n        for i in range(self.num_nodes):\n            node_out, mask = self.nodes[i](prev_nodes_out)\n            nodes_used_mask[:mask.size(0)] |= mask.to(node_out.device)\n            prev_nodes_out.append(node_out)\n\n        unused_nodes = torch.cat([out for used, out in zip(nodes_used_mask, prev_nodes_out) if not used], 1)\n        unused_nodes = F.relu(unused_nodes)\n        conv_weight = self.final_conv_w[:, ~nodes_used_mask, :, :, :]\n        conv_weight = conv_weight.view(conv_weight.size(0), -1, 1, 1)\n        out = F.conv2d(unused_nodes, conv_weight)\n        return prev, self.bn(out)",
  "class ENASMacroLayer(mutables.MutableScope):\n    \"\"\"\n    Builtin ENAS Marco Layer. With search space changing to layer level, the controller decides\n    what operation is employed and the previous layer to connect to for skip connections. The model\n    is made up of the same layers but the choice of each layer may be different.\n\n    Parameters\n    ---\n    key: str\n        the name of this layer\n    prev_labels: str\n        names of all previous layers\n    in_filters: int\n        the number of input channels\n    out_filters:\n        the number of output channels\n    \"\"\"\n    def __init__(self, key, prev_labels, in_filters, out_filters):\n        super().__init__(key)\n        self.in_filters = in_filters\n        self.out_filters = out_filters\n        self.mutable = mutables.LayerChoice([\n            ConvBranch(in_filters, out_filters, 3, 1, 1, separable=False),\n            ConvBranch(in_filters, out_filters, 3, 1, 1, separable=True),\n            ConvBranch(in_filters, out_filters, 5, 1, 2, separable=False),\n            ConvBranch(in_filters, out_filters, 5, 1, 2, separable=True),\n            PoolBranch('avg', in_filters, out_filters, 3, 1, 1),\n            PoolBranch('max', in_filters, out_filters, 3, 1, 1)\n        ])\n        if prev_labels:\n            self.skipconnect = mutables.InputChoice(choose_from=prev_labels, n_chosen=None)\n        else:\n            self.skipconnect = None\n        self.batch_norm = nn.BatchNorm2d(out_filters, affine=False)\n\n    def forward(self, prev_list):\n        \"\"\"\n        Parameters\n        ---\n        prev_list: list\n            The cell selects the last element of the list as input and applies an operation on it.\n            The cell chooses none/one/multiple tensor(s) as SkipConnect(s) from the list excluding\n            the last element.\n        \"\"\"\n        out = self.mutable(prev_list[-1])\n        if self.skipconnect is not None:\n            connection = self.skipconnect(prev_list[:-1])\n            if connection is not None:\n                out += connection\n        return self.batch_norm(out)",
  "class ENASMacroGeneralModel(nn.Module):\n    \"\"\"\n    The network is made up by stacking ENASMacroLayer. The Macro search space contains these layers.\n    Each layer chooses an operation from predefined ones and SkipConnect then forms a network.\n\n    Parameters\n    ---\n    num_layers: int\n        The number of layers contained in the network.\n    out_filters: int\n        The number of each layer's output channels.\n    in_channel: int\n        The number of input's channels.\n    num_classes: int\n        The number of classes for classification.\n    dropout_rate: float\n        Dropout layer's dropout rate before the final dense layer.\n    \"\"\"\n    def __init__(self, num_layers=12, out_filters=24, in_channels=3, num_classes=10,\n                 dropout_rate=0.0):\n        super().__init__()\n        self.num_layers = num_layers\n        self.num_classes = num_classes\n        self.out_filters = out_filters\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_channels, out_filters, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_filters)\n        )\n\n        pool_distance = self.num_layers // 3\n        self.pool_layers_idx = [pool_distance - 1, 2 * pool_distance - 1]\n        self.dropout_rate = dropout_rate\n        self.dropout = nn.Dropout(self.dropout_rate)\n\n        self.layers = nn.ModuleList()\n        self.pool_layers = nn.ModuleList()\n        labels = []\n        for layer_id in range(self.num_layers):\n            labels.append(\"layer_{}\".format(layer_id))\n            if layer_id in self.pool_layers_idx:\n                self.pool_layers.append(FactorizedReduce(self.out_filters, self.out_filters))\n            self.layers.append(ENASMacroLayer(labels[-1], labels[:-1], self.out_filters, self.out_filters))\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.dense = nn.Linear(self.out_filters, self.num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        Parameters\n        ---\n        x: torch.Tensor\n            the input of the network\n        \"\"\"\n        bs = x.size(0)\n        cur = self.stem(x)\n\n        layers = [cur]\n\n        for layer_id in range(self.num_layers):\n            cur = self.layers[layer_id](layers)\n            layers.append(cur)\n            if layer_id in self.pool_layers_idx:\n                for i, layer in enumerate(layers):\n                    layers[i] = self.pool_layers[self.pool_layers_idx.index(layer_id)](layer)\n                cur = layers[-1]\n\n        cur = self.gap(cur).view(bs, -1)\n        cur = self.dropout(cur)\n        logits = self.dense(cur)\n        return logits",
  "def __init__(self, cell_name, prev_labels, channels):\n        super().__init__()\n        self.input_choice = mutables.InputChoice(choose_from=prev_labels, n_chosen=1, return_mask=True,\n                                                 key=cell_name + \"_input\")\n        self.op_choice = mutables.LayerChoice([\n            SepConvBN(channels, channels, 3, 1),\n            SepConvBN(channels, channels, 5, 2),\n            Pool(\"avg\", 3, 1, 1),\n            Pool(\"max\", 3, 1, 1),\n            nn.Identity()\n        ], key=cell_name + \"_op\")",
  "def forward(self, prev_layers):\n        chosen_input, chosen_mask = self.input_choice(prev_layers)\n        cell_out = self.op_choice(chosen_input)\n        return cell_out, chosen_mask",
  "def __init__(self, node_name, prev_node_names, channels):\n        super().__init__(node_name)\n        self.cell_x = Cell(node_name + \"_x\", prev_node_names, channels)\n        self.cell_y = Cell(node_name + \"_y\", prev_node_names, channels)",
  "def forward(self, prev_layers):\n        out_x, mask_x = self.cell_x(prev_layers)\n        out_y, mask_y = self.cell_y(prev_layers)\n        return out_x + out_y, mask_x | mask_y",
  "def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.process = None\n        if in_channels != out_channels:\n            self.process = StdConv(in_channels, out_channels)",
  "def forward(self, x):\n        if self.process is None:\n            return x\n        return self.process(x)",
  "def __init__(self, num_nodes, in_channels_pp, in_channels_p, out_channels, reduction):\n        super().__init__()\n        self.reduction = reduction\n        if self.reduction:\n            self.reduce0 = FactorizedReduce(in_channels_pp, out_channels, affine=False)\n            self.reduce1 = FactorizedReduce(in_channels_p, out_channels, affine=False)\n            in_channels_pp = in_channels_p = out_channels\n        self.preproc0 = Calibration(in_channels_pp, out_channels)\n        self.preproc1 = Calibration(in_channels_p, out_channels)\n\n        self.num_nodes = num_nodes\n        name_prefix = \"reduce\" if reduction else \"normal\"\n        self.nodes = nn.ModuleList()\n        node_labels = [mutables.InputChoice.NO_KEY, mutables.InputChoice.NO_KEY]\n        for i in range(num_nodes):\n            node_labels.append(\"{}_node_{}\".format(name_prefix, i))\n            self.nodes.append(Node(node_labels[-1], node_labels[:-1], out_channels))\n        self.final_conv_w = nn.Parameter(torch.zeros(out_channels, self.num_nodes + 2, out_channels, 1, 1),\n                                         requires_grad=True)\n        self.bn = nn.BatchNorm2d(out_channels, affine=False)\n        self.reset_parameters()",
  "def reset_parameters(self):\n        nn.init.kaiming_normal_(self.final_conv_w)",
  "def forward(self, pprev, prev):\n        \"\"\"\n        Parameters\n        ---\n        pprev: torch.Tensor\n            the output of the previous previous layer\n        prev: torch.Tensor\n            the output of the previous layer\n        \"\"\"\n        if self.reduction:\n            pprev, prev = self.reduce0(pprev), self.reduce1(prev)\n        pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)\n\n        prev_nodes_out = [pprev_, prev_]\n        nodes_used_mask = torch.zeros(self.num_nodes + 2, dtype=torch.bool, device=prev.device)\n        for i in range(self.num_nodes):\n            node_out, mask = self.nodes[i](prev_nodes_out)\n            nodes_used_mask[:mask.size(0)] |= mask.to(node_out.device)\n            prev_nodes_out.append(node_out)\n\n        unused_nodes = torch.cat([out for used, out in zip(nodes_used_mask, prev_nodes_out) if not used], 1)\n        unused_nodes = F.relu(unused_nodes)\n        conv_weight = self.final_conv_w[:, ~nodes_used_mask, :, :, :]\n        conv_weight = conv_weight.view(conv_weight.size(0), -1, 1, 1)\n        out = F.conv2d(unused_nodes, conv_weight)\n        return prev, self.bn(out)",
  "def __init__(self, key, prev_labels, in_filters, out_filters):\n        super().__init__(key)\n        self.in_filters = in_filters\n        self.out_filters = out_filters\n        self.mutable = mutables.LayerChoice([\n            ConvBranch(in_filters, out_filters, 3, 1, 1, separable=False),\n            ConvBranch(in_filters, out_filters, 3, 1, 1, separable=True),\n            ConvBranch(in_filters, out_filters, 5, 1, 2, separable=False),\n            ConvBranch(in_filters, out_filters, 5, 1, 2, separable=True),\n            PoolBranch('avg', in_filters, out_filters, 3, 1, 1),\n            PoolBranch('max', in_filters, out_filters, 3, 1, 1)\n        ])\n        if prev_labels:\n            self.skipconnect = mutables.InputChoice(choose_from=prev_labels, n_chosen=None)\n        else:\n            self.skipconnect = None\n        self.batch_norm = nn.BatchNorm2d(out_filters, affine=False)",
  "def forward(self, prev_list):\n        \"\"\"\n        Parameters\n        ---\n        prev_list: list\n            The cell selects the last element of the list as input and applies an operation on it.\n            The cell chooses none/one/multiple tensor(s) as SkipConnect(s) from the list excluding\n            the last element.\n        \"\"\"\n        out = self.mutable(prev_list[-1])\n        if self.skipconnect is not None:\n            connection = self.skipconnect(prev_list[:-1])\n            if connection is not None:\n                out += connection\n        return self.batch_norm(out)",
  "def __init__(self, num_layers=12, out_filters=24, in_channels=3, num_classes=10,\n                 dropout_rate=0.0):\n        super().__init__()\n        self.num_layers = num_layers\n        self.num_classes = num_classes\n        self.out_filters = out_filters\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_channels, out_filters, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_filters)\n        )\n\n        pool_distance = self.num_layers // 3\n        self.pool_layers_idx = [pool_distance - 1, 2 * pool_distance - 1]\n        self.dropout_rate = dropout_rate\n        self.dropout = nn.Dropout(self.dropout_rate)\n\n        self.layers = nn.ModuleList()\n        self.pool_layers = nn.ModuleList()\n        labels = []\n        for layer_id in range(self.num_layers):\n            labels.append(\"layer_{}\".format(layer_id))\n            if layer_id in self.pool_layers_idx:\n                self.pool_layers.append(FactorizedReduce(self.out_filters, self.out_filters))\n            self.layers.append(ENASMacroLayer(labels[-1], labels[:-1], self.out_filters, self.out_filters))\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.dense = nn.Linear(self.out_filters, self.num_classes)",
  "def forward(self, x):\n        \"\"\"\n        Parameters\n        ---\n        x: torch.Tensor\n            the input of the network\n        \"\"\"\n        bs = x.size(0)\n        cur = self.stem(x)\n\n        layers = [cur]\n\n        for layer_id in range(self.num_layers):\n            cur = self.layers[layer_id](layers)\n            layers.append(cur)\n            if layer_id in self.pool_layers_idx:\n                for i, layer in enumerate(layers):\n                    layers[i] = self.pool_layers[self.pool_layers_idx.index(layer_id)](layer)\n                cur = layers[-1]\n\n        cur = self.gap(cur).view(bs, -1)\n        cur = self.dropout(cur)\n        logits = self.dense(cur)\n        return logits",
  "class StdConv(nn.Module):\n    def __init__(self, C_in, C_out):\n        super(StdConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=False),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.conv(x)",
  "class PoolBranch(nn.Module):\n    \"\"\"\n    Pooling structure for Macro search. First pass through a 1x1 Conv, then pooling operation followed by BatchNorm2d.\n\n    Parameters\n    ---\n    pool_type: str\n        only accept ``max`` for MaxPool and ``avg`` for AvgPool\n    C_in: int\n        the number of input channels\n    C_out: int\n        the number of output channels\n    kernal_size: int\n        size of the convolving kernel\n    stride: int\n\t    stride of the convolution\n    padding: int\n\t    zero-padding added to both sides of the input\n    \"\"\"\n    def __init__(self, pool_type, C_in, C_out, kernel_size, stride, padding, affine=False):\n        super().__init__()\n        self.preproc = StdConv(C_in, C_out)\n        self.pool = Pool(pool_type, kernel_size, stride, padding)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n\n    def forward(self, x):\n        out = self.preproc(x)\n        out = self.pool(out)\n        out = self.bn(out)\n        return out",
  "class SeparableConv(nn.Module):\n    def __init__(self, C_in, C_out, kernel_size, stride, padding):\n        super(SeparableConv, self).__init__()\n        self.depthwise = nn.Conv2d(C_in, C_in, kernel_size=kernel_size, padding=padding, stride=stride,\n                                   groups=C_in, bias=False)\n        self.pointwise = nn.Conv2d(C_in, C_out, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out",
  "class ConvBranch(nn.Module):\n    \"\"\"\n    Conv structure for Macro search. First pass through a 1x1 Conv,\n    then Conv operation with kernal_size equals 3 or 5 followed by BatchNorm and ReLU.\n\n    Parameters\n    ---\n    C_in: int\n        the number of input channels\n    C_out: int\n        the number of output channels\n    kernal_size: int\n        size of the convolving kernel\n    stride: int\n\t    stride of the convolution\n    padding: int\n\t    zero-padding added to both sides of the input\n    separable: True\n        is separable Conv is used\n    \"\"\"\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, separable):\n        super(ConvBranch, self).__init__()\n        self.preproc = StdConv(C_in, C_out)\n        if separable:\n            self.conv = SeparableConv(C_out, C_out, kernel_size, stride, padding)\n        else:\n            self.conv = nn.Conv2d(C_out, C_out, kernel_size, stride=stride, padding=padding)\n        self.postproc = nn.Sequential(\n            nn.BatchNorm2d(C_out, affine=False),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        out = self.preproc(x)\n        out = self.conv(out)\n        out = self.postproc(out)\n        return out",
  "class FactorizedReduce(nn.Module):\n    def __init__(self, C_in, C_out, affine=False):\n        super().__init__()\n        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n\n    def forward(self, x):\n        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)\n        out = self.bn(out)\n        return out",
  "class Pool(nn.Module):\n    \"\"\"\n    Pooling structure\n\n    Parameters\n    ---\n    pool_type: str\n        only accept ``max`` for MaxPool and ``avg`` for AvgPool\n    kernal_size: int\n        size of the convolving kernel\n    stride: int\n\t    stride of the convolution\n    padding: int\n\t    zero-padding added to both sides of the input\n    \"\"\"\n    def __init__(self, pool_type, kernel_size, stride, padding):\n        super().__init__()\n        if pool_type.lower() == 'max':\n            self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n        elif pool_type.lower() == 'avg':\n            self.pool = nn.AvgPool2d(kernel_size, stride, padding, count_include_pad=False)\n        else:\n            raise ValueError()\n\n    def forward(self, x):\n        return self.pool(x)",
  "class SepConvBN(nn.Module):\n    \"\"\"\n    Implement SepConv followed by BatchNorm. The structure is ReLU ==> SepConv ==> BN.\n\n    Parameters\n    ---\n    C_in: int\n        the number of imput channels\n    C_out: int\n        the number of output channels\n    kernal_size: int\n        size of the convolving kernel\n    padding: int\n        zero-padding added to both sides of the input\n    \"\"\"\n    def __init__(self, C_in, C_out, kernel_size, padding):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv = SeparableConv(C_in, C_out, kernel_size, 1, padding)\n        self.bn = nn.BatchNorm2d(C_out, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x",
  "def __init__(self, C_in, C_out):\n        super(StdConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=False),\n            nn.ReLU()\n        )",
  "def forward(self, x):\n        return self.conv(x)",
  "def __init__(self, pool_type, C_in, C_out, kernel_size, stride, padding, affine=False):\n        super().__init__()\n        self.preproc = StdConv(C_in, C_out)\n        self.pool = Pool(pool_type, kernel_size, stride, padding)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)",
  "def forward(self, x):\n        out = self.preproc(x)\n        out = self.pool(out)\n        out = self.bn(out)\n        return out",
  "def __init__(self, C_in, C_out, kernel_size, stride, padding):\n        super(SeparableConv, self).__init__()\n        self.depthwise = nn.Conv2d(C_in, C_in, kernel_size=kernel_size, padding=padding, stride=stride,\n                                   groups=C_in, bias=False)\n        self.pointwise = nn.Conv2d(C_in, C_out, kernel_size=1, bias=False)",
  "def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out",
  "def __init__(self, C_in, C_out, kernel_size, stride, padding, separable):\n        super(ConvBranch, self).__init__()\n        self.preproc = StdConv(C_in, C_out)\n        if separable:\n            self.conv = SeparableConv(C_out, C_out, kernel_size, stride, padding)\n        else:\n            self.conv = nn.Conv2d(C_out, C_out, kernel_size, stride=stride, padding=padding)\n        self.postproc = nn.Sequential(\n            nn.BatchNorm2d(C_out, affine=False),\n            nn.ReLU()\n        )",
  "def forward(self, x):\n        out = self.preproc(x)\n        out = self.conv(out)\n        out = self.postproc(out)\n        return out",
  "def __init__(self, C_in, C_out, affine=False):\n        super().__init__()\n        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)",
  "def forward(self, x):\n        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)\n        out = self.bn(out)\n        return out",
  "def __init__(self, pool_type, kernel_size, stride, padding):\n        super().__init__()\n        if pool_type.lower() == 'max':\n            self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n        elif pool_type.lower() == 'avg':\n            self.pool = nn.AvgPool2d(kernel_size, stride, padding, count_include_pad=False)\n        else:\n            raise ValueError()",
  "def forward(self, x):\n        return self.pool(x)",
  "def __init__(self, C_in, C_out, kernel_size, padding):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv = SeparableConv(C_in, C_out, kernel_size, 1, padding)\n        self.bn = nn.BatchNorm2d(C_out, affine=True)",
  "def forward(self, x):\n        x = self.relu(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x",
  "class Node(nn.Module):\n    def __init__(self, node_id, num_prev_nodes, channels, num_downsample_connect):\n        \"\"\"\n        builtin Darts Node structure\n\n        Parameters\n        ---\n        node_id: str\n        num_prev_nodes: int\n            the number of previous nodes in this cell\n        channels: int\n            output channels\n        num_downsample_connect: int\n            downsample the input node if this cell is reduction cell\n        \"\"\"\n        super().__init__()\n        self.ops = nn.ModuleList()\n        choice_keys = []\n        for i in range(num_prev_nodes):\n            stride = 2 if i < num_downsample_connect else 1\n            choice_keys.append(\"{}_p{}\".format(node_id, i))\n            self.ops.append(\n                mutables.LayerChoice(OrderedDict([\n                    (\"maxpool\", PoolBN('max', channels, 3, stride, 1, affine=False)),\n                    (\"avgpool\", PoolBN('avg', channels, 3, stride, 1, affine=False)),\n                    (\"skipconnect\",\n                     nn.Identity() if stride == 1 else FactorizedReduce(channels, channels, affine=False)),\n                    (\"sepconv3x3\", SepConv(channels, channels, 3, stride, 1, affine=False)),\n                    (\"sepconv5x5\", SepConv(channels, channels, 5, stride, 2, affine=False)),\n                    (\"dilconv3x3\", DilConv(channels, channels, 3, stride, 2, 2, affine=False)),\n                    (\"dilconv5x5\", DilConv(channels, channels, 5, stride, 4, 2, affine=False))\n                ]), key=choice_keys[-1]))\n        self.drop_path = DropPath()\n        self.input_switch = mutables.InputChoice(choose_from=choice_keys, n_chosen=2, key=\"{}_switch\".format(node_id))\n\n    def forward(self, prev_nodes):\n        assert len(self.ops) == len(prev_nodes)\n        out = [op(node) for op, node in zip(self.ops, prev_nodes)]\n        out = [self.drop_path(o) if o is not None else None for o in out]\n        return self.input_switch(out)",
  "class DartsCell(nn.Module):\n    \"\"\"\n    Builtin Darts Cell structure. There are ``n_nodes`` nodes in one cell, in which the first two nodes' values are\n    fixed to the results of previous previous cell and previous cell respectively. One node will connect all\n    the nodes after with predefined operations in a mutable way. The last node accepts five inputs from nodes\n    before and it concats all inputs in channels as the output of the current cell, and the number of output\n    channels is ``n_nodes`` times ``channels``.\n\n    Parameters\n    ---\n    n_nodes: int\n        the number of nodes contained in this cell\n    channels_pp: int\n        the number of previous previous cell's output channels\n    channels_p: int\n        the number of previous cell's output channels\n    channels: int\n        the number of output channels for each node\n    reduction_p: bool\n        Is previous cell a reduction cell\n    reduction: bool\n        is current cell a reduction cell\n    \"\"\"\n    def __init__(self, n_nodes, channels_pp, channels_p, channels, reduction_p, reduction):\n        super().__init__()\n        self.reduction = reduction\n        self.n_nodes = n_nodes\n\n        # If previous cell is reduction cell, current input size does not match with\n        # output size of cell[k-2]. So the output[k-2] should be reduced by preprocessing.\n        if reduction_p:\n            self.preproc0 = FactorizedReduce(channels_pp, channels, affine=False)\n        else:\n            self.preproc0 = StdConv(channels_pp, channels, 1, 1, 0, affine=False)\n        self.preproc1 = StdConv(channels_p, channels, 1, 1, 0, affine=False)\n\n        # generate dag\n        self.mutable_ops = nn.ModuleList()\n        for depth in range(2, self.n_nodes + 2):\n            self.mutable_ops.append(Node(\"{}_n{}\".format(\"reduce\" if reduction else \"normal\", depth),\n                                         depth, channels, 2 if reduction else 0))\n\n    def forward(self, pprev, prev):\n        \"\"\"\n        Parameters\n        ---\n        pprev: torch.Tensor\n            the output of the previous previous layer\n        prev: torch.Tensor\n            the output of the previous layer\n        \"\"\"\n        tensors = [self.preproc0(pprev), self.preproc1(prev)]\n        for node in self.mutable_ops:\n            cur_tensor = node(tensors)\n            tensors.append(cur_tensor)\n\n        output = torch.cat(tensors[2:], dim=1)\n        return output",
  "def __init__(self, node_id, num_prev_nodes, channels, num_downsample_connect):\n        \"\"\"\n        builtin Darts Node structure\n\n        Parameters\n        ---\n        node_id: str\n        num_prev_nodes: int\n            the number of previous nodes in this cell\n        channels: int\n            output channels\n        num_downsample_connect: int\n            downsample the input node if this cell is reduction cell\n        \"\"\"\n        super().__init__()\n        self.ops = nn.ModuleList()\n        choice_keys = []\n        for i in range(num_prev_nodes):\n            stride = 2 if i < num_downsample_connect else 1\n            choice_keys.append(\"{}_p{}\".format(node_id, i))\n            self.ops.append(\n                mutables.LayerChoice(OrderedDict([\n                    (\"maxpool\", PoolBN('max', channels, 3, stride, 1, affine=False)),\n                    (\"avgpool\", PoolBN('avg', channels, 3, stride, 1, affine=False)),\n                    (\"skipconnect\",\n                     nn.Identity() if stride == 1 else FactorizedReduce(channels, channels, affine=False)),\n                    (\"sepconv3x3\", SepConv(channels, channels, 3, stride, 1, affine=False)),\n                    (\"sepconv5x5\", SepConv(channels, channels, 5, stride, 2, affine=False)),\n                    (\"dilconv3x3\", DilConv(channels, channels, 3, stride, 2, 2, affine=False)),\n                    (\"dilconv5x5\", DilConv(channels, channels, 5, stride, 4, 2, affine=False))\n                ]), key=choice_keys[-1]))\n        self.drop_path = DropPath()\n        self.input_switch = mutables.InputChoice(choose_from=choice_keys, n_chosen=2, key=\"{}_switch\".format(node_id))",
  "def forward(self, prev_nodes):\n        assert len(self.ops) == len(prev_nodes)\n        out = [op(node) for op, node in zip(self.ops, prev_nodes)]\n        out = [self.drop_path(o) if o is not None else None for o in out]\n        return self.input_switch(out)",
  "def __init__(self, n_nodes, channels_pp, channels_p, channels, reduction_p, reduction):\n        super().__init__()\n        self.reduction = reduction\n        self.n_nodes = n_nodes\n\n        # If previous cell is reduction cell, current input size does not match with\n        # output size of cell[k-2]. So the output[k-2] should be reduced by preprocessing.\n        if reduction_p:\n            self.preproc0 = FactorizedReduce(channels_pp, channels, affine=False)\n        else:\n            self.preproc0 = StdConv(channels_pp, channels, 1, 1, 0, affine=False)\n        self.preproc1 = StdConv(channels_p, channels, 1, 1, 0, affine=False)\n\n        # generate dag\n        self.mutable_ops = nn.ModuleList()\n        for depth in range(2, self.n_nodes + 2):\n            self.mutable_ops.append(Node(\"{}_n{}\".format(\"reduce\" if reduction else \"normal\", depth),\n                                         depth, channels, 2 if reduction else 0))",
  "def forward(self, pprev, prev):\n        \"\"\"\n        Parameters\n        ---\n        pprev: torch.Tensor\n            the output of the previous previous layer\n        prev: torch.Tensor\n            the output of the previous layer\n        \"\"\"\n        tensors = [self.preproc0(pprev), self.preproc1(prev)]\n        for node in self.mutable_ops:\n            cur_tensor = node(tensors)\n            tensors.append(cur_tensor)\n\n        output = torch.cat(tensors[2:], dim=1)\n        return output",
  "class SPOSEvolution(Tuner):\n    \"\"\"\n    SPOS evolution tuner.\n\n    Parameters\n    ----------\n    max_epochs : int\n        Maximum number of epochs to run.\n    num_select : int\n        Number of survival candidates of each epoch.\n    num_population : int\n        Number of candidates at the start of each epoch. If candidates generated by\n        crossover and mutation are not enough, the rest will be filled with random\n        candidates.\n    m_prob : float\n        The probability of mutation.\n    num_crossover : int\n        Number of candidates generated by crossover in each epoch.\n    num_mutation : int\n        Number of candidates generated by mutation in each epoch.\n    \"\"\"\n\n    def __init__(self, max_epochs=20, num_select=10, num_population=50, m_prob=0.1,\n                 num_crossover=25, num_mutation=25):\n        assert num_population >= num_select\n        self.max_epochs = max_epochs\n        self.num_select = num_select\n        self.num_population = num_population\n        self.m_prob = m_prob\n        self.num_crossover = num_crossover\n        self.num_mutation = num_mutation\n        self.epoch = 0\n        self.candidates = []\n        self.search_space = None\n        self.random_state = np.random.RandomState(0)\n\n        # async status\n        self._to_evaluate_queue = deque()\n        self._sending_parameter_queue = deque()\n        self._pending_result_ids = set()\n        self._reward_dict = dict()\n        self._id2candidate = dict()\n        self._st_callback = None\n\n    def update_search_space(self, search_space):\n        \"\"\"\n        Handle the initialization/update event of search space.\n        \"\"\"\n        self._search_space = search_space\n        self._next_round()\n\n    def _next_round(self):\n        _logger.info(\"Epoch %d, generating...\", self.epoch)\n        if self.epoch == 0:\n            self._get_random_population()\n            self.export_results(self.candidates)\n        else:\n            best_candidates = self._select_top_candidates()\n            self.export_results(best_candidates)\n            if self.epoch >= self.max_epochs:\n                return\n            self.candidates = self._get_mutation(best_candidates) + self._get_crossover(best_candidates)\n            self._get_random_population()\n        self.epoch += 1\n\n    def _random_candidate(self):\n        chosen_arch = dict()\n        for key, val in self._search_space.items():\n            if val[\"_type\"] == LAYER_CHOICE:\n                choices = val[\"_value\"]\n                index = self.random_state.randint(len(choices))\n                chosen_arch[key] = {\"_value\": choices[index], \"_idx\": index}\n            elif val[\"_type\"] == INPUT_CHOICE:\n                raise NotImplementedError(\"Input choice is not implemented yet.\")\n        return chosen_arch\n\n    def _add_to_evaluate_queue(self, cand):\n        _logger.info(\"Generate candidate %s, adding to eval queue.\", self._get_architecture_repr(cand))\n        self._reward_dict[self._hashcode(cand)] = 0.\n        self._to_evaluate_queue.append(cand)\n\n    def _get_random_population(self):\n        while len(self.candidates) < self.num_population:\n            cand = self._random_candidate()\n            if self._is_legal(cand):\n                _logger.info(\"Random candidate generated.\")\n                self._add_to_evaluate_queue(cand)\n                self.candidates.append(cand)\n\n    def _get_crossover(self, best):\n        result = []\n        for _ in range(10 * self.num_crossover):\n            cand_p1 = best[self.random_state.randint(len(best))]\n            cand_p2 = best[self.random_state.randint(len(best))]\n            assert cand_p1.keys() == cand_p2.keys()\n            cand = {k: cand_p1[k] if self.random_state.randint(2) == 0 else cand_p2[k]\n                    for k in cand_p1.keys()}\n            if self._is_legal(cand):\n                result.append(cand)\n                self._add_to_evaluate_queue(cand)\n            if len(result) >= self.num_crossover:\n                break\n        _logger.info(\"Found %d architectures with crossover.\", len(result))\n        return result\n\n    def _get_mutation(self, best):\n        result = []\n        for _ in range(10 * self.num_mutation):\n            cand = best[self.random_state.randint(len(best))].copy()\n            mutation_sample = np.random.random_sample(len(cand))\n            for s, k in zip(mutation_sample, cand):\n                if s < self.m_prob:\n                    choices = self._search_space[k][\"_value\"]\n                    index = self.random_state.randint(len(choices))\n                    cand[k] = {\"_value\": choices[index], \"_idx\": index}\n            if self._is_legal(cand):\n                result.append(cand)\n                self._add_to_evaluate_queue(cand)\n            if len(result) >= self.num_mutation:\n                break\n        _logger.info(\"Found %d architectures with mutation.\", len(result))\n        return result\n\n    def _get_architecture_repr(self, cand):\n        return re.sub(r\"\\\".*?\\\": \\{\\\"_idx\\\": (\\d+), \\\"_value\\\": \\\".*?\\\"\\}\", r\"\\1\",\n                      self._hashcode(cand))\n\n    def _is_legal(self, cand):\n        if self._hashcode(cand) in self._reward_dict:\n            return False\n        return True\n\n    def _select_top_candidates(self):\n        reward_query = lambda cand: self._reward_dict[self._hashcode(cand)]\n        _logger.info(\"All candidate rewards: %s\", list(map(reward_query, self.candidates)))\n        result = sorted(self.candidates, key=reward_query, reverse=True)[:self.num_select]\n        _logger.info(\"Best candidate rewards: %s\", list(map(reward_query, result)))\n        return result\n\n    @staticmethod\n    def _hashcode(d):\n        return json.dumps(d, sort_keys=True)\n\n    def _bind_and_send_parameters(self):\n        \"\"\"\n        There are two types of resources: parameter ids and candidates. This function is called at\n        necessary times to bind these resources to send new trials with st_callback.\n        \"\"\"\n        result = []\n        while self._sending_parameter_queue and self._to_evaluate_queue:\n            parameter_id = self._sending_parameter_queue.popleft()\n            parameters = self._to_evaluate_queue.popleft()\n            self._id2candidate[parameter_id] = parameters\n            result.append(parameters)\n            self._pending_result_ids.add(parameter_id)\n            self._st_callback(parameter_id, parameters)\n            _logger.info(\"Send parameter [%d] %s.\", parameter_id, self._get_architecture_repr(parameters))\n        return result\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Callback function necessary to implement a tuner. This will put more parameter ids into the\n        parameter id queue.\n        \"\"\"\n        if \"st_callback\" in kwargs and self._st_callback is None:\n            self._st_callback = kwargs[\"st_callback\"]\n        for parameter_id in parameter_id_list:\n            self._sending_parameter_queue.append(parameter_id)\n        self._bind_and_send_parameters()\n        return []  # always not use this. might induce problem of over-sending\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Callback function. Receive a trial result.\n        \"\"\"\n        _logger.info(\"Candidate %d, reported reward %f\", parameter_id, value)\n        self._reward_dict[self._hashcode(self._id2candidate[parameter_id])] = value\n\n    def trial_end(self, parameter_id, success, **kwargs):\n        \"\"\"\n        Callback function when a trial is ended and resource is released.\n        \"\"\"\n        self._pending_result_ids.remove(parameter_id)\n        if not self._pending_result_ids and not self._to_evaluate_queue:\n            # a new epoch now\n            self._next_round()\n            assert self._st_callback is not None\n            self._bind_and_send_parameters()\n\n    def export_results(self, result):\n        \"\"\"\n        Export a number of candidates to `checkpoints` dir.\n\n        Parameters\n        ----------\n        result : dict\n            Chosen architectures to be exported.\n        \"\"\"\n        os.makedirs(\"checkpoints\", exist_ok=True)\n        for i, cand in enumerate(result):\n            converted = dict()\n            for cand_key, cand_val in cand.items():\n                onehot = [k == cand_val[\"_idx\"] for k in range(len(self._search_space[cand_key][\"_value\"]))]\n                converted[cand_key] = onehot\n            with open(os.path.join(\"checkpoints\", \"%03d_%03d.json\" % (self.epoch, i)), \"w\") as fp:\n                json.dump(converted, fp)",
  "def __init__(self, max_epochs=20, num_select=10, num_population=50, m_prob=0.1,\n                 num_crossover=25, num_mutation=25):\n        assert num_population >= num_select\n        self.max_epochs = max_epochs\n        self.num_select = num_select\n        self.num_population = num_population\n        self.m_prob = m_prob\n        self.num_crossover = num_crossover\n        self.num_mutation = num_mutation\n        self.epoch = 0\n        self.candidates = []\n        self.search_space = None\n        self.random_state = np.random.RandomState(0)\n\n        # async status\n        self._to_evaluate_queue = deque()\n        self._sending_parameter_queue = deque()\n        self._pending_result_ids = set()\n        self._reward_dict = dict()\n        self._id2candidate = dict()\n        self._st_callback = None",
  "def update_search_space(self, search_space):\n        \"\"\"\n        Handle the initialization/update event of search space.\n        \"\"\"\n        self._search_space = search_space\n        self._next_round()",
  "def _next_round(self):\n        _logger.info(\"Epoch %d, generating...\", self.epoch)\n        if self.epoch == 0:\n            self._get_random_population()\n            self.export_results(self.candidates)\n        else:\n            best_candidates = self._select_top_candidates()\n            self.export_results(best_candidates)\n            if self.epoch >= self.max_epochs:\n                return\n            self.candidates = self._get_mutation(best_candidates) + self._get_crossover(best_candidates)\n            self._get_random_population()\n        self.epoch += 1",
  "def _random_candidate(self):\n        chosen_arch = dict()\n        for key, val in self._search_space.items():\n            if val[\"_type\"] == LAYER_CHOICE:\n                choices = val[\"_value\"]\n                index = self.random_state.randint(len(choices))\n                chosen_arch[key] = {\"_value\": choices[index], \"_idx\": index}\n            elif val[\"_type\"] == INPUT_CHOICE:\n                raise NotImplementedError(\"Input choice is not implemented yet.\")\n        return chosen_arch",
  "def _add_to_evaluate_queue(self, cand):\n        _logger.info(\"Generate candidate %s, adding to eval queue.\", self._get_architecture_repr(cand))\n        self._reward_dict[self._hashcode(cand)] = 0.\n        self._to_evaluate_queue.append(cand)",
  "def _get_random_population(self):\n        while len(self.candidates) < self.num_population:\n            cand = self._random_candidate()\n            if self._is_legal(cand):\n                _logger.info(\"Random candidate generated.\")\n                self._add_to_evaluate_queue(cand)\n                self.candidates.append(cand)",
  "def _get_crossover(self, best):\n        result = []\n        for _ in range(10 * self.num_crossover):\n            cand_p1 = best[self.random_state.randint(len(best))]\n            cand_p2 = best[self.random_state.randint(len(best))]\n            assert cand_p1.keys() == cand_p2.keys()\n            cand = {k: cand_p1[k] if self.random_state.randint(2) == 0 else cand_p2[k]\n                    for k in cand_p1.keys()}\n            if self._is_legal(cand):\n                result.append(cand)\n                self._add_to_evaluate_queue(cand)\n            if len(result) >= self.num_crossover:\n                break\n        _logger.info(\"Found %d architectures with crossover.\", len(result))\n        return result",
  "def _get_mutation(self, best):\n        result = []\n        for _ in range(10 * self.num_mutation):\n            cand = best[self.random_state.randint(len(best))].copy()\n            mutation_sample = np.random.random_sample(len(cand))\n            for s, k in zip(mutation_sample, cand):\n                if s < self.m_prob:\n                    choices = self._search_space[k][\"_value\"]\n                    index = self.random_state.randint(len(choices))\n                    cand[k] = {\"_value\": choices[index], \"_idx\": index}\n            if self._is_legal(cand):\n                result.append(cand)\n                self._add_to_evaluate_queue(cand)\n            if len(result) >= self.num_mutation:\n                break\n        _logger.info(\"Found %d architectures with mutation.\", len(result))\n        return result",
  "def _get_architecture_repr(self, cand):\n        return re.sub(r\"\\\".*?\\\": \\{\\\"_idx\\\": (\\d+), \\\"_value\\\": \\\".*?\\\"\\}\", r\"\\1\",\n                      self._hashcode(cand))",
  "def _is_legal(self, cand):\n        if self._hashcode(cand) in self._reward_dict:\n            return False\n        return True",
  "def _select_top_candidates(self):\n        reward_query = lambda cand: self._reward_dict[self._hashcode(cand)]\n        _logger.info(\"All candidate rewards: %s\", list(map(reward_query, self.candidates)))\n        result = sorted(self.candidates, key=reward_query, reverse=True)[:self.num_select]\n        _logger.info(\"Best candidate rewards: %s\", list(map(reward_query, result)))\n        return result",
  "def _hashcode(d):\n        return json.dumps(d, sort_keys=True)",
  "def _bind_and_send_parameters(self):\n        \"\"\"\n        There are two types of resources: parameter ids and candidates. This function is called at\n        necessary times to bind these resources to send new trials with st_callback.\n        \"\"\"\n        result = []\n        while self._sending_parameter_queue and self._to_evaluate_queue:\n            parameter_id = self._sending_parameter_queue.popleft()\n            parameters = self._to_evaluate_queue.popleft()\n            self._id2candidate[parameter_id] = parameters\n            result.append(parameters)\n            self._pending_result_ids.add(parameter_id)\n            self._st_callback(parameter_id, parameters)\n            _logger.info(\"Send parameter [%d] %s.\", parameter_id, self._get_architecture_repr(parameters))\n        return result",
  "def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        \"\"\"\n        Callback function necessary to implement a tuner. This will put more parameter ids into the\n        parameter id queue.\n        \"\"\"\n        if \"st_callback\" in kwargs and self._st_callback is None:\n            self._st_callback = kwargs[\"st_callback\"]\n        for parameter_id in parameter_id_list:\n            self._sending_parameter_queue.append(parameter_id)\n        self._bind_and_send_parameters()\n        return []",
  "def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \"\"\"\n        Callback function. Receive a trial result.\n        \"\"\"\n        _logger.info(\"Candidate %d, reported reward %f\", parameter_id, value)\n        self._reward_dict[self._hashcode(self._id2candidate[parameter_id])] = value",
  "def trial_end(self, parameter_id, success, **kwargs):\n        \"\"\"\n        Callback function when a trial is ended and resource is released.\n        \"\"\"\n        self._pending_result_ids.remove(parameter_id)\n        if not self._pending_result_ids and not self._to_evaluate_queue:\n            # a new epoch now\n            self._next_round()\n            assert self._st_callback is not None\n            self._bind_and_send_parameters()",
  "def export_results(self, result):\n        \"\"\"\n        Export a number of candidates to `checkpoints` dir.\n\n        Parameters\n        ----------\n        result : dict\n            Chosen architectures to be exported.\n        \"\"\"\n        os.makedirs(\"checkpoints\", exist_ok=True)\n        for i, cand in enumerate(result):\n            converted = dict()\n            for cand_key, cand_val in cand.items():\n                onehot = [k == cand_val[\"_idx\"] for k in range(len(self._search_space[cand_key][\"_value\"]))]\n                converted[cand_key] = onehot\n            with open(os.path.join(\"checkpoints\", \"%03d_%03d.json\" % (self.epoch, i)), \"w\") as fp:\n                json.dump(converted, fp)",
  "class SPOSSupernetTrainingMutator(RandomMutator):\n    \"\"\"\n    A random mutator with flops limit.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model.\n    flops_func : callable\n        Callable that takes a candidate from `sample_search` and returns its candidate. When `flops_func`\n        is None, functions related to flops will be deactivated.\n    flops_lb : number\n        Lower bound of flops.\n    flops_ub : number\n        Upper bound of flops.\n    flops_bin_num : number\n        Number of bins divided for the interval of flops to ensure the uniformity. Bigger number will be more\n        uniform, but the sampling will be slower.\n    flops_sample_timeout : int\n        Maximum number of attempts to sample before giving up and use a random candidate.\n    \"\"\"\n    def __init__(self, model, flops_func=None, flops_lb=None, flops_ub=None,\n                 flops_bin_num=7, flops_sample_timeout=500):\n\n        super().__init__(model)\n        self._flops_func = flops_func\n        if self._flops_func is not None:\n            self._flops_bin_num = flops_bin_num\n            self._flops_bins = [flops_lb + (flops_ub - flops_lb) / flops_bin_num * i for i in range(flops_bin_num + 1)]\n            self._flops_sample_timeout = flops_sample_timeout\n\n    def sample_search(self):\n        \"\"\"\n        Sample a candidate for training. When `flops_func` is not None, candidates will be sampled uniformly\n        relative to flops.\n\n        Returns\n        -------\n        dict\n        \"\"\"\n        if self._flops_func is not None:\n            for times in range(self._flops_sample_timeout):\n                idx = np.random.randint(self._flops_bin_num)\n                cand = super().sample_search()\n                if self._flops_bins[idx] <= self._flops_func(cand) <= self._flops_bins[idx + 1]:\n                    _logger.debug(\"Sampled candidate flops %f in %d times.\", cand, times)\n                    return cand\n            _logger.warning(\"Failed to sample a flops-valid candidate within %d tries.\", self._flops_sample_timeout)\n        return super().sample_search()\n\n    def sample_final(self):\n        \"\"\"\n        Implement only to suffice the interface of Mutator.\n        \"\"\"\n        return self.sample_search()",
  "def __init__(self, model, flops_func=None, flops_lb=None, flops_ub=None,\n                 flops_bin_num=7, flops_sample_timeout=500):\n\n        super().__init__(model)\n        self._flops_func = flops_func\n        if self._flops_func is not None:\n            self._flops_bin_num = flops_bin_num\n            self._flops_bins = [flops_lb + (flops_ub - flops_lb) / flops_bin_num * i for i in range(flops_bin_num + 1)]\n            self._flops_sample_timeout = flops_sample_timeout",
  "def sample_search(self):\n        \"\"\"\n        Sample a candidate for training. When `flops_func` is not None, candidates will be sampled uniformly\n        relative to flops.\n\n        Returns\n        -------\n        dict\n        \"\"\"\n        if self._flops_func is not None:\n            for times in range(self._flops_sample_timeout):\n                idx = np.random.randint(self._flops_bin_num)\n                cand = super().sample_search()\n                if self._flops_bins[idx] <= self._flops_func(cand) <= self._flops_bins[idx + 1]:\n                    _logger.debug(\"Sampled candidate flops %f in %d times.\", cand, times)\n                    return cand\n            _logger.warning(\"Failed to sample a flops-valid candidate within %d tries.\", self._flops_sample_timeout)\n        return super().sample_search()",
  "def sample_final(self):\n        \"\"\"\n        Implement only to suffice the interface of Mutator.\n        \"\"\"\n        return self.sample_search()",
  "class SPOSSupernetTrainer(Trainer):\n    \"\"\"\n    This trainer trains a supernet that can be used for evolution search.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Model with mutables.\n    mutator : Mutator\n        A mutator object that has been initialized with the model.\n    loss : callable\n        Called with logits and targets. Returns a loss tensor.\n    metrics : callable\n        Returns a dict that maps metrics keys to metrics data.\n    optimizer : Optimizer\n        Optimizer that optimizes the model.\n    num_epochs : int\n        Number of epochs of training.\n    train_loader : iterable\n        Data loader of training. Raise ``StopIteration`` when one epoch is exhausted.\n    dataset_valid : iterable\n        Data loader of validation. Raise ``StopIteration`` when one epoch is exhausted.\n    batch_size : int\n        Batch size.\n    workers: int\n        Number of threads for data preprocessing. Not used for this trainer. Maybe removed in future.\n    device : torch.device\n        Device object. Either ``torch.device(\"cuda\")`` or ``torch.device(\"cpu\")``. When ``None``, trainer will\n        automatic detects GPU and selects GPU first.\n    log_frequency : int\n        Number of mini-batches to log metrics.\n    callbacks : list of Callback\n        Callbacks to plug into the trainer. See Callbacks.\n    \"\"\"\n\n    def __init__(self, model, loss, metrics,\n                 optimizer, num_epochs, train_loader, valid_loader,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None,\n                 callbacks=None):\n        assert torch.cuda.is_available()\n        super().__init__(model, mutator if mutator is not None else SPOSSupernetTrainingMutator(model),\n                         loss, metrics, optimizer, num_epochs, None, None,\n                         batch_size, workers, device, log_frequency, callbacks)\n\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n\n    def train_one_epoch(self, epoch):\n        self.model.train()\n        meters = AverageMeterGroup()\n        for step, (x, y) in enumerate(self.train_loader):\n            self.optimizer.zero_grad()\n            self.mutator.reset()\n            logits = self.model(x)\n            loss = self.loss(logits, y)\n            loss.backward()\n            self.optimizer.step()\n\n            metrics = self.metrics(logits, y)\n            metrics[\"loss\"] = loss.item()\n            meters.update(metrics)\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(\"Epoch [%s/%s] Step [%s/%s]  %s\", epoch + 1,\n                            self.num_epochs, step + 1, len(self.train_loader), meters)\n\n    def validate_one_epoch(self, epoch):\n        self.model.eval()\n        meters = AverageMeterGroup()\n        with torch.no_grad():\n            for step, (x, y) in enumerate(self.valid_loader):\n                self.mutator.reset()\n                logits = self.model(x)\n                loss = self.loss(logits, y)\n                metrics = self.metrics(logits, y)\n                metrics[\"loss\"] = loss.item()\n                meters.update(metrics)\n                if self.log_frequency is not None and step % self.log_frequency == 0:\n                    logger.info(\"Epoch [%s/%s] Validation Step [%s/%s]  %s\", epoch + 1,\n                                self.num_epochs, step + 1, len(self.valid_loader), meters)",
  "def __init__(self, model, loss, metrics,\n                 optimizer, num_epochs, train_loader, valid_loader,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None,\n                 callbacks=None):\n        assert torch.cuda.is_available()\n        super().__init__(model, mutator if mutator is not None else SPOSSupernetTrainingMutator(model),\n                         loss, metrics, optimizer, num_epochs, None, None,\n                         batch_size, workers, device, log_frequency, callbacks)\n\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader",
  "def train_one_epoch(self, epoch):\n        self.model.train()\n        meters = AverageMeterGroup()\n        for step, (x, y) in enumerate(self.train_loader):\n            self.optimizer.zero_grad()\n            self.mutator.reset()\n            logits = self.model(x)\n            loss = self.loss(logits, y)\n            loss.backward()\n            self.optimizer.step()\n\n            metrics = self.metrics(logits, y)\n            metrics[\"loss\"] = loss.item()\n            meters.update(metrics)\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(\"Epoch [%s/%s] Step [%s/%s]  %s\", epoch + 1,\n                            self.num_epochs, step + 1, len(self.train_loader), meters)",
  "def validate_one_epoch(self, epoch):\n        self.model.eval()\n        meters = AverageMeterGroup()\n        with torch.no_grad():\n            for step, (x, y) in enumerate(self.valid_loader):\n                self.mutator.reset()\n                logits = self.model(x)\n                loss = self.loss(logits, y)\n                metrics = self.metrics(logits, y)\n                metrics[\"loss\"] = loss.item()\n                meters.update(metrics)\n                if self.log_frequency is not None and step % self.log_frequency == 0:\n                    logger.info(\"Epoch [%s/%s] Validation Step [%s/%s]  %s\", epoch + 1,\n                                self.num_epochs, step + 1, len(self.valid_loader), meters)",
  "class CyclicIterator:\n    def __init__(self, loader, sampler, distributed):\n        self.loader = loader\n        self.sampler = sampler\n        self.epoch = 0\n        self.distributed = distributed\n        self._next_epoch()\n\n    def _next_epoch(self):\n        if self.distributed:\n            self.sampler.set_epoch(self.epoch)\n        self.iterator = iter(self.loader)\n        self.epoch += 1\n\n    def __len__(self):\n        return len(self.loader)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            return next(self.iterator)\n        except StopIteration:\n            self._next_epoch()\n            return next(self.iterator)",
  "class TorchTensorEncoder(json.JSONEncoder):\n    def default(self, o):  # pylint: disable=method-hidden\n        if isinstance(o, torch.Tensor):\n            return o.tolist()\n        return super().default(o)",
  "def accuracy(output, target, topk=(1,)):\n    \"\"\" Computes the precision@k for the specified values of k \"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    # one-hot case\n    if target.ndimension() > 1:\n        target = target.max(1)[1]\n\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(1.0 / batch_size))\n    return res",
  "def reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= float(os.environ[\"WORLD_SIZE\"])\n    return rt",
  "def reduce_metrics(metrics, distributed=False):\n    if distributed:\n        return {k: reduce_tensor(v).item() for k, v in metrics.items()}\n    return {k: v.item() for k, v in metrics.items()}",
  "def __init__(self, loader, sampler, distributed):\n        self.loader = loader\n        self.sampler = sampler\n        self.epoch = 0\n        self.distributed = distributed\n        self._next_epoch()",
  "def _next_epoch(self):\n        if self.distributed:\n            self.sampler.set_epoch(self.epoch)\n        self.iterator = iter(self.loader)\n        self.epoch += 1",
  "def __len__(self):\n        return len(self.loader)",
  "def __iter__(self):\n        return self",
  "def __next__(self):\n        try:\n            return next(self.iterator)\n        except StopIteration:\n            self._next_epoch()\n            return next(self.iterator)",
  "def default(self, o):  # pylint: disable=method-hidden\n        if isinstance(o, torch.Tensor):\n            return o.tolist()\n        return super().default(o)",
  "class RegularizedDartsMutator(DartsMutator):\n    \"\"\"\n    This is :class:`~nni.nas.pytorch.darts.DartsMutator` basically, with two differences.\n\n    1. Choices can be cut (bypassed). This is done by ``cut_choices``. Cutted choices will not be used in\n    forward pass and thus consumes no memory.\n\n    2. Regularization on choices, to prevent the mutator from overfitting on some choices.\n    \"\"\"\n\n    def reset(self):\n        \"\"\"\n        Warnings\n        --------\n        Renamed :func:`~reset_with_loss` to return regularization loss on reset.\n        \"\"\"\n        raise ValueError(\"You should probably call `reset_with_loss`.\")\n\n    def cut_choices(self, cut_num=2):\n        \"\"\"\n        Cut the choices with the smallest weights.\n        ``cut_num`` should be the accumulative number of cutting, e.g., if first time cutting\n        is 2, the second time should be 4 to cut another two.\n\n        Parameters\n        ----------\n        cut_num : int\n            Number of choices to cut, so far.\n\n        Warnings\n        --------\n        Though the parameters are set to :math:`-\\infty` to be bypassed, they will still receive gradient of 0,\n        which introduced ``nan`` problem when calling ``optimizer.step()``. To solve this issue, a simple way is to\n        reset nan to :math:`-\\infty` each time after the parameters are updated.\n        \"\"\"\n        # `cut_choices` is implemented but not used in current implementation of CdartsTrainer\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                _, idx = torch.topk(-self.choices[mutable.key], cut_num)\n                with torch.no_grad():\n                    for i in idx:\n                        self.choices[mutable.key][i] = -float(\"inf\")\n\n    def reset_with_loss(self):\n        \"\"\"\n        Resample and return loss. If loss is 0, to avoid device issue, it will return ``None``.\n\n        Currently loss penalty are proportional to the L1-norm of parameters corresponding\n        to modules if their type name contains certain substrings. These substrings include: ``poolwithoutbn``,\n        ``identity``, ``dilconv``.\n        \"\"\"\n        self._cache, reg_loss = self.sample_search()\n        return reg_loss\n\n    def sample_search(self):\n        result = super().sample_search()\n        loss = []\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                def need_reg(choice):\n                    return any(t in str(type(choice)).lower() for t in [\"poolwithoutbn\", \"identity\", \"dilconv\"])\n\n                for i, choice in enumerate(mutable.choices):\n                    if need_reg(choice):\n                        norm = torch.abs(self.choices[mutable.key][i])\n                        if norm < 1E10:\n                            loss.append(norm)\n        if not loss:\n            return result, None\n        return result, sum(loss)\n\n    def export(self, logger=None):\n        \"\"\"\n        Export an architecture with logger. Genotype will be printed with logger.\n\n        Returns\n        -------\n        dict\n            A mapping from mutable keys to decisions.\n        \"\"\"\n        result = self.sample_final()\n        if hasattr(self.model, \"plot_genotype\") and logger is not None:\n            genotypes = self.model.plot_genotype(result, logger)\n        return result, genotypes",
  "class RegularizedMutatorParallel(DistributedDataParallel):\n    \"\"\"\n    Parallelize :class:`~RegularizedDartsMutator`.\n\n    This makes :func:`~RegularizedDartsMutator.reset_with_loss` method parallelized,\n    also allowing :func:`~RegularizedDartsMutator.cut_choices` and :func:`~RegularizedDartsMutator.export`\n    to be easily accessible.\n    \"\"\"\n    def reset_with_loss(self):\n        \"\"\"\n        Parallelized :func:`~RegularizedDartsMutator.reset_with_loss`.\n        \"\"\"\n        result = self.module.reset_with_loss()\n        self.callback_queued = False\n        return result\n\n    def cut_choices(self, *args, **kwargs):\n        \"\"\"\n        Parallelized :func:`~RegularizedDartsMutator.cut_choices`.\n        \"\"\"\n        self.module.cut_choices(*args, **kwargs)\n\n    def export(self, logger):\n        \"\"\"\n        Parallelized :func:`~RegularizedDartsMutator.export`.\n        \"\"\"\n        return self.module.export(logger)",
  "class DartsDiscreteMutator(Mutator):\n    \"\"\"\n    A mutator that applies the final sampling result of a parent mutator on another model to train.\n\n    Parameters\n    ----------\n    model : nn.Module\n        The model to apply the mutator.\n    parent_mutator : Mutator\n        The mutator that provides ``sample_final`` method, that will be called to get the architecture.\n    \"\"\"\n    def __init__(self, model, parent_mutator):\n        super().__init__(model)\n        self.__dict__[\"parent_mutator\"] = parent_mutator  # avoid parameters to be included\n\n    def sample_search(self):\n        return self.parent_mutator.sample_final()",
  "def reset(self):\n        \"\"\"\n        Warnings\n        --------\n        Renamed :func:`~reset_with_loss` to return regularization loss on reset.\n        \"\"\"\n        raise ValueError(\"You should probably call `reset_with_loss`.\")",
  "def cut_choices(self, cut_num=2):\n        \"\"\"\n        Cut the choices with the smallest weights.\n        ``cut_num`` should be the accumulative number of cutting, e.g., if first time cutting\n        is 2, the second time should be 4 to cut another two.\n\n        Parameters\n        ----------\n        cut_num : int\n            Number of choices to cut, so far.\n\n        Warnings\n        --------\n        Though the parameters are set to :math:`-\\infty` to be bypassed, they will still receive gradient of 0,\n        which introduced ``nan`` problem when calling ``optimizer.step()``. To solve this issue, a simple way is to\n        reset nan to :math:`-\\infty` each time after the parameters are updated.\n        \"\"\"\n        # `cut_choices` is implemented but not used in current implementation of CdartsTrainer\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                _, idx = torch.topk(-self.choices[mutable.key], cut_num)\n                with torch.no_grad():\n                    for i in idx:\n                        self.choices[mutable.key][i] = -float(\"inf\")",
  "def reset_with_loss(self):\n        \"\"\"\n        Resample and return loss. If loss is 0, to avoid device issue, it will return ``None``.\n\n        Currently loss penalty are proportional to the L1-norm of parameters corresponding\n        to modules if their type name contains certain substrings. These substrings include: ``poolwithoutbn``,\n        ``identity``, ``dilconv``.\n        \"\"\"\n        self._cache, reg_loss = self.sample_search()\n        return reg_loss",
  "def sample_search(self):\n        result = super().sample_search()\n        loss = []\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                def need_reg(choice):\n                    return any(t in str(type(choice)).lower() for t in [\"poolwithoutbn\", \"identity\", \"dilconv\"])\n\n                for i, choice in enumerate(mutable.choices):\n                    if need_reg(choice):\n                        norm = torch.abs(self.choices[mutable.key][i])\n                        if norm < 1E10:\n                            loss.append(norm)\n        if not loss:\n            return result, None\n        return result, sum(loss)",
  "def export(self, logger=None):\n        \"\"\"\n        Export an architecture with logger. Genotype will be printed with logger.\n\n        Returns\n        -------\n        dict\n            A mapping from mutable keys to decisions.\n        \"\"\"\n        result = self.sample_final()\n        if hasattr(self.model, \"plot_genotype\") and logger is not None:\n            genotypes = self.model.plot_genotype(result, logger)\n        return result, genotypes",
  "def reset_with_loss(self):\n        \"\"\"\n        Parallelized :func:`~RegularizedDartsMutator.reset_with_loss`.\n        \"\"\"\n        result = self.module.reset_with_loss()\n        self.callback_queued = False\n        return result",
  "def cut_choices(self, *args, **kwargs):\n        \"\"\"\n        Parallelized :func:`~RegularizedDartsMutator.cut_choices`.\n        \"\"\"\n        self.module.cut_choices(*args, **kwargs)",
  "def export(self, logger):\n        \"\"\"\n        Parallelized :func:`~RegularizedDartsMutator.export`.\n        \"\"\"\n        return self.module.export(logger)",
  "def __init__(self, model, parent_mutator):\n        super().__init__(model)\n        self.__dict__[\"parent_mutator\"] = parent_mutator",
  "def sample_search(self):\n        return self.parent_mutator.sample_final()",
  "def need_reg(choice):\n                    return any(t in str(type(choice)).lower() for t in [\"poolwithoutbn\", \"identity\", \"dilconv\"])",
  "class InteractiveKLLoss(nn.Module):\n    def __init__(self, temperature):\n        super().__init__()\n        self.temperature = temperature\n        # self.kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n        self.kl_loss = nn.KLDivLoss()\n\n    def forward(self, student, teacher):\n        return self.kl_loss(F.log_softmax(student / self.temperature, dim=1),\n                            F.softmax(teacher / self.temperature, dim=1))",
  "class CdartsTrainer(object):\n    \"\"\"\n    CDARTS trainer.\n\n    Parameters\n    ----------\n    model_small : nn.Module\n        PyTorch model to be trained. This is the search network of CDARTS.\n    model_large : nn.Module\n        PyTorch model to be trained. This is the evaluation network of CDARTS.\n    criterion : callable\n        Receives logits and ground truth label, return a loss tensor, e.g., ``nn.CrossEntropyLoss()``.\n    loaders : list of torch.utils.data.DataLoader\n        List of train data and valid data loaders, for training weights and architecture weights respectively.\n    samplers : list of torch.utils.data.Sampler\n        List of train data and valid data samplers. This can be PyTorch standard samplers if not distributed.\n        In distributed mode, sampler needs to have ``set_epoch`` method. Refer to data utils in CDARTS example for details.\n    logger : logging.Logger\n        The logger for logging. Will use nni logger by default (if logger is ``None``).\n    regular_coeff : float\n        The coefficient of regular loss.\n    regular_ratio : float\n        The ratio of regular loss.\n    warmup_epochs : int\n        The epochs to warmup the search network\n    fix_head : bool\n        ``True`` if fixing the paramters of auxiliary heads, else unfix the paramters of auxiliary heads.\n    epochs : int\n        Number of epochs planned for training.\n    steps_per_epoch : int\n        Steps of one epoch.\n    loss_alpha : float\n        The loss coefficient.\n    loss_T : float\n        The loss coefficient.\n    distributed : bool\n        ``True`` if using distributed training, else non-distributed training.\n    log_frequency : int\n        Step count per logging.\n    grad_clip : float\n        Gradient clipping for weights.\n    interactive_type : string\n        ``kl`` or ``smoothl1``.\n    output_path : string\n        Log storage path.\n    w_lr : float\n        Learning rate of the search network parameters.\n    w_momentum : float\n        Momentum of the search and the evaluation network.\n    w_weight_decay : float\n        The weight decay the search and the evaluation network parameters.\n    alpha_lr : float\n        Learning rate of the architecture parameters.\n    alpha_weight_decay : float\n        The weight decay the architecture parameters.\n    nasnet_lr : float\n        Learning rate of the evaluation network parameters.\n    local_rank : int\n        The number of thread.\n    share_module : bool\n        ``True`` if sharing the stem and auxiliary heads, else not sharing these modules.\n    \"\"\"\n    def __init__(self, model_small, model_large, criterion, loaders, samplers, logger=None,\n                 regular_coeff=5, regular_ratio=0.2, warmup_epochs=2, fix_head=True,\n                 epochs=32, steps_per_epoch=None, loss_alpha=2, loss_T=2, distributed=True,\n                 log_frequency=10, grad_clip=5.0, interactive_type='kl', output_path='./outputs',\n                 w_lr=0.2, w_momentum=0.9, w_weight_decay=3e-4, alpha_lr=0.2, alpha_weight_decay=1e-4,\n                 nasnet_lr=0.2, local_rank=0, share_module=True):\n        if logger is None:\n            logger = logging.getLogger(__name__)\n        train_loader, valid_loader = loaders\n        train_sampler, valid_sampler = samplers\n        self.train_loader = CyclicIterator(train_loader, train_sampler, distributed)\n        self.valid_loader = CyclicIterator(valid_loader, valid_sampler, distributed)\n\n        self.regular_coeff = regular_coeff\n        self.regular_ratio = regular_ratio\n        self.warmup_epochs = warmup_epochs\n        self.fix_head = fix_head\n        self.epochs = epochs\n        self.steps_per_epoch = steps_per_epoch\n        if self.steps_per_epoch is None:\n            self.steps_per_epoch = min(len(self.train_loader), len(self.valid_loader))\n        self.loss_alpha = loss_alpha\n        self.grad_clip = grad_clip\n        if interactive_type == \"kl\":\n            self.interactive_loss = InteractiveKLLoss(loss_T)\n        elif interactive_type == \"smoothl1\":\n            self.interactive_loss = nn.SmoothL1Loss()\n        self.loss_T = loss_T\n        self.distributed = distributed\n        self.log_frequency = log_frequency\n        self.main_proc = not distributed or local_rank == 0\n\n        self.logger = logger\n        self.checkpoint_dir = output_path\n        if self.main_proc:\n            os.makedirs(self.checkpoint_dir, exist_ok=True)\n        if distributed:\n            torch.distributed.barrier()\n\n        self.model_small = model_small\n        self.model_large = model_large\n        if self.fix_head:\n            for param in self.model_small.aux_head.parameters():\n                param.requires_grad = False\n            for param in self.model_large.aux_head.parameters():\n                param.requires_grad = False\n\n        self.mutator_small = RegularizedDartsMutator(self.model_small).cuda()\n        self.mutator_large = DartsDiscreteMutator(self.model_large, self.mutator_small).cuda()\n        self.criterion = criterion\n\n        self.optimizer_small = torch.optim.SGD(self.model_small.parameters(), w_lr,\n                                               momentum=w_momentum, weight_decay=w_weight_decay)\n        self.optimizer_large = torch.optim.SGD(self.model_large.parameters(), nasnet_lr,\n                                               momentum=w_momentum, weight_decay=w_weight_decay)\n        self.optimizer_alpha = torch.optim.Adam(self.mutator_small.parameters(), alpha_lr,\n                                                betas=(0.5, 0.999), weight_decay=alpha_weight_decay)\n\n        if distributed:\n            apex.parallel.convert_syncbn_model(self.model_small)\n            apex.parallel.convert_syncbn_model(self.model_large)\n            self.model_small = DistributedDataParallel(self.model_small, delay_allreduce=True)\n            self.model_large = DistributedDataParallel(self.model_large, delay_allreduce=True)\n            self.mutator_small = RegularizedMutatorParallel(self.mutator_small, delay_allreduce=True)\n            if share_module:\n                self.model_small.callback_queued = True\n                self.model_large.callback_queued = True\n            # mutator large never gets optimized, so do not need parallelized\n\n    def _warmup(self, phase, epoch):\n        assert phase in [PHASE_SMALL, PHASE_LARGE]\n        if phase == PHASE_SMALL:\n            model, optimizer = self.model_small, self.optimizer_small\n        elif phase == PHASE_LARGE:\n            model, optimizer = self.model_large, self.optimizer_large\n        model.train()\n        meters = AverageMeterGroup()\n        for step in range(self.steps_per_epoch):\n            x, y = next(self.train_loader)\n            x, y = x.cuda(), y.cuda()\n\n            optimizer.zero_grad()\n            logits_main, _ = model(x)\n            loss = self.criterion(logits_main, y)\n            loss.backward()\n\n            self._clip_grad_norm(model)\n            optimizer.step()\n            prec1, prec5 = accuracy(logits_main, y, topk=(1, 5))\n            metrics = {\"prec1\": prec1, \"prec5\": prec5, \"loss\": loss}\n            metrics = reduce_metrics(metrics, self.distributed)\n            meters.update(metrics)\n            if self.main_proc and (step % self.log_frequency == 0 or step + 1 == self.steps_per_epoch):\n                self.logger.info(\"Epoch [%d/%d] Step [%d/%d] (%s)  %s\", epoch + 1, self.epochs,\n                                 step + 1, self.steps_per_epoch, phase, meters)\n\n    def _clip_grad_norm(self, model):\n        if isinstance(model, DistributedDataParallel):\n            nn.utils.clip_grad_norm_(model.module.parameters(), self.grad_clip)\n        else:\n            nn.utils.clip_grad_norm_(model.parameters(), self.grad_clip)\n\n    def _reset_nan(self, parameters):\n        with torch.no_grad():\n            for param in parameters:\n                for i, p in enumerate(param):\n                    if p != p:  # equivalent to `isnan(p)`\n                        param[i] = float(\"-inf\")\n\n    def _joint_train(self, epoch):\n        self.model_large.train()\n        self.model_small.train()\n        meters = AverageMeterGroup()\n        for step in range(self.steps_per_epoch):\n            trn_x, trn_y = next(self.train_loader)\n            val_x, val_y = next(self.valid_loader)\n            trn_x, trn_y = trn_x.cuda(), trn_y.cuda()\n            val_x, val_y = val_x.cuda(), val_y.cuda()\n\n            # step 1. optimize architecture\n            self.optimizer_alpha.zero_grad()\n            self.optimizer_large.zero_grad()\n            reg_decay = max(self.regular_coeff * (1 - float(epoch - self.warmup_epochs) / (\n                (self.epochs - self.warmup_epochs) * self.regular_ratio)), 0)\n            loss_regular = self.mutator_small.reset_with_loss()\n            if loss_regular:\n                loss_regular *= reg_decay\n            logits_search, emsemble_logits_search = self.model_small(val_x)\n            logits_main, emsemble_logits_main = self.model_large(val_x)\n            loss_cls = (self.criterion(logits_search, val_y) + self.criterion(logits_main, val_y)) / self.loss_alpha\n            loss_interactive = self.interactive_loss(emsemble_logits_search, emsemble_logits_main) * (self.loss_T ** 2) * self.loss_alpha\n            loss = loss_cls + loss_interactive + loss_regular\n            loss.backward()\n            self._clip_grad_norm(self.model_large)\n            self.optimizer_large.step()\n            self.optimizer_alpha.step()\n            # NOTE: need to call here `self._reset_nan(self.mutator_small.parameters())` if `cut_choices`\n\n            # step 2. optimize op weights\n            self.optimizer_small.zero_grad()\n            with torch.no_grad():\n                # resample architecture since parameters have been changed\n                self.mutator_small.reset_with_loss()\n            logits_search_train, _ = self.model_small(trn_x)\n            loss_weight = self.criterion(logits_search_train, trn_y)\n            loss_weight.backward()\n            self._clip_grad_norm(self.model_small)\n            self.optimizer_small.step()\n\n            metrics = {\"loss_cls\": loss_cls, \"loss_interactive\": loss_interactive,\n                       \"loss_regular\": loss_regular, \"loss_weight\": loss_weight}\n            metrics = reduce_metrics(metrics, self.distributed)\n            meters.update(metrics)\n\n            if self.main_proc and (step % self.log_frequency == 0 or step + 1 == self.steps_per_epoch):\n                self.logger.info(\"Epoch [%d/%d] Step [%d/%d] (joint)  %s\", epoch + 1, self.epochs,\n                                 step + 1, self.steps_per_epoch, meters)\n\n    def train(self):\n        for epoch in range(self.epochs):\n            if epoch < self.warmup_epochs:\n                with torch.no_grad():  # otherwise grads will be retained on the architecture params\n                    self.mutator_small.reset_with_loss()\n                self._warmup(PHASE_SMALL, epoch)\n            else:\n                with torch.no_grad():\n                    self.mutator_large.reset()\n                self._warmup(PHASE_LARGE, epoch)\n                self._joint_train(epoch)\n\n            self.export(os.path.join(self.checkpoint_dir, \"epoch_{:02d}.json\".format(epoch)),\n                        os.path.join(self.checkpoint_dir, \"epoch_{:02d}.genotypes\".format(epoch)))\n\n    def export(self, file, genotype_file):\n        if self.main_proc:\n            mutator_export, genotypes = self.mutator_small.export(self.logger)\n            with open(file, \"w\") as f:\n                json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n            with open(genotype_file, \"w\") as f:\n                f.write(str(genotypes))",
  "def __init__(self, temperature):\n        super().__init__()\n        self.temperature = temperature\n        # self.kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n        self.kl_loss = nn.KLDivLoss()",
  "def forward(self, student, teacher):\n        return self.kl_loss(F.log_softmax(student / self.temperature, dim=1),\n                            F.softmax(teacher / self.temperature, dim=1))",
  "def __init__(self, model_small, model_large, criterion, loaders, samplers, logger=None,\n                 regular_coeff=5, regular_ratio=0.2, warmup_epochs=2, fix_head=True,\n                 epochs=32, steps_per_epoch=None, loss_alpha=2, loss_T=2, distributed=True,\n                 log_frequency=10, grad_clip=5.0, interactive_type='kl', output_path='./outputs',\n                 w_lr=0.2, w_momentum=0.9, w_weight_decay=3e-4, alpha_lr=0.2, alpha_weight_decay=1e-4,\n                 nasnet_lr=0.2, local_rank=0, share_module=True):\n        if logger is None:\n            logger = logging.getLogger(__name__)\n        train_loader, valid_loader = loaders\n        train_sampler, valid_sampler = samplers\n        self.train_loader = CyclicIterator(train_loader, train_sampler, distributed)\n        self.valid_loader = CyclicIterator(valid_loader, valid_sampler, distributed)\n\n        self.regular_coeff = regular_coeff\n        self.regular_ratio = regular_ratio\n        self.warmup_epochs = warmup_epochs\n        self.fix_head = fix_head\n        self.epochs = epochs\n        self.steps_per_epoch = steps_per_epoch\n        if self.steps_per_epoch is None:\n            self.steps_per_epoch = min(len(self.train_loader), len(self.valid_loader))\n        self.loss_alpha = loss_alpha\n        self.grad_clip = grad_clip\n        if interactive_type == \"kl\":\n            self.interactive_loss = InteractiveKLLoss(loss_T)\n        elif interactive_type == \"smoothl1\":\n            self.interactive_loss = nn.SmoothL1Loss()\n        self.loss_T = loss_T\n        self.distributed = distributed\n        self.log_frequency = log_frequency\n        self.main_proc = not distributed or local_rank == 0\n\n        self.logger = logger\n        self.checkpoint_dir = output_path\n        if self.main_proc:\n            os.makedirs(self.checkpoint_dir, exist_ok=True)\n        if distributed:\n            torch.distributed.barrier()\n\n        self.model_small = model_small\n        self.model_large = model_large\n        if self.fix_head:\n            for param in self.model_small.aux_head.parameters():\n                param.requires_grad = False\n            for param in self.model_large.aux_head.parameters():\n                param.requires_grad = False\n\n        self.mutator_small = RegularizedDartsMutator(self.model_small).cuda()\n        self.mutator_large = DartsDiscreteMutator(self.model_large, self.mutator_small).cuda()\n        self.criterion = criterion\n\n        self.optimizer_small = torch.optim.SGD(self.model_small.parameters(), w_lr,\n                                               momentum=w_momentum, weight_decay=w_weight_decay)\n        self.optimizer_large = torch.optim.SGD(self.model_large.parameters(), nasnet_lr,\n                                               momentum=w_momentum, weight_decay=w_weight_decay)\n        self.optimizer_alpha = torch.optim.Adam(self.mutator_small.parameters(), alpha_lr,\n                                                betas=(0.5, 0.999), weight_decay=alpha_weight_decay)\n\n        if distributed:\n            apex.parallel.convert_syncbn_model(self.model_small)\n            apex.parallel.convert_syncbn_model(self.model_large)\n            self.model_small = DistributedDataParallel(self.model_small, delay_allreduce=True)\n            self.model_large = DistributedDataParallel(self.model_large, delay_allreduce=True)\n            self.mutator_small = RegularizedMutatorParallel(self.mutator_small, delay_allreduce=True)\n            if share_module:\n                self.model_small.callback_queued = True\n                self.model_large.callback_queued = True",
  "def _warmup(self, phase, epoch):\n        assert phase in [PHASE_SMALL, PHASE_LARGE]\n        if phase == PHASE_SMALL:\n            model, optimizer = self.model_small, self.optimizer_small\n        elif phase == PHASE_LARGE:\n            model, optimizer = self.model_large, self.optimizer_large\n        model.train()\n        meters = AverageMeterGroup()\n        for step in range(self.steps_per_epoch):\n            x, y = next(self.train_loader)\n            x, y = x.cuda(), y.cuda()\n\n            optimizer.zero_grad()\n            logits_main, _ = model(x)\n            loss = self.criterion(logits_main, y)\n            loss.backward()\n\n            self._clip_grad_norm(model)\n            optimizer.step()\n            prec1, prec5 = accuracy(logits_main, y, topk=(1, 5))\n            metrics = {\"prec1\": prec1, \"prec5\": prec5, \"loss\": loss}\n            metrics = reduce_metrics(metrics, self.distributed)\n            meters.update(metrics)\n            if self.main_proc and (step % self.log_frequency == 0 or step + 1 == self.steps_per_epoch):\n                self.logger.info(\"Epoch [%d/%d] Step [%d/%d] (%s)  %s\", epoch + 1, self.epochs,\n                                 step + 1, self.steps_per_epoch, phase, meters)",
  "def _clip_grad_norm(self, model):\n        if isinstance(model, DistributedDataParallel):\n            nn.utils.clip_grad_norm_(model.module.parameters(), self.grad_clip)\n        else:\n            nn.utils.clip_grad_norm_(model.parameters(), self.grad_clip)",
  "def _reset_nan(self, parameters):\n        with torch.no_grad():\n            for param in parameters:\n                for i, p in enumerate(param):\n                    if p != p:  # equivalent to `isnan(p)`\n                        param[i] = float(\"-inf\")",
  "def _joint_train(self, epoch):\n        self.model_large.train()\n        self.model_small.train()\n        meters = AverageMeterGroup()\n        for step in range(self.steps_per_epoch):\n            trn_x, trn_y = next(self.train_loader)\n            val_x, val_y = next(self.valid_loader)\n            trn_x, trn_y = trn_x.cuda(), trn_y.cuda()\n            val_x, val_y = val_x.cuda(), val_y.cuda()\n\n            # step 1. optimize architecture\n            self.optimizer_alpha.zero_grad()\n            self.optimizer_large.zero_grad()\n            reg_decay = max(self.regular_coeff * (1 - float(epoch - self.warmup_epochs) / (\n                (self.epochs - self.warmup_epochs) * self.regular_ratio)), 0)\n            loss_regular = self.mutator_small.reset_with_loss()\n            if loss_regular:\n                loss_regular *= reg_decay\n            logits_search, emsemble_logits_search = self.model_small(val_x)\n            logits_main, emsemble_logits_main = self.model_large(val_x)\n            loss_cls = (self.criterion(logits_search, val_y) + self.criterion(logits_main, val_y)) / self.loss_alpha\n            loss_interactive = self.interactive_loss(emsemble_logits_search, emsemble_logits_main) * (self.loss_T ** 2) * self.loss_alpha\n            loss = loss_cls + loss_interactive + loss_regular\n            loss.backward()\n            self._clip_grad_norm(self.model_large)\n            self.optimizer_large.step()\n            self.optimizer_alpha.step()\n            # NOTE: need to call here `self._reset_nan(self.mutator_small.parameters())` if `cut_choices`\n\n            # step 2. optimize op weights\n            self.optimizer_small.zero_grad()\n            with torch.no_grad():\n                # resample architecture since parameters have been changed\n                self.mutator_small.reset_with_loss()\n            logits_search_train, _ = self.model_small(trn_x)\n            loss_weight = self.criterion(logits_search_train, trn_y)\n            loss_weight.backward()\n            self._clip_grad_norm(self.model_small)\n            self.optimizer_small.step()\n\n            metrics = {\"loss_cls\": loss_cls, \"loss_interactive\": loss_interactive,\n                       \"loss_regular\": loss_regular, \"loss_weight\": loss_weight}\n            metrics = reduce_metrics(metrics, self.distributed)\n            meters.update(metrics)\n\n            if self.main_proc and (step % self.log_frequency == 0 or step + 1 == self.steps_per_epoch):\n                self.logger.info(\"Epoch [%d/%d] Step [%d/%d] (joint)  %s\", epoch + 1, self.epochs,\n                                 step + 1, self.steps_per_epoch, meters)",
  "def train(self):\n        for epoch in range(self.epochs):\n            if epoch < self.warmup_epochs:\n                with torch.no_grad():  # otherwise grads will be retained on the architecture params\n                    self.mutator_small.reset_with_loss()\n                self._warmup(PHASE_SMALL, epoch)\n            else:\n                with torch.no_grad():\n                    self.mutator_large.reset()\n                self._warmup(PHASE_LARGE, epoch)\n                self._joint_train(epoch)\n\n            self.export(os.path.join(self.checkpoint_dir, \"epoch_{:02d}.json\".format(epoch)),\n                        os.path.join(self.checkpoint_dir, \"epoch_{:02d}.genotypes\".format(epoch)))",
  "def export(self, file, genotype_file):\n        if self.main_proc:\n            mutator_export, genotypes = self.mutator_small.export(self.logger)\n            with open(file, \"w\") as f:\n                json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n            with open(genotype_file, \"w\") as f:\n                f.write(str(genotypes))",
  "class RandomMutator(Mutator):\n    \"\"\"\n    Random mutator that samples a random candidate in the search space each time ``reset()``.\n    It uses random function in PyTorch, so users can set seed in PyTorch to ensure deterministic behavior.\n    \"\"\"\n\n    def sample_search(self):\n        \"\"\"\n        Sample a random candidate.\n        \"\"\"\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                gen_index = torch.randint(high=len(mutable), size=(1, ))\n                result[mutable.key] = F.one_hot(gen_index, num_classes=len(mutable)).view(-1).bool()\n            elif isinstance(mutable, InputChoice):\n                if mutable.n_chosen is None:\n                    result[mutable.key] = torch.randint(high=2, size=(mutable.n_candidates,)).view(-1).bool()\n                else:\n                    perm = torch.randperm(mutable.n_candidates)\n                    mask = [i in perm[:mutable.n_chosen] for i in range(mutable.n_candidates)]\n                    result[mutable.key] = torch.tensor(mask, dtype=torch.bool)  # pylint: disable=not-callable\n        return result\n\n    def sample_final(self):\n        \"\"\"\n        Same as :meth:`sample_search`.\n        \"\"\"\n        return self.sample_search()",
  "def sample_search(self):\n        \"\"\"\n        Sample a random candidate.\n        \"\"\"\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                gen_index = torch.randint(high=len(mutable), size=(1, ))\n                result[mutable.key] = F.one_hot(gen_index, num_classes=len(mutable)).view(-1).bool()\n            elif isinstance(mutable, InputChoice):\n                if mutable.n_chosen is None:\n                    result[mutable.key] = torch.randint(high=2, size=(mutable.n_candidates,)).view(-1).bool()\n                else:\n                    perm = torch.randperm(mutable.n_candidates)\n                    mask = [i in perm[:mutable.n_chosen] for i in range(mutable.n_candidates)]\n                    result[mutable.key] = torch.tensor(mask, dtype=torch.bool)  # pylint: disable=not-callable\n        return result",
  "def sample_final(self):\n        \"\"\"\n        Same as :meth:`sample_search`.\n        \"\"\"\n        return self.sample_search()",
  "class CG_BOHB:\n    def __init__(self, configspace, min_points_in_model=None,\n                 top_n_percent=15, num_samples=64, random_fraction=1/3,\n                 bandwidth_factor=3, min_bandwidth=1e-3):\n        \"\"\"Fits for each given budget a kernel density estimator on the best N percent of the\n        evaluated configurations on this budget.\n\n\n        Parameters:\n        -----------\n        configspace: ConfigSpace\n            Configuration space object\n        top_n_percent: int\n            Determines the percentile of configurations that will be used as training data\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\n            for training.\n        min_points_in_model: int\n            minimum number of datapoints needed to fit a model\n        num_samples: int\n            number of samples drawn to optimize EI via sampling\n        random_fraction: float\n            fraction of random configurations returned\n        bandwidth_factor: float\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\n        min_bandwidth: float\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\n        \"\"\"\n        self.top_n_percent = top_n_percent\n        self.configspace = configspace\n        self.bw_factor = bandwidth_factor\n        self.min_bandwidth = min_bandwidth\n\n        self.min_points_in_model = min_points_in_model\n        if min_points_in_model is None:\n            self.min_points_in_model = len(self.configspace.get_hyperparameters())+1\n\n        if self.min_points_in_model < len(self.configspace.get_hyperparameters())+1:\n            logger.warning('Invalid min_points_in_model value. Setting it to %i', len(self.configspace.get_hyperparameters()) + 1)\n            self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n\n        self.num_samples = num_samples\n        self.random_fraction = random_fraction\n\n        hps = self.configspace.get_hyperparameters()\n\n        self.kde_vartypes = \"\"\n        self.vartypes = []\n\n        for h in hps:\n            if hasattr(h, 'choices'):\n                self.kde_vartypes += 'u'\n                self.vartypes += [len(h.choices)]\n            else:\n                self.kde_vartypes += 'c'\n                self.vartypes += [0]\n\n        self.vartypes = np.array(self.vartypes, dtype=int)\n\n        # store precomputed probs for the categorical parameters\n        self.cat_probs = []\n\n        self.configs = dict()\n        self.losses = dict()\n        self.good_config_rankings = dict()\n        self.kde_models = dict()\n\n    def largest_budget_with_model(self):\n        if not self.kde_models:\n            return -float('inf')\n        return max(self.kde_models.keys())\n\n    def sample_from_largest_budget(self, info_dict):\n        \"\"\"We opted for a single multidimensional KDE compared to the\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\n        seperated by budget. This function sample a configuration from\n        largest budget. Firstly we sample \"num_samples\" configurations,\n        then prefer one with the largest l(x)/g(x).\n\n        Parameters:\n        -----------\n        info_dict: dict\n            record the information of this configuration\n\n        Returns\n        -------\n        dict:\n            new configuration named sample\n        dict:\n            info_dict, record the information of this configuration\n        \"\"\"\n        best = np.inf\n        best_vector = None\n\n        budget = max(self.kde_models.keys())\n\n        l = self.kde_models[budget]['good'].pdf\n        g = self.kde_models[budget]['bad'].pdf\n\n        minimize_me = lambda x: max(1e-32, g(x))/max(l(x), 1e-32)\n\n        kde_good = self.kde_models[budget]['good']\n        kde_bad = self.kde_models[budget]['bad']\n\n        for i in range(self.num_samples):\n            idx = np.random.randint(0, len(kde_good.data))\n            datum = kde_good.data[idx]\n            vector = []\n\n            for m, bw, t in zip(datum, kde_good.bw, self.vartypes):\n\n                bw = max(bw, self.min_bandwidth)\n                if t == 0:\n                    bw = self.bw_factor*bw\n                    vector.append(sps.truncnorm.rvs(-m/bw, (1-m)/bw, loc=m, scale=bw))\n                else:\n                    if np.random.rand() < (1-bw):\n                        vector.append(int(m))\n                    else:\n                        vector.append(np.random.randint(t))\n            val = minimize_me(vector)\n\n            if not np.isfinite(val):\n                logger.warning('sampled vector: %s has EI value %s', vector, val)\n                logger.warning(\"data in the KDEs:\\n%s\\n%s\", kde_good.data, kde_bad.data)\n                logger.warning(\"bandwidth of the KDEs:\\n%s\\n%s\", kde_good.bw, kde_bad.bw)\n                logger.warning(\"l(x) = %s\", l(vector))\n                logger.warning(\"g(x) = %s\", g(vector))\n\n                # right now, this happens because a KDE does not contain all values for a categorical parameter\n                # this cannot be fixed with the statsmodels KDE, so for now, we are just going to evaluate this one\n                # if the good_kde has a finite value, i.e. there is no config with that value in the bad kde,\n                # so it shouldn't be terrible.\n                if np.isfinite(l(vector)):\n                    best_vector = vector\n                    break\n\n            if val < best:\n                best = val\n                best_vector = vector\n\n        if best_vector is None:\n            logger.debug(\"Sampling based optimization with %i samples failed -> using random configuration\", self.num_samples)\n            sample = self.configspace.sample_configuration().get_dictionary()\n            info_dict['model_based_pick'] = False\n\n        else:\n            logger.debug('best_vector: %s, %s, %s, %s', best_vector, best, l(best_vector), g(best_vector))\n            for i, _ in enumerate(best_vector):\n                hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n                if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                    best_vector[i] = int(np.rint(best_vector[i]))\n            sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n\n            sample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n                configuration_space=self.configspace,\n                configuration=sample)\n            info_dict['model_based_pick'] = True\n\n        return sample, info_dict\n\n    def get_config(self, budget):\n        \"\"\"Function to sample a new configuration\n        This function is called inside BOHB to query a new configuration\n\n        Parameters:\n        -----------\n        budget: float\n            the budget for which this configuration is scheduled\n\n        Returns\n        -------\n        config\n            return a valid configuration with parameters and budget\n        \"\"\"\n        logger.debug('start sampling a new configuration.')\n        sample = None\n        info_dict = {}\n\n        # If no model is available, sample from prior\n        # also mix in a fraction of random configs\n        if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n            sample = self.configspace.sample_configuration()\n            info_dict['model_based_pick'] = False\n\n        if sample is None:\n            sample, info_dict = self.sample_from_largest_budget(info_dict)\n\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n            configuration_space=self.configspace,\n            configuration=sample.get_dictionary()\n        ).get_dictionary()\n\n        logger.debug('done sampling a new configuration.')\n        sample['TRIAL_BUDGET'] = budget\n        return sample\n\n    def impute_conditional_data(self, array):\n        return_array = np.zeros(array.shape)\n        for i in range(array.shape[0]):\n            datum = np.copy(array[i])\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n            while np.any(nan_indices):\n                nan_idx = nan_indices[0]\n                valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n                if valid_indices:\n                    # pick one of them at random and overwrite all NaN values\n                    row_idx = np.random.choice(valid_indices)\n                    datum[nan_indices] = array[row_idx, nan_indices]\n                else:\n                    # no good point in the data has this value activated, so fill it with a valid but random value\n                    t = self.vartypes[nan_idx]\n                    if t == 0:\n                        datum[nan_idx] = np.random.rand()\n                    else:\n                        datum[nan_idx] = np.random.randint(t)\n                nan_indices = np.argwhere(np.isnan(datum)).flatten()\n            return_array[i, :] = datum\n        return return_array\n\n    def new_result(self, loss, budget, parameters, update_model=True):\n        \"\"\"\n        Function to register finished runs. Every time a run has finished, this function should be called\n        to register it with the loss.\n\n        Parameters:\n        -----------\n        loss: float\n            the loss of the parameters\n        budget: float\n            the budget of the parameters\n        parameters: dict\n            the parameters of this trial\n        update_model: bool\n            whether use this parameter to update BP model\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if loss is None:\n            # One could skip crashed results, but we decided\n            # assign a +inf loss and count them as bad configurations\n            loss = np.inf\n\n        if budget not in self.configs.keys():\n            self.configs[budget] = []\n            self.losses[budget] = []\n\n        # skip model building if we already have a bigger model\n        if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n            return\n\n        # We want to get a numerical representation of the configuration in the original space\n        conf = ConfigSpace.Configuration(self.configspace, parameters)\n        self.configs[budget].append(conf.get_array())\n        self.losses[budget].append(loss)\n\n        # skip model building:\n        # a) if not enough points are available\n        if len(self.configs[budget]) <= self.min_points_in_model - 1:\n            logger.debug(\"Only %i run(s) for budget %f available, need more than %s \\\n            -> can't build model!\", len(self.configs[budget]), budget, self.min_points_in_model+1)\n            return\n        # b) during warnm starting when we feed previous results in and only update once\n        if not update_model:\n            return\n\n        train_configs = np.array(self.configs[budget])\n        train_losses = np.array(self.losses[budget])\n\n        n_good = max(self.min_points_in_model, (self.top_n_percent * train_configs.shape[0])//100)\n        n_bad = max(self.min_points_in_model, ((100-self.top_n_percent)*train_configs.shape[0])//100)\n\n        # Refit KDE for the current budget\n        idx = np.argsort(train_losses)\n\n        train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n        train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good+n_bad]])\n\n        if train_data_good.shape[0] <= train_data_good.shape[1]:\n            return\n        if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n            return\n\n        #more expensive crossvalidation method\n        #bw_estimation = 'cv_ls'\n        # quick rule of thumb\n        bw_estimation = 'normal_reference'\n\n        bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n        good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n\n        bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n        good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n\n        self.kde_models[budget] = {\n            'good': good_kde,\n            'bad' : bad_kde\n        }\n\n        # update probs for the categorical parameters for later sampling\n        logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n',\n                     budget, n_good, n_bad, np.min(train_losses))",
  "def __init__(self, configspace, min_points_in_model=None,\n                 top_n_percent=15, num_samples=64, random_fraction=1/3,\n                 bandwidth_factor=3, min_bandwidth=1e-3):\n        \"\"\"Fits for each given budget a kernel density estimator on the best N percent of the\n        evaluated configurations on this budget.\n\n\n        Parameters:\n        -----------\n        configspace: ConfigSpace\n            Configuration space object\n        top_n_percent: int\n            Determines the percentile of configurations that will be used as training data\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\n            for training.\n        min_points_in_model: int\n            minimum number of datapoints needed to fit a model\n        num_samples: int\n            number of samples drawn to optimize EI via sampling\n        random_fraction: float\n            fraction of random configurations returned\n        bandwidth_factor: float\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\n        min_bandwidth: float\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\n        \"\"\"\n        self.top_n_percent = top_n_percent\n        self.configspace = configspace\n        self.bw_factor = bandwidth_factor\n        self.min_bandwidth = min_bandwidth\n\n        self.min_points_in_model = min_points_in_model\n        if min_points_in_model is None:\n            self.min_points_in_model = len(self.configspace.get_hyperparameters())+1\n\n        if self.min_points_in_model < len(self.configspace.get_hyperparameters())+1:\n            logger.warning('Invalid min_points_in_model value. Setting it to %i', len(self.configspace.get_hyperparameters()) + 1)\n            self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n\n        self.num_samples = num_samples\n        self.random_fraction = random_fraction\n\n        hps = self.configspace.get_hyperparameters()\n\n        self.kde_vartypes = \"\"\n        self.vartypes = []\n\n        for h in hps:\n            if hasattr(h, 'choices'):\n                self.kde_vartypes += 'u'\n                self.vartypes += [len(h.choices)]\n            else:\n                self.kde_vartypes += 'c'\n                self.vartypes += [0]\n\n        self.vartypes = np.array(self.vartypes, dtype=int)\n\n        # store precomputed probs for the categorical parameters\n        self.cat_probs = []\n\n        self.configs = dict()\n        self.losses = dict()\n        self.good_config_rankings = dict()\n        self.kde_models = dict()",
  "def largest_budget_with_model(self):\n        if not self.kde_models:\n            return -float('inf')\n        return max(self.kde_models.keys())",
  "def sample_from_largest_budget(self, info_dict):\n        \"\"\"We opted for a single multidimensional KDE compared to the\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\n        seperated by budget. This function sample a configuration from\n        largest budget. Firstly we sample \"num_samples\" configurations,\n        then prefer one with the largest l(x)/g(x).\n\n        Parameters:\n        -----------\n        info_dict: dict\n            record the information of this configuration\n\n        Returns\n        -------\n        dict:\n            new configuration named sample\n        dict:\n            info_dict, record the information of this configuration\n        \"\"\"\n        best = np.inf\n        best_vector = None\n\n        budget = max(self.kde_models.keys())\n\n        l = self.kde_models[budget]['good'].pdf\n        g = self.kde_models[budget]['bad'].pdf\n\n        minimize_me = lambda x: max(1e-32, g(x))/max(l(x), 1e-32)\n\n        kde_good = self.kde_models[budget]['good']\n        kde_bad = self.kde_models[budget]['bad']\n\n        for i in range(self.num_samples):\n            idx = np.random.randint(0, len(kde_good.data))\n            datum = kde_good.data[idx]\n            vector = []\n\n            for m, bw, t in zip(datum, kde_good.bw, self.vartypes):\n\n                bw = max(bw, self.min_bandwidth)\n                if t == 0:\n                    bw = self.bw_factor*bw\n                    vector.append(sps.truncnorm.rvs(-m/bw, (1-m)/bw, loc=m, scale=bw))\n                else:\n                    if np.random.rand() < (1-bw):\n                        vector.append(int(m))\n                    else:\n                        vector.append(np.random.randint(t))\n            val = minimize_me(vector)\n\n            if not np.isfinite(val):\n                logger.warning('sampled vector: %s has EI value %s', vector, val)\n                logger.warning(\"data in the KDEs:\\n%s\\n%s\", kde_good.data, kde_bad.data)\n                logger.warning(\"bandwidth of the KDEs:\\n%s\\n%s\", kde_good.bw, kde_bad.bw)\n                logger.warning(\"l(x) = %s\", l(vector))\n                logger.warning(\"g(x) = %s\", g(vector))\n\n                # right now, this happens because a KDE does not contain all values for a categorical parameter\n                # this cannot be fixed with the statsmodels KDE, so for now, we are just going to evaluate this one\n                # if the good_kde has a finite value, i.e. there is no config with that value in the bad kde,\n                # so it shouldn't be terrible.\n                if np.isfinite(l(vector)):\n                    best_vector = vector\n                    break\n\n            if val < best:\n                best = val\n                best_vector = vector\n\n        if best_vector is None:\n            logger.debug(\"Sampling based optimization with %i samples failed -> using random configuration\", self.num_samples)\n            sample = self.configspace.sample_configuration().get_dictionary()\n            info_dict['model_based_pick'] = False\n\n        else:\n            logger.debug('best_vector: %s, %s, %s, %s', best_vector, best, l(best_vector), g(best_vector))\n            for i, _ in enumerate(best_vector):\n                hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n                if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                    best_vector[i] = int(np.rint(best_vector[i]))\n            sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n\n            sample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n                configuration_space=self.configspace,\n                configuration=sample)\n            info_dict['model_based_pick'] = True\n\n        return sample, info_dict",
  "def get_config(self, budget):\n        \"\"\"Function to sample a new configuration\n        This function is called inside BOHB to query a new configuration\n\n        Parameters:\n        -----------\n        budget: float\n            the budget for which this configuration is scheduled\n\n        Returns\n        -------\n        config\n            return a valid configuration with parameters and budget\n        \"\"\"\n        logger.debug('start sampling a new configuration.')\n        sample = None\n        info_dict = {}\n\n        # If no model is available, sample from prior\n        # also mix in a fraction of random configs\n        if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n            sample = self.configspace.sample_configuration()\n            info_dict['model_based_pick'] = False\n\n        if sample is None:\n            sample, info_dict = self.sample_from_largest_budget(info_dict)\n\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n            configuration_space=self.configspace,\n            configuration=sample.get_dictionary()\n        ).get_dictionary()\n\n        logger.debug('done sampling a new configuration.')\n        sample['TRIAL_BUDGET'] = budget\n        return sample",
  "def impute_conditional_data(self, array):\n        return_array = np.zeros(array.shape)\n        for i in range(array.shape[0]):\n            datum = np.copy(array[i])\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n            while np.any(nan_indices):\n                nan_idx = nan_indices[0]\n                valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n                if valid_indices:\n                    # pick one of them at random and overwrite all NaN values\n                    row_idx = np.random.choice(valid_indices)\n                    datum[nan_indices] = array[row_idx, nan_indices]\n                else:\n                    # no good point in the data has this value activated, so fill it with a valid but random value\n                    t = self.vartypes[nan_idx]\n                    if t == 0:\n                        datum[nan_idx] = np.random.rand()\n                    else:\n                        datum[nan_idx] = np.random.randint(t)\n                nan_indices = np.argwhere(np.isnan(datum)).flatten()\n            return_array[i, :] = datum\n        return return_array",
  "def new_result(self, loss, budget, parameters, update_model=True):\n        \"\"\"\n        Function to register finished runs. Every time a run has finished, this function should be called\n        to register it with the loss.\n\n        Parameters:\n        -----------\n        loss: float\n            the loss of the parameters\n        budget: float\n            the budget of the parameters\n        parameters: dict\n            the parameters of this trial\n        update_model: bool\n            whether use this parameter to update BP model\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if loss is None:\n            # One could skip crashed results, but we decided\n            # assign a +inf loss and count them as bad configurations\n            loss = np.inf\n\n        if budget not in self.configs.keys():\n            self.configs[budget] = []\n            self.losses[budget] = []\n\n        # skip model building if we already have a bigger model\n        if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n            return\n\n        # We want to get a numerical representation of the configuration in the original space\n        conf = ConfigSpace.Configuration(self.configspace, parameters)\n        self.configs[budget].append(conf.get_array())\n        self.losses[budget].append(loss)\n\n        # skip model building:\n        # a) if not enough points are available\n        if len(self.configs[budget]) <= self.min_points_in_model - 1:\n            logger.debug(\"Only %i run(s) for budget %f available, need more than %s \\\n            -> can't build model!\", len(self.configs[budget]), budget, self.min_points_in_model+1)\n            return\n        # b) during warnm starting when we feed previous results in and only update once\n        if not update_model:\n            return\n\n        train_configs = np.array(self.configs[budget])\n        train_losses = np.array(self.losses[budget])\n\n        n_good = max(self.min_points_in_model, (self.top_n_percent * train_configs.shape[0])//100)\n        n_bad = max(self.min_points_in_model, ((100-self.top_n_percent)*train_configs.shape[0])//100)\n\n        # Refit KDE for the current budget\n        idx = np.argsort(train_losses)\n\n        train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n        train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good+n_bad]])\n\n        if train_data_good.shape[0] <= train_data_good.shape[1]:\n            return\n        if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n            return\n\n        #more expensive crossvalidation method\n        #bw_estimation = 'cv_ls'\n        # quick rule of thumb\n        bw_estimation = 'normal_reference'\n\n        bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n        good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n\n        bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n        good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n\n        self.kde_models[budget] = {\n            'good': good_kde,\n            'bad' : bad_kde\n        }\n\n        # update probs for the categorical parameters for later sampling\n        logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n',\n                     budget, n_good, n_bad, np.min(train_losses))",
  "def create_parameter_id():\n    \"\"\"Create an id\n\n    Returns\n    -------\n    int\n        parameter id\n    \"\"\"\n    global _next_parameter_id\n    _next_parameter_id += 1\n    return _next_parameter_id - 1",
  "def create_bracket_parameter_id(brackets_id, brackets_curr_decay, increased_id=-1):\n    \"\"\"Create a full id for a specific bracket's hyperparameter configuration\n\n    Parameters\n    ----------\n    brackets_id: int\n        brackets id\n    brackets_curr_decay: int\n        brackets curr decay\n    increased_id: int\n        increased id\n    Returns\n    -------\n    int\n        params id\n    \"\"\"\n    if increased_id == -1:\n        increased_id = str(create_parameter_id())\n    params_id = '_'.join([str(brackets_id),\n                          str(brackets_curr_decay),\n                          increased_id])\n    return params_id",
  "class Bracket:\n    \"\"\"\n    A bracket in BOHB, all the information of a bracket is managed by\n    an instance of this class.\n\n    Parameters\n    ----------\n    s: int\n        The current Successive Halving iteration index.\n    s_max: int\n        total number of Successive Halving iterations\n    eta: float\n        In each iteration, a complete run of sequential halving is executed. In it,\n\t\tafter evaluating each configuration on the same subset size, only a fraction of\n\t\t1/eta of them 'advances' to the next round.\n\tmax_budget : float\n\t\tThe largest budget to consider. Needs to be larger than min_budget!\n\t\tThe budgets will be geometrically distributed\n        :math:`a^2 + b^2 = c^2 \\\\sim \\\\eta^k` for :math:`k\\\\in [0, 1, ... , num\\\\_subsets - 1]`.\n    optimize_mode: str\n        optimize mode, 'maximize' or 'minimize'\n    \"\"\"\n    def __init__(self, s, s_max, eta, max_budget, optimize_mode):\n        self.s = s\n        self.s_max = s_max\n        self.eta = eta\n        self.max_budget = max_budget\n        self.optimize_mode = OptimizeMode(optimize_mode)\n\n        self.n = math.ceil((s_max + 1) * eta**s / (s + 1) - _epsilon)\n        self.r = max_budget / eta**s\n        self.i = 0\n        self.hyper_configs = []         # [ {id: params}, {}, ... ]\n        self.configs_perf = []          # [ {id: [seq, acc]}, {}, ... ]\n        self.num_configs_to_run = []    # [ n, n, n, ... ]\n        self.num_finished_configs = []  # [ n, n, n, ... ]\n        self.no_more_trial = False\n\n    def is_completed(self):\n        \"\"\"check whether this bracket has sent out all the hyperparameter configurations\"\"\"\n        return self.no_more_trial\n\n    def get_n_r(self):\n        \"\"\"return the values of n and r for the next round\"\"\"\n        return math.floor(self.n / self.eta**self.i + _epsilon), math.floor(self.r * self.eta**self.i +_epsilon)\n\n    def increase_i(self):\n        \"\"\"i means the ith round. Increase i by 1\"\"\"\n        self.i += 1\n\n    def set_config_perf(self, i, parameter_id, seq, value):\n        \"\"\"update trial's latest result with its sequence number, e.g., epoch number or batch number\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        parameter_id: int\n            the id of the trial/parameter\n        seq: int\n            sequence number, e.g., epoch number or batch number\n        value: int\n            latest result with sequence number seq\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if parameter_id in self.configs_perf[i]:\n            if self.configs_perf[i][parameter_id][0] < seq:\n                self.configs_perf[i][parameter_id] = [seq, value]\n        else:\n            self.configs_perf[i][parameter_id] = [seq, value]\n\n    def inform_trial_end(self, i):\n        \"\"\"If the trial is finished and the corresponding round (i.e., i) has all its trials finished,\n        it will choose the top k trials for the next round (i.e., i+1)\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n\n        Returns\n        -------\n        new trial or None:\n            If we have generated new trials after this trial end, we will return a new trial parameters.\n            Otherwise, we will return None.\n        \"\"\"\n        global _KEY\n        self.num_finished_configs[i] += 1\n        logger.debug('bracket id: %d, round: %d %d, finished: %d, all: %d',\n                     self.s, self.i, i, self.num_finished_configs[i], self.num_configs_to_run[i])\n        if self.num_finished_configs[i] >= self.num_configs_to_run[i] and self.no_more_trial is False:\n            # choose candidate configs from finished configs to run in the next round\n            assert self.i == i + 1\n            # finish this bracket\n            if self.i > self.s:\n                self.no_more_trial = True\n                return None\n            this_round_perf = self.configs_perf[i]\n            if self.optimize_mode is OptimizeMode.Maximize:\n                sorted_perf = sorted(this_round_perf.items(\n                ), key=lambda kv: kv[1][1], reverse=True)  # reverse\n            else:\n                sorted_perf = sorted(\n                    this_round_perf.items(), key=lambda kv: kv[1][1])\n            logger.debug(\n                'bracket %s next round %s, sorted hyper configs: %s', self.s, self.i, sorted_perf)\n            next_n, next_r = self.get_n_r()\n            logger.debug('bracket %s next round %s, next_n=%d, next_r=%d',\n                         self.s, self.i, next_n, next_r)\n            hyper_configs = dict()\n            for k in range(next_n):\n                params_id = sorted_perf[k][0]\n                params = self.hyper_configs[i][params_id]\n                params[_KEY] = next_r  # modify r\n                # generate new id\n                increased_id = params_id.split('_')[-1]\n                new_id = create_bracket_parameter_id(\n                    self.s, self.i, increased_id)\n                hyper_configs[new_id] = params\n            self._record_hyper_configs(hyper_configs)\n            return [[key, value] for key, value in hyper_configs.items()]\n        return None\n\n    def get_hyperparameter_configurations(self, num, r, config_generator):\n        \"\"\"generate num hyperparameter configurations from search space using Bayesian optimization\n\n        Parameters\n        ----------\n        num: int\n            the number of hyperparameter configurations\n\n        Returns\n        -------\n        list\n            a list of hyperparameter configurations. Format: [[key1, value1], [key2, value2], ...]\n        \"\"\"\n        global _KEY\n        assert self.i == 0\n        hyperparameter_configs = dict()\n        for _ in range(num):\n            params_id = create_bracket_parameter_id(self.s, self.i)\n            params = config_generator.get_config(r)\n            params[_KEY] = r\n            hyperparameter_configs[params_id] = params\n        self._record_hyper_configs(hyperparameter_configs)\n        return [[key, value] for key, value in hyperparameter_configs.items()]\n\n    def _record_hyper_configs(self, hyper_configs):\n        \"\"\"after generating one round of hyperconfigs, this function records the generated hyperconfigs,\n        creates a dict to record the performance when those hyperconifgs are running, set the number of finished configs\n        in this round to be 0, and increase the round number.\n\n        Parameters\n        ----------\n        hyper_configs: list\n            the generated hyperconfigs\n        \"\"\"\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()",
  "class BOHBClassArgsValidator(ClassArgsValidator):\n    def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('min_budget'): self.range('min_budget', int, 0, 9999),\n            Optional('max_budget'): self.range('max_budget', int, 0, 9999),\n            Optional('eta'): self.range('eta', int, 0, 9999),\n            Optional('min_points_in_model'): self.range('min_points_in_model', int, 0, 9999),\n            Optional('top_n_percent'): self.range('top_n_percent', int, 1, 99),\n            Optional('num_samples'): self.range('num_samples', int, 1, 9999),\n            Optional('random_fraction'): self.range('random_fraction', float, 0, 9999),\n            Optional('bandwidth_factor'): self.range('bandwidth_factor', float, 0, 9999),\n            Optional('min_bandwidth'): self.range('min_bandwidth', float, 0, 9999),\n        }).validate(kwargs)",
  "class BOHB(MsgDispatcherBase):\n    \"\"\"\n    BOHB performs robust and efficient hyperparameter optimization\n    at scale by combining the speed of Hyperband searches with the\n    guidance and guarantees of convergence of Bayesian Optimization.\n    Instead of sampling new configurations at random, BOHB uses\n    kernel density estimators to select promising candidates.\n\n    Parameters\n    ----------\n    optimize_mode: str\n        optimize mode, 'maximize' or 'minimize'\n    min_budget: float\n        The smallest budget to consider. Needs to be positive!\n    max_budget: float\n        The largest budget to consider. Needs to be larger than min_budget!\n        The budgets will be geometrically distributed\n        :math:`a^2 + b^2 = c^2 \\\\sim \\\\eta^k` for :math:`k\\\\in [0, 1, ... , num\\\\_subsets - 1]`.\n    eta: int\n        In each iteration, a complete run of sequential halving is executed. In it,\n        after evaluating each configuration on the same subset size, only a fraction of\n        1/eta of them 'advances' to the next round.\n        Must be greater or equal to 2.\n    min_points_in_model: int\n        number of observations to start building a KDE. Default 'None' means\n        dim+1, the bare minimum.\n    top_n_percent: int\n        percentage ( between 1 and 99, default 15) of the observations that are considered good.\n    num_samples: int\n        number of samples to optimize EI (default 64)\n    random_fraction: float\n    fraction of purely random configurations that are sampled from the\n        prior without the model.\n    bandwidth_factor: float\n        to encourage diversity, the points proposed to optimize EI, are sampled\n        from a 'widened' KDE where the bandwidth is multiplied by this factor (default: 3)\n    min_bandwidth: float\n        to keep diversity, even when all (good) samples have the same value for one of the parameters,\n        a minimum bandwidth (Default: 1e-3) is used instead of zero.\n    \"\"\"\n\n    def __init__(self,\n                 optimize_mode='maximize',\n                 min_budget=1,\n                 max_budget=3,\n                 eta=3,\n                 min_points_in_model=None,\n                 top_n_percent=15,\n                 num_samples=64,\n                 random_fraction=1/3,\n                 bandwidth_factor=3,\n                 min_bandwidth=1e-3):\n        super(BOHB, self).__init__()\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.min_budget = min_budget\n        self.max_budget = max_budget\n        self.eta = eta\n        self.min_points_in_model = min_points_in_model\n        self.top_n_percent = top_n_percent\n        self.num_samples = num_samples\n        self.random_fraction = random_fraction\n        self.bandwidth_factor = bandwidth_factor\n        self.min_bandwidth = min_bandwidth\n\n        # all the configs waiting for run\n        self.generated_hyper_configs = []\n        # all the completed configs\n        self.completed_hyper_configs = []\n\n        self.s_max = math.floor(\n            math.log(self.max_budget / self.min_budget, self.eta) + _epsilon)\n        # current bracket(s) number\n        self.curr_s = self.s_max\n        # In this case, tuner increases self.credit to issue a trial config sometime later.\n        self.credit = 0\n        self.brackets = dict()\n        self.search_space = None\n        # [key, value] = [parameter_id, parameter]\n        self.parameters = dict()\n\n        # config generator\n        self.cg = None\n\n        # record the latest parameter_id of the trial job trial_job_id.\n        # if there is no running parameter_id, self.job_id_para_id_map[trial_job_id] == None\n        # new trial job is added to this dict and finished trial job is removed from it.\n        self.job_id_para_id_map = dict()\n        # record the unsatisfied parameter request from trial jobs\n        self.unsatisfied_jobs = []\n\n    def handle_initialize(self, data):\n        \"\"\"Initialize Tuner, including creating Bayesian optimization-based parametric models\n        and search space formations\n\n        Parameters\n        ----------\n        data: search space\n            search space of this experiment\n\n        Raises\n        ------\n        ValueError\n            Error: Search space is None\n        \"\"\"\n        logger.info('start to handle_initialize')\n        # convert search space jason to ConfigSpace\n        self.handle_update_search_space(data)\n\n        # generate BOHB config_generator using Bayesian optimization\n        if self.search_space:\n            self.cg = CG_BOHB(configspace=self.search_space,\n                              min_points_in_model=self.min_points_in_model,\n                              top_n_percent=self.top_n_percent,\n                              num_samples=self.num_samples,\n                              random_fraction=self.random_fraction,\n                              bandwidth_factor=self.bandwidth_factor,\n                              min_bandwidth=self.min_bandwidth)\n        else:\n            raise ValueError('Error: Search space is None')\n        # generate first brackets\n        self.generate_new_bracket()\n        send(CommandType.Initialized, '')\n\n    def generate_new_bracket(self):\n        \"\"\"generate a new bracket\"\"\"\n        logger.debug(\n            'start to create a new SuccessiveHalving iteration, self.curr_s=%d', self.curr_s)\n        if self.curr_s < 0:\n            logger.info(\"s < 0, Finish this round of Hyperband in BOHB. Generate new round\")\n            self.curr_s = self.s_max\n        self.brackets[self.curr_s] = Bracket(\n            s=self.curr_s, s_max=self.s_max, eta=self.eta,\n            max_budget=self.max_budget, optimize_mode=self.optimize_mode\n        )\n        next_n, next_r = self.brackets[self.curr_s].get_n_r()\n        logger.debug(\n            'new SuccessiveHalving iteration, next_n=%d, next_r=%d', next_n, next_r)\n        # rewrite with TPE\n        generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(\n            next_n, next_r, self.cg)\n        self.generated_hyper_configs = generated_hyper_configs.copy()\n\n    def handle_request_trial_jobs(self, data):\n        \"\"\"recerive the number of request and generate trials\n\n        Parameters\n        ----------\n        data: int\n            number of trial jobs that nni manager ask to generate\n        \"\"\"\n        # Receive new request\n        self.credit += data\n\n        for _ in range(self.credit):\n            self._request_one_trial_job()\n\n    def _get_one_trial_job(self):\n        \"\"\"get one trial job, i.e., one hyperparameter configuration.\n\n        If this function is called, Command will be sent by BOHB:\n        a. If there is a parameter need to run, will return \"NewTrialJob\" with a dict:\n        {\n            'parameter_id': id of new hyperparameter\n            'parameter_source': 'algorithm'\n            'parameters': value of new hyperparameter\n        }\n        b. If BOHB don't have parameter waiting, will return \"NoMoreTrialJobs\" with\n        {\n            'parameter_id': '-1_0_0',\n            'parameter_source': 'algorithm',\n            'parameters': ''\n        }\n        \"\"\"\n        if not self.generated_hyper_configs:\n            ret = {\n                'parameter_id': '-1_0_0',\n                'parameter_source': 'algorithm',\n                'parameters': ''\n            }\n            send(CommandType.NoMoreTrialJobs, json_tricks.dumps(ret))\n            return None\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop(0)\n        ret = {\n            'parameter_id': params[0],\n            'parameter_source': 'algorithm',\n            'parameters': params[1]\n        }\n        self.parameters[params[0]] = params[1]\n        return ret\n\n    def _request_one_trial_job(self):\n        \"\"\"get one trial job, i.e., one hyperparameter configuration.\n\n        If this function is called, Command will be sent by BOHB:\n        a. If there is a parameter need to run, will return \"NewTrialJob\" with a dict:\n        {\n            'parameter_id': id of new hyperparameter\n            'parameter_source': 'algorithm'\n            'parameters': value of new hyperparameter\n        }\n        b. If BOHB don't have parameter waiting, will return \"NoMoreTrialJobs\" with\n        {\n            'parameter_id': '-1_0_0',\n            'parameter_source': 'algorithm',\n            'parameters': ''\n        }\n        \"\"\"\n        ret = self._get_one_trial_job()\n        if ret is not None:\n            send(CommandType.NewTrialJob, json_tricks.dumps(ret))\n            self.credit -= 1\n\n    def handle_update_search_space(self, data):\n        \"\"\"change json format to ConfigSpace format dict<dict> -> configspace\n\n        Parameters\n        ----------\n        data: JSON object\n            search space of this experiment\n        \"\"\"\n        search_space = data\n        cs = CS.ConfigurationSpace()\n        for var in search_space:\n            _type = str(search_space[var][\"_type\"])\n            if _type == 'choice':\n                cs.add_hyperparameter(CSH.CategoricalHyperparameter(\n                    var, choices=search_space[var][\"_value\"]))\n            elif _type == 'randint':\n                cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1] - 1))\n            elif _type == 'uniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1]))\n            elif _type == 'quniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1],\n                    q=search_space[var][\"_value\"][2]))\n            elif _type == 'loguniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1],\n                    log=True))\n            elif _type == 'qloguniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1],\n                    q=search_space[var][\"_value\"][2], log=True))\n            elif _type == 'normal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2]))\n            elif _type == 'qnormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2],\n                    q=search_space[var][\"_value\"][3]))\n            elif _type == 'lognormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2],\n                    log=True))\n            elif _type == 'qlognormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2],\n                    q=search_space[var][\"_value\"][3], log=True))\n            else:\n                raise ValueError(\n                    'unrecognized type in search_space, type is {}'.format(_type))\n\n        self.search_space = cs\n\n    def handle_trial_end(self, data):\n        \"\"\"receive the information of trial end and generate next configuaration.\n\n        Parameters\n        ----------\n        data: dict()\n            it has three keys: trial_job_id, event, hyper_params\n            trial_job_id: the id generated by training service\n            event: the job's state\n            hyper_params: the hyperparameters (a string) generated and returned by tuner\n        \"\"\"\n        logger.debug('Tuner handle trial end, result is %s', data)\n        hyper_params = json_tricks.loads(data['hyper_params'])\n        self._handle_trial_end(hyper_params['parameter_id'])\n        if data['trial_job_id'] in self.job_id_para_id_map:\n            del self.job_id_para_id_map[data['trial_job_id']]\n\n    def _send_new_trial(self):\n        while self.unsatisfied_jobs:\n            ret = self._get_one_trial_job()\n            if ret is None:\n                break\n            one_unsatisfied = self.unsatisfied_jobs.pop(0)\n            ret['trial_job_id'] = one_unsatisfied['trial_job_id']\n            ret['parameter_index'] = one_unsatisfied['parameter_index']\n            # update parameter_id in self.job_id_para_id_map\n            self.job_id_para_id_map[ret['trial_job_id']] = ret['parameter_id']\n            send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        for _ in range(self.credit):\n            self._request_one_trial_job()\n\n    def _handle_trial_end(self, parameter_id):\n        s, i, _ = parameter_id.split('_')\n        hyper_configs = self.brackets[int(s)].inform_trial_end(int(i))\n\n        if hyper_configs is not None:\n            logger.debug(\n                'bracket %s next round %s, hyper_configs: %s', s, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs\n        # Finish this bracket and generate a new bracket\n        elif self.brackets[int(s)].no_more_trial:\n            self.curr_s -= 1\n            self.generate_new_bracket()\n        self._send_new_trial()\n\n    def handle_report_metric_data(self, data):\n        \"\"\"reveice the metric data and update Bayesian optimization with final result\n\n        Parameters\n        ----------\n        data:\n            it is an object which has keys 'parameter_id', 'value', 'trial_job_id', 'type', 'sequence'.\n\n        Raises\n        ------\n        ValueError\n            Data type not supported\n        \"\"\"\n        logger.debug('handle report metric data = %s', data)\n        if 'value' in data:\n            data['value'] = json_tricks.loads(data['value'])\n        if data['type'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data['trial_job_id'] is not None\n            assert data['parameter_index'] is not None\n            assert data['trial_job_id'] in self.job_id_para_id_map\n            self._handle_trial_end(self.job_id_para_id_map[data['trial_job_id']])\n            ret = self._get_one_trial_job()\n            if ret is None:\n                self.unsatisfied_jobs.append({'trial_job_id': data['trial_job_id'], 'parameter_index': data['parameter_index']})\n            else:\n                ret['trial_job_id'] = data['trial_job_id']\n                ret['parameter_index'] = data['parameter_index']\n                # update parameter_id in self.job_id_para_id_map\n                self.job_id_para_id_map[data['trial_job_id']] = ret['parameter_id']\n                send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        else:\n            assert 'value' in data\n            value = extract_scalar_reward(data['value'])\n            if self.optimize_mode is OptimizeMode.Maximize:\n                reward = -value\n            else:\n                reward = value\n            assert 'parameter_id' in data\n            s, i, _ = data['parameter_id'].split('_')\n            logger.debug('bracket id = %s, metrics value = %s, type = %s', s, value, data['type'])\n            s = int(s)\n\n            # add <trial_job_id, parameter_id> to self.job_id_para_id_map here,\n            # because when the first parameter_id is created, trial_job_id is not known yet.\n            if data['trial_job_id'] in self.job_id_para_id_map:\n                assert self.job_id_para_id_map[data['trial_job_id']] == data['parameter_id']\n            else:\n                self.job_id_para_id_map[data['trial_job_id']] = data['parameter_id']\n\n            assert 'type' in data\n            if data['type'] == MetricType.FINAL:\n                # and PERIODICAL metric are independent, thus, not comparable.\n                assert 'sequence' in data\n                self.brackets[s].set_config_perf(\n                    int(i), data['parameter_id'], sys.maxsize, value)\n                self.completed_hyper_configs.append(data)\n\n                _parameters = self.parameters[data['parameter_id']]\n                _parameters.pop(_KEY)\n                # update BO with loss, max_s budget, hyperparameters\n                self.cg.new_result(loss=reward, budget=data['sequence'], parameters=_parameters, update_model=True)\n            elif data['type'] == MetricType.PERIODICAL:\n                self.brackets[s].set_config_perf(\n                    int(i), data['parameter_id'], data['sequence'], value)\n            else:\n                raise ValueError(\n                    'Data type not supported: {}'.format(data['type']))\n\n    def handle_add_customized_trial(self, data):\n        pass\n\n    def handle_import_data(self, data):\n        \"\"\"Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n\n        Raises\n        ------\n        AssertionError\n            data doesn't have required key 'parameter' and 'value'\n        \"\"\"\n        for entry in data:\n            entry['value'] = json_tricks.loads(entry['value'])\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            _value = extract_scalar_reward(_value)\n            budget_exist_flag = False\n            barely_params = dict()\n            for keys in _params:\n                if keys == _KEY:\n                    _budget = _params[keys]\n                    budget_exist_flag = True\n                else:\n                    barely_params[keys] = _params[keys]\n            if not budget_exist_flag:\n                _budget = self.max_budget\n                logger.info(\"Set \\\"TRIAL_BUDGET\\\" value to %s (max budget)\", self.max_budget)\n            if self.optimize_mode is OptimizeMode.Maximize:\n                reward = -_value\n            else:\n                reward = _value\n            self.cg.new_result(loss=reward, budget=_budget, parameters=barely_params, update_model=True)\n        logger.info(\"Successfully import tuning data to BOHB advisor.\")",
  "def __init__(self, s, s_max, eta, max_budget, optimize_mode):\n        self.s = s\n        self.s_max = s_max\n        self.eta = eta\n        self.max_budget = max_budget\n        self.optimize_mode = OptimizeMode(optimize_mode)\n\n        self.n = math.ceil((s_max + 1) * eta**s / (s + 1) - _epsilon)\n        self.r = max_budget / eta**s\n        self.i = 0\n        self.hyper_configs = []         # [ {id: params}, {}, ... ]\n        self.configs_perf = []          # [ {id: [seq, acc]}, {}, ... ]\n        self.num_configs_to_run = []    # [ n, n, n, ... ]\n        self.num_finished_configs = []  # [ n, n, n, ... ]\n        self.no_more_trial = False",
  "def is_completed(self):\n        \"\"\"check whether this bracket has sent out all the hyperparameter configurations\"\"\"\n        return self.no_more_trial",
  "def get_n_r(self):\n        \"\"\"return the values of n and r for the next round\"\"\"\n        return math.floor(self.n / self.eta**self.i + _epsilon), math.floor(self.r * self.eta**self.i +_epsilon)",
  "def increase_i(self):\n        \"\"\"i means the ith round. Increase i by 1\"\"\"\n        self.i += 1",
  "def set_config_perf(self, i, parameter_id, seq, value):\n        \"\"\"update trial's latest result with its sequence number, e.g., epoch number or batch number\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        parameter_id: int\n            the id of the trial/parameter\n        seq: int\n            sequence number, e.g., epoch number or batch number\n        value: int\n            latest result with sequence number seq\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if parameter_id in self.configs_perf[i]:\n            if self.configs_perf[i][parameter_id][0] < seq:\n                self.configs_perf[i][parameter_id] = [seq, value]\n        else:\n            self.configs_perf[i][parameter_id] = [seq, value]",
  "def inform_trial_end(self, i):\n        \"\"\"If the trial is finished and the corresponding round (i.e., i) has all its trials finished,\n        it will choose the top k trials for the next round (i.e., i+1)\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n\n        Returns\n        -------\n        new trial or None:\n            If we have generated new trials after this trial end, we will return a new trial parameters.\n            Otherwise, we will return None.\n        \"\"\"\n        global _KEY\n        self.num_finished_configs[i] += 1\n        logger.debug('bracket id: %d, round: %d %d, finished: %d, all: %d',\n                     self.s, self.i, i, self.num_finished_configs[i], self.num_configs_to_run[i])\n        if self.num_finished_configs[i] >= self.num_configs_to_run[i] and self.no_more_trial is False:\n            # choose candidate configs from finished configs to run in the next round\n            assert self.i == i + 1\n            # finish this bracket\n            if self.i > self.s:\n                self.no_more_trial = True\n                return None\n            this_round_perf = self.configs_perf[i]\n            if self.optimize_mode is OptimizeMode.Maximize:\n                sorted_perf = sorted(this_round_perf.items(\n                ), key=lambda kv: kv[1][1], reverse=True)  # reverse\n            else:\n                sorted_perf = sorted(\n                    this_round_perf.items(), key=lambda kv: kv[1][1])\n            logger.debug(\n                'bracket %s next round %s, sorted hyper configs: %s', self.s, self.i, sorted_perf)\n            next_n, next_r = self.get_n_r()\n            logger.debug('bracket %s next round %s, next_n=%d, next_r=%d',\n                         self.s, self.i, next_n, next_r)\n            hyper_configs = dict()\n            for k in range(next_n):\n                params_id = sorted_perf[k][0]\n                params = self.hyper_configs[i][params_id]\n                params[_KEY] = next_r  # modify r\n                # generate new id\n                increased_id = params_id.split('_')[-1]\n                new_id = create_bracket_parameter_id(\n                    self.s, self.i, increased_id)\n                hyper_configs[new_id] = params\n            self._record_hyper_configs(hyper_configs)\n            return [[key, value] for key, value in hyper_configs.items()]\n        return None",
  "def get_hyperparameter_configurations(self, num, r, config_generator):\n        \"\"\"generate num hyperparameter configurations from search space using Bayesian optimization\n\n        Parameters\n        ----------\n        num: int\n            the number of hyperparameter configurations\n\n        Returns\n        -------\n        list\n            a list of hyperparameter configurations. Format: [[key1, value1], [key2, value2], ...]\n        \"\"\"\n        global _KEY\n        assert self.i == 0\n        hyperparameter_configs = dict()\n        for _ in range(num):\n            params_id = create_bracket_parameter_id(self.s, self.i)\n            params = config_generator.get_config(r)\n            params[_KEY] = r\n            hyperparameter_configs[params_id] = params\n        self._record_hyper_configs(hyperparameter_configs)\n        return [[key, value] for key, value in hyperparameter_configs.items()]",
  "def _record_hyper_configs(self, hyper_configs):\n        \"\"\"after generating one round of hyperconfigs, this function records the generated hyperconfigs,\n        creates a dict to record the performance when those hyperconifgs are running, set the number of finished configs\n        in this round to be 0, and increase the round number.\n\n        Parameters\n        ----------\n        hyper_configs: list\n            the generated hyperconfigs\n        \"\"\"\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()",
  "def validate_class_args(self, **kwargs):\n        Schema({\n            'optimize_mode': self.choices('optimize_mode', 'maximize', 'minimize'),\n            Optional('min_budget'): self.range('min_budget', int, 0, 9999),\n            Optional('max_budget'): self.range('max_budget', int, 0, 9999),\n            Optional('eta'): self.range('eta', int, 0, 9999),\n            Optional('min_points_in_model'): self.range('min_points_in_model', int, 0, 9999),\n            Optional('top_n_percent'): self.range('top_n_percent', int, 1, 99),\n            Optional('num_samples'): self.range('num_samples', int, 1, 9999),\n            Optional('random_fraction'): self.range('random_fraction', float, 0, 9999),\n            Optional('bandwidth_factor'): self.range('bandwidth_factor', float, 0, 9999),\n            Optional('min_bandwidth'): self.range('min_bandwidth', float, 0, 9999),\n        }).validate(kwargs)",
  "def __init__(self,\n                 optimize_mode='maximize',\n                 min_budget=1,\n                 max_budget=3,\n                 eta=3,\n                 min_points_in_model=None,\n                 top_n_percent=15,\n                 num_samples=64,\n                 random_fraction=1/3,\n                 bandwidth_factor=3,\n                 min_bandwidth=1e-3):\n        super(BOHB, self).__init__()\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.min_budget = min_budget\n        self.max_budget = max_budget\n        self.eta = eta\n        self.min_points_in_model = min_points_in_model\n        self.top_n_percent = top_n_percent\n        self.num_samples = num_samples\n        self.random_fraction = random_fraction\n        self.bandwidth_factor = bandwidth_factor\n        self.min_bandwidth = min_bandwidth\n\n        # all the configs waiting for run\n        self.generated_hyper_configs = []\n        # all the completed configs\n        self.completed_hyper_configs = []\n\n        self.s_max = math.floor(\n            math.log(self.max_budget / self.min_budget, self.eta) + _epsilon)\n        # current bracket(s) number\n        self.curr_s = self.s_max\n        # In this case, tuner increases self.credit to issue a trial config sometime later.\n        self.credit = 0\n        self.brackets = dict()\n        self.search_space = None\n        # [key, value] = [parameter_id, parameter]\n        self.parameters = dict()\n\n        # config generator\n        self.cg = None\n\n        # record the latest parameter_id of the trial job trial_job_id.\n        # if there is no running parameter_id, self.job_id_para_id_map[trial_job_id] == None\n        # new trial job is added to this dict and finished trial job is removed from it.\n        self.job_id_para_id_map = dict()\n        # record the unsatisfied parameter request from trial jobs\n        self.unsatisfied_jobs = []",
  "def handle_initialize(self, data):\n        \"\"\"Initialize Tuner, including creating Bayesian optimization-based parametric models\n        and search space formations\n\n        Parameters\n        ----------\n        data: search space\n            search space of this experiment\n\n        Raises\n        ------\n        ValueError\n            Error: Search space is None\n        \"\"\"\n        logger.info('start to handle_initialize')\n        # convert search space jason to ConfigSpace\n        self.handle_update_search_space(data)\n\n        # generate BOHB config_generator using Bayesian optimization\n        if self.search_space:\n            self.cg = CG_BOHB(configspace=self.search_space,\n                              min_points_in_model=self.min_points_in_model,\n                              top_n_percent=self.top_n_percent,\n                              num_samples=self.num_samples,\n                              random_fraction=self.random_fraction,\n                              bandwidth_factor=self.bandwidth_factor,\n                              min_bandwidth=self.min_bandwidth)\n        else:\n            raise ValueError('Error: Search space is None')\n        # generate first brackets\n        self.generate_new_bracket()\n        send(CommandType.Initialized, '')",
  "def generate_new_bracket(self):\n        \"\"\"generate a new bracket\"\"\"\n        logger.debug(\n            'start to create a new SuccessiveHalving iteration, self.curr_s=%d', self.curr_s)\n        if self.curr_s < 0:\n            logger.info(\"s < 0, Finish this round of Hyperband in BOHB. Generate new round\")\n            self.curr_s = self.s_max\n        self.brackets[self.curr_s] = Bracket(\n            s=self.curr_s, s_max=self.s_max, eta=self.eta,\n            max_budget=self.max_budget, optimize_mode=self.optimize_mode\n        )\n        next_n, next_r = self.brackets[self.curr_s].get_n_r()\n        logger.debug(\n            'new SuccessiveHalving iteration, next_n=%d, next_r=%d', next_n, next_r)\n        # rewrite with TPE\n        generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(\n            next_n, next_r, self.cg)\n        self.generated_hyper_configs = generated_hyper_configs.copy()",
  "def handle_request_trial_jobs(self, data):\n        \"\"\"recerive the number of request and generate trials\n\n        Parameters\n        ----------\n        data: int\n            number of trial jobs that nni manager ask to generate\n        \"\"\"\n        # Receive new request\n        self.credit += data\n\n        for _ in range(self.credit):\n            self._request_one_trial_job()",
  "def _get_one_trial_job(self):\n        \"\"\"get one trial job, i.e., one hyperparameter configuration.\n\n        If this function is called, Command will be sent by BOHB:\n        a. If there is a parameter need to run, will return \"NewTrialJob\" with a dict:\n        {\n            'parameter_id': id of new hyperparameter\n            'parameter_source': 'algorithm'\n            'parameters': value of new hyperparameter\n        }\n        b. If BOHB don't have parameter waiting, will return \"NoMoreTrialJobs\" with\n        {\n            'parameter_id': '-1_0_0',\n            'parameter_source': 'algorithm',\n            'parameters': ''\n        }\n        \"\"\"\n        if not self.generated_hyper_configs:\n            ret = {\n                'parameter_id': '-1_0_0',\n                'parameter_source': 'algorithm',\n                'parameters': ''\n            }\n            send(CommandType.NoMoreTrialJobs, json_tricks.dumps(ret))\n            return None\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop(0)\n        ret = {\n            'parameter_id': params[0],\n            'parameter_source': 'algorithm',\n            'parameters': params[1]\n        }\n        self.parameters[params[0]] = params[1]\n        return ret",
  "def _request_one_trial_job(self):\n        \"\"\"get one trial job, i.e., one hyperparameter configuration.\n\n        If this function is called, Command will be sent by BOHB:\n        a. If there is a parameter need to run, will return \"NewTrialJob\" with a dict:\n        {\n            'parameter_id': id of new hyperparameter\n            'parameter_source': 'algorithm'\n            'parameters': value of new hyperparameter\n        }\n        b. If BOHB don't have parameter waiting, will return \"NoMoreTrialJobs\" with\n        {\n            'parameter_id': '-1_0_0',\n            'parameter_source': 'algorithm',\n            'parameters': ''\n        }\n        \"\"\"\n        ret = self._get_one_trial_job()\n        if ret is not None:\n            send(CommandType.NewTrialJob, json_tricks.dumps(ret))\n            self.credit -= 1",
  "def handle_update_search_space(self, data):\n        \"\"\"change json format to ConfigSpace format dict<dict> -> configspace\n\n        Parameters\n        ----------\n        data: JSON object\n            search space of this experiment\n        \"\"\"\n        search_space = data\n        cs = CS.ConfigurationSpace()\n        for var in search_space:\n            _type = str(search_space[var][\"_type\"])\n            if _type == 'choice':\n                cs.add_hyperparameter(CSH.CategoricalHyperparameter(\n                    var, choices=search_space[var][\"_value\"]))\n            elif _type == 'randint':\n                cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1] - 1))\n            elif _type == 'uniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1]))\n            elif _type == 'quniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1],\n                    q=search_space[var][\"_value\"][2]))\n            elif _type == 'loguniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1],\n                    log=True))\n            elif _type == 'qloguniform':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][\"_value\"][0], upper=search_space[var][\"_value\"][1],\n                    q=search_space[var][\"_value\"][2], log=True))\n            elif _type == 'normal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2]))\n            elif _type == 'qnormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2],\n                    q=search_space[var][\"_value\"][3]))\n            elif _type == 'lognormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2],\n                    log=True))\n            elif _type == 'qlognormal':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][\"_value\"][1], sigma=search_space[var][\"_value\"][2],\n                    q=search_space[var][\"_value\"][3], log=True))\n            else:\n                raise ValueError(\n                    'unrecognized type in search_space, type is {}'.format(_type))\n\n        self.search_space = cs",
  "def handle_trial_end(self, data):\n        \"\"\"receive the information of trial end and generate next configuaration.\n\n        Parameters\n        ----------\n        data: dict()\n            it has three keys: trial_job_id, event, hyper_params\n            trial_job_id: the id generated by training service\n            event: the job's state\n            hyper_params: the hyperparameters (a string) generated and returned by tuner\n        \"\"\"\n        logger.debug('Tuner handle trial end, result is %s', data)\n        hyper_params = json_tricks.loads(data['hyper_params'])\n        self._handle_trial_end(hyper_params['parameter_id'])\n        if data['trial_job_id'] in self.job_id_para_id_map:\n            del self.job_id_para_id_map[data['trial_job_id']]",
  "def _send_new_trial(self):\n        while self.unsatisfied_jobs:\n            ret = self._get_one_trial_job()\n            if ret is None:\n                break\n            one_unsatisfied = self.unsatisfied_jobs.pop(0)\n            ret['trial_job_id'] = one_unsatisfied['trial_job_id']\n            ret['parameter_index'] = one_unsatisfied['parameter_index']\n            # update parameter_id in self.job_id_para_id_map\n            self.job_id_para_id_map[ret['trial_job_id']] = ret['parameter_id']\n            send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        for _ in range(self.credit):\n            self._request_one_trial_job()",
  "def _handle_trial_end(self, parameter_id):\n        s, i, _ = parameter_id.split('_')\n        hyper_configs = self.brackets[int(s)].inform_trial_end(int(i))\n\n        if hyper_configs is not None:\n            logger.debug(\n                'bracket %s next round %s, hyper_configs: %s', s, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs\n        # Finish this bracket and generate a new bracket\n        elif self.brackets[int(s)].no_more_trial:\n            self.curr_s -= 1\n            self.generate_new_bracket()\n        self._send_new_trial()",
  "def handle_report_metric_data(self, data):\n        \"\"\"reveice the metric data and update Bayesian optimization with final result\n\n        Parameters\n        ----------\n        data:\n            it is an object which has keys 'parameter_id', 'value', 'trial_job_id', 'type', 'sequence'.\n\n        Raises\n        ------\n        ValueError\n            Data type not supported\n        \"\"\"\n        logger.debug('handle report metric data = %s', data)\n        if 'value' in data:\n            data['value'] = json_tricks.loads(data['value'])\n        if data['type'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data['trial_job_id'] is not None\n            assert data['parameter_index'] is not None\n            assert data['trial_job_id'] in self.job_id_para_id_map\n            self._handle_trial_end(self.job_id_para_id_map[data['trial_job_id']])\n            ret = self._get_one_trial_job()\n            if ret is None:\n                self.unsatisfied_jobs.append({'trial_job_id': data['trial_job_id'], 'parameter_index': data['parameter_index']})\n            else:\n                ret['trial_job_id'] = data['trial_job_id']\n                ret['parameter_index'] = data['parameter_index']\n                # update parameter_id in self.job_id_para_id_map\n                self.job_id_para_id_map[data['trial_job_id']] = ret['parameter_id']\n                send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        else:\n            assert 'value' in data\n            value = extract_scalar_reward(data['value'])\n            if self.optimize_mode is OptimizeMode.Maximize:\n                reward = -value\n            else:\n                reward = value\n            assert 'parameter_id' in data\n            s, i, _ = data['parameter_id'].split('_')\n            logger.debug('bracket id = %s, metrics value = %s, type = %s', s, value, data['type'])\n            s = int(s)\n\n            # add <trial_job_id, parameter_id> to self.job_id_para_id_map here,\n            # because when the first parameter_id is created, trial_job_id is not known yet.\n            if data['trial_job_id'] in self.job_id_para_id_map:\n                assert self.job_id_para_id_map[data['trial_job_id']] == data['parameter_id']\n            else:\n                self.job_id_para_id_map[data['trial_job_id']] = data['parameter_id']\n\n            assert 'type' in data\n            if data['type'] == MetricType.FINAL:\n                # and PERIODICAL metric are independent, thus, not comparable.\n                assert 'sequence' in data\n                self.brackets[s].set_config_perf(\n                    int(i), data['parameter_id'], sys.maxsize, value)\n                self.completed_hyper_configs.append(data)\n\n                _parameters = self.parameters[data['parameter_id']]\n                _parameters.pop(_KEY)\n                # update BO with loss, max_s budget, hyperparameters\n                self.cg.new_result(loss=reward, budget=data['sequence'], parameters=_parameters, update_model=True)\n            elif data['type'] == MetricType.PERIODICAL:\n                self.brackets[s].set_config_perf(\n                    int(i), data['parameter_id'], data['sequence'], value)\n            else:\n                raise ValueError(\n                    'Data type not supported: {}'.format(data['type']))",
  "def handle_add_customized_trial(self, data):\n        pass",
  "def handle_import_data(self, data):\n        \"\"\"Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n\n        Raises\n        ------\n        AssertionError\n            data doesn't have required key 'parameter' and 'value'\n        \"\"\"\n        for entry in data:\n            entry['value'] = json_tricks.loads(entry['value'])\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\", _completed_num, len(data))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\", _value)\n                continue\n            _value = extract_scalar_reward(_value)\n            budget_exist_flag = False\n            barely_params = dict()\n            for keys in _params:\n                if keys == _KEY:\n                    _budget = _params[keys]\n                    budget_exist_flag = True\n                else:\n                    barely_params[keys] = _params[keys]\n            if not budget_exist_flag:\n                _budget = self.max_budget\n                logger.info(\"Set \\\"TRIAL_BUDGET\\\" value to %s (max budget)\", self.max_budget)\n            if self.optimize_mode is OptimizeMode.Maximize:\n                reward = -_value\n            else:\n                reward = _value\n            self.cg.new_result(loss=reward, budget=_budget, parameters=barely_params, update_model=True)\n        logger.info(\"Successfully import tuning data to BOHB advisor.\")",
  "def _nni_rest_get(endpoint, api_path, response_type='json'):\n    _check_endpoint(endpoint)\n    uri = '{}/{}/{}'.format(endpoint.strip('/'), API_ROOT_PATH, api_path)\n    res = requests.get(uri)\n    if _http_succeed(res.status_code):\n        if response_type == 'json':\n            return res.json()\n        elif response_type == 'text':\n            return res.text\n        else:\n            raise RuntimeError('Incorrect response_type')\n    else:\n        return None",
  "def _http_succeed(status_code):\n    return status_code // 100 == 2",
  "def _create_process(cmd):\n    if sys.platform == 'win32':\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, creationflags=subprocess.CREATE_NEW_PROCESS_GROUP)\n    else:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n\n    while process.poll() is None:\n        output = process.stdout.readline()\n        if output:\n            print(output.decode('utf-8').strip())\n    return process.returncode",
  "def _check_endpoint(endpoint):\n    if endpoint is None:\n        raise RuntimeError(\"This instance hasn't been connect to an experiment.\")",
  "class TrialResult:\n    \"\"\"\n    TrialResult stores the result information of a trial job.\n\n    Parameters\n    ----------\n    json_obj: dict\n        Json object that stores the result information.\n\n    Attributes\n    ----------\n    parameter: dict\n        Hyper parameters for this trial.\n    value: serializable object, usually a number, or a dict with key \"default\" and other extra keys\n        Final result.\n    trialJobId: str\n        Trial job id.\n    \"\"\"\n    def __init__(self, json_obj):\n        self.parameter = None\n        self.value = None\n        self.trialJobId = None\n        for key in json_obj.keys():\n            if key == 'id':\n                setattr(self, 'trialJobId', json_obj[key])\n            elif hasattr(self, key):\n                setattr(self, key, json_obj[key])\n        self.value = json.loads(self.value)\n\n    def __repr__(self):\n        return \"TrialResult(parameter: {} value: {} trialJobId: {})\".format(self.parameter, self.value, self.trialJobId)",
  "class TrialMetricData:\n    \"\"\"\n    TrialMetricData stores the metric data of a trial job.\n    A trial job may have both intermediate metric and final metric.\n\n    Parameters\n    ----------\n    json_obj: dict\n        Json object that stores the metric data.\n\n    Attributes\n    ----------\n    timestamp: int\n        Time stamp.\n    trialJobId: str\n        Trial job id.\n    parameterId: int\n        Parameter id.\n    type: str\n        Metric type, `PERIODICAL` for intermediate result and `FINAL` for final result.\n    sequence: int\n        Sequence number in this trial.\n    data: serializable object, usually a number, or a dict with key \"default\" and other extra keys\n        Metric data.\n    \"\"\"\n    def __init__(self, json_obj):\n        self.timestamp = None\n        self.trialJobId = None\n        self.parameterId = None\n        self.type = None\n        self.sequence = None\n        self.data = None\n        for key in json_obj.keys():\n            setattr(self, key, json_obj[key])\n        self.data = json.loads(json.loads(self.data))\n\n    def __repr__(self):\n        return \"TrialMetricData(timestamp: {} trialJobId: {} parameterId: {} type: {} sequence: {} data: {})\" \\\n            .format(self.timestamp, self.trialJobId, self.parameterId, self.type, self.sequence, self.data)",
  "class TrialHyperParameters:\n    \"\"\"\n    TrialHyperParameters stores the hyper parameters of a trial job.\n\n    Parameters\n    ----------\n    json_obj: dict\n        Json object that stores the hyper parameters.\n\n    Attributes\n    ----------\n    parameter_id: int\n        Parameter id.\n    parameter_source: str\n        Parameter source.\n    parameters: dict\n        Hyper parameters.\n    parameter_index: int\n        Parameter index.\n    \"\"\"\n    def __init__(self, json_obj):\n        self.parameter_id = None\n        self.parameter_source = None\n        self.parameters = None\n        self.parameter_index = None\n        for key in json_obj.keys():\n            if hasattr(self, key):\n                setattr(self, key, json_obj[key])\n\n    def __repr__(self):\n        return \"TrialHyperParameters(parameter_id: {} parameter_source: {} parameters: {} parameter_index: {})\" \\\n            .format(self.parameter_id, self.parameter_source, self.parameters, self.parameter_index)",
  "class TrialJob:\n    \"\"\"\n    TrialJob stores the information of a trial job.\n\n    Parameters\n    ----------\n    json_obj: dict\n        json object that stores the hyper parameters\n\n    Attributes\n    ----------\n    trialJobId: str\n        Trial job id.\n    status: str\n        Job status.\n    hyperParameters: list of `nnicli.TrialHyperParameters`\n        See `nnicli.TrialHyperParameters`.\n    logPath: str\n        Log path.\n    startTime: int\n        Job start time (timestamp).\n    endTime: int\n        Job end time (timestamp).\n    finalMetricData: list of `nnicli.TrialMetricData`\n        See `nnicli.TrialMetricData`.\n    parameter_index: int\n        Parameter index.\n    \"\"\"\n    def __init__(self, json_obj):\n        self.trialJobId = None\n        self.status = None\n        self.hyperParameters = None\n        self.logPath = None\n        self.startTime = None\n        self.endTime = None\n        self.finalMetricData = None\n        self.stderrPath = None\n        for key in json_obj.keys():\n            if key == 'id':\n                setattr(self, 'trialJobId', json_obj[key])\n            elif hasattr(self, key):\n                setattr(self, key, json_obj[key])\n        if self.hyperParameters:\n            self.hyperParameters = [TrialHyperParameters(json.loads(e)) for e in self.hyperParameters]\n        if self.finalMetricData:\n            self.finalMetricData = [TrialMetricData(e) for e in self.finalMetricData]\n\n    def __repr__(self):\n        return (\"TrialJob(trialJobId: {} status: {} hyperParameters: {} logPath: {} startTime: {} \"\n                \"endTime: {} finalMetricData: {} stderrPath: {})\") \\\n                    .format(self.trialJobId, self.status, self.hyperParameters, self.logPath,\n                            self.startTime, self.endTime, self.finalMetricData, self.stderrPath)",
  "class Experiment:\n    def __init__(self):\n        self._endpoint = None\n        self._exp_id = None\n        self._port = None\n\n    @property\n    def endpoint(self):\n        return self._endpoint\n\n    @property\n    def exp_id(self):\n        return self._exp_id\n\n    @property\n    def port(self):\n        return self._port\n\n    def _exec_command(self, cmd, port=None):\n        if self._endpoint is not None:\n            raise RuntimeError('This instance has been connected to an experiment.')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to establish experiment, please check your config.')\n        else:\n            if port:\n                self._port = port\n            else:\n                self._port = 8080\n            self._endpoint = 'http://localhost:{}'.format(self._port)\n            self._exp_id = self.get_experiment_profile()['id']\n\n    def start_experiment(self, config_file, port=None, debug=False):\n        \"\"\"\n        Start an experiment with specified configuration file and connect to it.\n\n        Parameters\n        ----------\n        config_file: str\n            Path to the config file.\n        port: int\n            The port of restful server, bigger than 1024.\n        debug: boolean\n            Set debug mode.\n        \"\"\"\n        cmd = 'nnictl create --config {}'.format(config_file).split(' ')\n        if port:\n            cmd += '--port {}'.format(port).split(' ')\n        if debug:\n            cmd += ['--debug']\n        self._exec_command(cmd, port)\n\n    def resume_experiment(self, exp_id, port=None, debug=False):\n        \"\"\"\n        Resume a stopped experiment with specified experiment id\n\n        Parameters\n        ----------\n        exp_id: str\n            Experiment id.\n        port: int\n            The port of restful server, bigger than 1024.\n        debug: boolean\n            Set debug mode.\n        \"\"\"\n        cmd = 'nnictl resume {}'.format(exp_id).split(' ')\n        if port:\n            cmd += '--port {}'.format(port).split(' ')\n        if debug:\n            cmd += ['--debug']\n        self._exec_command(cmd, port)\n\n    def view_experiment(self, exp_id, port=None):\n        \"\"\"\n        View a stopped experiment with specified experiment id.\n\n        Parameters\n        ----------\n        exp_id: str\n            Experiment id.\n        port: int\n            The port of restful server, bigger than 1024.\n        \"\"\"\n        cmd = 'nnictl view {}'.format(exp_id).split(' ')\n        if port:\n            cmd += '--port {}'.format(port).split(' ')\n        self._exec_command(cmd, port)\n\n    def connect_experiment(self, endpoint):\n        \"\"\"\n        Connect to an existing experiment.\n\n        Parameters\n        ----------\n        endpoint: str\n            The endpoint of nni rest server, i.e, the url of Web UI. Should be a format like `http://ip:port`.\n        \"\"\"\n        if self._endpoint is not None:\n            raise RuntimeError('This instance has been connected to an experiment.')\n        self._endpoint = endpoint\n        try:\n            self._exp_id = self.get_experiment_profile()['id']\n        except TypeError:\n            raise RuntimeError('Invalid experiment endpoint.')\n        self._port = int(re.search(r':[0-9]+', self._endpoint).group().replace(':', ''))\n\n    def stop_experiment(self):\n        \"\"\"Stop the experiment.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl stop {}'.format(self._exp_id).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to stop experiment.')\n        self._endpoint = None\n        self._exp_id = None\n        self._port = None\n\n    def update_searchspace(self, filename):\n        \"\"\"\n        Update the experiment's search space.\n\n        Parameters\n        ----------\n        filename: str\n            Path to the searchspace file.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl update searchspace {} --filename {}'.format(self._exp_id, filename).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to update searchspace.')\n\n    def update_concurrency(self, value):\n        \"\"\"\n        Update an experiment's concurrency\n\n        Parameters\n        ----------\n        value: int\n            New concurrency value.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl update concurrency {} --value {}'.format(self._exp_id, value).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to update concurrency.')\n\n    def update_duration(self, value):\n        \"\"\"\n        Update an experiment's duration\n\n        Parameters\n        ----------\n        value: str\n            Strings like '1m' for one minute or '2h' for two hours.\n            SUFFIX may be 's' for seconds, 'm' for minutes, 'h' for hours or 'd' for days.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl update duration {} --value {}'.format(self._exp_id, value).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to update duration.')\n\n    def update_trailnum(self, value):\n        \"\"\"\n        Update an experiment's maxtrialnum\n\n        Parameters\n        ----------\n        value: int\n            New trailnum value.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl update trialnum {} --value {}'.format(self._exp_id, value).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to update trailnum.')\n\n    def get_experiment_status(self):\n        \"\"\"\n        Return experiment status as a dict.\n\n        Returns\n        ----------\n        dict\n            Experiment status.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        return _nni_rest_get(self._endpoint, STATUS_PATH)\n\n    def get_trial_job(self, trial_job_id):\n        \"\"\"\n        Return a trial job.\n\n        Parameters\n        ----------\n        trial_job_id: str\n            Trial job id.\n\n        Returns\n        ----------\n        nnicli.TrialJob\n            A `nnicli.TrialJob` instance corresponding to `trial_job_id`.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        assert trial_job_id is not None\n        trial_job = _nni_rest_get(self._endpoint, os.path.join(TRIAL_JOBS_PATH, trial_job_id))\n        return TrialJob(trial_job)\n\n    def list_trial_jobs(self):\n        \"\"\"\n        Return information for all trial jobs as a list.\n\n        Returns\n        ----------\n        list\n            List of `nnicli.TrialJob`.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        trial_jobs = _nni_rest_get(self._endpoint, TRIAL_JOBS_PATH)\n        return [TrialJob(e) for e in trial_jobs]\n\n    def get_job_statistics(self):\n        \"\"\"\n        Return trial job statistics information as a dict.\n\n        Returns\n        ----------\n        list\n            Job statistics information.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        return _nni_rest_get(self._endpoint, JOB_STATISTICS_PATH)\n\n    def get_job_metrics(self, trial_job_id=None):\n        \"\"\"\n        Return trial job metrics.\n\n        Parameters\n        ----------\n        trial_job_id: str\n            trial job id. if this parameter is None, all trail jobs' metrics will be returned.\n\n        Returns\n        ----------\n        dict\n            Each key is a trialJobId, the corresponding value is a list of `nnicli.TrialMetricData`.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        api_path = METRICS_PATH if trial_job_id is None else os.path.join(METRICS_PATH, trial_job_id)\n        output = {}\n        trail_metrics = _nni_rest_get(self._endpoint, api_path)\n        for metric in trail_metrics:\n            trial_id = metric[\"trialJobId\"]\n            if trial_id not in output:\n                output[trial_id] = [TrialMetricData(metric)]\n            else:\n                output[trial_id].append(TrialMetricData(metric))\n        return output\n\n    def export_data(self):\n        \"\"\"\n        Return exported information for all trial jobs.\n\n        Returns\n        ----------\n        list\n            List of `nnicli.TrialResult`.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        trial_results = _nni_rest_get(self._endpoint, EXPORT_DATA_PATH)\n        return [TrialResult(e) for e in trial_results]\n\n    def get_experiment_profile(self):\n        \"\"\"\n        Return experiment profile as a dict.\n\n        Returns\n        ----------\n        dict\n            The profile of the experiment.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        return _nni_rest_get(self._endpoint, EXPERIMENT_PATH)",
  "def __init__(self, json_obj):\n        self.parameter = None\n        self.value = None\n        self.trialJobId = None\n        for key in json_obj.keys():\n            if key == 'id':\n                setattr(self, 'trialJobId', json_obj[key])\n            elif hasattr(self, key):\n                setattr(self, key, json_obj[key])\n        self.value = json.loads(self.value)",
  "def __repr__(self):\n        return \"TrialResult(parameter: {} value: {} trialJobId: {})\".format(self.parameter, self.value, self.trialJobId)",
  "def __init__(self, json_obj):\n        self.timestamp = None\n        self.trialJobId = None\n        self.parameterId = None\n        self.type = None\n        self.sequence = None\n        self.data = None\n        for key in json_obj.keys():\n            setattr(self, key, json_obj[key])\n        self.data = json.loads(json.loads(self.data))",
  "def __repr__(self):\n        return \"TrialMetricData(timestamp: {} trialJobId: {} parameterId: {} type: {} sequence: {} data: {})\" \\\n            .format(self.timestamp, self.trialJobId, self.parameterId, self.type, self.sequence, self.data)",
  "def __init__(self, json_obj):\n        self.parameter_id = None\n        self.parameter_source = None\n        self.parameters = None\n        self.parameter_index = None\n        for key in json_obj.keys():\n            if hasattr(self, key):\n                setattr(self, key, json_obj[key])",
  "def __repr__(self):\n        return \"TrialHyperParameters(parameter_id: {} parameter_source: {} parameters: {} parameter_index: {})\" \\\n            .format(self.parameter_id, self.parameter_source, self.parameters, self.parameter_index)",
  "def __init__(self, json_obj):\n        self.trialJobId = None\n        self.status = None\n        self.hyperParameters = None\n        self.logPath = None\n        self.startTime = None\n        self.endTime = None\n        self.finalMetricData = None\n        self.stderrPath = None\n        for key in json_obj.keys():\n            if key == 'id':\n                setattr(self, 'trialJobId', json_obj[key])\n            elif hasattr(self, key):\n                setattr(self, key, json_obj[key])\n        if self.hyperParameters:\n            self.hyperParameters = [TrialHyperParameters(json.loads(e)) for e in self.hyperParameters]\n        if self.finalMetricData:\n            self.finalMetricData = [TrialMetricData(e) for e in self.finalMetricData]",
  "def __repr__(self):\n        return (\"TrialJob(trialJobId: {} status: {} hyperParameters: {} logPath: {} startTime: {} \"\n                \"endTime: {} finalMetricData: {} stderrPath: {})\") \\\n                    .format(self.trialJobId, self.status, self.hyperParameters, self.logPath,\n                            self.startTime, self.endTime, self.finalMetricData, self.stderrPath)",
  "def __init__(self):\n        self._endpoint = None\n        self._exp_id = None\n        self._port = None",
  "def endpoint(self):\n        return self._endpoint",
  "def exp_id(self):\n        return self._exp_id",
  "def port(self):\n        return self._port",
  "def _exec_command(self, cmd, port=None):\n        if self._endpoint is not None:\n            raise RuntimeError('This instance has been connected to an experiment.')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to establish experiment, please check your config.')\n        else:\n            if port:\n                self._port = port\n            else:\n                self._port = 8080\n            self._endpoint = 'http://localhost:{}'.format(self._port)\n            self._exp_id = self.get_experiment_profile()['id']",
  "def start_experiment(self, config_file, port=None, debug=False):\n        \"\"\"\n        Start an experiment with specified configuration file and connect to it.\n\n        Parameters\n        ----------\n        config_file: str\n            Path to the config file.\n        port: int\n            The port of restful server, bigger than 1024.\n        debug: boolean\n            Set debug mode.\n        \"\"\"\n        cmd = 'nnictl create --config {}'.format(config_file).split(' ')\n        if port:\n            cmd += '--port {}'.format(port).split(' ')\n        if debug:\n            cmd += ['--debug']\n        self._exec_command(cmd, port)",
  "def resume_experiment(self, exp_id, port=None, debug=False):\n        \"\"\"\n        Resume a stopped experiment with specified experiment id\n\n        Parameters\n        ----------\n        exp_id: str\n            Experiment id.\n        port: int\n            The port of restful server, bigger than 1024.\n        debug: boolean\n            Set debug mode.\n        \"\"\"\n        cmd = 'nnictl resume {}'.format(exp_id).split(' ')\n        if port:\n            cmd += '--port {}'.format(port).split(' ')\n        if debug:\n            cmd += ['--debug']\n        self._exec_command(cmd, port)",
  "def view_experiment(self, exp_id, port=None):\n        \"\"\"\n        View a stopped experiment with specified experiment id.\n\n        Parameters\n        ----------\n        exp_id: str\n            Experiment id.\n        port: int\n            The port of restful server, bigger than 1024.\n        \"\"\"\n        cmd = 'nnictl view {}'.format(exp_id).split(' ')\n        if port:\n            cmd += '--port {}'.format(port).split(' ')\n        self._exec_command(cmd, port)",
  "def connect_experiment(self, endpoint):\n        \"\"\"\n        Connect to an existing experiment.\n\n        Parameters\n        ----------\n        endpoint: str\n            The endpoint of nni rest server, i.e, the url of Web UI. Should be a format like `http://ip:port`.\n        \"\"\"\n        if self._endpoint is not None:\n            raise RuntimeError('This instance has been connected to an experiment.')\n        self._endpoint = endpoint\n        try:\n            self._exp_id = self.get_experiment_profile()['id']\n        except TypeError:\n            raise RuntimeError('Invalid experiment endpoint.')\n        self._port = int(re.search(r':[0-9]+', self._endpoint).group().replace(':', ''))",
  "def stop_experiment(self):\n        \"\"\"Stop the experiment.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl stop {}'.format(self._exp_id).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to stop experiment.')\n        self._endpoint = None\n        self._exp_id = None\n        self._port = None",
  "def update_searchspace(self, filename):\n        \"\"\"\n        Update the experiment's search space.\n\n        Parameters\n        ----------\n        filename: str\n            Path to the searchspace file.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl update searchspace {} --filename {}'.format(self._exp_id, filename).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to update searchspace.')",
  "def update_concurrency(self, value):\n        \"\"\"\n        Update an experiment's concurrency\n\n        Parameters\n        ----------\n        value: int\n            New concurrency value.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl update concurrency {} --value {}'.format(self._exp_id, value).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to update concurrency.')",
  "def update_duration(self, value):\n        \"\"\"\n        Update an experiment's duration\n\n        Parameters\n        ----------\n        value: str\n            Strings like '1m' for one minute or '2h' for two hours.\n            SUFFIX may be 's' for seconds, 'm' for minutes, 'h' for hours or 'd' for days.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl update duration {} --value {}'.format(self._exp_id, value).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to update duration.')",
  "def update_trailnum(self, value):\n        \"\"\"\n        Update an experiment's maxtrialnum\n\n        Parameters\n        ----------\n        value: int\n            New trailnum value.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        cmd = 'nnictl update trialnum {} --value {}'.format(self._exp_id, value).split(' ')\n        if _create_process(cmd) != 0:\n            raise RuntimeError('Failed to update trailnum.')",
  "def get_experiment_status(self):\n        \"\"\"\n        Return experiment status as a dict.\n\n        Returns\n        ----------\n        dict\n            Experiment status.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        return _nni_rest_get(self._endpoint, STATUS_PATH)",
  "def get_trial_job(self, trial_job_id):\n        \"\"\"\n        Return a trial job.\n\n        Parameters\n        ----------\n        trial_job_id: str\n            Trial job id.\n\n        Returns\n        ----------\n        nnicli.TrialJob\n            A `nnicli.TrialJob` instance corresponding to `trial_job_id`.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        assert trial_job_id is not None\n        trial_job = _nni_rest_get(self._endpoint, os.path.join(TRIAL_JOBS_PATH, trial_job_id))\n        return TrialJob(trial_job)",
  "def list_trial_jobs(self):\n        \"\"\"\n        Return information for all trial jobs as a list.\n\n        Returns\n        ----------\n        list\n            List of `nnicli.TrialJob`.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        trial_jobs = _nni_rest_get(self._endpoint, TRIAL_JOBS_PATH)\n        return [TrialJob(e) for e in trial_jobs]",
  "def get_job_statistics(self):\n        \"\"\"\n        Return trial job statistics information as a dict.\n\n        Returns\n        ----------\n        list\n            Job statistics information.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        return _nni_rest_get(self._endpoint, JOB_STATISTICS_PATH)",
  "def get_job_metrics(self, trial_job_id=None):\n        \"\"\"\n        Return trial job metrics.\n\n        Parameters\n        ----------\n        trial_job_id: str\n            trial job id. if this parameter is None, all trail jobs' metrics will be returned.\n\n        Returns\n        ----------\n        dict\n            Each key is a trialJobId, the corresponding value is a list of `nnicli.TrialMetricData`.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        api_path = METRICS_PATH if trial_job_id is None else os.path.join(METRICS_PATH, trial_job_id)\n        output = {}\n        trail_metrics = _nni_rest_get(self._endpoint, api_path)\n        for metric in trail_metrics:\n            trial_id = metric[\"trialJobId\"]\n            if trial_id not in output:\n                output[trial_id] = [TrialMetricData(metric)]\n            else:\n                output[trial_id].append(TrialMetricData(metric))\n        return output",
  "def export_data(self):\n        \"\"\"\n        Return exported information for all trial jobs.\n\n        Returns\n        ----------\n        list\n            List of `nnicli.TrialResult`.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        trial_results = _nni_rest_get(self._endpoint, EXPORT_DATA_PATH)\n        return [TrialResult(e) for e in trial_results]",
  "def get_experiment_profile(self):\n        \"\"\"\n        Return experiment profile as a dict.\n\n        Returns\n        ----------\n        dict\n            The profile of the experiment.\n        \"\"\"\n        _check_endpoint(self._endpoint)\n        return _nni_rest_get(self._endpoint, EXPERIMENT_PATH)",
  "def get_hdfs_client(args):\n    global _hdfs_client\n\n    if _hdfs_client is not None:\n        return _hdfs_client\n    # backward compatibility\n    hdfs_host = None\n\n    if args.hdfs_host:\n        hdfs_host = args.hdfs_host\n    elif args.pai_hdfs_host:\n        hdfs_host = args.pai_hdfs_host\n    else:\n        return None\n\n    if hdfs_host is not None and args.nni_hdfs_exp_dir is not None:\n        try:\n            if args.webhdfs_path:\n                _hdfs_client = HdfsClient(hosts='{0}:80'.format(hdfs_host), user_name=args.pai_user_name,\n                                          webhdfs_path=args.webhdfs_path, timeout=5)\n            else:\n                # backward compatibility\n                _hdfs_client = HdfsClient(hosts='{0}:{1}'.format(hdfs_host, '50070'), user_name=args.pai_user_name,\n                                          timeout=5)\n        except Exception as e:\n            nni_log(LogType.Error, 'Create HDFS client error: ' + str(e))\n            raise e\n    return _hdfs_client",
  "def main_loop(args):\n    '''main loop logic for trial keeper'''\n\n    if not os.path.exists(LOG_DIR):\n        os.makedirs(LOG_DIR)\n\n    trial_keeper_syslogger = RemoteLogger(args.nnimanager_ip, args.nnimanager_port, 'trial_keeper',\n                                          StdOutputType.Stdout, args.log_collection)\n    # redirect trial keeper's stdout and stderr to syslog\n    trial_syslogger_stdout = RemoteLogger(args.nnimanager_ip, args.nnimanager_port, 'trial', StdOutputType.Stdout,\n                                          args.log_collection)\n    sys.stdout = sys.stderr = trial_keeper_syslogger\n    hdfs_output_dir = None\n\n    if args.hdfs_output_dir:\n        hdfs_output_dir = args.hdfs_output_dir\n    elif args.pai_hdfs_output_dir:\n        hdfs_output_dir = args.pai_hdfs_output_dir\n\n    hdfs_client = get_hdfs_client(args)\n\n    if hdfs_client is not None:\n        copyHdfsDirectoryToLocal(args.nni_hdfs_exp_dir, os.getcwd(), hdfs_client)\n\n    if args.job_id_file:\n        with open(args.job_id_file, 'w') as job_file:\n            job_file.write(\"%d\" % os.getpid())\n\n    # Notice: We don't appoint env, which means subprocess wil inherit current environment and that is expected behavior\n    log_pipe_stdout = trial_syslogger_stdout.get_pipelog_reader()\n    process = Popen(args.trial_command, shell=True, stdout=log_pipe_stdout, stderr=log_pipe_stdout)\n    nni_log(LogType.Info, 'Trial keeper spawns a subprocess (pid {0}) to run command: {1}'.format(process.pid,\n                                                                                                  shlex.split(\n                                                                                                      args.trial_command)))\n\n    while True:\n        retCode = process.poll()\n        # child worker process exits and all stdout data is read\n        if retCode is not None and log_pipe_stdout.set_process_exit() and log_pipe_stdout.is_read_completed == True:\n            # In Windows, the retCode -1 is 4294967295. It's larger than c_long, and raise OverflowError.\n            # So covert it to int32.\n            retCode = ctypes.c_long(retCode).value\n            nni_log(LogType.Info, 'subprocess terminated. Exit code is {}. Quit'.format(retCode))\n            if hdfs_output_dir is not None:\n                # Copy local directory to hdfs for OpenPAI\n                nni_local_output_dir = os.environ['NNI_OUTPUT_DIR']\n                try:\n                    if copyDirectoryToHdfs(nni_local_output_dir, hdfs_output_dir, hdfs_client):\n                        nni_log(LogType.Info,\n                                'copy directory from {0} to {1} success!'.format(nni_local_output_dir, hdfs_output_dir))\n                    else:\n                        nni_log(LogType.Info,\n                                'copy directory from {0} to {1} failed!'.format(nni_local_output_dir, hdfs_output_dir))\n                except Exception as e:\n                    nni_log(LogType.Error, 'HDFS copy directory got exception: ' + str(e))\n                    raise e\n\n            # Exit as the retCode of subprocess(trial)\n            exit(retCode)\n            break\n\n        time.sleep(2)",
  "def trial_keeper_help_info(*args):\n    print('please run --help to see guidance')",
  "def check_version(args):\n    try:\n        trial_keeper_version = pkg_resources.get_distribution('nni').version\n    except pkg_resources.ResolutionError as err:\n        # package nni does not exist, try nni-tool package\n        nni_log(LogType.Error, 'Package nni does not exist!')\n        os._exit(1)\n    if not args.nni_manager_version:\n        # skip version check\n        nni_log(LogType.Warning, 'Skipping version check!')\n    else:\n        try:\n            trial_keeper_version = regular.search(trial_keeper_version).group('version')\n            nni_log(LogType.Info, 'trial_keeper_version is {0}'.format(trial_keeper_version))\n            nni_manager_version = regular.search(args.nni_manager_version).group('version')\n            nni_log(LogType.Info, 'nni_manager_version is {0}'.format(nni_manager_version))\n            log_entry = {}\n            if trial_keeper_version != nni_manager_version:\n                nni_log(LogType.Error, 'Version does not match!')\n                error_message = 'NNIManager version is {0}, TrialKeeper version is {1}, NNI version does not match!'.format(\n                    nni_manager_version, trial_keeper_version)\n                log_entry['tag'] = 'VCFail'\n                log_entry['msg'] = error_message\n                rest_post(gen_send_version_url(args.nnimanager_ip, args.nnimanager_port), json.dumps(log_entry), 10,\n                          False)\n                os._exit(1)\n            else:\n                nni_log(LogType.Info, 'Version match!')\n                log_entry['tag'] = 'VCSuccess'\n                rest_post(gen_send_version_url(args.nnimanager_ip, args.nnimanager_port), json.dumps(log_entry), 10,\n                          False)\n        except AttributeError as err:\n            nni_log(LogType.Error, err)",
  "def is_multi_phase():\n    return MULTI_PHASE and (MULTI_PHASE in ['True', 'true'])",
  "def download_parameter(meta_list, args):\n    \"\"\"\n    Download parameter file to local working directory.\n    meta_list format is defined in paiJobRestServer.ts\n    example meta_list:\n    [\n        {\"experimentId\":\"yWFJarYa\",\"trialId\":\"UpPkl\",\"filePath\":\"/chec/nni/experiments/yWFJarYa/trials/UpPkl/parameter_1.cfg\"},\n        {\"experimentId\":\"yWFJarYa\",\"trialId\":\"aIUMA\",\"filePath\":\"/chec/nni/experiments/yWFJarYa/trials/aIUMA/parameter_1.cfg\"}\n    ]\n    \"\"\"\n    nni_log(LogType.Debug, str(meta_list))\n    nni_log(LogType.Debug,\n            'NNI_SYS_DIR: {}, trial Id: {}, experiment ID: {}'.format(NNI_SYS_DIR, NNI_TRIAL_JOB_ID, NNI_EXP_ID))\n    nni_log(LogType.Debug, 'NNI_SYS_DIR files: {}'.format(os.listdir(NNI_SYS_DIR)))\n    for meta in meta_list:\n        if meta['experimentId'] == NNI_EXP_ID and meta['trialId'] == NNI_TRIAL_JOB_ID:\n            param_fp = os.path.join(NNI_SYS_DIR, os.path.basename(meta['filePath']))\n            if not os.path.exists(param_fp):\n                hdfs_client = get_hdfs_client(args)\n                copyHdfsFileToLocal(meta['filePath'], param_fp, hdfs_client, override=False)",
  "def fetch_parameter_file(args):\n    class FetchThread(threading.Thread):\n        def __init__(self, args):\n            super(FetchThread, self).__init__()\n            self.args = args\n\n        def run(self):\n            uri = gen_parameter_meta_url(self.args.nnimanager_ip, self.args.nnimanager_port)\n            nni_log(LogType.Info, uri)\n\n            while True:\n                res = rest_get(uri, 10)\n                nni_log(LogType.Debug, 'status code: {}'.format(res.status_code))\n                if res.status_code == 200:\n                    meta_list = res.json()\n                    download_parameter(meta_list, self.args)\n                else:\n                    nni_log(LogType.Warning, 'rest response: {}'.format(str(res)))\n                time.sleep(5)\n\n    fetch_file_thread = FetchThread(args)\n    fetch_file_thread.start()",
  "class FetchThread(threading.Thread):\n        def __init__(self, args):\n            super(FetchThread, self).__init__()\n            self.args = args\n\n        def run(self):\n            uri = gen_parameter_meta_url(self.args.nnimanager_ip, self.args.nnimanager_port)\n            nni_log(LogType.Info, uri)\n\n            while True:\n                res = rest_get(uri, 10)\n                nni_log(LogType.Debug, 'status code: {}'.format(res.status_code))\n                if res.status_code == 200:\n                    meta_list = res.json()\n                    download_parameter(meta_list, self.args)\n                else:\n                    nni_log(LogType.Warning, 'rest response: {}'.format(str(res)))\n                time.sleep(5)",
  "def __init__(self, args):\n            super(FetchThread, self).__init__()\n            self.args = args",
  "def run(self):\n            uri = gen_parameter_meta_url(self.args.nnimanager_ip, self.args.nnimanager_port)\n            nni_log(LogType.Info, uri)\n\n            while True:\n                res = rest_get(uri, 10)\n                nni_log(LogType.Debug, 'status code: {}'.format(res.status_code))\n                if res.status_code == 200:\n                    meta_list = res.json()\n                    download_parameter(meta_list, self.args)\n                else:\n                    nni_log(LogType.Warning, 'rest response: {}'.format(str(res)))\n                time.sleep(5)",
  "def rest_get(url, timeout):\n    '''Call rest get method'''\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http get to url {1}'.format(str(e), url))\n        return None",
  "def rest_post(url, data, timeout, rethrow_exception=False):\n    '''Call rest post method'''\n    try:\n        response = requests.post(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                 data=data, timeout=timeout)\n        return response\n    except Exception as e:\n        if rethrow_exception is True:\n            raise\n        print('Get exception {0} when sending http post to url {1}'.format(str(e), url))\n        return None",
  "def rest_put(url, data, timeout):\n    '''Call rest put method'''\n    try:\n        response = requests.put(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                data=data, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http put to url {1}'.format(str(e), url))\n        return None",
  "def rest_delete(url, timeout):\n    '''Call rest delete method'''\n    try:\n        response = requests.delete(url, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http delete to url {1}'.format(str(e), url))\n        return None",
  "class FileChannel(BaseChannel):\n\n    def __init__(self, args):\n        self.node_id = args.node_id\n        self.out_file = None\n        self.in_file = None\n        self.in_offset = 0\n        self.in_cache = b\"\"\n\n        super(FileChannel, self).__init__(args)\n\n    def _inner_open(self):\n        pass\n\n    def _inner_close(self):\n        if self.out_file is not None:\n            self.out_file.close()\n            self.out_file = None\n        if self.in_file is not None:\n            self.in_file.close()\n            self.in_file = None\n\n    def _inner_send(self, message):\n        if self.out_file is None:\n            if not os.path.exists(command_path):\n                os.makedirs(command_path, exist_ok=True)\n\n            if self.node_id is None:\n                file_name = os.path.join(command_path, \"%s.txt\" % runner_commands_file_name_prefix)\n            else:\n                file_name = os.path.join(command_path, \"%s_%s.txt\" % (\n                    runner_commands_file_name_prefix, self.node_id))\n            self.out_file = open(file_name, \"ab\")\n\n        self.out_file.write(message)\n        self.out_file.write(b'\\n')\n        self.out_file.flush()\n\n    def _open_manager_command(self):\n        full_name = os.path.join(command_path, manager_commands_file_name)\n\n        if self.in_file is not None and self.in_file.closed:\n            self.in_file = None\n\n        if self.in_file is None and os.path.exists(full_name):\n            self.in_file = open(full_name, \"rb\")\n            self.in_file.seek(self.in_offset)\n\n    def _inner_receive(self):\n        messages = []\n\n        if self.in_file is None:\n            self._open_manager_command()\n        if self.in_file is not None:\n            self.in_file.seek(0, os.SEEK_END)\n            new_offset = self.in_file.tell()\n            self.in_file.seek(self.in_offset, os.SEEK_SET)\n            count = new_offset - self.in_offset\n            if count > 0:\n                self.in_cache += self.in_file.read(count)\n                self.in_offset = new_offset\n                messages, self.in_cache = self._fetch_message(self.in_cache, True)\n        return messages",
  "def __init__(self, args):\n        self.node_id = args.node_id\n        self.out_file = None\n        self.in_file = None\n        self.in_offset = 0\n        self.in_cache = b\"\"\n\n        super(FileChannel, self).__init__(args)",
  "def _inner_open(self):\n        pass",
  "def _inner_close(self):\n        if self.out_file is not None:\n            self.out_file.close()\n            self.out_file = None\n        if self.in_file is not None:\n            self.in_file.close()\n            self.in_file = None",
  "def _inner_send(self, message):\n        if self.out_file is None:\n            if not os.path.exists(command_path):\n                os.makedirs(command_path, exist_ok=True)\n\n            if self.node_id is None:\n                file_name = os.path.join(command_path, \"%s.txt\" % runner_commands_file_name_prefix)\n            else:\n                file_name = os.path.join(command_path, \"%s_%s.txt\" % (\n                    runner_commands_file_name_prefix, self.node_id))\n            self.out_file = open(file_name, \"ab\")\n\n        self.out_file.write(message)\n        self.out_file.write(b'\\n')\n        self.out_file.flush()",
  "def _open_manager_command(self):\n        full_name = os.path.join(command_path, manager_commands_file_name)\n\n        if self.in_file is not None and self.in_file.closed:\n            self.in_file = None\n\n        if self.in_file is None and os.path.exists(full_name):\n            self.in_file = open(full_name, \"rb\")\n            self.in_file.seek(self.in_offset)",
  "def _inner_receive(self):\n        messages = []\n\n        if self.in_file is None:\n            self._open_manager_command()\n        if self.in_file is not None:\n            self.in_file.seek(0, os.SEEK_END)\n            new_offset = self.in_file.tell()\n            self.in_file.seek(self.in_offset, os.SEEK_SET)\n            count = new_offset - self.in_offset\n            if count > 0:\n                self.in_cache += self.in_file.read(count)\n                self.in_offset = new_offset\n                messages, self.in_cache = self._fetch_message(self.in_cache, True)\n        return messages",
  "class LogType(Enum):\n    Trace = 'TRACE'\n    Debug = 'DEBUG'\n    Info = 'INFO'\n    Warning = 'WARNING'\n    Error = 'ERROR'\n    Fatal = 'FATAL'",
  "class StdOutputType(Enum):\n    Stdout = 'stdout',\n    Stderr = 'stderr'",
  "def nni_log(log_type, log_message):\n    '''Log message into stdout'''\n    dt = datetime.now()\n    print('[{0}] {1} {2}'.format(dt, log_type.value, log_message), flush=True)",
  "class NNIRestLogHanlder(StreamHandler):\n    def __init__(self, host, port, tag, trial_id, channel, std_output_type=StdOutputType.Stdout):\n        StreamHandler.__init__(self)\n        self.host = host\n        self.port = port\n        self.tag = tag\n        self.std_output_type = std_output_type\n        self.trial_id = trial_id\n        self.channel = channel\n        self.orig_stdout = sys.__stdout__\n        self.orig_stderr = sys.__stderr__\n\n    def emit(self, record):\n        log_entry = {}\n        log_entry['tag'] = self.tag\n        log_entry['stdOutputType'] = self.std_output_type.name\n        log_entry['msg'] = self.format(record)\n\n        try:\n            if self.channel is None:\n                rest_post(gen_send_stdout_url(self.host, self.port), json.dumps(log_entry), 10, True)\n            else:\n                if self.trial_id is not None:\n                    log_entry[\"trial\"] = self.trial_id\n                self.channel.send(CommandType.StdOut, log_entry)\n        except Exception as e:\n            self.orig_stderr.write(str(e) + '\\n')\n            self.orig_stderr.flush()",
  "class RemoteLogger(object):\n    \"\"\"\n    NNI remote logger\n    \"\"\"\n\n    def __init__(self, syslog_host, syslog_port, tag, std_output_type, log_collection, trial_id=None, channel=None, log_level=logging.INFO):\n        '''\n        constructor\n        '''\n        logger_name = 'nni_syslog_{}'.format(tag)\n        # to prevent multiple trial logged in same logger\n        if trial_id is not None:\n            logger_name = '{}_{}'.format(logger_name, trial_id)\n        self.logger = logging.getLogger(logger_name)\n        self.log_level = log_level\n        self.logger.setLevel(self.log_level)\n        self.pipeReader = None\n        self.handler = NNIRestLogHanlder(syslog_host, syslog_port, tag, trial_id, channel)\n        self.logger.addHandler(self.handler)\n        if std_output_type == StdOutputType.Stdout:\n            self.orig_stdout = sys.__stdout__\n        else:\n            self.orig_stdout = sys.__stderr__\n        self.log_collection = log_collection\n\n    def get_pipelog_reader(self):\n        '''\n        Get pipe for remote logger\n        '''\n        self.pipeReader = PipeLogReader(self.logger, self.log_collection, logging.INFO)\n        return self.pipeReader\n\n    def flush(self):\n        '''\n        Add flush in handler\n        '''\n        for handler in self.logger.handlers:\n            handler.flush()\n\n    def write(self, buf):\n        '''\n        Write buffer data into logger/stdout\n        '''\n        for line in buf.rstrip().splitlines():\n            self.orig_stdout.write(line.rstrip() + '\\n')\n            self.orig_stdout.flush()\n            try:\n                self.logger.log(self.log_level, line.rstrip())\n            except Exception:\n                pass\n\n    def close(self):\n        '''\n        Close handlers and resources\n        '''\n        if self.pipeReader is not None:\n            self.pipeReader.set_process_exit()\n        for handler in self.logger.handlers:\n            handler.close()\n            self.logger.removeHandler(handler)",
  "class PipeLogReader(threading.Thread):\n    \"\"\"\n    The reader thread reads log data from pipe\n    \"\"\"\n\n    def __init__(self, logger, log_collection, log_level=logging.INFO):\n        \"\"\"Setup the object with a logger and a loglevel\n        and start the thread\n        \"\"\"\n        threading.Thread.__init__(self)\n        self.queue = Queue()\n        self.logger = logger\n        self.daemon = False\n        self.log_level = log_level\n        self.fdRead, self.fdWrite = os.pipe()\n        self.pipeReader = os.fdopen(self.fdRead)\n        self.orig_stdout = sys.__stdout__\n        self._is_read_completed = False\n        self.process_exit = False\n        self.log_collection = log_collection\n        self.log_pattern = re.compile(r'NNISDK_MEb\\'.*\\'$')\n\n        def _populateQueue(stream, queue):\n            '''\n            Collect lines from 'stream' and put them in 'quque'.\n            '''\n            time.sleep(1)\n            while True:\n                cur_process_exit = self.process_exit\n                try:\n                    line = self.queue.get(True, 5)\n                    try:\n                        self.logger.log(self.log_level, line.rstrip())\n                    except Exception:\n                        pass\n                except Exception:\n                    if cur_process_exit == True:\n                        self._is_read_completed = True\n                        break\n\n        self.pip_log_reader_thread = threading.Thread(target=_populateQueue, args=(self.pipeReader, self.queue))\n        self.pip_log_reader_thread.daemon = True\n        self.start()\n        self.pip_log_reader_thread.start()\n\n    def fileno(self):\n        \"\"\"Return the write file descriptor of the pipe\n        \"\"\"\n        return self.fdWrite\n\n    def run(self):\n        \"\"\"Run the thread, logging everything.\n           If the log_collection is 'none', the log content will not be enqueued\n        \"\"\"\n        for line in iter(self.pipeReader.readline, ''):\n            self.orig_stdout.write(line.rstrip() + '\\n')\n            self.orig_stdout.flush()\n\n            if self.log_collection == 'none':\n                search_result = self.log_pattern.search(line)\n                if search_result:\n                    metrics = search_result.group(0)\n                    self.queue.put(metrics+'\\n')\n            else:\n                self.queue.put(line)\n\n        self.pipeReader.close()\n\n    def close(self):\n        \"\"\"Close the write end of the pipe.\n        \"\"\"\n        os.close(self.fdWrite)\n\n    @property\n    def is_read_completed(self):\n        \"\"\"Return if read is completed\n        \"\"\"\n        return self._is_read_completed\n\n    def set_process_exit(self):\n        self.process_exit = True\n        return self.process_exit",
  "def __init__(self, host, port, tag, trial_id, channel, std_output_type=StdOutputType.Stdout):\n        StreamHandler.__init__(self)\n        self.host = host\n        self.port = port\n        self.tag = tag\n        self.std_output_type = std_output_type\n        self.trial_id = trial_id\n        self.channel = channel\n        self.orig_stdout = sys.__stdout__\n        self.orig_stderr = sys.__stderr__",
  "def emit(self, record):\n        log_entry = {}\n        log_entry['tag'] = self.tag\n        log_entry['stdOutputType'] = self.std_output_type.name\n        log_entry['msg'] = self.format(record)\n\n        try:\n            if self.channel is None:\n                rest_post(gen_send_stdout_url(self.host, self.port), json.dumps(log_entry), 10, True)\n            else:\n                if self.trial_id is not None:\n                    log_entry[\"trial\"] = self.trial_id\n                self.channel.send(CommandType.StdOut, log_entry)\n        except Exception as e:\n            self.orig_stderr.write(str(e) + '\\n')\n            self.orig_stderr.flush()",
  "def __init__(self, syslog_host, syslog_port, tag, std_output_type, log_collection, trial_id=None, channel=None, log_level=logging.INFO):\n        '''\n        constructor\n        '''\n        logger_name = 'nni_syslog_{}'.format(tag)\n        # to prevent multiple trial logged in same logger\n        if trial_id is not None:\n            logger_name = '{}_{}'.format(logger_name, trial_id)\n        self.logger = logging.getLogger(logger_name)\n        self.log_level = log_level\n        self.logger.setLevel(self.log_level)\n        self.pipeReader = None\n        self.handler = NNIRestLogHanlder(syslog_host, syslog_port, tag, trial_id, channel)\n        self.logger.addHandler(self.handler)\n        if std_output_type == StdOutputType.Stdout:\n            self.orig_stdout = sys.__stdout__\n        else:\n            self.orig_stdout = sys.__stderr__\n        self.log_collection = log_collection",
  "def get_pipelog_reader(self):\n        '''\n        Get pipe for remote logger\n        '''\n        self.pipeReader = PipeLogReader(self.logger, self.log_collection, logging.INFO)\n        return self.pipeReader",
  "def flush(self):\n        '''\n        Add flush in handler\n        '''\n        for handler in self.logger.handlers:\n            handler.flush()",
  "def write(self, buf):\n        '''\n        Write buffer data into logger/stdout\n        '''\n        for line in buf.rstrip().splitlines():\n            self.orig_stdout.write(line.rstrip() + '\\n')\n            self.orig_stdout.flush()\n            try:\n                self.logger.log(self.log_level, line.rstrip())\n            except Exception:\n                pass",
  "def close(self):\n        '''\n        Close handlers and resources\n        '''\n        if self.pipeReader is not None:\n            self.pipeReader.set_process_exit()\n        for handler in self.logger.handlers:\n            handler.close()\n            self.logger.removeHandler(handler)",
  "def __init__(self, logger, log_collection, log_level=logging.INFO):\n        \"\"\"Setup the object with a logger and a loglevel\n        and start the thread\n        \"\"\"\n        threading.Thread.__init__(self)\n        self.queue = Queue()\n        self.logger = logger\n        self.daemon = False\n        self.log_level = log_level\n        self.fdRead, self.fdWrite = os.pipe()\n        self.pipeReader = os.fdopen(self.fdRead)\n        self.orig_stdout = sys.__stdout__\n        self._is_read_completed = False\n        self.process_exit = False\n        self.log_collection = log_collection\n        self.log_pattern = re.compile(r'NNISDK_MEb\\'.*\\'$')\n\n        def _populateQueue(stream, queue):\n            '''\n            Collect lines from 'stream' and put them in 'quque'.\n            '''\n            time.sleep(1)\n            while True:\n                cur_process_exit = self.process_exit\n                try:\n                    line = self.queue.get(True, 5)\n                    try:\n                        self.logger.log(self.log_level, line.rstrip())\n                    except Exception:\n                        pass\n                except Exception:\n                    if cur_process_exit == True:\n                        self._is_read_completed = True\n                        break\n\n        self.pip_log_reader_thread = threading.Thread(target=_populateQueue, args=(self.pipeReader, self.queue))\n        self.pip_log_reader_thread.daemon = True\n        self.start()\n        self.pip_log_reader_thread.start()",
  "def fileno(self):\n        \"\"\"Return the write file descriptor of the pipe\n        \"\"\"\n        return self.fdWrite",
  "def run(self):\n        \"\"\"Run the thread, logging everything.\n           If the log_collection is 'none', the log content will not be enqueued\n        \"\"\"\n        for line in iter(self.pipeReader.readline, ''):\n            self.orig_stdout.write(line.rstrip() + '\\n')\n            self.orig_stdout.flush()\n\n            if self.log_collection == 'none':\n                search_result = self.log_pattern.search(line)\n                if search_result:\n                    metrics = search_result.group(0)\n                    self.queue.put(metrics+'\\n')\n            else:\n                self.queue.put(line)\n\n        self.pipeReader.close()",
  "def close(self):\n        \"\"\"Close the write end of the pipe.\n        \"\"\"\n        os.close(self.fdWrite)",
  "def is_read_completed(self):\n        \"\"\"Return if read is completed\n        \"\"\"\n        return self._is_read_completed",
  "def set_process_exit(self):\n        self.process_exit = True\n        return self.process_exit",
  "def _populateQueue(stream, queue):\n            '''\n            Collect lines from 'stream' and put them in 'quque'.\n            '''\n            time.sleep(1)\n            while True:\n                cur_process_exit = self.process_exit\n                try:\n                    line = self.queue.get(True, 5)\n                    try:\n                        self.logger.log(self.log_level, line.rstrip())\n                    except Exception:\n                        pass\n                except Exception:\n                    if cur_process_exit == True:\n                        self._is_read_completed = True\n                        break",
  "class BaseChannel(ABC):\n    def __init__(self, args):\n        self.is_keep_parsed = args.node_count > 1\n        self.args = args\n        self.node_id = self.args.node_id\n\n    @abstractmethod\n    def _inner_send(self, message):\n        pass\n\n    @abstractmethod\n    def _inner_receive(self):\n        return []\n\n    @abstractmethod\n    def _inner_open(self):\n        pass\n\n    @abstractmethod\n    def _inner_close(self):\n        pass\n\n    def open(self):\n        # initialize receive, send threads.\n        self.is_running = True\n        self.receive_queue = Queue()\n        self.receive_thread = threading.Thread(target=self._receive_loop)\n        self.receive_thread.start()\n        self.send_queue = Queue()\n        self.send_thread = threading.Thread(target=self._send_loop)\n        self.send_thread.start()\n\n        self._inner_open()\n\n        client_info = {\n            \"isReady\": True,\n            \"runnerId\": self.args.runner_id,\n            \"expId\": self.args.exp_id,\n        }\n        nni_log(LogType.Info, 'Channel: send ready information %s' % client_info)\n        self.send(CommandType.Initialized, client_info)\n\n    def close(self):\n        self.is_running = False\n        try:\n            self._inner_close()\n        except Exception as err:\n            # ignore any error on closing\n            print(\"error on closing channel: %s\" % err)\n\n    def send(self, command, data):\n        \"\"\"Send command to Training Service.\n        command: CommandType object.\n        data: string payload.\n        the message is sent synchronized.\n        \"\"\"\n        data[\"node\"] = self.node_id\n        data = json.dumps(data)\n        data = data.encode('utf8')\n        message = b'%b%014d%b' % (command.value, len(data), data)\n        self.send_queue.put(message)\n\n    def sent(self):\n        return self.send_queue.qsize() == 0\n\n    def received(self):\n        return self.receive_queue.qsize() > 0\n\n    def receive(self):\n        \"\"\"Receive a command from Training Service.\n        Returns a tuple of command (CommandType) and payload (str)\n        \"\"\"\n        command = None\n        data = None\n\n        try:\n            command_content = self.receive_queue.get(False)\n            if command_content is not None:\n                if (len(command_content) < 16):\n                    # invalid header\n                    nni_log(LogType.Error, 'incorrect command is found, command must be greater than 16 bytes!')\n                    return None, None\n                header = command_content[:16]\n                command = CommandType(header[:2])\n                length = int(header[2:])\n                if (len(command_content)-16 != length):\n                    nni_log(LogType.Error, 'incorrect command length, length {}, actual data length is {}, header {}.'\n                            .format(length, len(command_content)-16, header))\n                    return None, None\n                data = command_content[16:16+length]\n                data = json.loads(data.decode('utf8'))\n                if self.node_id is None:\n                    nni_log(LogType.Info, 'Received command, header: [%s], data: [%s]' % (header, data))\n                else:\n                    nni_log(LogType.Info, 'Received command(%s), header: [%s], data: [%s]' % (self.node_id, header, data))\n        except Empty:\n            # do nothing, if no command received.\n            pass\n        except Exception as identifier:\n            nni_log(LogType.Error, 'meet unhandled exception in base_channel: %s' % identifier)\n        return command, data\n\n    def _fetch_message(self, buffer, has_new_line=False):\n        messages = []\n        while(len(buffer)) >= 16:\n            header = buffer[:16]\n            length = int(header[2:])\n\n            message_length = length+16\n            total_length = message_length\n            if has_new_line:\n                total_length += 1\n\n            # break, if buffer is too short.\n            if len(buffer) < total_length:\n                break\n            data = buffer[16:message_length]\n            if has_new_line and 10 != buffer[total_length-1]:\n                nni_log(LogType.Error, 'end of message should be \\\\n, but got {}'.format(self.in_cache[total_length-1]))\n            buffer = buffer[total_length:]\n            messages.append(header + data)\n\n        return messages, buffer\n\n    def _receive_loop(self):\n        while (self.is_running):\n            messages = self._inner_receive()\n            if messages is not None:\n                for message in messages:\n                    self.receive_queue.put(message)\n            time.sleep(INTERVAL_SECONDS)\n\n    def _send_loop(self):\n        while (self.is_running):\n            message = None\n            try:\n                # no sleep, since it's a block call with INTERVAL_SECONDS second timeout\n                message = self.send_queue.get(True, INTERVAL_SECONDS)\n            except Empty:\n                # do nothing, if no command received.\n                pass\n            if message is not None:\n                self._inner_send(message)",
  "def __init__(self, args):\n        self.is_keep_parsed = args.node_count > 1\n        self.args = args\n        self.node_id = self.args.node_id",
  "def _inner_send(self, message):\n        pass",
  "def _inner_receive(self):\n        return []",
  "def _inner_open(self):\n        pass",
  "def _inner_close(self):\n        pass",
  "def open(self):\n        # initialize receive, send threads.\n        self.is_running = True\n        self.receive_queue = Queue()\n        self.receive_thread = threading.Thread(target=self._receive_loop)\n        self.receive_thread.start()\n        self.send_queue = Queue()\n        self.send_thread = threading.Thread(target=self._send_loop)\n        self.send_thread.start()\n\n        self._inner_open()\n\n        client_info = {\n            \"isReady\": True,\n            \"runnerId\": self.args.runner_id,\n            \"expId\": self.args.exp_id,\n        }\n        nni_log(LogType.Info, 'Channel: send ready information %s' % client_info)\n        self.send(CommandType.Initialized, client_info)",
  "def close(self):\n        self.is_running = False\n        try:\n            self._inner_close()\n        except Exception as err:\n            # ignore any error on closing\n            print(\"error on closing channel: %s\" % err)",
  "def send(self, command, data):\n        \"\"\"Send command to Training Service.\n        command: CommandType object.\n        data: string payload.\n        the message is sent synchronized.\n        \"\"\"\n        data[\"node\"] = self.node_id\n        data = json.dumps(data)\n        data = data.encode('utf8')\n        message = b'%b%014d%b' % (command.value, len(data), data)\n        self.send_queue.put(message)",
  "def sent(self):\n        return self.send_queue.qsize() == 0",
  "def received(self):\n        return self.receive_queue.qsize() > 0",
  "def receive(self):\n        \"\"\"Receive a command from Training Service.\n        Returns a tuple of command (CommandType) and payload (str)\n        \"\"\"\n        command = None\n        data = None\n\n        try:\n            command_content = self.receive_queue.get(False)\n            if command_content is not None:\n                if (len(command_content) < 16):\n                    # invalid header\n                    nni_log(LogType.Error, 'incorrect command is found, command must be greater than 16 bytes!')\n                    return None, None\n                header = command_content[:16]\n                command = CommandType(header[:2])\n                length = int(header[2:])\n                if (len(command_content)-16 != length):\n                    nni_log(LogType.Error, 'incorrect command length, length {}, actual data length is {}, header {}.'\n                            .format(length, len(command_content)-16, header))\n                    return None, None\n                data = command_content[16:16+length]\n                data = json.loads(data.decode('utf8'))\n                if self.node_id is None:\n                    nni_log(LogType.Info, 'Received command, header: [%s], data: [%s]' % (header, data))\n                else:\n                    nni_log(LogType.Info, 'Received command(%s), header: [%s], data: [%s]' % (self.node_id, header, data))\n        except Empty:\n            # do nothing, if no command received.\n            pass\n        except Exception as identifier:\n            nni_log(LogType.Error, 'meet unhandled exception in base_channel: %s' % identifier)\n        return command, data",
  "def _fetch_message(self, buffer, has_new_line=False):\n        messages = []\n        while(len(buffer)) >= 16:\n            header = buffer[:16]\n            length = int(header[2:])\n\n            message_length = length+16\n            total_length = message_length\n            if has_new_line:\n                total_length += 1\n\n            # break, if buffer is too short.\n            if len(buffer) < total_length:\n                break\n            data = buffer[16:message_length]\n            if has_new_line and 10 != buffer[total_length-1]:\n                nni_log(LogType.Error, 'end of message should be \\\\n, but got {}'.format(self.in_cache[total_length-1]))\n            buffer = buffer[total_length:]\n            messages.append(header + data)\n\n        return messages, buffer",
  "def _receive_loop(self):\n        while (self.is_running):\n            messages = self._inner_receive()\n            if messages is not None:\n                for message in messages:\n                    self.receive_queue.put(message)\n            time.sleep(INTERVAL_SECONDS)",
  "def _send_loop(self):\n        while (self.is_running):\n            message = None\n            try:\n                # no sleep, since it's a block call with INTERVAL_SECONDS second timeout\n                message = self.send_queue.get(True, INTERVAL_SECONDS)\n            except Empty:\n                # do nothing, if no command received.\n                pass\n            if message is not None:\n                self._inner_send(message)",
  "class WebChannel(BaseChannel):\n\n    def __init__(self, args):\n        self.node_id = args.node_id\n        self.args = args\n        self.client = None\n        self.in_cache = b\"\"\n        self.timeout = 10\n\n        super(WebChannel, self).__init__(args)\n\n        self._event_loop = None\n\n    def _inner_open(self):\n        url = \"ws://{}:{}\".format(self.args.nnimanager_ip, self.args.nnimanager_port)\n        try:\n            connect = asyncio.wait_for(websockets.connect(url), self.timeout)\n            self._event_loop = asyncio.get_event_loop()\n            client = self._event_loop.run_until_complete(connect)\n            self.client = client\n            nni_log(LogType.Info, 'WebChannel: connected with info %s' % url)\n        except asyncio.TimeoutError:\n            nni_log(LogType.Error, 'connect to %s timeout! Please make sure NNIManagerIP configured correctly, and accessable.' % url)\n            os._exit(1)\n\n    def _inner_close(self):\n        if self.client is not None:\n            self.client.close()\n            self.client = None\n            if self._event_loop.is_running():\n                self._event_loop.stop()\n            self._event_loop = None\n\n    def _inner_send(self, message):\n        loop = asyncio.new_event_loop()\n        loop.run_until_complete(self.client.send(message))\n\n    def _inner_receive(self):\n        messages = []\n        if self.client is not None:\n            received = self._event_loop.run_until_complete(self.client.recv())\n            # receive message is string, to get consistent result, encode it here.\n            self.in_cache += received.encode(\"utf8\")\n            messages, self.in_cache = self._fetch_message(self.in_cache)\n\n        return messages",
  "def __init__(self, args):\n        self.node_id = args.node_id\n        self.args = args\n        self.client = None\n        self.in_cache = b\"\"\n        self.timeout = 10\n\n        super(WebChannel, self).__init__(args)\n\n        self._event_loop = None",
  "def _inner_open(self):\n        url = \"ws://{}:{}\".format(self.args.nnimanager_ip, self.args.nnimanager_port)\n        try:\n            connect = asyncio.wait_for(websockets.connect(url), self.timeout)\n            self._event_loop = asyncio.get_event_loop()\n            client = self._event_loop.run_until_complete(connect)\n            self.client = client\n            nni_log(LogType.Info, 'WebChannel: connected with info %s' % url)\n        except asyncio.TimeoutError:\n            nni_log(LogType.Error, 'connect to %s timeout! Please make sure NNIManagerIP configured correctly, and accessable.' % url)\n            os._exit(1)",
  "def _inner_close(self):\n        if self.client is not None:\n            self.client.close()\n            self.client = None\n            if self._event_loop.is_running():\n                self._event_loop.stop()\n            self._event_loop = None",
  "def _inner_send(self, message):\n        loop = asyncio.new_event_loop()\n        loop.run_until_complete(self.client.send(message))",
  "def _inner_receive(self):\n        messages = []\n        if self.client is not None:\n            received = self._event_loop.run_until_complete(self.client.recv())\n            # receive message is string, to get consistent result, encode it here.\n            self.in_cache += received.encode(\"utf8\")\n            messages, self.in_cache = self._fetch_message(self.in_cache)\n\n        return messages",
  "class CommandType(Enum):\n    Initialize = b'IN'\n    RequestTrialJobs = b'GE'\n    ReportMetricData = b'ME'\n    ReportGpuInfo = b'GI'\n    UpdateSearchSpace = b'SS'\n    ImportData = b'FD'\n    AddCustomizedTrialJob = b'AD'\n    TrialEnd = b'EN'\n    Terminate = b'TE'\n    Ping = b'PI'\n\n    Initialized = b'ID'\n    NewTrialJob = b'TR'\n    SendTrialJobParameter = b'SP'\n    NoMoreTrialJobs = b'NO'\n    KillTrialJob = b'KI'\n    StdOut = b'SO'\n    VersionCheck = b'VC'",
  "def main_loop(args):\n    '''main loop logic for trial runner'''\n    idle_last_time = datetime.now()\n    gpu_refresh_last_time = datetime.now() - timedelta(minutes=1)\n\n    try:\n        trials = dict()\n\n        command_channel = args.command_channel\n        # command loop\n        while True:\n            command_type, command_data = command_channel.receive()\n            if command_type == CommandType.NewTrialJob:\n                trial_id = command_data[\"trialId\"]\n                if trial_id in trials.keys():\n                    trial = trials[trial_id]\n                    if trial.is_running():\n                        raise Exception('trial %s is running already, cannot start a new one' % trial.id)\n                    else:\n                        del trials[trial_id]\n                trial = Trial(args, command_data)\n                trial.run()\n                trials[trial_id] = trial\n            elif command_type == CommandType.KillTrialJob:\n                trial_id = command_data\n                if trial_id in trials.keys():\n                    trial = trials[trial_id]\n                    trial.kill(command_data)\n            elif command_type == CommandType.SendTrialJobParameter:\n                trial_id = command_data[\"trialId\"]\n                if trial_id in trials.keys():\n                    trial = trials[trial_id]\n                    trial.save_parameter_file(command_data)\n            elif command_type is not None:\n                raise Exception(\"unknown command %s\" % command_type)\n\n            trial_list = list(trials.values())\n            for trial in trial_list:\n                if trial is not None and trial.is_running():\n                    idle_last_time = datetime.now()\n                else:\n                    del trials[trial.id]\n\n            if (datetime.now() - idle_last_time).seconds > idle_timeout_seconds:\n                nni_log(LogType.Info, \"trial runner is idle more than {0} seconds, so exit.\".format(\n                    idle_timeout_seconds))\n                break\n\n            if args.enable_gpu_collect and (datetime.now() - gpu_refresh_last_time).seconds > gpu_refressh_interval_seconds:\n                # collect gpu information\n                gpu_info = collect_gpu_usage(args.node_id)\n                command_channel.send(CommandType.ReportGpuInfo, gpu_info)\n                gpu_refresh_last_time = datetime.now()\n            time.sleep(0.5)\n    except Exception as ex:\n        traceback.print_exc()\n        raise ex\n    finally:\n        nni_log(LogType.Info, \"main_loop exits.\")\n\n        trial_list = list(trials.values())\n        for trial in trial_list:\n            trial.kill()\n            del trials[trial.id]\n        # wait to send commands\n        for _ in range(10):\n            if command_channel.sent():\n                break\n            time.sleep(1)\n        command_channel.close()",
  "def trial_runner_help_info(*args):\n    print('please run --help to see guidance')",
  "def check_version(args):\n    try:\n        trial_runner_version = pkg_resources.get_distribution('nni').version\n    except pkg_resources.ResolutionError as err:\n        # package nni does not exist, try nni-tool package\n        nni_log(LogType.Error, 'Package nni does not exist!')\n        os._exit(1)\n    if not args.nni_manager_version:\n        # skip version check\n        nni_log(LogType.Warning, 'Skipping version check!')\n    else:\n        try:\n            command_channel = args.command_channel\n            trial_runner_version = regular.search(trial_runner_version).group('version')\n            nni_log(LogType.Info, '{0}: runner_version is {1}'.format(args.node_id, trial_runner_version))\n            nni_manager_version = regular.search(args.nni_manager_version).group('version')\n            nni_log(LogType.Info, '{0}: nni_manager_version is {1}'.format(args.node_id, nni_manager_version))\n            log_entry = {}\n            if trial_runner_version != nni_manager_version:\n                nni_log(LogType.Error, '{0}: Version does not match!'.format(args.node_id))\n                error_message = '{0}: NNIManager version is {1}, Trial runner version is {2}, NNI version does not match!'.format(\n                    args.node_id, nni_manager_version, trial_runner_version)\n                log_entry['tag'] = 'VCFail'\n                log_entry['msg'] = error_message\n                command_channel.send(CommandType.VersionCheck, log_entry)\n                while not command_channel.sent():\n                    time.sleep(1)\n                os._exit(1)\n            else:\n                nni_log(LogType.Info, '{0}: Version match!'.format(args.node_id))\n                log_entry['tag'] = 'VCSuccess'\n                command_channel.send(CommandType.VersionCheck, log_entry)\n        except AttributeError as err:\n            nni_log(LogType.Error, '{0}: {1}'.format(args.node_id, err))",
  "class AMLChannel(BaseChannel):\n    def __init__(self, args):\n        self.args = args\n        self.run = Run.get_context()\n        super(AMLChannel, self).__init__(args)\n        self.current_message_index = -1\n\n    def _inner_open(self):\n        pass\n\n    def _inner_close(self):\n        pass\n\n    def _inner_send(self, message):\n        try:\n            self.run.log('trial_runner', message.decode('utf8'))\n        except Exception as exception:\n            nni_log(LogType.Error, 'meet unhandled exception when send message: %s' % exception)\n\n    def _inner_receive(self):\n        messages = []\n        message_dict = self.run.get_metrics()\n        if 'nni_manager' not in message_dict:\n            return []\n        message_list = message_dict['nni_manager']\n        if not message_list:\n            return messages\n        if type(message_list) is list:\n            if self.current_message_index < len(message_list) - 1:\n                messages = message_list[self.current_message_index + 1 : len(message_list)]\n                self.current_message_index = len(message_list) - 1\n        elif self.current_message_index == -1:\n            messages = [message_list]\n            self.current_message_index += 1\n        newMessage = []\n        for message in messages:\n            # receive message is string, to get consistent result, encode it here.\n            newMessage.append(message.encode('utf8'))\n        return newMessage",
  "def __init__(self, args):\n        self.args = args\n        self.run = Run.get_context()\n        super(AMLChannel, self).__init__(args)\n        self.current_message_index = -1",
  "def _inner_open(self):\n        pass",
  "def _inner_close(self):\n        pass",
  "def _inner_send(self, message):\n        try:\n            self.run.log('trial_runner', message.decode('utf8'))\n        except Exception as exception:\n            nni_log(LogType.Error, 'meet unhandled exception when send message: %s' % exception)",
  "def _inner_receive(self):\n        messages = []\n        message_dict = self.run.get_metrics()\n        if 'nni_manager' not in message_dict:\n            return []\n        message_list = message_dict['nni_manager']\n        if not message_list:\n            return messages\n        if type(message_list) is list:\n            if self.current_message_index < len(message_list) - 1:\n                messages = message_list[self.current_message_index + 1 : len(message_list)]\n                self.current_message_index = len(message_list) - 1\n        elif self.current_message_index == -1:\n            messages = [message_list]\n            self.current_message_index += 1\n        newMessage = []\n        for message in messages:\n            # receive message is string, to get consistent result, encode it here.\n            newMessage.append(message.encode('utf8'))\n        return newMessage",
  "def collect_gpu_usage(node_id):\n    cmd = 'nvidia-smi -q -x'.split()\n    info = None\n    try:\n        smi_output = subprocess.check_output(cmd)\n        info = parse_nvidia_smi_result(smi_output)\n    except Exception:\n        traceback.print_exc()\n        info = gen_empty_gpu_metric()\n    return info",
  "def parse_nvidia_smi_result(smi):\n    try:\n        output = {}\n        xmldoc = minidom.parseString(smi)\n        gpuList = xmldoc.getElementsByTagName('gpu')\n        output[\"Timestamp\"] = time.asctime(time.localtime())\n        output[\"gpuCount\"] = len(gpuList)\n        output[\"gpuInfos\"] = []\n        for gpuIndex, gpu in enumerate(gpuList):\n            gpuInfo = {}\n            gpuInfo['index'] = gpuIndex\n            gpuInfo['gpuUtil'] = gpu.getElementsByTagName('utilization')[0]\\\n                .getElementsByTagName('gpu_util')[0]\\\n                .childNodes[0].data.replace(\"%\", \"\").strip()\n            gpuInfo['gpuMemUtil'] = gpu.getElementsByTagName('utilization')[0]\\\n                .getElementsByTagName('memory_util')[0]\\\n                .childNodes[0].data.replace(\"%\", \"\").strip()\n            processes = gpu.getElementsByTagName('processes')\n            runningProNumber = len(processes[0].getElementsByTagName('process_info'))\n            gpuInfo['activeProcessNum'] = runningProNumber\n\n            gpuInfo['gpuType'] = gpu.getElementsByTagName('product_name')[0]\\\n                .childNodes[0].data\n            memUsage = gpu.getElementsByTagName('fb_memory_usage')[0]\n            gpuInfo['gpuMemTotal'] = memUsage.getElementsByTagName('total')[0]\\\n                .childNodes[0].data.replace(\"MiB\", \"\").strip()\n            gpuInfo['gpuMemUsed'] = memUsage.getElementsByTagName('used')[0]\\\n                .childNodes[0].data.replace(\"MiB\", \"\").strip()\n            gpuInfo['gpuMemFree'] = memUsage.getElementsByTagName('free')[0]\\\n                .childNodes[0].data.replace(\"MiB\", \"\").strip()\n\n            output[\"gpuInfos\"].append(gpuInfo)\n    except Exception:\n        traceback.print_exc()\n        output = {}\n    return output",
  "def gen_empty_gpu_metric():\n    try:\n        output = {}\n        output[\"Timestamp\"] = time.asctime(time.localtime())\n        output[\"gpuCount\"] = 0\n        output[\"gpuInfos\"] = []\n    except Exception:\n        traceback.print_exc()\n        output = {}\n    return output",
  "def gen_send_stdout_url(ip, port):\n    '''Generate send stdout url'''\n    return '{0}:{1}{2}{3}/{4}/{5}'.format(BASE_URL.format(ip), port, API_ROOT_URL, STDOUT_API, NNI_EXP_ID, NNI_TRIAL_JOB_ID)",
  "def gen_send_version_url(ip, port):\n    '''Generate send error url'''\n    return '{0}:{1}{2}{3}/{4}/{5}'.format(BASE_URL.format(ip), port, API_ROOT_URL, VERSION_API, NNI_EXP_ID, NNI_TRIAL_JOB_ID)",
  "def gen_parameter_meta_url(ip, port):\n    '''Generate send error url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL.format(ip), port, API_ROOT_URL, PARAMETER_META_API)",
  "class Trial:\n    def __init__(self, args, data):\n        self.process = None\n        self.data = data\n        self.args = args\n        self.command_channel = args.command_channel\n        self.trial_syslogger_stdout = None\n\n        global NNI_TRIAL_JOB_ID\n        self.id = data[\"trialId\"]\n        if self.id is None:\n            raise Exception(\"trial_id is not found in %s\" % data)\n        os.environ['NNI_TRIAL_JOB_ID'] = self.id\n        NNI_TRIAL_JOB_ID = self.id\n\n        # for multiple nodes. If it's None, it means single node.\n        self.node_id = args.node_id\n        if self.node_id is None:\n            self.name = self.id\n        else:\n            self.name = \"%s_%s\" % (self.id, self.node_id)\n\n    def run(self):\n        # redirect trial's stdout and stderr to syslog\n        self.trial_syslogger_stdout = RemoteLogger(self.args.nnimanager_ip, self.args.nnimanager_port, 'trial', StdOutputType.Stdout,\n                                                   self.args.log_collection, self.id, self.args.command_channel)\n\n        nni_log(LogType.Info, \"%s: start to run trial\" % self.name)\n\n        trial_working_dir = os.path.realpath(os.path.join(os.curdir, \"..\", \"..\", \"trials\", self.id))\n        self.trial_output_dir = os.path.join(trial_working_dir, trial_output_path_name)\n        trial_code_dir = os.path.join(trial_working_dir, \"code\")\n        trial_nnioutput_dir = os.path.join(trial_working_dir, \"nnioutput\")\n\n        environ = os.environ.copy()\n        environ['NNI_TRIAL_SEQ_ID'] = str(self.data[\"sequenceId\"])\n        environ['NNI_OUTPUT_DIR'] = os.path.join(trial_working_dir, \"nnioutput\")\n        environ['NNI_SYS_DIR'] = trial_working_dir\n        self.working_dir = trial_working_dir\n\n        # prepare code and parameters\n        prepared_flag_file_name = os.path.join(trial_working_dir, \"trial_prepared\")\n        if not os.path.exists(trial_working_dir):\n            os.makedirs(trial_working_dir, exist_ok=True)\n\n            os.makedirs(self.trial_output_dir, exist_ok=True)\n            os.makedirs(trial_nnioutput_dir, exist_ok=True)\n            # prepare code\n            os.makedirs(trial_code_dir, exist_ok=True)\n            with tarfile.open(os.path.join(\"..\", \"nni-code.tar.gz\"), \"r:gz\") as tar:\n                tar.extractall(trial_code_dir)\n\n            # save parameters\n            nni_log(LogType.Info, '%s: saving parameter %s' % (self.name, self.data[\"parameter\"][\"value\"]))\n            parameter_file_name = os.path.join(trial_working_dir, \"parameter.cfg\")\n            with open(parameter_file_name, \"w\") as parameter_file:\n                parameter_file.write(self.data[\"parameter\"][\"value\"])\n\n            # ready flag\n            with open(prepared_flag_file_name, \"w\") as prepared_flag_file:\n                prepared_flag_file.write(\"%s\" % (int(datetime.now().timestamp() * 1000)))\n\n        # make sure code prepared by other node.\n        if self.node_id is not None:\n            while True:\n                if os.path.exists(prepared_flag_file_name):\n                    break\n                time.sleep(0.1)\n\n        trial_command = self.args.trial_command\n\n        gpuIndices = self.data.get('gpuIndices')\n        if (gpuIndices is not None):\n            trial_command = 'CUDA_VISIBLE_DEVICES=\"%s \" %s' % (gpuIndices, trial_command)\n\n        self.log_pipe_stdout = self.trial_syslogger_stdout.get_pipelog_reader()\n        self.process = Popen(trial_command, shell=True, stdout=self.log_pipe_stdout,\n                             stderr=self.log_pipe_stdout, cwd=trial_code_dir, env=dict(environ))\n        nni_log(LogType.Info, '{0}: spawns a subprocess (pid {1}) to run command: {2}'.\n                format(self.name, self.process.pid, shlex.split(trial_command)))\n\n    def save_parameter_file(self, command_data):\n        parameters = command_data[\"parameters\"]\n        file_index = int(parameters[\"index\"])\n        if file_index == 0:\n            parameter_file_name = \"parameter.cfg\"\n        else:\n            parameter_file_name = \"parameter_{}.cfg\".format(file_index)\n        parameter_file_name = os.path.join(self.working_dir, parameter_file_name)\n        with open(parameter_file_name, \"w\") as parameter_file:\n            nni_log(LogType.Info, '%s: saving parameter %s' % (self.name, parameters[\"value\"]))\n            parameter_file.write(parameters[\"value\"])\n\n    def is_running(self):\n        if (self.process is None):\n            return False\n\n        retCode = self.process.poll()\n        # child worker process exits and all stdout data is read\n        if retCode is not None and self.log_pipe_stdout.set_process_exit() and self.log_pipe_stdout.is_read_completed == True:\n            # In Windows, the retCode -1 is 4294967295. It's larger than c_long, and raise OverflowError.\n            # So covert it to int32.\n            retCode = ctypes.c_long(retCode).value\n            nni_log(LogType.Info, '{0}: subprocess terminated. Exit code is {1}.'.format(self.name, retCode))\n\n            end_time = int(datetime.now().timestamp() * 1000)\n            end_message = {\n                \"code\": retCode,\n                \"time\": end_time,\n                \"trial\": self.id,\n            }\n            self.command_channel.send(CommandType.TrialEnd, end_message)\n            self.cleanup()\n            return False\n        else:\n            return True\n\n    def kill(self, trial_id=None):\n        if trial_id == self.id or trial_id is None:\n            if self.process is not None:\n                nni_log(LogType.Info, \"%s: killing trial\" % self.name)\n                for child in psutil.Process(self.process.pid).children(True):\n                    child.kill()\n                self.process.kill()\n            self.cleanup()\n\n    def cleanup(self):\n        nni_log(LogType.Info, \"%s: clean up trial\" % self.name)\n        self.process = None\n        if self.log_pipe_stdout is not None:\n            self.log_pipe_stdout.set_process_exit()\n            self.log_pipe_stdout = None\n        if self.trial_syslogger_stdout is not None:\n            self.trial_syslogger_stdout.close()\n            self.trial_syslogger_stdout = None",
  "def __init__(self, args, data):\n        self.process = None\n        self.data = data\n        self.args = args\n        self.command_channel = args.command_channel\n        self.trial_syslogger_stdout = None\n\n        global NNI_TRIAL_JOB_ID\n        self.id = data[\"trialId\"]\n        if self.id is None:\n            raise Exception(\"trial_id is not found in %s\" % data)\n        os.environ['NNI_TRIAL_JOB_ID'] = self.id\n        NNI_TRIAL_JOB_ID = self.id\n\n        # for multiple nodes. If it's None, it means single node.\n        self.node_id = args.node_id\n        if self.node_id is None:\n            self.name = self.id\n        else:\n            self.name = \"%s_%s\" % (self.id, self.node_id)",
  "def run(self):\n        # redirect trial's stdout and stderr to syslog\n        self.trial_syslogger_stdout = RemoteLogger(self.args.nnimanager_ip, self.args.nnimanager_port, 'trial', StdOutputType.Stdout,\n                                                   self.args.log_collection, self.id, self.args.command_channel)\n\n        nni_log(LogType.Info, \"%s: start to run trial\" % self.name)\n\n        trial_working_dir = os.path.realpath(os.path.join(os.curdir, \"..\", \"..\", \"trials\", self.id))\n        self.trial_output_dir = os.path.join(trial_working_dir, trial_output_path_name)\n        trial_code_dir = os.path.join(trial_working_dir, \"code\")\n        trial_nnioutput_dir = os.path.join(trial_working_dir, \"nnioutput\")\n\n        environ = os.environ.copy()\n        environ['NNI_TRIAL_SEQ_ID'] = str(self.data[\"sequenceId\"])\n        environ['NNI_OUTPUT_DIR'] = os.path.join(trial_working_dir, \"nnioutput\")\n        environ['NNI_SYS_DIR'] = trial_working_dir\n        self.working_dir = trial_working_dir\n\n        # prepare code and parameters\n        prepared_flag_file_name = os.path.join(trial_working_dir, \"trial_prepared\")\n        if not os.path.exists(trial_working_dir):\n            os.makedirs(trial_working_dir, exist_ok=True)\n\n            os.makedirs(self.trial_output_dir, exist_ok=True)\n            os.makedirs(trial_nnioutput_dir, exist_ok=True)\n            # prepare code\n            os.makedirs(trial_code_dir, exist_ok=True)\n            with tarfile.open(os.path.join(\"..\", \"nni-code.tar.gz\"), \"r:gz\") as tar:\n                tar.extractall(trial_code_dir)\n\n            # save parameters\n            nni_log(LogType.Info, '%s: saving parameter %s' % (self.name, self.data[\"parameter\"][\"value\"]))\n            parameter_file_name = os.path.join(trial_working_dir, \"parameter.cfg\")\n            with open(parameter_file_name, \"w\") as parameter_file:\n                parameter_file.write(self.data[\"parameter\"][\"value\"])\n\n            # ready flag\n            with open(prepared_flag_file_name, \"w\") as prepared_flag_file:\n                prepared_flag_file.write(\"%s\" % (int(datetime.now().timestamp() * 1000)))\n\n        # make sure code prepared by other node.\n        if self.node_id is not None:\n            while True:\n                if os.path.exists(prepared_flag_file_name):\n                    break\n                time.sleep(0.1)\n\n        trial_command = self.args.trial_command\n\n        gpuIndices = self.data.get('gpuIndices')\n        if (gpuIndices is not None):\n            trial_command = 'CUDA_VISIBLE_DEVICES=\"%s \" %s' % (gpuIndices, trial_command)\n\n        self.log_pipe_stdout = self.trial_syslogger_stdout.get_pipelog_reader()\n        self.process = Popen(trial_command, shell=True, stdout=self.log_pipe_stdout,\n                             stderr=self.log_pipe_stdout, cwd=trial_code_dir, env=dict(environ))\n        nni_log(LogType.Info, '{0}: spawns a subprocess (pid {1}) to run command: {2}'.\n                format(self.name, self.process.pid, shlex.split(trial_command)))",
  "def save_parameter_file(self, command_data):\n        parameters = command_data[\"parameters\"]\n        file_index = int(parameters[\"index\"])\n        if file_index == 0:\n            parameter_file_name = \"parameter.cfg\"\n        else:\n            parameter_file_name = \"parameter_{}.cfg\".format(file_index)\n        parameter_file_name = os.path.join(self.working_dir, parameter_file_name)\n        with open(parameter_file_name, \"w\") as parameter_file:\n            nni_log(LogType.Info, '%s: saving parameter %s' % (self.name, parameters[\"value\"]))\n            parameter_file.write(parameters[\"value\"])",
  "def is_running(self):\n        if (self.process is None):\n            return False\n\n        retCode = self.process.poll()\n        # child worker process exits and all stdout data is read\n        if retCode is not None and self.log_pipe_stdout.set_process_exit() and self.log_pipe_stdout.is_read_completed == True:\n            # In Windows, the retCode -1 is 4294967295. It's larger than c_long, and raise OverflowError.\n            # So covert it to int32.\n            retCode = ctypes.c_long(retCode).value\n            nni_log(LogType.Info, '{0}: subprocess terminated. Exit code is {1}.'.format(self.name, retCode))\n\n            end_time = int(datetime.now().timestamp() * 1000)\n            end_message = {\n                \"code\": retCode,\n                \"time\": end_time,\n                \"trial\": self.id,\n            }\n            self.command_channel.send(CommandType.TrialEnd, end_message)\n            self.cleanup()\n            return False\n        else:\n            return True",
  "def kill(self, trial_id=None):\n        if trial_id == self.id or trial_id is None:\n            if self.process is not None:\n                nni_log(LogType.Info, \"%s: killing trial\" % self.name)\n                for child in psutil.Process(self.process.pid).children(True):\n                    child.kill()\n                self.process.kill()\n            self.cleanup()",
  "def cleanup(self):\n        nni_log(LogType.Info, \"%s: clean up trial\" % self.name)\n        self.process = None\n        if self.log_pipe_stdout is not None:\n            self.log_pipe_stdout.set_process_exit()\n            self.log_pipe_stdout = None\n        if self.trial_syslogger_stdout is not None:\n            self.trial_syslogger_stdout.close()\n            self.trial_syslogger_stdout = None",
  "def copyHdfsDirectoryToLocal(hdfsDirectory, localDirectory, hdfsClient):\n    '''Copy directory from HDFS to local'''\n    if not os.path.exists(localDirectory):\n        os.makedirs(localDirectory)\n    try:\n        listing = hdfsClient.list_status(hdfsDirectory)\n    except Exception as exception:\n        nni_log(LogType.Error, 'List hdfs directory {0} error: {1}'.format(hdfsDirectory, str(exception)))\n        raise exception\n\n    for f in listing:\n        if f.type == 'DIRECTORY':\n            subHdfsDirectory = posixpath.join(hdfsDirectory, f.pathSuffix)\n            subLocalDirectory = os.path.join(localDirectory, f.pathSuffix)\n            copyHdfsDirectoryToLocal(subHdfsDirectory, subLocalDirectory, hdfsClient)\n        elif f.type == 'FILE':\n            hdfsFilePath = posixpath.join(hdfsDirectory, f.pathSuffix)\n            localFilePath = os.path.join(localDirectory, f.pathSuffix)\n            copyHdfsFileToLocal(hdfsFilePath, localFilePath, hdfsClient)\n        else:\n            raise AssertionError('unexpected type {}'.format(f.type))",
  "def copyHdfsFileToLocal(hdfsFilePath, localFilePath, hdfsClient, override=True):\n    '''Copy file from HDFS to local'''\n    if not hdfsClient.exists(hdfsFilePath):\n        raise Exception('HDFS file {} does not exist!'.format(hdfsFilePath))\n    try:\n        file_status = hdfsClient.get_file_status(hdfsFilePath)\n        if file_status.type != 'FILE':\n            raise Exception('HDFS file path {} is not a file'.format(hdfsFilePath))\n    except Exception as exception:\n        nni_log(LogType.Error, 'Get hdfs file {0} status error: {1}'.format(hdfsFilePath, str(exception)))\n        raise exception\n\n    if os.path.exists(localFilePath) and override:\n        os.remove(localFilePath)\n    try:\n        hdfsClient.copy_to_local(hdfsFilePath, localFilePath)\n    except Exception as exception:\n        nni_log(LogType.Error, 'Copy hdfs file {0} to {1} error: {2}'.format(hdfsFilePath, localFilePath, str(exception)))\n        raise exception\n    nni_log(LogType.Info, 'Successfully copied hdfs file {0} to {1}, {2} bytes'.format(hdfsFilePath, localFilePath, file_status.length))",
  "def copyDirectoryToHdfs(localDirectory, hdfsDirectory, hdfsClient):\n    '''Copy directory from local to HDFS'''\n    if not os.path.exists(localDirectory):\n        raise Exception('Local Directory does not exist!')\n    hdfsClient.mkdirs(hdfsDirectory)\n    result = True\n    for file in os.listdir(localDirectory):\n        file_path = os.path.join(localDirectory, file)\n        if os.path.isdir(file_path):\n            hdfs_directory = os.path.join(hdfsDirectory, file)\n            try:\n                result = result and copyDirectoryToHdfs(file_path, hdfs_directory, hdfsClient)\n            except Exception as exception:\n                nni_log(LogType.Error,\n                        'Copy local directory {0} to hdfs directory {1} error: {2}'.format(file_path, hdfs_directory, str(exception)))\n                result = False\n        else:\n            hdfs_file_path = os.path.join(hdfsDirectory, file)\n            try:\n                result = result and copyFileToHdfs(file_path, hdfs_file_path, hdfsClient)\n            except Exception as exception:\n                nni_log(LogType.Error, 'Copy local file {0} to hdfs {1} error: {2}'.format(file_path, hdfs_file_path, str(exception)))\n                result = False\n    return result",
  "def copyFileToHdfs(localFilePath, hdfsFilePath, hdfsClient, override=True):\n    '''Copy a local file to HDFS directory'''\n    if not os.path.exists(localFilePath):\n        raise Exception('Local file Path does not exist!')\n    if os.path.isdir(localFilePath):\n        raise Exception('localFile should not a directory!')\n    if hdfsClient.exists(hdfsFilePath):\n        if override:\n            hdfsClient.delete(hdfsFilePath)\n        else:\n            return False\n    try:\n        hdfsClient.copy_from_local(localFilePath, hdfsFilePath)\n        return True\n    except Exception as exception:\n        nni_log(LogType.Error, 'Copy local file {0} to hdfs file {1} error: {2}'.format(localFilePath, hdfsFilePath, str(exception)))\n        return False",
  "def main(argv):\n    metrics_output_dir = os.environ['METRIC_OUTPUT_DIR']\n\n    cmd = 'nvidia-smi -q -x'.split()\n    while(True):\n        try:\n            smi_output = subprocess.check_output(cmd)\n        except Exception:\n            traceback.print_exc()\n            gen_empty_gpu_metric(metrics_output_dir)\n            break\n        parse_nvidia_smi_result(smi_output, metrics_output_dir)\n        # TODO: change to sleep time configurable via arguments\n        time.sleep(5)",
  "def parse_nvidia_smi_result(smi, outputDir):\n    try:\n        old_umask = os.umask(0)\n        xmldoc = minidom.parseString(smi)\n        gpuList = xmldoc.getElementsByTagName('gpu')\n        with open(os.path.join(outputDir, \"gpu_metrics\"), 'a') as outputFile:\n            outPut = {}\n            outPut[\"Timestamp\"] = time.asctime(time.localtime())\n            outPut[\"gpuCount\"] = len(gpuList)\n            outPut[\"gpuInfos\"] = []\n            for gpuIndex, gpu in enumerate(gpuList):\n                gpuInfo = {}\n                gpuInfo['index'] = gpuIndex\n                gpuInfo['gpuUtil'] = gpu.getElementsByTagName('utilization')[0]\\\n                    .getElementsByTagName('gpu_util')[0]\\\n                    .childNodes[0].data.replace(\"%\", \"\").strip()\n                gpuInfo['gpuMemUtil'] = gpu.getElementsByTagName('utilization')[0]\\\n                    .getElementsByTagName('memory_util')[0]\\\n                    .childNodes[0].data.replace(\"%\", \"\").strip()\n                processes = gpu.getElementsByTagName('processes')\n                runningProNumber = len(processes[0].getElementsByTagName('process_info'))\n                gpuInfo['activeProcessNum'] = runningProNumber\n\n                outPut[\"gpuInfos\"].append(gpuInfo)\n            print(outPut)\n            outputFile.write(\"{}\\n\".format(json.dumps(outPut, sort_keys=True)))\n            outputFile.flush()\n    except Exception as error:\n        # e_info = sys.exc_info()\n        print('gpu_metrics_collector error: %s' % error)\n    finally:\n        os.umask(old_umask)",
  "def gen_empty_gpu_metric(outputDir):\n    try:\n        old_umask = os.umask(0)\n        with open(os.path.join(outputDir, \"gpu_metrics\"), 'a') as outputFile:\n            outPut = {}\n            outPut[\"Timestamp\"] = time.asctime(time.localtime())\n            outPut[\"gpuCount\"] = 0\n            outPut[\"gpuInfos\"] = []\n            print(outPut)\n            outputFile.write(\"{}\\n\".format(json.dumps(outPut, sort_keys=True)))\n            outputFile.flush()\n    except Exception:\n        traceback.print_exc()\n    finally:\n        os.umask(old_umask)",
  "def parse_annotation_mutable_layers(code, lineno, nas_mode):\n    \"\"\"Parse the string of mutable layers in annotation.\n    Return a list of AST Expr nodes\n    code: annotation string (excluding '@')\n    nas_mode: the mode of NAS\n    \"\"\"\n    module = ast.parse(code)\n    assert type(module) is ast.Module, 'internal error #1'\n    assert len(module.body) == 1, 'Annotation mutable_layers contains more than one expression'\n    assert type(module.body[0]) is ast.Expr, 'Annotation is not expression'\n    call = module.body[0].value\n    nodes = []\n    mutable_id = 'mutable_block_' + str(lineno)\n    mutable_layer_cnt = 0\n    for arg in call.args:\n        fields = {'layer_choice': False,\n                  'fixed_inputs': False,\n                  'optional_inputs': False,\n                  'optional_input_size': False,\n                  'layer_output': False}\n        for k, value in zip(arg.keys, arg.values):\n            if k.id == 'layer_choice':\n                assert not fields['layer_choice'], 'Duplicated field: layer_choice'\n                assert type(value) is ast.List, 'Value of layer_choice should be a list'\n                call_funcs_keys = []\n                call_funcs_values = []\n                call_kwargs_values = []\n                for call in value.elts:\n                    assert type(call) is ast.Call, 'Element in layer_choice should be function call'\n                    call_name = astor.to_source(call).strip()\n                    call_funcs_keys.append(ast.Str(s=call_name))\n                    call_funcs_values.append(call.func)\n                    assert not call.args, 'Number of args without keyword should be zero'\n                    kw_args = []\n                    kw_values = []\n                    for kw in call.keywords:\n                        kw_args.append(ast.Str(s=kw.arg))\n                        kw_values.append(kw.value)\n                    call_kwargs_values.append(ast.Dict(keys=kw_args, values=kw_values))\n                call_funcs = ast.Dict(keys=call_funcs_keys, values=call_funcs_values)\n                call_kwargs = ast.Dict(keys=call_funcs_keys, values=call_kwargs_values)\n                fields['layer_choice'] = True\n            elif k.id == 'fixed_inputs':\n                assert not fields['fixed_inputs'], 'Duplicated field: fixed_inputs'\n                assert type(value) is ast.List, 'Value of fixed_inputs should be a list'\n                fixed_inputs = value\n                fields['fixed_inputs'] = True\n            elif k.id == 'optional_inputs':\n                assert not fields['optional_inputs'], 'Duplicated field: optional_inputs'\n                assert type(value) is ast.List, 'Value of optional_inputs should be a list'\n                var_names = [ast.Str(s=astor.to_source(var).strip()) for var in value.elts]\n                optional_inputs = ast.Dict(keys=var_names, values=value.elts)\n                fields['optional_inputs'] = True\n            elif k.id == 'optional_input_size':\n                assert not fields['optional_input_size'], 'Duplicated field: optional_input_size'\n                assert type(value) is ast.Num or type(value) is ast.List, \\\n                    'Value of optional_input_size should be a number or list'\n                optional_input_size = value\n                fields['optional_input_size'] = True\n            elif k.id == 'layer_output':\n                assert not fields['layer_output'], 'Duplicated field: layer_output'\n                assert type(value) is ast.Name, 'Value of layer_output should be ast.Name type'\n                layer_output = value\n                fields['layer_output'] = True\n            else:\n                raise AssertionError('Unexpected field in mutable layer')\n        # make call for this mutable layer\n        assert fields['layer_choice'], 'layer_choice must exist'\n        assert fields['layer_output'], 'layer_output must exist'\n        mutable_layer_id = 'mutable_layer_' + str(mutable_layer_cnt)\n        mutable_layer_cnt += 1\n        target_call_attr = ast.Attribute(value=ast.Name(id='nni', ctx=ast.Load()), attr='mutable_layer', ctx=ast.Load())\n        target_call_args = [ast.Str(s=mutable_id),\n                            ast.Str(s=mutable_layer_id),\n                            call_funcs,\n                            call_kwargs]\n        if fields['fixed_inputs']:\n            target_call_args.append(fixed_inputs)\n        else:\n            target_call_args.append(ast.List(elts=[]))\n        if fields['optional_inputs']:\n            target_call_args.append(optional_inputs)\n            assert fields['optional_input_size'], 'optional_input_size must exist when optional_inputs exists'\n            target_call_args.append(optional_input_size)\n        else:\n            target_call_args.append(ast.Dict(keys=[], values=[]))\n            target_call_args.append(ast.Num(n=0))\n        target_call_args.append(ast.Str(s=nas_mode))\n        if nas_mode in ['enas_mode', 'oneshot_mode', 'darts_mode']:\n            target_call_args.append(ast.Name(id='tensorflow'))\n        target_call = ast.Call(func=target_call_attr, args=target_call_args, keywords=[])\n        node = ast.Assign(targets=[layer_output], value=target_call)\n        nodes.append(node)\n    return nodes",
  "def parse_annotation(code):\n    \"\"\"Parse an annotation string.\n    Return an AST Expr node.\n    code: annotation string (excluding '@')\n    \"\"\"\n    module = ast.parse(code)\n    assert type(module) is ast.Module, 'internal error #1'\n    assert len(module.body) == 1, 'Annotation contains more than one expression'\n    assert type(module.body[0]) is ast.Expr, 'Annotation is not expression'\n    return module.body[0]",
  "def parse_annotation_function(code, func_name):\n    \"\"\"Parse an annotation function.\n    Return the value of `name` keyword argument and the AST Call node.\n    func_name: expected function name\n    \"\"\"\n    expr = parse_annotation(code)\n    call = expr.value\n    assert type(call) is ast.Call, 'Annotation is not a function call'\n\n    assert type(call.func) is ast.Attribute, 'Unexpected annotation function'\n    assert type(call.func.value) is ast.Name, 'Invalid annotation function name'\n    assert call.func.value.id == 'nni', 'Annotation is not a NNI function'\n    assert call.func.attr == func_name, 'internal error #2'\n\n    assert len(call.keywords) == 1, 'Annotation function contains more than one keyword argument'\n    assert call.keywords[0].arg == 'name', 'Annotation keyword argument is not \"name\"'\n    name = call.keywords[0].value\n\n    return name, call",
  "def parse_nni_variable(code):\n    \"\"\"Parse `nni.variable` expression.\n    Return the name argument and AST node of annotated expression.\n    code: annotation string\n    \"\"\"\n    name, call = parse_annotation_function(code, 'variable')\n\n    assert len(call.args) == 1, 'nni.variable contains more than one arguments'\n    arg = call.args[0]\n    assert type(arg) is ast.Call, 'Value of nni.variable is not a function call'\n    assert type(arg.func) is ast.Attribute, 'nni.variable value is not a NNI function'\n    assert type(arg.func.value) is ast.Name, 'nni.variable value is not a NNI function'\n    assert arg.func.value.id == 'nni', 'nni.variable value is not a NNI function'\n\n    name_str = astor.to_source(name).strip()\n    keyword_arg = ast.keyword(arg='name', value=ast.Str(s=name_str))\n    arg.keywords.append(keyword_arg)\n    if arg.func.attr == 'choice':\n        convert_args_to_dict(arg)\n\n    return name, arg",
  "def parse_nni_function(code):\n    \"\"\"Parse `nni.function_choice` expression.\n    Return the AST node of annotated expression and a list of dumped function call expressions.\n    code: annotation string\n    \"\"\"\n    name, call = parse_annotation_function(code, 'function_choice')\n    funcs = [ast.dump(func, False) for func in call.args]\n    convert_args_to_dict(call, with_lambda=True)\n\n    name_str = astor.to_source(name).strip()\n    call.keywords[0].value = ast.Str(s=name_str)\n\n    return call, funcs",
  "def convert_args_to_dict(call, with_lambda=False):\n    \"\"\"Convert all args to a dict such that every key and value in the dict is the same as the value of the arg.\n    Return the AST Call node with only one arg that is the dictionary\n    \"\"\"\n    keys, values = list(), list()\n    for arg in call.args:\n        if type(arg) in [ast.Str, ast.Num]:\n            arg_value = arg\n        else:\n            # if arg is not a string or a number, we use its source code as the key\n            arg_value = astor.to_source(arg).strip('\\n\"')\n            arg_value = ast.Str(str(arg_value))\n        arg = make_lambda(arg) if with_lambda else arg\n        keys.append(arg_value)\n        values.append(arg)\n    del call.args[:]\n    call.args.append(ast.Dict(keys=keys, values=values))\n\n    return call",
  "def make_lambda(call):\n    \"\"\"Wrap an AST Call node to lambda expression node.\n    call: ast.Call node\n    \"\"\"\n    empty_args = ast.arguments(args=[], vararg=None, kwarg=None, defaults=[])\n    return ast.Lambda(args=empty_args, body=call)",
  "def test_variable_equal(node1, node2):\n    \"\"\"Test whether two variables are the same.\"\"\"\n    if type(node1) is not type(node2):\n        return False\n    if isinstance(node1, ast.AST):\n        for k, v in vars(node1).items():\n            if k in ('lineno', 'col_offset', 'ctx'):\n                continue\n            if not test_variable_equal(v, getattr(node2, k)):\n                return False\n        return True\n    if isinstance(node1, list):\n        if len(node1) != len(node2):\n            return False\n        return all(test_variable_equal(n1, n2) for n1, n2 in zip(node1, node2))\n\n    return node1 == node2",
  "def replace_variable_node(node, annotation):\n    \"\"\"Replace a node annotated by `nni.variable`.\n    node: the AST node to replace\n    annotation: annotation string\n    \"\"\"\n    assert type(node) is ast.Assign, 'nni.variable is not annotating assignment expression'\n    assert len(node.targets) == 1, 'Annotated assignment has more than one left-hand value'\n    name, expr = parse_nni_variable(annotation)\n    assert test_variable_equal(node.targets[0], name), 'Annotated variable has wrong name'\n    node.value = expr\n    return node",
  "def replace_function_node(node, annotation):\n    \"\"\"Replace a node annotated by `nni.function_choice`.\n    node: the AST node to replace\n    annotation: annotation string\n    \"\"\"\n    target, funcs = parse_nni_function(annotation)\n    FuncReplacer(funcs, target).visit(node)\n    return node",
  "class FuncReplacer(ast.NodeTransformer):\n    \"\"\"To replace target function call expressions in a node annotated by `nni.function_choice`\"\"\"\n\n    def __init__(self, funcs, target):\n        \"\"\"Constructor.\n        funcs: list of dumped function call expressions to replace\n        target: use this AST node to replace matching expressions\n        \"\"\"\n        self.funcs = set(funcs)\n        self.target = target\n\n    def visit_Call(self, node):  # pylint: disable=invalid-name\n        if ast.dump(node, False) in self.funcs:\n            return self.target\n        return node",
  "class Transformer(ast.NodeTransformer):\n    \"\"\"Transform original code to annotated code\"\"\"\n\n    def __init__(self, nas_mode=None):\n        self.stack = []\n        self.last_line = 0\n        self.annotated = False\n        self.nas_mode = nas_mode\n\n    def visit(self, node):\n        if isinstance(node, (ast.expr, ast.stmt)):\n            self.last_line = node.lineno\n\n        # do nothing for root\n        if not self.stack:\n            return self._visit_children(node)\n\n        annotation = self.stack[-1]\n\n        # this is a standalone string, may be an annotation\n        if type(node) is ast.Expr and type(node.value) is ast.Str:\n            # must not annotate an annotation string\n            assert annotation is None, 'Annotating an annotation'\n            return self._visit_string(node)\n\n        if annotation is not None:  # this expression is annotated\n            self.stack[-1] = None  # so next expression is not\n            if annotation.startswith('nni.variable'):\n                return replace_variable_node(node, annotation)\n            if annotation.startswith('nni.function_choice'):\n                return replace_function_node(node, annotation)\n\n        return self._visit_children(node)\n\n    def _visit_string(self, node):\n        string = node.value.s\n        if string.startswith('@nni.'):\n            self.annotated = True\n        else:\n            return node  # not an annotation, ignore it\n\n        if string.startswith('@nni.training_update'):\n            expr = parse_annotation(string[1:])\n            call_node = expr.value\n            call_node.args.insert(0, ast.Str(s=self.nas_mode))\n            return expr\n\n        if string.startswith('@nni.report_intermediate_result') \\\n                or string.startswith('@nni.report_final_result') \\\n                or string.startswith('@nni.get_next_parameter'):\n            return parse_annotation(string[1:])  # expand annotation string to code\n\n        if string.startswith('@nni.mutable_layers'):\n            nodes = parse_annotation_mutable_layers(string[1:], node.lineno, self.nas_mode)\n            return nodes\n\n        if string.startswith('@nni.variable') \\\n                or string.startswith('@nni.function_choice'):\n            self.stack[-1] = string[1:]  # mark that the next expression is annotated\n            return None\n\n        raise AssertionError('Unexpected annotation function')\n\n    def _visit_children(self, node):\n        self.stack.append(None)\n        self.generic_visit(node)\n        annotation = self.stack.pop()\n        assert annotation is None, 'Annotation has no target'\n        return node",
  "def parse(code, nas_mode=None):\n    \"\"\"Annotate user code.\n    Return annotated code (str) if annotation detected; return None if not.\n    code: original user code (str),\n    nas_mode: the mode of NAS given that NAS interface is used\n    \"\"\"\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError('Bad Python code')\n\n    transformer = Transformer(nas_mode)\n    try:\n        transformer.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError('%d: %s' % (ast_tree.last_line, exc.args[0]))\n\n    if not transformer.annotated:\n        return None\n\n    last_future_import = -1\n    import_nni = ast.Import(names=[ast.alias(name='nni', asname=None)])\n    nodes = ast_tree.body\n    for i, _ in enumerate(nodes):\n        if type(nodes[i]) is ast.ImportFrom and nodes[i].module == '__future__':\n            last_future_import = i\n    nodes.insert(last_future_import + 1, import_nni)\n    # enas, oneshot and darts modes for tensorflow need tensorflow module, so we import it here\n    if nas_mode in ['enas_mode', 'oneshot_mode', 'darts_mode']:\n        import_tf = ast.Import(names=[ast.alias(name='tensorflow', asname=None)])\n        nodes.insert(last_future_import + 1, import_tf)\n\n    return astor.to_source(ast_tree)",
  "def __init__(self, funcs, target):\n        \"\"\"Constructor.\n        funcs: list of dumped function call expressions to replace\n        target: use this AST node to replace matching expressions\n        \"\"\"\n        self.funcs = set(funcs)\n        self.target = target",
  "def visit_Call(self, node):  # pylint: disable=invalid-name\n        if ast.dump(node, False) in self.funcs:\n            return self.target\n        return node",
  "def __init__(self, nas_mode=None):\n        self.stack = []\n        self.last_line = 0\n        self.annotated = False\n        self.nas_mode = nas_mode",
  "def visit(self, node):\n        if isinstance(node, (ast.expr, ast.stmt)):\n            self.last_line = node.lineno\n\n        # do nothing for root\n        if not self.stack:\n            return self._visit_children(node)\n\n        annotation = self.stack[-1]\n\n        # this is a standalone string, may be an annotation\n        if type(node) is ast.Expr and type(node.value) is ast.Str:\n            # must not annotate an annotation string\n            assert annotation is None, 'Annotating an annotation'\n            return self._visit_string(node)\n\n        if annotation is not None:  # this expression is annotated\n            self.stack[-1] = None  # so next expression is not\n            if annotation.startswith('nni.variable'):\n                return replace_variable_node(node, annotation)\n            if annotation.startswith('nni.function_choice'):\n                return replace_function_node(node, annotation)\n\n        return self._visit_children(node)",
  "def _visit_string(self, node):\n        string = node.value.s\n        if string.startswith('@nni.'):\n            self.annotated = True\n        else:\n            return node  # not an annotation, ignore it\n\n        if string.startswith('@nni.training_update'):\n            expr = parse_annotation(string[1:])\n            call_node = expr.value\n            call_node.args.insert(0, ast.Str(s=self.nas_mode))\n            return expr\n\n        if string.startswith('@nni.report_intermediate_result') \\\n                or string.startswith('@nni.report_final_result') \\\n                or string.startswith('@nni.get_next_parameter'):\n            return parse_annotation(string[1:])  # expand annotation string to code\n\n        if string.startswith('@nni.mutable_layers'):\n            nodes = parse_annotation_mutable_layers(string[1:], node.lineno, self.nas_mode)\n            return nodes\n\n        if string.startswith('@nni.variable') \\\n                or string.startswith('@nni.function_choice'):\n            self.stack[-1] = string[1:]  # mark that the next expression is annotated\n            return None\n\n        raise AssertionError('Unexpected annotation function')",
  "def _visit_children(self, node):\n        self.stack.append(None)\n        self.generic_visit(node)\n        annotation = self.stack.pop()\n        assert annotation is None, 'Annotation has no target'\n        return node",
  "class AnnotationTestCase(TestCase):\n    @classmethod\n    def setUpClass(cls):\n        os.chdir('nni_annotation')\n        if os.path.isdir('_generated'):\n            shutil.rmtree('_generated')\n\n    def test_search_space_generator(self):\n        shutil.copytree('testcase/annotated', '_generated/annotated')\n        search_space = generate_search_space('_generated/annotated')\n        with open('testcase/searchspace.json') as f:\n            self.assertEqual(search_space, json.load(f))\n\n    def test_code_generator(self):\n        code_dir = expand_annotations('testcase/usercode', '_generated/usercode', nas_mode='classic_mode')\n        self.assertEqual(code_dir, '_generated/usercode')\n        self._assert_source_equal('testcase/annotated/nas.py', '_generated/usercode/nas.py')\n        self._assert_source_equal('testcase/annotated/mnist.py', '_generated/usercode/mnist.py')\n        self._assert_source_equal('testcase/annotated/dir/simple.py', '_generated/usercode/dir/simple.py')\n        with open('testcase/usercode/nonpy.txt') as src, open('_generated/usercode/nonpy.txt') as dst:\n            assert src.read() == dst.read()\n\n    def test_annotation_detecting(self):\n        dir_ = 'testcase/usercode/non_annotation'\n        code_dir = expand_annotations(dir_, tempfile.mkdtemp())\n        self.assertEqual(code_dir, dir_)\n\n    def _assert_source_equal(self, src1, src2):\n        with open(src1) as f1, open(src2) as f2:\n            ast1 = ast.dump(ast.parse(f1.read()))\n            ast2 = ast.dump(ast.parse(f2.read()))\n        self.assertEqual(ast1, ast2)",
  "def setUpClass(cls):\n        os.chdir('nni_annotation')\n        if os.path.isdir('_generated'):\n            shutil.rmtree('_generated')",
  "def test_search_space_generator(self):\n        shutil.copytree('testcase/annotated', '_generated/annotated')\n        search_space = generate_search_space('_generated/annotated')\n        with open('testcase/searchspace.json') as f:\n            self.assertEqual(search_space, json.load(f))",
  "def test_code_generator(self):\n        code_dir = expand_annotations('testcase/usercode', '_generated/usercode', nas_mode='classic_mode')\n        self.assertEqual(code_dir, '_generated/usercode')\n        self._assert_source_equal('testcase/annotated/nas.py', '_generated/usercode/nas.py')\n        self._assert_source_equal('testcase/annotated/mnist.py', '_generated/usercode/mnist.py')\n        self._assert_source_equal('testcase/annotated/dir/simple.py', '_generated/usercode/dir/simple.py')\n        with open('testcase/usercode/nonpy.txt') as src, open('_generated/usercode/nonpy.txt') as dst:\n            assert src.read() == dst.read()",
  "def test_annotation_detecting(self):\n        dir_ = 'testcase/usercode/non_annotation'\n        code_dir = expand_annotations(dir_, tempfile.mkdtemp())\n        self.assertEqual(code_dir, dir_)",
  "def _assert_source_equal(self, src1, src2):\n        with open(src1) as f1, open(src2) as f2:\n            ast1 = ast.dump(ast.parse(f1.read()))\n            ast2 = ast.dump(ast.parse(f2.read()))\n        self.assertEqual(ast1, ast2)",
  "class SearchSpaceGenerator(ast.NodeTransformer):\n    \"\"\"Generate search space from smart parater APIs\"\"\"\n\n    def __init__(self, module_name):\n        self.module_name = module_name\n        self.search_space = {}\n        self.last_line = 0  # last parsed line, useful for error reporting\n\n    def generate_mutable_layer_search_space(self, args):\n        mutable_block = args[0].s\n        mutable_layer = args[1].s\n        key = self.module_name + '/' + mutable_block\n        args[0].s = key\n        if key not in self.search_space:\n            self.search_space[key] = {'_type': 'mutable_layer', '_value': {}}\n        self.search_space[key]['_value'][mutable_layer] = {\n            'layer_choice': [k.s for k in args[2].keys],\n            'optional_inputs': [k.s for k in args[5].keys],\n            'optional_input_size': args[6].n if isinstance(args[6], ast.Num) else [args[6].elts[0].n, args[6].elts[1].n]\n        }\n\n    def visit_Call(self, node):  # pylint: disable=invalid-name\n        self.generic_visit(node)\n\n        # ignore if the function is not 'nni.*'\n        if type(node.func) is not ast.Attribute:\n            return node\n        if type(node.func.value) is not ast.Name:\n            return node\n        if node.func.value.id != 'nni':\n            return node\n\n        # ignore if its not a search space function (e.g. `report_final_result`)\n        func = node.func.attr\n        if func not in _ss_funcs:\n            return node\n\n        self.last_line = node.lineno\n\n        if func == 'mutable_layer':\n            self.generate_mutable_layer_search_space(node.args)\n            return node\n\n        if node.keywords:\n            # there is a `name` argument\n            assert len(node.keywords) == 1, 'Smart parameter has keyword argument other than \"name\"'\n            assert node.keywords[0].arg == 'name', 'Smart paramater\\'s keyword argument is not \"name\"'\n            assert type(node.keywords[0].value) is ast.Str, 'Smart parameter\\'s name must be string literal'\n            name = node.keywords[0].value.s\n            specified_name = True\n        else:\n            # generate the missing name automatically\n            name = '__line' + str(str(node.args[-1].lineno))\n            specified_name = False\n            node.keywords = list()\n\n        if func in ('choice', 'function_choice'):\n            # we will use keys in the dict as the choices, which is generated by code_generator according to the args given by user\n            assert len(node.args) == 1, 'Smart parameter has arguments other than dict'\n            # check if it is a number or a string and get its value accordingly\n            args = [key.n if type(key) is ast.Num else key.s for key in node.args[0].keys]\n        else:\n            # arguments of other functions must be literal number\n            assert all(isinstance(ast.literal_eval(astor.to_source(arg)), numbers.Real) for arg in node.args), \\\n                'Smart parameter\\'s arguments must be number literals'\n            args = [ast.literal_eval(astor.to_source(arg)) for arg in node.args]\n\n        key = self.module_name + '/' + name + '/' + func\n        # store key in ast.Call\n        node.keywords.append(ast.keyword(arg='key', value=ast.Str(s=key)))\n\n        if func == 'function_choice':\n            func = 'choice'\n        value = {'_type': func, '_value': args}\n\n        if specified_name:\n            # multiple functions with same name must have identical arguments\n            old = self.search_space.get(key)\n            assert old is None or old == value, 'Different smart parameters have same name'\n        else:\n            # generated name must not duplicate\n            assert key not in self.search_space, 'Only one smart parameter is allowed in a line'\n\n        self.search_space[key] = value\n\n        return node",
  "def generate(module_name, code):\n    \"\"\"Generate search space.\n    Return a serializable search space object.\n    module_name: name of the module (str)\n    code: user code (str)\n    \"\"\"\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError('Bad Python code')\n\n    visitor = SearchSpaceGenerator(module_name)\n    try:\n        visitor.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError('%d: %s' % (visitor.last_line, exc.args[0]))\n    return visitor.search_space, astor.to_source(ast_tree)",
  "def __init__(self, module_name):\n        self.module_name = module_name\n        self.search_space = {}\n        self.last_line = 0",
  "def generate_mutable_layer_search_space(self, args):\n        mutable_block = args[0].s\n        mutable_layer = args[1].s\n        key = self.module_name + '/' + mutable_block\n        args[0].s = key\n        if key not in self.search_space:\n            self.search_space[key] = {'_type': 'mutable_layer', '_value': {}}\n        self.search_space[key]['_value'][mutable_layer] = {\n            'layer_choice': [k.s for k in args[2].keys],\n            'optional_inputs': [k.s for k in args[5].keys],\n            'optional_input_size': args[6].n if isinstance(args[6], ast.Num) else [args[6].elts[0].n, args[6].elts[1].n]\n        }",
  "def visit_Call(self, node):  # pylint: disable=invalid-name\n        self.generic_visit(node)\n\n        # ignore if the function is not 'nni.*'\n        if type(node.func) is not ast.Attribute:\n            return node\n        if type(node.func.value) is not ast.Name:\n            return node\n        if node.func.value.id != 'nni':\n            return node\n\n        # ignore if its not a search space function (e.g. `report_final_result`)\n        func = node.func.attr\n        if func not in _ss_funcs:\n            return node\n\n        self.last_line = node.lineno\n\n        if func == 'mutable_layer':\n            self.generate_mutable_layer_search_space(node.args)\n            return node\n\n        if node.keywords:\n            # there is a `name` argument\n            assert len(node.keywords) == 1, 'Smart parameter has keyword argument other than \"name\"'\n            assert node.keywords[0].arg == 'name', 'Smart paramater\\'s keyword argument is not \"name\"'\n            assert type(node.keywords[0].value) is ast.Str, 'Smart parameter\\'s name must be string literal'\n            name = node.keywords[0].value.s\n            specified_name = True\n        else:\n            # generate the missing name automatically\n            name = '__line' + str(str(node.args[-1].lineno))\n            specified_name = False\n            node.keywords = list()\n\n        if func in ('choice', 'function_choice'):\n            # we will use keys in the dict as the choices, which is generated by code_generator according to the args given by user\n            assert len(node.args) == 1, 'Smart parameter has arguments other than dict'\n            # check if it is a number or a string and get its value accordingly\n            args = [key.n if type(key) is ast.Num else key.s for key in node.args[0].keys]\n        else:\n            # arguments of other functions must be literal number\n            assert all(isinstance(ast.literal_eval(astor.to_source(arg)), numbers.Real) for arg in node.args), \\\n                'Smart parameter\\'s arguments must be number literals'\n            args = [ast.literal_eval(astor.to_source(arg)) for arg in node.args]\n\n        key = self.module_name + '/' + name + '/' + func\n        # store key in ast.Call\n        node.keywords.append(ast.keyword(arg='key', value=ast.Str(s=key)))\n\n        if func == 'function_choice':\n            func = 'choice'\n        value = {'_type': func, '_value': args}\n\n        if specified_name:\n            # multiple functions with same name must have identical arguments\n            old = self.search_space.get(key)\n            assert old is None or old == value, 'Different smart parameters have same name'\n        else:\n            # generated name must not duplicate\n            assert key not in self.search_space, 'Only one smart parameter is allowed in a line'\n\n        self.search_space[key] = value\n\n        return node",
  "def generate_search_space(code_dir):\n    \"\"\"Generate search space from Python source code.\n    Return a serializable search space object.\n    code_dir: directory path of source files (str)\n    \"\"\"\n    search_space = {}\n\n    if code_dir.endswith(slash):\n        code_dir = code_dir[:-1]\n\n    for subdir, _, files in os.walk(code_dir):\n        # generate module name from path\n        if subdir == code_dir:\n            package = ''\n        else:\n            assert subdir.startswith(code_dir + slash), subdir\n            prefix_len = len(code_dir) + 1\n            package = subdir[prefix_len:].replace(slash, '.') + '.'\n\n        for file_name in files:\n            if file_name.endswith('.py'):\n                path = os.path.join(subdir, file_name)\n                module = package + file_name[:-3]\n                search_space.update(_generate_file_search_space(path, module))\n\n    return search_space",
  "def _generate_file_search_space(path, module):\n    with open(path) as src:\n        try:\n            search_space, code = search_space_generator.generate(module, src.read())\n        except Exception as exc:  # pylint: disable=broad-except\n            if exc.args:\n                raise RuntimeError(path + ' ' + '\\n'.join(exc.args))\n            else:\n                raise RuntimeError('Failed to generate search space for %s: %r' % (path, exc))\n    with open(path, 'w') as dst:\n        dst.write(code)\n    return search_space",
  "def expand_annotations(src_dir, dst_dir, exp_id='', trial_id='', nas_mode=None):\n    \"\"\"Expand annotations in user code.\n    Return dst_dir if annotation detected; return src_dir if not.\n    src_dir: directory path of user code (str)\n    dst_dir: directory to place generated files (str)\n    nas_mode: the mode of NAS given that NAS interface is used\n    \"\"\"\n    if src_dir[-1] == slash:\n        src_dir = src_dir[:-1]\n\n    if dst_dir[-1] == slash:\n        dst_dir = dst_dir[:-1]\n\n    annotated = False\n\n    for src_subdir, dirs, files in os.walk(src_dir):\n        assert src_subdir.startswith(src_dir)\n        dst_subdir = src_subdir.replace(src_dir, dst_dir, 1)\n        os.makedirs(dst_subdir, exist_ok=True)\n\n        # generate module name from path\n        if src_subdir == src_dir:\n            package = ''\n        else:\n            assert src_subdir.startswith(src_dir + slash), src_subdir\n            prefix_len = len(src_dir) + 1\n            package = src_subdir[prefix_len:].replace(slash, '.') + '.'\n\n        for file_name in files:\n            src_path = os.path.join(src_subdir, file_name)\n            dst_path = os.path.join(dst_subdir, file_name)\n            if file_name.endswith('.py'):\n                if trial_id == '':\n                    annotated |= _expand_file_annotations(src_path, dst_path, nas_mode)\n                else:\n                    module = package + file_name[:-3]\n                    annotated |= _generate_specific_file(src_path, dst_path, exp_id, trial_id, module)\n            else:\n                shutil.copyfile(src_path, dst_path)\n\n        for dir_name in dirs:\n            os.makedirs(os.path.join(dst_subdir, dir_name), exist_ok=True)\n\n    return dst_dir if annotated else src_dir",
  "def _expand_file_annotations(src_path, dst_path, nas_mode):\n    with open(src_path) as src, open(dst_path, 'w') as dst:\n        try:\n            annotated_code = code_generator.parse(src.read(), nas_mode)\n            if annotated_code is None:\n                shutil.copyfile(src_path, dst_path)\n                return False\n            dst.write(annotated_code)\n            return True\n\n        except Exception as exc:  # pylint: disable=broad-except\n            if exc.args:\n                raise RuntimeError(src_path + ' ' + '\\n'.join(str(arg) for arg in exc.args))\n            else:\n                raise RuntimeError('Failed to expand annotations for %s: %r' % (src_path, exc))",
  "def _generate_specific_file(src_path, dst_path, exp_id, trial_id, module):\n    with open(src_path) as src, open(dst_path, 'w') as dst:\n        try:\n            with open(os.path.expanduser('~/nni-experiments/%s/trials/%s/parameter.cfg'%(exp_id, trial_id))) as fd:\n                para_cfg = json.load(fd)\n            annotated_code = specific_code_generator.parse(src.read(), para_cfg[\"parameters\"], module)\n            if annotated_code is None:\n                shutil.copyfile(src_path, dst_path)\n                return False\n            dst.write(annotated_code)\n            return True\n\n        except Exception as exc:  # pylint: disable=broad-except\n            if exc.args:\n                raise RuntimeError(src_path + ' ' + '\\n'.join(str(arg) for arg in exc.args))\n            else:\n                raise RuntimeError('Failed to expand annotations for %s: %r' % (src_path, exc))",
  "def parse_annotation_mutable_layers(code, lineno):\n    \"\"\"Parse the string of mutable layers in annotation.\n    Return a list of AST Expr nodes\n    code: annotation string (excluding '@')\n    \"\"\"\n    module = ast.parse(code)\n    assert type(module) is ast.Module, 'internal error #1'\n    assert len(module.body) == 1, 'Annotation mutable_layers contains more than one expression'\n    assert type(module.body[0]) is ast.Expr, 'Annotation is not expression'\n    call = module.body[0].value\n    nodes = []\n    mutable_id = prefix_name + '/mutable_block_' + str(lineno)\n    mutable_layer_cnt = 0\n    for arg in call.args:\n        fields = {'layer_choice': False,\n                  'fixed_inputs': False,\n                  'optional_inputs': False,\n                  'optional_input_size': False,\n                  'layer_output': False}\n        mutable_layer_id = 'mutable_layer_' + str(mutable_layer_cnt)\n        mutable_layer_cnt += 1\n        func_call = None\n        for k, value in zip(arg.keys, arg.values):\n            if k.id == 'layer_choice':\n                assert not fields['layer_choice'], 'Duplicated field: layer_choice'\n                assert type(value) is ast.List, 'Value of layer_choice should be a list'\n                for call in value.elts:\n                    assert type(call) is ast.Call, 'Element in layer_choice should be function call'\n                    call_name = astor.to_source(call).strip()\n                    if call_name == para_cfg[mutable_id][mutable_layer_id]['chosen_layer']:\n                        func_call = call\n                        assert not call.args, 'Number of args without keyword should be zero'\n                        break\n                fields['layer_choice'] = True\n            elif k.id == 'fixed_inputs':\n                assert not fields['fixed_inputs'], 'Duplicated field: fixed_inputs'\n                assert type(value) is ast.List, 'Value of fixed_inputs should be a list'\n                fixed_inputs = value\n                fields['fixed_inputs'] = True\n            elif k.id == 'optional_inputs':\n                assert not fields['optional_inputs'], 'Duplicated field: optional_inputs'\n                assert type(value) is ast.List, 'Value of optional_inputs should be a list'\n                var_names = [astor.to_source(var).strip() for var in value.elts]\n                chosen_inputs = para_cfg[mutable_id][mutable_layer_id]['chosen_inputs']\n                elts = []\n                for i in chosen_inputs:\n                    index = var_names.index(i)\n                    elts.append(value.elts[index])\n                optional_inputs = ast.List(elts=elts)\n                fields['optional_inputs'] = True\n            elif k.id == 'optional_input_size':\n                pass\n            elif k.id == 'layer_output':\n                assert not fields['layer_output'], 'Duplicated field: layer_output'\n                assert type(value) is ast.Name, 'Value of layer_output should be ast.Name type'\n                layer_output = value\n                fields['layer_output'] = True\n            else:\n                raise AssertionError('Unexpected field in mutable layer')\n        # make call for this mutable layer\n        assert fields['layer_choice'], 'layer_choice must exist'\n        assert fields['layer_output'], 'layer_output must exist'\n\n        if not fields['fixed_inputs']:\n            fixed_inputs = ast.List(elts=[])\n        if not fields['optional_inputs']:\n            optional_inputs = ast.List(elts=[])\n        inputs = ast.List(elts=[fixed_inputs, optional_inputs])\n\n        func_call.args.append(inputs)\n        node = ast.Assign(targets=[layer_output], value=func_call)\n        nodes.append(node)\n    return nodes",
  "def parse_annotation(code):\n    \"\"\"Parse an annotation string.\n    Return an AST Expr node.\n    code: annotation string (excluding '@')\n    \"\"\"\n    module = ast.parse(code)\n    assert type(module) is ast.Module, 'internal error #1'\n    assert len(module.body) == 1, 'Annotation contains more than one expression'\n    assert type(module.body[0]) is ast.Expr, 'Annotation is not expression'\n    return module.body[0]",
  "def parse_annotation_function(code, func_name):\n    \"\"\"Parse an annotation function.\n    Return the value of `name` keyword argument and the AST Call node.\n    func_name: expected function name\n    \"\"\"\n    expr = parse_annotation(code)\n    call = expr.value\n    assert type(call) is ast.Call, 'Annotation is not a function call'\n\n    assert type(call.func) is ast.Attribute, 'Unexpected annotation function'\n    assert type(call.func.value) is ast.Name, 'Invalid annotation function name'\n    assert call.func.value.id == 'nni', 'Annotation is not a NNI function'\n    assert call.func.attr == func_name, 'internal error #2'\n\n    assert len(call.keywords) == 1, 'Annotation function contains more than one keyword argument'\n    assert call.keywords[0].arg == 'name', 'Annotation keyword argument is not \"name\"'\n    name = call.keywords[0].value\n\n    return name, call",
  "def parse_nni_variable(code):\n    \"\"\"Parse `nni.variable` expression.\n    Return the name argument and AST node of annotated expression.\n    code: annotation string\n    \"\"\"\n    name, call = parse_annotation_function(code, 'variable')\n\n    assert len(call.args) == 1, 'nni.variable contains more than one arguments'\n    arg = call.args[0]\n    assert type(arg) is ast.Call, 'Value of nni.variable is not a function call'\n    assert type(arg.func) is ast.Attribute, 'nni.variable value is not a NNI function'\n    assert type(arg.func.value) is ast.Name, 'nni.variable value is not a NNI function'\n    assert arg.func.value.id == 'nni', 'nni.variable value is not a NNI function'\n\n    name_str = astor.to_source(name).strip()\n    keyword_arg = ast.keyword(arg='name', value=ast.Str(s=name_str))\n    arg.keywords.append(keyword_arg)\n    if arg.func.attr == 'choice':\n        convert_args_to_dict(arg)\n\n    return name, arg",
  "def parse_nni_function(code):\n    \"\"\"Parse `nni.function_choice` expression.\n    Return the AST node of annotated expression and a list of dumped function call expressions.\n    code: annotation string\n    \"\"\"\n    name, call = parse_annotation_function(code, 'function_choice')\n    funcs = [ast.dump(func, False) for func in call.args]\n    convert_args_to_dict(call, with_lambda=True)\n\n    name_str = astor.to_source(name).strip()\n    call.keywords[0].value = ast.Str(s=name_str)\n\n    return call, funcs",
  "def convert_args_to_dict(call, with_lambda=False):\n    \"\"\"Convert all args to a dict such that every key and value in the dict is the same as the value of the arg.\n    Return the AST Call node with only one arg that is the dictionary\n    \"\"\"\n    keys, values = list(), list()\n    for arg in call.args:\n        if type(arg) in [ast.Str, ast.Num]:\n            arg_value = arg\n        else:\n            # if arg is not a string or a number, we use its source code as the key\n            arg_value = astor.to_source(arg).strip('\\n\"')\n            arg_value = ast.Str(str(arg_value))\n        arg = make_lambda(arg) if with_lambda else arg\n        keys.append(arg_value)\n        values.append(arg)\n    del call.args[:]\n    call.args.append(ast.Dict(keys=keys, values=values))\n\n    return call",
  "def make_lambda(call):\n    \"\"\"Wrap an AST Call node to lambda expression node.\n    call: ast.Call node\n    \"\"\"\n    empty_args = ast.arguments(args=[], vararg=None, kwarg=None, defaults=[])\n    return ast.Lambda(args=empty_args, body=call)",
  "def test_variable_equal(node1, node2):\n    \"\"\"Test whether two variables are the same.\"\"\"\n    if type(node1) is not type(node2):\n        return False\n    if isinstance(node1, ast.AST):\n        for k, v in vars(node1).items():\n            if k in ('lineno', 'col_offset', 'ctx'):\n                continue\n            if not test_variable_equal(v, getattr(node2, k)):\n                return False\n        return True\n    if isinstance(node1, list):\n        if len(node1) != len(node2):\n            return False\n        return all(test_variable_equal(n1, n2) for n1, n2 in zip(node1, node2))\n\n    return node1 == node2",
  "def replace_variable_node(node, annotation):\n    \"\"\"Replace a node annotated by `nni.variable`.\n    node: the AST node to replace\n    annotation: annotation string\n    \"\"\"\n    assert type(node) is ast.Assign, 'nni.variable is not annotating assignment expression'\n    assert len(node.targets) == 1, 'Annotated assignment has more than one left-hand value'\n    name, expr = parse_nni_variable(annotation)\n    assert test_variable_equal(node.targets[0], name), 'Annotated variable has wrong name'\n    node.value = expr\n    return node",
  "def replace_function_node(node, annotation):\n    \"\"\"Replace a node annotated by `nni.function_choice`.\n    node: the AST node to replace\n    annotation: annotation string\n    \"\"\"\n    target, funcs = parse_nni_function(annotation)\n    FuncReplacer(funcs, target).visit(node)\n    return node",
  "class FuncReplacer(ast.NodeTransformer):\n    \"\"\"To replace target function call expressions in a node annotated by `nni.function_choice`\"\"\"\n\n    def __init__(self, funcs, target):\n        \"\"\"Constructor.\n        funcs: list of dumped function call expressions to replace\n        target: use this AST node to replace matching expressions\n        \"\"\"\n        self.funcs = set(funcs)\n        self.target = target\n\n    def visit_Call(self, node):  # pylint: disable=invalid-name\n        if ast.dump(node, False) in self.funcs:\n            return self.target\n        return node",
  "class Transformer(ast.NodeTransformer):\n    \"\"\"Transform original code to annotated code\"\"\"\n\n    def __init__(self):\n        self.stack = []\n        self.last_line = 0\n        self.annotated = False\n\n    def visit(self, node):\n        if isinstance(node, (ast.expr, ast.stmt)):\n            self.last_line = node.lineno\n\n        # do nothing for root\n        if not self.stack:\n            return self._visit_children(node)\n\n        annotation = self.stack[-1]\n\n        # this is a standalone string, may be an annotation\n        if type(node) is ast.Expr and type(node.value) is ast.Str:\n            # must not annotate an annotation string\n            assert annotation is None, 'Annotating an annotation'\n            return self._visit_string(node)\n\n        if annotation is not None:  # this expression is annotated\n            self.stack[-1] = None  # so next expression is not\n            if annotation.startswith('nni.variable'):\n                return replace_variable_node(node, annotation)\n            if annotation.startswith('nni.function_choice'):\n                return replace_function_node(node, annotation)\n\n        return self._visit_children(node)\n\n    def _visit_string(self, node):\n        string = node.value.s\n        if string.startswith('@nni.'):\n            self.annotated = True\n        else:\n            return node  # not an annotation, ignore it\n\n        if string.startswith('@nni.get_next_parameter'):\n            deprecated_message = \"'@nni.get_next_parameter' is deprecated in annotation due to inconvenience. \" \\\n                                 \"Please remove this line in the trial code.\"\n            print_warning(deprecated_message)\n            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n                                           args=[ast.Str(s='Get next parameter here...')], keywords=[]))\n\n        if string.startswith('@nni.training_update'):\n            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n                                           args=[ast.Str(s='Training update here...')], keywords=[]))\n\n        if string.startswith('@nni.report_intermediate_result'):\n            module = ast.parse(string[1:])\n            arg = module.body[0].value.args[0]\n            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n                                           args=[ast.Str(s='nni.report_intermediate_result: '), arg], keywords=[]))\n\n        if string.startswith('@nni.report_final_result'):\n            module = ast.parse(string[1:])\n            arg = module.body[0].value.args[0]\n            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n                                           args=[ast.Str(s='nni.report_final_result: '), arg], keywords=[]))\n\n        if string.startswith('@nni.mutable_layers'):\n            return parse_annotation_mutable_layers(string[1:], node.lineno)\n\n        if string.startswith('@nni.variable') \\\n                or string.startswith('@nni.function_choice'):\n            self.stack[-1] = string[1:]  # mark that the next expression is annotated\n            return None\n\n        raise AssertionError('Unexpected annotation function')\n\n    def _visit_children(self, node):\n        self.stack.append(None)\n        self.generic_visit(node)\n        annotation = self.stack.pop()\n        assert annotation is None, 'Annotation has no target'\n        return node",
  "def parse(code, para, module):\n    \"\"\"Annotate user code.\n    Return annotated code (str) if annotation detected; return None if not.\n    code: original user code (str)\n    \"\"\"\n    global para_cfg\n    global prefix_name\n    para_cfg = para\n    prefix_name = module\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError('Bad Python code')\n\n    transformer = Transformer()\n    try:\n        transformer.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError('%d: %s' % (ast_tree.last_line, exc.args[0]))\n\n    if not transformer.annotated:\n        return None\n\n    return astor.to_source(ast_tree)",
  "def __init__(self, funcs, target):\n        \"\"\"Constructor.\n        funcs: list of dumped function call expressions to replace\n        target: use this AST node to replace matching expressions\n        \"\"\"\n        self.funcs = set(funcs)\n        self.target = target",
  "def visit_Call(self, node):  # pylint: disable=invalid-name\n        if ast.dump(node, False) in self.funcs:\n            return self.target\n        return node",
  "def __init__(self):\n        self.stack = []\n        self.last_line = 0\n        self.annotated = False",
  "def visit(self, node):\n        if isinstance(node, (ast.expr, ast.stmt)):\n            self.last_line = node.lineno\n\n        # do nothing for root\n        if not self.stack:\n            return self._visit_children(node)\n\n        annotation = self.stack[-1]\n\n        # this is a standalone string, may be an annotation\n        if type(node) is ast.Expr and type(node.value) is ast.Str:\n            # must not annotate an annotation string\n            assert annotation is None, 'Annotating an annotation'\n            return self._visit_string(node)\n\n        if annotation is not None:  # this expression is annotated\n            self.stack[-1] = None  # so next expression is not\n            if annotation.startswith('nni.variable'):\n                return replace_variable_node(node, annotation)\n            if annotation.startswith('nni.function_choice'):\n                return replace_function_node(node, annotation)\n\n        return self._visit_children(node)",
  "def _visit_string(self, node):\n        string = node.value.s\n        if string.startswith('@nni.'):\n            self.annotated = True\n        else:\n            return node  # not an annotation, ignore it\n\n        if string.startswith('@nni.get_next_parameter'):\n            deprecated_message = \"'@nni.get_next_parameter' is deprecated in annotation due to inconvenience. \" \\\n                                 \"Please remove this line in the trial code.\"\n            print_warning(deprecated_message)\n            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n                                           args=[ast.Str(s='Get next parameter here...')], keywords=[]))\n\n        if string.startswith('@nni.training_update'):\n            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n                                           args=[ast.Str(s='Training update here...')], keywords=[]))\n\n        if string.startswith('@nni.report_intermediate_result'):\n            module = ast.parse(string[1:])\n            arg = module.body[0].value.args[0]\n            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n                                           args=[ast.Str(s='nni.report_intermediate_result: '), arg], keywords=[]))\n\n        if string.startswith('@nni.report_final_result'):\n            module = ast.parse(string[1:])\n            arg = module.body[0].value.args[0]\n            return ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n                                           args=[ast.Str(s='nni.report_final_result: '), arg], keywords=[]))\n\n        if string.startswith('@nni.mutable_layers'):\n            return parse_annotation_mutable_layers(string[1:], node.lineno)\n\n        if string.startswith('@nni.variable') \\\n                or string.startswith('@nni.function_choice'):\n            self.stack[-1] = string[1:]  # mark that the next expression is annotated\n            return None\n\n        raise AssertionError('Unexpected annotation function')",
  "def _visit_children(self, node):\n        self.stack.append(None)\n        self.generic_visit(node)\n        annotation = self.stack.pop()\n        assert annotation is None, 'Annotation has no target'\n        return node",
  "def rest_put(url, data, timeout, show_error=False):\n    '''Call rest put method'''\n    try:\n        response = requests.put(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                data=data, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None",
  "def rest_post(url, data, timeout, show_error=False):\n    '''Call rest post method'''\n    try:\n        response = requests.post(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                 data=data, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None",
  "def rest_get(url, timeout, show_error=False):\n    '''Call rest get method'''\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None",
  "def rest_delete(url, timeout, show_error=False):\n    '''Call rest delete method'''\n    try:\n        response = requests.delete(url, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None",
  "def check_rest_server(rest_port):\n    '''Check if restful server is ready'''\n    retry_count = 20\n    for _ in range(retry_count):\n        response = rest_get(check_status_url(rest_port), REST_TIME_OUT)\n        if response:\n            if response.status_code == 200:\n                return True, response\n            else:\n                return False, response\n        else:\n            time.sleep(1)\n    return  False, response",
  "def check_rest_server_quick(rest_port):\n    '''Check if restful server is ready, only check once'''\n    response = rest_get(check_status_url(rest_port), 5)\n    if response and response.status_code == 200:\n        return True, response\n    return False, None",
  "def check_response(response):\n    '''Check if a response is success according to status_code'''\n    if response and response.status_code == 200:\n        return True\n    return False",
  "def setType(key, valueType):\n    '''check key type'''\n    return And(valueType, error=SCHEMA_TYPE_ERROR % (key, valueType.__name__))",
  "def setChoice(key, *args):\n    '''check choice'''\n    return And(lambda n: n in args, error=SCHEMA_RANGE_ERROR % (key, str(args)))",
  "def setNumberRange(key, keyType, start, end):\n    '''check number range'''\n    return And(\n        And(keyType, error=SCHEMA_TYPE_ERROR % (key, keyType.__name__)),\n        And(lambda n: start <= n <= end, error=SCHEMA_RANGE_ERROR % (key, '(%s,%s)' % (start, end))),\n    )",
  "def setPathCheck(key):\n    '''check if path exist'''\n    return And(os.path.exists, error=SCHEMA_PATH_ERROR % key)",
  "class AlgoSchema:\n    \"\"\"\n    This class is the schema of 'tuner', 'assessor' and 'advisor' sections of experiment configuraion file.\n    For example:\n    AlgoSchema('tuner') creates the schema of tuner section.\n    \"\"\"\n\n    def __init__(self, algo_type):\n        \"\"\"\n        Parameters:\n        -----------\n        algo_type: str\n            One of ['tuner', 'assessor', 'advisor'].\n            'tuner': This AlgoSchema class create the schema of tuner section.\n            'assessor': This AlgoSchema class create the schema of assessor section.\n            'advisor': This AlgoSchema class create the schema of advisor section.\n        \"\"\"\n        assert algo_type in ['tuner', 'assessor', 'advisor']\n        self.algo_type = algo_type\n        self.algo_schema = {\n            Optional('codeDir'): setPathCheck('codeDir'),\n            Optional('classFileName'): setType('classFileName', str),\n            Optional('className'): setType('className', str),\n            Optional('classArgs'): dict,\n            Optional('includeIntermediateResults'): setType('includeIntermediateResults', bool),\n            Optional('gpuIndices'): Or(int, And(str, lambda x: len([int(i) for i in x.split(',')]) > 0), error='gpuIndex format error!'),\n        }\n        self.builtin_keys = {\n            'tuner': 'builtinTunerName',\n            'assessor': 'builtinAssessorName',\n            'advisor': 'builtinAdvisorName'\n        }\n        self.builtin_name_schema = {}\n        for k, n in self.builtin_keys.items():\n            self.builtin_name_schema[k] = {Optional(n): setChoice(n, *get_all_builtin_names(k+'s'))}\n\n        self.customized_keys = set(['codeDir', 'classFileName', 'className'])\n\n    def validate_class_args(self, class_args, algo_type, builtin_name):\n        if not builtin_name or not class_args:\n            return\n        meta = get_builtin_algo_meta(algo_type+'s', builtin_name)\n        if meta and 'accept_class_args' in meta and meta['accept_class_args'] == False:\n            raise SchemaError('classArgs is not allowed.')\n\n        validator = create_validator_instance(algo_type+'s', builtin_name)\n        if validator:\n            try:\n                validator.validate_class_args(**class_args)\n            except Exception as e:\n                raise SchemaError(str(e))\n\n    def missing_customized_keys(self, data):\n        return self.customized_keys - set(data.keys())\n\n    def validate_extras(self, data, algo_type):\n        builtin_key = self.builtin_keys[algo_type]\n        if (builtin_key in data) and (set(data.keys()) & self.customized_keys):\n            raise SchemaError('{} and {} cannot be specified at the same time.'.format(\n                builtin_key, set(data.keys()) & self.customized_keys\n            ))\n\n        if self.missing_customized_keys(data) and builtin_key not in data:\n            raise SchemaError('Either customized {} ({}) or builtin {} ({}) must be set.'.format(\n                algo_type, self.customized_keys, algo_type, builtin_key))\n\n        if not self.missing_customized_keys(data):\n            class_file_name = os.path.join(data['codeDir'], data['classFileName'])\n            if not os.path.isfile(class_file_name):\n                raise SchemaError('classFileName {} not found.'.format(class_file_name))\n\n        builtin_name = data.get(builtin_key)\n        class_args = data.get('classArgs')\n        self.validate_class_args(class_args, algo_type, builtin_name)\n\n    def validate(self, data):\n        self.algo_schema.update(self.builtin_name_schema[self.algo_type])\n        Schema(self.algo_schema).validate(data)\n        self.validate_extras(data, self.algo_type)",
  "class NNIConfigSchema:\n    def validate(self, data):\n        train_service = data['trainingServicePlatform']\n        Schema(common_schema['trainingServicePlatform']).validate(train_service)\n        train_service_schema = training_service_schema_dict[train_service]\n        train_service_schema.validate(data)\n        self.validate_extras(data)\n\n    def validate_extras(self, experiment_config):\n        self.validate_tuner_adivosr_assessor(experiment_config)\n        self.validate_pai_trial_conifg(experiment_config)\n        self.validate_kubeflow_operators(experiment_config)\n        self.validate_eth0_device(experiment_config)\n\n    def validate_tuner_adivosr_assessor(self, experiment_config):\n        if experiment_config.get('advisor'):\n            if experiment_config.get('assessor') or experiment_config.get('tuner'):\n                raise SchemaError('advisor could not be set with assessor or tuner simultaneously!')\n            self.validate_annotation_content(experiment_config, 'advisor', 'builtinAdvisorName')\n        else:\n            if not experiment_config.get('tuner'):\n                raise SchemaError('Please provide tuner spec!')\n            self.validate_annotation_content(experiment_config, 'tuner', 'builtinTunerName')\n\n    def validate_search_space_content(self, experiment_config):\n        '''Validate searchspace content,\n        if the searchspace file is not json format or its values does not contain _type and _value which must be specified,\n        it will not be a valid searchspace file'''\n        try:\n            search_space_content = json.load(open(experiment_config.get('searchSpacePath'), 'r'))\n            for value in search_space_content.values():\n                if not value.get('_type') or not value.get('_value'):\n                    raise SchemaError('please use _type and _value to specify searchspace!')\n        except Exception as e:\n            raise SchemaError('searchspace file is not a valid json format! ' + str(e))\n\n    def validate_kubeflow_operators(self, experiment_config):\n        '''Validate whether the kubeflow operators are valid'''\n        if experiment_config.get('kubeflowConfig'):\n            if experiment_config.get('kubeflowConfig').get('operator') == 'tf-operator':\n                if experiment_config.get('trial').get('master') is not None:\n                    raise SchemaError('kubeflow with tf-operator can not set master')\n                if experiment_config.get('trial').get('worker') is None:\n                    raise SchemaError('kubeflow with tf-operator must set worker')\n            elif experiment_config.get('kubeflowConfig').get('operator') == 'pytorch-operator':\n                if experiment_config.get('trial').get('ps') is not None:\n                    raise SchemaError('kubeflow with pytorch-operator can not set ps')\n                if experiment_config.get('trial').get('master') is None:\n                    raise SchemaError('kubeflow with pytorch-operator must set master')\n\n            if experiment_config.get('kubeflowConfig').get('storage') == 'nfs':\n                if experiment_config.get('kubeflowConfig').get('nfs') is None:\n                    raise SchemaError('please set nfs configuration!')\n            elif experiment_config.get('kubeflowConfig').get('storage') == 'azureStorage':\n                if experiment_config.get('kubeflowConfig').get('azureStorage') is None:\n                    raise SchemaError('please set azureStorage configuration!')\n            elif experiment_config.get('kubeflowConfig').get('storage') is None:\n                if experiment_config.get('kubeflowConfig').get('azureStorage'):\n                    raise SchemaError('please set storage type!')\n\n    def validate_annotation_content(self, experiment_config, spec_key, builtin_name):\n        '''\n        Valid whether useAnnotation and searchSpacePath is coexist\n        spec_key: 'advisor' or 'tuner'\n        builtin_name: 'builtinAdvisorName' or 'builtinTunerName'\n        '''\n        if experiment_config.get('useAnnotation'):\n            if experiment_config.get('searchSpacePath'):\n                raise SchemaError('If you set useAnnotation=true, please leave searchSpacePath empty')\n        else:\n            # validate searchSpaceFile\n            if experiment_config[spec_key].get(builtin_name) == 'NetworkMorphism':\n                return\n            if experiment_config[spec_key].get(builtin_name):\n                if experiment_config.get('searchSpacePath') is None:\n                    raise SchemaError('Please set searchSpacePath!')\n                self.validate_search_space_content(experiment_config)\n\n    def validate_pai_config_path(self, experiment_config):\n        '''validate paiConfigPath field'''\n        if experiment_config.get('trainingServicePlatform') == 'pai':\n            if experiment_config.get('trial', {}).get('paiConfigPath'):\n                # validate commands\n                pai_config = get_yml_content(experiment_config['trial']['paiConfigPath'])\n                taskRoles_dict = pai_config.get('taskRoles')\n                if not taskRoles_dict:\n                    raise SchemaError('Please set taskRoles in paiConfigPath config file!')\n            else:\n                pai_trial_fields_required_list = ['image', 'paiStorageConfigName', 'command']\n                for trial_field in pai_trial_fields_required_list:\n                    if experiment_config['trial'].get(trial_field) is None:\n                        raise SchemaError('Please set {0} in trial configuration,\\\n                                    or set additional pai configuration file path in paiConfigPath!'.format(trial_field))\n                pai_resource_fields_required_list = ['gpuNum', 'cpuNum', 'memoryMB']\n                for required_field in pai_resource_fields_required_list:\n                    if experiment_config['trial'].get(required_field) is None and \\\n                            experiment_config['paiConfig'].get(required_field) is None:\n                        raise SchemaError('Please set {0} in trial or paiConfig configuration,\\\n                                    or set additional pai configuration file path in paiConfigPath!'.format(required_field))\n\n    def validate_pai_trial_conifg(self, experiment_config):\n        '''validate the trial config in pai platform'''\n        if experiment_config.get('trainingServicePlatform') in ['pai', 'paiYarn']:\n            if experiment_config.get('trial').get('shmMB') and \\\n                    experiment_config['trial']['shmMB'] > experiment_config['trial']['memoryMB']:\n                raise SchemaError('shmMB should be no more than memoryMB!')\n            # backward compatibility\n            warning_information = '{0} is not supported in NNI anymore, please remove the field in config file!\\\n            please refer https://github.com/microsoft/nni/blob/master/docs/en_US/TrainingService/PaiMode.md#run-an-experiment\\\n            for the practices of how to get data and output model in trial code'\n            if experiment_config.get('trial').get('dataDir'):\n                print_warning(warning_information.format('dataDir'))\n            if experiment_config.get('trial').get('outputDir'):\n                print_warning(warning_information.format('outputDir'))\n            self.validate_pai_config_path(experiment_config)\n\n    def validate_eth0_device(self, experiment_config):\n        '''validate whether the machine has eth0 device'''\n        if experiment_config.get('trainingServicePlatform') not in ['local'] \\\n                and not experiment_config.get('nniManagerIp') \\\n                and 'eth0' not in netifaces.interfaces():\n            raise SchemaError('This machine does not contain eth0 network device, please set nniManagerIp in config file!')",
  "def __init__(self, algo_type):\n        \"\"\"\n        Parameters:\n        -----------\n        algo_type: str\n            One of ['tuner', 'assessor', 'advisor'].\n            'tuner': This AlgoSchema class create the schema of tuner section.\n            'assessor': This AlgoSchema class create the schema of assessor section.\n            'advisor': This AlgoSchema class create the schema of advisor section.\n        \"\"\"\n        assert algo_type in ['tuner', 'assessor', 'advisor']\n        self.algo_type = algo_type\n        self.algo_schema = {\n            Optional('codeDir'): setPathCheck('codeDir'),\n            Optional('classFileName'): setType('classFileName', str),\n            Optional('className'): setType('className', str),\n            Optional('classArgs'): dict,\n            Optional('includeIntermediateResults'): setType('includeIntermediateResults', bool),\n            Optional('gpuIndices'): Or(int, And(str, lambda x: len([int(i) for i in x.split(',')]) > 0), error='gpuIndex format error!'),\n        }\n        self.builtin_keys = {\n            'tuner': 'builtinTunerName',\n            'assessor': 'builtinAssessorName',\n            'advisor': 'builtinAdvisorName'\n        }\n        self.builtin_name_schema = {}\n        for k, n in self.builtin_keys.items():\n            self.builtin_name_schema[k] = {Optional(n): setChoice(n, *get_all_builtin_names(k+'s'))}\n\n        self.customized_keys = set(['codeDir', 'classFileName', 'className'])",
  "def validate_class_args(self, class_args, algo_type, builtin_name):\n        if not builtin_name or not class_args:\n            return\n        meta = get_builtin_algo_meta(algo_type+'s', builtin_name)\n        if meta and 'accept_class_args' in meta and meta['accept_class_args'] == False:\n            raise SchemaError('classArgs is not allowed.')\n\n        validator = create_validator_instance(algo_type+'s', builtin_name)\n        if validator:\n            try:\n                validator.validate_class_args(**class_args)\n            except Exception as e:\n                raise SchemaError(str(e))",
  "def missing_customized_keys(self, data):\n        return self.customized_keys - set(data.keys())",
  "def validate_extras(self, data, algo_type):\n        builtin_key = self.builtin_keys[algo_type]\n        if (builtin_key in data) and (set(data.keys()) & self.customized_keys):\n            raise SchemaError('{} and {} cannot be specified at the same time.'.format(\n                builtin_key, set(data.keys()) & self.customized_keys\n            ))\n\n        if self.missing_customized_keys(data) and builtin_key not in data:\n            raise SchemaError('Either customized {} ({}) or builtin {} ({}) must be set.'.format(\n                algo_type, self.customized_keys, algo_type, builtin_key))\n\n        if not self.missing_customized_keys(data):\n            class_file_name = os.path.join(data['codeDir'], data['classFileName'])\n            if not os.path.isfile(class_file_name):\n                raise SchemaError('classFileName {} not found.'.format(class_file_name))\n\n        builtin_name = data.get(builtin_key)\n        class_args = data.get('classArgs')\n        self.validate_class_args(class_args, algo_type, builtin_name)",
  "def validate(self, data):\n        self.algo_schema.update(self.builtin_name_schema[self.algo_type])\n        Schema(self.algo_schema).validate(data)\n        self.validate_extras(data, self.algo_type)",
  "def validate(self, data):\n        train_service = data['trainingServicePlatform']\n        Schema(common_schema['trainingServicePlatform']).validate(train_service)\n        train_service_schema = training_service_schema_dict[train_service]\n        train_service_schema.validate(data)\n        self.validate_extras(data)",
  "def validate_extras(self, experiment_config):\n        self.validate_tuner_adivosr_assessor(experiment_config)\n        self.validate_pai_trial_conifg(experiment_config)\n        self.validate_kubeflow_operators(experiment_config)\n        self.validate_eth0_device(experiment_config)",
  "def validate_tuner_adivosr_assessor(self, experiment_config):\n        if experiment_config.get('advisor'):\n            if experiment_config.get('assessor') or experiment_config.get('tuner'):\n                raise SchemaError('advisor could not be set with assessor or tuner simultaneously!')\n            self.validate_annotation_content(experiment_config, 'advisor', 'builtinAdvisorName')\n        else:\n            if not experiment_config.get('tuner'):\n                raise SchemaError('Please provide tuner spec!')\n            self.validate_annotation_content(experiment_config, 'tuner', 'builtinTunerName')",
  "def validate_search_space_content(self, experiment_config):\n        '''Validate searchspace content,\n        if the searchspace file is not json format or its values does not contain _type and _value which must be specified,\n        it will not be a valid searchspace file'''\n        try:\n            search_space_content = json.load(open(experiment_config.get('searchSpacePath'), 'r'))\n            for value in search_space_content.values():\n                if not value.get('_type') or not value.get('_value'):\n                    raise SchemaError('please use _type and _value to specify searchspace!')\n        except Exception as e:\n            raise SchemaError('searchspace file is not a valid json format! ' + str(e))",
  "def validate_kubeflow_operators(self, experiment_config):\n        '''Validate whether the kubeflow operators are valid'''\n        if experiment_config.get('kubeflowConfig'):\n            if experiment_config.get('kubeflowConfig').get('operator') == 'tf-operator':\n                if experiment_config.get('trial').get('master') is not None:\n                    raise SchemaError('kubeflow with tf-operator can not set master')\n                if experiment_config.get('trial').get('worker') is None:\n                    raise SchemaError('kubeflow with tf-operator must set worker')\n            elif experiment_config.get('kubeflowConfig').get('operator') == 'pytorch-operator':\n                if experiment_config.get('trial').get('ps') is not None:\n                    raise SchemaError('kubeflow with pytorch-operator can not set ps')\n                if experiment_config.get('trial').get('master') is None:\n                    raise SchemaError('kubeflow with pytorch-operator must set master')\n\n            if experiment_config.get('kubeflowConfig').get('storage') == 'nfs':\n                if experiment_config.get('kubeflowConfig').get('nfs') is None:\n                    raise SchemaError('please set nfs configuration!')\n            elif experiment_config.get('kubeflowConfig').get('storage') == 'azureStorage':\n                if experiment_config.get('kubeflowConfig').get('azureStorage') is None:\n                    raise SchemaError('please set azureStorage configuration!')\n            elif experiment_config.get('kubeflowConfig').get('storage') is None:\n                if experiment_config.get('kubeflowConfig').get('azureStorage'):\n                    raise SchemaError('please set storage type!')",
  "def validate_annotation_content(self, experiment_config, spec_key, builtin_name):\n        '''\n        Valid whether useAnnotation and searchSpacePath is coexist\n        spec_key: 'advisor' or 'tuner'\n        builtin_name: 'builtinAdvisorName' or 'builtinTunerName'\n        '''\n        if experiment_config.get('useAnnotation'):\n            if experiment_config.get('searchSpacePath'):\n                raise SchemaError('If you set useAnnotation=true, please leave searchSpacePath empty')\n        else:\n            # validate searchSpaceFile\n            if experiment_config[spec_key].get(builtin_name) == 'NetworkMorphism':\n                return\n            if experiment_config[spec_key].get(builtin_name):\n                if experiment_config.get('searchSpacePath') is None:\n                    raise SchemaError('Please set searchSpacePath!')\n                self.validate_search_space_content(experiment_config)",
  "def validate_pai_config_path(self, experiment_config):\n        '''validate paiConfigPath field'''\n        if experiment_config.get('trainingServicePlatform') == 'pai':\n            if experiment_config.get('trial', {}).get('paiConfigPath'):\n                # validate commands\n                pai_config = get_yml_content(experiment_config['trial']['paiConfigPath'])\n                taskRoles_dict = pai_config.get('taskRoles')\n                if not taskRoles_dict:\n                    raise SchemaError('Please set taskRoles in paiConfigPath config file!')\n            else:\n                pai_trial_fields_required_list = ['image', 'paiStorageConfigName', 'command']\n                for trial_field in pai_trial_fields_required_list:\n                    if experiment_config['trial'].get(trial_field) is None:\n                        raise SchemaError('Please set {0} in trial configuration,\\\n                                    or set additional pai configuration file path in paiConfigPath!'.format(trial_field))\n                pai_resource_fields_required_list = ['gpuNum', 'cpuNum', 'memoryMB']\n                for required_field in pai_resource_fields_required_list:\n                    if experiment_config['trial'].get(required_field) is None and \\\n                            experiment_config['paiConfig'].get(required_field) is None:\n                        raise SchemaError('Please set {0} in trial or paiConfig configuration,\\\n                                    or set additional pai configuration file path in paiConfigPath!'.format(required_field))",
  "def validate_pai_trial_conifg(self, experiment_config):\n        '''validate the trial config in pai platform'''\n        if experiment_config.get('trainingServicePlatform') in ['pai', 'paiYarn']:\n            if experiment_config.get('trial').get('shmMB') and \\\n                    experiment_config['trial']['shmMB'] > experiment_config['trial']['memoryMB']:\n                raise SchemaError('shmMB should be no more than memoryMB!')\n            # backward compatibility\n            warning_information = '{0} is not supported in NNI anymore, please remove the field in config file!\\\n            please refer https://github.com/microsoft/nni/blob/master/docs/en_US/TrainingService/PaiMode.md#run-an-experiment\\\n            for the practices of how to get data and output model in trial code'\n            if experiment_config.get('trial').get('dataDir'):\n                print_warning(warning_information.format('dataDir'))\n            if experiment_config.get('trial').get('outputDir'):\n                print_warning(warning_information.format('outputDir'))\n            self.validate_pai_config_path(experiment_config)",
  "def validate_eth0_device(self, experiment_config):\n        '''validate whether the machine has eth0 device'''\n        if experiment_config.get('trainingServicePlatform') not in ['local'] \\\n                and not experiment_config.get('nniManagerIp') \\\n                and 'eth0' not in netifaces.interfaces():\n            raise SchemaError('This machine does not contain eth0 network device, please set nniManagerIp in config file!')",
  "def nni_info(*args):\n    if args[0].version:\n        try:\n            print(pkg_resources.get_distribution('nni').version)\n        except pkg_resources.ResolutionError:\n            print_error('Get version failed, please use `pip3 list | grep nni` to check nni version!')\n    else:\n        print('please run \"nnictl {positional argument} --help\" to see nnictl guidance')",
  "def parse_args():\n    '''Definite the arguments users need to follow and input'''\n    parser = argparse.ArgumentParser(prog='nnictl', description='use nnictl command to control nni experiments')\n    parser.add_argument('--version', '-v', action='store_true')\n    parser.set_defaults(func=nni_info)\n\n    # create subparsers for args with sub values\n    subparsers = parser.add_subparsers()\n\n    # parse the command of auto generating search space\n    parser_start = subparsers.add_parser('ss_gen', help='automatically generate search space file from trial code')\n    parser_start.add_argument('--trial_command', '-t', required=True, dest='trial_command', help='the command for running trial code')\n    parser_start.add_argument('--trial_dir', '-d', default='./', dest='trial_dir', help='the directory for running the command')\n    parser_start.add_argument('--file', '-f', default='nni_auto_gen_search_space.json', dest='file', help='the path of search space file')\n    parser_start.set_defaults(func=search_space_auto_gen)\n\n    # parse start command\n    parser_start = subparsers.add_parser('create', help='create a new experiment')\n    parser_start.add_argument('--config', '-c', required=True, dest='config', help='the path of yaml config file')\n    parser_start.add_argument('--port', '-p', default=DEFAULT_REST_PORT, dest='port', help='the port of restful server')\n    parser_start.add_argument('--debug', '-d', action='store_true', help=' set debug mode')\n    parser_start.add_argument('--foreground', '-f', action='store_true', help=' set foreground mode, print log content to terminal')\n    parser_start.set_defaults(func=create_experiment)\n\n    # parse resume command\n    parser_resume = subparsers.add_parser('resume', help='resume a new experiment')\n    parser_resume.add_argument('id', nargs='?', help='The id of the experiment you want to resume')\n    parser_resume.add_argument('--port', '-p', default=DEFAULT_REST_PORT, dest='port', help='the port of restful server')\n    parser_resume.add_argument('--debug', '-d', action='store_true', help=' set debug mode')\n    parser_resume.add_argument('--foreground', '-f', action='store_true', help=' set foreground mode, print log content to terminal')\n    parser_resume.set_defaults(func=resume_experiment)\n\n    # parse view command\n    parser_view = subparsers.add_parser('view', help='view a stopped experiment')\n    parser_view.add_argument('id', nargs='?', help='The id of the experiment you want to view')\n    parser_view.add_argument('--port', '-p', default=DEFAULT_REST_PORT, dest='port', help='the port of restful server')\n    parser_view.set_defaults(func=view_experiment)\n\n    # parse update command\n    parser_updater = subparsers.add_parser('update', help='update the experiment')\n    #add subparsers for parser_updater\n    parser_updater_subparsers = parser_updater.add_subparsers()\n    parser_updater_searchspace = parser_updater_subparsers.add_parser('searchspace', help='update searchspace')\n    parser_updater_searchspace.add_argument('id', nargs='?', help='the id of experiment')\n    parser_updater_searchspace.add_argument('--filename', '-f', required=True)\n    parser_updater_searchspace.set_defaults(func=update_searchspace)\n    parser_updater_concurrency = parser_updater_subparsers.add_parser('concurrency', help='update concurrency')\n    parser_updater_concurrency.add_argument('id', nargs='?', help='the id of experiment')\n    parser_updater_concurrency.add_argument('--value', '-v', required=True)\n    parser_updater_concurrency.set_defaults(func=update_concurrency)\n    parser_updater_duration = parser_updater_subparsers.add_parser('duration', help='update duration')\n    parser_updater_duration.add_argument('id', nargs='?', help='the id of experiment')\n    parser_updater_duration.add_argument('--value', '-v', required=True, help='the unit of time should in {\\'s\\', \\'m\\', \\'h\\', \\'d\\'}')\n    parser_updater_duration.set_defaults(func=update_duration)\n    parser_updater_trialnum = parser_updater_subparsers.add_parser('trialnum', help='update maxtrialnum')\n    parser_updater_trialnum.add_argument('id', nargs='?', help='the id of experiment')\n    parser_updater_trialnum.add_argument('--value', '-v', required=True)\n    parser_updater_trialnum.set_defaults(func=update_trialnum)\n\n    #parse stop command\n    parser_stop = subparsers.add_parser('stop', help='stop the experiment')\n    parser_stop.add_argument('id', nargs='?', help='the id of experiment, use \\'all\\' to stop all running experiments')\n    parser_stop.add_argument('--port', '-p', dest='port', help='the port of restful server')\n    parser_stop.add_argument('--all', '-a', action='store_true', help='stop all of experiments')\n    parser_stop.set_defaults(func=stop_experiment)\n\n    #parse trial command\n    parser_trial = subparsers.add_parser('trial', help='get trial information')\n    #add subparsers for parser_trial\n    parser_trial_subparsers = parser_trial.add_subparsers()\n    parser_trial_ls = parser_trial_subparsers.add_parser('ls', help='list trial jobs')\n    parser_trial_ls.add_argument('id', nargs='?', help='the id of experiment')\n    parser_trial_ls.add_argument('--head', type=int, help='list the highest experiments on the default metric')\n    parser_trial_ls.add_argument('--tail', type=int, help='list the lowest experiments on the default metric')\n    parser_trial_ls.set_defaults(func=trial_ls)\n    parser_trial_kill = parser_trial_subparsers.add_parser('kill', help='kill trial jobs')\n    parser_trial_kill.add_argument('id', nargs='?', help='the id of experiment')\n    parser_trial_kill.add_argument('--trial_id', '-T', required=True, dest='trial_id', help='the id of trial to be killed')\n    parser_trial_kill.set_defaults(func=trial_kill)\n    parser_trial_codegen = parser_trial_subparsers.add_parser('codegen', help='generate trial code for a specific trial')\n    parser_trial_codegen.add_argument('id', nargs='?', help='the id of experiment')\n    parser_trial_codegen.add_argument('--trial_id', '-T', required=True, dest='trial_id', help='the id of trial to do code generation')\n    parser_trial_codegen.set_defaults(func=trial_codegen)\n\n    #parse experiment command\n    parser_experiment = subparsers.add_parser('experiment', help='get experiment information')\n    #add subparsers for parser_experiment\n    parser_experiment_subparsers = parser_experiment.add_subparsers()\n    parser_experiment_show = parser_experiment_subparsers.add_parser('show', help='show the information of experiment')\n    parser_experiment_show.add_argument('id', nargs='?', help='the id of experiment')\n    parser_experiment_show.set_defaults(func=list_experiment)\n    parser_experiment_status = parser_experiment_subparsers.add_parser('status', help='show the status of experiment')\n    parser_experiment_status.add_argument('id', nargs='?', help='the id of experiment')\n    parser_experiment_status.set_defaults(func=experiment_status)\n    parser_experiment_list = parser_experiment_subparsers.add_parser('list', help='list all of running experiment ids')\n    parser_experiment_list.add_argument('--all', action='store_true', default=False, help='list all of experiments')\n    parser_experiment_list.set_defaults(func=experiment_list)\n    parser_experiment_clean = parser_experiment_subparsers.add_parser('delete', help='clean up the experiment data')\n    parser_experiment_clean.add_argument('id', nargs='?', help='the id of experiment')\n    parser_experiment_clean.add_argument('--all', action='store_true', default=False, help='delete all of experiments')\n    parser_experiment_clean.set_defaults(func=experiment_clean)\n    #import tuning data\n    parser_import_data = parser_experiment_subparsers.add_parser('import', help='import additional data')\n    parser_import_data.add_argument('id', nargs='?', help='the id of experiment')\n    parser_import_data.add_argument('--filename', '-f', required=True)\n    parser_import_data.set_defaults(func=import_data)\n    #export trial data\n    parser_trial_export = parser_experiment_subparsers.add_parser('export', help='export trial job results to csv or json')\n    parser_trial_export.add_argument('id', nargs='?', help='the id of experiment')\n    parser_trial_export.add_argument('--type', '-t', choices=['json', 'csv'], required=True, dest='type', help='target file type')\n    parser_trial_export.add_argument('--filename', '-f', required=True, dest='path', help='target file path')\n    parser_trial_export.add_argument('--intermediate', '-i', action='store_true',\n                                     default=False, help='are intermediate results included')\n    parser_trial_export.set_defaults(func=export_trials_data)\n    #save an NNI experiment\n    parser_save_experiment = parser_experiment_subparsers.add_parser('save', help='save an experiment')\n    parser_save_experiment.add_argument('id', nargs='?', help='the id of experiment')\n    parser_save_experiment.add_argument('--path', '-p', required=False, help='the folder path to store nni experiment data, \\\n                                   default current working directory')\n    parser_save_experiment.add_argument('--saveCodeDir', '-s', action='store_true', default=False, help='save codeDir data \\\n                                   of the experiment')\n    parser_save_experiment.set_defaults(func=save_experiment)\n    #load an NNI experiment\n    parser_load_experiment = parser_experiment_subparsers.add_parser('load', help='load an experiment')\n    parser_load_experiment.add_argument('--path', '-p', required=True, help='the path of nni package file')\n    parser_load_experiment.add_argument('--codeDir', '-c', required=True, help='the path of codeDir for loaded experiment, \\\n                                   this path will also put the code in the loaded experiment package')\n    parser_load_experiment.add_argument('--logDir', '-l', required=False, help='the path of logDir for loaded experiment')\n    parser_load_experiment.set_defaults(func=load_experiment)\n\n    #parse platform command\n    parser_platform = subparsers.add_parser('platform', help='get platform information')\n    #add subparsers for parser_platform\n    parser_platform_subparsers = parser_platform.add_subparsers()\n    parser_platform_clean = parser_platform_subparsers.add_parser('clean', help='clean up the platform data')\n    parser_platform_clean.add_argument('--config', '-c', required=True, dest='config', help='the path of yaml config file')\n    parser_platform_clean.set_defaults(func=platform_clean)\n\n    #TODO:finish webui function\n    #parse board command\n    parser_webui = subparsers.add_parser('webui', help='get web ui information')\n    #add subparsers for parser_board\n    parser_webui_subparsers = parser_webui.add_subparsers()\n    parser_webui_url = parser_webui_subparsers.add_parser('url', help='show the url of web ui')\n    parser_webui_url.add_argument('id', nargs='?', help='the id of experiment')\n    parser_webui_url.set_defaults(func=webui_url)\n    parser_webui_nas = parser_webui_subparsers.add_parser('nas', help='show nas ui')\n    parser_webui_nas.add_argument('--port', default=6060, type=int, help='port of nas ui')\n    parser_webui_nas.add_argument('--logdir', default='.', type=str, help='the logdir where nas ui will read data')\n    parser_webui_nas.set_defaults(func=webui_nas)\n\n    #parse config command\n    parser_config = subparsers.add_parser('config', help='get config information')\n    parser_config_subparsers = parser_config.add_subparsers()\n    parser_config_show = parser_config_subparsers.add_parser('show', help='show the information of config')\n    parser_config_show.add_argument('id', nargs='?', help='the id of experiment')\n    parser_config_show.set_defaults(func=get_config)\n\n    #parse log command\n    parser_log = subparsers.add_parser('log', help='get log information')\n    # add subparsers for parser_log\n    parser_log_subparsers = parser_log.add_subparsers()\n    parser_log_stdout = parser_log_subparsers.add_parser('stdout', help='get stdout information')\n    parser_log_stdout.add_argument('id', nargs='?', help='the id of experiment')\n    parser_log_stdout.add_argument('--tail', '-T', dest='tail', type=int, help='get tail -100 content of stdout')\n    parser_log_stdout.add_argument('--head', '-H', dest='head', type=int, help='get head -100 content of stdout')\n    parser_log_stdout.add_argument('--path', action='store_true', default=False, help='get the path of stdout file')\n    parser_log_stdout.set_defaults(func=log_stdout)\n    parser_log_stderr = parser_log_subparsers.add_parser('stderr', help='get stderr information')\n    parser_log_stderr.add_argument('id', nargs='?', help='the id of experiment')\n    parser_log_stderr.add_argument('--tail', '-T', dest='tail', type=int, help='get tail -100 content of stderr')\n    parser_log_stderr.add_argument('--head', '-H', dest='head', type=int, help='get head -100 content of stderr')\n    parser_log_stderr.add_argument('--path', action='store_true', default=False, help='get the path of stderr file')\n    parser_log_stderr.set_defaults(func=log_stderr)\n    parser_log_trial = parser_log_subparsers.add_parser('trial', help='get trial log path')\n    parser_log_trial.add_argument('id', nargs='?', help='the id of experiment')\n    parser_log_trial.add_argument('--trial_id', '-T', dest='trial_id', help='find trial log path by id')\n    parser_log_trial.set_defaults(func=log_trial)\n\n    #parse package command\n    parser_package = subparsers.add_parser('package', help='control nni tuner and assessor packages')\n    # add subparsers for parser_package\n    parser_package_subparsers = parser_package.add_subparsers()\n    parser_package_install = parser_package_subparsers.add_parser('install', help='install packages')\n    parser_package_install.add_argument('source', nargs='?', help='installation source, can be a directory or whl file')\n    parser_package_install.add_argument('--name', '-n', dest='name', help='package name to be installed', required=False)\n    parser_package_install.set_defaults(func=package_install)\n\n    parser_package_uninstall = parser_package_subparsers.add_parser('uninstall', help='uninstall packages')\n    parser_package_uninstall.add_argument('name', nargs=1, help='package name to be uninstalled')\n    parser_package_uninstall.set_defaults(func=package_uninstall)\n\n    parser_package_show = parser_package_subparsers.add_parser('show', help='show the information of packages')\n    parser_package_show.add_argument('name', nargs=1, help='builtin name of the package')\n    parser_package_show.set_defaults(func=package_show)\n\n    parser_package_list = parser_package_subparsers.add_parser('list', help='list installed packages')\n    parser_package_list.add_argument('--all', action='store_true', help='list all builtin packages')\n    parser_package_list.set_defaults(func=package_list)\n\n    #parse tensorboard command\n    parser_tensorboard = subparsers.add_parser('tensorboard', help='manage tensorboard')\n    parser_tensorboard_subparsers = parser_tensorboard.add_subparsers()\n    parser_tensorboard_start = parser_tensorboard_subparsers.add_parser('start', help='start tensorboard')\n    parser_tensorboard_start.add_argument('id', nargs='?', help='the id of experiment')\n    parser_tensorboard_start.add_argument('--trial_id', '-T', dest='trial_id', help='the id of trial')\n    parser_tensorboard_start.add_argument('--port', dest='port', default=6006, help='the port to start tensorboard')\n    parser_tensorboard_start.set_defaults(func=start_tensorboard)\n    parser_tensorboard_stop = parser_tensorboard_subparsers.add_parser('stop', help='stop tensorboard')\n    parser_tensorboard_stop.add_argument('id', nargs='?', help='the id of experiment')\n    parser_tensorboard_stop.set_defaults(func=stop_tensorboard)\n\n    #parse top command\n    parser_top = subparsers.add_parser('top', help='monitor the experiment')\n    parser_top.add_argument('--time', '-t', dest='time', type=int, default=3, help='the time interval to update the experiment status, ' \\\n    'the unit is second')\n    parser_top.set_defaults(func=monitor_experiment)\n\n    args = parser.parse_args()\n    args.func(args)",
  "def expand_path(experiment_config, key):\n    '''Change '~' to user home directory'''\n    if experiment_config.get(key):\n        experiment_config[key] = os.path.expanduser(experiment_config[key])",
  "def parse_relative_path(root_path, experiment_config, key):\n    '''Change relative path to absolute path'''\n    if experiment_config.get(key) and not os.path.isabs(experiment_config.get(key)):\n        absolute_path = os.path.join(root_path, experiment_config.get(key))\n        print_normal('expand %s: %s to %s ' % (key, experiment_config[key], absolute_path))\n        experiment_config[key] = absolute_path",
  "def parse_time(time):\n    '''Change the time to seconds'''\n    unit = time[-1]\n    if unit not in ['s', 'm', 'h', 'd']:\n        raise SchemaError('the unit of time could only from {s, m, h, d}')\n    time = time[:-1]\n    if not time.isdigit():\n        raise SchemaError('time format error!')\n    parse_dict = {'s':1, 'm':60, 'h':3600, 'd':86400}\n    return int(time) * parse_dict[unit]",
  "def parse_path(experiment_config, config_path):\n    '''Parse path in config file'''\n    expand_path(experiment_config, 'searchSpacePath')\n    if experiment_config.get('trial'):\n        expand_path(experiment_config['trial'], 'codeDir')\n        if experiment_config['trial'].get('authFile'):\n            expand_path(experiment_config['trial'], 'authFile')\n        if experiment_config['trial'].get('ps'):\n            if experiment_config['trial']['ps'].get('privateRegistryAuthPath'):\n                expand_path(experiment_config['trial']['ps'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('master'):\n            if experiment_config['trial']['master'].get('privateRegistryAuthPath'):\n                expand_path(experiment_config['trial']['master'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('worker'):\n            if experiment_config['trial']['worker'].get('privateRegistryAuthPath'):\n                expand_path(experiment_config['trial']['worker'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('taskRoles'):\n            for index in range(len(experiment_config['trial']['taskRoles'])):\n                if experiment_config['trial']['taskRoles'][index].get('privateRegistryAuthPath'):\n                    expand_path(experiment_config['trial']['taskRoles'][index], 'privateRegistryAuthPath')\n    if experiment_config.get('tuner'):\n        expand_path(experiment_config['tuner'], 'codeDir')\n    if experiment_config.get('assessor'):\n        expand_path(experiment_config['assessor'], 'codeDir')\n    if experiment_config.get('advisor'):\n        expand_path(experiment_config['advisor'], 'codeDir')\n    if experiment_config.get('machineList'):\n        for index in range(len(experiment_config['machineList'])):\n            expand_path(experiment_config['machineList'][index], 'sshKeyPath')\n    if experiment_config['trial'].get('paiConfigPath'):\n        expand_path(experiment_config['trial'], 'paiConfigPath')\n\n    #if users use relative path, convert it to absolute path\n    root_path = os.path.dirname(config_path)\n    if experiment_config.get('searchSpacePath'):\n        parse_relative_path(root_path, experiment_config, 'searchSpacePath')\n    if experiment_config.get('trial'):\n        parse_relative_path(root_path, experiment_config['trial'], 'codeDir')\n        if experiment_config['trial'].get('authFile'):\n            parse_relative_path(root_path, experiment_config['trial'], 'authFile')\n        if experiment_config['trial'].get('ps'):\n            if experiment_config['trial']['ps'].get('privateRegistryAuthPath'):\n                parse_relative_path(root_path, experiment_config['trial']['ps'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('master'):\n            if experiment_config['trial']['master'].get('privateRegistryAuthPath'):\n                parse_relative_path(root_path, experiment_config['trial']['master'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('worker'):\n            if experiment_config['trial']['worker'].get('privateRegistryAuthPath'):\n                parse_relative_path(root_path, experiment_config['trial']['worker'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('taskRoles'):\n            for index in range(len(experiment_config['trial']['taskRoles'])):\n                if experiment_config['trial']['taskRoles'][index].get('privateRegistryAuthPath'):\n                    parse_relative_path(root_path, experiment_config['trial']['taskRoles'][index], 'privateRegistryAuthPath')\n    if experiment_config.get('tuner'):\n        parse_relative_path(root_path, experiment_config['tuner'], 'codeDir')\n    if experiment_config.get('assessor'):\n        parse_relative_path(root_path, experiment_config['assessor'], 'codeDir')\n    if experiment_config.get('advisor'):\n        parse_relative_path(root_path, experiment_config['advisor'], 'codeDir')\n    if experiment_config.get('machineList'):\n        for index in range(len(experiment_config['machineList'])):\n            parse_relative_path(root_path, experiment_config['machineList'][index], 'sshKeyPath')\n    if experiment_config['trial'].get('paiConfigPath'):\n        parse_relative_path(root_path, experiment_config['trial'], 'paiConfigPath')",
  "def set_default_values(experiment_config):\n    if experiment_config.get('maxExecDuration') is None:\n        experiment_config['maxExecDuration'] = '999d'\n    if experiment_config.get('maxTrialNum') is None:\n        experiment_config['maxTrialNum'] = 99999\n    if experiment_config['trainingServicePlatform'] == 'remote':\n        for index in range(len(experiment_config['machineList'])):\n            if experiment_config['machineList'][index].get('port') is None:\n                experiment_config['machineList'][index]['port'] = 22",
  "def validate_all_content(experiment_config, config_path):\n    '''Validate whether experiment_config is valid'''\n    parse_path(experiment_config, config_path)\n    set_default_values(experiment_config)\n\n    NNIConfigSchema().validate(experiment_config)\n\n    experiment_config['maxExecDuration'] = parse_time(experiment_config['maxExecDuration'])",
  "def check_environment():\n    '''check if paramiko is installed'''\n    try:\n        import paramiko\n    except:\n        install_package_command('paramiko')\n        import paramiko\n    return paramiko",
  "def copy_remote_directory_to_local(sftp, remote_path, local_path):\n    '''copy remote directory to local machine'''\n    try:\n        os.makedirs(local_path, exist_ok=True)\n        files = sftp.listdir(remote_path)\n        for file in files:\n            remote_full_path = os.path.join(remote_path, file)\n            local_full_path = os.path.join(local_path, file)\n            try:\n                if sftp.listdir(remote_full_path):\n                    copy_remote_directory_to_local(sftp, remote_full_path, local_full_path)\n            except:\n                sftp.get(remote_full_path, local_full_path)\n    except Exception:\n        pass",
  "def create_ssh_sftp_client(host_ip, port, username, password, ssh_key_path, passphrase):\n    '''create ssh client'''\n    try:\n        paramiko = check_environment()\n        conn = paramiko.Transport(host_ip, port)\n        if ssh_key_path is not None:\n            ssh_key = paramiko.RSAKey.from_private_key_file(ssh_key_path, password=passphrase)\n            conn.connect(username=username, pkey=ssh_key)\n        else:\n            conn.connect(username=username, password=password)\n        sftp = paramiko.SFTPClient.from_transport(conn)\n        return sftp\n    except Exception as exception:\n        print_error('Create ssh client error %s\\n' % exception)",
  "def remove_remote_directory(sftp, directory):\n    '''remove a directory in remote machine'''\n    try:\n        files = sftp.listdir(directory)\n        for file in files:\n            filepath = '/'.join([directory, file])\n            try:\n                sftp.remove(filepath)\n            except IOError:\n                remove_remote_directory(sftp, filepath)\n        sftp.rmdir(directory)\n    except IOError as err:\n        print_error(err)",
  "def validate_digit(value, start, end):\n    '''validate if a digit is valid'''\n    if not str(value).isdigit() or int(value) < start or int(value) > end:\n        raise ValueError('value (%s) must be a digit from %s to %s' % (value, start, end))",
  "def validate_file(path):\n    '''validate if a file exist'''\n    if not os.path.exists(path):\n        raise FileNotFoundError('%s is not a valid file path' % path)",
  "def validate_dispatcher(args):\n    '''validate if the dispatcher of the experiment supports importing data'''\n    nni_config = Config(get_config_filename(args)).get_config('experimentConfig')\n    if nni_config.get('tuner') and nni_config['tuner'].get('builtinTunerName'):\n        dispatcher_name = nni_config['tuner']['builtinTunerName']\n    elif nni_config.get('advisor') and nni_config['advisor'].get('builtinAdvisorName'):\n        dispatcher_name = nni_config['advisor']['builtinAdvisorName']\n    else: # otherwise it should be a customized one\n        return\n    if dispatcher_name not in TUNERS_SUPPORTING_IMPORT_DATA:\n        if dispatcher_name in TUNERS_NO_NEED_TO_IMPORT_DATA:\n            print_warning(\"There is no need to import data for %s\" % dispatcher_name)\n            exit(0)\n        else:\n            print_error(\"%s does not support importing addtional data\" % dispatcher_name)\n            exit(1)",
  "def load_search_space(path):\n    '''load search space content'''\n    content = json.dumps(get_json_content(path))\n    if not content:\n        raise ValueError('searchSpace file should not be empty')\n    return content",
  "def get_query_type(key):\n    '''get update query type'''\n    if key == 'trialConcurrency':\n        return '?update_type=TRIAL_CONCURRENCY'\n    if key == 'maxExecDuration':\n        return '?update_type=MAX_EXEC_DURATION'\n    if key == 'searchSpace':\n        return '?update_type=SEARCH_SPACE'\n    if key == 'maxTrialNum':\n        return '?update_type=MAX_TRIAL_NUM'",
  "def update_experiment_profile(args, key, value):\n    '''call restful server to update experiment profile'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(experiment_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            experiment_profile = json.loads(response.text)\n            experiment_profile['params'][key] = value\n            response = rest_put(experiment_url(rest_port)+get_query_type(key), json.dumps(experiment_profile), REST_TIME_OUT)\n            if response and check_response(response):\n                return response\n    else:\n        print_error('Restful server is not running...')\n    return None",
  "def update_searchspace(args):\n    validate_file(args.filename)\n    content = load_search_space(args.filename)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if update_experiment_profile(args, 'searchSpace', content):\n            print_normal('Update %s success!' % 'searchSpace')\n        else:\n            print_error('Update %s failed!' % 'searchSpace')",
  "def update_concurrency(args):\n    validate_digit(args.value, 1, 1000)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if update_experiment_profile(args, 'trialConcurrency', int(args.value)):\n            print_normal('Update %s success!' % 'concurrency')\n        else:\n            print_error('Update %s failed!' % 'concurrency')",
  "def update_duration(args):\n    #parse time, change time unit to seconds\n    args.value = parse_time(args.value)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if update_experiment_profile(args, 'maxExecDuration', int(args.value)):\n            print_normal('Update %s success!' % 'duration')\n        else:\n            print_error('Update %s failed!' % 'duration')",
  "def update_trialnum(args):\n    validate_digit(args.value, 1, 999999999)\n    if update_experiment_profile(args, 'maxTrialNum', int(args.value)):\n        print_normal('Update %s success!' % 'trialnum')\n    else:\n        print_error('Update %s failed!' % 'trialnum')",
  "def import_data(args):\n    '''import additional data to the experiment'''\n    validate_file(args.filename)\n    validate_dispatcher(args)\n    content = load_search_space(args.filename)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if import_data_to_restful_server(args, content):\n            pass\n        else:\n            print_error('Import data failed!')",
  "def import_data_to_restful_server(args, content):\n    '''call restful server to import data to the experiment'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_post(import_data_url(rest_port), content, REST_TIME_OUT)\n        if response and check_response(response):\n            return response\n    else:\n        print_error('Restful server is not running...')\n    return None",
  "def get_experiment_time(port):\n    '''get the startTime and endTime of an experiment'''\n    response = rest_get(experiment_url(port), REST_TIME_OUT)\n    if response and check_response(response):\n        content = convert_time_stamp_to_date(json.loads(response.text))\n        return content.get('startTime'), content.get('endTime')\n    return None, None",
  "def get_experiment_status(port):\n    '''get the status of an experiment'''\n    result, response = check_rest_server_quick(port)\n    if result:\n        return json.loads(response.text).get('status')\n    return None",
  "def update_experiment():\n    '''Update the experiment status in config file'''\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        return None\n    for key in experiment_dict.keys():\n        if isinstance(experiment_dict[key], dict):\n            if experiment_dict[key].get('status') != 'STOPPED':\n                nni_config = Config(experiment_dict[key]['fileName'])\n                rest_pid = nni_config.get_config('restServerPid')\n                if not detect_process(rest_pid):\n                    experiment_config.update_experiment(key, 'status', 'STOPPED')\n                    continue\n                rest_port = nni_config.get_config('restServerPort')\n                startTime, endTime = get_experiment_time(rest_port)\n                if startTime:\n                    experiment_config.update_experiment(key, 'startTime', startTime)\n                if endTime:\n                    experiment_config.update_experiment(key, 'endTime', endTime)\n                status = get_experiment_status(rest_port)\n                if status:\n                    experiment_config.update_experiment(key, 'status', status)",
  "def check_experiment_id(args, update=True):\n    '''check if the id is valid\n    '''\n    if update:\n        update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print_normal('There is no experiment running...')\n        return None\n    if not args.id:\n        running_experiment_list = []\n        for key in experiment_dict.keys():\n            if isinstance(experiment_dict[key], dict):\n                if experiment_dict[key].get('status') != 'STOPPED':\n                    running_experiment_list.append(key)\n            elif isinstance(experiment_dict[key], list):\n                # if the config file is old version, remove the configuration from file\n                experiment_config.remove_experiment(key)\n        if len(running_experiment_list) > 1:\n            print_error('There are multiple experiments, please set the experiment id...')\n            experiment_information = \"\"\n            for key in running_experiment_list:\n                experiment_information += EXPERIMENT_DETAIL_FORMAT % (key,\n                                                                      experiment_dict[key].get('experimentName', 'N/A'),\n                                                                      experiment_dict[key]['status'],\n                                                                      experiment_dict[key]['port'],\n                                                                      experiment_dict[key].get('platform'),\n                                                                      experiment_dict[key]['startTime'],\n                                                                      experiment_dict[key]['endTime'])\n            print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)\n            exit(1)\n        elif not running_experiment_list:\n            print_error('There is no experiment running.')\n            return None\n        else:\n            return running_experiment_list[0]\n    if experiment_dict.get(args.id):\n        return args.id\n    else:\n        print_error('Id not correct.')\n        return None",
  "def parse_ids(args):\n    '''Parse the arguments for nnictl stop\n    1.If port is provided and id is not specified, return the id who owns the port\n    2.If both port and id are provided, return the id if it owns the port, otherwise fail\n    3.If there is an id specified, return the corresponding id\n    4.If there is no id specified, and there is an experiment running, return the id, or return Error\n    5.If the id matches an experiment, nnictl will return the id.\n    6.If the id ends with *, nnictl will match all ids matchs the regular\n    7.If the id does not exist but match the prefix of an experiment id, nnictl will return the matched id\n    8.If the id does not exist but match multiple prefix of the experiment ids, nnictl will give id information\n    '''\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print_normal('Experiment is not running...')\n        return None\n    result_list = []\n    running_experiment_list = []\n    for key in experiment_dict.keys():\n        if isinstance(experiment_dict[key], dict):\n            if experiment_dict[key].get('status') != 'STOPPED':\n                running_experiment_list.append(key)\n        elif isinstance(experiment_dict[key], list):\n            # if the config file is old version, remove the configuration from file\n            experiment_config.remove_experiment(key)\n    if args.all:\n        return running_experiment_list\n    if args.port is not None:\n        for key in running_experiment_list:\n            if str(experiment_dict[key]['port']) == args.port:\n                result_list.append(key)\n        if args.id and result_list and args.id != result_list[0]:\n            print_error('Experiment id and resful server port not match')\n            exit(1)\n    elif not args.id:\n        if len(running_experiment_list) > 1:\n            print_error('There are multiple experiments, please set the experiment id...')\n            experiment_information = \"\"\n            for key in running_experiment_list:\n                experiment_information += EXPERIMENT_DETAIL_FORMAT % (key,\n                                                                      experiment_dict[key].get('experimentName', 'N/A'),\n                                                                      experiment_dict[key]['status'],\n                                                                      experiment_dict[key]['port'],\n                                                                      experiment_dict[key].get('platform'),\n                                                                      experiment_dict[key]['startTime'],\n                                                                      experiment_dict[key]['endTime'])\n            print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)\n            exit(1)\n        else:\n            result_list = running_experiment_list\n    elif args.id.endswith('*'):\n        for expId in running_experiment_list:\n            if expId.startswith(args.id[:-1]):\n                result_list.append(expId)\n    elif args.id in running_experiment_list:\n        result_list.append(args.id)\n    else:\n        for expId in running_experiment_list:\n            if expId.startswith(args.id):\n                result_list.append(expId)\n        if len(result_list) > 1:\n            print_error(args.id + ' is ambiguous, please choose ' + ' '.join(result_list))\n            return None\n    if not result_list and (args.id  or args.port):\n        print_error('There are no experiments matched, please set correct experiment id or restful server port')\n    elif not result_list:\n        print_error('There is no experiment running...')\n    return result_list",
  "def get_config_filename(args):\n    '''get the file name of config file'''\n    experiment_id = check_experiment_id(args)\n    if experiment_id is None:\n        print_error('Please set correct experiment id.')\n        exit(1)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    return experiment_dict[experiment_id]['fileName']",
  "def get_experiment_port(args):\n    '''get the port of experiment'''\n    experiment_id = check_experiment_id(args)\n    if experiment_id is None:\n        print_error('Please set correct experiment id.')\n        exit(1)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    return experiment_dict[experiment_id]['port']",
  "def convert_time_stamp_to_date(content):\n    '''Convert time stamp to date time format'''\n    start_time_stamp = content.get('startTime')\n    end_time_stamp = content.get('endTime')\n    if start_time_stamp:\n        start_time = datetime.fromtimestamp(start_time_stamp // 1000, timezone.utc).astimezone().strftime(\"%Y/%m/%d %H:%M:%S\")\n        content['startTime'] = str(start_time)\n    if end_time_stamp:\n        end_time = datetime.fromtimestamp(end_time_stamp // 1000, timezone.utc).astimezone().strftime(\"%Y/%m/%d %H:%M:%S\")\n        content['endTime'] = str(end_time)\n    return content",
  "def check_rest(args):\n    '''check if restful server is running'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    running, _ = check_rest_server_quick(rest_port)\n    if not running:\n        print_normal('Restful server is running...')\n    else:\n        print_normal('Restful server is not running...')",
  "def stop_experiment(args):\n    '''Stop the experiment which is running'''\n    if args.id and args.id == 'all':\n        print_warning('\\'nnictl stop all\\' is abolished, please use \\'nnictl stop --all\\' to stop all of experiments!')\n        exit(1)\n    experiment_id_list = parse_ids(args)\n    if experiment_id_list:\n        experiment_config = Experiments()\n        experiment_dict = experiment_config.get_all_experiments()\n        for experiment_id in experiment_id_list:\n            print_normal('Stopping experiment %s' % experiment_id)\n            nni_config = Config(experiment_dict[experiment_id]['fileName'])\n            rest_pid = nni_config.get_config('restServerPid')\n            if rest_pid:\n                kill_command(rest_pid)\n                tensorboard_pid_list = nni_config.get_config('tensorboardPidList')\n                if tensorboard_pid_list:\n                    for tensorboard_pid in tensorboard_pid_list:\n                        try:\n                            kill_command(tensorboard_pid)\n                        except Exception as exception:\n                            print_error(exception)\n                    nni_config.set_config('tensorboardPidList', [])\n            print_normal('Stop experiment success.')\n            experiment_config.update_experiment(experiment_id, 'status', 'STOPPED')\n            time_now = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n            experiment_config.update_experiment(experiment_id, 'endTime', str(time_now))",
  "def trial_ls(args):\n    '''List trial'''\n    def final_metric_data_cmp(lhs, rhs):\n        metric_l = json.loads(json.loads(lhs['finalMetricData'][0]['data']))\n        metric_r = json.loads(json.loads(rhs['finalMetricData'][0]['data']))\n        if isinstance(metric_l, float):\n            return metric_l - metric_r\n        elif isinstance(metric_l, dict):\n            return metric_l['default'] - metric_r['default']\n        else:\n            print_error('Unexpected data format. Please check your data.')\n            raise ValueError\n\n    if args.head and args.tail:\n        print_error('Head and tail cannot be set at the same time.')\n        return\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            content = json.loads(response.text)\n            if args.head:\n                assert args.head > 0, 'The number of requested data must be greater than 0.'\n                content = sorted(filter(lambda x: 'finalMetricData' in x, content),\n                                 key=cmp_to_key(final_metric_data_cmp), reverse=True)[:args.head]\n            elif args.tail:\n                assert args.tail > 0, 'The number of requested data must be greater than 0.'\n                content = sorted(filter(lambda x: 'finalMetricData' in x, content),\n                                 key=cmp_to_key(final_metric_data_cmp))[:args.tail]\n            for index, value in enumerate(content):\n                content[index] = convert_time_stamp_to_date(value)\n            print(json.dumps(content, indent=4, sort_keys=True, separators=(',', ':')))\n        else:\n            print_error('List trial failed...')\n    else:\n        print_error('Restful server is not running...')",
  "def trial_kill(args):\n    '''List trial'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_delete(trial_job_id_url(rest_port, args.trial_id), REST_TIME_OUT)\n        if response and check_response(response):\n            print(response.text)\n        else:\n            print_error('Kill trial job failed...')\n    else:\n        print_error('Restful server is not running...')",
  "def trial_codegen(args):\n    '''Generate code for a specific trial'''\n    print_warning('Currently, this command is only for nni nas programming interface.')\n    exp_id = check_experiment_id(args)\n    nni_config = Config(get_config_filename(args))\n    if not nni_config.get_config('experimentConfig')['useAnnotation']:\n        print_error('The experiment is not using annotation')\n        exit(1)\n    code_dir = nni_config.get_config('experimentConfig')['trial']['codeDir']\n    expand_annotations(code_dir, './exp_%s_trial_%s_code'%(exp_id, args.trial_id), exp_id, args.trial_id)",
  "def list_experiment(args):\n    '''Get experiment information'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(experiment_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            content = convert_time_stamp_to_date(json.loads(response.text))\n            print(json.dumps(content, indent=4, sort_keys=True, separators=(',', ':')))\n        else:\n            print_error('List experiment failed...')\n    else:\n        print_error('Restful server is not running...')",
  "def experiment_status(args):\n    '''Show the status of experiment'''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    result, response = check_rest_server_quick(rest_port)\n    if not result:\n        print_normal('Restful server is not running...')\n    else:\n        print(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(',', ':')))",
  "def log_internal(args, filetype):\n    '''internal function to call get_log_content'''\n    file_name = get_config_filename(args)\n    if filetype == 'stdout':\n        file_full_path = os.path.join(NNICTL_HOME_DIR, file_name, 'stdout')\n    else:\n        file_full_path = os.path.join(NNICTL_HOME_DIR, file_name, 'stderr')\n    print(check_output_command(file_full_path, head=args.head, tail=args.tail))",
  "def log_stdout(args):\n    '''get stdout log'''\n    log_internal(args, 'stdout')",
  "def log_stderr(args):\n    '''get stderr log'''\n    log_internal(args, 'stderr')",
  "def log_trial(args):\n    ''''get trial log path'''\n    trial_id_path_dict = {}\n    trial_id_list = []\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            content = json.loads(response.text)\n            for trial in content:\n                trial_id_list.append(trial.get('id'))\n                if trial.get('logPath'):\n                    trial_id_path_dict[trial.get('id')] = trial['logPath']\n    else:\n        print_error('Restful server is not running...')\n        exit(1)\n    if args.trial_id:\n        if args.trial_id not in trial_id_list:\n            print_error('Trial id {0} not correct, please check your command!'.format(args.trial_id))\n            exit(1)\n        if trial_id_path_dict.get(args.trial_id):\n            print_normal('id:' + args.trial_id + ' path:' + trial_id_path_dict[args.trial_id])\n        else:\n            print_error('Log path is not available yet, please wait...')\n            exit(1)\n    else:\n        print_normal('All of trial log info:')\n        for key in trial_id_path_dict:\n            print_normal('id:' + key + ' path:' + trial_id_path_dict[key])\n        if not trial_id_path_dict:\n            print_normal('None')",
  "def get_config(args):\n    '''get config info'''\n    nni_config = Config(get_config_filename(args))\n    print(nni_config.get_all_config())",
  "def webui_url(args):\n    '''show the url of web ui'''\n    nni_config = Config(get_config_filename(args))\n    print_normal('{0} {1}'.format('Web UI url:', ' '.join(nni_config.get_config('webuiUrl'))))",
  "def webui_nas(args):\n    '''launch nas ui'''\n    print_normal('Starting NAS UI...')\n    try:\n        entry_dir = get_nni_installation_path()\n        entry_file = os.path.join(entry_dir, 'nasui', 'server.js')\n        node_command = 'node'\n        if sys.platform == 'win32':\n            node_command = os.path.join(entry_dir[:-3], 'Scripts', 'node.exe')\n        cmds = [node_command, '--max-old-space-size=4096', entry_file, '--port', str(args.port), '--logdir', args.logdir]\n        subprocess.run(cmds)\n    except KeyboardInterrupt:\n        pass",
  "def local_clean(directory):\n    '''clean up local data'''\n    print_normal('removing folder {0}'.format(directory))\n    try:\n        shutil.rmtree(directory)\n    except FileNotFoundError:\n        print_error('{0} does not exist.'.format(directory))",
  "def remote_clean(machine_list, experiment_id=None):\n    '''clean up remote data'''\n    for machine in machine_list:\n        passwd = machine.get('passwd')\n        userName = machine.get('username')\n        host = machine.get('ip')\n        port = machine.get('port')\n        sshKeyPath = machine.get('sshKeyPath')\n        passphrase = machine.get('passphrase')\n        if experiment_id:\n            remote_dir = '/' + '/'.join(['tmp', 'nni-experiments', experiment_id])\n        else:\n            remote_dir = '/' + '/'.join(['tmp', 'nni-experiments'])\n        sftp = create_ssh_sftp_client(host, port, userName, passwd, sshKeyPath, passphrase)\n        print_normal('removing folder {0}'.format(host + ':' + str(port) + remote_dir))\n        remove_remote_directory(sftp, remote_dir)",
  "def hdfs_clean(host, user_name, output_dir, experiment_id=None):\n    '''clean up hdfs data'''\n    hdfs_client = HdfsClient(hosts='{0}:80'.format(host), user_name=user_name, webhdfs_path='/webhdfs/api/v1', timeout=5)\n    if experiment_id:\n        full_path = '/' + '/'.join([user_name, 'nni', 'experiments', experiment_id])\n    else:\n        full_path = '/' + '/'.join([user_name, 'nni', 'experiments'])\n    print_normal('removing folder {0} in hdfs'.format(full_path))\n    hdfs_client.delete(full_path, recursive=True)\n    if output_dir:\n        pattern = re.compile('hdfs://(?P<host>([0-9]{1,3}.){3}[0-9]{1,3})(:[0-9]{2,5})?(?P<baseDir>/.*)?')\n        match_result = pattern.match(output_dir)\n        if match_result:\n            output_host = match_result.group('host')\n            output_dir = match_result.group('baseDir')\n            #check if the host is valid\n            if output_host != host:\n                print_warning('The host in {0} is not consistent with {1}'.format(output_dir, host))\n            else:\n                if experiment_id:\n                    output_dir = output_dir + '/' + experiment_id\n                print_normal('removing folder {0} in hdfs'.format(output_dir))\n                hdfs_client.delete(output_dir, recursive=True)",
  "def experiment_clean(args):\n    '''clean up the experiment data'''\n    experiment_id_list = []\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if args.all:\n        experiment_id_list = list(experiment_dict.keys())\n    else:\n        if args.id is None:\n            print_error('please set experiment id.')\n            exit(1)\n        if args.id not in experiment_dict:\n            print_error('Cannot find experiment {0}.'.format(args.id))\n            exit(1)\n        experiment_id_list.append(args.id)\n    while True:\n        print('INFO: This action will delete experiment {0}, and it\\'s not recoverable.'.format(' '.join(experiment_id_list)))\n        inputs = input('INFO: do you want to continue?[y/N]:')\n        if not inputs.lower() or inputs.lower() in ['n', 'no']:\n            exit(0)\n        elif inputs.lower() not in ['y', 'n', 'yes', 'no']:\n            print_warning('please input Y or N.')\n        else:\n            break\n    for experiment_id in experiment_id_list:\n        nni_config = Config(experiment_dict[experiment_id]['fileName'])\n        platform = nni_config.get_config('experimentConfig').get('trainingServicePlatform')\n        experiment_id = nni_config.get_config('experimentId')\n        if platform == 'remote':\n            machine_list = nni_config.get_config('experimentConfig').get('machineList')\n            remote_clean(machine_list, experiment_id)\n        elif platform == 'pai':\n            host = nni_config.get_config('experimentConfig').get('paiConfig').get('host')\n            user_name = nni_config.get_config('experimentConfig').get('paiConfig').get('userName')\n            output_dir = nni_config.get_config('experimentConfig').get('trial').get('outputDir')\n            hdfs_clean(host, user_name, output_dir, experiment_id)\n        elif platform != 'local':\n            #TODO: support all platforms\n            print_warning('platform {0} clean up not supported yet.'.format(platform))\n            exit(0)\n        #clean local data\n        local_base_dir = nni_config.get_config('experimentConfig').get('logDir')\n        if not local_base_dir:\n            local_base_dir = NNI_HOME_DIR\n        local_experiment_dir = os.path.join(local_base_dir, experiment_id)\n        experiment_folder_name_list = ['checkpoint', 'db', 'log', 'trials']\n        for folder_name in experiment_folder_name_list:\n            local_clean(os.path.join(local_experiment_dir, folder_name))\n        if not os.listdir(local_experiment_dir):\n            local_clean(local_experiment_dir)\n        experiment_config = Experiments()\n        print_normal('removing metadata of experiment {0}'.format(experiment_id))\n        experiment_config.remove_experiment(experiment_id)\n        print_normal('Done.')",
  "def get_platform_dir(config_content):\n    '''get the dir list to be deleted'''\n    platform = config_content.get('trainingServicePlatform')\n    dir_list = []\n    if platform == 'remote':\n        machine_list = config_content.get('machineList')\n        for machine in machine_list:\n            host = machine.get('ip')\n            port = machine.get('port')\n            dir_list.append(host + ':' + str(port) + '/tmp/nni')\n    elif platform == 'pai':\n        host = config_content.get('paiConfig').get('host')\n        user_name = config_content.get('paiConfig').get('userName')\n        output_dir = config_content.get('trial').get('outputDir')\n        dir_list.append('server: {0}, path: {1}/nni'.format(host, user_name))\n        if output_dir:\n            dir_list.append(output_dir)\n    return dir_list",
  "def platform_clean(args):\n    '''clean up the experiment data'''\n    config_path = os.path.abspath(args.config)\n    if not os.path.exists(config_path):\n        print_error('Please set correct config path.')\n        exit(1)\n    config_content = get_yml_content(config_path)\n    platform = config_content.get('trainingServicePlatform')\n    if platform == 'local':\n        print_normal('it doesn\u2019t need to clean local platform.')\n        exit(0)\n    if platform not in ['remote', 'pai']:\n        print_normal('platform {0} not supported.'.format(platform))\n        exit(0)\n    update_experiment()\n    dir_list = get_platform_dir(config_content)\n    if not dir_list:\n        print_normal('No folder of NNI caches is found.')\n        exit(1)\n    while True:\n        print_normal('This command will remove below folders of NNI caches. If other users are using experiments' \\\n                     ' on below hosts, it will be broken.')\n        for value in dir_list:\n            print('       ' + value)\n        inputs = input('INFO: do you want to continue?[y/N]:')\n        if not inputs.lower() or inputs.lower() in ['n', 'no']:\n            exit(0)\n        elif inputs.lower() not in ['y', 'n', 'yes', 'no']:\n            print_warning('please input Y or N.')\n        else:\n            break\n    if platform == 'remote':\n        machine_list = config_content.get('machineList')\n        remote_clean(machine_list, None)\n    elif platform == 'pai':\n        host = config_content.get('paiConfig').get('host')\n        user_name = config_content.get('paiConfig').get('userName')\n        output_dir = config_content.get('trial').get('outputDir')\n        hdfs_clean(host, user_name, output_dir, None)\n    print_normal('Done.')",
  "def experiment_list(args):\n    '''get the information of all experiments'''\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print_normal('Cannot find experiments.')\n        exit(1)\n    experiment_id_list = []\n    if args.all:\n        for key in experiment_dict.keys():\n            experiment_id_list.append(key)\n    else:\n        for key in experiment_dict.keys():\n            if experiment_dict[key]['status'] != 'STOPPED':\n                experiment_id_list.append(key)\n        if not experiment_id_list:\n            print_warning('There is no experiment running...\\nYou can use \\'nnictl experiment list --all\\' to list all experiments.')\n    experiment_information = \"\"\n    for key in experiment_id_list:\n        experiment_information += EXPERIMENT_DETAIL_FORMAT % (key,\n                                                              experiment_dict[key].get('experimentName', 'N/A'),\n                                                              experiment_dict[key]['status'],\n                                                              experiment_dict[key]['port'],\n                                                              experiment_dict[key].get('platform'),\n                                                              experiment_dict[key]['startTime'],\n                                                              experiment_dict[key]['endTime'])\n    print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)",
  "def get_time_interval(time1, time2):\n    '''get the interval of two times'''\n    try:\n        #convert time to timestamp\n        time1 = time.mktime(time.strptime(time1, '%Y/%m/%d %H:%M:%S'))\n        time2 = time.mktime(time.strptime(time2, '%Y/%m/%d %H:%M:%S'))\n        seconds = (datetime.fromtimestamp(time2) - datetime.fromtimestamp(time1)).seconds\n        #convert seconds to day:hour:minute:second\n        days = seconds / 86400\n        seconds %= 86400\n        hours = seconds / 3600\n        seconds %= 3600\n        minutes = seconds / 60\n        seconds %= 60\n        return '%dd %dh %dm %ds' % (days, hours, minutes, seconds)\n    except:\n        return 'N/A'",
  "def show_experiment_info():\n    '''show experiment information in monitor'''\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print('There is no experiment running...')\n        exit(1)\n    experiment_id_list = []\n    for key in experiment_dict.keys():\n        if experiment_dict[key]['status'] != 'STOPPED':\n            experiment_id_list.append(key)\n    if not experiment_id_list:\n        print_warning('There is no experiment running...')\n        return\n    for key in experiment_id_list:\n        print(EXPERIMENT_MONITOR_INFO % (key, experiment_dict[key]['status'], experiment_dict[key]['port'], \\\n             experiment_dict[key].get('platform'), experiment_dict[key]['startTime'], \\\n             get_time_interval(experiment_dict[key]['startTime'], experiment_dict[key]['endTime'])))\n        print(TRIAL_MONITOR_HEAD)\n        running, response = check_rest_server_quick(experiment_dict[key]['port'])\n        if running:\n            response = rest_get(trial_jobs_url(experiment_dict[key]['port']), REST_TIME_OUT)\n            if response and check_response(response):\n                content = json.loads(response.text)\n                for index, value in enumerate(content):\n                    content[index] = convert_time_stamp_to_date(value)\n                    print(TRIAL_MONITOR_CONTENT % (content[index].get('id'), content[index].get('startTime'), \\\n                          content[index].get('endTime'), content[index].get('status')))\n        print(TRIAL_MONITOR_TAIL)",
  "def set_monitor(auto_exit, time_interval, port=None, pid=None):\n    '''set the experiment monitor engine'''\n    while True:\n        try:\n            if sys.platform == 'win32':\n                os.system('cls')\n            else:\n                os.system('clear')\n            update_experiment()\n            show_experiment_info()\n            if auto_exit:\n                status = get_experiment_status(port)\n                if status in ['DONE', 'ERROR', 'STOPPED']:\n                    print_normal('Experiment status is {0}.'.format(status))\n                    print_normal('Stopping experiment...')\n                    kill_command(pid)\n                    print_normal('Stop experiment success.')\n                    exit(0)\n            time.sleep(time_interval)\n        except KeyboardInterrupt:\n            if auto_exit:\n                print_normal('Stopping experiment...')\n                kill_command(pid)\n                print_normal('Stop experiment success.')\n            else:\n                print_normal('Exiting...')\n            exit(0)\n        except Exception as exception:\n            print_error(exception)\n            exit(1)",
  "def monitor_experiment(args):\n    '''monitor the experiment'''\n    if args.time <= 0:\n        print_error('please input a positive integer as time interval, the unit is second.')\n        exit(1)\n    set_monitor(False, args.time)",
  "def export_trials_data(args):\n    '''export experiment metadata and intermediate results to json or csv\n    '''\n    def groupby_trial_id(intermediate_results):\n        sorted(intermediate_results, key=lambda x: x['timestamp'])\n        groupby = dict()\n        for content in intermediate_results:\n            groupby.setdefault(content['trialJobId'], []).append(json.loads(content['data']))\n        return groupby\n\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    if not running:\n        print_error('Restful server is not running')\n        return\n    response = rest_get(export_data_url(rest_port), 20)\n    if response is not None and check_response(response):\n        content = json.loads(response.text)\n        if args.intermediate:\n            intermediate_results_response = rest_get(metric_data_url(rest_port), REST_TIME_OUT)\n            if not intermediate_results_response or not check_response(intermediate_results_response):\n                print_error('Error getting intermediate results.')\n                return\n            intermediate_results = groupby_trial_id(json.loads(intermediate_results_response.text))\n            for record in content:\n                record['intermediate'] = intermediate_results[record['id']]\n        if args.type == 'json':\n            with open(args.path, 'w') as file:\n                file.write(json.dumps(content))\n        elif args.type == 'csv':\n            trial_records = []\n            for record in content:\n                formated_record = dict()\n                if args.intermediate:\n                    formated_record['intermediate'] = '[' + ','.join(record['intermediate']) + ']'\n                record_value = json.loads(record['value'])\n                if not isinstance(record_value, (float, int)):\n                    formated_record.update({**record['parameter'], **record_value, **{'id': record['id']}})\n                else:\n                    formated_record.update({**record['parameter'], **{'reward': record_value, 'id': record['id']}})\n                trial_records.append(formated_record)\n            if not trial_records:\n                print_error('No trial results collected! Please check your trial log...')\n                exit(0)\n            with open(args.path, 'w', newline='') as file:\n                writer = csv.DictWriter(file, set.union(*[set(r.keys()) for r in trial_records]))\n                writer.writeheader()\n                writer.writerows(trial_records)\n        else:\n            print_error('Unknown type: %s' % args.type)\n            return\n    else:\n        print_error('Export failed...')",
  "def search_space_auto_gen(args):\n    '''dry run trial code to generate search space file'''\n    trial_dir = os.path.expanduser(args.trial_dir)\n    file_path = os.path.expanduser(args.file)\n    if not os.path.isabs(file_path):\n        file_path = os.path.join(os.getcwd(), file_path)\n    assert os.path.exists(trial_dir)\n    if os.path.exists(file_path):\n        print_warning('%s already exists, will be overwritten.' % file_path)\n    print_normal('Dry run to generate search space...')\n    Popen(args.trial_command, cwd=trial_dir, env=dict(os.environ, NNI_GEN_SEARCH_SPACE=file_path), shell=True).wait()\n    if not os.path.exists(file_path):\n        print_warning('Expected search space file \\'{}\\' generated, but not found.'.format(file_path))\n    else:\n        print_normal('Generate search space done: \\'{}\\'.'.format(file_path))",
  "def save_experiment(args):\n    '''save experiment data to a zip file'''\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if args.id is None:\n        print_error('Please set experiment id.')\n        exit(1)\n    if args.id not in experiment_dict:\n        print_error('Cannot find experiment {0}.'.format(args.id))\n        exit(1)\n    if experiment_dict[args.id].get('status') != 'STOPPED':\n        print_error('Can only save stopped experiment!')\n        exit(1)\n    print_normal('Saving...')\n    nni_config = Config(experiment_dict[args.id]['fileName'])\n    logDir = os.path.join(NNI_HOME_DIR, args.id)\n    if nni_config.get_config('logDir'):\n        logDir = os.path.join(nni_config.get_config('logDir'), args.id)\n    temp_root_dir = generate_temp_dir()\n\n    # Step1. Copy logDir to temp folder\n    if not os.path.exists(logDir):\n        print_error('logDir: %s does not exist!' % logDir)\n        exit(1)\n    temp_experiment_dir = os.path.join(temp_root_dir, 'experiment')\n    shutil.copytree(logDir, temp_experiment_dir)\n\n    # Step2. Copy nnictl metadata to temp folder\n    temp_nnictl_dir = os.path.join(temp_root_dir, 'nnictl')\n    os.makedirs(temp_nnictl_dir, exist_ok=True)\n    try:\n        with open(os.path.join(temp_nnictl_dir, '.experiment'), 'w') as file:\n            experiment_dict[args.id]['id'] = args.id\n            json.dump(experiment_dict[args.id], file)\n    except IOError:\n        print_error('Write file to %s failed!' % os.path.join(temp_nnictl_dir, '.experiment'))\n        exit(1)\n    nnictl_config_dir = os.path.join(NNICTL_HOME_DIR, experiment_dict[args.id]['fileName'])\n    shutil.copytree(nnictl_config_dir, os.path.join(temp_nnictl_dir, experiment_dict[args.id]['fileName']))\n\n    # Step3. Copy code dir\n    if args.saveCodeDir:\n        temp_code_dir = os.path.join(temp_root_dir, 'code')\n        shutil.copytree(nni_config.get_config('experimentConfig')['trial']['codeDir'], temp_code_dir)\n\n    # Step4. Archive folder\n    zip_package_name = 'nni_experiment_%s' % args.id\n    if args.path:\n        os.makedirs(args.path, exist_ok=True)\n        zip_package_name = os.path.join(args.path, zip_package_name)\n    shutil.make_archive(zip_package_name, 'zip', temp_root_dir)\n    print_normal('Save to %s.zip success!' % zip_package_name)\n\n    # Step5. Cleanup temp data\n    shutil.rmtree(temp_root_dir)",
  "def load_experiment(args):\n    '''load experiment data'''\n    package_path = os.path.expanduser(args.path)\n    if not os.path.exists(args.path):\n        print_error('file path %s does not exist!' % args.path)\n        exit(1)\n    temp_root_dir = generate_temp_dir()\n    shutil.unpack_archive(package_path, temp_root_dir)\n    print_normal('Loading...')\n    # Step1. Validation\n    if not os.path.exists(args.codeDir):\n        print_error('Invalid: codeDir path does not exist!')\n        exit(1)\n    if args.logDir:\n        if not os.path.exists(args.logDir):\n            print_error('Invalid: logDir path does not exist!')\n            exit(1)\n    experiment_temp_dir = os.path.join(temp_root_dir, 'experiment')\n    if not os.path.exists(os.path.join(experiment_temp_dir, 'db')):\n        print_error('Invalid archive file: db file does not exist!')\n        shutil.rmtree(temp_root_dir)\n        exit(1)\n    nnictl_temp_dir = os.path.join(temp_root_dir, 'nnictl')\n    if not os.path.exists(os.path.join(nnictl_temp_dir, '.experiment')):\n        print_error('Invalid archive file: nnictl metadata file does not exist!')\n        shutil.rmtree(temp_root_dir)\n        exit(1)\n    try:\n        with open(os.path.join(nnictl_temp_dir, '.experiment'), 'r') as file:\n            experiment_metadata = json.load(file)\n    except ValueError as err:\n        print_error('Invalid nnictl metadata file: %s' % err)\n        shutil.rmtree(temp_root_dir)\n        exit(1)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    experiment_id = experiment_metadata.get('id')\n    if experiment_id in experiment_dict:\n        print_error('Invalid: experiment id already exist!')\n        shutil.rmtree(temp_root_dir)\n        exit(1)\n    if not os.path.exists(os.path.join(nnictl_temp_dir, experiment_metadata.get('fileName'))):\n        print_error('Invalid: experiment metadata does not exist!')\n        shutil.rmtree(temp_root_dir)\n        exit(1)\n\n    # Step2. Copy nnictl metadata\n    src_path = os.path.join(nnictl_temp_dir, experiment_metadata.get('fileName'))\n    dest_path = os.path.join(NNICTL_HOME_DIR, experiment_metadata.get('fileName'))\n    if os.path.exists(dest_path):\n        shutil.rmtree(dest_path)\n    shutil.copytree(src_path, dest_path)\n\n    # Step3. Copy experiment data\n    nni_config = Config(experiment_metadata.get('fileName'))\n    nnictl_exp_config = nni_config.get_config('experimentConfig')\n    if args.logDir:\n        logDir = args.logDir\n        nnictl_exp_config['logDir'] = logDir\n    else:\n        if nnictl_exp_config.get('logDir'):\n            logDir = nnictl_exp_config['logDir']\n        else:\n            logDir = NNI_HOME_DIR\n    os.rename(os.path.join(temp_root_dir, 'experiment'), os.path.join(temp_root_dir, experiment_id))\n    src_path = os.path.join(os.path.join(temp_root_dir, experiment_id))\n    dest_path = os.path.join(os.path.join(logDir, experiment_id))\n    if os.path.exists(dest_path):\n        shutil.rmtree(dest_path)\n    shutil.copytree(src_path, dest_path)\n\n    # Step4. Copy code dir\n    codeDir = os.path.expanduser(args.codeDir)\n    if not os.path.isabs(codeDir):\n        codeDir = os.path.join(os.getcwd(), codeDir)\n        print_normal('Expand codeDir to %s' % codeDir)\n    nnictl_exp_config['trial']['codeDir'] = codeDir\n    archive_code_dir = os.path.join(temp_root_dir, 'code')\n    if os.path.exists(archive_code_dir):\n        file_list = os.listdir(archive_code_dir)\n        for file_name in file_list:\n            src_path = os.path.join(archive_code_dir, file_name)\n            target_path = os.path.join(codeDir, file_name)\n            if os.path.exists(target_path):\n                print_error('Copy %s failed, %s exist!' % (file_name, target_path))\n                continue\n            if os.path.isdir(src_path):\n                shutil.copytree(src_path, target_path)\n            else:\n                shutil.copy(src_path, target_path)\n\n    # Step5. Create experiment metadata\n    nni_config.set_config('experimentConfig', nnictl_exp_config)\n    experiment_config.add_experiment(experiment_id,\n                                     experiment_metadata.get('port'),\n                                     experiment_metadata.get('startTime'),\n                                     experiment_metadata.get('fileName'),\n                                     experiment_metadata.get('platform'),\n                                     experiment_metadata.get('experimentName'),\n                                     experiment_metadata.get('endTime'),\n                                     experiment_metadata.get('status'))\n    print_normal('Load experiment %s succsss!' % experiment_id)\n\n    # Step6. Cleanup temp data\n    shutil.rmtree(temp_root_dir)",
  "def final_metric_data_cmp(lhs, rhs):\n        metric_l = json.loads(json.loads(lhs['finalMetricData'][0]['data']))\n        metric_r = json.loads(json.loads(rhs['finalMetricData'][0]['data']))\n        if isinstance(metric_l, float):\n            return metric_l - metric_r\n        elif isinstance(metric_l, dict):\n            return metric_l['default'] - metric_r['default']\n        else:\n            print_error('Unexpected data format. Please check your data.')\n            raise ValueError",
  "def groupby_trial_id(intermediate_results):\n        sorted(intermediate_results, key=lambda x: x['timestamp'])\n        groupby = dict()\n        for content in intermediate_results:\n            groupby.setdefault(content['trialJobId'], []).append(json.loads(content['data']))\n        return groupby",
  "def get_log_path(config_file_name):\n    '''generate stdout and stderr log path'''\n    stdout_full_path = os.path.join(NNICTL_HOME_DIR, config_file_name, 'stdout')\n    stderr_full_path = os.path.join(NNICTL_HOME_DIR, config_file_name, 'stderr')\n    return stdout_full_path, stderr_full_path",
  "def print_log_content(config_file_name):\n    '''print log information'''\n    stdout_full_path, stderr_full_path = get_log_path(config_file_name)\n    print_normal(' Stdout:')\n    print(check_output_command(stdout_full_path))\n    print('\\n\\n')\n    print_normal(' Stderr:')\n    print(check_output_command(stderr_full_path))",
  "def start_rest_server(port, platform, mode, config_file_name, foreground=False, experiment_id=None, log_dir=None, log_level=None):\n    '''Run nni manager process'''\n    if detect_port(port):\n        print_error('Port %s is used by another process, please reset the port!\\n' \\\n        'You could use \\'nnictl create --help\\' to get help information' % port)\n        exit(1)\n\n    if (platform != 'local') and detect_port(int(port) + 1):\n        print_error('PAI mode need an additional adjacent port %d, and the port %d is used by another process!\\n' \\\n        'You could set another port to start experiment!\\n' \\\n        'You could use \\'nnictl create --help\\' to get help information' % ((int(port) + 1), (int(port) + 1)))\n        exit(1)\n\n    print_normal('Starting restful server...')\n\n    entry_dir = get_nni_installation_path()\n    if (not entry_dir) or (not os.path.exists(entry_dir)):\n        print_error('Fail to find nni under python library')\n        exit(1)\n    entry_file = os.path.join(entry_dir, 'main.js')\n\n    node_command = 'node'\n    if sys.platform == 'win32':\n        node_command = os.path.join(entry_dir[:-3], 'Scripts', 'node.exe')\n    cmds = [node_command, '--max-old-space-size=4096', entry_file, '--port', str(port), '--mode', platform]\n    if mode == 'view':\n        cmds += ['--start_mode', 'resume']\n        cmds += ['--readonly', 'true']\n    else:\n        cmds += ['--start_mode', mode]\n    if log_dir is not None:\n        cmds += ['--log_dir', log_dir]\n    if log_level is not None:\n        cmds += ['--log_level', log_level]\n    if mode in ['resume', 'view']:\n        cmds += ['--experiment_id', experiment_id]\n    if foreground:\n        cmds += ['--foreground', 'true']\n    stdout_full_path, stderr_full_path = get_log_path(config_file_name)\n    with open(stdout_full_path, 'a+') as stdout_file, open(stderr_full_path, 'a+') as stderr_file:\n        time_now = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n        #add time information in the header of log files\n        log_header = LOG_HEADER % str(time_now)\n        stdout_file.write(log_header)\n        stderr_file.write(log_header)\n        if sys.platform == 'win32':\n            from subprocess import CREATE_NEW_PROCESS_GROUP\n            if foreground:\n                process = Popen(cmds, cwd=entry_dir, stdout=PIPE, stderr=STDOUT, creationflags=CREATE_NEW_PROCESS_GROUP)\n            else:\n                process = Popen(cmds, cwd=entry_dir, stdout=stdout_file, stderr=stderr_file, creationflags=CREATE_NEW_PROCESS_GROUP)\n        else:\n            if foreground:\n                process = Popen(cmds, cwd=entry_dir, stdout=PIPE, stderr=PIPE)\n            else:\n                process = Popen(cmds, cwd=entry_dir, stdout=stdout_file, stderr=stderr_file)\n    return process, str(time_now)",
  "def set_trial_config(experiment_config, port, config_file_name):\n    '''set trial configuration'''\n    request_data = dict()\n    request_data['trial_config'] = experiment_config['trial']\n    response = rest_put(cluster_metadata_url(port), json.dumps(request_data), REST_TIME_OUT)\n    if check_response(response):\n        return True\n    else:\n        print('Error message is {}'.format(response.text))\n        _, stderr_full_path = get_log_path(config_file_name)\n        if response:\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(',', ':')))\n        return False",
  "def set_local_config(experiment_config, port, config_file_name):\n    '''set local configuration'''\n    request_data = dict()\n    if experiment_config.get('localConfig'):\n        request_data['local_config'] = experiment_config['localConfig']\n        if request_data['local_config']:\n            if request_data['local_config'].get('gpuIndices') and isinstance(request_data['local_config'].get('gpuIndices'), int):\n                request_data['local_config']['gpuIndices'] = str(request_data['local_config'].get('gpuIndices'))\n            if request_data['local_config'].get('maxTrialNumOnEachGpu'):\n                request_data['local_config']['maxTrialNumOnEachGpu'] = request_data['local_config'].get('maxTrialNumOnEachGpu')\n            if request_data['local_config'].get('useActiveGpu'):\n                request_data['local_config']['useActiveGpu'] = request_data['local_config'].get('useActiveGpu')\n        response = rest_put(cluster_metadata_url(port), json.dumps(request_data), REST_TIME_OUT)\n        err_message = ''\n        if not response or not check_response(response):\n            if response is not None:\n                err_message = response.text\n                _, stderr_full_path = get_log_path(config_file_name)\n                with open(stderr_full_path, 'a+') as fout:\n                    fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n            return False, err_message\n\n    return set_trial_config(experiment_config, port, config_file_name), None",
  "def set_remote_config(experiment_config, port, config_file_name):\n    '''Call setClusterMetadata to pass trial'''\n    #set machine_list\n    request_data = dict()\n    request_data['machine_list'] = experiment_config['machineList']\n    if request_data['machine_list']:\n        for i in range(len(request_data['machine_list'])):\n            if isinstance(request_data['machine_list'][i].get('gpuIndices'), int):\n                request_data['machine_list'][i]['gpuIndices'] = str(request_data['machine_list'][i].get('gpuIndices'))\n    # It needs to connect all remote machines, the time out of connection is 30 seconds.\n    # So timeout of this place should be longer.\n    response = rest_put(cluster_metadata_url(port), json.dumps(request_data), 60, True)\n    err_message = ''\n    if not response or not check_response(response):\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message",
  "def setNNIManagerIp(experiment_config, port, config_file_name):\n    '''set nniManagerIp'''\n    if experiment_config.get('nniManagerIp') is None:\n        return True, None\n    ip_config_dict = dict()\n    ip_config_dict['nni_manager_ip'] = {'nniManagerIp': experiment_config['nniManagerIp']}\n    response = rest_put(cluster_metadata_url(port), json.dumps(ip_config_dict), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    return True, None",
  "def set_pai_config(experiment_config, port, config_file_name):\n    '''set pai configuration'''\n    pai_config_data = dict()\n    pai_config_data['pai_config'] = experiment_config['paiConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(pai_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message",
  "def set_pai_yarn_config(experiment_config, port, config_file_name):\n    '''set paiYarn configuration'''\n    pai_yarn_config_data = dict()\n    pai_yarn_config_data['pai_yarn_config'] = experiment_config['paiYarnConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(pai_yarn_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message",
  "def set_kubeflow_config(experiment_config, port, config_file_name):\n    '''set kubeflow configuration'''\n    kubeflow_config_data = dict()\n    kubeflow_config_data['kubeflow_config'] = experiment_config['kubeflowConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(kubeflow_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message",
  "def set_frameworkcontroller_config(experiment_config, port, config_file_name):\n    '''set kubeflow configuration'''\n    frameworkcontroller_config_data = dict()\n    frameworkcontroller_config_data['frameworkcontroller_config'] = experiment_config['frameworkcontrollerConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(frameworkcontroller_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message",
  "def set_dlts_config(experiment_config, port, config_file_name):\n    '''set dlts configuration'''\n    dlts_config_data = dict()\n    dlts_config_data['dlts_config'] = experiment_config['dltsConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(dlts_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message",
  "def set_aml_config(experiment_config, port, config_file_name):\n    '''set aml configuration'''\n    aml_config_data = dict()\n    aml_config_data['aml_config'] = experiment_config['amlConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(aml_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message",
  "def set_experiment(experiment_config, mode, port, config_file_name):\n    '''Call startExperiment (rest POST /experiment) with yaml file content'''\n    request_data = dict()\n    request_data['authorName'] = experiment_config['authorName']\n    request_data['experimentName'] = experiment_config['experimentName']\n    request_data['trialConcurrency'] = experiment_config['trialConcurrency']\n    request_data['maxExecDuration'] = experiment_config['maxExecDuration']\n    request_data['maxTrialNum'] = experiment_config['maxTrialNum']\n    request_data['searchSpace'] = experiment_config.get('searchSpace')\n    request_data['trainingServicePlatform'] = experiment_config.get('trainingServicePlatform')\n\n    if experiment_config.get('description'):\n        request_data['description'] = experiment_config['description']\n    if experiment_config.get('multiPhase'):\n        request_data['multiPhase'] = experiment_config.get('multiPhase')\n    if experiment_config.get('multiThread'):\n        request_data['multiThread'] = experiment_config.get('multiThread')\n    if experiment_config.get('advisor'):\n        request_data['advisor'] = experiment_config['advisor']\n        if request_data['advisor'].get('gpuNum'):\n            print_error('gpuNum is deprecated, please use gpuIndices instead.')\n        if request_data['advisor'].get('gpuIndices') and isinstance(request_data['advisor'].get('gpuIndices'), int):\n            request_data['advisor']['gpuIndices'] = str(request_data['advisor'].get('gpuIndices'))\n    else:\n        request_data['tuner'] = experiment_config['tuner']\n        if request_data['tuner'].get('gpuNum'):\n            print_error('gpuNum is deprecated, please use gpuIndices instead.')\n        if request_data['tuner'].get('gpuIndices') and isinstance(request_data['tuner'].get('gpuIndices'), int):\n            request_data['tuner']['gpuIndices'] = str(request_data['tuner'].get('gpuIndices'))\n        if 'assessor' in experiment_config:\n            request_data['assessor'] = experiment_config['assessor']\n            if request_data['assessor'].get('gpuNum'):\n                print_error('gpuNum is deprecated, please remove it from your config file.')\n    #debug mode should disable version check\n    if experiment_config.get('debug') is not None:\n        request_data['versionCheck'] = not experiment_config.get('debug')\n    #validate version check\n    if experiment_config.get('versionCheck') is not None:\n        request_data['versionCheck'] = experiment_config.get('versionCheck')\n    if experiment_config.get('logCollection'):\n        request_data['logCollection'] = experiment_config.get('logCollection')\n\n    request_data['clusterMetaData'] = []\n    if experiment_config['trainingServicePlatform'] == 'local':\n        request_data['clusterMetaData'].append(\n            {'key':'codeDir', 'value':experiment_config['trial']['codeDir']})\n        request_data['clusterMetaData'].append(\n            {'key': 'command', 'value': experiment_config['trial']['command']})\n    elif experiment_config['trainingServicePlatform'] == 'remote':\n        request_data['clusterMetaData'].append(\n            {'key': 'machine_list', 'value': experiment_config['machineList']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'pai':\n        request_data['clusterMetaData'].append(\n            {'key': 'pai_config', 'value': experiment_config['paiConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'paiYarn':\n        request_data['clusterMetaData'].append(\n            {'key': 'pai_yarn_config', 'value': experiment_config['paiYarnConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'kubeflow':\n        request_data['clusterMetaData'].append(\n            {'key': 'kubeflow_config', 'value': experiment_config['kubeflowConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'frameworkcontroller':\n        request_data['clusterMetaData'].append(\n            {'key': 'frameworkcontroller_config', 'value': experiment_config['frameworkcontrollerConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    response = rest_post(experiment_url(port), json.dumps(request_data), REST_TIME_OUT, show_error=True)\n    if check_response(response):\n        return response\n    else:\n        _, stderr_full_path = get_log_path(config_file_name)\n        if response is not None:\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(',', ':')))\n            print_error('Setting experiment error, error message is {}'.format(response.text))\n        return None",
  "def set_platform_config(platform, experiment_config, port, config_file_name, rest_process):\n    '''call set_cluster_metadata for specific platform'''\n    print_normal('Setting {0} config...'.format(platform))\n    config_result, err_msg = None, None\n    if platform == 'local':\n        config_result, err_msg = set_local_config(experiment_config, port, config_file_name)\n    elif platform == 'remote':\n        config_result, err_msg = set_remote_config(experiment_config, port, config_file_name)\n    elif platform == 'pai':\n        config_result, err_msg = set_pai_config(experiment_config, port, config_file_name)\n    elif platform == 'paiYarn':\n        config_result, err_msg = set_pai_yarn_config(experiment_config, port, config_file_name)\n    elif platform == 'kubeflow':\n        config_result, err_msg = set_kubeflow_config(experiment_config, port, config_file_name)\n    elif platform == 'frameworkcontroller':\n        config_result, err_msg = set_frameworkcontroller_config(experiment_config, port, config_file_name)\n    elif platform == 'dlts':\n        config_result, err_msg = set_dlts_config(experiment_config, port, config_file_name)\n    elif platform == 'aml':\n        config_result, err_msg = set_aml_config(experiment_config, port, config_file_name)\n    else:\n        raise Exception(ERROR_INFO % 'Unsupported platform!')\n        exit(1)\n    if config_result:\n        print_normal('Successfully set {0} config!'.format(platform))\n    else:\n        print_error('Failed! Error is: {}'.format(err_msg))\n        try:\n            kill_command(rest_process.pid)\n        except Exception:\n            raise Exception(ERROR_INFO % 'Rest server stopped!')\n        exit(1)",
  "def launch_experiment(args, experiment_config, mode, config_file_name, experiment_id=None):\n    '''follow steps to start rest server and start experiment'''\n    nni_config = Config(config_file_name)\n    # check packages for tuner\n    package_name, module_name = None, None\n    if experiment_config.get('tuner') and experiment_config['tuner'].get('builtinTunerName'):\n        package_name = experiment_config['tuner']['builtinTunerName']\n        module_name, _ = get_builtin_module_class_name('tuners', package_name)\n    elif experiment_config.get('advisor') and experiment_config['advisor'].get('builtinAdvisorName'):\n        package_name = experiment_config['advisor']['builtinAdvisorName']\n        module_name, _ = get_builtin_module_class_name('advisors', package_name)\n    if package_name and module_name:\n        try:\n            stdout_full_path, stderr_full_path = get_log_path(config_file_name)\n            with open(stdout_full_path, 'a+') as stdout_file, open(stderr_full_path, 'a+') as stderr_file:\n                check_call([sys.executable, '-c', 'import %s'%(module_name)], stdout=stdout_file, stderr=stderr_file)\n        except CalledProcessError:\n            print_error('some errors happen when import package %s.' %(package_name))\n            print_log_content(config_file_name)\n            if package_name in INSTALLABLE_PACKAGE_META:\n                print_error('If %s is not installed, it should be installed through '\\\n                            '\\'nnictl package install --name %s\\''%(package_name, package_name))\n            exit(1)\n    log_dir = experiment_config['logDir'] if experiment_config.get('logDir') else None\n    log_level = experiment_config['logLevel'] if experiment_config.get('logLevel') else None\n    #view experiment mode do not need debug function, when view an experiment, there will be no new logs created\n    foreground = False\n    if mode != 'view':\n        foreground = args.foreground\n        if log_level not in ['trace', 'debug'] and (args.debug or experiment_config.get('debug') is True):\n            log_level = 'debug'\n    # start rest server\n    rest_process, start_time = start_rest_server(args.port, experiment_config['trainingServicePlatform'], \\\n                                                 mode, config_file_name, foreground, experiment_id, log_dir, log_level)\n    nni_config.set_config('restServerPid', rest_process.pid)\n    # Deal with annotation\n    if experiment_config.get('useAnnotation'):\n        path = os.path.join(tempfile.gettempdir(), get_user(), 'nni', 'annotation')\n        if not os.path.isdir(path):\n            os.makedirs(path)\n        path = tempfile.mkdtemp(dir=path)\n        nas_mode = experiment_config['trial'].get('nasMode', 'classic_mode')\n        code_dir = expand_annotations(experiment_config['trial']['codeDir'], path, nas_mode=nas_mode)\n        experiment_config['trial']['codeDir'] = code_dir\n        search_space = generate_search_space(code_dir)\n        experiment_config['searchSpace'] = json.dumps(search_space)\n        assert search_space, ERROR_INFO % 'Generated search space is empty'\n    elif experiment_config.get('searchSpacePath'):\n        search_space = get_json_content(experiment_config.get('searchSpacePath'))\n        experiment_config['searchSpace'] = json.dumps(search_space)\n    else:\n        experiment_config['searchSpace'] = json.dumps('')\n\n    # check rest server\n    running, _ = check_rest_server(args.port)\n    if running:\n        print_normal('Successfully started Restful server!')\n    else:\n        print_error('Restful server start failed!')\n        print_log_content(config_file_name)\n        try:\n            kill_command(rest_process.pid)\n        except Exception:\n            raise Exception(ERROR_INFO % 'Rest server stopped!')\n        exit(1)\n    if mode != 'view':\n        # set platform configuration\n        set_platform_config(experiment_config['trainingServicePlatform'], experiment_config, args.port,\\\n                            config_file_name, rest_process)\n\n    # start a new experiment\n    print_normal('Starting experiment...')\n    # set debug configuration\n    if mode != 'view' and experiment_config.get('debug') is None:\n        experiment_config['debug'] = args.debug\n    response = set_experiment(experiment_config, mode, args.port, config_file_name)\n    if response:\n        if experiment_id is None:\n            experiment_id = json.loads(response.text).get('experiment_id')\n        nni_config.set_config('experimentId', experiment_id)\n    else:\n        print_error('Start experiment failed!')\n        print_log_content(config_file_name)\n        try:\n            kill_command(rest_process.pid)\n        except Exception:\n            raise Exception(ERROR_INFO % 'Restful server stopped!')\n        exit(1)\n    if experiment_config.get('nniManagerIp'):\n        web_ui_url_list = ['{0}:{1}'.format(experiment_config['nniManagerIp'], str(args.port))]\n    else:\n        web_ui_url_list = get_local_urls(args.port)\n    nni_config.set_config('webuiUrl', web_ui_url_list)\n\n    # save experiment information\n    nnictl_experiment_config = Experiments()\n    nnictl_experiment_config.add_experiment(experiment_id, args.port, start_time, config_file_name,\n                                            experiment_config['trainingServicePlatform'],\n                                            experiment_config['experimentName'])\n\n    print_normal(EXPERIMENT_SUCCESS_INFO % (experiment_id, '   '.join(web_ui_url_list)))\n    if mode != 'view' and args.foreground:\n        try:\n            while True:\n                log_content = rest_process.stdout.readline().strip().decode('utf-8')\n                print(log_content)\n        except KeyboardInterrupt:\n            kill_command(rest_process.pid)\n            print_normal('Stopping experiment...')",
  "def create_experiment(args):\n    '''start a new experiment'''\n    config_file_name = ''.join(random.sample(string.ascii_letters + string.digits, 8))\n    nni_config = Config(config_file_name)\n    config_path = os.path.abspath(args.config)\n    if not os.path.exists(config_path):\n        print_error('Please set correct config path!')\n        exit(1)\n    experiment_config = get_yml_content(config_path)\n    try:\n        validate_all_content(experiment_config, config_path)\n    except Exception as e:\n        print_error(e)\n        exit(1)\n\n    nni_config.set_config('experimentConfig', experiment_config)\n    nni_config.set_config('restServerPort', args.port)\n    try:\n        launch_experiment(args, experiment_config, 'new', config_file_name)\n    except Exception as exception:\n        nni_config = Config(config_file_name)\n        restServerPid = nni_config.get_config('restServerPid')\n        if restServerPid:\n            kill_command(restServerPid)\n        print_error(exception)\n        exit(1)",
  "def manage_stopped_experiment(args, mode):\n    '''view a stopped experiment'''\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    experiment_id = None\n    #find the latest stopped experiment\n    if not args.id:\n        print_error('Please set experiment id! \\nYou could use \\'nnictl {0} id\\' to {0} a stopped experiment!\\n' \\\n        'You could use \\'nnictl experiment list --all\\' to show all experiments!'.format(mode))\n        exit(1)\n    else:\n        if experiment_dict.get(args.id) is None:\n            print_error('Id %s not exist!' % args.id)\n            exit(1)\n        if experiment_dict[args.id]['status'] != 'STOPPED':\n            print_error('Only stopped experiments can be {0}ed!'.format(mode))\n            exit(1)\n        experiment_id = args.id\n    print_normal('{0} experiment {1}...'.format(mode, experiment_id))\n    nni_config = Config(experiment_dict[experiment_id]['fileName'])\n    experiment_config = nni_config.get_config('experimentConfig')\n    experiment_id = nni_config.get_config('experimentId')\n    new_config_file_name = ''.join(random.sample(string.ascii_letters + string.digits, 8))\n    new_nni_config = Config(new_config_file_name)\n    new_nni_config.set_config('experimentConfig', experiment_config)\n    new_nni_config.set_config('restServerPort', args.port)\n    try:\n        launch_experiment(args, experiment_config, mode, new_config_file_name, experiment_id)\n    except Exception as exception:\n        nni_config = Config(new_config_file_name)\n        restServerPid = nni_config.get_config('restServerPid')\n        if restServerPid:\n            kill_command(restServerPid)\n        print_error(exception)\n        exit(1)",
  "def view_experiment(args):\n    '''view a stopped experiment'''\n    manage_stopped_experiment(args, 'view')",
  "def resume_experiment(args):\n    '''resume an experiment'''\n    manage_stopped_experiment(args, 'resume')",
  "def check_output_command(file_path, head=None, tail=None):\n    \"\"\"call check_output command to read content from a file\"\"\"\n    if os.path.exists(file_path):\n        if sys.platform == 'win32':\n            cmds = ['powershell.exe', 'type', file_path]\n            if head:\n                cmds += ['|', 'select', '-first', str(head)]\n            elif tail:\n                cmds += ['|', 'select', '-last', str(tail)]\n            return check_output(cmds, shell=True).decode('utf-8')\n        else:\n            cmds = ['cat', file_path]\n            if head:\n                cmds = ['head', '-' + str(head), file_path]\n            elif tail:\n                cmds = ['tail', '-' + str(tail), file_path]\n            return check_output(cmds, shell=False).decode('utf-8')\n    else:\n        print_error('{0} does not exist!'.format(file_path))\n        exit(1)",
  "def kill_command(pid):\n    \"\"\"kill command\"\"\"\n    if sys.platform == 'win32':\n        process = psutil.Process(pid=pid)\n        process.send_signal(signal.CTRL_BREAK_EVENT)\n    else:\n        cmds = ['kill', str(pid)]\n        call(cmds)",
  "def install_package_command(package_name):\n    \"\"\"\n    Install python package from pip.\n\n    Parameters\n    ----------\n    package_name: str\n        The name of package to be installed.\n    \"\"\"\n    call(_get_pip_install() + [package_name], shell=False)",
  "def install_requirements_command(requirements_path):\n    \"\"\"\n    Install packages from `requirements.txt` in `requirements_path`.\n\n    Parameters\n    ----------\n    requirements_path: str\n        Path to the directory that contains `requirements.txt`.\n    \"\"\"\n    return call(_get_pip_install() + [\"-r\", requirements_path], shell=False)",
  "def _get_pip_install():\n    python = \"python\" if sys.platform == \"win32\" else \"python3\"\n    ret = [python, \"-m\", \"pip\", \"install\"]\n    if \"CONDA_DEFAULT_ENV\" not in os.environ and \"VIRTUAL_ENV\" not in os.environ and \\\n            (sys.platform != \"win32\" and os.getuid() != 0):  # on unix and not running in root\n        ret.append(\"--user\")  # not in virtualenv or conda\n    return ret",
  "def call_pip_install(source):\n    return call(_get_pip_install() + [source])",
  "def call_pip_uninstall(module_name):\n    python = \"python\" if sys.platform == \"win32\" else \"python3\"\n    cmd = [python, \"-m\", \"pip\", \"uninstall\", module_name]\n    return call(cmd)",
  "class Config:\n    '''a util class to load and save config'''\n    def __init__(self, file_path):\n        config_path = os.path.join(NNICTL_HOME_DIR, str(file_path))\n        os.makedirs(config_path, exist_ok=True)\n        self.config_file = os.path.join(config_path, '.config')\n        self.config = self.read_file()\n\n    def get_all_config(self):\n        '''get all of config values'''\n        return json.dumps(self.config, indent=4, sort_keys=True, separators=(',', ':'))\n\n    def set_config(self, key, value):\n        '''set {key:value} paris to self.config'''\n        self.config = self.read_file()\n        self.config[key] = value\n        self.write_file()\n\n    def get_config(self, key):\n        '''get a value according to key'''\n        return self.config.get(key)\n\n    def write_file(self):\n        '''save config to local file'''\n        if self.config:\n            try:\n                with open(self.config_file, 'w') as file:\n                    json.dump(self.config, file)\n            except IOError as error:\n                print('Error:', error)\n                return\n\n    def read_file(self):\n        '''load config from local file'''\n        if os.path.exists(self.config_file):\n            try:\n                with open(self.config_file, 'r') as file:\n                    return json.load(file)\n            except ValueError:\n                return {}\n        return {}",
  "class Experiments:\n    '''Maintain experiment list'''\n    def __init__(self):\n        os.makedirs(NNICTL_HOME_DIR, exist_ok=True)\n        self.experiment_file = os.path.join(NNICTL_HOME_DIR, '.experiment')\n        self.experiments = self.read_file()\n\n    def add_experiment(self, expId, port, startTime, file_name, platform, experiment_name, endTime='N/A', status='INITIALIZED'):\n        '''set {key:value} paris to self.experiment'''\n        self.experiments[expId] = {}\n        self.experiments[expId]['port'] = port\n        self.experiments[expId]['startTime'] = startTime\n        self.experiments[expId]['endTime'] = endTime\n        self.experiments[expId]['status'] = status\n        self.experiments[expId]['fileName'] = file_name\n        self.experiments[expId]['platform'] = platform\n        self.experiments[expId]['experimentName'] = experiment_name\n        self.write_file()\n\n    def update_experiment(self, expId, key, value):\n        '''Update experiment'''\n        if expId not in self.experiments:\n            return False\n        self.experiments[expId][key] = value\n        self.write_file()\n        return True\n\n    def remove_experiment(self, expId):\n        '''remove an experiment by id'''\n        if expId in self.experiments:\n            fileName = self.experiments.pop(expId).get('fileName')\n            if fileName:\n                logPath = os.path.join(NNICTL_HOME_DIR, fileName)\n                try:\n                    shutil.rmtree(logPath)\n                except FileNotFoundError:\n                    print_error('{0} does not exist.'.format(logPath))\n        self.write_file()\n\n    def get_all_experiments(self):\n        '''return all of experiments'''\n        return self.experiments\n\n    def write_file(self):\n        '''save config to local file'''\n        try:\n            with open(self.experiment_file, 'w') as file:\n                json.dump(self.experiments, file)\n        except IOError as error:\n            print('Error:', error)\n            return ''\n\n    def read_file(self):\n        '''load config from local file'''\n        if os.path.exists(self.experiment_file):\n            try:\n                with open(self.experiment_file, 'r') as file:\n                    return json.load(file)\n            except ValueError:\n                return {}\n        return {}",
  "def __init__(self, file_path):\n        config_path = os.path.join(NNICTL_HOME_DIR, str(file_path))\n        os.makedirs(config_path, exist_ok=True)\n        self.config_file = os.path.join(config_path, '.config')\n        self.config = self.read_file()",
  "def get_all_config(self):\n        '''get all of config values'''\n        return json.dumps(self.config, indent=4, sort_keys=True, separators=(',', ':'))",
  "def set_config(self, key, value):\n        '''set {key:value} paris to self.config'''\n        self.config = self.read_file()\n        self.config[key] = value\n        self.write_file()",
  "def get_config(self, key):\n        '''get a value according to key'''\n        return self.config.get(key)",
  "def write_file(self):\n        '''save config to local file'''\n        if self.config:\n            try:\n                with open(self.config_file, 'w') as file:\n                    json.dump(self.config, file)\n            except IOError as error:\n                print('Error:', error)\n                return",
  "def read_file(self):\n        '''load config from local file'''\n        if os.path.exists(self.config_file):\n            try:\n                with open(self.config_file, 'r') as file:\n                    return json.load(file)\n            except ValueError:\n                return {}\n        return {}",
  "def __init__(self):\n        os.makedirs(NNICTL_HOME_DIR, exist_ok=True)\n        self.experiment_file = os.path.join(NNICTL_HOME_DIR, '.experiment')\n        self.experiments = self.read_file()",
  "def add_experiment(self, expId, port, startTime, file_name, platform, experiment_name, endTime='N/A', status='INITIALIZED'):\n        '''set {key:value} paris to self.experiment'''\n        self.experiments[expId] = {}\n        self.experiments[expId]['port'] = port\n        self.experiments[expId]['startTime'] = startTime\n        self.experiments[expId]['endTime'] = endTime\n        self.experiments[expId]['status'] = status\n        self.experiments[expId]['fileName'] = file_name\n        self.experiments[expId]['platform'] = platform\n        self.experiments[expId]['experimentName'] = experiment_name\n        self.write_file()",
  "def update_experiment(self, expId, key, value):\n        '''Update experiment'''\n        if expId not in self.experiments:\n            return False\n        self.experiments[expId][key] = value\n        self.write_file()\n        return True",
  "def remove_experiment(self, expId):\n        '''remove an experiment by id'''\n        if expId in self.experiments:\n            fileName = self.experiments.pop(expId).get('fileName')\n            if fileName:\n                logPath = os.path.join(NNICTL_HOME_DIR, fileName)\n                try:\n                    shutil.rmtree(logPath)\n                except FileNotFoundError:\n                    print_error('{0} does not exist.'.format(logPath))\n        self.write_file()",
  "def get_all_experiments(self):\n        '''return all of experiments'''\n        return self.experiments",
  "def write_file(self):\n        '''save config to local file'''\n        try:\n            with open(self.experiment_file, 'w') as file:\n                json.dump(self.experiments, file)\n        except IOError as error:\n            print('Error:', error)\n            return ''",
  "def read_file(self):\n        '''load config from local file'''\n        if os.path.exists(self.experiment_file):\n            try:\n                with open(self.experiment_file, 'r') as file:\n                    return json.load(file)\n            except ValueError:\n                return {}\n        return {}",
  "def metric_data_url(port):\n    '''get metric_data url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, METRIC_DATA_API)",
  "def check_status_url(port):\n    '''get check_status url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, CHECK_STATUS_API)",
  "def cluster_metadata_url(port):\n    '''get cluster_metadata_url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, CLUSTER_METADATA_API)",
  "def import_data_url(port):\n    '''get import_data_url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, IMPORT_DATA_API)",
  "def experiment_url(port):\n    '''get experiment_url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, EXPERIMENT_API)",
  "def trial_jobs_url(port):\n    '''get trial_jobs url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, TRIAL_JOBS_API)",
  "def trial_job_id_url(port, job_id):\n    '''get trial_jobs with id url'''\n    return '{0}:{1}{2}{3}/{4}'.format(BASE_URL, port, API_ROOT_URL, TRIAL_JOBS_API, job_id)",
  "def export_data_url(port):\n    '''get export_data url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, EXPORT_DATA_API)",
  "def tensorboard_url(port):\n    '''get tensorboard url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, TENSORBOARD_API)",
  "def get_local_urls(port):\n    '''get urls of local machine'''\n    url_list = []\n    for _, info in psutil.net_if_addrs().items():\n        for addr in info:\n            if socket.AddressFamily.AF_INET == addr.family:\n                url_list.append('http://{}:{}'.format(addr.address, port))\n    return url_list",
  "def parse_log_path(args, trial_content):\n    '''parse log path'''\n    path_list = []\n    host_list = []\n    for trial in trial_content:\n        if args.trial_id and args.trial_id != 'all' and trial.get('id') != args.trial_id:\n            continue\n        pattern = r'(?P<head>.+)://(?P<host>.+):(?P<path>.*)'\n        match = re.search(pattern, trial['logPath'])\n        if match:\n            path_list.append(match.group('path'))\n            host_list.append(match.group('host'))\n    if not path_list:\n        print_error('Trial id %s error!' % args.trial_id)\n        exit(1)\n    return path_list, host_list",
  "def copy_data_from_remote(args, nni_config, trial_content, path_list, host_list, temp_nni_path):\n    '''use ssh client to copy data from remote machine to local machien'''\n    machine_list = nni_config.get_config('experimentConfig').get('machineList')\n    machine_dict = {}\n    local_path_list = []\n    for machine in machine_list:\n        machine_dict[machine['ip']] = {'port': machine['port'], 'passwd': machine['passwd'], 'username': machine['username'],\n                                       'sshKeyPath': machine.get('sshKeyPath'), 'passphrase': machine.get('passphrase')}\n    for index, host in enumerate(host_list):\n        local_path = os.path.join(temp_nni_path, trial_content[index].get('id'))\n        local_path_list.append(local_path)\n        print_normal('Copying log data from %s to %s' % (host + ':' + path_list[index], local_path))\n        sftp = create_ssh_sftp_client(host, machine_dict[host]['port'], machine_dict[host]['username'], machine_dict[host]['passwd'],\n                                      machine_dict[host]['sshKeyPath'], machine_dict[host]['passphrase'])\n        copy_remote_directory_to_local(sftp, path_list[index], local_path)\n    print_normal('Copy done!')\n    return local_path_list",
  "def get_path_list(args, nni_config, trial_content, temp_nni_path):\n    '''get path list according to different platform'''\n    path_list, host_list = parse_log_path(args, trial_content)\n    platform = nni_config.get_config('experimentConfig').get('trainingServicePlatform')\n    if platform == 'local':\n        print_normal('Log path: %s' % ' '.join(path_list))\n        return path_list\n    elif platform == 'remote':\n        path_list = copy_data_from_remote(args, nni_config, trial_content, path_list, host_list, temp_nni_path)\n        print_normal('Log path: %s' % ' '.join(path_list))\n        return path_list\n    else:\n        print_error('Not supported platform!')\n        exit(1)",
  "def format_tensorboard_log_path(path_list):\n    new_path_list = []\n    for index, value in enumerate(path_list):\n        new_path_list.append('name%d:%s' % (index + 1, value))\n    return ','.join(new_path_list)",
  "def start_tensorboard_process(args, nni_config, path_list, temp_nni_path):\n    '''call cmds to start tensorboard process in local machine'''\n    if detect_port(args.port):\n        print_error('Port %s is used by another process, please reset port!' % str(args.port))\n        exit(1)\n    with open(os.path.join(temp_nni_path, 'tensorboard_stdout'), 'a+') as stdout_file, \\\n         open(os.path.join(temp_nni_path, 'tensorboard_stderr'), 'a+') as stderr_file:\n        log_dir_cmd = '--logdir_spec' if check_tensorboard_version() >= '2.0' else '--logdir'\n        cmds = ['tensorboard', log_dir_cmd, format_tensorboard_log_path(path_list), '--port', str(args.port)]\n        tensorboard_process = Popen(cmds, stdout=stdout_file, stderr=stderr_file)\n    url_list = get_local_urls(args.port)\n    print_green('Start tensorboard success!')\n    print_normal('Tensorboard urls: ' + '     '.join(url_list))\n    tensorboard_process_pid_list = nni_config.get_config('tensorboardPidList')\n    if tensorboard_process_pid_list is None:\n        tensorboard_process_pid_list = [tensorboard_process.pid]\n    else:\n        tensorboard_process_pid_list.append(tensorboard_process.pid)\n    nni_config.set_config('tensorboardPidList', tensorboard_process_pid_list)",
  "def stop_tensorboard(args):\n    '''stop tensorboard'''\n    experiment_id = check_experiment_id(args)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    config_file_name = experiment_dict[experiment_id]['fileName']\n    nni_config = Config(config_file_name)\n    tensorboard_pid_list = nni_config.get_config('tensorboardPidList')\n    if tensorboard_pid_list:\n        for tensorboard_pid in tensorboard_pid_list:\n            try:\n                cmds = ['kill', '-9', str(tensorboard_pid)]\n                call(cmds)\n            except Exception as exception:\n                print_error(exception)\n        nni_config.set_config('tensorboardPidList', [])\n        print_normal('Stop tensorboard success!')\n    else:\n        print_error('No tensorboard configuration!')",
  "def start_tensorboard(args):\n    '''start tensorboard'''\n    experiment_id = check_experiment_id(args)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    config_file_name = experiment_dict[experiment_id]['fileName']\n    nni_config = Config(config_file_name)\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    trial_content = None\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            trial_content = json.loads(response.text)\n        else:\n            print_error('List trial failed...')\n    else:\n        print_error('Restful server is not running...')\n    if not trial_content:\n        print_error('No trial information!')\n        exit(1)\n    if len(trial_content) > 1 and not args.trial_id:\n        print_error('There are multiple trials, please set trial id!')\n        exit(1)\n    experiment_id = nni_config.get_config('experimentId')\n    temp_nni_path = os.path.join(tempfile.gettempdir(), 'nni', experiment_id)\n    os.makedirs(temp_nni_path, exist_ok=True)\n\n    path_list = get_path_list(args, nni_config, trial_content, temp_nni_path)\n    start_tensorboard_process(args, nni_config, path_list, temp_nni_path)",
  "def install_by_name(package_name):\n    if package_name not in INSTALLABLE_PACKAGE_META:\n        raise RuntimeError('{} is not found in installable packages!'.format(package_name))\n\n    requirements_path = os.path.join(nni.__path__[0], INSTALLABLE_PACKAGE_META[package_name]['code_sub_dir'], 'requirements.txt')\n    assert os.path.exists(requirements_path)\n\n    return install_requirements_command(requirements_path)",
  "def package_install(args):\n    '''install packages'''\n    installed = False\n    try:\n        if args.name:\n            if install_by_name(args.name) == 0:\n                package_meta = {}\n                package_meta['type'] = INSTALLABLE_PACKAGE_META[args.name]['type']\n                package_meta['name'] = args.name\n                package_meta['class_name'] = INSTALLABLE_PACKAGE_META[args.name]['class_name']\n                package_meta['class_args_validator'] = INSTALLABLE_PACKAGE_META[args.name]['class_args_validator']\n                save_package_meta_data(package_meta)\n                print_green('{} installed!'.format(args.name))\n                installed = True\n        else:\n            package_meta = get_nni_meta(args.source)\n            if package_meta:\n                if call_pip_install(args.source) == 0:\n                    save_package_meta_data(package_meta)\n                    print_green('{} installed!'.format(package_meta['name']))\n                    installed = True\n    except Exception as e:\n        print_error(e)\n    if not installed:\n        print_error('installation failed!')",
  "def package_uninstall(args):\n    '''uninstall packages'''\n    name = args.name[0]\n    if name in get_not_installable_builtin_names():\n        print_error('{} can not be uninstalled!'.format(name))\n        exit(1)\n    meta = get_installed_package_meta(None, name)\n    if meta is None:\n        print_error('package {} not found!'.format(name))\n        return\n    if 'installed_package' in meta:\n        call_pip_uninstall(meta['installed_package'])\n    if remove_package_meta_data(name):\n        print_green('{} uninstalled sucessfully!'.format(name))\n    else:\n        print_error('Failed to uninstall {}!'.format(name))",
  "def package_show(args):\n    '''show specified packages'''\n    builtin_name = args.name[0]\n    meta = get_builtin_algo_meta(builtin_name=builtin_name)\n    if meta:\n        print(json.dumps(meta, indent=4))\n    else:\n        print_error('package {} not found'.format(builtin_name))",
  "def print_package_list(meta):\n    print('+-----------------+------------+-----------+--------=-------------+------------------------------------------+')\n    print('|      Name       |    Type    | Installed |      Class Name      |               Module Name                |')\n    print('+-----------------+------------+-----------+----------------------+------------------------------------------+')\n    MAX_MODULE_NAME = 38\n    for t in ['tuners', 'assessors', 'advisors']:\n        for p in meta[t]:\n            module_name = '.'.join(p['class_name'].split('.')[:-1])\n            if len(module_name) > MAX_MODULE_NAME:\n                module_name = module_name[:MAX_MODULE_NAME-3] + '...'\n            class_name = p['class_name'].split('.')[-1]\n            print('| {:15s} | {:10s} | {:9s} | {:20s} | {:40s} |'.format(p['name'], t, p['installed'], class_name, module_name[:38]))\n    print('+-----------------+------------+-----------+----------------------+------------------------------------------+')",
  "def package_list(args):\n    '''list all packages'''\n    if args.all:\n        meta = get_builtin_algo_meta()\n    else:\n        meta = read_installed_package_meta()\n\n    installed_names = defaultdict(list)\n    for t in ['tuners', 'assessors', 'advisors']:\n        for p in meta[t]:\n            p['installed'] = 'Yes'\n            installed_names[t].append(p['name'])\n    for k, v in INSTALLABLE_PACKAGE_META.items():\n        t = v['type']+'s'\n        if k not in installed_names[t]:\n            meta[t].append({\n                'name': k,\n                'class_name': v['class_name'],\n                'class_args_validator': v['class_args_validator'],\n                'installed': 'No'\n            })\n\n    print_package_list(meta)",
  "def save_package_meta_data(meta_data):\n    assert meta_data['type'] in PACKAGE_TYPES\n    assert 'name' in meta_data\n    assert 'class_name' in meta_data\n\n    config = read_installed_package_meta()\n\n    if meta_data['name'] in [x['name'] for x in config[meta_data['type']+'s']]:\n        raise ValueError('name %s already installed' % meta_data['name'])\n\n    package_meta = {k: meta_data[k] for k in ['name', 'class_name', 'class_args_validator'] if k in meta_data}\n    if 'package_name' in meta_data:\n        package_meta['installed_package'] = meta_data['package_name']\n    config[meta_data['type']+'s'].append(package_meta)\n    write_package_meta(config)",
  "def remove_package_meta_data(name):\n    config = read_installed_package_meta()\n\n    updated = False\n    for t in ALGO_TYPES:\n        for meta in config[t]:\n            if meta['name'] == name:\n                config[t].remove(meta)\n                updated = True\n    if updated:\n        write_package_meta(config)\n        return True\n    return False",
  "def get_nni_meta(source):\n    if not os.path.exists(source):\n        print_error('{} does not exist'.format(source))\n        return None\n\n    if os.path.isdir(source):\n        if not os.path.exists(os.path.join(source, 'setup.py')):\n            print_error('setup.py not found')\n            return None\n        pkg = pkginfo.Develop(source)\n    else:\n        if not source.endswith('.whl'):\n            print_error('File name {} must ends with \\'.whl\\''.format(source))\n            return False\n        pkg = pkginfo.Wheel(source)\n\n    classifiers = pkg.classifiers\n    meta = parse_classifiers(classifiers)\n    meta['package_name'] = pkg.name\n    return meta",
  "def parse_classifiers(classifiers):\n    parts = []\n    for c in classifiers:\n        if c.startswith('NNI Package'):\n            parts = [x.strip() for x in c.split('::')]\n            break\n    if len(parts) < 4 or not all(parts):\n        raise ValueError('Can not find correct NNI meta data in package classifiers.')\n    meta = {\n        'type': parts[1],\n        'name': parts[2],\n        'class_name': parts[3]\n    }\n    if len(parts) >= 5:\n        meta['class_args_validator'] = parts[4]\n\n    return meta",
  "def get_yml_content(file_path):\n    '''Load yaml file content'''\n    try:\n        with open(file_path, 'r') as file:\n            return yaml.load(file, Loader=yaml.Loader)\n    except yaml.scanner.ScannerError as err:\n        print_error('yaml file format error!')\n        print_error(err)\n        exit(1)\n    except Exception as exception:\n        print_error(exception)\n        exit(1)",
  "def get_json_content(file_path):\n    '''Load json file content'''\n    try:\n        with open(file_path, 'r') as file:\n            return json.load(file)\n    except TypeError as err:\n        print_error('json file format error!')\n        print_error(err)\n        return None",
  "def print_error(*content):\n    '''Print error information to screen'''\n    print(Fore.RED + ERROR_INFO + ' '.join([str(c) for c in content]) + Fore.RESET)",
  "def print_green(*content):\n    '''Print information to screen in green'''\n    print(Fore.GREEN + ' '.join([str(c) for c in content]) + Fore.RESET)",
  "def print_normal(*content):\n    '''Print error information to screen'''\n    print(NORMAL_INFO, *content)",
  "def print_warning(*content):\n    '''Print warning information to screen'''\n    print(Fore.YELLOW + WARNING_INFO + ' '.join([str(c) for c in content]) + Fore.RESET)",
  "def detect_process(pid):\n    '''Detect if a process is alive'''\n    try:\n        process = psutil.Process(pid)\n        return process.is_running()\n    except:\n        return False",
  "def detect_port(port):\n    '''Detect if the port is used'''\n    socket_test = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        socket_test.connect(('127.0.0.1', int(port)))\n        socket_test.close()\n        return True\n    except:\n        return False",
  "def get_user():\n    if sys.platform == 'win32':\n        return os.environ['USERNAME']\n    else:\n        return os.environ['USER']",
  "def check_tensorboard_version():\n    try:\n        import tensorboard\n        return tensorboard.__version__\n    except:\n        print_error('import tensorboard error!')\n        exit(1)",
  "def generate_temp_dir():\n    '''generate a temp folder'''\n    def generate_folder_name():\n        return os.path.join(tempfile.gettempdir(), 'nni', ''.join(random.sample(string.ascii_letters + string.digits, 8)))\n    temp_dir = generate_folder_name()\n    while os.path.exists(temp_dir):\n        temp_dir = generate_folder_name()\n    os.makedirs(temp_dir)\n    return temp_dir",
  "def generate_folder_name():\n        return os.path.join(tempfile.gettempdir(), 'nni', ''.join(random.sample(string.ascii_letters + string.digits, 8)))"
]