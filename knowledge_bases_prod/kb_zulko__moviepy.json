[
  "def _python_cmd(*args):\n    args = (sys.executable,) + args\n    return subprocess.call(args) == 0",
  "def _install(tarball, install_args=()):\n    # extracting the tarball\n    tmpdir = tempfile.mkdtemp()\n    log.warn('Extracting in %s', tmpdir)\n    old_wd = os.getcwd()\n    try:\n        os.chdir(tmpdir)\n        tar = tarfile.open(tarball)\n        _extractall(tar)\n        tar.close()\n\n        # going in the directory\n        subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])\n        os.chdir(subdir)\n        log.warn('Now working in %s', subdir)\n\n        # installing\n        log.warn('Installing Setuptools')\n        if not _python_cmd('setup.py', 'install', *install_args):\n            log.warn('Something went wrong during the installation.')\n            log.warn('See the error message above.')\n            # exitcode will be 2\n            return 2\n    finally:\n        os.chdir(old_wd)\n        shutil.rmtree(tmpdir)",
  "def _build_egg(egg, tarball, to_dir):\n    # extracting the tarball\n    tmpdir = tempfile.mkdtemp()\n    log.warn('Extracting in %s', tmpdir)\n    old_wd = os.getcwd()\n    try:\n        os.chdir(tmpdir)\n        tar = tarfile.open(tarball)\n        _extractall(tar)\n        tar.close()\n\n        # going in the directory\n        subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])\n        os.chdir(subdir)\n        log.warn('Now working in %s', subdir)\n\n        # building an egg\n        log.warn('Building a Setuptools egg in %s', to_dir)\n        _python_cmd('setup.py', '-q', 'bdist_egg', '--dist-dir', to_dir)\n\n    finally:\n        os.chdir(old_wd)\n        shutil.rmtree(tmpdir)\n    # returning the result\n    log.warn(egg)\n    if not os.path.exists(egg):\n        raise IOError('Could not build the egg.')",
  "def _do_download(version, download_base, to_dir, download_delay):\n    egg = os.path.join(to_dir, 'setuptools-%s-py%d.%d.egg'\n                       % (version, sys.version_info[0], sys.version_info[1]))\n    if not os.path.exists(egg):\n        tarball = download_setuptools(version, download_base,\n                                      to_dir, download_delay)\n        _build_egg(egg, tarball, to_dir)\n    sys.path.insert(0, egg)\n    import setuptools\n    setuptools.bootstrap_install_from = egg",
  "def use_setuptools(version=DEFAULT_VERSION, download_base=DEFAULT_URL,\n                   to_dir=os.curdir, download_delay=15):\n    # making sure we use the absolute path\n    to_dir = os.path.abspath(to_dir)\n    was_imported = 'pkg_resources' in sys.modules or \\\n        'setuptools' in sys.modules\n    try:\n        import pkg_resources\n    except ImportError:\n        return _do_download(version, download_base, to_dir, download_delay)\n    try:\n        pkg_resources.require(\"setuptools>=\" + version)\n        return\n    except pkg_resources.VersionConflict:\n        e = sys.exc_info()[1]\n        if was_imported:\n            sys.stderr.write(\n            \"The required version of setuptools (>=%s) is not available,\\n\"\n            \"and can't be installed while this script is running. Please\\n\"\n            \"install a more recent version first, using\\n\"\n            \"'easy_install -U setuptools'.\"\n            \"\\n\\n(Currently using %r)\\n\" % (version, e.args[0]))\n            sys.exit(2)\n        else:\n            del pkg_resources, sys.modules['pkg_resources']    # reload ok\n            return _do_download(version, download_base, to_dir,\n                                download_delay)\n    except pkg_resources.DistributionNotFound:\n        return _do_download(version, download_base, to_dir,\n                            download_delay)",
  "def download_setuptools(version=DEFAULT_VERSION, download_base=DEFAULT_URL,\n                        to_dir=os.curdir, delay=15):\n    \"\"\"Download setuptools from a specified location and return its filename\n\n    `version` should be a valid setuptools version number that is available\n    as an egg for download under the `download_base` URL (which should end\n    with a '/'). `to_dir` is the directory where the egg will be downloaded.\n    `delay` is the number of seconds to pause before an actual download\n    attempt.\n    \"\"\"\n    # making sure we use the absolute path\n    to_dir = os.path.abspath(to_dir)\n    try:\n        from urllib.request import urlopen\n    except ImportError:\n        from urllib2 import urlopen\n    tgz_name = \"setuptools-%s.tar.gz\" % version\n    url = download_base + tgz_name\n    saveto = os.path.join(to_dir, tgz_name)\n    src = dst = None\n    if not os.path.exists(saveto):  # Avoid repeated downloads\n        try:\n            log.warn(\"Downloading %s\", url)\n            src = urlopen(url)\n            # Read/write all in one block, so we don't create a corrupt file\n            # if the download is interrupted.\n            data = src.read()\n            dst = open(saveto, \"wb\")\n            dst.write(data)\n        finally:\n            if src:\n                src.close()\n            if dst:\n                dst.close()\n    return os.path.realpath(saveto)",
  "def _extractall(self, path=\".\", members=None):\n    \"\"\"Extract all members from the archive to the current working\n       directory and set owner, modification time and permissions on\n       directories afterwards. `path' specifies a different directory\n       to extract to. `members' is optional and must be a subset of the\n       list returned by getmembers().\n    \"\"\"\n    import copy\n    import operator\n    from tarfile import ExtractError\n    directories = []\n\n    if members is None:\n        members = self\n\n    for tarinfo in members:\n        if tarinfo.isdir():\n            # Extract directories with a safe mode.\n            directories.append(tarinfo)\n            tarinfo = copy.copy(tarinfo)\n            tarinfo.mode = 448  # decimal for oct 0700\n        self.extract(tarinfo, path)\n\n    # Reverse sort directories.\n    if sys.version_info < (2, 4):\n        def sorter(dir1, dir2):\n            return cmp(dir1.name, dir2.name)\n        directories.sort(sorter)\n        directories.reverse()\n    else:\n        directories.sort(key=operator.attrgetter('name'), reverse=True)\n\n    # Set correct owner, mtime and filemode on directories.\n    for tarinfo in directories:\n        dirpath = os.path.join(path, tarinfo.name)\n        try:\n            self.chown(tarinfo, dirpath)\n            self.utime(tarinfo, dirpath)\n            self.chmod(tarinfo, dirpath)\n        except ExtractError:\n            e = sys.exc_info()[1]\n            if self.errorlevel > 1:\n                raise\n            else:\n                self._dbg(1, \"tarfile: %s\" % e)",
  "def _build_install_args(options):\n    \"\"\"\n    Build the arguments to 'python setup.py install' on the setuptools package\n    \"\"\"\n    install_args = []\n    if options.user_install:\n        if sys.version_info < (2, 6):\n            log.warn(\"--user requires Python 2.6 or later\")\n            raise SystemExit(1)\n        install_args.append('--user')\n    return install_args",
  "def _parse_args():\n    \"\"\"\n    Parse the command line for options\n    \"\"\"\n    parser = optparse.OptionParser()\n    parser.add_option(\n        '--user', dest='user_install', action='store_true', default=False,\n        help='install in user site package (requires Python 2.6 or later)')\n    parser.add_option(\n        '--download-base', dest='download_base', metavar=\"URL\",\n        default=DEFAULT_URL,\n        help='alternative URL from where to download the setuptools package')\n    options, args = parser.parse_args()\n    # positional arguments are ignored\n    return options",
  "def main(version=DEFAULT_VERSION):\n    \"\"\"Install or upgrade setuptools and EasyInstall\"\"\"\n    options = _parse_args()\n    tarball = download_setuptools(download_base=options.download_base)\n    return _install(tarball, _build_install_args(options))",
  "def sorter(dir1, dir2):\n            return cmp(dir1.name, dir2.name)",
  "def preview(self, *args, **kwargs):\n        \"\"\"NOT AVAILABLE : clip.preview requires Pygame installed.\"\"\"\n        raise ImportError(\"clip.preview requires Pygame installed\")",
  "def show(self, *args, **kwargs):\n        \"\"\"NOT AVAILABLE : clip.show requires Pygame installed.\"\"\"\n        raise ImportError(\"clip.show requires Pygame installed\")",
  "def preview(self, *args, **kwargs):\n        \"\"\" NOT AVAILABLE : clip.preview requires Pygame installed.\"\"\"\n        raise ImportError(\"clip.preview requires Pygame installed\")",
  "def outplace(f, clip, *a, **k):\n    \"\"\" Applies f(clip.copy(), *a, **k) and returns clip.copy()\"\"\"\n    newclip = clip.copy()\n    f(newclip, *a, **k)\n    return newclip",
  "def convert_masks_to_RGB(f, clip, *a, **k):\n    \"\"\" If the clip is a mask, convert it to RGB before running the function \"\"\"\n    if clip.ismask:\n        clip = clip.to_RGB()\n    return f(clip, *a, **k)",
  "def apply_to_mask(f, clip, *a, **k):\n    \"\"\" This decorator will apply the same function f to the mask of\n        the clip created with f \"\"\"\n        \n    newclip = f(clip, *a, **k)\n    if hasattr(newclip, 'mask') and (newclip.mask is not None):\n        newclip.mask = f(newclip.mask, *a, **k)\n    return newclip",
  "def apply_to_audio(f, clip, *a, **k):\n    \"\"\" This decorator will apply the function f to the audio of\n        the clip created with f \"\"\"\n        \n    newclip = f(clip, *a, **k)\n    if hasattr(newclip, 'audio') and (newclip.audio is not None):\n        newclip.audio = f(newclip.audio, *a, **k)\n    return newclip",
  "def requires_duration(f, clip, *a, **k):\n    \"\"\" Raise an error if the clip has no duration.\"\"\"\n    \n    if clip.duration is None:\n        raise ValueError(\"Attribute 'duration' not set\")\n    else:\n        return f(clip, *a, **k)",
  "def audio_video_fx(f, clip, *a, **k):\n    \"\"\" Use an audio function on a video/audio clip\n    \n    This decorator tells that the function f (audioclip -> audioclip)\n    can be also used on a video clip, at which case it returns a\n    videoclip with unmodified video and modified audio.\n    \"\"\"\n    \n    if hasattr(clip, \"audio\"):\n        newclip = clip.copy()\n        if clip.audio is not None:\n            newclip.audio =  f(clip.audio, *a, **k)\n        return newclip\n    else:\n        return f(clip, *a, **k)",
  "def preprocess_args(fun,varnames):\n    \"\"\" Applies fun to variables in varnames before launching the function \"\"\"\n    \n    def wrapper(f, *a, **kw):\n        if hasattr(f, \"func_code\"):\n            func_code = f.func_code # Python 2\n        else:\n            func_code = f.__code__ # Python 3\n            \n        names = func_code.co_varnames\n        new_a = [fun(arg) if (name in varnames) else arg\n                 for (arg, name) in zip(a, names)]\n        new_kw = {k: fun(v) if k in varnames else v\n                 for (k,v) in kw.items()}\n        return f(*new_a, **new_kw)\n    return decorator.decorator(wrapper)",
  "def convert_to_seconds(varnames):\n    \"Converts the specified variables to seconds\"\n    return preprocess_args(cvsecs, varnames)",
  "def add_mask_if_none(f, clip, *a, **k):\n    \"\"\" Add a mask to the clip if there is none. \"\"\"        \n    if clip.mask is None:\n        clip = clip.add_mask()\n    return f(clip, *a, **k)",
  "def use_clip_fps_by_default(f, clip, *a, **k):\n    \"\"\" Will use clip.fps if no fps=... is provided in **k \"\"\"\n    \n    def fun(fps):\n        if fps is not None:\n            return fps\n        else:\n            if hasattr(clip, 'fps') and clip.fps is not None:\n                return clip.fps\n            else:\n                raise AttributeError(\"No 'fps' (frames per second) attribute specified\"\n                \" for function %s and the clip has no 'fps' attribute. Either\"\n                \" provide e.g. fps=24 in the arguments of the function, or define\"\n                \" the clip's fps with `clip.fps=24`\"%f.__name__)\n\n\n    if hasattr(f, \"func_code\"):\n        func_code = f.func_code # Python 2\n    else:\n        func_code = f.__code__ # Python 3\n        \n    names = func_code.co_varnames[1:]\n    \n    new_a = [fun(arg) if (name=='fps') else arg\n             for (arg, name) in zip(a, names)]\n    new_kw = {k: fun(v) if k=='fps' else v\n             for (k,v) in k.items()}\n\n    return f(clip, *new_a, **new_kw)",
  "def wrapper(f, *a, **kw):\n        if hasattr(f, \"func_code\"):\n            func_code = f.func_code # Python 2\n        else:\n            func_code = f.__code__ # Python 3\n            \n        names = func_code.co_varnames\n        new_a = [fun(arg) if (name in varnames) else arg\n                 for (arg, name) in zip(a, names)]\n        new_kw = {k: fun(v) if k in varnames else v\n                 for (k,v) in kw.items()}\n        return f(*new_a, **new_kw)",
  "def fun(fps):\n        if fps is not None:\n            return fps\n        else:\n            if hasattr(clip, 'fps') and clip.fps is not None:\n                return clip.fps\n            else:\n                raise AttributeError(\"No 'fps' (frames per second) attribute specified\"\n                \" for function %s and the clip has no 'fps' attribute. Either\"\n                \" provide e.g. fps=24 in the arguments of the function, or define\"\n                \" the clip's fps with `clip.fps=24`\"%f.__name__)",
  "def try_cmd(cmd):\n        try:\n            popen_params = { \"stdout\": sp.PIPE,\n                             \"stderr\": sp.PIPE,\n                              \"stdin\": DEVNULL\n                            }\n\n            \n            # This was added so that no extra unwanted window opens on windows\n            # when the child process is created\n            if os.name == \"nt\":\n                popen_params[\"creationflags\"] = 0x08000000\n\n            proc = sp.Popen(cmd, **popen_params)\n            proc.communicate()\n        except Exception as err:\n            return False, err\n        else:\n            return True, None",
  "def get_setting(varname):\n    \"\"\" Returns the value of a configuration variable. \"\"\" \n    gl = globals()\n    if varname not in gl.keys():\n        raise ValueError(\"Unknown setting %s\"%varname)\n    # Here, possibly add some code to raise exceptions if some\n    # parameter isn't set set properly, explaining on how to set it.\n    return gl[varname]",
  "def change_settings(new_settings={}, file=None):\n    \"\"\" Changes the value of configuration variables.\"\"\"\n    gl = globals()\n    if file is not None:\n        execfile(file)\n        gl.update(locals())\n    gl.update(new_settings)",
  "class Clip:\n\n    \"\"\"\n        \n     Base class of all clips (VideoClips and AudioClips).\n      \n       \n     Attributes\n     -----------\n     \n     start:\n       When the clip is included in a composition, time of the\n       composition at which the clip starts playing (in seconds). \n     \n     end:\n       When the clip is included in a composition, time of the\n       composition at which the clip starts playing (in seconds).\n     \n     duration:\n       Duration of the clip (in seconds). Some clips are infinite, in\n       this case their duration will be ``None``.\n     \n     \"\"\"\n    \n    # prefix for all tmeporary video and audio files.\n    # You can overwrite it with \n    # >>> Clip._TEMP_FILES_PREFIX = \"temp_\"\n    \n    _TEMP_FILES_PREFIX = 'TEMP_MPY_'\n\n    def __init__(self):\n\n        self.start = 0\n        self.end = None\n        self.duration = None\n        \n        self.memoize = False\n        self.memoized_t = None\n        self.memoize_frame  = None\n\n\n\n    def copy(self):\n        \"\"\" Shallow copy of the clip. \n        \n        Returns a shwallow copy of the clip whose mask and audio will\n        be shallow copies of the clip's mask and audio if they exist.\n        \n        This method is intensively used to produce new clips every time\n        there is an outplace transformation of the clip (clip.resize,\n        clip.subclip, etc.)\n        \"\"\"\n        \n        newclip = copy(self)\n        if hasattr(self, 'audio'):\n            newclip.audio = copy(self.audio)\n        if hasattr(self, 'mask'):\n            newclip.mask = copy(self.mask)\n            \n        return newclip\n    \n    @convert_to_seconds(['t'])\n    def get_frame(self, t):\n        \"\"\"\n        Gets a numpy array representing the RGB picture of the clip at time t\n        or (mono or stereo) value for a sound clip\n        \"\"\"\n        # Coming soon: smart error handling for debugging at this point \n        if self.memoize:\n            if t == self.memoized_t:\n                return self.memoized_frame\n            else:\n                frame = self.make_frame(t)\n                self.memoized_t = t\n                self.memoized_frame = frame\n                return frame\n        else:\n            return self.make_frame(t)\n\n    def fl(self, fun, apply_to=[] , keep_duration=True):\n        \"\"\" General processing of a clip.\n\n        Returns a new Clip whose frames are a transformation\n        (through function ``fun``) of the frames of the current clip.\n        \n        Parameters\n        -----------\n        \n        fun\n          A function with signature (gf,t -> frame) where ``gf`` will\n          represent the current clip's ``get_frame`` method,\n          i.e. ``gf`` is a function (t->image). Parameter `t` is a time\n          in seconds, `frame` is a picture (=Numpy array) which will be\n          returned by the transformed clip (see examples below).\n           \n        apply_to\n          Can be either ``'mask'``, or ``'audio'``, or\n          ``['mask','audio']``.\n          Specifies if the filter ``fl`` should also be applied to the\n          audio or the mask of the clip, if any.\n        \n        keep_duration\n          Set to True if the transformation does not change the\n          ``duration`` of the clip.\n          \n        Examples\n        --------\n        \n        In the following ``newclip`` a 100 pixels-high clip whose video\n        content scrolls from the top to the bottom of the frames of\n        ``clip``.\n        \n        >>> fl = lambda gf,t : gf(t)[int(t):int(t)+50, :]\n        >>> newclip = clip.fl(fl, apply_to='mask')\n        \n        \"\"\"\n\n        #mf = copy(self.make_frame)\n        newclip = self.set_make_frame(lambda t: fun(self.get_frame, t))\n        \n        if not keep_duration:\n            newclip.duration = None\n            newclip.end = None\n            \n        if isinstance(apply_to, str):\n            apply_to = [apply_to]\n\n        for attr in apply_to:\n            if hasattr(newclip, attr):\n                a = getattr(newclip, attr)\n                if a is not None:\n                    new_a =  a.fl(fun, keep_duration=keep_duration)\n                    setattr(newclip, attr, new_a)\n                    \n        return newclip\n\n    \n    \n    def fl_time(self, t_func, apply_to=[], keep_duration=False):\n        \"\"\"\n        Returns a Clip instance playing the content of the current clip\n        but with a modified timeline, time ``t`` being replaced by another\n        time `t_func(t)`.\n        \n        Parameters\n        -----------\n        \n        t_func:\n          A function ``t-> new_t``\n        \n        apply_to:\n          Can be either 'mask', or 'audio', or ['mask','audio'].\n          Specifies if the filter ``fl`` should also be applied to the\n          audio or the mask of the clip, if any.\n        \n        keep_duration:\n          ``False`` (default) if the transformation modifies the\n          ``duration`` of the clip.\n          \n        Examples\n        --------\n        \n        >>> # plays the clip (and its mask and sound) twice faster\n        >>> newclip = clip.fl_time(lambda: 2*t, apply_to=['mask','audio'])\n        >>>\n        >>> # plays the clip starting at t=3, and backwards:\n        >>> newclip = clip.fl_time(lambda: 3-t)\n        \n        \"\"\"\n        \n        return self.fl(lambda gf, t: gf(t_func(t)), apply_to,\n                                    keep_duration=keep_duration)\n    \n    \n    \n    def fx(self, func, *args, **kwargs):\n        \"\"\"\n        \n        Returns the result of ``func(self, *args, **kwargs)``.\n        for instance\n        \n        >>> newclip = clip.fx(resize, 0.2, method='bilinear')\n        \n        is equivalent to\n        \n        >>> newclip = resize(clip, 0.2, method='bilinear')\n        \n        The motivation of fx is to keep the name of the effect near its\n        parameters, when the effects are chained:\n        \n        >>> from moviepy.video.fx import volumex, resize, mirrorx\n        >>> clip.fx( volumex, 0.5).fx( resize, 0.3).fx( mirrorx )\n        >>> # Is equivalent, but clearer than\n        >>> resize( volumex( mirrorx( clip ), 0.5), 0.3)\n        \n        \"\"\"\n        \n        return func(self, *args, **kwargs)\n            \n    \n    \n    @apply_to_mask\n    @apply_to_audio\n    @convert_to_seconds(['t'])\n    @outplace\n    def set_start(self, t, change_end=True):\n        \"\"\"\n        Returns a copy of the clip, with the ``start`` attribute set\n        to ``t``, which can be expressed in seconds (15.35), in (min, sec),\n        in (hour, min, sec), or as a string: '01:03:05.35'.\n\n        \n        If ``change_end=True`` and the clip has a ``duration`` attribute,\n        the ``end`` atrribute of the clip will be updated to\n        ``start+duration``.\n        \n        If ``change_end=False`` and the clip has a ``end`` attribute,\n        the ``duration`` attribute of the clip will be updated to \n        ``end-start``\n        \n        These changes are also applied to the ``audio`` and ``mask``\n        clips of the current clip, if they exist.\n        \"\"\"\n        \n        self.start = t\n        if (self.duration is not None) and change_end:\n            self.end = t + self.duration\n        elif (self.end is not None):\n            self.duration = self.end - self.start\n    \n    \n    \n    @apply_to_mask\n    @apply_to_audio\n    @convert_to_seconds(['t'])\n    @outplace\n    def set_end(self, t):\n        \"\"\"\n        Returns a copy of the clip, with the ``end`` attribute set to\n        ``t``, which can be expressed in seconds (15.35), in (min, sec),\n        in (hour, min, sec), or as a string: '01:03:05.35'.\n        Also sets the duration of the mask and audio, if any,\n        of the returned clip.\n        \"\"\"\n        self.end = t\n        if self.start is None:\n            if self.duration is not None:\n                self.start = max(0, t - newclip.duration)\n        else:\n            self.duration = self.end - self.start\n\n\n    \n    @apply_to_mask\n    @apply_to_audio\n    @convert_to_seconds(['t'])\n    @outplace\n    def set_duration(self, t, change_end=True):\n        \"\"\"\n        Returns a copy of the clip, with the  ``duration`` attribute\n        set to ``t``, which can be expressed in seconds (15.35), in (min, sec),\n        in (hour, min, sec), or as a string: '01:03:05.35'.\n        Also sets the duration of the mask and audio, if any, of the\n        returned clip.\n        If change_end is False, the start attribute of the clip will\n        be modified in function of the duration and the preset end\n        of the clip.\n        \"\"\"\n        self.duration = t\n        if change_end:\n            self.end = None if (t is None) else (self.start + t)\n        else:\n            if duration is None:\n                raise Exception(\"Cannot change clip start when new\"\n                                 \"duration is None\")\n            self.start = self.end - t\n\n\n    @outplace\n    def set_make_frame(self, make_frame):\n        \"\"\"\n        Sets a ``make_frame`` attribute for the clip. Useful for setting\n        arbitrary/complicated videoclips.\n        \"\"\"\n        self.make_frame = make_frame\n\n    @outplace\n    def set_fps(self, fps):\n        \"\"\" Returns a copy of the clip with a new default fps for functions like\n        write_videofile, iterframe, etc. \"\"\" \n        self.fps = fps\n\n\n    @outplace\n    def set_ismask(self, ismask):\n        \"\"\" Says wheter the clip is a mask or not (ismask is a boolean)\"\"\" \n        self.ismask = ismask\n\n    @outplace\n    def set_memoize(self, memoize):\n        \"\"\" Sets wheter the clip should keep the last frame read in memory \"\"\" \n        self.memoize = memoize    \n    \n    @convert_to_seconds(['t'])\n    def is_playing(self, t):\n        \"\"\"\n        \n        If t is a time, returns true if t is between the start and\n        the end of the clip. t can be expressed in seconds (15.35),\n        in (min, sec), in (hour, min, sec), or as a string: '01:03:05.35'.\n        If t is a numpy array, returns False if none of the t is in\n        theclip, else returns a vector [b_1, b_2, b_3...] where b_i\n        is true iff tti is in the clip. \n        \"\"\"\n        \n        if isinstance(t, np.ndarray):\n            # is the whole list of t outside the clip ?\n            tmin, tmax = t.min(), t.max()\n            \n            if (self.end is not None) and (tmin >= self.end) :\n                return False\n            \n            if tmax < self.start:\n                return False\n            \n            # If we arrive here, a part of t falls in the clip\n            result = 1 * (t >= self.start)\n            if (self.end is not None):\n                result *= (t <= self.end)\n            return result\n        \n        else:\n            \n            return( (t >= self.start) and\n                    ((self.end is None) or (t < self.end) ) )\n    \n\n\n    @convert_to_seconds(['t_start', 't_end'])\n    @apply_to_mask\n    @apply_to_audio\n    def subclip(self, t_start=0, t_end=None):\n        \"\"\"\n        Returns a clip playing the content of the current clip\n        between times ``t_start`` and ``t_end``, which can be expressed\n        in seconds (15.35), in (min, sec), in (hour, min, sec), or as a\n        string: '01:03:05.35'.\n        If ``t_end`` is not provided, it is assumed to be the duration\n        of the clip (potentially infinite).\n        If ``t_end`` is a negative value, it is reset to\n        ``clip.duration + t_end. ``. For instance: ::\n        \n            >>> # cut the last two seconds of the clip:\n            >>> newclip = clip.subclip(0,-2)\n        \n        If ``t_end`` is provided or if the clip has a duration attribute,\n        the duration of the returned clip is set automatically.\n        \n        The ``mask`` and ``audio`` of the resulting subclip will be\n        subclips of ``mask`` and ``audio`` the original clip, if\n        they exist.\n        \"\"\"\n\n        if (self.duration is not None) and (t_start>self.duration):\n        \n            raise ValueError(\"t_start (%.02f) \"%t_start +\n                             \"should be smaller than the clip's \"+\n                             \"duration (%.02f).\"%self.duration)\n\n        newclip = self.fl_time(lambda t: t + t_start, apply_to=[])\n\n        if (t_end is None) and (self.duration is not None):\n        \n            t_end = self.duration\n        \n        elif (t_end is not None) and (t_end<0):\n        \n            if self.duration is None:\n        \n                print (\"Error: subclip with negative times (here %s)\"%(str((t_start, t_end)))\n                       +\" can only be extracted from clips with a ``duration``\")\n        \n            else:\n        \n                t_end = self.duration + t_end\n        \n        if (t_end is not None):\n        \n            newclip.duration = t_end - t_start\n            newclip.end = newclip.start + newclip.duration\n            \n        return newclip\n\n    \n    @apply_to_mask\n    @apply_to_audio\n    @convert_to_seconds(['ta', 'tb'])\n    def cutout(self, ta, tb):\n        \"\"\"\n        Returns a clip playing the content of the current clip but\n        skips the extract between ``ta`` and ``tb``, which can be\n        expressed in seconds (15.35), in (min, sec), in (hour, min, sec),\n        or as a string: '01:03:05.35'.\n        If the original clip has a ``duration`` attribute set,\n        the duration of the returned clip  is automatically computed as\n        `` duration - (tb - ta)``.\n        \n        The resulting clip's ``audio`` and ``mask`` will also be cutout\n        if they exist.\n        \"\"\"\n        \n        fl = lambda t: t + (t >= ta)*(tb - ta)\n        newclip = self.fl_time(fl)\n        \n        if self.duration is not None:\n        \n            return newclip.set_duration(self.duration - (tb - ta))\n        \n        else:\n        \n            return newclip\n\n    @requires_duration\n    @use_clip_fps_by_default\n    def iter_frames(self, fps=None, with_times = False, progress_bar=False,\n                    dtype=None):\n        \"\"\" Iterates over all the frames of the clip.\n        \n        Returns each frame of the clip as a HxWxN np.array,\n        where N=1 for mask clips and N=3 for RGB clips.\n        \n        This function is not really meant for video editing.\n        It provides an easy way to do frame-by-frame treatment of\n        a video, for fields like science, computer vision...\n        \n        The ``fps`` (frames per second) parameter is optional if the\n        clip already has a ``fps`` attribute.\n\n        Use dtype=\"uint8\" when using the pictures to write video, images... \n        \n        Examples\n        ---------\n        \n        >>> # prints the maximum of red that is contained\n        >>> # on the first line of each frame of the clip.\n        >>> from moviepy.editor import VideoFileClip\n        >>> myclip = VideoFileClip('myvideo.mp4')\n        >>> print ( [frame[0,:,0].max()\n                     for frame in myclip.iter_frames()])\n        \"\"\"\n\n        def generator():\n        \n            for t in np.arange(0, self.duration, 1.0/fps):\n        \n                frame = self.get_frame(t)\n        \n                if (dtype is not None) and (frame.dtype != dtype):\n        \n                    frame = frame.astype(dtype)\n\n                if with_times:\n        \n                    yield t, frame\n        \n                else:\n        \n                    yield frame\n        \n        if progress_bar:\n        \n            nframes = int(self.duration*fps)+1\n            return tqdm(generator(), total=nframes)\n\n        return generator()",
  "def __init__(self):\n\n        self.start = 0\n        self.end = None\n        self.duration = None\n        \n        self.memoize = False\n        self.memoized_t = None\n        self.memoize_frame  = None",
  "def copy(self):\n        \"\"\" Shallow copy of the clip. \n        \n        Returns a shwallow copy of the clip whose mask and audio will\n        be shallow copies of the clip's mask and audio if they exist.\n        \n        This method is intensively used to produce new clips every time\n        there is an outplace transformation of the clip (clip.resize,\n        clip.subclip, etc.)\n        \"\"\"\n        \n        newclip = copy(self)\n        if hasattr(self, 'audio'):\n            newclip.audio = copy(self.audio)\n        if hasattr(self, 'mask'):\n            newclip.mask = copy(self.mask)\n            \n        return newclip",
  "def get_frame(self, t):\n        \"\"\"\n        Gets a numpy array representing the RGB picture of the clip at time t\n        or (mono or stereo) value for a sound clip\n        \"\"\"\n        # Coming soon: smart error handling for debugging at this point \n        if self.memoize:\n            if t == self.memoized_t:\n                return self.memoized_frame\n            else:\n                frame = self.make_frame(t)\n                self.memoized_t = t\n                self.memoized_frame = frame\n                return frame\n        else:\n            return self.make_frame(t)",
  "def fl(self, fun, apply_to=[] , keep_duration=True):\n        \"\"\" General processing of a clip.\n\n        Returns a new Clip whose frames are a transformation\n        (through function ``fun``) of the frames of the current clip.\n        \n        Parameters\n        -----------\n        \n        fun\n          A function with signature (gf,t -> frame) where ``gf`` will\n          represent the current clip's ``get_frame`` method,\n          i.e. ``gf`` is a function (t->image). Parameter `t` is a time\n          in seconds, `frame` is a picture (=Numpy array) which will be\n          returned by the transformed clip (see examples below).\n           \n        apply_to\n          Can be either ``'mask'``, or ``'audio'``, or\n          ``['mask','audio']``.\n          Specifies if the filter ``fl`` should also be applied to the\n          audio or the mask of the clip, if any.\n        \n        keep_duration\n          Set to True if the transformation does not change the\n          ``duration`` of the clip.\n          \n        Examples\n        --------\n        \n        In the following ``newclip`` a 100 pixels-high clip whose video\n        content scrolls from the top to the bottom of the frames of\n        ``clip``.\n        \n        >>> fl = lambda gf,t : gf(t)[int(t):int(t)+50, :]\n        >>> newclip = clip.fl(fl, apply_to='mask')\n        \n        \"\"\"\n\n        #mf = copy(self.make_frame)\n        newclip = self.set_make_frame(lambda t: fun(self.get_frame, t))\n        \n        if not keep_duration:\n            newclip.duration = None\n            newclip.end = None\n            \n        if isinstance(apply_to, str):\n            apply_to = [apply_to]\n\n        for attr in apply_to:\n            if hasattr(newclip, attr):\n                a = getattr(newclip, attr)\n                if a is not None:\n                    new_a =  a.fl(fun, keep_duration=keep_duration)\n                    setattr(newclip, attr, new_a)\n                    \n        return newclip",
  "def fl_time(self, t_func, apply_to=[], keep_duration=False):\n        \"\"\"\n        Returns a Clip instance playing the content of the current clip\n        but with a modified timeline, time ``t`` being replaced by another\n        time `t_func(t)`.\n        \n        Parameters\n        -----------\n        \n        t_func:\n          A function ``t-> new_t``\n        \n        apply_to:\n          Can be either 'mask', or 'audio', or ['mask','audio'].\n          Specifies if the filter ``fl`` should also be applied to the\n          audio or the mask of the clip, if any.\n        \n        keep_duration:\n          ``False`` (default) if the transformation modifies the\n          ``duration`` of the clip.\n          \n        Examples\n        --------\n        \n        >>> # plays the clip (and its mask and sound) twice faster\n        >>> newclip = clip.fl_time(lambda: 2*t, apply_to=['mask','audio'])\n        >>>\n        >>> # plays the clip starting at t=3, and backwards:\n        >>> newclip = clip.fl_time(lambda: 3-t)\n        \n        \"\"\"\n        \n        return self.fl(lambda gf, t: gf(t_func(t)), apply_to,\n                                    keep_duration=keep_duration)",
  "def fx(self, func, *args, **kwargs):\n        \"\"\"\n        \n        Returns the result of ``func(self, *args, **kwargs)``.\n        for instance\n        \n        >>> newclip = clip.fx(resize, 0.2, method='bilinear')\n        \n        is equivalent to\n        \n        >>> newclip = resize(clip, 0.2, method='bilinear')\n        \n        The motivation of fx is to keep the name of the effect near its\n        parameters, when the effects are chained:\n        \n        >>> from moviepy.video.fx import volumex, resize, mirrorx\n        >>> clip.fx( volumex, 0.5).fx( resize, 0.3).fx( mirrorx )\n        >>> # Is equivalent, but clearer than\n        >>> resize( volumex( mirrorx( clip ), 0.5), 0.3)\n        \n        \"\"\"\n        \n        return func(self, *args, **kwargs)",
  "def set_start(self, t, change_end=True):\n        \"\"\"\n        Returns a copy of the clip, with the ``start`` attribute set\n        to ``t``, which can be expressed in seconds (15.35), in (min, sec),\n        in (hour, min, sec), or as a string: '01:03:05.35'.\n\n        \n        If ``change_end=True`` and the clip has a ``duration`` attribute,\n        the ``end`` atrribute of the clip will be updated to\n        ``start+duration``.\n        \n        If ``change_end=False`` and the clip has a ``end`` attribute,\n        the ``duration`` attribute of the clip will be updated to \n        ``end-start``\n        \n        These changes are also applied to the ``audio`` and ``mask``\n        clips of the current clip, if they exist.\n        \"\"\"\n        \n        self.start = t\n        if (self.duration is not None) and change_end:\n            self.end = t + self.duration\n        elif (self.end is not None):\n            self.duration = self.end - self.start",
  "def set_end(self, t):\n        \"\"\"\n        Returns a copy of the clip, with the ``end`` attribute set to\n        ``t``, which can be expressed in seconds (15.35), in (min, sec),\n        in (hour, min, sec), or as a string: '01:03:05.35'.\n        Also sets the duration of the mask and audio, if any,\n        of the returned clip.\n        \"\"\"\n        self.end = t\n        if self.start is None:\n            if self.duration is not None:\n                self.start = max(0, t - newclip.duration)\n        else:\n            self.duration = self.end - self.start",
  "def set_duration(self, t, change_end=True):\n        \"\"\"\n        Returns a copy of the clip, with the  ``duration`` attribute\n        set to ``t``, which can be expressed in seconds (15.35), in (min, sec),\n        in (hour, min, sec), or as a string: '01:03:05.35'.\n        Also sets the duration of the mask and audio, if any, of the\n        returned clip.\n        If change_end is False, the start attribute of the clip will\n        be modified in function of the duration and the preset end\n        of the clip.\n        \"\"\"\n        self.duration = t\n        if change_end:\n            self.end = None if (t is None) else (self.start + t)\n        else:\n            if duration is None:\n                raise Exception(\"Cannot change clip start when new\"\n                                 \"duration is None\")\n            self.start = self.end - t",
  "def set_make_frame(self, make_frame):\n        \"\"\"\n        Sets a ``make_frame`` attribute for the clip. Useful for setting\n        arbitrary/complicated videoclips.\n        \"\"\"\n        self.make_frame = make_frame",
  "def set_fps(self, fps):\n        \"\"\" Returns a copy of the clip with a new default fps for functions like\n        write_videofile, iterframe, etc. \"\"\" \n        self.fps = fps",
  "def set_ismask(self, ismask):\n        \"\"\" Says wheter the clip is a mask or not (ismask is a boolean)\"\"\" \n        self.ismask = ismask",
  "def set_memoize(self, memoize):\n        \"\"\" Sets wheter the clip should keep the last frame read in memory \"\"\" \n        self.memoize = memoize",
  "def is_playing(self, t):\n        \"\"\"\n        \n        If t is a time, returns true if t is between the start and\n        the end of the clip. t can be expressed in seconds (15.35),\n        in (min, sec), in (hour, min, sec), or as a string: '01:03:05.35'.\n        If t is a numpy array, returns False if none of the t is in\n        theclip, else returns a vector [b_1, b_2, b_3...] where b_i\n        is true iff tti is in the clip. \n        \"\"\"\n        \n        if isinstance(t, np.ndarray):\n            # is the whole list of t outside the clip ?\n            tmin, tmax = t.min(), t.max()\n            \n            if (self.end is not None) and (tmin >= self.end) :\n                return False\n            \n            if tmax < self.start:\n                return False\n            \n            # If we arrive here, a part of t falls in the clip\n            result = 1 * (t >= self.start)\n            if (self.end is not None):\n                result *= (t <= self.end)\n            return result\n        \n        else:\n            \n            return( (t >= self.start) and\n                    ((self.end is None) or (t < self.end) ) )",
  "def subclip(self, t_start=0, t_end=None):\n        \"\"\"\n        Returns a clip playing the content of the current clip\n        between times ``t_start`` and ``t_end``, which can be expressed\n        in seconds (15.35), in (min, sec), in (hour, min, sec), or as a\n        string: '01:03:05.35'.\n        If ``t_end`` is not provided, it is assumed to be the duration\n        of the clip (potentially infinite).\n        If ``t_end`` is a negative value, it is reset to\n        ``clip.duration + t_end. ``. For instance: ::\n        \n            >>> # cut the last two seconds of the clip:\n            >>> newclip = clip.subclip(0,-2)\n        \n        If ``t_end`` is provided or if the clip has a duration attribute,\n        the duration of the returned clip is set automatically.\n        \n        The ``mask`` and ``audio`` of the resulting subclip will be\n        subclips of ``mask`` and ``audio`` the original clip, if\n        they exist.\n        \"\"\"\n\n        if (self.duration is not None) and (t_start>self.duration):\n        \n            raise ValueError(\"t_start (%.02f) \"%t_start +\n                             \"should be smaller than the clip's \"+\n                             \"duration (%.02f).\"%self.duration)\n\n        newclip = self.fl_time(lambda t: t + t_start, apply_to=[])\n\n        if (t_end is None) and (self.duration is not None):\n        \n            t_end = self.duration\n        \n        elif (t_end is not None) and (t_end<0):\n        \n            if self.duration is None:\n        \n                print (\"Error: subclip with negative times (here %s)\"%(str((t_start, t_end)))\n                       +\" can only be extracted from clips with a ``duration``\")\n        \n            else:\n        \n                t_end = self.duration + t_end\n        \n        if (t_end is not None):\n        \n            newclip.duration = t_end - t_start\n            newclip.end = newclip.start + newclip.duration\n            \n        return newclip",
  "def cutout(self, ta, tb):\n        \"\"\"\n        Returns a clip playing the content of the current clip but\n        skips the extract between ``ta`` and ``tb``, which can be\n        expressed in seconds (15.35), in (min, sec), in (hour, min, sec),\n        or as a string: '01:03:05.35'.\n        If the original clip has a ``duration`` attribute set,\n        the duration of the returned clip  is automatically computed as\n        `` duration - (tb - ta)``.\n        \n        The resulting clip's ``audio`` and ``mask`` will also be cutout\n        if they exist.\n        \"\"\"\n        \n        fl = lambda t: t + (t >= ta)*(tb - ta)\n        newclip = self.fl_time(fl)\n        \n        if self.duration is not None:\n        \n            return newclip.set_duration(self.duration - (tb - ta))\n        \n        else:\n        \n            return newclip",
  "def iter_frames(self, fps=None, with_times = False, progress_bar=False,\n                    dtype=None):\n        \"\"\" Iterates over all the frames of the clip.\n        \n        Returns each frame of the clip as a HxWxN np.array,\n        where N=1 for mask clips and N=3 for RGB clips.\n        \n        This function is not really meant for video editing.\n        It provides an easy way to do frame-by-frame treatment of\n        a video, for fields like science, computer vision...\n        \n        The ``fps`` (frames per second) parameter is optional if the\n        clip already has a ``fps`` attribute.\n\n        Use dtype=\"uint8\" when using the pictures to write video, images... \n        \n        Examples\n        ---------\n        \n        >>> # prints the maximum of red that is contained\n        >>> # on the first line of each frame of the clip.\n        >>> from moviepy.editor import VideoFileClip\n        >>> myclip = VideoFileClip('myvideo.mp4')\n        >>> print ( [frame[0,:,0].max()\n                     for frame in myclip.iter_frames()])\n        \"\"\"\n\n        def generator():\n        \n            for t in np.arange(0, self.duration, 1.0/fps):\n        \n                frame = self.get_frame(t)\n        \n                if (dtype is not None) and (frame.dtype != dtype):\n        \n                    frame = frame.astype(dtype)\n\n                if with_times:\n        \n                    yield t, frame\n        \n                else:\n        \n                    yield frame\n        \n        if progress_bar:\n        \n            nframes = int(self.duration*fps)+1\n            return tqdm(generator(), total=nframes)\n\n        return generator()",
  "def generator():\n        \n            for t in np.arange(0, self.duration, 1.0/fps):\n        \n                frame = self.get_frame(t)\n        \n                if (dtype is not None) and (frame.dtype != dtype):\n        \n                    frame = frame.astype(dtype)\n\n                if with_times:\n        \n                    yield t, frame\n        \n                else:\n        \n                    yield frame",
  "def sys_write_flush(s):\n    \"\"\" Writes and flushes without delay a text in the console \"\"\"\n    sys.stdout.write(s)\n    sys.stdout.flush()",
  "def verbose_print(verbose, s):\n    \"\"\" Only prints s (with sys_write_flush) if verbose is True.\"\"\"\n    if verbose:\n        sys_write_flush(s)",
  "def subprocess_call(cmd, verbose=True, errorprint=True):\n    \"\"\" Executes the given subprocess command.\"\"\"\n\n    verbose_print(verbose, \"\\n[MoviePy] Running:\\n>>> \"+ \" \".join(cmd))\n\n    popen_params = {\"stdout\": DEVNULL,\n                    \"stderr\": sp.PIPE,\n                    \"stdin\": DEVNULL}\n\n    if os.name == \"nt\":\n        popen_params[\"creationflags\"] = 0x08000000\n\n    proc = sp.Popen(cmd, **popen_params)\n\n    out, err = proc.communicate() # proc.wait()\n    proc.stderr.close()\n\n    if proc.returncode:\n        verbose_print(errorprint, \"\\n[MoviePy] This command returned an error !\")\n        raise IOError(err.decode('utf8'))\n    else:\n        verbose_print(verbose, \"\\n... command successful.\\n\")\n\n    del proc",
  "def is_string(obj):\n    \"\"\" Returns true if s is string or string-like object,\n    compatible with Python 2 and Python 3.\"\"\"\n    try:\n        return isinstance(obj, basestring)\n    except NameError:\n        return isinstance(obj, str)",
  "def cvsecs(time):\n    \"\"\" Will convert any time into seconds.\n    Here are the accepted formats:\n\n    >>> cvsecs(15.4) -> 15.4 # seconds\n    >>> cvsecs( (1,21.5) ) -> 81.5 # (min,sec)\n    >>> cvsecs( (1,1,2) ) -> 3662 # (hr, min, sec)\n    >>> cvsecs('01:01:33.5') -> 3693.5  #(hr,min,sec)\n    >>> cvsecs('01:01:33.045') -> 3693.045\n    >>> cvsecs('01:01:33,5') #coma works too\n    \"\"\"\n\n    if is_string(time):\n        if (',' not in time) and ('.' not in time):\n            time = time + '.0'\n        expr = r\"(\\d+):(\\d+):(\\d+)[,|.](\\d+)\"\n        finds = re.findall(expr, time)[0]\n        nums = list( map(float, finds) )\n        return ( 3600*int(finds[0])\n                + 60*int(finds[1])\n                + int(finds[2])\n                + nums[3]/(10**len(finds[3])))\n\n    elif isinstance(time, tuple):\n        if len(time)== 3:\n            hr, mn, sec = time\n        elif len(time)== 2:\n            hr, mn, sec = 0, time[0], time[1]\n        return 3600*hr + 60*mn + sec\n\n    else:\n        return time",
  "def deprecated_version_of(f, oldname, newname=None):\n    \"\"\" Indicates that a function is deprecated and has a new name.\n\n    `f` is the new function, `oldname` the name of the deprecated\n    function, `newname` the name of `f`, which can be automatically\n    found.\n\n    Returns\n    ========\n\n    f_deprecated\n      A function that does the same thing as f, but with a docstring\n      and a printed message on call which say that the function is\n      deprecated and that you should use f instead.\n\n    Examples\n    =========\n\n    >>> # The badly named method 'to_file' is replaced by 'write_file'\n    >>> class Clip:\n    >>>    def write_file(self, some args):\n    >>>        # blablabla\n    >>>\n    >>> Clip.to_file = deprecated_version_of(Clip.write_file, 'to_file')\n    \"\"\"\n\n    if newname is None: newname = f.__name__\n\n    warning= (\"The function ``%s`` is deprecated and is kept temporarily \"\n              \"for backwards compatibility.\\nPlease use the new name, \"\n              \"``%s``, instead.\")%(oldname, newname)\n\n    def fdepr(*a, **kw):\n        warnings.warn(\"MoviePy: \" + warning, PendingDeprecationWarning)\n        return f(*a, **kw)\n    fdepr.__doc__ = warning\n\n    return fdepr",
  "def find_extension(codec):\n    for ext,infos in extensions_dict.items():\n        if ('codec' in infos) and codec in infos['codec']:\n            return ext\n    raise ValueError",
  "def fdepr(*a, **kw):\n        warnings.warn(\"MoviePy: \" + warning, PendingDeprecationWarning)\n        return f(*a, **kw)",
  "class AudioClip(Clip):\n    \"\"\" Base class for audio clips.\n    \n    See ``SoundClip`` and ``CompositeSoundClip`` for usable classes.\n    \n    An AudioClip is a Clip with a ``make_frame``  attribute of\n    the form `` t -> [ f_t ]`` for mono sound and\n    ``t-> [ f1_t, f2_t ]`` for stereo sound (the arrays are Numpy arrays).\n    The `f_t` are floats between -1 and 1. These bounds can be\n    trespassed wihtout problems (the program will put the\n    sound back into the bounds at conversion time, without much impact). \n    \n    Parameters\n    -----------\n    \n    make_frame\n      A function `t-> frame at time t`. The frame does not mean much\n      for a sound, it is just a float. What 'makes' the sound are\n      the variations of that float in the time.\n        \n    nchannels\n      Number of channels (one or two for mono or stereo).\n    \n    Examples\n    ---------\n    \n    >>> # Plays the note A (a sine wave of frequency 404HZ)\n    >>> import numpy as np\n    >>> make_frame = lambda t : 2*[ np.sin(404 * 2 * np.pi * t) ]\n    >>> clip = AudioClip(make_frame, duration=5)\n    >>> clip.preview()\n                     \n    \"\"\"\n    \n    def __init__(self, make_frame = None, duration=None):\n        Clip.__init__(self)\n        if make_frame is not None:\n            self.make_frame = make_frame\n            frame0 = self.get_frame(0)\n            if hasattr(frame0, '__iter__'):\n                self.nchannels = len(list(frame0))\n            else:\n                self.nchannels = 1\n        if duration is not None:\n            self.duration = duration\n            self.end = duration\n    \n    @requires_duration\n    def iter_chunks(self, chunksize=None, chunk_duration=None, fps=None,\n                    quantize=False, nbytes=2, progress_bar=False):\n        \"\"\" Iterator that returns the whole sound array of the clip by chunks\n        \"\"\"\n        if fps is None:\n            fps=self.fps\n        if chunk_duration is not None:\n            chunksize = int(chunk_duration*fps)\n        \n        totalsize = int(fps*self.duration)\n\n        if (totalsize % chunksize == 0):\n            nchunks = totalsize // chunksize\n        else:\n            nchunks = totalsize // chunksize + 1\n\n        pospos = list(range(0, totalsize,  chunksize))+[totalsize]\n\n        def generator():\n            for i in range(nchunks):\n                tt = (1.0/fps)*np.arange(pospos[i],pospos[i+1])\n                yield self.to_soundarray(tt, nbytes= nbytes, quantize=quantize, fps=fps,\n                                         buffersize=chunksize)\n\n        if progress_bar:\n            return tqdm(generator(), total=nchunks)\n        else:\n            return generator()\n\n    @requires_duration\n    def to_soundarray(self,tt=None, fps=None, quantize=False, nbytes=2, buffersize=50000):\n        \"\"\"\n        Transforms the sound into an array that can be played by pygame\n        or written in a wav file. See ``AudioClip.preview``.\n        \n        Parameters\n        ------------\n        \n        fps\n          Frame rate of the sound for the conversion.\n          44100 for top quality.\n        \n        nbytes\n          Number of bytes to encode the sound: 1 for 8bit sound,\n          2 for 16bit, 4 for 32bit sound.\n          \n        \"\"\"\n        if fps is None:\n            fps = self.fps\n       \n        stacker = np.vstack if self.nchannels==2 else np.hstack \n        max_duration = 1.0 * buffersize / fps\n        if (tt is None):\n            if self.duration>max_duration:\n                return stacker(self.iter_chunks(fps=fps, quantize=quantize, nbytes=2,\n                                                 chunksize=buffersize))\n            else:\n                tt = np.arange(0, self.duration, 1.0/fps)\n        \"\"\"\n        elif len(tt)> 1.5*buffersize:\n            nchunks = int(len(tt)/buffersize+1)\n            tt_chunks = np.array_split(tt, nchunks)\n            return stacker([self.to_soundarray(tt=ttc, buffersize=buffersize, fps=fps,\n                                        quantize=quantize, nbytes=nbytes)\n                              for ttc in tt_chunks])\n        \"\"\"\n        #print tt.max() - tt.min(), tt.min(), tt.max()\n        \n        snd_array = self.get_frame(tt)\n\n        if quantize:\n            snd_array = np.maximum(-0.99, np.minimum(0.99,snd_array))\n            inttype = {1:'int8',2:'int16', 4:'int32'}[nbytes]\n            snd_array= (2**(8*nbytes-1)*snd_array).astype(inttype)\n        \n        return snd_array\n\n    def max_volume(self, stereo=False, chunksize=50000, progress_bar=False):\n        \n        stereo = stereo and (self.nchannels == 2)\n\n        maxi = np.array([0,0]) if stereo else 0\n        for chunk in self.iter_chunks(chunksize=chunksize, progress_bar=progress_bar):\n            maxi =  np.maximum(maxi,abs(chunk).max(axis=0)) if stereo else max(maxi,abs(chunk).max())\n        return maxi\n\n\n\n    \n    @requires_duration\n    def write_audiofile(self,filename, fps=44100, nbytes=2,\n                     buffersize=2000, codec=None,\n                     bitrate=None, ffmpeg_params=None,\n                     write_logfile=False, verbose=True):\n        \"\"\" Writes an audio file from the AudioClip.\n\n\n        Parameters\n        -----------\n\n        filename\n          Name of the output file\n\n        fps\n          Frames per second\n\n        nbyte\n          Sample width (set to 2 for 16-bit sound, 4 for 32-bit sound)\n\n        codec\n          Which audio codec should be used. If None provided, the codec is\n          determined based on the extension of the filename. Choose\n          'pcm_s16le' for 16-bit wav and 'pcm_s32le' for 32-bit wav.\n\n        bitrate\n          Audio bitrate, given as a string like '50k', '500k', '3000k'.\n          Will determine the size and quality of the output file.\n          Note that it mainly an indicative goal, the bitrate won't\n          necessarily be the this in the output file.\n\n        ffmpeg_params\n          Any additional parameters you would like to pass, as a list\n          of terms, like ['-option1', 'value1', '-option2', 'value2']\n\n        write_logfile\n          If true, produces a detailed logfile named filename + '.log'\n          when writing the file\n\n        verbose\n          If True, displays informations\n\n\n        \"\"\"\n\n        if codec is None:\n            name, ext = os.path.splitext(os.path.basename(filename))\n            try:\n                codec = extensions_dict[ext[1:]]['codec'][0]\n            except KeyError:\n                raise ValueError(\"MoviePy couldn't find the codec associated \"\n                       \"with the filename. Provide the 'codec' parameter in \"\n                       \"write_videofile.\")\n\n        return ffmpeg_audiowrite(self, filename, fps, nbytes, buffersize,\n                      codec=codec, bitrate=bitrate, write_logfile=write_logfile,\n                      verbose=verbose, ffmpeg_params=ffmpeg_params)",
  "class AudioArrayClip(AudioClip):\n    \"\"\"\n    \n    An audio clip made from a sound array.\n    \n    Parameters\n    -----------\n    \n    array\n      A Numpy array representing the sound, of size Nx1 for mono,\n      Nx2 for stereo.\n       \n    fps\n      Frames per second : speed at which the sound is supposed to be\n      played.\n    \n    \"\"\"\n    \n    def __init__(self, array, fps):\n        \n        Clip.__init__(self)\n        self.array = array\n        self.fps = fps\n        self.duration = 1.0 * len(array) / fps\n        \n        \n        def make_frame(t):\n            \"\"\" complicated, but must be able to handle the case where t\n            is a list of the form sin(t) \"\"\"\n            \n            if isinstance(t, np.ndarray):\n                array_inds = (self.fps*t).astype(int)\n                in_array = (array_inds>0) & (array_inds < len(self.array))\n                result = np.zeros((len(t),2))\n                result[in_array] = self.array[array_inds[in_array]]\n                return result\n            else:\n                i = int(self.fps * t)\n                if i < 0 or i >= len(self.array):\n                    return 0*self.array[0]\n                else:\n                    return self.array[i]\n\n        self.make_frame = make_frame\n        self.nchannels = len(list(self.get_frame(0)))",
  "class CompositeAudioClip(AudioClip):\n\n    \"\"\" Clip made by composing several AudioClips.\n    \n    An audio clip made by putting together several audio clips.\n    \n    Parameters\n    ------------\n    \n    clips\n      List of audio clips, which may start playing at different times or\n      together. If all have their ``duration`` attribute set, the\n      duration of the composite clip is computed automatically.\n    \n    \"\"\"\n\n    def __init__(self, clips):\n\n        Clip.__init__(self)\n        self.clips = clips\n        \n        ends = [c.end for c in self.clips]\n        self.nchannels = max([c.nchannels for c in self.clips])\n        if not any([(e is None) for e in ends]):\n            self.duration = max(ends)\n            self.end = max(ends)\n\n        def make_frame(t):\n            \n            played_parts = [c.is_playing(t) for c in self.clips]\n            \n            sounds= [c.get_frame(t - c.start)*np.array([part]).T\n                     for c,part in zip(self.clips, played_parts)\n                     if (part is not False) ]\n                     \n            if isinstance(t,np.ndarray):\n                zero = np.zeros((len(t),self.nchannels))\n                \n            else:\n                zero = np.zeros(self.nchannels)\n                \n            return zero + sum(sounds)\n\n        self.make_frame = make_frame",
  "def concatenate_audioclips(clips):\n    durations = [c.duration for c in clips]\n    tt = np.cumsum([0]+durations) # start times, and end time.\n    newclips= [c.set_start(t) for c,t in zip(clips, tt)]\n    return CompositeAudioClip(newclips).set_duration(tt[-1])",
  "def __init__(self, make_frame = None, duration=None):\n        Clip.__init__(self)\n        if make_frame is not None:\n            self.make_frame = make_frame\n            frame0 = self.get_frame(0)\n            if hasattr(frame0, '__iter__'):\n                self.nchannels = len(list(frame0))\n            else:\n                self.nchannels = 1\n        if duration is not None:\n            self.duration = duration\n            self.end = duration",
  "def iter_chunks(self, chunksize=None, chunk_duration=None, fps=None,\n                    quantize=False, nbytes=2, progress_bar=False):\n        \"\"\" Iterator that returns the whole sound array of the clip by chunks\n        \"\"\"\n        if fps is None:\n            fps=self.fps\n        if chunk_duration is not None:\n            chunksize = int(chunk_duration*fps)\n        \n        totalsize = int(fps*self.duration)\n\n        if (totalsize % chunksize == 0):\n            nchunks = totalsize // chunksize\n        else:\n            nchunks = totalsize // chunksize + 1\n\n        pospos = list(range(0, totalsize,  chunksize))+[totalsize]\n\n        def generator():\n            for i in range(nchunks):\n                tt = (1.0/fps)*np.arange(pospos[i],pospos[i+1])\n                yield self.to_soundarray(tt, nbytes= nbytes, quantize=quantize, fps=fps,\n                                         buffersize=chunksize)\n\n        if progress_bar:\n            return tqdm(generator(), total=nchunks)\n        else:\n            return generator()",
  "def to_soundarray(self,tt=None, fps=None, quantize=False, nbytes=2, buffersize=50000):\n        \"\"\"\n        Transforms the sound into an array that can be played by pygame\n        or written in a wav file. See ``AudioClip.preview``.\n        \n        Parameters\n        ------------\n        \n        fps\n          Frame rate of the sound for the conversion.\n          44100 for top quality.\n        \n        nbytes\n          Number of bytes to encode the sound: 1 for 8bit sound,\n          2 for 16bit, 4 for 32bit sound.\n          \n        \"\"\"\n        if fps is None:\n            fps = self.fps\n       \n        stacker = np.vstack if self.nchannels==2 else np.hstack \n        max_duration = 1.0 * buffersize / fps\n        if (tt is None):\n            if self.duration>max_duration:\n                return stacker(self.iter_chunks(fps=fps, quantize=quantize, nbytes=2,\n                                                 chunksize=buffersize))\n            else:\n                tt = np.arange(0, self.duration, 1.0/fps)\n        \"\"\"\n        elif len(tt)> 1.5*buffersize:\n            nchunks = int(len(tt)/buffersize+1)\n            tt_chunks = np.array_split(tt, nchunks)\n            return stacker([self.to_soundarray(tt=ttc, buffersize=buffersize, fps=fps,\n                                        quantize=quantize, nbytes=nbytes)\n                              for ttc in tt_chunks])\n        \"\"\"\n        #print tt.max() - tt.min(), tt.min(), tt.max()\n        \n        snd_array = self.get_frame(tt)\n\n        if quantize:\n            snd_array = np.maximum(-0.99, np.minimum(0.99,snd_array))\n            inttype = {1:'int8',2:'int16', 4:'int32'}[nbytes]\n            snd_array= (2**(8*nbytes-1)*snd_array).astype(inttype)\n        \n        return snd_array",
  "def max_volume(self, stereo=False, chunksize=50000, progress_bar=False):\n        \n        stereo = stereo and (self.nchannels == 2)\n\n        maxi = np.array([0,0]) if stereo else 0\n        for chunk in self.iter_chunks(chunksize=chunksize, progress_bar=progress_bar):\n            maxi =  np.maximum(maxi,abs(chunk).max(axis=0)) if stereo else max(maxi,abs(chunk).max())\n        return maxi",
  "def write_audiofile(self,filename, fps=44100, nbytes=2,\n                     buffersize=2000, codec=None,\n                     bitrate=None, ffmpeg_params=None,\n                     write_logfile=False, verbose=True):\n        \"\"\" Writes an audio file from the AudioClip.\n\n\n        Parameters\n        -----------\n\n        filename\n          Name of the output file\n\n        fps\n          Frames per second\n\n        nbyte\n          Sample width (set to 2 for 16-bit sound, 4 for 32-bit sound)\n\n        codec\n          Which audio codec should be used. If None provided, the codec is\n          determined based on the extension of the filename. Choose\n          'pcm_s16le' for 16-bit wav and 'pcm_s32le' for 32-bit wav.\n\n        bitrate\n          Audio bitrate, given as a string like '50k', '500k', '3000k'.\n          Will determine the size and quality of the output file.\n          Note that it mainly an indicative goal, the bitrate won't\n          necessarily be the this in the output file.\n\n        ffmpeg_params\n          Any additional parameters you would like to pass, as a list\n          of terms, like ['-option1', 'value1', '-option2', 'value2']\n\n        write_logfile\n          If true, produces a detailed logfile named filename + '.log'\n          when writing the file\n\n        verbose\n          If True, displays informations\n\n\n        \"\"\"\n\n        if codec is None:\n            name, ext = os.path.splitext(os.path.basename(filename))\n            try:\n                codec = extensions_dict[ext[1:]]['codec'][0]\n            except KeyError:\n                raise ValueError(\"MoviePy couldn't find the codec associated \"\n                       \"with the filename. Provide the 'codec' parameter in \"\n                       \"write_videofile.\")\n\n        return ffmpeg_audiowrite(self, filename, fps, nbytes, buffersize,\n                      codec=codec, bitrate=bitrate, write_logfile=write_logfile,\n                      verbose=verbose, ffmpeg_params=ffmpeg_params)",
  "def __init__(self, array, fps):\n        \n        Clip.__init__(self)\n        self.array = array\n        self.fps = fps\n        self.duration = 1.0 * len(array) / fps\n        \n        \n        def make_frame(t):\n            \"\"\" complicated, but must be able to handle the case where t\n            is a list of the form sin(t) \"\"\"\n            \n            if isinstance(t, np.ndarray):\n                array_inds = (self.fps*t).astype(int)\n                in_array = (array_inds>0) & (array_inds < len(self.array))\n                result = np.zeros((len(t),2))\n                result[in_array] = self.array[array_inds[in_array]]\n                return result\n            else:\n                i = int(self.fps * t)\n                if i < 0 or i >= len(self.array):\n                    return 0*self.array[0]\n                else:\n                    return self.array[i]\n\n        self.make_frame = make_frame\n        self.nchannels = len(list(self.get_frame(0)))",
  "def __init__(self, clips):\n\n        Clip.__init__(self)\n        self.clips = clips\n        \n        ends = [c.end for c in self.clips]\n        self.nchannels = max([c.nchannels for c in self.clips])\n        if not any([(e is None) for e in ends]):\n            self.duration = max(ends)\n            self.end = max(ends)\n\n        def make_frame(t):\n            \n            played_parts = [c.is_playing(t) for c in self.clips]\n            \n            sounds= [c.get_frame(t - c.start)*np.array([part]).T\n                     for c,part in zip(self.clips, played_parts)\n                     if (part is not False) ]\n                     \n            if isinstance(t,np.ndarray):\n                zero = np.zeros((len(t),self.nchannels))\n                \n            else:\n                zero = np.zeros(self.nchannels)\n                \n            return zero + sum(sounds)\n\n        self.make_frame = make_frame",
  "def generator():\n            for i in range(nchunks):\n                tt = (1.0/fps)*np.arange(pospos[i],pospos[i+1])\n                yield self.to_soundarray(tt, nbytes= nbytes, quantize=quantize, fps=fps,\n                                         buffersize=chunksize)",
  "def make_frame(t):\n            \"\"\" complicated, but must be able to handle the case where t\n            is a list of the form sin(t) \"\"\"\n            \n            if isinstance(t, np.ndarray):\n                array_inds = (self.fps*t).astype(int)\n                in_array = (array_inds>0) & (array_inds < len(self.array))\n                result = np.zeros((len(t),2))\n                result[in_array] = self.array[array_inds[in_array]]\n                return result\n            else:\n                i = int(self.fps * t)\n                if i < 0 or i >= len(self.array):\n                    return 0*self.array[0]\n                else:\n                    return self.array[i]",
  "def make_frame(t):\n            \n            played_parts = [c.is_playing(t) for c in self.clips]\n            \n            sounds= [c.get_frame(t - c.start)*np.array([part]).T\n                     for c,part in zip(self.clips, played_parts)\n                     if (part is not False) ]\n                     \n            if isinstance(t,np.ndarray):\n                zero = np.zeros((len(t),self.nchannels))\n                \n            else:\n                zero = np.zeros(self.nchannels)\n                \n            return zero + sum(sounds)",
  "def audio_fadeout(clip, duration):\n    \"\"\" Return a sound clip where the sound fades out progressively\n        over ``duration`` seconds at the end of the clip. \"\"\"\n    \n    def fading(gf,t):\n        gft = gf(t)\n        \n        if np.isscalar(t):\n            factor = min(1.0 * (clip.duration - t) / duration, 1)\n            factor = np.array([factor,factor])\n        else:\n            factor = np.minimum( 1.0 * (clip.duration - t) / duration, 1)\n            factor = np.vstack([factor,factor]).T\n        return factor * gft\n    \n    return clip.fl(fading, keep_duration = True)",
  "def fading(gf,t):\n        gft = gf(t)\n        \n        if np.isscalar(t):\n            factor = min(1.0 * (clip.duration - t) / duration, 1)\n            factor = np.array([factor,factor])\n        else:\n            factor = np.minimum( 1.0 * (clip.duration - t) / duration, 1)\n            factor = np.vstack([factor,factor]).T\n        return factor * gft",
  "def audio_left_right(audioclip, left=1, right=1, merge=False):\n    \"\"\"\n    NOT YET FINISHED \n    \n    For a stereo audioclip, this function enables to change the volume\n    of the left and right channel separately (with the factors `left`\n    and `right`)\n    Makes a stereo audio clip in which the volume of left and right\n    is controllable\n    \"\"\"\n    funleft = (lambda t: left) if np.isscalar(left) else left\n    funright = (lambda t: right) if np.isscalar(right) else right",
  "def audio_fadein(clip, duration):\n    \"\"\" Return an audio (or video) clip that is first mute, then the\n        sound arrives progressively over ``duration`` seconds. \"\"\"\n        \n    def fading(gf,t):\n        gft = gf(t)\n        \n        if np.isscalar(t):\n            factor = min(1.0 * t / duration, 1)\n            factor = np.array([factor,factor])\n        else:\n            factor = np.minimum(1.0 * t / duration, 1)\n            factor = np.vstack([factor,factor]).T\n        return factor * gft\n    return clip.fl(fading, keep_duration = True)",
  "def fading(gf,t):\n        gft = gf(t)\n        \n        if np.isscalar(t):\n            factor = min(1.0 * t / duration, 1)\n            factor = np.array([factor,factor])\n        else:\n            factor = np.minimum(1.0 * t / duration, 1)\n            factor = np.vstack([factor,factor]).T\n        return factor * gft",
  "def audio_loop(audioclip, nloops=None, duration=None):\n    \"\"\" Loops over an audio clip.\n\n    Returns an audio clip that plays the given clip either\n    `nloops` times, or during `duration` seconds.\n\n    Examples\n    ========\n    \n    >>> from moviepy.editor import *\n    >>> videoclip = VideoFileClip('myvideo.mp4')\n    >>> music = AudioFileClip('music.ogg')\n    >>> audio = afx.audio_loop( music, duration=videoclip.duration)\n    >>> videoclip.set_audio(audio)\n\n    \"\"\"\n\n    if duration is not None:\n\n        nloops = int( duration/ audioclip.duration)+1\n        return concatenate_audioclips(nloops*[audioclip]).set_duration(duration)\n    \n    else:\n\n        return concatenate_audioclips(nloops*[audioclip])",
  "def volumex(clip, factor):\n    \"\"\" Returns a clip with audio volume multiplied by the\n    value `factor`. Can be applied to both audio and video clips.\n\n    This effect is loaded as a clip method when you use moviepy.editor,\n    so you can just write ``clip.volumex(2)``\n    \n    Examples\n    ---------\n\n    >>> newclip = volumex(clip, 2.0) # doubles audio volume\n    >>> newclip = clip.fx( volumex, 0.5) # half audio, use with fx\n    >>> newclip = clip.volumex(2) # only if you used \"moviepy.editor\"\n    \"\"\" \n    return clip.fl(lambda gf, t: factor * gf(t),\n    \t           keep_duration = True)",
  "class AudioFileClip(AudioClip):\n\n    \"\"\"\n    An audio clip read from a sound file, or an array.\n    The whole file is not loaded in memory. Instead, only a portion is\n    read and stored in memory. this portion includes frames before\n    and after the last frames read, so that it is fast to read the sound\n    backward and forward.\n    \n    Parameters\n    ------------\n    \n    snd\n      Either a soundfile name (of any extension supported by ffmpeg)\n      or an array representing a sound. If the soundfile is not a .wav,\n      it will be converted to .wav first, using the ``fps`` and\n      ``bitrate`` arguments. \n    \n    buffersize:\n      Size to load in memory (in number of frames)\n    \n    temp_wav:\n      Name for the temporary wav file in case conversion is required.\n      If not provided, the default will be filename.wav with some prefix.\n      If the temp_wav already exists it will not be rewritten.\n        \n        \n    Attributes\n    ------------\n    \n    nbytes\n      Number of bits per frame of the original audio file.\n      \n    fps\n      Number of frames per second in the audio file\n      \n    buffersize\n      See Parameters.\n      \n    Examples\n    ----------\n    \n    >>> snd = SoundClip(\"song.wav\")\n    >>> snd = SoundClip(\"song.mp3\", fps = 44100, bitrate=3000)\n    >>> snd = SoundClip(mySoundArray,fps=44100) # from a numeric array\n    \n    \"\"\"\n\n    def __init__(self, filename, buffersize=200000, nbytes=2, fps=44100):\n        \n\n        AudioClip.__init__(self)\n            \n        self.filename = filename\n        reader = FFMPEG_AudioReader(filename,fps=fps,nbytes=nbytes,\n                                         buffersize=buffersize)\n        \n        self.reader = reader\n        self.fps = fps\n        self.duration = reader.duration\n        self.end = reader.duration\n        \n        \n        self.make_frame =  lambda t: reader.get_frame(t)\n        self.nchannels = reader.nchannels\n    \n    \n    def coreader(self):\n        \"\"\" Returns a copy of the AudioFileClip, i.e. a new entrance point\n            to the audio file. Use copy when you have different clips\n            watching the audio file at different times. \"\"\"\n        return AudioFileClip(self.filename,self.buffersize)",
  "def __init__(self, filename, buffersize=200000, nbytes=2, fps=44100):\n        \n\n        AudioClip.__init__(self)\n            \n        self.filename = filename\n        reader = FFMPEG_AudioReader(filename,fps=fps,nbytes=nbytes,\n                                         buffersize=buffersize)\n        \n        self.reader = reader\n        self.fps = fps\n        self.duration = reader.duration\n        self.end = reader.duration\n        \n        \n        self.make_frame =  lambda t: reader.get_frame(t)\n        self.nchannels = reader.nchannels",
  "def coreader(self):\n        \"\"\" Returns a copy of the AudioFileClip, i.e. a new entrance point\n            to the audio file. Use copy when you have different clips\n            watching the audio file at different times. \"\"\"\n        return AudioFileClip(self.filename,self.buffersize)",
  "def preview(clip, fps=22050,  buffersize=4000, nbytes= 2,\n                 audioFlag=None, videoFlag=None):\n    \"\"\"\n    Plays the sound clip with pygame.\n    \n    Parameters\n    -----------\n    \n    fps\n       Frame rate of the sound. 44100 gives top quality, but may cause\n       problems if your computer is not fast enough and your clip is\n       complicated. If the sound jumps during the preview, lower it\n       (11025 is still fine, 5000 is tolerable).\n        \n    buffersize\n      The sound is not generated all at once, but rather made by bunches\n      of frames (chunks). ``buffersize`` is the size of such a chunk.\n      Try varying it if you meet audio problems (but you shouldn't\n      have to).\n    \n    nbytes:\n      Number of bytes to encode the sound: 1 for 8bit sound, 2 for\n      16bit, 4 for 32bit sound. 2 bytes is fine.\n    \n    audioFlag, videoFlag:\n      Instances of class threading events that are used to synchronize\n      video and audio during ``VideoClip.preview()``.\n    \n    \"\"\"\n                 \n    pg.mixer.quit()\n    \n    pg.mixer.init(fps, -8 * nbytes, clip.nchannels, 1024)\n    totalsize = int(fps*clip.duration)\n    pospos = np.array(list(range(0, totalsize,  buffersize))+[totalsize])\n    tt = (1.0/fps)*np.arange(pospos[0],pospos[1])\n    sndarray = clip.to_soundarray(tt,nbytes=nbytes, quantize=True)\n    chunk = pg.sndarray.make_sound(sndarray)\n    \n    if (audioFlag is not None) and (videoFlag is not None):\n        audioFlag.set()\n        videoFlag.wait()\n        \n    channel = chunk.play()\n    for i in range(1,len(pospos)-1):\n        tt = (1.0/fps)*np.arange(pospos[i],pospos[i+1])\n        sndarray = clip.to_soundarray(tt,nbytes=nbytes, quantize=True)\n        chunk = pg.sndarray.make_sound(sndarray)\n        while channel.get_queue():\n            time.sleep(0.003)\n            if (videoFlag!= None):\n                if not videoFlag.is_set():\n                    channel.stop()\n                    del channel\n                    return\n        channel.queue(chunk)",
  "class FFMPEG_AudioReader:\n    \"\"\"\n    A class to read the audio in either video files or audio files\n    using ffmpeg. ffmpeg will read any audio and transform them into\n    raw data.\n\n    Parameters\n    ------------\n\n    filename\n      Name of any video or audio file, like ``video.mp4`` or\n      ``sound.wav`` etc.\n\n    buffersize\n      The size of the buffer to use. Should be bigger than the buffer\n      used by ``to_audiofile``\n\n    print_infos\n      Print the ffmpeg infos on the file being read (for debugging)\n\n    fps\n      Desired frames per second in the decoded signal that will be\n      received from ffmpeg\n\n    nbytes\n      Desired number of bytes (1,2,4) in the signal that will be\n      received from ffmpeg\n\n    \"\"\"\n\n    def __init__(self, filename, buffersize, print_infos=False,\n                 fps=44100, nbytes=2, nchannels=2):\n\n        self.filename = filename\n        self.nbytes = nbytes\n        self.fps = fps\n        self.f = 's%dle'%(8*nbytes)\n        self.acodec = 'pcm_s%dle'%(8*nbytes)\n        self.nchannels = nchannels\n        infos = ffmpeg_parse_infos(filename)\n        self.duration = infos['duration']\n        if 'video_duration' in infos:\n            self.duration = infos['video_duration']\n        else:\n            self.duration = infos['duration']\n        self.infos = infos\n        self.proc = None\n\n        self.nframes = int(self.fps * self.duration)\n        self.buffersize= min( self.nframes+1, buffersize )\n        self.buffer= None\n        self.buffer_startframe = 1\n        self.initialize()\n        self.buffer_around(1)\n\n\n\n    def initialize(self, starttime = 0):\n        \"\"\" Opens the file, creates the pipe. \"\"\"\n\n        self.close_proc() # if any\n\n        if starttime !=0 :\n            offset = min(1,starttime)\n            i_arg = [\"-ss\", \"%.05f\"%(starttime-offset),\n                    '-i', self.filename, '-vn',\n                    \"-ss\", \"%.05f\"%offset]\n        else:\n            i_arg = [ '-i', self.filename,  '-vn']\n\n\n        cmd = ([get_setting(\"FFMPEG_BINARY\")] + i_arg +\n               [ '-loglevel', 'error',\n                 '-f', self.f,\n                '-acodec', self.acodec,\n                '-ar', \"%d\"%self.fps,\n                '-ac', '%d'%self.nchannels, '-'])\n\n        popen_params = {\"bufsize\": self.buffersize,\n                        \"stdout\": sp.PIPE,\n                        \"stderr\": sp.PIPE,\n                        \"stdin\": DEVNULL}\n\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n\n        self.proc = sp.Popen( cmd, **popen_params)\n\n        self.pos = np.round(self.fps*starttime)\n\n\n\n    def skip_chunk(self,chunksize):\n        s = self.proc.stdout.read(self.nchannels*chunksize*self.nbytes)\n        self.proc.stdout.flush()\n        self.pos = self.pos+chunksize\n\n\n\n    def read_chunk(self,chunksize):\n        L = self.nchannels*chunksize*self.nbytes\n        s = self.proc.stdout.read(L)\n        dt = {1: 'int8',2:'int16',4:'int32'}[self.nbytes]\n        result = np.fromstring(s, dtype=dt)\n        result = (1.0*result / 2**(8*self.nbytes-1)).\\\n                                 reshape((len(result)/self.nchannels,\n                                          self.nchannels))\n        #self.proc.stdout.flush()\n        self.pos = self.pos+chunksize\n        return result\n\n\n\n    def seek(self,pos):\n        \"\"\"\n        Reads a frame at time t. Note for coders: getting an arbitrary\n        frame in the video with ffmpeg can be painfully slow if some\n        decoding has to be done. This function tries to avoid fectching\n        arbitrary frames whenever possible, by moving between adjacent\n        frames.\n        \"\"\"\n        if (pos < self.pos) or (pos> (self.pos+1000000)):\n            t = 1.0*pos/self.fps\n            self.initialize(t)\n        elif pos > self.pos:\n            #print pos\n            self.skip_chunk(pos-self.pos)\n        # last case standing: pos = current pos\n        self.pos = pos\n\n\n\n    def close_proc(self):\n        if hasattr(self, 'proc') and self.proc is not None:\n            self.proc.terminate()\n            for std in [ self.proc.stdout,\n                         self.proc.stderr]:\n                std.close()\n            del self.proc\n\n    def get_frame(self, tt):\n\n        buffersize = self.buffersize\n        if isinstance(tt,np.ndarray):\n            # lazy implementation, but should not cause problems in\n            # 99.99 %  of the cases\n\n\n            # elements of t that are actually in the range of the\n            # audio file.\n\n            in_time = (tt>=0) & (tt < self.duration)\n\n            # The np.round in the next line is super-important.\n            # Removing it results in artifacts in the noise.\n            frames = np.round((self.fps*tt)).astype(int)[in_time]\n            fr_min, fr_max = frames.min(), frames.max()\n\n            if not (0 <=\n                     (fr_min - self.buffer_startframe)\n                          < len(self.buffer)):\n                self.buffer_around(fr_min)\n            elif not (0 <=\n                        (fr_max - self.buffer_startframe)\n                             < len(self.buffer)):\n                self.buffer_around(fr_max)\n\n            try:\n                result = np.zeros((len(tt),self.nchannels))\n                indices = frames - self.buffer_startframe\n                result[in_time] = self.buffer[indices]\n                return result\n            except IndexError as error:\n                raise IOError(\"Error in file %s, \"%(self.filename)+\n                       \"At time t=%.02f-%.02f seconds, \"%(tt[0], tt[-1])+\n                       \"indices wanted: %d-%d, \"%(indices.min(), indices.max())+\n                       \"but len(buffer)=%d\\n\"%(len(self.buffer))+ str(error))\n\n        else:\n\n            ind = int(self.fps*tt)\n            if ind<0 or ind> self.nframes: # out of time: return 0\n                return np.zeros(self.nchannels)\n\n            if not (0 <= (ind - self.buffer_startframe) <len(self.buffer)):\n                # out of the buffer: recenter the buffer\n                self.buffer_around(ind)\n\n            # read the frame in the buffer\n            return self.buffer[ind - self.buffer_startframe]\n\n\n    def buffer_around(self,framenumber):\n        \"\"\"\n        Fills the buffer with frames, centered on ``framenumber``\n        if possible\n        \"\"\"\n\n        # start-frame for the buffer\n        new_bufferstart = max(0,  framenumber - self.buffersize // 2)\n\n\n        if (self.buffer is not None):\n            current_f_end  = self.buffer_startframe + self.buffersize\n            if (new_bufferstart <\n                        current_f_end  <\n                               new_bufferstart + self.buffersize):\n                # We already have one bit of what must be read\n                conserved = current_f_end - new_bufferstart + 1\n                chunksize = self.buffersize-conserved\n                array = self.read_chunk(chunksize)\n                self.buffer = np.vstack([self.buffer[-conserved:], array])\n            else:\n                self.seek(new_bufferstart)\n                self.buffer =  self.read_chunk(self.buffersize)\n        else:\n            self.seek(new_bufferstart)\n            self.buffer =  self.read_chunk(self.buffersize)\n\n        self.buffer_startframe = new_bufferstart\n\n\n    def __del__(self):\n        self.close_proc()",
  "def __init__(self, filename, buffersize, print_infos=False,\n                 fps=44100, nbytes=2, nchannels=2):\n\n        self.filename = filename\n        self.nbytes = nbytes\n        self.fps = fps\n        self.f = 's%dle'%(8*nbytes)\n        self.acodec = 'pcm_s%dle'%(8*nbytes)\n        self.nchannels = nchannels\n        infos = ffmpeg_parse_infos(filename)\n        self.duration = infos['duration']\n        if 'video_duration' in infos:\n            self.duration = infos['video_duration']\n        else:\n            self.duration = infos['duration']\n        self.infos = infos\n        self.proc = None\n\n        self.nframes = int(self.fps * self.duration)\n        self.buffersize= min( self.nframes+1, buffersize )\n        self.buffer= None\n        self.buffer_startframe = 1\n        self.initialize()\n        self.buffer_around(1)",
  "def initialize(self, starttime = 0):\n        \"\"\" Opens the file, creates the pipe. \"\"\"\n\n        self.close_proc() # if any\n\n        if starttime !=0 :\n            offset = min(1,starttime)\n            i_arg = [\"-ss\", \"%.05f\"%(starttime-offset),\n                    '-i', self.filename, '-vn',\n                    \"-ss\", \"%.05f\"%offset]\n        else:\n            i_arg = [ '-i', self.filename,  '-vn']\n\n\n        cmd = ([get_setting(\"FFMPEG_BINARY\")] + i_arg +\n               [ '-loglevel', 'error',\n                 '-f', self.f,\n                '-acodec', self.acodec,\n                '-ar', \"%d\"%self.fps,\n                '-ac', '%d'%self.nchannels, '-'])\n\n        popen_params = {\"bufsize\": self.buffersize,\n                        \"stdout\": sp.PIPE,\n                        \"stderr\": sp.PIPE,\n                        \"stdin\": DEVNULL}\n\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n\n        self.proc = sp.Popen( cmd, **popen_params)\n\n        self.pos = np.round(self.fps*starttime)",
  "def skip_chunk(self,chunksize):\n        s = self.proc.stdout.read(self.nchannels*chunksize*self.nbytes)\n        self.proc.stdout.flush()\n        self.pos = self.pos+chunksize",
  "def read_chunk(self,chunksize):\n        L = self.nchannels*chunksize*self.nbytes\n        s = self.proc.stdout.read(L)\n        dt = {1: 'int8',2:'int16',4:'int32'}[self.nbytes]\n        result = np.fromstring(s, dtype=dt)\n        result = (1.0*result / 2**(8*self.nbytes-1)).\\\n                                 reshape((len(result)/self.nchannels,\n                                          self.nchannels))\n        #self.proc.stdout.flush()\n        self.pos = self.pos+chunksize\n        return result",
  "def seek(self,pos):\n        \"\"\"\n        Reads a frame at time t. Note for coders: getting an arbitrary\n        frame in the video with ffmpeg can be painfully slow if some\n        decoding has to be done. This function tries to avoid fectching\n        arbitrary frames whenever possible, by moving between adjacent\n        frames.\n        \"\"\"\n        if (pos < self.pos) or (pos> (self.pos+1000000)):\n            t = 1.0*pos/self.fps\n            self.initialize(t)\n        elif pos > self.pos:\n            #print pos\n            self.skip_chunk(pos-self.pos)\n        # last case standing: pos = current pos\n        self.pos = pos",
  "def close_proc(self):\n        if hasattr(self, 'proc') and self.proc is not None:\n            self.proc.terminate()\n            for std in [ self.proc.stdout,\n                         self.proc.stderr]:\n                std.close()\n            del self.proc",
  "def get_frame(self, tt):\n\n        buffersize = self.buffersize\n        if isinstance(tt,np.ndarray):\n            # lazy implementation, but should not cause problems in\n            # 99.99 %  of the cases\n\n\n            # elements of t that are actually in the range of the\n            # audio file.\n\n            in_time = (tt>=0) & (tt < self.duration)\n\n            # The np.round in the next line is super-important.\n            # Removing it results in artifacts in the noise.\n            frames = np.round((self.fps*tt)).astype(int)[in_time]\n            fr_min, fr_max = frames.min(), frames.max()\n\n            if not (0 <=\n                     (fr_min - self.buffer_startframe)\n                          < len(self.buffer)):\n                self.buffer_around(fr_min)\n            elif not (0 <=\n                        (fr_max - self.buffer_startframe)\n                             < len(self.buffer)):\n                self.buffer_around(fr_max)\n\n            try:\n                result = np.zeros((len(tt),self.nchannels))\n                indices = frames - self.buffer_startframe\n                result[in_time] = self.buffer[indices]\n                return result\n            except IndexError as error:\n                raise IOError(\"Error in file %s, \"%(self.filename)+\n                       \"At time t=%.02f-%.02f seconds, \"%(tt[0], tt[-1])+\n                       \"indices wanted: %d-%d, \"%(indices.min(), indices.max())+\n                       \"but len(buffer)=%d\\n\"%(len(self.buffer))+ str(error))\n\n        else:\n\n            ind = int(self.fps*tt)\n            if ind<0 or ind> self.nframes: # out of time: return 0\n                return np.zeros(self.nchannels)\n\n            if not (0 <= (ind - self.buffer_startframe) <len(self.buffer)):\n                # out of the buffer: recenter the buffer\n                self.buffer_around(ind)\n\n            # read the frame in the buffer\n            return self.buffer[ind - self.buffer_startframe]",
  "def buffer_around(self,framenumber):\n        \"\"\"\n        Fills the buffer with frames, centered on ``framenumber``\n        if possible\n        \"\"\"\n\n        # start-frame for the buffer\n        new_bufferstart = max(0,  framenumber - self.buffersize // 2)\n\n\n        if (self.buffer is not None):\n            current_f_end  = self.buffer_startframe + self.buffersize\n            if (new_bufferstart <\n                        current_f_end  <\n                               new_bufferstart + self.buffersize):\n                # We already have one bit of what must be read\n                conserved = current_f_end - new_bufferstart + 1\n                chunksize = self.buffersize-conserved\n                array = self.read_chunk(chunksize)\n                self.buffer = np.vstack([self.buffer[-conserved:], array])\n            else:\n                self.seek(new_bufferstart)\n                self.buffer =  self.read_chunk(self.buffersize)\n        else:\n            self.seek(new_bufferstart)\n            self.buffer =  self.read_chunk(self.buffersize)\n\n        self.buffer_startframe = new_bufferstart",
  "def __del__(self):\n        self.close_proc()",
  "class FFMPEG_AudioWriter:\n    \"\"\"\n    A class to write an AudioClip into an audio file.\n\n    Parameters\n    ------------\n\n    filename\n      Name of any video or audio file, like ``video.mp4`` or ``sound.wav`` etc.\n\n    size\n      Size (width,height) in pixels of the output video.\n\n    fps_input\n      Frames per second of the input audio (given by the AUdioClip being\n      written down).\n\n    codec\n      Name of the ffmpeg codec to use for the output.\n\n    bitrate:\n      A string indicating the bitrate of the final video. Only\n      relevant for codecs which accept a bitrate.\n\n    \"\"\"\n\n\n\n    def __init__(self, filename, fps_input, nbytes=2,\n                 nchannels = 2, codec='libfdk_aac', bitrate=None,\n                 input_video=None, logfile=None, ffmpeg_params=None):\n\n        self.filename = filename\n        self.codec= codec\n\n        if logfile is None:\n          logfile = sp.PIPE\n\n        cmd = ([ get_setting(\"FFMPEG_BINARY\"), '-y',\n            \"-loglevel\", \"error\" if logfile==sp.PIPE else \"info\",\n            \"-f\", 's%dle'%(8*nbytes),\n            \"-acodec\",'pcm_s%dle'%(8*nbytes),\n            '-ar', \"%d\"%fps_input,\n            '-ac',\"%d\"%nchannels,\n            '-i', '-']\n            + (['-vn'] if input_video is None else\n                 [ \"-i\", input_video, '-vcodec', 'copy'])\n            + ['-acodec', codec]\n            + ['-ar', \"%d\"%fps_input]\n            + ['-strict', '-2']  # needed to support codec 'aac'\n            + (['-ab',bitrate] if (bitrate is not None) else [])\n            + (ffmpeg_params if ffmpeg_params else [])\n            + [ filename ])\n\n        popen_params = {\"stdout\": DEVNULL,\n                        \"stderr\": logfile,\n                        \"stdin\": sp.PIPE}\n\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n\n        self.proc = sp.Popen(cmd, **popen_params)\n\n\n    def write_frames(self,frames_array):\n        try:\n            self.proc.stdin.write(frames_array.tostring())\n        except IOError as err:\n            ffmpeg_error = self.proc.stderr.read()\n            error = (str(err)+ (\"\\n\\nMoviePy error: FFMPEG encountered \"\n                     \"the following error while writing file %s:\"%self.filename\n                     + \"\\n\\n\"+ffmpeg_error))\n\n            if \"Unknown encoder\" in ffmpeg_error:\n\n                error = (error+(\"\\n\\nThe audio export failed because \"\n                    \"FFMPEG didn't find the specified codec for audio \"\n                    \"encoding (%s). Please install this codec or \"\n                    \"change the codec when calling to_videofile or \"\n                    \"to_audiofile. For instance for mp3:\\n\"\n                    \"   >>> to_videofile('myvid.mp4', audio_codec='libmp3lame')\"\n                    )%(self.codec))\n\n            elif \"incorrect codec parameters ?\" in ffmpeg_error:\n\n                error = error+(\"\\n\\nThe audio export \"\n                  \"failed, possibly because the codec specified for \"\n                  \"the video (%s) is not compatible with the given \"\n                  \"extension (%s). Please specify a valid 'codec' \"\n                  \"argument in to_videofile. This would be 'libmp3lame' \"\n                  \"for mp3, 'libvorbis' for ogg...\")%(self.codec, self.ext)\n\n            elif  \"encoder setup failed\":\n\n                error = error+(\"\\n\\nThe audio export \"\n                  \"failed, possily because the bitrate you specified \"\n                  \"was two high or too low for the video codec.\")\n\n            else:\n\n                error = error+(\"\\n\\nIn case it helps, make sure you are \"\n                  \"using a recent version of FFMPEG (the versions in the \"\n                  \"Ubuntu/Debian repos are deprecated).\")\n            raise IOError(error)\n\n\n\n    def close(self):\n        self.proc.stdin.close()\n        if self.proc.stderr is not None:\n            self.proc.stderr.close()\n        self.proc.wait()\n        del self.proc",
  "def ffmpeg_audiowrite(clip, filename, fps, nbytes, buffersize,\n                      codec='libvorbis', bitrate=None,\n                      write_logfile = False, verbose=True,\n                      ffmpeg_params=None):\n    \"\"\"\n    A function that wraps the FFMPEG_AudioWriter to write an AudioClip\n    to a file.\n    \"\"\"\n\n    if write_logfile:\n        logfile = open(filename + \".log\", 'w+')\n    else:\n        logfile = None\n\n    verbose_print(verbose, \"[MoviePy] Writing audio in %s\\n\"%filename)\n\n    writer = FFMPEG_AudioWriter(filename, fps, nbytes, clip.nchannels,\n                                codec=codec, bitrate=bitrate,\n                                logfile=logfile,\n                                ffmpeg_params=ffmpeg_params)\n\n    \n    \n    for chunk in clip.iter_chunks(chunksize=buffersize,\n                                  progress_bar=True, quantize=True,\n                                  nbytes= nbytes, fps=fps):\n        writer.write_frames(chunk)\n\n    \"\"\"\n    totalsize = int(fps*clip.duration)\n\n    if (totalsize % buffersize == 0):\n        nchunks = totalsize // buffersize\n    else:\n        nchunks = totalsize // buffersize + 1\n\n    pospos = list(range(0, totalsize,  buffersize))+[totalsize]\n    for i in tqdm(range(nchunks)):\n        tt = (1.0/fps)*np.arange(pospos[i],pospos[i+1])\n        sndarray = clip.to_soundarray(tt, nbytes= nbytes)\n        writer.write_frames(sndarray)\n    \"\"\"\n\n    writer.close()\n\n    if write_logfile:\n        logfile.close()\n\n    verbose_print(verbose, \"[MoviePy] Done.\\n\")",
  "def __init__(self, filename, fps_input, nbytes=2,\n                 nchannels = 2, codec='libfdk_aac', bitrate=None,\n                 input_video=None, logfile=None, ffmpeg_params=None):\n\n        self.filename = filename\n        self.codec= codec\n\n        if logfile is None:\n          logfile = sp.PIPE\n\n        cmd = ([ get_setting(\"FFMPEG_BINARY\"), '-y',\n            \"-loglevel\", \"error\" if logfile==sp.PIPE else \"info\",\n            \"-f\", 's%dle'%(8*nbytes),\n            \"-acodec\",'pcm_s%dle'%(8*nbytes),\n            '-ar', \"%d\"%fps_input,\n            '-ac',\"%d\"%nchannels,\n            '-i', '-']\n            + (['-vn'] if input_video is None else\n                 [ \"-i\", input_video, '-vcodec', 'copy'])\n            + ['-acodec', codec]\n            + ['-ar', \"%d\"%fps_input]\n            + ['-strict', '-2']  # needed to support codec 'aac'\n            + (['-ab',bitrate] if (bitrate is not None) else [])\n            + (ffmpeg_params if ffmpeg_params else [])\n            + [ filename ])\n\n        popen_params = {\"stdout\": DEVNULL,\n                        \"stderr\": logfile,\n                        \"stdin\": sp.PIPE}\n\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n\n        self.proc = sp.Popen(cmd, **popen_params)",
  "def write_frames(self,frames_array):\n        try:\n            self.proc.stdin.write(frames_array.tostring())\n        except IOError as err:\n            ffmpeg_error = self.proc.stderr.read()\n            error = (str(err)+ (\"\\n\\nMoviePy error: FFMPEG encountered \"\n                     \"the following error while writing file %s:\"%self.filename\n                     + \"\\n\\n\"+ffmpeg_error))\n\n            if \"Unknown encoder\" in ffmpeg_error:\n\n                error = (error+(\"\\n\\nThe audio export failed because \"\n                    \"FFMPEG didn't find the specified codec for audio \"\n                    \"encoding (%s). Please install this codec or \"\n                    \"change the codec when calling to_videofile or \"\n                    \"to_audiofile. For instance for mp3:\\n\"\n                    \"   >>> to_videofile('myvid.mp4', audio_codec='libmp3lame')\"\n                    )%(self.codec))\n\n            elif \"incorrect codec parameters ?\" in ffmpeg_error:\n\n                error = error+(\"\\n\\nThe audio export \"\n                  \"failed, possibly because the codec specified for \"\n                  \"the video (%s) is not compatible with the given \"\n                  \"extension (%s). Please specify a valid 'codec' \"\n                  \"argument in to_videofile. This would be 'libmp3lame' \"\n                  \"for mp3, 'libvorbis' for ogg...\")%(self.codec, self.ext)\n\n            elif  \"encoder setup failed\":\n\n                error = error+(\"\\n\\nThe audio export \"\n                  \"failed, possily because the bitrate you specified \"\n                  \"was two high or too low for the video codec.\")\n\n            else:\n\n                error = error+(\"\\n\\nIn case it helps, make sure you are \"\n                  \"using a recent version of FFMPEG (the versions in the \"\n                  \"Ubuntu/Debian repos are deprecated).\")\n            raise IOError(error)",
  "def close(self):\n        self.proc.stdin.close()\n        if self.proc.stderr is not None:\n            self.proc.stderr.close()\n        self.proc.wait()\n        del self.proc",
  "def find_audio_period(aclip, t_min=.1, t_max=2, t_res=.01):\n    \"\"\" Finds the period, in seconds of an audioclip.\n    \n    The beat is then given by bpm = 60/T\n\n    t_min and _tmax are bounds for the returned value, t_res\n    is the numerical precision\n    \"\"\"\n    chunksize = int(t_res*aclip.fps)\n    chunk_duration = 1.0*chunksize/aclip.fps\n    # v denotes the list of volumes\n    v = np.array([(c**2).sum() for c in\n                aclip.iter_chunks(chunksize)])\n    v = v-v.mean()\n    corrs = np.correlate(v, v, mode = 'full')[-len(v):]\n    corrs[:int(t_min/chunk_duration)]=0\n    corrs[int(t_max/chunk_duration):]=0\n    return chunk_duration*np.argmax(corrs)",
  "class VideoClip(Clip):\n    \"\"\"Base class for video clips.\n\n    See ``VideofileClip``, ``ImageClip`` etc. for more user-friendly\n    classes.\n\n\n    Parameters\n    -----------\n\n    ismask\n      `True` if the clip is going to be used as a mask.\n\n\n    Attributes\n    ----------\n\n    size\n      The size of the clip, (width,heigth), in pixels.\n\n    w, h\n      The width and height of the clip, in pixels.\n\n    ismask\n      Boolean set to `True` if the clip is a mask.\n\n    make_frame\n      A function ``t-> frame at time t`` where ``frame`` is a\n      w*h*3 RGB array.\n\n    mask (default None)\n      VideoClip mask attached to this clip. If mask is ``None``,\n                The video clip is fully opaque.\n\n    audio (default None)\n      An AudioClip instance containing the audio of the video clip.\n\n    pos\n      A function ``t->(x,y)`` where ``x,y`` is the position\n      of the clip when it is composed with other clips.\n      See ``VideoClip.set_pos`` for more details\n\n    relative_pos\n      See variable ``pos``.\n\n    \"\"\"\n\n    def __init__(self, make_frame=None, ismask=False, duration=None,\n                 has_constant_size=True):\n        Clip.__init__(self)\n        self.mask = None\n        self.audio = None\n        self.pos = lambda t: (0, 0)\n        self.relative_pos = False\n        if make_frame is not None:\n            self.make_frame = make_frame\n            self.size = self.get_frame(0).shape[:2][::-1]\n        self.ismask = ismask\n        self.has_constant_size=has_constant_size\n        if duration is not None:\n            self.duration = duration\n            self.end = duration\n\n    @property\n    def w(self):\n        return self.size[0]\n\n\n    @property\n    def h(self):\n        return self.size[1]\n\n\n    # ===============================================================\n    # EXPORT OPERATIONS\n\n\n    @convert_to_seconds(['t'])\n    @convert_masks_to_RGB\n    def save_frame(self, filename, t=0, withmask=True):\n        \"\"\" Save a clip's frame to an image file.\n\n        Saves the frame of clip corresponding to time ``t`` in\n        'filename'. ``t`` can be expressed in seconds (15.35), in\n        (min, sec), in (hour, min, sec), or as a string: '01:03:05.35'.\n\n        If ``withmask`` is ``True`` the mask is saved in\n        the alpha layer of the picture (only works with PNGs).\n\n        \"\"\"\n\n        im = self.get_frame(t) \n\n        if withmask and self.mask is not None:\n            mask = 255 * self.mask.get_frame(t)\n            im = np.dstack([im, mask]).astype('uint8')\n        else:\n            im = im.astype(\"uint8\")\n\n        imsave(filename, im)\n\n\n    @requires_duration\n    @use_clip_fps_by_default\n    @convert_masks_to_RGB\n    def write_videofile(self, filename, fps=None, codec=None,\n                        bitrate=None, audio=True, audio_fps=44100,\n                        preset=\"medium\",\n                        audio_nbytes=4, audio_codec=None,\n                        audio_bitrate=None, audio_bufsize=2000,\n                        temp_audiofile=None,\n                        rewrite_audio=True, remove_temp=True,\n                        write_logfile=False, verbose=True,\n                        threads=None, ffmpeg_params=None):\n\n        \"\"\"Write the clip to a videofile.\n\n        Parameters\n        -----------\n\n        filename\n          Name of the video file to write in.\n          The extension must correspond to the \"codec\" used (see below),\n          or simply be '.avi' (which will work with any codec).\n\n        fps\n          Number of frames per second in the resulting video file. If None is\n          provided, and the clip has an fps attribute, this fps will be used.\n\n        codec\n          Codec to use for image encoding. Can be any codec supported\n          by ffmpeg. If the filename is has extension '.mp4', '.ogv', '.webm',\n          the codec will be set accordingly, but you can still set it if you\n          don't like the default. For other extensions, the output filename\n          must be set accordingly.\n\n          Some examples of codecs are:\n\n          ``'libx264'`` (default codec for file extension ``.mp4``)\n          makes well-compressed videos (quality tunable using 'bitrate').\n\n\n          ``'mpeg4'`` (other codec for extension ``.mp4``) can be an alternative\n          to ``'libx264'``, and produces higher quality videos by default.\n\n\n          ``'rawvideo'`` (use file extension ``.avi``) will produce\n          a video of perfect quality, of possibly very huge size.\n\n\n          ``png`` (use file extension ``.avi``) will produce a video\n          of perfect quality, of smaller size than with ``rawvideo``\n\n\n          ``'libvorbis'`` (use file extension ``.ogv``) is a nice video\n          format, which is completely free/ open source. However not\n          everyone has the codecs installed by default on their machine.\n\n\n          ``'libvpx'`` (use file extension ``.webm``) is tiny a video\n          format well indicated for web videos (with HTML5). Open source.\n\n\n        audio\n          Either ``True``, ``False``, or a file name.\n          If ``True`` and the clip has an audio clip attached, this\n          audio clip will be incorporated as a soundtrack in the movie.\n          If ``audio`` is the name of an audio file, this audio file\n          will be incorporated as a soundtrack in the movie.\n\n        audiofps\n          frame rate to use when generating the sound.\n\n        temp_audiofile\n          the name of the temporary audiofile to be generated and\n          incorporated in the the movie, if any.\n\n        audio_codec\n          Which audio codec should be used. Examples are 'libmp3lame'\n          for '.mp3', 'libvorbis' for 'ogg', 'libfdk_aac':'m4a',\n          'pcm_s16le' for 16-bit wav and 'pcm_s32le' for 32-bit wav.\n          Default is 'libmp3lame', unless the video extension is 'ogv'\n          or 'webm', at which case the default is 'libvorbis'.\n\n        audio_bitrate\n          Audio bitrate, given as a string like '50k', '500k', '3000k'.\n          Will determine the size/quality of audio in the output file.\n          Note that it mainly an indicative goal, the bitrate won't\n          necessarily be the this in the final file.\n\n        preset\n          Sets the time that FFMPEG will spend optimizing the compression.\n          Choices are: ultrafast, superfast, fast, medium, slow, superslow.\n          Note that this does not impact the quality of the video, only the\n          size of the video file. So choose ultrafast when you are in a\n          hurry and file size does not matter.\n\n        threads\n          Number of threads to use for ffmpeg. Can speed up the writing of\n          the video on multicore computers\n\n        ffmpeg_params\n          Any additional ffmpeg parameters you would like to pass, as a list\n          of terms, like ['-option1', 'value1', '-option2', 'value2']\n\n        write_logfile\n          If true, will write log files for the audio and the video.\n          These will be files ending with '.log' with the name of the\n          output file in them.\n\n\n\n        Examples\n        ========\n\n        >>> from moviepy.editor import VideoFileClip\n        >>> clip = VideoFileClip(\"myvideo.mp4\").subclip(100,120)\n        >>> clip.write_videofile(\"my_new_video.mp4\")\n\n        \"\"\"\n\n        name, ext = os.path.splitext(os.path.basename(filename))\n        ext = ext[1:].lower()\n\n        if codec is None:\n\n            try:\n                codec = extensions_dict[ext]['codec'][0]\n            except KeyError:\n                raise ValueError(\"MoviePy couldn't find the codec associated \"\n                                 \"with the filename. Provide the 'codec' parameter in \"\n                                 \"write_videofile.\")\n\n        if audio_codec is None:\n            if (ext in ['ogv', 'webm']):\n                audio_codec = 'libvorbis'\n            else:\n                audio_codec = 'libmp3lame'\n        elif audio_codec == 'raw16':\n            audio_codec = 'pcm_s16le'\n        elif audio_codec == 'raw32':\n            audio_codec = 'pcm_s32le'\n\n        audiofile = audio if is_string(audio) else None\n        make_audio = ((audiofile is None) and (audio == True) and\n                      (self.audio is not None))\n\n        if make_audio:\n            # The audio will be the clip's audio\n            if temp_audiofile is not None:\n                audiofile = temp_audiofile\n\n            else:\n\n                # make a name for the temporary audio file\n\n                if audio_codec in extensions_dict:\n                    audio_ext = audio_codec\n                else:\n                    try:\n                        audio_ext = find_extension(audio_codec)\n                    except ValueError:\n\n                        raise ValueError(\n                            \"The audio_codec you chose is unknown by MoviePy. \"\n                            \"You should report this. In the meantime, you can specify a \"\n                            \"temp_audiofile with the right extension in write_videofile.\")\n\n                audiofile = (name + Clip._TEMP_FILES_PREFIX +\n                             \"wvf_snd.%s\" % audio_ext)\n\n        # enough cpu for multiprocessing ? USELESS RIGHT NOW, WILL COME AGAIN\n        # enough_cpu = (multiprocessing.cpu_count() > 1)\n\n        verbose_print(verbose, \"[MoviePy] >>>> Building video %s\\n\" % filename)\n\n        if make_audio:\n            self.audio.write_audiofile(audiofile, audio_fps,\n                                       audio_nbytes, audio_bufsize,\n                                       audio_codec, bitrate=audio_bitrate,\n                                       write_logfile=write_logfile,\n                                       verbose=verbose)\n\n        ffmpeg_write_video(self, filename, fps, codec,\n                           bitrate=bitrate,\n                           preset=preset,\n                           write_logfile=write_logfile,\n                           audiofile = audiofile,\n                           verbose=verbose, threads=threads,\n                           ffmpeg_params=ffmpeg_params)\n\n        if remove_temp and make_audio:\n            os.remove(audiofile)\n\n        verbose_print(verbose, \"[MoviePy] >>>> Video ready: %s \\n\\n\"%filename)\n\n\n    @requires_duration\n    @use_clip_fps_by_default\n    @convert_masks_to_RGB\n    def write_images_sequence(self, nameformat, fps=None, verbose=True,\n                              withmask=True):\n        \"\"\" Writes the videoclip to a sequence of image files.\n\n\n        Parameters\n        -----------\n\n        nameformat\n          A filename specifying the numerotation format and extension\n          of the pictures. For instance \"frame%03d.png\" for filenames\n          indexed with 3 digits and PNG format. Also possible:\n          \"some_folder/frame%04d.jpeg\", etc.\n\n        fps\n          Number of frames per second to consider when writing the\n          clip. If not specified, the clip's ``fps`` attribute will\n          be used if it has one.\n\n        withmask\n          will save the clip's mask (if any) as an alpha canal (PNGs only)\n\n        verbose\n          Verbose output ?\n\n\n        Returns\n        --------\n\n        names_list\n          A list of all the files generated.\n\n        Notes\n        ------\n\n        The resulting image sequence can be read using e.g. the class\n        ``DirectoryClip``.\n\n        \"\"\"\n\n        verbose_print(verbose, \"[MoviePy] Writing frames %s.\" % (nameformat))\n\n        tt = np.arange(0, self.duration, 1.0 / fps)\n\n        filenames = []\n        total = int(self.duration / fps) + 1\n        for i, t in tqdm(enumerate(tt), total=total):\n            name = nameformat % i\n            filenames.append(name)\n            self.save_frame(name, t, withmask=withmask)\n\n        verbose_print(verbose,\n                      \"[MoviePy]: Done writing frames %s.\\n\\n\" % (nameformat))\n\n        return filenames\n\n\n\n    @requires_duration\n    @convert_masks_to_RGB\n    def write_gif(self, filename, fps=None, program='imageio',\n                  opt='wu', fuzz=1, verbose=True,\n                  loop=0, dispose=False, colors=None, tempfiles=False):\n        \"\"\" Write the VideoClip to a GIF file.\n\n        Converts a VideoClip into an animated GIF using ImageMagick\n        or ffmpeg.\n\n\n        Parameters\n        -----------\n\n        filename\n          Name of the resulting gif file.\n\n        fps\n          Number of frames per second (see note below). If it\n            isn't provided, then the function will look for the clip's\n            ``fps`` attribute (VideoFileClip, for instance, have one).\n\n        program\n          Software to use for the conversion, either 'imageio' (this will use\n          the library FreeImage through ImageIO), or 'ImageMagick', or 'ffmpeg'.\n\n        opt\n          Optimalization to apply. If program='imageio', opt must be either 'wu'\n          (Wu) or 'nq' (Neuquant). If program='ImageMagick',\n          either 'optimizeplus' or 'OptimizeTransparency'.\n\n        fuzz\n          (ImageMagick only) Compresses the GIF by considering that\n          the colors that are less than fuzz% different are in fact\n          the same.\n\n\n        Notes\n        -----\n\n        The gif will be playing the clip in real time (you can\n        only change the frame rate). If you want the gif to be played\n        slower than the clip you will use ::\n\n            >>> # slow down clip 50% and make it a gif\n            >>> myClip.speedx(0.5).to_gif('myClip.gif')\n\n        \"\"\"\n\n        # A little sketchy at the moment, maybe move all that in write_gif,\n        #  refactor a little... we will see.\n\n        if program == 'imageio':\n            write_gif_with_image_io(self, filename, fps=fps, opt=opt, loop=loop,\n                                    verbose=verbose, colors=colors)\n        \n        elif tempfiles:\n            write_gif_with_tempfiles(self, filename, fps=fps,\n                                     program=program, opt=opt, fuzz=fuzz,\n                                     verbose=verbose,\n                                     loop=loop, dispose=dispose, colors=colors)\n        else:\n            write_gif(self, filename, fps=fps, program=program,\n                      opt=opt, fuzz=fuzz, verbose=verbose, loop=loop,\n                      dispose=dispose, colors=colors)\n\n    # -----------------------------------------------------------------\n    # F I L T E R I N G\n\n\n\n    def subfx(self, fx, ta=0, tb=None, **kwargs):\n        \"\"\" Apply a transformation to a part of the clip.\n\n        Returns a new clip in which the function ``fun`` (clip->clip)\n        has been applied to the subclip between times `ta` and `tb`\n        (in seconds).\n\n        Examples\n        ---------\n\n        >>> # The scene between times t=3s and t=6s in ``clip`` will be\n        >>> # be played twice slower in ``newclip``\n        >>> newclip = clip.subapply(lambda c:c.speedx(0.5) , 3,6)\n\n        \"\"\"\n\n        left = None if (ta == 0) else self.subclip(0, ta)\n        center = self.subclip(ta, tb).fx(fx, **kwargs)\n        right = None if (tb is None) else self.subclip(t_start=tb)\n\n        clips = [c for c in [left, center, right] if c is not None]\n\n        # beurk, have to find other solution\n        from moviepy.video.compositing.concatenate import concatenate_videoclips\n\n        return concatenate_videoclips(clips).set_start(self.start)\n\n    # IMAGE FILTERS\n\n\n    def fl_image(self, image_func, apply_to=[]):\n        \"\"\"\n        Modifies the images of a clip by replacing the frame\n        `get_frame(t)` by another frame,  `image_func(get_frame(t))`\n        \"\"\"\n        return self.fl(lambda gf, t: image_func(gf(t)), apply_to)\n\n    # --------------------------------------------------------------\n    # C O M P O S I T I N G\n\n\n    def blit_on(self, picture, t):\n        \"\"\"\n        Returns the result of the blit of the clip's frame at time `t`\n        on the given `picture`, the position of the clip being given\n        by the clip's ``pos`` attribute. Meant for compositing.\n        \"\"\"\n\n        hf, wf = framesize = picture.shape[:2]\n\n        if self.ismask and picture.max() != 0:\n            return np.minimum(1, picture + self.blit_on(np.zeros(framesize), t))\n\n        ct = t - self.start  # clip time\n\n        # GET IMAGE AND MASK IF ANY\n\n        img = self.get_frame(ct)\n        mask = (None if (self.mask is None) else\n                self.mask.get_frame(ct))\n        hi, wi = img.shape[:2]\n\n        # SET POSITION\n\n        pos = self.pos(ct)\n\n        # preprocess short writings of the position\n        if isinstance(pos, str):\n            pos = {'center': ['center', 'center'],\n                   'left': ['left', 'center'],\n                   'right': ['right', 'center'],\n                   'top': ['center', 'top'],\n                   'bottom': ['center', 'bottom']}[pos]\n        else:\n            pos = list(pos)\n\n        # is the position relative (given in % of the clip's size) ?\n        if self.relative_pos:\n            for i, dim in enumerate(wf, hf):\n                if not isinstance(pos[i], str):\n                    pos[i] = dim * pos[i]\n\n        if isinstance(pos[0], str):\n            D = {'left': 0, 'center': (wf - wi) / 2, 'right': wf - wi}\n            pos[0] = D[pos[0]]\n\n        if isinstance(pos[1], str):\n            D = {'top': 0, 'center': (hf - hi) / 2, 'bottom': hf - hi}\n            pos[1] = D[pos[1]]\n\n        pos = map(int, pos)\n\n        return blit(img, picture, pos, mask=mask, ismask=self.ismask)\n\n\n    def add_mask(self):\n        \"\"\" Add a mask VideoClip to the VideoClip.\n\n        Returns a copy of the clip with a completely opaque mask\n        (made of ones). This makes computations slower compared to\n        having a None mask but can be useful in many cases. Choose\n\n        Set ``constant_size`` to  `False` for clips with moving\n        image size.\n        \"\"\"\n        if self.has_constant_size:\n            mask = ColorClip(self.size, 1.0, ismask=True)\n            return self.set_mask(mask.set_duration(self.duration))\n        else:\n            make_frame = lambda t: np.ones(self.get_frame(t).shape[:2], dtype=float)\n            mask = VideoClip(ismask=True, make_frame=make_frame)\n            return self.set_mask(mask.set_duration(self.duration))\n\n\n    def on_color(self, size=None, color=(0, 0, 0), pos=None,\n                 col_opacity=None):\n        \"\"\" Place the clip on a colored background.\n\n        Returns a clip made of the current clip overlaid on a color\n        clip of a possibly bigger size. Can serve to flatten transparent\n        clips.\n\n        Parameters\n        -----------\n\n        size\n          Size (width, height) in pixels of the final clip.\n          By default it will be the size of the current clip.\n\n        bg_color\n          Background color of the final clip ([R,G,B]).\n\n        pos\n          Position of the clip in the final clip. 'center' is the default\n\n        col_opacity\n          Parameter in 0..1 indicating the opacity of the colored\n          background.\n\n        \"\"\"\n        from .compositing.CompositeVideoClip import CompositeVideoClip\n\n        if size is None:\n            size = self.size\n        if pos is None:\n            pos = 'center'\n        colorclip = ColorClip(size, color)\n\n        if col_opacity is not None:\n            colorclip = (ColorClip(size, color, duration=self.duration)\n                         .set_opacity(col_opacity))\n            result = CompositeVideoClip([colorclip, self.set_pos(pos)])\n        else:\n            result = CompositeVideoClip([self.set_pos(pos)],\n                                        size=size,\n                                        bg_color=color)\n\n        if (isinstance(self, ImageClip) and (not hasattr(pos, \"__call__\"))\n            and ((self.mask is None) or isinstance(self.mask, ImageClip))):\n            new_result = result.to_ImageClip()\n            if result.mask is not None:\n                new_result.mask = result.mask.to_ImageClip()\n            return new_result.set_duration(result.duration)\n\n        return result\n\n\n    @outplace\n    def set_make_frame(self, mf):\n        \"\"\" Change the clip's ``get_frame``.\n\n        Returns a copy of the VideoClip instance, with the make_frame\n        attribute set to `mf`.\n        \"\"\"\n        self.make_frame = mf\n        self.size = self.get_frame(0).shape[:2][::-1]\n\n\n    @outplace\n    def set_audio(self, audioclip):\n        \"\"\" Attach an AudioClip to the VideoClip.\n\n        Returns a copy of the VideoClip instance, with the `audio`\n        attribute set to ``audio``, which must be an AudioClip instance.\n        \"\"\"\n        self.audio = audioclip\n\n\n    @outplace\n    def set_mask(self, mask):\n        \"\"\" Set the clip's mask.\n\n        Returns a copy of the VideoClip with the mask attribute set to\n        ``mask``, which must be a greyscale (values in 0-1) VideoClip\"\"\"\n        assert ( (mask is None) or mask.ismask )\n        self.mask = mask\n\n\n    @add_mask_if_none\n    @outplace\n    def set_opacity(self, op):\n        \"\"\" Set the opacity/transparency level of the clip.\n\n        Returns a semi-transparent copy of the clip where the mask is\n        multiplied by ``op`` (any float, normally between 0 and 1).\n        \"\"\"\n\n        self.mask = self.mask.fl_image(lambda pic: op * pic)\n\n\n    @apply_to_mask\n    @outplace\n    def set_position(self, pos, relative=False):\n        \"\"\" Set the clip's position in compositions.\n\n        Sets the position that the clip will have when included\n        in compositions. The argument ``pos`` can be either a couple\n        ``(x,y)`` or a function ``t-> (x,y)``. `x` and `y` mark the\n        location of the top left corner of the clip, and can be\n        of several types.\n\n        Examples\n        ----------\n\n        >>> clip.set_pos((45,150)) # x=45, y=150\n        >>>\n        >>> # clip horizontally centered, at the top of the picture\n        >>> clip.set_pos((\"center\",\"top\"))\n        >>>\n        >>> # clip is at 40% of the width, 70% of the height:\n        >>> clip.set_pos((0.4,0.7), relative=True)\n        >>>\n        >>> # clip's position is horizontally centered, and moving up !\n        >>> clip.set_pos(lambda t: ('center', 50+t) )\n\n        \"\"\"\n\n        self.relative_pos = relative\n        if hasattr(pos, '__call__'):\n            self.pos = pos\n        else:\n            self.pos = lambda t: pos\n\n\n    #--------------------------------------------------------------\n    # CONVERSIONS TO OTHER TYPES\n\n\n\n    @convert_to_seconds(['t'])\n    def to_ImageClip(self, t=0, with_mask=True):\n        \"\"\"\n        Returns an ImageClip made out of the clip's frame at time ``t``,\n        which can be expressed in seconds (15.35), in (min, sec),\n        in (hour, min, sec), or as a string: '01:03:05.35'.\n        \"\"\"\n        newclip = ImageClip(self.get_frame(t), ismask=self.ismask)\n        if with_mask and self.mask is not None:\n            newclip.mask = self.mask.to_ImageClip(t)\n        return newclip\n\n\n    def to_mask(self, canal=0):\n        \"\"\"\n        Returns a mask a video clip made from the clip.\n        \"\"\"\n        if self.ismask:\n            return self\n        else:\n            newclip = self.fl_image(lambda pic:\n                                    1.0 * pic[:, :, canal] / 255)\n            newclip.ismask = True\n            return newclip\n\n\n    def to_RGB(self):\n        \"\"\"\n        Returns a non-mask video clip made from the mask video clip.\n        \"\"\"\n        if self.ismask:\n            f = lambda pic: np.dstack(3 * [255 * pic]).astype('uint8')\n            newclip = self.fl_image(f)\n            newclip.ismask = False\n            return newclip\n        else:\n            return self\n\n    #----------------------------------------------------------------\n    # Audio\n\n\n    @outplace\n    def without_audio(self):\n        \"\"\" Remove the clip's audio.\n\n        Return a copy of the clip with audio set to None.\n\n        \"\"\"\n        self.audio = None\n\n\n    @outplace\n    def afx(self, fun, *a, **k):\n        \"\"\" Transform the clip's audio.\n\n        Return a new clip whose audio has been transformed by ``fun``.\n\n        \"\"\"\n        self.audio = self.audio.fx(fun, *a, **k)",
  "class DataVideoClip(VideoClip):\n    \"\"\"\n    Class of video clips whose successive frames are functions\n    of successive datasets\n\n    Parameters\n    -----------\n    data\n      A liste of datasets, each dataset being used for one frame of the clip\n\n    data_to_frame\n      A function d -> video frame, where d is one element of the list `data`\n\n    fps\n      Number of frames per second in the animation\n\n    Examples\n    ---------\n    \"\"\"\n\n    def __init__(self, data, data_to_frame, fps, ismask=False,\n                 has_constant_size=True):\n        self.data = data\n        self.data_to_frame = data_to_frame\n        self.fps=fps\n        make_frame = lambda t: self.data_to_frame( self.data[int(self.fps*t)])\n        VideoClip.__init__(self, make_frame, ismask=ismask,\n               duration=1.0*len(data)/fps, has_constant_size=has_constant_size)",
  "class UpdatedVideoClip(VideoClip):\n    \"\"\"\n        \n    Class of clips whose make_frame requires some objects to\n    be updated. Particularly practical in science where some\n    algorithm needs to make some steps before a new frame can\n    be generated.\n    \n    UpdatedVideoClips have the following make_frame:\n\n    >>> def make_frame(t):\n    >>>     while self.world.clip_t < t:\n    >>>         world.update() # updates, and increases world.clip_t\n    >>>     return world.to_frame()\n\n    Parameters\n    -----------\n\n    world\n      An object with the following attributes:\n      - world.clip_t : the clip's time corresponding to the\n          world's state\n      - world.update() : update the world's state, (including\n        increasing world.clip_t of one time step)\n      - world.to_frame() : renders a frame depending on the world's state\n\n    ismask\n      True if the clip is a WxH mask with values in 0-1\n\n    duration\n      Duration of the clip, in seconds\n          \n    \"\"\"\n    \n    \n    def __init__(self, world, ismask=False, duration=None):\n        \n        self.world = world\n        def make_frame(t):\n            while self.world.clip_t < t:\n                world.update()\n            return world.to_frame()\n        VideoClip.__init__(self, make_frame= make_frame,\n                               ismask=ismask, duration=duration)",
  "class ImageClip(VideoClip):\n    \"\"\" Class for non-moving VideoClips.\n\n    A video clip originating from a picture. This clip will simply\n    display the given picture at all times.\n\n    Examples\n    ---------\n\n    >>> clip = ImageClip(\"myHouse.jpeg\")\n    >>> clip = ImageClip( someArray ) # a Numpy array represent\n\n    Parameters\n    -----------\n\n    img\n      Any picture file (png, tiff, jpeg, etc.) or any array representing\n      an RGB image (for instance a frame from a VideoClip).\n\n    ismask\n      Set this parameter to `True` if the clip is a mask.\n\n    transparent\n      Set this parameter to `True` (default) if you want the alpha layer\n      of the picture (if it exists) to be used as a mask.\n\n    Attributes\n    -----------\n\n    img\n      Array representing the image of the clip.\n\n    \"\"\"\n\n\n    def __init__(self, img, ismask=False, transparent=True,\n                 fromalpha=False, duration=None):\n\n        VideoClip.__init__(self, ismask=ismask, duration=duration)\n\n        if isinstance(img, str):\n            img = imread(img)\n\n        if len(img.shape) == 3:  # img is (now) a RGB(a) numpy array\n\n            if img.shape[2] == 4:\n                if fromalpha:\n                    img = 1.0 * img[:, :, 3] / 255\n                elif ismask:\n                    img = 1.0 * img[:, :, 0] / 255\n                elif transparent:\n                    self.mask = ImageClip(\n                        1.0 * img[:, :, 3] / 255, ismask=True)\n                    img = img[:, :, :3]\n            elif ismask:\n                img = 1.0 * img[:, :, 0] / 255\n\n        # if the image was just a 2D mask, it should arrive here\n        # unchanged\n        self.make_frame = lambda t: img\n        self.size = img.shape[:2][::-1]\n        self.img = img\n\n\n    def fl(self, fl, apply_to=[], keep_duration=True):\n        \"\"\" General transformation filter.\n\n        Equivalent to VideoClip.fl . The result is no more an\n        ImageClip, it has the class VideoClip (since it may be animated)\n        \"\"\"\n\n        # When we use fl on an image clip it may become animated.\n        # Therefore the result is not an ImageClip, just a VideoClip.\n        newclip = VideoClip.fl(self, fl, apply_to=apply_to,\n                               keep_duration=keep_duration)\n        newclip.__class__ = VideoClip\n        return newclip\n\n\n    @outplace\n    def fl_image(self, image_func, apply_to=[]):\n        \"\"\" Image-transformation filter.\n\n        Does the same as VideoClip.fl_image, but for ImageClip the\n        tranformed clip is computed once and for all at the beginning,\n        and not for each 'frame'.\n        \"\"\"\n\n        arr = image_func(self.get_frame(0))\n        self.size = arr.shape[:2][::-1]\n        self.make_frame = lambda t: arr\n        self.img = arr\n\n        for attr in apply_to:\n            if hasattr(self, attr):\n                a = getattr(self, attr)\n                if a is not None:\n                    new_a = a.fl_image(image_func)\n                    setattr(self, attr, new_a)\n\n\n    @outplace\n    def fl_time(self, time_func, apply_to=['mask', 'audio'],\n                keep_duration=False):\n        \"\"\" Time-transformation filter.\n\n        Applies a transformation to the clip's timeline\n        (see Clip.fl_time).\n\n        This method does nothing for ImageClips (but it may affect their\n        masks or their audios). The result is still an ImageClip.\n        \"\"\"\n\n        for attr in apply_to:\n            if hasattr(self, attr):\n                a = getattr(self, attr)\n                if a is not None:\n                    new_a = a.fl_time(time_func)\n                    setattr(self, attr, new_a)",
  "class ColorClip(ImageClip):\n    \"\"\" An ImageClip showing just one color.\n\n    Parameters\n    -----------\n\n    size\n      Size (width, height) in pixels of the clip.\n\n    color\n      If argument ``ismask`` is False, ``color`` indicates\n      the color in RGB of the clip (default is black). If `ismask``\n      is True, ``color`` must be  a float between 0 and 1 (default is 1)\n\n    ismask\n      Set to true if the clip will be used as a mask.\n    \"\"\"\n\n\n    def __init__(self, size, col=(0, 0, 0), ismask=False, duration=None):\n        w, h = size\n        shape = (h, w) if np.isscalar(col) else (h, w, len(col))\n        ImageClip.__init__(self, np.tile(col, w * h).reshape(shape),\n                           ismask=ismask, duration=duration)",
  "class TextClip(ImageClip):\n    \"\"\" Class for autogenerated text clips.\n\n    Creates an ImageClip originating from a script-generated text image.\n    Requires ImageMagick.\n\n    Parameters\n    -----------\n\n    txt\n      A string of the text to write. Can be replaced by argument\n      ``filename``.\n\n    filename\n      The name of a file in which there is the text to write.\n      Can be provided instead of argument ``txt``\n\n    size\n      Size of the picture in pixels. Can be auto-set if\n      method='label', but mandatory if method='caption'.\n      the height can be None, it will then be auto-determined.\n\n    bg_color\n      Color of the background. See ``TextClip.list('color')``\n      for a list of acceptable names.\n\n    color\n      Color of the background. See ``TextClip.list('color')`` for a\n      list of acceptable names.\n\n    font\n      Name of the font to use. See ``TextClip.list('font')`` for\n      the list of fonts you can use on your computer.\n\n    stroke_color\n      Color of the stroke (=contour line) of the text. If ``None``,\n      there will be no stroke.\n\n    stroke_width\n      Width of the stroke, in pixels. Can be a float, like 1.5.\n\n    method\n      Either 'label' (default, the picture will be autosized so as to fit\n      exactly the size) or 'caption' (the text will be drawn in a picture\n      with fixed size provided with the ``size`` argument). If `caption`,\n      the text will be wrapped automagically (sometimes it is buggy, not\n      my fault, complain to the ImageMagick crew) and can be aligned or\n      centered (see parameter ``align``).\n\n    kerning\n      Changes the default spacing between letters. For\n      nstance ``kerning=-1`` will make the letters 1 pixel nearer from\n      ach other compared to the default spacing.\n\n    align\n      center | East | West | South | North . Will only work if ``method``\n      is set to ``caption``\n\n    transparent\n      ``True`` (default) if you want to take into account the\n      transparency in the image.\n\n    \"\"\"\n\n\n    def __init__(self, txt=None, filename=None, size=None, color='black',\n                 bg_color='transparent', fontsize=None, font='Courier',\n                 stroke_color=None, stroke_width=1, method='label',\n                 kerning=None, align='center', interline=None,\n                 tempfilename=None, temptxt=None,\n                 transparent=True, remove_temp=True,\n                 print_cmd=False):\n\n\n        if txt is not None:\n            if temptxt is None:\n                temptxt_fd, temptxt = tempfile.mkstemp(suffix='.txt')\n                try:  # only in Python3 will this work\n                    os.write(temptxt_fd, bytes(txt, 'UTF8'))\n                except TypeError:  # oops, fall back to Python2\n                    os.write(temptxt_fd, txt)\n                os.close(temptxt_fd)\n            txt = '@' + temptxt\n        else:\n            # use a file instead of a text.\n            txt = \"@%\" + filename\n\n        if size is not None:\n            size = ('' if size[0] is None else str(size[0]),\n                    '' if size[1] is None else str(size[1]))\n\n        cmd = ( [get_setting(\"IMAGEMAGICK_BINARY\"),\n               \"-background\", bg_color,\n               \"-fill\", color,\n               \"-font\", font])\n\n        if fontsize is not None:\n            cmd += [\"-pointsize\", \"%d\" % fontsize]\n        if kerning is not None:\n            cmd += [\"-kerning\", \"%0.1f\" % kerning]\n        if stroke_color is not None:\n            cmd += [\"-stroke\", stroke_color, \"-strokewidth\",\n                    \"%.01f\" % stroke_width]\n        if size is not None:\n            cmd += [\"-size\", \"%sx%s\" % (size[0], size[1])]\n        if align is not None:\n            cmd += [\"-gravity\", align]\n        if interline is not None:\n            cmd += [\"-interline-spacing\", \"%d\" % interline]\n\n        if tempfilename is None:\n            tempfile_fd, tempfilename = tempfile.mkstemp(suffix='.png')\n            os.close(tempfile_fd)\n\n        cmd += [\"%s:%s\" % (method, txt),\n                \"-type\", \"truecolormatte\", \"PNG32:%s\" % tempfilename]\n\n        if print_cmd:\n            print( \" \".join(cmd) )\n        \n        try:\n            subprocess_call(cmd, verbose=False )\n        except (IOError,OSError) as err:\n            error = (\"MoviePy Error: creation of %s failed because \"\n              \"of the following error:\\n\\n%s.\\n\\n.\"%(filename, str(err))\n               + (\"This error can be due to the fact that \"\n                    \"ImageMagick is not installed on your computer, or \"\n                    \"(for Windows users) that you didn't specify the \"\n                    \"path to the ImageMagick binary in file conf.py, or.\"\n                    \"that the path you specified is incorrect\" ))\n            raise IOError(error)\n\n        ImageClip.__init__(self, tempfilename, transparent=transparent)\n        self.txt = txt\n        self.color = color\n        self.stroke_color = stroke_color\n\n        if remove_temp:\n            if os.path.exists(tempfilename):\n                os.remove(tempfilename)\n            if os.path.exists(temptxt):\n                os.remove(temptxt)\n\n\n    @staticmethod\n    def list(arg):\n        \"\"\" Returns the list of all valid entries for the argument of\n        ``TextClip`` given (can be ``font``, ``color``, etc...) \"\"\"\n\n        popen_params = {\"stdout\": sp.PIPE,\n                        \"stderr\": DEVNULL,\n                        \"stdin\": DEVNULL}\n\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n\n        process = sp.Popen([get_setting(\"IMAGEMAGICK_BINARY\"),\n                            '-list', arg], **popen_params)\n        result = process.communicate()[0]\n        lines = result.splitlines()\n\n        if arg == 'font':\n            return [l[8:] for l in lines if l.startswith(\"  Font:\")]\n        elif arg == 'color':\n            return [l.split(\" \")[1] for l in lines[2:]]\n\n    @staticmethod\n    def search(string, arg):\n        \"\"\" Returns the of all valid entries which contain ``string`` for the\n           argument ``arg`` of ``TextClip``, for instance\n\n           >>> # Find all the available fonts which contain \"Courier\"\n           >>> print ( TextClip.search('Courier', 'font') )\n \n        \"\"\"\n        string = string.lower()\n        names_list = TextClip.list(arg)\n        return [name for name in names_list if string in name.lower()]",
  "def __init__(self, make_frame=None, ismask=False, duration=None,\n                 has_constant_size=True):\n        Clip.__init__(self)\n        self.mask = None\n        self.audio = None\n        self.pos = lambda t: (0, 0)\n        self.relative_pos = False\n        if make_frame is not None:\n            self.make_frame = make_frame\n            self.size = self.get_frame(0).shape[:2][::-1]\n        self.ismask = ismask\n        self.has_constant_size=has_constant_size\n        if duration is not None:\n            self.duration = duration\n            self.end = duration",
  "def w(self):\n        return self.size[0]",
  "def h(self):\n        return self.size[1]",
  "def save_frame(self, filename, t=0, withmask=True):\n        \"\"\" Save a clip's frame to an image file.\n\n        Saves the frame of clip corresponding to time ``t`` in\n        'filename'. ``t`` can be expressed in seconds (15.35), in\n        (min, sec), in (hour, min, sec), or as a string: '01:03:05.35'.\n\n        If ``withmask`` is ``True`` the mask is saved in\n        the alpha layer of the picture (only works with PNGs).\n\n        \"\"\"\n\n        im = self.get_frame(t) \n\n        if withmask and self.mask is not None:\n            mask = 255 * self.mask.get_frame(t)\n            im = np.dstack([im, mask]).astype('uint8')\n        else:\n            im = im.astype(\"uint8\")\n\n        imsave(filename, im)",
  "def write_videofile(self, filename, fps=None, codec=None,\n                        bitrate=None, audio=True, audio_fps=44100,\n                        preset=\"medium\",\n                        audio_nbytes=4, audio_codec=None,\n                        audio_bitrate=None, audio_bufsize=2000,\n                        temp_audiofile=None,\n                        rewrite_audio=True, remove_temp=True,\n                        write_logfile=False, verbose=True,\n                        threads=None, ffmpeg_params=None):\n\n        \"\"\"Write the clip to a videofile.\n\n        Parameters\n        -----------\n\n        filename\n          Name of the video file to write in.\n          The extension must correspond to the \"codec\" used (see below),\n          or simply be '.avi' (which will work with any codec).\n\n        fps\n          Number of frames per second in the resulting video file. If None is\n          provided, and the clip has an fps attribute, this fps will be used.\n\n        codec\n          Codec to use for image encoding. Can be any codec supported\n          by ffmpeg. If the filename is has extension '.mp4', '.ogv', '.webm',\n          the codec will be set accordingly, but you can still set it if you\n          don't like the default. For other extensions, the output filename\n          must be set accordingly.\n\n          Some examples of codecs are:\n\n          ``'libx264'`` (default codec for file extension ``.mp4``)\n          makes well-compressed videos (quality tunable using 'bitrate').\n\n\n          ``'mpeg4'`` (other codec for extension ``.mp4``) can be an alternative\n          to ``'libx264'``, and produces higher quality videos by default.\n\n\n          ``'rawvideo'`` (use file extension ``.avi``) will produce\n          a video of perfect quality, of possibly very huge size.\n\n\n          ``png`` (use file extension ``.avi``) will produce a video\n          of perfect quality, of smaller size than with ``rawvideo``\n\n\n          ``'libvorbis'`` (use file extension ``.ogv``) is a nice video\n          format, which is completely free/ open source. However not\n          everyone has the codecs installed by default on their machine.\n\n\n          ``'libvpx'`` (use file extension ``.webm``) is tiny a video\n          format well indicated for web videos (with HTML5). Open source.\n\n\n        audio\n          Either ``True``, ``False``, or a file name.\n          If ``True`` and the clip has an audio clip attached, this\n          audio clip will be incorporated as a soundtrack in the movie.\n          If ``audio`` is the name of an audio file, this audio file\n          will be incorporated as a soundtrack in the movie.\n\n        audiofps\n          frame rate to use when generating the sound.\n\n        temp_audiofile\n          the name of the temporary audiofile to be generated and\n          incorporated in the the movie, if any.\n\n        audio_codec\n          Which audio codec should be used. Examples are 'libmp3lame'\n          for '.mp3', 'libvorbis' for 'ogg', 'libfdk_aac':'m4a',\n          'pcm_s16le' for 16-bit wav and 'pcm_s32le' for 32-bit wav.\n          Default is 'libmp3lame', unless the video extension is 'ogv'\n          or 'webm', at which case the default is 'libvorbis'.\n\n        audio_bitrate\n          Audio bitrate, given as a string like '50k', '500k', '3000k'.\n          Will determine the size/quality of audio in the output file.\n          Note that it mainly an indicative goal, the bitrate won't\n          necessarily be the this in the final file.\n\n        preset\n          Sets the time that FFMPEG will spend optimizing the compression.\n          Choices are: ultrafast, superfast, fast, medium, slow, superslow.\n          Note that this does not impact the quality of the video, only the\n          size of the video file. So choose ultrafast when you are in a\n          hurry and file size does not matter.\n\n        threads\n          Number of threads to use for ffmpeg. Can speed up the writing of\n          the video on multicore computers\n\n        ffmpeg_params\n          Any additional ffmpeg parameters you would like to pass, as a list\n          of terms, like ['-option1', 'value1', '-option2', 'value2']\n\n        write_logfile\n          If true, will write log files for the audio and the video.\n          These will be files ending with '.log' with the name of the\n          output file in them.\n\n\n\n        Examples\n        ========\n\n        >>> from moviepy.editor import VideoFileClip\n        >>> clip = VideoFileClip(\"myvideo.mp4\").subclip(100,120)\n        >>> clip.write_videofile(\"my_new_video.mp4\")\n\n        \"\"\"\n\n        name, ext = os.path.splitext(os.path.basename(filename))\n        ext = ext[1:].lower()\n\n        if codec is None:\n\n            try:\n                codec = extensions_dict[ext]['codec'][0]\n            except KeyError:\n                raise ValueError(\"MoviePy couldn't find the codec associated \"\n                                 \"with the filename. Provide the 'codec' parameter in \"\n                                 \"write_videofile.\")\n\n        if audio_codec is None:\n            if (ext in ['ogv', 'webm']):\n                audio_codec = 'libvorbis'\n            else:\n                audio_codec = 'libmp3lame'\n        elif audio_codec == 'raw16':\n            audio_codec = 'pcm_s16le'\n        elif audio_codec == 'raw32':\n            audio_codec = 'pcm_s32le'\n\n        audiofile = audio if is_string(audio) else None\n        make_audio = ((audiofile is None) and (audio == True) and\n                      (self.audio is not None))\n\n        if make_audio:\n            # The audio will be the clip's audio\n            if temp_audiofile is not None:\n                audiofile = temp_audiofile\n\n            else:\n\n                # make a name for the temporary audio file\n\n                if audio_codec in extensions_dict:\n                    audio_ext = audio_codec\n                else:\n                    try:\n                        audio_ext = find_extension(audio_codec)\n                    except ValueError:\n\n                        raise ValueError(\n                            \"The audio_codec you chose is unknown by MoviePy. \"\n                            \"You should report this. In the meantime, you can specify a \"\n                            \"temp_audiofile with the right extension in write_videofile.\")\n\n                audiofile = (name + Clip._TEMP_FILES_PREFIX +\n                             \"wvf_snd.%s\" % audio_ext)\n\n        # enough cpu for multiprocessing ? USELESS RIGHT NOW, WILL COME AGAIN\n        # enough_cpu = (multiprocessing.cpu_count() > 1)\n\n        verbose_print(verbose, \"[MoviePy] >>>> Building video %s\\n\" % filename)\n\n        if make_audio:\n            self.audio.write_audiofile(audiofile, audio_fps,\n                                       audio_nbytes, audio_bufsize,\n                                       audio_codec, bitrate=audio_bitrate,\n                                       write_logfile=write_logfile,\n                                       verbose=verbose)\n\n        ffmpeg_write_video(self, filename, fps, codec,\n                           bitrate=bitrate,\n                           preset=preset,\n                           write_logfile=write_logfile,\n                           audiofile = audiofile,\n                           verbose=verbose, threads=threads,\n                           ffmpeg_params=ffmpeg_params)\n\n        if remove_temp and make_audio:\n            os.remove(audiofile)\n\n        verbose_print(verbose, \"[MoviePy] >>>> Video ready: %s \\n\\n\"%filename)",
  "def write_images_sequence(self, nameformat, fps=None, verbose=True,\n                              withmask=True):\n        \"\"\" Writes the videoclip to a sequence of image files.\n\n\n        Parameters\n        -----------\n\n        nameformat\n          A filename specifying the numerotation format and extension\n          of the pictures. For instance \"frame%03d.png\" for filenames\n          indexed with 3 digits and PNG format. Also possible:\n          \"some_folder/frame%04d.jpeg\", etc.\n\n        fps\n          Number of frames per second to consider when writing the\n          clip. If not specified, the clip's ``fps`` attribute will\n          be used if it has one.\n\n        withmask\n          will save the clip's mask (if any) as an alpha canal (PNGs only)\n\n        verbose\n          Verbose output ?\n\n\n        Returns\n        --------\n\n        names_list\n          A list of all the files generated.\n\n        Notes\n        ------\n\n        The resulting image sequence can be read using e.g. the class\n        ``DirectoryClip``.\n\n        \"\"\"\n\n        verbose_print(verbose, \"[MoviePy] Writing frames %s.\" % (nameformat))\n\n        tt = np.arange(0, self.duration, 1.0 / fps)\n\n        filenames = []\n        total = int(self.duration / fps) + 1\n        for i, t in tqdm(enumerate(tt), total=total):\n            name = nameformat % i\n            filenames.append(name)\n            self.save_frame(name, t, withmask=withmask)\n\n        verbose_print(verbose,\n                      \"[MoviePy]: Done writing frames %s.\\n\\n\" % (nameformat))\n\n        return filenames",
  "def write_gif(self, filename, fps=None, program='imageio',\n                  opt='wu', fuzz=1, verbose=True,\n                  loop=0, dispose=False, colors=None, tempfiles=False):\n        \"\"\" Write the VideoClip to a GIF file.\n\n        Converts a VideoClip into an animated GIF using ImageMagick\n        or ffmpeg.\n\n\n        Parameters\n        -----------\n\n        filename\n          Name of the resulting gif file.\n\n        fps\n          Number of frames per second (see note below). If it\n            isn't provided, then the function will look for the clip's\n            ``fps`` attribute (VideoFileClip, for instance, have one).\n\n        program\n          Software to use for the conversion, either 'imageio' (this will use\n          the library FreeImage through ImageIO), or 'ImageMagick', or 'ffmpeg'.\n\n        opt\n          Optimalization to apply. If program='imageio', opt must be either 'wu'\n          (Wu) or 'nq' (Neuquant). If program='ImageMagick',\n          either 'optimizeplus' or 'OptimizeTransparency'.\n\n        fuzz\n          (ImageMagick only) Compresses the GIF by considering that\n          the colors that are less than fuzz% different are in fact\n          the same.\n\n\n        Notes\n        -----\n\n        The gif will be playing the clip in real time (you can\n        only change the frame rate). If you want the gif to be played\n        slower than the clip you will use ::\n\n            >>> # slow down clip 50% and make it a gif\n            >>> myClip.speedx(0.5).to_gif('myClip.gif')\n\n        \"\"\"\n\n        # A little sketchy at the moment, maybe move all that in write_gif,\n        #  refactor a little... we will see.\n\n        if program == 'imageio':\n            write_gif_with_image_io(self, filename, fps=fps, opt=opt, loop=loop,\n                                    verbose=verbose, colors=colors)\n        \n        elif tempfiles:\n            write_gif_with_tempfiles(self, filename, fps=fps,\n                                     program=program, opt=opt, fuzz=fuzz,\n                                     verbose=verbose,\n                                     loop=loop, dispose=dispose, colors=colors)\n        else:\n            write_gif(self, filename, fps=fps, program=program,\n                      opt=opt, fuzz=fuzz, verbose=verbose, loop=loop,\n                      dispose=dispose, colors=colors)",
  "def subfx(self, fx, ta=0, tb=None, **kwargs):\n        \"\"\" Apply a transformation to a part of the clip.\n\n        Returns a new clip in which the function ``fun`` (clip->clip)\n        has been applied to the subclip between times `ta` and `tb`\n        (in seconds).\n\n        Examples\n        ---------\n\n        >>> # The scene between times t=3s and t=6s in ``clip`` will be\n        >>> # be played twice slower in ``newclip``\n        >>> newclip = clip.subapply(lambda c:c.speedx(0.5) , 3,6)\n\n        \"\"\"\n\n        left = None if (ta == 0) else self.subclip(0, ta)\n        center = self.subclip(ta, tb).fx(fx, **kwargs)\n        right = None if (tb is None) else self.subclip(t_start=tb)\n\n        clips = [c for c in [left, center, right] if c is not None]\n\n        # beurk, have to find other solution\n        from moviepy.video.compositing.concatenate import concatenate_videoclips\n\n        return concatenate_videoclips(clips).set_start(self.start)",
  "def fl_image(self, image_func, apply_to=[]):\n        \"\"\"\n        Modifies the images of a clip by replacing the frame\n        `get_frame(t)` by another frame,  `image_func(get_frame(t))`\n        \"\"\"\n        return self.fl(lambda gf, t: image_func(gf(t)), apply_to)",
  "def blit_on(self, picture, t):\n        \"\"\"\n        Returns the result of the blit of the clip's frame at time `t`\n        on the given `picture`, the position of the clip being given\n        by the clip's ``pos`` attribute. Meant for compositing.\n        \"\"\"\n\n        hf, wf = framesize = picture.shape[:2]\n\n        if self.ismask and picture.max() != 0:\n            return np.minimum(1, picture + self.blit_on(np.zeros(framesize), t))\n\n        ct = t - self.start  # clip time\n\n        # GET IMAGE AND MASK IF ANY\n\n        img = self.get_frame(ct)\n        mask = (None if (self.mask is None) else\n                self.mask.get_frame(ct))\n        hi, wi = img.shape[:2]\n\n        # SET POSITION\n\n        pos = self.pos(ct)\n\n        # preprocess short writings of the position\n        if isinstance(pos, str):\n            pos = {'center': ['center', 'center'],\n                   'left': ['left', 'center'],\n                   'right': ['right', 'center'],\n                   'top': ['center', 'top'],\n                   'bottom': ['center', 'bottom']}[pos]\n        else:\n            pos = list(pos)\n\n        # is the position relative (given in % of the clip's size) ?\n        if self.relative_pos:\n            for i, dim in enumerate(wf, hf):\n                if not isinstance(pos[i], str):\n                    pos[i] = dim * pos[i]\n\n        if isinstance(pos[0], str):\n            D = {'left': 0, 'center': (wf - wi) / 2, 'right': wf - wi}\n            pos[0] = D[pos[0]]\n\n        if isinstance(pos[1], str):\n            D = {'top': 0, 'center': (hf - hi) / 2, 'bottom': hf - hi}\n            pos[1] = D[pos[1]]\n\n        pos = map(int, pos)\n\n        return blit(img, picture, pos, mask=mask, ismask=self.ismask)",
  "def add_mask(self):\n        \"\"\" Add a mask VideoClip to the VideoClip.\n\n        Returns a copy of the clip with a completely opaque mask\n        (made of ones). This makes computations slower compared to\n        having a None mask but can be useful in many cases. Choose\n\n        Set ``constant_size`` to  `False` for clips with moving\n        image size.\n        \"\"\"\n        if self.has_constant_size:\n            mask = ColorClip(self.size, 1.0, ismask=True)\n            return self.set_mask(mask.set_duration(self.duration))\n        else:\n            make_frame = lambda t: np.ones(self.get_frame(t).shape[:2], dtype=float)\n            mask = VideoClip(ismask=True, make_frame=make_frame)\n            return self.set_mask(mask.set_duration(self.duration))",
  "def on_color(self, size=None, color=(0, 0, 0), pos=None,\n                 col_opacity=None):\n        \"\"\" Place the clip on a colored background.\n\n        Returns a clip made of the current clip overlaid on a color\n        clip of a possibly bigger size. Can serve to flatten transparent\n        clips.\n\n        Parameters\n        -----------\n\n        size\n          Size (width, height) in pixels of the final clip.\n          By default it will be the size of the current clip.\n\n        bg_color\n          Background color of the final clip ([R,G,B]).\n\n        pos\n          Position of the clip in the final clip. 'center' is the default\n\n        col_opacity\n          Parameter in 0..1 indicating the opacity of the colored\n          background.\n\n        \"\"\"\n        from .compositing.CompositeVideoClip import CompositeVideoClip\n\n        if size is None:\n            size = self.size\n        if pos is None:\n            pos = 'center'\n        colorclip = ColorClip(size, color)\n\n        if col_opacity is not None:\n            colorclip = (ColorClip(size, color, duration=self.duration)\n                         .set_opacity(col_opacity))\n            result = CompositeVideoClip([colorclip, self.set_pos(pos)])\n        else:\n            result = CompositeVideoClip([self.set_pos(pos)],\n                                        size=size,\n                                        bg_color=color)\n\n        if (isinstance(self, ImageClip) and (not hasattr(pos, \"__call__\"))\n            and ((self.mask is None) or isinstance(self.mask, ImageClip))):\n            new_result = result.to_ImageClip()\n            if result.mask is not None:\n                new_result.mask = result.mask.to_ImageClip()\n            return new_result.set_duration(result.duration)\n\n        return result",
  "def set_make_frame(self, mf):\n        \"\"\" Change the clip's ``get_frame``.\n\n        Returns a copy of the VideoClip instance, with the make_frame\n        attribute set to `mf`.\n        \"\"\"\n        self.make_frame = mf\n        self.size = self.get_frame(0).shape[:2][::-1]",
  "def set_audio(self, audioclip):\n        \"\"\" Attach an AudioClip to the VideoClip.\n\n        Returns a copy of the VideoClip instance, with the `audio`\n        attribute set to ``audio``, which must be an AudioClip instance.\n        \"\"\"\n        self.audio = audioclip",
  "def set_mask(self, mask):\n        \"\"\" Set the clip's mask.\n\n        Returns a copy of the VideoClip with the mask attribute set to\n        ``mask``, which must be a greyscale (values in 0-1) VideoClip\"\"\"\n        assert ( (mask is None) or mask.ismask )\n        self.mask = mask",
  "def set_opacity(self, op):\n        \"\"\" Set the opacity/transparency level of the clip.\n\n        Returns a semi-transparent copy of the clip where the mask is\n        multiplied by ``op`` (any float, normally between 0 and 1).\n        \"\"\"\n\n        self.mask = self.mask.fl_image(lambda pic: op * pic)",
  "def set_position(self, pos, relative=False):\n        \"\"\" Set the clip's position in compositions.\n\n        Sets the position that the clip will have when included\n        in compositions. The argument ``pos`` can be either a couple\n        ``(x,y)`` or a function ``t-> (x,y)``. `x` and `y` mark the\n        location of the top left corner of the clip, and can be\n        of several types.\n\n        Examples\n        ----------\n\n        >>> clip.set_pos((45,150)) # x=45, y=150\n        >>>\n        >>> # clip horizontally centered, at the top of the picture\n        >>> clip.set_pos((\"center\",\"top\"))\n        >>>\n        >>> # clip is at 40% of the width, 70% of the height:\n        >>> clip.set_pos((0.4,0.7), relative=True)\n        >>>\n        >>> # clip's position is horizontally centered, and moving up !\n        >>> clip.set_pos(lambda t: ('center', 50+t) )\n\n        \"\"\"\n\n        self.relative_pos = relative\n        if hasattr(pos, '__call__'):\n            self.pos = pos\n        else:\n            self.pos = lambda t: pos",
  "def to_ImageClip(self, t=0, with_mask=True):\n        \"\"\"\n        Returns an ImageClip made out of the clip's frame at time ``t``,\n        which can be expressed in seconds (15.35), in (min, sec),\n        in (hour, min, sec), or as a string: '01:03:05.35'.\n        \"\"\"\n        newclip = ImageClip(self.get_frame(t), ismask=self.ismask)\n        if with_mask and self.mask is not None:\n            newclip.mask = self.mask.to_ImageClip(t)\n        return newclip",
  "def to_mask(self, canal=0):\n        \"\"\"\n        Returns a mask a video clip made from the clip.\n        \"\"\"\n        if self.ismask:\n            return self\n        else:\n            newclip = self.fl_image(lambda pic:\n                                    1.0 * pic[:, :, canal] / 255)\n            newclip.ismask = True\n            return newclip",
  "def to_RGB(self):\n        \"\"\"\n        Returns a non-mask video clip made from the mask video clip.\n        \"\"\"\n        if self.ismask:\n            f = lambda pic: np.dstack(3 * [255 * pic]).astype('uint8')\n            newclip = self.fl_image(f)\n            newclip.ismask = False\n            return newclip\n        else:\n            return self",
  "def without_audio(self):\n        \"\"\" Remove the clip's audio.\n\n        Return a copy of the clip with audio set to None.\n\n        \"\"\"\n        self.audio = None",
  "def afx(self, fun, *a, **k):\n        \"\"\" Transform the clip's audio.\n\n        Return a new clip whose audio has been transformed by ``fun``.\n\n        \"\"\"\n        self.audio = self.audio.fx(fun, *a, **k)",
  "def __init__(self, data, data_to_frame, fps, ismask=False,\n                 has_constant_size=True):\n        self.data = data\n        self.data_to_frame = data_to_frame\n        self.fps=fps\n        make_frame = lambda t: self.data_to_frame( self.data[int(self.fps*t)])\n        VideoClip.__init__(self, make_frame, ismask=ismask,\n               duration=1.0*len(data)/fps, has_constant_size=has_constant_size)",
  "def __init__(self, world, ismask=False, duration=None):\n        \n        self.world = world\n        def make_frame(t):\n            while self.world.clip_t < t:\n                world.update()\n            return world.to_frame()\n        VideoClip.__init__(self, make_frame= make_frame,\n                               ismask=ismask, duration=duration)",
  "def __init__(self, img, ismask=False, transparent=True,\n                 fromalpha=False, duration=None):\n\n        VideoClip.__init__(self, ismask=ismask, duration=duration)\n\n        if isinstance(img, str):\n            img = imread(img)\n\n        if len(img.shape) == 3:  # img is (now) a RGB(a) numpy array\n\n            if img.shape[2] == 4:\n                if fromalpha:\n                    img = 1.0 * img[:, :, 3] / 255\n                elif ismask:\n                    img = 1.0 * img[:, :, 0] / 255\n                elif transparent:\n                    self.mask = ImageClip(\n                        1.0 * img[:, :, 3] / 255, ismask=True)\n                    img = img[:, :, :3]\n            elif ismask:\n                img = 1.0 * img[:, :, 0] / 255\n\n        # if the image was just a 2D mask, it should arrive here\n        # unchanged\n        self.make_frame = lambda t: img\n        self.size = img.shape[:2][::-1]\n        self.img = img",
  "def fl(self, fl, apply_to=[], keep_duration=True):\n        \"\"\" General transformation filter.\n\n        Equivalent to VideoClip.fl . The result is no more an\n        ImageClip, it has the class VideoClip (since it may be animated)\n        \"\"\"\n\n        # When we use fl on an image clip it may become animated.\n        # Therefore the result is not an ImageClip, just a VideoClip.\n        newclip = VideoClip.fl(self, fl, apply_to=apply_to,\n                               keep_duration=keep_duration)\n        newclip.__class__ = VideoClip\n        return newclip",
  "def fl_image(self, image_func, apply_to=[]):\n        \"\"\" Image-transformation filter.\n\n        Does the same as VideoClip.fl_image, but for ImageClip the\n        tranformed clip is computed once and for all at the beginning,\n        and not for each 'frame'.\n        \"\"\"\n\n        arr = image_func(self.get_frame(0))\n        self.size = arr.shape[:2][::-1]\n        self.make_frame = lambda t: arr\n        self.img = arr\n\n        for attr in apply_to:\n            if hasattr(self, attr):\n                a = getattr(self, attr)\n                if a is not None:\n                    new_a = a.fl_image(image_func)\n                    setattr(self, attr, new_a)",
  "def fl_time(self, time_func, apply_to=['mask', 'audio'],\n                keep_duration=False):\n        \"\"\" Time-transformation filter.\n\n        Applies a transformation to the clip's timeline\n        (see Clip.fl_time).\n\n        This method does nothing for ImageClips (but it may affect their\n        masks or their audios). The result is still an ImageClip.\n        \"\"\"\n\n        for attr in apply_to:\n            if hasattr(self, attr):\n                a = getattr(self, attr)\n                if a is not None:\n                    new_a = a.fl_time(time_func)\n                    setattr(self, attr, new_a)",
  "def __init__(self, size, col=(0, 0, 0), ismask=False, duration=None):\n        w, h = size\n        shape = (h, w) if np.isscalar(col) else (h, w, len(col))\n        ImageClip.__init__(self, np.tile(col, w * h).reshape(shape),\n                           ismask=ismask, duration=duration)",
  "def __init__(self, txt=None, filename=None, size=None, color='black',\n                 bg_color='transparent', fontsize=None, font='Courier',\n                 stroke_color=None, stroke_width=1, method='label',\n                 kerning=None, align='center', interline=None,\n                 tempfilename=None, temptxt=None,\n                 transparent=True, remove_temp=True,\n                 print_cmd=False):\n\n\n        if txt is not None:\n            if temptxt is None:\n                temptxt_fd, temptxt = tempfile.mkstemp(suffix='.txt')\n                try:  # only in Python3 will this work\n                    os.write(temptxt_fd, bytes(txt, 'UTF8'))\n                except TypeError:  # oops, fall back to Python2\n                    os.write(temptxt_fd, txt)\n                os.close(temptxt_fd)\n            txt = '@' + temptxt\n        else:\n            # use a file instead of a text.\n            txt = \"@%\" + filename\n\n        if size is not None:\n            size = ('' if size[0] is None else str(size[0]),\n                    '' if size[1] is None else str(size[1]))\n\n        cmd = ( [get_setting(\"IMAGEMAGICK_BINARY\"),\n               \"-background\", bg_color,\n               \"-fill\", color,\n               \"-font\", font])\n\n        if fontsize is not None:\n            cmd += [\"-pointsize\", \"%d\" % fontsize]\n        if kerning is not None:\n            cmd += [\"-kerning\", \"%0.1f\" % kerning]\n        if stroke_color is not None:\n            cmd += [\"-stroke\", stroke_color, \"-strokewidth\",\n                    \"%.01f\" % stroke_width]\n        if size is not None:\n            cmd += [\"-size\", \"%sx%s\" % (size[0], size[1])]\n        if align is not None:\n            cmd += [\"-gravity\", align]\n        if interline is not None:\n            cmd += [\"-interline-spacing\", \"%d\" % interline]\n\n        if tempfilename is None:\n            tempfile_fd, tempfilename = tempfile.mkstemp(suffix='.png')\n            os.close(tempfile_fd)\n\n        cmd += [\"%s:%s\" % (method, txt),\n                \"-type\", \"truecolormatte\", \"PNG32:%s\" % tempfilename]\n\n        if print_cmd:\n            print( \" \".join(cmd) )\n        \n        try:\n            subprocess_call(cmd, verbose=False )\n        except (IOError,OSError) as err:\n            error = (\"MoviePy Error: creation of %s failed because \"\n              \"of the following error:\\n\\n%s.\\n\\n.\"%(filename, str(err))\n               + (\"This error can be due to the fact that \"\n                    \"ImageMagick is not installed on your computer, or \"\n                    \"(for Windows users) that you didn't specify the \"\n                    \"path to the ImageMagick binary in file conf.py, or.\"\n                    \"that the path you specified is incorrect\" ))\n            raise IOError(error)\n\n        ImageClip.__init__(self, tempfilename, transparent=transparent)\n        self.txt = txt\n        self.color = color\n        self.stroke_color = stroke_color\n\n        if remove_temp:\n            if os.path.exists(tempfilename):\n                os.remove(tempfilename)\n            if os.path.exists(temptxt):\n                os.remove(temptxt)",
  "def list(arg):\n        \"\"\" Returns the list of all valid entries for the argument of\n        ``TextClip`` given (can be ``font``, ``color``, etc...) \"\"\"\n\n        popen_params = {\"stdout\": sp.PIPE,\n                        \"stderr\": DEVNULL,\n                        \"stdin\": DEVNULL}\n\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n\n        process = sp.Popen([get_setting(\"IMAGEMAGICK_BINARY\"),\n                            '-list', arg], **popen_params)\n        result = process.communicate()[0]\n        lines = result.splitlines()\n\n        if arg == 'font':\n            return [l[8:] for l in lines if l.startswith(\"  Font:\")]\n        elif arg == 'color':\n            return [l.split(\" \")[1] for l in lines[2:]]",
  "def search(string, arg):\n        \"\"\" Returns the of all valid entries which contain ``string`` for the\n           argument ``arg`` of ``TextClip``, for instance\n\n           >>> # Find all the available fonts which contain \"Courier\"\n           >>> print ( TextClip.search('Courier', 'font') )\n \n        \"\"\"\n        string = string.lower()\n        names_list = TextClip.list(arg)\n        return [name for name in names_list if string in name.lower()]",
  "def make_frame(t):\n            while self.world.clip_t < t:\n                world.update()\n            return world.to_frame()",
  "def even_size(clip):\n    \"\"\" Crops the clip to make dimensions even.\n\n    \"\"\"\n\n    w,h = clip.size\n\n    if (w%2 == 0) and (h%2==0):\n        return clip\n    \n    if (w%2 != 0) and (h%2!=0):\n        fl_image = lambda a : a[:-1,:-1,:]\n    elif (w%2 != 0):\n        fl_image = lambda a : a[:,:-1,:]\n    else:\n        fl_image = lambda a : a[:-1,:,:]\n\n    return clip.fl_image(fl_image)",
  "def to_painting(image,saturation = 1.4,black = 0.006):\n    \"\"\" transforms any photo into some kind of painting \"\"\"\n    edges = sobel(image.mean(axis=2))\n    darkening =  black*(255*np.dstack(3*[edges]))\n    painting = saturation*image-darkening\n    return np.maximum(0,np.minimum(255,painting)).astype('uint8')",
  "def painting(clip, saturation = 1.4,black = 0.006):\n    \"\"\"\n    Transforms any photo into some kind of painting. Saturation\n    tells at which point the colors of the result should be\n    flashy. ``black`` gives the anount of black lines wanted.\n    Requires Scikit-image or Scipy installed.\n    \"\"\"\n    return clip.fl_image(lambda im : to_painting(im,saturation,black))",
  "def painting(clip, newsize=None, height=None, width=None):\n        raise IOError(\"fx painting needs scikit-image or scipy\")",
  "def f_accel_decel(t, old_d, new_d, abruptness=1, soonness=1.0):\n    \"\"\"\n    abruptness\n      negative abruptness (>-1): speed up down up\n      zero abruptness : no effect\n      positive abruptness: speed down up down\n      \n    soonness\n      for positive abruptness, determines how soon the\n      speedup occurs (0<soonness < inf)\n    \"\"\"\n    \n    a = 1.0+abruptness\n    def _f(t):\n        f1 = lambda t: (0.5)**(1-a)*(t**a)\n        f2 = lambda t: (1-f1(1-t))\n        return (t<.5)*f1(t) + (t>=.5)*f2(t) \n    \n    return old_d*_f((t/new_d)**soonness)",
  "def accel_decel(clip, new_duration=None, abruptness=1.0, soonness=1.0):\n    \"\"\"\n\n    new_duration\n      If None, will be that of the current clip.\n\n    abruptness\n      negative abruptness (>-1): speed up down up\n      zero abruptness : no effect\n      positive abruptness: speed down up down\n      \n    soonness\n      for positive abruptness, determines how soon the\n      speedup occurs (0<soonness < inf)\n    \"\"\"\n    \n    if new_duration is None:\n        new_duration = clip.duration\n    \n    fl = lambda t : f_accel_decel(t, clip.duration, new_duration,\n                                   abruptness, soonness)\n\n    return clip.fl_time(fl).set_duration(new_duration)",
  "def _f(t):\n        f1 = lambda t: (0.5)**(1-a)*(t**a)\n        f2 = lambda t: (1-f1(1-t))\n        return (t<.5)*f1(t) + (t>=.5)*f2(t)",
  "def freeze_region(clip, t=0, region=None, outside_region=None, mask=None):\n    \"\"\" Freezes one region of the clip while the rest remains animated.\n    \n    You can choose one of three methods by providing either `region`,\n    `outside_region`, or `mask`.\n\n    Parameters\n    -----------\n\n    t\n      Time at which to freeze the freezed region.\n\n    region\n      A tuple (x1, y1, x2, y2) defining the region of the screen (in pixels)\n      which will be freezed. You can provide outside_region or mask instead.\n\n    outside_region\n      A tuple (x1, y1, x2, y2) defining the region of the screen (in pixels)\n      which will be the only non-freezed region.\n\n    mask\n      If not None, will overlay a freezed version of the clip on the current clip,\n      with the provided mask. In other words, the \"visible\" pixels in the mask\n      indicate the freezed region in the final picture.\n\n    \"\"\"\n    \n    if region is not None:\n\n        x1, y1, x2, y2 = region\n        freeze = (clip.fx(crop, *region)\n                      .to_ImageClip(t=t)\n                      .set_duration(clip.duration)\n                      .set_position((x1,y1)))\n        return CompositeVideoClip([clip, freeze])\n    \n    elif outside_region is not None:\n        \n        x1, y1, x2, y2 = outside_region\n        animated_region = (clip.fx(crop, *outside_region)\n                               .set_position((x1,y1)))\n        freeze = (clip.to_ImageClip(t=t)\n                      .set_duration(clip.duration))\n        return CompositeVideoClip([freeze, animated_region])\n    \n    elif mask is not None:\n        freeze = (clip.to_ImageClip(t=t)\n                      .set_duration(clip.duration)\n                      .set_mask(mask))\n        return CompositeVideoClip([clip, freeze])",
  "def mask_color(clip, color=[0,0,0], thr=0, s=1):\n    \"\"\" Returns a new clip with a mask for transparency where the original\n    clip is of the given color.\n\n    You can also have a \"progressive\" mask by specifying a non-nul distance\n    threshold thr. In this case, if the distance between a pixel and the given\n    color is d, the transparency will be \n\n    d**s / (thr**s + d**s)\n    which is 1 when d>>thr and 0 for d<<thr, the stiffness of the effect being\n    parametrized by s\n    \"\"\"\n\n    # code a little sloppy, it just works.\n    hill = lambda x: (1.0*(x!=0) if (thr==0) else (x**s/ (thr**s+x**s)))\n    color = np.array(color)\n    flim = lambda im: hill(np.sqrt(((im-color)**2).sum(axis=2)))\n    mask = clip.fl_image(flim)\n    mask.ismask= True\n    newclip = clip.set_mask(mask)\n    return newclip",
  "def fadein(clip, duration, initial_color=None):\n    \"\"\"\n    Makes the clip progressively appear from some color (black by default),\n    over ``duration`` seconds at the beginning of the clip. Can be used for\n    masks too, where the initial color must be a number between 0 and 1.\n    For cross-fading (progressive appearance or disappearance of a clip\n    over another clip, see ``composition.crossfade``\n    \"\"\"\n\n    if initial_color is None:\n        initial_color = 0 if clip.ismask else [0,0,0]\n    \n    initial_color = np.array(initial_color)\n    \n    def fl(gf, t):\n        if t>=duration:\n            return gf(t)\n        else:\n            fading = (1.0*t/duration) \n            return fading*gf(t) + (1-fading)*initial_color\n\n    return clip.fl(fl)",
  "def fl(gf, t):\n        if t>=duration:\n            return gf(t)\n        else:\n            fading = (1.0*t/duration) \n            return fading*gf(t) + (1-fading)*initial_color",
  "def mask_or(clip, other_clip):\n    \"\"\" Returns the logical 'or' (max) between two masks.\n        other_clip can be a mask clip or a picture (np.array).\n        The result has the duration of 'clip' (if it has any)\n    \"\"\"\n\n    # To ensure that 'or' of two ImageClips will be an ImageClip.\n    if isinstance(other_clip, ImageClip):\n    \tother_clip = other_clip.img\n\n    if isinstance(other_clip, np.ndarray):\n    \treturn clip.fl_image(lambda f : np.maximum(f, other_clip))\n    else:\n    \treturn clip.fl(lambda gf, t : np.maximum(gf(t),\n                                                 other_clip.get_frame(t)))",
  "def mirror_y(clip, apply_to= \"mask\"):\n    \"\"\" flips the clip vertically (and its mask too, by default) \"\"\"\n    return clip.fl_image(lambda f : f[::-1], apply_to = apply_to)",
  "def freeze(clip, t=0, freeze_duration=None, total_duration=None,\n           padding_end=0):\n    \"\"\" Momentarily freeze the clip at time t.\n\n    Set `t='end'` to freeze the clip at the end (actually it will freeze on the\n    frame at time clip.duration - padding_end seconds).\n    With ``duration``you can specify the duration of the freeze.\n    With ``total_duration`` you can specify the total duration of\n    the clip and the freeze (i.e. the duration of the freeze is\n    automatically calculated). One of them must be provided.\n    \"\"\"\n\n    if t=='end':\n        t = clip.duration - padding_end\n\n    if freeze_duration is None:\n        freeze_duration = total_duration - clip.duration\n\n    before = [clip.subclip(0,t)] if (t!=0) else []\n    freeze = [clip.to_ImageClip(t).set_duration(freeze_duration)]\n    after = [clip.subclip(t)] if (t !=clip.duration) else []\n    return concatenate_videoclips(before+freeze+after)",
  "def make_loopable(clip, cross):\n    \"\"\"\n    Makes the clip fade in progressively at its own end, this way\n    it can be looped indefinitely. ``cross`` is the duration in seconds\n    of the fade-in.  \"\"\"  \n    d = clip.duration\n    clip2 = clip.fx(transfx.crossfadein, cross).\\\n                 set_start(d - cross)\n    return CompositeVideoClip([ clip, clip2 ]).\\\n                 subclip(cross,d)",
  "def time_symmetrize(clip):\n    \"\"\"\n    Returns a clip that plays the current clip once forwards and\n    then once backwards. This is very practival to make video that\n    loop well, e.g. to create animated GIFs.\n    This effect is automatically applied to the clip's mask and audio\n    if they exist.\n    \"\"\"\n    return concatenate_videoclips([clip, clip.fx( time_mirror )])",
  "def scroll(clip, h=None, w=None, x_speed=0, y_speed=0,\n           x_start=0, y_start=0, apply_to=\"mask\"):\n    \"\"\" Scrolls horizontally or vertically a clip, e.g. to make end\n        credits \"\"\"\n    if h is None: h = clip.h\n    if w is None: w = clip.w\n    \n    xmax = clip.w-w-1\n    ymax = clip.h-h-1\n\n    def f(gf,t):\n        x = max(0, min(xmax, x_start+ np.round(x_speed*t)))\n        y = max(0, min(ymax, y_start+ np.round(y_speed*t)))\n        return gf(t)[y:y+h, x:x+w]\n    \n    return clip.fl(f, apply_to = apply_to)",
  "def f(gf,t):\n        x = max(0, min(xmax, x_start+ np.round(x_speed*t)))\n        y = max(0, min(ymax, y_start+ np.round(y_speed*t)))\n        return gf(t)[y:y+h, x:x+w]",
  "def crop(clip, x1=None, y1=None, x2=None, y2=None,\n         width = None, height=None,\n         x_center= None, y_center=None):\n    \"\"\"\n    Returns a new clip in which just a rectangular subregion of the\n    original clip is conserved. x1,y1 indicates the top left corner and\n    x2,y2 is the lower right corner of the croped region.\n    All coordinates are in pixels. Float numbers are accepted.\n    \n    To crop an arbitrary rectangle:\n    \n    >>> crop(clip, x1=50, y1=60, x2=460, y2=275)\n    \n    Only remove the part above y=30:\n    \n    >>> crop(clip, y1=30)\n    \n    Crop a rectangle that starts 10 pixels left and is 200px wide\n    \n    >>> crop(clip, x1=10, width=200)\n    \n    Crop a rectangle centered in x,y=(300,400), width=50, height=150 :\n    \n    >>> crop(clip,  x_center=300 , y_center=400,\n                        width=50, height=150)\n    \n    Any combination of the above should work, like for this rectangle\n    centered in x=300, with explicit y-boundaries:\n    \n    >>> crop(x_center=300, width=400, y1=100, y2=600)\n    \n    \"\"\"\n    \n    \n    if width and (x1 is not None or x2 is not None):\n        if x1 is not None:\n            x2 = x1+width\n        else:\n            x1 = x2-width\n    \n    if height and (y1 is not None or y2 is not None):\n        if y1 is not None:\n            y2 = y1+height\n        else:\n            y1 = y2 - height\n    \n    if x_center:\n        x1, x2 = x_center - width/2, x_center + width/2\n    \n    if y_center:\n        y1, y2 = y_center - height/2, y_center + height/2\n    \n    if x1 is None:\n        x1 = 0\n    if y1 is None:\n        y1 = 0\n    if x2 is None:\n        x2 = clip.size[0]\n    if y2 is None:\n        y2 = clip.size[1]\n    \n    return clip.fl_image(\n            lambda pic: pic[int(y1):int(y2), int(x1):int(x2)],\n            apply_to=['mask'])",
  "def rotate(clip, angle, unit='deg', resample=\"bicubic\", expand=True):\n    \"\"\"\n    Change unit to 'rad' to define angles as radians.\n    If the angle is not one of 90, 180, -90, -180 (degrees) there will be\n    black borders. You can make them transparent with\n\n    >>> newclip = clip.add_mask().rotate(72)\n\n    Parameters\n    ===========\n\n    clip\n      A video clip\n\n    angle\n      Either a value or a function angle(t) representing the angle of rotation\n\n    unit\n      Unit of parameter `angle` (either `deg` for degrees or `rad` for radians)\n\n    resample\n      One of \"nearest\", \"bilinear\", or \"bicubic\".\n\n    expand\n      Only applIf False, the clip will maintain the same True, the clip will be resized so that the whole\n    \"\"\"\n    \n    resample = {\"bilinear\": Image.BILINEAR,\n                \"nearest\": Image.NEAREST,\n                \"bicubic\": Image.BICUBIC}[resample]\n\n    if not hasattr(angle, '__call__'):\n        # if angle is a constant, convert to a constant function\n        a = +angle\n        angle = lambda t: a\n\n    transpo = [1,0] if clip.ismask else [1,0,2]\n\n    def fl(gf, t):\n\n        a = angle(t)\n        im = gf(t)\n\n        if unit == 'rad':\n            a = 360.0*a/(2*np.pi)\n        \n        if (a==90) and expand:\n            return np.transpose(im, axes=transpo)[::-1]\n        elif (a==-90) and expand:\n            return np.transpose(im, axes=transpo)[:,::-1]\n        elif (a in [180, -180]) and expand:\n            return im[::-1,::-1]\n        elif not PIL_FOUND:\n            raise ValueError('Without \"Pillow\" installed, only angles 90, -90,'\n                             '180 are supported, please install \"Pillow\" with'\n                             \"pip install pillow\")\n        else:\n            return pil_rotater(im, a, resample=resample, expand=expand)\n\n    return clip.fl(fl, apply_to=[\"mask\"])",
  "def pil_rotater(pic, angle, resample, expand):\n        return np.array( Image.fromarray(pic).rotate(angle, expand=expand,\n                                                     resample=resample))",
  "def fl(gf, t):\n\n        a = angle(t)\n        im = gf(t)\n\n        if unit == 'rad':\n            a = 360.0*a/(2*np.pi)\n        \n        if (a==90) and expand:\n            return np.transpose(im, axes=transpo)[::-1]\n        elif (a==-90) and expand:\n            return np.transpose(im, axes=transpo)[:,::-1]\n        elif (a in [180, -180]) and expand:\n            return im[::-1,::-1]\n        elif not PIL_FOUND:\n            raise ValueError('Without \"Pillow\" installed, only angles 90, -90,'\n                             '180 are supported, please install \"Pillow\" with'\n                             \"pip install pillow\")\n        else:\n            return pil_rotater(im, a, resample=resample, expand=expand)",
  "def speedx(clip, factor = None, final_duration=None):\n    \"\"\"\n    Returns a clip playing the current clip but at a speed multiplied\n    by ``factor``. Instead of factor one can indicate the desired\n    ``final_duration`` of the clip, and the factor will be automatically\n    computed.\n    The same effect is applied to the clip's audio and mask if any.\n    \"\"\"\n    \n    if final_duration:\n        factor = 1.0* clip.duration / final_duration\n        \n    newclip = clip.fl_time(lambda t: factor * t, apply_to=['mask', 'audio'])\n    \n    if clip.duration is not None:\n        newclip = newclip.set_duration(1.0 * clip.duration / factor)\n    \n    return newclip",
  "def blackwhite(clip, RGB = [1,1,1], preserve_luminosity=True):\n    \"\"\" Desaturates the picture, makes it black and white.\n    Parameter RGB allows to set weights for the different color\n    channels.\n    If RBG is 'CRT_phosphor' a special set of values is used.\n    preserve_luminosity maintains the sum of RGB to 1.\"\"\"\n\n    if RGB == 'CRT_phosphor':\n        RGB = [0.2125, 0.7154, 0.0721]\n\n    R,G,B = 1.0*np.array(RGB)/ (sum(RGB) if preserve_luminosity else 1)\n    \n    def fl(im):\n        im = (R*im[:,:,0] + G*im[:,:,1] + B*im[:,:,2])\n        return np.dstack(3*[im]).astype('uint8')\n\n    return clip.fl_image(fl)",
  "def fl(im):\n        im = (R*im[:,:,0] + G*im[:,:,1] + B*im[:,:,2])\n        return np.dstack(3*[im]).astype('uint8')",
  "def loop(self, n=None, duration=None):\n    \"\"\"\n    Returns a clip that plays the current clip in an infinite loop.\n    Ideal for clips coming from gifs.\n    \n    Parameters\n    ------------\n    n\n      Number of times the clip should be played. If `None` the\n      the clip will loop indefinitely (i.e. with no set duration).\n\n    duration\n      Total duration of the clip. Can be specified instead of n.\n    \"\"\"\n    result = self.fl_time(lambda t: t % self.duration)\n    if n:\n        duration = n*self.duration\n    if duration:\n        result = result.set_duration(duration)\n    return result",
  "def headblur(clip,fx,fy,r_zone,r_blur=None):\n    \"\"\"\n    Returns a filter that will blurr a moving part (a head ?) of\n    the frames. The position of the blur at time t is\n    defined by (fx(t), fy(t)), the radius of the blurring\n    by ``r_zone`` and the intensity of the blurring by ``r_blur``.\n    Requires OpenCV for the circling and the blurring.\n    Automatically deals with the case where part of the image goes\n    offscreen.\n    \"\"\"\n    \n    if r_blur is None: r_blur = 2*r_zone/3\n    \n    def fl(gf,t):\n        \n        im = gf(t)\n        h,w,d = im.shape\n        x,y = int(fx(t)),int(fy(t))\n        x1,x2 = max(0,x-r_zone),min(x+r_zone,w)\n        y1,y2 = max(0,y-r_zone),min(y+r_zone,h)\n        region_size = y2-y1,x2-x1\n        \n        mask = np.zeros(region_size).astype('uint8')\n        cv2.circle(mask, (r_zone,r_zone), r_zone, 255, -1,\n                   lineType=cv2.CV_AA)\n                               \n        mask = np.dstack(3*[(1.0/255)*mask])\n        \n        orig = im[y1:y2, x1:x2]\n        blurred = cv2.blur(orig,(r_blur, r_blur))\n        im[y1:y2, x1:x2] = mask*blurred + (1-mask)*orig\n        return im\n    \n    return clip.fl(fl)",
  "def fl(gf,t):\n        \n        im = gf(t)\n        h,w,d = im.shape\n        x,y = int(fx(t)),int(fy(t))\n        x1,x2 = max(0,x-r_zone),min(x+r_zone,w)\n        y1,y2 = max(0,y-r_zone),min(y+r_zone,h)\n        region_size = y2-y1,x2-x1\n        \n        mask = np.zeros(region_size).astype('uint8')\n        cv2.circle(mask, (r_zone,r_zone), r_zone, 255, -1,\n                   lineType=cv2.CV_AA)\n                               \n        mask = np.dstack(3*[(1.0/255)*mask])\n        \n        orig = im[y1:y2, x1:x2]\n        blurred = cv2.blur(orig,(r_blur, r_blur))\n        im[y1:y2, x1:x2] = mask*blurred + (1-mask)*orig\n        return im",
  "def headblur(clip,fx,fy,r_zone,r_blur=None):\n        raise IOError(\"fx painting needs scikit-image or scipy\")",
  "def margin(clip, mar=None, left=0, right=0, top=0,\n           bottom=0, color=(0, 0, 0), opacity = 1.0):\n    \"\"\"\n    Draws an external margin all around the frame.\n    \n    :param mar: if not ``None``, then the new clip has a margin of\n        size ``mar`` in pixels on the left, right, top, and bottom.\n        \n    :param left, right, top, bottom: width of the margin in pixel\n        in these directions.\n        \n    :param color: color of the margin.\n    \n    :param mask_margin: value of the mask on the margin. Setting\n        this value to 0 yields transparent margins.\n    \n    \"\"\"\n\n    if (opacity != 1.0) and (clip.mask is None) and not (clip.ismask):\n        clip = clip.add_mask()\n\n    if mar is not None:\n        left = right = top = bottom = mar\n    \n    def make_bg(w,h):\n        new_w, new_h = w + left + right, h + top + bottom\n        if clip.ismask:\n            shape = (new_h, new_w)\n            bg = ( np.tile(opacity, (new_h, new_w))\n                       .astype(float)\n                       .reshape(shape))\n        else:\n            shape = (new_h, new_w, 3)\n            bg = np.tile(color, (new_h, new_w)).reshape(shape)\n        return bg\n        \n    if isinstance(clip, ImageClip):\n        \n        im =  make_bg(clip.w,clip.h)\n        im[top:top + clip.h, left:left + clip.w] = clip.img\n        return clip.fl_image(lambda pic:im)\n        \n    else:\n        \n        def fl(gf, t):\n            pic = gf(t)\n            h,w = pic.shape[:2]\n            im = make_bg(w,h)\n            im[top:top + h, left:left + w] = pic\n            return im\n        return clip.fl(fl)",
  "def make_bg(w,h):\n        new_w, new_h = w + left + right, h + top + bottom\n        if clip.ismask:\n            shape = (new_h, new_w)\n            bg = ( np.tile(opacity, (new_h, new_w))\n                       .astype(float)\n                       .reshape(shape))\n        else:\n            shape = (new_h, new_w, 3)\n            bg = np.tile(color, (new_h, new_w)).reshape(shape)\n        return bg",
  "def fl(gf, t):\n            pic = gf(t)\n            h,w = pic.shape[:2]\n            im = make_bg(w,h)\n            im[top:top + h, left:left + w] = pic\n            return im",
  "def resize(clip, newsize=None, height=None, width=None, apply_to_mask=True):\n    \"\"\" \n    Returns a video clip that is a resized version of the clip.\n    \n    Parameters\n    ------------\n    \n    newsize:\n      Can be either \n        - ``(height,width)`` in pixels or a float representing\n        - A scaling factor, like 0.5\n        - A function of time returning one of these.\n            \n    width:\n      width of the new clip in pixel. The height is then computed so\n      that the width/height ratio is conserved. \n            \n    height:\n      height of the new clip in pixel. The width is then computed so\n      that the width/height ratio is conserved.\n    \n    Examples\n    ----------\n             \n    >>> myClip.resize( (460,720) ) # New resolution: (460,720)\n    >>> myClip.resize(0.6) # width and heigth multiplied by 0.6\n    >>> myClip.resize(width=800) # height computed automatically.\n    >>> myClip.resize(lambda t : 1+0.02*t) # slow swelling of the clip\n    \n    \"\"\"\n\n    w, h = clip.size\n    \n    if newsize is not None:\n        \n        def trans_newsize(ns):\n            \n            if isinstance(ns, (int, float)):\n                return [ns * w, ns * h]\n            else:\n                return ns\n                \n        if hasattr(newsize, \"__call__\"):\n            \n            newsize2 = lambda t : trans_newsize(newsize(t))\n            \n            if clip.ismask:\n                \n                fun = lambda gf,t: (1.0*resizer((255 * gf(t)).astype('uint8'),\n                                                 newsize2(t))/255)\n            else:\n                \n                fun = lambda gf,t: resizer(gf(t).astype('uint8'),\n                                          newsize2(t))\n                \n            return clip.fl(fun, keep_duration=True,\n                           apply_to= ([\"mask\"] if apply_to_mask else []))\n            \n        else:\n            \n            newsize = trans_newsize(newsize)\n        \n\n    elif height is not None:\n        \n        if hasattr(height, \"__call__\"):\n            fun = lambda t : 1.0*int(height(t))/h\n            return resize(clip, fun)\n\n\n        else:\n\n            newsize = [w * height / h, height]\n        \n    elif width is not None:\n\n        if hasattr(width, \"__call__\"):\n            fun = lambda t : 1.0*width(t)/w\n            return resize(clip, fun)\n        \n        newsize = [width, h * width / w]\n        \n        \n    # From here, the resizing is constant (not a function of time), size=newsize\n\n    if clip.ismask:\n        fl = lambda pic: 1.0*resizer((255 * pic).astype('uint8'), newsize)/255.0\n            \n    else:\n        fl = lambda pic: resizer(pic.astype('uint8'), newsize)\n\n    newclip = clip.fl_image(fl)\n\n    if apply_to_mask and clip.mask is not None:\n        newclip.mask = resize(clip.mask, newsize, apply_to_mask=False)\n\n    return newclip",
  "def resizer (pic, newsize):\n        lx, ly = int(newsize[0]), int(newsize[1])\n        if lx > pic.shape[1] or ly > pic.shape[0]:\n            # For upsizing use linear for good quality & decent speed\n            interpolation = cv2.INTER_LINEAR\n        else:\n            # For dowsizing use area to prevent aliasing\n            interpolation = cv2.INTER_AREA\n        return cv2.resize(+pic.astype('uint8'), (lx, ly),\n                          interpolation=interpolation)",
  "def resize(clip, newsize=None, height=None, width=None):\n        raise ImportError(\"fx resize needs OpenCV or Scipy or PIL\")",
  "def trans_newsize(ns):\n            \n            if isinstance(ns, (int, float)):\n                return [ns * w, ns * h]\n            else:\n                return ns",
  "def resizer(pic, newsize):\n            newsize = list(map(int, newsize))[::-1]\n            shape = pic.shape\n            if len(shape)==3:\n                newshape = (newsize[0],newsize[1], shape[2] )\n            else:\n                newshape = (newsize[0],newsize[1])\n                \n            pilim = Image.fromarray(pic)\n            resized_pil = pilim.resize(newsize[::-1], Image.ANTIALIAS)\n            #arr = np.fromstring(resized_pil.tostring(), dtype='uint8')\n            #arr.reshape(newshape)\n            return np.array(resized_pil)",
  "def gamma_corr(clip, gamma):\n    \"\"\" Gamma-correction of a video clip \"\"\"\n    def fl(im):\n        corrected = (255*(1.0*im/255)**gamma)\n        return corrected.astype('uint8')\n    \n    return clip.fl_image(fl)",
  "def fl(im):\n        corrected = (255*(1.0*im/255)**gamma)\n        return corrected.astype('uint8')",
  "def colorx(clip, factor):\n    \"\"\" multiplies the clip's colors by the given factor, can be used\n        to decrease or increase the clip's brightness (is that the\n        reight word ?)\n    \"\"\"\n    return clip.fl_image( lambda pic: np.minimum(255,(factor*pic)).\n                                                        astype('uint8'))",
  "def supersample(clip, d, nframes):\n    \"\"\" Replaces each frame at time t by the mean of `nframes` equally spaced frames\n    taken in the interval [t-d, t+d]. This results in motion blur.\"\"\"\n    \n    def fl(gf, t):\n        tt = np.linspace(t-d, t+d, nframes)\n        avg = np.mean(1.0*np.array([gf(t_) for t_ in tt], dtype='uint16'), axis=0)\n        return avg.astype(\"uint8\")\n\n    return clip.fl(fl)",
  "def fl(gf, t):\n        tt = np.linspace(t-d, t+d, nframes)\n        avg = np.mean(1.0*np.array([gf(t_) for t_ in tt], dtype='uint16'), axis=0)\n        return avg.astype(\"uint8\")",
  "def blink(clip, d_on, d_off):\n    \"\"\"\n    Makes the clip blink. At each blink it will be displayed ``d_on``\n    seconds and disappear ``d_off`` seconds. Will only work in\n    composite clips.\n    \"\"\"\n    newclip = copy(clip)\n    if newclip.mask is None:\n        newclip = newclip.with_mask()\n    D = d_on + d_off\n    newclip.mask = newclip.mask.fl( lambda gf,t: gf(t)*((t % D) < d_on))\n    return newclip",
  "def lum_contrast(clip, lum = 0, contrast=0, contrast_thr=127):\n    \"\"\" luminosity-contrast correction of a clip \"\"\"\n    \n    def fl_image(im):\n        im = 1.0*im # float conversion\n        corrected = im + lum + contrast*(im-float(contrast_thr))\n        corrected[corrected < 0] = 0\n        corrected[corrected > 255] = 255\n        return corrected.astype('uint8')\n    \n    return clip.fl_image(fl_image)",
  "def fl_image(im):\n        im = 1.0*im # float conversion\n        corrected = im + lum + contrast*(im-float(contrast_thr))\n        corrected[corrected < 0] = 0\n        corrected[corrected > 255] = 255\n        return corrected.astype('uint8')",
  "def mask_and(clip, other_clip):\n    \"\"\" Returns the logical 'and' (min) between two masks.\n        other_clip can be a mask clip or a picture (np.array).\n        The result has the duration of 'clip' (if it has any)\n    \"\"\"\n\n    # To ensure that 'or' of two ImageClips will be an ImageClip.\n    if isinstance(other_clip, ImageClip):\n    \tother_clip = other_clip.img\n\n    if isinstance(other_clip, np.ndarray):\n    \treturn clip.fl_image(lambda f : np.minimum(f, other_clip))\n    else:\n    \treturn clip.fl(lambda gf, t : np.minimum(gf(t),\n    \t\t                                     other_clip.get_frame(t)))",
  "def mirror_x(clip, apply_to= \"mask\"):\n    \"\"\" flips the clip horizontally (and its mask too, by default) \"\"\"\n    return clip.fl_image(lambda f: f[:,::-1], apply_to = apply_to)",
  "def invert_colors(clip):\n    \"\"\" Returns the color-inversed clip.\n\n    The values of all pixels are replaced with (255-v) or (1-v) for masks \n    Black becomes white, green becomes purple, etc.\n    \"\"\"\n    maxi = (1.0 if clip.ismask else 255)\n    return clip.fl_image(lambda f : maxi - f)",
  "def time_mirror(self):\n    \"\"\"\n    Returns a clip that plays the current clip backwards.\n    The clip must have its ``duration`` attribute set.\n    The same effect is applied to the clip's audio and mask if any.\n    \"\"\"\n    return self.fl_time(lambda t: self.duration - t, keep_duration=True)",
  "def fadeout(clip, duration, final_color=None):\n    \"\"\"\n    Makes the clip progressively fade to some color (black by default),\n    over ``duration`` seconds at the end of the clip. Can be used for\n    masks too, where the final color must be a number between 0 and 1.\n    For cross-fading (progressive appearance or disappearance of a clip\n    over another clip, see ``composition.crossfade``\n    \"\"\"\n    \n    if final_color is None:\n        final_color = 0 if clip.ismask else [0,0,0]\n    \n    final_color = np.array(final_color)\n\n    def fl(gf, t):\n        if (clip.duration-t)>=duration:\n            return gf(t)\n        else:\n            fading = 1.0 * (clip.duration - t) / duration\n            return fading*gf(t) + (1-fading)*final_color\n\n    return clip.fl(fl)",
  "def fl(gf, t):\n        if (clip.duration-t)>=duration:\n            return gf(t)\n        else:\n            fading = 1.0 * (clip.duration - t) / duration\n            return fading*gf(t) + (1-fading)*final_color",
  "def on_color(clip, size=None, color=(0, 0, 0), pos=None, col_opacity=None):\n    \"\"\" \n    Returns a clip made of the current clip overlaid on a color\n    clip of a possibly bigger size. Can serve to flatten transparent\n    clips (ideal for previewing clips with masks).\n    \n    :param size: size of the final clip. By default it will be the\n       size of the current clip.\n    :param bg_color: the background color of the final clip\n    :param pos: the position of the clip in the final clip.\n    :param col_opacity: should the added zones be transparent ?\n    \"\"\"\n    \n    if size is None:\n        size = clip.size\n    if pos is None:\n        pos = 'center'\n    colorclip = ColorClip(size, color)\n    if col_opacity:\n        colorclip = colorclip.with_mask().set_opacity(col_opacity)\n\n    return CompositeVideoClip([colorclip, clip.set_pos(pos)],\n                              transparent=(col_opacity is not None))",
  "def concatenate_videoclips(clips, method=\"chain\", transition=None,\n                           bg_color=None, ismask=False, padding = 0):\n    \"\"\" Concatenates several video clips\n    \n    Returns a video clip made by clip by concatenating several video clips.\n    (Concatenated means that they will be played one after another).\n    \n    There are two methods:\n\n    - method=\"chain\": will produce a clip that simply outputs\n      the frames of the succesive clips, without any correction if they are\n      not of the same size of anything. If none of the clips have masks the\n      resulting clip has no mask, else the mask is a concatenation of masks\n      (using completely opaque for clips that don't have masks, obviously).\n      If you have clips of different size and you want to write directly the\n      result of the concatenation to a file, use the method \"compose\" instead.\n\n    - method=\"compose\", if the clips do not have the same\n      resolution, the final resolution will be such that no clip has\n       to be resized.\n       As a consequence the final clip has the height of the highest\n       clip and the width of the widest clip of the list. All the\n       clips with smaller dimensions will appear centered. The border\n       will be transparent if mask=True, else it will be of the\n       color specified by ``bg_color``.\n\n    If all clips with a fps attribute have the same fps, it becomes the fps of\n    the result.\n\n    Parameters\n    -----------\n\n    clips\n      A list of video clips which must all have their ``duration``\n      attributes set.\n\n    method\n      \"chain\" or \"compose\": see above.\n\n    transition\n      A clip that will be played between each two clips of the list.\n    \n    bg_color\n      Only for method='compose'. Color of the background.\n      Set to None for a transparent clip\n    \n    padding\n      Only for method='compose'. Duration during two consecutive clips.\n      Note that for negative padding, a clip will partly play at the same\n      time as the clip it follows (negative padding is cool for clips who fade\n      in on one another). A non-null padding automatically sets the method to\n      `compose`.\n           \n    \"\"\"\n\n    if transition is not None:\n        l = [[v, transition] for v in clips[:-1]]\n        clips = reduce(lambda x, y: x + y, l) + [clips[-1]]\n        transition = None\n\n    \n    tt = np.cumsum([0] + [c.duration for c in clips])\n\n    sizes = [v.size for v in clips]\n\n\n    w = max([r[0] for r in sizes])\n    h = max([r[1] for r in sizes])\n\n    tt = np.maximum(0, tt + padding*np.arange(len(tt)))\n    \n    if method == \"chain\":\n        def make_frame(t):\n            i = max([i for i, e in enumerate(tt) if e <= t])\n            return clips[i].get_frame(t - tt[i])\n        \n        result = VideoClip(ismask = ismask, make_frame = make_frame)\n        if any([c.mask is not None for c in clips]):\n            masks = [c.mask if (c.mask is not None) else\n                     ColorClip([1,1], col=1, ismask=True, duration=c.duration)\n                 #ColorClip(c.size, col=1, ismask=True).set_duration(c.duration)\n                     for c in clips]\n            result.mask = concatenate_videoclips(masks, method=\"chain\", ismask=True)\n            result.clips = clips\n\n\n    elif method == \"compose\":\n        result = CompositeVideoClip( [c.set_start(t).set_pos('center')\n                                for (c, t) in zip(clips, tt)],\n               size = (w, h), bg_color=bg_color, ismask=ismask)\n\n    result.tt = tt\n    \n    result.start_times = tt[:-1]\n    result.start, result.duration, result.end = 0, tt[-1] , tt[-1]\n    \n    audio_t = [(c.audio,t) for c,t in zip(clips,tt) if c.audio is not None]\n    if len(audio_t)>0:\n        result.audio = CompositeAudioClip([a.set_start(t)\n                                for a,t in audio_t])\n\n    fps_list = list(set([c.fps for c in clips if hasattr(c,'fps')]))\n    if len(fps_list)==1:\n        result.fps= fps_list[0]\n\n    return result",
  "def make_frame(t):\n            i = max([i for i, e in enumerate(tt) if e <= t])\n            return clips[i].get_frame(t - tt[i])",
  "def crossfadein(clip, duration):\n    \"\"\" Makes the clip appear progressively, over ``duration`` seconds.\n    Only works when the clip is included in a CompositeVideoClip.\n    \"\"\"\n    newclip = clip.copy()\n    newclip.mask = clip.mask.fx(fadein, duration)\n    return newclip",
  "def crossfadeout(clip, duration):\n    \"\"\" Makes the clip disappear progressively, over ``duration`` seconds.\n    Only works when the clip is included in a CompositeVideoClip.\n    \"\"\"\n    newclip = clip.copy()\n    newclip.mask = clip.mask.fx(fadeout, duration)\n    return newclip",
  "def slide_in(clip, duration, side):\n    \"\"\" Makes the clip arrive from one side of the screen.\n\n    Only works when the clip is included in a CompositeVideoClip,\n    and if the clip has the same size as the whole composition.\n\n    Parameters\n    ===========\n    \n    clip\n      A video clip.\n\n    duration\n      Time taken for the clip to be fully visible\n\n    side\n      Side of the screen where the clip comes from. One of\n      'top' | 'bottom' | 'left' | 'right'\n    \n    Examples\n    =========\n    \n    >>> from moviepy.editor import *\n    >>> clips = [... make a list of clips]\n    >>> slided_clips = [clip.fx( transfx.slide_in, 1, 'left')\n                        for clip in clips]\n    >>> final_clip = concatenate( slided_clips, padding=-1)\n\n    \"\"\"\n    w,h = clip.size\n    pos_dict = {'left' : lambda t: (min(0,w*(t/duration-1)),'center'),\n                'right' : lambda t: (max(0,w*(1-t/duration)),'center'),\n                'top' : lambda t: ('center',min(0,h*(t/duration-1))),\n                'bottom': lambda t: ('center',max(0,h*(1-t/duration)))}\n    \n    return clip.set_pos( pos_dict[side] )",
  "def slide_out(clip, duration, side):\n    \"\"\" Makes the clip go away by one side of the screen.\n\n    Only works when the clip is included in a CompositeVideoClip,\n    and if the clip has the same size as the whole composition.\n\n    Parameters\n    ===========\n    \n    clip\n      A video clip.\n\n    duration\n      Time taken for the clip to fully disappear.\n\n    side\n      Side of the screen where the clip goes. One of\n      'top' | 'bottom' | 'left' | 'right'\n    \n    Examples\n    =========\n    \n    >>> from moviepy.editor import *\n    >>> clips = [... make a list of clips]\n    >>> slided_clips = [clip.fx( transfx.slide_out, 1, 'bottom')\n                        for clip in clips]\n    >>> final_clip = concatenate( slided_clips, padding=-1)\n\n    \"\"\"\n\n    w,h = clip.size\n    t_s = clip.duration - duration # start time of the effect.\n    pos_dict = {'left' : lambda t: (min(0,w*(1-(t-ts)/duration)),'center'),\n                'right' : lambda t: (max(0,w*((t-ts)/duration-1)),'center'),\n                'top' : lambda t: ('center',min(0,h*(1-(t-ts)/duration))),\n                'bottom': lambda t: ('center',max(0,h*((t-ts)/duration-1))) }\n    \n    return clip.set_pos( pos_dict[side] )",
  "def make_loopable(clip, cross_duration):\n    \"\"\" Makes the clip fade in progressively at its own end, this way\n    it can be looped indefinitely. ``cross`` is the duration in seconds\n    of the fade-in.  \"\"\"  \n    d = clip.duration\n    clip2 = clip.fx(crossfadein, cross_duration).\\\n                 set_start(d - cross_duration)\n    return CompositeVideoClip([ clip, clip2 ]).\\\n                 subclip(cross_duration,d)",
  "class CompositeVideoClip(VideoClip):\n\n    \"\"\" \n    \n    A VideoClip made of other videoclips displayed together. This is the\n    base class for most compositions.\n    \n    Parameters\n    ----------\n\n    size\n      The size (height x width) of the final clip.\n\n    clips\n      A list of videoclips. Each clip of the list will\n      be displayed below the clips appearing after it in the list.\n      For each clip:\n       \n      - The attribute ``pos`` determines where the clip is placed.\n          See ``VideoClip.set_pos``\n      - The mask of the clip determines which parts are visible.\n        \n      Finally, if all the clips in the list have their ``duration``\n      attribute set, then the duration of the composite video clip\n      is computed automatically\n\n    bg_color\n      Color for the unmasked and unfilled regions. Set to None for these\n      regions to be transparent (will be slower).\n\n    use_bgclip\n      Set to True if the first clip in the list should be used as the\n      'background' on which all other clips are blitted. That first clip must\n      have the same size as the final clip. If it has no transparency, the final\n      clip will have no mask. \n    \n    If all clips with a fps attribute have the same fps, it becomes the fps of\n    the result.\n\n    \"\"\"\n\n    def __init__(self, clips, size=None, bg_color=None, use_bgclip=False,\n                 ismask=False):\n\n        if size is None:\n            size = clips[0].size\n\n        \n        if use_bgclip and (clips[0].mask is None):\n            transparent = False\n        else:\n            transparent = (bg_color is None)\n        \n        if bg_color is None:\n            bg_color = 0.0 if ismask else (0, 0, 0)\n\n        \n        fps_list = list(set([c.fps for c in clips if hasattr(c,'fps')]))\n        if len(fps_list)==1:\n            self.fps= fps_list[0]\n\n        VideoClip.__init__(self)\n        \n        self.size = size\n        self.ismask = ismask\n        self.clips = clips\n        self.bg_color = bg_color\n\n        if use_bgclip:\n            self.bg = clips[0]\n            self.clips = clips[1:]\n        else:\n            self.clips = clips\n            self.bg = ColorClip(size, col=self.bg_color)\n\n        \n        \n        # compute duration\n        ends = [c.end for c in self.clips]\n        if not any([(e is None) for e in ends]):\n            self.duration = max(ends)\n            self.end = max(ends)\n\n        # compute audio\n        audioclips = [v.audio for v in self.clips if v.audio is not None]\n        if len(audioclips) > 0:\n            self.audio = CompositeAudioClip(audioclips)\n\n        # compute mask if necessary\n        if transparent:\n            maskclips = [(c.mask if (c.mask is not None) else\n                          c.add_mask().mask).set_pos(c.pos)\n                          for c in self.clips]\n\n            self.mask = CompositeVideoClip(maskclips,self.size, ismask=True,\n                                               bg_color=0.0)\n\n        def make_frame(t):\n            \"\"\" The clips playing at time `t` are blitted over one\n                another. \"\"\"\n\n            f = self.bg.get_frame(t)\n            for c in self.playing_clips(t):\n                    f = c.blit_on(f, t)\n            return f\n\n        self.make_frame = make_frame\n\n    def playing_clips(self, t=0):\n        \"\"\" Returns a list of the clips in the composite clips that are\n            actually playing at the given time `t`. \"\"\"\n        return [c for c in self.clips if c.is_playing(t)]",
  "def clips_array(array, rows_widths=None, cols_widths=None,\n                bg_color = None):\n\n    \"\"\"\n\n    rows_widths\n      widths of the different rows in pixels. If None, is set automatically.\n\n    cols_widths\n      widths of the different colums in pixels. If None, is set automatically.\n\n    cols_widths\n    \n    bg_color\n       Fill color for the masked and unfilled regions. Set to None for these\n       regions to be transparent (will be slower).\n\n    \"\"\"\n    \n    array = np.array(array)\n    sizes_array = np.array([[c.size for c in line] for line in array])\n    \n    # find row width and col_widths automatically if not provided\n    if rows_widths is None:\n        rows_widths = sizes_array[:,:,1].max(axis=1)\n    if cols_widths is None:\n        cols_widths = sizes_array[:,:,0].max(axis=0)\n    \n    xx = np.cumsum([0]+list(cols_widths)) \n    yy = np.cumsum([0]+list(rows_widths))\n    \n    for j,(x,cw) in list(enumerate(zip(xx[:-1],cols_widths))):\n        for i,(y,rw) in list(enumerate(zip(yy[:-1],rows_widths))):\n            clip = array[i,j]\n            w,h = clip.size\n            if (w < cw) or (h < rw):\n                clip = (CompositeVideoClip([clip.set_pos('center')],\n                                          size = (cw,rw),\n                                          bg_color = bg_color).\n                                     set_duration(clip.duration))\n                \n            array[i,j] = clip.set_pos((x,y))\n                 \n    return CompositeVideoClip(array.flatten(), size = (xx[-1],yy[-1]),\n                              bg_color = bg_color)",
  "def __init__(self, clips, size=None, bg_color=None, use_bgclip=False,\n                 ismask=False):\n\n        if size is None:\n            size = clips[0].size\n\n        \n        if use_bgclip and (clips[0].mask is None):\n            transparent = False\n        else:\n            transparent = (bg_color is None)\n        \n        if bg_color is None:\n            bg_color = 0.0 if ismask else (0, 0, 0)\n\n        \n        fps_list = list(set([c.fps for c in clips if hasattr(c,'fps')]))\n        if len(fps_list)==1:\n            self.fps= fps_list[0]\n\n        VideoClip.__init__(self)\n        \n        self.size = size\n        self.ismask = ismask\n        self.clips = clips\n        self.bg_color = bg_color\n\n        if use_bgclip:\n            self.bg = clips[0]\n            self.clips = clips[1:]\n        else:\n            self.clips = clips\n            self.bg = ColorClip(size, col=self.bg_color)\n\n        \n        \n        # compute duration\n        ends = [c.end for c in self.clips]\n        if not any([(e is None) for e in ends]):\n            self.duration = max(ends)\n            self.end = max(ends)\n\n        # compute audio\n        audioclips = [v.audio for v in self.clips if v.audio is not None]\n        if len(audioclips) > 0:\n            self.audio = CompositeAudioClip(audioclips)\n\n        # compute mask if necessary\n        if transparent:\n            maskclips = [(c.mask if (c.mask is not None) else\n                          c.add_mask().mask).set_pos(c.pos)\n                          for c in self.clips]\n\n            self.mask = CompositeVideoClip(maskclips,self.size, ismask=True,\n                                               bg_color=0.0)\n\n        def make_frame(t):\n            \"\"\" The clips playing at time `t` are blitted over one\n                another. \"\"\"\n\n            f = self.bg.get_frame(t)\n            for c in self.playing_clips(t):\n                    f = c.blit_on(f, t)\n            return f\n\n        self.make_frame = make_frame",
  "def playing_clips(self, t=0):\n        \"\"\" Returns a list of the clips in the composite clips that are\n            actually playing at the given time `t`. \"\"\"\n        return [c for c in self.clips if c.is_playing(t)]",
  "def make_frame(t):\n            \"\"\" The clips playing at time `t` are blitted over one\n                another. \"\"\"\n\n            f = self.bg.get_frame(t)\n            for c in self.playing_clips(t):\n                    f = c.blit_on(f, t)\n            return f",
  "def write_gif_with_tempfiles(clip, filename, fps=None, program= 'ImageMagick',\n       opt=\"OptimizeTransparency\", fuzz=1, verbose=True,\n       loop=0, dispose=True, colors=None, tempfiles=False):\n    \"\"\" Write the VideoClip to a GIF file.\n\n\n    Converts a VideoClip into an animated GIF using ImageMagick\n    or ffmpeg. Does the same as write_gif (see this one for more\n    docstring), but writes every frame to a file instead of passing\n    them in the RAM. Useful on computers with little RAM.\n\n    \"\"\"\n\n    fileName, fileExtension = os.path.splitext(filename)\n    tt = np.arange(0,clip.duration, 1.0/fps)\n\n    tempfiles = []\n\n    verbose_print(verbose, \"\\n[MoviePy] Building file %s\\n\"%filename\n                  +40*\"-\"+\"\\n\")\n\n    verbose_print(verbose, \"[MoviePy] Generating GIF frames...\\n\")\n\n    total = int(clip.duration*fps)+1\n    for i, t in tqdm(enumerate(tt), total=total):\n\n        name = \"%s_GIFTEMP%04d.png\"%(fileName, i+1)\n        tempfiles.append(name)\n        clip.save_frame(name, t, withmask=True)\n\n    delay = int(100.0/fps)\n\n    if program == \"ImageMagick\":\n        verbose_print(verbose, \"[MoviePy] Optimizing GIF with ImageMagick... \")\n        cmd = [get_setting(\"IMAGEMAGICK_BINARY\"),\n              '-delay' , '%d'%delay,\n              \"-dispose\" ,\"%d\"%(2 if dispose else 1),\n              \"-loop\" , \"%d\"%loop,\n              \"%s_GIFTEMP*.png\"%fileName,\n              \"-coalesce\",\n              \"-layers\", \"%s\"%opt,\n              \"-fuzz\", \"%02d\"%fuzz + \"%\",\n              ]+([\"-colors\", \"%d\"%colors] if colors is not None else [])+[\n              filename]\n\n    elif program == \"ffmpeg\":\n\n        cmd = [get_setting(\"FFMPEG_BINARY\"), '-y',\n               '-f', 'image2', '-r',str(fps),\n               '-i', fileName+'_GIFTEMP%04d.png',\n               '-r',str(fps),\n               filename]\n\n    try:\n        subprocess_call( cmd, verbose = verbose )\n        verbose_print(verbose, \"[MoviePy] GIF %s is ready.\"%filename)\n\n    except (IOError,OSError) as err:\n\n        error = (\"MoviePy Error: creation of %s failed because \"\n          \"of the following error:\\n\\n%s.\\n\\n.\"%(filename, str(err)))\n\n        if program == \"ImageMagick\":\n            error = error + (\"This error can be due to the fact that \"\n                \"ImageMagick is not installed on your computer, or \"\n                \"(for Windows users) that you didn't specify the \"\n                \"path to the ImageMagick binary in file conf.py.\" )\n\n        raise IOError(error)\n\n    for f in tempfiles:\n        os.remove(f)",
  "def write_gif(clip, filename, fps=None, program= 'ImageMagick',\n           opt=\"OptimizeTransparency\", fuzz=1, verbose=True, withmask=True,\n           loop=0, dispose=True, colors=None):\n    \"\"\" Write the VideoClip to a GIF file, without temporary files.\n\n    Converts a VideoClip into an animated GIF using ImageMagick\n    or ffmpeg.\n\n\n    Parameters\n    -----------\n\n    filename\n      Name of the resulting gif file.\n\n    fps\n      Number of frames per second (see note below). If it\n        isn't provided, then the function will look for the clip's\n        ``fps`` attribute (VideoFileClip, for instance, have one).\n\n    program\n      Software to use for the conversion, either 'ImageMagick' or\n      'ffmpeg'.\n\n    opt\n      (ImageMagick only) optimalization to apply, either\n      'optimizeplus' or 'OptimizeTransparency'.\n\n    fuzz\n      (ImageMagick only) Compresses the GIF by considering that\n      the colors that are less than fuzz% different are in fact\n      the same.\n\n\n    Notes\n    -----\n\n    The gif will be playing the clip in real time (you can\n    only change the frame rate). If you want the gif to be played\n    slower than the clip you will use ::\n\n        >>> # slow down clip 50% and make it a gif\n        >>> myClip.speedx(0.5).write_gif('myClip.gif')\n\n    \"\"\"\n\n    #\n    # We use processes chained with pipes.\n    #\n    # if program == 'ffmpeg'\n    # frames --ffmpeg--> gif\n    #\n    # if program == 'ImageMagick' and optimize == (None, False)\n    # frames --ffmpeg--> bmp frames --ImageMagick--> gif\n    #\n    #\n    # if program == 'ImageMagick' and optimize != (None, False)\n    # frames -ffmpeg-> bmp frames -ImagMag-> gif -ImagMag-> better gif\n    #\n\n    delay= 100.0/fps\n\n    if clip.mask is None:\n        withmask = False\n\n    cmd1 = [get_setting(\"FFMPEG_BINARY\"), '-y', '-loglevel', 'error',\n            '-f', 'rawvideo',\n            '-vcodec','rawvideo', '-r', \"%.02f\"%fps,\n            '-s', \"%dx%d\"%(clip.w, clip.h),\n            '-pix_fmt', ('rgba' if withmask else 'rgb24'),\n            '-i', '-']\n\n    popen_params = {\"stdout\": DEVNULL,\n                    \"stderr\": DEVNULL,\n                    \"stdin\": DEVNULL}\n\n    if os.name == \"nt\":\n        popen_params[\"creationflags\"] = 0x08000000\n\n    if program == \"ffmpeg\":\n        popen_params[\"stdin\"] = sp.PIPE\n        popen_params[\"stdout\"] = DEVNULL\n\n        proc1 = sp.Popen(cmd1+[ '-pix_fmt', ('rgba' if withmask else 'rgb24'),\n                                '-r', \"%.02f\"%fps, filename], **popen_params)\n    else:\n\n        popen_params[\"stdin\"] = sp.PIPE\n        popen_params[\"stdout\"] = sp.PIPE\n\n        proc1 = sp.Popen(cmd1+ ['-f', 'image2pipe', '-vcodec', 'bmp', '-'],\n                         **popen_params)\n\n    if program == 'ImageMagick':\n\n        cmd2 = [get_setting(\"IMAGEMAGICK_BINARY\"), '-delay', \"%.02f\"%(delay),\n                \"-dispose\" ,\"%d\"%(2 if dispose else 1),\n                '-loop', '%d'%loop, '-', '-coalesce']\n\n        if (opt in [False, None]):\n            popen_params[\"stdin\"] = proc1.stdout\n            popen_params[\"stdout\"] = DEVNULL\n            proc2 = sp.Popen(cmd2+[filename], **popen_params)\n\n        else:\n            popen_params[\"stdin\"] = proc1.stdout\n            popen_params[\"stdout\"] = sp.PIPE\n            proc2 = sp.Popen(cmd2+['gif:-'], **popen_params)\n\n        if opt:\n\n            cmd3 = [get_setting(\"IMAGEMAGICK_BINARY\"), '-', '-layers', opt,\n                    '-fuzz', '%d'%fuzz+'%'\n                   ]+([\"-colors\", \"%d\"%colors] if colors is not None else [])+[\n                   filename]\n\n            popen_params[\"stdin\"] = proc2.stdout\n            popen_params[\"stdout\"] = DEVNULL\n            proc3 = sp.Popen(cmd3, **popen_params)\n\n    # We send all the frames to the first process\n    verbose_print(verbose, \"\\n[MoviePy] >>>> Building file %s\\n\"%filename)\n    verbose_print(verbose, \"[MoviePy] Generating GIF frames...\\n\")\n\n    try:\n\n        for t,frame in clip.iter_frames(fps=fps, progress_bar=True,\n                                        with_times=True,  dtype=\"uint8\"):\n            if withmask:\n                mask = 255 * clip.mask.get_frame(t)\n                frame = np.dstack([frame, mask]).astype('uint8')\n            proc1.stdin.write(frame.tostring())\n\n    except IOError as err:\n\n        error = (\"[MoviePy] Error: creation of %s failed because \"\n          \"of the following error:\\n\\n%s.\\n\\n.\"%(filename, str(err)))\n\n        if program == \"ImageMagick\":\n            error = error + (\"This can be due to the fact that \"\n                \"ImageMagick is not installed on your computer, or \"\n                \"(for Windows users) that you didn't specify the \"\n                \"path to the ImageMagick binary in file conf.py.\" )\n\n        raise IOError(error)\n    if program == 'ImageMagick':\n        verbose_print(verbose, \"[MoviePy] Optimizing the GIF with ImageMagick...\\n\")\n    proc1.stdin.close()\n    proc1.wait()\n    if program == 'ImageMagick':\n        proc2.wait()\n        if opt:\n            proc3.wait()\n    verbose_print(verbose, \"[MoviePy] >>>> File %s is ready !\"%filename)",
  "def write_gif_with_image_io(clip, filename, fps=None, opt='wu', loop=0,\n                            colors=None, verbose=True):\n    \"\"\"\n    Writes the gif with the Python library ImageIO (calls FreeImage).\n    \n    For the moment ImageIO is not installed with MoviePy. You need to install\n    imageio (pip install imageio) to use this.\n\n    Parameters\n    -----------\n    opt\n\n    \"\"\"\n\n    if colors is None:\n        colors=256\n\n    if not IMAGEIO_FOUND:\n      raise ImportError(\"Writing a gif with imageio requires ImageIO installed,\"\n                         \" with e.g. 'pip install imageio'\")\n\n    if fps is None:\n        fps = clip.fps\n\n    quantizer = 'wu' if opt!= 'nq' else 'nq' \n    writer = imageio.save(filename, duration=1.0/fps,\n                          quantizer=quantizer, palettesize=colors)\n\n    verbose_print(verbose, \"\\n[MoviePy] Building file %s with imageio\\n\"%filename)\n    \n    for frame in clip.iter_frames(fps=fps, progress_bar=True, dtype='uint8'):\n\n        writer.append_data(frame)",
  "class FFMPEG_VideoWriter:\n    \"\"\" A class for FFMPEG-based video writing.\n\n    A class to write videos using ffmpeg. ffmpeg will write in a large\n    choice of formats.\n\n    Parameters\n    -----------\n\n    filename\n      Any filename like 'video.mp4' etc. but if you want to avoid\n      complications it is recommended to use the generic extension\n      '.avi' for all your videos.\n\n    size\n      Size (width,height) of the output video in pixels.\n\n    fps\n      Frames per second in the output video file.\n\n    codec\n      FFMPEG codec. It seems that in terms of quality the hierarchy is\n      'rawvideo' = 'png' > 'mpeg4' > 'libx264'\n      'png' manages the same lossless quality as 'rawvideo' but yields\n      smaller files. Type ``ffmpeg -codecs`` in a terminal to get a list\n      of accepted codecs.\n\n      Note for default 'libx264': by default the pixel format yuv420p\n      is used. If the video dimensions are not both even (e.g. 720x405)\n      another pixel format is used, and this can cause problem in some\n      video readers.\n\n    audiofile\n      Optional: The name of an audio file that will be incorporated\n      to the video.\n\n    preset\n      Sets the time that FFMPEG will take to compress the video. The slower,\n      the better the compression rate. Possibilities are: ultrafast,superfast,\n      veryfast, faster, fast, medium (default), slow, slower, veryslow,\n      placebo.\n\n    bitrate\n      Only relevant for codecs which accept a bitrate. \"5000k\" offers\n      nice results in general.\n\n    withmask\n      Boolean. Set to ``True`` if there is a mask in the video to be\n      encoded.\n\n    \"\"\"\n\n    def __init__(self, filename, size, fps, codec=\"libx264\", audiofile=None,\n                 preset=\"medium\", bitrate=None, withmask=False,\n                 logfile=None, threads=None, ffmpeg_params=None):\n\n        if logfile is None:\n            logfile = sp.PIPE\n\n        self.filename = filename\n        self.codec = codec\n        self.ext = self.filename.split(\".\")[-1]\n\n        # order is important\n        cmd = [\n            get_setting(\"FFMPEG_BINARY\"),\n            '-y',\n            '-loglevel', 'error' if logfile == sp.PIPE else 'info',\n            '-f', 'rawvideo',\n            '-vcodec', 'rawvideo',\n            '-s', '%dx%d' % (size[0], size[1]),\n            '-pix_fmt', 'rgba' if withmask else 'rgb24',\n            '-r', '%.02f' % fps,\n            '-i', '-', '-an',\n        ]\n        if audiofile is not None:\n            cmd.extend([\n                '-i', audiofile,\n                '-acodec', 'copy'\n            ])\n        cmd.extend([\n            '-vcodec', codec,\n            '-preset', preset,\n        ])\n        if ffmpeg_params is not None:\n            cmd.extend(ffmpeg_params)\n        if bitrate is not None:\n            cmd.extend([\n                '-b', bitrate\n            ])\n\n        if threads is not None:\n            cmd.extend([\"-threads\", str(threads)])\n\n        if ((codec == 'libx264') and\n                (size[0] % 2 == 0) and\n                (size[1] % 2 == 0)):\n            cmd.extend([\n                '-pix_fmt', 'yuv420p'\n            ])\n        cmd.extend([\n            filename\n        ])\n\n        popen_params = {\"stdout\": DEVNULL,\n                        \"stderr\": logfile,\n                        \"stdin\": sp.PIPE}\n\n        # This was added so that no extra unwanted window opens on windows\n        # when the child process is created\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n        \n        self.proc = sp.Popen(cmd, **popen_params)\n\n\n    def write_frame(self, img_array):\n        \"\"\" Writes one frame in the file.\"\"\"\n        try:\n            self.proc.stdin.write(img_array.tostring())\n        except IOError as err:\n            ffmpeg_error = self.proc.stderr.read()\n            error = (str(err) + (\"\\n\\nMoviePy error: FFMPEG encountered \"\n                                 \"the following error while writing file %s:\"\n                                 \"\\n\\n %s\" % (self.filename, ffmpeg_error)))\n\n            if \"Unknown encoder\" in ffmpeg_error:\n\n                error = error+(\"\\n\\nThe video export \"\n                  \"failed because FFMPEG didn't find the specified \"\n                  \"codec for video encoding (%s). Please install \"\n                  \"this codec or change the codec when calling \"\n                  \"write_videofile. For instance:\\n\"\n                  \"  >>> clip.write_videofile('myvid.webm', codec='libvpx')\")%(self.codec)\n\n            elif \"incorrect codec parameters ?\" in ffmpeg_error:\n\n                 error = error+(\"\\n\\nThe video export \"\n                  \"failed, possibly because the codec specified for \"\n                  \"the video (%s) is not compatible with the given \"\n                  \"extension (%s). Please specify a valid 'codec' \"\n                  \"argument in write_videofile. This would be 'libx264' \"\n                  \"or 'mpeg4' for mp4, 'libtheora' for ogv, 'libvpx for webm. \"\n                  \"Another possible reason is that the audio codec was not \"\n                  \"compatible with the video codec. For instance the video \"\n                  \"extensions 'ogv' and 'webm' only allow 'libvorbis' (default) as a\"\n                  \"video codec.\"\n                  )%(self.codec, self.ext)\n\n            elif  \"encoder setup failed\" in ffmpeg_error:\n\n                error = error+(\"\\n\\nThe video export \"\n                  \"failed, possibly because the bitrate you specified \"\n                  \"was too high or too low for the video codec.\")\n\n            elif \"Invalid encoder type\" in ffmpeg_error:\n\n                error = error + (\"\\n\\nThe video export failed because the codec \"\n                  \"or file extension you provided is not a video\")\n\n\n            raise IOError(error)\n\n    def close(self):\n        self.proc.stdin.close()\n        if self.proc.stderr is not None:\n            self.proc.stderr.close()\n        self.proc.wait()\n\n        del self.proc",
  "def ffmpeg_write_video(clip, filename, fps, codec=\"libx264\", bitrate=None,\n                       preset=\"medium\", withmask=False, write_logfile=False,\n                       audiofile=None, verbose=True, threads=None, ffmpeg_params=None):\n    \"\"\" Write the clip to a videofile. See VideoClip.write_videofile for details\n    on the parameters.\n    \"\"\"\n    if write_logfile:\n        logfile = open(filename + \".log\", 'w+')\n    else:\n        logfile = None\n\n    verbose_print(verbose, \"[MoviePy] Writing video %s\\n\"%filename)\n    writer = FFMPEG_VideoWriter(filename, clip.size, fps, codec = codec,\n                                preset=preset, bitrate=bitrate, logfile=logfile,\n                                audiofile=audiofile, threads=threads,\n                                ffmpeg_params=ffmpeg_params)\n\n    nframes = int(clip.duration*fps)\n\n    for t,frame in clip.iter_frames(progress_bar=True, with_times=True,\n                                    fps=fps, dtype=\"uint8\"):\n        if withmask:\n            mask = (255*clip.mask.get_frame(t))\n            if mask.dtype != \"uint8\":\n                mask = mask.astype(\"uint8\")\n            frame = np.dstack([frame,mask])\n        \n        writer.write_frame(frame)\n\n    writer.close()\n\n    if write_logfile:\n        logfile.close()\n\n    verbose_print(verbose, \"[MoviePy] Done.\\n\")",
  "def ffmpeg_write_image(filename, image, logfile=False):\n    \"\"\" Writes an image (HxWx3 or HxWx4 numpy array) to a file, using\n        ffmpeg. \"\"\"\n    \n    if image.dtype != 'uint8':\n          image = image.astype(\"uint8\")\n\n    cmd = [ get_setting(\"FFMPEG_BINARY\"), '-y',\n           '-s', \"%dx%d\"%(image.shape[:2][::-1]),\n           \"-f\", 'rawvideo',\n           '-pix_fmt', \"rgba\" if (image.shape[2] == 4) else \"rgb24\",\n           '-i','-', filename]\n\n    if logfile:\n        log_file = open(filename + \".log\", 'w+')\n    else:\n        log_file = sp.PIPE\n\n    popen_params = {\"stdout\": DEVNULL,\n                    \"stderr\": log_file,\n                    \"stdin\": sp.PIPE}\n\n    if os.name == \"nt\":\n        popen_params[\"creationflags\"] = 0x08000000\n\n    proc = sp.Popen(cmd, **popen_params)\n    out, err = proc.communicate(image.tostring())\n\n    if proc.returncode:\n        err = \"\\n\".join([\"[MoviePy] Running : %s\\n\" % cmd,\n                         \"WARNING: this command returned an error:\",\n                         err.decode('utf8')])\n        raise IOError(err)\n\n    del proc",
  "def __init__(self, filename, size, fps, codec=\"libx264\", audiofile=None,\n                 preset=\"medium\", bitrate=None, withmask=False,\n                 logfile=None, threads=None, ffmpeg_params=None):\n\n        if logfile is None:\n            logfile = sp.PIPE\n\n        self.filename = filename\n        self.codec = codec\n        self.ext = self.filename.split(\".\")[-1]\n\n        # order is important\n        cmd = [\n            get_setting(\"FFMPEG_BINARY\"),\n            '-y',\n            '-loglevel', 'error' if logfile == sp.PIPE else 'info',\n            '-f', 'rawvideo',\n            '-vcodec', 'rawvideo',\n            '-s', '%dx%d' % (size[0], size[1]),\n            '-pix_fmt', 'rgba' if withmask else 'rgb24',\n            '-r', '%.02f' % fps,\n            '-i', '-', '-an',\n        ]\n        if audiofile is not None:\n            cmd.extend([\n                '-i', audiofile,\n                '-acodec', 'copy'\n            ])\n        cmd.extend([\n            '-vcodec', codec,\n            '-preset', preset,\n        ])\n        if ffmpeg_params is not None:\n            cmd.extend(ffmpeg_params)\n        if bitrate is not None:\n            cmd.extend([\n                '-b', bitrate\n            ])\n\n        if threads is not None:\n            cmd.extend([\"-threads\", str(threads)])\n\n        if ((codec == 'libx264') and\n                (size[0] % 2 == 0) and\n                (size[1] % 2 == 0)):\n            cmd.extend([\n                '-pix_fmt', 'yuv420p'\n            ])\n        cmd.extend([\n            filename\n        ])\n\n        popen_params = {\"stdout\": DEVNULL,\n                        \"stderr\": logfile,\n                        \"stdin\": sp.PIPE}\n\n        # This was added so that no extra unwanted window opens on windows\n        # when the child process is created\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n        \n        self.proc = sp.Popen(cmd, **popen_params)",
  "def write_frame(self, img_array):\n        \"\"\" Writes one frame in the file.\"\"\"\n        try:\n            self.proc.stdin.write(img_array.tostring())\n        except IOError as err:\n            ffmpeg_error = self.proc.stderr.read()\n            error = (str(err) + (\"\\n\\nMoviePy error: FFMPEG encountered \"\n                                 \"the following error while writing file %s:\"\n                                 \"\\n\\n %s\" % (self.filename, ffmpeg_error)))\n\n            if \"Unknown encoder\" in ffmpeg_error:\n\n                error = error+(\"\\n\\nThe video export \"\n                  \"failed because FFMPEG didn't find the specified \"\n                  \"codec for video encoding (%s). Please install \"\n                  \"this codec or change the codec when calling \"\n                  \"write_videofile. For instance:\\n\"\n                  \"  >>> clip.write_videofile('myvid.webm', codec='libvpx')\")%(self.codec)\n\n            elif \"incorrect codec parameters ?\" in ffmpeg_error:\n\n                 error = error+(\"\\n\\nThe video export \"\n                  \"failed, possibly because the codec specified for \"\n                  \"the video (%s) is not compatible with the given \"\n                  \"extension (%s). Please specify a valid 'codec' \"\n                  \"argument in write_videofile. This would be 'libx264' \"\n                  \"or 'mpeg4' for mp4, 'libtheora' for ogv, 'libvpx for webm. \"\n                  \"Another possible reason is that the audio codec was not \"\n                  \"compatible with the video codec. For instance the video \"\n                  \"extensions 'ogv' and 'webm' only allow 'libvorbis' (default) as a\"\n                  \"video codec.\"\n                  )%(self.codec, self.ext)\n\n            elif  \"encoder setup failed\" in ffmpeg_error:\n\n                error = error+(\"\\n\\nThe video export \"\n                  \"failed, possibly because the bitrate you specified \"\n                  \"was too high or too low for the video codec.\")\n\n            elif \"Invalid encoder type\" in ffmpeg_error:\n\n                error = error + (\"\\n\\nThe video export failed because the codec \"\n                  \"or file extension you provided is not a video\")\n\n\n            raise IOError(error)",
  "def close(self):\n        self.proc.stdin.close()\n        if self.proc.stderr is not None:\n            self.proc.stderr.close()\n        self.proc.wait()\n\n        del self.proc",
  "def sliders(f, sliders_properties, wait_for_validation = False):\n    \"\"\" A light GUI to manually explore and tune the outputs of \n        a function.\n        slider_properties is a list of dicts (arguments for Slider )\n        \n        def volume(x,y,z):\n            return x*y*z\n    \n        intervals = [ { 'label' :  'width',  'valmin': 1 , 'valmax': 5 },\n                  { 'label' :  'height',  'valmin': 1 , 'valmax': 5 },\n                  { 'label' :  'depth',  'valmin': 1 , 'valmax': 5 } ]\n        inputExplorer(volume,intervals)\n    \"\"\"\n        \n    nVars = len(sliders_properties)\n    slider_width = 1.0/nVars\n    \n    # CREATE THE CANVAS\n    \n    figure,ax = plt.subplots(1)\n    figure.canvas.set_window_title( \"Inputs for '%s'\"%(f.func_name) )\n    \n    # choose an appropriate height\n    \n    width,height = figure.get_size_inches()\n    height = min(0.5*nVars,8)\n    figure.set_size_inches(width,height,forward = True)\n    \n    \n    # hide the axis\n    ax.set_frame_on(False)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n\n    # CREATE THE SLIDERS\n    \n    sliders = []\n    \n    for i, properties in enumerate(sliders_properties):\n        \n        ax = plt.axes([0.1 , 0.95-0.9*(i+1)*slider_width,\n                       0.8 , 0.8* slider_width])\n        if not isinstance(properties,dict):\n            properties =dict(zip(['label','valmin', 'valmax', 'valinit'],\n                             properties))\n        sliders.append( Slider(ax=ax, **properties) )\n    \n    \n    # CREATE THE CALLBACK FUNCTIONS\n    \n    def on_changed(event) : \n        \n        res = f(*(s.val for s in sliders))\n        \n        if res is not None:\n            \n            print( res )\n    \n    def on_key_press(event):\n        \n        if event.key is 'enter':\n            \n            on_changed(event)   \n    \n    figure.canvas.mpl_connect('key_press_event', on_key_press)\n    \n    # AUTOMATIC UPDATE ?\n    \n    if not wait_for_validation:\n        \n        for s in sliders :\n            \n            s.on_changed(on_changed)\n    \n    \n    # DISPLAY THE SLIDERS\n    \n    plt.show()",
  "def on_changed(event) : \n        \n        res = f(*(s.val for s in sliders))\n        \n        if res is not None:\n            \n            print( res )",
  "def on_key_press(event):\n        \n        if event.key is 'enter':\n            \n            on_changed(event)",
  "class VideoFileClip(VideoClip):\n\n    \"\"\"\n    \n    A video clip originating from a movie file. For instance: ::\n    \n        >>> clip = VideofileClip(\"myHolidays.mp4\")\n        >>> clip2 = VideofileClip(\"myMaskVideo.avi\")\n    \n    \n    Parameters\n    ------------\n    \n    filename:\n      The name of the video file. It can have any extension supported\n      by ffmpeg: .ogv, .mp4, .mpeg, .avi, .mov etc.\n      \n    has_mask:\n      Set this to 'True' if there is a mask included in the videofile.\n      Video files rarely contain masks, but some video codecs enable\n      that. For istance if you have a MoviePy VideoClip with a mask you\n      can save it to a videofile with a mask. (see also \n      ``VideoClip.write_videofile`` for more details).\n    \n    audio:\n      Set to `False` if the clip doesn't have any audio or if you do not\n      wish to read the audio.\n      \n    Attributes\n    -----------\n    \n    filename:\n      Name of the original video file.\n    \n    fps:\n      Frames per second in the original file. \n        \n    \"\"\"\n\n    def __init__(self, filename, has_mask=False,\n                 audio=True, audio_buffersize = 200000,\n                 audio_fps=44100, audio_nbytes=2, verbose=False):\n        \n        VideoClip.__init__(self)\n        \n        # Make a reader\n        pix_fmt= \"rgba\" if has_mask else \"rgb24\"\n        reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt)\n        self.reader = reader\n        # Make some of the reader's attributes accessible from the clip\n        self.duration = self.reader.duration\n        self.end = self.reader.duration\n        \n        self.fps = self.reader.fps\n        self.size = self.reader.size\n\n        if has_mask:\n\n            self.make_frame = lambda t: reader.get_frame(t)[:,:,:3]\n            mask_mf =  lambda t: reader.get_frame(t)[:,:,3]/255.0\n            self.mask = (VideoClip(ismask = True, make_frame = mask_mf)\n                       .set_duration(self.duration))\n            self.mask.fps = self.fps\n\n        else:\n\n            self.make_frame = lambda t: reader.get_frame(t)\n        \n        # Make a reader for the audio, if any.\n        if audio and self.reader.infos['audio_found']:\n\n            self.audio = AudioFileClip(filename,\n                                       buffersize= audio_buffersize,\n                                       fps = audio_fps,\n                                       nbytes = audio_nbytes)\n\n    def __del__(self):\n      \"\"\" Close/delete the internal reader. \"\"\"\n      del self.reader",
  "def __init__(self, filename, has_mask=False,\n                 audio=True, audio_buffersize = 200000,\n                 audio_fps=44100, audio_nbytes=2, verbose=False):\n        \n        VideoClip.__init__(self)\n        \n        # Make a reader\n        pix_fmt= \"rgba\" if has_mask else \"rgb24\"\n        reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt)\n        self.reader = reader\n        # Make some of the reader's attributes accessible from the clip\n        self.duration = self.reader.duration\n        self.end = self.reader.duration\n        \n        self.fps = self.reader.fps\n        self.size = self.reader.size\n\n        if has_mask:\n\n            self.make_frame = lambda t: reader.get_frame(t)[:,:,:3]\n            mask_mf =  lambda t: reader.get_frame(t)[:,:,3]/255.0\n            self.mask = (VideoClip(ismask = True, make_frame = mask_mf)\n                       .set_duration(self.duration))\n            self.mask.fps = self.fps\n\n        else:\n\n            self.make_frame = lambda t: reader.get_frame(t)\n        \n        # Make a reader for the audio, if any.\n        if audio and self.reader.infos['audio_found']:\n\n            self.audio = AudioFileClip(filename,\n                                       buffersize= audio_buffersize,\n                                       fps = audio_fps,\n                                       nbytes = audio_nbytes)",
  "def __del__(self):\n      \"\"\" Close/delete the internal reader. \"\"\"\n      del self.reader",
  "def PIL_to_npimage(im):\n    \"\"\" Transforms a PIL/Pillow image into a numpy RGB(A) image.\n        Actually all this do is returning numpy.array(im).\"\"\"\n    return np.array(im)",
  "def mplfig_to_npimage(fig):\n    \"\"\" Converts a matplotlib figure to a RGB frame after updating the canvas\"\"\"\n    #  only the Agg backend now supports the tostring_rgb function\n    from matplotlib.backends.backend_agg import FigureCanvasAgg\n    canvas = FigureCanvasAgg(fig)\n    canvas.draw() # update/draw the elements\n\n    # get the width and the height to resize the matrix\n    l,b,w,h = canvas.figure.bbox.bounds\n    w, h = int(w), int(h)\n\n    #  exports the canvas to a string buffer and then to a numpy nd.array\n    buf = canvas.tostring_rgb()\n    image= np.fromstring(buf,dtype=np.uint8)\n    return image.reshape(h,w,3)",
  "def download_webfile(url, filename, overwrite=False):\n    \"\"\" Small utility to download the file at 'url' under name 'filename'.\n    If url is a youtube video ID like z410eauCnH it will download the video\n    using youtube-dl (install youtube-dl first !).\n    If the filename already exists and overwrite=False, nothing will happen.\n    \"\"\"\n    if os.path.exists(filename) and not overwrite:\n        return\n\n    if '.' in url:\n        urlretrieve(url, filename)\n    else:\n        try:\n            subprocess_call(['youtube-dl', url, '-o', filename])\n        except OSError as e:\n            raise OSError(e.message + '\\n A possible reason is that youtube-dl'\n                ' is not installed on your computer. Install it with '\n                ' \"pip install youtube-dl\"')",
  "def imdisplay(imarray, screen=None):\n    \"\"\"Splashes the given image array on the given pygame screen \"\"\"\n    a = pg.surfarray.make_surface(imarray.swapaxes(0, 1))\n    if screen is None:\n        screen = pg.display.set_mode(imarray.shape[:2][::-1])\n    screen.blit(a, (0, 0))\n    pg.display.flip()",
  "def show(clip, t=0, with_mask=True, interactive=False):\n    \"\"\"\n    Splashes the frame of clip corresponding to time ``t``.\n    \n    Parameters\n    ------------\n    \n    t\n      Time in seconds of the frame to display.\n    \n    with_mask\n      ``False`` if the clip has a mask but you want to see the clip\n      without the mask.\n    \n    \"\"\"\n\n    if isinstance(t, tuple):\n        t = cvsecs(*t)\n\n    if with_mask and (clip.mask is not None):\n        import moviepy.video.compositing.CompositeVideoClip as cvc\n        clip = cvc.CompositeVideoClip([clip.set_pos((0,0))])\n    img = clip.get_frame(t)\n    imdisplay(img)\n\n    if interactive:\n        result=[]\n        while True:\n            for event in pg.event.get():\n                if event.type == pg.KEYDOWN:\n                    if (event.key == pg.K_ESCAPE):\n                        print( \"Keyboard interrupt\" )\n                        return result\n                elif event.type == pg.MOUSEBUTTONDOWN:\n                     x,y = pg.mouse.get_pos()\n                     rgb = img[y,x]\n                     result.append({'position':(x,y), 'color':rgb})\n                     print( \"position, color : \", \"%s, %s\"%(\n                             str((x,y)),str(rgb)))\n            time.sleep(.03)",
  "def preview(clip, fps=15, audio=True, audio_fps=22050,\n             audio_buffersize=3000, audio_nbytes=2):\n    \"\"\" \n    Displays the clip in a window, at the given frames per second\n    (of movie) rate. It will avoid that the clip be played faster\n    than normal, but it cannot avoid the clip to be played slower\n    than normal if the computations are complex. In this case, try\n    reducing the ``fps``.\n    \n    Parameters\n    ------------\n    \n    fps\n      Number of frames per seconds in the displayed video.\n        \n    audio\n      ``True`` (default) if you want the clip's audio be played during\n      the preview.\n        \n    audiofps\n      The frames per second to use when generating the audio sound.\n      \n    \"\"\"\n    \n    import pygame as pg\n    \n    # compute and splash the first image\n    screen = pg.display.set_mode(clip.size)\n    \n    audio = audio and (clip.audio is not None)\n    \n    if audio:\n        # the sound will be played in parrallel. We are not\n        # parralellizing it on different CPUs because it seems that\n        # pygame and openCV already use several cpus it seems.\n        \n        # two synchro-flags to tell whether audio and video are ready\n        videoFlag = threading.Event()\n        audioFlag = threading.Event()\n        # launch the thread\n        audiothread = threading.Thread(target=clip.audio.preview,\n            args = (audio_fps,audio_buffersize, audio_nbytes,\n                    audioFlag, videoFlag))\n        audiothread.start()\n    \n    img = clip.get_frame(0)\n    imdisplay(img, screen)\n    if audio: # synchronize with audio\n        videoFlag.set() # say to the audio: video is ready\n        audioFlag.wait() # wait for the audio to be ready\n    \n    result = []\n    \n    t0 = time.time()\n    for t in np.arange(1.0 / fps, clip.duration-.001, 1.0 / fps):\n        \n        img = clip.get_frame(t)\n        \n        for event in pg.event.get():\n            if event.type == pg.KEYDOWN:\n                if (event.key == pg.K_ESCAPE):\n                    \n                    if audio:\n                        videoFlag.clear()\n                    print( \"Keyboard interrupt\" )\n                    return result\n                    \n            elif event.type == pg.MOUSEBUTTONDOWN:\n                x,y = pg.mouse.get_pos()\n                rgb = img[y,x]\n                result.append({'time':t, 'position':(x,y),\n                                'color':rgb})\n                print( \"time, position, color : \", \"%.03f, %s, %s\"%(\n                             t,str((x,y)),str(rgb)))\n                    \n        t1 = time.time()\n        time.sleep(max(0, t - (t1-t0)) )\n        imdisplay(img, screen)",
  "def ffmpeg_movie_from_frames(filename, folder, fps, digits=6):\n    \"\"\"\n    Writes a movie out of the frames (picture files) in a folder.\n    Almost deprecated.\n    \"\"\"\n    s = \"%\" + \"%02d\" % digits + \"d.png\"\n    cmd = [get_setting(\"FFMPEG_BINARY\"), \"-y\", \"-f\",\"image2\",\n             \"-r\", \"%d\"%fps,\n             \"-i\", os.path.join(folder,folder) + '/' + s,\n             \"-b\", \"%dk\"%bitrate,\n             \"-r\", \"%d\"%self.fps,\n             filename]\n    \n    subprocess_call(cmd)",
  "def ffmpeg_extract_subclip(filename, t1, t2, targetname=None):\n    \"\"\" makes a new video file playing video file ``filename`` between\n        the times ``t1`` and ``t2``. \"\"\"\n    name,ext = os.path.splitext(filename)\n    if not targetname:\n        T1, T2 = [int(1000*t) for t in [t1, t2]]\n        targetname = name+ \"%sSUB%d_%d.%s\"(name, T1, T2, ext)\n    \n    cmd = [get_setting(\"FFMPEG_BINARY\"),\"-y\",\n      \"-i\", filename,\n      \"-ss\", \"%0.2f\"%t1,\n      \"-t\", \"%0.2f\"%(t2-t1),\n      \"-vcodec\", \"copy\", \"-acodec\", \"copy\", targetname]\n    \n    subprocess_call(cmd)",
  "def ffmpeg_merge_video_audio(video,audio,output, vcodec='copy',\n                             acodec='copy', ffmpeg_output=False,\n                             verbose = True):\n    \"\"\" merges video file ``video`` and audio file ``audio`` into one\n        movie file ``output``. \"\"\"\n    cmd = [get_setting(\"FFMPEG_BINARY\"), \"-y\", \"-i\", audio,\"-i\", video,\n             \"-vcodec\", vcodec, \"-acodec\", acodec, output]\n             \n    subprocess_call(cmd, verbose = verbose)",
  "def ffmpeg_extract_audio(inputfile,output,bitrate=3000,fps=44100):\n    \"\"\" extract the sound from a video file and save it in ``output`` \"\"\"\n    cmd = [get_setting(\"FFMPEG_BINARY\"), \"-y\", \"-i\", inputfile, \"-ab\", \"%dk\"%bitrate,\n         \"-ar\", \"%d\"%fps, output]\n    subprocess_call(cmd)",
  "def ffmpeg_resize(video,output,size):\n    \"\"\" resizes ``video`` to new size ``size`` and write the result\n        in file ``output``. \"\"\"\n    cmd= [get_setting(\"FFMPEG_BINARY\"), \"-i\", video, \"-vf\", \"scale=%d:%d\"%(res[0], res[1]),\n             output]\n             \n    subprocess_call(cmd)",
  "class ImageSequenceClip(VideoClip):\n    \"\"\"\n    \n    A VideoClip made from a series of images.\n    \n\n    Parameters\n    -----------\n\n    sequence\n      Can be one of these:\n      - The name of a folder (containing only pictures). The pictures\n        will be considered in alphanumerical order.\n      - A list of names of image files. In this case you can choose to\n        load the pictures in memory pictures \n      - A list of Numpy arrays representing images. In this last case,\n        masks are not supported currently.\n\n    fps\n      Number of picture frames to read per second. Instead, you can provide\n      the duration of each image with durations (see below)\n\n    durations\n      List of the duration of each picture.\n\n    with_mask\n      Should the alpha layer of PNG images be considered as a mask ?\n\n    ismask\n      Will this sequence of pictures be used as an animated mask.\n\n    Notes\n    ------\n\n    If your sequence is made of image files, the only image kept in \n\n\n    \n    \"\"\"\n\n\n    def __init__(self, sequence, fps=None, durations=None, with_mask=True,\n                 ismask=False, load_images=False):\n\n        # CODE WRITTEN AS IT CAME, MAY BE IMPROVED IN THE FUTURE\n        \n        if (fps is None) and (duration is None):\n            raise ValueError(\"Please provide either 'fps' or 'durations'.\")\n        VideoClip.__init__(self, ismask=ismask)\n\n        # Parse the data\n\n        fromfiles = True\n\n        if isinstance(sequence, list):\n            if isinstance(sequence[0], str):\n                if load_images:\n                    sequence = [imread(f) for f in sequence]\n                    fromfiles = False\n                else:\n                    fromfiles= True\n            else:\n                # sequence is already a list of numpy arrays\n                fromfiles = False\n        else:\n            # sequence is a folder name, make it a list of files:\n            fromfiles = True\n            sequence = sorted([os.path.join(sequence, f)\n                        for f in os.listdir(sequence)])\n\n        self.fps = fps\n        if fps is not None:\n            durations = [1.0/fps for image in sequence]\n        self.durations = durations\n        self.images_starts = [0]+list(np.cumsum(durations))\n        self.duration = sum(durations)\n        self.end = self.duration\n        self.sequence = sequence\n        \n        def find_image_index(t):\n            return max([i for i in range(len(self.sequence))\n                              if self.images_starts[i]<=t])\n\n        if fromfiles:\n\n            self.lastindex = None\n            self.lastimage = None\n\n            def make_frame(t):\n            \n                index = find_image_index(t)\n\n                if index != self.lastindex:\n                    self.lastimage = imread(self.sequence[index])[:,:,:3] \n                    self.lastindex = index\n                \n                return self.lastimage\n\n            if with_mask and (imread(self.sequence[0]).shape[2]==4):\n\n                self.mask = VideoClip(ismask=True)\n                self.mask.lastindex = None\n                self.mask.lastimage = None\n\n                def mask_make_frame(t):\n            \n                    index = find_image_index(t)\n                    if index != self.mask.lastindex:\n                        frame = imread(self.sequence[index])[:,:,3]\n                        self.mask.lastimage = frame.astype(float)/255\n                        self.mask.lastindex = index\n\n                    return self.mask.lastimage\n\n                self.mask.make_frame = mask_make_frame\n                self.mask.size = mask_make_frame(0).shape[:2][::-1]\n\n\n        else:\n\n            def make_frame(t):\n            \n                index = find_image_index(t)\n                return self.sequence[index][:,:,:3]\n\n            if with_mask and (self.sequence[0].shape[2]==4):\n\n                self.mask = VideoClip(ismask=True)\n\n                def mask_make_frame(t):\n                    index = find_image_index(t)\n                    return 1.0*self.sequence[index][:,:,3]/255\n\n                self.mask.make_frame = mask_make_frame\n                self.mask.size = mask_make_frame(0).shape[:2][::-1]\n        \n            \n        self.make_frame = make_frame\n        self.size = make_frame(0).shape[:2][::-1]",
  "def __init__(self, sequence, fps=None, durations=None, with_mask=True,\n                 ismask=False, load_images=False):\n\n        # CODE WRITTEN AS IT CAME, MAY BE IMPROVED IN THE FUTURE\n        \n        if (fps is None) and (duration is None):\n            raise ValueError(\"Please provide either 'fps' or 'durations'.\")\n        VideoClip.__init__(self, ismask=ismask)\n\n        # Parse the data\n\n        fromfiles = True\n\n        if isinstance(sequence, list):\n            if isinstance(sequence[0], str):\n                if load_images:\n                    sequence = [imread(f) for f in sequence]\n                    fromfiles = False\n                else:\n                    fromfiles= True\n            else:\n                # sequence is already a list of numpy arrays\n                fromfiles = False\n        else:\n            # sequence is a folder name, make it a list of files:\n            fromfiles = True\n            sequence = sorted([os.path.join(sequence, f)\n                        for f in os.listdir(sequence)])\n\n        self.fps = fps\n        if fps is not None:\n            durations = [1.0/fps for image in sequence]\n        self.durations = durations\n        self.images_starts = [0]+list(np.cumsum(durations))\n        self.duration = sum(durations)\n        self.end = self.duration\n        self.sequence = sequence\n        \n        def find_image_index(t):\n            return max([i for i in range(len(self.sequence))\n                              if self.images_starts[i]<=t])\n\n        if fromfiles:\n\n            self.lastindex = None\n            self.lastimage = None\n\n            def make_frame(t):\n            \n                index = find_image_index(t)\n\n                if index != self.lastindex:\n                    self.lastimage = imread(self.sequence[index])[:,:,:3] \n                    self.lastindex = index\n                \n                return self.lastimage\n\n            if with_mask and (imread(self.sequence[0]).shape[2]==4):\n\n                self.mask = VideoClip(ismask=True)\n                self.mask.lastindex = None\n                self.mask.lastimage = None\n\n                def mask_make_frame(t):\n            \n                    index = find_image_index(t)\n                    if index != self.mask.lastindex:\n                        frame = imread(self.sequence[index])[:,:,3]\n                        self.mask.lastimage = frame.astype(float)/255\n                        self.mask.lastindex = index\n\n                    return self.mask.lastimage\n\n                self.mask.make_frame = mask_make_frame\n                self.mask.size = mask_make_frame(0).shape[:2][::-1]\n\n\n        else:\n\n            def make_frame(t):\n            \n                index = find_image_index(t)\n                return self.sequence[index][:,:,:3]\n\n            if with_mask and (self.sequence[0].shape[2]==4):\n\n                self.mask = VideoClip(ismask=True)\n\n                def mask_make_frame(t):\n                    index = find_image_index(t)\n                    return 1.0*self.sequence[index][:,:,3]/255\n\n                self.mask.make_frame = mask_make_frame\n                self.mask.size = mask_make_frame(0).shape[:2][::-1]\n        \n            \n        self.make_frame = make_frame\n        self.size = make_frame(0).shape[:2][::-1]",
  "def find_image_index(t):\n            return max([i for i in range(len(self.sequence))\n                              if self.images_starts[i]<=t])",
  "def make_frame(t):\n            \n                index = find_image_index(t)\n\n                if index != self.lastindex:\n                    self.lastimage = imread(self.sequence[index])[:,:,:3] \n                    self.lastindex = index\n                \n                return self.lastimage",
  "def make_frame(t):\n            \n                index = find_image_index(t)\n                return self.sequence[index][:,:,:3]",
  "def mask_make_frame(t):\n            \n                    index = find_image_index(t)\n                    if index != self.mask.lastindex:\n                        frame = imread(self.sequence[index])[:,:,3]\n                        self.mask.lastimage = frame.astype(float)/255\n                        self.mask.lastindex = index\n\n                    return self.mask.lastimage",
  "def mask_make_frame(t):\n                    index = find_image_index(t)\n                    return 1.0*self.sequence[index][:,:,3]/255",
  "class FFMPEG_VideoReader:\n\n    def __init__(self, filename, print_infos=False, bufsize = None,\n                 pix_fmt=\"rgb24\", check_duration=True):\n\n        self.filename = filename\n        infos = ffmpeg_parse_infos(filename, print_infos, check_duration)\n        self.fps = infos['video_fps']\n        self.size = infos['video_size']\n        self.duration = infos['video_duration']\n        self.ffmpeg_duration = infos['duration']\n        self.nframes = infos['video_nframes']\n\n        self.infos = infos\n\n        self.pix_fmt = pix_fmt\n        if pix_fmt == 'rgba':\n            self.depth = 4\n        else:\n            self.depth = 3\n\n        if bufsize is None:\n            w, h = self.size\n            bufsize = self.depth * w * h + 100\n\n        self.bufsize= bufsize\n        self.initialize()\n\n\n        self.pos = 1\n        self.lastread = self.read_frame()\n\n\n    def initialize(self, starttime=0):\n        \"\"\"Opens the file, creates the pipe. \"\"\"\n\n        self.close() # if any\n\n        if starttime != 0 :\n            offset = min(1, starttime)\n            i_arg = ['-ss', \"%.06f\" % (starttime - offset),\n                     '-i', self.filename,\n                     '-ss', \"%.06f\" % offset]\n        else:\n            i_arg = [ '-i', self.filename]\n\n\n        cmd = ([get_setting(\"FFMPEG_BINARY\")]+ i_arg +\n                ['-loglevel', 'error',\n                '-f', 'image2pipe',\n                \"-pix_fmt\", self.pix_fmt,\n                '-vcodec', 'rawvideo', '-'])\n\n        popen_params = {\"bufsize\": self.bufsize,\n                        \"stdout\": sp.PIPE,\n                        \"stderr\": sp.PIPE,\n                        \"stdin\": DEVNULL}\n\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n\n        self.proc = sp.Popen(cmd, **popen_params)\n\n\n\n\n\n    def skip_frames(self, n=1):\n        \"\"\"Reads and throws away n frames \"\"\"\n        w, h = self.size\n        for i in range(n):\n            self.proc.stdout.read(self.depth*w*h)\n            #self.proc.stdout.flush()\n        self.pos += n\n\n\n    def read_frame(self):\n        w, h = self.size\n        nbytes= self.depth*w*h\n\n        s = self.proc.stdout.read(nbytes)\n        if len(s) != nbytes:\n\n            warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n                   \"%d bytes wanted but %d bytes read,\"%(nbytes, len(s))+\n                   \"at frame %d/%d, at time %.02f/%.02f sec. \"%(\n                    self.pos,self.nframes,\n                    1.0*self.pos/self.fps,\n                    self.duration)+\n                   \"Using the last valid frame instead.\",\n                   UserWarning)\n\n            if not hasattr(self, 'lastread'):\n                raise IOError((\"MoviePy error: failed to read the first frame of \"\n                               \"video file %s. That might mean that the file is \"\n                               \"corrupted. That may also mean that you are using \"\n                               \"a deprecated version of FFMPEG. On Ubuntu/Debian \"\n                               \"for instance the version in the repos is deprecated. \"\n                               \"Please update to a recent version from the website.\")%(\n                                self.filename))\n\n            result = self.lastread\n\n        else:\n\n            result = np.fromstring(s, dtype='uint8')\n            result.shape =(h, w, len(s)//(w*h)) # reshape((h, w, len(s)//(w*h)))\n            self.lastread = result\n\n        return result\n\n    def get_frame(self, t):\n        \"\"\" Read a file video frame at time t.\n\n        Note for coders: getting an arbitrary frame in the video with\n        ffmpeg can be painfully slow if some decoding has to be done.\n        This function tries to avoid fectching arbitrary frames\n        whenever possible, by moving between adjacent frames.\n        \"\"\"\n\n        # these definitely need to be rechecked sometime. Seems to work.\n        \n        # I use that horrible '+0.00001' hack because sometimes due to numerical\n        # imprecisions a 3.0 can become a 2.99999999... which makes the int()\n        # go to the previous integer. This makes the fetching more robust in the\n        # case where you get the nth frame by writing get_frame(n/fps).\n        \n        pos = int(self.fps*t + 0.00001)+1\n\n        if pos == self.pos:\n            return self.lastread\n        else:\n            if(pos < self.pos) or (pos > self.pos+100):\n                self.initialize(t)\n                self.pos = pos\n            else:\n                self.skip_frames(pos-self.pos-1)\n            result = self.read_frame()\n            self.pos = pos\n            return result\n\n    def close(self):\n        if hasattr(self,'proc'):\n            self.proc.terminate()\n            self.proc.stdout.close()\n            self.proc.stderr.close()\n            del self.proc\n\n    def __del__(self):\n        self.close()\n        if hasattr(self,'lastread'):\n            del self.lastread",
  "def ffmpeg_read_image(filename, with_mask=True):\n    \"\"\" Read an image file (PNG, BMP, JPEG...).\n\n    Wraps FFMPEG_Videoreader to read just one image.\n    Returns an ImageClip.\n\n    This function is not meant to be used directly in MoviePy,\n    use ImageClip instead to make clips out of image files.\n\n    Parameters\n    -----------\n\n    filename\n      Name of the image file. Can be of any format supported by ffmpeg.\n\n    with_mask\n      If the image has a transparency layer, ``with_mask=true`` will save\n      this layer as the mask of the returned ImageClip\n\n    \"\"\"\n    if with_mask:\n        pix_fmt = 'rgba'\n    else:\n        pix_fmt = \"rgb24\"\n    reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt, check_duration=False)\n    im = reader.lastread\n    del reader\n    return im",
  "def ffmpeg_parse_infos(filename, print_infos=False, check_duration=True):\n    \"\"\"Get file infos using ffmpeg.\n\n    Returns a dictionnary with the fields:\n    \"video_found\", \"video_fps\", \"duration\", \"video_nframes\",\n    \"video_duration\", \"audio_found\", \"audio_fps\"\n\n    \"video_duration\" is slightly smaller than \"duration\" to avoid\n    fetching the uncomplete frames at the end, which raises an error.\n\n    \"\"\"\n\n\n    # open the file in a pipe, provoke an error, read output\n    is_GIF = filename.endswith('.gif')\n    cmd = [get_setting(\"FFMPEG_BINARY\"), \"-i\", filename]\n    if is_GIF:\n        cmd += [\"-f\", \"null\", \"/dev/null\"]\n\n    popen_params = {\"bufsize\": 10**5,\n                    \"stdout\": sp.PIPE,\n                    \"stderr\": sp.PIPE,\n                    \"stdin\": DEVNULL}\n\n    if os.name == \"nt\":\n        popen_params[\"creationflags\"] = 0x08000000\n\n    proc = sp.Popen(cmd, **popen_params)\n\n    proc.stdout.readline()\n    proc.terminate()\n    infos = proc.stderr.read().decode('utf8')\n    del proc\n\n    if print_infos:\n        # print the whole info text returned by FFMPEG\n        print( infos )\n\n\n    lines = infos.splitlines()\n    if \"No such file or directory\" in lines[-1]:\n        raise IOError((\"MoviePy error: the file %s could not be found !\\n\"\n                      \"Please check that you entered the correct \"\n                      \"path.\")%filename)\n\n    result = dict()\n\n\n    # get duration (in seconds)\n    result['duration'] = None\n\n    if check_duration:\n        try:\n            keyword = ('frame=' if is_GIF else 'Duration: ')\n            line = [l for l in lines if keyword in l][0]\n            match = re.findall(\"([0-9][0-9]:[0-9][0-9]:[0-9][0-9].[0-9][0-9])\", line)[0]\n            result['duration'] = cvsecs(match)\n        except:\n            raise IOError((\"MoviePy error: failed to read the duration of file %s.\\n\"\n                           \"Here are the file infos returned by ffmpeg:\\n\\n%s\")%(\n                              filename, infos))\n\n    # get the output line that speaks about video\n    lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n\n    result['video_found'] = ( lines_video != [] )\n\n    if result['video_found']:\n\n\n        try:\n            line = lines_video[0]\n\n            # get the size, of the form 460x320 (w x h)\n            match = re.search(\" [0-9]*x[0-9]*(,| )\", line)\n            s = list(map(int, line[match.start():match.end()-1].split('x')))\n            result['video_size'] = s\n        except:\n            raise IOError((\"MoviePy error: failed to read video dimensions in file %s.\\n\"\n                           \"Here are the file infos returned by ffmpeg:\\n\\n%s\")%(\n                              filename, infos))\n\n\n        # get the frame rate. Sometimes it's 'tbr', sometimes 'fps', sometimes\n        # tbc, and sometimes tbc/2...\n        # Current policy: Trust tbr first, then fps. If result is near from x*1000/1001\n        # where x is 23,24,25,50, replace by x*1000/1001 (very common case for the fps).\n\n        try:\n            match = re.search(\"( [0-9]*.| )[0-9]* tbr\", line)\n            tbr = float(line[match.start():match.end()].split(' ')[1])\n            result['video_fps'] = tbr\n\n        except:\n            match = re.search(\"( [0-9]*.| )[0-9]* fps\", line)\n            result['video_fps'] = float(line[match.start():match.end()].split(' ')[1])\n\n\n        # It is known that a fps of 24 is often written as 24000/1001\n        # but then ffmpeg nicely rounds it to 23.98, which we hate.\n        coef = 1000.0/1001.0\n        fps = result['video_fps']\n        for x in [23,24,25,30,50]:\n            if (fps!=x) and abs(fps - x*coef) < .01:\n                result['video_fps'] = x*coef\n\n        if check_duration:\n            result['video_nframes'] = int(result['duration']*result['video_fps'])+1\n            result['video_duration'] = result['duration']\n        else:\n            result['video_nframes'] = 1\n            result['video_duration'] = None\n        # We could have also recomputed the duration from the number\n        # of frames, as follows:\n        # >>> result['video_duration'] = result['video_nframes'] / result['video_fps']\n\n\n    lines_audio = [l for l in lines if ' Audio: ' in l]\n\n    result['audio_found'] = lines_audio != []\n\n    if result['audio_found']:\n        line = lines_audio[0]\n        try:\n            match = re.search(\" [0-9]* Hz\", line)\n            result['audio_fps'] = int(line[match.start()+1:match.end()])\n        except:\n            result['audio_fps'] = 'unknown'\n\n    return result",
  "def __init__(self, filename, print_infos=False, bufsize = None,\n                 pix_fmt=\"rgb24\", check_duration=True):\n\n        self.filename = filename\n        infos = ffmpeg_parse_infos(filename, print_infos, check_duration)\n        self.fps = infos['video_fps']\n        self.size = infos['video_size']\n        self.duration = infos['video_duration']\n        self.ffmpeg_duration = infos['duration']\n        self.nframes = infos['video_nframes']\n\n        self.infos = infos\n\n        self.pix_fmt = pix_fmt\n        if pix_fmt == 'rgba':\n            self.depth = 4\n        else:\n            self.depth = 3\n\n        if bufsize is None:\n            w, h = self.size\n            bufsize = self.depth * w * h + 100\n\n        self.bufsize= bufsize\n        self.initialize()\n\n\n        self.pos = 1\n        self.lastread = self.read_frame()",
  "def initialize(self, starttime=0):\n        \"\"\"Opens the file, creates the pipe. \"\"\"\n\n        self.close() # if any\n\n        if starttime != 0 :\n            offset = min(1, starttime)\n            i_arg = ['-ss', \"%.06f\" % (starttime - offset),\n                     '-i', self.filename,\n                     '-ss', \"%.06f\" % offset]\n        else:\n            i_arg = [ '-i', self.filename]\n\n\n        cmd = ([get_setting(\"FFMPEG_BINARY\")]+ i_arg +\n                ['-loglevel', 'error',\n                '-f', 'image2pipe',\n                \"-pix_fmt\", self.pix_fmt,\n                '-vcodec', 'rawvideo', '-'])\n\n        popen_params = {\"bufsize\": self.bufsize,\n                        \"stdout\": sp.PIPE,\n                        \"stderr\": sp.PIPE,\n                        \"stdin\": DEVNULL}\n\n        if os.name == \"nt\":\n            popen_params[\"creationflags\"] = 0x08000000\n\n        self.proc = sp.Popen(cmd, **popen_params)",
  "def skip_frames(self, n=1):\n        \"\"\"Reads and throws away n frames \"\"\"\n        w, h = self.size\n        for i in range(n):\n            self.proc.stdout.read(self.depth*w*h)\n            #self.proc.stdout.flush()\n        self.pos += n",
  "def read_frame(self):\n        w, h = self.size\n        nbytes= self.depth*w*h\n\n        s = self.proc.stdout.read(nbytes)\n        if len(s) != nbytes:\n\n            warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n                   \"%d bytes wanted but %d bytes read,\"%(nbytes, len(s))+\n                   \"at frame %d/%d, at time %.02f/%.02f sec. \"%(\n                    self.pos,self.nframes,\n                    1.0*self.pos/self.fps,\n                    self.duration)+\n                   \"Using the last valid frame instead.\",\n                   UserWarning)\n\n            if not hasattr(self, 'lastread'):\n                raise IOError((\"MoviePy error: failed to read the first frame of \"\n                               \"video file %s. That might mean that the file is \"\n                               \"corrupted. That may also mean that you are using \"\n                               \"a deprecated version of FFMPEG. On Ubuntu/Debian \"\n                               \"for instance the version in the repos is deprecated. \"\n                               \"Please update to a recent version from the website.\")%(\n                                self.filename))\n\n            result = self.lastread\n\n        else:\n\n            result = np.fromstring(s, dtype='uint8')\n            result.shape =(h, w, len(s)//(w*h)) # reshape((h, w, len(s)//(w*h)))\n            self.lastread = result\n\n        return result",
  "def get_frame(self, t):\n        \"\"\" Read a file video frame at time t.\n\n        Note for coders: getting an arbitrary frame in the video with\n        ffmpeg can be painfully slow if some decoding has to be done.\n        This function tries to avoid fectching arbitrary frames\n        whenever possible, by moving between adjacent frames.\n        \"\"\"\n\n        # these definitely need to be rechecked sometime. Seems to work.\n        \n        # I use that horrible '+0.00001' hack because sometimes due to numerical\n        # imprecisions a 3.0 can become a 2.99999999... which makes the int()\n        # go to the previous integer. This makes the fetching more robust in the\n        # case where you get the nth frame by writing get_frame(n/fps).\n        \n        pos = int(self.fps*t + 0.00001)+1\n\n        if pos == self.pos:\n            return self.lastread\n        else:\n            if(pos < self.pos) or (pos > self.pos+100):\n                self.initialize(t)\n                self.pos = pos\n            else:\n                self.skip_frames(pos-self.pos-1)\n            result = self.read_frame()\n            self.pos = pos\n            return result",
  "def close(self):\n        if hasattr(self,'proc'):\n            self.proc.terminate()\n            self.proc.stdout.close()\n            self.proc.stderr.close()\n            del self.proc",
  "def __del__(self):\n        self.close()\n        if hasattr(self,'lastread'):\n            del self.lastread",
  "def html_embed(clip, filetype=None, maxduration=60, rd_kwargs=None,\n               center=True, **html_kwargs):\n    \"\"\" Returns HTML5 code embedding the clip\n    \n    clip\n      Either a file name, or a clip to preview.\n      Either an image, a sound or a video. Clips will actually be\n      written to a file and embedded as if a filename was provided.\n\n\n    filetype\n      One of 'video','image','audio'. If None is given, it is determined\n      based on the extension of ``filename``, but this can bug.\n    \n    rd_kwargs\n      keyword arguments for the rendering, like {'fps':15, 'bitrate':'50k'}\n    \n\n    **html_kwargs\n      Allow you to give some options, like width=260, autoplay=True,\n      loop=1 etc.\n\n    Examples\n    =========\n\n    >>> import moviepy.editor as mpy\n    >>> # later ...\n    >>> clip.write_videofile(\"test.mp4\")\n    >>> mpy.ipython_display(\"test.mp4\", width=360)\n\n    >>> clip.audio.write_audiofile('test.ogg') # Sound !\n    >>> mpy.ipython_display('test.ogg')\n\n    >>> clip.write_gif(\"test.gif\")\n    >>> mpy.ipython_display('test.gif')\n\n    >>> clip.save_frame(\"first_frame.jpeg\")\n    >>> mpy.ipython_display(\"first_frame.jpeg\")\n\n    \"\"\"  \n    \n    if rd_kwargs is None:\n        rd_kwargs = {}\n\n    if \"Clip\" in str(clip.__class__):\n        TEMP_PREFIX = \"__temp__\"\n        if isinstance(clip,ImageClip):\n            filename = TEMP_PREFIX+\".png\"\n            kwargs = {'filename':filename, 'withmask':True}\n            kwargs.update(rd_kwargs)\n            clip.save_frame(**kwargs)\n        elif isinstance(clip,VideoClip):\n            filename = TEMP_PREFIX+\".mp4\"\n            kwargs = {'filename':filename, 'verbose':False, 'preset':'ultrafast'}\n            kwargs.update(rd_kwargs)\n            clip.write_videofile(**kwargs)\n        elif isinstance(clip,AudioClip):\n            filename = TEMP_PREFIX+\".mp3\"\n            kwargs = {'filename': filename, 'verbose':False}\n            kwargs.update(rd_kwargs)\n            clip.write_audiofile(**kwargs)\n        else:\n          raise ValueError(\"Unknown class for the clip. Cannot embed and preview.\")\n\n        return html_embed(filename, maxduration=maxduration, rd_kwargs=rd_kwargs,\n                           center=center, **html_kwargs)\n    \n    filename = clip\n    options = \" \".join([\"%s='%s'\"%(str(k), str(v)) for k,v in html_kwargs.items()])\n    name, ext = os.path.splitext(filename)\n    ext = ext[1:]\n\n    if filetype is None:\n        ext = filename.split('.')[-1].lower()\n        if ext == \"gif\":\n            filetype = 'image'\n        elif ext in extensions_dict:\n            filetype = extensions_dict[ext]['type']\n        else:\n            raise ValueError(\"No file type is known for the provided file. Please provide \"\n                             \"argument `filetype` (one of 'image', 'video', 'sound') to the \"\n                             \"ipython display function.\")\n    \n    \n    if filetype== 'video':\n        # The next lines set the HTML5-cvompatible extension and check that the\n        # extension is HTML5-valid\n        exts_htmltype = {'mp4': 'mp4', 'webm':'webm', 'ogv':'ogg'}\n        allowed_exts = \" \".join(exts_htmltype.keys()) \n        try:\n            ext = exts_htmltype[ext]\n        except:\n            raise ValueError(\"This video extension cannot be displayed in the \"\n                   \"IPython Notebook. Allowed extensions: \"+allowed_exts)\n    \n    if filetype in ['audio', 'video']:\n\n        duration = ffmpeg_parse_infos(filename)['duration']\n        if duration > maxduration:\n            raise ValueError(\"The duration of video %s (%.1f) exceeds the 'max_duration' \"%(filename, duration)+\n                             \"attribute. You can increase 'max_duration', \"\n                             \"but note that embedding large videos may take all the memory away !\")\n            \n    with open(filename, \"rb\") as f:\n        data= b64encode(f.read()).decode(\"utf-8\")\n\n    template = templates[filetype]\n\n    result = template%{'data':data, 'options':options, 'ext':ext}\n    if center:\n        result = r\"<div align=middle>%s</div>\"%result\n\n    return result",
  "def ipython_display(clip, filetype=None, maxduration=60, t=None, fps=None,\n                    rd_kwargs=None, center=True, **html_kwargs):\n    \"\"\"\n    clip\n      Either the name of a file, or a clip to preview. The clip will\n      actually be written to a file and embedded as if a filename was\n      provided.\n\n    filetype:\n      One of 'video','image','audio'. If None is given, it is determined\n      based on the extension of ``filename``, but this can bug.\n\n    maxduration\n      An error will be raised if the clip's duration is more than the indicated\n      value (in seconds), to avoid spoiling the  browser's cache and the RAM.\n\n    t\n      If not None, only the frame at time t will be displayed in the notebook,\n      instead of a video of the clip\n\n    fps\n      Enables to specify an fps, as required for clips whose fps is unknown.\n    \n    **kwargs:\n      Allow you to give some options, like width=260, etc. When editing\n      looping gifs, a good choice is loop=1, autoplay=1.\n    \n    Remarks: If your browser doesn't support HTML5, this should warn you.\n    If nothing is displayed, maybe your file or filename is wrong.\n    Important: The media will be physically embedded in the notebook.\n\n    Examples\n    =========\n\n    >>> import moviepy.editor as mpy\n    >>> # later ...\n    >>> clip.write_videofile(\"test.mp4\")\n    >>> mpy.ipython_display(\"test.mp4\", width=360)\n\n    >>> clip.audio.write_audiofile('test.ogg') # Sound !\n    >>> mpy.ipython_display('test.ogg')\n\n    >>> clip.write_gif(\"test.gif\")\n    >>> mpy.ipython_display('test.gif')\n\n    >>> clip.save_frame(\"first_frame.jpeg\")\n    >>> mpy.ipython_display(\"first_frame.jpeg\")\n    \"\"\"\n        \n    if not ipython_available:\n        raise ImportError(\"Only works inside an IPython Notebook\")\n\n    if rd_kwargs is None:\n        rd_kwargs = {}\n        \n    if fps is not None:\n        rd_kwargs['fps'] = fps\n\n    if t is not None:\n        clip = clip.to_ImageClip(t)\n\n    return HTML2(html_embed(clip, filetype=filetype, maxduration=maxduration,\n                center=center, rd_kwargs=rd_kwargs, **html_kwargs))",
  "class HTML2(HTML):\n        def __add__(self, other):\n            return HTML2(self.data+other.data)",
  "def __add__(self, other):\n            return HTML2(self.data+other.data)",
  "def credits1(creditfile,width,stretch=30,color='white',\n                 stroke_color='black', stroke_width=2,\n                 font='Impact-Normal',fontsize=60):\n    \"\"\"\n    \n    \n    Parameters\n    -----------\n    \n    creditfile\n      A text file whose content must be as follows: ::\n        \n        # This is a comment\n        # The next line says : leave 4 blank lines\n        .blank 4\n        \n        ..Executive Story Editor\n        MARCEL DURAND\n        \n        ..Associate Producers\n        MARTIN MARCEL\n        DIDIER MARTIN\n        \n        ..Music Supervisor\n        JEAN DIDIER\n    \n    width\n      Total width of the credits text in pixels\n      \n    gap\n      Gap in pixels between the jobs and the names.\n    \n    **txt_kw\n      Additional argument passed to TextClip (font, colors, etc.)\n    \n    \n    \n        \n    Returns\n    ---------\n    \n    image\n       An ImageClip instance that looks like this and can be scrolled\n       to make some credits :\n        \n        Executive Story Editor    MARCEL DURAND\n           Associate Producers    MARTIN MARCEL\n                                  DIDIER MARTIN\n              Music Supervisor    JEAN DIDIER\n              \n    \"\"\"\n    \n    \n    # PARSE THE TXT FILE\n    \n    with open(creditfile) as f:\n        lines = f.readlines()\n    \n    lines = filter(lambda x:not x.startswith('\\n'),lines) \n    texts = []\n    oneline=True\n    for l in  lines:\n        if not l.startswith('#'):\n            if l.startswith('.blank'):\n                for i in range(int(l.split(' ')[1])):\n                    texts.append(['\\n','\\n'])\n            elif  l.startswith('..'):\n                texts.append([l[2:],''])\n                oneline=True\n            else:\n                if oneline:\n                    texts.append(['',l])\n                    oneline=False\n                else:\n                    texts.append(['\\n',l])\n               \n    left,right = [ \"\".join(l) for l in zip(*texts)]\n    \n    # MAKE TWO COLUMNS FOR THE CREDITS\n    \n    left,right =  [TextClip(txt,color=color,stroke_color=stroke_color,\n                                stroke_width=stroke_width,font=font,\n                                fontsize=fontsize,align=al)\n               for txt,al in [(left,'East'),(right,'West')]]\n               \n\n    cc = CompositeVideoClip( [left, right.set_pos((left.w+gap,0))],\n                             size = (left.w+right.w+gap,right.h),\n                             transparent=True)\n    \n    # SCALE TO THE REQUIRED SIZE\n    \n    scaled = cc.fx(resize , width=width)\n    \n    # TRANSFORM THE WHOLE CREDIT CLIP INTO AN ImageCLip\n    \n    imclip = ImageClip(scaled.get_frame(0))\n    amask = ImageClip(scaled.mask.get_frame(0),ismask=True)\n    \n    return imclip.set_mask(amask)",
  "def blit(im1, im2, pos=[0, 0], mask=None, ismask=False):\n    \"\"\" Blit an image over another.\n    \n    Blits ``im1`` on ``im2`` as position ``pos=(x,y)``, using the\n    ``mask`` if provided. If ``im1`` and ``im2`` are mask pictures\n    (2D float arrays) then ``ismask`` must be ``True``.\n    \"\"\"\n\n    # xp1,yp1,xp2,yp2 = blit area on im2\n    # x1,y1,x2,y2 = area of im1 to blit on im2\n    xp, yp = pos\n    x1 = max(0, -xp)\n    y1 = max(0, -yp)\n    h1, w1 = im1.shape[:2]\n    h2, w2 = im2.shape[:2]\n    xp2 = min(w2, xp + w1)\n    yp2 = min(h2, yp + h1)\n    x2 = min(w1, w2 - xp)\n    y2 = min(h1, h2 - yp)\n    xp1 = max(0, xp)\n    yp1 = max(0, yp)\n\n    if (xp1 >= xp2) or (yp1 >= yp2):\n        return im2\n\n    blitted = im1[y1:y2, x1:x2]\n\n    new_im2 = +im2\n\n    if mask is not None:\n        mask = mask[y1:y2, x1:x2]\n        if len(im1.shape) == 3:\n            mask = np.dstack(3 * [mask])\n        blit_region = new_im2[yp1:yp2, xp1:xp2]\n        new_im2[yp1:yp2, xp1:xp2] = (\n            1.0 * mask * blitted + (1.0 - mask) * blit_region)\n    else:\n        new_im2[yp1:yp2, xp1:xp2] = blitted\n\n    return new_im2.astype('uint8') if (not ismask) else new_im2",
  "def color_gradient(size,p1,p2=None,vector=None, r=None, col1=0,col2=1.0,\n              shape='linear', offset = 0):\n    \"\"\"Draw a linear, bilinear, or radial gradient.\n    \n    The result is a picture of size ``size``, whose color varies\n    gradually from color `col1` in position ``p1`` to color ``col2``\n    in position ``p2``.\n    \n    If it is a RGB picture the result must be transformed into\n    a 'uint8' array to be displayed normally:\n     \n     \n    Parameters\n    ------------      \n    \n    size\n      Size (width, height) in pixels of the final picture/array.\n    \n    p1, p2\n      Coordinates (x,y) in pixels of the limit point for ``col1``\n      and ``col2``. The color 'before' ``p1`` is ``col1`` and it\n      gradually changes in the direction of ``p2`` until it is ``col2``\n      when it reaches ``p2``.\n    \n    vector\n      A vector [x,y] in pixels that can be provided instead of ``p2``.\n      ``p2`` is then defined as (p1 + vector).\n    \n    col1, col2\n      Either floats between 0 and 1 (for gradients used in masks)\n      or [R,G,B] arrays (for colored gradients).\n               \n    shape\n      'linear', 'bilinear', or 'circular'.\n      In a linear gradient the color varies in one direction,\n      from point ``p1`` to point ``p2``.\n      In a bilinear gradient it also varies symetrically form ``p1``\n      in the other direction.\n      In a circular gradient it goes from ``col1`` to ``col2`` in all\n      directions.\n    \n    offset\n      Real number between 0 and 1 indicating the fraction of the vector\n      at which the gradient actually starts. For instance if ``offset``\n      is 0.9 in a gradient going from p1 to p2, then the gradient will\n      only occur near p2 (before that everything is of color ``col1``)\n      If the offset is 0.9 in a radial gradient, the gradient will\n      occur in the region located between 90% and 100% of the radius,\n      this creates a blurry disc of radius d(p1,p2).  \n    \n    Returns\n    --------\n    \n    image\n      An Numpy array of dimensions (W,H,ncolors) of type float\n      representing the image of the gradient.\n      \n    \n    Examples\n    ---------\n    \n    >>> grad = color_gradient(blabla).astype('uint8')\n    \n    \"\"\"\n    \n    # np-arrayize and change x,y coordinates to y,x\n    w,h = size\n    \n    col1, col2 = map(lambda x : np.array(x).astype(float), [col1, col2])\n    \n    if shape == 'bilinear':\n        if vector is None:\n            vector = np.array(p2) - np.array(p1)\n        m1,m2 = [ color_gradient(size, p1, vector=v, col1 = 1.0, col2 = 0,\n                           shape = 'linear', offset= offset)\n                  for v in [vector,-vector]]\n                  \n        arr = np.maximum(m1,m2)\n        if col1.size > 1:\n            arr = np.dstack(3*[arr])\n        return arr*col1 + (1-arr)*col2\n        \n    \n    p1 = np.array(p1[::-1]).astype(float)\n    \n    if vector is None:\n        if p2 is not None:\n            p2 = np.array(p2[::-1])\n            vector = p2-p1\n    else:\n        vector = np.array(vector[::-1])\n        p2 = p1 + vector\n    \n    if vector is not None:    \n        norm = np.linalg.norm(vector)\n    \n    M = np.dstack(np.meshgrid(range(w),range(h))[::-1]).astype(float)\n    \n    if shape == 'linear':\n        \n        n_vec = vector/norm**2 # norm 1/norm(vector)\n        \n        p1 = p1 + offset*vector\n        arr = (M- p1).dot(n_vec)/(1-offset)\n        arr = np.minimum(1,np.maximum(0,arr))\n        if col1.size > 1:\n            arr = np.dstack(3*[arr])\n        return arr*col1 + (1-arr)*col2\n    \n    elif shape == 'radial':\n        if r is None:\n            r = norm\n        if r==0:\n            arr = np.ones((h,w))\n        else:\n            arr = (np.sqrt(((M- p1)**2).sum(axis=2)))-offset*r\n            arr = arr / ((1-offset)*r)\n            arr = np.minimum(1.0,np.maximum(0, arr) )\n            \n        if col1.size > 1:\n            arr = np.dstack(3*[arr])\n        return (1-arr)*col1 + arr*col2",
  "def color_split(size,x=None,y=None,p1=None,p2=None,vector=None,\n               col1=0,col2=1.0, grad_width=0):\n    \"\"\"Make an image splitted in 2 colored regions.\n    \n    Returns an array of size ``size`` divided in two regions called 1 and\n    2 in wht follows, and which will have colors col& and col2\n    respectively.\n    \n    Parameters\n    -----------\n    \n    x: (int)\n      If provided, the image is splitted horizontally in x, the left\n      region being region 1.\n        \n    y: (int)\n      If provided, the image is splitted vertically in y, the top region\n      being region 1.\n    \n    p1,p2:\n      Positions (x1,y1),(x2,y2) in pixels, where the numbers can be\n      floats. Region 1 is defined as the whole region on the left when\n      going from ``p1`` to ``p2``.\n    \n    p1, vector:\n      ``p1`` is (x1,y1) and vector (v1,v2), where the numbers can be\n      floats. Region 1 is then the region on the left when starting\n      in position ``p1`` and going in the direction given by ``vector``.\n       \n    gradient_width\n      If not zero, the split is not sharp, but gradual over a region of\n      width ``gradient_width`` (in pixels). This is preferable in many\n      situations (for instance for antialiasing). \n     \n    \n    Examples\n    ---------\n    \n    >>> size = [200,200]\n    >>> # an image with all pixels with x<50 =0, the others =1\n    >>> color_split(size, x=50, col1=0, col2=1)\n    >>> # an image with all pixels with y<50 red, the others green\n    >>> color_split(size, x=50, col1=[255,0,0], col2=[0,255,0])\n    >>> # An image splitted along an arbitrary line (see below) \n    >>> color_split(size, p1=[20,50], p2=[25,70] col1=0, col2=1)\n        \n    \"\"\"\n    \n    if grad_width or ( (x is None) and (y is None)):\n        if p2 is not None:\n            vector = (np.array(p2) - np.array(p1))\n        elif x is not None:\n            vector = np.array([0,-1.0])\n            p1 = np.array([x, 0])\n        elif y is not None:\n            vector = np.array([1.0, 0.0])\n            p1 = np.array([0,y])\n\n        x,y = vector\n        vector = np.array([y,-x]).astype('float')\n        norm = np.linalg.norm(vector)\n        vector =  max(0.1,grad_width)*vector/norm\n        return color_gradient(size,p1,vector=vector,\n                         col1 = col1, col2 = col2, shape='linear')\n    else:\n        \n        w,h = size\n        shape = (h, w) if np.isscalar(col1) else (h, w, len(col1))\n        arr = np.zeros(shape)\n        if x:\n            arr[:,:x] = col1\n            arr[:,x:] = col2\n        elif y:\n            arr[:y] = col1\n            arr[y:] = col2\n            \n        return arr\n     \n    # if we are here, it means we didn't exit with a proper 'return'\n    print( \"Arguments in color_split not understood !\" )\n    raise",
  "def circle(screensize, center, radius, col1=1.0, col2=0, blur=1):\n    \"\"\" Draw an image with a circle.\n    \n    Draws a circle of color ``col1``, on a background of color ``col2``,\n    on a screen of size ``screensize`` at the position ``center=(x,y)``,\n    with a radius ``radius`` but slightly blurred on the border by ``blur``\n    pixels\n    \"\"\"\n    return color_gradient(screensize,p1=center,r=radius, col1=col1,\n              col2=col2, shape='radial', offset = 0 if (radius==0) else\n                                              1.0*(radius-blur)/radius)",
  "def find_video_period(clip,fps=None,tmin=.3):\n    \"\"\" Finds the period of a video based on frames correlation \"\"\"\n    \n\n    frame = lambda t: clip.get_frame(t).flatten()\n    tt = np.arange(tmin,clip.duration,1.0/ fps)[1:]\n    ref = frame(0)\n    corrs = [ np.corrcoef(ref, frame(t))[0,1] for t in tt]\n    return tt[np.argmax(corrs)]",
  "class FramesMatch:\n    \"\"\"\n    \n    Parameters\n    -----------\n\n    t1\n      Starting time\n\n    t2\n      End time\n\n    d_min\n      Lower bound on the distance between the first and last frames\n\n    d_max\n      Upper bound on the distance between the first and last frames\n\n    \"\"\"\n\n    def __init__(self, t1, t2, d_min, d_max):\n        self.t1 = t1\n        self.t2 = t2\n        self.d_min = d_min\n        self.d_max = d_max\n        self.time_span = t2-t1\n\n    def __str__(self):\n\n        return '(%.04f, %.04f, %.04f, %.04f)'%(\n                self.t1, self.t2, self.d_min, self.d_max)\n\n    def __repr__(self):\n        return '(%.04f, %.04f, %.04f, %.04f)'%(\n                self.t1, self.t2, self.d_min, self.d_max)\n\n    def __iter__(self):\n        return [self.t1, self.t2, self.d_min, self.d_max].__iter__()",
  "class FramesMatches(list):\n\n    def __init__(self, lst):\n\n        list.__init__(self, sorted(lst, key=lambda e: e.d_max))\n\n    def best(self, n=1, percent=None):\n        if percent is not None:\n            n = len(self)*percent/100\n        return self[0] if n==1 else FramesMatches(self[:n])\n    \n    def filter(self, cond):\n        \"\"\"\n        Returns a FramesMatches object obtained by filtering out the FramesMatch\n        which do not satistify the condition ``cond``. ``cond`` is a function\n        (FrameMatch -> bool).\n\n        Examples\n        ---------\n        >>> # Only keep the matches corresponding to (> 1 second) sequences.\n        >>> new_matches = matches.filter( lambda match: match.time_span > 1)\n        \"\"\"\n        return FramesMatches(filter(cond, self))\n\n    def save(self, filename):\n        np.savetxt(filename, np.array([np.array(list(e)) for e in self]),\n                   fmt='%.03f', delimiter='\\t')\n\n    @staticmethod\n    def load(filename):\n        \"\"\" Loads a FramesMatches object from a file.\n        >>> matching_frames = FramesMatches.load(\"somefile\")\n        \"\"\"\n        arr = np.loadtxt(filename)\n        mfs = [FramesMatch(*e) for e in arr]\n        return FramesMatches(mfs)\n\n        \n    \n    @staticmethod\n    def from_clip(clip, dist_thr, max_d, fps=None):\n        \"\"\" Finds all the frames tht look alike in a clip, for instance to make a\n        looping gif.\n\n        This teturns a  FramesMatches object of the all pairs of frames with\n        (t2-t1 < max_d) and whose distance is under dist_thr.\n\n        This is well optimized routine and quite fast.\n\n        Examples\n        ---------\n        \n        We find all matching frames in a given video and turn the best match with\n        a duration of 1.5s or more into a GIF:\n\n        >>> from moviepy.editor import VideoFileClip\n        >>> from moviepy.video.tools.cuts import find_matching_frames\n        >>> clip = VideoFileClip(\"foo.mp4\").resize(width=200)\n        >>> matches = find_matching_frames(clip, 10, 3) # will take time\n        >>> best = matches.filter(lambda m: m.time_span > 1.5).best()\n        >>> clip.subclip(best.t1, best.t2).write_gif(\"foo.gif\")\n\n        Parameters\n        -----------\n\n        clip\n          A MoviePy video clip, possibly transformed/resized\n        \n        dist_thr\n          Distance above which a match is rejected\n        \n        max_d\n          Maximal duration (in seconds) between two matching frames\n        \n        fps\n          Frames per second (default will be clip.fps)\n        \n        \"\"\" \n        \n        N_pixels = clip.w * clip.h * 3\n        dot_product = lambda F1, F2: (F1*F2).sum()/N_pixels\n        F = {} # will store the frames and their mutual distances\n        \n        def distance(t1, t2):\n            uv = dot_product(F[t1]['frame'], F[t2]['frame'])\n            u, v = F[t1]['|F|sq'], F[t2]['|F|sq']\n            return np.sqrt(u+v - 2*uv)\n        \n        matching_frames = [] # the final result.\n        \n        for (t,frame) in clip.iter_frames(with_times=True, progress_bar=True):\n            \n            flat_frame = 1.0*frame.flatten()\n            F_norm_sq = dot_product(flat_frame, flat_frame)\n            F_norm = np.sqrt(F_norm_sq)\n            \n            for t2 in list(F.keys()):\n                # forget old frames, add 't' to the others frames\n                # check for early rejections based on differing norms\n                if (t-t2) > max_d:\n                    F.pop(t2)\n                else:\n                    F[t2][t] = {'min':abs(F[t2]['|F|'] - F_norm),\n                                'max':F[t2]['|F|'] + F_norm}\n                    F[t2][t]['rejected']= (F[t2][t]['min'] > dist_thr)\n            \n            t_F = sorted(F.keys())\n            \n            F[t] = {'frame': flat_frame, '|F|sq': F_norm_sq, '|F|': F_norm}\n                    \n            for i,t2 in enumerate(t_F):\n                # Compare F(t) to all the previous frames\n                \n                if F[t2][t]['rejected']:\n                    continue\n                \n                dist = distance(t, t2)\n                F[t2][t]['min'] = F[t2][t]['max'] = dist\n                F[t2][t]['rejected']  = (dist >= dist_thr)\n                \n                for t3 in t_F[i+1:]:\n                    # For all the next times t3, use d(F(t), F(t2)) to\n                    # update the bounds on d(F(t), F(t3)). See if you can\n                    # conclude on wether F(t) and F(t3) match.\n                    t3t, t2t3 = F[t3][t], F[t2][t3]\n                    t3t['max'] = min(t3t['max'], dist+ t2t3['max'])\n                    t3t['min'] = max(t3t['min'], dist - t2t3['max'],\n                                     t2t3['min'] - dist)\n                                          \n                    if t3t['min'] > dist_thr:\n                        t3t['rejected'] = True\n        \n            # Store all the good matches (t2,t)\n            matching_frames += [(t1, t, F[t1][t]['min'], F[t1][t]['max']) for t1 in F\n                                if (t1!=t) and not F[t1][t]['rejected']]\n                       \n        return FramesMatches([FramesMatch(*e) for e in matching_frames])\n\n\n\n    def select_scenes(self, match_thr, min_time_span, nomatch_thr=None,\n                      time_distance=0):\n        \"\"\"\n\n        match_thr\n          The smaller, the better-looping the gifs are.\n\n        min_time_span\n          Only GIFs with a duration longer than min_time_span (in seconds)\n          will be extracted.\n\n        nomatch_thr\n          If None, then it is chosen equal to match_thr\n\n        \"\"\"\n\n        if nomatch_thr is None:\n            nomatch_thr = match_thr\n\n        \n        dict_starts = defaultdict(lambda : [])\n        for (start, end, d_min, d_max) in self:\n            dict_starts[start].append([end, d_min, d_max])\n\n        starts_ends = sorted(dict_starts.items(), key = lambda k: k[0])\n        \n        result = []\n        min_start= 0\n        for start, ends_distances in starts_ends:\n\n            if start < min_start:\n                continue\n\n            ends = [end for (end, d_min, d_max) in ends_distances]\n            great_matches = [(end,d_min, d_max)\n                             for (end,d_min, d_max) in ends_distances\n                             if d_max<match_thr]\n            \n            great_long_matches = [(end,d_min, d_max)\n                                  for (end,d_min, d_max) in great_matches\n                                  if (end-start)>min_time_span]\n            \n            \n            if (great_long_matches == []):\n                continue # No GIF can be made starting at this time\n            \n            poor_matches = set([end for (end,d_min, d_max) in ends_distances\n                            if d_min>nomatch_thr])\n            short_matches = [end for end in ends\n                             if (end-start)<=0.6]\n            \n            if len( poor_matches.intersection(short_matches) ) == 0 :\n                continue\n    \n    \n            end = max([end for (end, d_min, d_max) in great_long_matches])\n            end, d_min, d_max = [e for e in great_long_matches if e[0]==end][0]\n            result.append(FramesMatch(start, end, d_min, d_max))\n            min_start = start + time_distance\n\n        return FramesMatches( result )\n\n\n    def write_gifs(self, clip, gif_dir):\n        \"\"\"\n\n        \"\"\"\n\n        for (start, end, _, _) in self: \n            name = \"%s/%08d_%08d.gif\"%(gif_dir, 100*start, 100*end)\n            clip.subclip(start, end).write_gif(name, verbose=False)",
  "def detect_scenes(clip=None, luminosities=None, thr=10,\n                  progress_bar=False, fps=None):\n    \"\"\" Detects scenes of a clip based on luminosity changes.\n    \n    Note that for large clip this may take some time\n    \n    Returns\n    --------\n    cuts, luminosities\n      cuts is a series of cuts [(0,t1), (t1,t2),...(...,tf)]\n      luminosities are the luminosities computed for each\n      frame of the clip.\n    \n    Parameters\n    -----------\n    \n    clip\n      A video clip. Can be None if a list of luminosities is\n      provided instead. If provided, the luminosity of each\n      frame of the clip will be computed. If the clip has no\n      'fps' attribute, you must provide it.\n    \n    luminosities\n      A list of luminosities, e.g. returned by detect_scenes\n      in a previous run.\n    \n    thr\n      Determines a threshold above which the 'luminosity jumps'\n      will be considered as scene changes. A scene change is defined\n      as a change between 2 consecutive frames that is larger than\n      (avg * thr) where avg is the average of the absolute changes\n      between consecutive frames.\n      \n    progress_bar\n      We all love progress bars ! Here is one for you, in option.\n      \n    fps\n      Must be provided if you provide no clip or a clip without\n      fps attribute.\n    \n    \n    \n    \n    \"\"\"\n        \n    if luminosities is None:\n        luminosities = [f.sum() for f in clip.iter_frames(\n                             fps=fps, dtype='uint32', progress_bar=1)]\n    \n    luminosities = np.array(luminosities, dtype=float)\n    if clip is not None:\n        end = clip.duration\n    else:\n        end = len(luminosities)*(1.0/fps) \n    lum_diffs = abs(np.diff(luminosities))\n    avg = lum_diffs.mean()\n    luminosity_jumps = 1+np.array(np.nonzero(lum_diffs> thr*avg))[0]\n    tt = [0]+list((1.0/fps) *luminosity_jumps) + [end]\n    #print tt\n    cuts = [(t1,t2) for t1,t2 in zip(tt,tt[1:])]\n    return cuts, luminosities",
  "def __init__(self, t1, t2, d_min, d_max):\n        self.t1 = t1\n        self.t2 = t2\n        self.d_min = d_min\n        self.d_max = d_max\n        self.time_span = t2-t1",
  "def __str__(self):\n\n        return '(%.04f, %.04f, %.04f, %.04f)'%(\n                self.t1, self.t2, self.d_min, self.d_max)",
  "def __repr__(self):\n        return '(%.04f, %.04f, %.04f, %.04f)'%(\n                self.t1, self.t2, self.d_min, self.d_max)",
  "def __iter__(self):\n        return [self.t1, self.t2, self.d_min, self.d_max].__iter__()",
  "def __init__(self, lst):\n\n        list.__init__(self, sorted(lst, key=lambda e: e.d_max))",
  "def best(self, n=1, percent=None):\n        if percent is not None:\n            n = len(self)*percent/100\n        return self[0] if n==1 else FramesMatches(self[:n])",
  "def filter(self, cond):\n        \"\"\"\n        Returns a FramesMatches object obtained by filtering out the FramesMatch\n        which do not satistify the condition ``cond``. ``cond`` is a function\n        (FrameMatch -> bool).\n\n        Examples\n        ---------\n        >>> # Only keep the matches corresponding to (> 1 second) sequences.\n        >>> new_matches = matches.filter( lambda match: match.time_span > 1)\n        \"\"\"\n        return FramesMatches(filter(cond, self))",
  "def save(self, filename):\n        np.savetxt(filename, np.array([np.array(list(e)) for e in self]),\n                   fmt='%.03f', delimiter='\\t')",
  "def load(filename):\n        \"\"\" Loads a FramesMatches object from a file.\n        >>> matching_frames = FramesMatches.load(\"somefile\")\n        \"\"\"\n        arr = np.loadtxt(filename)\n        mfs = [FramesMatch(*e) for e in arr]\n        return FramesMatches(mfs)",
  "def from_clip(clip, dist_thr, max_d, fps=None):\n        \"\"\" Finds all the frames tht look alike in a clip, for instance to make a\n        looping gif.\n\n        This teturns a  FramesMatches object of the all pairs of frames with\n        (t2-t1 < max_d) and whose distance is under dist_thr.\n\n        This is well optimized routine and quite fast.\n\n        Examples\n        ---------\n        \n        We find all matching frames in a given video and turn the best match with\n        a duration of 1.5s or more into a GIF:\n\n        >>> from moviepy.editor import VideoFileClip\n        >>> from moviepy.video.tools.cuts import find_matching_frames\n        >>> clip = VideoFileClip(\"foo.mp4\").resize(width=200)\n        >>> matches = find_matching_frames(clip, 10, 3) # will take time\n        >>> best = matches.filter(lambda m: m.time_span > 1.5).best()\n        >>> clip.subclip(best.t1, best.t2).write_gif(\"foo.gif\")\n\n        Parameters\n        -----------\n\n        clip\n          A MoviePy video clip, possibly transformed/resized\n        \n        dist_thr\n          Distance above which a match is rejected\n        \n        max_d\n          Maximal duration (in seconds) between two matching frames\n        \n        fps\n          Frames per second (default will be clip.fps)\n        \n        \"\"\" \n        \n        N_pixels = clip.w * clip.h * 3\n        dot_product = lambda F1, F2: (F1*F2).sum()/N_pixels\n        F = {} # will store the frames and their mutual distances\n        \n        def distance(t1, t2):\n            uv = dot_product(F[t1]['frame'], F[t2]['frame'])\n            u, v = F[t1]['|F|sq'], F[t2]['|F|sq']\n            return np.sqrt(u+v - 2*uv)\n        \n        matching_frames = [] # the final result.\n        \n        for (t,frame) in clip.iter_frames(with_times=True, progress_bar=True):\n            \n            flat_frame = 1.0*frame.flatten()\n            F_norm_sq = dot_product(flat_frame, flat_frame)\n            F_norm = np.sqrt(F_norm_sq)\n            \n            for t2 in list(F.keys()):\n                # forget old frames, add 't' to the others frames\n                # check for early rejections based on differing norms\n                if (t-t2) > max_d:\n                    F.pop(t2)\n                else:\n                    F[t2][t] = {'min':abs(F[t2]['|F|'] - F_norm),\n                                'max':F[t2]['|F|'] + F_norm}\n                    F[t2][t]['rejected']= (F[t2][t]['min'] > dist_thr)\n            \n            t_F = sorted(F.keys())\n            \n            F[t] = {'frame': flat_frame, '|F|sq': F_norm_sq, '|F|': F_norm}\n                    \n            for i,t2 in enumerate(t_F):\n                # Compare F(t) to all the previous frames\n                \n                if F[t2][t]['rejected']:\n                    continue\n                \n                dist = distance(t, t2)\n                F[t2][t]['min'] = F[t2][t]['max'] = dist\n                F[t2][t]['rejected']  = (dist >= dist_thr)\n                \n                for t3 in t_F[i+1:]:\n                    # For all the next times t3, use d(F(t), F(t2)) to\n                    # update the bounds on d(F(t), F(t3)). See if you can\n                    # conclude on wether F(t) and F(t3) match.\n                    t3t, t2t3 = F[t3][t], F[t2][t3]\n                    t3t['max'] = min(t3t['max'], dist+ t2t3['max'])\n                    t3t['min'] = max(t3t['min'], dist - t2t3['max'],\n                                     t2t3['min'] - dist)\n                                          \n                    if t3t['min'] > dist_thr:\n                        t3t['rejected'] = True\n        \n            # Store all the good matches (t2,t)\n            matching_frames += [(t1, t, F[t1][t]['min'], F[t1][t]['max']) for t1 in F\n                                if (t1!=t) and not F[t1][t]['rejected']]\n                       \n        return FramesMatches([FramesMatch(*e) for e in matching_frames])",
  "def select_scenes(self, match_thr, min_time_span, nomatch_thr=None,\n                      time_distance=0):\n        \"\"\"\n\n        match_thr\n          The smaller, the better-looping the gifs are.\n\n        min_time_span\n          Only GIFs with a duration longer than min_time_span (in seconds)\n          will be extracted.\n\n        nomatch_thr\n          If None, then it is chosen equal to match_thr\n\n        \"\"\"\n\n        if nomatch_thr is None:\n            nomatch_thr = match_thr\n\n        \n        dict_starts = defaultdict(lambda : [])\n        for (start, end, d_min, d_max) in self:\n            dict_starts[start].append([end, d_min, d_max])\n\n        starts_ends = sorted(dict_starts.items(), key = lambda k: k[0])\n        \n        result = []\n        min_start= 0\n        for start, ends_distances in starts_ends:\n\n            if start < min_start:\n                continue\n\n            ends = [end for (end, d_min, d_max) in ends_distances]\n            great_matches = [(end,d_min, d_max)\n                             for (end,d_min, d_max) in ends_distances\n                             if d_max<match_thr]\n            \n            great_long_matches = [(end,d_min, d_max)\n                                  for (end,d_min, d_max) in great_matches\n                                  if (end-start)>min_time_span]\n            \n            \n            if (great_long_matches == []):\n                continue # No GIF can be made starting at this time\n            \n            poor_matches = set([end for (end,d_min, d_max) in ends_distances\n                            if d_min>nomatch_thr])\n            short_matches = [end for end in ends\n                             if (end-start)<=0.6]\n            \n            if len( poor_matches.intersection(short_matches) ) == 0 :\n                continue\n    \n    \n            end = max([end for (end, d_min, d_max) in great_long_matches])\n            end, d_min, d_max = [e for e in great_long_matches if e[0]==end][0]\n            result.append(FramesMatch(start, end, d_min, d_max))\n            min_start = start + time_distance\n\n        return FramesMatches( result )",
  "def write_gifs(self, clip, gif_dir):\n        \"\"\"\n\n        \"\"\"\n\n        for (start, end, _, _) in self: \n            name = \"%s/%08d_%08d.gif\"%(gif_dir, 100*start, 100*end)\n            clip.subclip(start, end).write_gif(name, verbose=False)",
  "def distance(t1, t2):\n            uv = dot_product(F[t1]['frame'], F[t2]['frame'])\n            u, v = F[t1]['|F|sq'], F[t2]['|F|sq']\n            return np.sqrt(u+v - 2*uv)",
  "class SubtitlesClip(VideoClip):\n    \"\"\" A Clip that serves as \"subtitle track\" in videos.\n    \n    One particularity of this class is that the images of the\n    subtitle texts are not generated beforehand, but only if\n    needed.\n\n    Parameters\n    ==========\n\n    subtitles\n      Either the name of a file, or a list\n\n    Examples\n    =========\n    \n    >>> from moviepy.video.tools.subtitles import SubtitlesClip\n    >>> from moviepy.video.io.VideoFileClip import VideoFileClip\n    >>> generator = lambda txt: TextClip(txt, font='Georgia-Regular',\n                                        fontsize=24, color='white')\n    >>> sub = SubtitlesClip(\"subtitles.srt\", generator)\n    >>> myvideo = VideoFileClip(\"myvideo.avi\")\n    >>> final = CompositeVideoClip([clip, subtitles])\n    >>> final.to_videofile(\"final.mp4\", fps=myvideo.fps)\n    \n    \"\"\"\n\n    def __init__(self, subtitles, make_textclip=None):\n        \n        VideoClip.__init__(self, has_constant_size=False)\n\n        if isinstance( subtitles, str):\n            subtitles = file_to_subtitles(subtitles)\n\n        subtitles = [(map(cvsecs, tt),txt) for tt, txt in subtitles]\n        self.subtitles = subtitles\n        self.textclips = dict()\n\n        if make_textclip is None:\n\n            make_textclip = lambda txt: TextClip(txt, font='Georgia-Bold',\n                                        fontsize=24, color='white',\n                                        stroke_color='black', stroke_width=0.5)\n\n        self.make_textclip = make_textclip\n        self.start=0\n        self.duration = max([tb for ((ta,tb), txt) in self.subtitles])\n        self.end=self.duration\n        \n        def add_textclip_if_none(t):\n            \"\"\" Will generate a textclip if it hasn't been generated asked\n            to generate it yet. If there is no subtitle to show at t, return\n            false. \"\"\"\n            sub =[((ta,tb),txt) for ((ta,tb),txt) in self.textclips.keys()\n                   if (ta<=t<tb)]\n            if sub == []:\n                sub = [((ta,tb),txt) for ((ta,tb),txt) in self.subtitles if\n                       (ta<=t<tb)]\n                if sub == []:\n                    return False\n            sub = sub[0]\n            if sub not in self.textclips.keys():\n                self.textclips[sub] = self.make_textclip(sub[1])\n\n            return sub\n\n        def make_frame(t):\n            sub = add_textclip_if_none(t)\n            return (self.textclips[sub].get_frame(t) if sub\n                    else np.array([[[0,0,0]]]))\n\n        def make_mask_frame(t):\n            sub = add_textclip_if_none(t)\n            return (self.textclips[sub].mask.get_frame(t) if sub\n                    else np.array([[0]]))\n        \n        self.make_frame = make_frame\n        hasmask = (self.make_textclip('T').mask is not None)\n        self.mask = (VideoClip(make_mask_frame, ismask=True) if hasmask else None)\n\n    def in_subclip(self, t_start= None, t_end= None):\n        \"\"\" Returns a sequence of [(t1,t2), txt] covering all the given subclip\n        from t_start to t_end. The first and last times will be cropped so as\n        to be exactly t_start and t_end if possible. \"\"\"\n\n        def is_in_subclip(t1,t2):\n            try:\n                return (t_start<=t1<t_end) or (t_start< t2 <=t_end)\n            except:\n                return False\n        def try_cropping(t1,t2):\n            try:\n                return (max(t1, t_start), min(t2, t_end))\n            except:\n                return (t1, t2)\n        return [(try_cropping(t1,t2), txt) for ((t1,t2), txt) in self.subtitles\n                                               if is_in_subclip(t1,t2)]\n    \n\n\n    def __iter__(self):\n        return self.subtitles.__iter__()\n    \n\n\n    def __getitem__(self, k):\n        return self.subtitles[k]\n\n    \n\n    def __str__(self):\n\n        def to_srt(sub_element):\n            (ta, tb), txt = sub_element\n            fta, ftb = map(time_to_string, (ta, tb))\n            return \"%s - %s\\n%s\"%(fta, ftb, txt)\n        \n        return \"\\n\\n\".join(map(to_srt, self.subtitles))\n    \n\n\n    def match_expr(self, expr):\n\n        return SubtitlesClip([e for e in self.subtitles\n                              if re.findall(expr, e[1]) != []])\n    \n\n    def write_srt(self, filename):\n        with open(filename, 'w+') as f:\n            f.write(str(self))",
  "def file_to_subtitles(filename):\n    \"\"\" Converts a srt file into subtitles.\n\n    The returned list is of the form ``[((ta,tb),'some text'),...]``\n    and can be fed to SubtitlesClip.\n\n    Only works for '.srt' format for the moment.\n    \"\"\"\n\n    with open(filename,'r') as f:\n        lines = f.readlines()\n\n    times_texts = []\n    current_times , current_text = None, \"\"\n    \n    for line in lines:\n        times = re.findall(\"([0-9]*:[0-9]*:[0-9]*,[0-9]*)\", line)\n        if times != []:\n            current_times = map(cvsecs, times)\n        elif line.strip() == '':\n            times_texts.append((current_times, current_text.strip('\\n')))\n            current_times, current_text = None, \"\"\n        elif current_times is not None:\n            current_text = current_text + line\n    return times_texts",
  "def __init__(self, subtitles, make_textclip=None):\n        \n        VideoClip.__init__(self, has_constant_size=False)\n\n        if isinstance( subtitles, str):\n            subtitles = file_to_subtitles(subtitles)\n\n        subtitles = [(map(cvsecs, tt),txt) for tt, txt in subtitles]\n        self.subtitles = subtitles\n        self.textclips = dict()\n\n        if make_textclip is None:\n\n            make_textclip = lambda txt: TextClip(txt, font='Georgia-Bold',\n                                        fontsize=24, color='white',\n                                        stroke_color='black', stroke_width=0.5)\n\n        self.make_textclip = make_textclip\n        self.start=0\n        self.duration = max([tb for ((ta,tb), txt) in self.subtitles])\n        self.end=self.duration\n        \n        def add_textclip_if_none(t):\n            \"\"\" Will generate a textclip if it hasn't been generated asked\n            to generate it yet. If there is no subtitle to show at t, return\n            false. \"\"\"\n            sub =[((ta,tb),txt) for ((ta,tb),txt) in self.textclips.keys()\n                   if (ta<=t<tb)]\n            if sub == []:\n                sub = [((ta,tb),txt) for ((ta,tb),txt) in self.subtitles if\n                       (ta<=t<tb)]\n                if sub == []:\n                    return False\n            sub = sub[0]\n            if sub not in self.textclips.keys():\n                self.textclips[sub] = self.make_textclip(sub[1])\n\n            return sub\n\n        def make_frame(t):\n            sub = add_textclip_if_none(t)\n            return (self.textclips[sub].get_frame(t) if sub\n                    else np.array([[[0,0,0]]]))\n\n        def make_mask_frame(t):\n            sub = add_textclip_if_none(t)\n            return (self.textclips[sub].mask.get_frame(t) if sub\n                    else np.array([[0]]))\n        \n        self.make_frame = make_frame\n        hasmask = (self.make_textclip('T').mask is not None)\n        self.mask = (VideoClip(make_mask_frame, ismask=True) if hasmask else None)",
  "def in_subclip(self, t_start= None, t_end= None):\n        \"\"\" Returns a sequence of [(t1,t2), txt] covering all the given subclip\n        from t_start to t_end. The first and last times will be cropped so as\n        to be exactly t_start and t_end if possible. \"\"\"\n\n        def is_in_subclip(t1,t2):\n            try:\n                return (t_start<=t1<t_end) or (t_start< t2 <=t_end)\n            except:\n                return False\n        def try_cropping(t1,t2):\n            try:\n                return (max(t1, t_start), min(t2, t_end))\n            except:\n                return (t1, t2)\n        return [(try_cropping(t1,t2), txt) for ((t1,t2), txt) in self.subtitles\n                                               if is_in_subclip(t1,t2)]",
  "def __iter__(self):\n        return self.subtitles.__iter__()",
  "def __getitem__(self, k):\n        return self.subtitles[k]",
  "def __str__(self):\n\n        def to_srt(sub_element):\n            (ta, tb), txt = sub_element\n            fta, ftb = map(time_to_string, (ta, tb))\n            return \"%s - %s\\n%s\"%(fta, ftb, txt)\n        \n        return \"\\n\\n\".join(map(to_srt, self.subtitles))",
  "def match_expr(self, expr):\n\n        return SubtitlesClip([e for e in self.subtitles\n                              if re.findall(expr, e[1]) != []])",
  "def write_srt(self, filename):\n        with open(filename, 'w+') as f:\n            f.write(str(self))",
  "def add_textclip_if_none(t):\n            \"\"\" Will generate a textclip if it hasn't been generated asked\n            to generate it yet. If there is no subtitle to show at t, return\n            false. \"\"\"\n            sub =[((ta,tb),txt) for ((ta,tb),txt) in self.textclips.keys()\n                   if (ta<=t<tb)]\n            if sub == []:\n                sub = [((ta,tb),txt) for ((ta,tb),txt) in self.subtitles if\n                       (ta<=t<tb)]\n                if sub == []:\n                    return False\n            sub = sub[0]\n            if sub not in self.textclips.keys():\n                self.textclips[sub] = self.make_textclip(sub[1])\n\n            return sub",
  "def make_frame(t):\n            sub = add_textclip_if_none(t)\n            return (self.textclips[sub].get_frame(t) if sub\n                    else np.array([[[0,0,0]]]))",
  "def make_mask_frame(t):\n            sub = add_textclip_if_none(t)\n            return (self.textclips[sub].mask.get_frame(t) if sub\n                    else np.array([[0]]))",
  "def is_in_subclip(t1,t2):\n            try:\n                return (t_start<=t1<t_end) or (t_start< t2 <=t_end)\n            except:\n                return False",
  "def try_cropping(t1,t2):\n            try:\n                return (max(t1, t_start), min(t2, t_end))\n            except:\n                return (t1, t2)",
  "def to_srt(sub_element):\n            (ta, tb), txt = sub_element\n            fta, ftb = map(time_to_string, (ta, tb))\n            return \"%s - %s\\n%s\"%(fta, ftb, txt)",
  "class Interpolator:\n    \"\"\" Poorman's linear interpolator, doesn't require Scipy. \"\"\"\n    \n    def __init__(self, tt=None, ss=None, ttss = None, left=None, right=None):\n\n        if ttss is not None:\n            tt, ss = zip(*ttss)\n        \n        self.tt = 1.0*np.array(tt)\n        self.ss = 1.0*np.array(ss)\n        self.left = left\n        self.right = right\n        self.tmin, self.tmax = min(tt), max(tt)\n\n    def __call__(self, t):\n\n        return np.interp(t, self.tt, self.ss, self.left, self.right)",
  "class Trajectory:\n\n    def __init__(self, tt, xx, yy):\n\n        self.tt = 1.0*np.array(tt)\n        self.xx = np.array(xx)\n        self.yy = np.array(yy)\n        self.update_interpolators()\n\n    def __call__(self, t):\n        return np.array([self.xi(t), self.yi(t)])\n\n    def addx(self, x):\n\n        return Trajectory(self.tt, self.xx+x, self.yy)\n\n    def addy(self, y):\n\n        return Trajectory(self.tt, self.xx+y, self.yy)\n\n    def update_interpolators(self):\n        self.xi =  Interpolator(self.tt, self.xx)\n        self.yi =  Interpolator(self.tt, self.yy)\n    \n    def txy(self, tms=False):\n        return zip((1000 if tms else 1)*self.tt, self.xx, self.yy)\n\n    def to_file(self, filename):\n        np.savetxt(filename, np.array(self.txy(tms=True)),\n                   fmt=\"%d\", delimiter='\\t')\n\n    @staticmethod\n    def from_file(filename):\n        arr = np.loadtxt(filename, delimiter='\\t')\n        tt, xx, yy = arr.T\n        return Trajectory(1.0*tt/1000, xx, yy)\n\n    @staticmethod\n    def save_list(trajs, filename):\n        N = len(trajs)\n        arr = np.hstack([np.array(t.txy(tms=True)) for t in trajs])\n        np.savetxt( filename, arr, fmt=\"%d\", delimiter='\\t',\n                    header = \"\\t\".join(N*['t (ms)', 'x', 'y']))\n    \n    @staticmethod\n    def load_list(filename):\n\n        arr = np.loadtxt(filename, delimiter='\\t').T\n        Nlines = arr.shape[0]\n        return [Trajectory(tt=1.0*a[0]/1000, xx=a[1], yy=a[2])\n                for a in np.split(arr, Nlines/3)]",
  "def __init__(self, tt=None, ss=None, ttss = None, left=None, right=None):\n\n        if ttss is not None:\n            tt, ss = zip(*ttss)\n        \n        self.tt = 1.0*np.array(tt)\n        self.ss = 1.0*np.array(ss)\n        self.left = left\n        self.right = right\n        self.tmin, self.tmax = min(tt), max(tt)",
  "def __call__(self, t):\n\n        return np.interp(t, self.tt, self.ss, self.left, self.right)",
  "def __init__(self, tt, xx, yy):\n\n        self.tt = 1.0*np.array(tt)\n        self.xx = np.array(xx)\n        self.yy = np.array(yy)\n        self.update_interpolators()",
  "def __call__(self, t):\n        return np.array([self.xi(t), self.yi(t)])",
  "def addx(self, x):\n\n        return Trajectory(self.tt, self.xx+x, self.yy)",
  "def addy(self, y):\n\n        return Trajectory(self.tt, self.xx+y, self.yy)",
  "def update_interpolators(self):\n        self.xi =  Interpolator(self.tt, self.xx)\n        self.yi =  Interpolator(self.tt, self.yy)",
  "def txy(self, tms=False):\n        return zip((1000 if tms else 1)*self.tt, self.xx, self.yy)",
  "def to_file(self, filename):\n        np.savetxt(filename, np.array(self.txy(tms=True)),\n                   fmt=\"%d\", delimiter='\\t')",
  "def from_file(filename):\n        arr = np.loadtxt(filename, delimiter='\\t')\n        tt, xx, yy = arr.T\n        return Trajectory(1.0*tt/1000, xx, yy)",
  "def save_list(trajs, filename):\n        N = len(trajs)\n        arr = np.hstack([np.array(t.txy(tms=True)) for t in trajs])\n        np.savetxt( filename, arr, fmt=\"%d\", delimiter='\\t',\n                    header = \"\\t\".join(N*['t (ms)', 'x', 'y']))",
  "def load_list(filename):\n\n        arr = np.loadtxt(filename, delimiter='\\t').T\n        Nlines = arr.shape[0]\n        return [Trajectory(tt=1.0*a[0]/1000, xx=a[1], yy=a[2])\n                for a in np.split(arr, Nlines/3)]",
  "def manual_tracking(clip, t1=None, t2=None, fps=None, nobjects = 1,\n                    savefile = None):\n    \"\"\"\n    Allows manual tracking of an object(s) in the video clip between\n    times `t1` and `t2`. This displays the clip frame by frame\n    and you must click on the object(s) in each frame. If ``t2=None``\n    only the frame at ``t1`` is taken into account.\n    \n    Returns a list [(t1,x1,y1),(t2,x2,y2) etc... ] if there is one\n    object per frame, else returns a list whose elements are of the \n    form (ti, [(xi1,yi1), (xi2,yi2), ...] )\n    \n    Parameters\n    -------------\n\n    t1,t2:\n      times during which to track (defaults are start and\n      end of the clip). t1 and t2 can be expressed in seconds\n      like 15.35, in (min, sec), in (hour, min, sec), or as a\n      string: '01:03:05.35'.\n    fps:\n      Number of frames per second to freeze on. If None, the clip's\n      fps attribute is used instead.\n    nobjects:\n      Number of objects to click on each frame.\n    savefile:\n      If provided, the result is saved to a file, which makes\n      it easier to edit and re-use later.\n\n    Examples\n    ---------\n    \n    >>> from moviepy.editor import VideoFileClip\n    >>> from moviepy.tools.tracking import manual_tracking\n    >>> clip = VideoFileClip(\"myvideo.mp4\")\n    >>> # manually indicate 3 trajectories, save them to a file\n    >>> trajectories = manual_tracking(clip, t1=5, t2=7, fps=5,\n                                       nobjects=3, savefile=\"track.txt\")\n    >>> # ...\n    >>> # LATER, IN ANOTHER SCRIPT, RECOVER THESE TRAJECTORIES\n    >>> from moviepy.tools.tracking import Trajectory\n    >>> traj1, traj2, traj3 = Trajectory.load_list('track.txt')\n    >>> # If ever you only have one object being tracked, recover it with\n    >>> traj, =  Trajectory.load_list('track.txt')\n    \n    \"\"\"\n    \n    import pygame as pg\n\n    screen = pg.display.set_mode(clip.size)\n    step = 1.0 / fps\n    if (t1 is None) and (t2 is None):\n        t1,t2 = 0, clip.duration\n    elif (t2 is None):\n        t2 = t1 + step / 2\n    t = t1\n    txy_list = []\n    \n    def gatherClicks(t):\n        \n        imdisplay(clip.get_frame(t), screen)\n        objects_to_click = nobjects\n        clicks = []\n        while objects_to_click:\n\n            for event in pg.event.get():\n\n                if event.type == pg.KEYDOWN:\n                    if (event.key == pg.K_BACKSLASH):\n                        return \"return\"\n                    elif (event.key == pg.K_ESCAPE):\n                        raise KeyboardInterrupt()\n                        \n\n                elif event.type == pg.MOUSEBUTTONDOWN:\n                    x, y = pg.mouse.get_pos()\n                    clicks.append((x, y))\n                    objects_to_click -= 1\n                    \n        return clicks\n        \n    while t < t2:\n        \n        clicks  =gatherClicks(t)\n        if clicks == 'return':\n            txy_list.pop()\n            t -= step\n        else:\n            txy_list.append((t,clicks))\n            t += step\n\n    tt, xylist = zip(*txy_list) \n    result = []\n    for i in range(nobjects):\n        xys = [e[i] for e in xylist]\n        xx, yy = zip(*xys)\n        result.append(Trajectory(tt, xx, yy))\n    \n    if savefile is not None:\n        Trajectory.save_list(result, savefile)\n    return result",
  "def findAround(pic,pat,xy=None,r=None):\n    \"\"\"\n    find image pattern ``pat`` in ``pic[x +/- r, y +/- r]``.\n    if xy is none, consider the whole picture.\n    \"\"\"\n    \n    if xy and r:\n        h,w = pat.shape[:2]\n        x,y = xy\n        pic = pic[y-r : y+h+r , x-r : x+w+r]\n        \n    matches = cv2.matchTemplate(pat,pic,cv2.TM_CCOEFF_NORMED)\n    yf,xf = np.unravel_index(matches.argmax(),matches.shape)\n    return (x-r+xf,y-r+yf) if (xy and r) else (xf,yf)",
  "def autoTrack(clip, pattern, tt=None, fps=None, radius=20, xy0=None):\n    \"\"\"\n    Tracks a given pattern (small image array) in a video clip.\n    Returns [(x1,y1),(x2,y2)...] where xi,yi are\n    the coordinates of the pattern in the clip on frame i.\n    To select the frames you can either specify a list of times with ``tt``\n    or select a frame rate with ``fps``.\n    This algorithm assumes that the pattern's aspect does not vary much\n    and that the distance between two occurences of the pattern in\n    two consecutive frames is smaller than ``radius`` (if you set ``radius``\n    to -1 the pattern will be searched in the whole screen at each frame).\n    You can also provide the original position of the pattern with xy0.\n    \"\"\"\n\n    if not autotracking_possible:\n        raise IOError(\"Sorry, autotrack requires OpenCV for the moment. \"\n                      \"Install OpenCV (aka cv2) to use it.\")\n\n\n    if not xy0:\n        xy0 = findAround(clip.get_frame(tt[0]),pattern)\n    \n    if tt is None:\n        tt = np.arange(0, clip.duration, 1.0/fps)\n        \n    xys = [xy0]\n    for t in tt[1:]:\n        xys.append( findAround(clip.get_frame(t),pattern,\n                               xy=xys[-1],r=radius))\n    \n    xx,yy = zip(*xys)\n\n    return Trajectory(tt, xx, yy)",
  "def gatherClicks(t):\n        \n        imdisplay(clip.get_frame(t), screen)\n        objects_to_click = nobjects\n        clicks = []\n        while objects_to_click:\n\n            for event in pg.event.get():\n\n                if event.type == pg.KEYDOWN:\n                    if (event.key == pg.K_BACKSLASH):\n                        return \"return\"\n                    elif (event.key == pg.K_ESCAPE):\n                        raise KeyboardInterrupt()\n                        \n\n                elif event.type == pg.MOUSEBUTTONDOWN:\n                    x, y = pg.mouse.get_pos()\n                    clicks.append((x, y))\n                    objects_to_click -= 1\n                    \n        return clicks",
  "def findObjects(clip,rem_thr=500, preview=False):\n    \"\"\" \n    Returns a list of ImageClips representing each a separate object on\n    the screen.\n        \n    rem_thr : all objects found with size < rem_Thr will be\n         considered false positives and will be removed\n    \n    \"\"\"\n    \n    image = clip.get_frame(0)\n    if clip.mask is None:\n        clip = clip.add_mask()\n        \n    mask = clip.mask.get_frame(0)\n    labelled, num_features = ndi.measurements.label(image[:,:,0])\n    \n    #find the objects\n    slices = ndi.find_objects(labelled)\n    # cool trick to remove letter holes (in o,e,a, etc.)\n    slices = [e for e in slices if  mask[e[0],e[1]].mean() >0.2]\n    # remove very small slices\n    slices = [e for e in slices if  image[e[0],e[1]].size > rem_thr]\n    # Sort the slices from left to right\n    islices = sorted(enumerate(slices), key = lambda s : s[1][1].start)\n    \n    letters = []\n    for i,(ind,(sy,sx)) in enumerate(islices):\n        \"\"\" crop each letter separately \"\"\"\n        sy = slice(sy.start-1,sy.stop+1)\n        sx = slice(sx.start-1,sx.stop+1)\n        letter = image[sy,sx]\n        labletter = labelled[sy,sx]\n        maskletter = (labletter==(ind+1))*mask[sy,sx]\n        letter = ImageClip(image[sy,sx])\n        letter.mask = ImageClip( maskletter,ismask=True)\n        letter.screenpos = np.array((sx.start,sy.start))\n        letters.append(letter)\n    \n    if preview:\n        import matplotlib.pyplot as plt\n        print( \"found %d objects\"%(num_features) )\n        fig,ax = plt.subplots(2)\n        ax[0].axis('off')\n        ax[0].imshow(labelled)\n        ax[1].imshow([range(num_features)],interpolation='nearest')\n        ax[1].set_yticks([])\n        plt.show()\n    \n    return letters"
]