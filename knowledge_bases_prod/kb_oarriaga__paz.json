[
  "class DrawInferences(Callback):\n    \"\"\"Saves an image with its corresponding inferences\n\n    # Arguments\n        save_path: String. Path in which the images will be saved.\n        images: List of numpy arrays of shape.\n        pipeline: Function that takes as input an element of ''images''\n            and outputs a ''Dict'' with inferences.\n        topic: Key to the ''inferences'' dictionary containing as value the\n            drawn inferences.\n        verbose: Integer. If is bigger than 1 messages would be displayed.\n    \"\"\"\n    def __init__(self, save_path, images, pipeline, topic='image', verbose=1):\n        super(DrawInferences, self).__init__()\n        self.save_path = os.path.join(save_path, 'images')\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n        self.pipeline = pipeline\n        self.images = images\n        self.topic = topic\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs=None):\n        for image_arg, image in enumerate(self.images):\n            inferences = self.pipeline(image.copy())\n            epoch_name = 'epoch_%03d' % epoch\n            save_path = os.path.join(self.save_path, epoch_name)\n            if not os.path.exists(save_path):\n                os.makedirs(save_path)\n            image_name = 'image_%03d.png' % image_arg\n            image_name = os.path.join(save_path, image_name)\n            write_image(image_name, inferences[self.topic])\n        if self.verbose:\n            print('Saving predicted images in:', self.save_path)",
  "class LearningRateScheduler(Callback):\n    \"\"\" Callback for reducing learning rate at specific epochs.\n\n    # Arguments\n        learning_rate: float. Indicates the starting learning rate.\n        gamma_decay: float. In an scheduled epoch the learning rate\n            is multiplied by this factor.\n        scheduled_epochs: List of integers. Indicates in which epochs\n            the learning rate will be multiplied by the gamma decay factor.\n        verbose: Integer. If is bigger than 1 messages would be displayed.\n    \"\"\"\n    def __init__(\n            self, learning_rate, gamma_decay, scheduled_epochs, verbose=1):\n        super(LearningRateScheduler, self).__init__()\n        self.learning_rate = learning_rate\n        self.gamma_decay = gamma_decay\n        self.scheduled_epochs = scheduled_epochs\n        self.verbose = verbose\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n\n        learning_rate = float(K.get_value(self.model.optimizer.lr))\n        learning_rate = self.schedule(epoch)\n        if not isinstance(learning_rate, (float, np.float32, np.float64)):\n            raise ValueError('Learning rate should be float.')\n        K.set_value(self.model.optimizer.lr, learning_rate)\n        if self.verbose > 0:\n            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\n                  'rate to %s.' % (epoch + 1, learning_rate))\n\n    def schedule(self, epoch):\n        if epoch in self.scheduled_epochs:\n            self.learning_rate = self.learning_rate * self.gamma_decay\n        return self.learning_rate",
  "class EvaluateMAP(Callback):\n    \"\"\"Evaluates mean average precision (MAP) of an object detector.\n\n    # Arguments\n        data_manager: Data manager and loader class. See ''paz.datasets''\n            for examples.\n        detector: Tensorflow-Keras model.\n        period: Int. Indicates how often the evaluation is performed.\n        save_path: Str.\n        iou_thresh: Float.\n    \"\"\"\n    def __init__(\n            self, data_manager, detector, period, save_path, iou_thresh=0.5):\n        super(EvaluateMAP, self).__init__()\n        self.data_manager = data_manager\n        self.detector = detector\n        self.period = period\n        self.save_path = save_path\n        self.dataset = data_manager.load_data()\n        self.iou_thresh = iou_thresh\n        self.class_names = self.data_manager.class_names\n        self.class_dict = {}\n        for class_arg, class_name in enumerate(self.class_names):\n            self.class_dict[class_name] = class_arg\n\n    def on_epoch_end(self, epoch, logs):\n        if (epoch + 1) % self.period == 0:\n            result = evaluateMAP(\n                self.detector,\n                self.dataset,\n                self.class_dict,\n                iou_thresh=self.iou_thresh,\n                use_07_metric=True)\n\n            result_str = 'mAP: {:.4f}\\n'.format(result['map'])\n            metrics = {'mAP': result['map']}\n            for arg, ap in enumerate(result['ap']):\n                if arg == 0 or np.isnan(ap):  # skip background\n                    continue\n                metrics[self.class_names[arg]] = ap\n                result_str += '{:<16}: {:.4f}\\n'.format(\n                    self.class_names[arg], ap)\n            print(result_str)\n\n            # Saving the evaluation results\n            filename = os.path.join(self.save_path, 'MAP_Evaluation_Log.txt')\n            with open(filename, 'a') as eval_log_file:\n                eval_log_file.write('Epoch: {}\\n{}\\n'.format(\n                    str(epoch), result_str))",
  "def __init__(self, save_path, images, pipeline, topic='image', verbose=1):\n        super(DrawInferences, self).__init__()\n        self.save_path = os.path.join(save_path, 'images')\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n        self.pipeline = pipeline\n        self.images = images\n        self.topic = topic\n        self.verbose = verbose",
  "def on_epoch_end(self, epoch, logs=None):\n        for image_arg, image in enumerate(self.images):\n            inferences = self.pipeline(image.copy())\n            epoch_name = 'epoch_%03d' % epoch\n            save_path = os.path.join(self.save_path, epoch_name)\n            if not os.path.exists(save_path):\n                os.makedirs(save_path)\n            image_name = 'image_%03d.png' % image_arg\n            image_name = os.path.join(save_path, image_name)\n            write_image(image_name, inferences[self.topic])\n        if self.verbose:\n            print('Saving predicted images in:', self.save_path)",
  "def __init__(\n            self, learning_rate, gamma_decay, scheduled_epochs, verbose=1):\n        super(LearningRateScheduler, self).__init__()\n        self.learning_rate = learning_rate\n        self.gamma_decay = gamma_decay\n        self.scheduled_epochs = scheduled_epochs\n        self.verbose = verbose",
  "def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n\n        learning_rate = float(K.get_value(self.model.optimizer.lr))\n        learning_rate = self.schedule(epoch)\n        if not isinstance(learning_rate, (float, np.float32, np.float64)):\n            raise ValueError('Learning rate should be float.')\n        K.set_value(self.model.optimizer.lr, learning_rate)\n        if self.verbose > 0:\n            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\n                  'rate to %s.' % (epoch + 1, learning_rate))",
  "def schedule(self, epoch):\n        if epoch in self.scheduled_epochs:\n            self.learning_rate = self.learning_rate * self.gamma_decay\n        return self.learning_rate",
  "def __init__(\n            self, data_manager, detector, period, save_path, iou_thresh=0.5):\n        super(EvaluateMAP, self).__init__()\n        self.data_manager = data_manager\n        self.detector = detector\n        self.period = period\n        self.save_path = save_path\n        self.dataset = data_manager.load_data()\n        self.iou_thresh = iou_thresh\n        self.class_names = self.data_manager.class_names\n        self.class_dict = {}\n        for class_arg, class_name in enumerate(self.class_names):\n            self.class_dict[class_name] = class_arg",
  "def on_epoch_end(self, epoch, logs):\n        if (epoch + 1) % self.period == 0:\n            result = evaluateMAP(\n                self.detector,\n                self.dataset,\n                self.class_dict,\n                iou_thresh=self.iou_thresh,\n                use_07_metric=True)\n\n            result_str = 'mAP: {:.4f}\\n'.format(result['map'])\n            metrics = {'mAP': result['map']}\n            for arg, ap in enumerate(result['ap']):\n                if arg == 0 or np.isnan(ap):  # skip background\n                    continue\n                metrics[self.class_names[arg]] = ap\n                result_str += '{:<16}: {:.4f}\\n'.format(\n                    self.class_names[arg], ap)\n            print(result_str)\n\n            # Saving the evaluation results\n            filename = os.path.join(self.save_path, 'MAP_Evaluation_Log.txt')\n            with open(filename, 'a') as eval_log_file:\n                eval_log_file.write('Epoch: {}\\n{}\\n'.format(\n                    str(epoch), result_str))",
  "class KeypointNetLoss(object):\n    \"\"\"KeypointNet loss for discovering latent keypoints.\n\n    # Arguments\n        num_keypints: Int. Number of keypoints to discover.\n        focal_length: Float. Focal length of camera\n        rotation_noise: Float. Noise added to the estimation of the rotation.\n        separation_delta: Float. Delta used for the ''separation'' loss.\n        loss_weights: Dict. having as keys strings with the different losses\n            names e.g. ''consistency'' and as value the weight used for that\n            loss.\n\n    # References\n        - [Discovery of Latent 3D Keypoints via End-to-end\n            Geometric Reasoning](https://arxiv.org/pdf/1807.03146.pdf)\n    \"\"\"\n    def __init__(self, num_keypoints, focal_length, rotation_noise=0.1,\n                 separation_delta=0.05, loss_weights={\n                     'consistency': 1.0, 'silhouette': 1.0, 'separation': 1.0,\n                     'relative_pose': 0.2, 'variance': 0.5}):\n\n        self.num_keypoints = int(num_keypoints)\n        self.focal_length = focal_length\n        self.projector = Projector(focal_length)\n        self.rotation_noise = rotation_noise\n        self.separation_delta = separation_delta\n        self.loss_weights = loss_weights\n\n    def _reshape_matrix(self, matrix):\n        matrix = tf.reshape(matrix, [-1, 4, 4])\n        # transpose is for multiplying points with matrices from the left.\n        matrix = tf.transpose(matrix, [0, 2, 1])\n        return matrix\n\n    def _unpack_matrices(self, matrices):\n        world_to_A = self._reshape_matrix(matrices[:, 0, :])\n        world_to_B = self._reshape_matrix(matrices[:, 1, :])\n        A_to_world = self._reshape_matrix(matrices[:, 2, :])\n        B_to_world = self._reshape_matrix(matrices[:, 3, :])\n        return world_to_A, world_to_B, A_to_world, B_to_world\n\n    def _unpack_uvz_coordinates(self, uvz_coordinates):\n        uvz_A = uvz_coordinates[:, :self.num_keypoints, :]\n        uvz_B = uvz_coordinates[:, self.num_keypoints:, :]\n        return uvz_A, uvz_B\n\n    def _consistency(self, uvz_M, M_to_world, world_to_N, uvz_N):\n        keypoints_M = self.projector.unproject(uvz_M)\n        world_coordinates = tf.matmul(keypoints_M, M_to_world)\n        keypoints_M_in_N = tf.matmul(world_coordinates, world_to_N)\n        uvz_M_in_N = self.projector.project(keypoints_M_in_N)\n        squared_difference = tf.square(uvz_M_in_N - uvz_N)\n        l2_distance = tf.reduce_sum(squared_difference, axis=[1, 2])\n        consistency_loss = l2_distance / self.num_keypoints\n        return consistency_loss\n\n    def consistency(self, matrices, uvz_coordinates):\n        matrices = self._unpack_matrices(matrices)\n        world_to_A, world_to_B, A_to_world, B_to_world = matrices\n        uvz_A, uvz_B = self._unpack_uvz_coordinates(uvz_coordinates)\n        consistency_A = self._consistency(uvz_A, A_to_world, world_to_B, uvz_B)\n        consistency_B = self._consistency(uvz_B, B_to_world, world_to_A, uvz_A)\n        consistency_loss = (consistency_A + consistency_B) / 2.0\n        consistency_loss = self.loss_weights['consistency'] * consistency_loss\n        return consistency_loss\n\n    def _separation(self, uvz):\n        keypoints = self.projector.unproject(uvz)\n        keypoints_i = tf.tile(keypoints, [1, self.num_keypoints, 1])\n        keypoints_j = tf.tile(keypoints, [1, 1, self.num_keypoints])\n        keypoints_j = tf.reshape(keypoints_j, tf.shape(keypoints_i))\n        squared_difference = tf.square(keypoints_i - keypoints_j)\n        squared_l2_distance = tf.reduce_sum(squared_difference, axis=2)\n        separation_loss = tf.maximum(\n            -squared_l2_distance + self.separation_delta, 0.0)\n        separation_loss = tf.reshape(\n            separation_loss, [-1, self.num_keypoints, self.num_keypoints])\n        separation_loss = tf.reduce_sum(separation_loss, axis=[1, 2])\n        separation_loss = separation_loss / self.num_keypoints\n        return separation_loss\n\n    def separation(self, matrices, uvz_coordinates):\n        uvz_A, uvz_B = self._unpack_uvz_coordinates(uvz_coordinates)\n        separation_loss_A = self._separation(uvz_A)\n        separation_loss_B = self._separation(uvz_B)\n        separation_loss = (separation_loss_A + separation_loss_B) / 2.0\n        separation_loss = self.loss_weights['separation'] * separation_loss\n        return separation_loss\n\n    def relative_pose(self, matrices, uvz_coordinates):\n        matrices = self._unpack_matrices(matrices)\n        world_to_A, world_to_B, A_to_world, B_to_world = matrices\n        uvz_A, uvz_B = self._unpack_uvz_coordinates(uvz_coordinates)\n        keypoints_A = self.projector.unproject(uvz_A)\n        keypoints_B = self.projector.unproject(uvz_B)\n\n        A_to_B = tf.matmul(A_to_world, world_to_B)\n        rotation_A_to_B = A_to_B[:, :3, :3]\n        estimation_args = (keypoints_A, keypoints_B, self.rotation_noise)\n        estimated_rotation_A_to_B = self.estimate_rotation(*estimation_args)\n        estimated_rotation_A_to_B = estimated_rotation_A_to_B[:, :3, :3]\n        squared_A_to_B = tf.square(estimated_rotation_A_to_B - rotation_A_to_B)\n        squared_frobenius = tf.reduce_sum(squared_A_to_B, axis=[1, 2])\n        frobenius = tf.sqrt(squared_frobenius)\n        arcsin_arg = tf.minimum(1.0, frobenius / (2 * math.sqrt(2)))\n        angular_loss = 2.0 * tf.asin(arcsin_arg)\n        angular_loss = self.loss_weights['relative_pose'] * angular_loss\n        return angular_loss\n\n    def uvz_points(self, matrices, uvz_coordinates):\n        consistency_loss = self.consistency(matrices, uvz_coordinates)\n        separation_loss = self.separation(matrices, uvz_coordinates)\n        relative_pose_loss = self.relative_pose(matrices, uvz_coordinates)\n        uvz_loss = consistency_loss + separation_loss + relative_pose_loss\n        return uvz_loss\n\n    def _silhouette(self, alpha_channel, uv_volume):\n        alpha_channel = tf.greater(alpha_channel, tf.zeros_like(alpha_channel))\n        alpha_channel = tf.cast(alpha_channel, dtype=tf.float32)\n        alpha_channel = tf.expand_dims(alpha_channel, 1)\n        silhouette_loss = tf.reduce_sum(uv_volume * alpha_channel, axis=[2, 3])\n        silhouette_loss = -tf.math.log(silhouette_loss + 1e-12)\n        silhouette_loss = tf.reduce_mean(silhouette_loss, axis=-1)\n        return silhouette_loss\n\n    def silhouette(self, alpha_channels, uv_volumes):\n        alpha_channel_A = alpha_channels[:, :, :, 0]\n        alpha_channel_B = alpha_channels[:, :, :, 1]\n        uv_volume_A = uv_volumes[:, :self.num_keypoints, :, :]\n        uv_volume_B = uv_volumes[:, self.num_keypoints:, :, :]\n        silhouette_loss_A = self._silhouette(alpha_channel_A, uv_volume_A)\n        silhouette_loss_B = self._silhouette(alpha_channel_B, uv_volume_B)\n        silhouette_loss = (silhouette_loss_A + silhouette_loss_B) / 2.0\n        silhouette_loss = self.loss_weights['silhouette'] * silhouette_loss\n        return silhouette_loss\n\n    def _variance(self, uv_volume, range_u, range_v):\n        expected_keypoint_u = tf.reduce_sum(uv_volume * range_u, axis=[2, 3])\n        expected_keypoint_v = tf.reduce_sum(uv_volume * range_v, axis=[2, 3])\n        uv = tf.stack([expected_keypoint_u, expected_keypoint_v], -1)\n        uv = tf.reshape(uv, [-1, self.num_keypoints, 2])\n        uv = tf.reshape(uv, [tf.shape(uv)[0], tf.shape(uv)[1], 1, 1, 2])\n\n        ranges = tf.stack([range_u, range_v], axis=2)\n        ranges_sh = tf.shape(ranges)\n        ranges = tf.reshape(ranges, [1, 1, ranges_sh[0], ranges_sh[1], 2])\n        squared_difference = tf.reduce_sum(tf.square(uv - ranges), axis=4)\n        diff = squared_difference * uv_volume\n        diff = tf.reduce_sum(diff, axis=[2, 3])\n        variance = tf.reduce_mean(diff, axis=-1)\n        return variance\n\n    def variance(self, alpha_channels, uv_volumes):\n        uv_volume_A = uv_volumes[:, :self.num_keypoints, :, :]\n        uv_volume_B = uv_volumes[:, self.num_keypoints:, :, :]\n        feature_map_size = uv_volumes.shape[-1]\n\n        arange = np.arange(0.5, feature_map_size, 1)\n        arange = arange / (feature_map_size / 2) - 1\n        range_u, range_v = tf.meshgrid(arange, -arange)\n        range_u = tf.cast(range_u, dtype=tf.float32)\n        range_v = tf.cast(range_v, dtype=tf.float32)\n\n        variance_loss_A = self._variance(uv_volume_A, range_u, range_v)\n        variance_loss_B = self._variance(uv_volume_B, range_u, range_v)\n        variance_loss = (variance_loss_A + variance_loss_B) / 2.0\n        variance_loss = self.loss_weights['variance'] * variance_loss\n        return variance_loss\n\n    def uv_volumes(self, alpha_channels, uv_volumes):\n        variance_loss = self.variance(alpha_channels, uv_volumes)\n        silhouette_loss = self.silhouette(alpha_channels, uv_volumes)\n        uv_loss = variance_loss + silhouette_loss\n        return uv_loss\n\n    def estimate_rotation(self, keypoints_A, keypoints_B, noise=0.1):\n        \"\"\"Estimates the rotation between two sets of keypoints using\n        Kabsch algorithm.\n\n        The rotation is estimated by first subtracting mean from each\n        set of keypoints and computing SVD of the covariance matrix.\n\n        Arguments:\n            xyz0: [batch, num_kp, 3] The first set of keypoints.\n            xyz1: [batch, num_kp, 3] The second set of keypoints.\n            pconf: [batch, num_kp] The weights used to\n                   compute the rotation estimate.\n            noise: A number indicating the noise added to the keypoints.\n\n        Returns:\n            [batch, 3, 3] A batch of transposed 3 x 3 rotation matrices.\n        \"\"\"\n\n        pconf = tf.ones(\n            [tf.shape(keypoints_A)[0],\n             tf.shape(keypoints_A)[1]],\n            dtype=tf.float32) / self.num_keypoints\n\n        noise_A = tf.random.normal(tf.shape(keypoints_A), mean=0, stddev=noise)\n        noise_B = tf.random.normal(tf.shape(keypoints_B), mean=0, stddev=noise)\n        keypoints_A = keypoints_A + noise_A\n        keypoints_B = keypoints_B + noise_B\n        pconf2 = tf.expand_dims(pconf, 2)\n        center_A = tf.reduce_sum(pconf2 * keypoints_A, 1, keepdims=True)\n        center_B = tf.reduce_sum(pconf2 * keypoints_B, 1, keepdims=True)\n        x = keypoints_A - center_A\n        y = keypoints_B - center_B\n        weighted_x = tf.matmul(\n            x, tf.linalg.diag(pconf), transpose_a=True)\n        covariance = tf.matmul(weighted_x, y)\n        _, u, v = tf.linalg.svd(covariance, full_matrices=True)\n        d = tf.linalg.det(tf.matmul(v, u, transpose_b=True))\n        ud = tf.concat(\n            [u[:, :, :-1],\n             u[:, :, -1:] * tf.expand_dims(tf.expand_dims(d, 1), 1)],\n            axis=2)\n        return tf.matmul(ud, v, transpose_b=True)",
  "def __init__(self, num_keypoints, focal_length, rotation_noise=0.1,\n                 separation_delta=0.05, loss_weights={\n                     'consistency': 1.0, 'silhouette': 1.0, 'separation': 1.0,\n                     'relative_pose': 0.2, 'variance': 0.5}):\n\n        self.num_keypoints = int(num_keypoints)\n        self.focal_length = focal_length\n        self.projector = Projector(focal_length)\n        self.rotation_noise = rotation_noise\n        self.separation_delta = separation_delta\n        self.loss_weights = loss_weights",
  "def _reshape_matrix(self, matrix):\n        matrix = tf.reshape(matrix, [-1, 4, 4])\n        # transpose is for multiplying points with matrices from the left.\n        matrix = tf.transpose(matrix, [0, 2, 1])\n        return matrix",
  "def _unpack_matrices(self, matrices):\n        world_to_A = self._reshape_matrix(matrices[:, 0, :])\n        world_to_B = self._reshape_matrix(matrices[:, 1, :])\n        A_to_world = self._reshape_matrix(matrices[:, 2, :])\n        B_to_world = self._reshape_matrix(matrices[:, 3, :])\n        return world_to_A, world_to_B, A_to_world, B_to_world",
  "def _unpack_uvz_coordinates(self, uvz_coordinates):\n        uvz_A = uvz_coordinates[:, :self.num_keypoints, :]\n        uvz_B = uvz_coordinates[:, self.num_keypoints:, :]\n        return uvz_A, uvz_B",
  "def _consistency(self, uvz_M, M_to_world, world_to_N, uvz_N):\n        keypoints_M = self.projector.unproject(uvz_M)\n        world_coordinates = tf.matmul(keypoints_M, M_to_world)\n        keypoints_M_in_N = tf.matmul(world_coordinates, world_to_N)\n        uvz_M_in_N = self.projector.project(keypoints_M_in_N)\n        squared_difference = tf.square(uvz_M_in_N - uvz_N)\n        l2_distance = tf.reduce_sum(squared_difference, axis=[1, 2])\n        consistency_loss = l2_distance / self.num_keypoints\n        return consistency_loss",
  "def consistency(self, matrices, uvz_coordinates):\n        matrices = self._unpack_matrices(matrices)\n        world_to_A, world_to_B, A_to_world, B_to_world = matrices\n        uvz_A, uvz_B = self._unpack_uvz_coordinates(uvz_coordinates)\n        consistency_A = self._consistency(uvz_A, A_to_world, world_to_B, uvz_B)\n        consistency_B = self._consistency(uvz_B, B_to_world, world_to_A, uvz_A)\n        consistency_loss = (consistency_A + consistency_B) / 2.0\n        consistency_loss = self.loss_weights['consistency'] * consistency_loss\n        return consistency_loss",
  "def _separation(self, uvz):\n        keypoints = self.projector.unproject(uvz)\n        keypoints_i = tf.tile(keypoints, [1, self.num_keypoints, 1])\n        keypoints_j = tf.tile(keypoints, [1, 1, self.num_keypoints])\n        keypoints_j = tf.reshape(keypoints_j, tf.shape(keypoints_i))\n        squared_difference = tf.square(keypoints_i - keypoints_j)\n        squared_l2_distance = tf.reduce_sum(squared_difference, axis=2)\n        separation_loss = tf.maximum(\n            -squared_l2_distance + self.separation_delta, 0.0)\n        separation_loss = tf.reshape(\n            separation_loss, [-1, self.num_keypoints, self.num_keypoints])\n        separation_loss = tf.reduce_sum(separation_loss, axis=[1, 2])\n        separation_loss = separation_loss / self.num_keypoints\n        return separation_loss",
  "def separation(self, matrices, uvz_coordinates):\n        uvz_A, uvz_B = self._unpack_uvz_coordinates(uvz_coordinates)\n        separation_loss_A = self._separation(uvz_A)\n        separation_loss_B = self._separation(uvz_B)\n        separation_loss = (separation_loss_A + separation_loss_B) / 2.0\n        separation_loss = self.loss_weights['separation'] * separation_loss\n        return separation_loss",
  "def relative_pose(self, matrices, uvz_coordinates):\n        matrices = self._unpack_matrices(matrices)\n        world_to_A, world_to_B, A_to_world, B_to_world = matrices\n        uvz_A, uvz_B = self._unpack_uvz_coordinates(uvz_coordinates)\n        keypoints_A = self.projector.unproject(uvz_A)\n        keypoints_B = self.projector.unproject(uvz_B)\n\n        A_to_B = tf.matmul(A_to_world, world_to_B)\n        rotation_A_to_B = A_to_B[:, :3, :3]\n        estimation_args = (keypoints_A, keypoints_B, self.rotation_noise)\n        estimated_rotation_A_to_B = self.estimate_rotation(*estimation_args)\n        estimated_rotation_A_to_B = estimated_rotation_A_to_B[:, :3, :3]\n        squared_A_to_B = tf.square(estimated_rotation_A_to_B - rotation_A_to_B)\n        squared_frobenius = tf.reduce_sum(squared_A_to_B, axis=[1, 2])\n        frobenius = tf.sqrt(squared_frobenius)\n        arcsin_arg = tf.minimum(1.0, frobenius / (2 * math.sqrt(2)))\n        angular_loss = 2.0 * tf.asin(arcsin_arg)\n        angular_loss = self.loss_weights['relative_pose'] * angular_loss\n        return angular_loss",
  "def uvz_points(self, matrices, uvz_coordinates):\n        consistency_loss = self.consistency(matrices, uvz_coordinates)\n        separation_loss = self.separation(matrices, uvz_coordinates)\n        relative_pose_loss = self.relative_pose(matrices, uvz_coordinates)\n        uvz_loss = consistency_loss + separation_loss + relative_pose_loss\n        return uvz_loss",
  "def _silhouette(self, alpha_channel, uv_volume):\n        alpha_channel = tf.greater(alpha_channel, tf.zeros_like(alpha_channel))\n        alpha_channel = tf.cast(alpha_channel, dtype=tf.float32)\n        alpha_channel = tf.expand_dims(alpha_channel, 1)\n        silhouette_loss = tf.reduce_sum(uv_volume * alpha_channel, axis=[2, 3])\n        silhouette_loss = -tf.math.log(silhouette_loss + 1e-12)\n        silhouette_loss = tf.reduce_mean(silhouette_loss, axis=-1)\n        return silhouette_loss",
  "def silhouette(self, alpha_channels, uv_volumes):\n        alpha_channel_A = alpha_channels[:, :, :, 0]\n        alpha_channel_B = alpha_channels[:, :, :, 1]\n        uv_volume_A = uv_volumes[:, :self.num_keypoints, :, :]\n        uv_volume_B = uv_volumes[:, self.num_keypoints:, :, :]\n        silhouette_loss_A = self._silhouette(alpha_channel_A, uv_volume_A)\n        silhouette_loss_B = self._silhouette(alpha_channel_B, uv_volume_B)\n        silhouette_loss = (silhouette_loss_A + silhouette_loss_B) / 2.0\n        silhouette_loss = self.loss_weights['silhouette'] * silhouette_loss\n        return silhouette_loss",
  "def _variance(self, uv_volume, range_u, range_v):\n        expected_keypoint_u = tf.reduce_sum(uv_volume * range_u, axis=[2, 3])\n        expected_keypoint_v = tf.reduce_sum(uv_volume * range_v, axis=[2, 3])\n        uv = tf.stack([expected_keypoint_u, expected_keypoint_v], -1)\n        uv = tf.reshape(uv, [-1, self.num_keypoints, 2])\n        uv = tf.reshape(uv, [tf.shape(uv)[0], tf.shape(uv)[1], 1, 1, 2])\n\n        ranges = tf.stack([range_u, range_v], axis=2)\n        ranges_sh = tf.shape(ranges)\n        ranges = tf.reshape(ranges, [1, 1, ranges_sh[0], ranges_sh[1], 2])\n        squared_difference = tf.reduce_sum(tf.square(uv - ranges), axis=4)\n        diff = squared_difference * uv_volume\n        diff = tf.reduce_sum(diff, axis=[2, 3])\n        variance = tf.reduce_mean(diff, axis=-1)\n        return variance",
  "def variance(self, alpha_channels, uv_volumes):\n        uv_volume_A = uv_volumes[:, :self.num_keypoints, :, :]\n        uv_volume_B = uv_volumes[:, self.num_keypoints:, :, :]\n        feature_map_size = uv_volumes.shape[-1]\n\n        arange = np.arange(0.5, feature_map_size, 1)\n        arange = arange / (feature_map_size / 2) - 1\n        range_u, range_v = tf.meshgrid(arange, -arange)\n        range_u = tf.cast(range_u, dtype=tf.float32)\n        range_v = tf.cast(range_v, dtype=tf.float32)\n\n        variance_loss_A = self._variance(uv_volume_A, range_u, range_v)\n        variance_loss_B = self._variance(uv_volume_B, range_u, range_v)\n        variance_loss = (variance_loss_A + variance_loss_B) / 2.0\n        variance_loss = self.loss_weights['variance'] * variance_loss\n        return variance_loss",
  "def uv_volumes(self, alpha_channels, uv_volumes):\n        variance_loss = self.variance(alpha_channels, uv_volumes)\n        silhouette_loss = self.silhouette(alpha_channels, uv_volumes)\n        uv_loss = variance_loss + silhouette_loss\n        return uv_loss",
  "def estimate_rotation(self, keypoints_A, keypoints_B, noise=0.1):\n        \"\"\"Estimates the rotation between two sets of keypoints using\n        Kabsch algorithm.\n\n        The rotation is estimated by first subtracting mean from each\n        set of keypoints and computing SVD of the covariance matrix.\n\n        Arguments:\n            xyz0: [batch, num_kp, 3] The first set of keypoints.\n            xyz1: [batch, num_kp, 3] The second set of keypoints.\n            pconf: [batch, num_kp] The weights used to\n                   compute the rotation estimate.\n            noise: A number indicating the noise added to the keypoints.\n\n        Returns:\n            [batch, 3, 3] A batch of transposed 3 x 3 rotation matrices.\n        \"\"\"\n\n        pconf = tf.ones(\n            [tf.shape(keypoints_A)[0],\n             tf.shape(keypoints_A)[1]],\n            dtype=tf.float32) / self.num_keypoints\n\n        noise_A = tf.random.normal(tf.shape(keypoints_A), mean=0, stddev=noise)\n        noise_B = tf.random.normal(tf.shape(keypoints_B), mean=0, stddev=noise)\n        keypoints_A = keypoints_A + noise_A\n        keypoints_B = keypoints_B + noise_B\n        pconf2 = tf.expand_dims(pconf, 2)\n        center_A = tf.reduce_sum(pconf2 * keypoints_A, 1, keepdims=True)\n        center_B = tf.reduce_sum(pconf2 * keypoints_B, 1, keepdims=True)\n        x = keypoints_A - center_A\n        y = keypoints_B - center_B\n        weighted_x = tf.matmul(\n            x, tf.linalg.diag(pconf), transpose_a=True)\n        covariance = tf.matmul(weighted_x, y)\n        _, u, v = tf.linalg.svd(covariance, full_matrices=True)\n        d = tf.linalg.det(tf.matmul(v, u, transpose_b=True))\n        ud = tf.concat(\n            [u[:, :, :-1],\n             u[:, :, -1:] * tf.expand_dims(tf.expand_dims(d, 1), 1)],\n            axis=2)\n        return tf.matmul(ud, v, transpose_b=True)",
  "class MultiBoxLoss(object):\n    \"\"\"Multi-box loss for a single-shot detection architecture.\n\n    # Arguments\n        neg_pos_ratio: Int. Number of negatives used per positive box.\n        alpha: Float. Weight parameter for localization loss.\n        max_num_negatives: Int. Maximum number of negatives per batch.\n\n    # References\n        - [SSD: Single Shot MultiBox\n            Detector](https://arxiv.org/abs/1512.02325)\n    \"\"\"\n    def __init__(self, neg_pos_ratio=3, alpha=1.0, max_num_negatives=300):\n        self.alpha = alpha\n        self.neg_pos_ratio = neg_pos_ratio\n        self.max_num_negatives = max_num_negatives\n\n    def _smooth_l1(self, y_true, y_pred):\n        absolute_value_loss = K.abs(y_true - y_pred)\n        square_loss = 0.5 * (y_true - y_pred)**2\n        absolute_value_condition = K.less(absolute_value_loss, 1.0)\n        l1_smooth_loss = tf.where(\n            absolute_value_condition, square_loss, absolute_value_loss - 0.5)\n        return K.sum(l1_smooth_loss, axis=-1)\n\n    def _cross_entropy(self, y_true, y_pred):\n        y_pred = K.maximum(K.minimum(y_pred, 1 - 1e-15), 1e-15)\n        cross_entropy_loss = - K.sum(y_true * K.log(y_pred), axis=-1)\n        return cross_entropy_loss\n\n    def _calculate_masks(self, y_true):\n        negative_mask = y_true[:, :, 4]\n        positive_mask = 1.0 - negative_mask\n        return positive_mask, negative_mask\n\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"Computes localization and classification losses in a batch.\n\n        # Arguments\n            y_true: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with correct labels.\n            y_pred: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with predicted inferences.\n\n        # Returns\n            Tensor with loss per sample in batch.\n        \"\"\"\n        localization_loss = self.localization(y_true, y_pred)\n        positive_loss = self.positive_classification(y_true, y_pred)\n        negative_loss = self.negative_classification(y_true, y_pred)\n        return localization_loss + positive_loss + negative_loss\n\n    def localization(self, y_true, y_pred):\n        \"\"\"Computes localization loss in a batch.\n\n        # Arguments\n            y_true: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with correct labels.\n            y_pred: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with predicted inferences.\n\n        # Returns\n            Tensor with localization loss per sample in batch.\n        \"\"\"\n        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n        local_loss = self._smooth_l1(y_true[:, :, :4], y_pred[:, :, :4])\n        positive_mask, negative_mask = self._calculate_masks(y_true)\n        positive_local_losses = local_loss * positive_mask\n        positive_local_loss = tf.reduce_sum(positive_local_losses, axis=-1)\n        num_positives = tf.reduce_sum(tf.cast(positive_mask, 'float32'))\n        num_positives = tf.maximum(1.0, num_positives)\n        return (self.alpha * positive_local_loss * batch_size) / num_positives\n\n    def positive_classification(self, y_true, y_pred):\n        \"\"\"Computes positive classification loss in a batch. Positive boxes are those\n            boxes that contain an object.\n\n        # Arguments\n            y_true: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with correct labels.\n            y_pred: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with predicted inferences.\n\n        # Returns\n            Tensor with positive classification loss per sample in batch.\n        \"\"\"\n        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n        class_loss = self._cross_entropy(y_true[:, :, 4:], y_pred[:, :, 4:])\n        positive_mask, negative_mask = self._calculate_masks(y_true)\n        positive_class_losses = class_loss * positive_mask\n        positive_class_loss = K.sum(positive_class_losses, axis=-1)\n        num_positives = K.sum(K.cast(positive_mask, 'float32'))\n        num_positives = tf.maximum(1.0, num_positives)\n        return (positive_class_loss * batch_size) / num_positives\n\n    def negative_classification(self, y_true, y_pred):\n        \"\"\"Computes negative classification loss in a batch. Negative boxes are those\n            boxes that don't contain an object.\n\n        # Arguments\n            y_true: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with correct labels.\n            y_pred: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with predicted inferences.\n\n        # Returns\n            Tensor with negative classification loss per sample in batch.\n        \"\"\"\n        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n        class_loss = self._cross_entropy(y_true[:, :, 4:], y_pred[:, :, 4:])\n        positive_mask, negative_mask = self._calculate_masks(y_true)\n        num_positives_per_sample = K.cast(K.sum(positive_mask, -1), 'int32')\n        num_hard_negatives = self.neg_pos_ratio * num_positives_per_sample\n        num_negatives_per_sample = K.minimum(\n            num_hard_negatives, self.max_num_negatives)\n        negative_class_losses = class_loss * negative_mask\n        elements = (negative_class_losses, num_negatives_per_sample)\n        negative_class_loss = tf.map_fn(\n            lambda x: K.sum(tf.nn.top_k(x[0], x[1])[0]),\n            elements, dtype=tf.float32)\n        num_positives = K.sum(K.cast(positive_mask, 'float32'))\n        num_positives = tf.maximum(1.0, num_positives)\n        return (negative_class_loss * batch_size) / num_positives",
  "def __init__(self, neg_pos_ratio=3, alpha=1.0, max_num_negatives=300):\n        self.alpha = alpha\n        self.neg_pos_ratio = neg_pos_ratio\n        self.max_num_negatives = max_num_negatives",
  "def _smooth_l1(self, y_true, y_pred):\n        absolute_value_loss = K.abs(y_true - y_pred)\n        square_loss = 0.5 * (y_true - y_pred)**2\n        absolute_value_condition = K.less(absolute_value_loss, 1.0)\n        l1_smooth_loss = tf.where(\n            absolute_value_condition, square_loss, absolute_value_loss - 0.5)\n        return K.sum(l1_smooth_loss, axis=-1)",
  "def _cross_entropy(self, y_true, y_pred):\n        y_pred = K.maximum(K.minimum(y_pred, 1 - 1e-15), 1e-15)\n        cross_entropy_loss = - K.sum(y_true * K.log(y_pred), axis=-1)\n        return cross_entropy_loss",
  "def _calculate_masks(self, y_true):\n        negative_mask = y_true[:, :, 4]\n        positive_mask = 1.0 - negative_mask\n        return positive_mask, negative_mask",
  "def compute_loss(self, y_true, y_pred):\n        \"\"\"Computes localization and classification losses in a batch.\n\n        # Arguments\n            y_true: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with correct labels.\n            y_pred: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with predicted inferences.\n\n        # Returns\n            Tensor with loss per sample in batch.\n        \"\"\"\n        localization_loss = self.localization(y_true, y_pred)\n        positive_loss = self.positive_classification(y_true, y_pred)\n        negative_loss = self.negative_classification(y_true, y_pred)\n        return localization_loss + positive_loss + negative_loss",
  "def localization(self, y_true, y_pred):\n        \"\"\"Computes localization loss in a batch.\n\n        # Arguments\n            y_true: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with correct labels.\n            y_pred: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with predicted inferences.\n\n        # Returns\n            Tensor with localization loss per sample in batch.\n        \"\"\"\n        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n        local_loss = self._smooth_l1(y_true[:, :, :4], y_pred[:, :, :4])\n        positive_mask, negative_mask = self._calculate_masks(y_true)\n        positive_local_losses = local_loss * positive_mask\n        positive_local_loss = tf.reduce_sum(positive_local_losses, axis=-1)\n        num_positives = tf.reduce_sum(tf.cast(positive_mask, 'float32'))\n        num_positives = tf.maximum(1.0, num_positives)\n        return (self.alpha * positive_local_loss * batch_size) / num_positives",
  "def positive_classification(self, y_true, y_pred):\n        \"\"\"Computes positive classification loss in a batch. Positive boxes are those\n            boxes that contain an object.\n\n        # Arguments\n            y_true: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with correct labels.\n            y_pred: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with predicted inferences.\n\n        # Returns\n            Tensor with positive classification loss per sample in batch.\n        \"\"\"\n        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n        class_loss = self._cross_entropy(y_true[:, :, 4:], y_pred[:, :, 4:])\n        positive_mask, negative_mask = self._calculate_masks(y_true)\n        positive_class_losses = class_loss * positive_mask\n        positive_class_loss = K.sum(positive_class_losses, axis=-1)\n        num_positives = K.sum(K.cast(positive_mask, 'float32'))\n        num_positives = tf.maximum(1.0, num_positives)\n        return (positive_class_loss * batch_size) / num_positives",
  "def negative_classification(self, y_true, y_pred):\n        \"\"\"Computes negative classification loss in a batch. Negative boxes are those\n            boxes that don't contain an object.\n\n        # Arguments\n            y_true: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with correct labels.\n            y_pred: Tensor of shape '[batch_size, num_boxes, 4 + num_classes]'\n                with predicted inferences.\n\n        # Returns\n            Tensor with negative classification loss per sample in batch.\n        \"\"\"\n        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n        class_loss = self._cross_entropy(y_true[:, :, 4:], y_pred[:, :, 4:])\n        positive_mask, negative_mask = self._calculate_masks(y_true)\n        num_positives_per_sample = K.cast(K.sum(positive_mask, -1), 'int32')\n        num_hard_negatives = self.neg_pos_ratio * num_positives_per_sample\n        num_negatives_per_sample = K.minimum(\n            num_hard_negatives, self.max_num_negatives)\n        negative_class_losses = class_loss * negative_mask\n        elements = (negative_class_losses, num_negatives_per_sample)\n        negative_class_loss = tf.map_fn(\n            lambda x: K.sum(tf.nn.top_k(x[0], x[1])[0]),\n            elements, dtype=tf.float32)\n        num_positives = K.sum(K.cast(positive_mask, 'float32'))\n        num_positives = tf.maximum(1.0, num_positives)\n        return (negative_class_loss * batch_size) / num_positives",
  "def compute_F_beta_score(y_true, y_pred, beta=1.0, class_weights=1.0):\n    \"\"\"Computes the F beta score. The F beta score is the geometric mean\n    of the precision and recall, where the recall is B times more important\n    than the precision.\n\n    # Arguments\n        y_true: Tensor of shape ``(batch, H, W, num_channels)``.\n        y_pred: Tensor of shape ``(batch, H, W, num_channels)``.\n        beta: Float.\n        class_weights: Float or list of floats of shape ``(num_classes)``.\n\n    # Returns\n        Tensor of shape ``(batch)`` containing the F beta score per sample.\n    \"\"\"\n    true_positives = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n    false_positives = tf.reduce_sum(y_pred, axis=[1, 2]) - true_positives\n    false_negatives = tf.reduce_sum(y_true, axis=[1, 2]) - true_positives\n    B_squared = tf.math.pow(beta, 2)\n    numerator = (1.0 + B_squared) * true_positives\n    denominator = numerator + (B_squared * false_negatives) + false_positives\n    F_beta_score = numerator / (denominator + 1e-5)\n    return class_weights * F_beta_score",
  "class DiceLoss(Loss):\n    \"\"\"Computes the F beta loss. The F beta score is the geometric mean\n    of the precision and recall, where the recall is B times more important\n    than the precision.\n\n    # Arguments\n        beta: Float.\n        class_weights: Float or list of floats of shape ``(num_classes)``.\n    \"\"\"\n    def __init__(self, beta=1.0, class_weights=1.0):\n        super(DiceLoss, self).__init__()\n        self.beta = beta\n        self.class_weights = class_weights\n\n    def call(self, y_true, y_pred):\n        args = (self.beta, self.class_weights)\n        return 1.0 - compute_F_beta_score(y_true, y_pred, *args)",
  "def __init__(self, beta=1.0, class_weights=1.0):\n        super(DiceLoss, self).__init__()\n        self.beta = beta\n        self.class_weights = class_weights",
  "def call(self, y_true, y_pred):\n        args = (self.beta, self.class_weights)\n        return 1.0 - compute_F_beta_score(y_true, y_pred, *args)",
  "def split_alpha_mask(RGBA_mask):\n    \"\"\"Splits alpha mask and RGB image.\n\n    # Arguments\n        RGBA_mask: Tensor [batch, H, W, 4]\n\n    # Returns\n        Color tensor [batch, H, W, 3] and alpha tensor [batch, H, W, 1]\n    \"\"\"\n    color_mask = RGBA_mask[:, :, :, 0:3]\n    alpha_mask = RGBA_mask[:, :, :, 3:4]\n    return color_mask, alpha_mask",
  "def split_error_mask(RGBE_mask):\n    \"\"\"Splits error mask and RGB image.\n\n    # Arguments\n        RGBA_mask: Tensor [batch, H, W, 4]\n\n    # Returns\n        Color tensor [batch, H, W, 3] and error tensor [batch, H, W, 1]\n\n    \"\"\"\n    color_mask = RGBE_mask[:, :, :, 0:3]\n    error_mask = RGBE_mask[:, :, :, 3:4]\n    return color_mask, error_mask",
  "def compute_foreground_loss(RGB_true, RGB_pred, alpha_mask):\n    \"\"\"Computes foreground reconstruction L1 loss by using only positive alpha\n        mask values.\n\n    # Arguments\n        RGB_true: Tensor [batch, H, W, 3]. True RGB label values.\n        RGB_pred: Tensor [batch, H, W, 3]. Predicted RGB values.\n        alpha_mask: Tensor [batch, H, W, 1]. True normalized alpha mask values.\n\n    # Returns\n        Tensor [batch, H, W, 3] with foreground loss values.\n    \"\"\"\n    foreground_true = RGB_true * alpha_mask\n    foreground_pred = RGB_pred * alpha_mask\n    foreground_loss = tf.abs(foreground_true - foreground_pred)\n    return foreground_loss",
  "def compute_background_loss(RGB_true, RGB_pred, alpha_mask):\n    \"\"\"Computes the L1 reconstruction loss, weighting the inverted alpha\n        mask values in the predicted RGB image by beta.\n\n    # Arguments\n        RGB_true: Tensor [batch, H, W, 3]. True RGB label values.\n        RGB_pred: Tensor [batch, H, W, 3]. Predicted RGB values.\n        alpha_mask: Tensor [batch, H, W, 1]. True normalized alpha mask values.\n\n    # Returns\n        Tensor [batch, H, W, 3] with background loss values.\n    \"\"\"\n    background_true = RGB_true * (1.0 - alpha_mask)\n    background_pred = RGB_pred * (1.0 - alpha_mask)\n    background_loss = tf.abs(background_true - background_pred)\n    return background_loss",
  "def compute_weighted_reconstruction_loss(RGBA_true, RGB_pred, beta=3.0):\n    \"\"\"Computes the L1 reconstruction loss, weighting the positive alpha\n        mask values in the predicted RGB image by beta.\n\n    # Arguments\n        RGBA_true: Tensor [batch, H, W, 4]. Color with alpha mask label values.\n        RGB_pred: Tensor [batch, H, W, 3]. Predicted RGB values.\n        beta: Float. Value used to multiple positive alpha mask values.\n\n    # Returns\n        Tensor [batch, H, W] with weighted reconstruction loss values.\n\n    \"\"\"\n    RGB_true, alpha_mask = split_alpha_mask(RGBA_true)\n    foreground_loss = compute_foreground_loss(RGB_true, RGB_pred, alpha_mask)\n    background_loss = compute_background_loss(RGB_true, RGB_pred, alpha_mask)\n    reconstruction_loss = (beta * foreground_loss) + background_loss\n    return tf.reduce_mean(reconstruction_loss, axis=-1, keepdims=True)",
  "def normalized_image_to_normalized_device_coordinates(image):\n    \"\"\"Map image value from [0, 1] -> [-1, 1].\n    \"\"\"\n    return (image * 2.0) - 1.0",
  "def normalized_device_coordinates_to_normalized_image(image):\n    \"\"\"Map image value from [0, 1] -> [-1, 1].\n    \"\"\"\n    return (image + 1.0) / 2.0",
  "def compute_weighted_reconstruction_loss_with_error(\n        RGBA_true, RGBE_pred, beta=3.0):\n    \"\"\"Computes L1 reconstruction loss by multiplying positive alpha mask\n        by beta.\n\n    # Arguments\n        RGBA_true: Tensor [batch, H, W, 4]. Color with alpha mask label values.\n        RGBE_pred: Tensor [batch, H, W, 4]. Predicted RGB and error mask.\n        beta: Float. Value used to multiple positive alpha mask values.\n\n    # Returns\n        Tensor [batch, H, W] with weighted reconstruction loss values.\n\n    \"\"\"\n    RGB_pred, error_mask = split_error_mask(RGBE_pred)\n    loss = compute_weighted_reconstruction_loss(RGBA_true, RGB_pred, beta)\n    return loss",
  "class WeightedReconstruction(Loss):\n    \"\"\"Computes L1 reconstruction loss by multiplying positive alpha mask\n        by beta.\n\n    # Arguments\n        beta: Float. Value used to multiple positive alpha mask values.\n        RGBA_true: Tensor [batch, H, W, 4]. Color with alpha mask label values.\n        RGB_pred: Tensor [batch, H, W, 3]. Predicted RGB values.\n\n    # Returns\n        Tensor [batch, H, W] with weighted reconstruction loss values.\n\n    \"\"\"\n    def __init__(self, beta=3.0):\n        super(WeightedReconstruction, self).__init__()\n        self.beta = beta\n\n    def call(self, RGBA_true, RGB_pred):\n        loss = compute_weighted_reconstruction_loss(\n            RGBA_true, RGB_pred, self.beta)\n        return loss",
  "class WeightedReconstructionWithError(Loss):\n    \"\"\"Computes L1 reconstruction loss by multiplying positive alpha mask\n        by beta.\n\n    # Arguments\n        RGBA_true: Tensor [batch, H, W, 4]. Color with alpha mask label values.\n        RGBE_pred: Tensor [batch, H, W, 4]. Predicted RGB and error mask.\n        beta: Float. Value used to multiple positive alpha mask values.\n\n    # Returns\n        Tensor [batch, H, W] with weighted reconstruction loss values.\n\n    \"\"\"\n    def __init__(self, beta=3.0):\n        super(WeightedReconstructionWithError, self).__init__()\n        self.beta = beta\n\n    def call(self, RGBA_true, RGBE_pred):\n        reconstruction_loss = compute_weighted_reconstruction_loss_with_error(\n            RGBA_true, RGBE_pred, self.beta)\n        return reconstruction_loss",
  "def __init__(self, beta=3.0):\n        super(WeightedReconstruction, self).__init__()\n        self.beta = beta",
  "def call(self, RGBA_true, RGB_pred):\n        loss = compute_weighted_reconstruction_loss(\n            RGBA_true, RGB_pred, self.beta)\n        return loss",
  "def __init__(self, beta=3.0):\n        super(WeightedReconstructionWithError, self).__init__()\n        self.beta = beta",
  "def call(self, RGBA_true, RGBE_pred):\n        reconstruction_loss = compute_weighted_reconstruction_loss_with_error(\n            RGBA_true, RGBE_pred, self.beta)\n        return reconstruction_loss",
  "def compute_jaccard_score(y_true, y_pred, class_weights=1.0):\n    \"\"\"Computes the Jaccard score. The Jaccard score is the intersection\n    over union of the predicted with respect to real masks.\n\n    # Arguments\n        y_true: Tensor of shape ``(batch, H, W, num_channels)``.\n        y_pred: Tensor of shape ``(batch, H, W, num_channels)``.\n        class_weights: Float or list of floats of shape ``(num_classes)``.\n\n    # Returns\n        Tensor of shape ``(batch)`` containing the F beta score per sample.\n    \"\"\"\n    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n    union = tf.reduce_sum(y_true + y_pred, axis=[1, 2]) - intersection\n    jaccard_score = (intersection) / (union + 1e-5)\n    return class_weights * jaccard_score",
  "class JaccardLoss(Loss):\n    \"\"\"Computes the Jaccard loss. The Jaccard score is the intersection\n    over union of the predicted with respect to real masks.\n\n    # Arguments\n        class_weights: Float or list of floats of shape ``(num_classes)``.\n    \"\"\"\n    def __init__(self, class_weights=1.0):\n        super(JaccardLoss, self).__init__()\n        self.class_weights = class_weights\n\n    def call(self, y_true, y_pred):\n        return 1.0 - compute_jaccard_score(y_true, y_pred, self.class_weights)",
  "def __init__(self, class_weights=1.0):\n        super(JaccardLoss, self).__init__()\n        self.class_weights = class_weights",
  "def call(self, y_true, y_pred):\n        return 1.0 - compute_jaccard_score(y_true, y_pred, self.class_weights)",
  "def compute_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n    \"\"\"Computes the Focal loss. The Focal loss down weights\n        properly classified examples.\n\n    # Arguments\n        y_true: Tensor of shape ``(batch, H, W, num_channels)``.\n        y_pred: Tensor of shape ``(batch, H, W, num_channels)``.\n        gamma: Float.\n        alpha: Float.\n        class_weights: Float or list of floats of shape ``(num_classes)``.\n\n    # Returns\n        Tensor of shape ``(batch)`` containing the F beta score per sample.\n    \"\"\"\n    y_pred = tf.clip_by_value(y_pred, 1e-5, 1.0 - 1e-5)\n    modulator = alpha * tf.math.pow(1 - y_pred, gamma)\n    focal_loss = - modulator * y_true * tf.math.log(y_pred)\n    return focal_loss",
  "class FocalLoss(Loss):\n    \"\"\"Computes the Focal loss. The Focal loss down weights\n        properly classified examples.\n\n    # Arguments\n        gamma: Float.\n        alpha: Float.\n        class_weights: Float or list of floats of shape ``(num_classes)``.\n    \"\"\"\n    def __init__(self, gamma=2.0, alpha=0.25):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def call(self, y_true, y_pred):\n        return compute_focal_loss(y_true, y_pred, self.gamma, self.alpha)",
  "def __init__(self, gamma=2.0, alpha=0.25):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha",
  "def call(self, y_true, y_pred):\n        return compute_focal_loss(y_true, y_pred, self.gamma, self.alpha)",
  "class FERPlus(Loader):\n    \"\"\"Class for loading FER2013 emotion classification dataset.\n        with FERPlus labels.\n    # Arguments\n        path: String. Path to directory that has inside the files:\n            `fer2013.csv` and  `fer2013new.csv`\n        split: String. Valid option contain 'train', 'val' or 'test'.\n        class_names: String or list: If 'all' then it loads all default\n            class names.\n        image_size: List of length two. Indicates the shape in which\n            the image will be resized.\n\n    # References\n        - [FerPlus](https://www.kaggle.com/c/challenges-in-representation-\\\n                learning-facial-expression-recognition-challenge/data)\n        - [FER2013](https://arxiv.org/abs/1608.01041)\n    \"\"\"\n    def __init__(self, path, split='train', class_names='all',\n                 image_size=(48, 48)):\n\n        if class_names == 'all':\n            class_names = get_class_names('FERPlus')\n\n        super(FERPlus, self).__init__(path, split, class_names, 'FERPlus')\n\n        self.image_size = image_size\n        self.images_path = os.path.join(self.path, 'fer2013.csv')\n        self.labels_path = os.path.join(self.path, 'fer2013new.csv')\n        self.split_to_filter = {\n            'train': 'Training', 'val': 'PublicTest', 'test': 'PrivateTest'}\n\n    def load_data(self):\n        data = np.genfromtxt(self.images_path, str, '#', ',', 1)\n        data = data[data[:, -1] == self.split_to_filter[self.split]]\n        faces = np.zeros((len(data), *self.image_size))\n        for sample_arg, sample in enumerate(data):\n            face = np.array(sample[1].split(' '), dtype=int).reshape(48, 48)\n            face = resize_image(face, self.image_size)\n            faces[sample_arg, :, :] = face\n\n        emotions = np.genfromtxt(self.labels_path, str, '#', ',', 1)\n        emotions = emotions[emotions[:, 0] == self.split_to_filter[self.split]]\n        emotions = emotions[:, 2:10].astype(float)\n        N = np.sum(emotions, axis=1)\n        mask = N != 0\n        N, faces, emotions = N[mask], faces[mask], emotions[mask]\n        emotions = emotions / np.expand_dims(N, 1)\n\n        data = []\n        for face, emotion in zip(faces, emotions):\n            sample = {'image': face, 'label': emotion}\n            data.append(sample)\n        return data",
  "def __init__(self, path, split='train', class_names='all',\n                 image_size=(48, 48)):\n\n        if class_names == 'all':\n            class_names = get_class_names('FERPlus')\n\n        super(FERPlus, self).__init__(path, split, class_names, 'FERPlus')\n\n        self.image_size = image_size\n        self.images_path = os.path.join(self.path, 'fer2013.csv')\n        self.labels_path = os.path.join(self.path, 'fer2013new.csv')\n        self.split_to_filter = {\n            'train': 'Training', 'val': 'PublicTest', 'test': 'PrivateTest'}",
  "def load_data(self):\n        data = np.genfromtxt(self.images_path, str, '#', ',', 1)\n        data = data[data[:, -1] == self.split_to_filter[self.split]]\n        faces = np.zeros((len(data), *self.image_size))\n        for sample_arg, sample in enumerate(data):\n            face = np.array(sample[1].split(' '), dtype=int).reshape(48, 48)\n            face = resize_image(face, self.image_size)\n            faces[sample_arg, :, :] = face\n\n        emotions = np.genfromtxt(self.labels_path, str, '#', ',', 1)\n        emotions = emotions[emotions[:, 0] == self.split_to_filter[self.split]]\n        emotions = emotions[:, 2:10].astype(float)\n        N = np.sum(emotions, axis=1)\n        mask = N != 0\n        N, faces, emotions = N[mask], faces[mask], emotions[mask]\n        emotions = emotions / np.expand_dims(N, 1)\n\n        data = []\n        for face, emotion in zip(faces, emotions):\n            sample = {'image': face, 'label': emotion}\n            data.append(sample)\n        return data",
  "def get_class_names(dataset_name='VOC2007'):\n    \"\"\"Gets label names for the classes of the supported datasets.\n\n    # Arguments\n        dataset_name: String. Dataset name. Valid dataset names are:\n            VOC2007, VOC2012, COCO and YCBVideo.\n\n    # Returns\n       List of strings containing the class names for the dataset given.\n\n    # Raises\n        ValueError: in case of invalid dataset name\n    \"\"\"\n\n    if dataset_name in ['VOC2007', 'VOC2012', 'VOC']:\n\n        class_names = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n                       'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n                       'diningtable', 'dog', 'horse', 'motorbike', 'person',\n                       'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n\n    elif dataset_name == 'COCO':\n        class_names = ['background', 'person', 'bicycle', 'car', 'motorcycle',\n                       'airplane', 'bus', 'train', 'truck', 'boat',\n                       'traffic light', 'fire hydrant', 'stop sign',\n                       'parking meter', 'bench', 'bird', 'cat', 'dog',\n                       'horse', 'sheep', 'cow', 'elephant', 'bear',\n                       'zebra', 'giraffe', 'backpack', 'umbrella',\n                       'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n                       'snowboard', 'sports ball', 'kite', 'baseball bat',\n                       'baseball glove', 'skateboard', 'surfboard',\n                       'tennis racket', 'bottle', 'wine glass',\n                       'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n                       'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n                       'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n                       'potted plant', 'bed', 'dining table', 'toilet',\n                       'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n                       'cell phone', 'microwave', 'oven', 'toaster',\n                       'sink', 'refrigerator', 'book', 'clock', 'vase',\n                       'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n\n    elif dataset_name == 'COCO_EFFICIENTDET':\n        class_names = ['person', 'bicycle', 'car', 'motorcycle',\n                       'airplane', 'bus', 'train', 'truck', 'boat',\n                       'traffic light', 'fire hydrant', '0', 'stop sign',\n                       'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n                       'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\n                       '0', 'backpack', 'umbrella', '0', '0', 'handbag', 'tie',\n                       'suitcase', 'frisbee', 'skis', 'snowboard',\n                       'sports ball', 'kite', 'baseball bat', 'baseball glove',\n                       'skateboard', 'surfboard', 'tennis racket', 'bottle',\n                       '0', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n                       'bowl', 'banana', 'apple', 'sandwich', 'orange',\n                       'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n                       'cake', 'chair', 'couch', 'potted plant', 'bed', '0',\n                       'dining table', '0', '0', 'toilet', '0', 'tv', 'laptop',\n                       'mouse', 'remote', 'keyboard', 'cell phone',\n                       'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n                       '0', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n                       'hair drier', 'toothbrush']\n\n    elif dataset_name == 'YCBVideo':\n        class_names = ['background', '037_scissors', '008_pudding_box',\n                       '024_bowl', '005_tomato_soup_can', '007_tuna_fish_can',\n                       '010_potted_meat_can', '061_foam_brick', '011_banana',\n                       '035_power_drill', '004_sugar_box', '019_pitcher_base',\n                       '006_mustard_bottle', '036_wood_block',\n                       '009_gelatin_box', '051_large_clamp',\n                       '040_large_marker', '003_cracker_box',\n                       '025_mug', '052_extra_large_clamp',\n                       '021_bleach_cleanser', '002_master_chef_can']\n\n    elif dataset_name == 'FAT':\n        class_names = ['background', '037_scissors', '008_pudding_box',\n                       '024_bowl', '005_tomato_soup_can', '007_tuna_fish_can',\n                       '010_potted_meat_can', '061_foam_brick', '011_banana',\n                       '035_power_drill', '004_sugar_box', '019_pitcher_base',\n                       '006_mustard_bottle', '036_wood_block',\n                       '009_gelatin_box', '051_large_clamp',\n                       '040_large_marker', '003_cracker_box',\n                       '025_mug', '052_extra_large_clamp',\n                       '021_bleach_cleanser', '002_master_chef_can']\n\n    elif dataset_name == 'FERPlus':\n        return ['neutral', 'happiness', 'surprise', 'sadness',\n                'anger', 'disgust', 'fear', 'contempt']\n\n    elif dataset_name == 'FER':\n        return ['angry', 'disgust', 'fear', 'happy',\n                'sad', 'surprise', 'neutral']\n\n    elif dataset_name == 'IMDB':\n        return ['man', 'woman']\n\n    elif dataset_name == 'CityScapes':\n        return ['void', 'flat', 'construction',\n                'object', 'nature', 'sky', 'human', 'vehicle']\n\n    else:\n        raise ValueError('Invalid dataset', dataset_name)\n\n    return class_names",
  "def get_arg_to_class(class_names):\n    \"\"\"Constructs dictionary from argument to class names.\n\n    # Arguments\n        class_names: List of strings containing the class names.\n\n    # Returns\n        Dictionary mapping integer to class name.\n    \"\"\"\n\n    return dict(zip(list(range(len(class_names))), class_names))",
  "class FAT(Loader):\n    \"\"\" Dataset loader for the falling things dataset (FAT).\n\n    # Arguments\n        path: String indicating full path to dataset\n            e.g. /home/user/fat/\n        split: String determining the data split to load.\n            e.g. `train`, `val` or `test`\n        class_names: `all` or list. If list it should contain as elements\n            strings indicating each class name.\n\n    # References\n        - [Deep Object Pose\n            Estimation (DOPE)](https://github.com/NVlabs/Deep_Object_Pose)\n    \"\"\"\n    # TODO: Allow selection of class_names.\n    def __init__(self, path, split='train', class_names='all'):\n        if class_names == 'all':\n            class_names = get_class_names('FAT')\n        self.class_to_arg = dict(\n            zip(class_names, list(range(len(class_names)))))\n\n        super(FAT, self).__init__(path, split, class_names, 'FAT')\n\n    def load_data(self):\n        scene_names = glob(self.path + 'mixed/*')\n        image_paths, label_paths = [], []\n        for scene_name in scene_names:\n            scene_image_paths, scene_label_paths = [], []\n            for image_side in ['left', 'right']:\n                image_names = glob(scene_name + '/*%s.jpg' % image_side)\n                side_image_paths = sorted(image_names, key=self._base_number)\n                label_names = glob(scene_name + '/0*%s.json' % image_side)\n                side_label_paths = sorted(label_names, key=self._base_number)\n                scene_image_paths = scene_image_paths + side_image_paths\n                scene_label_paths = scene_label_paths + side_label_paths\n            image_paths = image_paths + scene_image_paths\n            label_paths = label_paths + scene_label_paths\n\n        self.data = []\n        progress_bar = Progbar(len(image_paths))\n        for sample_arg, sample in enumerate(zip(image_paths, label_paths)):\n            image_path, label_path = sample\n            if not self._valid_name_match(image_path, label_path):\n                raise ValueError('Invalid name match:', image_path, label_path)\n            boxes = self._extract_boxes(label_path)\n            if boxes is None:\n                continue\n            self.data.append({'image': image_path, 'boxes': boxes})\n            progress_bar.update(sample_arg + 1)\n        return self.data\n\n    def _extract_boxes(self, json_filename):\n        json_data = json.load(open(json_filename, 'r'))\n        num_objects = len(json_data['objects'])\n        if num_objects == 0:\n            return None\n        box_data = np.zeros((num_objects, 5))\n        for object_arg, object_data in enumerate(json_data['objects']):\n            bounding_box = object_data['bounding_box']\n            y_min, x_min = bounding_box['top_left']\n            y_max, x_max = bounding_box['bottom_right']\n            x_min, y_min = x_min / 960., y_min / 540.\n            x_max, y_max = x_max / 960., y_max / 540.\n            box_data[object_arg, :4] = x_min, y_min, x_max, y_max\n            class_name = object_data['class'][:-4]\n            box_data[object_arg, -1] = self.class_to_arg[class_name]\n        return box_data\n\n    def _base_number(self, filename):\n        order = os.path.basename(filename)\n        order = order.split('.')[0]\n        order = float(order)\n        return order\n\n    def _valid_name_match(self, image_path, label_path):\n        image_name = os.path.basename(image_path)\n        label_name = os.path.basename(label_path)\n        return image_name[:-3] == label_name[:-4]",
  "def __init__(self, path, split='train', class_names='all'):\n        if class_names == 'all':\n            class_names = get_class_names('FAT')\n        self.class_to_arg = dict(\n            zip(class_names, list(range(len(class_names)))))\n\n        super(FAT, self).__init__(path, split, class_names, 'FAT')",
  "def load_data(self):\n        scene_names = glob(self.path + 'mixed/*')\n        image_paths, label_paths = [], []\n        for scene_name in scene_names:\n            scene_image_paths, scene_label_paths = [], []\n            for image_side in ['left', 'right']:\n                image_names = glob(scene_name + '/*%s.jpg' % image_side)\n                side_image_paths = sorted(image_names, key=self._base_number)\n                label_names = glob(scene_name + '/0*%s.json' % image_side)\n                side_label_paths = sorted(label_names, key=self._base_number)\n                scene_image_paths = scene_image_paths + side_image_paths\n                scene_label_paths = scene_label_paths + side_label_paths\n            image_paths = image_paths + scene_image_paths\n            label_paths = label_paths + scene_label_paths\n\n        self.data = []\n        progress_bar = Progbar(len(image_paths))\n        for sample_arg, sample in enumerate(zip(image_paths, label_paths)):\n            image_path, label_path = sample\n            if not self._valid_name_match(image_path, label_path):\n                raise ValueError('Invalid name match:', image_path, label_path)\n            boxes = self._extract_boxes(label_path)\n            if boxes is None:\n                continue\n            self.data.append({'image': image_path, 'boxes': boxes})\n            progress_bar.update(sample_arg + 1)\n        return self.data",
  "def _extract_boxes(self, json_filename):\n        json_data = json.load(open(json_filename, 'r'))\n        num_objects = len(json_data['objects'])\n        if num_objects == 0:\n            return None\n        box_data = np.zeros((num_objects, 5))\n        for object_arg, object_data in enumerate(json_data['objects']):\n            bounding_box = object_data['bounding_box']\n            y_min, x_min = bounding_box['top_left']\n            y_max, x_max = bounding_box['bottom_right']\n            x_min, y_min = x_min / 960., y_min / 540.\n            x_max, y_max = x_max / 960., y_max / 540.\n            box_data[object_arg, :4] = x_min, y_min, x_max, y_max\n            class_name = object_data['class'][:-4]\n            box_data[object_arg, -1] = self.class_to_arg[class_name]\n        return box_data",
  "def _base_number(self, filename):\n        order = os.path.basename(filename)\n        order = order.split('.')[0]\n        order = float(order)\n        return order",
  "def _valid_name_match(self, image_path, label_path):\n        image_name = os.path.basename(image_path)\n        label_name = os.path.basename(label_path)\n        return image_name[:-3] == label_name[:-4]",
  "class MANOHandJoints:\n    num_joints = 21\n\n    links_origin = np.array(\n        [[-0.09566993092407175, 0.006383428857461439, 0.006186305280135194],\n         [-0.007572684283876889, 0.0011830717890578813, 0.026872294317232474],\n         [0.025106219230007748, 0.005192427198442781, 0.029089362428270107],\n         [0.04726213151699109, 0.00389400462527089, 0.028975245669040688],\n         [-0.001009489532234269, 0.004904465506518265, 0.0028287644658181762],\n         [0.03017318285240305, 0.006765794024899131, -0.0027657440521595294],\n         [0.053077823086293004, 0.005513689792181309, -0.006710258054895484],\n         [-0.026882958864187647, -0.003556899962987172, -0.03702303672314978],\n         [-0.009868550726482567, -0.0034950752461879167, -0.0495218116903115],\n         [0.0059983504802553515, -0.004186231140635538, -0.05985371909262174],\n         [-0.013934376495261512, 0.002426007704596194, -0.020486887752953],\n         [0.014379898506751226, 0.004493014962915457, -0.02558542625500547],\n         [0.03790041138358198, 0.0028049031381001317, -0.03321924042737473],\n         [-0.07158022412142973, -0.009138905684414268, 0.031999152568217934],\n         [-0.0519469835801523, -0.008247619132871264, 0.05569870581415224],\n         [-0.029729244228165815, -0.01368059029432867, 0.07022282411348789],\n         [0.07238572379473107, 0.002952405275404611, 0.027662233800221883],\n         [0.0789928213101902, 0.006146648960141516, -0.012040861038314803],\n         [0.023687395956832776, -0.005529320599435923, -0.0697884145827113],\n         [0.062491898017990564, 0.002426856258013015, -0.04066927095293306],\n         [-0.003715698261416634, -0.01635903331447523, 0.09410496964595245]])\n\n    labels = ['W',\n              'I0', 'I1', 'I2',\n              'M0', 'M1', 'M2',\n              'L0', 'L1', 'L2',\n              'R0', 'R1', 'R2',\n              'T0', 'T1', 'T2',\n              'I3', 'M3', 'L3', 'R3', 'T3']\n\n    parents = [None,\n               0, 1, 2,\n               0, 4, 5,\n               0, 7, 8,\n               0, 10, 11,\n               0, 13, 14,\n               3, 6, 9, 12, 15]\n\n    children = [[1, 4, 7, 10, 13],  # root_joint has multiple children\n                2, 3, 16, 5, 6, 17, 8, 9, 18, 11, 12, 19, 14, 15, 20]",
  "class MPIIHandJoints:\n    num_joints = 21\n\n    links_origin = np.array([[-0.99924976, 0.01561216, 0.0354427],\n                             [-0.74495521, -0.14824392, 0.30792697],\n                             [-0.53770379, -0.13883537, 0.558103],\n                             [-0.30317002, -0.19618662, 0.71142176],\n                             [-0.02856714, -0.22446067, 0.96352525],\n                             [-0.06928206, -0.03928359, 0.25380709],\n                             [0.27568132, 0.00303977, 0.27721079],\n                             [0.50956244, -0.01066658, 0.27600616],\n                             [0.77477083, -0.02060624, 0.26214581],\n                             [0., 0., 0.],\n                             [0.32916895, 0.01964846, -0.05905647],\n                             [0.57095375, 0.00643106, -0.10069535],\n                             [0.84451634, 0.01311267, -0.15696599],\n                             [-0.13643704, -0.02616297, -0.24612351],\n                             [0.16245268, -0.00434333, -0.29994444],\n                             [0.41073873, -0.02216329, -0.38052812],\n                             [0.67033013, -0.02615401, -0.45917176],\n                             [-0.27312421, -0.08931944, -0.42068156],\n                             [-0.09351757, -0.08866681, -0.5526205],\n                             [0.07397581, -0.09596275, -0.66168566],\n                             [0.26070401, -0.1101406, -0.76655779]])\n\n    labels = ['W',\n              'T0', 'T1', 'T2', 'T3',\n              'I0', 'I1', 'I2', 'I3',\n              'M0', 'M1', 'M2', 'M3',\n              'R0', 'R1', 'R2', 'R3',\n              'L0', 'L1', 'L2', 'L3']\n\n    parents = [None,\n               0, 1, 2, 3,\n               0, 5, 6, 7,\n               0, 9, 10, 11,\n               0, 13, 14, 15,\n               0, 17, 18, 19]\n\n    children = [[1, 5, 9, 13, 17],  # root_joint has multiple children\n                2, 3, 4, 6, 7, 8, 10, 11, 12, 14, 15, 16, 18, 19, 20]",
  "class OpenImages(Loader):\n    \"\"\" Dataset loader for the OpenImagesV4 dataset.\n\n    # Arguments\n        path: String indicating full path to dataset\n            e.g. /home/user/open_images/\n        split: String determining the data split to load.\n            e.g. `train`, `val` or `test`\n        class_names: `all` or list. If list it should contain as elements\n            the strings of the class names.\n\n    \"\"\"\n    # TODO Allow selection of subset of class names.\n    def __init__(self, path, split='train', class_names='all'):\n\n        if split == 'val':\n            split = 'validation'\n\n        if split not in ['train', 'validation', 'test']:\n            raise NameError('Invalid split name.')\n\n        super(OpenImages, self).__init__(\n            path, split, class_names, 'OpenImages')\n\n        self.machine_to_human_name = dict()\n        self.machine_to_arg = dict()\n        self.load_class_names()\n        self.class_distribution = dict()\n        for class_name in self.class_names:\n            self.class_distribution[class_name] = 0\n\n    def load_class_names(self):\n        classes_file = os.path.join(self.path, CLASS_DESCRIPTIONS_FILE)\n        class_data = np.loadtxt(classes_file, delimiter=\",\", dtype=str)\n\n        # class ID zero is background\n        self.machine_to_arg['background'] = 0\n        self.machine_to_human_name['background'] = 'background'\n        class_names, class_arg = [], 1\n        class_names.append('background')\n        for machine_name, human_name in class_data:\n\n            if self.class_names == 'all':\n                self.machine_to_human_name[machine_name] = human_name\n                self.machine_to_arg[machine_name] = class_arg\n                class_names.append(human_name)\n                class_arg = class_arg + 1\n\n            elif human_name in self.class_names:\n                self.machine_to_human_name[machine_name] = human_name\n                self.machine_to_arg[machine_name] = class_arg\n                class_names.append(human_name)\n                class_arg = class_arg + 1\n\n        self._class_names = class_names\n        self._num_classes = len(self.machine_to_arg)\n        print(\"Found {} {} classes\".format(self.num_classes, self.split))\n\n    def _get_num_lines(self, file_path):\n        file_data = open(file_path, \"r+\")\n        buf = mmap.mmap(file_data.fileno(), 0)\n        lines = 0\n        while buf.readline():\n            lines = lines + 1\n        return lines\n\n    def load_data(self):\n\n        data = dict()\n        annotations_filepath = os.path.join(\n            self.path, BBOX_ANNOTATIONS_FILE.format(self.split))\n        # num_lines = self._get_num_lines(annotations_filepath)\n        machine_names = self.machine_to_human_name.keys()\n        # load file manually, line by line, in order to reduce memory usage\n        with open(annotations_filepath, 'r') as annotations_file:\n            # skip header\n            annotations_file.readline()\n\n            for line in annotations_file:\n                row = line.split(\",\")\n\n                image_filename = row[0] + \".jpg\"\n                x_min = float(row[4])\n                x_max = float(row[5])\n                y_min = float(row[6])\n                y_max = float(row[7])\n\n                machine_name = row[2]\n                if machine_name not in machine_names:\n                    continue\n\n                human_name = self.machine_to_human_name[machine_name]\n\n                absolute_image_path = os.path.join(\n                    self.path, self.split, image_filename)\n\n                if human_name in self.class_names:\n                    class_arg = self.machine_to_arg[machine_name]\n\n                    if absolute_image_path not in data:\n                        data[absolute_image_path] = []\n\n                    sample_data = [x_min, y_min, x_max, y_max, class_arg]\n                    data[absolute_image_path].append(sample_data)\n                    self.class_distribution[human_name] += 1\n\n        formatted_data = []\n        for image_path, ground_truth in data.items():\n            sample = {'image': image_path, 'boxes': ground_truth}\n            formatted_data.append(sample)\n\n        msg = '{} split: loaded {} images with {} bounding box annotations'\n        num_of_boxes = sum(self.class_distribution.values())\n        print(msg.format(self.split, len(data), num_of_boxes))\n        return formatted_data",
  "def __init__(self, path, split='train', class_names='all'):\n\n        if split == 'val':\n            split = 'validation'\n\n        if split not in ['train', 'validation', 'test']:\n            raise NameError('Invalid split name.')\n\n        super(OpenImages, self).__init__(\n            path, split, class_names, 'OpenImages')\n\n        self.machine_to_human_name = dict()\n        self.machine_to_arg = dict()\n        self.load_class_names()\n        self.class_distribution = dict()\n        for class_name in self.class_names:\n            self.class_distribution[class_name] = 0",
  "def load_class_names(self):\n        classes_file = os.path.join(self.path, CLASS_DESCRIPTIONS_FILE)\n        class_data = np.loadtxt(classes_file, delimiter=\",\", dtype=str)\n\n        # class ID zero is background\n        self.machine_to_arg['background'] = 0\n        self.machine_to_human_name['background'] = 'background'\n        class_names, class_arg = [], 1\n        class_names.append('background')\n        for machine_name, human_name in class_data:\n\n            if self.class_names == 'all':\n                self.machine_to_human_name[machine_name] = human_name\n                self.machine_to_arg[machine_name] = class_arg\n                class_names.append(human_name)\n                class_arg = class_arg + 1\n\n            elif human_name in self.class_names:\n                self.machine_to_human_name[machine_name] = human_name\n                self.machine_to_arg[machine_name] = class_arg\n                class_names.append(human_name)\n                class_arg = class_arg + 1\n\n        self._class_names = class_names\n        self._num_classes = len(self.machine_to_arg)\n        print(\"Found {} {} classes\".format(self.num_classes, self.split))",
  "def _get_num_lines(self, file_path):\n        file_data = open(file_path, \"r+\")\n        buf = mmap.mmap(file_data.fileno(), 0)\n        lines = 0\n        while buf.readline():\n            lines = lines + 1\n        return lines",
  "def load_data(self):\n\n        data = dict()\n        annotations_filepath = os.path.join(\n            self.path, BBOX_ANNOTATIONS_FILE.format(self.split))\n        # num_lines = self._get_num_lines(annotations_filepath)\n        machine_names = self.machine_to_human_name.keys()\n        # load file manually, line by line, in order to reduce memory usage\n        with open(annotations_filepath, 'r') as annotations_file:\n            # skip header\n            annotations_file.readline()\n\n            for line in annotations_file:\n                row = line.split(\",\")\n\n                image_filename = row[0] + \".jpg\"\n                x_min = float(row[4])\n                x_max = float(row[5])\n                y_min = float(row[6])\n                y_max = float(row[7])\n\n                machine_name = row[2]\n                if machine_name not in machine_names:\n                    continue\n\n                human_name = self.machine_to_human_name[machine_name]\n\n                absolute_image_path = os.path.join(\n                    self.path, self.split, image_filename)\n\n                if human_name in self.class_names:\n                    class_arg = self.machine_to_arg[machine_name]\n\n                    if absolute_image_path not in data:\n                        data[absolute_image_path] = []\n\n                    sample_data = [x_min, y_min, x_max, y_max, class_arg]\n                    data[absolute_image_path].append(sample_data)\n                    self.class_distribution[human_name] += 1\n\n        formatted_data = []\n        for image_path, ground_truth in data.items():\n            sample = {'image': image_path, 'boxes': ground_truth}\n            formatted_data.append(sample)\n\n        msg = '{} split: loaded {} images with {} bounding box annotations'\n        num_of_boxes = sum(self.class_distribution.values())\n        print(msg.format(self.split, len(data), num_of_boxes))\n        return formatted_data",
  "def download(split):\n    \"\"\"Downloads omniglot dataset from original repository source.\n\n    # Arguments:\n        split: String indicating dataset split i.e. `train` or `test`.\n\n    # Returns:\n        filepath string to data split directory.\n    \"\"\"\n    ROOT_URL = 'https://github.com/brendenlake/omniglot/blob/master/python/'\n    split_to_name = {'train': 'images_background', 'test': 'images_evaluation'}\n    filename = split_to_name[split]\n    URL = ROOT_URL + filename + '.zip?raw=true'\n    directory = 'paz/datasets/omniglot'\n    filepath = get_file(None, URL, cache_subdir=directory, extract=True)\n    filepath = os.path.join(os.path.dirname(filepath), filename)\n    return filepath",
  "def build_keyname(string):\n    \"\"\"Builds keynames in lower case and without parenthesis.\n\n    # Arguments:\n        string: Keyname string.\n\n    # Returns\n        String name for easy dictionary access.\n    \"\"\"\n    string = os.path.basename(string)\n    translations = {ord('('): None, ord(')'): None}\n    return string.translate(translations).lower()",
  "def enumerate_filenames(root_path):\n    \"\"\"Enumerates all file names inside given path.\n\n    # Arguments\n        root_path: String, path in which to search.\n\n    # Returns\n        list of sorted file names inside root path.\n    \"\"\"\n    wildcard = os.path.join(root_path, '*')\n    directories = glob.glob(wildcard)\n    directories = sorted(directories)\n    return directories",
  "def load_shot(filepath, shape):\n    \"\"\"Loads images and preprocess it by resizing and normalizing it.\n\n    # Arguments\n        filepath: String indicating path to image.\n        shape: List of integers indicating new shape (height, width).\n\n    # Returns\n        image as numpy array.\n    \"\"\"\n    image = load_image(filepath, num_channels=1)\n    image = resize_image(image, (shape))\n    image = image / 255.0\n    return image",
  "def load_shots(shot_filepaths, shape):\n    \"\"\"Loads all images in character directory\n\n    # Arguments:\n        shot_filepaths: String. Filepath to character images.\n        shape: List of integers indicating new shape (height, width).\n\n    # Returns:\n        Image array with all shots\n    \"\"\"\n    shots = []\n    for shot_filepath in shot_filepaths:\n        shots.append(load_shot(shot_filepath, shape))\n    return np.array(shots)",
  "def load_characters(character_filepaths, shape):\n    \"\"\"Loads all characters in data directory.\n\n    # Arguments\n        character_filepaths: String indicating path to images.\n        shape: List of integers indicating new shape (height, width).\n\n    # Returns\n        Dictionary with key name character name and value image array.\n    \"\"\"\n    characters = {}\n    for character_filepath in character_filepaths:\n        character_name = build_keyname(character_filepath)\n        shot_filepaths = enumerate_filenames(character_filepath)\n        shots = load_shots(shot_filepaths, shape)\n        characters[character_name] = shots\n    return characters",
  "def load(split='train', shape=(28, 28), flat=True):\n    \"\"\"Loads omniglot dataset for in between and within alphabet sampling.\n\n    # Arguments\n        split: String. Either `train` or `test`. Indicates which split to load.\n        shape: List of two integers indicating resize shape `(H, W)`.\n        flat: Boolean. If `True` the returned data dictionary is organized\n            using each possible character as a class, with each key being a\n            number having as value an image array.\n            If `False` the returned data dictionary is organized using as keys\n            the language names and as value another dictionary with keys being\n            the character number, and as value the image array.\n            This is to perform either sampling between alpahabet (`flat=True`)\n            or to perform sampling within alphabet (`flat=False`).\n            Usually, neural few-shot learning algorithms have been tested using\n            in between alphabet sampling, but the original authors tested using\n            the more challenging within alphabet sampling.\n\n    # Returns\n        dictionary with class names as keys and image numpy arrays as values.\n    \"\"\"\n    filepath = download(split)\n    language_filepaths = enumerate_filenames(filepath)\n    languages = {}\n    for language_filepath in language_filepaths:\n        language_name = build_keyname(language_filepath)\n        character_directories = enumerate_filenames(language_filepath)\n        characters = load_characters(character_directories, shape)\n        languages[language_name] = characters\n    return flatten(languages) if flat else languages",
  "def flatten(dataset):\n    \"\"\"Removes language hierarchy by having classes as each possible character.\n\n    # Arguments:\n        dataset: Dictionary with key names language name, and as value a\n            dictionary with key names character names, and value an image array\n\n    # Returns:\n        Dictionary with key names as numbers and as values image arrays.\n    \"\"\"\n    flat_dataset = {}\n    flat_key = 0\n    for language_name, language in dataset.items():\n        for character_name, characters in language.items():\n            flat_dataset[flat_key] = characters\n            flat_key = flat_key + 1\n    return flat_dataset",
  "def sample_between_alphabet(RNG, dataset, num_ways, num_shots, num_tests=1):\n    \"\"\"Samples classification problems with flat dataset.\n    Each sample is a meta learning problem with classes from all languages.\n\n    # Arguments:\n        RNG: Numpy random number generator.\n        dataset: Dictionary.\n        num_ways: Int. Number of classes for each meta learning episode.\n        num_shots: Int. Number of train images used at each episode.\n        num_tests: In. Number of test images at each episode.\n\n    # Returns:\n        Two lists. First list has `(train_images, train_labels)` and\n        Second list has `(test_images, test_labels)`.\n    \"\"\"\n    # dataset is flat, easier for sampling without replacement\n    random_classes = RNG.choice(list(dataset.keys()), num_ways, replace=False)\n    test_images, test_labels = [], []\n    shot_images, shot_labels = [], []\n    num_samples = num_shots + num_tests  # num_shots + 1\n    for label, class_name in enumerate(random_classes):\n        images = RNG.choice(dataset[class_name], num_samples, replace=False)\n        labels = np.full(num_samples, label)\n        shot_images.append(images[:num_shots])\n        shot_labels.append(labels[:num_shots])\n        test_images.append(images[num_shots:])\n        test_labels.append(labels[num_shots:])\n    shot_images = np.concatenate(shot_images, axis=0)\n    shot_labels = np.concatenate(shot_labels, axis=0)\n    test_images = np.concatenate(test_images, axis=0)\n    test_labels = np.concatenate(test_labels, axis=0)\n    return (shot_images, shot_labels), (test_images, test_labels)",
  "def sample_within_alphabet(RNG, dataset, num_ways, num_shots, num_tests=1):\n    \"\"\"Samples classification problems with class hierarchical dataset.\n    Each sample is a meta learning problem with classes from the same language.\n\n    # Arguments:\n        RNG: Numpy random number generator.\n        dataset: Dictionary.\n        num_ways: Int. Number of classes for each meta learning episode.\n        num_shots: Int. Number of train images used at each episode.\n        num_tests: In. Number of test images at each episode.\n\n    # Returns:\n        Two lists. First list has `(train_images, train_labels)` and\n        Second list has `(test_images, test_labels)`.\n    \"\"\"\n    alphabet_name = RNG.choice(list(dataset.keys()))\n    alphabet = dataset[alphabet_name]\n    reuse = True if num_ways > len(alphabet) else False  # FIX as 2019 Lake\n    class_names = RNG.choice(list(alphabet.keys()), num_ways, reuse).tolist()\n    test_images, test_labels = [], []\n    shot_images, shot_labels = [], []\n    num_samples = num_shots + num_tests  # num_shots + 1\n    for label, class_name in enumerate(class_names):\n        images = RNG.choice(alphabet[class_name], num_samples, replace=False)\n        labels = np.full(num_samples, label)\n        shot_images.append(images[:num_shots])\n        shot_labels.append(labels[:num_shots])\n        test_images.append(images[num_shots:])\n        test_labels.append(labels[num_shots:])\n    shot_images = np.concatenate(shot_images, axis=0)\n    shot_labels = np.concatenate(shot_labels, axis=0)\n    test_images = np.concatenate(test_images, axis=0)\n    test_labels = np.concatenate(test_labels, axis=0)\n    return (shot_images, shot_labels), (test_images, test_labels)",
  "class Generator(Sequence):\n    \"\"\"Data generator for omniglot dataset with meta-learning episodes\n    # Arguments\n        sampler:\n        num_classes: Int. Number of classes for each meta learning episode.\n        num_support: Int. Number of train images used at each episode.\n        num_queries: In. Number of test images at each episode.\n        image_shape: List of integers indicating new shape (height, width).\n        num_steps: Int. Number of samples per epoch.\n    \"\"\"\n    def __init__(self, sampler, num_classes, num_support, num_queries,\n                 image_shape, num_steps=2000):\n        self.sampler = sampler\n        self.support_shape = (num_classes, num_support, *image_shape)\n        self.queries_shape = (num_classes, num_queries, *image_shape)\n        self.num_steps = num_steps\n\n    def __len__(self):\n        return self.num_steps\n\n    def __getitem__(self, idx):\n        (support, support_labels), (queries, queries_labels) = self.sampler()\n        support = np.reshape(support, self.support_shape)\n        queries = np.reshape(queries, self.queries_shape)\n        return {'support': support, 'queries': queries}, queries_labels",
  "def remove_classes(RNG, data, num_classes):\n    \"\"\"Removes classes by randomly taking out keys from data dictionary.\n\n    # Arguments:\n        RNG: Numpy random number generator.\n        data: Dictionary with keys as class names and values image arrays.\n\n    # Returns:\n        Dictionary with number of classes euqal to `num_classes`.\n    \"\"\"\n    keys = RNG.choice(len(data.keys()), num_classes, replace=False)\n    data = {key: data[key] for key in keys}\n    return data",
  "def split_data(data, validation_split):\n    \"\"\"Splits data keys into training and validation.\n\n    # Arguments:\n        data: Dictionary with keys as class names and values image arrays.\n        validation_split: Float between `[0, 1]`. Porcentange of training\n            data to be used for validation.\n\n    # Returns:\n        Two dictionaries with train and vlaidation data dictionaries.\n    \"\"\"\n    keys = list(data.keys())\n    num_train_keys = int(len(keys) * (1 - validation_split))\n    train_keys = keys[:num_train_keys]\n    valid_keys = keys[num_train_keys:]\n    train_data = {key: data[key] for key in train_keys}\n    valid_data = {key: data[key] for key in valid_keys}\n    return train_data, valid_data",
  "def plot_language(language):\n    \"\"\"Plots all characters in a language\n\n    # Arguments:\n        language: Dict with characters names as keys and image arrays as values\n\n    # Returns:\n        Image array with all characters.\n    \"\"\"\n    characters = []\n    for characters_name, images in language.items():\n        images = np.expand_dims(images, axis=-1)\n        characters.append(make_mosaic(images, (5, 4), 10))\n    characters = np.array(characters)\n    characters = make_mosaic(characters, (8, 7), 20)\n    return characters",
  "class Omniglot(Loader):\n    def __init__(self, split, shape, flat=True):\n        self.shape = shape\n        self.flat = flat\n        super(Omniglot, self).__init__(None, split, None, 'Omniglot')\n\n    def load_data(self):\n        return load(self.split, self.shape, self.flat)",
  "def __init__(self, sampler, num_classes, num_support, num_queries,\n                 image_shape, num_steps=2000):\n        self.sampler = sampler\n        self.support_shape = (num_classes, num_support, *image_shape)\n        self.queries_shape = (num_classes, num_queries, *image_shape)\n        self.num_steps = num_steps",
  "def __len__(self):\n        return self.num_steps",
  "def __getitem__(self, idx):\n        (support, support_labels), (queries, queries_labels) = self.sampler()\n        support = np.reshape(support, self.support_shape)\n        queries = np.reshape(queries, self.queries_shape)\n        return {'support': support, 'queries': queries}, queries_labels",
  "def __init__(self, split, shape, flat=True):\n        self.shape = shape\n        self.flat = flat\n        super(Omniglot, self).__init__(None, split, None, 'Omniglot')",
  "def load_data(self):\n        return load(self.split, self.shape, self.flat)",
  "class CityScapes(Loader):\n    \"\"\"CityScapes data manager for loading the paths of the RGB and\n        segmentation masks.\n\n    # Arguments\n        image_path: String. Path to RGB images e.g. '/home/user/leftImg8bit/'\n        label_path: String. Path to label masks e.g. '/home/user/gtFine/'\n        split: String. Valid option contain 'train', 'val' or 'test'.\n        class_names: String or list: If 'all' then it loads all default\n            class names.\n\n    # References\n        -[The Cityscapes Dataset for Semantic Urban Scene Understanding](\n        https://www.cityscapes-dataset.com/citation/)\n    \"\"\"\n    def __init__(self, image_path, label_path, split, class_names='all'):\n        if split not in ['train', 'val', 'test']:\n            raise ValueError('Invalid split name:', split)\n        self.image_path = os.path.join(image_path, split)\n        self.label_path = os.path.join(label_path, split)\n        if class_names == 'all':\n            class_names = get_class_names('CityScapes')\n        super(CityScapes, self).__init__(\n            None, split, class_names, 'CityScapes')\n\n    def load_data(self):\n        image_path = os.path.join(self.image_path, '*/*.png')\n        label_path = os.path.join(self.label_path, '*/*labelIds.png')\n        image_paths = glob.glob(image_path)\n        label_paths = glob.glob(label_path)\n        image_paths = sorted(image_paths)\n        label_paths = sorted(label_paths)\n        assert len(image_paths) == len(label_paths)\n        dataset = []\n        for image_path, label_path in zip(image_paths, label_paths):\n            sample = {'image_path': image_path, 'label_path': label_path}\n            dataset.append(sample)\n        return dataset",
  "def __init__(self, image_path, label_path, split, class_names='all'):\n        if split not in ['train', 'val', 'test']:\n            raise ValueError('Invalid split name:', split)\n        self.image_path = os.path.join(image_path, split)\n        self.label_path = os.path.join(label_path, split)\n        if class_names == 'all':\n            class_names = get_class_names('CityScapes')\n        super(CityScapes, self).__init__(\n            None, split, class_names, 'CityScapes')",
  "def load_data(self):\n        image_path = os.path.join(self.image_path, '*/*.png')\n        label_path = os.path.join(self.label_path, '*/*labelIds.png')\n        image_paths = glob.glob(image_path)\n        label_paths = glob.glob(label_path)\n        image_paths = sorted(image_paths)\n        label_paths = sorted(label_paths)\n        assert len(image_paths) == len(label_paths)\n        dataset = []\n        for image_path, label_path in zip(image_paths, label_paths):\n            sample = {'image_path': image_path, 'label_path': label_path}\n            dataset.append(sample)\n        return dataset",
  "class VOC(Loader):\n    \"\"\" Dataset loader for the falling things dataset (FAT).\n\n    # Arguments\n        data_path: Data path to VOC2007 annotations\n        split: String determining the data split to load.\n            e.g. `train`, `val` or `test`\n        class_names: `all` or list. If list it should contain as elements\n            strings indicating each class name.\n        name: String or list indicating with dataset or datasets to load.\n            e.g. ``VOC2007`` or ``[''VOC2007'', VOC2012]``.\n        with_difficult_samples: Boolean. If ``True`` flagged difficult boxes\n            will be added to the returned data.\n        evaluate: Boolean. If ``True`` returned data will be loaded without\n            normalization for a direct evaluation.\n\n    # Return\n        data: List of dictionaries with keys corresponding to the image paths\n        and values numpy arrays of shape ``[num_objects, 4 + 1]``\n        where the ``+ 1`` contains the ``class_arg`` and ``num_objects`` refers\n        to the amount of boxes in the image.\n\n    \"\"\"\n    # TODO check for split\n    def __init__(self, path=None, split='train', class_names='all',\n                 name='VOC2007', with_difficult_samples=True, evaluate=False):\n\n        super(VOC, self).__init__(path, split, class_names, name)\n\n        self.with_difficult_samples = with_difficult_samples\n        self.evaluate = evaluate\n        self._class_names = class_names\n        if class_names == 'all':\n            self._class_names = get_class_names('VOC')\n        self.images_path = None\n        self.arg_to_class = None\n\n    def load_data(self):\n        if ((self.name == 'VOC2007') or (self.name == 'VOC2012')):\n            ground_truth_data = self._load_VOC(self.name, self.split)\n        elif isinstance(self.name, list):\n            if not isinstance(self.split, list):\n                raise Exception(\"'split' should also be a list\")\n            if set(self.name).issubset(['VOC2007', 'VOC2012']):\n                data_A = self._load_VOC(self.name[0], self.split[0])\n                data_B = self._load_VOC(self.name[1], self.split[1])\n                ground_truth_data = data_A + data_B\n        else:\n            raise ValueError('Invalid name given.')\n        return ground_truth_data\n\n    def _load_VOC(self, dataset_name, split):\n        self.parser = VOCParser(dataset_name,\n                                split,\n                                self._class_names,\n                                self.with_difficult_samples,\n                                self.path,\n                                self.evaluate)\n        self.images_path = self.parser.images_path\n        self.arg_to_class = self.parser.arg_to_class\n        ground_truth_data = self.parser.load_data()\n        return ground_truth_data",
  "class VOCParser(object):\n    \"\"\" Preprocess the VOC2007 xml annotations data.\n\n    # TODO: Add background label\n\n    # Arguments\n        data_path: Data path to VOC2007 annotations\n\n    # Return\n        data: Dictionary which keys correspond to the image names\n        and values are numpy arrays of shape (num_objects, 4 + 1)\n        num_objects refers to the number of objects in that specific image\n    \"\"\"\n\n    def __init__(self, dataset_name='VOC2007', split='train',\n                 class_names='all', with_difficult_samples=True,\n                 dataset_path='../datasets/VOCdevkit/',\n                 evaluate=False):\n\n        if dataset_name not in ['VOC2007', 'VOC2012']:\n            raise Exception('Invalid dataset name.')\n\n        # creating data set prefix paths variables\n        self.dataset_name = dataset_name\n        self.dataset_path = os.path.join(dataset_path, dataset_name)\n        self.split = split\n        self.split_prefix = os.path.join(self.dataset_path, 'ImageSets/Main/')\n        self.annotations_path = os.path.join(self.dataset_path, 'Annotations/')\n        self.images_path = os.path.join(self.dataset_path, 'JPEGImages/')\n        self.with_difficult_samples = with_difficult_samples\n        self.evaluate = evaluate\n\n        self.class_names = class_names\n        if self.class_names == 'all':\n            self.class_names = get_class_names('VOC')\n        self.num_classes = len(self.class_names)\n        class_keys = np.arange(self.num_classes)\n        self.arg_to_class = dict(zip(class_keys, self.class_names))\n        self.class_to_arg = {value: key for key, value\n                             in self.arg_to_class.items()}\n        self.data = []\n        self._preprocess_XML()\n\n    def _load_filenames(self):\n        split_file = os.path.join(self.split_prefix, self.split) + '.txt'\n        splitted_filenames = []\n        for line in open(split_file):\n            filename = line.strip() + '.xml'\n            splitted_filenames.append(filename)\n        return splitted_filenames\n\n    def _preprocess_XML(self):\n        filenames = self._load_filenames()\n        for filename in filenames:\n            filename_path = self.annotations_path + filename\n            tree = ElementTree.parse(filename_path)\n            root = tree.getroot()\n            image_name = root.find('filename').text\n\n            box_data = []\n            difficulties = []\n\n            size_tree = root.find('size')\n            width = float(size_tree.find('width').text)\n            height = float(size_tree.find('height').text)\n            # check evaluate flag\n            if self.evaluate:\n                width = 1\n                height = 1\n            for object_tree in root.findall('object'):\n                difficulty = int(object_tree.find('difficult').text)\n\n                if difficulty == 1 and not (self.with_difficult_samples):\n                    continue\n\n                class_name = object_tree.find('name').text\n                if class_name in self.class_names:\n                    class_arg = self.class_to_arg[class_name]\n                    bounding_box = object_tree.find('bndbox')\n                    # VOC dataset format follows Matlab,\n                    # in which indexes start from 0\n                    xmin = (float(bounding_box.find('xmin').text) - 1.0) / width\n                    ymin = (float(bounding_box.find('ymin').text) - 1.0) / height\n                    xmax = (float(bounding_box.find('xmax').text) - 1.0) / width\n                    ymax = (float(bounding_box.find('ymax').text) - 1.0) / height\n\n                    box_data.append([xmin, ymin, xmax, ymax, class_arg])\n                    difficulties.append(difficulty)\n\n            if len(box_data) == 0:\n                continue\n\n            # self.data[self.images_path + image_name] = label_data\n            image_path = self.images_path + image_name\n            box_data = np.asarray(box_data)\n            difficulties = np.asarray(difficulties, dtype=bool)\n            if self.evaluate:\n                self.data.append({'image': image_path,\n                                  'boxes': box_data,\n                                  'difficulties': difficulties})\n            else:\n                self.data.append({'image': image_path, 'boxes': box_data})\n\n    def load_data(self):\n        return self.data",
  "def __init__(self, path=None, split='train', class_names='all',\n                 name='VOC2007', with_difficult_samples=True, evaluate=False):\n\n        super(VOC, self).__init__(path, split, class_names, name)\n\n        self.with_difficult_samples = with_difficult_samples\n        self.evaluate = evaluate\n        self._class_names = class_names\n        if class_names == 'all':\n            self._class_names = get_class_names('VOC')\n        self.images_path = None\n        self.arg_to_class = None",
  "def load_data(self):\n        if ((self.name == 'VOC2007') or (self.name == 'VOC2012')):\n            ground_truth_data = self._load_VOC(self.name, self.split)\n        elif isinstance(self.name, list):\n            if not isinstance(self.split, list):\n                raise Exception(\"'split' should also be a list\")\n            if set(self.name).issubset(['VOC2007', 'VOC2012']):\n                data_A = self._load_VOC(self.name[0], self.split[0])\n                data_B = self._load_VOC(self.name[1], self.split[1])\n                ground_truth_data = data_A + data_B\n        else:\n            raise ValueError('Invalid name given.')\n        return ground_truth_data",
  "def _load_VOC(self, dataset_name, split):\n        self.parser = VOCParser(dataset_name,\n                                split,\n                                self._class_names,\n                                self.with_difficult_samples,\n                                self.path,\n                                self.evaluate)\n        self.images_path = self.parser.images_path\n        self.arg_to_class = self.parser.arg_to_class\n        ground_truth_data = self.parser.load_data()\n        return ground_truth_data",
  "def __init__(self, dataset_name='VOC2007', split='train',\n                 class_names='all', with_difficult_samples=True,\n                 dataset_path='../datasets/VOCdevkit/',\n                 evaluate=False):\n\n        if dataset_name not in ['VOC2007', 'VOC2012']:\n            raise Exception('Invalid dataset name.')\n\n        # creating data set prefix paths variables\n        self.dataset_name = dataset_name\n        self.dataset_path = os.path.join(dataset_path, dataset_name)\n        self.split = split\n        self.split_prefix = os.path.join(self.dataset_path, 'ImageSets/Main/')\n        self.annotations_path = os.path.join(self.dataset_path, 'Annotations/')\n        self.images_path = os.path.join(self.dataset_path, 'JPEGImages/')\n        self.with_difficult_samples = with_difficult_samples\n        self.evaluate = evaluate\n\n        self.class_names = class_names\n        if self.class_names == 'all':\n            self.class_names = get_class_names('VOC')\n        self.num_classes = len(self.class_names)\n        class_keys = np.arange(self.num_classes)\n        self.arg_to_class = dict(zip(class_keys, self.class_names))\n        self.class_to_arg = {value: key for key, value\n                             in self.arg_to_class.items()}\n        self.data = []\n        self._preprocess_XML()",
  "def _load_filenames(self):\n        split_file = os.path.join(self.split_prefix, self.split) + '.txt'\n        splitted_filenames = []\n        for line in open(split_file):\n            filename = line.strip() + '.xml'\n            splitted_filenames.append(filename)\n        return splitted_filenames",
  "def _preprocess_XML(self):\n        filenames = self._load_filenames()\n        for filename in filenames:\n            filename_path = self.annotations_path + filename\n            tree = ElementTree.parse(filename_path)\n            root = tree.getroot()\n            image_name = root.find('filename').text\n\n            box_data = []\n            difficulties = []\n\n            size_tree = root.find('size')\n            width = float(size_tree.find('width').text)\n            height = float(size_tree.find('height').text)\n            # check evaluate flag\n            if self.evaluate:\n                width = 1\n                height = 1\n            for object_tree in root.findall('object'):\n                difficulty = int(object_tree.find('difficult').text)\n\n                if difficulty == 1 and not (self.with_difficult_samples):\n                    continue\n\n                class_name = object_tree.find('name').text\n                if class_name in self.class_names:\n                    class_arg = self.class_to_arg[class_name]\n                    bounding_box = object_tree.find('bndbox')\n                    # VOC dataset format follows Matlab,\n                    # in which indexes start from 0\n                    xmin = (float(bounding_box.find('xmin').text) - 1.0) / width\n                    ymin = (float(bounding_box.find('ymin').text) - 1.0) / height\n                    xmax = (float(bounding_box.find('xmax').text) - 1.0) / width\n                    ymax = (float(bounding_box.find('ymax').text) - 1.0) / height\n\n                    box_data.append([xmin, ymin, xmax, ymax, class_arg])\n                    difficulties.append(difficulty)\n\n            if len(box_data) == 0:\n                continue\n\n            # self.data[self.images_path + image_name] = label_data\n            image_path = self.images_path + image_name\n            box_data = np.asarray(box_data)\n            difficulties = np.asarray(difficulties, dtype=bool)\n            if self.evaluate:\n                self.data.append({'image': image_path,\n                                  'boxes': box_data,\n                                  'difficulties': difficulties})\n            else:\n                self.data.append({'image': image_path, 'boxes': box_data})",
  "def load_data(self):\n        return self.data",
  "class Shapes(Loader):\n    \"\"\" Loader for shapes synthetic dataset.\n\n    # Arguments\n        num_samples: Int indicating number of samples to load.\n        image_size: (height, width) of input image to load.\n        split: String determining the data split to load.\n            e.g. `train`, `val` or `test`\n        class_names: List of strings or `all`.\n        iou_thresh: Float intersection over union.\n        max_num_shapes: Int. maximum number of shapes in the image.\n\n    # Returns\n        List of dictionaries with keys `image`, `mask`, `box_data`\n            containing\n    \"\"\"\n    def __init__(self, num_samples, image_size, split='train',\n                 class_names='all', iou_thresh=0.3, max_num_shapes=3):\n        if class_names == 'all':\n            class_names = ['background', 'square', 'circle', 'triangle']\n        self.name_to_arg = dict(zip(class_names, range(len(class_names))))\n        self.arg_to_name = dict(zip(range(len(class_names)), class_names))\n        self.num_samples, self.image_size = num_samples, image_size\n        self.labels = ['image', 'masks', 'box_data']\n        self.iou_thresh = iou_thresh\n        self.max_num_shapes = max_num_shapes\n        super(Shapes, self).__init__(None, split, class_names, 'Shapes')\n\n    def load_data(self):\n        return [self.load_sample() for arg in range(self.num_samples)]\n\n    def load_sample(self):\n        shapes = self._sample_shapes(self.max_num_shapes, *self.image_size)\n        boxes = self._compute_bounding_boxes(shapes)\n        shapes, boxes = self._filter_shapes(boxes, shapes, self.iou_thresh)\n        image = self._draw_shapes(shapes)\n        masks = self._draw_masks(shapes)\n        class_args = [self.name_to_arg[name[0]] for name in shapes]\n        class_args = np.asarray(class_args).reshape(-1, 1)\n        box_data = np.concatenate([boxes, class_args], axis=1)\n        sample = dict(zip(self.labels, [image, masks, box_data]))\n        return sample\n\n    def _sample_shape(self, H, W, offset=20):\n        shape = np.random.choice(self.class_names[1:])\n        color = tuple(np.random.randint(0, 255, size=3).tolist())\n        center_x = np.random.randint(offset, W - offset - 1)\n        center_y = np.random.randint(offset, H - offset - 1)\n        size = np.random.randint(offset, H // 4)\n        return shape, color, (center_x, center_y, size)\n\n    def _sample_shapes(self, num_shapes, H, W, offset=20):\n        shapes = []\n        for shape_arg in range(num_shapes):\n            shapes.append(self._sample_shape(H, W, offset=20))\n        return shapes\n\n    def _compute_bounding_box(self, center_x, center_y, size):\n        x_min, y_min = center_x - size, center_y - size\n        x_max, y_max = center_x + size, center_y + size\n        box = [x_min, y_min, x_max, y_max]\n        return box\n\n    def _compute_bounding_boxes(self, shapes):\n        boxes = []\n        for shape in shapes:\n            center_x, center_y, size = shape[2]\n            box = self._compute_bounding_box(center_x, center_y, size)\n            boxes.append(box)\n        return np.asarray(boxes)\n\n    def _filter_shapes(self, boxes, shapes, iou_thresh):\n        scores = np.ones(len(boxes))  # all shapes have the same score\n        args, num_boxes = apply_non_max_suppression(boxes, scores, iou_thresh)\n        box_args = args[:num_boxes]\n        selected_shapes = []\n        for box_arg in box_args:\n            selected_shapes.append(shapes[box_arg])\n        return selected_shapes, boxes[box_args]\n\n    def _draw_shapes(self, shapes):\n        H, W = self.image_size\n        background_color = np.random.randint(0, 255, size=3)\n        image = np.ones([H, W, 3], dtype=np.uint8)\n        image = image * background_color.astype(np.uint8)\n        for shape, color, dimensions in shapes:\n            image = self._draw_shape(image, shape, dimensions, color)\n        return image\n\n    def _draw_shape(self, image, shape, dimensions, color):\n        center_x, center_y, size = dimensions\n        functions = [draw_square, draw_circle, draw_triangle]\n        draw = dict(zip(self.class_names[1:], functions))\n        image = draw[shape](image, (center_x, center_y), color, size)\n        return image\n\n    def _draw_masks(self, shapes):\n        H, W = self.image_size\n        class_masks = []\n        for class_mask in range(self.num_classes):\n            class_masks.append(np.zeros([H, W, 1]))\n        class_masks[0] = np.logical_not(class_masks[0])\n        for shape_arg, (shape, color, dimensions) in enumerate(shapes):\n            mask_arg = self.name_to_arg[shape]\n            class_mask = class_masks[mask_arg]\n            class_mask = self._draw_shape(\n                class_mask, shape, dimensions, (1, 1, 1))\n            class_masks[mask_arg] = class_mask\n            negative_mask = np.logical_not(class_mask)\n            background_mask = class_masks[0].copy()\n            class_masks[0] = np.logical_and(negative_mask, background_mask)\n        masks = np.concatenate(class_masks, axis=-1).astype(np.uint8)\n        return masks",
  "def __init__(self, num_samples, image_size, split='train',\n                 class_names='all', iou_thresh=0.3, max_num_shapes=3):\n        if class_names == 'all':\n            class_names = ['background', 'square', 'circle', 'triangle']\n        self.name_to_arg = dict(zip(class_names, range(len(class_names))))\n        self.arg_to_name = dict(zip(range(len(class_names)), class_names))\n        self.num_samples, self.image_size = num_samples, image_size\n        self.labels = ['image', 'masks', 'box_data']\n        self.iou_thresh = iou_thresh\n        self.max_num_shapes = max_num_shapes\n        super(Shapes, self).__init__(None, split, class_names, 'Shapes')",
  "def load_data(self):\n        return [self.load_sample() for arg in range(self.num_samples)]",
  "def load_sample(self):\n        shapes = self._sample_shapes(self.max_num_shapes, *self.image_size)\n        boxes = self._compute_bounding_boxes(shapes)\n        shapes, boxes = self._filter_shapes(boxes, shapes, self.iou_thresh)\n        image = self._draw_shapes(shapes)\n        masks = self._draw_masks(shapes)\n        class_args = [self.name_to_arg[name[0]] for name in shapes]\n        class_args = np.asarray(class_args).reshape(-1, 1)\n        box_data = np.concatenate([boxes, class_args], axis=1)\n        sample = dict(zip(self.labels, [image, masks, box_data]))\n        return sample",
  "def _sample_shape(self, H, W, offset=20):\n        shape = np.random.choice(self.class_names[1:])\n        color = tuple(np.random.randint(0, 255, size=3).tolist())\n        center_x = np.random.randint(offset, W - offset - 1)\n        center_y = np.random.randint(offset, H - offset - 1)\n        size = np.random.randint(offset, H // 4)\n        return shape, color, (center_x, center_y, size)",
  "def _sample_shapes(self, num_shapes, H, W, offset=20):\n        shapes = []\n        for shape_arg in range(num_shapes):\n            shapes.append(self._sample_shape(H, W, offset=20))\n        return shapes",
  "def _compute_bounding_box(self, center_x, center_y, size):\n        x_min, y_min = center_x - size, center_y - size\n        x_max, y_max = center_x + size, center_y + size\n        box = [x_min, y_min, x_max, y_max]\n        return box",
  "def _compute_bounding_boxes(self, shapes):\n        boxes = []\n        for shape in shapes:\n            center_x, center_y, size = shape[2]\n            box = self._compute_bounding_box(center_x, center_y, size)\n            boxes.append(box)\n        return np.asarray(boxes)",
  "def _filter_shapes(self, boxes, shapes, iou_thresh):\n        scores = np.ones(len(boxes))  # all shapes have the same score\n        args, num_boxes = apply_non_max_suppression(boxes, scores, iou_thresh)\n        box_args = args[:num_boxes]\n        selected_shapes = []\n        for box_arg in box_args:\n            selected_shapes.append(shapes[box_arg])\n        return selected_shapes, boxes[box_args]",
  "def _draw_shapes(self, shapes):\n        H, W = self.image_size\n        background_color = np.random.randint(0, 255, size=3)\n        image = np.ones([H, W, 3], dtype=np.uint8)\n        image = image * background_color.astype(np.uint8)\n        for shape, color, dimensions in shapes:\n            image = self._draw_shape(image, shape, dimensions, color)\n        return image",
  "def _draw_shape(self, image, shape, dimensions, color):\n        center_x, center_y, size = dimensions\n        functions = [draw_square, draw_circle, draw_triangle]\n        draw = dict(zip(self.class_names[1:], functions))\n        image = draw[shape](image, (center_x, center_y), color, size)\n        return image",
  "def _draw_masks(self, shapes):\n        H, W = self.image_size\n        class_masks = []\n        for class_mask in range(self.num_classes):\n            class_masks.append(np.zeros([H, W, 1]))\n        class_masks[0] = np.logical_not(class_masks[0])\n        for shape_arg, (shape, color, dimensions) in enumerate(shapes):\n            mask_arg = self.name_to_arg[shape]\n            class_mask = class_masks[mask_arg]\n            class_mask = self._draw_shape(\n                class_mask, shape, dimensions, (1, 1, 1))\n            class_masks[mask_arg] = class_mask\n            negative_mask = np.logical_not(class_mask)\n            background_mask = class_masks[0].copy()\n            class_masks[0] = np.logical_and(negative_mask, background_mask)\n        masks = np.concatenate(class_masks, axis=-1).astype(np.uint8)\n        return masks",
  "class FER(Loader):\n    \"\"\"Class for loading FER2013 emotion classification dataset.\n    # Arguments\n        path: String. Full path to fer2013.csv file.\n        split: String. Valid option contain 'train', 'val' or 'test'.\n        class_names: String or list: If 'all' then it loads all default\n            class names.\n        image_size: List of length two. Indicates the shape in which\n            the image will be resized.\n\n    # References\n        -[FER2013 Dataset and Challenge](kaggle.com/c/challenges-in-\\\n            representation-learning-facial-expression-recognition-challenge)\n    \"\"\"\n\n    def __init__(\n            self, path, split='train', class_names='all', image_size=(48, 48)):\n\n        if class_names == 'all':\n            class_names = get_class_names('FER')\n\n        path = os.path.join(path, 'fer2013.csv')\n        super(FER, self).__init__(path, split, class_names, 'FER')\n        self.image_size = image_size\n        self._split_to_filter = {'train': 'Training', 'val': 'PublicTest',\n                                 'test': 'PrivateTest'}\n\n    def load_data(self):\n        data = np.genfromtxt(self.path, str, delimiter=',', skip_header=1)\n        data = data[data[:, -1] == self._split_to_filter[self.split]]\n        faces = np.zeros((len(data), *self.image_size))\n        for sample_arg, sample in enumerate(data):\n            face = np.array(sample[1].split(' '), dtype=int).reshape(48, 48)\n            face = resize_image(face, self.image_size)\n            faces[sample_arg, :, :] = face\n        emotions = to_categorical(data[:, 0].astype(int), self.num_classes)\n\n        data = []\n        for face, emotion in zip(faces, emotions):\n            sample = {'image': face, 'label': emotion}\n            data.append(sample)\n        return data",
  "def __init__(\n            self, path, split='train', class_names='all', image_size=(48, 48)):\n\n        if class_names == 'all':\n            class_names = get_class_names('FER')\n\n        path = os.path.join(path, 'fer2013.csv')\n        super(FER, self).__init__(path, split, class_names, 'FER')\n        self.image_size = image_size\n        self._split_to_filter = {'train': 'Training', 'val': 'PublicTest',\n                                 'test': 'PrivateTest'}",
  "def load_data(self):\n        data = np.genfromtxt(self.path, str, delimiter=',', skip_header=1)\n        data = data[data[:, -1] == self._split_to_filter[self.split]]\n        faces = np.zeros((len(data), *self.image_size))\n        for sample_arg, sample in enumerate(data):\n            face = np.array(sample[1].split(' '), dtype=int).reshape(48, 48)\n            face = resize_image(face, self.image_size)\n            faces[sample_arg, :, :] = face\n        emotions = to_categorical(data[:, 0].astype(int), self.num_classes)\n\n        data = []\n        for face, emotion in zip(faces, emotions):\n            sample = {'image': face, 'label': emotion}\n            data.append(sample)\n        return data",
  "class UnsolvableMatrix(Exception):\n    \"\"\"\n    Exception raised for unsolvable matrices\n    \"\"\"\n    pass",
  "class DISALLOWED_OBJ(object):\n    pass",
  "def get_cover_matrix(shape):\n    \"\"\"Returns the initialized row and column cover matrix.\n\n    # Arguments\n        shape: Tuple. Shape of the cover matrix.\n    \"\"\"\n    row_covered = np.zeros(shape, dtype='bool')\n    col_covered = np.zeros(shape, dtype='bool')\n    return row_covered, col_covered",
  "def find_uncovered_zero(n, cost_matrix, row_covered, col_covered, i0, j0):\n    row = -1\n    col = -1\n    done = False\n    for row_arg in range(i0, n):\n        for col_arg in range(j0, n):\n            if (cost_matrix[row_arg][col_arg] == 0) and \\\n                    (not row_covered[row_arg]) and \\\n                    (not col_covered[col_arg]):\n                row = row_arg\n                col = col_arg\n                done = True\n        if done:\n            break\n    return (row, col)",
  "def find_star_in_row(n, row_arg, marked):\n    col = -1\n    for col_arg in range(n):\n        if marked[row_arg][col_arg] == 1:\n            col = col_arg\n            break\n    return col",
  "def find_star_in_col(n, col_arg, marked):\n    row = -1\n    for row_arg in range(n):\n        if marked[row_arg][col_arg] == 1:\n            row = row_arg\n            break\n    return row",
  "def find_prime_in_row(n, row_arg, marked):\n    col = -1\n    for col_arg in range(n):\n        if marked[row_arg][col_arg] == 2:\n            col = col_arg\n            break\n    return col",
  "def get_min_value(series):\n    values = []\n    for x in series:\n        if type(x) is not type(DISALLOWED):\n            values.append(x)\n    if len(values) == 0:\n        raise UnsolvableMatrix(\"One row is entirely DISALLOWED.\")\n    min_value = np.min(values)\n    return min_value",
  "def find_smallest_uncovered(n, row_covered, col_covered, cost_matrix):\n    minval = np.inf\n    for i in range(n):\n        for j in range(n):\n            if (not row_covered[i]) and (not col_covered[j]):\n                if cost_matrix[i][j] is not DISALLOWED and \\\n                        minval > cost_matrix[i][j]:\n                    minval = cost_matrix[i][j]\n    return minval",
  "def build_cube_points3D(width, height, depth):\n    \"\"\"Build the 3D points of a cube in the openCV coordinate system:\n                               4--------1\n                              /|       /|\n                             / |      / |\n                            3--------2  |\n                            |  8_____|__5\n                            | /      | /\n                            |/       |/\n                            7--------6\n\n                   Z (depth)\n                  /\n                 /_____X (width)\n                 |\n                 |\n                 Y (height)\n\n    # Arguments\n        height: float, height of the 3D box.\n        width: float,  width of the 3D box.\n        depth: float,  width of the 3D box.\n\n    # Returns\n        Numpy array of shape ``(8, 3)'' corresponding to 3D keypoints of a cube\n    \"\"\"\n    half_height, half_width, half_depth = height / 2., width / 2., depth / 2.\n    point_1 = [+half_width, -half_height, +half_depth]\n    point_2 = [+half_width, -half_height, -half_depth]\n    point_3 = [-half_width, -half_height, -half_depth]\n    point_4 = [-half_width, -half_height, +half_depth]\n    point_5 = [+half_width, +half_height, +half_depth]\n    point_6 = [+half_width, +half_height, -half_depth]\n    point_7 = [-half_width, +half_height, -half_depth]\n    point_8 = [-half_width, +half_height, +half_depth]\n    return np.array([point_1, point_2, point_3, point_4,\n                     point_5, point_6, point_7, point_8])",
  "def normalize_keypoints2D(points2D, height, width):\n    \"\"\"Transform points2D in image coordinates to normalized coordinates i.e.\n        [U, V] -> [-1, 1]. UV have maximum values of [W, H] respectively.\n\n             Image plane\n\n                 width\n           (0,0)-------->  (U)\n             |\n      height |\n             |\n             v\n\n            (V)\n\n    # Arguments\n        points2D: Numpy array of shape (num_keypoints, 2).\n        height: Int. Height of the image\n        width: Int. Width of the image\n\n    # Returns\n        Numpy array of shape (num_keypoints, 2).\n    \"\"\"\n    image_shape = np.array([width, height])\n    points2D = points2D / image_shape  # [W, 0], [0, H] -> [1,  0], [0,  1]\n    points2D = 2.0 * points2D          # [1, 0], [0, 1] -> [2,  0], [0,  2]\n    points2D = points2D - 1.0          # [2, 0], [0, 2] -> [-1, 1], [-1, 1]\n    return points2D",
  "def denormalize_keypoints2D(points2D, height, width):\n    \"\"\"Transform nomralized points2D to image UV coordinates i.e.\n        [-1, 1] -> [U, V]. UV have maximum values of [W, H] respectively.\n\n             Image plane\n\n           (0,0)-------->  (U)\n             |\n             |\n             |\n             v\n\n            (V)\n\n    # Arguments\n        points2D: Numpy array of shape (num_keypoints, 2).\n        height: Int. Height of the image\n        width: Int. Width of the image\n\n    # Returns\n        Numpy array of shape (num_keypoints, 2).\n    \"\"\"\n    image_shape = np.array([width, height])\n    points2D = points2D + 1.0          # [-1, 1], [-1, 1] -> [2, 0], [0, 2]\n    points2D = points2D / 2.0          # [2 , 0], [0 , 2] -> [1, 0], [0, 1]\n    points2D = points2D * image_shape  # [1 , 0], [0 , 1] -> [W, 0], [0, H]\n    return points2D",
  "def cascade_classifier(path):\n    \"\"\"OpenCV Cascade classifier.\n\n    # Arguments\n        path: String. Path to default openCV XML format.\n\n    # Returns\n        OpenCV classifier with ``detectMultiScale`` for inference..\n    \"\"\"\n    return cv2.CascadeClassifier(path)",
  "def solve_PNP(points3D, points2D, camera, solver):\n    \"\"\"Calculates 6D pose from 3D points and 2D keypoints correspondences.\n\n    # Arguments\n        points3D: Numpy array of shape ``(num_points, 3)``.\n            3D points known in advance.\n        points2D: Numpy array of shape ``(num_points, 2)``.\n            Predicted 2D keypoints of object.\n        camera: Instance of ''paz.backend.Camera'' containing as properties\n            the ''camera_intrinsics'' a Numpy array of shape ''(3, 3)''\n            usually calculated from the openCV ''calibrateCamera'' function,\n            and the ''distortion'' a Numpy array of shape ''(5)'' in which the\n            elements are usually obtained from the openCV\n            ''calibrateCamera'' function.\n        solver: Flag from e.g openCV.SOLVEPNP_UPNP.\n        distortion: Numpy array of shape of 5 elements calculated from\n            the openCV calibrateCamera function.\n\n    # Returns\n        A list containing success flag, rotation and translation components\n        of the 6D pose.\n    \"\"\"\n    return cv2.solvePnP(points3D, points2D, camera.intrinsics,\n                        camera.distortion, None, None, False, solver)",
  "def project_points3D(points3D, pose6D, camera):\n    \"\"\"Projects 3D points into a specific pose.\n\n    # Arguments\n        points3D: Numpy array of shape ``(num_points, 3)``.\n        pose6D: An instance of ``paz.abstract.Pose6D``.\n        camera: An instance of ``paz.backend.Camera`` object.\n\n    # Returns\n        Numpy array of shape ``(num_points, 2)``\n    \"\"\"\n    points2D, jacobian = cv2.projectPoints(\n        points3D, pose6D.rotation_vector, pose6D.translation,\n        camera.intrinsics, camera.distortion)\n    # openCV adds a dimension to projection i.e. (num_points, 1, 2)\n    points2D = np.squeeze(points2D, axis=1)\n    return points2D",
  "def project_to_image(rotation, translation, points3D, camera_intrinsics):\n    \"\"\"Project points3D to image plane using a perspective transformation.\n\n              Image plane\n\n           (0,0)-------->  (U)\n             |\n             |\n             |\n             v\n\n            (V)\n\n    # Arguments\n        rotation: Array (3, 3). Rotation matrix (Rco).\n        translation: Array (3). Translation (Tco).\n        points3D: Array (num_points, 3). Points 3D in object frame.\n        camera_intrinsics: Array of shape (3, 3). Diagonal elements represent\n            focal lenghts and last column the image center translation.\n\n    # Returns\n        Array (num_points, 2) in UV image space.\n    \"\"\"\n    if rotation.shape != (3, 3):\n        raise ValueError('Rotation matrix is not of shape (3, 3)')\n    if len(translation) != 3:\n        raise ValueError('Translation vector is not of length 3')\n    if len(points3D.shape) != 2:\n        raise ValueError('Points3D should have a shape (num_points, 3)')\n    if points3D.shape[1] != 3:\n        raise ValueError('Points3D should have a shape (num_points, 3)')\n    # TODO missing checks for camera intrinsics conditions\n    points3D = np.matmul(rotation, points3D.T).T + translation\n    x, y, z = np.split(points3D, 3, axis=1)\n    x_focal_length = camera_intrinsics[0, 0]\n    y_focal_length = camera_intrinsics[1, 1]\n    x_image_center = camera_intrinsics[0, 2]\n    y_image_center = camera_intrinsics[1, 2]\n    x_points = (x_focal_length * (x / z)) + x_image_center\n    y_points = (y_focal_length * (y / z)) + y_image_center\n    projected_points2D = np.concatenate([x_points, y_points], axis=1)\n    return projected_points2D",
  "def translate_points2D_origin(points2D, coordinates):\n    \"\"\"Translates points2D to a different origin\n\n    # Arguments\n        points2D: Array (num_points, 2)\n        coordinates: Array (4) containing (x_min, y_min, x_max, y_max)\n\n    # Returns\n        Translated points2D array (num_points, 2)\n    \"\"\"\n    x_min, y_min, x_max, y_max = coordinates\n    points2D[:, 0] = points2D[:, 0] + x_min\n    points2D[:, 1] = points2D[:, 1] + y_min\n    return points2D",
  "def translate_keypoints(keypoints, translation):\n    \"\"\"Translate keypoints.\n\n    # Arguments\n        kepoints: Numpy array of shape ``(num_keypoints, 2)``.\n        translation: A list of length two indicating the x,y translation values\n\n    # Returns\n        Numpy array\n    \"\"\"\n    return keypoints + translation",
  "def _preprocess_image_points2D(image_points2D):\n    \"\"\"Preprocessing image points for openCV's PnPRANSAC\n\n    # Arguments\n        image_points2D: Array of shape (num_points, 2)\n\n    # Returns\n        Contiguous float64 array of shape (num_points, 1, 2)\n    \"\"\"\n    num_points = len(image_points2D)\n    image_points2D = image_points2D.reshape(num_points, 1, 2)\n    image_points2D = image_points2D.astype(np.float64)\n    image_points2D = np.ascontiguousarray(image_points2D)\n    return image_points2D",
  "def solve_PnP_RANSAC(object_points3D, image_points2D, camera_intrinsics,\n                     inlier_threshold=5, num_iterations=100):\n    \"\"\"Returns rotation (Roc) and translation (Toc) vectors that transform\n        3D points in object frame to camera frame.\n\n                               O------------O\n                              /|           /|\n                             / |          / |\n                            O------------O  |\n                            |  |    z    |  |\n                            |  O____|____|__O\n                            |  /    |___y|  /   object\n                            | /    /     | /  coordinates\n                            |/    x      |/\n                            O------------O\n                                   ___\n                   Z                |\n                  /                 | Rco, Tco\n                 /_____X     <------|\n                 |\n                 |    camera\n                 Y  coordinates\n\n    # Arguments\n        object_points3D: Array (num_points, 3). Points 3D in object reference\n            frame. Represented as (0) in image above.\n        image_points2D: Array (num_points, 2). Points in 2D in camera UV space.\n        camera_intrinsics: Array of shape (3, 3). Diagonal elements represent\n            focal lenghts and last column the image center translation.\n        inlier_threshold: Number of inliers for RANSAC method.\n        num_iterations: Maximum number of iterations.\n\n    # Returns\n        Rotation vector in axis-angle form (3) and translation vector (3).\n    \"\"\"\n    if ((len(object_points3D) < 4) or (len(image_points2D) < 4)):\n        raise ValueError('Solve PnP requires at least 4 3D and 2D points')\n    image_points2D = _preprocess_image_points2D(image_points2D)\n    success, rotation_vector, translation, inliers = cv2.solvePnPRansac(\n        object_points3D, image_points2D, camera_intrinsics, None,\n        flags=cv2.SOLVEPNP_EPNP, reprojectionError=inlier_threshold,\n        iterationsCount=num_iterations)\n    translation = np.squeeze(translation, 1)\n    return success, rotation_vector, translation",
  "def arguments_to_image_points2D(row_args, col_args):\n    \"\"\"Convert array arguments into UV coordinates.\n\n              Image plane\n\n           (0,0)-------->  (U)\n             |\n             |\n             |\n             v\n\n            (V)\n\n    # Arguments\n        row_args: Array (num_rows).\n        col_args: Array (num_cols).\n\n    # Returns\n        Array (num_cols, num_rows) representing points2D in UV space.\n\n    # Notes\n        Arguments are row args (V) and col args (U). Image points are in UV\n            coordinates; thus, we concatenate them in that order\n            i.e. [col_args, row_args]\n    \"\"\"\n    row_args = row_args.reshape(-1, 1)\n    col_args = col_args.reshape(-1, 1)\n    image_points2D = np.concatenate([col_args, row_args], axis=1)  # (U, V)\n    return image_points2D",
  "def normalize_keypoints(keypoints, height, width):\n    \"\"\"Transform keypoints in image coordinates to normalized coordinates\n    # Arguments\n        keypoints: Numpy array of shape ``(num_keypoints, 2)``.\n        height: Int. Height of the image\n        width: Int. Width of the image\n    # Returns\n        Numpy array of shape ``(num_keypoints, 2)``.\n    \"\"\"\n    warn('DEPRECATED please use denomarlize_points2D')\n    normalized_keypoints = np.zeros_like(keypoints, dtype=np.float32)\n    for keypoint_arg, keypoint in enumerate(keypoints):\n        x, y = keypoint[:2]\n        # transform key-point coordinates to image coordinates\n        x = (((x + 0.5) - (width / 2.0)) / (width / 2))\n        y = (((height - 0.5 - y) - (height / 2.0)) / (height / 2))\n        normalized_keypoints[keypoint_arg][:2] = [x, y]\n    return normalized_keypoints",
  "def denormalize_keypoints(keypoints, height, width):\n    \"\"\"Transform normalized keypoint coordinates into image coordinates\n    # Arguments\n        keypoints: Numpy array of shape ``(num_keypoints, 2)``.\n        height: Int. Height of the image\n        width: Int. Width of the image\n    # Returns\n        Numpy array of shape ``(num_keypoints, 2)``.\n    \"\"\"\n    warn('DEPRECATED please use denomarlize_points2D')\n    for keypoint_arg, keypoint in enumerate(keypoints):\n        x, y = keypoint[:2]\n        # transform key-point coordinates to image coordinates\n        x = (min(max(x, -1), 1) * width / 2 + width / 2) - 0.5\n        # flip since the image coordinates for y are flipped\n        y = height - 0.5 - (min(max(y, -1), 1) * height / 2 + height / 2)\n        x, y = int(round(x)), int(round(y))\n        keypoints[keypoint_arg][:2] = [x, y]\n    return keypoints",
  "def rotate_point2D(point2D, rotation_angle):\n    \"\"\"Rotate keypoint.\n\n    # Arguments\n        point2D: keypoint [x, y]\n        rotation angle: Int. Angle of rotation.\n\n    # Returns\n        List of x and y rotated points\n    \"\"\"\n    rotation_angle = np.pi * rotation_angle / 180\n    sin_n, cos_n = np.sin(rotation_angle), np.cos(rotation_angle)\n    x_rotated = (point2D[0] * cos_n) - (point2D[1] * sin_n)\n    y_rotated = (point2D[0] * sin_n) + (point2D[1] * cos_n)\n    return [x_rotated, y_rotated]",
  "def transform_keypoint(keypoint, transform):\n    \"\"\" Transform keypoint.\n\n    # Arguments\n        keypoint2D: keypoint [x, y]\n        transform: Array. Transformation matrix\n    \"\"\"\n    keypoint = np.array([keypoint[0], keypoint[1], 1.]).T\n    transformed_keypoint = np.dot(transform, keypoint)\n    return transformed_keypoint",
  "def add_offset_to_point(keypoint_location, offset=0):\n    \"\"\" Add offset to keypoint location\n\n    # Arguments\n        keypoint_location: keypoint [y, x]\n        offset: Float.\n    \"\"\"\n    y, x = keypoint_location\n    y = y + offset\n    x = x + offset\n    return y, x",
  "def flip_keypoints_left_right(keypoints, image_size=(32, 32)):\n    \"\"\"Flip the detected 2D keypoints left to right.\n\n    # Arguments\n        keypoints: Array\n        image_size: list/tuple\n        axis: int\n\n    # Returns\n        flipped_keypoints: Numpy array\n    \"\"\"\n    x_coordinates, y_coordinates = np.split(keypoints, 2, axis=1)\n    flipped_x = image_size[0] - x_coordinates\n    keypoints = np.concatenate((flipped_x, y_coordinates), axis=1)\n    return keypoints",
  "def compute_orientation_vector(keypoints3D, parents):\n    \"\"\"Compute bone orientations from joint coordinates\n       (child joint - parent joint). The returned vectors are normalized.\n       For the root joint, it will be a zero vector.\n\n    # Arguments\n        keypoints3D : Numpy array [num_keypoints, 3]. Joint coordinates.\n        parents: Parents of the keypoints from kinematic chain\n\n    # Returns\n        Array [num_keypoints, 3]. The unit vectors from each child joint to\n        its parent joint. For the root joint, it's are zero vector.\n    \"\"\"\n    delta = []\n    for joint_arg in range(len(parents)):\n        parent = parents[joint_arg]\n        if parent is None:\n            delta.append(np.zeros(3))\n        else:\n            delta.append(keypoints3D[joint_arg] - keypoints3D[parent])\n    delta = np.stack(delta, 0)\n    return delta",
  "def rotate_keypoints3D(rotation_matrix, keypoints):\n    \"\"\"Rotatate the keypoints by using rotation matrix\n\n    # Arguments\n        Rotation matrix [N, 3, 3].\n        keypoints [N, 3]\n\n    # Returns\n        Rotated keypoints [N, 3]\n    \"\"\"\n    keypoint_xyz = np.einsum('ijk, ik -> ij', rotation_matrix, keypoints)\n    return keypoint_xyz",
  "def flip_along_x_axis(keypoints, axis=0):\n    \"\"\"Flip the keypoints along the x axis.\n\n    # Arguments\n        keypoints: Array\n        axis: int/list\n\n    # Returns\n        Flipped keypoints: Array\n    # \"\"\"\n    x, y, z = np.split(keypoints, 3, axis=1)\n    keypoints = np.concatenate((-x, y, z), axis=1)\n    return keypoints",
  "def uv_to_vu(keypoints):\n    \"\"\"Flips the uv coordinates to vu.\n\n    # Arguments\n        keypoints: Array.\n    \"\"\"\n    return keypoints[:, ::-1]",
  "def standardize(data, mean, scale):\n    \"\"\"It takes the data the mean and the standard deviation\n       and returns the standardized data\n\n    # Arguments\n        data: nxd matrix to normalize\n        mean: Array of means\n        scale: standard deviation\n\n    # Returns\n        standardized poses2D\n    # \"\"\"\n    return np.divide((data - mean), scale)",
  "def destandardize(data, mean, scale):\n    \"\"\"It takes the standardized data the mean and the standard\n       deviation and returns the destandardized data\n\n    # Arguments\n        data: nxd matrix to unnormalize\n        mean: Array of means\n        scale: standard deviation\n\n    # Returns\n        destandardized poses3D\n    \"\"\"\n    return (data * scale) + mean",
  "def initialize_translation(joints2D, camera_intrinsics, ratio):\n    \"\"\"Computes initial 3D translation of root joint\n\n    # Arguments\n        joints2D: 2D root joint from HigherHRNet\n        camera_intrinsics: camera intrinsic parameters\n        ratio: ration of sum of 3D bones to 2D bones\n\n    # Returns\n        Array of initial estimate of the global position\n        of the root joint in 3D\n    \"\"\"\n    focal_length = camera_intrinsics[0, 0]\n    image_center_x = camera_intrinsics[0, 2]\n    image_center_y = camera_intrinsics[1, 2]\n    z = focal_length * ratio\n    x = (joints2D[:, 0] - image_center_x) * ratio\n    y = (joints2D[:, 1] - image_center_y) * ratio\n    translation = np.array((x, y, z))\n    return translation.flatten()",
  "def solve_least_squares(solver, compute_joints_distance,\n                        initial_joints_translation, joints3D,\n                        poses2D, camera_intrinsics):\n    \"\"\"Solve the least squares\n\n    # Arguments\n        solver: from scipy.optimize import least_squares\n        compute_joints_distance: global_pose.compute_joints_distance\n        initial_root_translation: initial 3D translation of root joint\n        joints3D: 16 moving joints in 3D\n        poses2d: 2D poses\n        camera_intrinsics: camera intrinsic parameters\n\n    Returns\n        optimal translation of root joint for each person\n    \"\"\"\n    joints_translation = solver(\n        compute_joints_distance, initial_joints_translation, verbose=0,\n        args=(joints3D, poses2D, camera_intrinsics))\n    joints_translation = np.reshape(joints_translation.x, (-1, 3))\n    return joints_translation",
  "def get_bones_length(poses2D, poses3D, start_joints,\n                     end_joints=np.arange(1, 16)):\n    \"\"\"Computes sum of bone lengths in 3D\n\n    #Arguments\n        poses3D: list of predicted poses in 3D (Nx16x3)\n        poses2D: list of poses in 2D    (Nx32)\n\n    #Returns\n        sum_bones2D: array of sum of length of all bones in the 2D skeleton\n        sum_bones3D: array of sum of length of all bones in the 3D skeleton\n    \"\"\"\n    sum_bones2D = []\n    sum_bones3D = []\n    poses3D = np.reshape(poses3D, (poses3D.shape[0], 16, -1))\n    poses2D = np.reshape(poses2D, (poses2D.shape[0], 16, -1))\n\n    for person in poses2D:\n        person_sum_2D = 0\n        for arg in range(len(start_joints)):\n            bone_length = np.linalg.norm(person[start_joints[arg]] -\n                                         person[end_joints[arg]])\n            person_sum_2D = person_sum_2D + bone_length\n        sum_bones2D.append(person_sum_2D)\n\n    for person in poses3D:\n        person_sum_3D = 0\n        for arg in range(len(start_joints)):\n            bone_length = np.linalg.norm(person[start_joints[arg]] -\n                                         person[end_joints[arg]])\n            person_sum_3D = person_sum_3D + bone_length\n        sum_bones3D.append(person_sum_3D)\n\n    return np.array(sum_bones2D), np.array(sum_bones3D)",
  "def compute_reprojection_error(initial_translation, keypoints3D,\n                               keypoints2D, camera_intrinsics):\n    \"\"\"compute distance between each person joints\n\n    # Arguments\n        initial_translation: initial guess of position of joint\n        keypoints3D: 3D keypoints to be optimized (Nx16x3)\n        keypoints2D: 2D keypoints (Nx32)\n        camera_inrinsics: camera intrinsic parameters\n\n    # Returns\n        person_sum: sum of L2 distances between each joint per person\n    \"\"\"\n    initial_translation = np.reshape(initial_translation, (-1, 3))\n    new_poses3D = np.zeros((keypoints3D.shape))\n    for person in range(len(initial_translation)):\n        new_poses3D[person] = (keypoints3D[person] +\n                               initial_translation[person])\n    new_poses3D = new_poses3D.reshape((-1, 3))\n    rotation = np.identity(3)\n    translation = np.zeros((3,))\n    project2D = project_to_image(rotation, translation, new_poses3D,\n                                 camera_intrinsics)\n    joints_distance = np.linalg.norm(np.ravel(keypoints2D) -\n                                     np.ravel(project2D))\n\n    return np.sum(joints_distance)",
  "def merge_into_mean(keypoints2D, args_to_mean):\n    \"\"\"merge keypoints and take the mean\n\n    # Arguments:\n             keypoints2D: keypoints2D (Nx17x2)\n             args_to_mean: dict of joint indices\n\n    # Returns:\n             keypoints2D: keypoints2D after merging\n            \"\"\"\n    keypoints = np.array(keypoints2D.copy())\n    for point, joints_indices in args_to_mean.items():\n        keypoints[:, point] = (keypoints[:, joints_indices[0]] +\n                               keypoints[:, joints_indices[1]]) / 2\n    return keypoints",
  "def filter_keypoints(keypoints, args_to_joints):\n    \"\"\"filter keypoints.\n\n    # Arguments\n        keypoints: points in camera coordinates\n        args_to_joints: Array of joints indices\n\n    # Returns\n        filtered keypoints\n    # \"\"\"\n    return keypoints[:, args_to_joints, :]",
  "def filter_keypoints3D(keypoints3D, args_to_joints3D):\n    \"\"\"Selects 16 moving joints (Neck/Nose excluded) from 32 predicted\n       joints in 3D\n\n    # Arguments\n        keypoints3D: Nx96 points in camera coordinates\n        args_to_joints3D: list of indices\n\n    # Returns\n        filtered_joints_3D: Nx48 points (moving joints)\n    \"\"\"\n    keypoints_num = len(keypoints3D)\n    keypoints3D = np.reshape(keypoints3D, [keypoints_num, 32, 3])\n    joints3D = filter_keypoints(keypoints3D, args_to_joints3D)\n    return joints3D",
  "def filter_keypoints2D(keypoints2D, args_to_mean, h36m_to_coco_joints2D):\n    \"\"\"Selects 16 moving joints (Neck/Nose excluded) from 17 predicted\n       joints in 2D\n\n    # Arguments\n        keypoints3D: Nx17x2 points in camera coordinates\n        args_to_mean: keypoints indices\n        h36m_to_coco_joints2D: human36m dataset list of joints indices\n\n    # Returns\n        joints2D: Nx32 points (moving joints)\n    \"\"\"\n    joints2D = filter_keypoints(keypoints2D, h36m_to_coco_joints2D)\n    joints2D = np.reshape(joints2D, [joints2D.shape[0], -1])\n    return joints2D",
  "def compute_optimized_pose3D(keypoints3D, joint_translation,\n                             camera_intrinsics):\n    \"\"\"Compute the optimized 3D pose\n\n    # Arguments\n        keypoints3D: 3D keypoints\n        joint_translation: np array joints translation\n        camera_intrinsics: camera intrinsics parameters\n\n    # Returns\n        optimized_poses3D: np array of optimized posed3D\n    \"\"\"\n    optimized_pose3D = []\n    projected_pose2D = []\n    for person in range(keypoints3D.shape[0]):\n        translated_pose = keypoints3D[person] + joint_translation[person]\n        rotation = np.identity(3)\n        translation = np.zeros((3,))\n        translated_pose = translated_pose.reshape((-1, 3))\n        points = project_to_image(rotation, translation, translated_pose,\n                                  camera_intrinsics)\n        optimized_pose3D.append(translated_pose)\n        projected_pose2D.append(np.reshape(points, [1, 64]))\n    return np.array(optimized_pose3D), np.array(projected_pose2D)",
  "def human_pose3D_to_pose6D(poses3D):\n    \"\"\"\n    Estiate human pose 6D of the root joint from 3D pose of human joints.\n\n    # Arguments\n    poses3D: numpy array \n             3D pose of human joint\n\n    # return\n    rotation_matrix: numpy array\n                     rotation of human root joint\n    translation: list\n                 translation of human root joint\n    \"\"\"\n    right_hip = poses3D[1]\n    left_hip = poses3D[6]\n    thorax = poses3D[13]\n\n    # Calculate x, y, and z vectors\n    x_vector = right_hip - left_hip\n    projection_vector = thorax - left_hip\n\n    # Calculate projection of projection_vector onto x_vector\n    scalar_projection = np.dot(x_vector, projection_vector)\n    scalar_projection = scalar_projection / np.linalg.norm(x_vector) ** 2\n    projected_point = left_hip + scalar_projection * x_vector\n    z_vector = thorax - projected_point\n\n    # Normalize vectors\n    x_unit_vector = x_vector / np.linalg.norm(x_vector)\n    z_unit_vector = z_vector / np.linalg.norm(z_vector)\n    y_unit_vector = np.cross(z_unit_vector, x_unit_vector)\n\n    # Create rotation matrix\n    rotation_matrix = np.column_stack((x_unit_vector, y_unit_vector,\n                                       z_unit_vector))\n\n    # Convert translation units and return\n    translation = (poses3D[0] / 1e3).tolist()  # Convert mm to meters\n    return rotation_matrix, translation",
  "def to_center_form(boxes):\n    \"\"\"Transform from corner coordinates to center coordinates.\n\n    # Arguments\n        boxes: Numpy array with shape `(num_boxes, 4)`.\n\n    # Returns\n        Numpy array with shape `(num_boxes, 4)`.\n    \"\"\"\n    x_min, y_min = boxes[:, 0:1], boxes[:, 1:2]\n    x_max, y_max = boxes[:, 2:3], boxes[:, 3:4]\n    center_x = (x_max + x_min) / 2.0\n    center_y = (y_max + y_min) / 2.0\n    W = x_max - x_min\n    H = y_max - y_min\n    return np.concatenate([center_x, center_y, W, H], axis=1)",
  "def to_corner_form(boxes):\n    \"\"\"Transform from center coordinates to corner coordinates.\n\n    # Arguments\n        boxes: Numpy array with shape `(num_boxes, 4)`.\n\n    # Returns\n        Numpy array with shape `(num_boxes, 4)`.\n    \"\"\"\n    center_x, center_y = boxes[:, 0:1], boxes[:, 1:2]\n    W, H = boxes[:, 2:3], boxes[:, 3:4]\n    x_min = center_x - (W / 2.0)\n    x_max = center_x + (W / 2.0)\n    y_min = center_y - (H / 2.0)\n    y_max = center_y + (H / 2.0)\n    return np.concatenate([x_min, y_min, x_max, y_max], axis=1)",
  "def encode(matched, priors, variances=[0.1, 0.1, 0.2, 0.2]):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth\n    boxes we have matched (based on jaccard overlap) with the prior boxes.\n\n    # Arguments\n        matched: Numpy array of shape `(num_priors, 4)` with boxes in\n            point-form.\n        priors: Numpy array of shape `(num_priors, 4)` with boxes in\n            center-form.\n        variances: (list[float]) Variances of priorboxes\n\n    # Returns\n        encoded boxes: Numpy array of shape `(num_priors, 4)`.\n    \"\"\"\n    boxes = matched[:, :4]\n    boxes = to_center_form(boxes)\n    center_difference_x = boxes[:, 0:1] - priors[:, 0:1]\n    encoded_center_x = center_difference_x / priors[:, 2:3]\n    center_difference_y = boxes[:, 1:2] - priors[:, 1:2]\n    encoded_center_y = center_difference_y / priors[:, 3:4]\n    encoded_center_x = encoded_center_x / variances[0]\n    encoded_center_y = encoded_center_y / variances[1]\n    encoded_W = np.log((boxes[:, 2:3] / priors[:, 2:3]) + 1e-8)\n    encoded_H = np.log((boxes[:, 3:4] / priors[:, 3:4]) + 1e-8)\n    encoded_W = encoded_W / variances[2]\n    encoded_H = encoded_H / variances[3]\n    encoded_boxes = [encoded_center_x, encoded_center_y, encoded_W, encoded_H]\n    return np.concatenate(encoded_boxes + [matched[:, 4:]], axis=1)",
  "def decode(predictions, priors, variances=[0.1, 0.1, 0.2, 0.2]):\n    \"\"\"Decode default boxes into the ground truth boxes\n\n    # Arguments\n        loc: Numpy array of shape `(num_priors, 4)`.\n        priors: Numpy array of shape `(num_priors, 4)`.\n        variances: List of two floats. Variances of prior boxes.\n\n    # Returns\n        decoded boxes: Numpy array of shape `(num_priors, 4)`.\n    \"\"\"\n    center_x = predictions[:, 0:1] * priors[:, 2:3] * variances[0]\n    center_x = center_x + priors[:, 0:1]\n    center_y = predictions[:, 1:2] * priors[:, 3:4] * variances[1]\n    center_y = center_y + priors[:, 1:2]\n    W = priors[:, 2:3] * np.exp(predictions[:, 2:3] * variances[2])\n    H = priors[:, 3:4] * np.exp(predictions[:, 3:4] * variances[3])\n    boxes = np.concatenate([center_x, center_y, W, H], axis=1)\n    boxes = to_corner_form(boxes)\n    return np.concatenate([boxes, predictions[:, 4:]], 1)",
  "def compute_ious(boxes_A, boxes_B):\n    \"\"\"Calculates the intersection over union between `boxes_A` and `boxes_B`.\n    For each box present in the rows of `boxes_A` it calculates\n    the intersection over union with respect to all boxes in `boxes_B`.\n    The variables `boxes_A` and `boxes_B` contain the corner coordinates\n    of the left-top corner `(x_min, y_min)` and the right-bottom\n    `(x_max, y_max)` corner.\n\n    # Arguments\n        boxes_A: Numpy array with shape `(num_boxes_A, 4)`.\n        boxes_B: Numpy array with shape `(num_boxes_B, 4)`.\n\n    # Returns\n        Numpy array of shape `(num_boxes_A, num_boxes_B)`.\n    \"\"\"\n    xy_min = np.maximum(boxes_A[:, None, 0:2], boxes_B[:, 0:2])\n    xy_max = np.minimum(boxes_A[:, None, 2:4], boxes_B[:, 2:4])\n    intersection = np.maximum(0.0, xy_max - xy_min)\n    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n    areas_A = (boxes_A[:, 2] - boxes_A[:, 0]) * (boxes_A[:, 3] - boxes_A[:, 1])\n    areas_B = (boxes_B[:, 2] - boxes_B[:, 0]) * (boxes_B[:, 3] - boxes_B[:, 1])\n    # broadcasting for outer sum i.e. a sum of all possible combinations\n    union_area = (areas_A[:, np.newaxis] + areas_B) - intersection_area\n    union_area = np.maximum(union_area, 1e-8)\n    return np.clip(intersection_area / union_area, 0.0, 1.0)",
  "def compute_max_matches(boxes, prior_boxes):\n    iou_matrix = compute_ious(prior_boxes, boxes)\n    per_prior_which_box_iou = np.max(iou_matrix, axis=1)\n    per_prior_which_box_arg = np.argmax(iou_matrix, axis=1)\n    return per_prior_which_box_iou, per_prior_which_box_arg",
  "def get_matches_masks(boxes, prior_boxes, positive_iou=0.5, negative_iou=0.4):\n    prior_boxes = to_corner_form(prior_boxes)\n    max_matches = compute_max_matches(boxes, prior_boxes)\n    per_prior_which_box_iou, per_prior_which_box_arg = max_matches\n    positive_mask = np.greater_equal(per_prior_which_box_iou, positive_iou)\n    negative_mask = np.less(per_prior_which_box_iou, negative_iou)\n    not_ignoring_mask = np.logical_or(positive_mask, negative_mask)\n    # ignoring mask are all masks not positive or negative\n    ignoring_mask = np.logical_not(not_ignoring_mask)\n    return per_prior_which_box_arg, positive_mask, ignoring_mask",
  "def mask_classes(boxes, positive_mask, ignoring_mask):\n    class_indices = boxes[:, 4]\n    negative_mask = np.not_equal(positive_mask, 1.0)\n    class_indices = np.where(negative_mask, 0.0, class_indices)\n    # ignoring_mask = np.equal(ignoring_mask, 1.0)\n    # class_indices = np.where(ignoring_mask, -1.0, class_indices)\n    class_indices = np.expand_dims(class_indices, axis=-1)\n    boxes[:, 4:5] = class_indices\n    return boxes",
  "def match_beta(boxes, prior_boxes, positive_iou=0.5, negative_iou=0.0):\n    \"\"\"Matches each prior box with a ground truth box (box from `boxes`).\n    It then selects which matched box will be considered positive e.g. iou > .5\n    and returns for each prior box a ground truth box that is either positive\n    (with a class argument different than 0) or negative.\n\n    # Arguments\n        boxes: Numpy array of shape `(num_ground_truh_boxes, 4 + 1)`,\n            where the first the first four coordinates correspond to\n            box coordinates and the last coordinates is the class\n            argument. This boxes should be the ground truth boxes.\n        prior_boxes: Numpy array of shape `(num_prior_boxes, 4)`.\n            where the four coordinates are in center form coordinates.\n        positive_iou: Float between [0, 1]. Intersection over union\n            used to determine which box is considered a positive box.\n        negative_iou: Float between [0, 1]. Intersection over union\n            used to determine which box is considered a negative box.\n\n    # Returns\n        numpy array of shape `(num_prior_boxes, 4 + 1)`.\n            where the first the first four coordinates correspond to point\n            form box coordinates and the last coordinates is the class\n            argument.\n    \"\"\"\n    matches = get_matches_masks(boxes, prior_boxes, positive_iou, negative_iou)\n    per_prior_box_which_box_arg, positive_mask, ignoring_mask = matches\n    matched_boxes = np.take(boxes, per_prior_box_which_box_arg, axis=0)\n    matched_boxes = mask_classes(matched_boxes, positive_mask, ignoring_mask)\n    return matched_boxes",
  "def match(boxes, prior_boxes, iou_threshold=0.5):\n    \"\"\"Matches each prior box with a ground truth box (box from `boxes`).\n    It then selects which matched box will be considered positive e.g. iou > .5\n    and returns for each prior box a ground truth box that is either positive\n    (with a class argument different than 0) or negative.\n\n    # Arguments\n        boxes: Numpy array of shape `(num_ground_truh_boxes, 4 + 1)`,\n            where the first the first four coordinates correspond to\n            box coordinates and the last coordinates is the class\n            argument. This boxes should be the ground truth boxes.\n        prior_boxes: Numpy array of shape `(num_prior_boxes, 4)`.\n            where the four coordinates are in center form coordinates.\n        iou_threshold: Float between [0, 1]. Intersection over union\n            used to determine which box is considered a positive box.\n\n    # Returns\n        numpy array of shape `(num_prior_boxes, 4 + 1)`.\n            where the first the first four coordinates correspond to point\n            form box coordinates and the last coordinates is the class\n            argument.\n    \"\"\"\n    ious = compute_ious(boxes, to_corner_form(np.float32(prior_boxes)))\n    per_prior_which_box_iou = np.max(ious, axis=0)\n    per_prior_which_box_arg = np.argmax(ious, 0)\n\n    #  overwriting per_prior_which_box_arg if they are the best prior box\n    per_box_which_prior_arg = np.argmax(ious, 1)\n    per_prior_which_box_iou[per_box_which_prior_arg] = 2\n    for box_arg in range(len(per_box_which_prior_arg)):\n        best_prior_box_arg = per_box_which_prior_arg[box_arg]\n        per_prior_which_box_arg[best_prior_box_arg] = box_arg\n\n    matches = boxes[per_prior_which_box_arg]\n    matches[per_prior_which_box_iou < iou_threshold, 4] = 0\n    return matches",
  "def compute_iou(box, boxes):\n    \"\"\"Calculates the intersection over union between 'box' and all 'boxes'.\n    Both `box` and `boxes` are in corner coordinates.\n\n    # Arguments\n        box: Numpy array with length at least of 4.\n        boxes: Numpy array with shape `(num_boxes, 4)`.\n\n    # Returns\n        Numpy array of shape `(num_boxes, 1)`.\n    \"\"\"\n\n    x_min_A, y_min_A, x_max_A, y_max_A = box[:4]\n    x_min_B, y_min_B = boxes[:, 0], boxes[:, 1]\n    x_max_B, y_max_B = boxes[:, 2], boxes[:, 3]\n    # calculating the intersection\n    inner_x_min = np.maximum(x_min_B, x_min_A)\n    inner_y_min = np.maximum(y_min_B, y_min_A)\n    inner_x_max = np.minimum(x_max_B, x_max_A)\n    inner_y_max = np.minimum(y_max_B, y_max_A)\n    inner_w = np.maximum((inner_x_max - inner_x_min), 0)\n    inner_h = np.maximum((inner_y_max - inner_y_min), 0)\n    intersection_area = inner_w * inner_h\n    # calculating the union\n    box_area_B = (x_max_B - x_min_B) * (y_max_B - y_min_B)\n    box_area_A = (x_max_A - x_min_A) * (y_max_A - y_min_A)\n    union_area = box_area_A + box_area_B - intersection_area\n    intersection_over_union = intersection_area / union_area\n    return intersection_over_union",
  "def apply_non_max_suppression(boxes, scores, iou_thresh=.45, top_k=200):\n    \"\"\"Apply non maximum suppression.\n\n    # Arguments\n        boxes: Numpy array, box coordinates of shape `(num_boxes, 4)`\n            where each columns corresponds to x_min, y_min, x_max, y_max.\n        scores: Numpy array, of scores given for each box in `boxes`.\n        iou_thresh: float, intersection over union threshold for removing\n            boxes.\n        top_k: int, number of maximum objects per class.\n\n    # Returns\n        selected_indices: Numpy array, selected indices of kept boxes.\n        num_selected_boxes: int, number of selected boxes.\n    \"\"\"\n\n    selected_indices = np.zeros(shape=len(scores))\n    if boxes is None or len(boxes) == 0:\n        return selected_indices\n    x_min = boxes[:, 0]\n    y_min = boxes[:, 1]\n    x_max = boxes[:, 2]\n    y_max = boxes[:, 3]\n    areas = (x_max - x_min) * (y_max - y_min)\n    remaining_sorted_box_indices = np.argsort(scores)\n    remaining_sorted_box_indices = remaining_sorted_box_indices[-top_k:]\n\n    num_selected_boxes = 0\n    while len(remaining_sorted_box_indices) > 0:\n        best_score_args = remaining_sorted_box_indices[-1]\n        selected_indices[num_selected_boxes] = best_score_args\n        num_selected_boxes = num_selected_boxes + 1\n        if len(remaining_sorted_box_indices) == 1:\n            break\n\n        remaining_sorted_box_indices = remaining_sorted_box_indices[:-1]\n\n        best_x_min = x_min[best_score_args]\n        best_y_min = y_min[best_score_args]\n        best_x_max = x_max[best_score_args]\n        best_y_max = y_max[best_score_args]\n\n        remaining_x_min = x_min[remaining_sorted_box_indices]\n        remaining_y_min = y_min[remaining_sorted_box_indices]\n        remaining_x_max = x_max[remaining_sorted_box_indices]\n        remaining_y_max = y_max[remaining_sorted_box_indices]\n\n        inner_x_min = np.maximum(remaining_x_min, best_x_min)\n        inner_y_min = np.maximum(remaining_y_min, best_y_min)\n        inner_x_max = np.minimum(remaining_x_max, best_x_max)\n        inner_y_max = np.minimum(remaining_y_max, best_y_max)\n\n        inner_box_widths = inner_x_max - inner_x_min\n        inner_box_heights = inner_y_max - inner_y_min\n\n        inner_box_widths = np.maximum(inner_box_widths, 0.0)\n        inner_box_heights = np.maximum(inner_box_heights, 0.0)\n\n        intersections = inner_box_widths * inner_box_heights\n        remaining_box_areas = areas[remaining_sorted_box_indices]\n        best_area = areas[best_score_args]\n        unions = remaining_box_areas + best_area - intersections\n        intersec_over_union = intersections / unions\n        intersec_over_union_mask = intersec_over_union <= iou_thresh\n        remaining_sorted_box_indices = remaining_sorted_box_indices[\n            intersec_over_union_mask]\n\n    return selected_indices.astype(int), num_selected_boxes",
  "def nms_per_class(box_data, nms_thresh=.45, epsilon=0.01, top_k=200):\n    \"\"\"Applies non maximum suppression per class.\n    This function takes all the detections from the detector which\n    consists of boxes and their corresponding class scores to which it\n    applies non maximum suppression for every class independently and\n    then combines the result.\n\n    # Arguments\n        box_data: Array of shape `(num_nms_boxes, 4 + num_classes)`\n            containing the box coordinates as well as the predicted\n            scores of all the classes for all non suppressed boxes.\n        nms_thresh: Float, Non-maximum suppression threshold.\n        epsilon: Float, Filter scores with a lower confidence\n            value before performing non-maximum supression.\n        top_k: Int, Maximum number of boxes per class outputted by nms.\n\n    # Returns\n        Tuple: Containing an array non suppressed boxes of shape\n            `(num_nms_boxes, 4 + num_classes)` and an array\n            of corresponding class labels of shape `(num_nms_boxes, )`.\n    \"\"\"\n    decoded_boxes = box_data[:, :4]\n    class_predictions = box_data[:, 4:]\n    num_classes = class_predictions.shape[1]\n    nms_boxes = np.array([], dtype=float).reshape(0, box_data.shape[1])\n    class_labels = np.array([], dtype=int)\n    args = (decoded_boxes, class_predictions, epsilon, nms_thresh, top_k)\n    for class_arg in range(num_classes):\n        nms_boxes, class_labels = _nms_per_class(\n            nms_boxes, class_labels, class_arg, *args)\n    return nms_boxes, class_labels",
  "def _nms_per_class(nms_boxes, class_labels, class_arg, decoded_boxes,\n                   class_predictions, epsilon, nms_thresh, top_k):\n    \"\"\"Applies non maximum suppression for a given class.\n    This function takes all the detections that belong only to the given\n    single class and applies non maximum suppression for that class\n    alone and returns the resulting non suppressed boxes.\n\n    # Arguments\n        nms_boxes: Array of shape `(num_boxes, 4 + num_classes)`.\n        class_labels: Array of shape `(num_boxes, )`.\n        class_arg: Int, class index.\n        decoded_boxes: Array of shape `(num_prior_boxes, 4)`\n            containing the box coordinates of all the\n            non suppressed boxes.\n        class_predictions: Array of shape\n            `(num_nms_boxes, num_classes)` containing the predicted\n            scores of all the classes for all the non suppressed boxes.\n        epsilon: Float, Filter scores with a lower confidence\n            value before performing non-maximum supression.\n        nms_thresh: Float, Non-maximum suppression threshold.\n        top_k: Int, Maximum number of boxes per class outputted by nms.\n\n    # Returns\n        Tuple: Containing an array non suppressed boxes per class of\n            shape `(num_nms_boxes_per_class, 4 + num_classes) and an\n            array corresponding class labels of shape\n            `(num_nms_boxes_per_class, )`.\n    \"\"\"\n    scores, mask = pre_filter_nms(class_arg, class_predictions, epsilon)\n\n    if len(scores) != 0:\n        boxes = decoded_boxes[mask]\n        selected = apply_non_max_suppression(boxes, scores, nms_thresh, top_k)\n        indices, count = selected\n        selected_indices = indices[:count]\n        selected_boxes = boxes[selected_indices]\n        selected_classes = class_predictions[mask][selected_indices]\n        selections = np.concatenate((selected_boxes, selected_classes), axis=1)\n        nms_boxes = np.concatenate((nms_boxes, selections), axis=0)\n        class_label = np.repeat(class_arg, count)\n        class_labels = np.append(class_labels, class_label)\n    return nms_boxes, class_labels",
  "def pre_filter_nms(class_arg, class_predictions, epsilon):\n    \"\"\"Applies score filtering.\n    This function takes all the predicted scores of a given class and\n    filters out all the predictions less than the given `epsilon` value.\n\n    # Arguments\n        class_arg: Int, class index.\n        class_predictions: Array of shape\n            `(num_nms_boxes, num_classes)` containing the predicted\n            scores of all the classes for all the non suppressed boxes.\n        epsilon: Float, threshold value for score filtering.\n\n    # Returns\n        Tuple: Containing an array filtered scores of shape\n            `(num_pre_filtered_boxes, )` and an array filter mask of\n            shape `(num_prior_boxes, )`.\n    \"\"\"\n    mask = class_predictions[:, class_arg] >= epsilon\n    scores = class_predictions[:, class_arg][mask]\n    return scores, mask",
  "def merge_nms_box_with_class(box_data, class_labels):\n    \"\"\"Merges box coordinates with their corresponding class\n    defined by `class_labels` which is decided by best box geometry\n    by non maximum suppression (and not by the best scoring class)\n    into a single output.\n    This function retains only the predicted score of the class to\n    which the box belongs to and sets the scores of all the remaining\n    classes to zero, thereby combining box and class information in a\n    single variable.\n\n    # Arguments\n        box_data: Array of shape `(num_nms_boxes, 4 + num_classes)`\n            containing the box coordinates as well as the predicted\n            scores of all the classes for all non suppressed boxes.\n        class_labels: Array of shape `(num_nms_boxes, )` that contains\n            the indices of the class whose score is to be retained.\n\n    # Returns\n        boxes: Array of shape `(num_nms_boxes, 4 + num_classes)`,\n            containing coordinates of non supressed boxes along with\n            scores of the class to which the box belongs. The scores of\n            the other classes are zeros.\n    \"\"\"\n    decoded_boxes = box_data[:, :4]\n    class_predictions = box_data[:, 4:]\n    retained_class_score = suppress_other_class_scores(\n        class_predictions, class_labels)\n    box_data = np.concatenate((decoded_boxes, retained_class_score), axis=1)\n    return box_data",
  "def suppress_other_class_scores(class_predictions, class_labels):\n    \"\"\"Retains the score of class in `class_labels` and\n    sets other class scores to zero.\n\n    # Arguments\n        class_predictions: Array of shape\n            `(num_nms_boxes, num_classes)` containing the predicted\n            scores of all the classes for all the non suppressed boxes.\n        class_labels: Array of shape `(num_nms_boxes, )` that contains\n            the indices of the class whose score is to be retained.\n\n    # Returns\n        retained_class_score: Array of shape\n            `(num_nms_boxes, num_classes)` that consists of score at\n            only those location specified by 'class_labels' and zero\n            at other class locations.\n\n    # Note\n        This approach retains the scores of that class in\n        `class_predictions` defined by `class_labels` by generating\n        a boolean mask `score_suppress_mask` with elements True at the\n        locations where the score in `class_predictions` is to be\n        retained and False wherever the class score is to be suppressed.\n        This approach of retaining/suppressing scores does not make use\n        of for loop, if-else condition and direct value assignment\n        to arrays.\n    \"\"\"\n    num_nms_boxes, num_classes = class_predictions.shape\n    class_indices = np.arange(num_classes)\n    class_indices = np.expand_dims(class_indices, axis=0)\n    class_indices = np.repeat(class_indices, num_nms_boxes, axis=0)\n    class_labels = np.expand_dims(class_labels, axis=1)\n    class_labels = np.repeat(class_labels, num_classes, axis=1)\n    \"\"\"\n    The difference of class_indices and class_labels contains zero\n    at those locations of the result where the score is to be retained\n    whose boolean value is False while others being True. This\n    difference obtained as a boolean array gives a negative mask which\n    when inverted gives the score_suppress_mask.\n    \"\"\"\n    negative_mask = np.array(class_indices - class_labels, dtype=bool)\n    score_suppress_mask = np.logical_not(negative_mask)\n    retained_class_score = np.multiply(class_predictions, score_suppress_mask)\n    return retained_class_score",
  "def to_one_hot(class_indices, num_classes):\n    \"\"\" Transform from class index to one-hot encoded vector.\n\n    # Arguments\n        class_indices: Numpy array. One dimensional array specifying\n            the index argument of the class for each sample.\n        num_classes: Integer. Total number of classes.\n\n    # Returns\n        Numpy array with shape `(num_samples, num_classes)`.\n    \"\"\"\n    one_hot_vectors = np.zeros((len(class_indices), num_classes))\n    for vector_arg, class_args in enumerate(class_indices):\n        one_hot_vectors[vector_arg, class_args] = 1.0\n    return one_hot_vectors",
  "def make_box_square(box):\n    \"\"\"Makes box coordinates square with sides equal to the longest\n        original side.\n\n    # Arguments\n        box: Numpy array with shape `(4)` with point corner coordinates.\n\n    # Returns\n        returns: List of box coordinates ints.\n    \"\"\"\n    # TODO add ``calculate_center`` ``calculate_side_dimensions`` functions.\n    x_min, y_min, x_max, y_max = box[:4]\n    center_x = (x_max + x_min) / 2.0\n    center_y = (y_max + y_min) / 2.0\n    width = x_max - x_min\n    height = y_max - y_min\n\n    if height >= width:\n        half_box = height / 2.0\n        x_min = int(center_x - half_box)\n        x_max = int(center_x + half_box)\n\n    if width > height:\n        half_box = width / 2.0\n        y_min = int(center_y - half_box)\n        y_max = int(center_y + half_box)\n\n    return x_min, y_min, x_max, y_max",
  "def offset(coordinates, offset_scales):\n    \"\"\"Apply offsets to box coordinates\n\n    # Arguments\n        coordinates: List of floats containing coordinates in point form.\n        offset_scales: List of floats having x and y scales respectively.\n\n    # Returns\n        coordinates: List of floats containing coordinates in point form.\n            i.e. [x_min, y_min, x_max, y_max].\n    \"\"\"\n    x_min, y_min, x_max, y_max = coordinates\n    x_offset_scale, y_offset_scale = offset_scales\n    x_offset = (x_max - x_min) * x_offset_scale\n    y_offset = (y_max - y_min) * y_offset_scale\n    x_min = int(x_min - x_offset)\n    y_max = int(y_max + x_offset)\n    y_min = int(y_min - y_offset)\n    x_max = int(x_max + y_offset)\n    return (x_min, y_min, x_max, y_max)",
  "def clip(coordinates, image_shape):\n    \"\"\"Clip box to valid image coordinates\n    # Arguments\n        coordinates: List of floats containing coordinates in point form\n            i.e. [x_min, y_min, x_max, y_max].\n        image_shape: List of two integers indicating height and width of image\n            respectively.\n\n    # Returns\n        List of clipped coordinates.\n    \"\"\"\n    height, width = image_shape[:2]\n    x_min, y_min, x_max, y_max = coordinates\n    if x_min < 0:\n        x_min = 0\n    if y_min < 0:\n        y_min = 0\n    if x_max > width:\n        x_max = width\n    if y_max > height:\n        y_max = height\n    return x_min, y_min, x_max, y_max",
  "def denormalize_box(box, image_shape):\n    \"\"\"Scales corner box coordinates from normalized values to image dimensions\n\n    # Arguments\n        box: Numpy array containing corner box coordinates.\n        image_shape: List of integers with (height, width).\n\n    # Returns\n        returns: box corner coordinates in image dimensions\n    \"\"\"\n    x_min, y_min, x_max, y_max = box[:4]\n    height, width = image_shape\n    x_min = int(x_min * width)\n    y_min = int(y_min * height)\n    x_max = int(x_max * width)\n    y_max = int(y_max * height)\n    return (x_min, y_min, x_max, y_max)",
  "def flip_left_right(boxes, width):\n    \"\"\"Flips box coordinates from left-to-right and vice-versa.\n    # Arguments\n        boxes: Numpy array of shape `[num_boxes, 4]`.\n    # Returns\n        Numpy array of shape `[num_boxes, 4]`.\n    \"\"\"\n    flipped_boxes = boxes.copy()\n    flipped_boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n    return flipped_boxes",
  "def to_image_coordinates(boxes, image):\n    \"\"\"Transforms normalized box coordinates into image coordinates.\n    # Arguments\n        image: Numpy array.\n        boxes: Numpy array of shape `[num_boxes, N]` where N >= 4.\n    # Returns\n        Numpy array of shape `[num_boxes, N]`.\n    \"\"\"\n    height, width = image.shape[:2]\n    image_boxes = boxes.copy()\n    image_boxes[:, 0] = boxes[:, 0] * width\n    image_boxes[:, 2] = boxes[:, 2] * width\n    image_boxes[:, 1] = boxes[:, 1] * height\n    image_boxes[:, 3] = boxes[:, 3] * height\n    return image_boxes",
  "def to_normalized_coordinates(boxes, image):\n    \"\"\"Transforms coordinates in image dimensions to normalized coordinates.\n    # Arguments\n        image: Numpy array.\n        boxes: Numpy array of shape `[num_boxes, N]` where N >= 4.\n    # Returns\n        Numpy array of shape `[num_boxes, N]`.\n    \"\"\"\n    height, width = image.shape[:2]\n    normalized_boxes = boxes.copy()\n    normalized_boxes[:, 0] = boxes[:, 0] / width\n    normalized_boxes[:, 2] = boxes[:, 2] / width\n    normalized_boxes[:, 1] = boxes[:, 1] / height\n    normalized_boxes[:, 3] = boxes[:, 3] / height\n    return normalized_boxes",
  "def extract_bounding_box_corners(points3D):\n    \"\"\"Extracts the (x_min, y_min, z_min) and the (x_max, y_max, z_max)\n        coordinates from an array of  points3D\n    # Arguments\n        points3D: Array (num_points, 3)\n\n    # Returns\n        Left-down-bottom corner (x_min, y_min, z_min) and right-up-top\n            (x_max, y_max, z_max) corner.\n    \"\"\"\n    XYZ_min = np.min(points3D, axis=0)\n    XYZ_max = np.max(points3D, axis=0)\n    return XYZ_min, XYZ_max",
  "def filter_boxes(boxes, conf_thresh):\n    \"\"\"Filters given boxes based on scores.\n\n    # Arguments\n        boxes: Array of shape `(num_nms_boxes, 4 + num_classes)`.\n        conf_thresh: Float, Filter boxes with a confidence value\n            lower than this.\n\n    # Returns\n        confident_boxes: Array of shape\n            `(num_filtered_boxes, 4 + num_classes)`.\n    \"\"\"\n    class_predictions = boxes[:, 4:]\n    class_scores = np.max(class_predictions, axis=1)\n    confidence_mask = class_scores >= conf_thresh\n    confident_boxes = boxes[confidence_mask]\n    return confident_boxes",
  "def scale_box(predictions, image_scales):\n    \"\"\"\n    # Arguments\n        predictions: Array of shape `(num_boxes, num_classes+N)`\n            model predictions.\n        image_scales: Array of shape `()`, scale value of boxes.\n\n    # Returns\n        predictions: Array of shape `(num_boxes, num_classes+N)`\n            model predictions.\n    \"\"\"\n    boxes = predictions[:, :4]\n    scales = image_scales[np.newaxis][np.newaxis]\n    boxes = boxes * scales\n    predictions = np.concatenate([boxes, predictions[:, 4:]], 1)\n    return predictions",
  "def change_box_coordinates(outputs):\n    \"\"\"Converts box coordinates format from (y_min, x_min, y_max, x_max)\n    to (x_min, y_min, x_max, y_max).\n\n    # Arguments\n        outputs: Tensor, model output.\n\n    # Returns\n        outputs: Array, Processed outputs by merging the features\n            at all levels. Each row corresponds to box coordinate\n            offsets and sigmoid of the class logits.\n    \"\"\"\n    outputs = outputs[0]\n    boxes, classes = outputs[:, :4], outputs[:, 4:]\n    s1, s2, s3, s4 = np.hsplit(boxes, 4)\n    boxes = np.concatenate([s2, s1, s4, s3], axis=1)\n    boxes = boxes[np.newaxis]\n    classes = classes[np.newaxis]\n    outputs = np.concatenate([boxes, classes], axis=2)\n    return outputs",
  "def get_keypoints_heatmap(heatmaps, num_keypoints, indices=None, axis=1):\n    \"\"\"Extract the heatmaps that only contains the keypoints.\n\n    # Arguments\n        heatmaps: Numpy array of shape (1, 2*num_keypoints, H, W)\n        num_keypoints: Int.\n        indices: List. Indices of the heatmaps to extract.\n        axis: Int.\n\n    # Returns\n        keypoints: Numpy array of shape (1, num_keypoints, H, W)\n    \"\"\"\n    keypoints = np.take(heatmaps, np.arange(num_keypoints), axis)\n    if indices is not None:\n        keypoints = np.take(keypoints, indices, axis)\n    return keypoints",
  "def get_tags_heatmap(heatmaps, num_keypoints, indices=None, axis=1):\n    \"\"\"Extract the heatmaps that only contains the tags.\n\n    # Arguments\n        heatmaps: Numpy array of shape (1, 2*num_keypoints, H, W)\n        num_keypoints: Int.\n        indices: List. Indices of the heatmaps to extract.\n        axis: Int.\n\n    # Returns\n        tags: Numpy array of shape (1, num_keypoints, H, W)\n    \"\"\"\n    n = heatmaps.shape[axis]\n    tags = np.take(heatmaps, np.arange(num_keypoints, n), axis)\n    if indices is not None:\n        tags = np.take(tags, indices, axis)\n    return tags",
  "def get_keypoints_locations(indices, image_width):\n    \"\"\"Calculate the location of keypoints in an image.\n\n    # Arguments\n        indices: Numpy array. Indices of the keypoints in the heatmap.\n        Image width: Int.\n\n    # Returns\n        coordinate: Numpy array. locations of keypoints\n    \"\"\"\n    x = (indices % image_width).astype(np.int64)\n    y = (indices / image_width).astype(np.int64)\n    coordinates = np.stack((x, y), axis=3)\n    return np.squeeze(coordinates)",
  "def get_top_k_keypoints_numpy(heatmaps, k):\n    \"\"\"Numpy implementation of get_top_k_keypoints from heatmaps.\n\n    # Arguments\n        heatmaps: Keypoints heatmaps. Numpy array of shape\n                  (1, num_keypoints, H, W)\n        k: Int. Maximum number of instances to return.\n\n    # Returns\n        values: Numpy array. Value of heatmaps at top k keypoints\n        indices: Numpy array. Indices of top k keypoints.\n    \"\"\"\n    num_of_objects, num_of_keypoints = heatmaps.shape[:2]\n    indices = np.zeros((num_of_objects, num_of_keypoints, k), dtype=int)\n    values = np.zeros((num_of_objects, num_of_keypoints, k))\n    for object_arg in range(num_of_objects):\n        for keypoint_arg in range(num_of_keypoints):\n            top_k_indices = np.argsort(heatmaps[object_arg][keypoint_arg])[-k:]\n            top_k_values = heatmaps[object_arg][keypoint_arg][top_k_indices]\n            indices[object_arg][keypoint_arg] = top_k_indices\n            values[object_arg][keypoint_arg] = top_k_values\n    return np.squeeze(values), indices",
  "def get_valid_detections(detection, detection_thresh):\n    \"\"\"Accept the keypoints whose score is greater than the\n       detection threshold.\n\n    # Arguments\n        detection: Numpy array. Contains the location, value, and\n        tags of the keypoints\n        detection_thresh: Float. Detection threshold for the keypoint\n    \"\"\"\n    mask = detection[:, 2] > detection_thresh\n    valid_detection = detection[mask]\n    return valid_detection",
  "def sample_point_in_full_sphere(distance=1.0):\n    \"\"\"Get a point of the top of the unit sphere.\n\n    # Arguments\n        distance: Float, indicating distance to origin.\n\n    # Returns\n        sphere_point: List of spatial coordinates of a sphere.\n    \"\"\"\n    if distance <= 0:\n        raise ValueError('distance should be bigger than 0')\n    sphere_point = np.random.uniform(-1, 1, size=3)\n    return (distance * sphere_point) / np.linalg.norm(sphere_point)",
  "def sample_point_in_top_sphere(distance=1.0):\n    \"\"\"Get a point of the top of the unit sphere.\n\n    # Arguments\n        distance: Float, indicating distance to origin.\n\n    # Returns\n        sphere_point: List of spatial coordinates of a sphere.\n    \"\"\"\n    if distance <= 0:\n        raise ValueError('distance should be bigger than 0')\n    sphere_point = sample_point_in_full_sphere(distance)\n    if sphere_point[1] < 0:\n        sphere_point[1] = sphere_point[1] * -1\n    return sphere_point",
  "def sample_point_in_sphere(distance, top_only=False):\n    \"\"\" Samples random points from a sphere\n\n    # Arguments\n        distance: Float, indicating distance to origin.\n\n    # Returns:\n        List of spatial coordinates of a sphere.\n\n    \"\"\"\n    if distance <= 0:\n        raise ValueError('distance should be bigger than 0')\n    if top_only:\n        sphere_point = sample_point_in_top_sphere(distance)\n    else:\n        sphere_point = sample_point_in_full_sphere(distance)\n    return sphere_point",
  "def random_perturbation(localization, shift):\n    \"\"\"Adds noise to 'localization' vector coordinates.\n\n    # Arguments\n        localization: List of 3 floats.\n        shift: Float indicating a uniform distribution [-shift, shift].\n\n    # Returns\n        perturbed localization\n    \"\"\"\n    perturbation = np.random.uniform(-shift, shift, size=3)\n    return localization + perturbation",
  "def random_translation(localization, shift):\n    \"\"\"Adds noise to 'localization' vector coordinates.\n\n    # Arguments\n        localization: List of 3 floats.\n        shift: Float indicating a uniform distribution [-shift, shift].\n    # Returns\n        perturbed localization\n    \"\"\"\n    perturbation = np.zeros((3))\n    perturbation[:2] = np.random.uniform(-shift, shift, size=2)\n    return localization + perturbation",
  "def get_look_at_transform(camera_position, target_position):\n    \"\"\"Make transformation from target position to camera position\n    with orientation looking at the target position.\n\n    # Arguments\n        camera_position: Numpy-array of length 3. Camera position.\n        target_position: Numpy-array of length 3. Target position.\n    \"\"\"\n    camera_direction = camera_position - target_position\n    camera_direction = camera_direction / np.linalg.norm(camera_direction)\n    world_up = np.array([0.0, 1.0, 0.0])\n    camera_right = np.cross(world_up, camera_direction)\n    camera_right = camera_right / np.linalg.norm(camera_right)\n    camera_up = np.cross(camera_direction, camera_right)\n    camera_up = camera_up / np.linalg.norm(camera_up)\n    rotation_transform = np.zeros((4, 4))\n    rotation_transform[0, :3] = camera_right\n    rotation_transform[1, :3] = camera_up\n    rotation_transform[2, :3] = camera_direction\n    rotation_transform[-1, -1] = 1\n    translation_transform = np.eye(4)\n    translation_transform[:3, -1] = - camera_position\n    look_at_transform = np.matmul(rotation_transform, translation_transform)\n    return look_at_transform",
  "def compute_modelview_matrices(camera_origin, world_origin,\n                               roll=None, translate=None):\n    \"\"\"Compute model-view matrices from camera to origin and origin to camera.\n\n    # Arguments\n        camera_origin: Numpy-array of length 3 determining the camera origin\n        world_origin: Numpy-array of length 3 determining the world origin\n        roll: `None` or float. If `None` no roll is performed. If float\n        value should be between [0, 2*pi)\n\n    # Returns\n        Transformation from camera to world and world to camera.\n    \"\"\"\n    world_to_camera = get_look_at_transform(camera_origin, world_origin)\n    if roll is not None:\n        world_to_camera = roll_camera(world_to_camera, roll)\n    if translate is not None:\n        world_to_camera = translate_camera(world_to_camera, translate)\n    camera_to_world = np.linalg.inv(world_to_camera)\n    return camera_to_world, world_to_camera",
  "def roll_camera(world_to_camera, angle):\n    \"\"\" Roll camera coordinate system.\n\n    # Arguments:\n        world_to_camera: Numpy array containing the affine transformation.\n        max_roll: 'None' or float. If None, the camera is not rolled.\n            If float it should be a value between [0, 2*pi)\n    \"\"\"\n    angle = np.random.uniform(-angle, angle)\n    z_rotation = np.array(\n        [[np.cos(angle), -np.sin(angle), 0.],\n         [np.sin(angle), +np.cos(angle), 0.],\n         [0., 0., 1.]])\n    world_to_camera[:3, :3] = np.matmul(z_rotation, world_to_camera[:3, :3])\n    return world_to_camera",
  "def translate_camera(world_to_camera, translation):\n    \"\"\" Translate camera coordinate system in its XY plane.\n\n    # Arguments:\n        world_to_camera: Numpy array containing the affine transformation.\n        translation: List or array with two inputs.\n    \"\"\"\n    translation = np.random.uniform(-translation, translation, 2)\n    translation_transform = np.array(\n        [[1.0, 0.0, 0.0, translation[0]],\n         [0.0, 1.0, 0.0, translation[1]],\n         [0.0, 0.0, 1.0, 0.0],\n         [0.0, 0.0, 0.0, 1.0]])\n    world_to_camera = np.matmul(translation_transform, world_to_camera)\n    return world_to_camera",
  "def scale_translation(matrix, scale=10.0):\n    \"\"\" Changes the scale of the translation vector.\n    Used for changing the regression problem to a bigger scale.\n\n    # Arguments:\n        matrix: Numpy array of shape [4, 4]\n        scale: Float used to multiple all the translation component.\n\n    # Returns:\n        Numpy array of shape [4, 4]\n    \"\"\"\n    matrix[:3, -1] = matrix[:3, -1] * 10.\n    return matrix",
  "def sample_uniformly(value):\n    \"\"\" Samples from a uniform distribution.\n\n    # Arguments\n        values: List or float. If list it must have [min_value, max_value].\n\n    # Returns\n        Float\n    \"\"\"\n    if isinstance(value, list):\n        value = np.random.uniform(value[0], value[1])\n    return value",
  "def split_alpha_channel(image):\n    \"\"\" Splits alpha channel from an RGBD image.\n\n    # Arguments\n        image: Numpy array of shape [H, W, 4]\n\n    # Returns\n        List of two numpy arrays of shape [H, W, 3] and [H, W]\n    \"\"\"\n    image_shape = image.shape\n    if len(image_shape) != 3:\n        raise ValueError('Invalid image shape')\n    if image_shape[-1] != 4:\n        raise ValueError('Invalid number of channels')\n    return image[..., :3], image[..., 3:4]",
  "def build_anchors(image_shape, branches, num_scales, aspect_ratios, scale):\n    \"\"\"Builds anchor boxes in centre form for given model.\n    Anchor boxes a.k.a prior boxes are reference boxes built with\n    various scales and aspect ratio centered over every pixel in the\n    input image and branch tensors. They can be strided. Anchor boxes\n    define regions of image where objects are likely to be found. They\n    help object detector to accurately localize and classify objects at\n    the same time handling variations in object size and shape.\n\n    # Arguments\n        image_shape: List, input image shape.\n        branches: List, EfficientNet branch tensors.\n        num_scales: Int, number of anchor scales.\n        aspect_ratios: List, anchor box aspect ratios.\n        scale: Float, anchor box scale.\n\n    # Returns\n        anchor_boxes: Array of shape `(num_boxes, 4)`.\n    \"\"\"\n    num_scale_aspect = num_scales * len(aspect_ratios)\n    args = (image_shape, branches, num_scale_aspect)\n    octave = build_octaves(num_scales, aspect_ratios)\n    aspect = build_aspect(num_scales, aspect_ratios)\n    scales = build_scales(scale, num_scale_aspect)\n    anchor_boxes = []\n    for branch_arg in range(len(branches)):\n        stride = build_strides(branch_arg, *args)\n        boxes = make_branch_boxes(*stride, octave, aspect, scales, image_shape)\n        anchor_boxes.append(boxes.reshape([-1, 4]))\n    anchor_boxes = np.concatenate(anchor_boxes, axis=0).astype('float32')\n    return to_center_form(anchor_boxes)",
  "def build_octaves(num_scales, aspect_ratios):\n    \"\"\"Builds branch-wise EfficientNet anchor box octaves.\n    Octaves are values that differ from each other by a multiplicative\n    factor of 2. In case of EfficienDet the scales of anchor box,\n    which are integers raised to the power of 2 are normalized.\n    This makes the values differ from each other by a multiplicative\n    factor of approximately 1.2599. Therefore in this case it is not a\n    perfect octave however is an approximation of octave. The\n    following shows an example visualization of anchor boxes each with\n    same aspect ratio and scale but with different octaves.\n\n    +--------+       +---------------+       +----------------------+\n    |        |       |               |       |                      |\n    |  0.0   |       |               |       |                      |\n    |        |       |     0.33      |       |                      |\n    +--------+       |               |       |         0.67         |\n                     |               |       |                      |\n                     +---------------+       |                      |\n                                             |                      |\n                                             |                      |\n                                             +----------------------+\n\n    # Arguments\n        num_scales: Int, number of anchor scales.\n        aspect_ratios: List, anchor box aspect ratios.\n\n    # Returns\n        octave_normalized: Array of shape `(num_scale_aspect,)`.\n    \"\"\"\n    octave = np.repeat(list(range(num_scales)), len(aspect_ratios))\n    octave_normalized = octave / float(num_scales)\n    return octave_normalized",
  "def build_aspect(num_scales, aspect_ratios):\n    \"\"\"Builds branch-wise EfficientNet anchor box aspect ratios.\n    The aspect ratio of an anchor box refers to the ratio of its width\n    to its height. They define the shape of the object that the object\n    detector is trying to detect. If aspect ratio is 1, the anchor box\n    is a square. If it is greater than 1, the box is wider than it is\n    tall. If it is less than 1, the box is taller than its is wide. The\n    following shows visualization of anchor boxes each with same octave\n    and scale but different aspect ratios.\n\n    +--------+       +---------------+       +--------+\n    |        |       |               |       |        |\n    |  1.0   |       |      2.0      |       |        |\n    |        |       |               |       |  0.5   |\n    +--------+       +---------------+       |        |\n                                             |        |\n                                             |        |\n                                             +--------+\n\n    # Arguments\n        num_scales: Int, number of anchor scales.\n        aspect_ratios: List, anchor box aspect ratios.\n\n    # Returns\n        Array of shape `(num_scale_aspect,)`.\n    \"\"\"\n    return np.tile(aspect_ratios, num_scales)",
  "def build_scales(scale, num_scale_aspect):\n    \"\"\"Builds branch-wise EfficientNet anchor box scales.\n    Anchor box scale refers to the size of the anchor box. The scale of\n    the anchor box determines how large the box is in relation of the\n    object it is trying to detect. If the object detector is trying to\n    detect smaller objects, anchor box with smaller scales may be more\n    effective. If the object detector is trying to detect larger\n    objects, anchor box with larger scales my be more effective. The\n    following shows an example visualization of anchor boxes each with\n    same octave and aspect ratio but with different scales.\n\n    +--------+       +----------------+       +------------------------+\n    |        |       |                |       |                        |\n    |  1.0   |       |                |       |                        |\n    |        |       |                |       |                        |\n    +--------+       |       2.0      |       |                        |\n                     |                |       |                        |\n                     |                |       |           3.0          |\n                     |                |       |                        |\n                     +----------------+       |                        |\n                                              |                        |\n                                              |                        |\n                                              |                        |\n                                              +------------------------+\n\n    # Arguments\n        scale: Float, anchor box scale.\n        num_scale_aspect: Int, number of scale and aspect combinations.\n\n    # Returns\n        Array of shape `(num_scale_aspect,)`.\n    \"\"\"\n    return np.repeat(scale, num_scale_aspect)",
  "def build_strides(branch_arg, image_shape, branches, num_scale_aspect):\n    \"\"\"Builds branch-wise EfficientNet anchor box strides.\n    The stride of an anchor box determines how densely the anchor boxes\n    are placed in the image. A smaller stride means that the anchor\n    boxes are more densely packed and cover a larger area of the image,\n    while a larger stride means that the anchor boxes are less densely\n    packed and cover a smaller area of the image.\n    In general, a smaller stride is more effective at detecting smaller\n    objects, while a larger stride is more effective at detecting larger\n    objects. The optimal stride for a particular object detection system\n    will depend on the sizes of the objects that it is trying to detect\n    and the resolution of the input images. The following shows an\n    example visualization of anchor box's centre marked by + each with\n    same octave and aspect ratio and scale but with different strides.\n\n            8.0                     16.0                     32.0\n    +-----------------+      +-----------------+     +-----------------+\n    | + + + + + + + + |      |                 |     |   +    +    +   |\n    | + + + + + + + + |      |  +  +  +  +  +  |     |                 |\n    | + + + + + + + + |      |                 |     |                 |\n    | + + + + + + + + |      |  +  +  +  +  +  |     |   +    +    +   |\n    | + + + + + + + + |      |                 |     |                 |\n    | + + + + + + + + |      |  +  +  +  +  +  |     |                 |\n    | + + + + + + + + |      |                 |     |   +    +    +   |\n    +-----------------+      +-----------------+     +-----------------+\n\n    # Arguments\n        branch_arg: Int, branch index.\n        image_shape: List, input image shape.\n        branches: List, EfficientNet branch tensors.\n        num_scale_aspect: Int, count of scale aspect ratio combinations.\n\n    # Returns\n        Tuple: Containing strides in y and x direction.\n    \"\"\"\n    H_image, W_image = image_shape\n    feature_H, feature_W = branches[branch_arg].shape[1:3]\n    features_H = np.repeat(feature_H, num_scale_aspect).astype('float32')\n    features_W = np.repeat(feature_W, num_scale_aspect).astype('float32')\n    strides_y = H_image / features_H\n    strides_x = W_image / features_W\n    return strides_y, strides_x",
  "def make_branch_boxes(stride_y, stride_x, octave,\n                      aspect, scales, image_shape):\n    \"\"\"Builds branch-wise EfficientNet anchor boxes.\n\n    # Arguments\n        stride_y: Array of shape `(num_scale_aspect,)` y-axis stride.\n        stride_x: Array of shape `(num_scale_aspect,)` x-axis stride.\n        octave: Array of shape `(num_scale_aspect,)` octave scale.\n        aspect: Array of shape `(num_scale_aspect,)` aspect ratio.\n        scales: Array of shape `(num_scale_aspect,)` anchor box scales.\n        image_shape: List, input image shape.\n\n    # Returns\n        branch_boxes: Array of shape `(num_boxes,num_scale_aspect,4)`.\n    \"\"\"\n    branch_boxes = []\n    for branch_config in zip(stride_y, stride_x, scales, octave, aspect):\n        boxes = compute_box_coordinates(image_shape, *branch_config)\n        branch_boxes.append(np.expand_dims(boxes.T, axis=1))\n    branch_boxes = np.concatenate(branch_boxes, axis=1)\n    return branch_boxes",
  "def compute_box_coordinates(image_shape, stride_y, stride_x,\n                            scale, octave_scale, aspect):\n    \"\"\"Computes anchor box coordinates in corner form.\n\n    # Arguments\n        image_shape: List, input image shape.\n        stride_y: Array of shape `(num_scale_aspect,)` y-axis stride.\n        stride_x: Array of shape `(num_scale_aspect,)` x-axis stride.\n        scale: Array of shape `()`, anchor box scales.\n        octave_scale: Array of shape `()`, anchor box octave scale.\n        aspect: Array of shape `()`, anchor box aspect ratio.\n\n    # Returns\n        Tuple: Box coordinates in corner form.\n    \"\"\"\n    base_anchor = build_base_anchor(stride_y, stride_x, scale, octave_scale)\n    aspect_size = compute_aspect_size(aspect)\n    anchor_half_W, anchor_half_H = compute_anchor_dims(\n        *base_anchor, *aspect_size, image_shape)\n    center_x, center_y = compute_anchor_centres(\n        stride_y, stride_x, image_shape)\n    x_min, y_min = [center_x - anchor_half_W], [center_y - anchor_half_H]\n    x_max, y_max = [center_x + anchor_half_W], [center_y + anchor_half_H]\n    box_coordinates = np.concatenate((x_min, y_min, x_max, y_max), axis=0)\n    return box_coordinates",
  "def build_base_anchor(stride_y, stride_x, scale, octave_scale):\n    \"\"\"Builds base anchor's width and height.\n\n    # Arguments\n        stride_y: Array of shape `(num_scale_aspect,)` y-axis stride.\n        stride_x: Array of shape `(num_scale_aspect,)` x-axis stride.\n        scale: Float, anchor box scale.\n        octave_scale: Array of shape `()`, anchor box octave scale.\n\n    # Returns\n        Tuple: Base anchor width and height.\n    \"\"\"\n    base_anchor_W = scale * stride_x * (2 ** octave_scale)\n    base_anchor_H = scale * stride_y * (2 ** octave_scale)\n    return base_anchor_W, base_anchor_H",
  "def compute_aspect_size(aspect):\n    \"\"\"Computes aspect width and height.\n\n    # Arguments\n        aspect: Array of shape `()`, anchor box aspect ratio.\n\n    # Returns\n        Tuple: Aspect width and height.\n    \"\"\"\n    return np.sqrt(aspect), 1 / np.sqrt(aspect)",
  "def compute_anchor_dims(base_anchor_W, base_anchor_H,\n                        aspect_W, aspect_H, image_shape):\n    \"\"\"Compute anchor's half width and half height.\n\n    # Arguments\n        base_anchor_W: Array of shape (), base anchor width.\n        base_anchor_H: Array of shape (), base anchor height.\n        aspect_W: Array of shape (), aspect width.\n        aspect_H: Array of shape (), aspect height.\n        image_shape: List, input image shape.\n\n    # Returns\n        Tuple: Anchor's half width and height.\n    \"\"\"\n    H, W = image_shape\n    anchor_half_W = (base_anchor_W * aspect_W / 2.0)\n    anchor_half_H = (base_anchor_H * aspect_H / 2.0)\n    anchor_half_W_normalized = anchor_half_W / W\n    anchor_half_H_normalized = anchor_half_H / H\n    return anchor_half_W_normalized, anchor_half_H_normalized",
  "def compute_anchor_centres(stride_y, stride_x, image_shape):\n    \"\"\"Compute anchor centres normalized to image size.\n\n    # Arguments\n        stride_y: Array of shape `(num_scale_aspect,)` y-axis stride.\n        stride_x: Array of shape `(num_scale_aspect,)` x-axis stride.\n        image_shape: List, input image shape.\n\n    # Returns\n        Tuple: Normalized anchor centres.\n    \"\"\"\n    H, W = image_shape\n    x = np.arange(stride_x / 2, W, stride_x)\n    y = np.arange(stride_y / 2, H, stride_y)\n    center_x, center_y = np.meshgrid(x, y)\n    normalized_center_x = center_x.flatten() / W\n    normalized_center_y = center_y.flatten() / H\n    return normalized_center_x, normalized_center_y",
  "def calculate_relative_angle(absolute_rotation, links_origin_transform,\n                             parents=MANOHandJoints.parents):\n    \"\"\"Calculate the realtive joint rotation for the minimal hand joints.\n\n    # Arguments\n        absolute_angles : Array [num_joints, 4].\n        Absolute joint angle rotation for the minimal hand joints in\n        Euler representation.\n\n    # Returns\n        relative_angles: Array [num_joints, 3].\n        Relative joint rotation of the minimal hand joints in compact\n        axis angle representation.\n    \"\"\"\n    relative_angles = np.zeros((len(absolute_rotation), 3))\n    for angle_arg in range(len(absolute_rotation)):\n        rotation = absolute_rotation[angle_arg]\n        transform = to_affine_matrix(rotation, np.array([0, 0, 0]))\n        inverted_transform = np.linalg.inv(transform)\n        parent_arg = parents[angle_arg]\n        if parent_arg is not None:\n            link_transform = links_origin_transform[parent_arg]\n            child_to_parent_transform = np.dot(inverted_transform,\n                                               link_transform)\n            child_to_parent_rotation = child_to_parent_transform[:3, :3]\n            parent_to_child_rotation = np.linalg.inv(child_to_parent_rotation)\n            parent_to_child_rotation = rotation_matrix_to_compact_axis_angle(\n                parent_to_child_rotation)\n            relative_angles[angle_arg] = parent_to_child_rotation\n    return relative_angles",
  "def reorder_relative_angles(relative_angles, root_angle, children,\n                            root_joints=[1, 4, 7, 10, 13]):\n    \"\"\"Reorder the relative angles according to the kinematic chain\n\n    # Arguments\n        relative_angles: Array\n        root_angle: Array. root joint angle for the minimal hand\n        children: List, Indexes of the children in the kinematic chain.\n\n    # Returns\n        angles: Array. Reordered relative angles\n    \"\"\"\n    if root_angle.shape == (3, 3):\n        root_angle = rotation_matrix_to_compact_axis_angle(root_angle)\n    children_angles = relative_angles[children[1:], :]\n    children_angles = np.concatenate(\n        [np.expand_dims(root_angle, 0), children_angles])\n\n    # insest zero relative angle to the root joints\n    angles = np.insert(children_angles, root_joints, np.array([0, 0, 0]), 0)\n    return angles",
  "def change_link_order(joints, config1_labels, config2_labels):\n    \"\"\"Map data from config1_labels to config2_labels.\n\n    # Arguments\n        joints: Array\n        config1_labels: input joint configuration\n        config2_labels: output joint configuration\n\n    # Returns\n        Array: joints maped to the config2_labels\n    \"\"\"\n    mapped_joints = []\n    for joint_arg in range(len(config2_labels)):\n        joint_label = config2_labels[joint_arg]\n        joint_index_in_config1_labels = config1_labels.index(joint_label)\n        joint_in_config1_labels = joints[joint_index_in_config1_labels]\n        mapped_joints.append(joint_in_config1_labels)\n    mapped_joints = np.stack(mapped_joints, 0)\n    return mapped_joints",
  "def is_hand_open(relative_angles, joint_name_to_arg, thresh):\n    \"\"\"Check is the hand is open by calculating relative pip joint angle norm.\n\n       [(theta * ex), (theta * ey), (theta * ez)] = compact axis angle\n       ex, ey, ez = normalized_axis\n                  _______________________________________________\n       norm =    / (theta**2) * [(ex**2) + (ey**2) + (ez**2)]\n               \\/\n\n       => norm is directly proportional to the theta if axis is normalized.\n          If hand is open the relative angle of the pip joint will be less as\n          compared to the closed hand.\n\n    # Arguments\n        relative_angle: Array\n        joint_name_to_arg: Dictionary for the joints\n        thresh: Float. Threshold value for theta\n\n    # Returns\n        Boolean: Hand is open or closed.\n    \"\"\"\n    index_finger_pip_arg = joint_name_to_arg['index_finger_pip']\n    theta_index_pip = np.linalg.norm(relative_angles[index_finger_pip_arg])\n\n    middle_finger_pip_arg = joint_name_to_arg['middle_finger_pip']\n    theta_middle_pip = np.linalg.norm(relative_angles[middle_finger_pip_arg])\n\n    ring_finger_pip_arg = joint_name_to_arg['ring_finger_pip']\n    theta_ring_pip = np.linalg.norm(relative_angles[ring_finger_pip_arg])\n\n    pinky_finger_pip_arg = joint_name_to_arg['pinky_pip']\n    theta_pinky_pip = np.linalg.norm(relative_angles[pinky_finger_pip_arg])\n\n    if theta_index_pip > thresh and theta_middle_pip > thresh and \\\n            theta_ring_pip > thresh and theta_pinky_pip > thresh:\n        return False\n    else:\n        return True",
  "class Camera(object):\n    \"\"\"Camera abstract class.\n    By default this camera uses the openCV functionality.\n    It can be inherited to overwrite methods in case another camera API exists.\n    \"\"\"\n    def __init__(self, device_id=0, name='Camera', intrinsics=None,\n                 distortion=None):\n        # TODO load parameters from camera name. Use ``load`` method.\n        self.device_id = device_id\n        self.name = name\n        self.intrinsics = intrinsics\n        self.distortion = None\n        self._camera = None\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value\n\n    @property\n    def intrinsics(self):\n        return self._intrinsics\n\n    @intrinsics.setter\n    def intrinsics(self, value):\n        if value is None:\n            value = np.zeros((4))\n        self._intrinsics = value\n\n    @property\n    def distortion(self):\n        return self._distortion\n\n    @distortion.setter\n    def distortion(self, distortion):\n        self._distortion = distortion\n\n    def start(self):\n        \"\"\" Starts capturing device\n\n        # Returns\n            Camera object.\n        \"\"\"\n        self._camera = cv2.VideoCapture(self.device_id)\n        if self._camera is None or not self._camera.isOpened():\n            raise ValueError('Unable to open device', self.device_id)\n        return self._camera\n\n    def stop(self):\n        \"\"\" Stops capturing device.\n        \"\"\"\n        return self._camera.release()\n\n    def read(self):\n        \"\"\"Reads camera input and returns a frame.\n\n        # Returns\n            Image array.\n        \"\"\"\n        frame = self._camera.read()[1]\n        return frame\n\n    def is_open(self):\n        \"\"\"Checks if camera is open.\n\n        # Returns\n            Boolean\n        \"\"\"\n        return self._camera.isOpened()\n\n    def calibrate(self):\n        raise NotImplementedError\n\n    def save(self, filepath):\n        raise NotImplementedError\n\n    def load(self, filepath):\n        raise NotImplementedError\n\n    def intrinsics_from_HFOV(self, HFOV=70, image_shape=None):\n        \"\"\"Computes camera intrinsics using horizontal field of view (HFOV).\n\n        # Arguments\n            HFOV: Angle in degrees of horizontal field of view.\n            image_shape: List of two floats [height, width].\n\n        # Returns\n            camera intrinsics array (3, 3).\n\n        # Notes:\n\n                       \\           /      ^\n                        \\         /       |\n                         \\ lens  /        | w/2\n        horizontal field  \\     / alpha/2 |\n        of view (alpha)____\\( )/_________ |      image\n                           /( )\\          |      plane\n                          /     <-- f --> |\n                         /       \\        |\n                        /         \\       |\n                       /           \\      v\n\n                    Pinhole camera model\n\n        From the image above we know that: tan(alpha/2) = w/2f\n        -> f = w/2 * (1/tan(alpha/2))\n\n        alpha in webcams and phones is often between 50 and 70 degrees.\n        -> 0.7 w <= f <= w\n        \"\"\"\n        if image_shape is None:\n            self.start()\n            height, width = self.read().shape[0:2]\n            self.stop()\n        else:\n            height, width = image_shape[:2]\n\n        focal_length = (width / 2) * (1 / np.tan(np.deg2rad(HFOV / 2.0)))\n        intrinsics = np.array([[focal_length, 0, width / 2.0],\n                               [0, focal_length, height / 2.0],\n                               [0, 0, 1.0]])\n        self.intrinsics = intrinsics\n\n    def take_photo(self):\n        \"\"\"Starts camera, reads buffer and returns an image.\n        \"\"\"\n        self.start()\n        image = self.read()\n        # all pipelines start with RGB\n        image = convert_color_space(image, BGR2RGB)\n        self.stop()\n        return image",
  "class VideoPlayer(object):\n    \"\"\"Performs visualization inferences in a real-time video.\n\n    # Properties\n        image_size: List of two integers. Output size of the displayed image.\n        pipeline: Function. Should take RGB image as input and it should\n            output a dictionary with key 'image' containing a visualization\n            of the inferences. Built-in pipelines can be found in\n            ``paz/processing/pipelines``.\n\n    # Methods\n        run()\n        record()\n    \"\"\"\n\n    def __init__(self, image_size, pipeline, camera, topic='image'):\n        self.image_size = image_size\n        self.pipeline = pipeline\n        self.camera = camera\n        self.topic = topic\n\n    def step(self):\n        \"\"\" Runs the pipeline process once\n\n        # Returns\n            Inferences from ``pipeline``.\n        \"\"\"\n        if self.camera.is_open() is False:\n            raise ValueError('Camera has not started. Call ``start`` method.')\n\n        frame = self.camera.read()\n        if frame is None:\n            print('Frame: None')\n            return None\n        # all pipelines start with an RGB image\n        frame = convert_color_space(frame, BGR2RGB)\n        return self.pipeline(frame)\n\n    def run(self):\n        \"\"\"Opens camera and starts continuous inference using ``pipeline``,\n        until the user presses ``q`` inside the opened window.\n        \"\"\"\n        self.camera.start()\n        while True:\n            output = self.step()\n            if output is None:\n                continue\n            image = resize_image(output[self.topic], tuple(self.image_size))\n            show_image(image, 'inference', wait=False)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n        self.camera.stop()\n        cv2.destroyAllWindows()\n\n    def record(self, name='video.avi', fps=20, fourCC='XVID'):\n        \"\"\"Opens camera and records continuous inference using ``pipeline``.\n\n        # Arguments\n            name: String. Video name. Must include the postfix .avi.\n            fps: Int. Frames per second.\n            fourCC: String. Indicates the four character code of the video.\n            e.g. XVID, MJPG, X264.\n        \"\"\"\n        self.camera.start()\n        fourCC = cv2.VideoWriter_fourcc(*fourCC)\n        writer = cv2.VideoWriter(name, fourCC, fps, self.image_size)\n        while True:\n            output = self.step()\n            if output is None:\n                continue\n            image = resize_image(output['image'], tuple(self.image_size))\n            show_image(image, 'inference', wait=False)\n            writer.write(image)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n        self.camera.stop()\n        writer.release()\n        cv2.destroyAllWindows()\n\n    def record_from_file(self, video_file_path, name='video.avi',\n                         fps=20, fourCC='XVID'):\n        \"\"\"Load video and records continuous inference using ``pipeline``.\n\n        # Arguments\n            video_file_path: String. Path to the video file.\n            name: String. Output video name. Must include the postfix .avi.\n            fps: Int. Frames per second.\n            fourCC: String. Indicates the four character code of the video.\n            e.g. XVID, MJPG, X264.\n        \"\"\"\n\n        fourCC = cv2.VideoWriter_fourcc(*fourCC)\n        writer = cv2.VideoWriter(name, fourCC, fps, self.image_size)\n\n        video = cv2.VideoCapture(video_file_path)\n        if (video.isOpened() is False):\n            print(\"Error opening video  file\")\n\n        while video.isOpened():\n            is_frame_received, frame = video.read()\n            if not is_frame_received:\n                print(\"Frame not received. Exiting ...\")\n                break\n            if is_frame_received is True:\n                output = self.pipeline(frame)\n                if output is None:\n                    continue\n                image = resize_image(output['image'], tuple(self.image_size))\n                show_image(image, 'inference', wait=False)\n                writer.write(image)\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n\n        writer.release()\n        cv2.destroyAllWindows()",
  "def __init__(self, device_id=0, name='Camera', intrinsics=None,\n                 distortion=None):\n        # TODO load parameters from camera name. Use ``load`` method.\n        self.device_id = device_id\n        self.name = name\n        self.intrinsics = intrinsics\n        self.distortion = None\n        self._camera = None",
  "def name(self):\n        return self._name",
  "def name(self, value):\n        self._name = value",
  "def intrinsics(self):\n        return self._intrinsics",
  "def intrinsics(self, value):\n        if value is None:\n            value = np.zeros((4))\n        self._intrinsics = value",
  "def distortion(self):\n        return self._distortion",
  "def distortion(self, distortion):\n        self._distortion = distortion",
  "def start(self):\n        \"\"\" Starts capturing device\n\n        # Returns\n            Camera object.\n        \"\"\"\n        self._camera = cv2.VideoCapture(self.device_id)\n        if self._camera is None or not self._camera.isOpened():\n            raise ValueError('Unable to open device', self.device_id)\n        return self._camera",
  "def stop(self):\n        \"\"\" Stops capturing device.\n        \"\"\"\n        return self._camera.release()",
  "def read(self):\n        \"\"\"Reads camera input and returns a frame.\n\n        # Returns\n            Image array.\n        \"\"\"\n        frame = self._camera.read()[1]\n        return frame",
  "def is_open(self):\n        \"\"\"Checks if camera is open.\n\n        # Returns\n            Boolean\n        \"\"\"\n        return self._camera.isOpened()",
  "def calibrate(self):\n        raise NotImplementedError",
  "def save(self, filepath):\n        raise NotImplementedError",
  "def load(self, filepath):\n        raise NotImplementedError",
  "def intrinsics_from_HFOV(self, HFOV=70, image_shape=None):\n        \"\"\"Computes camera intrinsics using horizontal field of view (HFOV).\n\n        # Arguments\n            HFOV: Angle in degrees of horizontal field of view.\n            image_shape: List of two floats [height, width].\n\n        # Returns\n            camera intrinsics array (3, 3).\n\n        # Notes:\n\n                       \\           /      ^\n                        \\         /       |\n                         \\ lens  /        | w/2\n        horizontal field  \\     / alpha/2 |\n        of view (alpha)____\\( )/_________ |      image\n                           /( )\\          |      plane\n                          /     <-- f --> |\n                         /       \\        |\n                        /         \\       |\n                       /           \\      v\n\n                    Pinhole camera model\n\n        From the image above we know that: tan(alpha/2) = w/2f\n        -> f = w/2 * (1/tan(alpha/2))\n\n        alpha in webcams and phones is often between 50 and 70 degrees.\n        -> 0.7 w <= f <= w\n        \"\"\"\n        if image_shape is None:\n            self.start()\n            height, width = self.read().shape[0:2]\n            self.stop()\n        else:\n            height, width = image_shape[:2]\n\n        focal_length = (width / 2) * (1 / np.tan(np.deg2rad(HFOV / 2.0)))\n        intrinsics = np.array([[focal_length, 0, width / 2.0],\n                               [0, focal_length, height / 2.0],\n                               [0, 0, 1.0]])\n        self.intrinsics = intrinsics",
  "def take_photo(self):\n        \"\"\"Starts camera, reads buffer and returns an image.\n        \"\"\"\n        self.start()\n        image = self.read()\n        # all pipelines start with RGB\n        image = convert_color_space(image, BGR2RGB)\n        self.stop()\n        return image",
  "def __init__(self, image_size, pipeline, camera, topic='image'):\n        self.image_size = image_size\n        self.pipeline = pipeline\n        self.camera = camera\n        self.topic = topic",
  "def step(self):\n        \"\"\" Runs the pipeline process once\n\n        # Returns\n            Inferences from ``pipeline``.\n        \"\"\"\n        if self.camera.is_open() is False:\n            raise ValueError('Camera has not started. Call ``start`` method.')\n\n        frame = self.camera.read()\n        if frame is None:\n            print('Frame: None')\n            return None\n        # all pipelines start with an RGB image\n        frame = convert_color_space(frame, BGR2RGB)\n        return self.pipeline(frame)",
  "def run(self):\n        \"\"\"Opens camera and starts continuous inference using ``pipeline``,\n        until the user presses ``q`` inside the opened window.\n        \"\"\"\n        self.camera.start()\n        while True:\n            output = self.step()\n            if output is None:\n                continue\n            image = resize_image(output[self.topic], tuple(self.image_size))\n            show_image(image, 'inference', wait=False)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n        self.camera.stop()\n        cv2.destroyAllWindows()",
  "def record(self, name='video.avi', fps=20, fourCC='XVID'):\n        \"\"\"Opens camera and records continuous inference using ``pipeline``.\n\n        # Arguments\n            name: String. Video name. Must include the postfix .avi.\n            fps: Int. Frames per second.\n            fourCC: String. Indicates the four character code of the video.\n            e.g. XVID, MJPG, X264.\n        \"\"\"\n        self.camera.start()\n        fourCC = cv2.VideoWriter_fourcc(*fourCC)\n        writer = cv2.VideoWriter(name, fourCC, fps, self.image_size)\n        while True:\n            output = self.step()\n            if output is None:\n                continue\n            image = resize_image(output['image'], tuple(self.image_size))\n            show_image(image, 'inference', wait=False)\n            writer.write(image)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n        self.camera.stop()\n        writer.release()\n        cv2.destroyAllWindows()",
  "def record_from_file(self, video_file_path, name='video.avi',\n                         fps=20, fourCC='XVID'):\n        \"\"\"Load video and records continuous inference using ``pipeline``.\n\n        # Arguments\n            video_file_path: String. Path to the video file.\n            name: String. Output video name. Must include the postfix .avi.\n            fps: Int. Frames per second.\n            fourCC: String. Indicates the four character code of the video.\n            e.g. XVID, MJPG, X264.\n        \"\"\"\n\n        fourCC = cv2.VideoWriter_fourcc(*fourCC)\n        writer = cv2.VideoWriter(name, fourCC, fps, self.image_size)\n\n        video = cv2.VideoCapture(video_file_path)\n        if (video.isOpened() is False):\n            print(\"Error opening video  file\")\n\n        while video.isOpened():\n            is_frame_received, frame = video.read()\n            if not is_frame_received:\n                print(\"Frame not received. Exiting ...\")\n                break\n            if is_frame_received is True:\n                output = self.pipeline(frame)\n                if output is None:\n                    continue\n                image = resize_image(output['image'], tuple(self.image_size))\n                show_image(image, 'inference', wait=False)\n                writer.write(image)\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n\n        writer.release()\n        cv2.destroyAllWindows()",
  "def append_values(dictionary, lists, keys):\n    \"\"\"Append dictionary values to lists\n\n    # Arguments\n        dictionary: dict\n        lists: List of lists\n        keys: Keys to dictionary values\n    \"\"\"\n    if len(keys) != len(lists):\n        assert ValueError('keys and lists must have same length')\n    for key_arg, key in enumerate(keys):\n        lists[key_arg].append(dictionary[key])\n    return lists",
  "def append_lists(intro_lists, outro_lists):\n    \"\"\"Appends multiple new values in intro lists to multiple outro lists\n\n    # Arguments\n        intro_lists: List of lists\n        outro_lists: List of lists\n\n    # Returns\n        Lists with new values of intro lists\n    \"\"\"\n    for intro_list, outro_list in zip(intro_lists, outro_lists):\n        outro_list.append(intro_list)\n    return outro_lists",
  "def get_upper_multiple(x, multiple=64):\n    \"\"\"Returns the upper multiple of 'multiple' to the x.\n\n    # Arguments\n        x: Int.\n        multiple: Int.\n\n    # Returns\n        upper multiple. Int.\n    \"\"\"\n    x = x + (multiple - 1)\n    floor_value = x // multiple\n    upper_multiple = floor_value * multiple\n    return upper_multiple",
  "def resize_with_same_aspect_ratio(image, input_size, multiple=64):\n    \"\"\"Resize the sort side of the input image to input_size and keep\n    the aspect ratio.\n\n    # Arguments\n        input_size: Dimension to be resized. Int.\n        H: Int.\n        W: Int.\n\n    # Returns\n        resized H and W.\n    \"\"\"\n    H, W = np.sort(image.shape[:2])\n    H_resized = int(input_size)\n    W_resized = input_size / H\n    W_resized = W_resized * W\n    W_resized = int(get_upper_multiple(W_resized, multiple))\n    size = np.array([W_resized, H_resized])\n    return size",
  "def get_transformation_scale(image, size, scaling_factor):\n    \"\"\"Caluclte scale of resized H and W.\n\n    # Arguments\n        H: Int.\n        H_resized: Int.\n        H_resized: Int.\n        scaling_factor: Int.\n\n    # Returns\n        scaled H and W\n    \"\"\"\n    H, W = image.shape[:2]\n    H_resized, W_resized = size\n\n    if H < W:\n        H_resized, W_resized = W_resized, H_resized\n    H, W = np.sort([H, W])\n\n    scale_H = H / scaling_factor\n    aspect_ratio = W_resized / H_resized\n    scale_W = aspect_ratio * scale_H\n    scale = np.array([scale_W, scale_H])\n    return scale",
  "def compare_vertical_neighbours(x, y, image, offset=0.25):\n    \"\"\"Compare two vertical neighbors and add an offset to the smaller one.\n\n    # Arguments\n        x: Int. x coordinate of pixel to be compared.\n        y: Int. y coordinate of pixel to be compared.\n        image: Array.\n        offset: Float.\n    \"\"\"\n    int_x, int_y = int(x), int(y)\n    lower_y = min(int_y + 1, image.shape[1] - 1)\n    upper_y = max(int_y - 1, 0)\n    if image[int_x, lower_y] > image[int_x, upper_y]:\n        y = y + offset\n    else:\n        y = y - offset\n    return y",
  "def compare_horizontal_neighbours(x, y, image, offset=0.25):\n    \"\"\"Compare two horizontal neighbors and add an offset to the smaller one.\n\n    # Arguments\n        x: Int. x coordinate of pixel to be compared.\n        y: Int. y coordinate of pixel to be compared.\n        image: Array.\n        offset: Float.\n    \"\"\"\n    int_x, int_y = int(x), int(y)\n    left_x = max(0, int_x - 1)\n    right_x = min(int_x + 1, image.shape[0] - 1)\n    if image[right_x, int_y] > image[left_x, int_y]:\n        x = x + offset\n    else:\n        x = x - offset\n    return x",
  "def get_all_indices_of_array(array):\n    \"\"\"Get all the indices of an array.\n\n    # Arguments\n        array: Array\n\n    # Returns\n        Array. Array with the indices of the input array\n    \"\"\"\n    all_indices = np.ndarray(array.shape)\n    all_indices.fill(True)\n    all_indices = np.where(all_indices)\n    all_indices = np.array(all_indices).T\n    print(all_indices)\n    return all_indices",
  "def gather_nd(array, indices, axis):\n    \"\"\"Take the value from the input array on the given indices along the\n    given axis.\n\n    # Arguments\n        array: Array\n        indices: list/Array. values to be gathered from\n        axis: Int. Axis along which to gather values.\n\n    # Returns\n        Array. Gathered values from the input array\n    \"\"\"\n    gathered = np.take_along_axis(array, indices, axis=axis)\n    return gathered",
  "def calculate_norm(vector, order=None, axis=None):\n    \"\"\"Calculates the norm of vector.\n\n    # Arguments\n        x: List of spatial coordinates (x, y, z)\n    \"\"\"\n    return np.linalg.norm(vector, ord=order, axis=axis)",
  "def tensor_to_numpy(tensor):\n    \"\"\"Convert a tensor to a Array.\n\n    # Arguments\n        tensor: multidimensional array of type tensor\n    \"\"\"\n    return tensor.cpu().numpy()",
  "def pad_matrix(matrix, pool_size=(3, 3), strides=(1, 1),\n               padding='valid', value=0):\n    \"\"\"Pad an array\n\n    # Arguments\n        matrix: Array.\n        padding: String. Type of padding\n        value: Int. Value to be added in the padded area.\n        poolsize: Int. How many rows and colums to be padded for 'same' padding\n    \"\"\"\n    matrix = np.array(matrix)\n    H, W = matrix.shape[:2]\n    if padding == 'valid':\n        padding = ((0, 0), (0, 0))\n    if padding == 'square':\n        if H > W:\n            padding = ((0, 0), (0, H - W))\n        else:\n            padding = ((0, W - H), (0, 0))\n    if padding == 'same':\n        if isinstance(pool_size, int):\n            pool_size = (pool_size, pool_size)\n        if isinstance(strides, int):\n            strides = (strides, strides)\n        if H % strides[0] == 0:\n            height_pad = np.max((pool_size[0] - strides[0]), 0)\n        else:\n            height_pad = np.max(pool_size[0] - (H % strides[0]), 0)\n        if W % strides[1] == 0:\n            width_pad = np.max((pool_size[1] - strides[1]), 0)\n        else:\n            width_pad = np.max(pool_size[1] - (W % strides[1]), 0)\n\n        pad_top = height_pad // 2\n        pad_bottom = height_pad - pad_top\n        pad_left = width_pad // 2\n        pad_right = width_pad - pad_left\n        padding = ((pad_top, pad_bottom), (pad_left, pad_right))\n    return np.pad(matrix, padding, mode='constant', constant_values=value)",
  "def max_pooling_2d(image, pool_size=3, strides=1, padding='same'):\n    \"\"\"Returns the maximum pooled value of an image.\n\n    # Arguments\n        image: Array.\n        poolsize: Int or list of len 2. Window size for each pool\n        padding: String. Type of padding\n    \"\"\"\n    if not isinstance(strides, int):\n        strides = strides[0]\n    if not isinstance(pool_size, int):\n        pool_size = pool_size[0]\n\n    if padding == 'valid':\n        max_image = np.zeros((image.shape[0] - pool_size + 1,\n                              image.shape[1] - pool_size + 1))\n    if padding == 'same':\n        max_image = np.zeros_like(image)\n\n    image = pad_matrix(image, pool_size, strides, padding)\n    H, W = image.shape[:2]\n    for y in range(0, H - pool_size + 1, strides):\n        for x in range(0, W - pool_size + 1, strides):\n            max_image[y][x] = np.max(image[y:y + pool_size, x:x + pool_size])\n    return max_image",
  "def predict(x, model, preprocess=None, postprocess=None):\n    \"\"\"Preprocess, predict and postprocess input.\n    # Arguments\n        x: Input to model\n        model: Callable i.e. Keras model.\n        preprocess: Callable, used for preprocessing input x.\n        postprocess: Callable, used for postprocessing output of model.\n\n    # Note\n        If model outputs a tf.Tensor is converted automatically to numpy array.\n    \"\"\"\n    if preprocess is not None:\n        x = preprocess(x)\n    y = model(x)\n    if isinstance(y, tf.Tensor):\n        y = y.numpy()\n    if postprocess is not None:\n        y = postprocess(y)\n    return y",
  "def cast_image(image, dtype):\n    \"\"\"Casts an image into a different type\n\n    # Arguments\n        image: Numpy array.\n        dtype: String or np.dtype.\n\n    # Returns\n        Numpy array.\n    \"\"\"\n    return image.astype(dtype)",
  "def random_saturation(image, lower=0.3, upper=1.5):\n    \"\"\"Applies random saturation to an RGB image.\n\n    # Arguments\n        image: Numpy array representing an image RGB format.\n        lower: Float.\n        upper: Float.\n    \"\"\"\n    image = convert_color_space(image, RGB2HSV)\n    image = cast_image(image, np.float32)\n    image[:, :, 1] = image[:, :, 1] * np.random.uniform(lower, upper)\n    image[:, :, 1] = np.clip(image[:, :, 1], 0, 255)\n    image = cast_image(image, np.uint8)\n    image = convert_color_space(image, HSV2RGB)\n    return image",
  "def random_brightness(image, delta=32):\n    \"\"\"Applies random brightness to an RGB image.\n\n    # Arguments\n        image: Numpy array representing an image RGB format.\n        delta: Int.\n    \"\"\"\n    image = cast_image(image, np.float32)\n    random_brightness = np.random.uniform(-delta, delta)\n    image = image + random_brightness\n    image = np.clip(image, 0, 255)\n    image = cast_image(image, np.uint8)\n    return image",
  "def random_contrast(image, lower=0.5, upper=1.5):\n    \"\"\"Applies random contrast to an RGB image.\n\n    # Arguments\n        image: Numpy array representing an image RGB format.\n        lower: Float.\n        upper: Float.\n    \"\"\"\n    alpha = np.random.uniform(lower, upper)\n    image = cast_image(image, np.float32)\n    image = image * alpha\n    image = np.clip(image, 0, 255)\n    image = cast_image(image, np.uint8)\n    return image",
  "def random_hue(image, delta=18):\n    \"\"\"Applies random hue to an RGB image.\n\n    # Arguments\n        image: Numpy array representing an image RGB format.\n        delta: Int.\n    \"\"\"\n    image = convert_color_space(image, RGB2HSV)\n    image = cast_image(image, np.float32)\n    image[:, :, 0] = image[:, :, 0] + np.random.uniform(-delta, delta)\n    image[:, :, 0][image[:, :, 0] > 179.0] -= 179.0\n    image[:, :, 0][image[:, :, 0] < 0.0] += 179.0\n    image = cast_image(image, np.uint8)\n    image = convert_color_space(image, HSV2RGB)\n    return image",
  "def flip_left_right(image):\n    \"\"\"Flips an image left and right.\n\n    # Arguments\n        image: Numpy array.\n    \"\"\"\n    return image[:, ::-1]",
  "def random_flip_left_right(image):\n    \"\"\"Applies random left or right flip.\n\n    # Arguments\n        image: Numpy array.\n    \"\"\"\n    if np.random.uniform([1], 0, 2) == 1:\n        image = flip_left_right(image)\n    return image",
  "def crop_image(image, crop_box):\n    \"\"\"Resize image.\n\n    # Arguments\n        image: Numpy array.\n        crop_box: List of four ints.\n\n    # Returns\n        Numpy array.\n    \"\"\"\n    if (type(image) != np.ndarray):\n        raise ValueError(\n            'Recieved Image is not of type numpy array', type(image))\n    else:\n        cropped_image = image[crop_box[0]:crop_box[2],\n                              crop_box[1]:crop_box[3], :]\n    return cropped_image",
  "def image_to_normalized_device_coordinates(image):\n    \"\"\"Map image value from [0, 255] -> [-1, 1].\n    \"\"\"\n    return (image / 127.5) - 1.0",
  "def normalized_device_coordinates_to_image(image):\n    \"\"\"Map normalized value from [-1, 1] -> [0, 255].\n    \"\"\"\n    return (image + 1.0) * 127.5",
  "def random_shape_crop(image, shape):\n    \"\"\"Randomly crops an image of the given ``shape``.\n\n    # Arguments\n        image: Numpy array.\n        shape: List of two ints ''(H, W)''.\n\n    # Returns\n        Numpy array of cropped image.\n    \"\"\"\n    H, W = image.shape[:2]\n    if (shape[0] >= H) or (shape[1] >= W):\n        return None\n    x_min = np.random.randint(0, W - shape[1])\n    y_min = np.random.randint(0, H - shape[0])\n    x_max = int(x_min + shape[1])\n    y_max = int(y_min + shape[0])\n    cropped_image = image[y_min:y_max, x_min:x_max]\n    return cropped_image",
  "def make_random_plain_image(shape):\n    \"\"\"Makes random plain image by sampling three random values.\n\n    # Arguments\n        shape: Image shape e.g. ''(H, W, 3)''.\n\n    # Returns\n        Numpy array of shape ''(H, W, 3)''.\n    \"\"\"\n    if len(shape) != 3:\n        raise ValueError('``shape`` must have three values')\n    return (np.ones(shape) * np.random.randint(0, 256, shape[-1]))",
  "def blend_alpha_channel(image, background):\n    \"\"\"Blends image with background using an alpha channel.\n\n    # Arguments\n        image: Numpy array with alpha channel. Shape must be ''(H, W, 4)''\n        background: Numpy array of shape ''(H, W, 3)''.\n    \"\"\"\n    if image.shape[-1] != 4:\n        raise ValueError('``image`` does not contain an alpha mask.')\n    foreground, alpha = np.split(image, [3], -1)\n    alpha = alpha / 255.0\n    background = (1.0 - alpha) * background.astype(float)\n    image = (alpha * foreground.astype(float)) + background\n    return image.astype('uint8')",
  "def concatenate_alpha_mask(image, alpha_mask):\n    \"\"\"Concatenates alpha mask to image.\n\n    # Arguments\n        image: Numpy array of shape ''(H, W, 3)''.\n        alpha_mask: Numpy array array of shape ''(H, W)''.\n\n    # Returns\n        Numpy array of shape ''(H, W, 4)''.\n    \"\"\"\n    return np.concatenate([image, alpha_mask], axis=2)",
  "def split_and_normalize_alpha_channel(image):\n    \"\"\"Splits alpha channel from an RGBA image and normalizes alpha channel.\n\n    # Arguments\n        image: Numpy array of shape ''(H, W, 4)''.\n\n    # Returns\n        List of two numpy arrays containing respectively the image and the\n            alpha channel.\n    \"\"\"\n    if image.shape[-1] != 4:\n        raise ValueError('Provided image does not contain alpha mask.')\n    image, alpha_channel = np.split(image, [3], -1)\n    alpha_channel = alpha_channel / 255.0\n    return image, alpha_channel",
  "def random_image_blur(image):\n    \"\"\"Applies random choice blur.\n\n    # Arguments\n        image: Numpy array of shape ''(H, W, 3)''.\n\n    # Returns\n        Numpy array.\n    \"\"\"\n    blur = np.random.choice([gaussian_image_blur, median_image_blur])\n    return blur(image)",
  "def translate_image(image, translation, fill_color):\n    \"\"\"Translate image.\n\n    # Arguments\n        image: Numpy array.\n        translation: A list of length two indicating the x,y translation values\n        fill_color: List of three floats representing a color.\n\n    # Returns\n        Numpy array\n    \"\"\"\n    matrix = np.zeros((2, 3), dtype=np.float32)\n    matrix[0, 0], matrix[1, 1] = 1.0, 1.0\n    matrix[0, 2], matrix[1, 2] = translation\n    image = warp_affine(image, matrix, fill_color)\n    return image",
  "def sample_scaled_translation(delta_scale, image_shape):\n    \"\"\"Samples a scaled translation from a uniform distribution.\n\n    # Arguments\n        delta_scale: List with two elements having the normalized deltas.\n            e.g. ''[.25, .25]''.\n        image_shape: List containing the height and width of the image.\n    \"\"\"\n    x_delta_scale, y_delta_scale = delta_scale\n    x = image_shape[1] * np.random.uniform(-x_delta_scale, x_delta_scale)\n    y = image_shape[0] * np.random.uniform(-y_delta_scale, y_delta_scale)\n    return [x, y]",
  "def replace_lower_than_threshold(source, threshold=1e-3, replacement=0.0):\n    \"\"\"Replace values from source that are lower than the given threshold.\n    This function doesn't create a new array but does replacement in place.\n\n    # Arguments\n        source: Array.\n        threshold: Float. Values lower than this value will be replaced.\n        replacement: Float. Value taken by elements lower than threshold.\n\n    # Returns\n        Array of same shape as source.\n    \"\"\"\n    lower_than_epsilon = source < threshold\n    source[lower_than_epsilon] = replacement\n    return source",
  "def normalize_min_max(x, x_min, x_max):\n    \"\"\"Normalized data using it's maximum and minimum values\n\n    # Arguments\n        x: array\n        x_min: minimum value of x\n        x_max: maximum value of x\n\n    # Returns\n        min-max normalized data\n    \"\"\"\n    return (x - x_min) / (x_max - x_min)",
  "def calculate_image_center(image):\n    '''\n    Return image center.\n\n    # Arguments\n        image: Numpy array.\n\n    # Returns\n        image center.\n    '''\n    H, W = image.shape[:2]\n    center_W = W / 2.0\n    center_H = H / 2.0\n    return center_W, center_H",
  "def get_scaling_factor(image, scale=1, shape=(128, 128)):\n    '''\n    Return scaling factor for the image.\n\n    # Arguments\n        image: Numpy array.\n        scale: Int.\n        shape: Tuple of integers. eg. (128, 128)\n\n    # Returns\n        scaling factor: Numpy array of size 2\n    '''\n    H, W = image.shape[:2]\n    H_scale = H / shape[0]\n    W_scale = W / shape[1]\n    return np.array([W_scale * scale, H_scale * scale])",
  "def scale_resize(image, image_size):\n    \"\"\"Resizes and crops image by returning the scales to original\n    image.\n\n    Args:\n        image: Numpy array, raw image.\n        image_size: Int, size of the image.\n\n    Returns:\n        Tuple: output_image, image_scale.\n    \"\"\"\n    H, W = image.shape[0], image.shape[1]\n    image_scale_x = image_size / W\n    image_scale_y = image_size / H\n    image_scale = min(image_scale_x, image_scale_y)\n    scaled_H = int(H * image_scale)\n    scaled_W = int(W * image_scale)\n    scaled_image = resize_image(image, (scaled_W, scaled_H))\n    scaled_image = scaled_image[:image_size, :image_size, :]\n    output_image = np.zeros((image_size, image_size, image.shape[2]))\n    output_image[:scaled_image.shape[0],\n                 :scaled_image.shape[1],\n                 :scaled_image.shape[2]] = scaled_image\n    image_scale = np.array(1 / image_scale)\n    output_image = output_image[np.newaxis]\n    return output_image, image_scale",
  "def resize_image(image, size, method=BILINEAR):\n    \"\"\"Resize image.\n\n    # Arguments\n        image: Numpy array.\n        size: List of two ints.\n        method: Flag indicating interpolation method i.e.\n            paz.backend.image.CUBIC\n\n    # Returns\n        Numpy array.\n    \"\"\"\n    if (type(image) != np.ndarray):\n        raise ValueError(\n            'Recieved Image is not of type numpy array', type(image))\n    else:\n        return cv2.resize(image, size, interpolation=method)",
  "def convert_color_space(image, flag):\n    \"\"\"Convert image to a different color space.\n\n    # Arguments\n        image: Numpy array.\n        flag: PAZ or openCV flag. e.g. paz.backend.image.RGB2BGR.\n\n    # Returns\n        Numpy array.\n    \"\"\"\n    return cv2.cvtColor(image, flag)",
  "def load_image(filepath, num_channels=3):\n    \"\"\"Load image from a ''filepath''.\n\n    # Arguments\n        filepath: String indicating full path to the image.\n        num_channels: Int.\n\n    # Returns\n        Numpy array.\n    \"\"\"\n    if num_channels not in [1, 3, 4]:\n        raise ValueError('Invalid number of channels')\n\n    image = cv2.imread(filepath, _CHANNELS_TO_FLAG[num_channels])\n    if num_channels == 3:\n        image = convert_color_space(image, BGR2RGB)\n    elif num_channels == 4:\n        image = convert_color_space(image, BGRA2RGBA)\n    return image",
  "def show_image(image, name='image', wait=True):\n    \"\"\"Shows RGB image in an external window.\n\n    # Arguments\n        image: Numpy array\n        name: String indicating the window name.\n        wait: Boolean. If ''True'' window stays open until user presses a key.\n            If ''False'' windows closes immediately.\n    \"\"\"\n    if image.dtype != np.uint8:\n        raise ValueError('``image`` must be of type ``uint8``')\n    # openCV default color space is BGR\n    image = convert_color_space(image, RGB2BGR)\n    cv2.imshow(name, image)\n    if wait:\n        while True:\n            if cv2.waitKey(0) & 0xFF == ord('q'):\n                break\n        cv2.destroyAllWindows()",
  "def warp_affine(image, matrix, fill_color=[0, 0, 0], size=None):\n    \"\"\" Transforms `image` using an affine `matrix` transformation.\n\n    # Arguments\n        image: Numpy array.\n        matrix: Numpy array of shape (2,3) indicating affine transformation.\n        fill_color: List/tuple representing BGR use for filling empty space.\n    \"\"\"\n    if size is not None:\n        width, height = size\n    else:\n        height, width = image.shape[:2]\n    return cv2.warpAffine(\n        image, matrix, (width, height), borderValue=fill_color)",
  "def write_image(filepath, image):\n    \"\"\"Writes an image inside ``filepath``. If ``filepath`` doesn't exist\n        it makes a directory. If ``image`` has three channels the image is\n        converted into BGR and then written. This is done such that this\n        function compatible with ``load_image``.\n\n    # Arguments\n        filepath: String with image path. It should include postfix e.g. .png\n        image: Numpy array.\n    \"\"\"\n    directory_name = os.path.dirname(filepath)\n    if (not os.path.exists(directory_name) and (len(directory_name) > 0)):\n        os.makedirs(directory_name)\n    if image.shape[-1] == 3:\n        image = convert_color_space(image, RGB2BGR)\n    return cv2.imwrite(filepath, image)",
  "def gaussian_image_blur(image, kernel_size=(5, 5)):\n    \"\"\"Applies Gaussian blur to an image.\n\n    # Arguments\n        image: Numpy array of shape ''(H, W, 4)''.\n        kernel_size: List of two ints e.g. ''(5, 5)''.\n\n    # Returns\n        Numpy array\n    \"\"\"\n    return cv2.GaussianBlur(image, kernel_size, 0)",
  "def median_image_blur(image, apperture=5):\n    \"\"\"Applies median blur to an image.\n\n    # Arguments\n        image: Numpy array of shape ''(H, W, 3)''.\n        apperture. Int.\n\n    # Returns\n        Numpy array.\n    \"\"\"\n    return cv2.medianBlur(image, apperture)",
  "def get_rotation_matrix(center, degrees, scale=1.0):\n    \"\"\"Returns a 2D rotation matrix.\n\n    # Arguments\n        center: List of two integer values.\n        degrees: Float indicating the angle in degrees.\n\n    # Returns\n        Numpy array\n    \"\"\"\n    return cv2.getRotationMatrix2D(center, degrees, scale)",
  "def get_affine_transform(source_points, destination_points):\n    '''\n    Return the transformation matrix.\n\n    # Arguments\n        source_points: Numpy array.\n        destination_points: Numpy array.\n\n    # Returns\n        Transformation matrix.\n    '''\n    return cv2.getAffineTransform(source_points, destination_points)",
  "def draw_square(image, center, color, size):\n    \"\"\"Draw a square in an image\n\n    # Arguments\n        image: Array ``(H, W, 3)``\n        center: List ``(2)`` with ``(x, y)`` values in openCV coordinates.\n        size: Float. Length of square size.\n        color: List ``(3)`` indicating RGB colors.\n\n    # Returns\n        Array ``(H, W, 3)`` with square.\n    \"\"\"\n    center_x, center_y = center\n    x_min, y_min = center_x - size, center_y - size\n    x_max, y_max = center_x + size, center_y + size\n    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), tuple(color), FILLED)\n    return image",
  "def draw_circle(image, center, color=GREEN, radius=5):\n    \"\"\"Draw a circle in an image\n\n    # Arguments\n        image: Array ``(H, W, 3)``\n        center: List ``(2)`` with ``(x, y)`` values in openCV coordinates.\n        radius: Float. Radius of circle.\n        color: Tuple ``(3)`` indicating the RGB colors.\n\n    # Returns\n        Array ``(H, W, 3)`` with circle.\n    \"\"\"\n    cv2.circle(image, tuple(center), radius, tuple(color), FILLED)\n    return image",
  "def draw_triangle(image, center, color, size):\n    \"\"\"Draw a triangle in an image\n\n    # Arguments\n        image: Array ``(H, W, 3)``\n        center: List ``(2)`` containing ``(x_center, y_center)``.\n        size: Float. Length of square size.\n        color: Tuple ``(3)`` indicating the RGB colors.\n\n    # Returns\n        Array ``(H, W, 3)`` with triangle.\n    \"\"\"\n    center_x, center_y = center\n    vertex_A = (center_x, center_y - size)\n    vertex_B = (center_x - size, center_y + size)\n    vertex_C = (center_x + size, center_y + size)\n    points = np.array([[vertex_A, vertex_B, vertex_C]], dtype=np.int32)\n    cv2.fillPoly(image, points, tuple(color))\n    return image",
  "def draw_keypoint(image, point, color=GREEN, radius=5):\n    \"\"\" Draws a circle in image.\n\n    # Arguments\n        image: Numpy array of shape ``[H, W, 3]``.\n        point: List of length two indicating ``(y, x)``\n            openCV coordinates.\n        color: List of length three indicating RGB color of point.\n        radius: Integer indicating the radius of the point to be drawn.\n\n    # Returns\n        Numpy array with shape ``[H, W, 3]``. Image with circle.\n    \"\"\"\n    cv2.circle(image, tuple(point), radius, (0, 0, 0), FILLED)\n    inner_radius = int(0.8 * radius)\n    cv2.circle(image, tuple(point), inner_radius, tuple(color), FILLED)\n    return image",
  "def put_text(image, text, point, scale, color, thickness):\n    \"\"\"Draws text in image.\n\n    # Arguments\n        image: Numpy array.\n        text: String. Text to be drawn.\n        point: Tuple of coordinates indicating the top corner of the text.\n        scale: Float. Scale of text.\n        color: Tuple of integers. RGB color coordinates.\n        thickness: Integer. Thickness of the lines used for drawing text.\n\n    # Returns\n        Numpy array with shape ``[H, W, 3]``. Image with text.\n    \"\"\"\n    # cv2.putText returns an image in contrast to other drawing cv2 functions.\n    return cv2.putText(image, text, point, FONT, scale, color, thickness, LINE)",
  "def draw_line(image, point_A, point_B, color=GREEN, thickness=5):\n    \"\"\" Draws a line in image from ``point_A`` to ``point_B``.\n\n    # Arguments\n        image: Numpy array of shape ``[H, W, 3]``.\n        point_A: List of length two indicating ``(y, x)`` openCV coordinates.\n        point_B: List of length two indicating ``(y, x)`` openCV coordinates.\n        color: List of length three indicating RGB color of point.\n        thickness: Integer indicating the thickness of the line to be drawn.\n\n    # Returns\n        Numpy array with shape ``[H, W, 3]``. Image with line.\n    \"\"\"\n    cv2.line(image, tuple(point_A), tuple(point_B), tuple(color), thickness)\n    return image",
  "def draw_rectangle(image, corner_A, corner_B, color, thickness):\n    \"\"\" Draws a filled rectangle from ``corner_A`` to ``corner_B``.\n\n    # Arguments\n        image: Numpy array of shape ``[H, W, 3]``.\n        corner_A: List of length two indicating ``(y, x)`` openCV coordinates.\n        corner_B: List of length two indicating ``(y, x)`` openCV coordinates.\n        color: List of length three indicating RGB color of point.\n        thickness: Integer/openCV Flag. Thickness of rectangle line.\n            or for filled use cv2.FILLED flag.\n\n    # Returns\n        Numpy array with shape ``[H, W, 3]``. Image with rectangle.\n    \"\"\"\n    return cv2.rectangle(\n        image, tuple(corner_A), tuple(corner_B), tuple(color), thickness)",
  "def draw_dot(image, point, color=GREEN, radius=5, filled=FILLED):\n    \"\"\" Draws a dot (small rectangle) in image.\n\n    # Arguments\n        image: Numpy array of shape ``[H, W, 3]``.\n        point: List of length two indicating ``(y, x)`` openCV coordinates.\n        color: List of length three indicating RGB color of point.\n        radius: Integer indicating the radius of the point to be drawn.\n        filled: Boolean. If `True` rectangle is filled with `color`.\n\n    # Returns\n        Numpy array with shape ``[H, W, 3]``. Image with dot.\n    \"\"\"\n    # drawing outer black rectangle\n    point_A = (int(point[0] - radius), int(point[1] - radius))\n    point_B = (int(point[0] + radius), int(point[1] + radius))\n    draw_rectangle(image, tuple(point_A), tuple(point_B), color, filled)\n\n    # drawing innner rectangle with given `color`\n    inner_radius = int(0.8 * radius)\n    point_A = (int(point[0] - inner_radius), int(point[1] - inner_radius))\n    point_B = (int(point[0] + inner_radius), int(point[1] + inner_radius))\n    draw_rectangle(image, tuple(point_A), tuple(point_B), color, filled)\n    return image",
  "def draw_cube(image, points, color=GREEN, thickness=2, radius=5):\n    \"\"\"Draws a cube in image.\n\n    # Arguments\n        image: Numpy array of shape (H, W, 3).\n        points: List of length 8  having each element a list\n            of length two indicating (U, V) openCV coordinates.\n        color: List of length three indicating RGB color of point.\n        thickness: Integer indicating the thickness of the line to be drawn.\n        radius: Integer indicating the radius of corner points to be drawn.\n\n    # Returns\n        Numpy array with shape (H, W, 3). Image with cube.\n    \"\"\"\n    if points.shape != (8, 2):\n        raise ValueError('Cube points 2D must be of shape (8, 2)')\n\n    # draw bottom\n    draw_line(image, points[0], points[1], color, thickness)\n    draw_line(image, points[1], points[2], color, thickness)\n    draw_line(image, points[3], points[2], color, thickness)\n    draw_line(image, points[3], points[0], color, thickness)\n\n    # draw top\n    draw_line(image, points[4], points[5], color, thickness)\n    draw_line(image, points[6], points[5], color, thickness)\n    draw_line(image, points[6], points[7], color, thickness)\n    draw_line(image, points[4], points[7], color, thickness)\n\n    # draw sides\n    draw_line(image, points[0], points[4], color, thickness)\n    draw_line(image, points[7], points[3], color, thickness)\n    draw_line(image, points[5], points[1], color, thickness)\n    draw_line(image, points[2], points[6], color, thickness)\n\n    # draw X mark on top\n    draw_line(image, points[4], points[6], color, thickness)\n    draw_line(image, points[5], points[7], color, thickness)\n\n    # draw dots\n    [draw_dot(image, np.squeeze(point), color, radius) for point in points]\n    return image",
  "def draw_filled_polygon(image, vertices, color):\n    \"\"\" Draws filled polygon\n\n    # Arguments\n        image: Numpy array.\n        vertices: List of elements each having a list\n            of length two indicating ``(y, x)`` openCV coordinates.\n        color: Numpy array specifying RGB color of the polygon.\n\n    # Returns\n        Numpy array with shape ``[H, W, 3]``. Image with polygon.\n    \"\"\"\n    cv2.fillPoly(image, [vertices], color)\n    return image",
  "def draw_random_polygon(image, max_radius_scale=.5):\n    \"\"\"Draw random polygon image.\n\n    # Arguments\n        image: Numpy array with shape ``[H, W, 3]``.\n        max_radius_scale: Float between [0, 1].\n\n    # Returns\n        Numpy array with shape ``[H, W, 3]``. Image with polygon.\n    \"\"\"\n    height, width = image.shape[:2]\n    max_distance = np.max((height, width)) * max_radius_scale\n    num_vertices = np.random.randint(3, 7)\n    angle_between_vertices = 2 * np.pi / num_vertices\n    initial_angle = np.random.uniform(0, 2 * np.pi)\n    center = np.random.rand(2) * np.array([width, height])\n    vertices = np.zeros((num_vertices, 2), dtype=np.int32)\n    for vertex_arg in range(num_vertices):\n        angle = initial_angle + (vertex_arg * angle_between_vertices)\n        vertex = np.array([np.cos(angle), np.sin(angle)])\n        vertex = np.random.uniform(0, max_distance) * vertex\n        vertices[vertex_arg] = (vertex + center).astype(np.int32)\n    color = np.random.randint(0, 256, 3).tolist()\n    draw_filled_polygon(image, vertices, color)\n    return image",
  "def lincolor(num_colors, saturation=1, value=1, normalized=False):\n    \"\"\"Creates a list of RGB colors linearly sampled from HSV space with\n        randomised Saturation and Value.\n\n    # Arguments\n        num_colors: Int.\n        saturation: Float or `None`. If float indicates saturation.\n            If `None` it samples a random value.\n        value: Float or `None`. If float indicates value.\n            If `None` it samples a random value.\n        normalized: Bool. If True, RGB colors are returned between [0, 1]\n            if False, RGB colors are between [0, 255].\n\n    # Returns\n        List, for which each element contains a list with RGB color\n    \"\"\"\n    RGB_colors = []\n    hues = [value / num_colors for value in range(0, num_colors)]\n    for hue in hues:\n\n        if saturation is None:\n            saturation = random.uniform(0.6, 1)\n\n        if value is None:\n            value = random.uniform(0.5, 1)\n\n        RGB_color = colorsys.hsv_to_rgb(hue, saturation, value)\n        if not normalized:\n            RGB_color = [int(color * 255) for color in RGB_color]\n        RGB_colors.append(RGB_color)\n    return RGB_colors",
  "def make_mosaic(images, shape, border=0):\n    \"\"\" Creates an image mosaic.\n\n    # Arguments\n        images: Numpy array of shape (num_images, height, width, num_channels)\n        shape: List of two integers indicating the mosaic shape.\n        border: Integer indicating the border per image.\n\n    # Returns\n        A numpy array containing all images.\n\n    # Exceptions\n        Shape must satisfy `len(images) > shape[0] * shape[1]`\n    \"\"\"\n    num_images, H, W, num_channels = images.shape\n    num_rows, num_cols = shape\n    if num_images > (num_rows * num_cols):\n        raise ValueError('Number of images is bigger than shape')\n\n    total_rows = (num_rows * H) + ((num_rows - 1) * border)\n    total_cols = (num_cols * W) + ((num_cols - 1) * border)\n    mosaic = np.ones((total_rows, total_cols, num_channels))\n\n    padded_H = H + border\n    padded_W = W + border\n\n    for image_arg, image in enumerate(images):\n        row = int(np.floor(image_arg / num_cols))\n        col = image_arg % num_cols\n        mosaic[row * padded_H:row * padded_H + H,\n               col * padded_W:col * padded_W + W, :] = image\n    return mosaic",
  "def draw_points2D(image, points2D, colors):\n    \"\"\"Draws a pixel for all points2D in UV space using only numpy.\n\n    # Arguments\n        image: Array (H, W).\n        keypoints: Array (num_points, U, V). Keypoints in image space\n        colors: Array (num_points, 3). Colors in RGB space.\n\n    # Returns\n        Array with drawn points.\n    \"\"\"\n    points2D = points2D.astype(int)\n    U = points2D[:, 0]\n    V = points2D[:, 1]\n    image[V, U, :] = colors\n    return image",
  "def draw_keypoints_link(image, keypoints, link_args, link_orders, link_colors,\n                        check_scores=False, link_width=2):\n    \"\"\" Draw link between the keypoints.\n\n    # Arguments\n        images: Numpy array.\n        keypoints: Keypoint(k0, k1, ...) locations in the image. Numpy array.\n        link_args: Keypoint labels. Dictionary. {'k0':0, 'k1':1, ...}\n        link_orders: List of tuple. [('k0', 'k1'),('kl', 'k2'), ...]\n        link_colors: Color of each link. List of list\n        check_scores: Condition to draw links. Boolean.\n\n    # Returns\n        A numpy array containing drawn link between the keypoints.\n    \"\"\"\n    for pair_arg, pair in enumerate(link_orders):\n        color = link_colors[pair_arg]\n        point1 = keypoints[link_args[pair[0]]]\n        point2 = keypoints[link_args[pair[1]]]\n        if check_scores:\n            if point1[2] > 0 and point2[2] > 0:\n                draw_line(image, (int(point1[0]), int(point1[1])),\n                                 (int(point2[0]), int(point2[1])),\n                          color, link_width)\n        else:\n            draw_line(image, (int(point1[0]), int(point1[1])),\n                             (int(point2[0]), int(point2[1])),\n                      color, link_width)\n    return image",
  "def draw_keypoints(image, keypoints, keypoint_colors, check_scores=False,\n                   keypoint_radius=6):\n    \"\"\" Draw a circle at keypoints.\n\n    # Arguments\n        images: Numpy array.\n        keypoints: Keypoint locations in the image. Numpy array.\n        keypoint_colors: Color of each keypoint. List of list\n        check_scores: Condition to draw keypoint. Boolean.\n\n    # Returns\n        A numpy array containing circle at each keypoints.\n    \"\"\"\n    for keypoint_arg, keypoint in enumerate(keypoints):\n        color = keypoint_colors[keypoint_arg]\n        if check_scores:\n            if keypoint[2] > 0:\n                draw_keypoint(\n                    image, (int(keypoint[0]),\n                            int(keypoint[1])), color, keypoint_radius)\n        else:\n            draw_keypoint(image, (int(keypoint[0]), int(keypoint[1])), color,\n                          keypoint_radius)\n    return image",
  "def points3D_to_RGB(points3D, object_sizes):\n    \"\"\"Transforms points3D in object frame to RGB color space.\n    # Arguments\n        points3D: Array (num_points, 3). Points3D a\n        object_sizes: Array (3) indicating the\n            (width, height, depth) of object.\n\n    # Returns\n        Array of ints (num_points, 3) in RGB space.\n    \"\"\"\n    # TODO add domain and codomain transform as comments\n    colors = points3D / (0.5 * object_sizes)\n    colors = colors + 1.0\n    colors = colors * 127.5\n    colors = colors.astype(np.uint8)\n    return colors",
  "def draw_RGB_mask(image, points2D, points3D, object_sizes):\n    \"\"\"Draws RGB mask by transforming points3D to RGB space and putting in\n        them in their 2D coordinates (points2D)\n\n    # Arguments\n        image: Array (H, W, 3).\n        points2D: Array (num_points, 2)\n        points3D: Array (num_points, 3)\n        object_sizes: Array (x_size, y_size, z_size)\n\n    # Returns\n        Image array with drawn masks\n    \"\"\"\n    color = points3D_to_RGB(points3D, object_sizes)\n    image = draw_points2D(image, points2D, color)\n    return image",
  "def draw_RGB_masks(image, points2D, points3D, object_sizes):\n    \"\"\"Draws RGB masks by transforming points3D to RGB space and putting in\n        them in their 2D coordinates (points2D)\n\n    # Arguments\n        image: Array (H, W, 3).\n        points2D: Array (num_samples, num_points, 2)\n        points3D: Array (num_samples, num_points, 3)\n        object_sizes: Array (x_size, y_size, z_size)\n\n    # Returns\n        Image array with drawn masks\n    \"\"\"\n    for instance_points2D, instance_points3D in zip(points2D, points3D):\n        image = draw_RGB_mask(\n            image, instance_points2D, instance_points3D, object_sizes)\n    return image",
  "def draw_human_pose6D(image, rotation, translation, camaera_intrinsics):\n    \"\"\"Draw basis vectors for human pose 6D\n\n    # Arguments\n        image: numpy array\n        rotation: numpy array of size (3 x 3)\n        translations: list of length 3\n        camera_matrix: numpy array\n\n    # Returns\n        image: numpy array\n               image with basis vectors of rotaion and translation.\n    \"\"\"\n    points3D = np.array([[1, 0, 0],\n                         [0, 1, 0],\n                         [0, 0, 1]])\n    points2D = project_to_image(rotation, translation,\n                                points3D, camaera_intrinsics)\n    points2D = points2D.astype(np.int32)\n\n    x = points2D[0]\n    y = points2D[1]\n    z = points2D[2]\n\n    x_hat = (x / np.linalg.norm(x) * 60).astype(np.int32)\n    y_hat = (y / np.linalg.norm(y) * 60).astype(np.int32)\n    z_hat = (z / np.linalg.norm(z) * 60).astype(np.int32)\n\n    offset = [50, 50]\n    image = draw_line(image, offset, x_hat + offset,\n                      color=[255, 0, 0], thickness=4)\n    image = draw_line(image, offset, y_hat + offset,\n                      color=[0, 255, 0], thickness=4)\n    image = draw_line(image, offset, z_hat + offset,\n                      color=[0, 0, 255], thickness=4)\n    return image",
  "def cast_image(image, dtype):\n    return tf.image.convert_image_dtype(image, dtype)",
  "def load_image(filepath, num_channels=3):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_image(image, num_channels, expand_animations=False)\n    return image",
  "def resize(image, size):\n    return tf.image.resize(image, size)",
  "def random_saturation(image, upper, lower):\n    return tf.image.random_saturation(image, lower, upper)",
  "def random_brightness(image, max_delta):\n    return tf.image.random_brightness(image, max_delta)",
  "def random_contrast(image, lower, upper):\n    return tf.image.random_contrast(image, lower, upper)",
  "def random_hue(image, max_delta):\n    return tf.image.random_hue(image, max_delta)",
  "def random_image_quality(image, lower, upper):\n    return tf.image.random_jpeg_quality(image, lower, upper)",
  "def _RGB_to_grayscale(image):\n    return tf.image.rgb_to_grayscale(image)",
  "def _RGB_to_HSV(image):\n    return tf.image.rgb_to_hsv(image)",
  "def _HSV_to_RGB(image):\n    return tf.image.hsv_to_rgb(image)",
  "def _reverse_channels(image):\n    channels = tf.unstack(image, axis=-1)\n    image = tf.stack([channels[2], channels[1], channels[0]], axis=-1)\n    return image",
  "def convert_color_space(image, flag):\n    if flag == RGB2BGR:\n        image = _reverse_channels(image)\n\n    elif flag == BGR2RGB:\n        image = _reverse_channels(image)\n\n    elif flag == RGB2GRAY:\n        image = _RGB_to_grayscale(image)\n\n    elif flag == RGB2HSV:\n        image = _RGB_to_HSV(image)\n\n    elif flag == HSV2RGB:\n        image = _HSV_to_RGB(image)\n\n    elif flag == RGB2HSV:\n        image = _RGB_to_HSV(image)\n\n    else:\n        raise ValueError('Invalid flag transformation:', flag)\n\n    return image",
  "def random_crop(image, size):\n    return tf.image.random_crop(image, size)",
  "def split_alpha_channel(image):\n    if image.shape[-1] != 4:\n        raise ValueError('Provided image does not contain alpha mask.')\n    image, alpha_channel = tf.split(image, [3], -1)\n    alpha_channel = alpha_channel / 255.0\n    return image, alpha_channel",
  "def alpha_blend(foreground, background, alpha_channel):\n    return (alpha_channel * foreground) + ((1.0 - alpha_channel) * background)",
  "def random_plain_background(image):\n    \"\"\"Adds random plain background to image using a normalized alpha channel\n    # Arguments\n        image: Float array-like with shape (H, W, 4).\n        alpha_channel: Float array-like. Normalized alpha channel for blending.\n    \"\"\"\n    image, alpha_channel = split_alpha_channel(image)\n    random_color = tf.random.uniform([3], 0, 255)\n    random_color = tf.reshape(random_color, [1, 1, 3])\n    H, W = image.shape[:2]\n    background = tf.tile(random_color, [H, W, 1])\n    return alpha_blend(image, background, alpha_channel)",
  "def random_cropped_background(image, background):\n    image, alpha_channel = split_alpha_channel(image)\n    background = random_crop(background, size=image.shape)\n    return alpha_blend(image, background, alpha_channel)",
  "def flip_left_right(image):\n    return tf.image.flip_left_right(image)",
  "def random_flip_left_right(image):\n    if tf.random.uniform([1], 0, 2) == 1:\n        image = flip_left_right(image)\n    return image",
  "def imagenet_preprocess_input(image, data_format=None, mode='torch'):\n    image = tf.keras.applications.imagenet_utils.preprocess_input(image,\n                                                                  data_format,\n                                                                  mode)\n    return image",
  "def rotation_vector_to_rotation_matrix(rotation_vector):\n    \"\"\"Transforms rotation vector (axis-angle) form to rotation matrix.\n\n    # Arguments\n        rotation_vector: Array (3). Rotation vector in axis-angle form.\n\n    # Returns\n        Array (3, 3) rotation matrix.\n    \"\"\"\n    rotation_matrix = np.eye(3)\n    cv2.Rodrigues(rotation_vector, rotation_matrix)\n    return rotation_matrix",
  "def build_rotation_matrix_z(angle):\n    \"\"\"Builds rotation matrix in Z axis.\n\n    # Arguments\n        angle: Float. Angle in radians.\n\n    # Return\n        Array (3, 3) rotation matrix in Z axis.\n    \"\"\"\n    cos_angle = np.cos(angle)\n    sin_angle = np.sin(angle)\n    rotation_matrix_z = np.array([[+cos_angle, -sin_angle, 0.0],\n                                  [+sin_angle, +cos_angle, 0.0],\n                                  [0.0, 0.0, 1.0]])\n    return rotation_matrix_z",
  "def build_rotation_matrix_x(angle):\n    \"\"\"Builds rotation matrix in X axis.\n\n    # Arguments\n        angle: Float. Angle in radians.\n\n    # Return\n        Array (3, 3) rotation matrix in Z axis.\n    \"\"\"\n    cos_angle = np.cos(angle)\n    sin_angle = np.sin(angle)\n    rotation_matrix_x = np.array([[1.0, 0.0, 0.0],\n                                  [0.0, +cos_angle, -sin_angle],\n                                  [0.0, +sin_angle, +cos_angle]])\n    return rotation_matrix_x",
  "def build_rotation_matrix_y(angle):\n    \"\"\"Builds rotation matrix in Y axis.\n\n    # Arguments\n        angle: Float. Angle in radians.\n\n    # Return\n        Array (3, 3) rotation matrix in Z axis.\n    \"\"\"\n    cos_angle = np.cos(angle)\n    sin_angle = np.sin(angle)\n    rotation_matrix_y = np.array([[+cos_angle, 0.0, +sin_angle],\n                                  [0.0, 1.0, 0.0],\n                                  [-sin_angle, 0.0, +cos_angle]])\n    return rotation_matrix_y",
  "def compute_norm_SO3(rotation_mesh, rotation):\n    \"\"\"Computes norm between SO3 elements.\n\n    # Arguments\n        rotation_mesh: Array (3, 3), rotation matrix.\n        rotation: Array (3, 3), rotation matrix.\n\n    # Returns\n        Scalar representing the distance between both rotation matrices.\n    \"\"\"\n    difference = np.dot(np.linalg.inv(rotation), rotation_mesh) - np.eye(3)\n    distance = np.linalg.norm(difference, ord='fro')\n    return distance",
  "def calculate_canonical_rotation(rotation_mesh, rotations):\n    \"\"\"Returns the rotation matrix closest to rotation mesh.\n\n    # Arguments\n        rotation_mesh: Array (3, 3), rotation matrix.\n        rotations: List of array of (3, 3), rotation matrices.\n\n    # Returns\n        Element of list closest to rotation mesh.\n    \"\"\"\n    norms = [compute_norm_SO3(rotation_mesh, R) for R in rotations]\n    closest_rotation_arg = np.argmin(norms)\n    closest_rotation = rotations[closest_rotation_arg]\n    canonical_rotation = np.linalg.inv(closest_rotation)\n    return canonical_rotation",
  "def rotation_matrix_to_axis_angle(rotation_matrix):\n    \"\"\"Transforms rotation matrix to axis angle.\n\n    # Arguments\n        Rotation matrix [3, 3].\n\n    # Returns\n        axis_angle: Array containing axis angle represent [wx, wy, wz, theta].\n    \"\"\"\n    cos_theta = (np.trace(rotation_matrix) - 1.0) / 2.0\n    angle = np.arccos(cos_theta)\n    axis = np.array([rotation_matrix[2, 1] - rotation_matrix[1, 2],\n                     rotation_matrix[0, 2] - rotation_matrix[2, 0],\n                     rotation_matrix[1, 0] - rotation_matrix[0, 1]])\n    axis = axis / np.linalg.norm(axis)\n    axis_angle = np.hstack([axis, angle])\n    return axis_angle",
  "def rotation_matrix_to_compact_axis_angle(matrix):\n    \"\"\"Transforms rotation matrix to compact axis angle.\n\n    # Arguments\n        Rotation matrix [3, 3].\n\n    # Returns\n        compact axis_angle\n    \"\"\"\n    axis_angle = rotation_matrix_to_axis_angle(matrix)\n    axis = axis_angle[:3]\n    angle = axis_angle[3]\n    compact_axis_angle = axis * angle\n    return compact_axis_angle",
  "def to_affine_matrix(rotation_matrix, translation):\n    \"\"\"Builds affine matrix from rotation matrix and translation vector.\n\n    # Arguments\n        rotation_matrix: Array (3, 3). Representing a rotation matrix.\n        translation: Array (3). Translation vector.\n\n    # Returns\n        Array (4, 4) representing an affine matrix.\n    \"\"\"\n    if len(translation) != 3:\n        raise ValueError('Translation should be of lenght 3')\n    if rotation_matrix.shape != (3, 3):\n        raise ValueError('Rotation matrix should be of shape (3, 3)')\n    translation = translation.reshape(3, 1)\n    affine_top = np.concatenate([rotation_matrix, translation], axis=1)\n    affine_row = np.array([[0.0, 0.0, 0.0, 1.0]])\n    affine_matrix = np.concatenate([affine_top, affine_row], axis=0)\n    return affine_matrix",
  "def to_affine_matrices(rotations, translations):\n    \"\"\"Construct affine matrices for rotation matrices vector and\n       translation vector.\n\n    # Arguments\n        ratations: Rotation matrix vector [N, 3, 3].\n        translations: Translation vector [N, 3].\n\n    # Returns\n        Transformation matrix [N, 4, 4]\n    \"\"\"\n    affine_matrices = []\n    for rotation, translation in zip(rotations, translations):\n        transformation = to_affine_matrix(rotation, translation)\n        affine_matrices.append(transformation)\n    return np.array(affine_matrices)",
  "def rotation_vector_to_quaternion(rotation_vector):\n    \"\"\"Transforms rotation vector into quaternion.\n\n    # Arguments\n        rotation_vector: Numpy array of shape ``[3]``.\n\n    # Returns\n        Numpy array representing a quaternion having a shape ``[4]``.\n    \"\"\"\n    theta = np.linalg.norm(rotation_vector)\n    rotation_axis = rotation_vector / theta\n    half_theta = 0.5 * theta\n    norm = np.sin(half_theta)\n    quaternion = np.array([\n        norm * rotation_axis[0],\n        norm * rotation_axis[1],\n        norm * rotation_axis[2],\n        np.cos(half_theta)])\n    return quaternion",
  "def homogenous_quaternion_to_rotation_matrix(quaternion):\n    \"\"\"Transforms quaternion to rotation matrix.\n\n    # Arguments\n        quaternion: Array containing quaternion value [q1, q2, q3, w0].\n\n    # Returns\n        Rotation matrix [3, 3].\n\n    # Note\n        If quaternion is not a unit quaternion the rotation matrix is not\n        unitary but still orthogonal i.e. the outputted rotation matrix is\n        a scalar multiple of a rotation matrix.\n    \"\"\"\n    q1, q2, q3, w0 = quaternion\n\n    r11 = w0**2 + q1**2 - q2**2 - q3**2\n    r12 = 2 * ((q1 * q2) - (w0 * q3))\n    r13 = 2 * ((w0 * q2) + (q1 * q3))\n\n    r21 = 2 * ((w0 * q3) + (q1 * q2))\n    r22 = w0**2 - q1**2 + q2**2 - q3**2\n    r23 = 2 * ((q2 * q3) - (w0 * q1))\n\n    r31 = 2 * ((q1 * q3) - (w0 * q2))\n    r32 = 2 * ((w0 * q1) + (q2 * q3))\n    r33 = w0**2 - q1**2 - q2**2 + q3**2\n\n    rotation_matrix = np.array([[r11, r12, r13],\n                                [r21, r22, r23],\n                                [r31, r32, r33]])\n    return rotation_matrix",
  "def quaternion_to_rotation_matrix(quaternion):\n    \"\"\"Transforms quaternion to rotation matrix.\n\n    # Arguments\n        quaternion: Array containing quaternion value [q1, q2, q3, w0].\n\n    # Returns\n        Rotation matrix [3, 3].\n\n    # Note\n        \"If the quaternion \"is not a unit quaternion then the homogeneous form\n        is still a scalar multiple of a rotation matrix, while the\n        inhomogeneous form is in general no longer an orthogonal matrix.\n        This is why in numerical work the homogeneous form is to be preferred\n        if distortion is to be avoided.\" [wikipedia](https://en.wikipedia.org/\n            wiki/Conversion_between_quaternions_and_Euler_angles)\n    \"\"\"\n    matrix = homogenous_quaternion_to_rotation_matrix(quaternion)\n    return matrix",
  "def rotation_matrix_to_quaternion(rotation_matrix):\n    \"\"\"Transforms rotation matrix to quaternion.\n\n    # Arguments\n        Rotation matrix [3, 3].\n\n    # Returns\n        quaternion: Array containing quaternion value [q1, q2, q3, w0].\n    \"\"\"\n    rotation_matrix = rotation_matrix[:3, :3]\n    trace = np.trace(rotation_matrix)\n    w0 = np.sqrt(1.0 + trace) / 2\n    q1 = 0.25 * (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / w0\n    q2 = 0.25 * (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / w0\n    q3 = 0.25 * (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / w0\n    quaternion = np.array([q1, q2, q3, w0])\n    return quaternion",
  "def get_quaternion_conjugate(quaternion):\n    \"\"\"Estimate conjugate of a quaternion.\n\n    # Arguments\n        quaternion: Array containing quaternion value [q1, q2, q3, w0].\n\n    # Returns\n        quaternion: Array containing quaternion value [-q1, -q2, -q3, w0].\n    \"\"\"\n    q1, q2, q3, w0 = quaternion\n    return np.array([-q1, -q2, -q3, w0])",
  "def quaternions_to_rotation_matrices(quaternions):\n    \"\"\"Transform quaternion vectors to rotation matrix vector.\n\n    # Arguments\n        quaternions [N, 4].\n\n    # Returns\n        Rotated matrices [N, 3, 3]\n    \"\"\"\n    rotation_matrices = []\n    for quaternion in quaternions:\n        rotation_matrix = quaternion_to_rotation_matrix(quaternion)\n        rotation_matrices.append(rotation_matrix)\n    return np.array(rotation_matrices)",
  "class SolvePNP(Processor):\n    \"\"\"Calculates 6D pose from 3D points and 2D keypoints correspondences.\n\n    # Arguments\n        model_points: Numpy array of shape ``[num_points, 3]``.\n            Model 3D points known in advance.\n        camera: Instance of ''paz.backend.Camera'' containing as properties\n            the ``camera_intrinsics`` a Numpy array of shape ``[3, 3]``\n            usually calculated from the openCV ``calibrateCamera`` function,\n            and the ``distortion`` a Numpy array of shape ``[5]`` in which the\n            elements are usually obtained from the openCV\n            ``calibrateCamera`` function.\n        solver: Flag specifying solvers. Current solvers are:\n            ``paz.processors.LEVENBERG_MARQUARDT`` and ``paz.processors.UPNP``.\n\n    # Returns\n        Instance from ``Pose6D`` message.\n    \"\"\"\n    def __init__(self, points3D, camera, solver=LEVENBERG_MARQUARDT):\n        super(SolvePNP, self).__init__()\n        self.points3D = points3D\n        self.camera = camera\n        self.solver = solver\n        self.num_keypoints = len(points3D)\n\n    def call(self, keypoints):\n        keypoints = keypoints[:, :2]\n        keypoints = keypoints.astype(np.float64)\n        keypoints = keypoints.reshape((self.num_keypoints, 1, 2))\n\n        (success, rotation, translation) = solve_PNP(\n            self.points3D, keypoints, self.camera, self.solver)\n\n        return Pose6D.from_rotation_vector(rotation, translation)",
  "class SolveChangingObjectPnPRANSAC(Processor):\n    \"\"\"Returns rotation (Roc) and translation (Toc) vectors that transform\n        3D points in object frame to camera frame.\n\n                               O------------O\n                              /|           /|\n                             / |          / |\n                            O------------O  |\n                            |  |    z    |  |\n                            |  O____|____|__O\n                            |  /    |___y|  /   object\n                            | /    /     | /  coordinates\n                            |/    x      |/\n                            O------------O\n                                   ___\n                   Z                |\n                  /                 | Rco, Tco\n                 /_____X     <------|\n                 |\n                 |    camera\n                 Y  coordinates\n\n    # Arguments\n        object_points3D: Array (num_points, 3). Points 3D in object reference\n            frame. Represented as (0) in image above.\n        image_points2D: Array (num_points, 2). Points in 2D in camera UV space.\n        camera_intrinsics: Array of shape (3, 3). Diagonal elements represent\n            focal lenghts and last column the image center translation.\n        inlier_threshold: Number of inliers for RANSAC method.\n        num_iterations: Maximum number of iterations.\n\n    # Returns\n        Boolean indicating success, rotation vector in axis-angle form (3)\n            and translation vector (3).\n    \"\"\"\n\n    def __init__(self, camera_intrinsics, inlier_thresh=5, num_iterations=100):\n        super(SolveChangingObjectPnPRANSAC, self).__init__()\n        self.camera_intrinsics = camera_intrinsics\n        self.inlier_thresh = inlier_thresh\n        self.num_iterations = num_iterations\n        self.MIN_REQUIRED_POINTS = 4\n\n    def call(self, object_points3D, image_points2D):\n        success, rotation_vector, translation = solve_PnP_RANSAC(\n            object_points3D, image_points2D, self.camera_intrinsics,\n            self.inlier_thresh, self.num_iterations)\n        rotation_vector = np.squeeze(rotation_vector)\n        return success, rotation_vector, translation",
  "class Translation3DFromBoxWidth(Processor):\n    \"\"\"Computes 3D translation from box width and real width ratio.\n\n    # Arguments\n        camera: Instance of ''paz.backend.Camera'' containing as properties\n            the ``camera_intrinsics`` a Numpy array of shape ``[3, 3]``\n            usually calculated from the openCV ``calibrateCamera`` function,\n            and the ``distortion`` a Numpy array of shape ``[5]`` in which the\n            elements are usually obtained from the openCV\n            ``calibrateCamera`` function.\n        real_width: Real width of the predicted box2D.\n\n    # Returns\n        Array (num_boxes, 3) containing all 3D translations.\n    \"\"\"\n    def __init__(self, camera, real_width=0.3):\n        super(Translation3DFromBoxWidth, self).__init__()\n        self.camera = camera\n        self.real_width = real_width\n        self.focal_length = self.camera.intrinsics[0, 0]\n        self.u_camera_center = self.camera.intrinsics[0, 2]\n        self.v_camera_center = self.camera.intrinsics[1, 2]\n\n    def call(self, boxes2D):\n        hands_center = []\n        for box in boxes2D:\n            u_box_center, v_box_center = box.center\n            z_center = (self.real_width * self.focal_length) / box.width\n            u = u_box_center - self.u_camera_center\n            v = v_box_center - self.v_camera_center\n            x_center = (z_center * u) / self.focal_length\n            y_center = (z_center * v) / self.focal_length\n            hands_center.append([x_center, y_center, z_center])\n        return np.array(hands_center)",
  "def __init__(self, points3D, camera, solver=LEVENBERG_MARQUARDT):\n        super(SolvePNP, self).__init__()\n        self.points3D = points3D\n        self.camera = camera\n        self.solver = solver\n        self.num_keypoints = len(points3D)",
  "def call(self, keypoints):\n        keypoints = keypoints[:, :2]\n        keypoints = keypoints.astype(np.float64)\n        keypoints = keypoints.reshape((self.num_keypoints, 1, 2))\n\n        (success, rotation, translation) = solve_PNP(\n            self.points3D, keypoints, self.camera, self.solver)\n\n        return Pose6D.from_rotation_vector(rotation, translation)",
  "def __init__(self, camera_intrinsics, inlier_thresh=5, num_iterations=100):\n        super(SolveChangingObjectPnPRANSAC, self).__init__()\n        self.camera_intrinsics = camera_intrinsics\n        self.inlier_thresh = inlier_thresh\n        self.num_iterations = num_iterations\n        self.MIN_REQUIRED_POINTS = 4",
  "def call(self, object_points3D, image_points2D):\n        success, rotation_vector, translation = solve_PnP_RANSAC(\n            object_points3D, image_points2D, self.camera_intrinsics,\n            self.inlier_thresh, self.num_iterations)\n        rotation_vector = np.squeeze(rotation_vector)\n        return success, rotation_vector, translation",
  "def __init__(self, camera, real_width=0.3):\n        super(Translation3DFromBoxWidth, self).__init__()\n        self.camera = camera\n        self.real_width = real_width\n        self.focal_length = self.camera.intrinsics[0, 0]\n        self.u_camera_center = self.camera.intrinsics[0, 2]\n        self.v_camera_center = self.camera.intrinsics[1, 2]",
  "def call(self, boxes2D):\n        hands_center = []\n        for box in boxes2D:\n            u_box_center, v_box_center = box.center\n            z_center = (self.real_width * self.focal_length) / box.width\n            u = u_box_center - self.u_camera_center\n            v = v_box_center - self.v_camera_center\n            x_center = (z_center * u) / self.focal_length\n            y_center = (z_center * v) / self.focal_length\n            hands_center.append([x_center, y_center, z_center])\n        return np.array(hands_center)",
  "class CastImage(Processor):\n    \"\"\"Cast image to given dtype.\n\n    # Arguments\n        dtype: Str or np.dtype\n    \"\"\"\n    def __init__(self, dtype):\n        self.dtype = dtype\n        super(CastImage, self).__init__()\n\n    def call(self, image):\n        return cast_image(image, self.dtype)",
  "class SubtractMeanImage(Processor):\n    \"\"\"Subtract channel-wise mean to image.\n\n    # Arguments\n        mean: List of length 3, containing the channel-wise mean.\n    \"\"\"\n    def __init__(self, mean):\n        self.mean = mean\n        super(SubtractMeanImage, self).__init__()\n\n    def call(self, image):\n        return image - self.mean",
  "class AddMeanImage(Processor):\n    \"\"\"Adds channel-wise mean to image.\n\n    # Arguments\n        mean: List of length 3, containing the channel-wise mean.\n    \"\"\"\n    def __init__(self, mean):\n        self.mean = mean\n        super(AddMeanImage, self).__init__()\n\n    def call(self, image):\n        return image + self.mean",
  "class NormalizeImage(Processor):\n    \"\"\"Normalize image by diving all values by 255.0.\n    \"\"\"\n    def __init__(self):\n        super(NormalizeImage, self).__init__()\n\n    def call(self, image):\n        return image / 255.0",
  "class DenormalizeImage(Processor):\n    \"\"\"Denormalize image by multiplying all values by 255.0.\n    \"\"\"\n    def __init__(self):\n        super(DenormalizeImage, self).__init__()\n\n    def call(self, image):\n        return image * 255.0",
  "class LoadImage(Processor):\n    \"\"\"Loads image.\n\n    # Arguments\n        num_channels: Integer, valid integers are: 1, 3 and 4.\n    \"\"\"\n    def __init__(self, num_channels=3):\n        self.num_channels = num_channels\n        super(LoadImage, self).__init__()\n\n    def call(self, image):\n        return load_image(image, self.num_channels)",
  "class RandomSaturation(Processor):\n    \"\"\"Applies random saturation to an image in RGB space.\n\n    # Arguments\n        lower: Float, lower bound for saturation factor.\n        upper: Float, upper bound for saturation factor.\n    \"\"\"\n    def __init__(self, lower=0.3, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        super(RandomSaturation, self).__init__()\n\n    def call(self, image):\n        return random_saturation(image, self.lower, self.upper)",
  "class RandomBrightness(Processor):\n    \"\"\"Adjust random brightness to an image in RGB space.\n\n    # Arguments\n        max_delta: Float.\n    \"\"\"\n    def __init__(self, delta=32):\n        self.delta = delta\n        super(RandomBrightness, self).__init__()\n\n    def call(self, image):\n        return random_brightness(image, self.delta)",
  "class RandomContrast(Processor):\n    \"\"\"Applies random contrast to an image in RGB\n\n    # Arguments\n        lower: Float, indicating the lower bound of the random number\n            to be multiplied with the BGR/RGB image.\n        upper: Float, indicating the upper bound of the random number\n        to be multiplied with the BGR/RGB image.\n    \"\"\"\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        super(RandomContrast, self).__init__()\n\n    def call(self, image):\n        return random_contrast(image, self.lower, self.upper)",
  "class RandomHue(Processor):\n    \"\"\"Applies random hue to an image in RGB space.\n\n    # Arguments\n        delta: Int, indicating the range (-delta, delta ) of possible\n            hue values.\n    \"\"\"\n    def __init__(self, delta=18):\n        self.delta = delta\n        super(RandomHue, self).__init__()\n\n    def call(self, image):\n        return random_hue(image, self.delta)",
  "class ResizeImage(Processor):\n    \"\"\"Resize image.\n\n    # Arguments\n        size: List of two ints.\n    \"\"\"\n    def __init__(self, shape, method=BILINEAR):\n        self.shape = shape\n        self.method = method\n        super(ResizeImage, self).__init__()\n\n    def call(self, image):\n        return resize_image(image, self.shape, self.method)",
  "class ResizeImages(Processor):\n    \"\"\"Resize list of images.\n\n    # Arguments\n        size: List of two ints.\n    \"\"\"\n    def __init__(self, shape):\n        self.shape = shape\n        super(ResizeImages, self).__init__()\n\n    def call(self, images):\n        return [resize_image(image, self.shape) for image in images]",
  "class RandomImageBlur(Processor):\n    \"\"\"Randomizes image quality\n\n    # Arguments\n        probability: Float between [0, 1]. Assigns probability of how\n            often a random image blur is applied.\n    \"\"\"\n    def __init__(self, probability=0.5):\n        super(RandomImageBlur, self).__init__()\n        self.probability = probability\n\n    def call(self, image):\n        if self.probability >= np.random.rand():\n            image = random_image_blur(image)\n        return image",
  "class RandomGaussianBlur(Processor):\n    \"\"\"Randomizes image quality\n\n    # Arguments\n        probability: Float between [0, 1]. Assigns probability of how\n            often a random image blur is applied.\n    \"\"\"\n    def __init__(self, kernel_size=(5, 5), probability=0.5):\n        super(RandomGaussianBlur, self).__init__()\n        self.kernel_size = kernel_size\n        self.probability = probability\n\n    def call(self, image):\n        if self.probability >= np.random.rand():\n            image = gaussian_image_blur(image, self.kernel_size)\n        return image",
  "class RandomFlipImageLeftRight(Processor):\n    \"\"\"Randomly flip the image left or right\n    \"\"\"\n    def __init__(self):\n        super(RandomFlipImageLeftRight, self).__init__()\n\n    def call(self, image):\n        return random_flip_left_right(image)",
  "class ConvertColorSpace(Processor):\n    \"\"\"Converts image to a different color space.\n\n    # Arguments\n        flag: Flag found in ``processors``indicating transform e.g.\n            ``pr.BGR2RGB``\n    \"\"\"\n    def __init__(self, flag):\n        self.flag = flag\n        super(ConvertColorSpace, self).__init__()\n\n    def call(self, image):\n        return convert_color_space(image, self.flag)",
  "class ShowImage(Processor):\n    \"\"\"Shows image in a separate window.\n\n    # Arguments\n        window_name: String. Window name.\n        wait: Boolean\n    \"\"\"\n    def __init__(self, window_name='image', wait=True):\n        super(ShowImage, self).__init__()\n        self.window_name = window_name\n        self.wait = wait\n\n    def call(self, image):\n        return show_image(image, self.window_name, self.wait)",
  "class ImageDataProcessor(Processor):\n    \"\"\"Wrapper for Keras ImageDataGenerator\n\n    # Arguments\n        generator: An instantiated Keras ImageDataGenerator\n    \"\"\"\n    def __init__(self, generator):\n        super(ImageDataProcessor, self).__init__()\n        self.generator = generator\n\n    def call(self, image):\n        random_parameters = self.generator.get_random_transform(image.shape)\n        image = self.generator.apply_transform(image, random_parameters)\n        image = self.generator.standardize(image)\n        return image",
  "class AlphaBlending(Processor):\n    \"\"\"Blends image to background using the image's alpha channel.\n    \"\"\"\n    def __init__(self):\n        super(AlphaBlending, self).__init__()\n\n    def call(self, image, background):\n        return blend_alpha_channel(image, background)",
  "class RandomShapeCrop(Processor):\n    \"\"\"Randomly crops a part of an image of always the same given ``shape``.\n\n    # Arguments\n        shape: List of two ints [height, width].\n            Dimensions of image to be cropped.\n    \"\"\"\n    def __init__(self, shape):\n        super(RandomShapeCrop, self).__init__()\n        self.shape = shape\n\n    def call(self, image):\n        return random_shape_crop(image, self.shape)",
  "class MakeRandomPlainImage(Processor):\n    \"\"\"Makes random plain image by randomly sampling an RGB color.\n\n    # Arguments\n        shape: List of two ints [height, width].\n            Dimensions of plain image to be generated.\n    \"\"\"\n    def __init__(self, shape):\n        super(MakeRandomPlainImage, self).__init__()\n        self.shape = shape\n\n    def call(self):\n        return make_random_plain_image(self.shape)",
  "class ConcatenateAlphaMask(Processor):\n    \"\"\"Concatenates alpha mask to original image.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super(ConcatenateAlphaMask, self).__init__(**kwargs)\n\n    def call(self, image, alpha_mask):\n        return concatenate_alpha_mask(image, alpha_mask)",
  "class BlendRandomCroppedBackground(Processor):\n    \"\"\"Blends image with a randomly cropped background.\n\n    # Arguments\n        background_paths: List of strings. Each element of the list is a\n            full-path to an image used for cropping a background.\n    \"\"\"\n    def __init__(self, background_paths):\n        super(BlendRandomCroppedBackground, self).__init__()\n        if not isinstance(background_paths, list):\n            raise ValueError('``background_paths`` must be list')\n        if len(background_paths) == 0:\n            raise ValueError('No paths given in ``background_paths``')\n        self.background_paths = background_paths\n\n    def call(self, image):\n        random_arg = np.random.randint(0, len(self.background_paths))\n        background_path = self.background_paths[random_arg]\n        background = load_image(background_path)\n        background = random_shape_crop(background, image.shape[:2])\n        if background is None:\n            H, W, num_channels = image.shape\n            # background contains always a channel less\n            num_channels = num_channels - 1\n            background = make_random_plain_image((H, W, num_channels))\n        return blend_alpha_channel(image, background)",
  "class AddOcclusion(Processor):\n    \"\"\"Adds a random occlusion to image by generating random vertices and\n        drawing a polygon.\n\n    # Arguments\n        max_radius_scale: Float between [0, 1].\n            Value multiplied with largest image dimension to obtain the maximum\n                radius possible of a vertex in the occlusion polygon.\n        probability: Float between [0, 1]. Assigns probability of how\n            often an occlusion to an image is generated.\n    \"\"\"\n    def __init__(self, max_radius_scale=0.5, probability=0.5):\n        super(AddOcclusion, self).__init__()\n        self.max_radius_scale = max_radius_scale\n        self.probability = probability\n\n    def _random_vertices(self, center, max_radius, min_vertices, max_vertices):\n        num_vertices = np.random.randint(min_vertices, max_vertices)\n        angle_delta = 2 * np.pi / num_vertices\n        initial_angle = np.random.uniform(0, 2 * np.pi)\n        angles = initial_angle + np.arange(0, num_vertices) * angle_delta\n        x_component = np.cos(angles).reshape(-1, 1)\n        y_component = np.sin(angles).reshape(-1, 1)\n        vertices = np.concatenate([x_component, y_component], -1)\n        random_lengths = np.random.uniform(0, max_radius, num_vertices)\n        random_lengths = random_lengths.reshape(num_vertices, 1)\n        vertices = vertices * random_lengths\n        vertices = vertices + center\n        return vertices.astype(np.int32)\n\n    def add_occlusion(self, image, max_radius_scale):\n        height, width = image.shape[:2]\n        max_radius = np.max((height, width)) * max_radius_scale\n        center = np.random.rand(2) * np.array([width, height])\n        vertices = self._random_vertices(center, max_radius, 3, 7)\n        color = np.random.randint(0, 256, 3).tolist()\n        return draw_filled_polygon(image, vertices, color)\n\n    def call(self, image):\n        if self.probability >= np.random.rand():\n            image = self.add_occlusion(image, self.max_radius_scale)\n        return image",
  "class RandomImageCrop(Processor):\n    \"\"\"Crops randomly a rectangle from an image.\n\n    # Arguments\n        crop_factor: Float between ``[0, 1]``.\n        probability: Float between ``[0, 1]``.\n    \"\"\"\n    def __init__(self, crop_factor=0.3, probability=0.5):\n        self.crop_factor = crop_factor\n        self.probability = probability\n        super(RandomImageCrop, self).__init__()\n\n    def call(self, image):\n        if self.probability < np.random.rand():\n            return image\n        H, W = image.shape[:2]\n        W_crop = np.random.uniform(self.crop_factor * W, W)\n        H_crop = np.random.uniform(self.crop_factor * H, H)\n        x_min = np.random.uniform(W - W_crop)\n        y_min = np.random.uniform(H - H_crop)\n        x_max = x_min + W_crop\n        y_max = y_min + H_crop\n        cropped_image = image[int(x_min):int(x_max), int(y_min):int(y_max), :]\n        return cropped_image",
  "class ImageToNormalizedDeviceCoordinates(Processor):\n    \"\"\"Map image value from [0, 255] -> [-1, 1].\n    \"\"\"\n    def __init__(self):\n        super(ImageToNormalizedDeviceCoordinates, self).__init__()\n\n    def call(self, image):\n        return image_to_normalized_device_coordinates(image)",
  "class NormalizedDeviceCoordinatesToImage(Processor):\n    \"\"\"Map normalized value from [-1, 1] -> [0, 255].\n    \"\"\"\n    def __init__(self):\n        super(NormalizedDeviceCoordinatesToImage, self).__init__()\n\n    def call(self, image):\n        return normalized_device_coordinates_to_image(image)",
  "class ReplaceLowerThanThreshold(Processor):\n    def __init__(self, threshold=1e-8, replacement=0.0):\n        super(ReplaceLowerThanThreshold, self).__init__()\n        self.threshold = threshold\n        self.replacement = replacement\n\n    def call(self, values):\n        return replace_lower_than_threshold(\n            values, self.threshold, self.replacement)",
  "class GetNonZeroValues(Processor):\n    def __init__(self):\n        super(GetNonZeroValues, self).__init__()\n\n    def call(self, array):\n        channel_wise_sum = np.sum(array, axis=2)\n        non_zero_arguments = np.nonzero(channel_wise_sum)\n        return array[non_zero_arguments]",
  "class GetNonZeroArguments(Processor):\n    def __init__(self):\n        super(GetNonZeroArguments, self).__init__()\n\n    def call(self, array):\n        channel_wise_sum = np.sum(array, axis=2)\n        non_zero_rows, non_zero_columns = np.nonzero(channel_wise_sum)\n        return non_zero_rows, non_zero_columns",
  "class ImagenetPreprocessInput(Processor):\n    def __init__(self):\n        super(ImagenetPreprocessInput, self).__init__()\n\n    def call(self, image):\n        return imagenet_preprocess_input(image)",
  "class FlipLeftRightImage(Processor):\n    \"\"\"Flips an image left and right.\n\n    # Arguments\n        image: Numpy array.\n    \"\"\"\n    def __init__(self):\n        super(FlipLeftRightImage, self).__init__()\n\n    def call(self, image):\n        return flip_left_right(image)",
  "class DivideStandardDeviationImage(Processor):\n    \"\"\"Divide channel-wise standard deviation to image.\n\n    # Arguments\n        standard_deviation: List of length 3, containing the\n            channel-wise standard deviation.\n\n    # Properties\n        standard_deviation: List.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self, standard_deviation):\n        self.standard_deviation = standard_deviation\n        super(DivideStandardDeviationImage, self).__init__()\n\n    def call(self, image):\n        return image / self.standard_deviation",
  "class ScaledResize(Processor):\n    \"\"\"Resizes image by returning the scales to original image.\n\n    # Arguments\n        image_size: Int, desired size of the model input.\n\n    # Properties\n        image_size: Int.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self, image_size):\n        self.image_size = image_size\n        super(ScaledResize, self).__init__()\n\n    def call(self, image):\n        \"\"\"\n        # Arguments\n            image: Array, raw input image.\n        \"\"\"\n        output_image, image_scale = scale_resize(image, self.image_size)\n        return output_image, image_scale",
  "def __init__(self, dtype):\n        self.dtype = dtype\n        super(CastImage, self).__init__()",
  "def call(self, image):\n        return cast_image(image, self.dtype)",
  "def __init__(self, mean):\n        self.mean = mean\n        super(SubtractMeanImage, self).__init__()",
  "def call(self, image):\n        return image - self.mean",
  "def __init__(self, mean):\n        self.mean = mean\n        super(AddMeanImage, self).__init__()",
  "def call(self, image):\n        return image + self.mean",
  "def __init__(self):\n        super(NormalizeImage, self).__init__()",
  "def call(self, image):\n        return image / 255.0",
  "def __init__(self):\n        super(DenormalizeImage, self).__init__()",
  "def call(self, image):\n        return image * 255.0",
  "def __init__(self, num_channels=3):\n        self.num_channels = num_channels\n        super(LoadImage, self).__init__()",
  "def call(self, image):\n        return load_image(image, self.num_channels)",
  "def __init__(self, lower=0.3, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        super(RandomSaturation, self).__init__()",
  "def call(self, image):\n        return random_saturation(image, self.lower, self.upper)",
  "def __init__(self, delta=32):\n        self.delta = delta\n        super(RandomBrightness, self).__init__()",
  "def call(self, image):\n        return random_brightness(image, self.delta)",
  "def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        super(RandomContrast, self).__init__()",
  "def call(self, image):\n        return random_contrast(image, self.lower, self.upper)",
  "def __init__(self, delta=18):\n        self.delta = delta\n        super(RandomHue, self).__init__()",
  "def call(self, image):\n        return random_hue(image, self.delta)",
  "def __init__(self, shape, method=BILINEAR):\n        self.shape = shape\n        self.method = method\n        super(ResizeImage, self).__init__()",
  "def call(self, image):\n        return resize_image(image, self.shape, self.method)",
  "def __init__(self, shape):\n        self.shape = shape\n        super(ResizeImages, self).__init__()",
  "def call(self, images):\n        return [resize_image(image, self.shape) for image in images]",
  "def __init__(self, probability=0.5):\n        super(RandomImageBlur, self).__init__()\n        self.probability = probability",
  "def call(self, image):\n        if self.probability >= np.random.rand():\n            image = random_image_blur(image)\n        return image",
  "def __init__(self, kernel_size=(5, 5), probability=0.5):\n        super(RandomGaussianBlur, self).__init__()\n        self.kernel_size = kernel_size\n        self.probability = probability",
  "def call(self, image):\n        if self.probability >= np.random.rand():\n            image = gaussian_image_blur(image, self.kernel_size)\n        return image",
  "def __init__(self):\n        super(RandomFlipImageLeftRight, self).__init__()",
  "def call(self, image):\n        return random_flip_left_right(image)",
  "def __init__(self, flag):\n        self.flag = flag\n        super(ConvertColorSpace, self).__init__()",
  "def call(self, image):\n        return convert_color_space(image, self.flag)",
  "def __init__(self, window_name='image', wait=True):\n        super(ShowImage, self).__init__()\n        self.window_name = window_name\n        self.wait = wait",
  "def call(self, image):\n        return show_image(image, self.window_name, self.wait)",
  "def __init__(self, generator):\n        super(ImageDataProcessor, self).__init__()\n        self.generator = generator",
  "def call(self, image):\n        random_parameters = self.generator.get_random_transform(image.shape)\n        image = self.generator.apply_transform(image, random_parameters)\n        image = self.generator.standardize(image)\n        return image",
  "def __init__(self):\n        super(AlphaBlending, self).__init__()",
  "def call(self, image, background):\n        return blend_alpha_channel(image, background)",
  "def __init__(self, shape):\n        super(RandomShapeCrop, self).__init__()\n        self.shape = shape",
  "def call(self, image):\n        return random_shape_crop(image, self.shape)",
  "def __init__(self, shape):\n        super(MakeRandomPlainImage, self).__init__()\n        self.shape = shape",
  "def call(self):\n        return make_random_plain_image(self.shape)",
  "def __init__(self, **kwargs):\n        super(ConcatenateAlphaMask, self).__init__(**kwargs)",
  "def call(self, image, alpha_mask):\n        return concatenate_alpha_mask(image, alpha_mask)",
  "def __init__(self, background_paths):\n        super(BlendRandomCroppedBackground, self).__init__()\n        if not isinstance(background_paths, list):\n            raise ValueError('``background_paths`` must be list')\n        if len(background_paths) == 0:\n            raise ValueError('No paths given in ``background_paths``')\n        self.background_paths = background_paths",
  "def call(self, image):\n        random_arg = np.random.randint(0, len(self.background_paths))\n        background_path = self.background_paths[random_arg]\n        background = load_image(background_path)\n        background = random_shape_crop(background, image.shape[:2])\n        if background is None:\n            H, W, num_channels = image.shape\n            # background contains always a channel less\n            num_channels = num_channels - 1\n            background = make_random_plain_image((H, W, num_channels))\n        return blend_alpha_channel(image, background)",
  "def __init__(self, max_radius_scale=0.5, probability=0.5):\n        super(AddOcclusion, self).__init__()\n        self.max_radius_scale = max_radius_scale\n        self.probability = probability",
  "def _random_vertices(self, center, max_radius, min_vertices, max_vertices):\n        num_vertices = np.random.randint(min_vertices, max_vertices)\n        angle_delta = 2 * np.pi / num_vertices\n        initial_angle = np.random.uniform(0, 2 * np.pi)\n        angles = initial_angle + np.arange(0, num_vertices) * angle_delta\n        x_component = np.cos(angles).reshape(-1, 1)\n        y_component = np.sin(angles).reshape(-1, 1)\n        vertices = np.concatenate([x_component, y_component], -1)\n        random_lengths = np.random.uniform(0, max_radius, num_vertices)\n        random_lengths = random_lengths.reshape(num_vertices, 1)\n        vertices = vertices * random_lengths\n        vertices = vertices + center\n        return vertices.astype(np.int32)",
  "def add_occlusion(self, image, max_radius_scale):\n        height, width = image.shape[:2]\n        max_radius = np.max((height, width)) * max_radius_scale\n        center = np.random.rand(2) * np.array([width, height])\n        vertices = self._random_vertices(center, max_radius, 3, 7)\n        color = np.random.randint(0, 256, 3).tolist()\n        return draw_filled_polygon(image, vertices, color)",
  "def call(self, image):\n        if self.probability >= np.random.rand():\n            image = self.add_occlusion(image, self.max_radius_scale)\n        return image",
  "def __init__(self, crop_factor=0.3, probability=0.5):\n        self.crop_factor = crop_factor\n        self.probability = probability\n        super(RandomImageCrop, self).__init__()",
  "def call(self, image):\n        if self.probability < np.random.rand():\n            return image\n        H, W = image.shape[:2]\n        W_crop = np.random.uniform(self.crop_factor * W, W)\n        H_crop = np.random.uniform(self.crop_factor * H, H)\n        x_min = np.random.uniform(W - W_crop)\n        y_min = np.random.uniform(H - H_crop)\n        x_max = x_min + W_crop\n        y_max = y_min + H_crop\n        cropped_image = image[int(x_min):int(x_max), int(y_min):int(y_max), :]\n        return cropped_image",
  "def __init__(self):\n        super(ImageToNormalizedDeviceCoordinates, self).__init__()",
  "def call(self, image):\n        return image_to_normalized_device_coordinates(image)",
  "def __init__(self):\n        super(NormalizedDeviceCoordinatesToImage, self).__init__()",
  "def call(self, image):\n        return normalized_device_coordinates_to_image(image)",
  "def __init__(self, threshold=1e-8, replacement=0.0):\n        super(ReplaceLowerThanThreshold, self).__init__()\n        self.threshold = threshold\n        self.replacement = replacement",
  "def call(self, values):\n        return replace_lower_than_threshold(\n            values, self.threshold, self.replacement)",
  "def __init__(self):\n        super(GetNonZeroValues, self).__init__()",
  "def call(self, array):\n        channel_wise_sum = np.sum(array, axis=2)\n        non_zero_arguments = np.nonzero(channel_wise_sum)\n        return array[non_zero_arguments]",
  "def __init__(self):\n        super(GetNonZeroArguments, self).__init__()",
  "def call(self, array):\n        channel_wise_sum = np.sum(array, axis=2)\n        non_zero_rows, non_zero_columns = np.nonzero(channel_wise_sum)\n        return non_zero_rows, non_zero_columns",
  "def __init__(self):\n        super(ImagenetPreprocessInput, self).__init__()",
  "def call(self, image):\n        return imagenet_preprocess_input(image)",
  "def __init__(self):\n        super(FlipLeftRightImage, self).__init__()",
  "def call(self, image):\n        return flip_left_right(image)",
  "def __init__(self, standard_deviation):\n        self.standard_deviation = standard_deviation\n        super(DivideStandardDeviationImage, self).__init__()",
  "def call(self, image):\n        return image / self.standard_deviation",
  "def __init__(self, image_size):\n        self.image_size = image_size\n        super(ScaledResize, self).__init__()",
  "def call(self, image):\n        \"\"\"\n        # Arguments\n            image: Array, raw input image.\n        \"\"\"\n        output_image, image_scale = scale_resize(image, self.image_size)\n        return output_image, image_scale",
  "class Munkres(Processor):\n    \"\"\"\n    Provides an implementation of the Munkres algorithm.\n\n    # References\n    https://brc2.com/the-algorithm-workshop/\n    https://software.clapper.org/munkres/\n    https://github.com/bmc/munkres\n    \"\"\"\n    def __init__(self):\n        super(Munkres, self).__init__()\n        self.Z0_r = 0\n        self.Z0_c = 0\n        self.done = False\n        self.steps = {1: self._step1,\n                      2: self._step2,\n                      3: self._step3,\n                      4: self._step4,\n                      5: self._step5,\n                      6: self._step6}\n\n    def compute(self, cost_matrix):\n        self.H, self.W = np.array(cost_matrix).shape[:2]\n        self.cost_matrix = pad_matrix(cost_matrix, padding='square')\n        self.n = len(self.cost_matrix)\n        self.marked = np.zeros((self.n, self.n), dtype='int')\n        self.path = np.zeros((self.n * 2, self.n * 2), dtype='int')\n        self.row_covered = np.zeros((self.n, 1), 'bool')\n        self.col_covered = np.zeros((self.n, 1), 'bool')\n\n        step = 1\n        while not self.done:\n            step_func = self.steps[step]\n            step = step_func()\n            if step == 7:\n                break\n        cost = []\n\n        for row_arg in range(self.H):\n            for col_arg in range(self.W):\n                if self.marked[row_arg][col_arg]:\n                    cost = cost + [(row_arg, col_arg)]\n        return cost\n\n    def _convert_path(self, path, count):\n        for i in range(count+1):\n            if self.marked[path[i][0]][path[i][1]] == 1:\n                self.marked[path[i][0]][path[i][1]] = 0\n            else:\n                self.marked[path[i][0]][path[i][1]] = 1\n\n    def _erase_primes(self):\n        for i in range(self.n):\n            for j in range(self.n):\n                if self.marked[i][j] == 2:\n                    self.marked[i][j] = 0\n\n    def _step1(self):\n        '''\n        For each row of the matrix, find the smallest element and subtract\n        it from every element in its row.  Go to Step 2.\n        '''\n        for row in range(self.n):\n            min_value = get_min_value(self.cost_matrix[row])\n            for col in range(self.n):\n                if type(self.cost_matrix[row][col]) is not type(DISALLOWED):\n                    self.cost_matrix[row][col] = \\\n                        self.cost_matrix[row][col] - min_value\n        return 2\n\n    def _step2(self):\n        '''\n        In the resulting matrix, look for a zero (Z). Star Z if there isn't\n        a starred zero in its row or column. For each element in the matrix,\n        repeat the process. Continue to Step 3.\n        '''\n        for row in range(self.n):\n            for col in range(self.n):\n                if (self.cost_matrix[row][col] == 0) and \\\n                    (not self.row_covered[row]) and \\\n                        (not self.col_covered[col]):\n                    self.marked[row][col] = 1\n                    self.row_covered[row] = True\n                    self.col_covered[col] = True\n                    break\n        self.row_covered, self.col_covered = get_cover_matrix((self.n, 1))\n        return 3\n\n    def _step3(self):\n        '''\n        Cover each column containing a starred zero.  If K columns are covered,\n        the starred zeros describe a complete set of unique assignments.\n        In this case, Go to DONE, otherwise, Go to Step 4.\n        '''\n        count = 0\n        for row in range(self.n):\n            for col in range(self.n):\n                if self.marked[row][col] == 1 and not self.col_covered[col]:\n                    self.col_covered[col] = True\n                    count = count + 1\n\n        if count >= self.n:\n            step = 7\n        else:\n            step = 4\n        return step\n\n    def _step4(self):\n        '''\n        Find a noncovered zero and prime it.  If there is no starred zero in\n        the row containing this primed zero, Go to Step 5. Otherwise, cover\n        this row and uncover the column containing the starred zero. Continue\n        in this manner until there are no uncovered zeros left. Save the\n        smallest uncovered value and Go to Step 6.\n        '''\n        done = False\n        row = 0\n        col = 0\n        star_col = -1\n        while not done:\n            (row, col) = find_uncovered_zero(self.n, self.cost_matrix,\n                                             self.row_covered,\n                                             self.col_covered, row, col)\n\n            if row < 0:\n                done = True\n                step = 6\n            else:\n                self.marked[row][col] = 2\n                star_col = find_star_in_row(self.n, row, self.marked)\n                if star_col >= 0:\n                    col = star_col\n                    self.row_covered[row] = True\n                    self.col_covered[col] = False\n                else:\n                    done = True\n                    self.Z0_r = row\n                    self.Z0_c = col\n                    step = 5\n        return step\n\n    def _step5(self):\n        '''\n        Construct a series of alternating primed and starred zeros as follows.\n        Let Z0 represent the uncovered primed zero found in Step 4. Let Z1\n        denote the starred zero in the column of Z0 (if any). Let Z2 denote\n        the primed zero in the row of Z1 (there will always be one). Continue\n        until the series terminates at a primed zero that has no starred zero\n        in its column. Unstar each starred zero of the series, star each primed\n        zero of the series, erase all primes and uncover every line in the\n        matrix. Return to Step 3.\n        '''\n        count = 0\n        path = self.path\n        path[count][0] = self.Z0_r\n        path[count][1] = self.Z0_c\n        done = False\n        while not done:\n            row = find_star_in_col(self.n, path[count][1], self.marked)\n            if row >= 0:\n                count += 1\n                path[count][0] = row\n                path[count][1] = path[count-1][1]\n            else:\n                done = True\n\n            if not done:\n                col = find_prime_in_row(self.n, path[count][0], self.marked)\n                count += 1\n                path[count][0] = path[count-1][0]\n                path[count][1] = col\n\n        self._convert_path(path, count)\n        self.row_covered, self.col_covered = get_cover_matrix((self.n, 1))\n        self._erase_primes()\n        return 3\n\n    def _step6(self):\n        '''\n        Add the value found in Step 4 to every element of each covered row,\n        and subtract it from every element of each uncovered column. Return\n        to Step 4 without altering any stars, primes, or covered lines.\n        '''\n        minval = find_smallest_uncovered(self.n, self.row_covered,\n                                         self.col_covered,\n                                         self.cost_matrix)\n        events = 0\n        for row_arg in range(self.n):\n            for col_arg in range(self.n):\n                if type(self.cost_matrix[row_arg][col_arg]) \\\n                        is type(DISALLOWED):\n                    continue\n                if self.row_covered[row_arg]:\n                    self.cost_matrix[row_arg][col_arg] += minval\n                    events += 1\n                if not self.col_covered[col_arg]:\n                    self.cost_matrix[row_arg][col_arg] -= minval\n                    events += 1\n                if self.row_covered[row_arg] and not self.col_covered[col_arg]:\n                    events -= 2\n        if (events == 0):\n            raise UnsolvableMatrix(\"Matrix cannot be solved!\")\n        return 4",
  "def __init__(self):\n        super(Munkres, self).__init__()\n        self.Z0_r = 0\n        self.Z0_c = 0\n        self.done = False\n        self.steps = {1: self._step1,\n                      2: self._step2,\n                      3: self._step3,\n                      4: self._step4,\n                      5: self._step5,\n                      6: self._step6}",
  "def compute(self, cost_matrix):\n        self.H, self.W = np.array(cost_matrix).shape[:2]\n        self.cost_matrix = pad_matrix(cost_matrix, padding='square')\n        self.n = len(self.cost_matrix)\n        self.marked = np.zeros((self.n, self.n), dtype='int')\n        self.path = np.zeros((self.n * 2, self.n * 2), dtype='int')\n        self.row_covered = np.zeros((self.n, 1), 'bool')\n        self.col_covered = np.zeros((self.n, 1), 'bool')\n\n        step = 1\n        while not self.done:\n            step_func = self.steps[step]\n            step = step_func()\n            if step == 7:\n                break\n        cost = []\n\n        for row_arg in range(self.H):\n            for col_arg in range(self.W):\n                if self.marked[row_arg][col_arg]:\n                    cost = cost + [(row_arg, col_arg)]\n        return cost",
  "def _convert_path(self, path, count):\n        for i in range(count+1):\n            if self.marked[path[i][0]][path[i][1]] == 1:\n                self.marked[path[i][0]][path[i][1]] = 0\n            else:\n                self.marked[path[i][0]][path[i][1]] = 1",
  "def _erase_primes(self):\n        for i in range(self.n):\n            for j in range(self.n):\n                if self.marked[i][j] == 2:\n                    self.marked[i][j] = 0",
  "def _step1(self):\n        '''\n        For each row of the matrix, find the smallest element and subtract\n        it from every element in its row.  Go to Step 2.\n        '''\n        for row in range(self.n):\n            min_value = get_min_value(self.cost_matrix[row])\n            for col in range(self.n):\n                if type(self.cost_matrix[row][col]) is not type(DISALLOWED):\n                    self.cost_matrix[row][col] = \\\n                        self.cost_matrix[row][col] - min_value\n        return 2",
  "def _step2(self):\n        '''\n        In the resulting matrix, look for a zero (Z). Star Z if there isn't\n        a starred zero in its row or column. For each element in the matrix,\n        repeat the process. Continue to Step 3.\n        '''\n        for row in range(self.n):\n            for col in range(self.n):\n                if (self.cost_matrix[row][col] == 0) and \\\n                    (not self.row_covered[row]) and \\\n                        (not self.col_covered[col]):\n                    self.marked[row][col] = 1\n                    self.row_covered[row] = True\n                    self.col_covered[col] = True\n                    break\n        self.row_covered, self.col_covered = get_cover_matrix((self.n, 1))\n        return 3",
  "def _step3(self):\n        '''\n        Cover each column containing a starred zero.  If K columns are covered,\n        the starred zeros describe a complete set of unique assignments.\n        In this case, Go to DONE, otherwise, Go to Step 4.\n        '''\n        count = 0\n        for row in range(self.n):\n            for col in range(self.n):\n                if self.marked[row][col] == 1 and not self.col_covered[col]:\n                    self.col_covered[col] = True\n                    count = count + 1\n\n        if count >= self.n:\n            step = 7\n        else:\n            step = 4\n        return step",
  "def _step4(self):\n        '''\n        Find a noncovered zero and prime it.  If there is no starred zero in\n        the row containing this primed zero, Go to Step 5. Otherwise, cover\n        this row and uncover the column containing the starred zero. Continue\n        in this manner until there are no uncovered zeros left. Save the\n        smallest uncovered value and Go to Step 6.\n        '''\n        done = False\n        row = 0\n        col = 0\n        star_col = -1\n        while not done:\n            (row, col) = find_uncovered_zero(self.n, self.cost_matrix,\n                                             self.row_covered,\n                                             self.col_covered, row, col)\n\n            if row < 0:\n                done = True\n                step = 6\n            else:\n                self.marked[row][col] = 2\n                star_col = find_star_in_row(self.n, row, self.marked)\n                if star_col >= 0:\n                    col = star_col\n                    self.row_covered[row] = True\n                    self.col_covered[col] = False\n                else:\n                    done = True\n                    self.Z0_r = row\n                    self.Z0_c = col\n                    step = 5\n        return step",
  "def _step5(self):\n        '''\n        Construct a series of alternating primed and starred zeros as follows.\n        Let Z0 represent the uncovered primed zero found in Step 4. Let Z1\n        denote the starred zero in the column of Z0 (if any). Let Z2 denote\n        the primed zero in the row of Z1 (there will always be one). Continue\n        until the series terminates at a primed zero that has no starred zero\n        in its column. Unstar each starred zero of the series, star each primed\n        zero of the series, erase all primes and uncover every line in the\n        matrix. Return to Step 3.\n        '''\n        count = 0\n        path = self.path\n        path[count][0] = self.Z0_r\n        path[count][1] = self.Z0_c\n        done = False\n        while not done:\n            row = find_star_in_col(self.n, path[count][1], self.marked)\n            if row >= 0:\n                count += 1\n                path[count][0] = row\n                path[count][1] = path[count-1][1]\n            else:\n                done = True\n\n            if not done:\n                col = find_prime_in_row(self.n, path[count][0], self.marked)\n                count += 1\n                path[count][0] = path[count-1][0]\n                path[count][1] = col\n\n        self._convert_path(path, count)\n        self.row_covered, self.col_covered = get_cover_matrix((self.n, 1))\n        self._erase_primes()\n        return 3",
  "def _step6(self):\n        '''\n        Add the value found in Step 4 to every element of each covered row,\n        and subtract it from every element of each uncovered column. Return\n        to Step 4 without altering any stars, primes, or covered lines.\n        '''\n        minval = find_smallest_uncovered(self.n, self.row_covered,\n                                         self.col_covered,\n                                         self.cost_matrix)\n        events = 0\n        for row_arg in range(self.n):\n            for col_arg in range(self.n):\n                if type(self.cost_matrix[row_arg][col_arg]) \\\n                        is type(DISALLOWED):\n                    continue\n                if self.row_covered[row_arg]:\n                    self.cost_matrix[row_arg][col_arg] += minval\n                    events += 1\n                if not self.col_covered[col_arg]:\n                    self.cost_matrix[row_arg][col_arg] -= minval\n                    events += 1\n                if self.row_covered[row_arg] and not self.col_covered[col_arg]:\n                    events -= 2\n        if (events == 0):\n            raise UnsolvableMatrix(\"Matrix cannot be solved!\")\n        return 4",
  "class ProjectKeypoints(Processor):\n    \"\"\"Projects homogenous keypoints (4D) in the camera coordinates system into\n        image coordinates using a projective transformation.\n\n    # Arguments\n        projector: Instance of ''paz.models.Project''.\n        keypoints: Numpy array of shape ''(num_keypoints, 3)''\n    \"\"\"\n    def __init__(self, projector, keypoints):\n        self.projector = projector\n        self.keypoints = keypoints\n        super(ProjectKeypoints, self).__init__()\n\n    def call(self, world_to_camera):\n        keypoints = np.matmul(self.keypoints, world_to_camera.T)\n        keypoints = np.expand_dims(keypoints, 0)\n        keypoints = self.projector.project(keypoints)[0]\n        return keypoints",
  "class NormalizeKeypoints2D(Processor):\n    \"\"\"Transform keypoints in image-size coordinates to normalized coordinates.\n\n    # Arguments\n        image_size: List of two ints indicating ''(height, width)''\n    \"\"\"\n    def __init__(self, image_size):\n        self.image_size = image_size\n        super(NormalizeKeypoints2D, self).__init__()\n\n    def call(self, keypoints):\n        height, width = self.image_size[0:2]\n        keypoints = normalize_keypoints2D(keypoints, height, width)\n        return keypoints",
  "class DenormalizeKeypoints2D(Processor):\n    \"\"\"Transform normalized keypoints coordinates into image-size coordinates.\n\n    # Arguments\n        image_size: List of two floats having height and width of image.\n    \"\"\"\n    def __init__(self):\n        super(DenormalizeKeypoints2D, self).__init__()\n\n    def call(self, keypoints, image):\n        height, width = image.shape[0:2]\n        keypoints = denormalize_keypoints2D(keypoints, height, width)\n        return keypoints",
  "class NormalizeKeypoints(Processor):\n    \"\"\"Transform keypoints in image-size coordinates to normalized coordinates.\n\n    # Arguments\n        image_size: List of two ints indicating ''(height, width)''\n    \"\"\"\n    def __init__(self, image_size):\n        self.image_size = image_size\n        warn('DEPRECATED please use normalize_keypoints2D')\n        super(NormalizeKeypoints, self).__init__()\n\n    def call(self, keypoints):\n        height, width = self.image_size[0:2]\n        keypoints = normalize_keypoints(keypoints, height, width)\n        return keypoints",
  "class DenormalizeKeypoints(Processor):\n    \"\"\"Transform normalized keypoints coordinates into image-size coordinates.\n\n    # Arguments\n        image_size: List of two floats having height and width of image.\n    \"\"\"\n    def __init__(self):\n        warn('DEPRECATED please use denormalize_keypoints2D')\n        super(DenormalizeKeypoints, self).__init__()\n\n    def call(self, keypoints, image):\n        height, width = image.shape[0:2]\n        keypoints = denormalize_keypoints(keypoints, height, width)\n        return keypoints",
  "class RemoveKeypointsDepth(Processor):\n    \"\"\"Removes Z component from keypoints.\n    \"\"\"\n    def __init__(self):\n        super(RemoveKeypointsDepth, self).__init__()\n\n    def call(self, keypoints):\n        return keypoints[:, :2]",
  "class PartitionKeypoints(Processor):\n    \"\"\"Partitions keypoints from shape [num_keypoints, 2] into a list of the\n    form ((2), (2), ....) and length equal to num_of_keypoints.\n    \"\"\"\n    def __init__(self):\n        super(PartitionKeypoints, self).__init__()\n\n    def call(self, keypoints):\n        keypoints = np.vsplit(keypoints, len(keypoints))\n        keypoints = [np.squeeze(keypoint) for keypoint in keypoints]\n        partioned_keypoints = []\n        for keypoint_arg, keypoint in enumerate(keypoints):\n            partioned_keypoints.append(keypoint)\n        return np.asarray(partioned_keypoints)",
  "class ChangeKeypointsCoordinateSystem(Processor):\n    \"\"\"Changes ``keypoints`` 2D coordinate system using ``box2D`` coordinates\n        to locate the new origin at the openCV image origin (top-left).\n    \"\"\"\n    def __init__(self):\n        super(ChangeKeypointsCoordinateSystem, self).__init__()\n\n    def call(self, keypoints, box2D):\n        x_min, y_min, x_max, y_max = box2D.coordinates\n        keypoints[:, 0] = keypoints[:, 0] + x_min\n        keypoints[:, 1] = keypoints[:, 1] + y_min\n        return keypoints",
  "class TranslateKeypoints(Processor):\n    \"\"\"Applies a translation to keypoints.\n    The translation is a list of length two indicating the x, y values.\n    \"\"\"\n    def __init__(self):\n        super(TranslateKeypoints, self).__init__()\n\n    def call(self, keypoints, translation):\n        return translate_keypoints(keypoints, translation)",
  "class ArgumentsToImageKeypoints2D(Processor):\n    \"\"\"Convert array arguments into UV coordinates.\n\n              Image plane\n\n           (0,0)-------->  (U)\n             |\n             |\n             |\n             v\n\n            (V)\n\n    # Arguments\n        row_args: Array (num_rows).\n        col_args: Array (num_cols).\n\n    # Returns\n        Array (num_cols, num_rows) representing points2D in UV space.\n\n    # Notes\n        Arguments are row args (V) and col args (U). Image points are in UV\n            coordinates; thus, we concatenate them in that order\n            i.e. [col_args, row_args]\n    \"\"\"\n    def __init__(self):\n        super(ArgumentsToImageKeypoints2D, self).__init__()\n\n    def call(self, row_args, col_args):\n        image_points2D = arguments_to_image_points2D(row_args, col_args)\n        return image_points2D",
  "class ScaleKeypoints(Processor):\n    \"\"\"Scale keypoints to input image shape.\n\n    # Arguments\n        keypoints: Array. Detected keypoints by the model\n        image: Array. Input image.\n\n    # Returns\n        Scaled keypoints: Array. keypoints scaled to input image shape.\n    \"\"\"\n    def __init__(self, scale=1, shape=(128, 128)):\n        super(ScaleKeypoints, self).__init__()\n        self.scale = scale\n        self.shape = shape\n\n    def call(self, keypoints, image):\n        scale = get_scaling_factor(image, self.scale, self.shape)\n        scaled_keypoints = keypoints * scale\n        return np.array(scaled_keypoints, dtype=np.uint)",
  "class ComputeOrientationVector(Processor):\n    \"\"\"Calculate the orientation of keypoints links with 3D keypoints.\n\n    # Arguments\n        keypoints: Array. 3D keypoints\n\n    # Returns\n        orientation: Array. Orientation of keypoint links\n    \"\"\"\n    def __init__(self, parents):\n        super(ComputeOrientationVector, self).__init__()\n        self.parents = parents\n\n    def call(self, keypoints):\n        orientation = compute_orientation_vector(keypoints, self.parents)\n        return orientation",
  "class MergeKeypoints2D(Processor):\n    def __init__(self, args_to_mean):\n        \"\"\" Merges keypoints together then takes the mean of the keypoints\n\n        # Arguments\n            args_to_mean: keypoints indices\n\n        # Returns\n            Filtered keypoints2D\n        \"\"\"\n        super(MergeKeypoints2D, self).__init__()\n        self.args_to_mean = args_to_mean\n\n    def call(self, keypoints2D):\n        return merge_into_mean(keypoints2D, self.args_to_mean)",
  "class FilterKeypoints2D(Processor):\n    def __init__(self, args_to_mean, h36m_to_coco_joints2D):\n        \"\"\" Filter keypoints2D\n\n        # Arguments\n            args_to_mean: keypoints indices\n            h36m_to_coco_joints2D: h36m joints indices\n\n        # Returns\n            Filtered keypoints2D\n        \"\"\"\n        super(FilterKeypoints2D, self).__init__()\n        self.h36m_to_coco_joints2D = h36m_to_coco_joints2D\n        self.args_to_mean = args_to_mean\n\n    def call(self, keypoints2D):\n        return filter_keypoints2D(keypoints2D, self.args_to_mean,\n                                  self.h36m_to_coco_joints2D)",
  "class StandardizeKeypoints2D(Processor):\n    def __init__(self, data_mean2D, data_stdev2D):\n        \"\"\" Standardize 2D keypoints\n\n        # Arguments\n            data_mean2D: mean 2D\n            data_stdev2D: standard deviation 2D\n\n        # Return\n            standerized keypoints2D\n        \"\"\"\n        self.mean = data_mean2D\n        self.stdev = data_stdev2D\n        super(StandardizeKeypoints2D, self).__init__()\n\n    def call(self, keypoints2D):\n        return standardize(keypoints2D, self.mean, self.stdev)",
  "class DestandardizeKeypoints2D(Processor):\n    def __init__(self, data_mean3D, data_stdev3D, dim_to_use):\n        \"\"\" Destandardize 2D keypoints\n\n        # Arguments\n            data_mean3D: mean 3D\n            data_stdev3D: standard deviation 3D\n            dim_to_use: dimensions to use\n\n        # Return\n            detandardize 2D keypoints\n        \"\"\"\n        self.mean = data_mean3D\n        self.stdev = data_stdev3D\n        self.valid = dim_to_use\n        super(DestandardizeKeypoints2D, self).__init__()\n\n    def call(self, keypoints2D):\n        data = keypoints2D.reshape(-1, 48)\n        rearanged_data = np.zeros((len(data), len(self.mean)),\n                                  dtype=np.float32)\n        rearanged_data[:, self.valid] = data\n        destandardize_data = destandardize(rearanged_data, self.mean,\n                                           self.stdev)\n        return destandardize_data",
  "class OptimizeHumanPose3D(Processor):\n    \"\"\" Optimize human 3D pose\n\n    #Arguments\n        solver: library solver\n        camera_intrinsics: camera intrinsic parameters\n\n    #Returns\n        keypoints3D, optimized keypoints3D\n    \"\"\"\n    def __init__(self, args_to_joints3D, solver, camera_intrinsics):\n        super(OptimizeHumanPose3D, self).__init__()\n        self.args_to_joints3D = args_to_joints3D\n        self.camera_intrinsics = camera_intrinsics\n        self.filter_keypoints2D = SequentialProcessor(\n            [pr.MergeKeypoints2D(args_to_mean),\n             pr.FilterKeypoints2D(args_to_mean, h36m_to_coco_joints2D)])\n        self.solver = solver\n\n    def call(self, keypoints3D, keypoints2D):\n        joints3D = filter_keypoints3D(keypoints3D, self.args_to_joints3D)\n        joints2D = self.filter_keypoints2D(keypoints2D)\n        root2D = joints2D[:, :2]\n        length2D, length3D = get_bones_length(\n            joints2D, keypoints3D, human_start_joints)\n        ratio = length3D / length2D\n        initial_joint_translation = initialize_translation(\n            root2D, self.camera_intrinsics, ratio)\n        joint_translation = solve_least_squares(\n            self.solver, compute_reprojection_error, initial_joint_translation,\n            joints3D, joints2D, self.camera_intrinsics)\n        optimized_poses3D, projection2D = compute_optimized_pose3D(\n            keypoints3D, joint_translation, self.camera_intrinsics)\n        return joints2D, joints3D, optimized_poses3D, projection2D",
  "def __init__(self, projector, keypoints):\n        self.projector = projector\n        self.keypoints = keypoints\n        super(ProjectKeypoints, self).__init__()",
  "def call(self, world_to_camera):\n        keypoints = np.matmul(self.keypoints, world_to_camera.T)\n        keypoints = np.expand_dims(keypoints, 0)\n        keypoints = self.projector.project(keypoints)[0]\n        return keypoints",
  "def __init__(self, image_size):\n        self.image_size = image_size\n        super(NormalizeKeypoints2D, self).__init__()",
  "def call(self, keypoints):\n        height, width = self.image_size[0:2]\n        keypoints = normalize_keypoints2D(keypoints, height, width)\n        return keypoints",
  "def __init__(self):\n        super(DenormalizeKeypoints2D, self).__init__()",
  "def call(self, keypoints, image):\n        height, width = image.shape[0:2]\n        keypoints = denormalize_keypoints2D(keypoints, height, width)\n        return keypoints",
  "def __init__(self, image_size):\n        self.image_size = image_size\n        warn('DEPRECATED please use normalize_keypoints2D')\n        super(NormalizeKeypoints, self).__init__()",
  "def call(self, keypoints):\n        height, width = self.image_size[0:2]\n        keypoints = normalize_keypoints(keypoints, height, width)\n        return keypoints",
  "def __init__(self):\n        warn('DEPRECATED please use denormalize_keypoints2D')\n        super(DenormalizeKeypoints, self).__init__()",
  "def call(self, keypoints, image):\n        height, width = image.shape[0:2]\n        keypoints = denormalize_keypoints(keypoints, height, width)\n        return keypoints",
  "def __init__(self):\n        super(RemoveKeypointsDepth, self).__init__()",
  "def call(self, keypoints):\n        return keypoints[:, :2]",
  "def __init__(self):\n        super(PartitionKeypoints, self).__init__()",
  "def call(self, keypoints):\n        keypoints = np.vsplit(keypoints, len(keypoints))\n        keypoints = [np.squeeze(keypoint) for keypoint in keypoints]\n        partioned_keypoints = []\n        for keypoint_arg, keypoint in enumerate(keypoints):\n            partioned_keypoints.append(keypoint)\n        return np.asarray(partioned_keypoints)",
  "def __init__(self):\n        super(ChangeKeypointsCoordinateSystem, self).__init__()",
  "def call(self, keypoints, box2D):\n        x_min, y_min, x_max, y_max = box2D.coordinates\n        keypoints[:, 0] = keypoints[:, 0] + x_min\n        keypoints[:, 1] = keypoints[:, 1] + y_min\n        return keypoints",
  "def __init__(self):\n        super(TranslateKeypoints, self).__init__()",
  "def call(self, keypoints, translation):\n        return translate_keypoints(keypoints, translation)",
  "def __init__(self):\n        super(ArgumentsToImageKeypoints2D, self).__init__()",
  "def call(self, row_args, col_args):\n        image_points2D = arguments_to_image_points2D(row_args, col_args)\n        return image_points2D",
  "def __init__(self, scale=1, shape=(128, 128)):\n        super(ScaleKeypoints, self).__init__()\n        self.scale = scale\n        self.shape = shape",
  "def call(self, keypoints, image):\n        scale = get_scaling_factor(image, self.scale, self.shape)\n        scaled_keypoints = keypoints * scale\n        return np.array(scaled_keypoints, dtype=np.uint)",
  "def __init__(self, parents):\n        super(ComputeOrientationVector, self).__init__()\n        self.parents = parents",
  "def call(self, keypoints):\n        orientation = compute_orientation_vector(keypoints, self.parents)\n        return orientation",
  "def __init__(self, args_to_mean):\n        \"\"\" Merges keypoints together then takes the mean of the keypoints\n\n        # Arguments\n            args_to_mean: keypoints indices\n\n        # Returns\n            Filtered keypoints2D\n        \"\"\"\n        super(MergeKeypoints2D, self).__init__()\n        self.args_to_mean = args_to_mean",
  "def call(self, keypoints2D):\n        return merge_into_mean(keypoints2D, self.args_to_mean)",
  "def __init__(self, args_to_mean, h36m_to_coco_joints2D):\n        \"\"\" Filter keypoints2D\n\n        # Arguments\n            args_to_mean: keypoints indices\n            h36m_to_coco_joints2D: h36m joints indices\n\n        # Returns\n            Filtered keypoints2D\n        \"\"\"\n        super(FilterKeypoints2D, self).__init__()\n        self.h36m_to_coco_joints2D = h36m_to_coco_joints2D\n        self.args_to_mean = args_to_mean",
  "def call(self, keypoints2D):\n        return filter_keypoints2D(keypoints2D, self.args_to_mean,\n                                  self.h36m_to_coco_joints2D)",
  "def __init__(self, data_mean2D, data_stdev2D):\n        \"\"\" Standardize 2D keypoints\n\n        # Arguments\n            data_mean2D: mean 2D\n            data_stdev2D: standard deviation 2D\n\n        # Return\n            standerized keypoints2D\n        \"\"\"\n        self.mean = data_mean2D\n        self.stdev = data_stdev2D\n        super(StandardizeKeypoints2D, self).__init__()",
  "def call(self, keypoints2D):\n        return standardize(keypoints2D, self.mean, self.stdev)",
  "def __init__(self, data_mean3D, data_stdev3D, dim_to_use):\n        \"\"\" Destandardize 2D keypoints\n\n        # Arguments\n            data_mean3D: mean 3D\n            data_stdev3D: standard deviation 3D\n            dim_to_use: dimensions to use\n\n        # Return\n            detandardize 2D keypoints\n        \"\"\"\n        self.mean = data_mean3D\n        self.stdev = data_stdev3D\n        self.valid = dim_to_use\n        super(DestandardizeKeypoints2D, self).__init__()",
  "def call(self, keypoints2D):\n        data = keypoints2D.reshape(-1, 48)\n        rearanged_data = np.zeros((len(data), len(self.mean)),\n                                  dtype=np.float32)\n        rearanged_data[:, self.valid] = data\n        destandardize_data = destandardize(rearanged_data, self.mean,\n                                           self.stdev)\n        return destandardize_data",
  "def __init__(self, args_to_joints3D, solver, camera_intrinsics):\n        super(OptimizeHumanPose3D, self).__init__()\n        self.args_to_joints3D = args_to_joints3D\n        self.camera_intrinsics = camera_intrinsics\n        self.filter_keypoints2D = SequentialProcessor(\n            [pr.MergeKeypoints2D(args_to_mean),\n             pr.FilterKeypoints2D(args_to_mean, h36m_to_coco_joints2D)])\n        self.solver = solver",
  "def call(self, keypoints3D, keypoints2D):\n        joints3D = filter_keypoints3D(keypoints3D, self.args_to_joints3D)\n        joints2D = self.filter_keypoints2D(keypoints2D)\n        root2D = joints2D[:, :2]\n        length2D, length3D = get_bones_length(\n            joints2D, keypoints3D, human_start_joints)\n        ratio = length3D / length2D\n        initial_joint_translation = initialize_translation(\n            root2D, self.camera_intrinsics, ratio)\n        joint_translation = solve_least_squares(\n            self.solver, compute_reprojection_error, initial_joint_translation,\n            joints3D, joints2D, self.camera_intrinsics)\n        optimized_poses3D, projection2D = compute_optimized_pose3D(\n            keypoints3D, joint_translation, self.camera_intrinsics)\n        return joints2D, joints3D, optimized_poses3D, projection2D",
  "class TransposeOutput(Processor):\n    \"\"\"Transpose the output of the HigherHRNet model\n    # Arguments\n        axes: List or tuple\n        Output: List of numpy array\n\n    \"\"\"\n    def __init__(self, axes):\n        super(TransposeOutput, self).__init__()\n        self.axes = axes\n\n    def call(self, outputs):\n        for arg in range(len(outputs)):\n            outputs[arg] = np.transpose(outputs[arg], self.axes)\n        return outputs",
  "class ScaleOutput(Processor):\n    \"\"\"Scale the output of the HigherHRNet model\n    # Arguments\n        scaling_factor: Int.\n        full_scaling: Boolean. If all the array of array are to be scaled.\n        Output: List of numpy array\n\n    \"\"\"\n    def __init__(self, scale_factor, full_scaling=False):\n        super(ScaleOutput, self).__init__()\n        self.scale_factor = int(scale_factor)\n        self.full_scaling = full_scaling\n\n    def _resize_output(self, output, size):\n        resized_output = []\n        for heatmap_arg, heatmap in enumerate(output):\n            resized_heatmaps = []\n            for keypoint_arg in range(len(heatmap)):\n                resized = resize_image(output[heatmap_arg][keypoint_arg], size)\n                resized_heatmaps.append(resized)\n            resized_heatmaps = np.stack(resized_heatmaps, axis=0)\n        resized_output.append(resized_heatmaps)\n        resized_output = np.stack(resized_output, axis=0)\n        return resized_output\n\n    def call(self, outputs):\n        for arg in range(len(outputs)):\n            H, W = outputs[arg].shape[-2:]\n            H, W = self.scale_factor * H, self.scale_factor * W\n            if self.full_scaling:\n                outputs[arg] = self._resize_output(outputs[arg], (W, H))\n            else:\n                if len(outputs) > 1 and arg != len(outputs) - 1:\n                    outputs[arg] = self._resize_output(outputs[arg], (W, H))\n        return outputs",
  "class GetHeatmaps(Processor):\n    \"\"\"Get Heatmaps from the model output.\n    # Arguments\n        flipped_keypoint_order: List of length 17 (number of keypoints).\n            Flipped list of keypoint order.\n        outputs: List of numpy arrays. Output of HigherHRNet model\n        with_flip: Boolean. indicates whether to flip the output\n\n    # Returns\n        heatmaps: Numpy array of shape (1, num_keypoints, H, W)\n    \"\"\"\n    def __init__(self, flipped_keypoint_order):\n        super(GetHeatmaps, self).__init__()\n        self.indices = flipped_keypoint_order\n        self.num_keypoints = len(flipped_keypoint_order)\n\n    def call(self, outputs, with_flip):\n        num_heatmaps = 0\n        heatmap_sum = 0\n        if with_flip:\n            for output in outputs:\n                output = np.flip(output, [3])\n                heatmap_sum = heatmap_sum + get_keypoints_heatmap(\n                    output, self.num_keypoints, indices=self.indices)\n                num_heatmaps = num_heatmaps + 1\n\n        if not with_flip:\n            for output in outputs:\n                heatmap_sum = heatmap_sum + get_keypoints_heatmap(\n                    output, self.num_keypoints)\n                num_heatmaps = num_heatmaps + 1\n\n        heatmaps = heatmap_sum / num_heatmaps\n        return heatmaps",
  "class GetTags(Processor):\n    \"\"\"Get Tags from the model output.\n    # Arguments\n        flipped_keypoint_order: List of length 17 (number of keypoints).\n            Flipped list of keypoint order.\n        outputs: List of numpy arrays. Output of HigherHRNet model\n        with_flip: Boolean. indicates whether to flip the output\n\n    # Returns\n        Tags: Numpy array of shape (1, num_keypoints, H, W)\n    \"\"\"\n    def __init__(self, flipped_keypoint_order):\n        super(GetTags, self).__init__()\n        self.indices = flipped_keypoint_order\n        self.num_keypoints = len(flipped_keypoint_order)\n\n    def call(self, outputs, with_flip):\n        output = outputs[0]\n        if not with_flip:\n            tags = get_tags_heatmap(output, self.num_keypoints)\n\n        if with_flip:\n            output = np.flip(output, [3])\n            tags = get_tags_heatmap(output, self.num_keypoints, self.indices)\n        return tags",
  "class RemoveLastElement(Processor):\n    \"\"\"Remove last element of array\n    # Arguments\n        x: array or list of arrays\n\n    \"\"\"\n    def __init__(self):\n        super(RemoveLastElement, self).__init__()\n\n    def call(self, x):\n        if all(isinstance(each, list) for each in x):\n            return [each[:, :-1] for each in x]\n        else:\n            return x[:, :-1]",
  "class AggregateResults(Processor):\n    \"\"\"Aggregate heatmaps and tags to get final heatmaps and tags for\n       processing.\n    # Arguments\n        heatmaps: Numpy array of shape (1, num_keypoints, H, W)\n        Tags: Numpy array of shape (1, num_keypoints, H, W)\n\n    # Returns\n        heatmaps: Numpy array of shape (1, num_keypoints, H, W)\n        Tags: Numpy array of shape (1, num_keypoints, H, W, 2)\n    \"\"\"\n\n    def __init__(self, with_flip=False):\n        super(AggregateResults, self).__init__()\n        self.with_flip = with_flip\n\n    def _expand_tags_dimension(self, tags):\n        updated_tags = []\n        for tag in tags:\n            updated_tags.append(np.expand_dims(tag, -1))\n        return updated_tags\n\n    def _calculate_heatmaps_average(self, heatmaps):\n        if self.with_flip:\n            heatmaps_average = (heatmaps[0] + heatmaps[1]) / 2.0\n        else:\n            heatmaps_average = heatmaps[0]\n        return heatmaps_average\n\n    def call(self, heatmaps, tags):\n        heatmaps_average = self._calculate_heatmaps_average(heatmaps)\n        heatmaps = heatmaps_average + heatmaps_average\n        tags = self._expand_tags_dimension(tags)\n        tags = np.concatenate(tags, 4)\n        return heatmaps, tags",
  "class TopKDetections(Processor):\n    \"\"\"Extract out the top k detections\n    # Arguments\n        k: Int. Maximum number of instances to be detected.\n        use_numpy: Boolean. Whether to use numpy functions or tf functions.\n        heatmaps: Numpy array of shape (1, num_joints, H, W)\n        Tags: Numpy array of shape (1, num_joints, H, W, 2)\n\n    # Returns\n        top_k_detections: Numpy array. Contains the top k keypoints locations\n                          of the detection with their value and tags.\n    \"\"\"\n    def __init__(self, k, use_numpy=False):\n        super(TopKDetections, self).__init__()\n        self.k = k\n        self.use_numpy = use_numpy\n\n    def _max_pooing_2d(self, heatmaps, pool_size, strides, padding,\n                       use_numpy=False):\n        if use_numpy:\n            heatmaps = np.squeeze(heatmaps)\n            heatmaps = np.transpose(heatmaps, [2, 0, 1])\n            max_heatmaps = np.zeros_like(heatmaps)\n            for arg, heatmap in enumerate(heatmaps):\n                max_heatmaps[arg] = max_pooling_2d(heatmap, pool_size,\n                                                   strides, padding)\n            max_heatmaps = np.transpose(max_heatmaps, [1, 2, 0])\n            max_pooled_values = np.expand_dims(max_heatmaps, 0)\n        else:\n            max_pooled_values = tf.keras.layers.MaxPooling2D(\n                pool_size, strides, padding)(heatmaps)\n        return max_pooled_values\n\n    def _filter_heatmaps(self, heatmaps):\n        heatmaps = np.transpose(heatmaps, [0, 2, 3, 1])\n        maximum_values = self._max_pooing_2d(heatmaps, pool_size=3, strides=1,\n                                             padding='same',\n                                             use_numpy=self.use_numpy)\n        maximum_values = np.equal(maximum_values, heatmaps)\n        maximum_values = maximum_values.astype(np.float32)\n        filtered_heatmaps = heatmaps * maximum_values\n        filtered_heatmaps = np.transpose(filtered_heatmaps, [0, 3, 1, 2])\n        return filtered_heatmaps\n\n    def _get_top_k_keypoints(self, heatmaps, k, use_numpy):\n        if use_numpy:\n            top_k_keypoints, indices = get_top_k_keypoints_numpy(heatmaps, k)\n        else:\n            top_k_keypoints, indices = tf.math.top_k(heatmaps, k)\n            top_k_keypoints = np.squeeze(top_k_keypoints)\n            indices = tensor_to_numpy(indices)\n        return top_k_keypoints, indices\n\n    def _get_top_k_tags(self, tags, indices):\n        indices = np.expand_dims(indices, -1)\n        gathered = gather_nd(tags, indices, axis=2)\n        return np.squeeze(gathered)\n\n    def call(self, heatmaps, tags):\n        tags = tags.astype(np.int64)\n        heatmaps = self._filter_heatmaps(heatmaps)\n        num_images, keypoints_count, H, W = heatmaps.shape[:4]\n        heatmaps = np.reshape(heatmaps, [num_images, keypoints_count, -1])\n        tags = np.reshape(tags, [num_images, keypoints_count, W * H, -1])\n\n        top_k_keypoints, indices = self._get_top_k_keypoints(\n            heatmaps, self.k, self.use_numpy)\n        top_k_tags = self._get_top_k_tags(tags, indices)\n        top_k_locations = get_keypoints_locations(indices, W)\n\n        top_k_keypoints = np.expand_dims(top_k_keypoints, axis=-1)\n        top_k_detections = np.concatenate((top_k_locations,\n                                           top_k_keypoints,\n                                           top_k_tags), 2)\n        return top_k_detections",
  "class GroupKeypointsByTag(Processor):\n    \"\"\"Group the keypoints with their respective tags value.\n    # Arguments\n        keypoint_order: List of length 17 (number of keypoints).\n        tag_thresh: Float.\n        detection_thresh: Float.\n        Detection: Numpy array containing the location, value and tags\n                   of top k keypoints\n\n    # Returns\n        grouped_keypoints: Numpy array. keypoints grouped by tag\n    \"\"\"\n    def __init__(self, keypoint_order, tag_thresh, detection_thresh):\n        super(GroupKeypointsByTag, self).__init__()\n        self.keypoint_order = keypoint_order\n        self.tag_thresh = tag_thresh\n        self.detection_thresh = detection_thresh\n        self.munkres = pr.Munkres()\n\n    def _update_dictionary(self, tags, keypoints, arg,\n                           default, keypoint_dict, tag_dict):\n        for tag, keypoint in zip(tags, keypoints):\n            key = tag[0]\n            keypoint_dict.setdefault(key, np.copy(default))[arg] = keypoint\n            tag_dict[key] = [tag]\n\n    def _group_tags(self, grouped_keys, tag_dict):\n        grouped_tags = []\n        for arg in grouped_keys:\n            grouped_tags.append(np.mean(tag_dict[arg], axis=0))\n        return grouped_tags\n\n    def call(self, detections):\n        keypoint_dict, tag_dict = {}, {}\n        default = np.zeros((detections.shape[0], detections.shape[-1]))\n\n        for arg, keypoint_arg in enumerate(self.keypoint_order):\n            keypoints = get_valid_detections(detections[keypoint_arg],\n                                             self.detection_thresh)\n            tags = keypoints[:, -2:]\n            if arg == 0 or len(keypoint_dict) == 0:\n                self._update_dictionary(tags, keypoints, keypoint_arg,\n                                        default, keypoint_dict, tag_dict)\n            else:\n                grouped_keys = list(keypoint_dict.keys())\n                grouped_tags = self._group_tags(grouped_keys, tag_dict)\n                difference = np.expand_dims(tags, 1) - np.expand_dims(\n                    grouped_tags, 0)\n                norm = calculate_norm(difference, order=2, axis=2)\n                norm = pad_matrix(norm, padding='square', value=1e10)\n                lowest_cost = self.munkres.compute(norm)\n                lowest_cost = np.array(lowest_cost).astype(np.int32)\n\n                for row_arg, col_arg in lowest_cost:\n                    if norm[row_arg][col_arg] < self.tag_thresh:\n                        key = grouped_keys[col_arg]\n                        keypoint_dict[key][keypoint_arg] = keypoints[row_arg]\n                        tag_dict[key].append(tags[row_arg])\n                    else:\n                        self._update_dictionary(tags, keypoints, keypoint_arg,\n                                                default, keypoint_dict,\n                                                tag_dict)\n        grouped_keypoints = list(keypoint_dict.values())\n        return [np.array(grouped_keypoints)]",
  "class AdjustKeypointsLocations(Processor):\n    \"\"\"Adjust the keypoint locations by removing the margins.\n    # Arguments\n        heatmaps: Numpy array.\n        grouped_keypoints: numpy array. keypoints grouped by tag\n    \"\"\"\n    def __init__(self):\n        super(AdjustKeypointsLocations, self).__init__()\n\n    def call(self, heatmaps, grouped_keypoints):\n        for batch_id, objects in enumerate(grouped_keypoints):\n            for object_id, object in enumerate(objects):\n                for keypoint_id, keypoint in enumerate(object):\n                    heatmap = heatmaps[batch_id][keypoint_id]\n                    if keypoint[2] > 0:\n                        y, x = keypoint[0:2]\n                        y = compare_vertical_neighbours(x, y, heatmap)\n                        x = compare_horizontal_neighbours(x, y, heatmap)\n                        grouped_keypoints[batch_id][\n                            object_id, keypoint_id, 0:2] = add_offset_to_point(\n                                (y, x), offset=0.5)\n        return grouped_keypoints",
  "class GetScores(Processor):\n    \"\"\"Calculate the score of the detection results.\n    # Arguments\n        grouped_keypoints: numpy array. keypoints grouped by tag\n    \"\"\"\n    def __init__(self):\n        super(GetScores, self).__init__()\n\n    def call(self, grouped_keypoints):\n        score = []\n        for keypoint in grouped_keypoints:\n            score.append(keypoint[:, 2].mean())\n        return score",
  "class RefineKeypointsLocations(Processor):\n    \"\"\"Refine the keypoint locations by removing the margins.\n    # Arguments\n        heatmaps: Numpy array.\n        Tgas: Numpy array.\n        grouped_keypoints: numpy array. keypoints grouped by tag\n    \"\"\"\n    def __init__(self):\n        super(RefineKeypointsLocations, self).__init__()\n\n    def _calculate_tags_mean(self, keypoints, tags):\n        keypoints_tags = []\n        for arg in range(keypoints.shape[0]):\n            if keypoints[arg, 2] > 0:\n                x, y = keypoints[arg][:2].astype(np.int32)\n                keypoints_tags.append(tags[arg, y, x])\n        tags_mean = np.mean(keypoints_tags, axis=0)\n        tags_mean = np.expand_dims(tags_mean, axis=[0, 1])\n        return tags_mean\n\n    def _normalize_heatmap(self, arg, tags, tags_mean, heatmap):\n        normalized_tags = (tags[arg, :, :] - tags_mean)\n        normalized_tags_squared_sum = (normalized_tags ** 2).sum(axis=2)\n        return heatmap - np.round(np.sqrt(normalized_tags_squared_sum))\n\n    def _find_max_position(self, heatmap_per_keypoint,\n                           normalized_heatmap_per_keypoint):\n        max_indices = np.argmax(normalized_heatmap_per_keypoint)\n        shape = heatmap_per_keypoint.shape\n        x, y = np.unravel_index(max_indices, shape)\n        return x, y\n\n    def _update_keypoints(self, keypoints, updated_keypoints, heatmaps):\n        updated_keypoints = np.array(updated_keypoints)\n        for i in range(heatmaps.shape[0]):\n            if updated_keypoints[i, 2] > 0 and keypoints[i, 2] == 0:\n                keypoints[i, :3] = updated_keypoints[i, :3]\n        return keypoints\n\n    def call(self, heatmaps, tags, grouped_keypoints):\n        if len(tags.shape) == 3:\n            tags = np.expand_dims(tags, -1)\n        for arg in range(len(grouped_keypoints)):\n            tags_mean = self._calculate_tags_mean(grouped_keypoints[arg], tags)\n            updated_keypoints = []\n            for keypoint_arg in range(grouped_keypoints[arg].shape[0]):\n                heatmap_per_keypoint = heatmaps[keypoint_arg, :, :]\n                normalized_heatmap_per_keypoint = self._normalize_heatmap(\n                    keypoint_arg, tags, tags_mean, heatmap_per_keypoint)\n\n                x, y = self._find_max_position(\n                    heatmap_per_keypoint, normalized_heatmap_per_keypoint)\n                max_heatmaps_value = heatmap_per_keypoint[x, y]\n                x, y = add_offset_to_point((x, y), offset=0.5)\n                y = compare_vertical_neighbours(x, y, heatmap_per_keypoint)\n                x = compare_horizontal_neighbours(x, y, heatmap_per_keypoint)\n                updated_keypoints.append((y, x, max_heatmaps_value))\n\n            grouped_keypoints[arg] = self._update_keypoints(\n                grouped_keypoints[arg], updated_keypoints, heatmaps)\n        return grouped_keypoints",
  "class TransformKeypoints(Processor):\n    \"\"\"Transform keypoint.\n\n    # Arguments\n        grouped_keypoints: numpy array. keypoints grouped by tag\n        transform: Numpy array. Transformation matrix\n    \"\"\"\n    def __init__(self):\n        super(TransformKeypoints, self).__init__()\n\n    def call(self, grouped_keypointss, transform):\n        transformed_keypointss = []\n        for keypointss in grouped_keypointss:\n            for keypoints in keypointss:\n                keypoints[0:2] = transform_keypoint(keypoints[0:2],\n                                                    transform)[:2]\n            transformed_keypointss.append(keypointss[:, :3])\n        return transformed_keypointss",
  "class ExtractKeypointsLocations(Processor):\n    \"\"\"Extract keypoint location.\n\n    # Arguments\n        keypoints: numpy array\n    \"\"\"\n    def __init__(self):\n        super(ExtractKeypointsLocations, self).__init__()\n\n    def call(self, keypoints):\n        for keypoints_arg in range(len(keypoints)):\n            keypoints[keypoints_arg] = keypoints[keypoints_arg][:, :2]\n        return keypoints",
  "def __init__(self, axes):\n        super(TransposeOutput, self).__init__()\n        self.axes = axes",
  "def call(self, outputs):\n        for arg in range(len(outputs)):\n            outputs[arg] = np.transpose(outputs[arg], self.axes)\n        return outputs",
  "def __init__(self, scale_factor, full_scaling=False):\n        super(ScaleOutput, self).__init__()\n        self.scale_factor = int(scale_factor)\n        self.full_scaling = full_scaling",
  "def _resize_output(self, output, size):\n        resized_output = []\n        for heatmap_arg, heatmap in enumerate(output):\n            resized_heatmaps = []\n            for keypoint_arg in range(len(heatmap)):\n                resized = resize_image(output[heatmap_arg][keypoint_arg], size)\n                resized_heatmaps.append(resized)\n            resized_heatmaps = np.stack(resized_heatmaps, axis=0)\n        resized_output.append(resized_heatmaps)\n        resized_output = np.stack(resized_output, axis=0)\n        return resized_output",
  "def call(self, outputs):\n        for arg in range(len(outputs)):\n            H, W = outputs[arg].shape[-2:]\n            H, W = self.scale_factor * H, self.scale_factor * W\n            if self.full_scaling:\n                outputs[arg] = self._resize_output(outputs[arg], (W, H))\n            else:\n                if len(outputs) > 1 and arg != len(outputs) - 1:\n                    outputs[arg] = self._resize_output(outputs[arg], (W, H))\n        return outputs",
  "def __init__(self, flipped_keypoint_order):\n        super(GetHeatmaps, self).__init__()\n        self.indices = flipped_keypoint_order\n        self.num_keypoints = len(flipped_keypoint_order)",
  "def call(self, outputs, with_flip):\n        num_heatmaps = 0\n        heatmap_sum = 0\n        if with_flip:\n            for output in outputs:\n                output = np.flip(output, [3])\n                heatmap_sum = heatmap_sum + get_keypoints_heatmap(\n                    output, self.num_keypoints, indices=self.indices)\n                num_heatmaps = num_heatmaps + 1\n\n        if not with_flip:\n            for output in outputs:\n                heatmap_sum = heatmap_sum + get_keypoints_heatmap(\n                    output, self.num_keypoints)\n                num_heatmaps = num_heatmaps + 1\n\n        heatmaps = heatmap_sum / num_heatmaps\n        return heatmaps",
  "def __init__(self, flipped_keypoint_order):\n        super(GetTags, self).__init__()\n        self.indices = flipped_keypoint_order\n        self.num_keypoints = len(flipped_keypoint_order)",
  "def call(self, outputs, with_flip):\n        output = outputs[0]\n        if not with_flip:\n            tags = get_tags_heatmap(output, self.num_keypoints)\n\n        if with_flip:\n            output = np.flip(output, [3])\n            tags = get_tags_heatmap(output, self.num_keypoints, self.indices)\n        return tags",
  "def __init__(self):\n        super(RemoveLastElement, self).__init__()",
  "def call(self, x):\n        if all(isinstance(each, list) for each in x):\n            return [each[:, :-1] for each in x]\n        else:\n            return x[:, :-1]",
  "def __init__(self, with_flip=False):\n        super(AggregateResults, self).__init__()\n        self.with_flip = with_flip",
  "def _expand_tags_dimension(self, tags):\n        updated_tags = []\n        for tag in tags:\n            updated_tags.append(np.expand_dims(tag, -1))\n        return updated_tags",
  "def _calculate_heatmaps_average(self, heatmaps):\n        if self.with_flip:\n            heatmaps_average = (heatmaps[0] + heatmaps[1]) / 2.0\n        else:\n            heatmaps_average = heatmaps[0]\n        return heatmaps_average",
  "def call(self, heatmaps, tags):\n        heatmaps_average = self._calculate_heatmaps_average(heatmaps)\n        heatmaps = heatmaps_average + heatmaps_average\n        tags = self._expand_tags_dimension(tags)\n        tags = np.concatenate(tags, 4)\n        return heatmaps, tags",
  "def __init__(self, k, use_numpy=False):\n        super(TopKDetections, self).__init__()\n        self.k = k\n        self.use_numpy = use_numpy",
  "def _max_pooing_2d(self, heatmaps, pool_size, strides, padding,\n                       use_numpy=False):\n        if use_numpy:\n            heatmaps = np.squeeze(heatmaps)\n            heatmaps = np.transpose(heatmaps, [2, 0, 1])\n            max_heatmaps = np.zeros_like(heatmaps)\n            for arg, heatmap in enumerate(heatmaps):\n                max_heatmaps[arg] = max_pooling_2d(heatmap, pool_size,\n                                                   strides, padding)\n            max_heatmaps = np.transpose(max_heatmaps, [1, 2, 0])\n            max_pooled_values = np.expand_dims(max_heatmaps, 0)\n        else:\n            max_pooled_values = tf.keras.layers.MaxPooling2D(\n                pool_size, strides, padding)(heatmaps)\n        return max_pooled_values",
  "def _filter_heatmaps(self, heatmaps):\n        heatmaps = np.transpose(heatmaps, [0, 2, 3, 1])\n        maximum_values = self._max_pooing_2d(heatmaps, pool_size=3, strides=1,\n                                             padding='same',\n                                             use_numpy=self.use_numpy)\n        maximum_values = np.equal(maximum_values, heatmaps)\n        maximum_values = maximum_values.astype(np.float32)\n        filtered_heatmaps = heatmaps * maximum_values\n        filtered_heatmaps = np.transpose(filtered_heatmaps, [0, 3, 1, 2])\n        return filtered_heatmaps",
  "def _get_top_k_keypoints(self, heatmaps, k, use_numpy):\n        if use_numpy:\n            top_k_keypoints, indices = get_top_k_keypoints_numpy(heatmaps, k)\n        else:\n            top_k_keypoints, indices = tf.math.top_k(heatmaps, k)\n            top_k_keypoints = np.squeeze(top_k_keypoints)\n            indices = tensor_to_numpy(indices)\n        return top_k_keypoints, indices",
  "def _get_top_k_tags(self, tags, indices):\n        indices = np.expand_dims(indices, -1)\n        gathered = gather_nd(tags, indices, axis=2)\n        return np.squeeze(gathered)",
  "def call(self, heatmaps, tags):\n        tags = tags.astype(np.int64)\n        heatmaps = self._filter_heatmaps(heatmaps)\n        num_images, keypoints_count, H, W = heatmaps.shape[:4]\n        heatmaps = np.reshape(heatmaps, [num_images, keypoints_count, -1])\n        tags = np.reshape(tags, [num_images, keypoints_count, W * H, -1])\n\n        top_k_keypoints, indices = self._get_top_k_keypoints(\n            heatmaps, self.k, self.use_numpy)\n        top_k_tags = self._get_top_k_tags(tags, indices)\n        top_k_locations = get_keypoints_locations(indices, W)\n\n        top_k_keypoints = np.expand_dims(top_k_keypoints, axis=-1)\n        top_k_detections = np.concatenate((top_k_locations,\n                                           top_k_keypoints,\n                                           top_k_tags), 2)\n        return top_k_detections",
  "def __init__(self, keypoint_order, tag_thresh, detection_thresh):\n        super(GroupKeypointsByTag, self).__init__()\n        self.keypoint_order = keypoint_order\n        self.tag_thresh = tag_thresh\n        self.detection_thresh = detection_thresh\n        self.munkres = pr.Munkres()",
  "def _update_dictionary(self, tags, keypoints, arg,\n                           default, keypoint_dict, tag_dict):\n        for tag, keypoint in zip(tags, keypoints):\n            key = tag[0]\n            keypoint_dict.setdefault(key, np.copy(default))[arg] = keypoint\n            tag_dict[key] = [tag]",
  "def _group_tags(self, grouped_keys, tag_dict):\n        grouped_tags = []\n        for arg in grouped_keys:\n            grouped_tags.append(np.mean(tag_dict[arg], axis=0))\n        return grouped_tags",
  "def call(self, detections):\n        keypoint_dict, tag_dict = {}, {}\n        default = np.zeros((detections.shape[0], detections.shape[-1]))\n\n        for arg, keypoint_arg in enumerate(self.keypoint_order):\n            keypoints = get_valid_detections(detections[keypoint_arg],\n                                             self.detection_thresh)\n            tags = keypoints[:, -2:]\n            if arg == 0 or len(keypoint_dict) == 0:\n                self._update_dictionary(tags, keypoints, keypoint_arg,\n                                        default, keypoint_dict, tag_dict)\n            else:\n                grouped_keys = list(keypoint_dict.keys())\n                grouped_tags = self._group_tags(grouped_keys, tag_dict)\n                difference = np.expand_dims(tags, 1) - np.expand_dims(\n                    grouped_tags, 0)\n                norm = calculate_norm(difference, order=2, axis=2)\n                norm = pad_matrix(norm, padding='square', value=1e10)\n                lowest_cost = self.munkres.compute(norm)\n                lowest_cost = np.array(lowest_cost).astype(np.int32)\n\n                for row_arg, col_arg in lowest_cost:\n                    if norm[row_arg][col_arg] < self.tag_thresh:\n                        key = grouped_keys[col_arg]\n                        keypoint_dict[key][keypoint_arg] = keypoints[row_arg]\n                        tag_dict[key].append(tags[row_arg])\n                    else:\n                        self._update_dictionary(tags, keypoints, keypoint_arg,\n                                                default, keypoint_dict,\n                                                tag_dict)\n        grouped_keypoints = list(keypoint_dict.values())\n        return [np.array(grouped_keypoints)]",
  "def __init__(self):\n        super(AdjustKeypointsLocations, self).__init__()",
  "def call(self, heatmaps, grouped_keypoints):\n        for batch_id, objects in enumerate(grouped_keypoints):\n            for object_id, object in enumerate(objects):\n                for keypoint_id, keypoint in enumerate(object):\n                    heatmap = heatmaps[batch_id][keypoint_id]\n                    if keypoint[2] > 0:\n                        y, x = keypoint[0:2]\n                        y = compare_vertical_neighbours(x, y, heatmap)\n                        x = compare_horizontal_neighbours(x, y, heatmap)\n                        grouped_keypoints[batch_id][\n                            object_id, keypoint_id, 0:2] = add_offset_to_point(\n                                (y, x), offset=0.5)\n        return grouped_keypoints",
  "def __init__(self):\n        super(GetScores, self).__init__()",
  "def call(self, grouped_keypoints):\n        score = []\n        for keypoint in grouped_keypoints:\n            score.append(keypoint[:, 2].mean())\n        return score",
  "def __init__(self):\n        super(RefineKeypointsLocations, self).__init__()",
  "def _calculate_tags_mean(self, keypoints, tags):\n        keypoints_tags = []\n        for arg in range(keypoints.shape[0]):\n            if keypoints[arg, 2] > 0:\n                x, y = keypoints[arg][:2].astype(np.int32)\n                keypoints_tags.append(tags[arg, y, x])\n        tags_mean = np.mean(keypoints_tags, axis=0)\n        tags_mean = np.expand_dims(tags_mean, axis=[0, 1])\n        return tags_mean",
  "def _normalize_heatmap(self, arg, tags, tags_mean, heatmap):\n        normalized_tags = (tags[arg, :, :] - tags_mean)\n        normalized_tags_squared_sum = (normalized_tags ** 2).sum(axis=2)\n        return heatmap - np.round(np.sqrt(normalized_tags_squared_sum))",
  "def _find_max_position(self, heatmap_per_keypoint,\n                           normalized_heatmap_per_keypoint):\n        max_indices = np.argmax(normalized_heatmap_per_keypoint)\n        shape = heatmap_per_keypoint.shape\n        x, y = np.unravel_index(max_indices, shape)\n        return x, y",
  "def _update_keypoints(self, keypoints, updated_keypoints, heatmaps):\n        updated_keypoints = np.array(updated_keypoints)\n        for i in range(heatmaps.shape[0]):\n            if updated_keypoints[i, 2] > 0 and keypoints[i, 2] == 0:\n                keypoints[i, :3] = updated_keypoints[i, :3]\n        return keypoints",
  "def call(self, heatmaps, tags, grouped_keypoints):\n        if len(tags.shape) == 3:\n            tags = np.expand_dims(tags, -1)\n        for arg in range(len(grouped_keypoints)):\n            tags_mean = self._calculate_tags_mean(grouped_keypoints[arg], tags)\n            updated_keypoints = []\n            for keypoint_arg in range(grouped_keypoints[arg].shape[0]):\n                heatmap_per_keypoint = heatmaps[keypoint_arg, :, :]\n                normalized_heatmap_per_keypoint = self._normalize_heatmap(\n                    keypoint_arg, tags, tags_mean, heatmap_per_keypoint)\n\n                x, y = self._find_max_position(\n                    heatmap_per_keypoint, normalized_heatmap_per_keypoint)\n                max_heatmaps_value = heatmap_per_keypoint[x, y]\n                x, y = add_offset_to_point((x, y), offset=0.5)\n                y = compare_vertical_neighbours(x, y, heatmap_per_keypoint)\n                x = compare_horizontal_neighbours(x, y, heatmap_per_keypoint)\n                updated_keypoints.append((y, x, max_heatmaps_value))\n\n            grouped_keypoints[arg] = self._update_keypoints(\n                grouped_keypoints[arg], updated_keypoints, heatmaps)\n        return grouped_keypoints",
  "def __init__(self):\n        super(TransformKeypoints, self).__init__()",
  "def call(self, grouped_keypointss, transform):\n        transformed_keypointss = []\n        for keypointss in grouped_keypointss:\n            for keypoints in keypointss:\n                keypoints[0:2] = transform_keypoint(keypoints[0:2],\n                                                    transform)[:2]\n            transformed_keypointss.append(keypointss[:, :3])\n        return transformed_keypointss",
  "def __init__(self):\n        super(ExtractKeypointsLocations, self).__init__()",
  "def call(self, keypoints):\n        for keypoints_arg in range(len(keypoints)):\n            keypoints[keypoints_arg] = keypoints[keypoints_arg][:, :2]\n        return keypoints",
  "class SquareBoxes2D(Processor):\n    \"\"\"Transforms bounding rectangular boxes into square bounding boxes.\n    \"\"\"\n    def __init__(self):\n        super(SquareBoxes2D, self).__init__()\n\n    def call(self, boxes2D):\n        for box2D in boxes2D:\n            box2D.coordinates = make_box_square(box2D.coordinates)\n        return boxes2D",
  "class DenormalizeBoxes2D(Processor):\n    \"\"\"Denormalizes boxes shapes to be in accordance to the original\n    image size.\n\n    # Arguments:\n        image_size: List containing height and width of an image.\n    \"\"\"\n    def __init__(self):\n        super(DenormalizeBoxes2D, self).__init__()\n\n    def call(self, image, boxes2D):\n        shape = image.shape[:2]\n        for box2D in boxes2D:\n            box2D.coordinates = denormalize_box(box2D.coordinates, shape)\n        return boxes2D",
  "class RoundBoxes2D(Processor):\n    \"\"\"Round to integer box coordinates.\n    \"\"\"\n    def __init__(self):\n        super(RoundBoxes2D, self).__init__()\n\n    def call(self, boxes2D):\n        for box2D in boxes2D:\n            box2D.coordinates = [int(x) for x in box2D.coordinates]\n        return boxes2D",
  "class FilterClassBoxes2D(Processor):\n    \"\"\"Filters boxes with valid class names.\n\n    # Arguments\n        valid_class_names: List of strings indicating class names to be kept.\n    \"\"\"\n    def __init__(self, valid_class_names):\n        self.valid_class_names = valid_class_names\n        super(FilterClassBoxes2D, self).__init__()\n\n    def call(self, boxes2D):\n        filtered_boxes2D = []\n        for box2D in boxes2D:\n            if box2D.class_name in self.valid_class_names:\n                filtered_boxes2D.append(box2D)\n        return filtered_boxes2D",
  "class CropBoxes2D(Processor):\n    \"\"\"Creates a list of images cropped from the bounding boxes.\n\n    # Arguments\n        offset_scales: List of floats having x and y scales respectively.\n    \"\"\"\n    def __init__(self):\n        super(CropBoxes2D, self).__init__()\n\n    def call(self, image, boxes2D):\n        image_crops = []\n        for box2D in boxes2D:\n            x_min, y_min, x_max, y_max = box2D.coordinates\n            image_crops.append(image[y_min:y_max, x_min:x_max])\n        return image_crops",
  "class ClipBoxes2D(Processor):\n    \"\"\"Clips boxes coordinates into the image dimensions\"\"\"\n    def __init__(self):\n        super(ClipBoxes2D, self).__init__()\n\n    def call(self, image, boxes2D):\n        image_height, image_width = image.shape[:2]\n        for box2D in boxes2D:\n            box2D.coordinates = clip(box2D.coordinates, image.shape[:2])\n        return boxes2D",
  "class OffsetBoxes2D(Processor):\n    \"\"\"Offsets the height and widht of a list of ``Boxes2D``.\n\n    # Arguments\n        offsets: Float between [0, 1].\n    \"\"\"\n    def __init__(self, offsets):\n        super(OffsetBoxes2D, self).__init__()\n        self.offsets = offsets\n\n    def call(self, boxes2D):\n        for box2D in boxes2D:\n            box2D.coordinates = offset(box2D.coordinates, self.offsets)\n        return boxes2D",
  "class ToBoxes2D(Processor):\n    \"\"\"Transforms boxes from dataset into `Boxes2D` messages.\n\n    # Arguments\n        class_names: List of class names ordered with respect to the\n            class indices from the dataset ``boxes``.\n        one_hot_encoded: Bool, indicating if scores are one hot vectors.\n        default_score: Float, score to set.\n        default_class: Str, class to set.\n        box_method: Int, method to convert boxes to ``Boxes2D``.\n\n    # Properties\n        one_hot_encoded: Bool.\n        box_processor: Callable.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(\n            self, class_names=None, one_hot_encoded=False,\n            default_score=1.0, default_class=None, box_method=0):\n        if class_names is not None:\n            arg_to_class = dict(zip(range(len(class_names)), class_names))\n        self.one_hot_encoded = one_hot_encoded\n        method_to_processor = {\n            0: BoxesWithOneHotVectorsToBoxes2D(arg_to_class),\n            1: BoxesToBoxes2D(default_score, default_class),\n            2: BoxesWithClassArgToBoxes2D(arg_to_class, default_score)}\n        self.box_processor = method_to_processor[box_method]\n        super(ToBoxes2D, self).__init__()\n\n    def call(self, box_data):\n        return self.box_processor(box_data)",
  "class BoxesToBoxes2D(Processor):\n    \"\"\"Transforms boxes from dataset into `Boxes2D` messages given no\n    class names and score.\n\n    # Arguments\n        default_score: Float, score to set.\n        default_class: Str, class to set.\n\n    # Properties\n        default_score: Float.\n        default_class: Str.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self, default_score=1.0, default_class=None):\n        self.default_score = default_score\n        self.default_class = default_class\n        super(BoxesToBoxes2D, self).__init__()\n\n    def call(self, box_data):\n        boxes2D = []\n        for box in box_data:\n            boxes2D.append(\n                Box2D(box[:4], self.default_score, self.default_class))\n        return boxes2D",
  "class BoxesWithOneHotVectorsToBoxes2D(Processor):\n    \"\"\"Transforms boxes from dataset into `Boxes2D` messages given boxes\n    with scores as one hot vectors.\n\n    # Arguments\n        arg_to_class: List, of classes.\n\n    # Properties\n        arg_to_class: List.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self, arg_to_class):\n        self.arg_to_class = arg_to_class\n        super(BoxesWithOneHotVectorsToBoxes2D, self).__init__()\n\n    def call(self, box_data):\n        boxes2D = []\n        for box in box_data:\n            class_scores = box[4:]\n            class_arg = np.argmax(class_scores)\n            score = class_scores[class_arg]\n            class_name = self.arg_to_class[class_arg]\n            boxes2D.append(Box2D(box[:4], score, class_name))\n        return boxes2D",
  "class BoxesWithClassArgToBoxes2D(Processor):\n    \"\"\"Transforms boxes from dataset into `Boxes2D` messages given boxes\n    with class argument.\n\n    # Arguments\n        default_score: Float, score to set.\n        arg_to_class: List, of classes.\n\n    # Properties\n        default_score: Float.\n        arg_to_class: List.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self, arg_to_class, default_score=1.0):\n        self.default_score = default_score\n        self.arg_to_class = arg_to_class\n        super(BoxesWithClassArgToBoxes2D, self).__init__()\n\n    def call(self, box_data):\n        boxes2D = []\n        for box in box_data:\n            class_name = self.arg_to_class[box[-1]]\n            boxes2D.append(Box2D(box[:4], self.default_score, class_name))\n        return boxes2D",
  "class RoundBoxes(Processor):\n    \"\"\"Rounds the floating value coordinates of the box coordinates\n    into integer type.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self):\n        super(RoundBoxes, self).__init__()\n\n    def call(self, boxes2D):\n        for box2D in boxes2D:\n            box2D.coordinates = box2D.coordinates.astype(int)\n        return boxes2D",
  "class MatchBoxes(Processor):\n    \"\"\"Match prior boxes with ground truth boxes.\n\n    # Arguments\n        prior_boxes: Numpy array of shape (num_boxes, 4).\n        iou: Float in [0, 1]. Intersection over union in which prior boxes\n            will be considered positive. A positive box is box with a class\n            different than `background`.\n        variance: List of two floats.\n    \"\"\"\n    def __init__(self, prior_boxes, iou=.5):\n        self.prior_boxes = prior_boxes\n        self.iou = iou\n        super(MatchBoxes, self).__init__()\n\n    def call(self, boxes):\n        boxes = match(boxes, self.prior_boxes, self.iou)\n        return boxes",
  "class EncodeBoxes(Processor):\n    \"\"\"Encodes bounding boxes.\n\n    # Arguments\n        prior_boxes: Numpy array of shape (num_boxes, 4).\n        variances: List of two float values.\n    \"\"\"\n    def __init__(self, prior_boxes, variances=[0.1, 0.1, 0.2, 0.2]):\n        self.prior_boxes = prior_boxes\n        self.variances = variances\n        super(EncodeBoxes, self).__init__()\n\n    def call(self, boxes):\n        encoded_boxes = encode(boxes, self.prior_boxes, self.variances)\n        return encoded_boxes",
  "class DecodeBoxes(Processor):\n    \"\"\"Decodes bounding boxes.\n\n    # Arguments\n        prior_boxes: Numpy array of shape (num_boxes, 4).\n        variances: List of two float values.\n    \"\"\"\n    def __init__(self, prior_boxes, variances=[0.1, 0.1, 0.2, 0.2]):\n        self.prior_boxes = prior_boxes\n        self.variances = variances\n        super(DecodeBoxes, self).__init__()\n\n    def call(self, boxes):\n        decoded_boxes = decode(boxes, self.prior_boxes, self.variances)\n        return decoded_boxes",
  "class NonMaximumSuppressionPerClass(Processor):\n    \"\"\"Applies non maximum suppression per class.\n\n    # Arguments\n        nms_thresh: Float between [0, 1].\n        epsilon: Float between [0, 1].\n    \"\"\"\n    def __init__(self, nms_thresh=.45, epsilon=0.01):\n        self.nms_thresh = nms_thresh\n        self.epsilon = epsilon\n        super(NonMaximumSuppressionPerClass, self).__init__()\n\n    def call(self, box_data):\n        box_data, class_labels = nms_per_class(\n            box_data, self.nms_thresh, self.epsilon)\n        return box_data, class_labels",
  "class MergeNMSBoxWithClass(Processor):\n    \"\"\"Merges box coordinates with their corresponding class\n    defined by `class_labels` which is decided by best box geometry\n    by non maximum suppression (and not by the best scoring class)\n    into a single output.\n    \"\"\"\n    def __init__(self):\n        super(MergeNMSBoxWithClass, self).__init__()\n\n    def call(self, box_data, class_labels):\n        box_data = merge_nms_box_with_class(box_data, class_labels)\n        return box_data",
  "class FilterBoxes(Processor):\n    \"\"\"Filters boxes outputted from function ``detect`` as\n    ``Box2D`` messages.\n\n    # Arguments\n        class_names: List of class names.\n        conf_thresh: Float between [0, 1].\n    \"\"\"\n    def __init__(self, class_names, conf_thresh=0.5):\n        self.class_names = class_names\n        self.conf_thresh = conf_thresh\n        self.arg_to_class = dict(zip(\n            list(range(len(self.class_names))), self.class_names))\n        super(FilterBoxes, self).__init__()\n\n    def call(self, box_data):\n        box_data = filter_boxes(box_data, self.conf_thresh)\n        return box_data",
  "class CropImage(Processor):\n    \"\"\"Crop images using a list of ``box2D``.\n    \"\"\"\n    def __init__(self):\n        super(CropImage, self).__init__()\n\n    def call(self, image, box2D):\n        x_min, y_min, x_max, y_max = box2D.coordinates\n        return image[y_min:y_max, x_min:x_max]",
  "class RemoveClass(Processor):\n    \"\"\"Remove a particular class from the pipeline.\n\n    # Arguments\n        class_names: List, indicating given class names.\n        class_arg: Int, index of the class to be removed.\n        renormalize: Bool, if true scores are renormalized.\n\n    # Properties\n        class_arg: Int.\n        renormalize: Bool\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self, class_names, class_arg=None, renormalize=False):\n        self.class_arg = class_arg\n        self.renormalize = renormalize\n        if class_arg is not None:\n            del class_names[class_arg]\n        super(RemoveClass, self).__init__()\n\n    def call(self, box_data):\n        if not self.renormalize and self.class_arg is not None:\n            box_data = np.delete(box_data, 4 + self.class_arg, axis=1)\n        elif self.renormalize:\n            raise NotImplementedError\n        return box_data",
  "class ScaleBox(Processor):\n    \"\"\"Scale box coordinates of the prediction.\n\n    # Arguments\n        scales: Array of shape `()`, value to scale boxes.\n\n    # Properties\n        scales: Int.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self):\n        super(ScaleBox, self).__init__()\n\n    def call(self, boxes, scales):\n        boxes = scale_box(boxes, scales)\n        return boxes",
  "def __init__(self):\n        super(SquareBoxes2D, self).__init__()",
  "def call(self, boxes2D):\n        for box2D in boxes2D:\n            box2D.coordinates = make_box_square(box2D.coordinates)\n        return boxes2D",
  "def __init__(self):\n        super(DenormalizeBoxes2D, self).__init__()",
  "def call(self, image, boxes2D):\n        shape = image.shape[:2]\n        for box2D in boxes2D:\n            box2D.coordinates = denormalize_box(box2D.coordinates, shape)\n        return boxes2D",
  "def __init__(self):\n        super(RoundBoxes2D, self).__init__()",
  "def call(self, boxes2D):\n        for box2D in boxes2D:\n            box2D.coordinates = [int(x) for x in box2D.coordinates]\n        return boxes2D",
  "def __init__(self, valid_class_names):\n        self.valid_class_names = valid_class_names\n        super(FilterClassBoxes2D, self).__init__()",
  "def call(self, boxes2D):\n        filtered_boxes2D = []\n        for box2D in boxes2D:\n            if box2D.class_name in self.valid_class_names:\n                filtered_boxes2D.append(box2D)\n        return filtered_boxes2D",
  "def __init__(self):\n        super(CropBoxes2D, self).__init__()",
  "def call(self, image, boxes2D):\n        image_crops = []\n        for box2D in boxes2D:\n            x_min, y_min, x_max, y_max = box2D.coordinates\n            image_crops.append(image[y_min:y_max, x_min:x_max])\n        return image_crops",
  "def __init__(self):\n        super(ClipBoxes2D, self).__init__()",
  "def call(self, image, boxes2D):\n        image_height, image_width = image.shape[:2]\n        for box2D in boxes2D:\n            box2D.coordinates = clip(box2D.coordinates, image.shape[:2])\n        return boxes2D",
  "def __init__(self, offsets):\n        super(OffsetBoxes2D, self).__init__()\n        self.offsets = offsets",
  "def call(self, boxes2D):\n        for box2D in boxes2D:\n            box2D.coordinates = offset(box2D.coordinates, self.offsets)\n        return boxes2D",
  "def __init__(\n            self, class_names=None, one_hot_encoded=False,\n            default_score=1.0, default_class=None, box_method=0):\n        if class_names is not None:\n            arg_to_class = dict(zip(range(len(class_names)), class_names))\n        self.one_hot_encoded = one_hot_encoded\n        method_to_processor = {\n            0: BoxesWithOneHotVectorsToBoxes2D(arg_to_class),\n            1: BoxesToBoxes2D(default_score, default_class),\n            2: BoxesWithClassArgToBoxes2D(arg_to_class, default_score)}\n        self.box_processor = method_to_processor[box_method]\n        super(ToBoxes2D, self).__init__()",
  "def call(self, box_data):\n        return self.box_processor(box_data)",
  "def __init__(self, default_score=1.0, default_class=None):\n        self.default_score = default_score\n        self.default_class = default_class\n        super(BoxesToBoxes2D, self).__init__()",
  "def call(self, box_data):\n        boxes2D = []\n        for box in box_data:\n            boxes2D.append(\n                Box2D(box[:4], self.default_score, self.default_class))\n        return boxes2D",
  "def __init__(self, arg_to_class):\n        self.arg_to_class = arg_to_class\n        super(BoxesWithOneHotVectorsToBoxes2D, self).__init__()",
  "def call(self, box_data):\n        boxes2D = []\n        for box in box_data:\n            class_scores = box[4:]\n            class_arg = np.argmax(class_scores)\n            score = class_scores[class_arg]\n            class_name = self.arg_to_class[class_arg]\n            boxes2D.append(Box2D(box[:4], score, class_name))\n        return boxes2D",
  "def __init__(self, arg_to_class, default_score=1.0):\n        self.default_score = default_score\n        self.arg_to_class = arg_to_class\n        super(BoxesWithClassArgToBoxes2D, self).__init__()",
  "def call(self, box_data):\n        boxes2D = []\n        for box in box_data:\n            class_name = self.arg_to_class[box[-1]]\n            boxes2D.append(Box2D(box[:4], self.default_score, class_name))\n        return boxes2D",
  "def __init__(self):\n        super(RoundBoxes, self).__init__()",
  "def call(self, boxes2D):\n        for box2D in boxes2D:\n            box2D.coordinates = box2D.coordinates.astype(int)\n        return boxes2D",
  "def __init__(self, prior_boxes, iou=.5):\n        self.prior_boxes = prior_boxes\n        self.iou = iou\n        super(MatchBoxes, self).__init__()",
  "def call(self, boxes):\n        boxes = match(boxes, self.prior_boxes, self.iou)\n        return boxes",
  "def __init__(self, prior_boxes, variances=[0.1, 0.1, 0.2, 0.2]):\n        self.prior_boxes = prior_boxes\n        self.variances = variances\n        super(EncodeBoxes, self).__init__()",
  "def call(self, boxes):\n        encoded_boxes = encode(boxes, self.prior_boxes, self.variances)\n        return encoded_boxes",
  "def __init__(self, prior_boxes, variances=[0.1, 0.1, 0.2, 0.2]):\n        self.prior_boxes = prior_boxes\n        self.variances = variances\n        super(DecodeBoxes, self).__init__()",
  "def call(self, boxes):\n        decoded_boxes = decode(boxes, self.prior_boxes, self.variances)\n        return decoded_boxes",
  "def __init__(self, nms_thresh=.45, epsilon=0.01):\n        self.nms_thresh = nms_thresh\n        self.epsilon = epsilon\n        super(NonMaximumSuppressionPerClass, self).__init__()",
  "def call(self, box_data):\n        box_data, class_labels = nms_per_class(\n            box_data, self.nms_thresh, self.epsilon)\n        return box_data, class_labels",
  "def __init__(self):\n        super(MergeNMSBoxWithClass, self).__init__()",
  "def call(self, box_data, class_labels):\n        box_data = merge_nms_box_with_class(box_data, class_labels)\n        return box_data",
  "def __init__(self, class_names, conf_thresh=0.5):\n        self.class_names = class_names\n        self.conf_thresh = conf_thresh\n        self.arg_to_class = dict(zip(\n            list(range(len(self.class_names))), self.class_names))\n        super(FilterBoxes, self).__init__()",
  "def call(self, box_data):\n        box_data = filter_boxes(box_data, self.conf_thresh)\n        return box_data",
  "def __init__(self):\n        super(CropImage, self).__init__()",
  "def call(self, image, box2D):\n        x_min, y_min, x_max, y_max = box2D.coordinates\n        return image[y_min:y_max, x_min:x_max]",
  "def __init__(self, class_names, class_arg=None, renormalize=False):\n        self.class_arg = class_arg\n        self.renormalize = renormalize\n        if class_arg is not None:\n            del class_names[class_arg]\n        super(RemoveClass, self).__init__()",
  "def call(self, box_data):\n        if not self.renormalize and self.class_arg is not None:\n            box_data = np.delete(box_data, 4 + self.class_arg, axis=1)\n        elif self.renormalize:\n            raise NotImplementedError\n        return box_data",
  "def __init__(self):\n        super(ScaleBox, self).__init__()",
  "def call(self, boxes, scales):\n        boxes = scale_box(boxes, scales)\n        return boxes",
  "class Render(Processor):\n    \"\"\"Render images and labels.\n\n    # Arguments\n        renderer: Object that renders images and labels using a method\n            ''render_sample()''.\n    \"\"\"\n    def __init__(self, renderer):\n        super(Render, self).__init__()\n        self.renderer = renderer\n\n    def call(self):\n        return self.renderer.render()",
  "def __init__(self, renderer):\n        super(Render, self).__init__()\n        self.renderer = renderer",
  "def call(self):\n        return self.renderer.render()",
  "class DrawBoxes2D(Processor):\n    \"\"\"Draws bounding boxes from Boxes2D messages.\n\n    # Arguments\n        class_names: List of strings.\n        colors: List of lists containing the color values\n        weighted: Boolean. If ``True`` the colors are weighted with the\n            score of the bounding box.\n        scale: Float. Scale of drawn text.\n    \"\"\"\n    def __init__(self, class_names=None, colors=None,\n                 weighted=False, scale=0.7, with_score=True):\n        self.class_names = class_names\n        self.colors = colors\n        self.weighted = weighted\n        self.with_score = with_score\n        self.scale = scale\n\n        if (self.class_names is not None and\n                not isinstance(self.class_names, list)):\n            raise TypeError(\"Class name should be of type 'List of strings'\")\n\n        if (self.colors is not None and\n                not all(isinstance(color, list) for color in self.colors)):\n            raise TypeError(\"Colors should be of type 'List of lists'\")\n\n        if self.colors is None:\n            self.colors = lincolor(len(self.class_names))\n\n        if self.class_names is not None:\n            self.class_to_color = dict(zip(self.class_names, self.colors))\n        else:\n            self.class_to_color = {None: self.colors, '': self.colors}\n        super(DrawBoxes2D, self).__init__()\n\n    def call(self, image, boxes2D):\n        for box2D in boxes2D:\n            x_min, y_min, x_max, y_max = box2D.coordinates\n            class_name = box2D.class_name\n            color = self.class_to_color[class_name]\n            if self.weighted:\n                color = [int(channel * box2D.score) for channel in color]\n            if self.with_score:\n                text = '{:0.2f}, {}'.format(box2D.score, class_name)\n            if not self.with_score:\n                text = '{}'.format(class_name)\n            put_text(image, text, (x_min, y_min - 10), self.scale, color, 1)\n            draw_rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n        return image",
  "class DrawKeypoints2D(Processor):\n    \"\"\"Draws keypoints into image.\n\n    # Arguments\n        num_keypoints: Int. Used initialize colors for each keypoint\n        radius: Float. Approximate radius of the circle in pixel coordinates.\n    \"\"\"\n    def __init__(self, num_keypoints, radius=3, normalized=False):\n        super(DrawKeypoints2D, self).__init__()\n        self.colors = lincolor(num_keypoints, normalized=normalized)\n        self.radius = radius\n\n    def call(self, image, keypoints):\n        for keypoint_arg, keypoint in enumerate(keypoints):\n            color = self.colors[keypoint_arg]\n            draw_keypoint(image, keypoint.astype('int'), color, self.radius)\n        return image",
  "class DrawBoxes3D(Processor):\n    def __init__(self, camera, class_to_dimensions,\n                 color=GREEN, thickness=5, radius=2):\n        \"\"\"Draw boxes 3D of multiple objects\n\n        # Arguments\n            camera: Instance of ``paz.backend.camera.Camera''.\n            class_to_dimensions: Dictionary that has as keys the\n                class names and as value a list [model_height, model_width]\n            thickness: Int. Thickness of 3D box\n        \"\"\"\n        super(DrawBoxes3D, self).__init__()\n        self.camera = camera\n        self.class_to_dimensions = class_to_dimensions\n        self.class_to_points = self._build_class_to_points(class_to_dimensions)\n        self.color = color\n        self.radius = radius\n        self.thickness = thickness\n\n    def _build_class_to_points(self, class_to_dimensions):\n        class_to_points = {}\n        for class_name, dimensions in self.class_to_dimensions.items():\n            width, height, depth = dimensions\n            points = build_cube_points3D(width, height, depth)\n            class_to_points[class_name] = points\n        return class_to_points\n\n    def call(self, image, pose6D):\n        points3D = self.class_to_points[pose6D.class_name]\n        points2D = project_points3D(points3D, pose6D, self.camera)\n        points2D = points2D.astype(np.int32)\n        draw_cube(image, points2D, self.color, self.thickness, self.radius)\n        return image",
  "class DrawRandomPolygon(Processor):\n    \"\"\" Adds occlusion to image\n\n    # Arguments\n        max_radius_scale: Maximum radius in scale with respect to image i.e.\n                each vertex radius from the polygon is sampled\n                from ``[0, max_radius_scale]``. This radius is later\n                multiplied by the image dimensions.\n    \"\"\"\n    def __init__(self, max_radius_scale=.5):\n        super(DrawRandomPolygon, self).__init__()\n        self.max_radius_scale = max_radius_scale\n\n    def call(self, image):\n        return draw_random_polygon(image)",
  "def draw_pose6D(image, pose6D, points3D, intrinsics, thickness):\n    \"\"\"Draws cube in image by projecting points3D with intrinsics and pose6D.\n\n    # Arguments\n        image: Array (H, W).\n        pose6D: paz.abstract.Pose6D instance.\n        intrinsics: Array (3, 3). Camera intrinsics for projecting\n            3D rays into 2D image.\n        points3D: Array (num_points, 3).\n        thickness: Positive integer indicating line thickness.\n\n    # Returns\n        Image array (H, W) with drawn inferences.\n    \"\"\"\n    quaternion, translation = pose6D.quaternion, pose6D.translation\n    rotation = quaternion_to_rotation_matrix(quaternion)\n    points2D = project_to_image(rotation, translation, points3D, intrinsics)\n    image = draw_cube(image, points2D.astype(np.int32), thickness=thickness)\n    return image",
  "class DrawPoses6D(Processor):\n    \"\"\"Draws multiple cubes in image by projecting points3D.\n\n    # Arguments\n        object_sizes: Array (3) indicating (x, y, z) sizes of object.\n        camera_intrinsics: Array (3, 3).\n            Camera intrinsics for projecting 3D rays into 2D image.\n        thickness: Positive integer indicating line thickness.\n\n    # Returns\n        Image array (H, W) with drawn inferences.\n    \"\"\"\n    def __init__(self, object_sizes, camera_intrinsics, thickness=2):\n        self.points3D = build_cube_points3D(*object_sizes)\n        self.intrinsics = camera_intrinsics\n        self.thickness = thickness\n\n    def call(self, image, poses6D):\n        if poses6D is None:\n            return image\n        if not isinstance(poses6D, list):\n            raise ValueError('Poses6D must be a list of Pose6D messages')\n        for pose6D in poses6D:\n            image = draw_pose6D(\n                image, pose6D, self.points3D, self.intrinsics, self.thickness)\n        return image",
  "class DrawPose6D(Processor):\n    \"\"\"Draws a single cube in image by projecting points3D.\n\n    # Arguments\n        object_sizes: Array (3) indicating (x, y, z) sizes of object.\n        camera_intrinsics: Array (3, 3).\n            Camera intrinsics for projecting 3D rays into 2D image.\n        thickness: Positive integer indicating line thickness.\n\n    # Returns\n        Image array (H, W) with drawn inferences.\n    \"\"\"\n    def __init__(self, object_sizes, camera_intrinsics, thickness=2):\n        self.points3D = build_cube_points3D(*object_sizes)\n        self.intrinsics = camera_intrinsics\n        self.thickness = thickness\n\n    def call(self, image, pose6D):\n        if pose6D is None:\n            return image\n        image = draw_pose6D(\n            image, pose6D, self.points3D, self.intrinsics, self.thickness)\n        return image",
  "class DrawHumanSkeleton(Processor):\n    \"\"\" Draw human pose skeleton on image.\n\n    # Arguments\n        images: Numpy array.\n        grouped_joints: Joint locations of all the person model detected\n                        in the image. List of numpy array.\n        dataset: String.\n        check_scores: Boolean. Flag to check score before drawing.\n\n    # Returns\n        A numpy array containing pose skeleton.\n    \"\"\"\n    def __init__(self, dataset, check_scores, link_width=2, keypoint_radius=4):\n        super(DrawHumanSkeleton, self).__init__()\n        self.link_orders = HUMAN_JOINT_CONFIG[dataset]['part_orders']\n        self.link_colors = HUMAN_JOINT_CONFIG[dataset]['part_color']\n        self.link_args = HUMAN_JOINT_CONFIG[dataset]['part_arg']\n        self.keypoint_colors = HUMAN_JOINT_CONFIG[dataset]['joint_color']\n        self.check_scores = check_scores\n        self.link_width = link_width\n        self.keypoint_radius = keypoint_radius\n\n    def call(self, image, grouped_joints):\n        for one_person_joints in grouped_joints:\n            image = draw_keypoints_link(\n                image, one_person_joints, self.link_args, self.link_orders,\n                self.link_colors, self.check_scores, self.link_width)\n            image = draw_keypoints(image, one_person_joints,\n                                   self.keypoint_colors, self.check_scores,\n                                   self.keypoint_radius)\n        return image",
  "class DrawHandSkeleton(Processor):\n    \"\"\" Draw hand pose skeleton on image.\n\n    # Arguments\n        image: Array (H, W, 3)\n        keypoints: Array. All the joint locations detected by model\n                        in the image.\n    # Returns\n        A numpy array containing pose skeleton.\n    \"\"\"\n    def __init__(self, check_scores=False, link_width=2, keypoint_radius=4):\n        super(DrawHandSkeleton, self).__init__()\n        self.link_orders = MINIMAL_HAND_CONFIG['part_orders']\n        self.link_colors = MINIMAL_HAND_CONFIG['part_color']\n        self.link_args = MINIMAL_HAND_CONFIG['part_arg']\n        self.keypoint_colors = MINIMAL_HAND_CONFIG['joint_color']\n        self.check_scores = check_scores\n        self.link_width = link_width\n        self.keypoint_radius = keypoint_radius\n\n    def call(self, image, keypoints):\n        image = draw_keypoints_link(\n            image, keypoints, self.link_args, self.link_orders,\n            self.link_colors, self.check_scores, self.link_width)\n        image = draw_keypoints(image, keypoints, self.keypoint_colors,\n                               self.check_scores, self.keypoint_radius)\n        return image",
  "class DrawRGBMask(Processor):\n    \"\"\"Draws RGB mask by transforming points3D to RGB space and putting in\n        them in their 2D coordinates (points2D)\n\n    # Arguments\n        object_sizes: Array (x_size, y_size, z_size)\n    \"\"\"\n    def __init__(self, object_sizes):\n        super(DrawRGBMask, self).__init__()\n        self.object_sizes = object_sizes\n\n    def call(self, image, points2D, points3D):\n        image = draw_RGB_mask(image, points2D, points3D, self.object_sizes)\n        return image",
  "class DrawRGBMasks(Processor):\n    \"\"\"Draws RGB masks by transforming points3D to RGB space and putting in\n        them in their 2D coordinates (points2D)\n\n    # Arguments\n        object_sizes: Array (x_size, y_size, z_size)\n    \"\"\"\n    def __init__(self, object_sizes):\n        super(DrawRGBMasks, self).__init__()\n        self.object_sizes = object_sizes\n\n    def call(self, image, points2D, points3D):\n        return draw_RGB_masks(image, points2D, points3D, self.object_sizes)",
  "class DrawText(Processor):\n    \"\"\"Draws text to image.\n\n    # Arguments\n        color: List. Color of text to\n        thickness: Int. Thickness of text.\n        scale: Int. Size scale for text.\n        message: Str. Text to be added on the image.\n        location: List/tuple of int. Pixel corordinte in image to add text.\n    \"\"\"\n    def __init__(self, color=GREEN, thickness=2, scale=1):\n        super(DrawText, self).__init__()\n        self.color = color\n        self.thickness = thickness\n        self.scale = scale\n\n    def call(self, image, message, location=(50, 50)):\n        image = put_text(image, message, location, self.scale,\n                         self.color, self.thickness)\n        return image",
  "class DrawHumanPose6D(Processor):\n    \"\"\"Draw basis vectors for human pose 6D\n\n    # Arguments\n        image: numpy array\n        rotation: numpy array of size (3 x 3)\n        translations: list of length 3\n\n    # Returns\n        image: numpy array\n    \"\"\"\n    def __init__(self, camera_intrinsics):\n        super(DrawHumanPose6D, self).__init__()\n        self.K = camera_intrinsics\n\n    def call(self, image, rotation, translation):\n        image = draw_human_pose6D(image, rotation, translation, self.K)\n        return image",
  "def __init__(self, class_names=None, colors=None,\n                 weighted=False, scale=0.7, with_score=True):\n        self.class_names = class_names\n        self.colors = colors\n        self.weighted = weighted\n        self.with_score = with_score\n        self.scale = scale\n\n        if (self.class_names is not None and\n                not isinstance(self.class_names, list)):\n            raise TypeError(\"Class name should be of type 'List of strings'\")\n\n        if (self.colors is not None and\n                not all(isinstance(color, list) for color in self.colors)):\n            raise TypeError(\"Colors should be of type 'List of lists'\")\n\n        if self.colors is None:\n            self.colors = lincolor(len(self.class_names))\n\n        if self.class_names is not None:\n            self.class_to_color = dict(zip(self.class_names, self.colors))\n        else:\n            self.class_to_color = {None: self.colors, '': self.colors}\n        super(DrawBoxes2D, self).__init__()",
  "def call(self, image, boxes2D):\n        for box2D in boxes2D:\n            x_min, y_min, x_max, y_max = box2D.coordinates\n            class_name = box2D.class_name\n            color = self.class_to_color[class_name]\n            if self.weighted:\n                color = [int(channel * box2D.score) for channel in color]\n            if self.with_score:\n                text = '{:0.2f}, {}'.format(box2D.score, class_name)\n            if not self.with_score:\n                text = '{}'.format(class_name)\n            put_text(image, text, (x_min, y_min - 10), self.scale, color, 1)\n            draw_rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n        return image",
  "def __init__(self, num_keypoints, radius=3, normalized=False):\n        super(DrawKeypoints2D, self).__init__()\n        self.colors = lincolor(num_keypoints, normalized=normalized)\n        self.radius = radius",
  "def call(self, image, keypoints):\n        for keypoint_arg, keypoint in enumerate(keypoints):\n            color = self.colors[keypoint_arg]\n            draw_keypoint(image, keypoint.astype('int'), color, self.radius)\n        return image",
  "def __init__(self, camera, class_to_dimensions,\n                 color=GREEN, thickness=5, radius=2):\n        \"\"\"Draw boxes 3D of multiple objects\n\n        # Arguments\n            camera: Instance of ``paz.backend.camera.Camera''.\n            class_to_dimensions: Dictionary that has as keys the\n                class names and as value a list [model_height, model_width]\n            thickness: Int. Thickness of 3D box\n        \"\"\"\n        super(DrawBoxes3D, self).__init__()\n        self.camera = camera\n        self.class_to_dimensions = class_to_dimensions\n        self.class_to_points = self._build_class_to_points(class_to_dimensions)\n        self.color = color\n        self.radius = radius\n        self.thickness = thickness",
  "def _build_class_to_points(self, class_to_dimensions):\n        class_to_points = {}\n        for class_name, dimensions in self.class_to_dimensions.items():\n            width, height, depth = dimensions\n            points = build_cube_points3D(width, height, depth)\n            class_to_points[class_name] = points\n        return class_to_points",
  "def call(self, image, pose6D):\n        points3D = self.class_to_points[pose6D.class_name]\n        points2D = project_points3D(points3D, pose6D, self.camera)\n        points2D = points2D.astype(np.int32)\n        draw_cube(image, points2D, self.color, self.thickness, self.radius)\n        return image",
  "def __init__(self, max_radius_scale=.5):\n        super(DrawRandomPolygon, self).__init__()\n        self.max_radius_scale = max_radius_scale",
  "def call(self, image):\n        return draw_random_polygon(image)",
  "def __init__(self, object_sizes, camera_intrinsics, thickness=2):\n        self.points3D = build_cube_points3D(*object_sizes)\n        self.intrinsics = camera_intrinsics\n        self.thickness = thickness",
  "def call(self, image, poses6D):\n        if poses6D is None:\n            return image\n        if not isinstance(poses6D, list):\n            raise ValueError('Poses6D must be a list of Pose6D messages')\n        for pose6D in poses6D:\n            image = draw_pose6D(\n                image, pose6D, self.points3D, self.intrinsics, self.thickness)\n        return image",
  "def __init__(self, object_sizes, camera_intrinsics, thickness=2):\n        self.points3D = build_cube_points3D(*object_sizes)\n        self.intrinsics = camera_intrinsics\n        self.thickness = thickness",
  "def call(self, image, pose6D):\n        if pose6D is None:\n            return image\n        image = draw_pose6D(\n            image, pose6D, self.points3D, self.intrinsics, self.thickness)\n        return image",
  "def __init__(self, dataset, check_scores, link_width=2, keypoint_radius=4):\n        super(DrawHumanSkeleton, self).__init__()\n        self.link_orders = HUMAN_JOINT_CONFIG[dataset]['part_orders']\n        self.link_colors = HUMAN_JOINT_CONFIG[dataset]['part_color']\n        self.link_args = HUMAN_JOINT_CONFIG[dataset]['part_arg']\n        self.keypoint_colors = HUMAN_JOINT_CONFIG[dataset]['joint_color']\n        self.check_scores = check_scores\n        self.link_width = link_width\n        self.keypoint_radius = keypoint_radius",
  "def call(self, image, grouped_joints):\n        for one_person_joints in grouped_joints:\n            image = draw_keypoints_link(\n                image, one_person_joints, self.link_args, self.link_orders,\n                self.link_colors, self.check_scores, self.link_width)\n            image = draw_keypoints(image, one_person_joints,\n                                   self.keypoint_colors, self.check_scores,\n                                   self.keypoint_radius)\n        return image",
  "def __init__(self, check_scores=False, link_width=2, keypoint_radius=4):\n        super(DrawHandSkeleton, self).__init__()\n        self.link_orders = MINIMAL_HAND_CONFIG['part_orders']\n        self.link_colors = MINIMAL_HAND_CONFIG['part_color']\n        self.link_args = MINIMAL_HAND_CONFIG['part_arg']\n        self.keypoint_colors = MINIMAL_HAND_CONFIG['joint_color']\n        self.check_scores = check_scores\n        self.link_width = link_width\n        self.keypoint_radius = keypoint_radius",
  "def call(self, image, keypoints):\n        image = draw_keypoints_link(\n            image, keypoints, self.link_args, self.link_orders,\n            self.link_colors, self.check_scores, self.link_width)\n        image = draw_keypoints(image, keypoints, self.keypoint_colors,\n                               self.check_scores, self.keypoint_radius)\n        return image",
  "def __init__(self, object_sizes):\n        super(DrawRGBMask, self).__init__()\n        self.object_sizes = object_sizes",
  "def call(self, image, points2D, points3D):\n        image = draw_RGB_mask(image, points2D, points3D, self.object_sizes)\n        return image",
  "def __init__(self, object_sizes):\n        super(DrawRGBMasks, self).__init__()\n        self.object_sizes = object_sizes",
  "def call(self, image, points2D, points3D):\n        return draw_RGB_masks(image, points2D, points3D, self.object_sizes)",
  "def __init__(self, color=GREEN, thickness=2, scale=1):\n        super(DrawText, self).__init__()\n        self.color = color\n        self.thickness = thickness\n        self.scale = scale",
  "def call(self, image, message, location=(50, 50)):\n        image = put_text(image, message, location, self.scale,\n                         self.color, self.thickness)\n        return image",
  "def __init__(self, camera_intrinsics):\n        super(DrawHumanPose6D, self).__init__()\n        self.K = camera_intrinsics",
  "def call(self, image, rotation, translation):\n        image = draw_human_pose6D(image, rotation, translation, self.K)\n        return image",
  "class RotationVectorToQuaternion(Processor):\n    \"\"\"Transforms rotation vector into quaternion.\n    \"\"\"\n    def __init__(self):\n        super(RotationVectorToQuaternion, self).__init__()\n\n    def call(self, rotation_vector):\n        quaternion = rotation_vector_to_quaternion(rotation_vector)\n        return quaternion",
  "class RotationVectorToRotationMatrix(Processor):\n    \"\"\"Transforms rotation vector into a rotation matrix.\n    \"\"\"\n    def __init__(self):\n        super(RotationVectorToRotationMatrix, self).__init__()\n\n    def call(self, rotation_vector):\n        return rotation_vector_to_rotation_matrix(rotation_vector)",
  "class ToAffineMatrix(Processor):\n    \"\"\"Builds affine matrix from a rotation matrix and a translation vector.\n    \"\"\"\n    def __init__(self):\n        super(ToAffineMatrix, self).__init__()\n\n    def call(self, rotation_matrix, translation):\n        affine_matrix = to_affine_matrix(rotation_matrix, translation)\n        return affine_matrix",
  "def __init__(self):\n        super(RotationVectorToQuaternion, self).__init__()",
  "def call(self, rotation_vector):\n        quaternion = rotation_vector_to_quaternion(rotation_vector)\n        return quaternion",
  "def __init__(self):\n        super(RotationVectorToRotationMatrix, self).__init__()",
  "def call(self, rotation_vector):\n        return rotation_vector_to_rotation_matrix(rotation_vector)",
  "def __init__(self):\n        super(ToAffineMatrix, self).__init__()",
  "def call(self, rotation_matrix, translation):\n        affine_matrix = to_affine_matrix(rotation_matrix, translation)\n        return affine_matrix",
  "class RandomFlipBoxesLeftRight(Processor):\n    \"\"\"Flips image and implemented labels horizontally.\n    \"\"\"\n    def __init__(self):\n        super(RandomFlipBoxesLeftRight, self).__init__()\n\n    def call(self, image, boxes):\n        if np.random.randint(0, 2):\n            boxes = flip_left_right(boxes, image.shape[1])\n            image = image[:, ::-1]\n        return image, boxes",
  "class ToImageBoxCoordinates(Processor):\n    \"\"\"Convert normalized box coordinates to image-size box coordinates.\n    \"\"\"\n    def __init__(self):\n        super(ToImageBoxCoordinates, self).__init__()\n\n    def call(self, image, boxes):\n        boxes = to_image_coordinates(boxes, image)\n        return image, boxes",
  "class ToNormalizedBoxCoordinates(Processor):\n    \"\"\"Convert image-size box coordinates to normalized box coordinates.\n    \"\"\"\n    def __init__(self):\n        super(ToNormalizedBoxCoordinates, self).__init__()\n\n    def call(self, image, boxes):\n        boxes = to_normalized_coordinates(boxes, image)\n        return image, boxes",
  "class RandomSampleCrop(Processor):\n    \"\"\"Crops image while adjusting the normalized corner form\n    bounding boxes.\n\n    # Arguments\n        probability: Float between ''[0, 1]''.\n    \"\"\"\n    def __init__(self, probability=0.50, max_trials=50):\n        self.probability = probability\n        self.max_trials = max_trials\n        self.jaccard_min_max = (\n            None,\n            (0.1, np.inf),\n            (0.3, np.inf),\n            (0.7, np.inf),\n            (0.9, np.inf),\n            (-np.inf, np.inf))\n\n    def call(self, image, boxes):\n\n        if self.probability < np.random.rand():\n            return image, boxes\n\n        labels = boxes[:, -1:]\n        boxes = boxes[:, :4]\n        H_original, W_original = image.shape[:2]\n\n        mode = np.random.randint(0, len(self.jaccard_min_max), 1)[0]\n        if self.jaccard_min_max[mode] is not None:\n            min_iou, max_iou = self.jaccard_min_max[mode]\n            for trial_arg in range(self.max_trials):\n                W = np.random.uniform(0.3 * W_original, W_original)\n                H = np.random.uniform(0.3 * H_original, H_original)\n                aspect_ratio = H / W\n                if (aspect_ratio < 0.5) or (aspect_ratio > 2):\n                    continue\n                x_min = np.random.uniform(W_original - W)\n                y_min = np.random.uniform(H_original - H)\n                x_max = int(x_min + W)\n                y_max = int(y_min + H)\n                x_min = int(x_min)\n                y_min = int(y_min)\n\n                image_crop_box = np.array([x_min, y_min, x_max, y_max])\n                overlap = compute_iou(image_crop_box, boxes)\n                if ((overlap.max() < min_iou) or (overlap.min() > max_iou)):\n                    continue\n\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n                centers_above_x_min = x_min < centers[:, 0]\n                centers_above_y_min = y_min < centers[:, 1]\n                centers_below_x_max = x_max > centers[:, 0]\n                centers_below_y_max = y_max > centers[:, 1]\n                mask = (centers_above_x_min * centers_above_y_min *\n                        centers_below_x_max * centers_below_y_max)\n                if not mask.any():\n                    continue\n\n                cropped_image = image[y_min:y_max, x_min:x_max, :].copy()\n                masked_boxes = boxes[mask, :].copy()\n                masked_labels = labels[mask].copy()\n                # should we use the box left and top corner or the crop's\n                masked_boxes[:, :2] = np.maximum(masked_boxes[:, :2],\n                                                 image_crop_box[:2])\n                # adjust to crop (by substracting crop's left,top)\n                masked_boxes[:, :2] -= image_crop_box[:2]\n                masked_boxes[:, 2:] = np.minimum(masked_boxes[:, 2:],\n                                                 image_crop_box[2:])\n                # adjust to crop (by substracting crop's left,top)\n                masked_boxes[:, 2:] -= image_crop_box[:2]\n                return cropped_image, np.hstack([masked_boxes, masked_labels])\n\n        boxes = np.hstack([boxes, labels])\n        return image, boxes",
  "class Expand(Processor):\n    \"\"\"Expand image size up to 2x, 3x, 4x and fill values with mean color.\n    This transformation is applied with a probability of 50%.\n\n    # Arguments\n        max_ratio: Float.\n        mean: None/List: If `None` expanded image is filled with\n            the image mean.\n        probability: Float between ''[0, 1]''.\n    \"\"\"\n    def __init__(self, max_ratio=2, mean=None, probability=0.5):\n        super(Expand, self).__init__()\n        self.max_ratio = max_ratio\n        self.mean = mean\n        self.probability = probability\n\n    def call(self, image, boxes):\n        if self.probability < np.random.rand():\n            return image, boxes\n        height, width, num_channels = image.shape\n        ratio = np.random.uniform(1, self.max_ratio)\n        left = np.random.uniform(0, width * ratio - width)\n        top = np.random.uniform(0, height * ratio - height)\n        expanded_image = np.zeros((int(height * ratio),\n                                   int(width * ratio), num_channels),\n                                  dtype=image.dtype)\n\n        if self.mean is None:\n            expanded_image[:, :, :] = np.mean(image, axis=(0, 1))\n        else:\n            expanded_image[:, :, :] = self.mean\n\n        expanded_image[int(top):int(top + height),\n                       int(left):int(left + width)] = image\n        expanded_boxes = boxes.copy()\n        expanded_boxes[:, 0:2] = boxes[:, 0:2] + (int(left), int(top))\n        expanded_boxes[:, 2:4] = boxes[:, 2:4] + (int(left), int(top))\n        return expanded_image, expanded_boxes",
  "class ApplyTranslation(Processor):\n    \"\"\"Applies a translation of image and labels.\n\n    # Arguments\n        translation: A list of length two indicating the x,y translation values\n        fill_color: List of three integers indicating the\n            color values e.g. ''[0, 0, 0]''\n    \"\"\"\n    def __init__(self, translation, fill_color=None):\n        super(ApplyTranslation, self).__init__()\n        self._matrix = np.zeros((2, 3), dtype=np.float32)\n        self._matrix[0, 0], self._matrix[1, 1] = 1.0, 1.0\n        self.fill_color = fill_color\n        self.translation = translation\n\n    @property\n    def translation(self):\n        return self._translation\n\n    @translation.setter\n    def translation(self, translation):\n        if translation is None:\n            self._translation = None\n        elif len(translation) == 2:\n            self._translation = translation\n            self._matrix[0, 2], self._matrix[1, 2] = translation\n        else:\n            raise ValueError('Translation should be `None` or have length two')\n\n    def call(self, image, keypoints=None):\n        height, width = image.shape[:2]\n        if self.fill_color is None:\n            fill_color = np.mean(image, axis=(0, 1))\n        image = warp_affine(image, self._matrix, fill_color)\n        if keypoints is not None:\n            keypoints[:, 0] = keypoints[:, 0] + self.translation[0]\n            keypoints[:, 1] = keypoints[:, 1] + self.translation[1]\n            return image, keypoints\n        return image",
  "class RandomTranslation(Processor):\n    \"\"\"Applies a random translation to image and labels\n\n    # Arguments\n        delta_scale: List with two elements having the normalized deltas.\n            e.g. ''[.25, .25]''.\n\n        fill_color: List of three integers indicating the\n            color values e.g. ''[0, 0, 0]''.\n    \"\"\"\n    def __init__(\n            self, delta_scale=[0.25, 0.25], fill_color=None):\n        super(RandomTranslation, self).__init__()\n        self.delta_scale = delta_scale\n        self.apply_translation = ApplyTranslation(None, fill_color)\n\n    @property\n    def delta_scale(self):\n        return self._delta_scale\n\n    @delta_scale.setter\n    def delta_scale(self, delta_scale):\n        x_delta_scale, y_delta_scale = delta_scale\n        if (x_delta_scale < 0) or (y_delta_scale < 0):\n            raise ValueError('Delta scale values should be a positive scalar')\n        self._delta_scale = delta_scale\n\n    def call(self, image):\n        height, width = image.shape[:2]\n        x_delta_scale, y_delta_scale = self.delta_scale\n        x = image.shape[1] * np.random.uniform(-x_delta_scale, x_delta_scale)\n        y = image.shape[0] * np.random.uniform(-y_delta_scale, y_delta_scale)\n        self.apply_translation.translation = [x, y]\n        return self.apply_translation(image)",
  "class RandomKeypointTranslation(Processor):\n    \"\"\"Applies a random translation to image and keypoints.\n\n    # Arguments\n        delta_scale: List with two elements having the normalized deltas.\n            e.g. ''[.25, .25]''.\n        fill_color: ''None'' or List of three integers indicating the\n            color values e.g. ''[0, 0, 0]''. If ''None'' mean channel values of\n            the image will be calculated as fill values.\n        probability: Float between ''[0, 1]''.\n    \"\"\"\n    def __init__(self, delta_scale=[.2, .2], fill_color=None, probability=0.5):\n        super(RandomKeypointTranslation, self).__init__()\n        self.delta_scale = delta_scale\n        self.fill_color = fill_color\n        self.probability = probability\n\n    @property\n    def probability(self):\n        return self._probability\n\n    @probability.setter\n    def probability(self, value):\n        if not (0.0 < value <= 1.0):\n            raise ValueError('Probability should be between \"[0, 1]\".')\n        self._probability = value\n\n    @property\n    def delta_scale(self):\n        return self._delta_scale\n\n    @delta_scale.setter\n    def delta_scale(self, delta_scale):\n        x_delta_scale, y_delta_scale = delta_scale\n        if (x_delta_scale < 0) or (y_delta_scale < 0):\n            raise ValueError('Delta scale values should be positive')\n        if (x_delta_scale > 1) or (y_delta_scale > 1):\n            raise ValueError('Delta scale values should be less than one')\n        self._delta_scale = delta_scale\n\n    def _sample_random_translation(self, delta_scale, image_shape):\n        x_delta_scale, y_delta_scale = delta_scale\n        x = image_shape[1] * np.random.uniform(-x_delta_scale, x_delta_scale)\n        y = image_shape[0] * np.random.uniform(-y_delta_scale, y_delta_scale)\n        return [x, y]\n\n    def call(self, image, keypoints):\n        if self.probability >= np.random.rand():\n            shape = image.shape[:2]\n            translation = sample_scaled_translation(self.delta_scale, shape)\n            if self.fill_color is None:\n                fill_color = np.mean(image, axis=(0, 1))\n            image = translate_image(image, translation, fill_color)\n            keypoints = translate_keypoints(keypoints, translation)\n        return image, keypoints",
  "class RandomKeypointRotation(Processor):\n    \"\"\"Randomly rotate an images with its corresponding keypoints.\n\n    # Arguments\n        rotation_range: Int. indicating the max and min values in degrees\n            of the uniform distribution ''[-range, range]'' from which the\n            angles are sampled.\n        fill_color: ''None'' or List of three integers indicating the\n            color values e.g. ''[0, 0, 0]''. If ''None'' mean channel values of\n            the image will be calculated as fill values.\n    \"\"\"\n    def __init__(self, rotation_range=30, fill_color=None, probability=0.5):\n        super(RandomKeypointRotation, self).__init__()\n        self.rotation_range = rotation_range\n        self.fill_color = fill_color\n        self.probability = probability\n\n    @property\n    def probability(self):\n        return self._probability\n\n    @probability.setter\n    def probability(self, value):\n        if not (0.0 < value <= 1.0):\n            raise ValueError('Probability should be between \"[0, 1]\".')\n        self._probability = value\n\n    def _calculate_image_center(self, image):\n        return (int(image.shape[0] / 2), int(image.shape[1] / 2))\n\n    def _rotate_image(self, image, degrees):\n        center = self._calculate_image_center(image)\n        matrix = get_rotation_matrix(center, degrees)\n        if self.fill_color is None:\n            fill_color = np.mean(image, axis=(0, 1))\n        return warp_affine(image, matrix, fill_color)\n\n    def _degrees_to_radians(self, degrees):\n        # negative sign changes rotation direction to follow openCV convention.\n        return - (3.14159 / 180) * degrees\n\n    def _build_rotation_matrix(self, radians):\n        return np.array([[np.cos(radians), - np.sin(radians)],\n                         [np.sin(radians), + np.cos(radians)]])\n\n    def _rotate_keypoints(self, keypoints, radians, image_center):\n        keypoints = keypoints - image_center\n        matrix = self._build_rotation_matrix(radians)\n        keypoints = np.matmul(matrix, keypoints.T).T\n        keypoints = keypoints + image_center\n        return keypoints\n\n    def _sample_rotation(self, rotation_range):\n        return np.random.uniform(-rotation_range, rotation_range)\n\n    def call(self, image, keypoints):\n        if self.probability >= np.random.rand():\n            degrees = self._sample_rotation(self.rotation_range)\n            image = self._rotate_image(image, degrees)\n            center = self._calculate_image_center(image)\n            radians = self._degrees_to_radians(degrees)\n            keypoints = self._rotate_keypoints(keypoints, radians, center)\n        return image, keypoints",
  "class RandomRotation(Processor):\n    \"\"\"Randomly rotate an images\n\n    # Arguments\n        rotation_range: Int. indicating the max and min values in degrees\n            of the uniform distribution ``[-range, range]`` from which the\n            angles are sampled.\n        fill_color: ''None'' or List of three integers indicating the\n            color values e.g. ``[0, 0, 0]``. If ``None`` mean channel values of\n            the image will be calculated as fill values.\n        probability: Float between 0 and 1.\n    \"\"\"\n    def __init__(self, rotation_range=30, fill_color=None, probability=0.5):\n        super(RandomRotation, self).__init__()\n        self.rotation_range = rotation_range\n        self.fill_color = fill_color\n        self.probability = probability\n\n    @property\n    def probability(self):\n        return self._probability\n\n    @probability.setter\n    def probability(self, value):\n        if not (0.0 < value <= 1.0):\n            raise ValueError('Probability should be between \"[0, 1]\".')\n        self._probability = value\n\n    def _calculate_image_center(self, image):\n        return (int(image.shape[0] / 2), int(image.shape[1] / 2))\n\n    def _rotate_image(self, image, degrees):\n        center = self._calculate_image_center(image)\n        matrix = get_rotation_matrix(center, degrees)\n        if self.fill_color is None:\n            fill_color = np.mean(image, axis=(0, 1))\n        return warp_affine(image, matrix, fill_color)\n\n    def _sample_rotation(self, rotation_range):\n        return np.random.uniform(-rotation_range, rotation_range)\n\n    def call(self, image):\n        if self.probability >= np.random.rand():\n            degrees = self._sample_rotation(self.rotation_range)\n            image = self._rotate_image(image, degrees)\n        return image",
  "class TranslateImage(Processor):\n    \"\"\"Applies a translation of image.\n    The translation is a list of length two indicating the x, y values.\n\n    # Arguments\n        fill_color: List of three integers indicating the\n            color values e.g. ``[0, 0, 0]``\n    \"\"\"\n    def __init__(self, fill_color=None):\n        super(TranslateImage, self).__init__()\n        self.fill_color = fill_color\n\n    def call(self, image, translation):\n        return translate_image(image, translation, self.fill_color)",
  "class GetTransformationSize(Processor):\n    \"\"\"Calculate the transformation size for the imgae.\n    The size is tuple of length two indicating the x, y values.\n\n    # Arguments\n        image: Numpy array\n    \"\"\"\n    def __init__(self, input_size, multiple):\n        super(GetTransformationSize, self).__init__()\n        self.input_size = input_size\n        self.multiple = multiple\n\n    def call(self, image):\n        size = resize_with_same_aspect_ratio(image, self.input_size,\n                                             self.multiple)\n        H, W = image.shape[:2]\n        if W < H:\n            size[0], size[1] = size[1], size[0]\n        return size",
  "class GetTransformationScale(Processor):\n    \"\"\"Calculate the transformation scale for the imgae.\n    The scale is a numpy array of size two indicating the\n    width and height scale.\n\n    # Arguments\n        image: Numpy array\n        size: Numpy array of length 2\n    \"\"\"\n    def __init__(self, scaling_factor):\n        super(GetTransformationScale, self).__init__()\n        self.scaling_factor = scaling_factor\n\n    def call(self, image, size):\n        scale = get_transformation_scale(image, size, self.scaling_factor)\n        H, W = image.shape[:2]\n        if W < H:\n            scale[0], scale[1] = scale[1], scale[0]\n        return scale",
  "class GetSourceDestinationPoints(Processor):\n    \"\"\"Returns the source and destination points for affine transformation.\n\n    # Arguments\n        center: Numpy array of shape (2,). Center coordinates of image\n        scale: Numpy array of shape (2,). Scale of width and height of image\n        size: List of length 2. Size of image\n    \"\"\"\n    def __init__(self, scaling_factor):\n        super(GetSourceDestinationPoints, self).__init__()\n        self.scaling_factor = scaling_factor\n\n    def _calculate_third_point(self, point2D_a, point2D_b):\n        difference = point2D_a - point2D_b\n        return point2D_a + np.array([-difference[1],\n                                     difference[0]], dtype=np.float32)\n\n    def _get_transformation_source_point(self, scale, center):\n        scale = scale * self.scaling_factor\n        center_W = scale[0] / 2\n        direction_vector = rotate_point2D([0, -center_W], 0)\n        points = np.zeros((3, 2), dtype=np.float32)\n        points[0, :] = center\n        points[1, :] = center + direction_vector\n        points[2:, :] = self._calculate_third_point(points[0, :], points[1, :])\n        return points\n\n    def _get_transformation_destination_point(self, output_size):\n        center_W, center_H = np.array(output_size[:2]) / 2\n        direction_vector = np.array([0, -center_W], np.float32)\n        points = np.zeros((3, 2), dtype=np.float32)\n        points[0, :] = [center_W, center_H]\n        points[1, :] = np.array([center_W, center_H]) + direction_vector\n        points[2:, :] = self._calculate_third_point(points[0, :], points[1, :])\n        return points\n\n    def call(self, center, scale, size):\n        if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n            scale = np.array([scale, scale])\n        source_point = self._get_transformation_source_point(scale, center)\n        destination_point = self._get_transformation_destination_point(size)\n        return source_point, destination_point",
  "class GetImageCenter(Processor):\n    \"\"\"Calculate the center of the image and add an offset to the center.\n\n    # Arguments\n        image: Numpy array\n        offset: Float\n    \"\"\"\n    def __init__(self, offset=0.5):\n        super(GetImageCenter, self).__init__()\n        self.offset = offset\n\n    def _add_offset(self, x, offset):\n        return (x + offset)\n\n    def call(self, image):\n        center_W, center_H = calculate_image_center(image)\n        center_W = int(self._add_offset(center_W, self.offset))\n        center_H = int(self._add_offset(center_H, self.offset))\n        return np.array([center_W, center_H])",
  "class WarpAffine(Processor):\n    \"\"\"Applies an affine transformation to an image\n\n    # Arguments\n        image: Numpy array\n        transform: Numpy array. Transformation matrix\n        size: Numpy array. Transformation size\n    \"\"\"\n    def __init__(self):\n        super(WarpAffine, self).__init__()\n\n    def call(self, image, transform, size):\n        image = warp_affine(image, transform, size=size)\n        return image",
  "def __init__(self):\n        super(RandomFlipBoxesLeftRight, self).__init__()",
  "def call(self, image, boxes):\n        if np.random.randint(0, 2):\n            boxes = flip_left_right(boxes, image.shape[1])\n            image = image[:, ::-1]\n        return image, boxes",
  "def __init__(self):\n        super(ToImageBoxCoordinates, self).__init__()",
  "def call(self, image, boxes):\n        boxes = to_image_coordinates(boxes, image)\n        return image, boxes",
  "def __init__(self):\n        super(ToNormalizedBoxCoordinates, self).__init__()",
  "def call(self, image, boxes):\n        boxes = to_normalized_coordinates(boxes, image)\n        return image, boxes",
  "def __init__(self, probability=0.50, max_trials=50):\n        self.probability = probability\n        self.max_trials = max_trials\n        self.jaccard_min_max = (\n            None,\n            (0.1, np.inf),\n            (0.3, np.inf),\n            (0.7, np.inf),\n            (0.9, np.inf),\n            (-np.inf, np.inf))",
  "def call(self, image, boxes):\n\n        if self.probability < np.random.rand():\n            return image, boxes\n\n        labels = boxes[:, -1:]\n        boxes = boxes[:, :4]\n        H_original, W_original = image.shape[:2]\n\n        mode = np.random.randint(0, len(self.jaccard_min_max), 1)[0]\n        if self.jaccard_min_max[mode] is not None:\n            min_iou, max_iou = self.jaccard_min_max[mode]\n            for trial_arg in range(self.max_trials):\n                W = np.random.uniform(0.3 * W_original, W_original)\n                H = np.random.uniform(0.3 * H_original, H_original)\n                aspect_ratio = H / W\n                if (aspect_ratio < 0.5) or (aspect_ratio > 2):\n                    continue\n                x_min = np.random.uniform(W_original - W)\n                y_min = np.random.uniform(H_original - H)\n                x_max = int(x_min + W)\n                y_max = int(y_min + H)\n                x_min = int(x_min)\n                y_min = int(y_min)\n\n                image_crop_box = np.array([x_min, y_min, x_max, y_max])\n                overlap = compute_iou(image_crop_box, boxes)\n                if ((overlap.max() < min_iou) or (overlap.min() > max_iou)):\n                    continue\n\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n                centers_above_x_min = x_min < centers[:, 0]\n                centers_above_y_min = y_min < centers[:, 1]\n                centers_below_x_max = x_max > centers[:, 0]\n                centers_below_y_max = y_max > centers[:, 1]\n                mask = (centers_above_x_min * centers_above_y_min *\n                        centers_below_x_max * centers_below_y_max)\n                if not mask.any():\n                    continue\n\n                cropped_image = image[y_min:y_max, x_min:x_max, :].copy()\n                masked_boxes = boxes[mask, :].copy()\n                masked_labels = labels[mask].copy()\n                # should we use the box left and top corner or the crop's\n                masked_boxes[:, :2] = np.maximum(masked_boxes[:, :2],\n                                                 image_crop_box[:2])\n                # adjust to crop (by substracting crop's left,top)\n                masked_boxes[:, :2] -= image_crop_box[:2]\n                masked_boxes[:, 2:] = np.minimum(masked_boxes[:, 2:],\n                                                 image_crop_box[2:])\n                # adjust to crop (by substracting crop's left,top)\n                masked_boxes[:, 2:] -= image_crop_box[:2]\n                return cropped_image, np.hstack([masked_boxes, masked_labels])\n\n        boxes = np.hstack([boxes, labels])\n        return image, boxes",
  "def __init__(self, max_ratio=2, mean=None, probability=0.5):\n        super(Expand, self).__init__()\n        self.max_ratio = max_ratio\n        self.mean = mean\n        self.probability = probability",
  "def call(self, image, boxes):\n        if self.probability < np.random.rand():\n            return image, boxes\n        height, width, num_channels = image.shape\n        ratio = np.random.uniform(1, self.max_ratio)\n        left = np.random.uniform(0, width * ratio - width)\n        top = np.random.uniform(0, height * ratio - height)\n        expanded_image = np.zeros((int(height * ratio),\n                                   int(width * ratio), num_channels),\n                                  dtype=image.dtype)\n\n        if self.mean is None:\n            expanded_image[:, :, :] = np.mean(image, axis=(0, 1))\n        else:\n            expanded_image[:, :, :] = self.mean\n\n        expanded_image[int(top):int(top + height),\n                       int(left):int(left + width)] = image\n        expanded_boxes = boxes.copy()\n        expanded_boxes[:, 0:2] = boxes[:, 0:2] + (int(left), int(top))\n        expanded_boxes[:, 2:4] = boxes[:, 2:4] + (int(left), int(top))\n        return expanded_image, expanded_boxes",
  "def __init__(self, translation, fill_color=None):\n        super(ApplyTranslation, self).__init__()\n        self._matrix = np.zeros((2, 3), dtype=np.float32)\n        self._matrix[0, 0], self._matrix[1, 1] = 1.0, 1.0\n        self.fill_color = fill_color\n        self.translation = translation",
  "def translation(self):\n        return self._translation",
  "def translation(self, translation):\n        if translation is None:\n            self._translation = None\n        elif len(translation) == 2:\n            self._translation = translation\n            self._matrix[0, 2], self._matrix[1, 2] = translation\n        else:\n            raise ValueError('Translation should be `None` or have length two')",
  "def call(self, image, keypoints=None):\n        height, width = image.shape[:2]\n        if self.fill_color is None:\n            fill_color = np.mean(image, axis=(0, 1))\n        image = warp_affine(image, self._matrix, fill_color)\n        if keypoints is not None:\n            keypoints[:, 0] = keypoints[:, 0] + self.translation[0]\n            keypoints[:, 1] = keypoints[:, 1] + self.translation[1]\n            return image, keypoints\n        return image",
  "def __init__(\n            self, delta_scale=[0.25, 0.25], fill_color=None):\n        super(RandomTranslation, self).__init__()\n        self.delta_scale = delta_scale\n        self.apply_translation = ApplyTranslation(None, fill_color)",
  "def delta_scale(self):\n        return self._delta_scale",
  "def delta_scale(self, delta_scale):\n        x_delta_scale, y_delta_scale = delta_scale\n        if (x_delta_scale < 0) or (y_delta_scale < 0):\n            raise ValueError('Delta scale values should be a positive scalar')\n        self._delta_scale = delta_scale",
  "def call(self, image):\n        height, width = image.shape[:2]\n        x_delta_scale, y_delta_scale = self.delta_scale\n        x = image.shape[1] * np.random.uniform(-x_delta_scale, x_delta_scale)\n        y = image.shape[0] * np.random.uniform(-y_delta_scale, y_delta_scale)\n        self.apply_translation.translation = [x, y]\n        return self.apply_translation(image)",
  "def __init__(self, delta_scale=[.2, .2], fill_color=None, probability=0.5):\n        super(RandomKeypointTranslation, self).__init__()\n        self.delta_scale = delta_scale\n        self.fill_color = fill_color\n        self.probability = probability",
  "def probability(self):\n        return self._probability",
  "def probability(self, value):\n        if not (0.0 < value <= 1.0):\n            raise ValueError('Probability should be between \"[0, 1]\".')\n        self._probability = value",
  "def delta_scale(self):\n        return self._delta_scale",
  "def delta_scale(self, delta_scale):\n        x_delta_scale, y_delta_scale = delta_scale\n        if (x_delta_scale < 0) or (y_delta_scale < 0):\n            raise ValueError('Delta scale values should be positive')\n        if (x_delta_scale > 1) or (y_delta_scale > 1):\n            raise ValueError('Delta scale values should be less than one')\n        self._delta_scale = delta_scale",
  "def _sample_random_translation(self, delta_scale, image_shape):\n        x_delta_scale, y_delta_scale = delta_scale\n        x = image_shape[1] * np.random.uniform(-x_delta_scale, x_delta_scale)\n        y = image_shape[0] * np.random.uniform(-y_delta_scale, y_delta_scale)\n        return [x, y]",
  "def call(self, image, keypoints):\n        if self.probability >= np.random.rand():\n            shape = image.shape[:2]\n            translation = sample_scaled_translation(self.delta_scale, shape)\n            if self.fill_color is None:\n                fill_color = np.mean(image, axis=(0, 1))\n            image = translate_image(image, translation, fill_color)\n            keypoints = translate_keypoints(keypoints, translation)\n        return image, keypoints",
  "def __init__(self, rotation_range=30, fill_color=None, probability=0.5):\n        super(RandomKeypointRotation, self).__init__()\n        self.rotation_range = rotation_range\n        self.fill_color = fill_color\n        self.probability = probability",
  "def probability(self):\n        return self._probability",
  "def probability(self, value):\n        if not (0.0 < value <= 1.0):\n            raise ValueError('Probability should be between \"[0, 1]\".')\n        self._probability = value",
  "def _calculate_image_center(self, image):\n        return (int(image.shape[0] / 2), int(image.shape[1] / 2))",
  "def _rotate_image(self, image, degrees):\n        center = self._calculate_image_center(image)\n        matrix = get_rotation_matrix(center, degrees)\n        if self.fill_color is None:\n            fill_color = np.mean(image, axis=(0, 1))\n        return warp_affine(image, matrix, fill_color)",
  "def _degrees_to_radians(self, degrees):\n        # negative sign changes rotation direction to follow openCV convention.\n        return - (3.14159 / 180) * degrees",
  "def _build_rotation_matrix(self, radians):\n        return np.array([[np.cos(radians), - np.sin(radians)],\n                         [np.sin(radians), + np.cos(radians)]])",
  "def _rotate_keypoints(self, keypoints, radians, image_center):\n        keypoints = keypoints - image_center\n        matrix = self._build_rotation_matrix(radians)\n        keypoints = np.matmul(matrix, keypoints.T).T\n        keypoints = keypoints + image_center\n        return keypoints",
  "def _sample_rotation(self, rotation_range):\n        return np.random.uniform(-rotation_range, rotation_range)",
  "def call(self, image, keypoints):\n        if self.probability >= np.random.rand():\n            degrees = self._sample_rotation(self.rotation_range)\n            image = self._rotate_image(image, degrees)\n            center = self._calculate_image_center(image)\n            radians = self._degrees_to_radians(degrees)\n            keypoints = self._rotate_keypoints(keypoints, radians, center)\n        return image, keypoints",
  "def __init__(self, rotation_range=30, fill_color=None, probability=0.5):\n        super(RandomRotation, self).__init__()\n        self.rotation_range = rotation_range\n        self.fill_color = fill_color\n        self.probability = probability",
  "def probability(self):\n        return self._probability",
  "def probability(self, value):\n        if not (0.0 < value <= 1.0):\n            raise ValueError('Probability should be between \"[0, 1]\".')\n        self._probability = value",
  "def _calculate_image_center(self, image):\n        return (int(image.shape[0] / 2), int(image.shape[1] / 2))",
  "def _rotate_image(self, image, degrees):\n        center = self._calculate_image_center(image)\n        matrix = get_rotation_matrix(center, degrees)\n        if self.fill_color is None:\n            fill_color = np.mean(image, axis=(0, 1))\n        return warp_affine(image, matrix, fill_color)",
  "def _sample_rotation(self, rotation_range):\n        return np.random.uniform(-rotation_range, rotation_range)",
  "def call(self, image):\n        if self.probability >= np.random.rand():\n            degrees = self._sample_rotation(self.rotation_range)\n            image = self._rotate_image(image, degrees)\n        return image",
  "def __init__(self, fill_color=None):\n        super(TranslateImage, self).__init__()\n        self.fill_color = fill_color",
  "def call(self, image, translation):\n        return translate_image(image, translation, self.fill_color)",
  "def __init__(self, input_size, multiple):\n        super(GetTransformationSize, self).__init__()\n        self.input_size = input_size\n        self.multiple = multiple",
  "def call(self, image):\n        size = resize_with_same_aspect_ratio(image, self.input_size,\n                                             self.multiple)\n        H, W = image.shape[:2]\n        if W < H:\n            size[0], size[1] = size[1], size[0]\n        return size",
  "def __init__(self, scaling_factor):\n        super(GetTransformationScale, self).__init__()\n        self.scaling_factor = scaling_factor",
  "def call(self, image, size):\n        scale = get_transformation_scale(image, size, self.scaling_factor)\n        H, W = image.shape[:2]\n        if W < H:\n            scale[0], scale[1] = scale[1], scale[0]\n        return scale",
  "def __init__(self, scaling_factor):\n        super(GetSourceDestinationPoints, self).__init__()\n        self.scaling_factor = scaling_factor",
  "def _calculate_third_point(self, point2D_a, point2D_b):\n        difference = point2D_a - point2D_b\n        return point2D_a + np.array([-difference[1],\n                                     difference[0]], dtype=np.float32)",
  "def _get_transformation_source_point(self, scale, center):\n        scale = scale * self.scaling_factor\n        center_W = scale[0] / 2\n        direction_vector = rotate_point2D([0, -center_W], 0)\n        points = np.zeros((3, 2), dtype=np.float32)\n        points[0, :] = center\n        points[1, :] = center + direction_vector\n        points[2:, :] = self._calculate_third_point(points[0, :], points[1, :])\n        return points",
  "def _get_transformation_destination_point(self, output_size):\n        center_W, center_H = np.array(output_size[:2]) / 2\n        direction_vector = np.array([0, -center_W], np.float32)\n        points = np.zeros((3, 2), dtype=np.float32)\n        points[0, :] = [center_W, center_H]\n        points[1, :] = np.array([center_W, center_H]) + direction_vector\n        points[2:, :] = self._calculate_third_point(points[0, :], points[1, :])\n        return points",
  "def call(self, center, scale, size):\n        if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n            scale = np.array([scale, scale])\n        source_point = self._get_transformation_source_point(scale, center)\n        destination_point = self._get_transformation_destination_point(size)\n        return source_point, destination_point",
  "def __init__(self, offset=0.5):\n        super(GetImageCenter, self).__init__()\n        self.offset = offset",
  "def _add_offset(self, x, offset):\n        return (x + offset)",
  "def call(self, image):\n        center_W, center_H = calculate_image_center(image)\n        center_W = int(self._add_offset(center_W, self.offset))\n        center_H = int(self._add_offset(center_H, self.offset))\n        return np.array([center_W, center_H])",
  "def __init__(self):\n        super(WarpAffine, self).__init__()",
  "def call(self, image, transform, size):\n        image = warp_affine(image, transform, size=size)\n        return image",
  "class ChangeLinkOrder(pr.Processor):\n    \"\"\"Map data from one config to another.\n\n    # Arguments\n        joints: Array\n        config1_labels: input joint configuration\n        config2_labels: output joint configuration\n\n    # Returns\n        Array: joints maped to the config2_labels\n    \"\"\"\n    def __init__(self, config1_labels, config2_labels):\n        super(ChangeLinkOrder, self).__init__()\n        self.config1_labels = config1_labels\n        self.config2_labels = config2_labels\n\n    def call(self, joints):\n        mapped_joints = change_link_order(joints, self.config1_labels,\n                                          self.config2_labels)\n        return mapped_joints",
  "class CalculateRelativeAngles(pr.Processor):\n    \"\"\"Compute the realtive joint rotation for the minimal hand joints and map\n       it to the output_config kinematic chain form.\n\n    # Arguments\n        absolute_quaternions : Array [num_joints, 4].\n        Absolute joint angle rotation for the minimal hand joints in\n        quaternion representation [q1, q2, q3, w0].\n\n    # Returns\n        relative_angles: Array [num_joints, 3].\n        Relative joint rotation of the minimal hand joints in compact\n        axis angle representation.\n    \"\"\"\n    def __init__(self, right_hand=False, input_config=MANOHandJoints,\n                 output_config=MPIIHandJoints):\n        super(CalculateRelativeAngles, self).__init__()\n        output_labels = output_config.labels\n        input_labels = input_config.labels\n        links_origin = input_config.links_origin\n        self.parents = input_config.parents\n        self.children = output_config.children\n        if right_hand:\n            links_origin = flip_along_x_axis(links_origin)\n        self.links_orientation = compute_orientation_vector(\n            links_origin, self.parents)\n        self.quaternions_to_rotations = pr.SequentialProcessor([\n            pr.ChangeLinkOrder(output_labels, input_labels),\n            quaternions_to_rotation_matrices])\n        self.calculate_relative_angle = pr.SequentialProcessor([\n            calculate_relative_angle,\n            pr.ChangeLinkOrder(input_labels, output_labels)])\n\n    def call(self, absolute_quaternions):\n        absolute_rotation = self.quaternions_to_rotations(absolute_quaternions)\n        rotated_links_origin = rotate_keypoints3D(\n            absolute_rotation, self.links_orientation)\n        rotated_links_origin_transform = to_affine_matrices(\n            absolute_rotation, rotated_links_origin)\n        relative_angles = self.calculate_relative_angle(\n            absolute_rotation, rotated_links_origin_transform, self.parents)\n        relative_angles = reorder_relative_angles(\n            relative_angles, absolute_rotation[0], self.children)\n        return relative_angles",
  "class IsHandOpen(pr.Processor):\n    \"\"\"Check is the hand is open by by using the relative angles of the joint.\n    # Arguments\n        joint_name_to_arg: Dictionary for the joints\n        thresh: Float. Threshold value for theta\n        relative_angle: Array\n\n    # Returns\n        String: Hand is open or closed.\n    \"\"\"\n    def __init__(self, joint_name_to_arg=hand_part_arg, thresh=0.4):\n        super(IsHandOpen, self).__init__()\n        self.joint_name_to_arg = joint_name_to_arg\n        self.thresh = thresh\n\n    def call(self, relative_angles):\n        return is_hand_open(relative_angles, self.joint_name_to_arg,\n                            self.thresh)",
  "def __init__(self, config1_labels, config2_labels):\n        super(ChangeLinkOrder, self).__init__()\n        self.config1_labels = config1_labels\n        self.config2_labels = config2_labels",
  "def call(self, joints):\n        mapped_joints = change_link_order(joints, self.config1_labels,\n                                          self.config2_labels)\n        return mapped_joints",
  "def __init__(self, right_hand=False, input_config=MANOHandJoints,\n                 output_config=MPIIHandJoints):\n        super(CalculateRelativeAngles, self).__init__()\n        output_labels = output_config.labels\n        input_labels = input_config.labels\n        links_origin = input_config.links_origin\n        self.parents = input_config.parents\n        self.children = output_config.children\n        if right_hand:\n            links_origin = flip_along_x_axis(links_origin)\n        self.links_orientation = compute_orientation_vector(\n            links_origin, self.parents)\n        self.quaternions_to_rotations = pr.SequentialProcessor([\n            pr.ChangeLinkOrder(output_labels, input_labels),\n            quaternions_to_rotation_matrices])\n        self.calculate_relative_angle = pr.SequentialProcessor([\n            calculate_relative_angle,\n            pr.ChangeLinkOrder(input_labels, output_labels)])",
  "def call(self, absolute_quaternions):\n        absolute_rotation = self.quaternions_to_rotations(absolute_quaternions)\n        rotated_links_origin = rotate_keypoints3D(\n            absolute_rotation, self.links_orientation)\n        rotated_links_origin_transform = to_affine_matrices(\n            absolute_rotation, rotated_links_origin)\n        relative_angles = self.calculate_relative_angle(\n            absolute_rotation, rotated_links_origin_transform, self.parents)\n        relative_angles = reorder_relative_angles(\n            relative_angles, absolute_rotation[0], self.children)\n        return relative_angles",
  "def __init__(self, joint_name_to_arg=hand_part_arg, thresh=0.4):\n        super(IsHandOpen, self).__init__()\n        self.joint_name_to_arg = joint_name_to_arg\n        self.thresh = thresh",
  "def call(self, relative_angles):\n        return is_hand_open(relative_angles, self.joint_name_to_arg,\n                            self.thresh)",
  "class ControlMap(Processor):\n    \"\"\"Controls which inputs are passed ''processor'' and the order of its\n        outputs.\n\n    # Arguments\n        processor: Function e.g. a ''paz.processor''\n        intro_indices: List of Ints.\n        outro_indices: List of Ints.\n        keep: ''None'' or dictionary. If ``None`` control maps operates\n            without explicitly retaining an input. If dict it must contain\n            as keys the input args to be kept and as values where they should\n            be located at the end.\n    \"\"\"\n    def __init__(self, processor, intro_indices=[0], outro_indices=[0],\n                 keep=None):\n        self.processor = processor\n        if not isinstance(intro_indices, list):\n            raise ValueError('``intro_indices`` must be a list')\n        if not isinstance(outro_indices, list):\n            raise ValueError('``outro_indices`` must be a list')\n        self.intro_indices = intro_indices\n        self.outro_indices = outro_indices\n        name = '-'.join([self.__class__.__name__, self.processor.name])\n        self.keep = keep\n        super(ControlMap, self).__init__(name)\n\n    def _select(self, inputs, indices):\n        return [inputs[index] for index in indices]\n\n    def _remove(self, inputs, indices):\n        return [inputs[i] for i in range(len(inputs)) if i not in indices]\n\n    def _split(self, inputs, indices):\n        return self._select(inputs, indices), self._remove(inputs, indices)\n\n    def _insert(self, args, extra_args, indices):\n        [args.insert(index, arg) for index, arg in zip(indices, extra_args)]\n        return args\n\n    def call(self, *args):\n        selected_args, remaining_args = self._split(args, self.intro_indices)\n        processed_args = self.processor(*selected_args)\n        if not isinstance(processed_args, tuple):\n            processed_args = [processed_args]\n        return_args = self._insert(\n            remaining_args, processed_args, self.outro_indices)\n\n        if self.keep is not None:\n            keep_intro = list(self.keep.keys())\n            keep_outro = list(self.keep.values())\n            keep_args = self._select(args, keep_intro)\n            return_args = self._insert(return_args, keep_args, keep_outro)\n\n        return tuple(return_args)",
  "class ExpandDomain(ControlMap):\n    \"\"\"Extends number of inputs a function can take applying the identity\n    function to all new/extended inputs.\n    e.g. For a given function f(x) = y. If g = ExtendInputs(f), we can\n    now have g(x, x1, x2, ..., xn) = y, x1, x2, ..., xn.\n\n    # Arguments\n        processor: Function e.g. any procesor in ''paz.processors''.\n    \"\"\"\n    def __init__(self, processor):\n        super(ExpandDomain, self).__init__(processor)",
  "class CopyDomain(Processor):\n    \"\"\"Copies ''intro_indices'' and places it ''outro_indices''.\n\n    # Arguments\n        intro_indices: List of Ints.\n        outro_indices: List of Ints.\n    \"\"\"\n    def __init__(self, intro_indices, outro_indices):\n        super(CopyDomain, self).__init__()\n        if not isinstance(intro_indices, list):\n            raise ValueError('``intro_indices`` must be a list')\n        if not isinstance(outro_indices, list):\n            raise ValueError('``outro_indices`` must be a list')\n        self.intro_indices = intro_indices\n        self.outro_indices = outro_indices\n\n    def _select(self, inputs, indices):\n        return [inputs[index] for index in indices]\n\n    def _insert(self, args, axes, values):\n        [args.insert(axis, value) for axis, value in zip(axes, values)]\n        return args\n\n    def call(self, *args):\n        selections = self._select(args, self.intro_indices)\n        args = self._insert(list(args), self.outro_indices, selections)\n        return tuple(args)",
  "class UnpackDictionary(Processor):\n    \"\"\"Unpacks dictionary into a tuple.\n    # Arguments\n        order: List of strings containing the keys of the dictionary.\n            The order of the list is the order in which the tuple\n            would be ordered.\n    \"\"\"\n    def __init__(self, order):\n        if not isinstance(order, list):\n            raise ValueError('``order`` must be a list')\n        self.order = order\n        super(UnpackDictionary, self).__init__()\n\n    def call(self, kwargs):\n        args = tuple([kwargs[name] for name in self.order])\n        return args",
  "class WrapOutput(Processor):\n    \"\"\"Wraps arguments in dictionary\n\n    # Arguments\n        keys: List of strings representing the keys used to wrap the inputs.\n            The order of the list must correspond to the same order of\n            inputs (''args'').\n    \"\"\"\n    def __init__(self, keys):\n        if not isinstance(keys, list):\n            raise ValueError('``order`` must be a list')\n        self.keys = keys\n        super(WrapOutput, self).__init__()\n\n    def call(self, *args):\n        return dict(zip(self.keys, args))",
  "class ExtendInputs(Processor):\n    \"\"\"Extends number of inputs a function can take applying the identity\n    function to all new/extended inputs.\n    e.g. For a given function f(x) = y. If g = ExtendInputs(f), we can\n    now have g(x, x1, x2, ..., xn) = y, x1, x2, ..., xn.\n\n    # Arguments\n        processor: Function e.g. any procesor in ''paz.processors''.\n    \"\"\"\n    def __init__(self, processor):\n        self.processor = processor\n        name = '-'.join([self.__class__.__name__, self.processor.name])\n        super(ExtendInputs, self).__init__(name)\n\n    def call(self, X, *args):\n        return self.processor(X), args",
  "class Concatenate(Processor):\n    \"\"\"Concatenates a list of arrays in given ''axis''.\n\n    # Arguments\n        axis: Int.\n    \"\"\"\n    def __init__(self, axis):\n        super(Concatenate, self)\n        self.axis = axis\n\n    def call(self, inputs):\n        return np.concatenate(inputs, self.axis)",
  "class SequenceWrapper(Processor):\n    \"\"\"Wraps arguments to directly use\n    ''paz.abstract.ProcessingSequence'' or\n    ''paz.abstract.GeneratingSequence''.\n\n    # Arguments\n        inputs_info: Dictionary containing an integer per key representing\n            the argument to grab, and as value a dictionary containing the\n            tensor name as key and the tensor shape of a single sample as value\n            e.g. {0: {'input_image': [300, 300, 3]}, 1: {'depth': [300, 300]}}.\n            The values given here are for the inputs of the model.\n        labels_info: Dictionary containing an integer per key representing\n            the argument to grab, and as value a dictionary containing the\n            tensor name as key and the tensor shape of a single sample as value\n            e.g. {2: {'classes': [10]}}.\n            The values given here are for the labels of the model.\n    \"\"\"\n    def __init__(self, inputs_info, labels_info):\n        if not isinstance(inputs_info, dict):\n            raise ValueError('``inputs_info`` must be a dictionary')\n        self.inputs_info = inputs_info\n        if not isinstance(labels_info, dict):\n            raise ValueError('``inputs_info`` must be a dictionary')\n        self.labels_info = labels_info\n        self.inputs_name_to_shape = self._extract_name_to_shape(inputs_info)\n        self.labels_name_to_shape = self._extract_name_to_shape(labels_info)\n        self.ordered_input_names = self._extract_ordered_names(inputs_info)\n        self.ordered_label_names = self._extract_ordered_names(labels_info)\n        super(SequenceWrapper, self).__init__()\n\n    def _extract_name_to_shape(self, info):\n        name_to_shape = {}\n        for values in info.values():\n            for key, value in values.items():\n                name_to_shape[key] = value\n        return name_to_shape\n\n    def _extract_ordered_names(self, info):\n        arguments = list(info.keys())\n        arguments.sort()\n        names = []\n        for argument in arguments:\n            names.append(list(info[argument].keys())[0])\n        return names\n\n    def _wrap(self, args, info):\n        wrap = {}\n        for arg, name_to_shape in info.items():\n            name = list(name_to_shape.keys())[0]\n            wrap[name] = args[arg]\n        return wrap\n\n    def call(self, *args):\n        inputs = self._wrap(args, self.inputs_info)\n        labels = self._wrap(args, self.labels_info)\n        return {'inputs': inputs, 'labels': labels}",
  "class Predict(Processor):\n    \"\"\"Perform input preprocessing, model prediction and output postprocessing.\n\n    # Arguments\n        model: Class with a ''predict'' method e.g. a Keras model.\n        preprocess: Function applied to given inputs.\n        postprocess: Function applied to outputted predictions from model.\n    \"\"\"\n    def __init__(self, model, preprocess=None, postprocess=None):\n        super(Predict, self).__init__()\n        self.model = model\n        self.preprocess = preprocess\n        self.postprocess = postprocess\n\n    def call(self, x):\n        return predict(x, self.model, self.preprocess, self.postprocess)",
  "class ToClassName(Processor):\n    def __init__(self, labels):\n        super(ToClassName, self).__init__()\n        self.labels = labels\n\n    def call(self, x):\n        return self.labels[np.argmax(x)]",
  "class ExpandDims(Processor):\n    \"\"\"Expand dimension of given array.\n\n    # Arguments\n        axis: Int.\n    \"\"\"\n    def __init__(self, axis):\n        super(ExpandDims, self).__init__()\n        self.axis = axis\n\n    def call(self, x):\n        return np.expand_dims(x, self.axis)",
  "class SelectElement(Processor):\n    \"\"\"Selects element of input value.\n\n    # Arguments\n        index: Int. argument to select from ''inputs''.\n    \"\"\"\n    def __init__(self, index):\n        super(SelectElement, self).__init__()\n        self.index = index\n\n    def call(self, inputs):\n        return inputs[self.index]",
  "class BoxClassToOneHotVector(Processor):\n    \"\"\"Transform box data with class index to a one-hot encoded vector.\n\n    # Arguments\n        num_classes: Integer. Total number of classes.\n    \"\"\"\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        super(BoxClassToOneHotVector, self).__init__()\n\n    def call(self, boxes):\n        class_indices = boxes[:, 4].astype('int')\n        one_hot_vectors = to_one_hot(class_indices, self.num_classes)\n        one_hot_vectors = one_hot_vectors.reshape(-1, self.num_classes)\n        boxes = np.hstack([boxes[:, :4], one_hot_vectors.astype('float')])\n        return boxes",
  "class Squeeze(Processor):\n    \"\"\"Wrap around numpy `squeeze` due to common use before model predict.\n    # Arguments\n        expand_dims: Int or list of Ints.\n        topic: String.\n    \"\"\"\n    def __init__(self, axis):\n        super(Squeeze, self).__init__()\n        self.axis = axis\n\n    def call(self, x):\n        return np.squeeze(x, axis=self.axis)",
  "class Copy(Processor):\n    \"\"\"Copies value passed to function.\n    \"\"\"\n    def __init__(self):\n        super(Copy, self).__init__()\n\n    def call(self, x):\n        return x.copy()",
  "class Lambda(object):\n    \"\"\"Applies a lambda function as a processor transformation.\n\n    # Arguments\n        function: Function.\n    \"\"\"\n\n    def __init__(self, function):\n        self.function = function\n\n    def __call__(self, x):\n        return self.function(x)",
  "class StochasticProcessor(Processor):\n    def __init__(self, probability=0.5, name=None):\n        \"\"\"Adds stochasticity to the user implemented ``call`` function\n\n        # Arguments:\n            probability: Probability of calling ``call`` function\n\n        # Example:\n        ```python\n        class RandomAdd(StochasticProcessor):\n        def __init__(self, probability=0.5):\n            super(StochasticProcessor, self).__init__(probability)\n\n        def call(self, x):\n            return x + 1\n\n        random_add = RandomAdd(probability=0.5)\n        # value can be either 1.0 or 2.0\n        value = random_add(1.0)\n        ```\n        \"\"\"\n        super(StochasticProcessor, self).__init__(name=name)\n        self.probability = probability\n\n    def call(self, X):\n        raise NotImplementedError\n\n    def __call__(self, X):\n        if self.probability >= np.random.rand():\n            return self.call(X)\n        return X",
  "class Stochastic(Processor):\n    def __init__(self, function, probability=0.5, name=None):\n        \"\"\"Adds stochasticity to a given ``function``\n\n        # Arguments:\n            function: Callable object i.e. python function or\n                ``paz.abstract.Processor``.\n            probability: Probability of calling ``function``.\n\n        # Example:\n        ```python\n        stochastic_add_one = Stochastic(lambda x: x + 1, 0.5)\n        # value can be either 0.0 or 1.0\n        value = random_add(0.0)\n        ```\n        \"\"\"\n        super(Stochastic, self).__init__(name=name)\n        self.function = function\n        self.probability = probability\n\n    @property\n    def probability(self):\n        return self._probability\n\n    @probability.setter\n    def probability(self, probability):\n        assert 0.0 <= probability <= 1.0, 'Probability must be between 0 and 1'\n        self._probability = probability\n\n    def call(self, X):\n        if self.probability >= np.random.rand():\n            return self.function(X)\n        return X",
  "class UnwrapDictionary(Processor):\n    \"\"\"Unwraps a dictionry into a list given the key order.\n    \"\"\"\n    def __init__(self, keys):\n        super(UnwrapDictionary, self).__init__()\n        self.keys = keys\n\n    def call(self, dictionary):\n        return [dictionary[key] for key in self.keys]",
  "class Scale(Processor):\n    \"\"\"Scales an input.\n    \"\"\"\n    def __init__(self, scales):\n        super(Scale, self).__init__()\n        self.scales = scales\n\n    def call(self, values):\n        return self.scales * values",
  "class AppendValues(Processor):\n    \"\"\"Append dictionary values to lists\n\n    # Arguments\n        keys: Keys to dictionary values\n    \"\"\"\n    def __init__(self, keys):\n        super(AppendValues, self).__init__()\n        self.keys = keys\n\n    def call(self, dictionary, lists):\n        return append_values(dictionary, lists, self.keys)",
  "class BooleanToTextMessage(Processor):\n    \"\"\"Convert a boolean to text message.\n    # Arguments\n        true_message: String. Message for true case.\n        false_message: String. Message for false case.\n        Flag: Boolean.\n\n    # Returns\n        message: String.\n    \"\"\"\n    def __init__(self, true_message, false_message):\n        super(BooleanToTextMessage, self).__init__()\n        self.true_message = true_message\n        self.false_message = false_message\n\n    def call(self, flag):\n        if flag:\n            message = self.true_message\n        else:\n            message = self.false_message\n        return message",
  "class PrintTopics(Processor):\n    \"\"\"Prints topics\n    # Arguments\n        topics: List of keys to the inputted dictionary\n\n    # Returns\n        Returns same dictionary but outputs to terminal topic values.\n    \"\"\"\n    def __init__(self, topics):\n        super(PrintTopics, self).__init__()\n        self.topics = topics\n\n    def call(self, dictionary):\n        [print(dictionary[topic]) for topic in self.topics]\n        return dictionary",
  "def __init__(self, processor, intro_indices=[0], outro_indices=[0],\n                 keep=None):\n        self.processor = processor\n        if not isinstance(intro_indices, list):\n            raise ValueError('``intro_indices`` must be a list')\n        if not isinstance(outro_indices, list):\n            raise ValueError('``outro_indices`` must be a list')\n        self.intro_indices = intro_indices\n        self.outro_indices = outro_indices\n        name = '-'.join([self.__class__.__name__, self.processor.name])\n        self.keep = keep\n        super(ControlMap, self).__init__(name)",
  "def _select(self, inputs, indices):\n        return [inputs[index] for index in indices]",
  "def _remove(self, inputs, indices):\n        return [inputs[i] for i in range(len(inputs)) if i not in indices]",
  "def _split(self, inputs, indices):\n        return self._select(inputs, indices), self._remove(inputs, indices)",
  "def _insert(self, args, extra_args, indices):\n        [args.insert(index, arg) for index, arg in zip(indices, extra_args)]\n        return args",
  "def call(self, *args):\n        selected_args, remaining_args = self._split(args, self.intro_indices)\n        processed_args = self.processor(*selected_args)\n        if not isinstance(processed_args, tuple):\n            processed_args = [processed_args]\n        return_args = self._insert(\n            remaining_args, processed_args, self.outro_indices)\n\n        if self.keep is not None:\n            keep_intro = list(self.keep.keys())\n            keep_outro = list(self.keep.values())\n            keep_args = self._select(args, keep_intro)\n            return_args = self._insert(return_args, keep_args, keep_outro)\n\n        return tuple(return_args)",
  "def __init__(self, processor):\n        super(ExpandDomain, self).__init__(processor)",
  "def __init__(self, intro_indices, outro_indices):\n        super(CopyDomain, self).__init__()\n        if not isinstance(intro_indices, list):\n            raise ValueError('``intro_indices`` must be a list')\n        if not isinstance(outro_indices, list):\n            raise ValueError('``outro_indices`` must be a list')\n        self.intro_indices = intro_indices\n        self.outro_indices = outro_indices",
  "def _select(self, inputs, indices):\n        return [inputs[index] for index in indices]",
  "def _insert(self, args, axes, values):\n        [args.insert(axis, value) for axis, value in zip(axes, values)]\n        return args",
  "def call(self, *args):\n        selections = self._select(args, self.intro_indices)\n        args = self._insert(list(args), self.outro_indices, selections)\n        return tuple(args)",
  "def __init__(self, order):\n        if not isinstance(order, list):\n            raise ValueError('``order`` must be a list')\n        self.order = order\n        super(UnpackDictionary, self).__init__()",
  "def call(self, kwargs):\n        args = tuple([kwargs[name] for name in self.order])\n        return args",
  "def __init__(self, keys):\n        if not isinstance(keys, list):\n            raise ValueError('``order`` must be a list')\n        self.keys = keys\n        super(WrapOutput, self).__init__()",
  "def call(self, *args):\n        return dict(zip(self.keys, args))",
  "def __init__(self, processor):\n        self.processor = processor\n        name = '-'.join([self.__class__.__name__, self.processor.name])\n        super(ExtendInputs, self).__init__(name)",
  "def call(self, X, *args):\n        return self.processor(X), args",
  "def __init__(self, axis):\n        super(Concatenate, self)\n        self.axis = axis",
  "def call(self, inputs):\n        return np.concatenate(inputs, self.axis)",
  "def __init__(self, inputs_info, labels_info):\n        if not isinstance(inputs_info, dict):\n            raise ValueError('``inputs_info`` must be a dictionary')\n        self.inputs_info = inputs_info\n        if not isinstance(labels_info, dict):\n            raise ValueError('``inputs_info`` must be a dictionary')\n        self.labels_info = labels_info\n        self.inputs_name_to_shape = self._extract_name_to_shape(inputs_info)\n        self.labels_name_to_shape = self._extract_name_to_shape(labels_info)\n        self.ordered_input_names = self._extract_ordered_names(inputs_info)\n        self.ordered_label_names = self._extract_ordered_names(labels_info)\n        super(SequenceWrapper, self).__init__()",
  "def _extract_name_to_shape(self, info):\n        name_to_shape = {}\n        for values in info.values():\n            for key, value in values.items():\n                name_to_shape[key] = value\n        return name_to_shape",
  "def _extract_ordered_names(self, info):\n        arguments = list(info.keys())\n        arguments.sort()\n        names = []\n        for argument in arguments:\n            names.append(list(info[argument].keys())[0])\n        return names",
  "def _wrap(self, args, info):\n        wrap = {}\n        for arg, name_to_shape in info.items():\n            name = list(name_to_shape.keys())[0]\n            wrap[name] = args[arg]\n        return wrap",
  "def call(self, *args):\n        inputs = self._wrap(args, self.inputs_info)\n        labels = self._wrap(args, self.labels_info)\n        return {'inputs': inputs, 'labels': labels}",
  "def __init__(self, model, preprocess=None, postprocess=None):\n        super(Predict, self).__init__()\n        self.model = model\n        self.preprocess = preprocess\n        self.postprocess = postprocess",
  "def call(self, x):\n        return predict(x, self.model, self.preprocess, self.postprocess)",
  "def __init__(self, labels):\n        super(ToClassName, self).__init__()\n        self.labels = labels",
  "def call(self, x):\n        return self.labels[np.argmax(x)]",
  "def __init__(self, axis):\n        super(ExpandDims, self).__init__()\n        self.axis = axis",
  "def call(self, x):\n        return np.expand_dims(x, self.axis)",
  "def __init__(self, index):\n        super(SelectElement, self).__init__()\n        self.index = index",
  "def call(self, inputs):\n        return inputs[self.index]",
  "def __init__(self, num_classes):\n        self.num_classes = num_classes\n        super(BoxClassToOneHotVector, self).__init__()",
  "def call(self, boxes):\n        class_indices = boxes[:, 4].astype('int')\n        one_hot_vectors = to_one_hot(class_indices, self.num_classes)\n        one_hot_vectors = one_hot_vectors.reshape(-1, self.num_classes)\n        boxes = np.hstack([boxes[:, :4], one_hot_vectors.astype('float')])\n        return boxes",
  "def __init__(self, axis):\n        super(Squeeze, self).__init__()\n        self.axis = axis",
  "def call(self, x):\n        return np.squeeze(x, axis=self.axis)",
  "def __init__(self):\n        super(Copy, self).__init__()",
  "def call(self, x):\n        return x.copy()",
  "def __init__(self, function):\n        self.function = function",
  "def __call__(self, x):\n        return self.function(x)",
  "def __init__(self, probability=0.5, name=None):\n        \"\"\"Adds stochasticity to the user implemented ``call`` function\n\n        # Arguments:\n            probability: Probability of calling ``call`` function\n\n        # Example:\n        ```python\n        class RandomAdd(StochasticProcessor):\n        def __init__(self, probability=0.5):\n            super(StochasticProcessor, self).__init__(probability)\n\n        def call(self, x):\n            return x + 1\n\n        random_add = RandomAdd(probability=0.5)\n        # value can be either 1.0 or 2.0\n        value = random_add(1.0)\n        ```\n        \"\"\"\n        super(StochasticProcessor, self).__init__(name=name)\n        self.probability = probability",
  "def call(self, X):\n        raise NotImplementedError",
  "def __call__(self, X):\n        if self.probability >= np.random.rand():\n            return self.call(X)\n        return X",
  "def __init__(self, function, probability=0.5, name=None):\n        \"\"\"Adds stochasticity to a given ``function``\n\n        # Arguments:\n            function: Callable object i.e. python function or\n                ``paz.abstract.Processor``.\n            probability: Probability of calling ``function``.\n\n        # Example:\n        ```python\n        stochastic_add_one = Stochastic(lambda x: x + 1, 0.5)\n        # value can be either 0.0 or 1.0\n        value = random_add(0.0)\n        ```\n        \"\"\"\n        super(Stochastic, self).__init__(name=name)\n        self.function = function\n        self.probability = probability",
  "def probability(self):\n        return self._probability",
  "def probability(self, probability):\n        assert 0.0 <= probability <= 1.0, 'Probability must be between 0 and 1'\n        self._probability = probability",
  "def call(self, X):\n        if self.probability >= np.random.rand():\n            return self.function(X)\n        return X",
  "def __init__(self, keys):\n        super(UnwrapDictionary, self).__init__()\n        self.keys = keys",
  "def call(self, dictionary):\n        return [dictionary[key] for key in self.keys]",
  "def __init__(self, scales):\n        super(Scale, self).__init__()\n        self.scales = scales",
  "def call(self, values):\n        return self.scales * values",
  "def __init__(self, keys):\n        super(AppendValues, self).__init__()\n        self.keys = keys",
  "def call(self, dictionary, lists):\n        return append_values(dictionary, lists, self.keys)",
  "def __init__(self, true_message, false_message):\n        super(BooleanToTextMessage, self).__init__()\n        self.true_message = true_message\n        self.false_message = false_message",
  "def call(self, flag):\n        if flag:\n            message = self.true_message\n        else:\n            message = self.false_message\n        return message",
  "def __init__(self, topics):\n        super(PrintTopics, self).__init__()\n        self.topics = topics",
  "def call(self, dictionary):\n        [print(dictionary[topic]) for topic in self.topics]\n        return dictionary",
  "def compute_matches(dataset, detector, class_to_arg, iou_thresh=0.5):\n    \"\"\"\n    Arguments:\n        dataset: List of dictionaries containing 'image' as key and a\n            numpy array representing an image as value.\n        detector : Function for performing inference\n        class_to_arg: Dict. of class names and their id\n        iou_thresh (float): A prediction is correct if its Intersection over\n            Union with the ground truth is above this value..\n\n    Returns:\n        num_positives: Dict. containing number of positives for each class\n        score: Dict. containing matching scores of boxes for each class\n        match: Dict. containing match/non-match info of boxes in each class\n    \"\"\"\n    # classes_count = len(np.unique(np.concatenate(ground_truth_class_args)))\n    num_classes = len(class_to_arg)\n    num_positives = {label_id: 0 for label_id in range(1, num_classes + 1)}\n    score = {label_id: [] for label_id in range(1, num_classes + 1)}\n    match = {label_id: [] for label_id in range(1, num_classes + 1)}\n    for sample in dataset:\n        # obtaining ground truths\n        ground_truth_boxes = np.array(sample['boxes'][:, :4])\n        ground_truth_class_args = np.array(sample['boxes'][:, 4])\n        if 'difficulties' in sample.keys():\n            difficulties = np.array(sample['difficulties'])\n        else:\n            difficulties = None\n        # obtaining predictions\n        image = load_image(sample['image'])\n        results = detector(image)\n        predicted_boxes, predicted_class_args, predicted_scores = [], [], []\n        for box2D in results['boxes2D']:\n            predicted_scores.append(box2D.score)\n            predicted_class_args.append(class_to_arg[box2D.class_name])\n            predicted_boxes.append(list(box2D.coordinates))\n        predicted_boxes = np.array(predicted_boxes, dtype=np.float32)\n        predicted_class_args = np.array(predicted_class_args)\n        predicted_scores = np.array(predicted_scores, dtype=np.float32)\n        # setting difficulties to ``Easy`` if they are None\n        if difficulties is None:\n            difficulties = np.zeros(len(ground_truth_boxes), dtype=bool)\n        # iterating over each class present in the image\n        class_args = np.concatenate(\n            (predicted_class_args, ground_truth_class_args))\n        class_args = np.unique(class_args).astype(int)\n        for class_arg in class_args:\n            # masking predictions by class\n            class_mask = class_arg == predicted_class_args\n            class_predicted_boxes = predicted_boxes[class_mask]\n            class_predicted_scores = predicted_scores[class_mask]\n            # sort score from maximum to minimum for masked predictions\n            sorted_args = class_predicted_scores.argsort()[::-1]\n            class_predicted_boxes = class_predicted_boxes[sorted_args]\n            class_predicted_scores = class_predicted_scores[sorted_args]\n            # masking ground truths by class\n            class_mask = class_arg == ground_truth_class_args\n            class_ground_truth_boxes = ground_truth_boxes[class_mask]\n            class_difficulties = difficulties[class_mask]\n            # the number of positives equals the number of easy boxes\n            num_easy = np.logical_not(class_difficulties).sum()\n            num_positives[class_arg] = num_positives[class_arg] + num_easy\n            # add all predicted scores to scores\n            score[class_arg].extend(class_predicted_scores)\n            # if not predicted boxes for this class continue\n            if len(class_predicted_boxes) == 0:\n                continue\n            # if not ground truth boxes continue but add zeros as matches\n            if len(class_ground_truth_boxes) == 0:\n                match[class_arg].extend((0,) * len(class_predicted_boxes))\n                continue\n\n            # evaluation on VOC follows integer typed bounding boxes.\n            class_predicted_boxes = class_predicted_boxes.copy()\n            class_predicted_boxes[:, 2:] = (\n                class_predicted_boxes[:, 2:] + 1)\n            class_ground_truth_boxes = class_ground_truth_boxes.copy()\n            class_ground_truth_boxes[:, 2:] = (\n                class_ground_truth_boxes[:, 2:] + 1)\n\n            ious = compute_ious(\n                class_predicted_boxes, class_ground_truth_boxes)\n            ground_truth_args = ious.argmax(axis=1)\n            # set -1 if there is no matching ground truth\n            ground_truth_args[ious.max(axis=1) < iou_thresh] = -1\n            selected = np.zeros(len(class_ground_truth_boxes), dtype=bool)\n            for ground_truth_arg in ground_truth_args:\n                if ground_truth_arg >= 0:\n                    if class_difficulties[ground_truth_arg]:\n                        match[class_arg].append(-1)\n                    else:\n                        if not selected[ground_truth_arg]:\n                            match[class_arg].append(1)\n                        else:\n                            match[class_arg].append(0)\n                    selected[ground_truth_arg] = True\n                else:\n                    match[class_arg].append(0)\n    return num_positives, score, match",
  "def calculate_relevance_metrics(num_positives, scores, matches):\n    \"\"\"Calculates precision and recall.\n    Arguments:\n        num_positives: Dict. with number of positives for each class\n        scores: Dict. with matching scores of boxes for each class\n        matches: Dict. wth match/non-match info for boxes for each class\n    Returns:\n        precision: Dict. with precision values per class\n        recall : Dict. with recall values per class\n    \"\"\"\n    num_classes = max(num_positives.keys()) + 1\n    precision, recall = [None] * num_classes, [None] * num_classes\n    for class_arg in num_positives.keys():\n        class_positive_matches = np.array(matches[class_arg], dtype=np.int8)\n        class_scores = np.array(scores[class_arg])\n        order = class_scores.argsort()[::-1]\n        class_positive_matches = class_positive_matches[order]\n        true_positives = np.cumsum(class_positive_matches == 1)\n        false_positives = np.cumsum(class_positive_matches == 0)\n        precision[class_arg] = (\n            true_positives / (false_positives + true_positives))\n        if num_positives[class_arg] > 0:\n            recall[class_arg] = true_positives / num_positives[class_arg]\n    return precision, recall",
  "def calculate_average_precisions(precision, recall, use_07_metric=False):\n    \"\"\"Calculate average precisions based based on PASCAL VOC evaluation\n    Arguments:\n        num_positives: Dict. with number of positives for each class\n        scores: Dict. with matching scores of boxes for each class\n        matches: Dict. wth match/non-match info for boxes for each class\n    Returns:\n    \"\"\"\n\n    num_classes = len(precision)\n    average_precisions = np.empty(num_classes)\n    for class_arg in range(num_classes):\n        if precision[class_arg] is None or recall[class_arg] is None:\n            average_precisions[class_arg] = np.nan\n            continue\n\n        if use_07_metric:\n            # 11 point metric\n            average_precisions[class_arg] = 0\n            for t in np.arange(0., 1.1, 0.1):\n                if np.sum(recall[class_arg] >= t) == 0:\n                    p_interpolation = 0\n                else:\n                    p_interpolation = np.max(\n                        np.nan_to_num(\n                            precision[class_arg]\n                        )[recall[class_arg] >= t]\n                    )\n                average_precision_class = average_precisions[class_arg]\n                average_precision_class = (average_precision_class +\n                                           (p_interpolation / 11))\n                average_precisions[class_arg] = average_precision_class\n\n        else:\n            # first append sentinel values at the end\n            average_precision = np.concatenate(\n                ([0], np.nan_to_num(precision[class_arg]), [0]))\n            average_recall = np.concatenate(([0], recall[class_arg], [1]))\n\n            average_precision = np.maximum.accumulate(\n                average_precision[::-1])[::-1]\n\n            # to calculate area under PR curve, look for points\n            # where X axis (recall) changes value\n            recall_change_arg = np.where(\n                average_recall[1:] != average_recall[:-1])[0]\n\n            # and sum (\\Delta recall) * precision\n            average_precisions[class_arg] = np.sum(\n                (average_recall[recall_change_arg + 1] -\n                 average_recall[recall_change_arg]) *\n                average_precision[recall_change_arg + 1])\n    return average_precisions",
  "def evaluateMAP(detector, dataset, class_to_arg, iou_thresh=0.5,\n                use_07_metric=False):\n    \"\"\"Calculate average precisions based on evaluation code of PASCAL VOC.\n    Arguments:\n        dataset: List of dictionaries containing 'image' as key and a\n            numpy array representing an image as value.\n        detector : Function for performing inference\n        class_to_arg: Dict. of class names and their id\n        iou_thresh: Float indicating intersection over union threshold for\n            assigning a prediction as correct.\n    # Returns:\n    \"\"\"\n    positives, score, match = compute_matches(\n        dataset, detector, class_to_arg, iou_thresh)\n    precision, recall = calculate_relevance_metrics(positives, score, match)\n    average_precisions = calculate_average_precisions(\n        precision, recall, use_07_metric)\n    return {'ap': average_precisions, 'map': np.nanmean(average_precisions)}",
  "class EstimatePoseKeypoints(Processor):\n    def __init__(self, detect, estimate_keypoints, camera, offsets,\n                 model_points, class_to_dimensions, radius=3, thickness=1):\n        \"\"\"Pose estimation pipeline using keypoints.\n\n        # Arguments\n            detect: Function that outputs a dictionary with a key\n                ``boxes2D`` having a list of ``Box2D`` messages.\n            estimate_keypoints: Function that outputs a dictionary\n                with a key ``keypoints`` with numpy array as value\n            camera: Instance of ``paz.backend.camera.Camera`` with\n                camera intrinsics.\n            offsets: List of floats indicating the scaled offset to\n                be added to the ``Box2D`` coordinates.\n            model_points: Numpy array of shape ``(num_keypoints, 3)``\n                indicating the 3D coordinates of the predicted keypoints\n                from the ``esimate_keypoints`` function.\n            class_to_dimensions: Dictionary with keys being the class labels\n                of the predicted ``Box2D`` messages and the values a list of\n                three integers indicating the width, height and depth of the\n                object e.g. {'PowerDrill': [30, 20, 10]}.\n            radius: Int. radius of keypoint to be drawn.\n            thickness: Int. thickness of 3D box.\n\n        # Returns\n            A function that takes an RGB image and outputs the following\n            inferences as keys of a dictionary:\n                ``image``, ``boxes2D``, ``keypoints`` and ``poses6D``.\n\n        \"\"\"\n        super(EstimatePoseKeypoints, self).__init__()\n        self.num_keypoints = estimate_keypoints.num_keypoints\n        self.detect = detect\n        self.estimate_keypoints = estimate_keypoints\n        self.square = SequentialProcessor()\n        self.square.add(pr.SquareBoxes2D())\n        self.square.add(pr.OffsetBoxes2D(offsets))\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.change_coordinates = pr.ChangeKeypointsCoordinateSystem()\n        self.solve_PNP = pr.SolvePNP(model_points, camera)\n        self.draw_keypoints = pr.DrawKeypoints2D(self.num_keypoints, radius)\n        self.draw_box = pr.DrawBoxes3D(camera, class_to_dimensions,\n                                       thickness=thickness)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D', 'keypoints', 'poses6D'])\n\n    def call(self, image):\n        boxes2D = self.detect(image)['boxes2D']\n        boxes2D = self.square(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        poses6D, keypoints2D = [], []\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            keypoints = self.estimate_keypoints(cropped_image)['keypoints']\n            keypoints = self.change_coordinates(keypoints, box2D)\n            pose6D = self.solve_PNP(keypoints)\n            image = self.draw_keypoints(image, keypoints)\n            image = self.draw_box(image, pose6D)\n            keypoints2D.append(keypoints)\n            poses6D.append(pose6D)\n        return self.wrap(image, boxes2D, keypoints2D, poses6D)",
  "class HeadPoseKeypointNet2D32(EstimatePoseKeypoints):\n    \"\"\"Head pose estimation pipeline using a ``HaarCascade`` face detector\n        and a pre-trained ``KeypointNet2D`` estimation model.\n\n        # Arguments\n            camera: Instance of ``paz.backend.camera.Camera`` with\n                camera intrinsics.\n            offsets: List of floats indicating the scaled offset to\n                be added to the ``Box2D`` coordinates.\n            radius: Int. radius of keypoint to be drawn.\n\n        # Example\n            ``` python\n            from paz.pipelines import HeadPoseKeypointNet2D32\n\n            estimate_pose = HeadPoseKeypointNet2D32()\n\n            # apply directly to an image (numpy-array)\n            inferences = estimate_pose(image)\n            ```\n\n        # Returns\n            A function that takes an RGB image and outputs the following\n            inferences as keys of a dictionary:\n                ``image``, ``boxes2D``, ``keypoints`` and ``poses6D``.\n        \"\"\"\n    def __init__(self, camera, offsets=[0, 0], radius=5, thickness=2):\n        detect = HaarCascadeFrontalFace(draw=False)\n        estimate_keypoints = FaceKeypointNet2D32(draw=False)\n        \"\"\"\n                               4--------1\n                              /|       /|\n                             / |      / |\n                            3--------2  |\n                            |  8_____|__5\n                            | /      | /\n                            |/       |/\n                            7--------6\n\n                   Z (depth)\n                  /\n                 /_____X (width)\n                 |\n                 |\n                 Y (height)\n        \"\"\"\n        KEYPOINTS3D = np.array([\n            [-220, 1138, 678],  # left--center-eye\n            [+220, 1138, 678],  # right-center-eye\n            [-131, 1107, 676],  # left--eye close to nose\n            [-294, 1123, 610],  # left--eye close to ear\n            [+131, 1107, 676],  # right-eye close to nose\n            [+294, 1123, 610],  # right-eye close to ear\n            [-106, 1224, 758],  # left--eyebrow close to nose\n            [-375, 1208, 585],  # left--eyebrow close to ear\n            [+106, 1224, 758],  # right-eyebrow close to nose\n            [+375, 1208, 585],  # right-eyebrow close to ear\n            [0.0, 919, 909],  # nose\n            [-183, 683, 691],  # lefty-lip\n            [+183, 683, 691],  # right-lip\n            [0.0, 754, 826],  # up---lip\n            [0.0, 645, 815],  # down-lip\n        ])\n        KEYPOINTS3D = KEYPOINTS3D - np.mean(KEYPOINTS3D, axis=0)\n        super(HeadPoseKeypointNet2D32, self).__init__(\n            detect, estimate_keypoints, camera, offsets,\n            KEYPOINTS3D, {None: [900, 1200, 800]}, radius, thickness)",
  "class SingleInstancePIX2POSE6D(Processor):\n    \"\"\"Predicts a single pose6D from an image. Optionally if a box2D message is\n        given it translates the predicted points2D to new origin located at\n        box2D top-left corner.\n\n    # Arguments\n        model: Keras segmentation model.\n        object_sizes: Array (3) determining the (width, height, depth)\n        camera: PAZ Camera with intrinsic matrix.\n        epsilon: Float. Values below this value would be replaced by 0.\n        resize: Boolean. If True RGB mask is resized before computing PnP.\n        class_name: Str indicating object name.\n        draw: Boolean. If True drawing functions are applied to output image.\n\n    # Returns\n        Dictionary with inferred points2D, points3D, pose6D and image.\n    \"\"\"\n    def __init__(self, model, object_sizes, camera,\n                 epsilon=0.15, resize=False, class_name=None, draw=True):\n        super(SingleInstancePIX2POSE6D, self).__init__()\n        self.camera = camera\n        self.pix2points = Pix2Points(model, object_sizes, epsilon, resize)\n        self.solvePnP = pr.SolveChangingObjectPnPRANSAC(self.camera.intrinsics)\n        self.draw_pose6D = pr.DrawPose6D(object_sizes, self.camera.intrinsics)\n        self.wrap = pr.WrapOutput(['image', 'points2D', 'points3D', 'pose6D'])\n        self.class_name = str(class_name)\n        self.object_sizes = object_sizes\n        self.draw = draw\n\n    def call(self, image, box2D=None):\n        inferences = self.pix2points(image)\n        points2D = inferences['points2D']\n        points3D = inferences['points3D']\n        points2D = denormalize_keypoints2D(points2D, *image.shape[:2])\n        if box2D is not None:\n            points2D = translate_points2D_origin(points2D, box2D.coordinates)\n            self.class_name = box2D.class_name\n        pose6D = None\n        if len(points3D) > self.solvePnP.MIN_REQUIRED_POINTS:\n            success, R, T = self.solvePnP(points3D, points2D)\n            if success:\n                pose6D = Pose6D.from_rotation_vector(R, T, self.class_name)\n        if (self.draw and (box2D is None) and (pose6D is not None)):\n            colors = points3D_to_RGB(points3D, self.object_sizes)\n            image = draw_points2D(image, points2D, colors)\n            image = self.draw_pose6D(image, pose6D)\n        inferences = self.wrap(image, points2D, points3D, pose6D)\n        return inferences",
  "class MultiInstancePIX2POSE6D(Processor):\n    \"\"\"Predicts poses6D of multiple instances the same object from an image.\n\n    # Arguments\n        estimate_pose: Function that takes as input an image and outputs a\n            dictionary with points2D, points3D and pose6D messages e.g\n            SingleInstancePIX2POSE6D\n        offsets: List of length two containing floats e.g. (x_scale, y_scale)\n        camera: PAZ Camera with intrinsic matrix.\n        draw: Boolean. If True drawing functions are applied to output image.\n\n    # Returns\n        Dictionary with inferred boxes2D, poses6D and image.\n    \"\"\"\n    def __init__(self, estimate_pose, offsets, camera=None, draw=True):\n        super(MultiInstancePIX2POSE6D, self).__init__()\n        self.draw = draw\n        self.estimate_pose = estimate_pose\n        self.object_sizes = self.estimate_pose.object_sizes\n        self.camera = self.estimate_pose.camera if camera is None else camera\n        valid_names = [self.estimate_pose.class_name]\n        self.postprocess_boxes = PostprocessBoxes2D(offsets, valid_names)\n\n        self.append_values = pr.AppendValues(\n            ['pose6D', 'points2D', 'points3D'])\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.draw_RGBmask = pr.DrawRGBMasks(self.object_sizes)\n        self.draw_boxes2D = pr.DrawBoxes2D(valid_names, colors=[[0, 255, 0]])\n        self.draw_poses6D = pr.DrawPoses6D(\n            self.object_sizes, camera.intrinsics)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D', 'poses6D'])\n\n    def call(self, image, boxes2D):\n        boxes2D = self.postprocess_boxes(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        poses6D, points2D, points3D = [], [], []\n        for crop, box2D in zip(cropped_images, boxes2D):\n            inferences = self.estimate_pose(crop, box2D)\n            self.append_values(inferences, [poses6D, points2D, points3D])\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n            image = self.draw_RGBmask(image, points2D, points3D)\n            image = self.draw_poses6D(image, poses6D)\n        return self.wrap(image, boxes2D, poses6D)",
  "class SinglePowerDrillPIX2POSE6D(SingleInstancePIX2POSE6D):\n    \"\"\"Predicts the pose6D of the YCB 035_power_drill object from an image.\n        Optionally if a box2D message is given it translates the predicted\n        points2D to new origin located at box2D top-left corner.\n\n    # Arguments\n        camera: PAZ Camera with intrinsic matrix.\n        epsilon: Float. Values below this value would be replaced by 0.\n        resize: Boolean. If True RGB mask is resized before computing PnP.\n        draw: Boolean. If True drawing functions are applied to output image.\n\n    # Returns\n        Dictionary with inferred points2D, points3D, pose6D and image.\n    \"\"\"\n    def __init__(self, camera, epsilon=0.15, resize=False, draw=True):\n        model = UNET_VGG16(3, (128, 128, 3))\n        URL = ('https://github.com/oarriaga/altamira-data/'\n               'releases/download/v0.13/')\n        name = 'UNET-VGG16_POWERDRILL_weights.hdf5'\n        weights_path = get_file(name, URL + name, cache_subdir='paz/models')\n        print('Loading %s model weights' % weights_path)\n        model.load_weights(weights_path)\n        object_sizes = np.array([1840, 1870, 520]) / 10000\n        class_name = '035_power_drill'\n        super(SinglePowerDrillPIX2POSE6D, self).__init__(\n            model, object_sizes, camera, epsilon, resize, class_name, draw)",
  "class MultiPowerDrillPIX2POSE6D(MultiInstancePIX2POSE6D):\n    \"\"\"Predicts poses6D of multiple instances the YCB 035_power_drill object\n        from an image.\n\n    # Arguments\n        camera: PAZ Camera with intrinsic matrix.\n        offsets: List of length two containing floats e.g. (x_scale, y_scale)\n        epsilon: Float. Values below this value would be replaced by 0.\n        resize: Boolean. If True RGB mask is resized before computing PnP.\n        draw: Boolean. If True drawing functions are applied to output image.\n\n    # Returns\n        Dictionary with inferred boxes2D, poses6D and image.\n    \"\"\"\n    def __init__(self, camera, offsets, epsilon=0.15, resize=False, draw=True):\n        estimate_pose = SinglePowerDrillPIX2POSE6D(\n            camera, epsilon, resize, draw=False)\n        super(MultiPowerDrillPIX2POSE6D, self).__init__(\n            estimate_pose, offsets, camera, draw)",
  "class PIX2POSEPowerDrill(Processor):\n    \"\"\"PIX2POSE inference pipeline with SSD300 trained on FAT and UNET-VGG16\n        trained with domain randomization for the YCB object 035_power_drill.\n\n    # Arguments\n        score_thresh: Float between [0, 1] for object detector.\n        nms_thresh: Float between [0, 1] indicating the non-maximum supression.\n        offsets: List of length two containing floats e.g. (x_scale, y_scale)\n        epsilon: Float. Values below this value would be replaced by 0.\n        draw: Boolean. If ``True`` prediction are drawn in the returned image.\n\n    # Returns\n        Dictionary with inferred boxes2D, poses6D and image.\n    \"\"\"\n    def __init__(self, camera, score_thresh=0.50, nms_thresh=0.45,\n                 offsets=[0.5, 0.5], epsilon=0.15, resize=False, draw=True):\n        self.detect = SSD300FAT(score_thresh, nms_thresh, draw=False)\n        self.estimate_pose = MultiPowerDrillPIX2POSE6D(\n            camera, offsets, epsilon, resize, draw)\n\n    def call(self, image):\n        return self.estimate_pose(image, self.detect(image)['boxes2D'])",
  "class MultiInstanceMultiClassPIX2POSE6D(Processor):\n    \"\"\"Predicts poses6D of multiple instances of multiple objects from an image\n\n    # Arguments\n        detect: Function that takes as input an image and outputs a dictionary\n            containing Boxes2D messages.\n        name_to_model: Dictionary with class name as key and as value a\n            Keras segmentation model.\n        name_to_size: Dictionary with class name as key and as value the\n            object sizes.\n        camera: PAZ Camera with intrinsic matrix.\n        offsets: List of length two containing floats e.g. (x_scale, y_scale)\n        epsilon: Float. Values below this value would be replaced by 0.\n        resize: Boolean. If True RGB mask is resized before computing PnP.\n        draw: Boolean. If True drawing functions are applied to output image.\n\n    # Returns\n        Dictionary with inferred boxes2D, poses6D and image.\n    \"\"\"\n    def __init__(self, detect, name_to_model, name_to_size, camera, offsets,\n                 epsilon=0.15, resize=False, draw=True):\n        super(MultiInstanceMultiClassPIX2POSE6D, self).__init__()\n        if set(name_to_model.keys()) != set(name_to_size.keys()):\n            raise ValueError('models and sizes must have same class names')\n        self.detect = detect\n        self.name_to_pix2points = self._build_pix2points(\n            name_to_model, name_to_size, epsilon, resize)\n        valid_names = list(self.name_to_model.keys())\n        self.postprocess_boxes = PostprocessBoxes2D(offsets, valid_names)\n        self.draw_boxes2D = pr.DrawBoxes2D(valid_names)\n        self.draw_RGBmask = self._build_draw_RGBmask(name_to_size)\n        self.draw_pose6D = self._build_draw_pose6D(name_to_size, camera)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D', 'points3D', 'poses6D'])\n        self.solvePnP = pr.SolveChangingObjectPnPRANSAC(camera.intrinsics)\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.draw = draw\n\n    def _build_pix2points(self, name_to_model, name_to_size, epsilon, resize):\n        name_to_pix2points = {}\n        print(name_to_model)\n        for name, model in name_to_model.items():\n            pix2points = Pix2Points(model, name_to_size[name], epsilon, resize)\n            name_to_pix2points[name] = pix2points\n        return name_to_pix2points\n\n    def _build_draw_pose6D(self, name_to_size, camera):\n        name_to_draw = {}\n        for name, object_sizes in name_to_size.items():\n            draw = pr.DrawPose6D(object_sizes, camera.intrinsics)\n            name_to_draw[name] = draw\n        return name_to_draw\n\n    def _build_draw_RGBmask(self, name_to_size):\n        name_to_draw = {}\n        for name, object_sizes in name_to_size.items():\n            draw = pr.DrawRGBMask(object_sizes)\n            name_to_draw[name] = draw\n        return name_to_draw\n\n    def estimate_pose(self, image, box2D):\n        inferences = self.name_to_pix2points[box2D.class_name](image)\n        points2D = inferences['points2D']\n        points3D = inferences['points3D']\n        points2D = denormalize_keypoints2D(points2D, *image.shape[:2])\n        points2D = translate_points2D_origin(points2D, box2D.coordinates)\n        pose6D = None\n        if len(points3D) > self.solvePnP.MIN_REQUIRED_POINTS:\n            success, R, T = self.solvePnP(points3D, points2D)\n            if success:\n                pose6D = Pose6D.from_rotation_vector(R, T, box2D.class_name)\n        return points2D, points3D, pose6D\n\n    def call(self, image):\n        boxes2D = self.detect(image)['boxes2D']\n        boxes2D = self.postprocess_boxes(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        points2D, points3D, poses6D = [], [], []\n        for crop, box2D in zip(cropped_images, boxes2D):\n            inferences = self.estimate_pose(crop, box2D)\n            append_lists(inferences, [points2D, points3D, poses6D])\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n            for box2D, pose6D in zip(boxes2D, poses6D):\n                name = box2D.class_name\n                image = self.draw_pose6D[name](image, pose6D)\n            for box2D, p2D, p3D in zip(boxes2D, points2D, points3D):\n                image = self.draw_RGBmask[name](image, p2D, p3D)\n        return self.wrap(image, boxes2D, points3D, poses6D)",
  "class PIX2YCBTools6D(MultiInstanceMultiClassPIX2POSE6D):\n    \"\"\"Predicts poses6D of multiple instances of the YCB tools:\n        '035_power_drill', '051_large_clamp', '037_scissors'\n\n    # Arguments\n        camera: PAZ Camera with intrinsic matrix.\n        score_thresh: Float between [0, 1] for filtering Boxes2D.\n        nsm_thresh: Float between [0, 1] non-maximum-supression filtering.\n        offsets: List of length two containing floats e.g. (x_scale, y_scale)\n        epsilon: Float. Values below this value would be replaced by 0.\n        resize: Boolean. If True RGB mask is resized before computing PnP.\n        draw: Boolean. If True drawing functions are applied to output image.\n\n    # Returns\n        Dictionary with inferred boxes2D, poses6D and image.\n    \"\"\"\n    def __init__(self, camera, score_thresh=0.45, nms_thresh=0.15,\n                 offsets=[0.25, 0.25], epsilon=0.15, resize=False, draw=True):\n\n        self.detect = SSD300FAT(score_thresh, nms_thresh, draw=False)\n        self.name_to_sizes = self._build_name_to_sizes()\n        self.name_to_model = self._build_name_to_model()\n        super(PIX2YCBTools6D, self).__init__(\n            self.detect, self.name_to_model, self.name_to_sizes, camera,\n            offsets, epsilon, resize, draw)\n\n    def _build_name_to_model(self):\n        URL = ('https://github.com/oarriaga/altamira-data/'\n               'releases/download/v0.13/')\n\n        UNET_power_drill = UNET_VGG16(3, (128, 128, 3))\n        name = 'UNET-VGG16_POWERDRILL_weights.hdf5'\n        weights_path = get_file(name, URL + name, cache_subdir='paz/models')\n        UNET_power_drill.load_weights(weights_path)\n\n        UNET_large_clamp = UNET_VGG16(3, (128, 128, 3))\n        name = 'UNET-VGG16_LARGE-CLAMP_weights.hdf5'\n        weights_path = get_file(name, URL + name, cache_subdir='paz/models')\n        UNET_large_clamp.load_weights(weights_path)\n\n        UNET_scissors = UNET_VGG16(3, (128, 128, 3))\n        name = 'UNET-VGG16_SCISSORS_weights.hdf5'\n        weights_path = get_file(name, URL + name, cache_subdir='paz/models')\n        UNET_scissors.load_weights(weights_path)\n\n        name_to_model = {'035_power_drill': UNET_power_drill,\n                         '051_large_clamp': UNET_large_clamp,\n                         '037_scissors': UNET_scissors\n                         }\n        return name_to_model\n\n    def _build_name_to_sizes(self):\n        name_to_sizes = {\n            '035_power_drill': np.array([1840, 1874, 572]) / 10000,\n            '051_large_clamp': np.array([2022, 1652, 362]) / 10000,\n            '037_scissors': np.array([960, 2014, 156]) / 10000\n        }\n        return name_to_sizes",
  "def __init__(self, detect, estimate_keypoints, camera, offsets,\n                 model_points, class_to_dimensions, radius=3, thickness=1):\n        \"\"\"Pose estimation pipeline using keypoints.\n\n        # Arguments\n            detect: Function that outputs a dictionary with a key\n                ``boxes2D`` having a list of ``Box2D`` messages.\n            estimate_keypoints: Function that outputs a dictionary\n                with a key ``keypoints`` with numpy array as value\n            camera: Instance of ``paz.backend.camera.Camera`` with\n                camera intrinsics.\n            offsets: List of floats indicating the scaled offset to\n                be added to the ``Box2D`` coordinates.\n            model_points: Numpy array of shape ``(num_keypoints, 3)``\n                indicating the 3D coordinates of the predicted keypoints\n                from the ``esimate_keypoints`` function.\n            class_to_dimensions: Dictionary with keys being the class labels\n                of the predicted ``Box2D`` messages and the values a list of\n                three integers indicating the width, height and depth of the\n                object e.g. {'PowerDrill': [30, 20, 10]}.\n            radius: Int. radius of keypoint to be drawn.\n            thickness: Int. thickness of 3D box.\n\n        # Returns\n            A function that takes an RGB image and outputs the following\n            inferences as keys of a dictionary:\n                ``image``, ``boxes2D``, ``keypoints`` and ``poses6D``.\n\n        \"\"\"\n        super(EstimatePoseKeypoints, self).__init__()\n        self.num_keypoints = estimate_keypoints.num_keypoints\n        self.detect = detect\n        self.estimate_keypoints = estimate_keypoints\n        self.square = SequentialProcessor()\n        self.square.add(pr.SquareBoxes2D())\n        self.square.add(pr.OffsetBoxes2D(offsets))\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.change_coordinates = pr.ChangeKeypointsCoordinateSystem()\n        self.solve_PNP = pr.SolvePNP(model_points, camera)\n        self.draw_keypoints = pr.DrawKeypoints2D(self.num_keypoints, radius)\n        self.draw_box = pr.DrawBoxes3D(camera, class_to_dimensions,\n                                       thickness=thickness)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D', 'keypoints', 'poses6D'])",
  "def call(self, image):\n        boxes2D = self.detect(image)['boxes2D']\n        boxes2D = self.square(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        poses6D, keypoints2D = [], []\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            keypoints = self.estimate_keypoints(cropped_image)['keypoints']\n            keypoints = self.change_coordinates(keypoints, box2D)\n            pose6D = self.solve_PNP(keypoints)\n            image = self.draw_keypoints(image, keypoints)\n            image = self.draw_box(image, pose6D)\n            keypoints2D.append(keypoints)\n            poses6D.append(pose6D)\n        return self.wrap(image, boxes2D, keypoints2D, poses6D)",
  "def __init__(self, camera, offsets=[0, 0], radius=5, thickness=2):\n        detect = HaarCascadeFrontalFace(draw=False)\n        estimate_keypoints = FaceKeypointNet2D32(draw=False)\n        \"\"\"\n                               4--------1\n                              /|       /|\n                             / |      / |\n                            3--------2  |\n                            |  8_____|__5\n                            | /      | /\n                            |/       |/\n                            7--------6\n\n                   Z (depth)\n                  /\n                 /_____X (width)\n                 |\n                 |\n                 Y (height)\n        \"\"\"\n        KEYPOINTS3D = np.array([\n            [-220, 1138, 678],  # left--center-eye\n            [+220, 1138, 678],  # right-center-eye\n            [-131, 1107, 676],  # left--eye close to nose\n            [-294, 1123, 610],  # left--eye close to ear\n            [+131, 1107, 676],  # right-eye close to nose\n            [+294, 1123, 610],  # right-eye close to ear\n            [-106, 1224, 758],  # left--eyebrow close to nose\n            [-375, 1208, 585],  # left--eyebrow close to ear\n            [+106, 1224, 758],  # right-eyebrow close to nose\n            [+375, 1208, 585],  # right-eyebrow close to ear\n            [0.0, 919, 909],  # nose\n            [-183, 683, 691],  # lefty-lip\n            [+183, 683, 691],  # right-lip\n            [0.0, 754, 826],  # up---lip\n            [0.0, 645, 815],  # down-lip\n        ])\n        KEYPOINTS3D = KEYPOINTS3D - np.mean(KEYPOINTS3D, axis=0)\n        super(HeadPoseKeypointNet2D32, self).__init__(\n            detect, estimate_keypoints, camera, offsets,\n            KEYPOINTS3D, {None: [900, 1200, 800]}, radius, thickness)",
  "def __init__(self, model, object_sizes, camera,\n                 epsilon=0.15, resize=False, class_name=None, draw=True):\n        super(SingleInstancePIX2POSE6D, self).__init__()\n        self.camera = camera\n        self.pix2points = Pix2Points(model, object_sizes, epsilon, resize)\n        self.solvePnP = pr.SolveChangingObjectPnPRANSAC(self.camera.intrinsics)\n        self.draw_pose6D = pr.DrawPose6D(object_sizes, self.camera.intrinsics)\n        self.wrap = pr.WrapOutput(['image', 'points2D', 'points3D', 'pose6D'])\n        self.class_name = str(class_name)\n        self.object_sizes = object_sizes\n        self.draw = draw",
  "def call(self, image, box2D=None):\n        inferences = self.pix2points(image)\n        points2D = inferences['points2D']\n        points3D = inferences['points3D']\n        points2D = denormalize_keypoints2D(points2D, *image.shape[:2])\n        if box2D is not None:\n            points2D = translate_points2D_origin(points2D, box2D.coordinates)\n            self.class_name = box2D.class_name\n        pose6D = None\n        if len(points3D) > self.solvePnP.MIN_REQUIRED_POINTS:\n            success, R, T = self.solvePnP(points3D, points2D)\n            if success:\n                pose6D = Pose6D.from_rotation_vector(R, T, self.class_name)\n        if (self.draw and (box2D is None) and (pose6D is not None)):\n            colors = points3D_to_RGB(points3D, self.object_sizes)\n            image = draw_points2D(image, points2D, colors)\n            image = self.draw_pose6D(image, pose6D)\n        inferences = self.wrap(image, points2D, points3D, pose6D)\n        return inferences",
  "def __init__(self, estimate_pose, offsets, camera=None, draw=True):\n        super(MultiInstancePIX2POSE6D, self).__init__()\n        self.draw = draw\n        self.estimate_pose = estimate_pose\n        self.object_sizes = self.estimate_pose.object_sizes\n        self.camera = self.estimate_pose.camera if camera is None else camera\n        valid_names = [self.estimate_pose.class_name]\n        self.postprocess_boxes = PostprocessBoxes2D(offsets, valid_names)\n\n        self.append_values = pr.AppendValues(\n            ['pose6D', 'points2D', 'points3D'])\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.draw_RGBmask = pr.DrawRGBMasks(self.object_sizes)\n        self.draw_boxes2D = pr.DrawBoxes2D(valid_names, colors=[[0, 255, 0]])\n        self.draw_poses6D = pr.DrawPoses6D(\n            self.object_sizes, camera.intrinsics)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D', 'poses6D'])",
  "def call(self, image, boxes2D):\n        boxes2D = self.postprocess_boxes(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        poses6D, points2D, points3D = [], [], []\n        for crop, box2D in zip(cropped_images, boxes2D):\n            inferences = self.estimate_pose(crop, box2D)\n            self.append_values(inferences, [poses6D, points2D, points3D])\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n            image = self.draw_RGBmask(image, points2D, points3D)\n            image = self.draw_poses6D(image, poses6D)\n        return self.wrap(image, boxes2D, poses6D)",
  "def __init__(self, camera, epsilon=0.15, resize=False, draw=True):\n        model = UNET_VGG16(3, (128, 128, 3))\n        URL = ('https://github.com/oarriaga/altamira-data/'\n               'releases/download/v0.13/')\n        name = 'UNET-VGG16_POWERDRILL_weights.hdf5'\n        weights_path = get_file(name, URL + name, cache_subdir='paz/models')\n        print('Loading %s model weights' % weights_path)\n        model.load_weights(weights_path)\n        object_sizes = np.array([1840, 1870, 520]) / 10000\n        class_name = '035_power_drill'\n        super(SinglePowerDrillPIX2POSE6D, self).__init__(\n            model, object_sizes, camera, epsilon, resize, class_name, draw)",
  "def __init__(self, camera, offsets, epsilon=0.15, resize=False, draw=True):\n        estimate_pose = SinglePowerDrillPIX2POSE6D(\n            camera, epsilon, resize, draw=False)\n        super(MultiPowerDrillPIX2POSE6D, self).__init__(\n            estimate_pose, offsets, camera, draw)",
  "def __init__(self, camera, score_thresh=0.50, nms_thresh=0.45,\n                 offsets=[0.5, 0.5], epsilon=0.15, resize=False, draw=True):\n        self.detect = SSD300FAT(score_thresh, nms_thresh, draw=False)\n        self.estimate_pose = MultiPowerDrillPIX2POSE6D(\n            camera, offsets, epsilon, resize, draw)",
  "def call(self, image):\n        return self.estimate_pose(image, self.detect(image)['boxes2D'])",
  "def __init__(self, detect, name_to_model, name_to_size, camera, offsets,\n                 epsilon=0.15, resize=False, draw=True):\n        super(MultiInstanceMultiClassPIX2POSE6D, self).__init__()\n        if set(name_to_model.keys()) != set(name_to_size.keys()):\n            raise ValueError('models and sizes must have same class names')\n        self.detect = detect\n        self.name_to_pix2points = self._build_pix2points(\n            name_to_model, name_to_size, epsilon, resize)\n        valid_names = list(self.name_to_model.keys())\n        self.postprocess_boxes = PostprocessBoxes2D(offsets, valid_names)\n        self.draw_boxes2D = pr.DrawBoxes2D(valid_names)\n        self.draw_RGBmask = self._build_draw_RGBmask(name_to_size)\n        self.draw_pose6D = self._build_draw_pose6D(name_to_size, camera)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D', 'points3D', 'poses6D'])\n        self.solvePnP = pr.SolveChangingObjectPnPRANSAC(camera.intrinsics)\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.draw = draw",
  "def _build_pix2points(self, name_to_model, name_to_size, epsilon, resize):\n        name_to_pix2points = {}\n        print(name_to_model)\n        for name, model in name_to_model.items():\n            pix2points = Pix2Points(model, name_to_size[name], epsilon, resize)\n            name_to_pix2points[name] = pix2points\n        return name_to_pix2points",
  "def _build_draw_pose6D(self, name_to_size, camera):\n        name_to_draw = {}\n        for name, object_sizes in name_to_size.items():\n            draw = pr.DrawPose6D(object_sizes, camera.intrinsics)\n            name_to_draw[name] = draw\n        return name_to_draw",
  "def _build_draw_RGBmask(self, name_to_size):\n        name_to_draw = {}\n        for name, object_sizes in name_to_size.items():\n            draw = pr.DrawRGBMask(object_sizes)\n            name_to_draw[name] = draw\n        return name_to_draw",
  "def estimate_pose(self, image, box2D):\n        inferences = self.name_to_pix2points[box2D.class_name](image)\n        points2D = inferences['points2D']\n        points3D = inferences['points3D']\n        points2D = denormalize_keypoints2D(points2D, *image.shape[:2])\n        points2D = translate_points2D_origin(points2D, box2D.coordinates)\n        pose6D = None\n        if len(points3D) > self.solvePnP.MIN_REQUIRED_POINTS:\n            success, R, T = self.solvePnP(points3D, points2D)\n            if success:\n                pose6D = Pose6D.from_rotation_vector(R, T, box2D.class_name)\n        return points2D, points3D, pose6D",
  "def call(self, image):\n        boxes2D = self.detect(image)['boxes2D']\n        boxes2D = self.postprocess_boxes(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        points2D, points3D, poses6D = [], [], []\n        for crop, box2D in zip(cropped_images, boxes2D):\n            inferences = self.estimate_pose(crop, box2D)\n            append_lists(inferences, [points2D, points3D, poses6D])\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n            for box2D, pose6D in zip(boxes2D, poses6D):\n                name = box2D.class_name\n                image = self.draw_pose6D[name](image, pose6D)\n            for box2D, p2D, p3D in zip(boxes2D, points2D, points3D):\n                image = self.draw_RGBmask[name](image, p2D, p3D)\n        return self.wrap(image, boxes2D, points3D, poses6D)",
  "def __init__(self, camera, score_thresh=0.45, nms_thresh=0.15,\n                 offsets=[0.25, 0.25], epsilon=0.15, resize=False, draw=True):\n\n        self.detect = SSD300FAT(score_thresh, nms_thresh, draw=False)\n        self.name_to_sizes = self._build_name_to_sizes()\n        self.name_to_model = self._build_name_to_model()\n        super(PIX2YCBTools6D, self).__init__(\n            self.detect, self.name_to_model, self.name_to_sizes, camera,\n            offsets, epsilon, resize, draw)",
  "def _build_name_to_model(self):\n        URL = ('https://github.com/oarriaga/altamira-data/'\n               'releases/download/v0.13/')\n\n        UNET_power_drill = UNET_VGG16(3, (128, 128, 3))\n        name = 'UNET-VGG16_POWERDRILL_weights.hdf5'\n        weights_path = get_file(name, URL + name, cache_subdir='paz/models')\n        UNET_power_drill.load_weights(weights_path)\n\n        UNET_large_clamp = UNET_VGG16(3, (128, 128, 3))\n        name = 'UNET-VGG16_LARGE-CLAMP_weights.hdf5'\n        weights_path = get_file(name, URL + name, cache_subdir='paz/models')\n        UNET_large_clamp.load_weights(weights_path)\n\n        UNET_scissors = UNET_VGG16(3, (128, 128, 3))\n        name = 'UNET-VGG16_SCISSORS_weights.hdf5'\n        weights_path = get_file(name, URL + name, cache_subdir='paz/models')\n        UNET_scissors.load_weights(weights_path)\n\n        name_to_model = {'035_power_drill': UNET_power_drill,\n                         '051_large_clamp': UNET_large_clamp,\n                         '037_scissors': UNET_scissors\n                         }\n        return name_to_model",
  "def _build_name_to_sizes(self):\n        name_to_sizes = {\n            '035_power_drill': np.array([1840, 1874, 572]) / 10000,\n            '051_large_clamp': np.array([2022, 1652, 362]) / 10000,\n            '037_scissors': np.array([960, 2014, 156]) / 10000\n        }\n        return name_to_sizes",
  "class AugmentImage(SequentialProcessor):\n    \"\"\"Augments an RGB image by randomly changing contrast, brightness\n        saturation and hue.\n    \"\"\"\n    def __init__(self):\n        super(AugmentImage, self).__init__()\n        self.add(pr.RandomContrast())\n        self.add(pr.RandomBrightness())\n        self.add(pr.RandomSaturation(0.7))\n        self.add(pr.RandomHue())",
  "class PreprocessImage(SequentialProcessor):\n    \"\"\"Preprocess RGB image by resizing it to the given ``shape``. If a\n    ``mean`` is given it is substracted from image and it not the image gets\n    normalized.\n\n    # Arguments\n        shape: List of two Ints.\n        mean: List of three Ints indicating the per-channel mean to be\n            subtracted.\n    \"\"\"\n    def __init__(self, shape, mean=pr.BGR_IMAGENET_MEAN):\n        super(PreprocessImage, self).__init__()\n        self.add(pr.ResizeImage(shape))\n        self.add(pr.CastImage(float))\n        if mean is None:\n            self.add(pr.NormalizeImage())\n        else:\n            self.add(pr.SubtractMeanImage(mean))",
  "class AutoEncoderPredictor(SequentialProcessor):\n    \"\"\"Pipeline for predicting values from an auto-encoder.\n\n    # Arguments\n        model: Keras model.\n    \"\"\"\n    def __init__(self, model):\n        super(AutoEncoderPredictor, self).__init__()\n        preprocess = SequentialProcessor(\n            [pr.ResizeImage(model.input_shape[1:3]),\n             pr.ConvertColorSpace(pr.RGB2BGR),\n             pr.NormalizeImage(),\n             pr.ExpandDims(0)])\n        self.add(pr.Predict(model, preprocess))\n        self.add(pr.Squeeze(0))\n        self.add(pr.DenormalizeImage())\n        self.add(pr.CastImage('uint8'))\n        self.add(pr.WrapOutput(['image']))",
  "class EncoderPredictor(SequentialProcessor):\n    \"\"\"Pipeline for predicting latent vector of an encoder.\n\n    # Arguments\n        model: Keras model.\n    \"\"\"\n    def __init__(self, encoder):\n        super(EncoderPredictor, self).__init__()\n        self.encoder = encoder\n        preprocess = SequentialProcessor([\n            pr.ConvertColorSpace(pr.RGB2BGR),\n            pr.ResizeImage(encoder.input_shape[1:3]),\n            pr.NormalizeImage(),\n            pr.ExpandDims(0)])\n        self.add(pr.Predict(encoder, preprocess, pr.Squeeze(0)))",
  "class DecoderPredictor(SequentialProcessor):\n    \"\"\"Pipeline for predicting decoded image from a latent vector.\n\n    # Arguments\n        model: Keras model.\n    \"\"\"\n    def __init__(self, decoder):\n        self.decoder = decoder\n        super(DecoderPredictor, self).__init__()\n        self.add(pr.Predict(decoder, pr.ExpandDims(0), pr.Squeeze(0)))\n        self.add(pr.DenormalizeImage())\n        self.add(pr.CastImage('uint8'))\n        self.add(pr.ConvertColorSpace(pr.BGR2RGB))",
  "class PreprocessImageHigherHRNet(pr.Processor):\n    \"\"\"Transform the image according to the HigherHRNet model requirement.\n    # Arguments\n        scaling_factor: Int. scale factor for image dimensions.\n        input_size: Int. resize the first dimension of image to input size.\n        inverse: Boolean. Reverse the affine transform input.\n        image: Numpy array. Input image\n\n    # Returns\n        image: resized and transformed image\n        center: center of the image\n        scale: scaled image dimensions\n    \"\"\"\n    def __init__(self, scaling_factor=200, input_size=512, multiple=64):\n        super(PreprocessImageHigherHRNet, self).__init__()\n        self.get_image_center = pr.GetImageCenter()\n        self.get_size = pr.GetTransformationSize(input_size, multiple)\n        self.get_scale = pr.GetTransformationScale(scaling_factor)\n        self.get_source_destination_point = pr.GetSourceDestinationPoints(\n            scaling_factor)\n        self.transform_image = pr.SequentialProcessor(\n            [pr.WarpAffine(), pr.ImagenetPreprocessInput(), pr.ExpandDims(0)])\n\n    def call(self, image):\n        center = self.get_image_center(image)\n        size = self.get_size(image)\n        scale = self.get_scale(image, size)\n        source_point, destination_point = self.get_source_destination_point(\n            center, scale, size)\n        transform = get_affine_transform(source_point, destination_point)\n        image = self.transform_image(image, transform, size)\n        return image, center, scale",
  "def __init__(self):\n        super(AugmentImage, self).__init__()\n        self.add(pr.RandomContrast())\n        self.add(pr.RandomBrightness())\n        self.add(pr.RandomSaturation(0.7))\n        self.add(pr.RandomHue())",
  "def __init__(self, shape, mean=pr.BGR_IMAGENET_MEAN):\n        super(PreprocessImage, self).__init__()\n        self.add(pr.ResizeImage(shape))\n        self.add(pr.CastImage(float))\n        if mean is None:\n            self.add(pr.NormalizeImage())\n        else:\n            self.add(pr.SubtractMeanImage(mean))",
  "def __init__(self, model):\n        super(AutoEncoderPredictor, self).__init__()\n        preprocess = SequentialProcessor(\n            [pr.ResizeImage(model.input_shape[1:3]),\n             pr.ConvertColorSpace(pr.RGB2BGR),\n             pr.NormalizeImage(),\n             pr.ExpandDims(0)])\n        self.add(pr.Predict(model, preprocess))\n        self.add(pr.Squeeze(0))\n        self.add(pr.DenormalizeImage())\n        self.add(pr.CastImage('uint8'))\n        self.add(pr.WrapOutput(['image']))",
  "def __init__(self, encoder):\n        super(EncoderPredictor, self).__init__()\n        self.encoder = encoder\n        preprocess = SequentialProcessor([\n            pr.ConvertColorSpace(pr.RGB2BGR),\n            pr.ResizeImage(encoder.input_shape[1:3]),\n            pr.NormalizeImage(),\n            pr.ExpandDims(0)])\n        self.add(pr.Predict(encoder, preprocess, pr.Squeeze(0)))",
  "def __init__(self, decoder):\n        self.decoder = decoder\n        super(DecoderPredictor, self).__init__()\n        self.add(pr.Predict(decoder, pr.ExpandDims(0), pr.Squeeze(0)))\n        self.add(pr.DenormalizeImage())\n        self.add(pr.CastImage('uint8'))\n        self.add(pr.ConvertColorSpace(pr.BGR2RGB))",
  "def __init__(self, scaling_factor=200, input_size=512, multiple=64):\n        super(PreprocessImageHigherHRNet, self).__init__()\n        self.get_image_center = pr.GetImageCenter()\n        self.get_size = pr.GetTransformationSize(input_size, multiple)\n        self.get_scale = pr.GetTransformationScale(scaling_factor)\n        self.get_source_destination_point = pr.GetSourceDestinationPoints(\n            scaling_factor)\n        self.transform_image = pr.SequentialProcessor(\n            [pr.WarpAffine(), pr.ImagenetPreprocessInput(), pr.ExpandDims(0)])",
  "def call(self, image):\n        center = self.get_image_center(image)\n        size = self.get_size(image)\n        scale = self.get_scale(image, size)\n        source_point, destination_point = self.get_source_destination_point(\n            center, scale, size)\n        transform = get_affine_transform(source_point, destination_point)\n        image = self.transform_image(image, transform, size)\n        return image, center, scale",
  "class PredictRGBMask(SequentialProcessor):\n    \"\"\"Predicts RGB mask from a segmentation model\n    # Arguments\n        model: Keras segmentation model.\n        epsilon: Float. Values below this value would be replaced by 0.\n    \"\"\"\n    def __init__(self, model, epsilon=0.15):\n        super(PredictRGBMask, self).__init__()\n        self.add(pr.ResizeImage(model.input_shape[1:3]))\n        self.add(pr.NormalizeImage())\n        self.add(pr.ExpandDims(0))\n        self.add(pr.Predict(model))\n        self.add(pr.Squeeze(0))\n        self.add(pr.ReplaceLowerThanThreshold(epsilon))\n        self.add(pr.DenormalizeImage())\n        self.add(pr.CastImage('uint8'))",
  "class RGBMaskToObjectPoints3D(SequentialProcessor):\n    \"\"\"Predicts 3D keypoints from an RGB mask.\n    # Arguments\n        object_sizes: Array (3) determining the (width, height, depth)\n    \"\"\"\n    def __init__(self, object_sizes):\n        super(RGBMaskToObjectPoints3D, self).__init__()\n        self.add(pr.GetNonZeroValues())\n        self.add(pr.ImageToNormalizedDeviceCoordinates())\n        self.add(pr.Scale(object_sizes / 2.0))",
  "class RGBMaskToImagePoints2D(SequentialProcessor):\n    \"\"\"Predicts 2D image keypoints from an RGB mask.\n    \"\"\"\n    def __init__(self):\n        super(RGBMaskToImagePoints2D, self).__init__()\n        self.add(pr.GetNonZeroArguments())\n        self.add(pr.ArgumentsToImageKeypoints2D())",
  "class Pix2Points(Processor):\n    \"\"\"Predicts RGB_mask and corresponding points2D and points3D.\n\n    # Arguments\n        model: Keras segmentation model.\n        object_sizes: Array (3) determining the (width, height, depth)\n        epsilon: Float. Values below this value would be replaced by 0.\n        resize: Boolean. If True RGB mask is resized to original shape.\n        method: Interpolation method to use if resize is True.\n\n    # Note\n        Compare with and without RGB interpolation.\n    \"\"\"\n    def __init__(self, model, object_sizes, epsilon=0.15,\n                 resize=False, method=BILINEAR):\n        self.model = model\n        self.resize = resize\n        self.method = method\n        self.object_sizes = object_sizes\n        self.predict_RGBMask = PredictRGBMask(model, epsilon)\n        self.mask_to_points3D = RGBMaskToObjectPoints3D(self.object_sizes)\n        self.mask_to_points2D = RGBMaskToImagePoints2D()\n        self.wrap = pr.WrapOutput(['points2D', 'points3D', 'RGB_mask'])\n\n    def call(self, image):\n        RGB_mask = self.predict_RGBMask(image)\n        if self.resize:\n            H, W, num_channels = image.shape\n            RGB_mask = resize_image(RGB_mask, (W, H), self.method)\n        else:\n            H, W = self.model.output_shape[1:3]\n        points3D = self.mask_to_points3D(RGB_mask)\n        points2D = self.mask_to_points2D(RGB_mask)\n        points2D = normalize_keypoints2D(points2D, H, W)\n        return self.wrap(points2D, points3D, RGB_mask)",
  "def __init__(self, model, epsilon=0.15):\n        super(PredictRGBMask, self).__init__()\n        self.add(pr.ResizeImage(model.input_shape[1:3]))\n        self.add(pr.NormalizeImage())\n        self.add(pr.ExpandDims(0))\n        self.add(pr.Predict(model))\n        self.add(pr.Squeeze(0))\n        self.add(pr.ReplaceLowerThanThreshold(epsilon))\n        self.add(pr.DenormalizeImage())\n        self.add(pr.CastImage('uint8'))",
  "def __init__(self, object_sizes):\n        super(RGBMaskToObjectPoints3D, self).__init__()\n        self.add(pr.GetNonZeroValues())\n        self.add(pr.ImageToNormalizedDeviceCoordinates())\n        self.add(pr.Scale(object_sizes / 2.0))",
  "def __init__(self):\n        super(RGBMaskToImagePoints2D, self).__init__()\n        self.add(pr.GetNonZeroArguments())\n        self.add(pr.ArgumentsToImageKeypoints2D())",
  "def __init__(self, model, object_sizes, epsilon=0.15,\n                 resize=False, method=BILINEAR):\n        self.model = model\n        self.resize = resize\n        self.method = method\n        self.object_sizes = object_sizes\n        self.predict_RGBMask = PredictRGBMask(model, epsilon)\n        self.mask_to_points3D = RGBMaskToObjectPoints3D(self.object_sizes)\n        self.mask_to_points2D = RGBMaskToImagePoints2D()\n        self.wrap = pr.WrapOutput(['points2D', 'points3D', 'RGB_mask'])",
  "def call(self, image):\n        RGB_mask = self.predict_RGBMask(image)\n        if self.resize:\n            H, W, num_channels = image.shape\n            RGB_mask = resize_image(RGB_mask, (W, H), self.method)\n        else:\n            H, W = self.model.output_shape[1:3]\n        points3D = self.mask_to_points3D(RGB_mask)\n        points2D = self.mask_to_points2D(RGB_mask)\n        points2D = normalize_keypoints2D(points2D, H, W)\n        return self.wrap(points2D, points3D, RGB_mask)",
  "class KeypointNetSharedAugmentation(SequentialProcessor):\n    \"\"\"Wraps ``RenderTwoViews`` as a sequential processor for using it directly\n        with a ``paz.GeneratingSequence``.\n\n    # Arguments\n        renderer: ``RenderTwoViews`` processor.\n        size: Image size.\n    \"\"\"\n    def __init__(self, renderer, size):\n        super(KeypointNetSharedAugmentation, self).__init__()\n        self.renderer = renderer\n        self.size = size\n        self.add(RenderTwoViews(self.renderer))\n        self.add(pr.SequenceWrapper(\n            {0: {'image_A': [size, size, 3]},\n             1: {'image_B': [size, size, 3]}},\n            {2: {'matrices': [4, 4 * 4]},\n             3: {'alpha_channels': [size, size, 2]}}))",
  "class KeypointNetInference(Processor):\n    \"\"\"Performs inference from a ``KeypointNetShared`` model.\n\n    # Arguments\n        model: Keras model for predicting keypoints.\n        num_keypoints: Int or None. If None ``num_keypoints`` is\n            tried to be inferred from ``model.output_shape``\n        radius: Int. used for drawing the predicted keypoints.\n    \"\"\"\n    def __init__(self, model, num_keypoints=None, radius=5):\n        super(KeypointNetInference, self).__init__()\n        self.num_keypoints, self.radius = num_keypoints, radius\n        if self.num_keypoints is None:\n            self.num_keypoints = model.output_shape[1]\n\n        preprocessing = SequentialProcessor()\n        preprocessing.add(pr.NormalizeImage())\n        preprocessing.add(pr.ExpandDims(axis=0))\n        self.predict_keypoints = SequentialProcessor()\n        self.predict_keypoints.add(pr.Predict(model, preprocessing))\n        self.predict_keypoints.add(pr.SelectElement(0))\n        self.predict_keypoints.add(pr.Squeeze(axis=0))\n        self.postprocess_keypoints = SequentialProcessor()\n        self.postprocess_keypoints.add(pr.DenormalizeKeypoints())\n        self.postprocess_keypoints.add(pr.RemoveKeypointsDepth())\n        self.draw = pr.DrawKeypoints2D(self.num_keypoints, self.radius, False)\n        self.wrap = pr.WrapOutput(['image', 'keypoints'])\n\n    def call(self, image):\n        keypoints = self.predict_keypoints(image)\n        keypoints = self.postprocess_keypoints(keypoints, image)\n        image = self.draw(image, keypoints)\n        return self.wrap(image, keypoints)",
  "class EstimateKeypoints2D(Processor):\n    \"\"\"Basic 2D keypoint prediction pipeline.\n\n    # Arguments\n        model: Keras model for predicting keypoints.\n        num_keypoints: Int or None. If None ``num_keypoints`` is\n            tried to be inferred from ``model.output_shape``\n        draw: Boolean indicating if inferences should be drawn.\n        radius: Int. used for drawing the predicted keypoints.\n    \"\"\"\n    def __init__(self, model, num_keypoints, draw=True, radius=3,\n                 color=pr.RGB2BGR):\n        self.model = model\n        self.num_keypoints = num_keypoints\n        self.draw, self.radius, self.color = draw, radius, color\n        self.preprocess = SequentialProcessor()\n        self.preprocess.add(pr.ResizeImage(self.model.input_shape[1:3]))\n        self.preprocess.add(pr.ConvertColorSpace(self.color))\n        self.preprocess.add(pr.NormalizeImage())\n        self.preprocess.add(pr.ExpandDims(0))\n        self.preprocess.add(pr.ExpandDims(-1))\n        self.predict = pr.Predict(model, self.preprocess, pr.Squeeze(0))\n        self.denormalize = pr.DenormalizeKeypoints()\n        self.draw = pr.DrawKeypoints2D(self.num_keypoints, self.radius, False)\n        self.wrap = pr.WrapOutput(['image', 'keypoints'])\n\n    def call(self, image):\n        keypoints = self.predict(image)\n        keypoints = self.denormalize(keypoints, image)\n        if self.draw:\n            image = self.draw(image, keypoints)\n        return self.wrap(image, keypoints)",
  "class FaceKeypointNet2D32(EstimateKeypoints2D):\n    \"\"\"KeypointNet2D model trained with Kaggle Facial Detection challenge.\n\n    # Arguments\n        draw: Boolean indicating if inferences should be drawn.\n        radius: Int. used for drawing the predicted keypoints.\n\n    # Example\n        ``` python\n        from paz.pipelines import FaceKeypointNet2D32\n\n        estimate_keypoints= FaceKeypointNet2D32()\n\n        # apply directly to an image (numpy-array)\n        inference = estimate_keypoints(image)\n        ```\n    # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``keypoints``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a numpy array representing the keypoints.\n    \"\"\"\n    def __init__(self, draw=True, radius=3):\n        model = KeypointNet2D((96, 96, 1), 15, 32, 0.1)\n        self.weights_URL = ('https://github.com/oarriaga/altamira-data/'\n                            'releases/download/v0.7/')\n        weights_path = self.get_weights_path(model)\n        model.load_weights(weights_path)\n        super(FaceKeypointNet2D32, self).__init__(\n            model, 15, draw, radius, pr.RGB2GRAY)\n\n    def get_weights_path(self, model):\n        model_name = '_'.join(['FaceKP', model.name, '32', '15'])\n        model_name = '%s_weights.hdf5' % model_name\n        URL = self.weights_URL + model_name\n        return get_file(model_name, URL, cache_subdir='paz/models')",
  "class GetKeypoints(Processor):\n    \"\"\"Extract out the top k keypoints heatmaps and group the keypoints with\n       their respective tags value. Adjust and refine the keypoint locations\n       by removing the margins.\n    # Arguments\n        max_num_instance: Int. Maximum number of instances to be detected.\n        keypoint_order: List of length 17 (number of keypoints).\n        heatmaps: Numpy array of shape (1, num_keypoints, H, W)\n        Tags: Numpy array of shape (1, num_keypoints, H, W, 2)\n\n    # Returns\n        grouped_keypoints: numpy array. keypoints grouped by tag\n        scores: int: score for the keypoint\n    \"\"\"\n    def __init__(self, max_num_instance, keypoint_order, detection_thresh=0.2,\n                 tag_thresh=1):\n        super(GetKeypoints, self).__init__()\n        self.group_keypoints = pr.SequentialProcessor(\n            [pr.TopKDetections(max_num_instance), pr.GroupKeypointsByTag(\n                keypoint_order, tag_thresh, detection_thresh)])\n        self.adjust_keypoints = pr.AdjustKeypointsLocations()\n        self.get_scores = pr.GetScores()\n        self.refine_keypoints = pr.RefineKeypointsLocations()\n\n    def call(self, heatmaps, tags, adjust=True, refine=True):\n        grouped_keypoints = self.group_keypoints(heatmaps, tags)\n        if adjust:\n            grouped_keypoints = self.adjust_keypoints(\n                heatmaps, grouped_keypoints)[0]\n        scores = self.get_scores(grouped_keypoints)\n        if refine:\n            grouped_keypoints = self.refine_keypoints(\n                heatmaps[0], tags[0], grouped_keypoints)\n        return grouped_keypoints, scores",
  "class TransformKeypoints(Processor):\n    \"\"\"Transform the keypoint coordinates.\n    # Arguments\n        grouped_keypoints: Numpy array. keypoints grouped by tag\n        center: Tuple. center of the imput image\n        scale: Float. scaled imput image dimension\n        shape: Tuple/List\n\n    # Returns\n        transformed_keypoints: keypoint location with respect to the\n                               input image\n    \"\"\"\n    def __init__(self, inverse=False):\n        super(TransformKeypoints, self).__init__()\n        self.inverse = inverse\n        self.get_source_destination_point = pr.GetSourceDestinationPoints(\n            scaling_factor=200)\n        self.transform_keypoints = pr.TransformKeypoints()\n\n    def call(self, grouped_keypoints, center, scale, shape):\n        source_point, destination_point = self.get_source_destination_point(\n            center, scale, shape)\n        if self.inverse:\n            source_point, destination_point = destination_point, source_point\n        transform = get_affine_transform(source_point, destination_point)\n        transformed_keypoints = self.transform_keypoints(grouped_keypoints,\n                                                         transform)\n        return transformed_keypoints",
  "class HigherHRNetHumanPose2D(Processor):\n    \"\"\"Estimate human pose 2D keypoints and draw a skeleton.\n\n    # Arguments\n        model: Weights trained on HigherHRNet model.\n        keypoint_order: List of length 17 (number of keypoints).\n            where the keypoints are listed order wise.\n        flipped_keypoint_order: List of length 17 (number of keypoints).\n            Flipped list of keypoint order.\n        dataset: String. Name of the dataset used for training the model.\n        data_with_center: Boolean. True is the model is trained using the\n            center.\n\n    # Returns\n        dictonary with the following keys:\n            image: contains the image with skeleton drawn on it.\n            keypoints: location of keypoints\n            score: score of detection\n    \"\"\"\n    def __init__(self, dataset='COCO', data_with_center=False,\n                 max_num_people=30, with_flip=True, draw=True):\n        super(HigherHRNetHumanPose2D, self).__init__()\n        keypoint_order = JOINT_CONFIG[dataset]\n        flipped_keypoint_order = FLIP_CONFIG[dataset]\n        self.with_flip = with_flip\n        self.draw = draw\n        self.model = HigherHRNet(weights=dataset)\n        self.transform_image = PreprocessImageHigherHRNet()\n        self.get_heatmaps_and_tags = pr.SequentialProcessor(\n            [GetHeatmapsAndTags(self.model, flipped_keypoint_order,\n             with_flip, data_with_center), pr.AggregateResults(with_flip)])\n        self.get_keypoints = GetKeypoints(max_num_people, keypoint_order)\n        self.transform_keypoints = TransformKeypoints(inverse=True)\n        self.draw_skeleton = pr.DrawHumanSkeleton(dataset, check_scores=True)\n        self.extract_keypoints_locations = pr.ExtractKeypointsLocations()\n        self.wrap = pr.WrapOutput(['image', 'keypoints', 'scores'])\n\n    def call(self, image):\n        resized_image, center, scale = self.transform_image(image)\n        heatmaps, tags = self.get_heatmaps_and_tags(resized_image)\n        keypoints, scores = self.get_keypoints(heatmaps, tags)\n        shape = [heatmaps.shape[3], heatmaps.shape[2]]\n        keypoints = self.transform_keypoints(keypoints, center, scale, shape)\n        if self.draw:\n            image = self.draw_skeleton(image, keypoints)\n        keypoints = self.extract_keypoints_locations(keypoints)\n        return self.wrap(image, keypoints, scores)",
  "class DetNetHandKeypoints(pr.Processor):\n    \"\"\"Estimate 2D and 3D keypoints from minimal hand and draw a skeleton.\n\n    # Arguments\n        shape: List/tuple. Input image shape for DetNet model.\n        draw: Boolean. Draw hand skeleton if true.\n        right_hand: Boolean. If 'True', detect keypoints for right hand, else\n                    detect keypoints for left hand.\n        input_image: Array\n\n    # Returns\n        image: contains the image with skeleton drawn on it.\n        keypoints2D: Array [num_joints, 2]. 2D location of keypoints.\n        keypoints3D: Array [num_joints, 3]. 3D location of keypoints.\n    \"\"\"\n    def __init__(self, shape=(128, 128), draw=True, right_hand=False):\n        super(DetNetHandKeypoints).__init__()\n        self.draw = draw\n        self.right_hand = right_hand\n        self.preprocess = pr.SequentialProcessor()\n        self.preprocess.add(pr.ResizeImage(shape))\n        self.preprocess.add(pr.ExpandDims(axis=0))\n        if self.right_hand:\n            self.preprocess.add(pr.FlipLeftRightImage())\n        self.predict = pr.Predict(model=DetNet(), preprocess=self.preprocess)\n        self.scale_keypoints = pr.ScaleKeypoints(scale=4, shape=shape)\n        self.draw_skeleton = pr.DrawHandSkeleton()\n        self.wrap = pr.WrapOutput(['image', 'keypoints3D', 'keypoints2D'])\n\n    def call(self, image):\n        keypoints3D, keypoints2D = self.predict(image)\n        keypoints3D = keypoints3D.numpy()\n        keypoints2D = keypoints2D.numpy()\n        if self.right_hand:\n            keypoints2D = flip_keypoints_left_right(keypoints2D)\n        keypoints2D = uv_to_vu(keypoints2D)\n        keypoints2D = self.scale_keypoints(keypoints2D, image)\n        if self.draw:\n            image = self.draw_skeleton(image, keypoints2D)\n        return self.wrap(image, keypoints3D, keypoints2D)",
  "class MinimalHandPoseEstimation(pr.Processor):\n    \"\"\"Estimate 2D and 3D keypoints from minimal hand and draw a skeleton.\n       Estimate absolute and relative joint angle for the minimal hand joints\n       using the 3D keypoint locations.\n\n    # Arguments\n        draw: Boolean. Draw hand skeleton if true.\n        right_hand: Boolean. If 'True', detect keypoints for right hand, else\n                    detect keypoints for left hand.\n\n    # Returns\n        image: contains the image with skeleton drawn on it.\n        keypoints2D: Array [num_joints, 2]. 2D location of keypoints.\n        keypoints3D: Array [num_joints, 3]. 3D location of keypoints.\n        absolute_angles: Array [num_joints, 4]. quaternion repesentation\n        relative_angles: Array [num_joints, 3]. axis-angle repesentation\n    \"\"\"\n    def __init__(self, draw=True, right_hand=False):\n        super(MinimalHandPoseEstimation, self).__init__()\n        self.keypoints_estimator = DetNetHandKeypoints(draw=draw,\n                                                       right_hand=right_hand)\n        self.angle_estimator = IKNetHandJointAngles(right_hand=right_hand)\n        self.wrap = pr.WrapOutput(['image', 'keypoints3D', 'keypoints2D',\n                                   'absolute_angles', 'relative_angles'])\n\n    def call(self, image):\n        keypoints = self.keypoints_estimator(image)\n        angles = self.angle_estimator(keypoints['keypoints3D'])\n        return self.wrap(keypoints['image'], keypoints['keypoints3D'],\n                         keypoints['keypoints2D'], angles['absolute_angles'],\n                         angles['relative_angles'])",
  "class DetectMinimalHand(pr.Processor):\n    def __init__(self, detect, estimate_keypoints, offsets=[0, 0], radius=3):\n        \"\"\"Minimal hand detection and keypoint estimator pipeline.\n\n        # Arguments\n            detect: Function for detecting objects. The output should be a\n                dictionary with key ``Boxes2D`` containing a list\n                of ``Boxes2D`` messages.\n            estimate_keypoints: Function for estimating keypoints. The output\n                should be a dictionary with key ``keypoints`` containing\n                a numpy array of keypoints.\n            offsets: List of two elements. Each element must be between [0, 1].\n            radius: Int indicating the radius of the keypoints to be drawn.\n        \"\"\"\n        super(DetectMinimalHand, self).__init__()\n        self.class_names = ['OPEN', 'CLOSE']\n        self.colors = lincolor(len(self.class_names))\n        self.detect = detect\n        self.estimate_keypoints = estimate_keypoints\n        self.classify_hand_closure = pr.SequentialProcessor(\n            [pr.IsHandOpen(), pr.BooleanToTextMessage('OPEN', 'CLOSE')])\n        self.square = pr.SequentialProcessor()\n        self.square.add(pr.SquareBoxes2D())\n        self.square.add(pr.OffsetBoxes2D(offsets))\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.change_coordinates = pr.ChangeKeypointsCoordinateSystem()\n        self.draw = pr.DrawHandSkeleton(keypoint_radius=radius)\n        self.draw_boxes = pr.DrawBoxes2D(self.class_names, self.colors,\n                                         with_score=False)\n        self.wrap = pr.WrapOutput(\n            ['image', 'boxes2D', 'keypoints2D', 'keypoints3D'])\n\n    def call(self, image):\n        boxes2D = self.detect(image.copy())['boxes2D']\n        boxes2D = self.square(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        keypoints2D = []\n        keypoints3D = []\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            inference = self.estimate_keypoints(cropped_image)\n            keypoints = self.change_coordinates(\n                inference['keypoints2D'], box2D)\n            hand_closure_status = self.classify_hand_closure(\n                inference['relative_angles'])\n            box2D.class_name = hand_closure_status\n            keypoints2D.append(keypoints)\n            keypoints3D.append(inference['keypoints3D'])\n            image = self.draw(image, keypoints)\n        image = self.draw_boxes(image, boxes2D)\n        return self.wrap(image, boxes2D, keypoints2D, keypoints3D)",
  "class EstimateHumanPose3D(Processor):\n    \"\"\" Estimate human pose 3D from 2D human pose.\n\n    # Arguments\n    input_shape: tuple\n    num_keypoints: Int. Number of keypoints.\n\n    # Return\n        keypoints3D: human pose 3D\n    \"\"\"\n    def __init__(self, input_shape=(32,), num_keypoints=16):\n        super(EstimateHumanPose3D, self).__init__()\n        self.model = SimpleBaseline(input_shape, num_keypoints)\n        self.preprocessing = SequentialProcessor(\n            [pr.MergeKeypoints2D(args_to_mean),\n             pr.FilterKeypoints2D(args_to_mean, h36m_to_coco_joints2D),\n             pr.StandardizeKeypoints2D(data_mean2D, data_stdev2D)])\n        self.postprocess = pr.DestandardizeKeypoints2D(\n            data_mean3D, data_stdev3D, dim_to_use3D)\n        self.predict = pr.Predict(self.model, self.preprocessing,\n                                  self.postprocess)\n\n    def call(self, keypoints2D):\n        keypoints3D = self.predict(keypoints2D)\n        return keypoints3D",
  "class EstimateHumanPose(pr.Processor):\n    \"\"\" Estimates 2D and 3D keypoints of human from an image\n\n    # Arguments\n        estimate_keypoints_3D: 3D simple baseline model\n        args_to_mean: keypoints indices\n        h36m_to_coco_joints2D: h36m joints indices\n\n    # Returns\n        keypoints2D, keypoints3D\n    \"\"\"\n    def __init__(self, solver, camera_intrinsics,\n                 args_to_joints3D=args_to_joints3D, filter=True, draw=True,\n                 draw_pose=True):\n        super(EstimateHumanPose, self).__init__()\n        self.pose3D = []\n        self.pose6D = []\n        self.draw = draw\n        self.filter = filter\n        self.draw_pose = draw_pose\n        self.estimate_keypoints_2D = HigherHRNetHumanPose2D(draw=draw)\n        self.estimate_keypoints_3D = EstimateHumanPose3D()\n        self.optimize = pr.OptimizeHumanPose3D(\n            args_to_joints3D, solver, camera_intrinsics)\n        self.draw_text = pr.DrawText(scale=0.5, thickness=1)\n        self.draw_pose6D = pr.DrawHumanPose6D(camera_intrinsics)\n        self.wrap = pr.WrapOutput(['image', 'keypoints2D', 'keypoints3D',\n                                   'pose6D'])\n\n    def call(self, image):\n        inferences2D = self.estimate_keypoints_2D(image)\n        keypoints2D = inferences2D['keypoints']\n        if self.draw:\n            image = inferences2D['image']\n        if len(keypoints2D) > 0:\n            keypoints3D = self.estimate_keypoints_3D(keypoints2D)\n            keypoints3D = np.reshape(keypoints3D, (-1, 32, 3))\n            optimized_output = self.optimize(keypoints3D, keypoints2D)\n            joints2D, joints3D, self.pose3D, projection2D = optimized_output\n            self.pose6D = human_pose3D_to_pose6D(self.pose3D[0])\n            if self.draw_pose:\n                rotation, translation = self.pose6D\n                image = self.draw_pose6D(image, rotation, translation)\n                translation = [\"%.2f\" % item for item in translation]\n                image = self.draw_text(image, str(translation), (30, 30))\n        return self.wrap(image, keypoints2D, self.pose3D, self.pose6D)",
  "def __init__(self, renderer, size):\n        super(KeypointNetSharedAugmentation, self).__init__()\n        self.renderer = renderer\n        self.size = size\n        self.add(RenderTwoViews(self.renderer))\n        self.add(pr.SequenceWrapper(\n            {0: {'image_A': [size, size, 3]},\n             1: {'image_B': [size, size, 3]}},\n            {2: {'matrices': [4, 4 * 4]},\n             3: {'alpha_channels': [size, size, 2]}}))",
  "def __init__(self, model, num_keypoints=None, radius=5):\n        super(KeypointNetInference, self).__init__()\n        self.num_keypoints, self.radius = num_keypoints, radius\n        if self.num_keypoints is None:\n            self.num_keypoints = model.output_shape[1]\n\n        preprocessing = SequentialProcessor()\n        preprocessing.add(pr.NormalizeImage())\n        preprocessing.add(pr.ExpandDims(axis=0))\n        self.predict_keypoints = SequentialProcessor()\n        self.predict_keypoints.add(pr.Predict(model, preprocessing))\n        self.predict_keypoints.add(pr.SelectElement(0))\n        self.predict_keypoints.add(pr.Squeeze(axis=0))\n        self.postprocess_keypoints = SequentialProcessor()\n        self.postprocess_keypoints.add(pr.DenormalizeKeypoints())\n        self.postprocess_keypoints.add(pr.RemoveKeypointsDepth())\n        self.draw = pr.DrawKeypoints2D(self.num_keypoints, self.radius, False)\n        self.wrap = pr.WrapOutput(['image', 'keypoints'])",
  "def call(self, image):\n        keypoints = self.predict_keypoints(image)\n        keypoints = self.postprocess_keypoints(keypoints, image)\n        image = self.draw(image, keypoints)\n        return self.wrap(image, keypoints)",
  "def __init__(self, model, num_keypoints, draw=True, radius=3,\n                 color=pr.RGB2BGR):\n        self.model = model\n        self.num_keypoints = num_keypoints\n        self.draw, self.radius, self.color = draw, radius, color\n        self.preprocess = SequentialProcessor()\n        self.preprocess.add(pr.ResizeImage(self.model.input_shape[1:3]))\n        self.preprocess.add(pr.ConvertColorSpace(self.color))\n        self.preprocess.add(pr.NormalizeImage())\n        self.preprocess.add(pr.ExpandDims(0))\n        self.preprocess.add(pr.ExpandDims(-1))\n        self.predict = pr.Predict(model, self.preprocess, pr.Squeeze(0))\n        self.denormalize = pr.DenormalizeKeypoints()\n        self.draw = pr.DrawKeypoints2D(self.num_keypoints, self.radius, False)\n        self.wrap = pr.WrapOutput(['image', 'keypoints'])",
  "def call(self, image):\n        keypoints = self.predict(image)\n        keypoints = self.denormalize(keypoints, image)\n        if self.draw:\n            image = self.draw(image, keypoints)\n        return self.wrap(image, keypoints)",
  "def __init__(self, draw=True, radius=3):\n        model = KeypointNet2D((96, 96, 1), 15, 32, 0.1)\n        self.weights_URL = ('https://github.com/oarriaga/altamira-data/'\n                            'releases/download/v0.7/')\n        weights_path = self.get_weights_path(model)\n        model.load_weights(weights_path)\n        super(FaceKeypointNet2D32, self).__init__(\n            model, 15, draw, radius, pr.RGB2GRAY)",
  "def get_weights_path(self, model):\n        model_name = '_'.join(['FaceKP', model.name, '32', '15'])\n        model_name = '%s_weights.hdf5' % model_name\n        URL = self.weights_URL + model_name\n        return get_file(model_name, URL, cache_subdir='paz/models')",
  "def __init__(self, max_num_instance, keypoint_order, detection_thresh=0.2,\n                 tag_thresh=1):\n        super(GetKeypoints, self).__init__()\n        self.group_keypoints = pr.SequentialProcessor(\n            [pr.TopKDetections(max_num_instance), pr.GroupKeypointsByTag(\n                keypoint_order, tag_thresh, detection_thresh)])\n        self.adjust_keypoints = pr.AdjustKeypointsLocations()\n        self.get_scores = pr.GetScores()\n        self.refine_keypoints = pr.RefineKeypointsLocations()",
  "def call(self, heatmaps, tags, adjust=True, refine=True):\n        grouped_keypoints = self.group_keypoints(heatmaps, tags)\n        if adjust:\n            grouped_keypoints = self.adjust_keypoints(\n                heatmaps, grouped_keypoints)[0]\n        scores = self.get_scores(grouped_keypoints)\n        if refine:\n            grouped_keypoints = self.refine_keypoints(\n                heatmaps[0], tags[0], grouped_keypoints)\n        return grouped_keypoints, scores",
  "def __init__(self, inverse=False):\n        super(TransformKeypoints, self).__init__()\n        self.inverse = inverse\n        self.get_source_destination_point = pr.GetSourceDestinationPoints(\n            scaling_factor=200)\n        self.transform_keypoints = pr.TransformKeypoints()",
  "def call(self, grouped_keypoints, center, scale, shape):\n        source_point, destination_point = self.get_source_destination_point(\n            center, scale, shape)\n        if self.inverse:\n            source_point, destination_point = destination_point, source_point\n        transform = get_affine_transform(source_point, destination_point)\n        transformed_keypoints = self.transform_keypoints(grouped_keypoints,\n                                                         transform)\n        return transformed_keypoints",
  "def __init__(self, dataset='COCO', data_with_center=False,\n                 max_num_people=30, with_flip=True, draw=True):\n        super(HigherHRNetHumanPose2D, self).__init__()\n        keypoint_order = JOINT_CONFIG[dataset]\n        flipped_keypoint_order = FLIP_CONFIG[dataset]\n        self.with_flip = with_flip\n        self.draw = draw\n        self.model = HigherHRNet(weights=dataset)\n        self.transform_image = PreprocessImageHigherHRNet()\n        self.get_heatmaps_and_tags = pr.SequentialProcessor(\n            [GetHeatmapsAndTags(self.model, flipped_keypoint_order,\n             with_flip, data_with_center), pr.AggregateResults(with_flip)])\n        self.get_keypoints = GetKeypoints(max_num_people, keypoint_order)\n        self.transform_keypoints = TransformKeypoints(inverse=True)\n        self.draw_skeleton = pr.DrawHumanSkeleton(dataset, check_scores=True)\n        self.extract_keypoints_locations = pr.ExtractKeypointsLocations()\n        self.wrap = pr.WrapOutput(['image', 'keypoints', 'scores'])",
  "def call(self, image):\n        resized_image, center, scale = self.transform_image(image)\n        heatmaps, tags = self.get_heatmaps_and_tags(resized_image)\n        keypoints, scores = self.get_keypoints(heatmaps, tags)\n        shape = [heatmaps.shape[3], heatmaps.shape[2]]\n        keypoints = self.transform_keypoints(keypoints, center, scale, shape)\n        if self.draw:\n            image = self.draw_skeleton(image, keypoints)\n        keypoints = self.extract_keypoints_locations(keypoints)\n        return self.wrap(image, keypoints, scores)",
  "def __init__(self, shape=(128, 128), draw=True, right_hand=False):\n        super(DetNetHandKeypoints).__init__()\n        self.draw = draw\n        self.right_hand = right_hand\n        self.preprocess = pr.SequentialProcessor()\n        self.preprocess.add(pr.ResizeImage(shape))\n        self.preprocess.add(pr.ExpandDims(axis=0))\n        if self.right_hand:\n            self.preprocess.add(pr.FlipLeftRightImage())\n        self.predict = pr.Predict(model=DetNet(), preprocess=self.preprocess)\n        self.scale_keypoints = pr.ScaleKeypoints(scale=4, shape=shape)\n        self.draw_skeleton = pr.DrawHandSkeleton()\n        self.wrap = pr.WrapOutput(['image', 'keypoints3D', 'keypoints2D'])",
  "def call(self, image):\n        keypoints3D, keypoints2D = self.predict(image)\n        keypoints3D = keypoints3D.numpy()\n        keypoints2D = keypoints2D.numpy()\n        if self.right_hand:\n            keypoints2D = flip_keypoints_left_right(keypoints2D)\n        keypoints2D = uv_to_vu(keypoints2D)\n        keypoints2D = self.scale_keypoints(keypoints2D, image)\n        if self.draw:\n            image = self.draw_skeleton(image, keypoints2D)\n        return self.wrap(image, keypoints3D, keypoints2D)",
  "def __init__(self, draw=True, right_hand=False):\n        super(MinimalHandPoseEstimation, self).__init__()\n        self.keypoints_estimator = DetNetHandKeypoints(draw=draw,\n                                                       right_hand=right_hand)\n        self.angle_estimator = IKNetHandJointAngles(right_hand=right_hand)\n        self.wrap = pr.WrapOutput(['image', 'keypoints3D', 'keypoints2D',\n                                   'absolute_angles', 'relative_angles'])",
  "def call(self, image):\n        keypoints = self.keypoints_estimator(image)\n        angles = self.angle_estimator(keypoints['keypoints3D'])\n        return self.wrap(keypoints['image'], keypoints['keypoints3D'],\n                         keypoints['keypoints2D'], angles['absolute_angles'],\n                         angles['relative_angles'])",
  "def __init__(self, detect, estimate_keypoints, offsets=[0, 0], radius=3):\n        \"\"\"Minimal hand detection and keypoint estimator pipeline.\n\n        # Arguments\n            detect: Function for detecting objects. The output should be a\n                dictionary with key ``Boxes2D`` containing a list\n                of ``Boxes2D`` messages.\n            estimate_keypoints: Function for estimating keypoints. The output\n                should be a dictionary with key ``keypoints`` containing\n                a numpy array of keypoints.\n            offsets: List of two elements. Each element must be between [0, 1].\n            radius: Int indicating the radius of the keypoints to be drawn.\n        \"\"\"\n        super(DetectMinimalHand, self).__init__()\n        self.class_names = ['OPEN', 'CLOSE']\n        self.colors = lincolor(len(self.class_names))\n        self.detect = detect\n        self.estimate_keypoints = estimate_keypoints\n        self.classify_hand_closure = pr.SequentialProcessor(\n            [pr.IsHandOpen(), pr.BooleanToTextMessage('OPEN', 'CLOSE')])\n        self.square = pr.SequentialProcessor()\n        self.square.add(pr.SquareBoxes2D())\n        self.square.add(pr.OffsetBoxes2D(offsets))\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.change_coordinates = pr.ChangeKeypointsCoordinateSystem()\n        self.draw = pr.DrawHandSkeleton(keypoint_radius=radius)\n        self.draw_boxes = pr.DrawBoxes2D(self.class_names, self.colors,\n                                         with_score=False)\n        self.wrap = pr.WrapOutput(\n            ['image', 'boxes2D', 'keypoints2D', 'keypoints3D'])",
  "def call(self, image):\n        boxes2D = self.detect(image.copy())['boxes2D']\n        boxes2D = self.square(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        keypoints2D = []\n        keypoints3D = []\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            inference = self.estimate_keypoints(cropped_image)\n            keypoints = self.change_coordinates(\n                inference['keypoints2D'], box2D)\n            hand_closure_status = self.classify_hand_closure(\n                inference['relative_angles'])\n            box2D.class_name = hand_closure_status\n            keypoints2D.append(keypoints)\n            keypoints3D.append(inference['keypoints3D'])\n            image = self.draw(image, keypoints)\n        image = self.draw_boxes(image, boxes2D)\n        return self.wrap(image, boxes2D, keypoints2D, keypoints3D)",
  "def __init__(self, input_shape=(32,), num_keypoints=16):\n        super(EstimateHumanPose3D, self).__init__()\n        self.model = SimpleBaseline(input_shape, num_keypoints)\n        self.preprocessing = SequentialProcessor(\n            [pr.MergeKeypoints2D(args_to_mean),\n             pr.FilterKeypoints2D(args_to_mean, h36m_to_coco_joints2D),\n             pr.StandardizeKeypoints2D(data_mean2D, data_stdev2D)])\n        self.postprocess = pr.DestandardizeKeypoints2D(\n            data_mean3D, data_stdev3D, dim_to_use3D)\n        self.predict = pr.Predict(self.model, self.preprocessing,\n                                  self.postprocess)",
  "def call(self, keypoints2D):\n        keypoints3D = self.predict(keypoints2D)\n        return keypoints3D",
  "def __init__(self, solver, camera_intrinsics,\n                 args_to_joints3D=args_to_joints3D, filter=True, draw=True,\n                 draw_pose=True):\n        super(EstimateHumanPose, self).__init__()\n        self.pose3D = []\n        self.pose6D = []\n        self.draw = draw\n        self.filter = filter\n        self.draw_pose = draw_pose\n        self.estimate_keypoints_2D = HigherHRNetHumanPose2D(draw=draw)\n        self.estimate_keypoints_3D = EstimateHumanPose3D()\n        self.optimize = pr.OptimizeHumanPose3D(\n            args_to_joints3D, solver, camera_intrinsics)\n        self.draw_text = pr.DrawText(scale=0.5, thickness=1)\n        self.draw_pose6D = pr.DrawHumanPose6D(camera_intrinsics)\n        self.wrap = pr.WrapOutput(['image', 'keypoints2D', 'keypoints3D',\n                                   'pose6D'])",
  "def call(self, image):\n        inferences2D = self.estimate_keypoints_2D(image)\n        keypoints2D = inferences2D['keypoints']\n        if self.draw:\n            image = inferences2D['image']\n        if len(keypoints2D) > 0:\n            keypoints3D = self.estimate_keypoints_3D(keypoints2D)\n            keypoints3D = np.reshape(keypoints3D, (-1, 32, 3))\n            optimized_output = self.optimize(keypoints3D, keypoints2D)\n            joints2D, joints3D, self.pose3D, projection2D = optimized_output\n            self.pose6D = human_pose3D_to_pose6D(self.pose3D[0])\n            if self.draw_pose:\n                rotation, translation = self.pose6D\n                image = self.draw_pose6D(image, rotation, translation)\n                translation = [\"%.2f\" % item for item in translation]\n                image = self.draw_text(image, str(translation), (30, 30))\n        return self.wrap(image, keypoints2D, self.pose3D, self.pose6D)",
  "class GetHeatmapsAndTags(pr.Processor):\n    \"\"\"Get Heatmaps and Tags from the model output.\n    # Arguments\n        model: Model weights trained on HigherHRNet model.\n        flipped_keypoint_order: List of length 17 (number of keypoints).\n            Flipped list of keypoint order.\n        data_with_center: Boolean. True is the model is trained using the\n            center.\n        image: Numpy array. Input image of shape (H, W)\n\n    # Returns\n        heatmaps: Numpy array of shape (1, num_keypoints, H, W)\n        Tags: Numpy array of shape (1, num_keypoints, H, W)\n    \"\"\"\n    def __init__(self, model, flipped_keypoint_order, with_flip,\n                 data_with_center, scale_output=True, axes=[0, 3, 1, 2]):\n        super(GetHeatmapsAndTags, self).__init__()\n        self.with_flip = with_flip\n        self.predict = pr.SequentialProcessor(\n            [pr.Predict(model), pr.TransposeOutput(axes), pr.ScaleOutput(2)])\n        self.get_heatmaps = pr.GetHeatmaps(flipped_keypoint_order)\n        self.get_tags = pr.GetTags(flipped_keypoint_order)\n        self.postprocess = pr.SequentialProcessor()\n        if data_with_center:\n            self.postprocess.add(pr.RemoveLastElement())\n        if scale_output:\n            self.postprocess.add(pr.ScaleOutput(2, full_scaling=True))\n\n    def call(self, image):\n        outputs = self.predict(image)\n        heatmaps = self.get_heatmaps(outputs, with_flip=False)\n        tags = self.get_tags(outputs, with_flip=False)\n        if self.with_flip:\n            outputs = self.predict(np.flip(image, [2]))\n            heatmaps_flip = self.get_heatmaps(outputs, self.with_flip)\n            tags_flip = self.get_tags(outputs, self.with_flip)\n            heatmaps = [heatmaps, heatmaps_flip]\n            tags = [tags, tags_flip]\n        heatmaps = self.postprocess(heatmaps)\n        tags = self.postprocess(tags)\n        return heatmaps, tags",
  "def __init__(self, model, flipped_keypoint_order, with_flip,\n                 data_with_center, scale_output=True, axes=[0, 3, 1, 2]):\n        super(GetHeatmapsAndTags, self).__init__()\n        self.with_flip = with_flip\n        self.predict = pr.SequentialProcessor(\n            [pr.Predict(model), pr.TransposeOutput(axes), pr.ScaleOutput(2)])\n        self.get_heatmaps = pr.GetHeatmaps(flipped_keypoint_order)\n        self.get_tags = pr.GetTags(flipped_keypoint_order)\n        self.postprocess = pr.SequentialProcessor()\n        if data_with_center:\n            self.postprocess.add(pr.RemoveLastElement())\n        if scale_output:\n            self.postprocess.add(pr.ScaleOutput(2, full_scaling=True))",
  "def call(self, image):\n        outputs = self.predict(image)\n        heatmaps = self.get_heatmaps(outputs, with_flip=False)\n        tags = self.get_tags(outputs, with_flip=False)\n        if self.with_flip:\n            outputs = self.predict(np.flip(image, [2]))\n            heatmaps_flip = self.get_heatmaps(outputs, self.with_flip)\n            tags_flip = self.get_tags(outputs, self.with_flip)\n            heatmaps = [heatmaps, heatmaps_flip]\n            tags = [tags, tags_flip]\n        heatmaps = self.postprocess(heatmaps)\n        tags = self.postprocess(tags)\n        return heatmaps, tags",
  "class AugmentBoxes(SequentialProcessor):\n    \"\"\"Perform data augmentation with bounding boxes.\n\n    # Arguments\n        mean: List of three elements used to fill empty image spaces.\n    \"\"\"\n    def __init__(self, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentBoxes, self).__init__()\n        self.add(pr.ToImageBoxCoordinates())\n        self.add(pr.Expand(mean=mean))\n        self.add(pr.RandomSampleCrop(1.0))\n        self.add(pr.RandomFlipBoxesLeftRight())\n        self.add(pr.ToNormalizedBoxCoordinates())",
  "class PreprocessBoxes(SequentialProcessor):\n    \"\"\"Preprocess bounding boxes\n\n    # Arguments\n        num_classes: Int.\n        prior_boxes: Numpy array of shape ``[num_boxes, 4]`` containing\n            prior/default bounding boxes.\n        IOU: Float. Intersection over union used to match boxes.\n        variances: List of two floats indicating variances to be encoded\n            for encoding bounding boxes.\n    \"\"\"\n    def __init__(self, num_classes, prior_boxes, IOU, variances):\n        super(PreprocessBoxes, self).__init__()\n        self.add(pr.MatchBoxes(prior_boxes, IOU),)\n        self.add(pr.EncodeBoxes(prior_boxes, variances))\n        self.add(pr.BoxClassToOneHotVector(num_classes))",
  "class AugmentDetection(SequentialProcessor):\n    \"\"\"Augment boxes and images for object detection.\n\n    # Arguments\n        prior_boxes: Numpy array of shape ``[num_boxes, 4]`` containing\n            prior/default bounding boxes.\n        split: Flag from `paz.processors.TRAIN`, ``paz.processors.VAL``\n            or ``paz.processors.TEST``. Certain transformations would take\n            place depending on the flag.\n        num_classes: Int.\n        size: Int. Image size.\n        mean: List of three elements indicating the per channel mean.\n        IOU: Float. Intersection over union used to match boxes.\n        variances: List of two floats indicating variances to be encoded\n            for encoding bounding boxes.\n    \"\"\"\n    def __init__(self, prior_boxes, split=pr.TRAIN, num_classes=21, size=300,\n                 mean=pr.BGR_IMAGENET_MEAN, IOU=.5,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(AugmentDetection, self).__init__()\n        # image processors\n        self.augment_image = AugmentImage()\n        # self.augment_image.add(pr.ConvertColorSpace(pr.RGB2BGR))\n        self.preprocess_image = PreprocessImage((size, size), mean)\n        self.preprocess_image.insert(0, pr.ConvertColorSpace(pr.RGB2BGR))\n\n        # box processors\n        self.augment_boxes = AugmentBoxes()\n        args = (num_classes, prior_boxes, IOU, variances)\n        self.preprocess_boxes = PreprocessBoxes(*args)\n\n        # pipeline\n        self.add(pr.UnpackDictionary(['image', 'boxes']))\n        self.add(pr.ControlMap(pr.LoadImage(), [0], [0]))\n        if split == pr.TRAIN:\n            self.add(pr.ControlMap(self.augment_image, [0], [0]))\n            self.add(pr.ControlMap(self.augment_boxes, [0, 1], [0, 1]))\n        self.add(pr.ControlMap(self.preprocess_image, [0], [0]))\n        self.add(pr.ControlMap(self.preprocess_boxes, [1], [1]))\n        self.add(pr.SequenceWrapper(\n            {0: {'image': [size, size, 3]}},\n            {1: {'boxes': [len(prior_boxes), 4 + num_classes]}}))",
  "class PostprocessBoxes2D(SequentialProcessor):\n    \"\"\"Filters, squares and offsets 2D bounding boxes\n\n    # Arguments\n        valid_names: List of strings containing class names to keep.\n        offsets: List of length two containing floats e.g. (x_scale, y_scale)\n    \"\"\"\n    def __init__(self, offsets, valid_names=None):\n        super(PostprocessBoxes2D, self).__init__()\n        if valid_names is not None:\n            self.add(pr.FilterClassBoxes2D(valid_names))\n        self.add(pr.SquareBoxes2D())\n        self.add(pr.OffsetBoxes2D(offsets))",
  "class DetectSingleShot(Processor):\n    \"\"\"Single-shot object detection prediction.\n\n    # Arguments\n        model: Keras model.\n        class_names: List of strings indicating the class names.\n        preprocess: Callable, pre-processing pipeline.\n        postprocess: Callable, post-processing pipeline.\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        variances: List, of floats.\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n    \"\"\"\n    def __init__(self, model, class_names, score_thresh, nms_thresh,\n                 preprocess=None, postprocess=None,\n                 variances=[0.1, 0.1, 0.2, 0.2], draw=True):\n        self.model = model\n        self.class_names = class_names\n        self.score_thresh = score_thresh\n        self.nms_thresh = nms_thresh\n        self.variances = variances\n        self.draw = draw\n        if preprocess is None:\n            preprocess = SSDPreprocess(model)\n        if postprocess is None:\n            postprocess = SSDPostprocess(\n                model, class_names, score_thresh, nms_thresh)\n\n        super(DetectSingleShot, self).__init__()\n        self.predict = pr.Predict(self.model, preprocess, postprocess)\n        self.denormalize = pr.DenormalizeBoxes2D()\n        self.draw_boxes2D = pr.DrawBoxes2D(self.class_names)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D'])\n\n    def call(self, image):\n        boxes2D = self.predict(image)\n        boxes2D = self.denormalize(image, boxes2D)\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n        return self.wrap(image, boxes2D)",
  "class SSDPreprocess(SequentialProcessor):\n    \"\"\"Preprocessing pipeline for SSD.\n\n    # Arguments\n        model: Keras model.\n        mean: List, of three elements indicating the per channel mean.\n        color_space: Int, specifying the color space to transform.\n    \"\"\"\n    def __init__(\n            self, model, mean=pr.BGR_IMAGENET_MEAN, color_space=pr.RGB2BGR):\n        super(SSDPreprocess, self).__init__()\n        self.add(pr.ResizeImage(model.input_shape[1:3]))\n        self.add(pr.ConvertColorSpace(color_space))\n        self.add(pr.SubtractMeanImage(mean))\n        self.add(pr.CastImage(float))\n        self.add(pr.ExpandDims(axis=0))",
  "class SSDPostprocess(SequentialProcessor):\n    \"\"\"Postprocessing pipeline for SSD.\n\n    # Arguments\n        model: Keras model.\n        class_names: List, of strings indicating the class names.\n        score_thresh: Float, between [0, 1]\n        nms_thresh: Float, between [0, 1].\n        variances: List, of floats.\n        class_arg: Int, index of class to be removed.\n        box_method: Int, type of boxes to boxes2D conversion method.\n    \"\"\"\n    def __init__(self, model, class_names, score_thresh, nms_thresh,\n                 variances=[0.1, 0.1, 0.2, 0.2], class_arg=0, box_method=0):\n        super(SSDPostprocess, self).__init__()\n        self.add(pr.Squeeze(axis=None))\n        self.add(pr.DecodeBoxes(model.prior_boxes, variances))\n        self.add(pr.RemoveClass(class_names, class_arg, renormalize=False))\n        self.add(pr.NonMaximumSuppressionPerClass(nms_thresh))\n        self.add(pr.MergeNMSBoxWithClass())\n        self.add(pr.FilterBoxes(class_names, score_thresh))\n        self.add(pr.ToBoxes2D(class_names, box_method))",
  "class DetectSingleShotEfficientDet(Processor):\n    \"\"\"Single-shot object detection prediction for EfficientDet models.\n\n    # Arguments\n        model: Keras model.\n        class_names: List of strings indicating class names.\n        preprocess: Callable, preprocessing pipeline.\n        postprocess: Callable, postprocessing pipeline.\n        draw: Bool. If ``True`` prediction are drawn on the\n            returned image.\n\n    # Properties\n        model: Keras model.\n        draw: Bool.\n        preprocess: Callable.\n        postprocess: Callable.\n        draw_boxes2D: Callable.\n        wrap: Callable.\n\n    # Methods\n        call()\n    \"\"\"\n    def __init__(self, model, class_names, score_thresh, nms_thresh,\n                 preprocess=None, postprocess=None, draw=True):\n        self.model = model\n        self.draw = draw\n        self.draw_boxes2D = pr.DrawBoxes2D(class_names)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D'])\n        if preprocess is None:\n            self.preprocess = EfficientDetPreprocess(model)\n        if postprocess is None:\n            self.postprocess = EfficientDetPostprocess(\n                model, class_names, score_thresh, nms_thresh)\n        super(DetectSingleShotEfficientDet, self).__init__()\n\n    def call(self, image):\n        preprocessed_image, image_scales = self.preprocess(image)\n        outputs = self.model(preprocessed_image)\n        outputs = change_box_coordinates(outputs)\n        boxes2D = self.postprocess(outputs, image_scales)\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n        return self.wrap(image, boxes2D)",
  "class EfficientDetPreprocess(SequentialProcessor):\n    \"\"\"Preprocessing pipeline for EfficientDet.\n\n    # Arguments\n        model: Keras model.\n        mean: Tuple, containing mean per channel on ImageNet.\n        standard_deviation: Tuple, containing standard deviations\n            per channel on ImageNet.\n    \"\"\"\n    def __init__(self, model, mean=pr.RGB_IMAGENET_MEAN,\n                 standard_deviation=pr.RGB_IMAGENET_STDEV):\n        super(EfficientDetPreprocess, self).__init__()\n        self.add(pr.CastImage(float))\n        self.add(pr.SubtractMeanImage(mean=mean))\n        self.add(pr.DivideStandardDeviationImage(standard_deviation))\n        self.add(pr.ScaledResize(image_size=model.input_shape[1]))",
  "class EfficientDetPostprocess(Processor):\n    \"\"\"Postprocessing pipeline for EfficientDet.\n\n    # Arguments\n        model: Keras model.\n        class_names: List of strings indicating class names.\n        score_thresh: Float between [0, 1].\n        nms_thresh: Float between [0, 1].\n        variances: List of float values.\n        class_arg: Int, index of the class to be removed.\n        renormalize: Bool, if true scores are renormalized.\n        method: Int, method to convert boxes to ``Boxes2D``.\n    \"\"\"\n    def __init__(self, model, class_names, score_thresh, nms_thresh,\n                 variances=[1.0, 1.0, 1.0, 1.0], class_arg=None):\n        super(EfficientDetPostprocess, self).__init__()\n        model.prior_boxes = model.prior_boxes * model.input_shape[1]\n        self.postprocess = pr.SequentialProcessor([\n            pr.Squeeze(axis=None),\n            pr.DecodeBoxes(model.prior_boxes, variances),\n            pr.RemoveClass(class_names, class_arg)])\n        self.scale = pr.ScaleBox()\n        self.nms_per_class = pr.NonMaximumSuppressionPerClass(nms_thresh)\n        self.merge_box_and_class = pr.MergeNMSBoxWithClass()\n        self.filter_boxes = pr.FilterBoxes(class_names, score_thresh)\n        self.to_boxes2D = pr.ToBoxes2D(class_names)\n        self.round_boxes = pr.RoundBoxes2D()\n\n    def call(self, output, image_scale):\n        box_data = self.postprocess(output)\n        box_data = self.scale(box_data, image_scale)\n        box_data, class_labels = self.nms_per_class(box_data)\n        box_data = self.merge_box_and_class(box_data, class_labels)\n        box_data = self.filter_boxes(box_data)\n        boxes2D = self.to_boxes2D(box_data)\n        boxes2D = self.round_boxes(boxes2D)\n        return boxes2D",
  "class SSD512COCO(DetectSingleShot):\n    \"\"\"Single-shot inference pipeline with SSD512 trained on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the returned image.\n\n    # Example\n        ``` python\n        from paz.pipelines import SSD512COCO\n\n        detect = SSD512COCO()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n     # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``boxes2D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a list of ``paz.abstract.messages.Boxes2D``.\n\n    # Reference\n        - [SSD: Single Shot MultiBox\n            Detector](https://arxiv.org/abs/1512.02325)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        model = SSD512()\n        names = get_class_names('COCO')\n        super(SSD512COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class SSD512YCBVideo(DetectSingleShot):\n    \"\"\"Single-shot inference pipeline with SSD512 trained on YCBVideo.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the returned image.\n\n    # Example\n        ``` python\n        from paz.pipelines import SSD512YCBVideo\n\n        detect = SSD512YCBVideo()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n\n    # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``boxes2D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a list of ``paz.abstract.messages.Boxes2D``.\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('YCBVideo')\n        model = SSD512(head_weights='YCBVideo', num_classes=len(names))\n        super(SSD512YCBVideo, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class SSD300VOC(DetectSingleShot):\n    \"\"\"Single-shot inference pipeline with SSD300 trained on VOC.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the returned image.\n\n    # Example\n        ``` python\n        from paz.pipelines import SSD300VOC\n\n        detect = SSD300VOC()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n\n    # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``boxes2D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a list of ``paz.abstract.messages.Boxes2D``.\n\n    # Reference\n        - [SSD: Single Shot MultiBox\n            Detector](https://arxiv.org/abs/1512.02325)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        model = SSD300()\n        names = get_class_names('VOC')\n        super(SSD300VOC, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class SSD300FAT(DetectSingleShot):\n    \"\"\"Single-shot inference pipeline with SSD300 trained on FAT.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the returned image.\n\n    # Example\n        ``` python\n        from paz.pipelines import SSD300FAT\n\n        detect = SSD300FAT()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n    # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``boxes2D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a list of ``paz.abstract.messages.Boxes2D``.\n\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        model = SSD300(22, 'FAT', 'FAT')\n        names = get_class_names('FAT')\n        super(SSD300FAT, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class DetectHaarCascade(Processor):\n    \"\"\"HaarCascade prediction pipeline/function from RGB-image.\n\n    # Arguments\n        detector: An instantiated ``HaarCascadeDetector`` model.\n        offsets: List of two elements. Each element must be between [0, 1].\n        class_names: List of strings.\n        draw: Boolean. If ``True`` prediction are drawn in the returned image.\n\n    # Returns\n        A function for predicting bounding box detections.\n    \"\"\"\n    def __init__(self, detector, class_names=None, colors=None, draw=True):\n        super(DetectHaarCascade, self).__init__()\n        self.detector = detector\n        self.class_names = class_names\n        self.colors = colors\n        self.draw = draw\n        RGB2GRAY = pr.ConvertColorSpace(pr.RGB2GRAY)\n        postprocess = SequentialProcessor()\n        postprocess.add(pr.ToBoxes2D(self.class_names, box_method=2))\n        self.predict = pr.Predict(self.detector, RGB2GRAY, postprocess)\n        self.draw_boxes2D = pr.DrawBoxes2D(self.class_names, self.colors)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D'])\n\n    def call(self, image):\n        boxes2D = self.predict(image)\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n        return self.wrap(image, boxes2D)",
  "class HaarCascadeFrontalFace(DetectHaarCascade):\n    \"\"\"HaarCascade pipeline for detecting frontal faces\n\n    # Arguments\n        class_name: String indicating the class name.\n        color: List indicating the RGB color e.g. ``[0, 255, 0]``.\n        draw: Boolean. If ``False`` the bounding boxes are not drawn.\n\n    # Example\n        ``` python\n        from paz.pipelines import HaarCascadeFrontalFace\n\n        detect = HaarCascadeFrontalFace()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n    # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``boxes2D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a list of ``paz.abstract.messages.Boxes2D``.\n\n    \"\"\"\n    def __init__(self, class_name='Face', color=[0, 255, 0], draw=True):\n        self.model = HaarCascadeDetector('frontalface_default', class_arg=0)\n        super(HaarCascadeFrontalFace, self).__init__(\n            self.model, [class_name], [color], draw)",
  "class DetectMiniXceptionFER(Processor):\n    \"\"\"Emotion classification and detection pipeline.\n\n    # Returns\n        Dictionary with ``image`` and ``boxes2D``.\n\n    # Example\n        ``` python\n        from paz.pipelines import DetectMiniXceptionFER\n\n        detect = DetectMiniXceptionFER()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n    # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``boxes2D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a list of ``paz.abstract.messages.Boxes2D``.\n\n    # References\n       - [Real-time Convolutional Neural Networks for Emotion and\n            Gender Classification](https://arxiv.org/abs/1710.07557)\n    \"\"\"\n    def __init__(self, offsets=[0, 0], colors=EMOTION_COLORS):\n        super(DetectMiniXceptionFER, self).__init__()\n        self.offsets = offsets\n        self.colors = colors\n\n        # detection\n        self.detect = HaarCascadeFrontalFace()\n        self.square = SequentialProcessor()\n        self.square.add(pr.SquareBoxes2D())\n        self.square.add(pr.OffsetBoxes2D(offsets))\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n\n        # classification\n        self.classify = MiniXceptionFER()\n\n        # drawing and wrapping\n        self.class_names = self.classify.class_names\n        self.draw = pr.DrawBoxes2D(self.class_names, self.colors, True)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D'])\n\n    def call(self, image):\n        boxes2D = self.detect(image.copy())['boxes2D']\n        boxes2D = self.square(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            predictions = self.classify(cropped_image)\n            box2D.class_name = predictions['class_name']\n            box2D.score = np.amax(predictions['scores'])\n        image = self.draw(image, boxes2D)\n        return self.wrap(image, boxes2D)",
  "class DetectKeypoints2D(Processor):\n    def __init__(self, detect, estimate_keypoints, offsets=[0, 0], radius=3):\n        \"\"\"General detection and keypoint estimator pipeline.\n\n        # Arguments\n            detect: Function for detecting objects. The output should be a\n                dictionary with key ``Boxes2D`` containing a list\n                of ``Boxes2D`` messages.\n            estimate_keypoints: Function for estimating keypoints. The output\n                should be a dictionary with key ``keypoints`` containing\n                a numpy array of keypoints.\n            offsets: List of two elements. Each element must be between [0, 1].\n            radius: Int indicating the radius of the keypoints to be drawn.\n        \"\"\"\n        super(DetectKeypoints2D, self).__init__()\n        self.detect = detect\n        self.estimate_keypoints = estimate_keypoints\n        self.num_keypoints = estimate_keypoints.num_keypoints\n        self.square = SequentialProcessor()\n        self.square.add(pr.SquareBoxes2D())\n        self.square.add(pr.OffsetBoxes2D(offsets))\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.change_coordinates = pr.ChangeKeypointsCoordinateSystem()\n        self.draw = pr.DrawKeypoints2D(self.num_keypoints, radius, False)\n        self.draw_boxes = pr.DrawBoxes2D(detect.class_names, detect.colors)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D', 'keypoints'])\n\n    def call(self, image):\n        boxes2D = self.detect(image)['boxes2D']\n        boxes2D = self.square(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        keypoints2D = []\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            keypoints = self.estimate_keypoints(cropped_image)['keypoints']\n            keypoints = self.change_coordinates(keypoints, box2D)\n            keypoints2D.append(keypoints)\n            image = self.draw(image, keypoints)\n        image = self.draw_boxes(image, boxes2D)\n        return self.wrap(image, boxes2D, keypoints2D)",
  "class DetectFaceKeypointNet2D32(DetectKeypoints2D):\n    \"\"\"Frontal face detection pipeline with facial keypoint estimation.\n\n    # Arguments\n        offsets: List of two elements. Each element must be between [0, 1].\n        radius: Int indicating the radius of the keypoints to be drawn.\n\n    # Example\n        ``` python\n        from paz.pipelines import DetectFaceKeypointNet2D32\n\n        detect = DetectFaceKeypointNet2D32()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n    # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``boxes2D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a list of ``paz.abstract.messages.Boxes2D``.\n\n    \"\"\"\n    def __init__(self, offsets=[0, 0], radius=3):\n        detect = HaarCascadeFrontalFace(draw=False)\n        estimate_keypoints = FaceKeypointNet2D32(draw=False)\n        super(DetectFaceKeypointNet2D32, self).__init__(\n            detect, estimate_keypoints, offsets, radius)",
  "class SSD512HandDetection(DetectSingleShot):\n    \"\"\"Minimal hand detection with SSD512Custom trained on OPenImageV6.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the returned image.\n\n    # Example\n        ``` python\n        from paz.pipelines import SSD512HandDetection\n\n        detect = SSD512HandDetection()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n     # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image`` and ``boxes2D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences and a list of ``paz.abstract.messages.Boxes2D``.\n\n    # Reference\n        - [SSD: Single Shot MultiBox\n            Detector](https://arxiv.org/abs/1512.02325)\n    \"\"\"\n    def __init__(self, score_thresh=0.40, nms_thresh=0.45, draw=True):\n        class_names = ['background', 'hand']\n        num_classes = len(class_names)\n        model = SSD512(num_classes, base_weights='OIV6Hand',\n                       head_weights='OIV6Hand')\n        super(SSD512HandDetection, self).__init__(\n            model, class_names, score_thresh, nms_thresh, draw=draw)",
  "class SSD512MinimalHandPose(DetectMinimalHand):\n    \"\"\"Hand detection and minimal hand pose estimation pipeline.\n\n    # Arguments\n        right_hand: Boolean. True for right hand inference.\n        offsets: List of two elements. Each element must be between [0, 1].\n\n    # Example\n        ``` python\n        from paz.pipelines import SSD512MinimalHandPose\n\n        detect = SSD512MinimalHandPose()\n\n        # apply directly to an image (numpy-array)\n        inferences = detect(image)\n        ```\n\n    # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``image``,  ``boxes2D``,\n        ``Keypoints2D``, ``Keypoints3D``.\n        The corresponding values of these keys contain the image with the drawn\n        inferences.\n    \"\"\"\n    def __init__(self, right_hand=False, offsets=[0.25, 0.25]):\n        detector = SSD512HandDetection()\n        keypoint_estimator = MinimalHandPoseEstimation(right_hand)\n        super(SSD512MinimalHandPose, self).__init__(\n            detector, keypoint_estimator, offsets)",
  "class EFFICIENTDETD0COCO(DetectSingleShotEfficientDet):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD0 trained\n    on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD0(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD0COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class EFFICIENTDETD1COCO(DetectSingleShotEfficientDet):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD1 trained\n    on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD1(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD1COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class EFFICIENTDETD2COCO(DetectSingleShotEfficientDet):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD2 trained\n    on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD2(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD2COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class EFFICIENTDETD3COCO(DetectSingleShotEfficientDet):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD3 trained\n    on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD3(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD3COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class EFFICIENTDETD4COCO(DetectSingleShotEfficientDet):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD4 trained\n    on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD4(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD4COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class EFFICIENTDETD5COCO(DetectSingleShotEfficientDet):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD5 trained\n    on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD5(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD5COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class EFFICIENTDETD6COCO(DetectSingleShotEfficientDet):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD6 trained\n    on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD6(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD6COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class EFFICIENTDETD7COCO(DetectSingleShotEfficientDet):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD7 trained\n    on COCO.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD7(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD7COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class EFFICIENTDETD0VOC(DetectSingleShot):\n    \"\"\"Single-shot inference pipeline with EFFICIENTDETD0 trained\n    on VOC.\n\n    # Arguments\n        score_thresh: Float between [0, 1]\n        nms_thresh: Float between [0, 1].\n        draw: Boolean. If ``True`` prediction are drawn in the\n            returned image.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('VOC')\n        model = EFFICIENTDETD0(num_classes=len(names),\n                               base_weights='VOC', head_weights='VOC')\n        super(EFFICIENTDETD0VOC, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, mean=pr.BGR_IMAGENET_MEAN):\n        super(AugmentBoxes, self).__init__()\n        self.add(pr.ToImageBoxCoordinates())\n        self.add(pr.Expand(mean=mean))\n        self.add(pr.RandomSampleCrop(1.0))\n        self.add(pr.RandomFlipBoxesLeftRight())\n        self.add(pr.ToNormalizedBoxCoordinates())",
  "def __init__(self, num_classes, prior_boxes, IOU, variances):\n        super(PreprocessBoxes, self).__init__()\n        self.add(pr.MatchBoxes(prior_boxes, IOU),)\n        self.add(pr.EncodeBoxes(prior_boxes, variances))\n        self.add(pr.BoxClassToOneHotVector(num_classes))",
  "def __init__(self, prior_boxes, split=pr.TRAIN, num_classes=21, size=300,\n                 mean=pr.BGR_IMAGENET_MEAN, IOU=.5,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(AugmentDetection, self).__init__()\n        # image processors\n        self.augment_image = AugmentImage()\n        # self.augment_image.add(pr.ConvertColorSpace(pr.RGB2BGR))\n        self.preprocess_image = PreprocessImage((size, size), mean)\n        self.preprocess_image.insert(0, pr.ConvertColorSpace(pr.RGB2BGR))\n\n        # box processors\n        self.augment_boxes = AugmentBoxes()\n        args = (num_classes, prior_boxes, IOU, variances)\n        self.preprocess_boxes = PreprocessBoxes(*args)\n\n        # pipeline\n        self.add(pr.UnpackDictionary(['image', 'boxes']))\n        self.add(pr.ControlMap(pr.LoadImage(), [0], [0]))\n        if split == pr.TRAIN:\n            self.add(pr.ControlMap(self.augment_image, [0], [0]))\n            self.add(pr.ControlMap(self.augment_boxes, [0, 1], [0, 1]))\n        self.add(pr.ControlMap(self.preprocess_image, [0], [0]))\n        self.add(pr.ControlMap(self.preprocess_boxes, [1], [1]))\n        self.add(pr.SequenceWrapper(\n            {0: {'image': [size, size, 3]}},\n            {1: {'boxes': [len(prior_boxes), 4 + num_classes]}}))",
  "def __init__(self, offsets, valid_names=None):\n        super(PostprocessBoxes2D, self).__init__()\n        if valid_names is not None:\n            self.add(pr.FilterClassBoxes2D(valid_names))\n        self.add(pr.SquareBoxes2D())\n        self.add(pr.OffsetBoxes2D(offsets))",
  "def __init__(self, model, class_names, score_thresh, nms_thresh,\n                 preprocess=None, postprocess=None,\n                 variances=[0.1, 0.1, 0.2, 0.2], draw=True):\n        self.model = model\n        self.class_names = class_names\n        self.score_thresh = score_thresh\n        self.nms_thresh = nms_thresh\n        self.variances = variances\n        self.draw = draw\n        if preprocess is None:\n            preprocess = SSDPreprocess(model)\n        if postprocess is None:\n            postprocess = SSDPostprocess(\n                model, class_names, score_thresh, nms_thresh)\n\n        super(DetectSingleShot, self).__init__()\n        self.predict = pr.Predict(self.model, preprocess, postprocess)\n        self.denormalize = pr.DenormalizeBoxes2D()\n        self.draw_boxes2D = pr.DrawBoxes2D(self.class_names)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D'])",
  "def call(self, image):\n        boxes2D = self.predict(image)\n        boxes2D = self.denormalize(image, boxes2D)\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n        return self.wrap(image, boxes2D)",
  "def __init__(\n            self, model, mean=pr.BGR_IMAGENET_MEAN, color_space=pr.RGB2BGR):\n        super(SSDPreprocess, self).__init__()\n        self.add(pr.ResizeImage(model.input_shape[1:3]))\n        self.add(pr.ConvertColorSpace(color_space))\n        self.add(pr.SubtractMeanImage(mean))\n        self.add(pr.CastImage(float))\n        self.add(pr.ExpandDims(axis=0))",
  "def __init__(self, model, class_names, score_thresh, nms_thresh,\n                 variances=[0.1, 0.1, 0.2, 0.2], class_arg=0, box_method=0):\n        super(SSDPostprocess, self).__init__()\n        self.add(pr.Squeeze(axis=None))\n        self.add(pr.DecodeBoxes(model.prior_boxes, variances))\n        self.add(pr.RemoveClass(class_names, class_arg, renormalize=False))\n        self.add(pr.NonMaximumSuppressionPerClass(nms_thresh))\n        self.add(pr.MergeNMSBoxWithClass())\n        self.add(pr.FilterBoxes(class_names, score_thresh))\n        self.add(pr.ToBoxes2D(class_names, box_method))",
  "def __init__(self, model, class_names, score_thresh, nms_thresh,\n                 preprocess=None, postprocess=None, draw=True):\n        self.model = model\n        self.draw = draw\n        self.draw_boxes2D = pr.DrawBoxes2D(class_names)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D'])\n        if preprocess is None:\n            self.preprocess = EfficientDetPreprocess(model)\n        if postprocess is None:\n            self.postprocess = EfficientDetPostprocess(\n                model, class_names, score_thresh, nms_thresh)\n        super(DetectSingleShotEfficientDet, self).__init__()",
  "def call(self, image):\n        preprocessed_image, image_scales = self.preprocess(image)\n        outputs = self.model(preprocessed_image)\n        outputs = change_box_coordinates(outputs)\n        boxes2D = self.postprocess(outputs, image_scales)\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n        return self.wrap(image, boxes2D)",
  "def __init__(self, model, mean=pr.RGB_IMAGENET_MEAN,\n                 standard_deviation=pr.RGB_IMAGENET_STDEV):\n        super(EfficientDetPreprocess, self).__init__()\n        self.add(pr.CastImage(float))\n        self.add(pr.SubtractMeanImage(mean=mean))\n        self.add(pr.DivideStandardDeviationImage(standard_deviation))\n        self.add(pr.ScaledResize(image_size=model.input_shape[1]))",
  "def __init__(self, model, class_names, score_thresh, nms_thresh,\n                 variances=[1.0, 1.0, 1.0, 1.0], class_arg=None):\n        super(EfficientDetPostprocess, self).__init__()\n        model.prior_boxes = model.prior_boxes * model.input_shape[1]\n        self.postprocess = pr.SequentialProcessor([\n            pr.Squeeze(axis=None),\n            pr.DecodeBoxes(model.prior_boxes, variances),\n            pr.RemoveClass(class_names, class_arg)])\n        self.scale = pr.ScaleBox()\n        self.nms_per_class = pr.NonMaximumSuppressionPerClass(nms_thresh)\n        self.merge_box_and_class = pr.MergeNMSBoxWithClass()\n        self.filter_boxes = pr.FilterBoxes(class_names, score_thresh)\n        self.to_boxes2D = pr.ToBoxes2D(class_names)\n        self.round_boxes = pr.RoundBoxes2D()",
  "def call(self, output, image_scale):\n        box_data = self.postprocess(output)\n        box_data = self.scale(box_data, image_scale)\n        box_data, class_labels = self.nms_per_class(box_data)\n        box_data = self.merge_box_and_class(box_data, class_labels)\n        box_data = self.filter_boxes(box_data)\n        boxes2D = self.to_boxes2D(box_data)\n        boxes2D = self.round_boxes(boxes2D)\n        return boxes2D",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        model = SSD512()\n        names = get_class_names('COCO')\n        super(SSD512COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('YCBVideo')\n        model = SSD512(head_weights='YCBVideo', num_classes=len(names))\n        super(SSD512YCBVideo, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        model = SSD300()\n        names = get_class_names('VOC')\n        super(SSD300VOC, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        model = SSD300(22, 'FAT', 'FAT')\n        names = get_class_names('FAT')\n        super(SSD300FAT, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, detector, class_names=None, colors=None, draw=True):\n        super(DetectHaarCascade, self).__init__()\n        self.detector = detector\n        self.class_names = class_names\n        self.colors = colors\n        self.draw = draw\n        RGB2GRAY = pr.ConvertColorSpace(pr.RGB2GRAY)\n        postprocess = SequentialProcessor()\n        postprocess.add(pr.ToBoxes2D(self.class_names, box_method=2))\n        self.predict = pr.Predict(self.detector, RGB2GRAY, postprocess)\n        self.draw_boxes2D = pr.DrawBoxes2D(self.class_names, self.colors)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D'])",
  "def call(self, image):\n        boxes2D = self.predict(image)\n        if self.draw:\n            image = self.draw_boxes2D(image, boxes2D)\n        return self.wrap(image, boxes2D)",
  "def __init__(self, class_name='Face', color=[0, 255, 0], draw=True):\n        self.model = HaarCascadeDetector('frontalface_default', class_arg=0)\n        super(HaarCascadeFrontalFace, self).__init__(\n            self.model, [class_name], [color], draw)",
  "def __init__(self, offsets=[0, 0], colors=EMOTION_COLORS):\n        super(DetectMiniXceptionFER, self).__init__()\n        self.offsets = offsets\n        self.colors = colors\n\n        # detection\n        self.detect = HaarCascadeFrontalFace()\n        self.square = SequentialProcessor()\n        self.square.add(pr.SquareBoxes2D())\n        self.square.add(pr.OffsetBoxes2D(offsets))\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n\n        # classification\n        self.classify = MiniXceptionFER()\n\n        # drawing and wrapping\n        self.class_names = self.classify.class_names\n        self.draw = pr.DrawBoxes2D(self.class_names, self.colors, True)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D'])",
  "def call(self, image):\n        boxes2D = self.detect(image.copy())['boxes2D']\n        boxes2D = self.square(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            predictions = self.classify(cropped_image)\n            box2D.class_name = predictions['class_name']\n            box2D.score = np.amax(predictions['scores'])\n        image = self.draw(image, boxes2D)\n        return self.wrap(image, boxes2D)",
  "def __init__(self, detect, estimate_keypoints, offsets=[0, 0], radius=3):\n        \"\"\"General detection and keypoint estimator pipeline.\n\n        # Arguments\n            detect: Function for detecting objects. The output should be a\n                dictionary with key ``Boxes2D`` containing a list\n                of ``Boxes2D`` messages.\n            estimate_keypoints: Function for estimating keypoints. The output\n                should be a dictionary with key ``keypoints`` containing\n                a numpy array of keypoints.\n            offsets: List of two elements. Each element must be between [0, 1].\n            radius: Int indicating the radius of the keypoints to be drawn.\n        \"\"\"\n        super(DetectKeypoints2D, self).__init__()\n        self.detect = detect\n        self.estimate_keypoints = estimate_keypoints\n        self.num_keypoints = estimate_keypoints.num_keypoints\n        self.square = SequentialProcessor()\n        self.square.add(pr.SquareBoxes2D())\n        self.square.add(pr.OffsetBoxes2D(offsets))\n        self.clip = pr.ClipBoxes2D()\n        self.crop = pr.CropBoxes2D()\n        self.change_coordinates = pr.ChangeKeypointsCoordinateSystem()\n        self.draw = pr.DrawKeypoints2D(self.num_keypoints, radius, False)\n        self.draw_boxes = pr.DrawBoxes2D(detect.class_names, detect.colors)\n        self.wrap = pr.WrapOutput(['image', 'boxes2D', 'keypoints'])",
  "def call(self, image):\n        boxes2D = self.detect(image)['boxes2D']\n        boxes2D = self.square(boxes2D)\n        boxes2D = self.clip(image, boxes2D)\n        cropped_images = self.crop(image, boxes2D)\n        keypoints2D = []\n        for cropped_image, box2D in zip(cropped_images, boxes2D):\n            keypoints = self.estimate_keypoints(cropped_image)['keypoints']\n            keypoints = self.change_coordinates(keypoints, box2D)\n            keypoints2D.append(keypoints)\n            image = self.draw(image, keypoints)\n        image = self.draw_boxes(image, boxes2D)\n        return self.wrap(image, boxes2D, keypoints2D)",
  "def __init__(self, offsets=[0, 0], radius=3):\n        detect = HaarCascadeFrontalFace(draw=False)\n        estimate_keypoints = FaceKeypointNet2D32(draw=False)\n        super(DetectFaceKeypointNet2D32, self).__init__(\n            detect, estimate_keypoints, offsets, radius)",
  "def __init__(self, score_thresh=0.40, nms_thresh=0.45, draw=True):\n        class_names = ['background', 'hand']\n        num_classes = len(class_names)\n        model = SSD512(num_classes, base_weights='OIV6Hand',\n                       head_weights='OIV6Hand')\n        super(SSD512HandDetection, self).__init__(\n            model, class_names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, right_hand=False, offsets=[0.25, 0.25]):\n        detector = SSD512HandDetection()\n        keypoint_estimator = MinimalHandPoseEstimation(right_hand)\n        super(SSD512MinimalHandPose, self).__init__(\n            detector, keypoint_estimator, offsets)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD0(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD0COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD1(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD1COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD2(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD2COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD3(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD3COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD4(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD4COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD5(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD5COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD6(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD6COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('COCO_EFFICIENTDET')\n        model = EFFICIENTDETD7(num_classes=len(names),\n                               base_weights='COCO', head_weights='COCO')\n        super(EFFICIENTDETD7COCO, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "def __init__(self, score_thresh=0.60, nms_thresh=0.45, draw=True):\n        names = get_class_names('VOC')\n        model = EFFICIENTDETD0(num_classes=len(names),\n                               base_weights='VOC', head_weights='VOC')\n        super(EFFICIENTDETD0VOC, self).__init__(\n            model, names, score_thresh, nms_thresh, draw=draw)",
  "class MiniXceptionFER(SequentialProcessor):\n    \"\"\"Mini Xception pipeline for classifying emotions from RGB faces.\n\n    # Example\n        ``` python\n        from paz.pipelines import MiniXceptionFER\n\n        classify = MiniXceptionFER()\n\n        # apply directly to an image (numpy-array)\n        inference = classify(image)\n        ```\n\n     # Returns\n        A function that takes an RGB image and outputs the predictions\n        as a dictionary with ``keys``: ``class_names`` and ``scores``.\n\n    # References\n       - [Real-time Convolutional Neural Networks for Emotion and\n            Gender Classification](https://arxiv.org/abs/1710.07557)\n\n    \"\"\"\n    def __init__(self):\n        super(MiniXceptionFER, self).__init__()\n        self.classifier = MiniXception((48, 48, 1), 7, weights='FER')\n        self.class_names = get_class_names('FER')\n\n        preprocess = PreprocessImage(self.classifier.input_shape[1:3], None)\n        preprocess.insert(0, pr.ConvertColorSpace(pr.RGB2GRAY))\n        preprocess.add(pr.ExpandDims(0))\n        preprocess.add(pr.ExpandDims(-1))\n        self.add(pr.Predict(self.classifier, preprocess))\n        self.add(pr.CopyDomain([0], [1]))\n        self.add(pr.ControlMap(pr.ToClassName(self.class_names), [0], [0]))\n        self.add(pr.WrapOutput(['class_name', 'scores']))",
  "class ClassifyHandClosure(SequentialProcessor):\n    \"\"\"Pipeline to classify minimal hand closure status.\n\n    # Example\n        ``` python\n        from paz.pipelines import ClassifyHandClosure\n\n        classify = ClassifyHandClosure()\n\n        # apply directly to an image (numpy-array)\n        inference = classify(image)\n        ```\n\n     # Returns\n        A function that takes an RGB image and outputs an image with class\n        status drawn on it.\n    \"\"\"\n    def __init__(self, draw=True, right_hand=False):\n        super(ClassifyHandClosure, self).__init__()\n        self.add(MinimalHandPoseEstimation(draw, right_hand))\n        self.add(pr.UnpackDictionary(['image', 'relative_angles']))\n        self.add(pr.ControlMap(pr.IsHandOpen(), [1], [1]))\n        self.add(pr.ControlMap(pr.BooleanToTextMessage('OPEN', 'CLOSE'),\n                               [1], [1]))\n        if draw:\n            self.add(pr.ControlMap(pr.DrawText(), [0, 1], [0], {1: 1}))\n        self.add(pr.WrapOutput(['image', 'status']))",
  "def __init__(self):\n        super(MiniXceptionFER, self).__init__()\n        self.classifier = MiniXception((48, 48, 1), 7, weights='FER')\n        self.class_names = get_class_names('FER')\n\n        preprocess = PreprocessImage(self.classifier.input_shape[1:3], None)\n        preprocess.insert(0, pr.ConvertColorSpace(pr.RGB2GRAY))\n        preprocess.add(pr.ExpandDims(0))\n        preprocess.add(pr.ExpandDims(-1))\n        self.add(pr.Predict(self.classifier, preprocess))\n        self.add(pr.CopyDomain([0], [1]))\n        self.add(pr.ControlMap(pr.ToClassName(self.class_names), [0], [0]))\n        self.add(pr.WrapOutput(['class_name', 'scores']))",
  "def __init__(self, draw=True, right_hand=False):\n        super(ClassifyHandClosure, self).__init__()\n        self.add(MinimalHandPoseEstimation(draw, right_hand))\n        self.add(pr.UnpackDictionary(['image', 'relative_angles']))\n        self.add(pr.ControlMap(pr.IsHandOpen(), [1], [1]))\n        self.add(pr.ControlMap(pr.BooleanToTextMessage('OPEN', 'CLOSE'),\n                               [1], [1]))\n        if draw:\n            self.add(pr.ControlMap(pr.DrawText(), [0, 1], [0], {1: 1}))\n        self.add(pr.WrapOutput(['image', 'status']))",
  "class RenderTwoViews(Processor):\n    \"\"\"Renders two views along with their transformations.\n\n    # Arguments\n        renderer: A class with a method ``render`` that outputs\n            two lists. The first list contains two numpy arrays\n            representing the images e.g. ``(image_A, image_B)`` each\n            of shape ``[H, W, 3]``.\n            The other list contains three numpy arrays representing the\n            transformations from the origin to the cameras and the\n            two alpha channels of both images e.g.\n            ``[matrices, alpha_channel_A, alpha_channel_B]``.\n            ``matrices`` is a numpy array of shape ``(4, 4 * 4)``.\n            Each row is a matrix of ``4 x 4`` representing the following\n            transformations respectively: ``world_to_A``, ``world_to_B``,\n            ``A_to_world`` and  ``B_to_world``.\n            The shape of each ``alpha_channel`` should be ``[H, W]``.\n    \"\"\"\n    def __init__(self, renderer):\n        super(RenderTwoViews, self).__init__()\n        self.render = pr.Render(renderer)\n\n        self.preprocess_image = SequentialProcessor()\n        self.preprocess_image.add(pr.ConvertColorSpace(pr.RGB2BGR))\n        self.preprocess_image.add(pr.NormalizeImage())\n\n        self.preprocess_alpha = SequentialProcessor()\n        self.preprocess_alpha.add(pr.NormalizeImage())\n        self.concatenate = pr.Concatenate(-1)\n\n    def call(self):\n        data = self.render()\n        image_A = self.preprocess_image(data['image_A'])\n        image_B = self.preprocess_image(data['image_B'])\n        alpha_A = self.preprocess_alpha(data['alpha_A'])\n        alpha_B = self.preprocess_alpha(data['alpha_B'])\n        alpha_channels = self.concatenate([alpha_A, alpha_B])\n        matrices = data['matrices']\n        return image_A, image_B, matrices, alpha_channels",
  "class RandomizeRenderedImage(SequentialProcessor):\n    \"\"\"Performs alpha blending and data-augmentation to an image and\n        it's alpha channel.\n    image_paths: List of strings indicating the paths to the images used for\n        the background.\n    num_occlusions: Int. number of occlusions to be added to the image.\n    max_radius_scale: Float between [0, 1] indicating the maximum radius in\n        scale of the image size.\n    \"\"\"\n    def __init__(self, image_paths, num_occlusions=1, max_radius_scale=0.5):\n        super(RandomizeRenderedImage, self).__init__()\n        self.add(pr.ConcatenateAlphaMask())\n        self.add(pr.BlendRandomCroppedBackground(image_paths))\n        for arg in range(num_occlusions):\n            self.add(pr.AddOcclusion(max_radius_scale))\n        self.add(pr.RandomImageBlur())\n        self.add(AugmentImage())",
  "def __init__(self, renderer):\n        super(RenderTwoViews, self).__init__()\n        self.render = pr.Render(renderer)\n\n        self.preprocess_image = SequentialProcessor()\n        self.preprocess_image.add(pr.ConvertColorSpace(pr.RGB2BGR))\n        self.preprocess_image.add(pr.NormalizeImage())\n\n        self.preprocess_alpha = SequentialProcessor()\n        self.preprocess_alpha.add(pr.NormalizeImage())\n        self.concatenate = pr.Concatenate(-1)",
  "def call(self):\n        data = self.render()\n        image_A = self.preprocess_image(data['image_A'])\n        image_B = self.preprocess_image(data['image_B'])\n        alpha_A = self.preprocess_alpha(data['alpha_A'])\n        alpha_B = self.preprocess_alpha(data['alpha_B'])\n        alpha_channels = self.concatenate([alpha_A, alpha_B])\n        matrices = data['matrices']\n        return image_A, image_B, matrices, alpha_channels",
  "def __init__(self, image_paths, num_occlusions=1, max_radius_scale=0.5):\n        super(RandomizeRenderedImage, self).__init__()\n        self.add(pr.ConcatenateAlphaMask())\n        self.add(pr.BlendRandomCroppedBackground(image_paths))\n        for arg in range(num_occlusions):\n            self.add(pr.AddOcclusion(max_radius_scale))\n        self.add(pr.RandomImageBlur())\n        self.add(AugmentImage())",
  "class IKNetHandJointAngles(pr.Processor):\n    \"\"\"Estimate absolute and relative joint angle for the minimal hand joints\n       using the 3D keypoint locations.\n\n    # Arguments\n        links_origin: Array. Reference pose of the minimal hand joints.\n        parent: List. Parents of the keypoints from kinematic chain\n        right_hand: Boolean. If 'True', estimate angles for right hand, else\n                    estimate angles for left hand.\n        keypoints3D: Array [num_joints, 3]. 3D location of keypoints.\n\n    # Returns\n        absolute_angles: Array [num_joints, 4]. quaternion repesentation\n        relative_angles: Array [num_joints, 3]. axis-angle repesentation\n    \"\"\"\n    def __init__(self, links_origin=MPIIHandJoints.links_origin,\n                 parents=MPIIHandJoints.parents, right_hand=False):\n        super(IKNetHandJointAngles, self).__init__()\n        self.calculate_orientation = pr.ComputeOrientationVector(parents)\n        self.links_origin = links_origin\n        self.right_hand = right_hand\n        if self.right_hand:\n            self.links_origin = flip_along_x_axis(self.links_origin)\n        self.links_delta = self.calculate_orientation(self.links_origin)\n        self.concatenate = pr.Concatenate(0)\n        self.compute_absolute_angles = pr.SequentialProcessor(\n            [pr.ExpandDims(0), IKNet(), pr.Squeeze(0)])\n        self.compute_relative_angles = pr.CalculateRelativeAngles()\n        self.wrap = pr.WrapOutput(['absolute_angles', 'relative_angles'])\n\n    def call(self, keypoints3D):\n        delta = self.calculate_orientation(keypoints3D)\n        pack = self.concatenate(\n            [keypoints3D, delta, self.links_origin, self.links_delta])\n        absolute_angles = self.compute_absolute_angles(pack)\n        relative_angles = self.compute_relative_angles(absolute_angles)\n        return self.wrap(absolute_angles, relative_angles)",
  "def __init__(self, links_origin=MPIIHandJoints.links_origin,\n                 parents=MPIIHandJoints.parents, right_hand=False):\n        super(IKNetHandJointAngles, self).__init__()\n        self.calculate_orientation = pr.ComputeOrientationVector(parents)\n        self.links_origin = links_origin\n        self.right_hand = right_hand\n        if self.right_hand:\n            self.links_origin = flip_along_x_axis(self.links_origin)\n        self.links_delta = self.calculate_orientation(self.links_origin)\n        self.concatenate = pr.Concatenate(0)\n        self.compute_absolute_angles = pr.SequentialProcessor(\n            [pr.ExpandDims(0), IKNet(), pr.Squeeze(0)])\n        self.compute_relative_angles = pr.CalculateRelativeAngles()\n        self.wrap = pr.WrapOutput(['absolute_angles', 'relative_angles'])",
  "def call(self, keypoints3D):\n        delta = self.calculate_orientation(keypoints3D)\n        pack = self.concatenate(\n            [keypoints3D, delta, self.links_origin, self.links_delta])\n        absolute_angles = self.compute_absolute_angles(pack)\n        relative_angles = self.compute_relative_angles(absolute_angles)\n        return self.wrap(absolute_angles, relative_angles)",
  "class SequenceExtra(Sequence):\n    def __init__(self, pipeline, batch_size, as_list=False):\n        if not isinstance(pipeline, SequentialProcessor):\n            raise ValueError('``processor`` must be a ``SequentialProcessor``')\n        self.output_wrapper = pipeline.processors[-1]\n        self.pipeline = pipeline\n        self.inputs_name_to_shape = self.output_wrapper.inputs_name_to_shape\n        self.labels_name_to_shape = self.output_wrapper.labels_name_to_shape\n        self.ordered_input_names = self.output_wrapper.ordered_input_names\n        self.ordered_label_names = self.output_wrapper.ordered_label_names\n        self.batch_size = batch_size\n        self.as_list = as_list\n\n    def make_empty_batches(self, name_to_shape):\n        batch = {}\n        for name, shape in name_to_shape.items():\n            batch[name] = np.zeros((self.batch_size, *shape))\n        return batch\n\n    def _to_list(self, batch, names):\n        return [batch[name] for name in names]\n\n    def _place_sample(self, sample, sample_arg, batch):\n        for name, data in sample.items():\n            batch[name][sample_arg] = data\n\n    def _get_unprocessed_batch(self, data, batch_index):\n        batch_arg_A = self.batch_size * (batch_index)\n        batch_arg_B = self.batch_size * (batch_index + 1)\n        unprocessed_batch = data[batch_arg_A:batch_arg_B]\n        return unprocessed_batch\n\n    def __getitem__(self, batch_index):\n        inputs = self.make_empty_batches(self.inputs_name_to_shape)\n        labels = self.make_empty_batches(self.labels_name_to_shape)\n        inputs, labels = self.process_batch(inputs, labels, batch_index)\n        if self.as_list:\n            inputs = self._to_list(inputs, self.ordered_input_names)\n            labels = self._to_list(labels, self.ordered_label_names)\n        return inputs, labels\n\n    def process_batch(self, inputs, labels, batch_index=None):\n        raise NotImplementedError",
  "class ProcessingSequence(SequenceExtra):\n    \"\"\"Sequence generator used for processing samples given in ``data``.\n\n    # Arguments\n        processor: Function, used for processing elements of ``data``.\n        batch_size: Int.\n        data: List. Each element of the list is processed by ``processor``.\n        as_list: Bool, if True ``inputs`` and ``labels`` are dispatched as\n            lists. If false ``inputs`` and ``labels`` are dispatched as\n            dictionaries.\n    \"\"\"\n    def __init__(self, processor, batch_size, data, as_list=False):\n        self.data = data\n        super(ProcessingSequence, self).__init__(\n            processor, batch_size, as_list)\n\n    def __len__(self):\n        return int(np.ceil(len(self.data) / float(self.batch_size)))\n\n    def process_batch(self, inputs, labels, batch_index):\n        unprocessed_batch = self._get_unprocessed_batch(self.data, batch_index)\n\n        for sample_arg, unprocessed_sample in enumerate(unprocessed_batch):\n            sample = self.pipeline(unprocessed_sample.copy())\n            self._place_sample(sample['inputs'], sample_arg, inputs)\n            self._place_sample(sample['labels'], sample_arg, labels)\n        return inputs, labels",
  "class GeneratingSequence(SequenceExtra):\n    \"\"\"Sequence generator used for generating samples.\n\n    # Arguments\n        processor: Function used for generating and processing ``samples``.\n        batch_size: Int.\n        num_steps: Int. Number of steps for each epoch.\n        as_list: Bool, if True ``inputs`` and ``labels`` are dispatched as\n            lists. If false ``inputs`` and ``labels`` are dispatched as\n            dictionaries.\n    \"\"\"\n    def __init__(self, processor, batch_size, num_steps, as_list=False):\n        self.num_steps = num_steps\n        super(GeneratingSequence, self).__init__(\n            processor, batch_size, as_list)\n\n    def __len__(self):\n        return self.num_steps\n\n    def process_batch(self, inputs, labels, batch_index):\n        for sample_arg in range(self.batch_size):\n            sample = self.pipeline()\n            self._place_sample(sample['inputs'], sample_arg, inputs)\n            self._place_sample(sample['labels'], sample_arg, labels)\n        return inputs, labels",
  "def __init__(self, pipeline, batch_size, as_list=False):\n        if not isinstance(pipeline, SequentialProcessor):\n            raise ValueError('``processor`` must be a ``SequentialProcessor``')\n        self.output_wrapper = pipeline.processors[-1]\n        self.pipeline = pipeline\n        self.inputs_name_to_shape = self.output_wrapper.inputs_name_to_shape\n        self.labels_name_to_shape = self.output_wrapper.labels_name_to_shape\n        self.ordered_input_names = self.output_wrapper.ordered_input_names\n        self.ordered_label_names = self.output_wrapper.ordered_label_names\n        self.batch_size = batch_size\n        self.as_list = as_list",
  "def make_empty_batches(self, name_to_shape):\n        batch = {}\n        for name, shape in name_to_shape.items():\n            batch[name] = np.zeros((self.batch_size, *shape))\n        return batch",
  "def _to_list(self, batch, names):\n        return [batch[name] for name in names]",
  "def _place_sample(self, sample, sample_arg, batch):\n        for name, data in sample.items():\n            batch[name][sample_arg] = data",
  "def _get_unprocessed_batch(self, data, batch_index):\n        batch_arg_A = self.batch_size * (batch_index)\n        batch_arg_B = self.batch_size * (batch_index + 1)\n        unprocessed_batch = data[batch_arg_A:batch_arg_B]\n        return unprocessed_batch",
  "def __getitem__(self, batch_index):\n        inputs = self.make_empty_batches(self.inputs_name_to_shape)\n        labels = self.make_empty_batches(self.labels_name_to_shape)\n        inputs, labels = self.process_batch(inputs, labels, batch_index)\n        if self.as_list:\n            inputs = self._to_list(inputs, self.ordered_input_names)\n            labels = self._to_list(labels, self.ordered_label_names)\n        return inputs, labels",
  "def process_batch(self, inputs, labels, batch_index=None):\n        raise NotImplementedError",
  "def __init__(self, processor, batch_size, data, as_list=False):\n        self.data = data\n        super(ProcessingSequence, self).__init__(\n            processor, batch_size, as_list)",
  "def __len__(self):\n        return int(np.ceil(len(self.data) / float(self.batch_size)))",
  "def process_batch(self, inputs, labels, batch_index):\n        unprocessed_batch = self._get_unprocessed_batch(self.data, batch_index)\n\n        for sample_arg, unprocessed_sample in enumerate(unprocessed_batch):\n            sample = self.pipeline(unprocessed_sample.copy())\n            self._place_sample(sample['inputs'], sample_arg, inputs)\n            self._place_sample(sample['labels'], sample_arg, labels)\n        return inputs, labels",
  "def __init__(self, processor, batch_size, num_steps, as_list=False):\n        self.num_steps = num_steps\n        super(GeneratingSequence, self).__init__(\n            processor, batch_size, as_list)",
  "def __len__(self):\n        return self.num_steps",
  "def process_batch(self, inputs, labels, batch_index):\n        for sample_arg in range(self.batch_size):\n            sample = self.pipeline()\n            self._place_sample(sample['inputs'], sample_arg, inputs)\n            self._place_sample(sample['labels'], sample_arg, labels)\n        return inputs, labels",
  "class Box2D(object):\n    \"\"\"Bounding box 2D coordinates with class label and score.\n\n    # Properties\n        coordinates: List of float/integers indicating the\n            ``[x_min, y_min, x_max, y_max]`` coordinates.\n        score: Float. Indicates the score of label associated to the box.\n        class_name: String indicating the class label name of the object.\n\n    # Methods\n        contains()\n    \"\"\"\n    def __init__(self, coordinates, score, class_name=None):\n        x_min, y_min, x_max, y_max = coordinates\n        self.coordinates = coordinates\n        self.class_name = class_name\n        self.score = score\n\n    @property\n    def coordinates(self):\n        return self._coordinates\n\n    @coordinates.setter\n    def coordinates(self, coordinates):\n        x_min, y_min, x_max, y_max = coordinates\n        if x_min >= x_max:\n            raise ValueError('Invalid coordinate input x_min >= x_max')\n        if y_min >= y_max:\n            raise ValueError('Invalid coordinate input y_min >= y_max')\n\n        self._coordinates = coordinates\n\n    @property\n    def class_name(self):\n        return self._class_name\n\n    @class_name.setter\n    def class_name(self, class_name):\n        self._class_name = class_name\n\n    @property\n    def score(self):\n        return self._score\n\n    @score.setter\n    def score(self, score):\n        self._score = score\n\n    @property\n    def center(self):\n        x_center = (self._coordinates[0] + self._coordinates[2]) / 2.0\n        y_center = (self._coordinates[1] + self._coordinates[3]) / 2.0\n        return x_center, y_center\n\n    @property\n    def width(self):\n        return abs(self.coordinates[2] - self.coordinates[0])\n\n    @property\n    def height(self):\n        return abs(self.coordinates[3] - self.coordinates[1])\n\n    def __repr__(self):\n        return \"Box2D({}, {}, {}, {}, {}, {})\".format(\n            self.coordinates[0], self.coordinates[1],\n            self.coordinates[2], self.coordinates[3],\n            self.score, self.class_name)\n\n    def contains(self, point):\n        \"\"\"Checks if point is inside bounding box.\n\n        # Arguments\n            point: Numpy array of size 2.\n\n        # Returns\n            Boolean. 'True' if 'point' is inside bounding box.\n                'False' otherwise.\n        \"\"\"\n        assert len(point) == 2\n        x_min, y_min, x_max, y_max = self.coordinates\n        inside_range_x = (point[0] >= x_min) and (point[0] <= x_max)\n        inside_range_y = (point[1] >= y_min) and (point[1] <= y_max)\n        return (inside_range_x and inside_range_y)",
  "class Pose6D(object):\n    \"\"\" Pose estimation results with 6D coordinates.\n\n        # Properties\n            quaternion: List of 4 floats indicating (w, x, y, z) components.\n            translation: List of 3 floats indicating (x, y, z)\n                translation components.\n            class_name: String or ``None`` indicating the class label name of\n                the object.\n\n        # Class Methods\n            from_rotation_vector: Instantiates a ``Pose6D`` object using a\n                rotation and a translation vector.\n    \"\"\"\n    def __init__(self, quaternion, translation, class_name=None):\n        self.quaternion = quaternion\n        self.translation = translation\n        self.class_name = class_name\n\n    @property\n    def quaternion(self):\n        return self._quaternion\n\n    @quaternion.setter\n    def quaternion(self, coordinates):\n        self._quaternion = coordinates\n\n    @property\n    def translation(self):\n        return self._translation\n\n    @translation.setter\n    def translation(self, coordinates):\n        self._translation = coordinates\n\n    @property\n    def class_name(self):\n        return self._class_name\n\n    @class_name.setter\n    def class_name(self, class_name):\n        self._class_name = class_name\n\n    @classmethod\n    def from_rotation_vector(\n            cls, rotation_vector, translation, class_name=None):\n        quaternion = rotation_vector_to_quaternion(rotation_vector)\n        pose6D = cls(quaternion, translation, class_name)\n        pose6D.rotation_vector = rotation_vector\n        return pose6D\n\n    def __repr__(self):\n        quaternion_message = ' Quaternion: ({}, {}, {}, {}) '.format(\n            self.quaternion[0], self.quaternion[1],\n            self.quaternion[2], self.quaternion[3])\n        translation_message = ' Translation: ({}, {}, {}) '.format(\n            self.translation[0], self.translation[1], self.translation[2])\n        pose_message = ['Pose6D: ', quaternion_message, translation_message]\n        pose_message = '\\n \\t'.join(pose_message)\n        return pose_message",
  "class Keypoint3D(object):\n    def __init__(self, coordinates, class_name=None):\n        coordinates = coordinates\n        class_name = class_name\n\n    @property\n    def coordinates(self, coordinates):\n        return self._coordinates\n\n    @coordinates.setter\n    def coordinates(self, coordinates):\n        num_keypoints = len(coordinates)\n        if num_keypoints != 3:\n            raise ValueError('Invalid 3D Keypoint length:', num_keypoints)\n        self._coordinates = coordinates\n\n    def project():\n        raise NotImplementedError\n\n    def unproject():\n        raise NotImplementedError",
  "def __init__(self, coordinates, score, class_name=None):\n        x_min, y_min, x_max, y_max = coordinates\n        self.coordinates = coordinates\n        self.class_name = class_name\n        self.score = score",
  "def coordinates(self):\n        return self._coordinates",
  "def coordinates(self, coordinates):\n        x_min, y_min, x_max, y_max = coordinates\n        if x_min >= x_max:\n            raise ValueError('Invalid coordinate input x_min >= x_max')\n        if y_min >= y_max:\n            raise ValueError('Invalid coordinate input y_min >= y_max')\n\n        self._coordinates = coordinates",
  "def class_name(self):\n        return self._class_name",
  "def class_name(self, class_name):\n        self._class_name = class_name",
  "def score(self):\n        return self._score",
  "def score(self, score):\n        self._score = score",
  "def center(self):\n        x_center = (self._coordinates[0] + self._coordinates[2]) / 2.0\n        y_center = (self._coordinates[1] + self._coordinates[3]) / 2.0\n        return x_center, y_center",
  "def width(self):\n        return abs(self.coordinates[2] - self.coordinates[0])",
  "def height(self):\n        return abs(self.coordinates[3] - self.coordinates[1])",
  "def __repr__(self):\n        return \"Box2D({}, {}, {}, {}, {}, {})\".format(\n            self.coordinates[0], self.coordinates[1],\n            self.coordinates[2], self.coordinates[3],\n            self.score, self.class_name)",
  "def contains(self, point):\n        \"\"\"Checks if point is inside bounding box.\n\n        # Arguments\n            point: Numpy array of size 2.\n\n        # Returns\n            Boolean. 'True' if 'point' is inside bounding box.\n                'False' otherwise.\n        \"\"\"\n        assert len(point) == 2\n        x_min, y_min, x_max, y_max = self.coordinates\n        inside_range_x = (point[0] >= x_min) and (point[0] <= x_max)\n        inside_range_y = (point[1] >= y_min) and (point[1] <= y_max)\n        return (inside_range_x and inside_range_y)",
  "def __init__(self, quaternion, translation, class_name=None):\n        self.quaternion = quaternion\n        self.translation = translation\n        self.class_name = class_name",
  "def quaternion(self):\n        return self._quaternion",
  "def quaternion(self, coordinates):\n        self._quaternion = coordinates",
  "def translation(self):\n        return self._translation",
  "def translation(self, coordinates):\n        self._translation = coordinates",
  "def class_name(self):\n        return self._class_name",
  "def class_name(self, class_name):\n        self._class_name = class_name",
  "def from_rotation_vector(\n            cls, rotation_vector, translation, class_name=None):\n        quaternion = rotation_vector_to_quaternion(rotation_vector)\n        pose6D = cls(quaternion, translation, class_name)\n        pose6D.rotation_vector = rotation_vector\n        return pose6D",
  "def __repr__(self):\n        quaternion_message = ' Quaternion: ({}, {}, {}, {}) '.format(\n            self.quaternion[0], self.quaternion[1],\n            self.quaternion[2], self.quaternion[3])\n        translation_message = ' Translation: ({}, {}, {}) '.format(\n            self.translation[0], self.translation[1], self.translation[2])\n        pose_message = ['Pose6D: ', quaternion_message, translation_message]\n        pose_message = '\\n \\t'.join(pose_message)\n        return pose_message",
  "def __init__(self, coordinates, class_name=None):\n        coordinates = coordinates\n        class_name = class_name",
  "def coordinates(self, coordinates):\n        return self._coordinates",
  "def coordinates(self, coordinates):\n        num_keypoints = len(coordinates)\n        if num_keypoints != 3:\n            raise ValueError('Invalid 3D Keypoint length:', num_keypoints)\n        self._coordinates = coordinates",
  "def project():\n        raise NotImplementedError",
  "def unproject():\n        raise NotImplementedError",
  "class Loader(object):\n    \"\"\"Abstract class for loading a dataset.\n\n    # Arguments\n        path: String. Path to data.\n        split: String. Dataset split e.g. traing, val, test.\n        class_names: List of strings. Label names of the classes.\n        name: String. Dataset name.\n\n    # Properties\n        name: Str.\n        path: Str.\n        split: Str or Flag.\n        class_names: List of strings.\n        num_classes: Int.\n\n    # Methods\n        load_data()\n    \"\"\"\n    def __init__(self, path, split, class_names, name):\n        self.path = path\n        self.split = split\n        self.class_names = class_names\n        self.name = name\n\n    def load_data(self):\n        \"\"\"Abstract method for loading dataset.\n\n        # Returns\n            dictionary containing absolute image paths as keys, and\n            ground truth vectors as values.\n        \"\"\"\n        raise NotImplementedError()\n\n    # Name of the dataset (VOC2007, COCO, OpenImagesV4, etc)\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    #  Path to the dataset, ideally loaded from a configuration file.\n    @property\n    def path(self):\n        return self._path\n\n    @path.setter\n    def path(self, path):\n        self._path = path\n\n    # Kind of split to use, either train, validation, test, or trainval.\n    @property\n    def split(self):\n        return self._split\n\n    @split.setter\n    def split(self, split):\n        self._split = split\n\n    # List of class names to train/test.\n    @property\n    def class_names(self):\n        return self._class_names\n\n    @class_names.setter\n    def class_names(self, class_names):\n        # assert type(class_names) == list\n        self._class_names = class_names\n\n    @property\n    def num_classes(self):\n        if isinstance(self.class_names, list):\n            return len(self.class_names)\n        else:\n            raise ValueError('class names are not a list')",
  "def __init__(self, path, split, class_names, name):\n        self.path = path\n        self.split = split\n        self.class_names = class_names\n        self.name = name",
  "def load_data(self):\n        \"\"\"Abstract method for loading dataset.\n\n        # Returns\n            dictionary containing absolute image paths as keys, and\n            ground truth vectors as values.\n        \"\"\"\n        raise NotImplementedError()",
  "def name(self):\n        return self._name",
  "def name(self, name):\n        self._name = name",
  "def path(self):\n        return self._path",
  "def path(self, path):\n        self._path = path",
  "def split(self):\n        return self._split",
  "def split(self, split):\n        self._split = split",
  "def class_names(self):\n        return self._class_names",
  "def class_names(self, class_names):\n        # assert type(class_names) == list\n        self._class_names = class_names",
  "def num_classes(self):\n        if isinstance(self.class_names, list):\n            return len(self.class_names)\n        else:\n            raise ValueError('class names are not a list')",
  "class Processor(object):\n    \"\"\"Abstract class for creating a processor unit.\n\n    # Arguments\n        name: String indicating name of the processing unit.\n\n    # Methods\n        call()\n\n    # Example\n    ```python\n    class NormalizeImage(Processor):\n    def __init__(self):\n        super(NormalizeImage, self).__init__()\n\n    def call(self, image):\n        return image / 255.0\n    ```\n\n    # Why this name?\n        Originally PAZ was only meant for pre-processing pipelines that\n        included data-augmentation, normalization, etc. However, I found\n        out that we could use the same API for post-processing; therefore,\n        I thought at the time that ``Processor`` would be adequate to describe\n        the capacity of both pre-processing and post-processing.\n        Names that I also thought could have worked were: ``Function``,\n        ``Functor`` but I didn't want to use those since I thought they could\n        also cause confusion. Similarly, in Keras this abstraction is\n        interpreted as a ``Layer`` but here I don't think that abstraction\n        is adequate. A layer of computation maybe? So after having this\n        thoughts swirling around I decided to go with ``Processor``\n        and try to be explicit about my mental jugglery hoping the name\n        doesn't cause much mental overhead.\n    \"\"\"\n    def __init__(self, name=None):\n        self.name = name\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        if name is None:\n            name = self.__class__.__name__\n        self._name = name\n\n    def call(self, X):\n        \"\"\"Custom user's logic should be implemented here.\n        \"\"\"\n        raise NotImplementedError\n\n    def __call__(self, *args, **kwargs):\n        return self.call(*args, **kwargs)",
  "class SequentialProcessor(object):\n    \"\"\"Abstract class for creating a sequential pipeline of processors.\n\n    # Arguments\n        processors: List of instantiated child classes of ``Processor``\n            classes.\n        name: String indicating name of the processing unit.\n\n    # Methods\n        add()\n        remove()\n        pop()\n        insert()\n        get_processor()\n\n    # Example\n    ```python\n    AugmentImage = SequentialProcessor()\n    AugmentImage.add(pr.RandomContrast())\n    AugmentImage.add(pr.RandomBrightness())\n    augment_image = AugmentImage()\n\n    transformed_image = augment_image(image)\n    ```\n    \"\"\"\n    def __init__(self, processors=None, name=None):\n        self.processors = []\n        if processors is not None:\n            [self.add(processor) for processor in processors]\n        self.name = name\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        if name is None:\n            name = self.__class__.__name__\n        self._name = name\n\n    def add(self, processor):\n        \"\"\"Adds a process to the sequence of processes to be applied to input.\n\n        # Arguments\n            processor: An instantiated child class of of ``Processor``.\n        \"\"\"\n        self.processors.append(processor)\n\n    def __call__(self, *args, **kwargs):\n        # first call can take list or dictionary values.\n        args = self.processors[0](*args, **kwargs)\n        # further calls can be a tuple or single values.\n        for processor in self.processors[1:]:\n            if isinstance(args, tuple):\n                args = processor(*args)\n            else:\n                args = processor(args)\n        return args\n\n    def remove(self, name):\n        \"\"\"Removes processor from sequence\n\n        # Arguments\n            name: String indicating the process name\n        \"\"\"\n        for processor in self.processors:\n            if processor.name == name:\n                self.processors.remove(processor)\n\n    def pop(self, index=-1):\n        \"\"\"Pops processor in given index from sequence\n\n        # Arguments\n            index: Int.\n        \"\"\"\n        return self.processors.pop(index)\n\n    def insert(self, index, processor):\n        \"\"\"Inserts ``processor`` to self.processors queue at ``index``\n\n        # Argument\n            index: Int.\n            processor: An instantiated child class of of ``Processor``.\n        \"\"\"\n        return self.processors.insert(index, processor)\n\n    def get_processor(self, name):\n        \"\"\"Gets processor from sequencer\n\n        # Arguments\n            name: String indicating the process name\n        \"\"\"\n        for processor in self.processors:\n            if processor.name == name:\n                return processor",
  "def __init__(self, name=None):\n        self.name = name",
  "def name(self):\n        return self._name",
  "def name(self, name):\n        if name is None:\n            name = self.__class__.__name__\n        self._name = name",
  "def call(self, X):\n        \"\"\"Custom user's logic should be implemented here.\n        \"\"\"\n        raise NotImplementedError",
  "def __call__(self, *args, **kwargs):\n        return self.call(*args, **kwargs)",
  "def __init__(self, processors=None, name=None):\n        self.processors = []\n        if processors is not None:\n            [self.add(processor) for processor in processors]\n        self.name = name",
  "def name(self):\n        return self._name",
  "def name(self, name):\n        if name is None:\n            name = self.__class__.__name__\n        self._name = name",
  "def add(self, processor):\n        \"\"\"Adds a process to the sequence of processes to be applied to input.\n\n        # Arguments\n            processor: An instantiated child class of of ``Processor``.\n        \"\"\"\n        self.processors.append(processor)",
  "def __call__(self, *args, **kwargs):\n        # first call can take list or dictionary values.\n        args = self.processors[0](*args, **kwargs)\n        # further calls can be a tuple or single values.\n        for processor in self.processors[1:]:\n            if isinstance(args, tuple):\n                args = processor(*args)\n            else:\n                args = processor(args)\n        return args",
  "def remove(self, name):\n        \"\"\"Removes processor from sequence\n\n        # Arguments\n            name: String indicating the process name\n        \"\"\"\n        for processor in self.processors:\n            if processor.name == name:\n                self.processors.remove(processor)",
  "def pop(self, index=-1):\n        \"\"\"Pops processor in given index from sequence\n\n        # Arguments\n            index: Int.\n        \"\"\"\n        return self.processors.pop(index)",
  "def insert(self, index, processor):\n        \"\"\"Inserts ``processor`` to self.processors queue at ``index``\n\n        # Argument\n            index: Int.\n            processor: An instantiated child class of of ``Processor``.\n        \"\"\"\n        return self.processors.insert(index, processor)",
  "def get_processor(self, name):\n        \"\"\"Gets processor from sequencer\n\n        # Arguments\n            name: String indicating the process name\n        \"\"\"\n        for processor in self.processors:\n            if processor.name == name:\n                return processor",
  "class Conv2DNormalization(Layer):\n    \"\"\"Normalization layer as described in ParseNet paper.\n\n    # Arguments\n        scale: Float determining how much to scale the features.\n        axis: Integer specifying axis of image channels.\n\n    # Returns\n        Feature map tensor normalized with an L2 norm and then scaled.\n\n    # References\n        - [ParseNet: Looking Wider to\n            See Better](https://arxiv.org/abs/1506.04579)\n    \"\"\"\n    def __init__(self, scale, axis=3, **kwargs):\n        self.scale = scale\n        self.axis = axis\n        super(Conv2DNormalization, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.gamma = self.add_weight(\n            name='gamma', shape=(input_shape[self.axis]),\n            initializer=Constant(self.scale), trainable=True)\n        # super(Conv2DNormalization, self).build(input_shape)\n\n    def output_shape(self, input_shape):\n        return input_shape\n\n    def call(self, x, mask=None):\n        return self.gamma * K.l2_normalize(x, self.axis)",
  "class SubtractScalar(Layer):\n    \"\"\"Subtracts scalar value to tensor.\n\n    # Arguments\n        constant: Float. Value to be subtracted to all tensor values.\n    \"\"\"\n    def __init__(self, constant, **kwargs):\n        self.constant = constant\n        super(SubtractScalar, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(SubtractScalar, self).build(input_shape)\n\n    def call(self, x):\n        return x - self.constant\n\n    def compute_output_shape(self, input_shape):\n        return input_shape",
  "class ExpectedValue2D(Layer):\n    \"\"\"Calculates the expected value along ''axes''.\n\n    # Arguments\n        axes: List of integers. Axes for which the expected value\n            will be calculated.\n    \"\"\"\n    def __init__(self, axes=[2, 3], **kwargs):\n        self.axes = axes\n        super(ExpectedValue2D, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.num_keypoints = input_shape[1]\n        self.feature_map_size = input_shape[2]\n        super(ExpectedValue2D, self).build(input_shape)\n\n    def call(self, x):\n        range_x, range_y = self.meshgrid(self.feature_map_size)\n        expected_x = K.sum(x * range_x, axis=self.axes)\n        expected_y = K.sum(x * range_y, axis=self.axes)\n        keypoints_stack = K.stack([expected_x, expected_y], -1)\n        keypoints = K.reshape(keypoints_stack, [-1, self.num_keypoints, 2])\n        return keypoints\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.num_keypoints, 2)\n\n    def meshgrid(self, feature_map_size):\n        \"\"\" Returns a meshgrid ranging from [-1, 1] in x, y axes.\"\"\"\n        r = np.arange(0.5, feature_map_size, 1) / (feature_map_size / 2) - 1\n        range_x, range_y = tf.meshgrid(r, -r)\n        return K.cast(range_x, 'float32'), K.cast(range_y, 'float32')",
  "class ExpectedDepth(Layer):\n    \"\"\"Calculates the expected depth along ''axes''.\n    This layer takes two inputs. First input is a depth estimation tensor.\n    Second input is a probability map of the keypoints.\n    It multiplies both values and calculates the expected depth.\n\n    # Arguments\n        axes: List of integers. Axes for which the expected value\n            will be calculated.\n    \"\"\"\n    def __init__(self, axes=[2, 3], **kwargs):\n        self.axes = axes\n        super(ExpectedDepth, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.num_keypoints = input_shape[0][1]\n        super(ExpectedDepth, self).build(input_shape)\n\n    def call(self, x):\n        z_volume, uv_volume = x\n        z = K.sum(z_volume * uv_volume, axis=self.axes)\n        z = K.expand_dims(z, -1)\n        return z\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], self.num_keypoints, 1)",
  "def __init__(self, scale, axis=3, **kwargs):\n        self.scale = scale\n        self.axis = axis\n        super(Conv2DNormalization, self).__init__(**kwargs)",
  "def build(self, input_shape):\n        self.gamma = self.add_weight(\n            name='gamma', shape=(input_shape[self.axis]),\n            initializer=Constant(self.scale), trainable=True)",
  "def output_shape(self, input_shape):\n        return input_shape",
  "def call(self, x, mask=None):\n        return self.gamma * K.l2_normalize(x, self.axis)",
  "def __init__(self, constant, **kwargs):\n        self.constant = constant\n        super(SubtractScalar, self).__init__(**kwargs)",
  "def build(self, input_shape):\n        super(SubtractScalar, self).build(input_shape)",
  "def call(self, x):\n        return x - self.constant",
  "def compute_output_shape(self, input_shape):\n        return input_shape",
  "def __init__(self, axes=[2, 3], **kwargs):\n        self.axes = axes\n        super(ExpectedValue2D, self).__init__(**kwargs)",
  "def build(self, input_shape):\n        self.num_keypoints = input_shape[1]\n        self.feature_map_size = input_shape[2]\n        super(ExpectedValue2D, self).build(input_shape)",
  "def call(self, x):\n        range_x, range_y = self.meshgrid(self.feature_map_size)\n        expected_x = K.sum(x * range_x, axis=self.axes)\n        expected_y = K.sum(x * range_y, axis=self.axes)\n        keypoints_stack = K.stack([expected_x, expected_y], -1)\n        keypoints = K.reshape(keypoints_stack, [-1, self.num_keypoints, 2])\n        return keypoints",
  "def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.num_keypoints, 2)",
  "def meshgrid(self, feature_map_size):\n        \"\"\" Returns a meshgrid ranging from [-1, 1] in x, y axes.\"\"\"\n        r = np.arange(0.5, feature_map_size, 1) / (feature_map_size / 2) - 1\n        range_x, range_y = tf.meshgrid(r, -r)\n        return K.cast(range_x, 'float32'), K.cast(range_y, 'float32')",
  "def __init__(self, axes=[2, 3], **kwargs):\n        self.axes = axes\n        super(ExpectedDepth, self).__init__(**kwargs)",
  "def build(self, input_shape):\n        self.num_keypoints = input_shape[0][1]\n        super(ExpectedDepth, self).build(input_shape)",
  "def call(self, x):\n        z_volume, uv_volume = x\n        z = K.sum(z_volume * uv_volume, axis=self.axes)\n        z = K.expand_dims(z, -1)\n        return z",
  "def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], self.num_keypoints, 1)",
  "def SSD300(num_classes=21, base_weights='VOC', head_weights='VOC',\n           input_shape=(300, 300, 3), num_priors=[4, 6, 6, 6, 4, 4],\n           l2_loss=0.0005, return_base=False, trainable_base=True):\n\n    \"\"\"Single-shot-multibox detector for 300x300x3 BGR input images.\n    # Arguments\n        num_classes: Integer. Specifies the number of class labels.\n        base_weights: String or None. If string should be a valid dataset name.\n            Current valid datasets include `VOC` `FAT` and `VGG`.\n        head_weights: String or None. If string should be a valid dataset name.\n            Current valid datasets include `VOC` and `FAT`.\n        input_shape: List of integers. Input shape to the model including only\n            spatial and channel resolution e.g. (300, 300, 3).\n        num_priors: List of integers. Number of default box shapes\n            used in each detection layer.\n        l2_loss: Float. l2 regularization loss for convolutional layers.\n        return_base: Boolean. If `True` the model returned is just\n            the original base.\n        trainable_base: Boolean. If `True` the base model\n            weights are also trained.\n\n    # Reference\n        - [SSD: Single Shot MultiBox\n            Detector](https://arxiv.org/abs/1512.02325)\n    \"\"\"\n\n    if base_weights not in ['VGG', 'VOC', 'FAT', None]:\n        raise ValueError('Invalid `base_weights`:', base_weights)\n\n    if head_weights not in ['VOC', 'FAT', None]:\n        raise ValueError('Invalid `base_weights`:', base_weights)\n\n    if ((base_weights == 'VGG') and (head_weights is not None)):\n        raise NotImplementedError('Invalid `base_weights` with head_weights')\n\n    if ((base_weights is None) and (head_weights is not None)):\n        raise NotImplementedError('Invalid `base_weights` with head_weights')\n\n    if ((num_classes != 21) and (head_weights == 'VOC')):\n        raise ValueError('Invalid `head_weights` with given `num_classes`')\n\n    if ((num_classes != 22) and (head_weights == 'FAT')):\n        raise ValueError('Invalid `head_weights` with given `num_classes`')\n\n    image = Input(shape=input_shape, name='image')\n\n    # Block 1 -----------------------------------------------------------------\n    conv1_1 = Conv2D(64, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv1_1')(image)\n    conv1_2 = Conv2D(64, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv1_2')(conv1_1)\n    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n                         padding='same', )(conv1_2)\n\n    # Block 2 -----------------------------------------------------------------\n    conv2_1 = Conv2D(128, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv2_1')(pool1)\n    conv2_2 = Conv2D(128, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv2_2')(conv2_1)\n    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n                         padding='same')(conv2_2)\n\n    # Block 3 -----------------------------------------------------------------\n    conv3_1 = Conv2D(256, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv3_1')(pool2)\n    conv3_2 = Conv2D(256, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv3_2')(conv3_1)\n    conv3_3 = Conv2D(256, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv3_3')(conv3_2)\n    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n                         padding='same')(conv3_3)\n\n    # Block 4 -----------------------------------------------------------------\n    conv4_1 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv4_1')(pool3)\n    conv4_2 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv4_2')(conv4_1)\n    conv4_3 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv4_3')(conv4_2)\n    conv4_3_norm = Conv2DNormalization(20, name='branch_1')(conv4_3)\n    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n                         padding='same')(conv4_3)\n\n    # Block 5 -----------------------------------------------------------------\n    conv5_1 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv5_1')(pool4)\n    conv5_2 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv5_2')(conv5_1)\n    conv5_3 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv5_3')(conv5_2)\n    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1),\n                         padding='same')(conv5_3)\n\n    # Dense 6/7 --------------------------------------------------------------\n    pool5z = ZeroPadding2D(padding=(6, 6))(pool5)\n    fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6),\n                 padding='valid', activation='relu',\n                 kernel_regularizer=l2(l2_loss),\n                 trainable=trainable_base,\n                 name='fc6')(pool5z)\n\n    fc7 = Conv2D(1024, (1, 1), padding='same',\n                 activation='relu',\n                 kernel_regularizer=l2(l2_loss),\n                 trainable=trainable_base,\n                 name='branch_2')(fc6)\n\n    # EXTRA layers in SSD -----------------------------------------------------\n    # Block 6 -----------------------------------------------------------------\n    conv6_1 = Conv2D(256, (1, 1), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss))(fc7)\n    conv6_1z = ZeroPadding2D()(conv6_1)\n    conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), padding='valid',\n                     activation='relu', name='branch_3',\n                     kernel_regularizer=l2(l2_loss))(conv6_1z)\n\n    # Block 7 -----------------------------------------------------------------\n    conv7_1 = Conv2D(128, (1, 1), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss))(conv6_2)\n    conv7_1z = ZeroPadding2D()(conv7_1)\n    conv7_2 = Conv2D(256, (3, 3), padding='valid', strides=(2, 2),\n                     activation='relu', name='branch_4',\n                     kernel_regularizer=l2(l2_loss))(conv7_1z)\n\n    # Block 8 -----------------------------------------------------------------\n    conv8_1 = Conv2D(128, (1, 1), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss))(conv7_2)\n    conv8_2 = Conv2D(256, (3, 3), padding='valid', strides=(1, 1),\n                     activation='relu', name='branch_5',\n                     kernel_regularizer=l2(l2_loss))(conv8_1)\n\n    # Block 9 -----------------------------------------------------------------\n    conv9_1 = Conv2D(128, (1, 1), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss))(conv8_2)\n    conv9_2 = Conv2D(256, (3, 3), padding='valid', strides=(1, 1),\n                     activation='relu', name='branch_6',\n                     kernel_regularizer=l2(l2_loss))(conv9_1)\n\n    branch_tensors = [conv4_3_norm, fc7, conv6_2, conv7_2, conv8_2, conv9_2]\n    if return_base:\n        outputs = branch_tensors\n    else:\n        outputs = create_multibox_head(\n            branch_tensors, num_classes, num_priors, l2_loss)\n\n    model = Model(inputs=image, outputs=outputs, name='SSD300')\n\n    if ((base_weights is not None) or (head_weights is not None)):\n        model_filename = ['SSD300', str(base_weights), str(head_weights)]\n        model_filename = '_'.join(['-'.join(model_filename), 'weights.hdf5'])\n        weights_path = get_file(model_filename, WEIGHT_PATH + model_filename,\n                                cache_subdir='paz/models')\n        print('Loading %s model weights' % weights_path)\n        finetunning_model_names = ['SSD300-VGG-None_weights.hdf5',\n                                   'SSD300-VOC-None_weights.hdf5']\n        by_name = True if model_filename in finetunning_model_names else False\n        model.load_weights(weights_path, by_name=by_name)\n    model.prior_boxes = create_prior_boxes('VOC')\n    return model",
  "def create_multibox_head(tensors, num_classes, num_priors, l2_loss=0.0005,\n                         num_regressions=4, l2_norm=False, batch_norm=False):\n    \"\"\"Adds multibox head with classification and regression output tensors.\n\n    # Arguments\n        tensors: List of tensors.\n        num_classes: Int. Number of classes.\n        num_priors. List of integers. Length should equal to tensors length.\n            Each integer represents the amount of bounding boxes shapes in\n            each feature map value.\n        l2_loss: Float. L2 loss value to be added to convolutional layers.\n        num_regressions: Number of values to be regressed per prior box.\n            e.g. for 2D bounding boxes we regress 4 coordinates.\n        l2_norm: Boolean. If `True` l2 normalization layer is applied to\n            each before a convolutional layer.\n        batch_norm: Boolean. If `True` batch normalization is applied after\n            each convolutional layer.\n    \"\"\"\n    classification_layers, regression_layers = [], []\n    for layer_arg, base_layer in enumerate(tensors):\n        if l2_norm:\n            base_layer = Conv2DNormalization(20)(base_layer)\n\n        # classification leaf -------------------------------------------------\n        num_kernels = num_priors[layer_arg] * num_classes\n        class_leaf = Conv2D(num_kernels, 3, padding='same',\n                            kernel_regularizer=l2(l2_loss))(base_layer)\n        if batch_norm:\n            class_leaf = BatchNormalization()(class_leaf)\n        class_leaf = Flatten()(class_leaf)\n        classification_layers.append(class_leaf)\n\n        # regression leaf -----------------------------------------------------\n        num_kernels = num_priors[layer_arg] * num_regressions\n        regress_leaf = Conv2D(num_kernels, 3, padding='same',\n                              kernel_regularizer=l2(l2_loss))(base_layer)\n        if batch_norm:\n            regress_leaf = BatchNormalization()(regress_leaf)\n\n        regress_leaf = Flatten()(regress_leaf)\n        regression_layers.append(regress_leaf)\n\n    classifications = Concatenate(axis=1)(classification_layers)\n    regressions = Concatenate(axis=1)(regression_layers)\n    num_boxes = K.int_shape(regressions)[-1] // num_regressions\n    classifications = Reshape((num_boxes, num_classes))(classifications)\n    classifications = Activation('softmax')(classifications)\n    regressions = Reshape((num_boxes, num_regressions))(regressions)\n    outputs = Concatenate(\n        axis=2, name='boxes')([regressions, classifications])\n    return outputs",
  "def create_prior_boxes(configuration_name='VOC'):\n    configuration = get_prior_box_configuration(configuration_name)\n    image_size = configuration['image_size']\n    feature_map_sizes = configuration['feature_map_sizes']\n    min_sizes = configuration['min_sizes']\n    max_sizes = configuration['max_sizes']\n    steps = configuration['steps']\n    model_aspect_ratios = configuration['aspect_ratios']\n    mean = []\n    for feature_map_arg, feature_map_size in enumerate(feature_map_sizes):\n        step = steps[feature_map_arg]\n        min_size = min_sizes[feature_map_arg]\n        max_size = max_sizes[feature_map_arg]\n        aspect_ratios = model_aspect_ratios[feature_map_arg]\n        for y, x in product(range(feature_map_size), repeat=2):\n            f_k = image_size / step\n            center_x = (x + 0.5) / f_k\n            center_y = (y + 0.5) / f_k\n            s_k = min_size / image_size\n            mean = mean + [center_x, center_y, s_k, s_k]\n            s_k_prime = np.sqrt(s_k * (max_size / image_size))\n            mean = mean + [center_x, center_y, s_k_prime, s_k_prime]\n            for aspect_ratio in aspect_ratios:\n                mean = mean + [center_x, center_y, s_k * np.sqrt(aspect_ratio),\n                               s_k / np.sqrt(aspect_ratio)]\n                mean = mean + [center_x, center_y, s_k / np.sqrt(aspect_ratio),\n                               s_k * np.sqrt(aspect_ratio)]\n\n    output = np.asarray(mean).reshape((-1, 4))\n    # output = np.clip(output, 0, 1)\n    return output",
  "def get_prior_box_configuration(configuration_name='VOC'):\n    if configuration_name in {'VOC', 'FAT'}:\n        configuration = {\n            'feature_map_sizes': [38, 19, 10, 5, 3, 1],\n            'image_size': 300,\n            'steps': [8, 16, 32, 64, 100, 300],\n            'min_sizes': [30, 60, 111, 162, 213, 264],\n            'max_sizes': [60, 111, 162, 213, 264, 315],\n            'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n            'variance': [0.1, 0.2]}\n\n    elif configuration_name in {'COCO', 'YCBVideo'}:\n        configuration = {\n            'feature_map_sizes': [64, 32, 16, 8, 4, 2, 1],\n            'image_size': 512,\n            'steps': [8, 16, 32, 64, 128, 256, 512],\n            'min_sizes': [21, 51, 133, 215, 297, 379, 461],\n            'max_sizes': [51, 133, 215, 297, 379, 461, 542],\n            'aspect_ratios': [[2], [2, 3], [2, 3],\n                              [2, 3], [2, 3], [2], [2]],\n            'variance': [0.1, 0.2]}\n    else:\n        raise ValueError('Invalid configuration name:', configuration_name)\n    return configuration",
  "class HaarCascadeDetector(object):\n    \"\"\"Haar cascade face detector.\n\n    # Arguments\n        path: String. Postfix to default openCV haarcascades XML files, see [1]\n            e.g. `eye`, `frontalface_alt2`, `fullbody`\n        class_arg: Int. Class label argument.\n        scale = Float. Scale for image reduction\n        neighbors: Int. Minimum neighbors\n\n    # Reference\n        - [Haar\n            Cascades](https://github.com/opencv/opencv/tree/master/data/haarcascades)\n    \"\"\"\n\n    def __init__(self, weights='frontalface_default', class_arg=None,\n                 scale=1.3, neighbors=5):\n        self.weights = weights\n        self.name = 'haarcascade_' + weights + '.xml'\n        self.url = WEIGHT_PATH + self.name\n        self.path = get_file(self.name, self.url, cache_subdir='paz/models')\n        self.model = cv2.CascadeClassifier(self.path)\n        self.class_arg = class_arg\n        self.scale = scale\n        self.neighbors = neighbors\n\n    def __call__(self, gray_image):\n        \"\"\" Detects faces from gray images.\n\n        # Arguments\n            gray_image: Numpy array of shape ``(H, W, 2)``.\n\n        # Returns\n            Numpy array of shape ``(num_boxes, 4)``.\n        \"\"\"\n        if len(gray_image.shape) != 2:\n            raise ValueError('Invalid gray image shape:', gray_image.shape)\n        args = (gray_image, self.scale, self.neighbors)\n        boxes = self.model.detectMultiScale(*args)\n        boxes_point_form = np.zeros_like(boxes)\n        if len(boxes) != 0:\n            boxes_point_form[:, 0] = boxes[:, 0]\n            boxes_point_form[:, 1] = boxes[:, 1]\n            boxes_point_form[:, 2] = boxes[:, 0] + boxes[:, 2]\n            boxes_point_form[:, 3] = boxes[:, 1] + boxes[:, 3]\n            if self.class_arg is not None:\n                class_args = np.ones((len(boxes_point_form), 1))\n                class_args = class_args * self.class_arg\n                boxes_point_form = np.hstack((boxes_point_form, class_args))\n        return boxes_point_form.astype('int')",
  "def __init__(self, weights='frontalface_default', class_arg=None,\n                 scale=1.3, neighbors=5):\n        self.weights = weights\n        self.name = 'haarcascade_' + weights + '.xml'\n        self.url = WEIGHT_PATH + self.name\n        self.path = get_file(self.name, self.url, cache_subdir='paz/models')\n        self.model = cv2.CascadeClassifier(self.path)\n        self.class_arg = class_arg\n        self.scale = scale\n        self.neighbors = neighbors",
  "def __call__(self, gray_image):\n        \"\"\" Detects faces from gray images.\n\n        # Arguments\n            gray_image: Numpy array of shape ``(H, W, 2)``.\n\n        # Returns\n            Numpy array of shape ``(num_boxes, 4)``.\n        \"\"\"\n        if len(gray_image.shape) != 2:\n            raise ValueError('Invalid gray image shape:', gray_image.shape)\n        args = (gray_image, self.scale, self.neighbors)\n        boxes = self.model.detectMultiScale(*args)\n        boxes_point_form = np.zeros_like(boxes)\n        if len(boxes) != 0:\n            boxes_point_form[:, 0] = boxes[:, 0]\n            boxes_point_form[:, 1] = boxes[:, 1]\n            boxes_point_form[:, 2] = boxes[:, 0] + boxes[:, 2]\n            boxes_point_form[:, 3] = boxes[:, 1] + boxes[:, 3]\n            if self.class_arg is not None:\n                class_args = np.ones((len(boxes_point_form), 1))\n                class_args = class_args * self.class_arg\n                boxes_point_form = np.hstack((boxes_point_form, class_args))\n        return boxes_point_form.astype('int')",
  "def SSD512(num_classes=81, base_weights='COCO', head_weights='COCO',\n           input_shape=(512, 512, 3), num_priors=[4, 6, 6, 6, 6, 4, 4],\n           l2_loss=0.0005, return_base=False, trainable_base=True):\n    \"\"\"Single-shot-multibox detector for 512x512x3 BGR input images.\n    # Arguments\n        num_classes: Integer. Specifies the number of class labels.\n        base_weights: String or None. If string should be a valid dataset name.\n            Current valid datasets include `COCO` and `OIV6Hand`.\n        head_weights: String or None. If string should be a valid dataset name.\n            Current valid datasets include `COCO`, `YCBVideo` and `OIV6Hand`.\n        input_shape: List of integers. Input shape to the model including only\n            spatial and channel resolution e.g. (512, 512, 3).\n        num_priors: List of integers. Number of default box shapes\n            used in each detection layer.\n        l2_loss: Float. l2 regularization loss for convolutional layers.\n        return_base: Boolean. If `True` the model returned is just\n            the original base.\n        trainable_base: Boolean. If `True` the base model\n            weights are also trained.\n\n    # Reference\n        - [SSD: Single Shot MultiBox\n            Detector](https://arxiv.org/abs/1512.02325)\n    \"\"\"\n\n    if base_weights not in ['COCO', 'OIV6Hand', None]:\n        raise ValueError('Invalid `base_weights`:', base_weights)\n\n    if head_weights not in ['COCO', 'YCBVideo', 'OIV6Hand', None]:\n        raise ValueError('Invalid `head_weights`:', head_weights)\n\n    if ((base_weights == 'OIV6Hand') and (head_weights != 'OIV6Hand')):\n        raise NotImplementedError('Invalid `base_weights` with head_weights')\n\n    if ((num_classes != 81) and (head_weights == 'COCO')):\n        raise ValueError('Invalid `head_weights` with given `num_classes`')\n\n    if ((num_classes != 22) and (head_weights == 'YCBVideo')):\n        raise ValueError('Invalid `head_weights` with given `num_classes`')\n\n    if ((num_classes != 2) and (head_weights == 'OIV6Hand')):\n        raise ValueError('Invalid `head_weights` with given `num_classes`')\n\n    image = Input(shape=input_shape, name='image')\n\n    # Block 1 -----------------------------------------------------------------\n    conv1_1 = Conv2D(64, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv1_1')(image)\n    conv1_2 = Conv2D(64, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv1_2')(conv1_1)\n    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n                         padding='same', )(conv1_2)\n\n    # Block 2 -----------------------------------------------------------------\n    conv2_1 = Conv2D(128, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv2_1')(pool1)\n    conv2_2 = Conv2D(128, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv2_2')(conv2_1)\n    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n                         padding='same')(conv2_2)\n\n    # Block 3 -----------------------------------------------------------------\n    conv3_1 = Conv2D(256, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv3_1')(pool2)\n    conv3_2 = Conv2D(256, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv3_2')(conv3_1)\n    conv3_3 = Conv2D(256, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv3_3')(conv3_2)\n    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n                         padding='same')(conv3_3)\n\n    # Block 4 -----------------------------------------------------------------\n    conv4_1 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv4_1')(pool3)\n    conv4_2 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv4_2')(conv4_1)\n    conv4_3 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv4_3')(conv4_2)\n    conv4_3_norm = Conv2DNormalization(20, name='branch_1')(conv4_3)\n    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n                         padding='same')(conv4_3)\n\n    # Block 5 -----------------------------------------------------------------\n    conv5_1 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv5_1')(pool4)\n    conv5_2 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv5_2')(conv5_1)\n    conv5_3 = Conv2D(512, (3, 3), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     trainable=trainable_base,\n                     name='conv5_3')(conv5_2)\n    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1),\n                         padding='same')(conv5_3)\n\n    # Dense 6/7 --------------------------------------------------------------\n    pool5z = ZeroPadding2D(padding=(6, 6))(pool5)\n    fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6),\n                 padding='valid', activation='relu',\n                 kernel_regularizer=l2(l2_loss),\n                 trainable=trainable_base,\n                 name='fc6')(pool5z)\n    fc7 = Conv2D(1024, (1, 1), padding='same',\n                 activation='relu',\n                 kernel_regularizer=l2(l2_loss),\n                 trainable=trainable_base,\n                 name='branch_2')(fc6)\n\n    # EXTRA layers in SSD -----------------------------------------------------\n\n    # Block 6 -----------------------------------------------------------------\n    conv6_1 = Conv2D(256, (1, 1), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     name='conv6_1')(fc7)\n    conv6_1z = ZeroPadding2D()(conv6_1)\n    conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), padding='valid',\n                     activation='relu', name='branch_3',\n                     kernel_regularizer=l2(l2_loss))(conv6_1z)\n\n    # Block 7 -----------------------------------------------------------------\n    conv7_1 = Conv2D(128, (1, 1), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     name='conv7_1')(conv6_2)\n    conv7_1z = ZeroPadding2D()(conv7_1)\n    conv7_2 = Conv2D(256, (3, 3), padding='valid', strides=(2, 2),\n                     activation='relu', name='branch_4',\n                     kernel_regularizer=l2(l2_loss))(conv7_1z)\n\n    # Block 8 -----------------------------------------------------------------\n    conv8_1 = Conv2D(128, (1, 1), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     name='conv8_1')(conv7_2)\n    conv8_1z = ZeroPadding2D()(conv8_1)\n    conv8_2 = Conv2D(256, (3, 3), padding='valid', strides=(2, 2),\n                     activation='relu', name='branch_5',\n                     kernel_regularizer=l2(l2_loss))(conv8_1z)\n\n    # Block 9 -----------------------------------------------------------------\n    conv9_1 = Conv2D(128, (1, 1), padding='same', activation='relu',\n                     kernel_regularizer=l2(l2_loss),\n                     name='conv9_1')(conv8_2)\n    conv9_1z = ZeroPadding2D()(conv9_1)\n    conv9_2 = Conv2D(256, (3, 3), padding='valid', strides=(2, 2),\n                     activation='relu', name='branch_6',\n                     kernel_regularizer=l2(l2_loss))(conv9_1z)\n\n    # Block 10 ----------------------------------------------------------------\n    conv10_1 = Conv2D(128, (1, 1), padding='same', activation='relu',\n                      kernel_regularizer=l2(l2_loss),\n                      name='conv10_1')(conv9_2)\n    conv10_1z = ZeroPadding2D()(conv10_1)\n    conv10_2 = Conv2D(256, (4, 4), padding='valid', strides=(1, 1),\n                      activation='relu', name='branch_7',\n                      kernel_regularizer=l2(l2_loss))(conv10_1z)\n\n    branch_tensors = [conv4_3_norm, fc7, conv6_2, conv7_2,\n                      conv8_2, conv9_2, conv10_2]\n    if return_base:\n        output_tensor = branch_tensors\n\n    else:\n        output_tensor = create_multibox_head(\n            branch_tensors, num_classes, num_priors, l2_loss)\n\n    model = Model(inputs=image, outputs=output_tensor, name='SSD512')\n\n    if ((base_weights is not None) or (head_weights is not None)):\n        model_filename = [str(base_weights), str(head_weights)]\n        model_filename = '_'.join(['SSD512', '-'.join(model_filename),\n                                   'weights.hdf5'])\n        weights_path = get_file(model_filename, WEIGHT_PATH + model_filename,\n                                cache_subdir='paz/models')\n        print('Loading %s model weights' % weights_path)\n\n        model.load_weights(weights_path)\n\n    model.prior_boxes = create_prior_boxes('COCO')\n    return model",
  "def EFFICIENTNET(image, scaling_coefficients, D_divisor=8, excite_ratio=0.25,\n                 kernel_sizes=[3, 3, 5, 3, 5, 5, 3],\n                 repeats=[1, 2, 2, 3, 3, 4, 1],\n                 intro_filters=[32, 16, 24, 40, 80, 112, 192],\n                 outro_filters=[16, 24, 40, 80, 112, 192, 320],\n                 expand_ratios=[1, 6, 6, 6, 6, 6, 6],\n                 strides=[[1, 1], [2, 2], [2, 2], [2, 2],\n                          [1, 1], [2, 2], [1, 1]]):\n    \"\"\"A function implementing EfficientNet.\n\n    # Arguments\n        image: Tensor of shape `(batch_size, input_shape)`, input image.\n        scaling_coefficients: List, EfficientNet scaling coefficients.\n        D_divisor: Int, network depth divisor.\n        excite_ratio: Float, block's squeeze excite ratio.\n        kernel_sizes: List, kernel sizes.\n        repeats: Int, number of block repeats.\n        intro_filters: Int, block's input filters.\n        outro_filters: Int, block's output filters.\n        expand_ratios: Int, MBConv block expansion ratio.\n        strides: List, filter strides.\n\n    # Returns\n        x: List, output features.\n\n    # Raises\n        ValueError: when repeats is not greater than zero.\n\n    # References\n        [EfficientNet: Rethinking Model Scaling for\n         Convolutional Neural Networks]\n        (https://arxiv.org/pdf/1905.11946.pdf)\n    \"\"\"\n    assert (repeats > np.zeros_like(repeats)).sum() == len(repeats)\n\n    W_coefficient, D_coefficient, survival_rate = scaling_coefficients\n    x = conv_block(image, intro_filters, W_coefficient, D_divisor)\n    x = MBconv_blocks(\n        x, kernel_sizes, intro_filters, outro_filters,\n        W_coefficient, D_coefficient, D_divisor, repeats,\n        excite_ratio, survival_rate, strides, expand_ratios)\n    return x",
  "def conv_block(image, intro_filters, width_coefficient, depth_divisor):\n    \"\"\"Builds EfficientNet's first convolutional layer.\n\n    # Arguments\n        image: Tensor of shape `(batch_size, input_shape)`, input image.\n        intro_filters: Int, block's input filters.\n        width_coefficient: Float, width coefficient.\n        depth_divisor: Int, network depth divisor.\n\n    # Returns\n        x: Tensor, output features.\n    \"\"\"\n    filters = scale_filters(intro_filters[0], width_coefficient, depth_divisor)\n    x = Conv2D(filters, [3, 3], [2, 2], 'same', 'channels_last', [1, 1], 1,\n               None, False, kernel_initializer)(image)\n    x = BatchNormalization()(x)\n    x = tf.nn.swish(x)\n    return x",
  "def scale_filters(filters, width_coefficient, depth_divisor):\n    \"\"\"Scales filters using depth divisor.\n\n    # Arguments\n        filters: Int, filters to be rounded.\n        width_coefficient: Float, width coefficient.\n        depth_divisor: Int, network depth divisor.\n\n    # Returns\n        scaled_filters: Int, scaled filters.\n    \"\"\"\n    filters_scaled_by_width = filters * width_coefficient\n    half_depth = depth_divisor / 2\n    filters_rounded = int(filters_scaled_by_width + half_depth)\n    filters_standardized = filters_rounded // depth_divisor\n    threshold = filters_standardized * depth_divisor\n    scaled_filters = int(max(depth_divisor, threshold))\n    if scaled_filters < 0.9 * filters_scaled_by_width:\n        scaled_filters = int(scaled_filters + depth_divisor)\n    return scaled_filters",
  "def kernel_initializer(shape, dtype=None):\n    \"\"\"Initialize convolutional kernel with\n    zero centred Gaussian distribution.\n\n    # Arguments\n        shape: variable shape.\n        dtype: variable dtype.\n\n    # Returns\n        variable initialization.\n    \"\"\"\n    kernel_height, kernel_width, _, outro_filters = shape\n    fan_output = int(kernel_height * kernel_width * outro_filters)\n    return tf.random.normal(shape, 0.0, np.sqrt(2.0 / fan_output), dtype)",
  "def MBconv_blocks(x, kernel_sizes, intro_filters, outro_filters, W_coefficient,\n                  D_coefficient, D_divisor, repeats, excite_ratio,\n                  survival_rate, strides, expand_ratios):\n    \"\"\"Builds EfficientNet's MBConv blocks.\n    MBConv stands for Mobile Inverted Bottleneck Convolution.\n\n    # Arguments\n        x: Tensor, input features.\n        kernel_sizes: List, kernel sizes.\n        intro_filters: Int, block's input filters.\n        outro_filters: Int, block's output filters.\n        W_coefficient: Float, width coefficient.\n        D_coefficient: Float, network depth scaling coefficient.\n        D_divisor: Int, network depth divisor.\n        repeats: Int, number of block repeats.\n        excite_ratio: Float, block's squeeze excite ratio.\n        survival_rate: Float, survival probability to drop features.\n        strides: List, filter strides.\n        expand_ratios: List, MBConv block's expansion ratio.\n\n    # Returns\n        feature_maps: List, of output features.\n    \"\"\"\n    feature_append_mask = [stride[0] == 2 for stride in strides[1:]]\n    feature_append_mask.append(True)\n\n    intro_filters = [scale_filters(intro_filter, W_coefficient, D_divisor)\n                     for intro_filter in intro_filters]\n    outro_filters = [scale_filters(outro_filter, W_coefficient, D_divisor)\n                     for outro_filter in outro_filters]\n    repeats = [round_repeats(repeat, D_coefficient) for repeat in repeats]\n    excite_ratios = [excite_ratio] * len(outro_filters)\n    survival_rates = [survival_rate] * len(outro_filters)\n\n    iterator_1 = list(zip(intro_filters, outro_filters, strides, repeats))\n    iterator_2 = list(zip(kernel_sizes, survival_rates, expand_ratios,\n                          excite_ratios))\n    feature_maps = []\n    for feature_arg, args in enumerate(zip(iterator_1, iterator_2)):\n        repeat_args, block_args = args\n        x = MB_repeat(x, *repeat_args, block_args)\n        if feature_append_mask[feature_arg]:\n            feature_maps.append(x)\n    return feature_maps",
  "def round_repeats(repeats, depth_coefficient):\n    \"\"\"Round number of block repeats using depth divisor.\n\n    # Arguments\n        repeats: Int, number of multiplier blocks.\n        depth_coefficient: Float, network depth scaling coefficient.\n\n    # Returns\n        Int: Rounded block repeats.\n    \"\"\"\n    return int(math.ceil(depth_coefficient * repeats))",
  "def MB_repeat(x, intro_filter, outro_filter, stride, repeats, block_args):\n    \"\"\"Computes given MBConv block's features.\n\n    # Arguments\n        x: Tensor, input features.\n        intro_filter: Int, block's input filter.\n        outro_filter: Int, block's output filter.\n        stride: Int, filter strides.\n        repeats: Int, number of block repeats.\n        block_args: Tuple, holding kernel_sizes, survival_rates,\n            expand_ratios, excite_ratios.\n\n    # Returns\n        Tensor: Output features.\n    \"\"\"\n    for _ in range(repeats):\n        x = MB_block(x, intro_filter, outro_filter, stride, *block_args)\n        intro_filter, stride = outro_filter, [1, 1]\n    return x",
  "def MB_block(inputs, intro_filters, outro_filters, strides, kernel_size,\n             survival_rate, expand_ratio, excite_ratio):\n    \"\"\"Initialize Mobile Inverted Residual Bottleneck block.\n\n    # Arguments\n        inputs: Tensor, input features to MB block.\n        intro_filters: Int, block's input filters.\n        outro_filters: Int, block's output filters.\n        strides: List, conv block filter strides.\n        kernel_size: Int, conv block kernel size.\n        survival_rate: Float, survival probability to drop features.\n        expand_ratio: Int, conv block expansion ratio.\n        excite_ratio: Float, squeeze excite block ratio.\n\n    # Returns\n        x: Tensor, output features.\n\n    # References\n        [MobileNetV2: Inverted Residuals and Linear Bottlenecks]\n        (https://arxiv.org/pdf/1801.04381.pdf)\n        [EfficientNet: Rethinking Model Scaling for\n         Convolutional Neural Networks]\n        (https://arxiv.org/pdf/1905.11946.pdf)\n    \"\"\"\n    filters = intro_filters * expand_ratio\n    x = MB_input(inputs, filters, expand_ratio)\n    x = MB_convolution(x, kernel_size, strides)\n    x = MB_squeeze_excitation(x, intro_filters, expand_ratio, excite_ratio)\n    x = MB_output(x, inputs, intro_filters, outro_filters, strides,\n                  survival_rate)\n    return x",
  "def MB_input(inputs, filters, expand_ratio):\n    if expand_ratio != 1:\n        x = MB_conv2D(inputs, filters, use_bias=False)\n        x = BatchNormalization()(x)\n        x = tf.nn.swish(x)\n    else:\n        x = inputs\n    return x",
  "def MB_conv2D(x, filters, use_bias=False):\n    kwargs = {'padding': 'same', 'kernel_initializer': kernel_initializer}\n    return Conv2D(filters, 1, use_bias=use_bias, **kwargs)(x)",
  "def MB_convolution(x, kernel_size, strides):\n    kwargs = {'padding': 'same', 'depthwise_initializer': kernel_initializer}\n    x = DepthwiseConv2D(kernel_size, strides, use_bias=False, **kwargs)(x)\n    x = BatchNormalization()(x)\n    x = tf.nn.swish(x)\n    return x",
  "def MB_squeeze_excitation(x, intro_filters, expand_ratio, excite_ratio):\n    num_reduced_filters = max(1, int(intro_filters * excite_ratio))\n    SE = tf.reduce_mean(x, [1, 2], keepdims=True)\n    SE = MB_conv2D(SE, num_reduced_filters, use_bias=True)\n    SE = tf.nn.swish(SE)\n    SE = MB_conv2D(SE, intro_filters * expand_ratio, use_bias=True)\n    SE = tf.sigmoid(SE)\n    return SE * x",
  "def MB_output(x, inputs, intro_filters, outro_filters, strides, survival_rate):\n    x = MB_conv2D(x, outro_filters, use_bias=False)\n    x = BatchNormalization()(x)\n    all_strides_one = all(stride == 1 for stride in strides)\n    if all_strides_one and intro_filters == outro_filters:\n        if survival_rate:\n            x = apply_drop_connect(x, False, survival_rate)\n        x = tf.add(x, inputs)\n    return x",
  "def apply_drop_connect(x, is_training, survival_rate):\n    \"\"\"Drops conv with given survival probability.\n\n    # Arguments\n        x: Tensor, input feature map to undergo drop connection.\n        is_training: Bool specifying training phase.\n        survival_rate: Float, survival probability to drop features.\n\n    # Returns\n        output: Tensor, output feature map after drop connect.\n\n    # References\n        [Deep Networks with Stochastic Depth]\n        (https://arxiv.org/pdf/1603.09382.pdf)\n    \"\"\"\n    if not is_training:\n        output = x\n    else:\n        batch_size = tf.shape(x)[0]\n        kwargs = {\"shape\": [batch_size, 1, 1, 1], \"dtype\": x.dtype}\n        random_tensor = survival_rate + tf.random.uniform(**kwargs)\n        binary_tensor = tf.floor(random_tensor)\n        output = (x * binary_tensor) / survival_rate\n    return output",
  "def EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                 FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                 anchor_scale, fusion, return_base, model_name, EfficientNet,\n                 num_scales=3, aspect_ratios=[1.0, 2.0, 0.5],\n                 survival_rate=None, num_dims=4):\n    \"\"\"Creates EfficientDet model.\n\n    # Arguments\n        image: Tensor of shape `(batch_size, input_shape)`.\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        EfficientNet: List, containing branch tensors.\n        num_scales: Int, number of anchor box scales.\n        aspect_ratios: List, anchor boxes aspect ratios.\n        survival_rate: Float, specifying survival probability.\n        num_dims: Int, number of output dimensions to regress.\n\n    # Returns\n        model: EfficientDet model.\n\n    # References\n        [Google AutoML repository implementation of EfficientDet](\n        https://github.com/google/automl/tree/master/efficientdet)\n    \"\"\"\n    if base_weights not in ['COCO', 'VOC', None]:\n        raise ValueError('Invalid base_weights: ', base_weights)\n    if head_weights not in ['COCO', 'VOC', None]:\n        raise ValueError('Invalid head_weights: ', head_weights)\n    if (base_weights is None) and (head_weights == 'COCO'):\n        raise NotImplementedError('Invalid `base_weights` with head_weights')\n    if (base_weights is None) and (head_weights == 'VOC'):\n        raise NotImplementedError('Invalid `base_weights` with head_weights')\n\n    branches, middles, skips = EfficientNet_to_BiFPN(\n        EfficientNet, FPN_num_filters)\n    for _ in range(FPN_cell_repeats):\n        middles, skips = BiFPN(middles, skips, FPN_num_filters, fusion)\n\n    if return_base:\n        outputs = middles\n    else:\n        outputs = build_detector_head(\n            middles, num_classes, num_dims, aspect_ratios, num_scales,\n            FPN_num_filters, box_class_repeats, survival_rate)\n\n    model = Model(inputs=image, outputs=outputs, name=model_name)\n\n    if ((base_weights == 'COCO') and (head_weights == 'COCO')):\n        model_filename = '-'.join([model_name, str(base_weights),\n                                   str(head_weights) + '_weights.hdf5'])\n    if ((base_weights == 'VOC') and (head_weights == 'VOC')):\n        model_filename = '-'.join([model_name, str(base_weights),\n                                   str(head_weights) + '_weights.hdf5'])\n    elif ((base_weights == 'COCO') and (head_weights is None)):\n        model_filename = '-'.join([model_name, str(base_weights),\n                                   str(head_weights) + '_weights.hdf5'])\n\n    if not ((base_weights is None) and (head_weights is None)):\n        weights_path = get_file(model_filename, WEIGHT_PATH + model_filename,\n                                cache_subdir='paz/models')\n        print('Loading %s model weights' % weights_path)\n        finetunning_model_names = ['efficientdet-d0-COCO-None_weights.hdf5',\n                                   'efficientdet-d1-COCO-None_weights.hdf5',\n                                   'efficientdet-d2-COCO-None_weights.hdf5',\n                                   'efficientdet-d3-COCO-None_weights.hdf5',\n                                   'efficientdet-d4-COCO-None_weights.hdf5',\n                                   'efficientdet-d5-COCO-None_weights.hdf5',\n                                   'efficientdet-d6-COCO-None_weights.hdf5',\n                                   'efficientdet-d7-COCO-None_weights.hdf5']\n        by_name = True if model_filename in finetunning_model_names else False\n        model.load_weights(weights_path, by_name=by_name)\n\n    image_shape = image.shape[1:3].as_list()\n    model.prior_boxes = build_anchors(\n        image_shape, branches, num_scales, aspect_ratios, anchor_scale)\n    return model",
  "def EFFICIENTDETD0(num_classes=90, base_weights='COCO', head_weights='COCO',\n                   input_shape=(512, 512, 3), FPN_num_filters=64,\n                   FPN_cell_repeats=3, box_class_repeats=3, anchor_scale=4.0,\n                   fusion='fast', return_base=False,\n                   model_name='efficientdet-d0',\n                   scaling_coefficients=(1.0, 1.0, 0.8)):\n    \"\"\"Instantiates EfficientDet-D0 model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D0 model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb0 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb0)\n    return model",
  "def EFFICIENTDETD1(num_classes=90, base_weights='COCO', head_weights='COCO',\n                   input_shape=(640, 640, 3), FPN_num_filters=88,\n                   FPN_cell_repeats=4, box_class_repeats=3, anchor_scale=4.0,\n                   fusion='fast', return_base=False,\n                   model_name='efficientdet-d1',\n                   scaling_coefficients=(1.0, 1.1, 0.8)):\n    \"\"\"Instantiates EfficientDet-D1 model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D1 model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb1 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb1)\n    return model",
  "def EFFICIENTDETD2(num_classes=90, base_weights='COCO', head_weights='COCO',\n                   input_shape=(768, 768, 3), FPN_num_filters=112,\n                   FPN_cell_repeats=5, box_class_repeats=3, anchor_scale=4.0,\n                   fusion='fast', return_base=False,\n                   model_name='efficientdet-d2',\n                   scaling_coefficients=(1.1, 1.2, 0.7)):\n    \"\"\"Instantiate EfficientDet-D2 model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D2 model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb2 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb2)\n    return model",
  "def EFFICIENTDETD3(num_classes=90, base_weights='COCO', head_weights='COCO',\n                   input_shape=(896, 896, 3), FPN_num_filters=160,\n                   FPN_cell_repeats=6, box_class_repeats=4, anchor_scale=4.0,\n                   fusion='fast', return_base=False,\n                   model_name='efficientdet-d3',\n                   scaling_coefficients=(1.2, 1.4, 0.7)):\n    \"\"\"Instantiates EfficientDet-D3 model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D3 model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb3 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb3)\n    return model",
  "def EFFICIENTDETD4(num_classes=90, base_weights='COCO', head_weights='COCO',\n                   input_shape=(1024, 1024, 3), FPN_num_filters=224,\n                   FPN_cell_repeats=7, box_class_repeats=4, anchor_scale=4.0,\n                   fusion='fast', return_base=False,\n                   model_name='efficientdet-d4',\n                   scaling_coefficients=(1.4, 1.8, 0.6)):\n    \"\"\"Instantiates EfficientDet-D4 model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D4 model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb4 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb4)\n    return model",
  "def EFFICIENTDETD5(num_classes=90, base_weights='COCO', head_weights='COCO',\n                   input_shape=(1280, 1280, 3), FPN_num_filters=288,\n                   FPN_cell_repeats=7, box_class_repeats=4, anchor_scale=4.0,\n                   fusion='fast', return_base=False,\n                   model_name='efficientdet-d5',\n                   scaling_coefficients=(1.6, 2.2, 0.6)):\n    \"\"\"Instantiates EfficientDet-D5 model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D5 model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb5 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb5)\n    return model",
  "def EFFICIENTDETD6(num_classes=90, base_weights='COCO', head_weights='COCO',\n                   input_shape=(1280, 1280, 3), FPN_num_filters=384,\n                   FPN_cell_repeats=8, box_class_repeats=5, anchor_scale=5.0,\n                   fusion='sum', return_base=False,\n                   model_name='efficientdet-d6',\n                   scaling_coefficients=(1.8, 2.6, 0.5)):\n    \"\"\"Instantiates EfficientDet-D6 model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D6 model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb6 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb6)\n    return model",
  "def EFFICIENTDETD7(num_classes=90, base_weights='COCO', head_weights='COCO',\n                   input_shape=(1536, 1536, 3), FPN_num_filters=384,\n                   FPN_cell_repeats=8, box_class_repeats=5, anchor_scale=5.0,\n                   fusion='sum', return_base=False,\n                   model_name='efficientdet-d7',\n                   scaling_coefficients=(1.8, 2.6, 0.5)):\n    \"\"\"Instantiates EfficientDet-D7 model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D7 model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb6 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb6)\n    return model",
  "def EFFICIENTDETD7x(num_classes=90, base_weights='COCO', head_weights='COCO',\n                    input_shape=(1536, 1536, 3), FPN_num_filters=384,\n                    FPN_cell_repeats=8, box_class_repeats=5, anchor_scale=4.0,\n                    fusion='sum', return_base=False,\n                    model_name='efficientdet-d7x',\n                    scaling_coefficients=(2.0, 3.1, 0.5)):\n    \"\"\"Instantiates EfficientDet-D7x model.\n\n    # Arguments\n        num_classes: Int, number of object classes.\n        base_weights: Str, base weights name.\n        head_weights: Str, head weights name.\n        input_shape: Tuple, holding input image size.\n        FPN_num_filters: Int, number of FPN filters.\n        FPN_cell_repeats: Int, number of FPN blocks.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        anchor_scale: Int, number of anchor scales.\n        fusion: Str, feature fusion weighting method.\n        return_base: Bool, whether to return base or not.\n        model_name: Str, EfficientDet model name.\n        scaling_coefficients: Tuple, EfficientNet scaling coefficients.\n\n    # Returns\n        model: EfficientDet-D7x model.\n    \"\"\"\n    image = Input(shape=input_shape, name='image')\n    EfficientNetb7 = EFFICIENTNET(image, scaling_coefficients)\n    model = EFFICIENTDET(image, num_classes, base_weights, head_weights,\n                         FPN_num_filters, FPN_cell_repeats, box_class_repeats,\n                         anchor_scale, fusion, return_base, model_name,\n                         EfficientNetb7)\n    return model",
  "class GetDropConnect(Layer):\n    \"\"\"Dropout for model layers.\n    DropConnect is similar to dropout, but instead of setting\n    activations to zero, it sets a fraction of the weights in a layer to\n    zero. This helps to prevent overfitting by reducing the complexity\n    of the model and encouraging the model to rely on a more diverse set\n    of weights.\n\n    # Arguments\n        survival_rate: Float, survival probability to drop features.\n\n    # Properties\n        survival_rate: Float.\n\n    # Methods\n        call()\n\n    # References\n        [Deep Networks with Stochastic Depth]\n        (https://arxiv.org/pdf/1603.09382.pdf)\n    \"\"\"\n    def __init__(self, survival_rate, **kwargs):\n        super(GetDropConnect, self).__init__(**kwargs)\n        self.survival_rate = survival_rate\n\n    def call(self, features, training=None):\n        if training:\n            batch_size = tf.shape(features)[0]\n            random_tensor = self.survival_rate\n            kwargs = {\"shape\": [batch_size, 1, 1, 1], \"dtype\": features.dtype}\n            random_tensor = random_tensor + tf.random.uniform(**kwargs)\n            binary_tensor = tf.floor(random_tensor)\n            output = (features / self.survival_rate) * binary_tensor\n            return output\n        else:\n            return features",
  "class FuseFeature(Layer):\n    \"\"\"Fuse features from different resolutions and return a\n    weighted sum. The resulting weighted sum is the fused feature.\n    Lower layers of the network tend to extract more basic features,\n    such as edges and shapes, while higher layers extract more complex\n    features that are useful for making predictions.\n    This class implements function that combines features from various\n    levels/layers of the model. This helps to combine the strengths of\n    different features and create a more robust and accurate\n    representation of the input image.\n\n    # Arguments\n        fusion: Str, feature fusion method.\n\n    # Properties\n        fusion: Str.\n\n    # Methods\n        build()\n        call()\n        _fuse_fast()\n        _fuse_sum()\n        get_config()\n\n    # References\n        [EfficientDet: Scalable and Efficient Object Detection]\n        (https://arxiv.org/pdf/1911.09070.pdf)\n    \"\"\"\n    def __init__(self, fusion, **kwargs):\n        super().__init__(**kwargs)\n        self.fusion = fusion\n        if fusion == 'fast':\n            self.fuse_method = self._fuse_fast\n        elif fusion == 'sum':\n            self.fuse_method = self._fuse_sum\n        else:\n            raise ValueError('FPN weight fusion is not defined')\n\n    def build(self, input_shape):\n        num_in = len(input_shape)\n        args = (self.name, (num_in,), tf.float32,\n                tf.keras.initializers.constant(1 / num_in))\n        self.w = self.add_weight(*args, trainable=True)\n\n    def call(self, inputs, fusion):\n        inputs = [input for input in inputs if input is not None]\n        return self.fuse_method(inputs)\n\n    def _fuse_fast(self, inputs):\n        w = tf.keras.activations.relu(self.w)\n\n        pre_activations = []\n        for input_arg in range(len(inputs)):\n            pre_activations.append(w[input_arg] * inputs[input_arg])\n        x = tf.reduce_sum(pre_activations, 0)\n        x = x / (tf.reduce_sum(w) + 0.0001)\n        return x\n\n    def _fuse_sum(self, inputs):\n        x = inputs[0]\n        for node in inputs[1:]:\n            x = x + node\n        return x\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({'fusion': self.fusion})\n        return config",
  "def __init__(self, survival_rate, **kwargs):\n        super(GetDropConnect, self).__init__(**kwargs)\n        self.survival_rate = survival_rate",
  "def call(self, features, training=None):\n        if training:\n            batch_size = tf.shape(features)[0]\n            random_tensor = self.survival_rate\n            kwargs = {\"shape\": [batch_size, 1, 1, 1], \"dtype\": features.dtype}\n            random_tensor = random_tensor + tf.random.uniform(**kwargs)\n            binary_tensor = tf.floor(random_tensor)\n            output = (features / self.survival_rate) * binary_tensor\n            return output\n        else:\n            return features",
  "def __init__(self, fusion, **kwargs):\n        super().__init__(**kwargs)\n        self.fusion = fusion\n        if fusion == 'fast':\n            self.fuse_method = self._fuse_fast\n        elif fusion == 'sum':\n            self.fuse_method = self._fuse_sum\n        else:\n            raise ValueError('FPN weight fusion is not defined')",
  "def build(self, input_shape):\n        num_in = len(input_shape)\n        args = (self.name, (num_in,), tf.float32,\n                tf.keras.initializers.constant(1 / num_in))\n        self.w = self.add_weight(*args, trainable=True)",
  "def call(self, inputs, fusion):\n        inputs = [input for input in inputs if input is not None]\n        return self.fuse_method(inputs)",
  "def _fuse_fast(self, inputs):\n        w = tf.keras.activations.relu(self.w)\n\n        pre_activations = []\n        for input_arg in range(len(inputs)):\n            pre_activations.append(w[input_arg] * inputs[input_arg])\n        x = tf.reduce_sum(pre_activations, 0)\n        x = x / (tf.reduce_sum(w) + 0.0001)\n        return x",
  "def _fuse_sum(self, inputs):\n        x = inputs[0]\n        for node in inputs[1:]:\n            x = x + node\n        return x",
  "def get_config(self):\n        config = super().get_config().copy()\n        config.update({'fusion': self.fusion})\n        return config",
  "def build_detector_head(middles, num_classes, num_dims, aspect_ratios,\n                        num_scales, FPN_num_filters, box_class_repeats,\n                        survival_rate):\n    \"\"\"Builds EfficientDet object detector's head.\n    The built head includes ClassNet and BoxNet for classification and\n    regression respectively.\n\n    # Arguments\n        middles: List, BiFPN layer output.\n        num_classes: Int, number of object classes.\n        num_dims: Int, number of output dimensions to regress.\n        aspect_ratios: List, anchor boxes aspect ratios.\n        num_scales: Int, number of anchor box scales.\n        FPN_num_filters: Int, number of FPN filters.\n        box_class_repeats: Int, Number of regression\n            and classification blocks.\n        survival_rate: Float, used in drop connect.\n\n    # Returns\n        outputs: Tensor of shape `[num_boxes, num_classes+num_dims]`\n    \"\"\"\n    num_anchors = len(aspect_ratios) * num_scales\n    args = (middles, num_anchors, FPN_num_filters,\n            box_class_repeats, survival_rate)\n    class_outputs = ClassNet(*args, num_classes)\n    boxes_outputs = BoxesNet(*args, num_dims)\n    classes = Concatenate(axis=1)(class_outputs)\n    regressions = Concatenate(axis=1)(boxes_outputs)\n    num_boxes = K.int_shape(regressions)[-1] // num_dims\n    classes = Reshape((num_boxes, num_classes))(classes)\n    classes = Activation('softmax')(classes)\n    regressions = Reshape((num_boxes, num_dims))(regressions)\n    outputs = Concatenate(axis=2, name='boxes')([regressions, classes])\n    return outputs",
  "def ClassNet(features, num_anchors=9, num_filters=32, num_blocks=4,\n             survival_rate=None, num_classes=90):\n    \"\"\"Initializes ClassNet.\n\n    # Arguments\n        features: List, input features.\n        num_anchors: Int, number of anchors.\n        num_filters: Int, number of intermediate layer filters.\n        num_blocks: Int, Number of intermediate layers.\n        survival_rate: Float, used in drop connect.\n        num_classes: Int, number of object classes.\n\n    # Returns\n        class_outputs: List, ClassNet outputs per level.\n    \"\"\"\n    bias_initializer = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n    num_filters = [num_filters, num_classes * num_anchors]\n    return build_head(features, num_blocks, num_filters, survival_rate,\n                      bias_initializer)",
  "def BoxesNet(features, num_anchors=9, num_filters=32, num_blocks=4,\n             survival_rate=None, num_dims=4):\n    \"\"\"Initializes BoxNet.\n\n    # Arguments\n        features: List, input features.\n        num_anchors: Int, number of anchors.\n        num_filters: Int, number of intermediate layer filters.\n        num_blocks: Int, Number of intermediate layers.\n        survival_rate: Float, used by drop connect.\n        num_dims: Int, number of output dimensions to regress.\n\n    # Returns\n        boxes_outputs: List, BoxNet outputs per level.\n    \"\"\"\n    bias_initializer = tf.zeros_initializer()\n    num_filters = [num_filters, num_dims * num_anchors]\n    return build_head(features, num_blocks, num_filters, survival_rate,\n                      bias_initializer)",
  "def build_head(middle_features, num_blocks, num_filters,\n               survival_rate, bias_initializer):\n    \"\"\"Builds ClassNet/BoxNet head.\n\n    # Arguments\n        middle_features: Tuple. input features.\n        num_blocks: Int, number of intermediate layers.\n        num_filters: Int, number of intermediate layer filters.\n        survival_rate: Float, used by drop connect.\n        bias_initializer: Callable, bias initializer.\n\n    # Returns\n        head_outputs: List, with head outputs.\n    \"\"\"\n    conv_blocks = build_head_conv2D(\n        num_blocks, num_filters[0], tf.zeros_initializer())\n    final_head_conv = build_head_conv2D(1, num_filters[1], bias_initializer)[0]\n    head_outputs = []\n    for x in middle_features:\n        for block_arg in range(num_blocks):\n            x = conv_blocks[block_arg](x)\n            x = BatchNormalization()(x)\n            x = tf.nn.swish(x)\n            if block_arg > 0 and survival_rate:\n                x = x + GetDropConnect(survival_rate=survival_rate)(x)\n        x = final_head_conv(x)\n        x = Flatten()(x)\n        head_outputs.append(x)\n    return head_outputs",
  "def build_head_conv2D(num_blocks, num_filters, bias_initializer):\n    \"\"\"Builds head convolutional blocks.\n\n    # Arguments\n        num_blocks: Int, number of intermediate layers.\n        num_filters: Int, number of intermediate layer filters.\n        bias_initializer: Callable, bias initializer.\n\n    # Returns\n        conv_blocks: List, head convolutional blocks.\n    \"\"\"\n    conv_blocks = []\n    args_1 = (num_filters, 3, (1, 1), 'same', 'channels_last', (1, 1),\n              1, None, True)\n    for _ in range(num_blocks):\n        args_2 = (tf.initializers.variance_scaling(),\n                  tf.initializers.variance_scaling(), bias_initializer)\n        conv_blocks.append(SeparableConv2D(*args_1, *args_2))\n    return conv_blocks",
  "def EfficientNet_to_BiFPN(branches, num_filters):\n    \"\"\"Preprocess EfficientNet branches prior to feeding BiFPN block.\n    The branches generated by the EfficientNet backbone consists of\n    features P1, P2, P3, P4, and P5. However, the BiFPN block requires\n    features P3, P4, P5, P6, and P7. This function generates features\n    P3 to P7 from EfficientNet branches that can be fed to the BiFPN\n    block.\n\n    # Arguments\n        branches: List, EfficientNet feature maps.\n        num_filters: Int, number of intermediate layer filters.\n\n    # Returns\n        branches, middles, skips: List, extended branch\n            and preprocessed feature maps.\n    \"\"\"\n    branches = extend_branch(branches, num_filters)\n    P3, P4, P5, P6, P7 = branches\n    P3_middle = conv_batchnorm_block(P3, num_filters)\n    P4_middle = conv_batchnorm_block(P4, num_filters)\n    P5_middle = conv_batchnorm_block(P5, num_filters)\n    middles = [P3_middle, P4_middle, P5_middle, P6, P7]\n\n    P4_skip = conv_batchnorm_block(P4, num_filters)\n    P5_skip = conv_batchnorm_block(P5, num_filters)\n    skips = [None, P4_skip, P5_skip, P6, None]\n    return [branches, middles, skips]",
  "def extend_branch(branches, num_filters):\n    \"\"\"Extends branches to comply with BiFPN.\n    The input branchs includes features P1-P5. This function extends the\n    EfficientNet backbone generated branch. The extended branch contains\n    features P3-P7.\n\n    # Arguments\n        branches: List, EfficientNet feature maps.\n        num_filters: Int, number of intermediate layer filters.\n\n    # Returns\n        middles, skips: List, modified branch.\n    \"\"\"\n    _, _, P3, P4, P5 = branches\n    P6, P7 = build_branch(P5, num_filters)\n    branches_extended = [P3, P4, P5, P6, P7]\n    return branches_extended",
  "def build_branch(P5, num_filters):\n    \"\"\"Builds feature maps P6 and P7 from P5.\n\n    # Arguments\n        P5: Tensor of shape `(batch_size, 16, 16, 320)`,\n            EfficientNet's 5th layer output.\n        num_filters: Int, number of intermediate layer filters.\n\n    # Returns\n        P6, P7: List, EfficientNet's 6th and 7th layer output.\n    \"\"\"\n    P6 = conv_batchnorm_block(P5, num_filters)\n    P6 = MaxPooling2D(3, 2, 'same')(P6)\n    P7 = MaxPooling2D(3, 2, 'same')(P6)\n    return [P6, P7]",
  "def conv_batchnorm_block(x, num_filters):\n    \"\"\"Builds 2D convolution and batch normalization layers.\n\n    # Arguments\n        x: Tensor, input feature map.\n        num_filters: Int, number of intermediate layer filters.\n\n    # Returns\n        x: Tensor. Feature after convolution and batch normalization.\n    \"\"\"\n    x = Conv2D(num_filters, 1, 1, 'same')(x)\n    x = BatchNormalization()(x)\n    return x",
  "def BiFPN(middles, skips, num_filters, fusion):\n    \"\"\"BiFPN block.\n    BiFPN stands for Bidirectional Feature Pyramid Network.\n\n    # Arguments\n        middles: List, BiFPN layer output.\n        skips: List, skip feature map from BiFPN node.\n        num_filters: Int, number of intermediate layer filters.\n        fusion: Str, feature fusion method.\n\n    # Returns\n        middles, middles: List, BiFPN block output.\n    \"\"\"\n    P3_middle, P4_middle, P5_middle, P6_middle, P7_middle = middles\n    _, P4_skip, P5_skip, P6_skip, _ = skips\n\n    # Downpropagation ---------------------------------------------------------\n    args = (num_filters, fusion)\n    P7_up = UpSampling2D()(P7_middle)\n    P6_top_down = node_BiFPN(P7_up, P6_middle, None, None, *args)\n    P6_up = UpSampling2D()(P6_top_down)\n    P5_top_down = node_BiFPN(P6_up, P5_middle, None, None, *args)\n    P5_up = UpSampling2D()(P5_top_down)\n    P4_top_down = node_BiFPN(P5_up, P4_middle, None, None, *args)\n    P4_up = UpSampling2D()(P4_top_down)\n    P3_out = node_BiFPN(P4_up, P3_middle, None, None, *args)\n\n    # Upward propagation ------------------------------------------------------\n    P3_down = MaxPooling2D(3, 2, 'same')(P3_out)\n    P4_out = node_BiFPN(None, P4_top_down, P3_down, P4_skip, *args)\n    P4_down = MaxPooling2D(3, 2, 'same')(P4_out)\n    P5_out = node_BiFPN(None, P5_top_down, P4_down, P5_skip, *args)\n    P5_down = MaxPooling2D(3, 2, 'same')(P5_out)\n    P6_out = node_BiFPN(None, P6_top_down, P5_down, P6_skip, *args)\n    P6_down = MaxPooling2D(3, 2, 'same')(P6_out)\n    P7_out = node_BiFPN(None, P7_middle, P6_down, None, *args)\n\n    middles = [P3_out, P4_out, P5_out, P6_out, P7_out]\n    return [middles, middles]",
  "def node_BiFPN(up, middle, down, skip, num_filters, fusion):\n    \"\"\"Simulates a single node of BiFPN block.\n\n    # Arguments\n        up: Tensor, upsampled feature map.\n        middle: Tensor, preprocessed feature map.\n        down: Tensor, downsampled feature map.\n        skip: Tensor, skip feature map.\n        num_filters: Int, number of intermediate layer filters.\n        fusion: Str, feature fusion method.\n\n    # Returns\n        middle: Tensor, BiFPN node output.\n    \"\"\"\n    is_layer_one = down is None\n    if is_layer_one:\n        to_fuse = [middle, up]\n    else:\n        to_fuse = [middle, down] if skip is None else [skip, middle, down]\n    middle = FuseFeature(fusion=fusion)(to_fuse, fusion)\n    middle = tf.nn.swish(middle)\n    middle = SeparableConv2D(num_filters, 3, 1, 'same', use_bias=True)(middle)\n    middle = BatchNormalization()(middle)\n    return middle",
  "def convolution_block(inputs, filters, kernel_size=3, activation='relu'):\n    \"\"\"UNET convolution block containing Conv2D -> BatchNorm -> Activation\n\n    # Arguments\n        inputs: Keras/tensorflow tensor input.\n        filters: Int. Number of filters.\n        kernel_size: Int. Kernel size of convolutions.\n        activation: String. Activation used convolution.\n\n    # Returns\n        Keras/tensorflow tensor.\n    \"\"\"\n    kwargs = {'use_bias': False, 'kernel_initializer': 'he_uniform'}\n    x = Conv2D(filters, kernel_size, (1, 1), 'same', **kwargs)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(activation)(x)\n    return x",
  "def upsample_block(x, filters, branch):\n    \"\"\"UNET upsample block. This block upsamples ``x``, concatenates a\n    ``branch`` tensor and applies two convolution blocks:\n    Upsample -> Concatenate -> 2 x ConvBlock.\n\n    # Arguments\n        x: Keras/tensorflow tensor.\n        filters: Int. Number of filters\n        branch: Tensor to be concatated to the upsamples ``x`` tensor.\n\n    # Returns\n        A Keras tensor.\n    \"\"\"\n    x = UpSampling2D(size=2)(x)\n    x = Concatenate(axis=3)([x, branch])\n    x = convolution_block(x, filters)\n    x = convolution_block(x, filters)\n    return x",
  "def transpose_block(x, filters, branch):\n    \"\"\"UNET transpose block. This block upsamples ``x``, concatenates a\n    ``branch`` tensor and applies two convolution blocks:\n    Conv2DTranspose -> Concatenate -> 2 x ConvBlock.\n\n    # Arguments\n        x: Keras/tensorflow tensor.\n        filters: Int. Number of filters\n        branch: Tensor to be concatated to the upsamples ``x`` tensor.\n\n    # Returns\n        A Keras tensor.\n    \"\"\"\n    x = Conv2DTranspose(filters, 4, (2, 2), 'same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Concatenate(axis=3)([x, branch])\n    x = convolution_block(x, filters)\n    return x",
  "def freeze_model(model):\n    \"\"\"Freezes gradient pass for the entire model\n\n    # Arguments:\n        model: Keras/tensorflow model\n\n    # Returns:\n        A Keras/tensorflow model\n    \"\"\"\n    for layer in model.layers:\n        layer.trainable = False\n    return model",
  "def get_tensors(model, layer_names):\n    \"\"\"Gets all the tensor outputs of the given layer names.\n\n    # Arguments\n        model: Keras/tensorflow model.\n        layer_names: List of strings which each string is a layer name.\n\n    # Returns\n        List of Keras tensors.\n    \"\"\"\n    tensors = []\n    for layer_name in layer_names:\n        tensors.append(model.get_layer(layer_name).output)\n    return model, tensors",
  "def build_backbone(BACKBONE, shape, branch_names, weights,\n                   frozen=False, input_tensor=None):\n    \"\"\"Builds ``BACKBONE`` class for UNET model.\n\n    # Arguments\n        BACKBONE: Class for instantiating a backbone model\n        shape: List of integers: ``(H, W, num_channels)``.\n        branch_names: List of strings containing layer names of ``BACKBONE()``.\n        weights: String or ``None``.\n        frozen: Boolean. If True ``BACKBONE()`` updates are frozen.\n        input_tensor: Input tensor. If given ``shape`` is overwritten and this\n            tensor is used instead as input.\n\n    # Returns\n    \"\"\"\n    kwargs = {'include_top': False, 'input_shape': shape, 'weights': weights}\n    if input_tensor is not None:\n        kwargs.pop('input_shape')\n        kwargs['input_tensor'] = input_tensor\n    backbone = BACKBONE(**kwargs)\n\n    if frozen:\n        backbone = freeze_model(backbone)\n\n    backbone, branch_tensors = get_tensors(backbone, branch_names)\n    return backbone, branch_tensors",
  "def build_UNET(num_classes, backbone, branch_tensors,\n               decoder, decoder_filters, activation, name):\n    \"\"\"Build UNET with a given ``backbone`` model.\n\n    # Arguments\n        num_classes: Integer used for output number of channels.\n        backbone: Instantiated backbone model.\n        branch_tensors: List of tensors from ``backbone`` model\n        decoder: Function used for upsampling and decoding the output.\n        decoder_filters: List of integers used in each application of decoder.\n        activation: Output activation of the model.\n        name: String. indicating the name of the model.\n\n    # Returns\n        A UNET Keras/tensorflow model.\n    \"\"\"\n    inputs, x = backbone.input, backbone.output\n    if isinstance(backbone.layers[-1], MaxPooling2D):\n        x = convolution_block(x, 512)\n        x = convolution_block(x, 512)\n\n    for branch, filters in zip(branch_tensors, decoder_filters):\n        x = decoder(x, filters, branch)\n\n    kwargs = {'use_bias': True, 'kernel_initializer': 'glorot_uniform'}\n    x = Conv2D(num_classes, 3, (1, 1), 'same', **kwargs)(x)\n    outputs = Activation(activation, name='masks')(x)\n    model = Model(inputs, outputs, name=name)\n    return model",
  "def UNET(input_shape, num_classes, branch_names, BACKBONE, weights,\n         freeze_backbone=False, activation='sigmoid', decoder_type='upsample',\n         decoder_filters=[256, 128, 64, 32, 16], input_tensor=None,\n         name='UNET'):\n    \"\"\"Build a generic UNET model with a given ``BACKBONE`` class.\n\n    # Arguments\n        input_shape: List of integers: ``(H, W, num_channels)``.\n        num_classes: Integer used for output number of channels.\n        branch_names: List of strings containing layer names of ``BACKBONE()``.\n        BACKBONE: Class for instantiating a backbone model\n        weights: String indicating backbone weights e.g.\n            ''imagenet'', ``None``.\n        freeze_backbone: Boolean. If True ``BACKBONE()`` updates are frozen.\n        decoder_type: String indicating decoding function e.g.\n            ''upsample ''transpose''.\n        decoder_filters: List of integers used in each application of decoder.\n        activation: Output activation of the model.\n        input_tensor: Input tensor. If given ``shape`` is overwritten and this\n            tensor is used instead as input.\n        name: String. indicating the name of the model.\n\n    # Returns\n        A UNET Keras/tensorflow model.\n    \"\"\"\n    args = [BACKBONE, input_shape, branch_names,\n            weights, freeze_backbone, input_tensor]\n    backbone, branch_tensors = build_backbone(*args)\n    if decoder_type == 'upsample':\n        decoder = upsample_block\n    if decoder_type == 'transpose':\n        decoder = transpose_block\n\n    model = build_UNET(num_classes, backbone, branch_tensors, decoder,\n                       decoder_filters, activation, name)\n    return model",
  "def UNET_VGG16(num_classes=1, input_shape=(224, 224, 3), weights='imagenet',\n               freeze_backbone=False, activation='sigmoid',\n               decoder_type='upsample',\n               decode_filters=[256, 128, 64, 32, 16]):\n    \"\"\"Build a UNET model with a ``VGG16`` backbone.\n\n    # Arguments\n        input_shape: List of integers: ``(H, W, num_channels)``.\n        num_classes: Integer used for output number of channels.\n        branch_names: List of strings containing layer names of ``BACKBONE()``.\n        BACKBONE: Class for instantiating a backbone model\n        weights: String indicating backbone weights e.g.\n            ''imagenet'', ``None``.\n        freeze_backbone: Boolean. If True ``BACKBONE()`` updates are frozen.\n        decoder_type: String indicating decoding function e.g.\n            ''upsample ''transpose''.\n        decoder_filters: List of integers used in each application of decoder.\n        activation: Output activation of the model.\n        input_tensor: Input tensor. If given ``shape`` is overwritten and this\n            tensor is used instead as input.\n        name: String. indicating the name of the model.\n\n    # Returns\n        A UNET-VGG16 Keras/tensorflow model.\n    \"\"\"\n    VGG16_branches = ['block5_conv3', 'block4_conv3', 'block3_conv3',\n                      'block2_conv2', 'block1_conv2']\n    return UNET(input_shape, num_classes, VGG16_branches, VGG16, weights,\n                freeze_backbone, activation, decoder_type, decode_filters,\n                name='UNET-VGG16')",
  "def UNET_VGG19(num_classes=1, input_shape=(224, 224, 3), weights='imagenet',\n               freeze_backbone=False, activation='sigmoid',\n               decoder_type='upsample',\n               decode_filters=[256, 128, 64, 32, 16]):\n    \"\"\"Build a UNET model with a ``VGG19`` backbone.\n\n    # Arguments\n        input_shape: List of integers: ``(H, W, num_channels)``.\n        num_classes: Integer used for output number of channels.\n        branch_names: List of strings containing layer names of ``BACKBONE()``.\n        BACKBONE: Class for instantiating a backbone model\n        weights: String indicating backbone weights e.g.\n            ''imagenet'', ``None``.\n        freeze_backbone: Boolean. If True ``BACKBONE()`` updates are frozen.\n        decoder_type: String indicating decoding function e.g.\n            ''upsample ''transpose''.\n        decoder_filters: List of integers used in each application of decoder.\n        activation: Output activation of the model.\n        input_tensor: Input tensor. If given ``shape`` is overwritten and this\n            tensor is used instead as input.\n        name: String. indicating the name of the model.\n\n    # Returns\n        A UNET-VGG19 Keras/tensorflow model.\n    \"\"\"\n\n    VGG19_branches = ['block5_conv4', 'block4_conv4', 'block3_conv4',\n                      'block2_conv2', 'block1_conv2']\n    return UNET(input_shape, num_classes, VGG19_branches, VGG19, weights,\n                freeze_backbone, activation, decoder_type, decode_filters,\n                name='UNET-VGG19')",
  "def UNET_RESNET50(num_classes=1, input_shape=(224, 224, 3), weights='imagenet',\n                  freeze_backbone=False, activation='sigmoid',\n                  decoder_type='upsample',\n                  decode_filters=[256, 128, 64, 32, 16]):\n    \"\"\"Build a UNET model with a ``RESNET50V2`` backbone.\n\n    # Arguments\n        input_shape: List of integers: ``(H, W, num_channels)``.\n        num_classes: Integer used for output number of channels.\n        branch_names: List of strings containing layer names of ``BACKBONE()``.\n        BACKBONE: Class for instantiating a backbone model\n        weights: String indicating backbone weights e.g.\n            ''imagenet'', ``None``.\n        freeze_backbone: Boolean. If True ``BACKBONE()`` updates are frozen.\n        decoder_type: String indicating decoding function e.g.\n            ''upsample ''transpose''.\n        decoder_filters: List of integers used in each application of decoder.\n        activation: Output activation of the model.\n        input_tensor: Input tensor. If given ``shape`` is overwritten and this\n            tensor is used instead as input.\n        name: String. indicating the name of the model.\n\n    # Returns\n        A UNET-RESNET50V2 Keras/tensorflow model.\n    \"\"\"\n    RESNET50_branches = ['conv4_block6_1_relu', 'conv3_block4_1_relu',\n                         'conv2_block3_1_relu', 'conv1_conv', 'input_resnet50']\n    input_tensor = Input(input_shape, name='input_resnet50')\n    return UNET(input_shape, num_classes, RESNET50_branches, ResNet50V2,\n                weights, freeze_backbone, activation, decoder_type,\n                decode_filters, input_tensor, 'UNET-RESNET50')",
  "def dense_block(input_x, num_keypoints, rate):\n    \"\"\"Make a bi-linear block with optional residual connection\n    # Arguments\n        input_x: the batch that enters the block\n        num_keypoints: integer. The size of the linear units\n        rate: float [0,1]. Probability of dropping something out\n\n    # Returns\n        x: the batch after it leaves the block\n    \"\"\"\n    kwargs = {'kernel_initializer': HeNormal(), 'bias_initializer': HeNormal(),\n              'kernel_constraint': MaxNorm(max_value=1)}\n    x = Dense(num_keypoints, **kwargs)(input_x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Dropout(rate)(x)\n    x = Dense(num_keypoints, **kwargs, )(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Dropout(rate)(x)\n    x = (x + input_x)\n    return x",
  "def SimpleBaseline(input_shape=(32,), num_keypoints=16, keypoints_dim=3,\n                   hidden_dim=1024, num_layers=2, rate=1, weights='human36m'):\n    \"\"\"Model that predicts 3D keypoints from 2D keypoints\n    # Arguments\n        num_keypoints: numer of kepoints\n        keypoints_dim: dimension of keypoints\n        hidden_dim: size of hidden layers\n        input_shape: size of the input\n        num_layers: number of layers\n        rate: dropout drop rate\n\n    # Returns\n        keypoints3D estimation model\n    \"\"\"\n    inputs = Input(shape=input_shape)\n    kwargs = {'kernel_initializer': HeNormal(), 'bias_initializer': HeNormal(),\n              'kernel_constraint': MaxNorm(max_value=1)}\n    x = Dense(hidden_dim, use_bias=True, **kwargs)(inputs)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Dropout(rate)(x)\n    for layer in range(num_layers):\n        x = dense_block(x, hidden_dim, rate)\n    x = Dense(num_keypoints * keypoints_dim, **kwargs)(x)\n    x = Reshape((num_keypoints, keypoints_dim))(x)\n    model = Model(inputs, outputs=x)\n    if weights == 'human36m':\n        URL = ('https://github.com/oarriaga/altamira-data/releases/download/'\n               'v0.17/SIMPLE-BASELINES.hdf5')\n        filename = os.path.basename(URL)\n        weights_path = get_file(filename, URL, cache_subdir='paz/models')\n        model.load_weights(weights_path)\n    return model",
  "def block(x, num_filters, dilation_rate, alpha, name, kernel_size=(3, 3)):\n    x = Conv2D(num_filters, kernel_size, dilation_rate=dilation_rate,\n               padding='same', name=name)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha)(x)\n    return x",
  "def KeypointNet2D(input_shape, num_keypoints, filters=64, alpha=0.1):\n    \"\"\"Model for discovering keypoint locations in 2D space, modified from\n\n    # Arguments\n        input_shape: List of integers indicating ``[H, W, num_channels]``.\n        num_keypoints: Int. Number of keypoints to discover.\n        filters: Int. Number of filters used in convolutional layers.\n        alpha: Float. Alpha parameter of leaky relu.\n\n    # Returns\n        Keras/tensorflow model\n\n    # References\n        - [Discovery of Latent 3D Keypoints via End-to-end\n            Geometric Reasoning](https://arxiv.org/abs/1807.03146)\n    \"\"\"\n    width, height = input_shape[:2]\n    base = input_tensor = Input(input_shape, name='image')\n    for base_arg, rate in enumerate([1, 1, 2, 4, 8, 16, 1, 2, 4, 8, 16, 1]):\n        name = 'conv2D_base-%s' % base_arg\n        base = block(base, filters, (rate, rate), alpha, name)\n\n    name = 'uv_volume_features-%s'\n    uv_volume = Conv2D(num_keypoints, (3, 3),\n                       padding='same', name=name % 0)(base)\n    uv_volume = Permute([3, 1, 2], name=name % 1)(uv_volume)\n    volume_shape = [num_keypoints, width * height]\n    uv_volume = Reshape(volume_shape, name=name % 2)(uv_volume)\n    uv_volume = Activation('softmax', name=name % 3)(uv_volume)\n    volume_shape = [num_keypoints, width, height]\n    uv_volume = Reshape(volume_shape, name='uv_volume')(uv_volume)\n    uv = ExpectedValue2D(name='keypoints')(uv_volume)\n    model = Model(input_tensor, uv, name='keypointnet2D')\n    return model",
  "def KeypointNet(input_shape, num_keypoints, depth=.2, filters=64, alpha=0.1):\n    \"\"\"Keypointnet model for discovering keypoint locations in 3D space\n\n    # Arguments\n        input_shape: List of integers indicating ``[H, W, num_channels)``.\n        num_keypoints: Int. Number of keypoints to discover.\n        depth: Float. Prior depth (centimeters) of keypoints.\n        filters: Int. Number of filters used in convolutional layers.\n        alpha: Float. Alpha parameter of leaky relu.\n\n    # Returns\n        Keras/tensorflow model\n\n    # References\n        - [Discovery of Latent 3D Keypoints via End-to-end\n            Geometric Reasoning](https://arxiv.org/abs/1807.03146)\n    \"\"\"\n    width, height = input_shape[:2]\n    base = input_tensor = Input(input_shape, name='image')\n    for base_arg, rate in enumerate([1, 1, 2, 4, 8, 16, 1, 2, 4, 8, 16, 1]):\n        name = 'conv2D_base-%s' % base_arg\n        base = block(base, filters, (rate, rate), alpha, name)\n\n    name = 'uv_volume_features-%s'\n    uv_volume = Conv2D(num_keypoints, (3, 3),\n                       padding='same', name=name % 0)(base)\n    uv_volume = Permute([3, 1, 2], name=name % 1)(uv_volume)\n    volume_shape = [num_keypoints, width * height]\n    uv_volume = Reshape(volume_shape, name=name % 2)(uv_volume)\n    uv_volume = Activation('softmax', name=name % 3)(uv_volume)\n    volume_shape = [num_keypoints, width, height]\n    uv_volume = Reshape(volume_shape, name='uv_volume')(uv_volume)\n    uv = ExpectedValue2D(name='expected_uv')(uv_volume)\n\n    name = 'depth_volume_features-%s'\n    depth_volume = Conv2D(num_keypoints, (3, 3),\n                          padding='same', name=name % 0)(base)\n    depth_volume = SubtractScalar(depth, name=name % 1)(depth_volume)\n    depth_volume = Permute([3, 1, 2], name='depth_volume')(depth_volume)\n    z = ExpectedDepth(name='expected_z')([depth_volume, uv_volume])\n    uvz = Concatenate(axis=-1, name='uvz_points')([uv, z])\n    model = Model(input_tensor, [uvz, uv_volume], name='keypointnet')\n    return model",
  "def KeypointNetShared(input_shape, num_keypoints, depth, filters, alpha):\n    \"\"\"Keypointnet shared model with two views as input.\n\n    # Arguments\n        input_shape: List of integers indicating ``[H, W, num_channels]``.\n        num_keypoints: Int. Number of keypoints to discover.\n        depth: Float. Prior depth (centimeters) of keypoints.\n        filters: Int. Number of filters used in convolutional layers.\n        alpha: Float. Alpha parameter of leaky relu.\n\n    # Returns\n        Keras/tensorflow model\n\n    # References\n        - [Discovery of Latent 3D Keypoints via End-to-end\n            Geometric Reasoning](https://arxiv.org/abs/1807.03146)\n    \"\"\"\n\n    model_args = (input_shape, num_keypoints, depth, filters, alpha)\n    keypointnet = KeypointNet(*model_args)\n    image_A = Input(input_shape, name='image_A')\n    image_B = Input(input_shape, name='image_B')\n    uvz_A, uv_volume_A = keypointnet(image_A)\n    uvz_B, uv_volume_B = keypointnet(image_B)\n    uvz_points = Concatenate(axis=1, name='uvz_points-shared')([uvz_A, uvz_B])\n    uv_volumes = Concatenate(axis=1, name='uv_volumes-shared')(\n        [uv_volume_A, uv_volume_B])\n    inputs, outputs = [image_A, image_B], [uvz_points, uv_volumes]\n    return Model(inputs, outputs, name='keypointnet-shared')",
  "def dense(x, num_units):\n    x = Dense(num_units, activation=None, kernel_regularizer=l2(0.5 * 1.0),\n              kernel_initializer=truncated_normal(stddev=0.01))(x)\n    return x",
  "def block(x, num_units):\n    x = dense(x, num_units)\n    x = BatchNormalization()(x)\n    return x",
  "def normalize(x):\n    norm = tf.norm(x, axis=-1, keepdims=True)\n    norm = tf.maximum(norm, 1e-6)\n    normalized_x = x / norm\n    return normalized_x",
  "def reorder_quaternions(quaternions):\n    w = quaternions[:, :, 0:1]\n    qs = quaternions[:, :, 1:4]\n    quaternions = tf.concat((qs, w), axis=-1)\n    return quaternions",
  "def IKNet(input_shape=(84, 3), num_keypoints=21, depth=6, width=1024):\n    \"\"\"IKNet: Estimate absolute joint angle for the minimal hand keypoints.\n\n    # Arguments\n        input_shape: [num_keypoint x 4, 3]. Contains 3D keypoints, bone\n                     orientation, refrence keypoint, refrence bone orientation.\n        num_keypoints: Int. Number of keypoints.\n\n    # Returns\n        Tensorflow-Keras model.\n        absolute joint angle in quaternion representation.\n\n    # Reference\n        - [Monocular Real-time Hand Shape and Motion Capture using Multi-modal\n           Data](https://arxiv.org/abs/2003.09572)\n    \"\"\"\n    input = Input(shape=input_shape, dtype=tf.float32)\n    x = Reshape([1, -1])(input)\n\n    for depth_arg in range(depth):\n        x = block(x, width)\n        x = Activation('sigmoid')(x)\n    x = dense(x, num_keypoints * 4)\n    x = Reshape([num_keypoints, 4])(x)\n    x = normalize(x)\n\n    positive_mask = tf.tile(x[:, :, 0:1] > 0, [1, 1, 4])\n    quaternions = tf.where(positive_mask, x, -x)\n    quaternions = reorder_quaternions(quaternions)\n\n    model = Model(input, outputs=[quaternions])\n\n    URL = ('https://github.com/oarriaga/altamira-data/releases/download/'\n           'v0.14/iknet_weight.hdf5')\n    filename = os.path.basename(URL)\n    weights_path = get_file(filename, URL, cache_subdir='paz/models')\n    print('==> Loading %s model weights' % weights_path)\n    model.load_weights(weights_path)\n    return model",
  "def zero_padding(tensor, pad_1, pad_2):\n    pad_mat = np.array([[0, 0],\n                        [pad_1, pad_2],\n                        [pad_1, pad_2],\n                        [0, 0]])\n    return tf.pad(tensor, paddings=pad_mat)",
  "def block(tensor, filters, kernel_size, strides, name, rate=1, with_relu=True):\n    if strides == 1:\n        x = Conv2D(filters, kernel_size, strides, padding='SAME',\n                   use_bias=False, dilation_rate=rate,\n                   kernel_regularizer=l2(0.5 * 1.0), name=name + '/conv2d',\n                   kernel_initializer=VarianceScaling(\n                       mode=\"fan_avg\", distribution=\"uniform\"))(tensor)\n    else:\n        pad_1 = (kernel_size - 1) // 2\n        pad_2 = (kernel_size - 1) - pad_1\n        x = zero_padding(tensor, pad_1, pad_2)\n        x = Conv2D(filters, kernel_size, strides, padding='VALID',\n                   use_bias=False, dilation_rate=rate,\n                   kernel_regularizer=l2(0.5 * (1.0)), name=name + '/conv2d',\n                   kernel_initializer=VarianceScaling(\n                       mode=\"fan_avg\", distribution=\"uniform\"))(x)\n    x = BatchNormalization(name=name + '/batch_normalization')(x)\n    if with_relu:\n        x = ReLU()(x)\n    return x",
  "def bottleneck(tensor, filters, strides, name, rate=1):\n    shape = tensor.get_shape()[-1]\n    if shape == filters:\n        if strides == 1:\n            x = tensor\n        else:\n            x = MaxPool2D(strides, strides, 'SAME')(tensor)\n    else:\n        x = block(tensor, filters, 1, strides, name + '/shortcut',\n                  with_relu=False)\n    residual = block(tensor, (filters // 4), 1, 1, name + '/conv1')\n    residual = block(residual, (filters // 4), 3, strides, name + '/conv2',\n                     rate)\n    residual = block(residual, filters, 1, 1, name + '/conv3',\n                     with_relu=False)\n    output = ReLU()(x + residual)\n    return output",
  "def resnet50(tensor, name):\n    x = block(tensor, 64, 7, 2, name + '/conv1')\n    for arg in range(2):\n        x = bottleneck(x, 256, 1, name + '/block1/unit%d' % (arg + 1))\n    x = bottleneck(x, 256, 2, name + '/block1/unit3')\n    for arg in range(4):\n        x = bottleneck(x, 512, 1, name + '/block2/unit%d' % (arg + 1), 2)\n    for arg in range(6):\n        x = bottleneck(x, 1024, 1, name + '/block3/unit%d' % (arg + 1), 4)\n    x = block(x, 256, 3, 1, name + '/squeeze')\n    return x",
  "def net2D(features, num_keypoints, name):\n    x = block(features, 256, 3, 1, name + '/project')\n    heat_map = Conv2D(num_keypoints, 1, strides=1, padding='SAME',\n                      activation='sigmoid', name=name + '/prediction/conv2d',\n                      kernel_initializer=truncated_normal(stddev=0.01))(x)\n    return heat_map",
  "def net3D(features, num_keypoints, name, need_norm=False):\n    x = block(features, 256, 3, 1, name + '/project')\n    delta_map = Conv2D(num_keypoints * 3, 1, strides=1, padding='SAME',\n                       name=name + '/prediction/conv2d',\n                       kernel_initializer=truncated_normal(stddev=0.01))(x)\n    if need_norm:\n        delta_map_norm = tf.norm(delta_map, axis=-1, keepdims=True)\n        delta_map = delta_map / tf.maximum(delta_map_norm, 1e-6)\n\n    H, W = features.get_shape()[1:3]\n    delta_map = Reshape([H, W, num_keypoints, 3])(delta_map)\n    if need_norm:\n        return delta_map, delta_map_norm\n    return delta_map",
  "def get_pose_tile(N):\n    x = np.linspace(-1, 1, 32)\n    x = np.stack([np.tile(x.reshape([1, 32]), [32, 1]),\n                  np.tile(x.reshape([32, 1]), [1, 32])], -1)\n    x = np.expand_dims(x, 0)\n    x = tf.constant(x, dtype=tf.float32)\n    pose_tile = tf.tile(x, [N, 1, 1, 1])\n    return pose_tile",
  "def tf_heatmap_to_uv(heatmap):\n    shape = tf.shape(heatmap)\n    heatmap = tf.reshape(heatmap, (shape[0], -1, shape[3]))\n    argmax = tf.math.argmax(heatmap, axis=1, output_type=tf.int32)\n    argmax_x = argmax // shape[2]\n    argmax_y = argmax % shape[2]\n    uv = tf.stack((argmax_x, argmax_y), axis=1)\n    uv = tf.transpose(a=uv, perm=[0, 2, 1])\n    return uv",
  "def DetNet(input_shape=(128, 128, 3), num_keypoints=21):\n    \"\"\"DetNet: Estimate 3D keypoint positions of minimal hand from input\n               color image.\n\n    # Arguments\n        input_shape: Shape for 128x128 RGB image of **left hand**.\n                     List of integers. Input shape to the model including only\n                     spatial and channel resolution e.g. (128, 128, 3).\n        num_keypoints: Int. Number of keypoints.\n\n    # Returns\n        Tensorflow-Keras model.\n        xyz: Numpy array [num_keypoints, 3]. Normalized 3D keypoint locations.\n        uv: Numpy array [num_keypoints, 2]. The uv coordinates of the keypoints\n            on the heat map, whose resolution is 32x32.\n\n    # Reference\n        -[Monocular Real-time Hand Shape and Motion Capture using Multi-modal\n          Data](https://arxiv.org/abs/2003.09572)\n    \"\"\"\n\n    image = Input(shape=input_shape, dtype=tf.uint8)\n    x = tf.cast(image, tf.float32) / 255\n\n    name = 'prior_based_hand'\n    features = resnet50(x, name + '/resnet')\n    pose_tile = get_pose_tile(tf.shape(x)[0])\n    features = concatenate([features, pose_tile], -1)\n\n    heat_map = net2D(features, num_keypoints, name + '/hmap_0')\n    features = concatenate([features, heat_map], axis=-1)\n\n    delta_map = net3D(features, num_keypoints, name + '/dmap_0')\n    delta_map_reshaped = Reshape([32, 32, num_keypoints * 3])(delta_map)\n    features = concatenate([features, delta_map_reshaped], -1)\n\n    location_map = net3D(features, num_keypoints, name + '/lmap_0')\n    location_map_reshaped = Reshape([32, 32, num_keypoints * 3])(location_map)\n    features = concatenate([features, location_map_reshaped], -1)\n\n    uv = tf_heatmap_to_uv(heat_map)\n    xyz = tf.gather_nd(\n        tf.transpose(location_map, perm=[0, 3, 1, 2, 4]), uv, batch_dims=2)[0]\n    uv = uv[0]\n\n    model = Model(image, outputs=[xyz, uv])\n\n    URL = ('https://github.com/oarriaga/altamira-data/releases/download'\n           '/v0.14/detnet_weights.hdf5')\n    filename = os.path.basename(URL)\n    weights_path = get_file(filename, URL, cache_subdir='paz/models')\n    print('==> Loading %s model weights' % weights_path)\n    model.load_weights(weights_path)\n    return model",
  "class Projector(object):\n    \"\"\"Projects keypoints from image coordinates to 3D space and viceversa.\n    This model uses the camera focal length and the depth estimation of a point\n    to project it to image coordinates. It works with numpy matrices or\n    tensorflow values. See ``use_numpy``.\n\n    # Arguments\n        focal_length: Float. Focal length of camera used to generate keypoints.\n        use_numpy: Boolean. If `True` both unproject and project functions\n            take numpy arrays as inputs. If `False` takes tf.tensors as inputs.\n    \"\"\"\n    def __init__(self, focal_length, use_numpy=False):\n        self.focal_length = focal_length\n        self.project = self._project_keras\n        self.unproject = self._unproject_keras\n        if use_numpy:\n            self.project = self._project_numpy\n            self.unproject = self._unproject_numpy\n\n    def _project_keras(self, xyzw):\n        z = xyzw[:, :, 2:3] + 1e-8\n        x = - (self.focal_length / z) * xyzw[:, :, 0:1]\n        y = - (self.focal_length / z) * xyzw[:, :, 1:2]\n        return K.concatenate([x, y, z], axis=2)\n\n    def _project_numpy(self, xyzw):\n        z = xyzw[:, :, 2:3] + 1e-8\n        x = - (self.focal_length / z) * xyzw[:, :, 0:1]\n        y = - (self.focal_length / z) * xyzw[:, :, 1:2]\n        return np.concatenate([x, y, z], axis=2)\n\n    def _unproject_keras(self, xyz):\n        z = xyz[:, :, 2:3]\n        x = - (z / self.focal_length) * xyz[:, :, 0:1]\n        y = - (z / self.focal_length) * xyz[:, :, 1:2]\n        w = K.ones_like(z)\n        xyzw = K.concatenate([x, y, z, w], axis=2)\n        return xyzw\n\n    def _unproject_numpy(self, xyz):\n        z = xyz[:, :, 2:3]\n        x = - (z / self.focal_length) * xyz[:, :, 0:1]\n        y = - (z / self.focal_length) * xyz[:, :, 1:2]\n        w = np.ones_like(z)\n        xyzw = np.concatenate([x, y, z, w], axis=2)\n        return xyzw",
  "def __init__(self, focal_length, use_numpy=False):\n        self.focal_length = focal_length\n        self.project = self._project_keras\n        self.unproject = self._unproject_keras\n        if use_numpy:\n            self.project = self._project_numpy\n            self.unproject = self._unproject_numpy",
  "def _project_keras(self, xyzw):\n        z = xyzw[:, :, 2:3] + 1e-8\n        x = - (self.focal_length / z) * xyzw[:, :, 0:1]\n        y = - (self.focal_length / z) * xyzw[:, :, 1:2]\n        return K.concatenate([x, y, z], axis=2)",
  "def _project_numpy(self, xyzw):\n        z = xyzw[:, :, 2:3] + 1e-8\n        x = - (self.focal_length / z) * xyzw[:, :, 0:1]\n        y = - (self.focal_length / z) * xyzw[:, :, 1:2]\n        return np.concatenate([x, y, z], axis=2)",
  "def _unproject_keras(self, xyz):\n        z = xyz[:, :, 2:3]\n        x = - (z / self.focal_length) * xyz[:, :, 0:1]\n        y = - (z / self.focal_length) * xyz[:, :, 1:2]\n        w = K.ones_like(z)\n        xyzw = K.concatenate([x, y, z, w], axis=2)\n        return xyzw",
  "def _unproject_numpy(self, xyz):\n        z = xyz[:, :, 2:3]\n        x = - (z / self.focal_length) * xyz[:, :, 0:1]\n        y = - (z / self.focal_length) * xyz[:, :, 1:2]\n        w = np.ones_like(z)\n        xyzw = np.concatenate([x, y, z, w], axis=2)\n        return xyzw",
  "def dense_block(x, blocks, growth_rate):\n    for block_arg in range(blocks):\n        x1 = Conv2D(4 * growth_rate, 1, use_bias=False)(x)\n        x1 = BatchNormalization(epsilon=1.001e-5)(x)\n        x1 = Activation('relu')(x1)\n        x1 = Conv2D(growth_rate, 3, padding='same', use_bias=False)(x1)\n        x1 = BatchNormalization(epsilon=1.001e-5)(x1)\n        x1 = Activation('relu')(x1)\n        x = Concatenate(axis=-1)([x, x1])\n    return x",
  "def residual_block(x, num_kernels, strides=1):\n    residual = x\n    x = Conv2D(num_kernels, 3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(num_kernels, 3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Add()([x, residual])\n    x = Activation('relu')(x)\n    return x",
  "def transition_block(x, alpha):\n    filters = int(K.int_shape(x)[-1] * alpha)\n    x = Conv2D(filters, 1, strides=2, use_bias=False)(x)\n    x = BatchNormalization(epsilon=1.001e-5)(x)\n    x = Activation('relu')(x)\n    return x",
  "def stem(x, filters):\n    x = Conv2D(filters, 3, padding='same', strides=(2, 2), use_bias=False)(x)\n    x = BatchNormalization(epsilon=1.1e-5)(x)\n    x = Activation('relu')(x)\n    x = Conv2D(filters, 3, padding='same', strides=(2, 2), use_bias=False)(x)\n    x = BatchNormalization(epsilon=1.1e-5)(x)\n    x = Activation('relu')(x)\n    return x",
  "def fuse(tensors, base_kernels=32):\n    all_tensors = []\n    for x_tensor_arg, x in enumerate(tensors):\n        x_to_y_tensors = []\n        for y_tensor_arg in range(len(tensors)):\n            # step: how much the feature map is upsampled or downsampled\n            steps = x_tensor_arg - y_tensor_arg\n\n            if steps == 0:\n                num_kernels = K.int_shape(x)[-1]\n                y = Conv2D(num_kernels, 3, padding='same',\n                           strides=1, use_bias=False)(x)\n                y = BatchNormalization(epsilon=1.1e-5)(y)\n                y = Activation('relu')(y)\n\n            if steps < 0:\n                y = x\n                for step in range(abs(steps)):\n                    num_kernels = int(K.int_shape(x)[-1] * (step + 1))\n                    y = Conv2D(num_kernels, 3, strides=2,\n                               padding='same', use_bias=False)(y)\n                    y = BatchNormalization(epsilon=1.1e-5)(y)\n                    y = Activation('relu')(y)\n\n            if steps > 0:\n                num_kernels = int(K.int_shape(x)[-1] / steps)\n                y = Conv2D(num_kernels, 1, use_bias=False)(x)\n                y = BatchNormalization(epsilon=1.1e-5)(y)\n                y = Activation('relu')(y)\n                y = UpSampling2D(size=(2**steps, 2**steps))(y)\n\n            x_to_y_tensors.append(y)\n        all_tensors.append(x_to_y_tensors)\n\n    output_tensors = []\n    for reciever_arg in range(len(tensors)):\n        same_resolution_tensors = []\n        for giver_arg in range(len(tensors)):\n            tensor = all_tensors[giver_arg][reciever_arg]\n            same_resolution_tensors.append(tensor)\n        x = Concatenate()(same_resolution_tensors)\n        num_kernels = base_kernels * (2 ** (reciever_arg))\n        x = Conv2D(num_kernels, 1, use_bias=False)(x)\n        x = BatchNormalization(epsilon=1.1e-5)(x)\n        x = Activation('relu')(x)\n        output_tensors.append(x)\n    return output_tensors",
  "def bottleneck(x, filters=64, expansion=4):\n    residual = x\n    x = Conv2D(filters, 1, use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(filters, 3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(filters * expansion, 1, use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Add()([x, residual])\n    x = Activation('relu')(x)\n    return x",
  "def HRNetDense(input_shape=(128, 128, 3), num_keypoints=20, growth_rate=4):\n    # stem\n    inputs = Input(shape=input_shape)\n    x1 = stem(inputs, 64)\n    x1 = Conv2D(64 * 4, 1, padding='same', use_bias=False)(x1)\n    x1 = BatchNormalization()(x1)\n    for block in range(4):\n        x1 = bottleneck(x1)\n\n    # stage I\n    x1 = Conv2D(32, 3, padding='same', use_bias=False)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = Activation('relu')(x1)\n    x2 = transition_block(x1, 2)\n    print('stage 1', x1.shape, x2.shape)\n\n    # stage II\n    x1 = dense_block(x1, 4, growth_rate)\n    x2 = dense_block(x2, 4, growth_rate)\n    x1, x2 = fuse([x1, x2])\n    x3 = transition_block(x2, 0.5)\n    print('stage 2', x1.shape, x2.shape, x3.shape)\n\n    # stage III\n    x1 = dense_block(x1, 4, growth_rate)\n    x2 = dense_block(x2, 4, growth_rate)\n    x3 = dense_block(x3, 4, growth_rate)\n    x1, x2, x3 = fuse([x1, x2, x3])\n    x4 = transition_block(x3, 0.5)\n    print('stage 3', x1.shape, x2.shape, x3.shape, x4.shape)\n\n    # stage IV\n    x1 = dense_block(x1, 3, growth_rate)\n    x2 = dense_block(x2, 3, growth_rate)\n    x3 = dense_block(x3, 3, growth_rate)\n    x4 = dense_block(x4, 3, growth_rate)\n    x1, x2, x3, x4 = fuse([x1, x2, x3, x4])\n    print('stage 4', x1.shape, x2.shape, x3.shape, x4.shape)\n\n    x2 = UpSampling2D(size=(2, 2))(x2)\n    x3 = UpSampling2D(size=(4, 4))(x3)\n    x4 = UpSampling2D(size=(8, 8))(x4)\n    x = Concatenate()([x1, x2, x3, x4])\n\n    # head\n    x = Conv2D(480, 1)(x)\n    x = BatchNormalization(epsilon=1.001e-5)(x)\n    x = Activation('relu')(x)\n    x = Conv2D(num_keypoints, 1)(x)\n\n    # extra\n    x = BatchNormalization(epsilon=1.001e-5)(x)\n    x = Activation('relu')(x)\n    x = UpSampling2D(size=(4, 4), interpolation='bilinear')(x)\n    x = Permute([3, 1, 2])(x)\n    x = Reshape([num_keypoints, input_shape[0] * input_shape[1]])(x)\n    x = Activation('softmax')(x)\n    x = Reshape([num_keypoints, input_shape[0], input_shape[1]])(x)\n    outputs = ExpectedValue2D(name='expected_uv')(x)\n    model = Model(inputs, outputs, name='hrnet-dense')\n    return model",
  "def HRNetResidual(input_shape=(128, 128, 3), num_keypoints=20):\n    \"\"\"Instantiates HRNET Residual model\n\n    # Arguments\n        input_shape: List of three elements e.g. ''(H, W, 3)''\n        num_keypoints: Int.\n\n    # Returns\n        Tensorflow-Keras model.\n\n    # References\n       -[High-Resolution Representations for Labeling Pixels\n            and Regions](https://arxiv.org/pdf/1904.04514.pdf)\n    \"\"\"\n\n    # stem\n    inputs = Input(shape=input_shape, name='image')\n    x1 = stem(inputs, 64)\n    x1 = Conv2D(64 * 4, 1, padding='same', use_bias=False)(x1)\n    x1 = BatchNormalization()(x1)\n    for block in range(4):\n        x1 = bottleneck(x1)\n\n    # stage I\n    x1 = Conv2D(32, 3, padding='same', use_bias=False)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = Activation('relu')(x1)\n    x2 = transition_block(x1, 2)\n\n    # stage II\n    for block in range(4):\n        x1 = residual_block(x1, 32)\n        x2 = residual_block(x2, 64)\n    x1, x2 = fuse([x1, x2])\n    x3 = transition_block(x2, 2)\n\n    # stage III\n    for module in range(4):\n        for block in range(4):\n            x1 = residual_block(x1, 32)\n            x2 = residual_block(x2, 64)\n            x3 = residual_block(x3, 128)\n        x1, x2, x3 = fuse([x1, x2, x3])\n    x4 = transition_block(x3, 2)\n\n    # stage IV\n    for module in range(3):\n        for block in range(4):\n            x1 = residual_block(x1, 32)\n            x2 = residual_block(x2, 64)\n            x3 = residual_block(x3, 128)\n            x4 = residual_block(x4, 256)\n        x1, x2, x3, x4 = fuse([x1, x2, x3, x4])\n\n    # head\n    x2 = UpSampling2D(size=(2, 2))(x2)\n    x3 = UpSampling2D(size=(4, 4))(x3)\n    x4 = UpSampling2D(size=(8, 8))(x4)\n    x = Concatenate()([x1, x2, x3, x4])\n    x = Conv2D(480, 1)(x)\n    x = BatchNormalization(epsilon=1.001e-5)(x)\n    x = Activation('relu')(x)\n    x = Conv2D(num_keypoints, 1)(x)\n\n    # extra\n    x = BatchNormalization(epsilon=1.001e-5)(x)\n    x = Activation('relu')(x)\n    x = UpSampling2D(size=(4, 4), interpolation='bilinear')(x)\n    x = Permute([3, 1, 2])(x)\n    x = Reshape([num_keypoints, input_shape[0] * input_shape[1]])(x)\n    x = Activation('softmax')(x)\n    x = Reshape([num_keypoints, input_shape[0], input_shape[1]])(x)\n    outputs = ExpectedValue2D(name='keypoints')(x)\n    model = Model(inputs, outputs, name='hrnet-residual')\n    return model",
  "def conv_block(x):\n    \"\"\"Basic convolution block used for prototypical networks.\n\n    # Arguments\n        x: Tensor.\n\n    # Returns\n        Tensor\n    \"\"\"\n    x = Conv2D(filters=64, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = MaxPool2D((2, 2))(x)\n    return x",
  "def ProtoEmbedding(image_shape, num_blocks):\n    \"\"\"Embedding convolutional network used for proto-typical networks\n\n    # Arguments:\n        image_shape: List with image shape `(H, W, channels)`.\n        num_blocks: Ints. Number of convolution blocks.\n\n    # Returns:\n        Keras model.\n\n    # References:\n        [prototypical networks](https://arxiv.org/abs/1703.05175)\n    \"\"\"\n    x = inputs = Input(image_shape)\n    for _ in range(num_blocks):\n        x = conv_block(x)\n    z = Flatten()(x)\n    return Model(inputs, z, name='EMBEDDING')",
  "class FullReshape(Layer):\n    \"\"\"Reshapes all tensor dimensions including the batch dimension.\n    \"\"\"\n    def __init__(self, shape, **kwargs):\n        super(FullReshape, self).__init__(**kwargs)\n        self.shape = shape\n\n    def call(self, x):\n        return tf.reshape(x, self.shape)",
  "class ComputePrototypes(Layer):\n    def __init__(self, axis=1, **kwargs):\n        super(ComputePrototypes, self).__init__(**kwargs)\n        self.axis = axis\n\n    def call(self, z_support):\n        class_prototypes = tf.reduce_mean(z_support, axis=self.axis)\n        return class_prototypes",
  "def compute_pairwise_distances(x, y):\n    \"\"\"Compute euclidean distance for each vector x with each vector y\n\n    # Arguments:\n        x: Tensor with shape `(n, vector_dim)`\n        y: Tensor with shape `(m, vector_dim)`\n\n    # Returns:\n        Tensor with shape `(n, m)` where each value pair n, m corresponds to\n        the distance between the vector `n` of `x` with the vector `m` of `y`\n    \"\"\"\n    n = x.shape[0]\n    m = y.shape[0]\n    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)",
  "class ComputePairwiseDistances(Layer):\n    def __init__(self, **kwargs):\n        super(ComputePairwiseDistances, self).__init__(**kwargs)\n\n    def call(self, z_queries, class_prototypes):\n        return compute_pairwise_distances(z_queries, class_prototypes)",
  "def ProtoNet(embed, num_classes, num_support, num_queries, image_shape):\n    \"\"\"Prototypical networks used for few-shot classification\n    # Arguments:\n        embed: Keras network for embedding images into metric space.\n        num_classes: Number of `ways` for few-shot classification.\n        num_support: Number of `shots` used for meta learning.\n        num_queries: Number of test images to query.\n        image_shape: List with image shape `(H, W, channels)`.\n\n    # Returns:\n        Keras model.\n\n    # References:\n        [prototypical networks](https://arxiv.org/abs/1703.05175)\n    \"\"\"\n    support = Input((num_support, *image_shape), num_classes, name='support')\n    queries = Input((num_queries, *image_shape), num_classes, name='queries')\n    z_support = FullReshape((num_classes * num_support, *image_shape))(support)\n    z_queries = FullReshape((num_classes * num_queries, *image_shape))(queries)\n    z_support = embed(z_support)\n    z_queries = embed(z_queries)\n    z_dim = embed.output_shape[-1]\n    z_support = FullReshape((num_classes, num_support, z_dim))(z_support)\n    z_queries = FullReshape((num_classes * num_queries, z_dim))(z_queries)\n    class_prototypes = ComputePrototypes(axis=1)(z_support)\n    distances = ComputePairwiseDistances()(z_queries, class_prototypes)\n    outputs = Softmax()(-distances)\n    return Model(inputs=[support, queries], outputs=outputs, name='PROTONET')",
  "def __init__(self, shape, **kwargs):\n        super(FullReshape, self).__init__(**kwargs)\n        self.shape = shape",
  "def call(self, x):\n        return tf.reshape(x, self.shape)",
  "def __init__(self, axis=1, **kwargs):\n        super(ComputePrototypes, self).__init__(**kwargs)\n        self.axis = axis",
  "def call(self, z_support):\n        class_prototypes = tf.reduce_mean(z_support, axis=self.axis)\n        return class_prototypes",
  "def __init__(self, **kwargs):\n        super(ComputePairwiseDistances, self).__init__(**kwargs)",
  "def call(self, z_queries, class_prototypes):\n        return compute_pairwise_distances(z_queries, class_prototypes)",
  "def xception_block(input_tensor, num_kernels, l2_reg=0.01):\n    \"\"\"Xception core block.\n\n    # Arguments\n        input_tenso: Keras tensor.\n        num_kernels: Int. Number of convolutional kernels in block.\n        l2_reg: Float. l2 regression.\n\n    # Returns\n        output tensor for the block.\n    \"\"\"\n    residual = Conv2D(num_kernels, 1, strides=(2, 2),\n                      padding='same', use_bias=False)(input_tensor)\n    residual = BatchNormalization()(residual)\n    x = SeparableConv2D(\n        num_kernels, 3, padding='same',\n        kernel_regularizer=l2(l2_reg), use_bias=False)(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = SeparableConv2D(num_kernels, 3, padding='same',\n                        kernel_regularizer=l2(l2_reg), use_bias=False)(x)\n    x = BatchNormalization()(x)\n\n    x = MaxPooling2D(3, strides=(2, 2), padding='same')(x)\n    x = Add()([x, residual])\n    return x",
  "def build_xception(\n        input_shape, num_classes, stem_kernels, block_kernels, l2_reg=0.01):\n    \"\"\"Function for instantiating an Xception model.\n\n    # Arguments\n        input_shape: List corresponding to the input shape of the model.\n        num_classes: Integer.\n        stem_kernels: List of integers. Each element of the list indicates\n            the number of kernels used as stem blocks.\n        block_kernels: List of integers. Each element of the list Indicates\n            the number of kernels used in the xception blocks.\n        l2_reg. Float. L2 regularization used in the convolutional kernels.\n\n    # Returns\n        Tensorflow-Keras model.\n\n    # References\n        - [Xception: Deep Learning with Depthwise Separable\n            Convolutions](https://arxiv.org/abs/1610.02357)\n    \"\"\"\n\n    x = inputs = Input(input_shape, name='image')\n    for num_kernels in stem_kernels:\n        x = Conv2D(num_kernels, 3, kernel_regularizer=l2(l2_reg),\n                   use_bias=False, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n\n    for num_kernels in block_kernels:\n        x = xception_block(x, num_kernels, l2_reg)\n\n    x = Conv2D(num_classes, 3, kernel_regularizer=l2(l2_reg),\n               padding='same')(x)\n    # x = BatchNormalization()(x)\n    x = GlobalAveragePooling2D()(x)\n    output = Activation('softmax', name='label')(x)\n\n    model_name = '-'.join(['XCEPTION',\n                           str(input_shape[0]),\n                           str(stem_kernels[0]),\n                           str(len(block_kernels))\n                           ])\n    model = Model(inputs, output, name=model_name)\n    return model",
  "def MiniXception(input_shape, num_classes, weights=None):\n    \"\"\"Build MiniXception (see references).\n\n    # Arguments\n        input_shape: List of three integers e.g. ``[H, W, 3]``\n        num_classes: Int.\n        weights: ``None`` or string with pre-trained dataset. Valid datasets\n            include only ``FER``.\n\n    # Returns\n        Tensorflow-Keras model.\n\n    # References\n       - [Real-time Convolutional Neural Networks for Emotion and\n            Gender Classification](https://arxiv.org/abs/1710.07557)\n    \"\"\"\n    if weights == 'FER':\n        filename = 'fer2013_mini_XCEPTION.119-0.65.hdf5'\n        path = get_file(filename, URL + filename, cache_subdir='paz/models')\n        model = load_model(path)\n    else:\n        stem_kernels = [32, 64]\n        block_data = [128, 128, 256, 256, 512, 512, 1024]\n        model_inputs = (input_shape, num_classes, stem_kernels, block_data)\n        model = build_xception(*model_inputs)\n    model._name = 'MINI-XCEPTION'\n    return model",
  "def stem(tensor, filters):\n    x = ZeroPadding2D(padding=(1, 1), name='pad')(tensor)\n    x = Conv2D(filters, 3, strides=2, use_bias=False, name='conv1')(x)\n    x = BatchNormalization(momentum=0.1, epsilon=1e-05,\n                           name='bn1')(x, training=False)\n    x = ReLU(name='relu')(x)\n    x = ZeroPadding2D(padding=(1, 1), name='pad_1')(x)\n    x = Conv2D(filters, 3, strides=2, use_bias=False, name='conv2')(x)\n    x = BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn2')(x)\n    x = ReLU()(x)\n    return x",
  "def bottleneck(tensor, filters, expansion, downsample=None, name=None):\n    residual = tensor\n    x = Conv2D(filters, 1, use_bias=False, name=name + '.conv1')(tensor)\n    x = BatchNormalization(momentum=0.1, epsilon=1e-05, name=name + '.bn1')(x)\n    x = ReLU(name=name + '.relu')(x)\n    x = Conv2D(filters, 3, padding='same',\n               use_bias=False, name=name + '.conv2')(x)\n    x = BatchNormalization(momentum=0.1, epsilon=1.0e-5, name=name + '.bn2')(x)\n    x = ReLU()(x)\n    x = Conv2D(filters * expansion, 1, use_bias=False, name=name + '.conv3')(x)\n    x = BatchNormalization(momentum=0.1, epsilon=1.0e-5, name=name + '.bn3')(x)\n    if downsample is not None:\n        x1 = Conv2D(256, 1, use_bias=False,\n                    name='layer1.0.downsample.0')(tensor)\n        residual = BatchNormalization(momentum=0.1, epsilon=1e-05,\n                                      name='layer1.0.downsample.1')(x1)\n    x = Add()([x, residual])\n    x = ReLU()(x)\n    return x",
  "def basic_block(tensor, filters, name=None):\n    residual = tensor\n    x = Conv2D(filters, 3, padding='same',\n               use_bias=False, name=name + '.conv1')(tensor)\n    x = BatchNormalization(momentum=0.1, epsilon=1e-05, name=name + '.bn1')(x)\n    x = ReLU(name=name + '.relu')(x)\n    x = Conv2D(filters, 3, padding='same',\n               use_bias=False, name=name + '.conv2')(x)\n    x = BatchNormalization(momentum=0.1, epsilon=1e-05, name=name + '.bn2')(x)\n    x = Add()([x, residual])\n    x = ReLU()(x)\n    return x",
  "def transition_block(tensor, alpha, name):\n    in_channels = K.int_shape(tensor)[-1]\n    if in_channels == 256:\n        filters = 32 * alpha\n    else:\n        filters = in_channels * alpha\n    x = ZeroPadding2D(padding=(1, 1))(tensor)\n    x = Conv2D(filters, 3, strides=2, use_bias=False, name=name + '0.0')(x)\n    x = BatchNormalization(momentum=0.1, epsilon=1e-05, name=name + '0.1')(x)\n    x = ReLU()(x)\n    return x",
  "def blocks_in_branch(tensors, stage, in_channels, name):\n    if stage != len(tensors):\n        raise ValueError('''outputs {} feed to fuse_layers must to be same as\n                         num_branches {}'''.format(tensors, stage))\n    for arg in range(stage):\n        filters = in_channels * (2 ** arg)\n        tensors[arg] = basic_block(tensors[arg], filters,\n                                   name=name[:18] + str(arg) + '.' + name[18:])\n    return tensors",
  "def final_layers(num_keypoints, with_AE_loss=None, num_deconv=1):\n    final_layers = []\n    if with_AE_loss[0]:\n        output_channels = num_keypoints * 2\n    else:\n        output_channels = num_keypoints\n    x = Conv2D(output_channels, 1, padding='same', name='final_layers.0')\n    final_layers.append(x)\n\n    for arg in range(num_deconv):\n        if with_AE_loss[arg + 1]:\n            output_channels = num_keypoints * 2\n        else:\n            output_channels = num_keypoints\n        x1 = Conv2D(output_channels, 1, padding='same', name='final_layers.1')\n        final_layers.append(x1)\n    return final_layers",
  "def deconv_layers(tensor, output_channels, num_deconv=1):\n    for arg in range(num_deconv):\n        x = Conv2DTranspose(output_channels, 4, strides=2,\n                            padding='same', use_bias=False,\n                            name='deconv_layers.0.0.0')(tensor)\n        x = BatchNormalization(momentum=0.1, epsilon=1e-05,\n                               name='deconv_layers.0.0.1')(x)\n        x = ReLU()(x)\n        for block in range(4):\n            x = basic_block(\n                x, output_channels,\n                name='deconv_layers.0.' + str(block + 1) + '.' + '0')\n    return x",
  "def get_names(name, branch_arg, stage_arg, counter, iterations=0):\n    name1 = '.'.join((name, str(branch_arg), str(stage_arg),\n                     str(iterations + counter)))\n    name2 = '.'.join((name, str(branch_arg), str(stage_arg),\n                     str(iterations + counter + .1)))\n    return [name1, name2]",
  "def fuse_layers(tensors, stage, output_branches, filters=32, name=None):\n    if stage != len(tensors):\n        raise ValueError('''outputs {} feed to fuse_layers must to be same as\n                         num_branches {}'''.format(tensors, stage))\n    all_tensors = []\n    for branch_arg in range(output_branches):\n        x_to_y_tensors = []\n        for stage_arg in range(stage):\n            # step: how much the feature map is upsampled or downsampled\n            steps = stage_arg - branch_arg\n            if steps == 0:\n                y = tensors[branch_arg]\n\n            elif steps > 0:  # upsample\n                name0 = '.'.join((name, str(branch_arg), str(stage_arg)))\n                y = upsample(tensors[stage_arg], filters * (2 ** branch_arg),\n                             size=(2**steps, 2**steps), name=name0)\n\n            elif steps < 0:  # downsample\n                y_flag = False\n                iterations = 0\n                for k in range((-1 * steps) - 1):\n                    iterations += 1\n                    if y_flag:\n                        name1 = get_names(name, branch_arg, stage_arg, 1.0)\n                        y = downsample(y, filters * (2 ** stage_arg), name1)\n\n                    else:\n                        name2 = get_names(name, branch_arg, stage_arg, 0.0)\n                        y = downsample(tensors[stage_arg],\n                                       filters * (2 ** stage_arg), name2,\n                                       with_padding=False)\n                    y = ReLU()(y)\n                    y_flag = True\n\n                if not y_flag:\n                    tensors[stage_arg] = ZeroPadding2D()(tensors[stage_arg])\n                    name3 = get_names(name, branch_arg, stage_arg, 0.0)\n                    y = downsample(tensors[stage_arg],\n                                   filters * (2 ** branch_arg), name3,\n                                   with_padding=False)\n\n                else:\n                    name4 = get_names(name, branch_arg, stage_arg,\n                                      .0, iterations)\n                    y = downsample(y, filters * (2 ** branch_arg), name4)\n            x_to_y_tensors.append(y)\n\n        all_tensors.append(x_to_y_tensors)\n\n    x_fused = []\n    for x_tensor_arg in range(len(all_tensors)):\n        for y_tensor_arg in range(len(x_to_y_tensors)):\n            if y_tensor_arg == 0:\n                x_fused.append(all_tensors[x_tensor_arg][0])\n            else:\n                x = Add()([x_fused[x_tensor_arg],\n                          all_tensors[x_tensor_arg][y_tensor_arg]])\n                x_fused[x_tensor_arg] = x\n\n    for x_fused_arg in range(len(x_fused)):\n        x_fused[x_fused_arg] = ReLU()(x_fused[x_fused_arg])\n    return x_fused",
  "def upsample(tensor, filters, size, name=None):\n    x = Conv2D(filters, 1, use_bias=False, name=name + '.0')(tensor)\n    x = BatchNormalization(momentum=0.1, epsilon=1e-05, name=name + '.1')(x)\n    x = UpSampling2D(size=size, interpolation='nearest', name=name + '.2')(x)\n    return x",
  "def downsample(tensor, filters, name=None, with_padding=True):\n    if with_padding:\n        tensor = ZeroPadding2D(padding=(1, 1))(tensor)\n    x = Conv2D(filters, 3, strides=2, use_bias=False, name=name[0])(tensor)\n    x = BatchNormalization(momentum=0.1, epsilon=1e-05, name=name[1])(x)\n    return x",
  "def HigherHRNet(weights='COCO', input_shape=(None, None, 3), num_keypoints=17,\n                with_AE_loss=[True, False]):\n    \"\"\"Human pose estimation detector for any input size of images.\n    # Arguments\n        weights: String or None. If string should be a valid dataset name.\n            Current valid datasets include `COCO`.\n        input_shape: List of integers. Input shape to the model including only\n            spatial and channel resolution e.g. (512, 512, 3).\n        num_keypoints: Int. Number of joints.\n        with_AE_loss: List of boolean.\n\n    # Reference\n        - [HigherHRNet: Scale-Aware Representation Learning for Bottom-Up\n           Human Pose Estimation](https://arxiv.org/abs/1908.10357)\n    \"\"\"\n\n    image = Input(shape=input_shape, name='image')\n    x = stem(image, 64)\n    # print(f\"check 1 stem TF ==> {x.get_shape()}\")\n\n    # First group of bottleneck (resnet) modules\n    # Stage 1 -----------------------------------------------------------------\n    x = bottleneck(x, filters=64, expansion=4,\n                   downsample=True, name='layer1' + '.0')\n    for block in range(3):\n        x = bottleneck(x, filters=64, expansion=4,\n                       downsample=None, name='layer1' + '.' + str(block + 1))\n\n    x_list = []\n    # Creation of the first two branches (one full and one half resolution)\n    x1 = Conv2D(32, 3, strides=1, padding='same',\n                use_bias=False, name='transition1.0.0')(x)\n    x1 = BatchNormalization(momentum=0.1, name='transition1.0.1')(x1)\n    x1 = ReLU()(x1)\n\n    x_list.append(x1)\n    x_list.append(transition_block(x, 2, name='transition1.1.'))\n\n    # Stage 2 -----------------------------------------------------------------\n    for block in range(4):\n        x_list = blocks_in_branch(x_list, stage=2, in_channels=32,\n                                  name='stage2.0.branches.' + str(block))\n    x_list = fuse_layers(x_list, stage=2, output_branches=2,\n                         name='stage2.0.fuse_layers')\n    x_list.append(transition_block(x_list[1], 2, name='transition2.2.'))\n\n    # Stage 3 -----------------------------------------------------------------\n    for module in range(4):\n        for block in range(4):\n            name = 'stage3.' + str(module) + '.branches.' + str(block)\n            x_list = blocks_in_branch(x_list, stage=3,\n                                      in_channels=32, name=name)\n        x_list = fuse_layers(x_list, stage=3, output_branches=3,\n                             name='stage3.' + str(module) + '.fuse_layers')\n    x_list.append(transition_block(x_list[2], 2, name='transition3.3.'))\n\n    # Stage 4 -----------------------------------------------------------------\n    for module in range(3):\n        for block in range(4):\n            name = 'stage4.' + str(module) + '.branches.' + str(block)\n            x_list = blocks_in_branch(x_list, stage=4,\n                                      in_channels=32, name=name)\n        if module == 2:\n            name = 'stage4.' + str(module) + '.fuse_layers'\n            x_list = fuse_layers(x_list, stage=4, output_branches=1, name=name)\n        else:\n            name = 'stage4.' + str(module) + '.fuse_layers'\n            x_list = fuse_layers(x_list, stage=4, output_branches=4, name=name)\n\n    final_outputs = []\n    x2 = x_list[0]\n    output = final_layers(num_keypoints, with_AE_loss=with_AE_loss)[0](x2)\n    final_outputs.append(output)\n\n    x2 = concatenate((x2, output), -1)\n    x2 = deconv_layers(x2, 32)\n    x2 = final_layers(num_keypoints, with_AE_loss=with_AE_loss)[1](x2)\n    final_outputs.append(x2)\n\n    model = Model(image, outputs=final_outputs, name='HigherHRNet')\n\n    if(weights == 'COCO'):\n        URL = ('https://github.com/oarriaga/altamira-data/releases/download'\n               '/v0.10/HigherHRNet_weights.hdf5')\n        filename = os.path.basename(URL)\n        weights_path = get_file(filename, URL, cache_subdir='paz/models')\n        print('==> Loading %s model weights' % weights_path)\n        model.load_weights(weights_path)\n    return model",
  "def docstring(original):\n    \"\"\"Doctors (documents) `target` `Callable` with `original` docstring.\n\n    # Arguments:\n        original: Object with documentation string.\n\n    # Returns\n        Function that replaces `target` docstring with `original` docstring.\n    \"\"\"\n    def wrapper(target):\n        target.__doc__ = original.__doc__\n        return target\n    return wrapper",
  "def wrapper(target):\n        target.__doc__ = original.__doc__\n        return target",
  "def build_directory(root='experiments', label=None):\n    \"\"\"Builds and makes directory with time date and user given label.\n\n    # Arguments:\n        root: String with partial or full path.\n        label: String user label.\n\n    # Returns\n        Full directory path\n    \"\"\"\n    directory_name = build_directory_name(root, label)\n    make_directory(directory_name)\n    return directory_name",
  "def build_directory_name(root, label=None):\n    \"\"\"Build directory name with time date and user label\n\n    # Arguments:\n        root: String with partial or full path.\n        label: String user label.\n\n    # Returns\n        Full directory path\n    \"\"\"\n    directory_name = [datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")]\n    if label is not None:\n        directory_name.extend([label])\n    directory_name = '_'.join(directory_name)\n    return os.path.join(root, directory_name)",
  "def make_directory(directory_name):\n    \"\"\"Makes directory.\n\n    # Arguments:\n        directory_name: String. Directory name.\n    \"\"\"\n    Path(directory_name).mkdir(parents=True, exist_ok=True)",
  "def write_dictionary(dictionary, directory, filename, indent=4):\n    \"\"\"Writes dictionary as json file.\n\n    # Arguments:\n        dictionary: Dictionary to write in memory.\n        directory: String. Directory name.\n        filename: String. Filename.\n        indent: Number of spaces between keys.\n    \"\"\"\n    fielpath = os.path.join(directory, filename)\n    filedata = open(fielpath, 'w')\n    json.dump(dictionary, filedata, indent=indent)",
  "def write_weights(model, directory, name=None):\n    \"\"\"Writes Keras weights in memory.\n\n    # Arguments:\n        model: Keras model.\n        directory: String. Directory name.\n        name: String or `None`. Weights filename.\n    \"\"\"\n    name = model.name if name is None else name\n    weights_path = os.path.join(directory, name + '_weights.hdf5')\n    model.save_weights(weights_path)",
  "def find_path(wildcard):\n    filenames = glob.glob(wildcard)\n    filepaths = []\n    for filename in filenames:\n        if os.path.isdir(filename):\n            filepaths.append(filename)\n    return max(filepaths, key=os.path.getmtime)",
  "def load_latest(wildcard, filename):\n    filepath = find_path(wildcard)\n    filepath = os.path.join(filepath, filename)\n    filedata = open(filepath, 'r')\n    parameters = json.load(filedata)\n    return parameters"
]